# tf.nn.leaky_relu

View source on GitHub  Compute the Leaky ReLU activation function.

#### View aliases

Compat aliases for migration See Migration guide for more details.
`tf.compat.v1.nn.leaky_relu`

    
    tf.nn.leaky_relu(
        features, alpha=0.2, name=None
    )
    
Source: Rectifier Nonlinearities Improve Neural Network Acoustic Models. AL Maas, AY Hannun, AY Ng - Proc. ICML, 2013. Args: features: A `Tensor` representing preactivation values. Must be one of the following types: `float16`, `float32`, `float64`, `int32`, `int64`. alpha: Slope of the activation function at x < 0\. name: A name for the operation (optional). | Returns  
---  
The activation value.  
#### References:

Rectifier Nonlinearities Improve Neural Network Acoustic Models: Maas et al.,
2013 (pdf)

Â© 2020 The TensorFlow Authors. All rights reserved.  
Licensed under the Creative Commons Attribution License 3.0.  
Code samples licensed under the Apache 2.0 License.  
https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/nn/leaky_relu

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

