# Module: tf.distribute.experimental

Public API for tf.distribute.experimental namespace.

## Modules

`coordinator` module: Public API for tf.distribute.experimental.coordinator
namespace.

`partitioners` module: Public API for tf.distribute.experimental.partitioners
namespace.

## Classes

`class CentralStorageStrategy`: A one-machine strategy that puts all variables
on a single device.

`class CollectiveCommunication`: Cross device communication implementation.

`class CollectiveHints`: Hints for collective operations like AllReduce.

`class CommunicationImplementation`: Cross device communication
implementation.

`class CommunicationOptions`: Options for cross device communications like
All-reduce.

`class MultiWorkerMirroredStrategy`: A distribution strategy for synchronous
training on multiple workers.

`class ParameterServerStrategy`: An multi-worker tf.distribute strategy with
parameter servers.

`class TPUStrategy`: Synchronous training on TPUs and TPU Pods.

`class ValueContext`: A class wrapping information needed by a distribute
function.

Â© 2020 The TensorFlow Authors. All rights reserved.  
Licensed under the Creative Commons Attribution License 3.0.  
Code samples licensed under the Apache 2.0 License.  
https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/distribute/experimental

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

