# tf.raw_ops.ApplyProximalAdagrad

Update 'var' and 'accum' according to FOBOS with Adagrad learning rate.

#### View aliases

Compat aliases for migration

See Migration guide for more details.

`tf.compat.v1.raw_ops.ApplyProximalAdagrad`

    
    tf.raw_ops.ApplyProximalAdagrad(
        var, accum, lr, l1, l2, grad, use_locking=False, name=None
    )
    
accum += grad * grad prox_v = var - lr * grad * (1 / sqrt(accum)) var =
sign(prox_v)/(1+lr*l2) * max{|prox_v|-lr*l1,0}

Args  
---  
`var` |  A mutable `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`. Should be from a Variable().   
`accum` |  A mutable `Tensor`. Must have the same type as `var`. Should be from a Variable().   
`lr` |  A `Tensor`. Must have the same type as `var`. Scaling factor. Must be a scalar.   
`l1` |  A `Tensor`. Must have the same type as `var`. L1 regularization. Must be a scalar.   
`l2` |  A `Tensor`. Must have the same type as `var`. L2 regularization. Must be a scalar.   
`grad` |  A `Tensor`. Must have the same type as `var`. The gradient.   
`use_locking` |  An optional `bool`. Defaults to `False`. If True, updating of the var and accum tensors will be protected by a lock; otherwise the behavior is undefined, but may exhibit less contention.   
`name` |  A name for the operation (optional).   
Returns  
---  
A mutable `Tensor`. Has the same type as `var`.  
Â© 2020 The TensorFlow Authors. All rights reserved.  
Licensed under the Creative Commons Attribution License 3.0.  
Code samples licensed under the Apache 2.0 License.  
https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/raw_ops/ApplyProximalAdagrad

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

