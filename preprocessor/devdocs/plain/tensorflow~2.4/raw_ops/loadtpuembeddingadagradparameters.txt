# tf.raw_ops.LoadTPUEmbeddingAdagradParameters

Load Adagrad embedding parameters.

#### View aliases

Compat aliases for migration

See Migration guide for more details.

`tf.compat.v1.raw_ops.LoadTPUEmbeddingAdagradParameters`

    
    tf.raw_ops.LoadTPUEmbeddingAdagradParameters(
        parameters, accumulators, num_shards, shard_id, table_id=-1,
        table_name='', config='', name=None
    )
    
An op that loads optimization parameters into HBM for embedding. Must be
preceded by a ConfigureTPUEmbeddingHost op that sets up the correct embedding
table configuration. For example, this op is used to install parameters that
are loaded from a checkpoint before a training loop is executed.

Args  
---  
`parameters` |  A `Tensor` of type `float32`. Value of parameters used in the Adagrad optimization algorithm.   
`accumulators` |  A `Tensor` of type `float32`. Value of accumulators used in the Adagrad optimization algorithm.   
`num_shards` |  An `int`.   
`shard_id` |  An `int`.   
`table_id` |  An optional `int`. Defaults to `-1`.   
`table_name` |  An optional `string`. Defaults to `""`.   
`config` |  An optional `string`. Defaults to `""`.   
`name` |  A name for the operation (optional).   
Returns  
---  
The created Operation.  
Â© 2020 The TensorFlow Authors. All rights reserved.  
Licensed under the Creative Commons Attribution License 3.0.  
Code samples licensed under the Apache 2.0 License.  
https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/raw_ops/LoadTPUEmbeddingAdagradParameters

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

