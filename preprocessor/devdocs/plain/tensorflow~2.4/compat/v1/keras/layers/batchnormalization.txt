# tf.compat.v1.keras.layers.BatchNormalization

Layer that normalizes its inputs.

Inherits From: `Layer`, `Module`

    
    tf.compat.v1.keras.layers.BatchNormalization(
        axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,
        beta_initializer='zeros', gamma_initializer='ones',
        moving_mean_initializer='zeros',
        moving_variance_initializer='ones', beta_regularizer=None,
        gamma_regularizer=None, beta_constraint=None, gamma_constraint=None,
        renorm=False, renorm_clipping=None, renorm_momentum=0.99, fused=None,
        trainable=True, virtual_batch_size=None, adjustment=None, name=None, **kwargs
    )
    
Batch normalization applies a transformation that maintains the mean output
close to 0 and the output standard deviation close to 1.

Importantly, batch normalization works differently during training and during
inference.

During training (i.e. when using `fit()` or when calling the layer/model with
the argument `training=True`), the layer normalizes its output using the mean
and standard deviation of the current batch of inputs. That is to say, for
each channel being normalized, the layer returns `(batch - mean(batch)) /
(var(batch) + epsilon) * gamma + beta`, where:

  * `epsilon` is small constant (configurable as part of the constructor arguments)
  * `gamma` is a learned scaling factor (initialized as 1), which can be disabled by passing `scale=False` to the constructor.
  * `beta` is a learned offset factor (initialized as 0), which can be disabled by passing `center=False` to the constructor.

During inference (i.e. when using `evaluate()` or `predict()` or when calling
the layer/model with the argument `training=False` (which is the default), the
layer normalizes its output using a moving average of the mean and standard
deviation of the batches it has seen during training. That is to say, it
returns `(batch - self.moving_mean) / (self.moving_var + epsilon) * gamma +
beta`.

`self.moving_mean` and `self.moving_var` are non-trainable variables that are
updated each time the layer in called in training mode, as such:

  * `moving_mean = moving_mean * momentum + mean(batch) * (1 - momentum)`
  * `moving_var = moving_var * momentum + var(batch) * (1 - momentum)`

As such, the layer will only normalize its inputs during inference after
having been trained on data that has similar statistics as the inference data.

Arguments  
---  
`axis` |  Integer or a list of integers, the axis that should be normalized (typically the features axis). For instance, after a `Conv2D` layer with `data_format="channels_first"`, set `axis=1` in `BatchNormalization`.   
`momentum` |  Momentum for the moving average.   
`epsilon` |  Small float added to variance to avoid dividing by zero.   
`center` |  If True, add offset of `beta` to normalized tensor. If False, `beta` is ignored.   
`scale` |  If True, multiply by `gamma`. If False, `gamma` is not used. When the next layer is linear (also e.g. `nn.relu`), this can be disabled since the scaling will be done by the next layer.   
`beta_initializer` |  Initializer for the beta weight.   
`gamma_initializer` |  Initializer for the gamma weight.   
`moving_mean_initializer` |  Initializer for the moving mean.   
`moving_variance_initializer` |  Initializer for the moving variance.   
`beta_regularizer` |  Optional regularizer for the beta weight.   
`gamma_regularizer` |  Optional regularizer for the gamma weight.   
`beta_constraint` |  Optional constraint for the beta weight.   
`gamma_constraint` |  Optional constraint for the gamma weight.   
`renorm` |  Whether to use Batch Renormalization. This adds extra variables during training. The inference is the same for either value of this parameter.   
`renorm_clipping` |  A dictionary that may map keys 'rmax', 'rmin', 'dmax' to scalar `Tensors` used to clip the renorm correction. The correction `(r, d)` is used as `corrected_value = normalized_value * r + d`, with `r` clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin, dmax are set to inf, 0, inf, respectively.   
`renorm_momentum` |  Momentum used to update the moving means and standard deviations with renorm. Unlike `momentum`, this affects training and should be neither too small (which would add noise) nor too large (which would give stale estimates). Note that `momentum` is still applied to get the means and variances for inference.   
`fused` |  if `None` or `True`, use a faster, fused implementation if possible. If `False`, use the system recommended implementation.   
`trainable` |  Boolean, if `True` the variables will be marked as trainable.   
`virtual_batch_size` |  An `int`. By default, `virtual_batch_size` is `None`, which means batch normalization is performed across the whole batch. When `virtual_batch_size` is not `None`, instead perform "Ghost Batch Normalization", which creates virtual sub-batches which are each normalized separately (with shared gamma, beta, and moving statistics). Must divide the actual batch size during execution.   
`adjustment` |  A function taking the `Tensor` containing the (dynamic) shape of the input tensor and returning a pair (scale, bias) to apply to the normalized values (before gamma and beta), only during training. For example, if axis==-1, `adjustment = lambda shape: ( tf.random.uniform(shape[-1:], 0.93, 1.07), tf.random.uniform(shape[-1:], -0.1, 0.1))` will scale the normalized value by up to 7% up or down, then shift the result by up to 0.1 (with independent scaling and bias for each feature but shared across all examples), and finally apply gamma and/or beta. If `None`, no adjustment is applied. Cannot be specified if virtual_batch_size is specified.   
#### Call arguments:

  * `inputs`: Input tensor (of any rank).
  * `training`: Python boolean indicating whether the layer should behave in training mode or in inference mode. 
    * `training=True`: The layer will normalize its inputs using the mean and variance of the current batch of inputs.
    * `training=False`: The layer will normalize its inputs using the mean and variance of its moving statistics, learned during training.

Input shape: Arbitrary. Use the keyword argument `input_shape` (tuple of
integers, does not include the samples axis) when using this layer as the
first layer in a model.

Output shape: Same shape as input.

#### Reference:

  * Ioffe and Szegedy, 2015.

Â© 2020 The TensorFlow Authors. All rights reserved.  
Licensed under the Creative Commons Attribution License 3.0.  
Code samples licensed under the Apache 2.0 License.  
https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/compat/v1/keras/layers/BatchNormalization

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

