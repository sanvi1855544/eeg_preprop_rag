# Module: tf.compat.v1.distribute.experimental

Public API for tf.distribute.experimental namespace.

## Classes

`class CentralStorageStrategy`: A one-machine strategy that puts all variables
on a single device.

`class CollectiveCommunication`: Cross device communication implementation.

`class CollectiveHints`: Hints for collective operations like AllReduce.

`class CommunicationImplementation`: Cross device communication
implementation.

`class CommunicationOptions`: Options for cross device communications like
All-reduce.

`class MultiWorkerMirroredStrategy`: A distribution strategy for synchronous
training on multiple workers.

`class ParameterServerStrategy`: An asynchronous multi-worker parameter server
tf.distribute strategy.

`class TPUStrategy`: TPU distribution strategy implementation.

Â© 2020 The TensorFlow Authors. All rights reserved.  
Licensed under the Creative Commons Attribution License 3.0.  
Code samples licensed under the Apache 2.0 License.  
https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/compat/v1/distribute/experimental

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

