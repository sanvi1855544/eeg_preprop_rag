# tf.test.is_gpu_available

|  View source on GitHub  Returns whether TensorFlow can access a GPU.
(deprecated)

#### View aliases

Compat aliases for migration See Migration guide for more details.
`tf.compat.v1.test.is_gpu_available`

    
    tf.test.is_gpu_available(
        cuda_only=False, min_cuda_compute_capability=None
    )
    
Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use `tf.config.list_physical_devices('GPU')` instead.Warning: if a non-GPU version of the package is installed, the function would also return False. Use `tf.test.is_built_with_cuda` to validate if TensorFlow was build with CUDA support. | Args  
---  
`cuda_only` |  limit the search to CUDA GPUs.   
`min_cuda_compute_capability` |  a (major,minor) pair that indicates the minimum CUDA compute capability required, or None if no requirement.   
Note that the keyword arg name "cuda_only" is misleading (since routine will
return true when a GPU device is available irrespective of whether TF was
built with CUDA support or ROCm support. However no changes here because

++ Changing the name "cuda_only" to something more generic would break
backward compatibility

++ Adding an equivalent "rocm_only" would require the implementation check the
build type. This in turn would require doing the same for CUDA and thus
potentially break backward compatibility

++ Adding a new "cuda_or_rocm_only" would not break backward compatibility,
but would require most (if not all) callers to update the call to use
"cuda_or_rocm_only" instead of "cuda_only"

Returns  
---  
True if a GPU device of the requested kind is available.  
Â© 2020 The TensorFlow Authors. All rights reserved.  
Licensed under the Creative Commons Attribution License 3.0.  
Code samples licensed under the Apache 2.0 License.  
https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/test/is_gpu_available

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

