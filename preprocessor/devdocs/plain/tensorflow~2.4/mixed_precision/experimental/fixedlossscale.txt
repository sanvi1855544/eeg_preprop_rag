# tf.mixed_precision.experimental.FixedLossScale

Loss scale with a fixed value.

Inherits From: `LossScale`

#### View aliases

Main aliases

`tf.train.experimental.FixedLossScale`

Compat aliases for migration

See Migration guide for more details.

`tf.compat.v1.mixed_precision.FixedLossScale`,
`tf.compat.v1.mixed_precision.experimental.FixedLossScale`,
`tf.compat.v1.train.experimental.FixedLossScale`

    
    tf.mixed_precision.experimental.FixedLossScale(
        loss_scale_value
    )
    
Warning: This class is deprecated and will be unexposed from the TF 2
namespace starting in TensorFlow 2.5. In TensorFlow 2.5, this class will only
be accessible as `tf.compat.v1.mixed_precision.FixedLossScale`. Additionally
in 2.5, you will no longer be able to pass a `FixedLossScale` to a
`tf.keras.mixed_precision.Policy`. All the functionality in this class has
been merged into `tf.keras.mixed_precision.LossScaleOptimizer`, so this class
is no longer needed.

The loss scale is not updated for the lifetime of instances of this class. A
given instance of this class always returns the same number when called.

Args  
---  
`loss_scale_value` |  A Python float. Its ideal value varies depending on models to run. Choosing a too small loss_scale might affect model quality; a too big loss_scale might cause inf or nan. There is no single right loss_scale to apply. There is no harm choosing a relatively big number as long as no nan or inf is encountered in training.   
Raises  
---  
`ValueError` |  If loss_scale_value is less than 1.   
## Methods

### `from_config`

View source

    
    @classmethod
    from_config(
        config
    )
    
Creates the LossScale from its config.

### `get_config`

View source

    
    get_config()
    
Returns the config of this loss scale.

### `update`

View source

    
    update(
        grads
    )
    
Updates the value of the loss scale.

The loss scale will be potentially updated, based on the value of `grads`. The
tensor returned by calling this class is only updated when this function is
evaluated.

In eager mode, this directly updates the loss scale, so that calling
`__call__` will return the newly updated loss scale. In graph mode, this
returns an op that, when evaluated, updates the loss scale.

This function also returns a `should_apply_gradients` bool. If False,
gradients should not be applied to the variables that step, as nonfinite
gradients were found, and the loss scale has been be updated to reduce the
chance of finding nonfinite gradients in the next step. Some loss scale
classes will always return True, as they cannot adjust themselves in response
to nonfinite gradients.

When a DistributionStrategy is used, this function may only be called in a
cross-replica context.

Args  
---  
`grads` |  A nested structure of unscaled gradients, each which is the gradient of the loss with respect to a weight. The gradients should have already been divided by the loss scale being before passed to this function. 'None' gradients are accepted, and are ignored.   
Returns  
---  
`update_op` |  In eager mode, None. In graph mode, an op to update the loss scale.   
`should_apply_gradients` |  Either a bool or a scalar boolean tensor. If False, the caller should skip applying `grads` to the variables this step.   
### `__call__`

View source

    
    __call__()
    
Returns the current loss scale as a scalar `float32` tensor.

Â© 2020 The TensorFlow Authors. All rights reserved.  
Licensed under the Creative Commons Attribution License 3.0.  
Code samples licensed under the Apache 2.0 License.  
https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/mixed_precision/experimental/FixedLossScale

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

