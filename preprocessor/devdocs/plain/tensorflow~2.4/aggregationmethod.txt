# tf.AggregationMethod

View source on GitHub  A class listing aggregation methods used to combine
gradients.

#### View aliases

Compat aliases for migration See Migration guide for more details.
`tf.compat.v1.AggregationMethod` Computing partial derivatives can require
aggregating gradient contributions. This class lists the various methods that
can be used to combine gradients in the graph. The following aggregation
methods are part of the stable API for aggregating gradients:

  * `ADD_N`: All of the gradient terms are summed as part of one operation using the "AddN" op (see `tf.add_n`). This method has the property that all gradients must be ready and buffered separately in memory before any aggregation is performed.
  * `DEFAULT`: The system-chosen default aggregation method.

The following aggregation methods are experimental and may not be supported in
future releases:

  * `EXPERIMENTAL_TREE`: Gradient terms are summed in pairs using using the "AddN" op. This method of summing gradients may reduce performance, but it can improve memory utilization because the gradients can be released earlier.

| Class Variables  
---  
ADD_N  |  `0`  
DEFAULT  |  `0`  
EXPERIMENTAL_ACCUMULATE_N  |  `2`  
EXPERIMENTAL_TREE  |  `1`  
Â© 2020 The TensorFlow Authors. All rights reserved.  
Licensed under the Creative Commons Attribution License 3.0.  
Code samples licensed under the Apache 2.0 License.  
https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/AggregationMethod

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

