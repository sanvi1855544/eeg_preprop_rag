# Module: tf.data.experimental

Experimental API for building input pipelines.

This module contains experimental `Dataset` sources and transformations that
can be used in conjunction with the `tf.data.Dataset` API. Note that the
`tf.data.experimental` API is not subject to the same backwards compatibility
guarantees as `tf.data`, but we will provide deprecation advice in advance of
removing existing functionality.

See Importing Data for an overview.

## Modules

`service` module: API for using the tf.data service.

## Classes

`class AutoShardPolicy`: Represents the type of auto-sharding we enable.

`class CheckpointInputPipelineHook`: Checkpoints input pipeline state every N
steps or seconds.

`class CsvDataset`: A Dataset comprising lines from one or more CSV files.

`class DistributeOptions`: Represents options for distributed data processing.

`class MapVectorizationOptions`: Represents options for the MapVectorization
optimization.

`class OptimizationOptions`: Represents options for dataset optimizations.

`class Optional`: Represents a value that may or may not be present.

`class RandomDataset`: A `Dataset` of pseudorandom values.

`class Reducer`: A reducer is used for reducing a set of elements.

`class SqlDataset`: A `Dataset` consisting of the results from a SQL query.

`class StatsAggregator`: A stateful resource that aggregates statistics from
one or more iterators.

`class StatsOptions`: Represents options for collecting dataset stats using
`StatsAggregator`.

`class TFRecordWriter`: Writes a dataset to a TFRecord file.

`class ThreadingOptions`: Represents options for dataset threading.

## Functions

`Counter(...)`: Creates a `Dataset` that counts from `start` in steps of size
`step`.

`assert_cardinality(...)`: Asserts the cardinality of the input dataset.

`bucket_by_sequence_length(...)`: A transformation that buckets elements in a
`Dataset` by length.

`bytes_produced_stats(...)`: Records the number of bytes produced by each
element of the input dataset.

`cardinality(...)`: Returns the cardinality of `dataset`, if known.

`choose_from_datasets(...)`: Creates a dataset that deterministically chooses
elements from `datasets`.

`copy_to_device(...)`: A transformation that copies dataset elements to the
given `target_device`.

`dense_to_ragged_batch(...)`: A transformation that batches ragged elements
into `tf.RaggedTensor`s.

`dense_to_sparse_batch(...)`: A transformation that batches ragged elements
into `tf.sparse.SparseTensor`s.

`enumerate_dataset(...)`: A transformation that enumerates the elements of a
dataset. (deprecated)

`from_variant(...)`: Constructs a dataset from the given variant and
structure.

`get_next_as_optional(...)`: Returns a `tf.experimental.Optional` with the
next element of the iterator. (deprecated)

`get_single_element(...)`: Returns the single element in `dataset` as a nested
structure of tensors.

`get_structure(...)`: Returns the type signature for elements of the input
dataset / iterator.

`group_by_reducer(...)`: A transformation that groups elements and performs a
reduction.

`group_by_window(...)`: A transformation that groups windows of elements by
key and reduces them.

`ignore_errors(...)`: Creates a `Dataset` from another `Dataset` and silently
ignores any errors.

`latency_stats(...)`: Records the latency of producing each element of the
input dataset.

`load(...)`: Loads a previously saved dataset.

`make_batched_features_dataset(...)`: Returns a `Dataset` of feature
dictionaries from `Example` protos.

`make_csv_dataset(...)`: Reads CSV files into a dataset.

`make_saveable_from_iterator(...)`: Returns a SaveableObject for
saving/restoring iterator state using Saver. (deprecated)

`map_and_batch(...)`: Fused implementation of `map` and `batch`. (deprecated)

`parallel_interleave(...)`: A parallel version of the `Dataset.interleave()`
transformation. (deprecated)

`parse_example_dataset(...)`: A transformation that parses `Example` protos
into a `dict` of tensors.

`prefetch_to_device(...)`: A transformation that prefetches dataset values to
the given `device`.

`rejection_resample(...)`: A transformation that resamples a dataset to
achieve a target distribution.

`sample_from_datasets(...)`: Samples elements at random from the datasets in
`datasets`.

`save(...)`: Saves the content of the given dataset.

`scan(...)`: A transformation that scans a function across an input dataset.

`shuffle_and_repeat(...)`: Shuffles and repeats a Dataset, reshuffling with
each repetition. (deprecated)

`snapshot(...)`: API to persist the output of the input dataset.

`take_while(...)`: A transformation that stops dataset iteration based on a
`predicate`.

`to_variant(...)`: Returns a variant representing the given dataset.

`unbatch(...)`: Splits elements of a dataset into multiple elements on the
batch dimension. (deprecated)

`unique(...)`: Creates a `Dataset` from another `Dataset`, discarding
duplicates.

Other Members  
---  
AUTOTUNE  |  `-1`  
INFINITE_CARDINALITY  |  `-1`  
UNKNOWN_CARDINALITY  |  `-2`  
Â© 2020 The TensorFlow Authors. All rights reserved.  
Licensed under the Creative Commons Attribution License 3.0.  
Code samples licensed under the Apache 2.0 License.  
https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/data/experimental

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

