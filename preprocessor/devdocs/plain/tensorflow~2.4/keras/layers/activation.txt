# tf.keras.layers.Activation

View source on GitHub  Applies an activation function to an output. Inherits
From: `Layer`, `Module`

#### View aliases

Compat aliases for migration See Migration guide for more details.
`tf.compat.v1.keras.layers.Activation`

    
    tf.keras.layers.Activation(
        activation, **kwargs
    )
    
| Arguments  
---  
`activation` |  Activation function, such as `tf.nn.relu`, or string name of built-in activation function, such as "relu".   
#### Usage:

    
    layer = tf.keras.layers.Activation('relu')
    output = layer([-3.0, -1.0, 0.0, 2.0])
    list(output.numpy())
    [0.0, 0.0, 0.0, 2.0]
    layer = tf.keras.layers.Activation(tf.nn.relu)
    output = layer([-3.0, -1.0, 0.0, 2.0])
    list(output.numpy())
    [0.0, 0.0, 0.0, 2.0]
    
#### Input shape:

Arbitrary. Use the keyword argument `input_shape` (tuple of integers, does not
include the batch axis) when using this layer as the first layer in a model.

#### Output shape:

Same shape as input.

Â© 2020 The TensorFlow Authors. All rights reserved.  
Licensed under the Creative Commons Attribution License 3.0.  
Code samples licensed under the Apache 2.0 License.  
https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/layers/Activation

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

