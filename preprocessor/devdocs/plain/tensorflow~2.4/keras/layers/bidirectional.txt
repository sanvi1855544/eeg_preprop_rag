# tf.keras.layers.Bidirectional

View source on GitHub  Bidirectional wrapper for RNNs. Inherits From:
`Wrapper`, `Layer`, `Module`

#### View aliases

Compat aliases for migration See Migration guide for more details.
`tf.compat.v1.keras.layers.Bidirectional`

    
    tf.keras.layers.Bidirectional(
        layer, merge_mode='concat', weights=None, backward_layer=None,
        **kwargs
    )
    
| Arguments  
---  
`layer` |  `keras.layers.RNN` instance, such as `keras.layers.LSTM` or `keras.layers.GRU`. It could also be a `keras.layers.Layer` instance that meets the following criteria: 
  1. Be a sequence-processing layer (accepts 3D+ inputs).
  2. Have a `go_backwards`, `return_sequences` and `return_state` attribute (with the same semantics as for the `RNN` class).
  3. Have an `input_spec` attribute.
  4. Implement serialization via `get_config()` and `from_config()`. Note that the recommended way to create new RNN layers is to write a custom RNN cell and use it with `keras.layers.RNN`, instead of subclassing `keras.layers.Layer` directly. 

  
`merge_mode` |  Mode by which outputs of the forward and backward RNNs will be combined. One of {'sum', 'mul', 'concat', 'ave', None}. If None, the outputs will not be combined, they will be returned as a list. Default value is 'concat'.   
`backward_layer` |  Optional `keras.layers.RNN`, or `keras.layers.Layer` instance to be used to handle backwards input processing. If `backward_layer` is not provided, the layer instance passed as the `layer` argument will be used to generate the backward layer automatically. Note that the provided `backward_layer` layer should have properties matching those of the `layer` argument, in particular it should have the same values for `stateful`, `return_states`, `return_sequence`, etc. In addition, `backward_layer` and `layer` should have different `go_backwards` argument values. A `ValueError` will be raised if these requirements are not met.   
#### Call arguments:

The call arguments for this layer are the same as those of the wrapped RNN
layer. Beware that when passing the `initial_state` argument during the call
of this layer, the first half in the list of elements in the `initial_state`
list will be passed to the forward RNN call and the last half in the list of
elements will be passed to the backward RNN call.

Raises  
---  
`ValueError` | 
  1. If `layer` or `backward_layer` is not a `Layer` instance.
  2. In case of invalid `merge_mode` argument.
  3. If `backward_layer` has mismatched properties compared to `layer`. 

  
#### Examples:

    
    model = Sequential()
    model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10)))
    model.add(Bidirectional(LSTM(10)))
    model.add(Dense(5))
    model.add(Activation('softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
    
     # With custom backward layer
     model = Sequential()
     forward_layer = LSTM(10, return_sequences=True)
     backward_layer = LSTM(10, activation='relu', return_sequences=True,
                           go_backwards=True)
     model.add(Bidirectional(forward_layer, backward_layer=backward_layer,
                             input_shape=(5, 10)))
     model.add(Dense(5))
     model.add(Activation('softmax'))
     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
    
Attributes  
---  
`constraints` |   
## Methods

### `reset_states`

View source

    
    reset_states()
    
Â© 2020 The TensorFlow Authors. All rights reserved.  
Licensed under the Creative Commons Attribution License 3.0.  
Code samples licensed under the Apache 2.0 License.  
https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/layers/Bidirectional

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

