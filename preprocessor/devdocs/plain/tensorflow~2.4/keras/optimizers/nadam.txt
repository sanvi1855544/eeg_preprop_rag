# tf.keras.optimizers.Nadam

View source on GitHub  Optimizer that implements the NAdam algorithm. Inherits
From: `Optimizer`

#### View aliases

Main aliases `tf.optimizers.Nadam` Compat aliases for migration See Migration
guide for more details. `tf.compat.v1.keras.optimizers.Nadam`

    
    tf.keras.optimizers.Nadam(
        learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07,
        name='Nadam', **kwargs
    )
    
Much like Adam is essentially RMSprop with momentum, Nadam is Adam with Nesterov momentum. | Args  
---  
`learning_rate` |  A Tensor or a floating point value. The learning rate.   
`beta_1` |  A float value or a constant float tensor. The exponential decay rate for the 1st moment estimates.   
`beta_2` |  A float value or a constant float tensor. The exponential decay rate for the exponentially weighted infinity norm.   
`epsilon` |  A small constant for numerical stability.   
`name` |  Optional name for the operations created when applying gradients. Defaults to `"Nadam"`.   
`**kwargs` |  Keyword arguments. Allowed to be one of `"clipnorm"` or `"clipvalue"`. `"clipnorm"` (float) clips gradients by norm; `"clipvalue"` (float) clips gradients by value.   
#### Reference:

  * Dozat, 2015.

Raises  
---  
`ValueError` |  in case of any invalid argument.   
Â© 2020 The TensorFlow Authors. All rights reserved.  
Licensed under the Creative Commons Attribution License 3.0.  
Code samples licensed under the Apache 2.0 License.  
https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/optimizers/Nadam

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

