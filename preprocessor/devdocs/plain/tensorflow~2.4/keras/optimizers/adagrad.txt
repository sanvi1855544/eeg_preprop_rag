# tf.keras.optimizers.Adagrad

View source on GitHub  Optimizer that implements the Adagrad algorithm.
Inherits From: `Optimizer`

#### View aliases

Main aliases `tf.optimizers.Adagrad` Compat aliases for migration See
Migration guide for more details. `tf.compat.v1.keras.optimizers.Adagrad`

    
    tf.keras.optimizers.Adagrad(
        learning_rate=0.001, initial_accumulator_value=0.1, epsilon=1e-07,
        name='Adagrad', **kwargs
    )
    
Adagrad is an optimizer with parameter-specific learning rates, which are adapted relative to how frequently a parameter gets updated during training. The more updates a parameter receives, the smaller the updates. | Args  
---  
`learning_rate` |  A `Tensor`, floating point value, or a schedule that is a `tf.keras.optimizers.schedules.LearningRateSchedule`. The learning rate.   
`initial_accumulator_value` |  A floating point value. Starting value for the accumulators, must be non-negative.   
`epsilon` |  A small floating point value to avoid zero denominator.   
`name` |  Optional name prefix for the operations created when applying gradients. Defaults to `"Adagrad"`.   
`**kwargs` |  Keyword arguments. Allowed to be one of `"clipnorm"` or `"clipvalue"`. `"clipnorm"` (float) clips gradients by norm; `"clipvalue"` (float) clips gradients by value.   
#### Reference:

  * Duchi et al., 2011.

Raises  
---  
`ValueError` |  in case of any invalid argument.   
Â© 2020 The TensorFlow Authors. All rights reserved.  
Licensed under the Creative Commons Attribution License 3.0.  
Code samples licensed under the Apache 2.0 License.  
https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/optimizers/Adagrad

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

