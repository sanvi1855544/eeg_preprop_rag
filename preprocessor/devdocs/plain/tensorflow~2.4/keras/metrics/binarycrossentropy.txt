# tf.keras.metrics.BinaryCrossentropy

View source on GitHub  Computes the crossentropy metric between the labels and
predictions. Inherits From: `Mean`, `Metric`, `Layer`, `Module`

#### View aliases

Main aliases `tf.metrics.BinaryCrossentropy` Compat aliases for migration See
Migration guide for more details.
`tf.compat.v1.keras.metrics.BinaryCrossentropy`

    
    tf.keras.metrics.BinaryCrossentropy(
        name='binary_crossentropy', dtype=None, from_logits=False,
        label_smoothing=0
    )
    
This is the crossentropy metric class to be used when there are only two label classes (0 and 1). | Args  
---  
`name` |  (Optional) string name of the metric instance.   
`dtype` |  (Optional) data type of the metric result.   
`from_logits` |  (Optional )Whether output is expected to be a logits tensor. By default, we consider that output encodes a probability distribution.   
`label_smoothing` |  (Optional) Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`".   
#### Standalone usage:

    
    m = tf.keras.metrics.BinaryCrossentropy()
    m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
    m.result().numpy()
    0.81492424
    
    
    m.reset_states()
    m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],
                   sample_weight=[1, 0])
    m.result().numpy()
    0.9162905
    
Usage with `compile()` API:

    
    model.compile(
        optimizer='sgd',
        loss='mse',
        metrics=[tf.keras.metrics.BinaryCrossentropy()])
    
## Methods

### `reset_states`

View source

    
    reset_states()
    
Resets all of the metric state variables.

This function is called between epochs/steps, when a metric is evaluated
during training.

### `result`

View source

    
    result()
    
Computes and returns the metric value tensor.

Result computation is an idempotent operation that simply calculates the
metric value using the state variables.

### `update_state`

View source

    
    update_state(
        y_true, y_pred, sample_weight=None
    )
    
Accumulates metric statistics.

`y_true` and `y_pred` should have the same shape.

Args  
---  
`y_true` |  Ground truth values. shape = `[batch_size, d0, .. dN]`.   
`y_pred` |  The predicted values. shape = `[batch_size, d0, .. dN]`.   
`sample_weight` |  Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)).   
Returns  
---  
Update op.  
Â© 2020 The TensorFlow Authors. All rights reserved.  
Licensed under the Creative Commons Attribution License 3.0.  
Code samples licensed under the Apache 2.0 License.  
https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/metrics/BinaryCrossentropy

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

