# tf.keras.metrics.CategoricalCrossentropy

View source on GitHub  Computes the crossentropy metric between the labels and
predictions. Inherits From: `Mean`, `Metric`, `Layer`, `Module`

#### View aliases

Main aliases `tf.metrics.CategoricalCrossentropy` Compat aliases for migration
See Migration guide for more details.
`tf.compat.v1.keras.metrics.CategoricalCrossentropy`

    
    tf.keras.metrics.CategoricalCrossentropy(
        name='categorical_crossentropy', dtype=None, from_logits=False,
        label_smoothing=0
    )
    
This is the crossentropy metric class to be used when there are multiple label classes (2 or more). Here we assume that labels are given as a `one_hot` representation. eg., When labels values are [2, 0, 1], `y_true` = [[0, 0, 1], [1, 0, 0], [0, 1, 0]]. | Args  
---  
`name` |  (Optional) string name of the metric instance.   
`dtype` |  (Optional) data type of the metric result.   
`from_logits` |  (Optional) Whether output is expected to be a logits tensor. By default, we consider that output encodes a probability distribution.   
`label_smoothing` |  (Optional) Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`"   
#### Standalone usage:

    
    # EPSILON = 1e-7, y = y_true, y` = y_pred
    # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)
    # y` = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]
    # xent = -sum(y * log(y'), axis = -1)
    #      = -((log 0.95), (log 0.1))
    #      = [0.051, 2.302]
    # Reduced xent = (0.051 + 2.302) / 2
    m = tf.keras.metrics.CategoricalCrossentropy()
    m.update_state([[0, 1, 0], [0, 0, 1]],
                   [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])
    m.result().numpy()
    1.1769392
    
    
    m.reset_states()
    m.update_state([[0, 1, 0], [0, 0, 1]],
                   [[0.05, 0.95, 0], [0.1, 0.8, 0.1]],
                   sample_weight=tf.constant([0.3, 0.7]))
    m.result().numpy()
    1.6271976
    
Usage with `compile()` API:

    
    model.compile(
      optimizer='sgd',
      loss='mse',
      metrics=[tf.keras.metrics.CategoricalCrossentropy()])
    
## Methods

### `reset_states`

View source

    
    reset_states()
    
Resets all of the metric state variables.

This function is called between epochs/steps, when a metric is evaluated
during training.

### `result`

View source

    
    result()
    
Computes and returns the metric value tensor.

Result computation is an idempotent operation that simply calculates the
metric value using the state variables.

### `update_state`

View source

    
    update_state(
        y_true, y_pred, sample_weight=None
    )
    
Accumulates metric statistics.

`y_true` and `y_pred` should have the same shape.

Args  
---  
`y_true` |  Ground truth values. shape = `[batch_size, d0, .. dN]`.   
`y_pred` |  The predicted values. shape = `[batch_size, d0, .. dN]`.   
`sample_weight` |  Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)).   
Returns  
---  
Update op.  
Â© 2020 The TensorFlow Authors. All rights reserved.  
Licensed under the Creative Commons Attribution License 3.0.  
Code samples licensed under the Apache 2.0 License.  
https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/metrics/CategoricalCrossentropy

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

