# torch.nn.quantized.dynamic

## Linear

`class torch.nn.quantized.dynamic.Linear(in_features, out_features,
bias_=True, dtype=torch.qint8)` [source]

    
A dynamic quantized linear module with floating point tensor as inputs and
outputs. We adopt the same interface as `torch.nn.Linear`, please see
https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.

Similar to `torch.nn.Linear`, attributes will be randomly initialized at
module creation time and will be overwritten later

Variables

    
  * ~Linear.weight (Tensor) – the non-learnable quantized weights of the module which are of shape (out_features,in_features)(\text{out\\_features}, \text{in\\_features}) .
  * ~Linear.bias (Tensor) – the non-learnable floating point bias of the module of shape (out_features)(\text{out\\_features}) . If `bias` is `True`, the values are initialized to zero.

Examples:

    
    >>> m = nn.quantized.dynamic.Linear(20, 30)
    >>> input = torch.randn(128, 20)
    >>> output = m(input)
    >>> print(output.size())
    torch.Size([128, 30])
    
`classmethod from_float(mod)` [source]

    
Create a dynamic quantized module from a float module or qparams_dict

Parameters

    
mod (Module) – a float module, either produced by torch.quantization utilities
or provided by the user

## LSTM

`class torch.nn.quantized.dynamic.LSTM(*args, **kwargs)` [source]

    
A dynamic quantized LSTM module with floating point tensor as inputs and
outputs. We adopt the same interface as `torch.nn.LSTM`, please see
https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM for documentation.

Examples:

    
    >>> rnn = nn.LSTM(10, 20, 2)
    >>> input = torch.randn(5, 3, 10)
    >>> h0 = torch.randn(2, 3, 20)
    >>> c0 = torch.randn(2, 3, 20)
    >>> output, (hn, cn) = rnn(input, (h0, c0))
    
## LSTMCell

`class torch.nn.quantized.dynamic.LSTMCell(*args, **kwargs)` [source]

    
A long short-term memory (LSTM) cell.

A dynamic quantized LSTMCell module with floating point tensor as inputs and
outputs. Weights are quantized to 8 bits. We adopt the same interface as
`torch.nn.LSTMCell`, please see
https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell for documentation.

Examples:

    
    >>> rnn = nn.LSTMCell(10, 20)
    >>> input = torch.randn(6, 3, 10)
    >>> hx = torch.randn(3, 20)
    >>> cx = torch.randn(3, 20)
    >>> output = []
    >>> for i in range(6):
            hx, cx = rnn(input[i], (hx, cx))
            output.append(hx)
    
## GRUCell

`class torch.nn.quantized.dynamic.GRUCell(input_size, hidden_size, bias=True,
dtype=torch.qint8)` [source]

    
A gated recurrent unit (GRU) cell

A dynamic quantized GRUCell module with floating point tensor as inputs and
outputs. Weights are quantized to 8 bits. We adopt the same interface as
`torch.nn.GRUCell`, please see
https://pytorch.org/docs/stable/nn.html#torch.nn.GRUCell for documentation.

Examples:

    
    >>> rnn = nn.GRUCell(10, 20)
    >>> input = torch.randn(6, 3, 10)
    >>> hx = torch.randn(3, 20)
    >>> output = []
    >>> for i in range(6):
            hx = rnn(input[i], hx)
            output.append(hx)
    
## RNNCell

`class torch.nn.quantized.dynamic.RNNCell(input_size, hidden_size, bias=True,
nonlinearity='tanh', dtype=torch.qint8)` [source]

    
An Elman RNN cell with tanh or ReLU non-linearity. A dynamic quantized RNNCell
module with floating point tensor as inputs and outputs. Weights are quantized
to 8 bits. We adopt the same interface as `torch.nn.RNNCell`, please see
https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell for documentation.

Examples:

    
    >>> rnn = nn.RNNCell(10, 20)
    >>> input = torch.randn(6, 3, 10)
    >>> hx = torch.randn(3, 20)
    >>> output = []
    >>> for i in range(6):
            hx = rnn(input[i], hx)
            output.append(hx)
    
© 2019 Torch Contributors  
Licensed under the 3-clause BSD License.  
https://pytorch.org/docs/1.8.0/torch.nn.quantized.dynamic.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

