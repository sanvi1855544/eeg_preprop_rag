# BatchNorm2d

`class torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1,
affine=True, track_running_stats=True)` [source]

    
Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with
additional channel dimension) as described in the paper Batch Normalization:
Accelerating Deep Network Training by Reducing Internal Covariate Shift .

y=x−E[x]Var[x]+ϵ∗γ+βy = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] +
\epsilon}} * \gamma + \beta

The mean and standard-deviation are calculated per-dimension over the mini-
batches and γ\gamma and β\beta are learnable parameter vectors of size `C`
(where `C` is the input size). By default, the elements of γ\gamma are set to
1 and the elements of β\beta are set to 0. The standard-deviation is
calculated via the biased estimator, equivalent to `torch.var(input,
unbiased=False)`.

Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default `momentum` of 0.1.

If `track_running_stats` is set to `False`, this layer then does not keep
running estimates, and batch statistics are instead used during evaluation
time as well.

Note

This `momentum` argument is different from one used in optimizer classes and
the conventional notion of momentum. Mathematically, the update rule for
running statistics here is x^new=(1−momentum)×x^+momentum×xt\hat{x}_\text{new}
= (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t , where
x^\hat{x} is the estimated statistic and xtx_t is the new observed value.

Because the Batch Normalization is done over the `C` dimension, computing
statistics on `(N, H, W)` slices, it’s common terminology to call this Spatial
Batch Normalization.

Parameters

    
  * num_features – CC from an expected input of size (N,C,H,W)(N, C, H, W)
  * eps – a value added to the denominator for numerical stability. Default: 1e-5
  * momentum – the value used for the running_mean and running_var computation. Can be set to `None` for cumulative moving average (i.e. simple average). Default: 0.1
  * affine – a boolean value that when set to `True`, this module has learnable affine parameters. Default: `True`
  * track_running_stats – a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics, and initializes statistics buffers `running_mean` and `running_var` as `None`. When these buffers are `None`, this module always uses batch statistics. in both training and eval modes. Default: `True`

Shape:

    
  * Input: (N,C,H,W)(N, C, H, W)
  * Output: (N,C,H,W)(N, C, H, W) (same shape as input)

Examples:

    
    >>> # With Learnable Parameters
    >>> m = nn.BatchNorm2d(100)
    >>> # Without Learnable Parameters
    >>> m = nn.BatchNorm2d(100, affine=False)
    >>> input = torch.randn(20, 100, 35, 45)
    >>> output = m(input)
    
© 2019 Torch Contributors  
Licensed under the 3-clause BSD License.  
https://pytorch.org/docs/1.8.0/generated/torch.nn.BatchNorm2d.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

