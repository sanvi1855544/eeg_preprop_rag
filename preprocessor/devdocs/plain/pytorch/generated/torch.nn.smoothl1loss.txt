# SmoothL1Loss

`class torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean',
beta=1.0)` [source]

    
Creates a criterion that uses a squared term if the absolute element-wise
error falls below beta and an L1 term otherwise. It is less sensitive to
outliers than the `torch.nn.MSELoss` and in some cases prevents exploding
gradients (e.g. see `Fast R-CNN` paper by Ross Girshick). Omitting a scaling
factor of `beta`, this loss is also known as the Huber loss:

loss(x,y)=1n∑izi\text{loss}(x, y) = \frac{1}{n} \sum_{i} z_{i}

where ziz_{i} is given by:

zi={0.5(xi−yi)2/beta,if ∣xi−yi∣<beta∣xi−yi∣−0.5∗beta,otherwise z_{i} =
\begin{cases} 0.5 (x_i - y_i)^2 / beta, & \text{if } |x_i - y_i| < beta \\\
|x_i - y_i| - 0.5 * beta, & \text{otherwise } \end{cases}

xx and yy arbitrary shapes with a total of nn elements each the sum operation
still operates over all the elements, and divides by nn .

`beta` is an optional parameter that defaults to 1.

Note: When `beta` is set to 0, this is equivalent to `L1Loss`. Passing a
negative value in for `beta` will result in an exception.

The division by nn can be avoided if sets `reduction = 'sum'`.

Parameters

    
  * size_average (bool, optional) – Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when `reduce` is `False`. Default: `True`
  * reduce (bool, optional) – Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`
  * reduction (string, optional) – Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`
  * beta (float, optional) – Specifies the threshold at which to change between L1 and L2 loss. This value defaults to 1.0.

Shape:

    
  * Input: (N,∗)(N, *) where ∗* means, any number of additional dimensions
  * Target: (N,∗)(N, *) , same shape as the input
  * Output: scalar. If `reduction` is `'none'`, then (N,∗)(N, *) , same shape as the input

© 2019 Torch Contributors  
Licensed under the 3-clause BSD License.  
https://pytorch.org/docs/1.8.0/generated/torch.nn.SmoothL1Loss.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

