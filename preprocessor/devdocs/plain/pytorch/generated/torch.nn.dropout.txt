# Dropout

`class torch.nn.Dropout(p=0.5, inplace=False)` [source]

    
During training, randomly zeroes some of the elements of the input tensor with
probability `p` using samples from a Bernoulli distribution. Each channel will
be zeroed out independently on every forward call.

This has proven to be an effective technique for regularization and preventing
the co-adaptation of neurons as described in the paper Improving neural
networks by preventing co-adaptation of feature detectors .

Furthermore, the outputs are scaled by a factor of 11−p\frac{1}{1-p} during
training. This means that during evaluation the module simply computes an
identity function.

Parameters

    
  * p – probability of an element to be zeroed. Default: 0.5
  * inplace – If set to `True`, will do this operation in-place. Default: `False`

Shape:

    
  * Input: (∗)(*) . Input can be of any shape
  * Output: (∗)(*) . Output is of the same shape as input

Examples:

    
    >>> m = nn.Dropout(p=0.2)
    >>> input = torch.randn(20, 16)
    >>> output = m(input)
    
© 2019 Torch Contributors  
Licensed under the 3-clause BSD License.  
https://pytorch.org/docs/1.8.0/generated/torch.nn.Dropout.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

