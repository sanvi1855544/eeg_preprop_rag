# no_grad

`class torch.no_grad` [source]

    
Context-manager that disabled gradient calculation.

Disabling gradient calculation is useful for inference, when you are sure that
you will not call `Tensor.backward()`. It will reduce memory consumption for
computations that would otherwise have `requires_grad=True`.

In this mode, the result of every computation will have `requires_grad=False`,
even when the inputs have `requires_grad=True`.

This context manager is thread local; it will not affect computation in other
threads.

Also functions as a decorator. (Make sure to instantiate with parenthesis.)

Example:

    
    >>> x = torch.tensor([1], requires_grad=True)
    >>> with torch.no_grad():
    ...   y = x * 2
    >>> y.requires_grad
    False
    >>> @torch.no_grad()
    ... def doubler(x):
    ...     return x * 2
    >>> z = doubler(x)
    >>> z.requires_grad
    False
    
Â© 2019 Torch Contributors  
Licensed under the 3-clause BSD License.  
https://pytorch.org/docs/1.8.0/generated/torch.no_grad.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

