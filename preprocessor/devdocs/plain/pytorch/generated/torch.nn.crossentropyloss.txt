# CrossEntropyLoss

`class torch.nn.CrossEntropyLoss(weight=None, size_average=None,
ignore_index=-100, reduce=None, reduction='mean')` [source]

    
This criterion combines `LogSoftmax` and `NLLLoss` in one single class.

It is useful when training a classification problem with `C` classes. If
provided, the optional argument `weight` should be a 1D `Tensor` assigning
weight to each of the classes. This is particularly useful when you have an
unbalanced training set.

The `input` is expected to contain raw, unnormalized scores for each class.

`input` has to be a Tensor of size either (minibatch,C)(minibatch, C) or
(minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K) with K≥1K \geq 1
for the `K`-dimensional case (described later).

This criterion expects a class index in the range [0,C−1][0, C-1] as the
`target` for each value of a 1D tensor of size `minibatch`; if `ignore_index`
is specified, this criterion also accepts this class index (this index may not
necessarily be in the class range).

The loss can be described as:

loss(x,class)=−log⁡(exp⁡(x[class])∑jexp⁡(x[j]))=−x[class]+log⁡(∑jexp⁡(x[j]))\text{loss}(x,
class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right) =
-x[class] + \log\left(\sum_j \exp(x[j])\right)

or in the case of the `weight` argument being specified:

loss(x,class)=weight[class](−x[class]+log⁡(∑jexp⁡(x[j])))\text{loss}(x, class)
= weight[class] \left(-x[class] + \log\left(\sum_j \exp(x[j])\right)\right)

The losses are averaged across observations for each minibatch. If the
`weight` argument is specified then this is a weighted average:

loss=∑i=1Nloss(i,class[i])∑i=1Nweight[class[i]]\text{loss} =
\frac{\sum^{N}_{i=1} loss(i, class[i])}{\sum^{N}_{i=1} weight[class[i]]}

Can also be used for higher dimension inputs, such as 2D images, by providing
an input of size (minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K)
with K≥1K \geq 1 , where KK is the number of dimensions, and a target of
appropriate shape (see below).

Parameters

    
  * weight (Tensor, optional) – a manual rescaling weight given to each class. If given, has to be a Tensor of size `C`
  * size_average (bool, optional) – Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when `reduce` is `False`. Default: `True`
  * ignore_index (int, optional) – Specifies a target value that is ignored and does not contribute to the input gradient. When `size_average` is `True`, the loss is averaged over non-ignored targets.
  * reduce (bool, optional) – Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`
  * reduction (string, optional) – Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the weighted mean of the output is taken, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`

Shape:

    
  * Input: (N,C)(N, C) where `C = number of classes`, or (N,C,d1,d2,...,dK)(N, C, d_1, d_2, ..., d_K) with K≥1K \geq 1 in the case of `K`-dimensional loss.
  * Target: (N)(N) where each value is 0≤targets[i]≤C−10 \leq \text{targets}[i] \leq C-1 , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K) with K≥1K \geq 1 in the case of K-dimensional loss.
  * Output: scalar. If `reduction` is `'none'`, then the same size as the target: (N)(N) , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K) with K≥1K \geq 1 in the case of K-dimensional loss.

Examples:

    
    >>> loss = nn.CrossEntropyLoss()
    >>> input = torch.randn(3, 5, requires_grad=True)
    >>> target = torch.empty(3, dtype=torch.long).random_(5)
    >>> output = loss(input, target)
    >>> output.backward()
    
© 2019 Torch Contributors  
Licensed under the 3-clause BSD License.  
https://pytorch.org/docs/1.8.0/generated/torch.nn.CrossEntropyLoss.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

