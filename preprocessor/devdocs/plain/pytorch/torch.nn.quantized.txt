# torch.nn.quantized

This module implements the quantized versions of the nn modules and
functionals.

## Functional interface

Functional interface (quantized).

`torch.nn.quantized.functional.linear(input, weight, bias=None, scale=None,
zero_point=None)` [source]

    
Applies a linear transformation to the incoming quantized data: y=xAT+by =
xA^T + b . See `Linear`

Note

Current implementation packs weights on every call, which has penalty on
performance. If you want to avoid the overhead, use `Linear`.

Parameters

    
  * input (Tensor) – Quantized input of type `torch.quint8`
  * weight (Tensor) – Quantized weight of type `torch.qint8`
  * bias (Tensor) – None or fp32 bias of type `torch.float`
  * scale (double) – output scale. If None, derived from the input scale
  * zero_point (long) – output zero point. If None, derived from the input zero_point

Shape:

    
  * Input: (N,∗,in_features)(N, *, in\\_features) where `*` means any number of additional dimensions
  * Weight: (out_features,in_features)(out\\_features, in\\_features)
  * Bias: (out_features)(out\\_features)
  * Output: (N,∗,out_features)(N, *, out\\_features)

`torch.nn.quantized.functional.conv1d(input, weight, bias, stride=1,
padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0,
zero_point=0, dtype=torch.quint8)` [source]

    
Applies a 1D convolution over a quantized 1D input composed of several input
planes.

See `Conv1d` for details and output shape.

Parameters

    
  * input – quantized input tensor of shape (minibatch,in_channels,iW)(\text{minibatch} , \text{in\\_channels} , iW)
  * weight – quantized filters of shape (out_channels,in_channelsgroups,iW)(\text{out\\_channels} , \frac{\text{in\\_channels}}{\text{groups}} , iW)
  * bias – non-quantized bias tensor of shape (out_channels)(\text{out\\_channels}) . The tensor type must be `torch.float`.
  * stride – the stride of the convolving kernel. Can be a single number or a tuple `(sW,)`. Default: 1
  * padding – implicit paddings on both sides of the input. Can be a single number or a tuple `(padW,)`. Default: 0
  * dilation – the spacing between kernel elements. Can be a single number or a tuple `(dW,)`. Default: 1
  * groups – split input into groups, in_channels\text{in\\_channels} should be divisible by the number of groups. Default: 1
  * padding_mode – the padding mode to use. Only “zeros” is supported for quantized convolution at the moment. Default: “zeros”
  * scale – quantization scale for the output. Default: 1.0
  * zero_point – quantization zero_point for the output. Default: 0
  * dtype – quantization data type to use. Default: `torch.quint8`

Examples:

    
    >>> from torch.nn.quantized import functional as qF
    >>> filters = torch.randn(33, 16, 3, dtype=torch.float)
    >>> inputs = torch.randn(20, 16, 50, dtype=torch.float)
    >>> bias = torch.randn(33, dtype=torch.float)
    >>>
    >>> scale, zero_point = 1.0, 0
    >>> dtype_inputs = torch.quint8
    >>> dtype_filters = torch.qint8
    >>>
    >>> q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype_filters)
    >>> q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype_inputs)
    >>> qF.conv1d(q_inputs, q_filters, bias, padding=1, scale=scale, zero_point=zero_point)
    
`torch.nn.quantized.functional.conv2d(input, weight, bias, stride=1,
padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0,
zero_point=0, dtype=torch.quint8)` [source]

    
Applies a 2D convolution over a quantized 2D input composed of several input
planes.

See `Conv2d` for details and output shape.

Parameters

    
  * input – quantized input tensor of shape (minibatch,in_channels,iH,iW)(\text{minibatch} , \text{in\\_channels} , iH , iW)
  * weight – quantized filters of shape (out_channels,in_channelsgroups,kH,kW)(\text{out\\_channels} , \frac{\text{in\\_channels}}{\text{groups}} , kH , kW)
  * bias – non-quantized bias tensor of shape (out_channels)(\text{out\\_channels}) . The tensor type must be `torch.float`.
  * stride – the stride of the convolving kernel. Can be a single number or a tuple `(sH, sW)`. Default: 1
  * padding – implicit paddings on both sides of the input. Can be a single number or a tuple `(padH, padW)`. Default: 0
  * dilation – the spacing between kernel elements. Can be a single number or a tuple `(dH, dW)`. Default: 1
  * groups – split input into groups, in_channels\text{in\\_channels} should be divisible by the number of groups. Default: 1
  * padding_mode – the padding mode to use. Only “zeros” is supported for quantized convolution at the moment. Default: “zeros”
  * scale – quantization scale for the output. Default: 1.0
  * zero_point – quantization zero_point for the output. Default: 0
  * dtype – quantization data type to use. Default: `torch.quint8`

Examples:

    
    >>> from torch.nn.quantized import functional as qF
    >>> filters = torch.randn(8, 4, 3, 3, dtype=torch.float)
    >>> inputs = torch.randn(1, 4, 5, 5, dtype=torch.float)
    >>> bias = torch.randn(8, dtype=torch.float)
    >>>
    >>> scale, zero_point = 1.0, 0
    >>> dtype_inputs = torch.quint8
    >>> dtype_filters = torch.qint8
    >>>
    >>> q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype_filters)
    >>> q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype_inputs)
    >>> qF.conv2d(q_inputs, q_filters, bias, padding=1, scale=scale, zero_point=zero_point)
    
`torch.nn.quantized.functional.conv3d(input, weight, bias, stride=1,
padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0,
zero_point=0, dtype=torch.quint8)` [source]

    
Applies a 3D convolution over a quantized 3D input composed of several input
planes.

See `Conv3d` for details and output shape.

Parameters

    
  * input – quantized input tensor of shape (minibatch,in_channels,iD,iH,iW)(\text{minibatch} , \text{in\\_channels} , iD , iH , iW)
  * weight – quantized filters of shape (out_channels,in_channelsgroups,kD,kH,kW)(\text{out\\_channels} , \frac{\text{in\\_channels}}{\text{groups}} , kD , kH , kW)
  * bias – non-quantized bias tensor of shape (out_channels)(\text{out\\_channels}) . The tensor type must be `torch.float`.
  * stride – the stride of the convolving kernel. Can be a single number or a tuple `(sD, sH, sW)`. Default: 1
  * padding – implicit paddings on both sides of the input. Can be a single number or a tuple `(padD, padH, padW)`. Default: 0
  * dilation – the spacing between kernel elements. Can be a single number or a tuple `(dD, dH, dW)`. Default: 1
  * groups – split input into groups, in_channels\text{in\\_channels} should be divisible by the number of groups. Default: 1
  * padding_mode – the padding mode to use. Only “zeros” is supported for quantized convolution at the moment. Default: “zeros”
  * scale – quantization scale for the output. Default: 1.0
  * zero_point – quantization zero_point for the output. Default: 0
  * dtype – quantization data type to use. Default: `torch.quint8`

Examples:

    
    >>> from torch.nn.quantized import functional as qF
    >>> filters = torch.randn(8, 4, 3, 3, 3, dtype=torch.float)
    >>> inputs = torch.randn(1, 4, 5, 5, 5, dtype=torch.float)
    >>> bias = torch.randn(8, dtype=torch.float)
    >>>
    >>> scale, zero_point = 1.0, 0
    >>> dtype_inputs = torch.quint8
    >>> dtype_filters = torch.qint8
    >>>
    >>> q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype_filters)
    >>> q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype_inputs)
    >>> qF.conv3d(q_inputs, q_filters, bias, padding=1, scale=scale, zero_point=zero_point)
    
`torch.nn.quantized.functional.max_pool2d(input, kernel_size, stride=None,
padding=0, dilation=1, ceil_mode=False, return_indices=False)` [source]

    
Applies a 2D max pooling over a quantized input signal composed of several
quantized input planes.

Note

The input quantization parameters are propagated to the output.

See `MaxPool2d` for details.

`torch.nn.quantized.functional.adaptive_avg_pool2d(input, output_size)`
[source]

    
Applies a 2D adaptive average pooling over a quantized input signal composed
of several quantized input planes.

Note

The input quantization parameters propagate to the output.

See `AdaptiveAvgPool2d` for details and output shape.

Parameters

    
output_size – the target output size (single integer or double-integer tuple)

`torch.nn.quantized.functional.avg_pool2d(input, kernel_size, stride=None,
padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)`
[source]

    
Applies 2D average-pooling operation in kH×kWkH \times kW regions by step size
sH×sWsH \times sW steps. The number of output features is equal to the number
of input planes.

Note

The input quantization parameters propagate to the output.

See `AvgPool2d` for details and output shape.

Parameters

    
  * input – quantized input tensor (minibatch,in_channels,iH,iW)(\text{minibatch} , \text{in\\_channels} , iH , iW)
  * kernel_size – size of the pooling region. Can be a single number or a tuple `(kH, kW)`
  * stride – stride of the pooling operation. Can be a single number or a tuple `(sH, sW)`. Default: `kernel_size`
  * padding – implicit zero paddings on both sides of the input. Can be a single number or a tuple `(padH, padW)`. Default: 0
  * ceil_mode – when True, will use `ceil` instead of `floor` in the formula to compute the output shape. Default: `False`
  * count_include_pad – when True, will include the zero-padding in the averaging calculation. Default: `True`
  * divisor_override – if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None

`torch.nn.quantized.functional.interpolate(input, size=None,
scale_factor=None, mode='nearest', align_corners=None)` [source]

    
Down/up samples the input to either the given `size` or the given
`scale_factor`

See `torch.nn.functional.interpolate()` for implementation details.

The input dimensions are interpreted in the form: `mini-batch x channels x
[optional depth] x [optional height] x width`.

Note

The input quantization parameters propagate to the output.

Note

Only 2D/3D input is supported for quantized inputs

Note

Only the following modes are supported for the quantized inputs:

  * `bilinear`
  * `nearest`

Parameters

    
  * input (Tensor) – the input tensor
  * size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]) – output spatial size.
  * scale_factor (float or Tuple[float]) – multiplier for spatial size. Has to match input size if it is a tuple.
  * mode (str) – algorithm used for upsampling: `'nearest'` | `'bilinear'`
  * align_corners (bool, optional) – Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`

`torch.nn.quantized.functional.hardswish(input, scale, zero_point)` [source]

    
This is the quantized version of `hardswish()`.

Parameters

    
  * input – quantized input
  * scale – quantization scale of the output tensor
  * zero_point – quantization zero point of the output tensor

`torch.nn.quantized.functional.upsample(input, size=None, scale_factor=None,
mode='nearest', align_corners=None)` [source]

    
Upsamples the input to either the given `size` or the given `scale_factor`

Warning

This function is deprecated in favor of
`torch.nn.quantized.functional.interpolate()`. This is equivalent with
`nn.quantized.functional.interpolate(...)`.

See `torch.nn.functional.interpolate()` for implementation details.

The input dimensions are interpreted in the form: `mini-batch x channels x
[optional depth] x [optional height] x width`.

Note

The input quantization parameters propagate to the output.

Note

Only 2D input is supported for quantized inputs

Note

Only the following modes are supported for the quantized inputs:

  * `bilinear`
  * `nearest`

Parameters

    
  * input (Tensor) – quantized input tensor
  * size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]) – output spatial size.
  * scale_factor (float or Tuple[float]) – multiplier for spatial size. Has to be an integer.
  * mode (string) – algorithm used for upsampling: `'nearest'` | `'bilinear'`
  * align_corners (bool, optional) – Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`

Warning

With `align_corners = True`, the linearly interpolating modes (`bilinear`)
don’t proportionally align the output and input pixels, and thus the output
values can depend on the input size. This was the default behavior for these
modes up to version 0.3.1. Since then, the default behavior is `align_corners
= False`. See `Upsample` for concrete examples on how this affects the
outputs.

`torch.nn.quantized.functional.upsample_bilinear(input, size=None,
scale_factor=None)` [source]

    
Upsamples the input, using bilinear upsampling.

Warning

This function is deprecated in favor of
`torch.nn.quantized.functional.interpolate()`. This is equivalent with
`nn.quantized.functional.interpolate(..., mode='bilinear',
align_corners=True)`.

Note

The input quantization parameters propagate to the output.

Note

Only 2D inputs are supported

Parameters

    
  * input (Tensor) – quantized input
  * size (int or Tuple[int, int]) – output spatial size.
  * scale_factor (int or Tuple[int, int]) – multiplier for spatial size

`torch.nn.quantized.functional.upsample_nearest(input, size=None,
scale_factor=None)` [source]

    
Upsamples the input, using nearest neighbours’ pixel values.

Warning

This function is deprecated in favor of
`torch.nn.quantized.functional.interpolate()`. This is equivalent with
`nn.quantized.functional.interpolate(..., mode='nearest')`.

Note

The input quantization parameters propagate to the output.

Note

Only 2D inputs are supported

Parameters

    
  * input (Tensor) – quantized input
  * size (int or Tuple[int, int] or Tuple[int, int, int]) – output spatial size.
  * scale_factor (int) – multiplier for spatial size. Has to be an integer.

## ReLU6

`class torch.nn.quantized.ReLU6(inplace=False)` [source]

    
Applies the element-wise function:

ReLU6(x)=min⁡(max⁡(x0,x),q(6))\text{ReLU6}(x) = \min(\max(x_0, x), q(6)) ,
where x0x_0 is the zero_point, and q(6)q(6) is the quantized representation of
number 6.

Parameters

    
inplace – can optionally do the operation in-place. Default: `False`

Shape:

    
  * Input: (N,∗)(N, *) where `*` means, any number of additional dimensions
  * Output: (N,∗)(N, *) , same shape as the input

![_images/ReLU61.png](data:image/png;base64,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)

Examples:

    
    >>> m = nn.quantized.ReLU6()
    >>> input = torch.randn(2)
    >>> input = torch.quantize_per_tensor(input, 1.0, 0, dtype=torch.qint32)
    >>> output = m(input)
    
## ELU

`class torch.nn.quantized.ELU(scale, zero_point, alpha=1.0)` [source]

    
This is the quantized equivalent of `ELU`.

Parameters

    
  * scale – quantization scale of the output tensor
  * zero_point – quantization zero point of the output tensor
  * alpha – the alpha constant

## Hardswish

`class torch.nn.quantized.Hardswish(scale, zero_point)` [source]

    
This is the quantized version of `Hardswish`.

Parameters

    
  * scale – quantization scale of the output tensor
  * zero_point – quantization zero point of the output tensor

## Conv1d

`class torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size,
stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')`
[source]

    
Applies a 1D convolution over a quantized input signal composed of several
quantized input planes.

For details on input arguments, parameters, and implementation see `Conv1d`.

Note

Only `zeros` is supported for the `padding_mode` argument.

Note

Only `torch.quint8` is supported for the input data type.

Variables

    
  * ~Conv1d.weight (Tensor) – packed tensor derived from the learnable weight parameter.
  * ~Conv1d.scale (Tensor) – scalar for the output scale
  * ~Conv1d.zero_point (Tensor) – scalar for the output zero point

See `Conv1d` for other attributes.

Examples:

    
    >>> m = nn.quantized.Conv1d(16, 33, 3, stride=2)
    >>> input = torch.randn(20, 16, 100)
    >>> # quantize input to quint8
    >>> q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0,
                                            dtype=torch.quint8)
    >>> output = m(q_input)
    
`classmethod from_float(mod)` [source]

    
Creates a quantized module from a float module or qparams_dict.

Parameters

    
mod (Module) – a float module, either produced by torch.quantization utilities
or provided by the user

## Conv2d

`class torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size,
stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')`
[source]

    
Applies a 2D convolution over a quantized input signal composed of several
quantized input planes.

For details on input arguments, parameters, and implementation see `Conv2d`.

Note

Only `zeros` is supported for the `padding_mode` argument.

Note

Only `torch.quint8` is supported for the input data type.

Variables

    
  * ~Conv2d.weight (Tensor) – packed tensor derived from the learnable weight parameter.
  * ~Conv2d.scale (Tensor) – scalar for the output scale
  * ~Conv2d.zero_point (Tensor) – scalar for the output zero point

See `Conv2d` for other attributes.

Examples:

    
    >>> # With square kernels and equal stride
    >>> m = nn.quantized.Conv2d(16, 33, 3, stride=2)
    >>> # non-square kernels and unequal stride and with padding
    >>> m = nn.quantized.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
    >>> # non-square kernels and unequal stride and with padding and dilation
    >>> m = nn.quantized.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))
    >>> input = torch.randn(20, 16, 50, 100)
    >>> # quantize input to quint8
    >>> q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)
    >>> output = m(q_input)
    
`classmethod from_float(mod)` [source]

    
Creates a quantized module from a float module or qparams_dict.

Parameters

    
mod (Module) – a float module, either produced by torch.quantization utilities
or provided by the user

## Conv3d

`class torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size,
stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')`
[source]

    
Applies a 3D convolution over a quantized input signal composed of several
quantized input planes.

For details on input arguments, parameters, and implementation see `Conv3d`.

Note

Only `zeros` is supported for the `padding_mode` argument.

Note

Only `torch.quint8` is supported for the input data type.

Variables

    
  * ~Conv3d.weight (Tensor) – packed tensor derived from the learnable weight parameter.
  * ~Conv3d.scale (Tensor) – scalar for the output scale
  * ~Conv3d.zero_point (Tensor) – scalar for the output zero point

See `Conv3d` for other attributes.

Examples:

    
    >>> # With square kernels and equal stride
    >>> m = nn.quantized.Conv3d(16, 33, 3, stride=2)
    >>> # non-square kernels and unequal stride and with padding
    >>> m = nn.quantized.Conv3d(16, 33, (3, 5, 5), stride=(1, 2, 2), padding=(1, 2, 2))
    >>> # non-square kernels and unequal stride and with padding and dilation
    >>> m = nn.quantized.Conv3d(16, 33, (3, 5, 5), stride=(1, 2, 2), padding=(1, 2, 2), dilation=(1, 2, 2))
    >>> input = torch.randn(20, 16, 56, 56, 56)
    >>> # quantize input to quint8
    >>> q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)
    >>> output = m(q_input)
    
`classmethod from_float(mod)` [source]

    
Creates a quantized module from a float module or qparams_dict.

Parameters

    
mod (Module) – a float module, either produced by torch.quantization utilities
or provided by the user

## FloatFunctional

`class torch.nn.quantized.FloatFunctional` [source]

    
State collector class for float operations.

The instance of this class can be used instead of the `torch.` prefix for some
operations. See example usage below.

Note

This class does not provide a `forward` hook. Instead, you must use one of the
underlying functions (e.g. `add`).

Examples:

    
    >>> f_add = FloatFunctional()
    >>> a = torch.tensor(3.0)
    >>> b = torch.tensor(4.0)
    >>> f_add.add(a, b)  # Equivalent to ``torch.add(a, b)``
    
Valid operation names:

    
  * add
  * cat
  * mul
  * add_relu
  * add_scalar
  * mul_scalar

## QFunctional

`class torch.nn.quantized.QFunctional` [source]

    
Wrapper class for quantized operations.

The instance of this class can be used instead of the `torch.ops.quantized`
prefix. See example usage below.

Note

This class does not provide a `forward` hook. Instead, you must use one of the
underlying functions (e.g. `add`).

Examples:

    
    >>> q_add = QFunctional()
    >>> a = torch.quantize_per_tensor(torch.tensor(3.0), 1.0, 0, torch.qint32)
    >>> b = torch.quantize_per_tensor(torch.tensor(4.0), 1.0, 0, torch.qint32)
    >>> q_add.add(a, b)  # Equivalent to ``torch.ops.quantized.add(a, b, 1.0, 0)``
    
Valid operation names:

    
  * add
  * cat
  * mul
  * add_relu
  * add_scalar
  * mul_scalar

## Quantize

`class torch.nn.quantized.Quantize(scale, zero_point, dtype)` [source]

    
Quantizes an incoming tensor

Parameters

    
  * scale – scale of the output Quantized Tensor
  * zero_point – zero_point of output Quantized Tensor
  * dtype – data type of output Quantized Tensor

Variables

    
zero_point, dtype (`scale`,) –

Examples::

    
    
    >>> t = torch.tensor([[1., -1.], [1., -1.]])
    >>> scale, zero_point, dtype = 1.0, 2, torch.qint8
    >>> qm = Quantize(scale, zero_point, dtype)
    >>> qt = qm(t)
    >>> print(qt)
    tensor([[ 1., -1.],
            [ 1., -1.]], size=(2, 2), dtype=torch.qint8, scale=1.0, zero_point=2)
    
## DeQuantize

`class torch.nn.quantized.DeQuantize` [source]

    
Dequantizes an incoming tensor

Examples::

    
    
    >>> input = torch.tensor([[1., -1.], [1., -1.]])
    >>> scale, zero_point, dtype = 1.0, 2, torch.qint8
    >>> qm = Quantize(scale, zero_point, dtype)
    >>> quantized_input = qm(input)
    >>> dqm = DeQuantize()
    >>> dequantized = dqm(quantized_input)
    >>> print(dequantized)
    tensor([[ 1., -1.],
            [ 1., -1.]], dtype=torch.float32)
    
## Linear

`class torch.nn.quantized.Linear(in_features, out_features, bias_=True,
dtype=torch.qint8)` [source]

    
A quantized linear module with quantized tensor as inputs and outputs. We
adopt the same interface as `torch.nn.Linear`, please see
https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.

Similar to `Linear`, attributes will be randomly initialized at module
creation time and will be overwritten later

Variables

    
  * ~Linear.weight (Tensor) – the non-learnable quantized weights of the module of shape (out_features,in_features)(\text{out\\_features}, \text{in\\_features}) .
  * ~Linear.bias (Tensor) – the non-learnable bias of the module of shape (out_features)(\text{out\\_features}) . If `bias` is `True`, the values are initialized to zero.
  * ~Linear.scale – `scale` parameter of output Quantized Tensor, type: double
  * ~Linear.zero_point – `zero_point` parameter for output Quantized Tensor, type: long

Examples:

    
    >>> m = nn.quantized.Linear(20, 30)
    >>> input = torch.randn(128, 20)
    >>> input = torch.quantize_per_tensor(input, 1.0, 0, torch.quint8)
    >>> output = m(input)
    >>> print(output.size())
    torch.Size([128, 30])
    
`classmethod from_float(mod)` [source]

    
Create a quantized module from a float module or qparams_dict

Parameters

    
mod (Module) – a float module, either produced by torch.quantization utilities
or provided by the user

## BatchNorm2d

`class torch.nn.quantized.BatchNorm2d(num_features, eps=1e-05, momentum=0.1)`
[source]

    
This is the quantized version of `BatchNorm2d`.

## BatchNorm3d

`class torch.nn.quantized.BatchNorm3d(num_features, eps=1e-05, momentum=0.1)`
[source]

    
This is the quantized version of `BatchNorm3d`.

## LayerNorm

`class torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale,
zero_point, eps=1e-05, elementwise_affine=True)` [source]

    
This is the quantized version of `LayerNorm`.

Additional args:

    
  * scale \- quantization scale of the output, type: double.
  * zero_point \- quantization zero point of the output, type: long.

## GroupNorm

`class torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias,
scale, zero_point, eps=1e-05, affine=True)` [source]

    
This is the quantized version of `GroupNorm`.

Additional args:

    
  * scale \- quantization scale of the output, type: double.
  * zero_point \- quantization zero point of the output, type: long.

## InstanceNorm1d

`class torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale,
zero_point, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)`
[source]

    
This is the quantized version of `InstanceNorm1d`.

Additional args:

    
  * scale \- quantization scale of the output, type: double.
  * zero_point \- quantization zero point of the output, type: long.

## InstanceNorm2d

`class torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale,
zero_point, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)`
[source]

    
This is the quantized version of `InstanceNorm2d`.

Additional args:

    
  * scale \- quantization scale of the output, type: double.
  * zero_point \- quantization zero point of the output, type: long.

## InstanceNorm3d

`class torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale,
zero_point, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)`
[source]

    
This is the quantized version of `InstanceNorm3d`.

Additional args:

    
  * scale \- quantization scale of the output, type: double.
  * zero_point \- quantization zero point of the output, type: long.

## Embedding

`class torch.nn.quantized.Embedding(num_embeddings, embedding_dim,
padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False,
sparse=False, _weight=None, dtype=torch.quint8)` [source]

    
A quantized Embedding module with quantized packed weights as inputs. We adopt
the same interface as `torch.nn.Embedding`, please see
https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding for documentation.

Similar to `Embedding`, attributes will be randomly initialized at module
creation time and will be overwritten later

Variables

    
~Embedding.weight (Tensor) – the non-learnable quantized weights of the module
of shape (num_embeddings,embedding_dim)(\text{num\\_embeddings},
\text{embedding\\_dim}) .

Examples::

    
    
    >>> m = nn.quantized.Embedding(num_embeddings=10, embedding_dim=12)
    >>> indices = torch.tensor([9, 6, 5, 7, 8, 8, 9, 2, 8])
    >>> output = m(indices)
    >>> print(output.size())
    torch.Size([9, 12]
    
`classmethod from_float(mod)` [source]

    
Create a quantized embedding module from a float module

Parameters

    
mod (Module) – a float module, either produced by torch.quantization utilities
or provided by user

## EmbeddingBag

`class torch.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim,
max_norm=None, norm_type=2.0, scale_grad_by_freq=False, mode='sum',
sparse=False, _weight=None, include_last_offset=False, dtype=torch.quint8)`
[source]

    
A quantized EmbeddingBag module with quantized packed weights as inputs. We
adopt the same interface as `torch.nn.EmbeddingBag`, please see
https://pytorch.org/docs/stable/nn.html#torch.nn.EmbeddingBag for
documentation.

Similar to `EmbeddingBag`, attributes will be randomly initialized at module
creation time and will be overwritten later

Variables

    
~EmbeddingBag.weight (Tensor) – the non-learnable quantized weights of the
module of shape (num_embeddings,embedding_dim)(\text{num\\_embeddings},
\text{embedding\\_dim}) .

Examples::

    
    
    >>> m = nn.quantized.EmbeddingBag(num_embeddings=10, embedding_dim=12, include_last_offset=True, mode='sum')
    >>> indices = torch.tensor([9, 6, 5, 7, 8, 8, 9, 2, 8, 6, 6, 9, 1, 6, 8, 8, 3, 2, 3, 6, 3, 6, 5, 7, 0, 8, 4, 6, 5, 8, 2, 3])
    >>> offsets = torch.tensor([0, 19, 20, 28, 28, 32])
    >>> output = m(indices, offsets)
    >>> print(output.size())
    torch.Size([5, 12]
    
`classmethod from_float(mod)` [source]

    
Create a quantized embedding_bag module from a float module

Parameters

    
mod (Module) – a float module, either produced by torch.quantization utilities
or provided by user

© 2019 Torch Contributors  
Licensed under the 3-clause BSD License.  
https://pytorch.org/docs/1.8.0/torch.nn.quantized.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

