# torch.backends

`torch.backends` controls the behavior of various backends that PyTorch
supports.

These backends include:

  * `torch.backends.cuda`
  * `torch.backends.cudnn`
  * `torch.backends.mkl`
  * `torch.backends.mkldnn`
  * `torch.backends.openmp`

## torch.backends.cuda

`torch.backends.cuda.is_built()` [source]

    
Returns whether PyTorch is built with CUDA support. Note that this doesn’t
necessarily mean CUDA is available; just that if this PyTorch binary were run
a machine with working CUDA drivers and devices, we would be able to use it.

`torch.backends.cuda.matmul.allow_tf32`

    
A `bool` that controls whether TensorFloat-32 tensor cores may be used in
matrix multiplications on Ampere or newer GPUs. See TensorFloat-32(TF32) on
Ampere devices.

`torch.backends.cuda.cufft_plan_cache`

    
`cufft_plan_cache` caches the cuFFT plans

`size`

    
A readonly `int` that shows the number of plans currently in the cuFFT plan
cache.

`max_size`

    
A `int` that controls cache capacity of cuFFT plan.

`clear()`

    
Clears the cuFFT plan cache.

## torch.backends.cudnn

`torch.backends.cudnn.version()` [source]

    
Returns the version of cuDNN

`torch.backends.cudnn.is_available()` [source]

    
Returns a bool indicating if CUDNN is currently available.

`torch.backends.cudnn.enabled`

    
A `bool` that controls whether cuDNN is enabled.

`torch.backends.cudnn.allow_tf32`

    
A `bool` that controls where TensorFloat-32 tensor cores may be used in cuDNN
convolutions on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere
devices.

`torch.backends.cudnn.deterministic`

    
A `bool` that, if True, causes cuDNN to only use deterministic convolution
algorithms. See also `torch.are_deterministic_algorithms_enabled()` and
`torch.use_deterministic_algorithms()`.

`torch.backends.cudnn.benchmark`

    
A `bool` that, if True, causes cuDNN to benchmark multiple convolution
algorithms and select the fastest.

## torch.backends.mkl

`torch.backends.mkl.is_available()` [source]

    
Returns whether PyTorch is built with MKL support.

## torch.backends.mkldnn

`torch.backends.mkldnn.is_available()` [source]

    
Returns whether PyTorch is built with MKL-DNN support.

## torch.backends.openmp

`torch.backends.openmp.is_available()` [source]

    
Returns whether PyTorch is built with OpenMP support.

© 2019 Torch Contributors  
Licensed under the 3-clause BSD License.  
https://pytorch.org/docs/1.8.0/backends.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

