# sklearn.ensemble.StackingClassifier

`class sklearn.ensemble.StackingClassifier(estimators, final_estimator=None,
*, cv=None, stack_method='auto', n_jobs=None, passthrough=False, verbose=0)`
[source]

    
Stack of estimators with a final classifier.

Stacked generalization consists in stacking the output of individual estimator
and use a classifier to compute the final prediction. Stacking allows to use
the strength of each individual estimator by using their output as input of a
final estimator.

Note that `estimators_` are fitted on the full `X` while `final_estimator_` is
trained using cross-validated predictions of the base estimators using
`cross_val_predict`.

Read more in the User Guide.

New in version 0.22.

Parameters

    
`estimatorslist of (str, estimator)`

    
Base estimators which will be stacked together. Each element of the list is
defined as a tuple of string (i.e. name) and an estimator instance. An
estimator can be set to ‘drop’ using `set_params`.

`final_estimatorestimator, default=None`

    
A classifier which will be used to combine the base estimators. The default
classifier is a `LogisticRegression`.

`cvint, cross-validation generator or an iterable, default=None`

    
Determines the cross-validation splitting strategy used in `cross_val_predict`
to train `final_estimator`. Possible inputs for cv are:

  * None, to use the default 5-fold cross validation,
  * integer, to specify the number of folds in a (Stratified) KFold,
  * An object to be used as a cross-validation generator,
  * An iterable yielding train, test splits.

For integer/None inputs, if the estimator is a classifier and y is either
binary or multiclass, `StratifiedKFold` is used. In all other cases, `KFold`
is used.

Refer User Guide for the various cross-validation strategies that can be used
here.

Note

A larger number of split will provide no benefits if the number of training
samples is large enough. Indeed, the training time will increase. `cv` is not
used for model evaluation but for prediction.

`stack_method{‘auto’, ‘predict_proba’, ‘decision_function’, ‘predict’},
default=’auto’`

    
Methods called for each base estimator. It can be:

  * if ‘auto’, it will try to invoke, for each estimator, `'predict_proba'`, `'decision_function'` or `'predict'` in that order.
  * otherwise, one of `'predict_proba'`, `'decision_function'` or `'predict'`. If the method is not implemented by the estimator, it will raise an error.

`n_jobsint, default=None`

    
The number of jobs to run in parallel all `estimators` `fit`. `None` means 1
unless in a `joblib.parallel_backend` context. -1 means using all processors.
See Glossary for more details.

`passthroughbool, default=False`

    
When False, only the predictions of estimators will be used as training data
for `final_estimator`. When True, the `final_estimator` is trained on the
predictions as well as the original training data.

`verboseint, default=0`

    
Verbosity level.

Attributes

    
`classes_ndarray of shape (n_classes,)`

    
Class labels.

`estimators_list of estimators`

    
The elements of the estimators parameter, having been fitted on the training
data. If an estimator has been set to `'drop'`, it will not appear in
`estimators_`.

`named_estimators_Bunch`

    
Attribute to access any fitted sub-estimators by name.

`final_estimator_estimator`

    
The classifier which predicts given the output of `estimators_`.

`stack_method_list of str`

    
The method used by each base estimator.

#### Notes

When `predict_proba` is used by each estimator (i.e. most of the time for
`stack_method='auto'` or specifically for `stack_method='predict_proba'`), The
first column predicted by each estimator will be dropped in the case of a
binary classification problem. Indeed, both feature will be perfectly
collinear.

#### References

`1`

    
Wolpert, David H. “Stacked generalization.” Neural networks 5.2 (1992):
241-259.

#### Examples

    
    >>> from sklearn.datasets import load_iris
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> from sklearn.svm import LinearSVC
    >>> from sklearn.linear_model import LogisticRegression
    >>> from sklearn.preprocessing import StandardScaler
    >>> from sklearn.pipeline import make_pipeline
    >>> from sklearn.ensemble import StackingClassifier
    >>> X, y = load_iris(return_X_y=True)
    >>> estimators = [
    ...     ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),
    ...     ('svr', make_pipeline(StandardScaler(),
    ...                           LinearSVC(random_state=42)))
    ... ]
    >>> clf = StackingClassifier(
    ...     estimators=estimators, final_estimator=LogisticRegression()
    ... )
    >>> from sklearn.model_selection import train_test_split
    >>> X_train, X_test, y_train, y_test = train_test_split(
    ...     X, y, stratify=y, random_state=42
    ... )
    >>> clf.fit(X_train, y_train).score(X_test, y_test)
    0.9...
    
#### Methods

`decision_function`(X) | Predict decision function for samples in X using `final_estimator_.decision_function`.  
---|---  
`fit`(X, y[, sample_weight]) | Fit the estimators.  
`fit_transform`(X[, y]) | Fit to data, then transform it.  
`get_params`([deep]) | Get the parameters of an estimator from the ensemble.  
`predict`(X, **predict_params) | Predict target for X.  
`predict_proba`(X) | Predict class probabilities for X using `final_estimator_.predict_proba`.  
`score`(X, y[, sample_weight]) | Return the mean accuracy on the given test data and labels.  
`set_params`(**params) | Set the parameters of an estimator from the ensemble.  
`transform`(X) | Return class labels or probabilities for X for each estimator.  
`decision_function(X)` [source]

    
Predict decision function for samples in X using
`final_estimator_.decision_function`.

Parameters

    
`X{array-like, sparse matrix} of shape (n_samples, n_features)`

    
Training vectors, where n_samples is the number of samples and n_features is
the number of features.

Returns

    
`decisionsndarray of shape (n_samples,), (n_samples, n_classes), or
(n_samples, n_classes * (n_classes-1) / 2)`

    
The decision function computed the final estimator.

`fit(X, y, sample_weight=None)` [source]

    
Fit the estimators.

Parameters

    
`X{array-like, sparse matrix} of shape (n_samples, n_features)`

    
Training vectors, where `n_samples` is the number of samples and `n_features`
is the number of features.

`yarray-like of shape (n_samples,)`

    
Target values.

`sample_weightarray-like of shape (n_samples,), default=None`

    
Sample weights. If None, then samples are equally weighted. Note that this is
supported only if all underlying estimators support sample weights.

Returns

    
`selfobject`

`fit_transform(X, y=None, **fit_params)` [source]

    
Fit to data, then transform it.

Fits transformer to `X` and `y` with optional parameters `fit_params` and
returns a transformed version of `X`.

Parameters

    
`Xarray-like of shape (n_samples, n_features)`

    
Input samples.

`yarray-like of shape (n_samples,) or (n_samples, n_outputs), default=None`

    
Target values (None for unsupervised transformations).

`**fit_paramsdict`

    
Additional fit parameters.

Returns

    
`X_newndarray array of shape (n_samples, n_features_new)`

    
Transformed array.

`get_params(deep=True)` [source]

    
Get the parameters of an estimator from the ensemble.

Returns the parameters given in the constructor as well as the estimators
contained within the `estimators` parameter.

Parameters

    
`deepbool, default=True`

    
Setting it to True gets the various estimators and the parameters of the
estimators as well.

`property n_features_in_`

    
Number of features seen during fit.

`predict(X, **predict_params)` [source]

    
Predict target for X.

Parameters

    
`X{array-like, sparse matrix} of shape (n_samples, n_features)`

    
Training vectors, where n_samples is the number of samples and n_features is
the number of features.

`**predict_paramsdict of str -> obj`

    
Parameters to the `predict` called by the `final_estimator`. Note that this
may be used to return uncertainties from some estimators with `return_std` or
`return_cov`. Be aware that it will only accounts for uncertainty in the final
estimator.

Returns

    
`y_predndarray of shape (n_samples,) or (n_samples, n_output)`

    
Predicted targets.

`predict_proba(X)` [source]

    
Predict class probabilities for X using `final_estimator_.predict_proba`.

Parameters

    
`X{array-like, sparse matrix} of shape (n_samples, n_features)`

    
Training vectors, where n_samples is the number of samples and n_features is
the number of features.

Returns

    
`probabilitiesndarray of shape (n_samples, n_classes) or list of ndarray of
shape (n_output,)`

    
The class probabilities of the input samples.

`score(X, y, sample_weight=None)` [source]

    
Return the mean accuracy on the given test data and labels.

In multi-label classification, this is the subset accuracy which is a harsh
metric since you require for each sample that each label set be correctly
predicted.

Parameters

    
`Xarray-like of shape (n_samples, n_features)`

    
Test samples.

`yarray-like of shape (n_samples,) or (n_samples, n_outputs)`

    
True labels for `X`.

`sample_weightarray-like of shape (n_samples,), default=None`

    
Sample weights.

Returns

    
`scorefloat`

    
Mean accuracy of `self.predict(X)` wrt. `y`.

`set_params(**params)` [source]

    
Set the parameters of an estimator from the ensemble.

Valid parameter keys can be listed with `get_params()`. Note that you can
directly set the parameters of the estimators contained in `estimators`.

Parameters

    
`**paramskeyword arguments`

    
Specific parameters using e.g. `set_params(parameter_name=new_value)`. In
addition, to setting the parameters of the estimator, the individual estimator
of the estimators can also be set, or can be removed by setting them to
‘drop’.

`transform(X)` [source]

    
Return class labels or probabilities for X for each estimator.

Parameters

    
`X{array-like, sparse matrix} of shape (n_samples, n_features)`

    
Training vectors, where `n_samples` is the number of samples and `n_features`
is the number of features.

Returns

    
`y_predsndarray of shape (n_samples, n_estimators) or (n_samples, n_classes *
n_estimators)`

    
Prediction outputs for each estimator.

## Examples using `sklearn.ensemble.StackingClassifier`

![Release Highlights for scikit-learn
0.22](data:image/png;base64,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)

Release Highlights for scikit-learn 0.22

© 2007–2020 The scikit-learn developers  
Licensed under the 3-clause BSD License.  
https://scikit-
learn.org/0.24/modules/generated/sklearn.ensemble.StackingClassifier.html

