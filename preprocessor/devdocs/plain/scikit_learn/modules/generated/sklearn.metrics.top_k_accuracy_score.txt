# sklearn.metrics.top_k_accuracy_score

`sklearn.metrics.top_k_accuracy_score(y_true, y_score, *, k=2, normalize=True,
sample_weight=None, labels=None)` [source]

    
Top-k Accuracy classification score.

This metric computes the number of times where the correct label is among the
top `k` labels predicted (ranked by predicted scores). Note that the
multilabel case isn’t covered here.

Read more in the User Guide

Parameters

    
`y_truearray-like of shape (n_samples,)`

    
True labels.

`y_scorearray-like of shape (n_samples,) or (n_samples, n_classes)`

    
Target scores. These can be either probability estimates or non-thresholded
decision values (as returned by decision_function on some classifiers). The
binary case expects scores with shape (n_samples,) while the multiclass case
expects scores with shape (n_samples, n_classes). In the nulticlass case, the
order of the class scores must correspond to the order of `labels`, if
provided, or else to the numerical or lexicographical order of the labels in
`y_true`.

`kint, default=2`

    
Number of most likely outcomes considered to find the correct label.

`normalizebool, default=True`

    
If `True`, return the fraction of correctly classified samples. Otherwise,
return the number of correctly classified samples.

`sample_weightarray-like of shape (n_samples,), default=None`

    
Sample weights. If `None`, all samples are given the same weight.

`labelsarray-like of shape (n_classes,), default=None`

    
Multiclass only. List of labels that index the classes in `y_score`. If
`None`, the numerical or lexicographical order of the labels in `y_true` is
used.

Returns

    
`scorefloat`

    
The top-k accuracy score. The best performance is 1 with `normalize == True`
and the number of samples with `normalize == False`.

See also

`accuracy_score`

#### Notes

In cases where two or more labels are assigned equal predicted scores, the
labels with the highest indices will be chosen first. This might impact the
result if the correct label falls after the threshold because of that.

#### Examples

    
    >>> import numpy as np
    >>> from sklearn.metrics import top_k_accuracy_score
    >>> y_true = np.array([0, 1, 2, 2])
    >>> y_score = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2
    ...                     [0.3, 0.4, 0.2],  # 1 is in top 2
    ...                     [0.2, 0.4, 0.3],  # 2 is in top 2
    ...                     [0.7, 0.2, 0.1]]) # 2 isn't in top 2
    >>> top_k_accuracy_score(y_true, y_score, k=2)
    0.75
    >>> # Not normalizing gives the number of "correctly" classified samples
    >>> top_k_accuracy_score(y_true, y_score, k=2, normalize=False)
    3
    
© 2007–2020 The scikit-learn developers  
Licensed under the 3-clause BSD License.  
https://scikit-
learn.org/0.24/modules/generated/sklearn.metrics.top_k_accuracy_score.html

