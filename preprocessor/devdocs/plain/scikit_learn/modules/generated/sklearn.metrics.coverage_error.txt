# sklearn.metrics.coverage_error

`sklearn.metrics.coverage_error(y_true, y_score, *, sample_weight=None)`
[source]

    
Coverage error measure.

Compute how far we need to go through the ranked scores to cover all true
labels. The best value is equal to the average number of labels in `y_true`
per sample.

Ties in `y_scores` are broken by giving maximal rank that would have been
assigned to all tied values.

Note: Our implementation’s score is 1 greater than the one given in Tsoumakas
et al., 2010. This extends it to handle the degenerate case in which an
instance has 0 true labels.

Read more in the User Guide.

Parameters

    
`y_truendarray of shape (n_samples, n_labels)`

    
True binary labels in binary indicator format.

`y_scorendarray of shape (n_samples, n_labels)`

    
Target scores, can either be probability estimates of the positive class,
confidence values, or non-thresholded measure of decisions (as returned by
“decision_function” on some classifiers).

`sample_weightarray-like of shape (n_samples,), default=None`

    
Sample weights.

Returns

    
`coverage_errorfloat`

#### References

`1`

    
Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010). Mining multi-label data. In
Data mining and knowledge discovery handbook (pp. 667-685). Springer US.

© 2007–2020 The scikit-learn developers  
Licensed under the 3-clause BSD License.  
https://scikit-
learn.org/0.24/modules/generated/sklearn.metrics.coverage_error.html

