# sklearn.linear_model.RidgeClassifierCV

`class sklearn.linear_model.RidgeClassifierCV(alphas=0.1, 1.0, 10.0, *,
fit_intercept=True, normalize=False, scoring=None, cv=None, class_weight=None,
store_cv_values=False)` [source]

    
Ridge classifier with built-in cross-validation.

See glossary entry for cross-validation estimator.

By default, it performs Leave-One-Out Cross-Validation. Currently, only the
n_features > n_samples case is handled efficiently.

Read more in the User Guide.

Parameters

    
`alphasndarray of shape (n_alphas,), default=(0.1, 1.0, 10.0)`

    
Array of alpha values to try. Regularization strength; must be a positive
float. Regularization improves the conditioning of the problem and reduces the
variance of the estimates. Larger values specify stronger regularization.
Alpha corresponds to `1 / (2C)` in other linear models such as
`LogisticRegression` or `LinearSVC`.

`fit_interceptbool, default=True`

    
Whether to calculate the intercept for this model. If set to false, no
intercept will be used in calculations (i.e. data is expected to be centered).

`normalizebool, default=False`

    
This parameter is ignored when `fit_intercept` is set to False. If True, the
regressors X will be normalized before regression by subtracting the mean and
dividing by the l2-norm. If you wish to standardize, please use
`StandardScaler` before calling `fit` on an estimator with `normalize=False`.

`scoringstring, callable, default=None`

    
A string (see model evaluation documentation) or a scorer callable object /
function with signature `scorer(estimator, X, y)`.

`cvint, cross-validation generator or an iterable, default=None`

    
Determines the cross-validation splitting strategy. Possible inputs for cv
are:

  * None, to use the efficient Leave-One-Out cross-validation
  * integer, to specify the number of folds.
  * CV splitter,
  * An iterable yielding (train, test) splits as arrays of indices.

Refer User Guide for the various cross-validation strategies that can be used
here.

`class_weightdict or ‘balanced’, default=None`

    
Weights associated with classes in the form `{class_label: weight}`. If not
given, all classes are supposed to have weight one.

The “balanced” mode uses the values of y to automatically adjust weights
inversely proportional to class frequencies in the input data as `n_samples /
(n_classes * np.bincount(y))`

`store_cv_valuesbool, default=False`

    
Flag indicating if the cross-validation values corresponding to each alpha
should be stored in the `cv_values_` attribute (see below). This flag is only
compatible with `cv=None` (i.e. using Leave-One-Out Cross-Validation).

Attributes

    
`cv_values_ndarray of shape (n_samples, n_targets, n_alphas), optional`

    
Cross-validation values for each alpha (if `store_cv_values=True` and
`cv=None`). After `fit()` has been called, this attribute will contain the
mean squared errors (by default) or the values of the `{loss,score}_func`
function (if provided in the constructor). This attribute exists only when
`store_cv_values` is True.

`coef_ndarray of shape (1, n_features) or (n_targets, n_features)`

    
Coefficient of the features in the decision function.

`coef_` is of shape (1, n_features) when the given problem is binary.

`intercept_float or ndarray of shape (n_targets,)`

    
Independent term in decision function. Set to 0.0 if `fit_intercept = False`.

`alpha_float`

    
Estimated regularization parameter.

`best_score_float`

    
Score of base estimator with best alpha.

New in version 0.23.

`classes_ndarray of shape (n_classes,)`

    
The classes labels.

See also

`Ridge`

    
Ridge regression.

`RidgeClassifier`

    
Ridge classifier.

`RidgeCV`

    
Ridge regression with built-in cross validation.

#### Notes

For multi-class classification, n_class classifiers are trained in a one-
versus-all approach. Concretely, this is implemented by taking advantage of
the multi-variate response support in Ridge.

#### Examples

    
    >>> from sklearn.datasets import load_breast_cancer
    >>> from sklearn.linear_model import RidgeClassifierCV
    >>> X, y = load_breast_cancer(return_X_y=True)
    >>> clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
    >>> clf.score(X, y)
    0.9630...
    
#### Methods

`decision_function`(X) | Predict confidence scores for samples.  
---|---  
`fit`(X, y[, sample_weight]) | Fit Ridge classifier with cv.  
`get_params`([deep]) | Get parameters for this estimator.  
`predict`(X) | Predict class labels for samples in X.  
`score`(X, y[, sample_weight]) | Return the mean accuracy on the given test data and labels.  
`set_params`(**params) | Set the parameters of this estimator.  
`decision_function(X)` [source]

    
Predict confidence scores for samples.

The confidence score for a sample is proportional to the signed distance of
that sample to the hyperplane.

Parameters

    
`Xarray-like or sparse matrix, shape (n_samples, n_features)`

    
Samples.

Returns

    
array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)

    
Confidence scores per (sample, class) combination. In the binary case,
confidence score for self.classes_[1] where >0 means this class would be
predicted.

`fit(X, y, sample_weight=None)` [source]

    
Fit Ridge classifier with cv.

Parameters

    
`Xndarray of shape (n_samples, n_features)`

    
Training vectors, where n_samples is the number of samples and n_features is
the number of features. When using GCV, will be cast to float64 if necessary.

`yndarray of shape (n_samples,)`

    
Target values. Will be cast to X’s dtype if necessary.

`sample_weightfloat or ndarray of shape (n_samples,), default=None`

    
Individual weights for each sample. If given a float, every sample will have
the same weight.

Returns

    
`selfobject`

`get_params(deep=True)` [source]

    
Get parameters for this estimator.

Parameters

    
`deepbool, default=True`

    
If True, will return the parameters for this estimator and contained
subobjects that are estimators.

Returns

    
`paramsdict`

    
Parameter names mapped to their values.

`predict(X)` [source]

    
Predict class labels for samples in X.

Parameters

    
`Xarray-like or sparse matrix, shape (n_samples, n_features)`

    
Samples.

Returns

    
`Carray, shape [n_samples]`

    
Predicted class label per sample.

`score(X, y, sample_weight=None)` [source]

    
Return the mean accuracy on the given test data and labels.

In multi-label classification, this is the subset accuracy which is a harsh
metric since you require for each sample that each label set be correctly
predicted.

Parameters

    
`Xarray-like of shape (n_samples, n_features)`

    
Test samples.

`yarray-like of shape (n_samples,) or (n_samples, n_outputs)`

    
True labels for `X`.

`sample_weightarray-like of shape (n_samples,), default=None`

    
Sample weights.

Returns

    
`scorefloat`

    
Mean accuracy of `self.predict(X)` wrt. `y`.

`set_params(**params)` [source]

    
Set the parameters of this estimator.

The method works on simple estimators as well as on nested objects (such as
`Pipeline`). The latter have parameters of the form `<component>__<parameter>`
so that it’s possible to update each component of a nested object.

Parameters

    
`**paramsdict`

    
Estimator parameters.

Returns

    
`selfestimator instance`

    
Estimator instance.

© 2007–2020 The scikit-learn developers  
Licensed under the 3-clause BSD License.  
https://scikit-
learn.org/0.24/modules/generated/sklearn.linear_model.RidgeClassifierCV.html

