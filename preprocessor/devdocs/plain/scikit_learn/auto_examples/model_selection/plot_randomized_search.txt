# Comparing randomized search and grid search for hyperparameter estimation

Compare randomized search and grid search for optimizing hyperparameters of a
linear SVM with SGD training. All parameters that influence the learning are
searched simultaneously (except for the number of estimators, which poses a
time / quality tradeoff).

The randomized search and the grid search explore exactly the same space of
parameters. The result in parameter settings is quite similar, while the run
time for randomized search is drastically lower.

The performance is may slightly worse for the randomized search, and is likely
due to a noise effect and would not carry over to a held-out test set.

Note that in practice, one would not search over this many different
parameters simultaneously using grid search, but pick only the ones deemed
most important.

Out:

    
    RandomizedSearchCV took 32.09 seconds for 20 candidates parameter settings.
    Model with rank: 1
    Mean validation score: 0.920 (std: 0.028)
    Parameters: {'alpha': 0.07316411520495676, 'average': False, 'l1_ratio': 0.29007760721044407}
    
    Model with rank: 2
    Mean validation score: 0.920 (std: 0.029)
    Parameters: {'alpha': 0.0005223493320259539, 'average': True, 'l1_ratio': 0.7936977033574206}
    
    Model with rank: 3
    Mean validation score: 0.918 (std: 0.031)
    Parameters: {'alpha': 0.00025790124268693137, 'average': True, 'l1_ratio': 0.5699649107012649}
    
    GridSearchCV took 192.81 seconds for 100 candidate parameter settings.
    Model with rank: 1
    Mean validation score: 0.931 (std: 0.026)
    Parameters: {'alpha': 0.0001, 'average': True, 'l1_ratio': 0.0}
    
    Model with rank: 2
    Mean validation score: 0.928 (std: 0.030)
    Parameters: {'alpha': 0.0001, 'average': True, 'l1_ratio': 0.1111111111111111}
    
    Model with rank: 3
    Mean validation score: 0.927 (std: 0.026)
    Parameters: {'alpha': 0.0001, 'average': True, 'l1_ratio': 0.5555555555555556}
    
    
    print(__doc__)
    
    import numpy as np
    
    from time import time
    import scipy.stats as stats
    from sklearn.utils.fixes import loguniform
    
    from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
    from sklearn.datasets import load_digits
    from sklearn.linear_model import SGDClassifier
    
    # get some data
    X, y = load_digits(return_X_y=True)
    
    # build a classifier
    clf = SGDClassifier(loss='hinge', penalty='elasticnet',
                        fit_intercept=True)
    
    
    # Utility function to report best scores
    def report(results, n_top=3):
        for i in range(1, n_top + 1):
            candidates = np.flatnonzero(results['rank_test_score'] == i)
            for candidate in candidates:
                print("Model with rank: {0}".format(i))
                print("Mean validation score: {0:.3f} (std: {1:.3f})"
                      .format(results['mean_test_score'][candidate],
                              results['std_test_score'][candidate]))
                print("Parameters: {0}".format(results['params'][candidate]))
                print("")
    
    
    # specify parameters and distributions to sample from
    param_dist = {'average': [True, False],
                  'l1_ratio': stats.uniform(0, 1),
                  'alpha': loguniform(1e-4, 1e0)}
    
    # run randomized search
    n_iter_search = 20
    random_search = RandomizedSearchCV(clf, param_distributions=param_dist,
                                       n_iter=n_iter_search)
    
    start = time()
    random_search.fit(X, y)
    print("RandomizedSearchCV took %.2f seconds for %d candidates"
          " parameter settings." % ((time() - start), n_iter_search))
    report(random_search.cv_results_)
    
    # use a full grid over all parameters
    param_grid = {'average': [True, False],
                  'l1_ratio': np.linspace(0, 1, num=10),
                  'alpha': np.power(10, np.arange(-4, 1, dtype=float))}
    
    # run grid search
    grid_search = GridSearchCV(clf, param_grid=param_grid)
    start = time()
    grid_search.fit(X, y)
    
    print("GridSearchCV took %.2f seconds for %d candidate parameter settings."
          % (time() - start, len(grid_search.cv_results_['params'])))
    report(grid_search.cv_results_)
    
Total running time of the script: ( 3 minutes 44.991 seconds)

![Launch
binder](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAG0AAAAUCAMAAACwC0NtAAABWVBMVEW7u7vo6OjCwsLMzMzz8/P4+Pju7u68vLzd3d24uLjx8fHm5ubT09PAwMDJycnPz8+urq7Ly8va2tq5ubnq6uqSkpLBwcH09PS6urrypVmvr6/i4uLDw8O0tLSjo6PX19e9u7pYm8qxsbGrq6u+vr7s7OzZ2dng4OCVlZXk5OTIyMiWlpampqaoqKj6+vrGxsagoKC9vb2ZmZn29vb///+Nq8Kzs7PSspNlnshfnsr2o1HnZYHXc46dnZ2Li4vS0tKPj4+3t7fOtJm2urztpmKGqsTarYFcmsnTjZzkaYXOl6Kqtr7lqnHEqrD1olLMmqW+t7jWsIzGt6hrnsbQk6Gis7/IoavgbIfhcoprlcHYaom+oH2Elq50kLzV1dWHh4fDuK2Xr8GZsMDHtqatsLrglH/sfm60pLHKqqDWnJHJcJDtgWzoknG8dJaLqr/0lVqeo7jlbX7owswW4TTQAAACXklEQVRIx+3V52/iMBgH4DcsBwfCCCkhQICMS4G0AQKUXqFpumh7Pa57l9t7//9fzlRHWeXbCemkvpEs+ZWtR/rZVqAKj/WvK7u+v7m/np2RdpjokG9pNthRIrF0dJjYfDWLGM/ed76dAWx31v923NS0tWXvRCtD90bTN9Sy1Ona3u6Hp1939+Cw04/SRU9bizwTLTbUG9OtoRZGU7Hn5xefEz9/Nz9tJt4ONDOGky7Q52AuCbStYRvKOmZcyENjKjWqPYnFdZSiQbd9vhCkyMyDUBQzBbC7WmhMe7Gyk92+vrm9ud7ODjRKLQZoiKmgakBTRcULdKboZlDVXMtERzWvMU915xhgknwag2auRaook2/JlhqKi+EJ7QR+5NqX7XYNBtq8wtIZYFQoUxAtc2ELsORIYbS4AGpgLEnZqetqHpKOIGLVKwnSIqIKrKkVIm6FG09y9+Jj4/Z789fy8rOBlsddd7enBYlGDt0Cyw+CiDwcmGOaDRC808ix+lRysIKFtG4g4IKIe/JK7qycf7kicW7l3txrQS9fYomWAptoqZ6mu8HEvVsyrmkIomxPCxLNwQVgSZKpkkT7I6HJa3Jw0rxq7h3AVu71veaPMfkkNlmc1PuaEs/HOYSJZo9qOo5pRl8D0aI0DRkeBnedhzSAd5fHpxunjfskw0Xg5YV5w9+qiBUeigrASxD5esXgVgGM1shmg68srDkyT7aAfxUUXuFXuTovSxWQSg++gq3c8nE7VxvqcOE6iA6kRRD6LSecfvgNObIwPBOFu/3y1De3UWvkGrWNx9/T/1B/AFudUABdgF9sAAAAAElFTkSuQmCC)

`Download Python source code: plot_randomized_search.py`

`Download Jupyter notebook: plot_randomized_search.ipynb`

© 2007–2020 The scikit-learn developers  
Licensed under the 3-clause BSD License.  
https://scikit-
learn.org/0.24/auto_examples/model_selection/plot_randomized_search.html

