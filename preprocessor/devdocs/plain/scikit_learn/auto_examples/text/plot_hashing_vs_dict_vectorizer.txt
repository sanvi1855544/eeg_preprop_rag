# FeatureHasher and DictVectorizer Comparison

Compares FeatureHasher and DictVectorizer by using both to vectorize text
documents.

The example demonstrates syntax and speed only; it doesn’t actually do
anything useful with the extracted vectors. See the example scripts
{document_classification_20newsgroups,clustering}.py for actual learning on
text documents.

A discrepancy between the number of terms reported for DictVectorizer and for
FeatureHasher is to be expected due to hash collisions.

Out:

    
    Usage: /home/circleci/project/examples/text/plot_hashing_vs_dict_vectorizer.py [n_features_for_hashing]
        The default number of features is 2**18.
    
    Loading 20 newsgroups training data
    3803 documents - 6.245MB
    
    DictVectorizer
    done in 1.982352s at 3.150MB/s
    Found 47928 unique terms
    
    FeatureHasher on frequency dicts
    done in 0.957983s at 6.519MB/s
    Found 43873 unique terms
    
    FeatureHasher on raw tokens
    done in 0.895940s at 6.970MB/s
    Found 43873 unique terms
    
    
    # Author: Lars Buitinck
    # License: BSD 3 clause
    from collections import defaultdict
    import re
    import sys
    from time import time
    
    import numpy as np
    
    from sklearn.datasets import fetch_20newsgroups
    from sklearn.feature_extraction import DictVectorizer, FeatureHasher
    
    
    def n_nonzero_columns(X):
        """Returns the number of non-zero columns in a CSR matrix X."""
        return len(np.unique(X.nonzero()[1]))
    
    
    def tokens(doc):
        """Extract tokens from doc.
    
        This uses a simple regex to break strings into tokens. For a more
        principled approach, see CountVectorizer or TfidfVectorizer.
        """
        return (tok.lower() for tok in re.findall(r"\w+", doc))
    
    
    def token_freqs(doc):
        """Extract a dict mapping tokens from doc to their frequencies."""
        freq = defaultdict(int)
        for tok in tokens(doc):
            freq[tok] += 1
        return freq
    
    
    categories = [
        'alt.atheism',
        'comp.graphics',
        'comp.sys.ibm.pc.hardware',
        'misc.forsale',
        'rec.autos',
        'sci.space',
        'talk.religion.misc',
    ]
    # Uncomment the following line to use a larger set (11k+ documents)
    # categories = None
    
    print(__doc__)
    print("Usage: %s [n_features_for_hashing]" % sys.argv[0])
    print("    The default number of features is 2**18.")
    print()
    
    try:
        n_features = int(sys.argv[1])
    except IndexError:
        n_features = 2 ** 18
    except ValueError:
        print("not a valid number of features: %r" % sys.argv[1])
        sys.exit(1)
    
    
    print("Loading 20 newsgroups training data")
    raw_data, _ = fetch_20newsgroups(subset='train', categories=categories,
                                     return_X_y=True)
    data_size_mb = sum(len(s.encode('utf-8')) for s in raw_data) / 1e6
    print("%d documents - %0.3fMB" % (len(raw_data), data_size_mb))
    print()
    
    print("DictVectorizer")
    t0 = time()
    vectorizer = DictVectorizer()
    vectorizer.fit_transform(token_freqs(d) for d in raw_data)
    duration = time() - t0
    print("done in %fs at %0.3fMB/s" % (duration, data_size_mb / duration))
    print("Found %d unique terms" % len(vectorizer.get_feature_names()))
    print()
    
    print("FeatureHasher on frequency dicts")
    t0 = time()
    hasher = FeatureHasher(n_features=n_features)
    X = hasher.transform(token_freqs(d) for d in raw_data)
    duration = time() - t0
    print("done in %fs at %0.3fMB/s" % (duration, data_size_mb / duration))
    print("Found %d unique terms" % n_nonzero_columns(X))
    print()
    
    print("FeatureHasher on raw tokens")
    t0 = time()
    hasher = FeatureHasher(n_features=n_features, input_type="string")
    X = hasher.transform(tokens(d) for d in raw_data)
    duration = time() - t0
    print("done in %fs at %0.3fMB/s" % (duration, data_size_mb / duration))
    print("Found %d unique terms" % n_nonzero_columns(X))
    
Total running time of the script: ( 0 minutes 4.185 seconds)

![Launch
binder](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAG0AAAAUCAMAAACwC0NtAAABWVBMVEW7u7vo6OjCwsLMzMzz8/P4+Pju7u68vLzd3d24uLjx8fHm5ubT09PAwMDJycnPz8+urq7Ly8va2tq5ubnq6uqSkpLBwcH09PS6urrypVmvr6/i4uLDw8O0tLSjo6PX19e9u7pYm8qxsbGrq6u+vr7s7OzZ2dng4OCVlZXk5OTIyMiWlpampqaoqKj6+vrGxsagoKC9vb2ZmZn29vb///+Nq8Kzs7PSspNlnshfnsr2o1HnZYHXc46dnZ2Li4vS0tKPj4+3t7fOtJm2urztpmKGqsTarYFcmsnTjZzkaYXOl6Kqtr7lqnHEqrD1olLMmqW+t7jWsIzGt6hrnsbQk6Gis7/IoavgbIfhcoprlcHYaom+oH2Elq50kLzV1dWHh4fDuK2Xr8GZsMDHtqatsLrglH/sfm60pLHKqqDWnJHJcJDtgWzoknG8dJaLqr/0lVqeo7jlbX7owswW4TTQAAACXklEQVRIx+3V52/iMBgH4DcsBwfCCCkhQICMS4G0AQKUXqFpumh7Pa57l9t7//9fzlRHWeXbCemkvpEs+ZWtR/rZVqAKj/WvK7u+v7m/np2RdpjokG9pNthRIrF0dJjYfDWLGM/ed76dAWx31v923NS0tWXvRCtD90bTN9Sy1Ona3u6Hp1939+Cw04/SRU9bizwTLTbUG9OtoRZGU7Hn5xefEz9/Nz9tJt4ONDOGky7Q52AuCbStYRvKOmZcyENjKjWqPYnFdZSiQbd9vhCkyMyDUBQzBbC7WmhMe7Gyk92+vrm9ud7ODjRKLQZoiKmgakBTRcULdKboZlDVXMtERzWvMU915xhgknwag2auRaook2/JlhqKi+EJ7QR+5NqX7XYNBtq8wtIZYFQoUxAtc2ELsORIYbS4AGpgLEnZqetqHpKOIGLVKwnSIqIKrKkVIm6FG09y9+Jj4/Z789fy8rOBlsddd7enBYlGDt0Cyw+CiDwcmGOaDRC808ix+lRysIKFtG4g4IKIe/JK7qycf7kicW7l3txrQS9fYomWAptoqZ6mu8HEvVsyrmkIomxPCxLNwQVgSZKpkkT7I6HJa3Jw0rxq7h3AVu71veaPMfkkNlmc1PuaEs/HOYSJZo9qOo5pRl8D0aI0DRkeBnedhzSAd5fHpxunjfskw0Xg5YV5w9+qiBUeigrASxD5esXgVgGM1shmg68srDkyT7aAfxUUXuFXuTovSxWQSg++gq3c8nE7VxvqcOE6iA6kRRD6LSecfvgNObIwPBOFu/3y1De3UWvkGrWNx9/T/1B/AFudUABdgF9sAAAAAElFTkSuQmCC)

`Download Python source code: plot_hashing_vs_dict_vectorizer.py`

`Download Jupyter notebook: plot_hashing_vs_dict_vectorizer.ipynb`

© 2007–2020 The scikit-learn developers  
Licensed under the 3-clause BSD License.  
https://scikit-
learn.org/0.24/auto_examples/text/plot_hashing_vs_dict_vectorizer.html

