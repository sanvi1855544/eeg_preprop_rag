# Linear

`class torch.nn.Linear(in_features, out_features, bias=True, device=None,
dtype=None)` [source]

    
Applies a linear transformation to the incoming data: y=xAT+by = xA^T + b

This module supports TensorFloat32.

On certain ROCm devices, when using float16 inputs this module will use
different precision for backward.

Parameters

    
  * in_features (int) – size of each input sample
  * out_features (int) – size of each output sample
  * bias (bool) – If set to `False`, the layer will not learn an additive bias. Default: `True`

Shape:

    
  * Input: (∗,Hin)(*, H_{in}) where ∗* means any number of dimensions including none and Hin=in_featuresH_{in} = \text{in\\_features}.
  * Output: (∗,Hout)(*, H_{out}) where all but the last dimension are the same shape as the input and Hout=out_featuresH_{out} = \text{out\\_features}.

Variables

    
  * weight (torch.Tensor) – the learnable weights of the module of shape (out_features,in_features)(\text{out\\_features}, \text{in\\_features}). The values are initialized from U(−k,k)\mathcal{U}(-\sqrt{k}, \sqrt{k}), where k=1in_featuresk = \frac{1}{\text{in\\_features}}
  * bias – the learnable bias of the module of shape (out_features)(\text{out\\_features}). If `bias` is `True`, the values are initialized from U(−k,k)\mathcal{U}(-\sqrt{k}, \sqrt{k}) where k=1in_featuresk = \frac{1}{\text{in\\_features}}

Examples:

    
    >>> m = nn.Linear(20, 30)
    >>> input = torch.randn(128, 20)
    >>> output = m(input)
    >>> print(output.size())
    torch.Size([128, 30])
    
© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/generated/torch.nn.Linear.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

