# Softmax

`class torch.nn.Softmax(dim=None)` [source]

    
Applies the Softmax function to an n-dimensional input Tensor rescaling them
so that the elements of the n-dimensional output Tensor lie in the range [0,1]
and sum to 1.

Softmax is defined as:

Softmax(xi)=exp⁡(xi)∑jexp⁡(xj)\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j
\exp(x_j)}

When the input Tensor is a sparse tensor then the unspecified values are
treated as `-inf`.

Shape:

    
  * Input: (∗)(*) where `*` means, any number of additional dimensions
  * Output: (∗)(*), same shape as the input

Returns

    
a Tensor of the same dimension and shape as the input with values in the range
[0, 1]

Parameters

    
dim (int) – A dimension along which Softmax will be computed (so every slice
along dim will sum to 1).

Return type

    
None

Note

This module doesn’t work directly with NLLLoss, which expects the Log to be
computed between the Softmax and itself. Use `LogSoftmax` instead (it’s faster
and has better numerical properties).

Examples:

    
    >>> m = nn.Softmax(dim=1)
    >>> input = torch.randn(2, 3)
    >>> output = m(input)
    
© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/generated/torch.nn.Softmax.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

