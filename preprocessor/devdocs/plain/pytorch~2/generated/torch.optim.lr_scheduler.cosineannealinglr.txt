# CosineAnnealingLR

`class torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0,
last_epoch=-1, verbose=False)` [source]

    
Set the learning rate of each parameter group using a cosine annealing
schedule, where ηmax\eta_{max} is set to the initial lr and TcurT_{cur} is the
number of epochs since the last restart in SGDR:

ηt=ηmin+12(ηmax−ηmin)(1+cos⁡(TcurTmaxπ)),Tcur≠(2k+1)Tmax;ηt+1=ηt+12(ηmax−ηmin)(1−cos⁡(1Tmaxπ)),Tcur=(2k+1)Tmax.\begin{aligned}
\eta_t & = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1 +
\cos\left(\frac{T_{cur}}{T_{max}}\pi\right)\right), & T_{cur} \neq
(2k+1)T_{max}; \\\ \eta_{t+1} & = \eta_{t} + \frac{1}{2}(\eta_{max} -
\eta_{min}) \left(1 - \cos\left(\frac{1}{T_{max}}\pi\right)\right), & T_{cur}
= (2k+1)T_{max}. \end{aligned}

When last_epoch=-1, sets initial lr as lr. Notice that because the schedule is
defined recursively, the learning rate can be simultaneously modified outside
this scheduler by other operators. If the learning rate is set solely by this
scheduler, the learning rate at each step becomes:

ηt=ηmin+12(ηmax−ηmin)(1+cos⁡(TcurTmaxπ))\eta_t = \eta_{min} +
\frac{1}{2}(\eta_{max} - \eta_{min})\left(1 +
\cos\left(\frac{T_{cur}}{T_{max}}\pi\right)\right)

It has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts.
Note that this only implements the cosine annealing part of SGDR, and not the
restarts.

Parameters

    
  * optimizer (Optimizer) – Wrapped optimizer.
  * T_max (int) – Maximum number of iterations.
  * eta_min (float) – Minimum learning rate. Default: 0.
  * last_epoch (int) – The index of last epoch. Default: -1.
  * verbose (bool) – If `True`, prints a message to stdout for each update. Default: `False`.

`get_last_lr()`

    
Return last computed learning rate by current scheduler.

`load_state_dict(state_dict)`

    
Loads the schedulers state.

Parameters

    
state_dict (dict) – scheduler state. Should be an object returned from a call
to `state_dict()`.

`print_lr(is_verbose, group, lr, epoch=None)`

    
Display the current learning rate.

`state_dict()`

    
Returns the state of the scheduler as a `dict`.

It contains an entry for every variable in self.__dict__ which is not the
optimizer.

© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

