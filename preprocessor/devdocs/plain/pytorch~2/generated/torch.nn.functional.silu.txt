# torch.nn.functional.silu

`torch.nn.functional.silu(input, inplace=False)` [source]

    
Applies the Sigmoid Linear Unit (SiLU) function, element-wise. The SiLU
function is also known as the swish function.

silu(x)=x∗σ(x),where σ(x) is the logistic sigmoid.\text{silu}(x) = x *
\sigma(x), \text{where } \sigma(x) \text{ is the logistic sigmoid.}

Note

See Gaussian Error Linear Units (GELUs) where the SiLU (Sigmoid Linear Unit)
was originally coined, and see Sigmoid-Weighted Linear Units for Neural
Network Function Approximation in Reinforcement Learning and Swish: a Self-
Gated Activation Function where the SiLU was experimented with later.

See `SiLU` for more details.

Return type

    
Tensor

© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/generated/torch.nn.functional.silu.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

