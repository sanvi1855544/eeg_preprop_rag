# BatchNorm1d

`class torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1,
affine=True, track_running_stats=True, device=None, dtype=None)` [source]

    
Applies Batch Normalization over a 2D or 3D input as described in the paper
Batch Normalization: Accelerating Deep Network Training by Reducing Internal
Covariate Shift .

y=x−E[x]Var[x]+ϵ∗γ+βy = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] +
\epsilon}} * \gamma + \beta

The mean and standard-deviation are calculated per-dimension over the mini-
batches and γ\gamma and β\beta are learnable parameter vectors of size `C`
(where `C` is the number of features or channels of the input). By default,
the elements of γ\gamma are set to 1 and the elements of β\beta are set to 0.
At train time in the forward pass, the standard-deviation is calculated via
the biased estimator, equivalent to `torch.var(input, unbiased=False)`.
However, the value stored in the moving average of the standard-deviation is
calculated via the unbiased estimator, equivalent to `torch.var(input,
unbiased=True)`.

Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default `momentum` of 0.1.

If `track_running_stats` is set to `False`, this layer then does not keep
running estimates, and batch statistics are instead used during evaluation
time as well.

Note

This `momentum` argument is different from one used in optimizer classes and
the conventional notion of momentum. Mathematically, the update rule for
running statistics here is x^new=(1−momentum)×x^+momentum×xt\hat{x}_\text{new}
= (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t, where
x^\hat{x} is the estimated statistic and xtx_t is the new observed value.

Because the Batch Normalization is done over the `C` dimension, computing
statistics on `(N, L)` slices, it’s common terminology to call this Temporal
Batch Normalization.

Parameters

    
  * num_features (int) – number of features or channels CC of the input
  * eps (float) – a value added to the denominator for numerical stability. Default: 1e-5
  * momentum (float) – the value used for the running_mean and running_var computation. Can be set to `None` for cumulative moving average (i.e. simple average). Default: 0.1
  * affine (bool) – a boolean value that when set to `True`, this module has learnable affine parameters. Default: `True`
  * track_running_stats (bool) – a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics, and initializes statistics buffers `running_mean` and `running_var` as `None`. When these buffers are `None`, this module always uses batch statistics. in both training and eval modes. Default: `True`

Shape:

    
  * Input: (N,C)(N, C) or (N,C,L)(N, C, L), where NN is the batch size, CC is the number of features or channels, and LL is the sequence length
  * Output: (N,C)(N, C) or (N,C,L)(N, C, L) (same shape as input)

Examples:

    
    >>> # With Learnable Parameters
    >>> m = nn.BatchNorm1d(100)
    >>> # Without Learnable Parameters
    >>> m = nn.BatchNorm1d(100, affine=False)
    >>> input = torch.randn(20, 100)
    >>> output = m(input)
    
© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/generated/torch.nn.BatchNorm1d.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

