# torch.nn.functional.mish

`torch.nn.functional.mish(input, inplace=False)` [source]

    
Applies the Mish function, element-wise. Mish: A Self Regularized Non-
Monotonic Neural Activation Function.

Mish(x)=x∗Tanh(Softplus(x))\text{Mish}(x) = x *
\text{Tanh}(\text{Softplus}(x))

Note

See Mish: A Self Regularized Non-Monotonic Neural Activation Function

See `Mish` for more details.

Return type

    
Tensor

© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/generated/torch.nn.functional.mish.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

