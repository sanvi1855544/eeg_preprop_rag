# LSTM

`class torch.ao.nn.quantizable.LSTM(input_size, hidden_size, num_layers=1,
bias=True, batch_first=False, dropout=0.0, bidirectional=False, device=None,
dtype=None)` [source]

    
A quantizable long short-term memory (LSTM).

For the description and the argument types, please, refer to `LSTM`

Variables

    
layers – instances of the `_LSTMLayer`

Note

To access the weights and biases, you need to access them per layer. See
examples below.

Examples:

    
    >>> import torch.ao.nn.quantizable as nnqa
    >>> rnn = nnqa.LSTM(10, 20, 2)
    >>> input = torch.randn(5, 3, 10)
    >>> h0 = torch.randn(2, 3, 20)
    >>> c0 = torch.randn(2, 3, 20)
    >>> output, (hn, cn) = rnn(input, (h0, c0))
    >>> # To get the weights:
    >>> print(rnn.layers[0].weight_ih)
    tensor([[...]])
    >>> print(rnn.layers[0].weight_hh)
    AssertionError: There is no reverse path in the non-bidirectional layer
    
© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/generated/torch.ao.nn.quantizable.LSTM.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

