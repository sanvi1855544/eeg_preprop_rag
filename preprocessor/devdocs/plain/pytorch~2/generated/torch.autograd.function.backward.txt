# torch.autograd.Function.backward

`static Function.backward(ctx, *grad_outputs)`

    
Defines a formula for differentiating the operation with backward mode
automatic differentiation (alias to the vjp function).

This function is to be overridden by all subclasses.

It must accept a context `ctx` as the first argument, followed by as many
outputs as the `forward()` returned (None will be passed in for non tensor
outputs of the forward function), and it should return as many tensors, as
there were inputs to `forward()`. Each argument is the gradient w.r.t the
given output, and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not requiring
grads, you can just pass None as a gradient for that input.

The context can be used to retrieve tensors saved during the forward pass. It
also has an attribute `ctx.needs_input_grad` as a tuple of booleans
representing whether each input needs gradient. E.g., `backward()` will have
`ctx.needs_input_grad[0] = True` if the first input to `forward()` needs
gradient computed w.r.t. the output.

Return type

    
Any

Â© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/generated/torch.autograd.Function.backward.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

