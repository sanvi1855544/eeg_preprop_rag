# ChainedScheduler

`class torch.optim.lr_scheduler.ChainedScheduler(schedulers)` [source]

    
Chains list of learning rate schedulers. It takes a list of chainable learning
rate schedulers and performs consecutive step() functions belonging to them by
just one call.

Parameters

    
schedulers (list) – List of chained schedulers.

#### Example

    
    >>> # Assuming optimizer uses lr = 1. for all groups
    >>> # lr = 0.09     if epoch == 0
    >>> # lr = 0.081    if epoch == 1
    >>> # lr = 0.729    if epoch == 2
    >>> # lr = 0.6561   if epoch == 3
    >>> # lr = 0.59049  if epoch >= 4
    >>> scheduler1 = ConstantLR(self.opt, factor=0.1, total_iters=2)
    >>> scheduler2 = ExponentialLR(self.opt, gamma=0.9)
    >>> scheduler = ChainedScheduler([scheduler1, scheduler2])
    >>> for epoch in range(100):
    >>>     train(...)
    >>>     validate(...)
    >>>     scheduler.step()
    
`get_last_lr()`

    
Return last computed learning rate by current scheduler.

`load_state_dict(state_dict)` [source]

    
Loads the schedulers state.

Parameters

    
state_dict (dict) – scheduler state. Should be an object returned from a call
to `state_dict()`.

`print_lr(is_verbose, group, lr, epoch=None)`

    
Display the current learning rate.

`state_dict()` [source]

    
Returns the state of the scheduler as a `dict`.

It contains an entry for every variable in self.__dict__ which is not the
optimizer. The wrapped scheduler states will also be saved.

© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/generated/torch.optim.lr_scheduler.ChainedScheduler.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

