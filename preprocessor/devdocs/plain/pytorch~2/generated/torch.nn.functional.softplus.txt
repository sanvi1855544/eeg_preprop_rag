# torch.nn.functional.softplus

`torch.nn.functional.softplus(input, beta=1, threshold=20) → Tensor`

    
Applies element-wise, the function
Softplus(x)=1β∗log⁡(1+exp⁡(β∗x))\text{Softplus}(x) = \frac{1}{\beta} * \log(1
+ \exp(\beta * x)).

For numerical stability the implementation reverts to the linear function when
input×β>thresholdinput \times \beta > threshold.

See `Softplus` for more details.

© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/generated/torch.nn.functional.softplus.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

