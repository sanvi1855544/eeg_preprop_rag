# torch.nn.functional.log_softmax

`torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3, dtype=None)`
[source]

    
Applies a softmax followed by a logarithm.

While mathematically equivalent to log(softmax(x)), doing these two operations
separately is slower and numerically unstable. This function uses an
alternative formulation to compute the output and gradient correctly.

See `LogSoftmax` for more details.

Parameters

    
  * input (Tensor) – input
  * dim (int) – A dimension along which log_softmax will be computed.
  * dtype (`torch.dtype`, optional) – the desired data type of returned tensor. If specified, the input tensor is cast to `dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.

Return type

    
Tensor

© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/generated/torch.nn.functional.log_softmax.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

