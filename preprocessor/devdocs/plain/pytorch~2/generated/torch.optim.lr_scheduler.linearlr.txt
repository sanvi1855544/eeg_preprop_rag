# LinearLR

`class torch.optim.lr_scheduler.LinearLR(optimizer,
start_factor=0.3333333333333333, end_factor=1.0, total_iters=5, last_epoch=-1,
verbose=False)` [source]

    
Decays the learning rate of each parameter group by linearly changing small
multiplicative factor until the number of epoch reaches a pre-defined
milestone: total_iters. Notice that such decay can happen simultaneously with
other changes to the learning rate from outside this scheduler. When
last_epoch=-1, sets initial lr as lr.

Parameters

    
  * optimizer (Optimizer) – Wrapped optimizer.
  * start_factor (float) – The number we multiply learning rate in the first epoch. The multiplication factor changes towards end_factor in the following epochs. Default: 1./3.
  * end_factor (float) – The number we multiply learning rate at the end of linear changing process. Default: 1.0.
  * total_iters (int) – The number of iterations that multiplicative factor reaches to 1. Default: 5.
  * last_epoch (int) – The index of the last epoch. Default: -1.
  * verbose (bool) – If `True`, prints a message to stdout for each update. Default: `False`.

#### Example

    
    >>> # Assuming optimizer uses lr = 0.05 for all groups
    >>> # lr = 0.025    if epoch == 0
    >>> # lr = 0.03125  if epoch == 1
    >>> # lr = 0.0375   if epoch == 2
    >>> # lr = 0.04375  if epoch == 3
    >>> # lr = 0.05    if epoch >= 4
    >>> scheduler = LinearLR(self.opt, start_factor=0.5, total_iters=4)
    >>> for epoch in range(100):
    >>>     train(...)
    >>>     validate(...)
    >>>     scheduler.step()
    
`get_last_lr()`

    
Return last computed learning rate by current scheduler.

`load_state_dict(state_dict)`

    
Loads the schedulers state.

Parameters

    
state_dict (dict) – scheduler state. Should be an object returned from a call
to `state_dict()`.

`print_lr(is_verbose, group, lr, epoch=None)`

    
Display the current learning rate.

`state_dict()`

    
Returns the state of the scheduler as a `dict`.

It contains an entry for every variable in self.__dict__ which is not the
optimizer.

© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/generated/torch.optim.lr_scheduler.LinearLR.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

