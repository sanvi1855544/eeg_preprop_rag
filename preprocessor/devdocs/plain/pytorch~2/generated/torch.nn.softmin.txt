# Softmin

`class torch.nn.Softmin(dim=None)` [source]

    
Applies the Softmin function to an n-dimensional input Tensor rescaling them
so that the elements of the n-dimensional output Tensor lie in the range `[0,
1]` and sum to 1.

Softmin is defined as:

Softmin(xi)=exp⁡(−xi)∑jexp⁡(−xj)\text{Softmin}(x_{i}) =
\frac{\exp(-x_i)}{\sum_j \exp(-x_j)}

Shape:

    
  * Input: (∗)(*) where `*` means, any number of additional dimensions
  * Output: (∗)(*), same shape as the input

Parameters

    
dim (int) – A dimension along which Softmin will be computed (so every slice
along dim will sum to 1).

Returns

    
a Tensor of the same dimension and shape as the input, with values in the
range [0, 1]

Return type

    
None

Examples:

    
    >>> m = nn.Softmin(dim=1)
    >>> input = torch.randn(2, 3)
    >>> output = m(input)
    
© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/generated/torch.nn.Softmin.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

