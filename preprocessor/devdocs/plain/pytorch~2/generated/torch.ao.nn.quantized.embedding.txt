# Embedding

`class torch.ao.nn.quantized.Embedding(num_embeddings, embedding_dim,
padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False,
sparse=False, _weight=None, dtype=torch.quint8)` [source]

    
A quantized Embedding module with quantized packed weights as inputs. We adopt
the same interface as `torch.nn.Embedding`, please see
https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding for documentation.

Similar to `Embedding`, attributes will be randomly initialized at module
creation time and will be overwritten later

Variables

    
weight (Tensor) – the non-learnable quantized weights of the module of shape
(num_embeddings,embedding_dim)(\text{num\\_embeddings},
\text{embedding\\_dim}).

Examples::

    
    
    >>> m = nn.quantized.Embedding(num_embeddings=10, embedding_dim=12)
    >>> indices = torch.tensor([9, 6, 5, 7, 8, 8, 9, 2, 8])
    >>> output = m(indices)
    >>> print(output.size())
    torch.Size([9, 12])
    
`classmethod from_float(mod)` [source]

    
Create a quantized embedding module from a float module

Parameters

    
mod (Module) – a float module, either produced by torch.ao.quantization
utilities or provided by user

© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/generated/torch.ao.nn.quantized.Embedding.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

