# torch.nn.functional.selu

`torch.nn.functional.selu(input, inplace=False) → Tensor` [source]

    
Applies element-wise,
SELU(x)=scale∗(max⁡(0,x)+min⁡(0,α∗(exp⁡(x)−1)))\text{SELU}(x) = scale *
(\max(0,x) + \min(0, \alpha * (\exp(x) - 1))), with
α=1.6732632423543772848170429916717\alpha=1.6732632423543772848170429916717
and
scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.

See `SELU` for more details.

Return type

    
Tensor

© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/generated/torch.nn.functional.selu.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

