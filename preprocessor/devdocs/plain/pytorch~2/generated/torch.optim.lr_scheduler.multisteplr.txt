# MultiStepLR

`class torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1,
last_epoch=-1, verbose=False)` [source]

    
Decays the learning rate of each parameter group by gamma once the number of
epoch reaches one of the milestones. Notice that such decay can happen
simultaneously with other changes to the learning rate from outside this
scheduler. When last_epoch=-1, sets initial lr as lr.

Parameters

    
  * optimizer (Optimizer) – Wrapped optimizer.
  * milestones (list) – List of epoch indices. Must be increasing.
  * gamma (float) – Multiplicative factor of learning rate decay. Default: 0.1.
  * last_epoch (int) – The index of last epoch. Default: -1.
  * verbose (bool) – If `True`, prints a message to stdout for each update. Default: `False`.

#### Example

    
    >>> # Assuming optimizer uses lr = 0.05 for all groups
    >>> # lr = 0.05     if epoch < 30
    >>> # lr = 0.005    if 30 <= epoch < 80
    >>> # lr = 0.0005   if epoch >= 80
    >>> scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)
    >>> for epoch in range(100):
    >>>     train(...)
    >>>     validate(...)
    >>>     scheduler.step()
    
`get_last_lr()`

    
Return last computed learning rate by current scheduler.

`load_state_dict(state_dict)`

    
Loads the schedulers state.

Parameters

    
state_dict (dict) – scheduler state. Should be an object returned from a call
to `state_dict()`.

`print_lr(is_verbose, group, lr, epoch=None)`

    
Display the current learning rate.

`state_dict()`

    
Returns the state of the scheduler as a `dict`.

It contains an entry for every variable in self.__dict__ which is not the
optimizer.

© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/generated/torch.optim.lr_scheduler.MultiStepLR.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

