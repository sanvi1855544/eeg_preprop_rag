# FXFloatFunctional

`class torch.ao.nn.quantized.FXFloatFunctional(*args, **kwargs)` [source]

    
module to replace FloatFunctional module before FX graph mode quantization,
since activation_post_process will be inserted in top level module directly

Valid operation names:

    
  * add
  * cat
  * mul
  * add_relu
  * add_scalar
  * mul_scalar

Â© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/generated/torch.ao.nn.quantized.FXFloatFunctional.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

