# dual_level

`class torch.autograd.forward_ad.dual_level` [source]

    
Context-manager that enables forward AD. All forward AD computation must be
performed in a `dual_level` context.

Note

The `dual_level` context appropriately enters and exit the dual level to
controls the current forward AD level, which is used by default by the other
functions in this API.

We currently don’t plan to support nested `dual_level` contexts, however, so
only a single forward AD level is supported. To compute higher-order forward
grads, one can use `torch.func.jvp()`.

Example:

    
    >>> x = torch.tensor([1])
    >>> x_t = torch.tensor([1])
    >>> with dual_level():
    ...     inp = make_dual(x, x_t)
    ...     # Do computations with inp
    ...     out = your_fn(inp)
    ...     _, grad = unpack_dual(out)
    >>> grad is None
    False
    >>> # After exiting the level, the grad is deleted
    >>> _, grad_after = unpack_dual(out)
    >>> grad is None
    True
    
Please see the forward-mode AD tutorial for detailed steps on how to use this
API.

© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/generated/torch.autograd.forward_ad.dual_level.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

