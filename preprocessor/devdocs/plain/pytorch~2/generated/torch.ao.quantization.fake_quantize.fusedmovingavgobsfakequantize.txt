# FusedMovingAvgObsFakeQuantize

`class
torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize(observer=<class
'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0,
quant_max=255, **observer_kwargs)` [source]

    
Fused module that is used to observe the input tensor (compute min/max),
compute scale/zero_point and fake_quantize the tensor. This module uses
calculation similar MovingAverageMinMaxObserver for the inputs, to compute the
min/max values in order to compute the scale/zero_point. The qscheme input in
the observer is used to differentiate between symmetric/affine quantization
scheme.

The output of this module is given by x_out = (clamp(round(x/scale +
zero_point), quant_min, quant_max)-zero_point)*scale

Similar to `FakeQuantize`, and accepts the same attributes as the base class.

Â© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/generated/torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

