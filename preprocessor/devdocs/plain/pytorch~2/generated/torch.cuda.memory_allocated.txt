# torch.cuda.memory_allocated

`torch.cuda.memory_allocated(device=None)` [source]

    
Returns the current GPU memory occupied by tensors in bytes for a given
device.

Parameters

    
device (torch.device or int, optional) – selected device. Returns statistic
for the current device, given by `current_device()`, if `device` is `None`
(default).

Return type

    
int

Note

This is likely less than the amount shown in `nvidia-smi` since some unused
memory can be held by the caching allocator and some context needs to be
created on GPU. See Memory management for more details about GPU memory
management.

© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/generated/torch.cuda.memory_allocated.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

