# celu

`class torch.ao.nn.quantized.functional.celu(input, scale, zero_point,
alpha=1.)` [source]

    
Applies the quantized CELU function element-wise.

CELU(x)=max⁡(0,x)+min⁡(0,α∗(exp⁡(x/α)−1))\text{CELU}(x) = \max(0,x) + \min(0,
\alpha * (\exp(x / \alpha) - 1))

Parameters

    
  * input (Tensor) – quantized input
  * alpha (float) – the α\alpha value for the CELU formulation. Default: 1.0

Return type

    
Tensor

© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/generated/torch.ao.nn.quantized.functional.celu.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

