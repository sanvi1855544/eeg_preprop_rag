# torch.nn.utils.clip_grad_value_

`torch.nn.utils.clip_grad_value_(parameters, clip_value, foreach=None)`
[source]

    
Clips gradient of an iterable of parameters at specified value.

Gradients are modified in-place.

Parameters

    
  * parameters (Iterable[Tensor] or Tensor) – an iterable of Tensors or a single Tensor that will have gradients normalized
  * clip_value (float) – maximum allowed value of the gradients. The gradients are clipped in the range [-clip_value,clip_value]\left[\text{-clip\\_value}, \text{clip\\_value}\right]
  * foreach (bool) – use the faster foreach-based implementation If `None`, use the foreach implementation for CUDA and CPU native tensors and silently fall back to the slow implementation for other device types. Default: `None`

© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/generated/torch.nn.utils.clip_grad_value_.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

