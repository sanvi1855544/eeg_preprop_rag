# torch.cuda.change_current_allocator

`torch.cuda.change_current_allocator(allocator)` [source]

    
Changes the currently used memory allocator to be the one provided. If the
current allocator has already been used/initialized, this function will error.

Parameters

    
allocator (torch.cuda.memory._CUDAAllocator) – allocator to be set as the
active one.

Note

See Memory management for details on creating and using a custom allocator

© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/generated/torch.cuda.change_current_allocator.html

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

