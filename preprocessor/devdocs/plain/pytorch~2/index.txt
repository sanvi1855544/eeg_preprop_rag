# PyTorch documentation

PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.

Features described in this documentation are classified by release status:

Stable: These features will be maintained long-term and there should generally
be no major performance limitations or gaps in documentation. We also expect
to maintain backwards compatibility (although breaking changes can happen and
notice will be given one release ahead of time).

Beta: These features are tagged as Beta because the API may change based on
user feedback, because the performance needs to improve, or because coverage
across operators is not yet complete. For Beta features, we are committing to
seeing the feature through to the Stable classification. We are not, however,
committing to backwards compatibility.

Prototype: These features are typically not available as part of binary
distributions like PyPI or Conda, except sometimes behind run-time flags, and
are at an early stage for feedback and testing.

Community

  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers

Developer Notes

  * CUDA Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ

Language Bindings

  * C++
  * Javadoc
  * torch::deploy

Python API

  * torch
    * Tensors
    * Generators
    * Random sampling
    * Serialization
    * Parallelism
    * Locally disabling gradient computation
    * Math operations
    * Utilities
    * Symbolic Numbers
    * Export Path
    * Optimizations
    * Operator Tags
  * torch.nn
    * Parameter
    * UninitializedParameter
    * UninitializedBuffer
    * Containers
    * Convolution Layers
    * Pooling layers
    * Padding Layers
    * Non-linear Activations (weighted sum, nonlinearity)
    * Non-linear Activations (other)
    * Normalization Layers
    * Recurrent Layers
    * Transformer Layers
    * Linear Layers
    * Dropout Layers
    * Sparse Layers
    * Distance Functions
    * Loss Functions
    * Vision Layers
    * Shuffle Layers
    * DataParallel Layers (multi-GPU, distributed)
    * Utilities
    * Quantized Functions
    * Lazy Modules Initialization
  * torch.nn.functional
    * Convolution functions
    * Pooling functions
    * Attention Mechanisms
    * Non-linear activation functions
    * Linear functions
    * Dropout functions
    * Sparse functions
    * Distance functions
    * Loss functions
    * Vision functions
    * DataParallel functions (multi-GPU, distributed)
  * torch.Tensor
    * Data types
    * Initializing and basic operations
    * Tensor class reference
  * Tensor Attributes
    * torch.dtype
    * torch.device
    * torch.layout
    * torch.memory_format
  * Tensor Views
  * torch.amp
    * Autocasting
    * Gradient Scaling
    * Autocast Op Reference
  * torch.autograd
    * torch.autograd.backward
    * torch.autograd.grad
    * Forward-mode Automatic Differentiation
    * Functional higher level API
    * Locally disabling gradient computation
    * Default gradient layouts
    * In-place operations on Tensors
    * Variable (deprecated)
    * Tensor autograd functions
    * Function
    * Context method mixins
    * Numerical gradient checking
    * Profiler
    * Anomaly detection
    * Autograd graph
  * torch.library
    * `Library`
    * `fallthrough_kernel()`
  * torch.cpu
    * torch.cpu.current_stream
    * torch.cpu.is_available
    * torch.cpu.synchronize
    * torch.cpu.stream
    * torch.cpu.device_count
    * StreamContext
    * Streams and events
  * torch.cuda
    * StreamContext
    * torch.cuda.can_device_access_peer
    * torch.cuda.current_blas_handle
    * torch.cuda.current_device
    * torch.cuda.current_stream
    * torch.cuda.default_stream
    * device
    * torch.cuda.device_count
    * device_of
    * torch.cuda.get_arch_list
    * torch.cuda.get_device_capability
    * torch.cuda.get_device_name
    * torch.cuda.get_device_properties
    * torch.cuda.get_gencode_flags
    * torch.cuda.get_sync_debug_mode
    * torch.cuda.init
    * torch.cuda.ipc_collect
    * torch.cuda.is_available
    * torch.cuda.is_initialized
    * torch.cuda.memory_usage
    * torch.cuda.set_device
    * torch.cuda.set_stream
    * torch.cuda.set_sync_debug_mode
    * torch.cuda.stream
    * torch.cuda.synchronize
    * torch.cuda.utilization
    * torch.cuda.temperature
    * torch.cuda.power_draw
    * torch.cuda.clock_rate
    * torch.cuda.OutOfMemoryError
    * Random Number Generator
    * Communication collectives
    * Streams and events
    * Graphs (beta)
    * Memory management
    * NVIDIA Tools Extension (NVTX)
    * Jiterator (beta)
    * Stream Sanitizer (prototype)
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
    * Active Memory Timeline
    * Allocator State History
  * Snapshot API Reference
    * `_record_memory_history()`
    * `_snapshot()`
    * `_dump_snapshot()`
  * torch.mps
    * torch.mps.synchronize
    * torch.mps.get_rng_state
    * torch.mps.set_rng_state
    * torch.mps.manual_seed
    * torch.mps.seed
    * torch.mps.empty_cache
    * torch.mps.set_per_process_memory_fraction
    * torch.mps.current_allocated_memory
    * torch.mps.driver_allocated_memory
    * MPS Profiler
    * MPS Event
  * torch.backends
    * torch.backends.cpu
    * torch.backends.cuda
    * torch.backends.cudnn
    * torch.backends.mps
    * torch.backends.mkl
    * torch.backends.mkldnn
    * torch.backends.openmp
    * torch.backends.opt_einsum
    * torch.backends.xeon
  * torch.export
    * Overview
    * Exporting a PyTorch Model
    * Limitations of torch.export
    * Read More
    * API Reference
  * torch.distributed
    * Backends
    * Basics
    * Initialization
    * Post-Initialization
    * Distributed Key-Value Store
    * Groups
    * Point-to-point communication
    * Synchronous and asynchronous collective operations
    * Collective functions
    * Profiling Collective Communication
    * Multi-GPU collective functions
    * Third-party backends
    * Launch utility
    * Spawn utility
    * Debugging `torch.distributed` applications
    * Logging
  * torch.distributed.algorithms.join
    * `Join`
    * `Joinable`
    * `JoinHook`
  * torch.distributed.elastic
    * Get Started
    * Documentation
  * torch.distributed.fsdp
    * `FullyShardedDataParallel`
    * `BackwardPrefetch`
    * `ShardingStrategy`
    * `MixedPrecision`
    * `CPUOffload`
    * `StateDictConfig`
    * `FullStateDictConfig`
    * `ShardedStateDictConfig`
    * `LocalStateDictConfig`
    * `OptimStateDictConfig`
    * `FullOptimStateDictConfig`
    * `ShardedOptimStateDictConfig`
    * `LocalOptimStateDictConfig`
    * `StateDictSettings`
  * torch.distributed.optim
    * `DistributedOptimizer`
    * `PostLocalSGDOptimizer`
    * `ZeroRedundancyOptimizer`
  * torch.distributed.tensor.parallel
    * `parallelize_module()`
    * `RowwiseParallel`
    * `ColwiseParallel`
    * `PairwiseParallel`
    * `SequenceParallel`
    * `make_input_replicate_1d()`
    * `make_input_reshard_replicate()`
    * `make_input_shard_1d()`
    * `make_input_shard_1d_last_dim()`
    * `make_output_replicate_1d()`
    * `make_output_reshard_tensor()`
    * `make_output_shard_1d()`
    * `make_output_tensor()`
    * `enable_2d_with_fsdp()`
    * `pre_dp_module_transform()`
  * torch.distributed.checkpoint
    * `load_state_dict()`
    * `save_state_dict()`
    * `StorageReader`
    * `StorageWriter`
    * `LoadPlanner`
    * `LoadPlan`
    * `ReadItem`
    * `SavePlanner`
    * `SavePlan`
    * `WriteItem`
    * `FileSystemReader`
    * `FileSystemWriter`
    * `DefaultSavePlanner`
    * `DefaultLoadPlanner`
  * torch.distributions
    * Score function
    * Pathwise derivative
    * Distribution
    * ExponentialFamily
    * Bernoulli
    * Beta
    * Binomial
    * Categorical
    * Cauchy
    * Chi2
    * ContinuousBernoulli
    * Dirichlet
    * Exponential
    * FisherSnedecor
    * Gamma
    * Geometric
    * Gumbel
    * HalfCauchy
    * HalfNormal
    * Independent
    * Kumaraswamy
    * LKJCholesky
    * Laplace
    * LogNormal
    * LowRankMultivariateNormal
    * MixtureSameFamily
    * Multinomial
    * MultivariateNormal
    * NegativeBinomial
    * Normal
    * OneHotCategorical
    * Pareto
    * Poisson
    * RelaxedBernoulli
    * LogitRelaxedBernoulli
    * RelaxedOneHotCategorical
    * StudentT
    * TransformedDistribution
    * Uniform
    * VonMises
    * Weibull
    * Wishart
    * `KL Divergence`
    * `Transforms`
    * `Constraints`
    * `Constraint Registry`
  * torch.compiler
    * Read More
  * torch.fft
    * Fast Fourier Transforms
    * Helper Functions
  * torch.func
    * What are composable function transforms?
    * Why composable function transforms?
    * Read More
  * torch.futures
    * `Future`
    * `collect_all()`
    * `wait_all()`
  * torch.fx
    * Overview
    * Writing Transformations
    * Debugging
    * Limitations of Symbolic Tracing
    * API Reference
  * torch.hub
    * Publishing models
    * Loading models from Hub
  * torch.jit
    * TorchScript Language Reference
    * Creating TorchScript Code
    * Mixing Tracing and Scripting
    * TorchScript Language
    * Built-in Functions and Modules
    * Debugging
    * Frequently Asked Questions
    * Known Issues
    * Appendix
  * torch.linalg
    * Matrix Properties
    * Decompositions
    * Solvers
    * Inverses
    * Matrix Functions
    * Matrix Products
    * Tensor Operations
    * Misc
    * Experimental Functions
  * torch.monitor
    * API Reference
  * torch.signal
    * torch.signal.windows
  * torch.special
    * Functions
  * torch.overrides
    * Functions
  * torch.package
    * Tutorials
    * How do I…
    * Explanation
    * API Reference
  * torch.profiler
    * Overview
    * API Reference
    * Intel Instrumentation and Tracing Technology APIs
  * torch.nn.init
    * `calculate_gain()`
    * `uniform_()`
    * `normal_()`
    * `constant_()`
    * `ones_()`
    * `zeros_()`
    * `eye_()`
    * `dirac_()`
    * `xavier_uniform_()`
    * `xavier_normal_()`
    * `kaiming_uniform_()`
    * `kaiming_normal_()`
    * `trunc_normal_()`
    * `orthogonal_()`
    * `sparse_()`
  * torch.onnx
    * Overview
    * TorchDynamo-based ONNX Exporter
    * TorchScript-based ONNX Exporter
    * Contributing / Developing
  * torch.optim
    * How to use an optimizer
    * Base class
    * Algorithms
    * How to adjust learning rate
    * Weight Averaging (SWA and EMA)
  * Complex Numbers
    * Creating Complex Tensors
    * Transition from the old representation
    * Accessing real and imag
    * Angle and abs
    * Linear Algebra
    * Serialization
    * Autograd
  * DDP Communication Hooks
    * How to Use a Communication Hook?
    * What Does a Communication Hook Operate On?
    * Default Communication Hooks
    * PowerSGD Communication Hook
    * Debugging Communication Hooks
    * Checkpointing of Communication Hooks
    * Acknowledgements
  * Pipeline Parallelism
    * Model Parallelism using multiple GPUs
    * Pipelined Execution
    * Pipe APIs in PyTorch
    * Tutorials
    * Acknowledgements
  * Quantization
    * Introduction to Quantization
    * Quantization API Summary
    * Quantization Stack
    * Quantization Support Matrix
    * Quantization API Reference
    * Quantization Backend Configuration
    * Quantization Accuracy Debugging
    * Quantization Customizations
    * Best Practices
    * Frequently Asked Questions
    * Common Errors
  * Distributed RPC Framework
    * Basics
    * RPC
    * RRef
    * RemoteModule
    * Distributed Autograd Framework
    * Distributed Optimizer
    * Design Notes
    * Tutorials
  * torch.random
    * `fork_rng()`
    * `get_rng_state()`
    * `initial_seed()`
    * `manual_seed()`
    * `seed()`
    * `set_rng_state()`
  * torch.masked
    * Introduction
    * Supported Operators
  * torch.nested
    * Introduction
    * Construction
    * size
    * unbind
    * Nested tensor constructor and conversion functions
    * Supported operations
  * torch.sparse
    * Why and when to use sparsity
    * Functionality overview
    * Operator overview
    * Sparse Semi-Structured Tensors
    * Sparse COO tensors
    * Sparse Compressed Tensors
    * Supported operations
  * torch.Storage
    * `TypedStorage`
    * `UntypedStorage`
    * `DoubleStorage`
    * `FloatStorage`
    * `HalfStorage`
    * `LongStorage`
    * `IntStorage`
    * `ShortStorage`
    * `CharStorage`
    * `ByteStorage`
    * `BoolStorage`
    * `BFloat16Storage`
    * `ComplexDoubleStorage`
    * `ComplexFloatStorage`
    * `QUInt8Storage`
    * `QInt8Storage`
    * `QInt32Storage`
    * `QUInt4x2Storage`
    * `QUInt2x4Storage`
  * torch.testing
    * `assert_close()`
    * `make_tensor()`
    * `assert_allclose()`
  * torch.utils
    * torch.utils.rename_privateuse1_backend
    * torch.utils.generate_methods_for_privateuse1_backend
    * torch.utils.get_cpp_backtrace
    * torch.utils.set_module
  * torch.utils.benchmark
    * `Timer`
    * `Measurement`
    * `CallgrindStats`
    * `FunctionCounts`
  * torch.utils.bottleneck
  * torch.utils.checkpoint
    * `checkpoint()`
    * `checkpoint_sequential()`
  * torch.utils.cpp_extension
    * `CppExtension()`
    * `CUDAExtension()`
    * `BuildExtension()`
    * `load()`
    * `load_inline()`
    * `include_paths()`
    * `get_compiler_abi_compatibility_and_version()`
    * `verify_ninja_availability()`
    * `is_ninja_available()`
  * torch.utils.data
    * Dataset Types
    * Data Loading Order and `Sampler`
    * Loading Batched and Non-Batched Data
    * Single- and Multi-process Data Loading
    * Memory Pinning
  * torch.utils.jit
  * torch.utils.dlpack
    * `from_dlpack()`
    * `to_dlpack()`
  * torch.utils.mobile_optimizer
    * `optimize_for_mobile()`
  * torch.utils.model_zoo
    * `load_url()`
  * torch.utils.tensorboard
    * `SummaryWriter`
  * Type Info
    * torch.finfo
    * torch.iinfo
  * Named Tensors
    * Creating named tensors
    * Named dimensions
    * Name propagation semantics
    * Explicit alignment by names
    * Manipulating dimensions
    * Autograd support
    * Currently supported operations and subsystems
    * Named tensor API reference
  * Named Tensors operator coverage
    * Keeps input names
    * Removes dimensions
    * Unifies names from inputs
    * Permutes dimensions
    * Contracts away dims
    * Factory functions
    * out function and in-place variants
  * torch.__config__
    * `show()`
    * `parallel_info()`
  * torch._logging
    * torch._logging.set_logs

Libraries

  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices

# Indices and tables

  * Index
  * Module Index

© 2024, PyTorch Contributors  
PyTorch has a BSD-style license, as found in the LICENSE file.  
https://pytorch.org/docs/2.1/

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

