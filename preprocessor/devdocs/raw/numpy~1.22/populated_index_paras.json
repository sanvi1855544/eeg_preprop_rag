[{"name": "()", "path": "glossary", "type": "Glossary", "text": ["A parenthesized number followed by a comma denotes a tuple with one element. The trailing comma distinguishes a one-element tuple from a parenthesized n.", "In a dimension entry, instructs NumPy to choose the length that will keep the total number of array elements the same.", "An Ellipsis.", "When indexing an array, shorthand that the missing axes, if they exist, are full slices.", "It can be used at most once; a[...,0,...] raises an IndexError.", "The Python slice operator. In ndarrays, slicing can be applied to every axis:", "Trailing slices can be omitted:", "In contrast to Python, where slicing creates a copy, in NumPy slicing creates a view.", "For details, see Combining advanced and basic indexing.", "In a dtype declaration, indicates that the data is little-endian (the bracket is big on the right).", "In a dtype declaration, indicates that the data is big-endian (the bracket is big on the left).", "Rather than using a scalar or slice as an index, an axis can be indexed with an array, providing fine-grained selection. This is known as advanced indexing or \u201cfancy indexing\u201d.", "An operation along axis n of array a behaves as if its argument were an array of slices of a where each slice has a successive index of axis n.", "For example, if a is a 3 x N array, an operation along axis 0 behaves as if its argument were an array containing slices of each row:", "To make it concrete, we can pick the operation to be the array-reversal function numpy.flip, which accepts an axis argument. We construct a 3 x 4 array a:", "Reversing along axis 0 (the row axis) yields", "Recalling the definition of along an axis, flip along axis 0 is treating its argument as if it were", "and the result of np.flip(a,axis=0) is to reverse the slices:", "Used synonymously in the NumPy docs with ndarray.", "Any scalar or sequence that can be interpreted as an ndarray. In addition to ndarrays and scalars this category includes lists (possibly nested and with different element types) and tuples. Any argument accepted by numpy.array is array_like.", "An array scalar is an instance of the types/classes float32, float64, etc.. For uniformity in handling operands, NumPy treats a scalar as an array of zero dimension. In contrast, a 0-dimensional array is an ndarray instance containing precisely one value.", "Another term for an array dimension. Axes are numbered left to right; axis 0 is the first element in the shape tuple.", "In a two-dimensional vector, the elements of axis 0 are rows and the elements of axis 1 are columns.", "In higher dimensions, the picture changes. NumPy prints higher-dimensional vectors as replications of row-by-column building blocks, as in this three-dimensional vector:", "a is depicted as a two-element array whose elements are 2x3 vectors. From this point of view, rows and columns are the final two axes, respectively, in any shape.", "This rule helps you anticipate how a vector will be printed, and conversely how to find the index of any of the printed elements. For instance, in the example, the last two values of 8\u2019s index must be 0 and 2. Since 8 appears in the second of the two 2x3\u2019s, the first index must be 1:", "A convenient way to count dimensions in a printed vector is to count [ symbols after the open-parenthesis. This is useful in distinguishing, say, a (1,2,3) shape from a (2,3) shape:", "If an array does not own its memory, then its base attribute returns the object whose memory the array is referencing. That object may be referencing the memory from still another object, so the owning object may be a.base.base.base.... Some writers erroneously claim that testing base determines if arrays are views. For the correct way, see numpy.shares_memory.", "See Endianness.", "Basic Linear Algebra Subprograms", "broadcasting is NumPy\u2019s ability to process ndarrays of different sizes as if all were the same size.", "It permits an elegant do-what-I-mean behavior where, for instance, adding a scalar to a vector adds the scalar value to every element.", "Ordinarly, vector operands must all be the same size, because NumPy works element by element \u2013 for instance, c = a * b is", "But in certain useful cases, NumPy can duplicate data along \u201cmissing\u201d axes or \u201ctoo-short\u201d dimensions so shapes will match. The duplication costs no memory or time. For details, see Broadcasting.", "Same as row-major.", "See Row- and column-major order.", "See view.", "See axis.", "The datatype describing the (identically typed) elements in an ndarray. It can be changed to reinterpret the array contents. For details, see Data type objects (dtype).", "Another term for advanced indexing.", "In a structured data type, each subtype is called a field. The field has a name (a string), a type (any valid dtype), and an optional title. See Data type objects (dtype).", "Same as column-major.", "See ravel.", "All elements of a homogeneous array have the same type. ndarrays, in contrast to Python lists, are homogeneous. The type can be complicated, as in a structured array, but all elements have that type.", "NumPy object arrays, which contain references to Python objects, fill the role of heterogeneous arrays.", "The size of the dtype element in bytes.", "See Endianness.", "A boolean array used to select only certain elements for an operation:", "Bad or missing data can be cleanly ignored by putting it in a masked array, which has an internal boolean array indicating invalid entries. Operations with masked arrays ignore these entries.", "For details, see Masked arrays.", "NumPy\u2019s two-dimensional matrix class should no longer be used; use regular ndarrays.", "NumPy\u2019s basic structure.", "An array whose dtype is object; that is, it contains references to Python objects. Indexing the array dereferences the Python objects, so unlike other ndarrays, an object array has the ability to hold heterogeneous objects.", "numpy.ravel  and numpy.flatten  both flatten an ndarray. ravel will return a view if possible; flatten always returns a copy.", "Flattening collapses a multimdimensional array to a single dimension; details of how this is done (for instance, whether a[n+1] should be the next row or next column) are parameters.", "A structured array with allowing access in an attribute style (a.field) in addition to a['field']. For details, see numpy.recarray.", "See Row- and column-major order. NumPy creates arrays in row-major order by default.", "In NumPy, usually a synonym for array scalar.", "A tuple showing the length of each dimension of an ndarray. The length of the tuple itself is the number of dimensions (numpy.ndim). The product of the tuple elements is the number of elements in the array. For details, see numpy.ndarray.shape.", "Physical memory is one-dimensional; strides provide a mechanism to map a given index to an address in memory. For an N-dimensional array, its strides attribute is an N-element tuple; advancing from index i to index i+1 on axis n means adding a.strides[n] bytes to the address.", "Strides are computed automatically from an array\u2019s dtype and shape, but can be directly specified using as_strided.", "For details, see numpy.ndarray.strides.", "To see how striding underlies the power of NumPy views, see The NumPy array: a structure for efficient numerical computation. ", "Array whose dtype is a structured data type.", "Users can create arbitrarily complex dtypes that can include other arrays and dtypes. These composite dtypes are called structured data types.", "An array nested in a structured data type, as b is here:", "An element of a structured datatype that behaves like an ndarray.", "An alias for a field name in a structured datatype.", "In NumPy, usually a synonym for dtype. For the more general Python meaning, see here.", "NumPy\u2019s fast element-by-element computation (vectorization) gives a choice which function gets applied. The general term for the function is ufunc, short for universal function. NumPy routines have built-in ufuncs, but users can also write their own.", "NumPy hands off array processing to C, where looping and computation are much faster than in Python. To exploit this, programmers using NumPy eliminate Python loops in favor of array-to-array operations. vectorization can refer both to the C offloading and to structuring NumPy code to leverage it.", "Without touching underlying data, NumPy can make one array appear to change its datatype and shape.", "An array created this way is a view, and NumPy often exploits the performance gain of using a view versus making a new array.", "A potential drawback is that writing to a view can alter the original as well. If this is a problem, NumPy instead needs to create a physically distinct array \u2013 a copy.", "Some NumPy routines always return views, some always return copies, some may return one or the other, and for some the choice can be specified. Responsibility for managing views and copies falls to the programmer. numpy.shares_memory will check whether b is a view of a, but an exact answer isn\u2019t always feasible, as the documentation page explains."]}, {"name": "--overwrite-signature", "path": "f2py/usage", "type": "Using F2PY", "text": ["F2PY can be used either as a command line tool f2py or as a Python module numpy.f2py. While we try to provide the command line tool as part of the numpy setup, some platforms like Windows make it difficult to reliably put the executables on the PATH. We will refer to f2py in this document but you may have to run it as a module:", "If you run f2py with no arguments, and the line numpy Version at the end matches the NumPy version printed from python -m numpy.f2py, then you can use the shorter version. If not, or if you cannot run f2py, you should replace all calls to f2py here with the longer version.", "When used as a command line tool, f2py has three major modes, distinguished by the usage of -c and -h switches:", "To scan Fortran sources and generate a signature file, use", "Note", "A Fortran source file can contain many routines, and it is often not necessary to allow all routines be usable from Python. In such cases, either specify which routines should be wrapped (in the only: .. : part) or which routines F2PY should ignored (in the skip: .. : part).", "If <filename.pyf> is specified as stdout then signatures are written to standard output instead of a file.", "Among other options (see below), the following can be used in this mode:", "Overwrites an existing signature file.", "To construct an extension module, use", "The constructed extension module is saved as <modulename>module.c to the current directory.", "Here <fortran files> may also contain signature files. Among other options (see below), the following options can be used in this mode:", "Adds debugging hooks to the extension module. When using this extension module, various diagnostic information about the wrapper is written to the standard output, for example, the values of variables, the steps taken, etc.", "Add a CPP #include statement to the extension module source. <includefile> should be given in one of the following forms", "The include statement is inserted just before the wrapper functions. This feature enables using arbitrary C functions (defined in <includefile>) in F2PY generated wrappers.", "Note", "This option is deprecated. Use usercode statement to specify C code snippets directly in signature files.", "Create Fortran subroutine wrappers to Fortran functions. --wrap-functions is default because it ensures maximum portability and compiler independence.", "Search include files from given directories.", "List system resources found by numpy_distutils/system_info.py. For example, try f2py --help-link lapack_opt.", "To build an extension module, use", "If <fortran files> contains a signature file, then the source for an extension module is constructed, all Fortran and C sources are compiled, and finally all object and library files are linked to the extension module <modulename>.so which is saved into the current directory.", "If <fortran files> does not contain a signature file, then an extension module is constructed by scanning all Fortran source codes for routine signatures, before proceeding to build the extension module.", "Among other options (see below) and options described for previous modes, the following options can be used in this mode:", "List the available Fortran compilers.", "List the available Fortran compilers.", "Specify a Fortran compiler type by vendor.", "Specify the path to a F77 compiler", "Specify the path to a F77 compiler", "Specify the path to a F90 compiler", "Specify the path to a F90 compiler", "Specify F77 compiler flags", "Specify F90 compiler flags", "Specify optimization flags", "Specify architecture specific optimization flags", "Compile without optimization flags", "Compile without arch-dependent optimization flags", "Compile with debugging information", "Use the library <libname> when linking.", "Define macro <macro> as <defn>.", "Define macro <macro>", "Append directory <dir> to the list of directories searched for include files.", "Add directory <dir> to the list of directories to be searched for -l.", "Link the extension module with <resource> as defined by numpy_distutils/system_info.py. E.g. to link with optimized LAPACK libraries (vecLib on MacOSX, ATLAS elsewhere), use --link-lapack_opt. See also --help-link switch.", "Note", "The f2py -c option must be applied either to an existing .pyf file (plus the source/object/library files) or one must specify the -m <modulename> option (plus the sources/object/library files). Use one of the following options:", "or", "For more information, see the Building C and C++ Extensions Python documentation for details.", "When building an extension module, a combination of the following macros may be required for non-gcc Fortran compilers:", "To test the performance of F2PY generated interfaces, use -DF2PY_REPORT_ATEXIT. Then a report of various timings is printed out at the exit of Python. This feature may not work on all platforms, currently only Linux platform is supported.", "To see whether F2PY generated interface performs copies of array arguments, use -DF2PY_REPORT_ON_ARRAY_COPY=<int>. When the size of an array argument is larger than <int>, a message about the coping is sent to stderr.", "Name of an extension module. Default is untitled.", "Warning", "Don\u2019t use this option if a signature file (*.pyf) is used.", "Do [not] lower the cases in <fortran files>. By default, --lower is assumed with -h switch, and --no-lower without the -h switch.", "All F2PY generated files are created in <dirname>. Default is tempfile.mkdtemp().", "Run quietly.", "Run with extra verbosity.", "Print the F2PY version and exit.", "Execute f2py without any options to get an up-to-date list of available options.", "Warning", "The current Python interface to the f2py module is not mature and may change in the future.", "Fortran to Python Interface Generator.", "Build extension module from a Fortran 77 source string with f2py.", "Fortran source of module / subroutine to compile", "Changed in version 1.16.0: Accept str as well as bytes", "The name of the compiled python module", "Additional parameters passed to f2py", "Changed in version 1.16.0: A list of args may also be provided.", "Print f2py output to screen", "Name of the file where the fortran source is written. The default is to use a temporary file with the extension provided by the extension parameter", "Filename extension if source_fn is not provided. The extension tells which fortran standard is used. The default is f, which implies F77 standard.", "New in version 1.11.0.", "If True, return a subprocess.CompletedProcess containing the stdout and stderr of the compile process, instead of just the status code.", "New in version 1.20.0.", "0 on success, or a subprocess.CompletedProcess if full_output=True", "Return the directory that contains the fortranobject.c and .h files.", "Note", "This function is not needed when building an extension with numpy.distutils directly from .f and/or .pyf files in one go.", "Python extension modules built with f2py-generated code need to use fortranobject.c as a source file, and include the fortranobject.h header. This function can be used to obtain the directory containing both of these files.", "Absolute path to the directory containing fortranobject.c and fortranobject.h.", "See also", "function that returns the numpy include directory", "New in version 1.22.0.", "Unless the build system you are using has specific support for f2py, building a Python extension using a .pyf signature file is a two-step process. For a module mymod:", "Step 2: build your Python extension module. This requires the following source files:", "Equivalent to running:", "where <args>=string.join(<list>,' '), but in Python. Unless -h is used, this function returns a dictionary containing information on generated modules and their dependencies on source files. For example, the command f2py -m scalar scalar.f can be executed from Python as follows", "You cannot build extension modules with this function, that is, using -c is not allowed. Use compile command instead"]}, {"name": "1", "path": "reference/arrays.scalars", "type": "Scalars", "text": ["Python defines only one type of a particular data class (there is only one integer type, one floating-point type, etc.). This can be convenient in applications that don\u2019t need to be concerned with all the ways data can be represented in a computer. For scientific computing, however, more control is often needed.", "In NumPy, there are 24 new fundamental Python types to describe different types of scalars. These type descriptors are mostly based on the types available in the C language that CPython is written in, with several additional types compatible with Python\u2019s types.", "Array scalars have the same attributes and methods as ndarrays. 1 This allows one to treat items of an array partly on the same footing as arrays, smoothing out rough edges that result when mixing scalar and array operations.", "Array scalars live in a hierarchy (see the Figure below) of data types. They can be detected using the hierarchy: For example, isinstance(val, np.generic) will return True if val is an array scalar object. Alternatively, what kind of array scalar is present can be determined using other members of the data type hierarchy. Thus, for example isinstance(val, np.complexfloating) will return True if val is a complex valued type, while isinstance(val, np.flexible) will return true if val is one of the flexible itemsize array types (str_, bytes_, void).", "Figure: Hierarchy of type objects representing the array data types. Not shown are the two integer types intp and uintp which just point to the integer type that holds a pointer for the platform. All the number types can be obtained using bit-width names as well.", "However, array scalars are immutable, so none of the array scalar attributes are settable.", "The built-in scalar types are shown below. The C-like names are associated with character codes, which are shown in their descriptions. Use of the character codes, however, is discouraged.", "Some of the scalar types are essentially equivalent to fundamental Python types and therefore inherit from them as well as from the generic array scalar type:", "Array scalar type", "Related Python type", "Inherits?", "int_", "int", "Python 2 only", "float_", "float", "yes", "complex_", "complex", "yes", "bytes_", "bytes", "yes", "str_", "str", "yes", "bool_", "bool", "no", "datetime64", "datetime.datetime", "no", "timedelta64", "datetime.timedelta", "no", "The bool_ data type is very similar to the Python bool but does not inherit from it because Python\u2019s bool does not allow itself to be inherited from, and on the C-level the size of the actual bool data is not the same as a Python Boolean scalar.", "Warning", "The int_ type does not inherit from the int built-in under Python 3, because type int is no longer a fixed-width integer type.", "Tip", "The default data type in NumPy is float_.", "Base class for numpy scalar types.", "Class from which most (all?) numpy scalar types are derived. For consistency, exposes the same API as ndarray, despite many consequent attributes being either \u201cget-only,\u201d or completely irrelevant. This is the class from which it is strongly suggested users should derive custom scalar types.", "Abstract base class of all numeric scalar types.", "Abstract base class of all integer scalar types.", "Note", "The numpy integer types mirror the behavior of C integers, and can therefore be subject to Overflow Errors.", "Abstract base class of all signed integer scalar types.", "Signed integer type, compatible with C char.", "'b'", "numpy.int8: 8-bit signed integer (-128 to 127).", "Signed integer type, compatible with C short.", "'h'", "numpy.int16: 16-bit signed integer (-32_768 to 32_767).", "Signed integer type, compatible with C int.", "'i'", "numpy.int32: 32-bit signed integer (-2_147_483_648 to 2_147_483_647).", "Signed integer type, compatible with Python int and C long.", "'l'", "numpy.int64: 64-bit signed integer (-9_223_372_036_854_775_808 to 9_223_372_036_854_775_807).", "numpy.intp: Signed integer large enough to fit pointer, compatible with C intptr_t.", "Signed integer type, compatible with C long long.", "'q'", "Abstract base class of all unsigned integer scalar types.", "Unsigned integer type, compatible with C unsigned char.", "'B'", "numpy.uint8: 8-bit unsigned integer (0 to 255).", "Unsigned integer type, compatible with C unsigned short.", "'H'", "numpy.uint16: 16-bit unsigned integer (0 to 65_535).", "Unsigned integer type, compatible with C unsigned int.", "'I'", "numpy.uint32: 32-bit unsigned integer (0 to 4_294_967_295).", "Unsigned integer type, compatible with C unsigned long.", "'L'", "numpy.uint64: 64-bit unsigned integer (0 to 18_446_744_073_709_551_615).", "numpy.uintp: Unsigned integer large enough to fit pointer, compatible with C uintptr_t.", "Signed integer type, compatible with C unsigned long long.", "'Q'", "Abstract base class of all numeric scalar types with a (potentially) inexact representation of the values in its range, such as floating-point numbers.", "Note", "Inexact scalars are printed using the fewest decimal digits needed to distinguish their value from other values of the same datatype, by judicious rounding. See the unique parameter of format_float_positional and format_float_scientific.", "This means that variables with equal binary values but whose datatypes are of different precisions may display differently:", "Note that none of these floats hold the exact value \\(\\frac{1}{10}\\); f16 prints as 0.1 because it is as close to that value as possible, whereas the other types do not as they have more precision and therefore have closer values.", "Conversely, floating-point scalars of different precisions which approximate the same decimal value may compare unequal despite printing identically:", "Abstract base class of all floating-point scalar types.", "Half-precision floating-point number type.", "'e'", "numpy.float16: 16-bit-precision floating-point number type: sign bit, 5 bits exponent, 10 bits mantissa.", "Single-precision floating-point number type, compatible with C float.", "'f'", "numpy.float32: 32-bit-precision floating-point number type: sign bit, 8 bits exponent, 23 bits mantissa.", "Double-precision floating-point number type, compatible with Python float and C double.", "'d'", "numpy.float_", "numpy.float64: 64-bit precision floating-point number type: sign bit, 11 bits exponent, 52 bits mantissa.", "Extended-precision floating-point number type, compatible with C long double but not necessarily with IEEE 754 quadruple-precision.", "'g'", "numpy.longfloat", "numpy.float128: 128-bit extended-precision floating-point number type.", "Abstract base class of all complex number scalar types that are made up of floating-point numbers.", "Complex number type composed of two single-precision floating-point numbers.", "'F'", "numpy.singlecomplex", "numpy.complex64: Complex number type composed of 2 32-bit-precision floating-point numbers.", "Complex number type composed of two double-precision floating-point numbers, compatible with Python complex.", "'D'", "numpy.cfloat", "numpy.complex_", "numpy.complex128: Complex number type composed of 2 64-bit-precision floating-point numbers.", "Complex number type composed of two extended-precision floating-point numbers.", "'G'", "numpy.clongfloat", "numpy.longcomplex", "numpy.complex256: Complex number type composed of 2 128-bit extended-precision floating-point numbers.", "Boolean type (True or False), stored as a byte.", "Warning", "The bool_ type is not a subclass of the int_ type (the bool_ is not even a number type). This is different than Python\u2019s default implementation of bool as a sub-class of int.", "'?'", "numpy.bool8", "If created from a 64-bit integer, it represents an offset from 1970-01-01T00:00:00. If created from string, the string can be in ISO 8601 date or datetime format.", "See Datetimes and Timedeltas for more information.", "'M'", "A timedelta stored as a 64-bit integer.", "See Datetimes and Timedeltas for more information.", "'m'", "Any Python object.", "'O'", "Note", "The data actually stored in object arrays (i.e., arrays having dtype object_) are references to Python objects, not the objects themselves. Hence, object arrays behave more like usual Python lists, in the sense that their contents need not be of the same Python type.", "The object type is also special because an array containing object_ items does not return an object_ object on item access, but instead returns the actual object that the array item refers to.", "The following data types are flexible: they have no predefined size and the data they describe can be of different length in different arrays. (In the character codes # is an integer denoting how many elements the data type consists of.)", "Abstract base class of all scalar types without predefined length. The actual size of these types depends on the specific np.dtype instantiation.", "A byte string.", "When used in arrays, this type strips trailing null bytes.", "'S'", "numpy.string_", "A unicode string.", "When used in arrays, this type strips trailing null codepoints.", "Unlike the builtin str, this supports the Buffer Protocol, exposing its contents as UCS4:", "'U'", "numpy.unicode_", "Either an opaque sequence of bytes, or a structure.", "Structured void scalars can only be constructed via extraction from Structured arrays:", "'V'", "Warning", "See Note on string types.", "Numeric Compatibility: If you used old typecode characters in your Numeric code (which was never recommended), you will need to change some of them to the new characters. In particular, the needed changes are c -> S1, b -> B, 1 -> b, s -> h, w ->\nH, and u -> I. These changes make the type character convention more consistent with other Python modules such as the struct module.", "Along with their (mostly) C-derived names, the integer, float, and complex data-types are also available using a bit-width convention so that an array of the right size can always be ensured. Two aliases (numpy.intp and numpy.uintp) pointing to the integer type that is sufficiently large to hold a C pointer are also provided.", "alias of numpy.bool_", "Aliases for the signed integer types (one of numpy.byte, numpy.short, numpy.intc, numpy.int_ and numpy.longlong) with the specified number of bits.", "Compatible with the C99 int8_t, int16_t, int32_t, and int64_t, respectively.", "Alias for the unsigned integer types (one of numpy.ubyte, numpy.ushort, numpy.uintc, numpy.uint and numpy.ulonglong) with the specified number of bits.", "Compatible with the C99 uint8_t, uint16_t, uint32_t, and uint64_t, respectively.", "Alias for the signed integer type (one of numpy.byte, numpy.short, numpy.intc, numpy.int_ and np.longlong) that is the same size as a pointer.", "Compatible with the C intptr_t.", "'p'", "Alias for the unsigned integer type (one of numpy.ubyte, numpy.ushort, numpy.uintc, numpy.uint and np.ulonglong) that is the same size as a pointer.", "Compatible with the C uintptr_t.", "'P'", "alias of numpy.half", "alias of numpy.single", "alias of numpy.double", "Alias for numpy.longdouble, named after its size in bits. The existence of these aliases depends on the platform.", "alias of numpy.csingle", "alias of numpy.cdouble", "Alias for numpy.clongdouble, named after its size in bits. The existence of these aliases depends on the platform.", "The first two of these are conveniences which resemble the names of the builtin types, in the same style as bool_, int_, str_, bytes_, and object_:", "alias of numpy.double", "alias of numpy.cdouble", "Some more use alternate naming conventions for extended-precision floats and complex numbers:", "alias of numpy.longdouble", "alias of numpy.csingle", "alias of numpy.cdouble", "alias of numpy.clongdouble", "alias of numpy.clongdouble", "The following aliases originate from Python 2, and it is recommended that they not be used in new code.", "alias of numpy.bytes_", "alias of numpy.str_", "The array scalar objects have an array priority of NPY_SCALAR_PRIORITY (-1,000,000.0). They also do not (yet) have a ctypes attribute. Otherwise, they share the same attributes as arrays:", "generic.flags", "The integer value of flags.", "generic.shape", "Tuple of array dimensions.", "generic.strides", "Tuple of bytes steps in each dimension.", "generic.ndim", "The number of array dimensions.", "generic.data", "Pointer to start of data.", "generic.size", "The number of elements in the gentype.", "generic.itemsize", "The length of one element in bytes.", "generic.base", "Scalar attribute identical to the corresponding array attribute.", "generic.dtype", "Get array data-descriptor.", "generic.real", "The real part of the scalar.", "generic.imag", "The imaginary part of the scalar.", "generic.flat", "A 1-D view of the scalar.", "generic.T", "Scalar attribute identical to the corresponding array attribute.", "generic.__array_interface__", "Array protocol: Python side", "generic.__array_struct__", "Array protocol: struct", "generic.__array_priority__", "Array priority.", "generic.__array_wrap__", "sc.__array_wrap__(obj) return scalar from array", "See also", "Indexing routines, Data type objects (dtype)", "Array scalars can be indexed like 0-dimensional arrays: if x is an array scalar,", "Array scalars have exactly the same methods as arrays. The default behavior of these methods is to internally convert the scalar to an equivalent 0-dimensional array and to call the corresponding array method. In addition, math operations on array scalars are defined so that the same hardware flags are set and used to interpret the results as for ufunc, so that the error state used for ufuncs also carries over to the math on array scalars.", "The exceptions to the above rules are given below:", "generic.__array__", "sc.__array__(dtype) return 0-dim array from scalar with specified dtype", "generic.__array_wrap__", "sc.__array_wrap__(obj) return scalar from array", "generic.squeeze", "Scalar method identical to the corresponding array attribute.", "generic.byteswap", "Scalar method identical to the corresponding array attribute.", "generic.__reduce__", "Helper for pickle.", "generic.__setstate__", "generic.setflags", "Scalar method identical to the corresponding array attribute.", "Utility method for typing:", "number.__class_getitem__(item, /)", "Return a parametrized wrapper around the number type.", "There are two ways to effectively define a new array scalar type (apart from composing structured types dtypes from the built-in scalar types): One way is to simply subclass the ndarray and overwrite the methods of interest. This will work to a degree, but internally certain behaviors are fixed by the data type of the array. To fully customize the data type of an array you need to define a new data-type, and register it with NumPy. Such new types can only be defined in C, using the NumPy C-API."]}, {"name": "1", "path": "reference/random/parallel", "type": "Parallel Applications", "text": ["There are three strategies implemented that can be used to produce repeatable pseudo-random numbers across multiple processes (local or distributed).", "SeedSequence implements an algorithm to process a user-provided seed, typically as an integer of some size, and to convert it into an initial state for a BitGenerator. It uses hashing techniques to ensure that low-quality seeds are turned into high quality initial states (at least, with very high probability).", "For example, MT19937 has a state consisting of 624 uint32 integers. A naive way to take a 32-bit integer seed would be to just set the last element of the state to the 32-bit seed and leave the rest 0s. This is a valid state for MT19937, but not a good one. The Mersenne Twister algorithm suffers if there are too many 0s. Similarly, two adjacent 32-bit integer seeds (i.e. 12345 and 12346) would produce very similar streams.", "SeedSequence avoids these problems by using successions of integer hashes with good avalanche properties to ensure that flipping any bit in the input input has about a 50% chance of flipping any bit in the output. Two input seeds that are very close to each other will produce initial states that are very far from each other (with very high probability). It is also constructed in such a way that you can provide arbitrary-sized integers or lists of integers. SeedSequence will take all of the bits that you provide and mix them together to produce however many bits the consuming BitGenerator needs to initialize itself.", "These properties together mean that we can safely mix together the usual user-provided seed with simple incrementing counters to get BitGenerator states that are (to very high probability) independent of each other. We can wrap this together into an API that is easy to use and difficult to misuse.", "Child SeedSequence objects can also spawn to make grandchildren, and so on. Each SeedSequence has its position in the tree of spawned SeedSequence objects mixed in with the user-provided seed to generate independent (with very high probability) streams.", "This feature lets you make local decisions about when and how to split up streams without coordination between processes. You do not have to preallocate space to avoid overlapping or request streams from a common global service. This general \u201ctree-hashing\u201d scheme is not unique to numpy but not yet widespread. Python has increasingly-flexible mechanisms for parallelization available, and this scheme fits in very well with that kind of use.", "Using this scheme, an upper bound on the probability of a collision can be estimated if one knows the number of streams that you derive. SeedSequence hashes its inputs, both the seed and the spawn-tree-path, down to a 128-bit pool by default. The probability that there is a collision in that pool, pessimistically-estimated (1), will be about \\(n^2*2^{-128}\\) where n is the number of streams spawned. If a program uses an aggressive million streams, about \\(2^{20}\\), then the probability that at least one pair of them are identical is about \\(2^{-88}\\), which is in solidly-ignorable territory (2).", "The algorithm is carefully designed to eliminate a number of possible ways to collide. For example, if one only does one level of spawning, it is guaranteed that all states will be unique. But it\u2019s easier to estimate the naive upper bound on a napkin and take comfort knowing that the probability is actually lower.", "In this calculation, we can mostly ignore the amount of numbers drawn from each stream. See Upgrading PCG64 with PCG64DXSM for the technical details about PCG64. The other PRNGs we provide have some extra protection built in that avoids overlaps if the SeedSequence pools differ in the slightest bit. PCG64DXSM has \\(2^{127}\\) separate cycles determined by the seed in addition to the position in the \\(2^{128}\\) long period for each cycle, so one has to both get on or near the same cycle and seed a nearby position in the cycle. Philox has completely independent cycles determined by the seed. SFC64 incorporates a 64-bit counter so every unique seed is at least \\(2^{64}\\) iterations away from any other seed. And finally, MT19937 has just an unimaginably huge period. Getting a collision internal to SeedSequence is the way a failure would be observed.", "Philox is a counter-based RNG based which generates values by encrypting an incrementing counter using weak cryptographic primitives. The seed determines the key that is used for the encryption. Unique keys create unique, independent streams. Philox lets you bypass the seeding algorithm to directly set the 128-bit key. Similar, but different, keys will still create independent streams.", "This scheme does require that you avoid reusing stream IDs. This may require coordination between the parallel processes.", "jumped advances the state of the BitGenerator as-if a large number of random numbers have been drawn, and returns a new instance with this state. The specific number of draws varies by BitGenerator, and ranges from \\(2^{64}\\) to \\(2^{128}\\). Additionally, the as-if draws also depend on the size of the default random number produced by the specific BitGenerator. The BitGenerators that support jumped, along with the period of the BitGenerator, the size of the jump and the bits in the default unsigned random are listed below.", "BitGenerator", "Period", "Jump Size", "Bits per Draw", "MT19937", "\\(2^{19937}-1\\)", "\\(2^{128}\\)", "32", "PCG64", "\\(2^{128}\\)", "\\(~2^{127}\\) (3)", "64", "PCG64DXSM", "\\(2^{128}\\)", "\\(~2^{127}\\) (3)", "64", "Philox", "\\(2^{256}\\)", "\\(2^{128}\\)", "64", "The jump size is \\((\\phi-1)*2^{128}\\) where \\(\\phi\\) is the golden ratio. As the jumps wrap around the period, the actual distances between neighboring streams will slowly grow smaller than the jump size, but using the golden ratio this way is a classic method of constructing a low-discrepancy sequence that spreads out the states around the period optimally. You will not be able to jump enough to make those distances small enough to overlap in your lifetime.", "jumped can be used to produce long blocks which should be long enough to not overlap.", "When using jumped, one does have to take care not to jump to a stream that was already used. In the above example, one could not later use blocked_rng[0].jumped() as it would overlap with blocked_rng[1]. Like with the independent streams, if the main process here wants to split off 10 more streams by jumping, then it needs to start with range(10, 20), otherwise it would recreate the same streams. On the other hand, if you carefully construct the streams, then you are guaranteed to have streams that do not overlap."]}, {"name": "1", "path": "user/basics.broadcasting", "type": "User Guide", "text": ["See also", "numpy.broadcast", "The term broadcasting describes how NumPy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is \u201cbroadcast\u201d across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations. There are, however, cases where broadcasting is a bad idea because it leads to inefficient use of memory that slows computation.", "NumPy operations are usually done on pairs of arrays on an element-by-element basis. In the simplest case, the two arrays must have exactly the same shape, as in the following example:", "NumPy\u2019s broadcasting rule relaxes this constraint when the arrays\u2019 shapes meet certain constraints. The simplest broadcasting example occurs when an array and a scalar value are combined in an operation:", "The result is equivalent to the previous example where b was an array. We can think of the scalar b being stretched during the arithmetic operation into an array with the same shape as a. The new elements in b, as shown in Figure 1, are simply copies of the original scalar. The stretching analogy is only conceptual. NumPy is smart enough to use the original scalar value without actually making copies so that broadcasting operations are as memory and computationally efficient as possible.", "Figure 1", "In the simplest example of broadcasting, the scalar b is stretched to become an array of same shape as a so the shapes are compatible for element-by-element multiplication.", "The code in the second example is more efficient than that in the first because broadcasting moves less memory around during the multiplication (b is a scalar rather than an array).", "When operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing (i.e. rightmost) dimensions and works its way left. Two dimensions are compatible when", "If these conditions are not met, a ValueError: operands could not be broadcast together exception is thrown, indicating that the arrays have incompatible shapes. The size of the resulting array is the size that is not 1 along each axis of the inputs.", "Arrays do not need to have the same number of dimensions. For example, if you have a 256x256x3 array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:", "When either of the dimensions compared is one, the other is used. In other words, dimensions with size 1 are stretched or \u201ccopied\u201d to match the other.", "In the following example, both the A and B arrays have axes with length one that are expanded to a larger size during the broadcast operation:", "A set of arrays is called \u201cbroadcastable\u201d to the same shape if the above rules produce a valid result.", "For example, if a.shape is (5,1), b.shape is (1,6), c.shape is (6,) and d.shape is () so that d is a scalar, then a, b, c, and d are all broadcastable to dimension (5,6); and", "Here are some more examples:", "Here are examples of shapes that do not broadcast:", "An example of broadcasting when a 1-d array is added to a 2-d array:", "As shown in Figure 2, b is added to each row of a. In Figure 3, an exception is raised because of the incompatible shapes.", "Figure 2", "A one dimensional array added to a two dimensional array results in broadcasting if number of 1-d array elements matches the number of 2-d array columns.", "Figure 3", "When the trailing dimensions of the arrays are unequal, broadcasting fails because it is impossible to align the values in the rows of the 1st array with the elements of the 2nd arrays for element-by-element addition.", "Broadcasting provides a convenient way of taking the outer product (or any other outer operation) of two arrays. The following example shows an outer addition operation of two 1-d arrays:", "Figure 4", "In some cases, broadcasting stretches both arrays to form an output array larger than either of the initial arrays.", "Here the newaxis index operator inserts a new axis into a, making it a two-dimensional 4x1 array. Combining the 4x1 array with b, which has shape (3,), yields a 4x3 array.", "Broadcasting comes up quite often in real world problems. A typical example occurs in the vector quantization (VQ) algorithm used in information theory, classification, and other related areas. The basic operation in VQ finds the closest point in a set of points, called codes in VQ jargon, to a given point, called the observation. In the very simple, two-dimensional case shown below, the values in observation describe the weight and height of an athlete to be classified. The codes represent different classes of athletes. 1 Finding the closest point requires calculating the distance between observation and each of the codes. The shortest distance provides the best match. In this example, codes[0] is the closest class indicating that the athlete is likely a basketball player.", "In this example, the observation array is stretched to match the shape of the codes array:", "Figure 5", "The basic operation of vector quantization calculates the distance between an object to be classified, the dark square, and multiple known codes, the gray circles. In this simple case, the codes represent individual classes. More complex cases use multiple codes per class.", "Typically, a large number of observations, perhaps read from a database, are compared to a set of codes. Consider this scenario:", "The three-dimensional array, diff, is a consequence of broadcasting, not a necessity for the calculation. Large data sets will generate a large intermediate array that is computationally inefficient. Instead, if each observation is calculated individually using a Python loop around the code in the two-dimensional example above, a much smaller array is used.", "Broadcasting is a powerful tool for writing short and usually intuitive code that does its computations very efficiently in C. However, there are cases when broadcasting uses unnecessarily large amounts of memory for a particular algorithm. In these cases, it is better to write the algorithm\u2019s outer loop in Python. This may also produce more readable code, as algorithms that use broadcasting tend to become more difficult to interpret as the number of dimensions in the broadcast increases.", "In this example, weight has more impact on the distance calculation than height because of the larger values. In practice, it is important to normalize the height and weight, often by their standard deviation across the data set, so that both have equal influence on the distance calculation."]}, {"name": "1", "path": "reference/routines.polynomials", "type": "Polynomials", "text": ["Polynomials in NumPy can be created, manipulated, and even fitted using the convenience classes of the numpy.polynomial package, introduced in NumPy 1.4.", "Prior to NumPy 1.4, numpy.poly1d was the class of choice and it is still available in order to maintain backward compatibility. However, the newer polynomial package is more complete and its convenience classes provide a more consistent, better-behaved interface for working with polynomial expressions. Therefore numpy.polynomial is recommended for new coding.", "Note", "Terminology", "The term polynomial module refers to the old API defined in numpy.lib.polynomial, which includes the numpy.poly1d class and the polynomial functions prefixed with poly accessible from the numpy namespace (e.g. numpy.polyadd, numpy.polyval, numpy.polyfit, etc.).", "The term polynomial package refers to the new API defined in numpy.polynomial, which includes the convenience classes for the different kinds of polynomials (numpy.polynomial.Polynomial, numpy.polynomial.Chebyshev, etc.).", "As noted above, the poly1d class and associated functions defined in numpy.lib.polynomial, such as numpy.polyfit and numpy.poly, are considered legacy and should not be used in new code. Since NumPy version 1.4, the numpy.polynomial package is preferred for working with polynomials.", "The following table highlights some of the main differences between the legacy polynomial module and the polynomial package for common tasks. The Polynomial class is imported for brevity:", "How to\u2026", "Legacy (numpy.poly1d)", "numpy.polynomial", "Create a polynomial object from coefficients 1", "p = np.poly1d([1, 2, 3])", "p = Polynomial([3, 2, 1])", "Create a polynomial object from roots", "r = np.poly([-1, 1]) p = np.poly1d(r)", "p = Polynomial.fromroots([-1, 1])", "Fit a polynomial of degree deg to data", "np.polyfit(x, y, deg)", "Polynomial.fit(x, y, deg)", "Note the reversed ordering of the coefficients", "There are significant differences between numpy.lib.polynomial and numpy.polynomial. The most significant difference is the ordering of the coefficients for the polynomial expressions. The various routines in numpy.polynomial all deal with series whose coefficients go from degree zero upward, which is the reverse order of the poly1d convention. The easy way to remember this is that indices correspond to degree, i.e., coef[i] is the coefficient of the term of degree i.", "Though the difference in convention may be confusing, it is straightforward to convert from the legacy polynomial API to the new. For example, the following demonstrates how you would convert a numpy.poly1d instance representing the expression \\(x^{2} + 2x + 3\\) to a Polynomial instance representing the same expression:", "In addition to the coef attribute, polynomials from the polynomial package also have domain and window attributes. These attributes are most relevant when fitting polynomials to data, though it should be noted that polynomials with different domain and window attributes are not considered equal, and can\u2019t be mixed in arithmetic:", "See the documentation for the convenience classes for further details on the domain and window attributes.", "Another major difference between the legacy polynomial module and the polynomial package is polynomial fitting. In the old module, fitting was done via the polyfit function. In the polynomial package, the fit class method is preferred. For example, consider a simple linear fit to the following data:", "With the legacy polynomial module, a linear fit (i.e. polynomial of degree 1) could be applied to these data with polyfit:", "With the new polynomial API, the fit class method is preferred:", "Note that the coefficients are given in the scaled domain defined by the linear mapping between the window and domain. convert can be used to get the coefficients in the unscaled data domain.", "In addition to standard power series polynomials, the polynomial package provides several additional kinds of polynomials including Chebyshev, Hermite (two subtypes), Laguerre, and Legendre polynomials. Each of these has an associated convenience class available from the numpy.polynomial namespace that provides a consistent interface for working with polynomials regardless of their type.", "Documentation pertaining to specific functions defined for each kind of polynomial individually can be found in the corresponding module documentation:"]}, {"name": "1", "path": "reference/routines.polynomials.chebyshev", "type": "Chebyshev Series ( \n        \n         numpy.polynomial.chebyshev\n        \n        )", "text": ["This module provides a number of objects (mostly functions) useful for dealing with Chebyshev series, including a Chebyshev class that encapsulates the usual arithmetic operations. (General information on how this module represents and works with such polynomials is in the docstring for its \u201cparent\u201d sub-package, numpy.polynomial).", "Chebyshev(coef[, domain, window])", "A Chebyshev series class.", "chebdomain", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "chebzero", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "chebone", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "chebx", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "chebadd(c1, c2)", "Add one Chebyshev series to another.", "chebsub(c1, c2)", "Subtract one Chebyshev series from another.", "chebmulx(c)", "Multiply a Chebyshev series by x.", "chebmul(c1, c2)", "Multiply one Chebyshev series by another.", "chebdiv(c1, c2)", "Divide one Chebyshev series by another.", "chebpow(c, pow[, maxpower])", "Raise a Chebyshev series to a power.", "chebval(x, c[, tensor])", "Evaluate a Chebyshev series at points x.", "chebval2d(x, y, c)", "Evaluate a 2-D Chebyshev series at points (x, y).", "chebval3d(x, y, z, c)", "Evaluate a 3-D Chebyshev series at points (x, y, z).", "chebgrid2d(x, y, c)", "Evaluate a 2-D Chebyshev series on the Cartesian product of x and y.", "chebgrid3d(x, y, z, c)", "Evaluate a 3-D Chebyshev series on the Cartesian product of x, y, and z.", "chebder(c[, m, scl, axis])", "Differentiate a Chebyshev series.", "chebint(c[, m, k, lbnd, scl, axis])", "Integrate a Chebyshev series.", "chebfromroots(roots)", "Generate a Chebyshev series with given roots.", "chebroots(c)", "Compute the roots of a Chebyshev series.", "chebvander(x, deg)", "Pseudo-Vandermonde matrix of given degree.", "chebvander2d(x, y, deg)", "Pseudo-Vandermonde matrix of given degrees.", "chebvander3d(x, y, z, deg)", "Pseudo-Vandermonde matrix of given degrees.", "chebgauss(deg)", "Gauss-Chebyshev quadrature.", "chebweight(x)", "The weight function of the Chebyshev polynomials.", "chebcompanion(c)", "Return the scaled companion matrix of c.", "chebfit(x, y, deg[, rcond, full, w])", "Least squares fit of Chebyshev series to data.", "chebpts1(npts)", "Chebyshev points of the first kind.", "chebpts2(npts)", "Chebyshev points of the second kind.", "chebtrim(c[, tol])", "Remove \"small\" \"trailing\" coefficients from a polynomial.", "chebline(off, scl)", "Chebyshev series whose graph is a straight line.", "cheb2poly(c)", "Convert a Chebyshev series to a polynomial.", "poly2cheb(pol)", "Convert a polynomial to a Chebyshev series.", "chebinterpolate(func, deg[, args])", "Interpolate a function at the Chebyshev points of the first kind.", "numpy.polynomial", "The implementations of multiplication, division, integration, and differentiation use the algebraic identities [1]:", "where", "These identities allow a Chebyshev series to be expressed as a finite, symmetric Laurent series. In this module, this sort of Laurent series is referred to as a \u201cz-series.\u201d", "A. T. Benjamin, et al., \u201cCombinatorial Trigonometry with Chebyshev Polynomials,\u201d Journal of Statistical Planning and Inference 14, 2008 (https://web.archive.org/web/20080221202153/https://www.math.hmc.edu/~benjamin/papers/CombTrig.pdf, pg. 4)"]}, {"name": "add_data_dir()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.add_data_dir", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": ["Recursively add files under data_path to data_files list.", "Recursively add files under data_path to the list of data_files to be installed (and distributed). The data_path can be either a relative path-name, or an absolute path-name, or a 2-tuple where the first argument shows where in the install directory the data directory should be installed to.", "Argument can be either", "Rules for installation paths:", "For example suppose the source directory contains fun/foo.dat and fun/bar/car.dat:", "Will install data-files to the locations:"]}, {"name": "add_data_files()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.add_data_files", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": ["Add data files to configuration data_files.", "Argument(s) can be either", "The form of each element of the files sequence is very flexible allowing many combinations of where to get the files from the package and where they should ultimately be installed on the system. The most basic usage is for an element of the files argument sequence to be a simple filename. This will cause that file from the local path to be installed to the installation path of the self.name package (package path). The file argument can also be a relative path in which case the entire relative path will be installed into the package directory. Finally, the file can be an absolute path name in which case the file will be found at the absolute path name but installed to the package path.", "This basic behavior can be augmented by passing a 2-tuple in as the file argument. The first element of the tuple should specify the relative path (under the package install directory) where the remaining sequence of files should be installed to (it has nothing to do with the file-names in the source distribution). The second element of the tuple is the sequence of files that should be installed. The files in this sequence can be filenames, relative paths, or absolute paths. For absolute paths the file will be installed in the top-level package installation directory (regardless of the first argument). Filenames and relative path names will be installed in the package install directory under the path name given as the first element of the tuple.", "Rules for installation paths:", "An additional feature is that the path to a data-file can actually be a function that takes no arguments and returns the actual path(s) to the data-files. This is useful when the data files are generated while building the package.", "Add files to the list of data_files to be included with the package.", "will install these data files to:", "where <package install directory> is the package (or sub-package) directory such as \u2018/usr/lib/python2.4/site-packages/mypackage\u2019 (\u2018C: Python2.4 Lib site-packages mypackage\u2019) or \u2018/usr/lib/python2.4/site- packages/mypackage/mysubpackage\u2019 (\u2018C: Python2.4 Lib site-packages mypackage mysubpackage\u2019)."]}, {"name": "add_extension()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.add_extension", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": ["Add extension to configuration.", "Create and add an Extension instance to the ext_modules list. This method also takes the following optional keyword arguments that are passed on to the Extension constructor.", "name of the extension", "list of the sources. The list of sources may contain functions (called source generators) which must take an extension instance and a build directory as inputs and return a source file or list of source files or None. If None is returned then no sources are generated. If the Extension instance has no sources after processing all source generators, then no extension module is built.", "The depends list contains paths to files or directories that the sources of the extension module depend on. If any path in the depends list is newer than the extension module, then the module will be rebuilt.", "dict or list of dict of keywords to be appended to keywords.", "The self.paths(\u2026) method is applied to all lists that may contain paths."]}, {"name": "add_headers()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.add_headers", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": ["Add installable headers to configuration.", "Add the given sequence of files to the beginning of the headers list. By default, headers will be installed under <python- include>/<self.name.replace(\u2018.\u2019,\u2019/\u2019)>/ directory. If an item of files is a tuple, then its first argument specifies the actual installation location relative to the <python-include> path.", "Argument(s) can be either:"]}, {"name": "add_include_dirs()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.add_include_dirs", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": ["Add paths to configuration include directories.", "Add the given sequence of paths to the beginning of the include_dirs list. This list will be visible to all extension modules of the current package."]}, {"name": "add_installed_library()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.add_installed_library", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": ["Similar to add_library, but the specified library is installed.", "Most C libraries used with distutils are only used to build python extensions, but libraries built through this method will be installed so that they can be reused by third-party packages.", "Name of the installed library.", "List of the library\u2019s source files. See add_library for details.", "Path to install the library, relative to the current sub-package.", "The following keys are allowed:", "See also", "The best way to encode the options required to link against the specified C libraries is to use a \u201clibname.ini\u201d file, and use get_info to retrieve the required options (see add_npy_pkg_config for more information)."]}, {"name": "add_library()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.add_library", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": ["Add library to configuration.", "Name of the extension.", "List of the sources. The list of sources may contain functions (called source generators) which must take an extension instance and a build directory as inputs and return a source file or list of source files or None. If None is returned then no sources are generated. If the Extension instance has no sources after processing all source generators, then no extension module is built.", "The following keys are allowed:"]}, {"name": "add_npy_pkg_config()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.add_npy_pkg_config", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": ["Generate and install a npy-pkg config file from a template.", "The config file generated from template is installed in the given install directory, using subst_dict for variable substitution.", "The path of the template, relatively to the current package path.", "Where to install the npy-pkg config file, relatively to the current package path.", "If given, any string of the form @key@ will be replaced by subst_dict[key] in the template file when installed. The install prefix is always available through the variable @prefix@, since the install prefix is not easy to get reliably from setup.py.", "See also", "This works for both standard installs and in-place builds, i.e. the @prefix@ refer to the source directory for in-place builds.", "Assuming the foo.ini.in file has the following content:", "The generated file will have the following content:", "and will be installed as foo.ini in the \u2018lib\u2019 subpath.", "When cross-compiling with numpy distutils, it might be necessary to use modified npy-pkg-config files. Using the default/generated files will link with the host libraries (i.e. libnpymath.a). For cross-compilation you of-course need to link with target libraries, while using the host Python installation.", "You can copy out the numpy/core/lib/npy-pkg-config directory, add a pkgdir value to the .ini files and set NPY_PKG_CONFIG_PATH environment variable to point to the directory with the modified npy-pkg-config files.", "Example npymath.ini modified for cross-compilation:"]}, {"name": "add_scripts()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.add_scripts", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": ["Add scripts to configuration.", "Add the sequence of files to the beginning of the scripts list. Scripts will be installed under the <prefix>/bin/ directory."]}, {"name": "add_subpackage()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.add_subpackage", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": ["Add a sub-package to the current Configuration instance.", "This is useful in a setup.py script for adding sub-packages to a package.", "name of the subpackage", "if given, the subpackage path such as the subpackage is in subpackage_path / subpackage_name. If None,the subpackage is assumed to be located in the local path / subpackage_name."]}, {"name": "Additional Git Resources", "path": "dev/gitwash/git_resources", "type": "Development", "text": ["There are many ways of working with git; here are some posts on the rules of thumb that other projects have come up with:", "You can get these on your own machine with (e.g) git help push or (same thing) git push --help, but, for convenience, here are the online manual pages for some common commands:"]}, {"name": "Advanced debugging tools", "path": "dev/development_advanced_debugging", "type": "Development", "text": ["If you reached here, you want to dive into, or use, more advanced tooling. This is usually not necessary for first time contributors and most day-to-day development. These are used more rarely, for example close to a new NumPy release, or when a large or particular complex change was made.", "Since not all of these tools are used on a regular bases and only available on some systems, please expect differences, issues, or quirks; we will be happy to help if you get stuck and appreciate any improvements or suggestions to these workflows.", "Most development will not require more than a typical debugging toolchain as shown in Debugging. But for example memory leaks can be particularly subtle or difficult to narrow down.", "We do not expect any of these tools to be run by most contributors. However, you can ensure that we can track down such issues more easily easier:", "This will help us catch any oversights before your change is released and means you do not have to worry about making reference counting errors, which can be intimidating.", "Debug builds of Python are easily available for example on debian systems, and can be used on all platforms. Running a test or terminal is usually as easy as:", "and were already mentioned in Debugging.", "A Python debug build will help:", "Python debug builds allows to check correct reference counting. This works using the additional commands:", "Running the test suite only with a debug python build will not find many errors on its own. An additional advantage of a debug build of Python is that it allows detecting memory leaks.", "A tool to make this easier is pytest-leaks, which can be installed using pip. Unfortunately, pytest itself may leak memory, but good results can usually (currently) be achieved by removing:", "from numpy/conftest.py (This may change with new pytest-leaks versions or pytest updates).", "This allows to run the test suite, or part of it, conveniently:", "where -R2:3 is the pytest-leaks command (see its documentation), the -s causes output to print and may be necessary (in some versions captured output was detected as a leak).", "Note that some tests are known (or even designed) to leak references, we try to mark them, but expect some false positives.", "Valgrind is a powerful tool to find certain memory access problems and should be run on complicated C code. Basic use of valgrind usually requires no more than:", "where PYTHONMALLOC=malloc is necessary to avoid false positives from python itself. Depending on the system and valgrind version, you may see more false positives. valgrind supports \u201csuppressions\u201d to ignore some of these, and Python does have a suppression file (and even a compile time option) which may help if you find it necessary.", "Valgrind helps:", "Find many memory leaks. Note that for most leaks the python debug build approach (and pytest-leaks) is much more sensitive. The reason is that valgrind can only detect if memory is definitely lost. If:", "Has incorrect reference counting for dtype, this is a bug, but valgrind cannot see it because np.dtype(np.int64) always returns the same object. However, not all dtypes are singletons, so this might leak memory for different input. In rare cases NumPy uses malloc and not the Python memory allocators which are invisible to the Python debug build. malloc should normally be avoided, but there are some exceptions (e.g. the PyArray_Dims structure is public API and cannot use the Python allocators.)", "Even though using valgrind for memory leak detection is slow and less sensitive it can be a convenient: you can run most programs with valgrind without modification.", "Things to be aware of:", "A big advantage of valgrind is that it has no requirements aside from valgrind itself (although you probably want to use debug builds for better tracebacks).", "You can run the test suite with valgrind which may be sufficient when you are only interested in a few tests:", "Note the --continue-on-collection-errors, which is currently necessary due to missing longdouble support causing failures (this will usually not be necessary if you do not run the full test suite).", "If you wish to detect memory leaks you will also require --show-leak-kinds=definite and possibly more valgrind options. Just as for pytest-leaks certain tests are known to leak cause errors in valgrind and may or may not be marked as such.", "We have developed pytest-valgrind which:", "Please refer to its README for more information (it includes an example command for NumPy)."]}, {"name": "Advanced F2PY use cases", "path": "f2py/advanced", "type": "Advanced F2PY use cases", "text": ["User-defined Python C/API functions can be defined inside signature files using usercode and pymethoddef statements (they must be used inside the python module block). For example, the following signature file spam.pyf", "wraps the C library function system():", "In Python this can then be used as:", "The following example illustrates how to add user-defined variables to a F2PY generated extension module by modifying the dictionary of a F2PY generated module. Consider the following signature file (compiled with f2py -c var.pyf):", "Notice that the second usercode statement must be defined inside an interface block and the module dictionary is available through the variable d (see varmodule.c generated by f2py var.pyf for additional details).", "Usage in Python:", "Currently, F2PY can handle only <type spec>(kind=<kindselector>) declarations where <kindselector> is a numeric integer (e.g. 1, 2, 4,\u2026), but not a function call KIND(..) or any other expression. F2PY needs to know what would be the corresponding C type and a general solution for that would be too complicated to implement.", "However, F2PY provides a hook to overcome this difficulty, namely, users can define their own <Fortran type> to <C type> maps. For example, if Fortran 90 code contains:", "then create a mapping file containing a Python dictionary:", "for instance.", "Use the --f2cmap command-line option to pass the file name to F2PY. By default, F2PY assumes file name is .f2py_f2cmap in the current working directory.", "More generally, the f2cmap file must contain a dictionary with items:", "that defines mapping between Fortran type:", "and the corresponding <C type>. The <C type> can be one of the following:", "For more information, see the F2Py source code numpy/f2py/capi_maps.py."]}, {"name": "Array creation", "path": "user/basics.creation", "type": "User Guide", "text": ["See also", "Array creation routines", "There are 6 general mechanisms for creating arrays:", "You can use these methods to create ndarrays or Structured arrays. This document will cover general methods for ndarray creation.", "NumPy arrays can be defined using Python sequences such as lists and tuples. Lists and tuples are defined using [...] and (...), respectively. Lists and tuples can define ndarray creation:", "When you use numpy.array to define a new array, you should consider the dtype of the elements in the array, which can be specified explicitly. This feature gives you more control over the underlying data structures and how the elements are handled in C/C++ functions. If you are not careful with dtype assignments, you can get unwanted overflow, as such", "An 8-bit signed integer represents integers from -128 to 127. Assigning the int8 array to integers outside of this range results in overflow. This feature can often be misunderstood. If you perform calculations with mismatching dtypes, you can get unwanted results, for example:", "Notice when you perform operations with two arrays of the same dtype: uint32, the resulting array is the same type. When you perform operations with different dtype, NumPy will assign a new type that satisfies all of the array elements involved in the computation, here uint32 and int32 can both be represented in as int64.", "The default NumPy behavior is to create arrays in either 64-bit signed integers or double precision floating point numbers, int64 and float, respectively. If you expect your arrays to be a certain type, then you need to specify the dtype while you create the array.", "NumPy has over 40 built-in functions for creating arrays as laid out in the Array creation routines. These functions can be split into roughly three categories, based on the dimension of the array they create:", "The 1D array creation functions e.g. numpy.linspace and numpy.arange generally need at least two inputs, start and stop.", "numpy.arange creates arrays with regularly incrementing values. Check the documentation for complete information and examples. A few examples are shown:", "Note: best practice for numpy.arange is to use integer start, end, and step values. There are some subtleties regarding dtype. In the second example, the dtype is defined. In the third example, the array is dtype=float to accommodate the step size of 0.1. Due to roundoff error, the stop value is sometimes included.", "numpy.linspace will create arrays with a specified number of elements, and spaced equally between the specified beginning and end values. For example:", "The advantage of this creation function is that you guarantee the number of elements and the starting and end point. The previous arange(start, stop, step) will not include the value stop.", "The 2D array creation functions e.g. numpy.eye, numpy.diag, and numpy.vander define properties of special matrices represented as 2D arrays.", "np.eye(n, m) defines a 2D identity matrix. The elements where i=j (row index and column index are equal) are 1 and the rest are 0, as such:", "numpy.diag can define either a square 2D array with given values along the diagonal or if given a 2D array returns a 1D array that is only the diagonal elements. The two array creation functions can be helpful while doing linear algebra, as such:", "vander(x, n) defines a Vandermonde matrix as a 2D NumPy array. Each column of the Vandermonde matrix is a decreasing power of the input 1D array or list or tuple, x where the highest polynomial order is n-1. This array creation routine is helpful in generating linear least squares models, as such:", "The ndarray creation functions e.g. numpy.ones, numpy.zeros, and random define arrays based upon the desired shape. The ndarray creation functions can create arrays with any dimension by specifying how many dimensions and length along that dimension in a tuple or list.", "numpy.zeros will create an array filled with 0 values with the specified shape. The default dtype is float64:", "numpy.ones will create an array filled with 1 values. It is identical to zeros in all other respects as such:", "The random method of the result of default_rng will create an array filled with random values between 0 and 1. It is included with the numpy.random library. Below, two arrays are created with shapes (2,3) and (2,3,2), respectively. The seed is set to 42 so you can reproduce these pseudorandom numbers:", "numpy.indices will create a set of arrays (stacked as a one-higher dimensioned array), one per dimension with each representing variation in that dimension:", "This is particularly useful for evaluating functions of multiple dimensions on a regular grid.", "Once you have created arrays, you can replicate, join, or mutate those existing arrays to create new arrays. When you assign an array or its elements to a new variable, you have to explicitly numpy.copy the array, otherwise the variable is a view into the original array. Consider the following example:", "In this example, you did not create a new array. You created a variable, b that viewed the first 2 elements of a. When you added 1 to b you would get the same result by adding 1 to a[:2]. If you want to create a new array, use the numpy.copy array creation routine as such:", "For more information and examples look at Copies and Views.", "There are a number of routines to join existing arrays e.g. numpy.vstack, numpy.hstack, and numpy.block. Here is an example of joining four 2-by-2 arrays into a 4-by-4 array using block:", "Other routines use similar syntax to join ndarrays. Check the routine\u2019s documentation for further examples and syntax.", "This is the most common case of large array creation. The details depend greatly on the format of data on disk. This section gives general pointers on how to handle various formats. For more detailed examples of IO look at How to Read and Write files.", "Various fields have standard formats for array data. The following lists the ones with known Python libraries to read them and return NumPy arrays (there may be others for which it is possible to read and convert to NumPy arrays so check the last section as well)", "Examples of formats that cannot be read directly but for which it is not hard to convert are those formats supported by libraries like PIL (able to read and write many image formats such as jpg, png, etc).", "Delimited files such as comma separated value (csv) and tab separated value (tsv) files are used for programs like Excel and LabView. Python functions can read and parse these files line-by-line. NumPy has two standard routines for importing a file with delimited data numpy.loadtxt and numpy.genfromtxt. These functions have more involved use cases in Reading and writing files. A simple example given a simple.csv:", "Importing simple.csv is accomplished using loadtxt:", "More generic ASCII files can be read using scipy.io and Pandas.", "There are a variety of approaches one can use. If the file has a relatively simple format then one can write a simple I/O library and use the NumPy fromfile() function and .tofile() method to read and write NumPy arrays directly (mind your byteorder though!) If a good C or C++ library exists that read the data, one can wrap that library with a variety of techniques though that certainly is much more work and requires significantly more advanced knowledge to interface with C or C++.", "NumPy is the fundamental library for array containers in the Python Scientific Computing stack. Many Python libraries, including SciPy, Pandas, and OpenCV, use NumPy ndarrays as the common format for data exchange, These libraries can create, operate on, and work with NumPy arrays."]}, {"name": "Array creation routines", "path": "reference/routines.array-creation", "type": "Array creation routines", "text": ["See also", "Array creation", "empty(shape[, dtype, order, like])", "Return a new array of given shape and type, without initializing entries.", "empty_like(prototype[, dtype, order, subok, ...])", "Return a new array with the same shape and type as a given array.", "eye(N[, M, k, dtype, order, like])", "Return a 2-D array with ones on the diagonal and zeros elsewhere.", "identity(n[, dtype, like])", "Return the identity array.", "ones(shape[, dtype, order, like])", "Return a new array of given shape and type, filled with ones.", "ones_like(a[, dtype, order, subok, shape])", "Return an array of ones with the same shape and type as a given array.", "zeros(shape[, dtype, order, like])", "Return a new array of given shape and type, filled with zeros.", "zeros_like(a[, dtype, order, subok, shape])", "Return an array of zeros with the same shape and type as a given array.", "full(shape, fill_value[, dtype, order, like])", "Return a new array of given shape and type, filled with fill_value.", "full_like(a, fill_value[, dtype, order, ...])", "Return a full array with the same shape and type as a given array.", "array(object[, dtype, copy, order, subok, ...])", "Create an array.", "asarray(a[, dtype, order, like])", "Convert the input to an array.", "asanyarray(a[, dtype, order, like])", "Convert the input to an ndarray, but pass ndarray subclasses through.", "ascontiguousarray(a[, dtype, like])", "Return a contiguous array (ndim >= 1) in memory (C order).", "asmatrix(data[, dtype])", "Interpret the input as a matrix.", "copy(a[, order, subok])", "Return an array copy of the given object.", "frombuffer(buffer[, dtype, count, offset, like])", "Interpret a buffer as a 1-dimensional array.", "fromfile(file[, dtype, count, sep, offset, like])", "Construct an array from data in a text or binary file.", "fromfunction(function, shape, *[, dtype, like])", "Construct an array by executing a function over each coordinate.", "fromiter(iter, dtype[, count, like])", "Create a new 1-dimensional array from an iterable object.", "fromstring(string[, dtype, count, like])", "A new 1-D array initialized from text data in a string.", "loadtxt(fname[, dtype, comments, delimiter, ...])", "Load data from a text file.", "Note", "numpy.rec is the preferred alias for numpy.core.records.", "core.records.array(obj[, dtype, shape, ...])", "Construct a record array from a wide-variety of objects.", "core.records.fromarrays(arrayList[, dtype, ...])", "Create a record array from a (flat) list of arrays", "core.records.fromrecords(recList[, dtype, ...])", "Create a recarray from a list of records in text form.", "core.records.fromstring(datastring[, dtype, ...])", "Create a record array from binary data", "core.records.fromfile(fd[, dtype, shape, ...])", "Create an array from binary file data", "Note", "numpy.char is the preferred alias for numpy.core.defchararray.", "core.defchararray.array(obj[, itemsize, ...])", "Create a chararray.", "core.defchararray.asarray(obj[, itemsize, ...])", "Convert the input to a chararray, copying the data only if necessary.", "arange([start,] stop[, step,][, dtype, like])", "Return evenly spaced values within a given interval.", "linspace(start, stop[, num, endpoint, ...])", "Return evenly spaced numbers over a specified interval.", "logspace(start, stop[, num, endpoint, base, ...])", "Return numbers spaced evenly on a log scale.", "geomspace(start, stop[, num, endpoint, ...])", "Return numbers spaced evenly on a log scale (a geometric progression).", "meshgrid(*xi[, copy, sparse, indexing])", "Return coordinate matrices from coordinate vectors.", "mgrid", "nd_grid instance which returns a dense multi-dimensional \"meshgrid\".", "ogrid", "nd_grid instance which returns an open multi-dimensional \"meshgrid\".", "diag(v[, k])", "Extract a diagonal or construct a diagonal array.", "diagflat(v[, k])", "Create a two-dimensional array with the flattened input as a diagonal.", "tri(N[, M, k, dtype, like])", "An array with ones at and below the given diagonal and zeros elsewhere.", "tril(m[, k])", "Lower triangle of an array.", "triu(m[, k])", "Upper triangle of an array.", "vander(x[, N, increasing])", "Generate a Vandermonde matrix.", "mat(data[, dtype])", "Interpret the input as a matrix.", "bmat(obj[, ldict, gdict])", "Build a matrix object from a string, nested sequence, or array."]}, {"name": "Array manipulation routines", "path": "reference/routines.array-manipulation", "type": "Array manipulation routines", "text": ["copyto(dst, src[, casting, where])", "Copies values from one array to another, broadcasting as necessary.", "shape(a)", "Return the shape of an array.", "reshape(a, newshape[, order])", "Gives a new shape to an array without changing its data.", "ravel(a[, order])", "Return a contiguous flattened array.", "ndarray.flat", "A 1-D iterator over the array.", "ndarray.flatten([order])", "Return a copy of the array collapsed into one dimension.", "moveaxis(a, source, destination)", "Move axes of an array to new positions.", "rollaxis(a, axis[, start])", "Roll the specified axis backwards, until it lies in a given position.", "swapaxes(a, axis1, axis2)", "Interchange two axes of an array.", "ndarray.T", "The transposed array.", "transpose(a[, axes])", "Reverse or permute the axes of an array; returns the modified array.", "atleast_1d(*arys)", "Convert inputs to arrays with at least one dimension.", "atleast_2d(*arys)", "View inputs as arrays with at least two dimensions.", "atleast_3d(*arys)", "View inputs as arrays with at least three dimensions.", "broadcast", "Produce an object that mimics broadcasting.", "broadcast_to(array, shape[, subok])", "Broadcast an array to a new shape.", "broadcast_arrays(*args[, subok])", "Broadcast any number of arrays against each other.", "expand_dims(a, axis)", "Expand the shape of an array.", "squeeze(a[, axis])", "Remove axes of length one from a.", "asarray(a[, dtype, order, like])", "Convert the input to an array.", "asanyarray(a[, dtype, order, like])", "Convert the input to an ndarray, but pass ndarray subclasses through.", "asmatrix(data[, dtype])", "Interpret the input as a matrix.", "asfarray(a[, dtype])", "Return an array converted to a float type.", "asfortranarray(a[, dtype, like])", "Return an array (ndim >= 1) laid out in Fortran order in memory.", "ascontiguousarray(a[, dtype, like])", "Return a contiguous array (ndim >= 1) in memory (C order).", "asarray_chkfinite(a[, dtype, order])", "Convert the input to an array, checking for NaNs or Infs.", "asscalar(a)", "Convert an array of size 1 to its scalar equivalent.", "require(a[, dtype, requirements, like])", "Return an ndarray of the provided type that satisfies requirements.", "concatenate([axis, out, dtype, casting])", "Join a sequence of arrays along an existing axis.", "stack(arrays[, axis, out])", "Join a sequence of arrays along a new axis.", "block(arrays)", "Assemble an nd-array from nested lists of blocks.", "vstack(tup)", "Stack arrays in sequence vertically (row wise).", "hstack(tup)", "Stack arrays in sequence horizontally (column wise).", "dstack(tup)", "Stack arrays in sequence depth wise (along third axis).", "column_stack(tup)", "Stack 1-D arrays as columns into a 2-D array.", "row_stack(tup)", "Stack arrays in sequence vertically (row wise).", "split(ary, indices_or_sections[, axis])", "Split an array into multiple sub-arrays as views into ary.", "array_split(ary, indices_or_sections[, axis])", "Split an array into multiple sub-arrays.", "dsplit(ary, indices_or_sections)", "Split array into multiple sub-arrays along the 3rd axis (depth).", "hsplit(ary, indices_or_sections)", "Split an array into multiple sub-arrays horizontally (column-wise).", "vsplit(ary, indices_or_sections)", "Split an array into multiple sub-arrays vertically (row-wise).", "tile(A, reps)", "Construct an array by repeating A the number of times given by reps.", "repeat(a, repeats[, axis])", "Repeat elements of an array.", "delete(arr, obj[, axis])", "Return a new array with sub-arrays along an axis deleted.", "insert(arr, obj, values[, axis])", "Insert values along the given axis before the given indices.", "append(arr, values[, axis])", "Append values to the end of an array.", "resize(a, new_shape)", "Return a new array with the specified shape.", "trim_zeros(filt[, trim])", "Trim the leading and/or trailing zeros from a 1-D array or sequence.", "unique(ar[, return_index, return_inverse, ...])", "Find the unique elements of an array.", "flip(m[, axis])", "Reverse the order of elements in an array along the given axis.", "fliplr(m)", "Reverse the order of elements along axis 1 (left/right).", "flipud(m)", "Reverse the order of elements along axis 0 (up/down).", "reshape(a, newshape[, order])", "Gives a new shape to an array without changing its data.", "roll(a, shift[, axis])", "Roll array elements along a given axis.", "rot90(m[, k, axes])", "Rotate an array by 90 degrees in the plane specified by axes."]}, {"name": "Array objects", "path": "reference/arrays", "type": "Array objects", "text": ["NumPy provides an N-dimensional array type, the ndarray, which describes a collection of \u201citems\u201d of the same type. The items can be indexed using for example N integers.", "All ndarrays are homogeneous: every item takes up the same size block of memory, and all blocks are interpreted in exactly the same way. How each item in the array is to be interpreted is specified by a separate data-type object, one of which is associated with every array. In addition to basic types (integers, floats, etc.), the data type objects can also represent data structures.", "An item extracted from an array, e.g., by indexing, is represented by a Python object whose type is one of the array scalar types built in NumPy. The array scalars allow easy manipulation of also more complicated arrangements of data.", "Figure Conceptual diagram showing the relationship between the three fundamental objects used to describe the data in an array: 1) the ndarray itself, 2) the data-type object that describes the layout of a single fixed-size element of the array, 3) the array-scalar Python object that is returned when a single element of the array is accessed."]}, {"name": "Binary operations", "path": "reference/routines.bitwise", "type": "Binary operations", "text": ["bitwise_and(x1, x2, /[, out, where, ...])", "Compute the bit-wise AND of two arrays element-wise.", "bitwise_or(x1, x2, /[, out, where, casting, ...])", "Compute the bit-wise OR of two arrays element-wise.", "bitwise_xor(x1, x2, /[, out, where, ...])", "Compute the bit-wise XOR of two arrays element-wise.", "invert(x, /[, out, where, casting, order, ...])", "Compute bit-wise inversion, or bit-wise NOT, element-wise.", "left_shift(x1, x2, /[, out, where, casting, ...])", "Shift the bits of an integer to the left.", "right_shift(x1, x2, /[, out, where, ...])", "Shift the bits of an integer to the right.", "packbits(a, /[, axis, bitorder])", "Packs the elements of a binary-valued array into bits in a uint8 array.", "unpackbits(a, /[, axis, count, bitorder])", "Unpacks elements of a uint8 array into a binary-valued output array.", "binary_repr(num[, width])", "Return the binary representation of the input number as a string."]}, {"name": "Bit Generators", "path": "reference/random/bit_generators/index", "type": "Bit Generators", "text": ["The random values produced by Generator originate in a BitGenerator. The BitGenerators do not directly provide random numbers and only contains methods used for seeding, getting or setting the state, jumping or advancing the state, and for accessing low-level wrappers for consumption by code that can efficiently access the functions provided, e.g., numba.", "The included BitGenerators are:", "BitGenerator([seed])", "Base Class for generic BitGenerators, which provide a stream of random bits based on different algorithms."]}, {"name": "broadcast.index", "path": "reference/generated/numpy.broadcast.index", "type": "Standard array subclasses", "text": ["attribute", "current index in broadcasted result"]}, {"name": "broadcast.iters", "path": "reference/generated/numpy.broadcast.iters", "type": "Standard array subclasses", "text": ["attribute", "tuple of iterators along self\u2019s \u201ccomponents.\u201d", "Returns a tuple of numpy.flatiter objects, one for each \u201ccomponent\u201d of self.", "See also"]}, {"name": "broadcast.nd", "path": "reference/generated/numpy.broadcast.nd", "type": "Standard array subclasses", "text": ["attribute", "Number of dimensions of broadcasted result. For code intended for NumPy 1.12.0 and later the more consistent ndim is preferred."]}, {"name": "broadcast.ndim", "path": "reference/generated/numpy.broadcast.ndim", "type": "Standard array subclasses", "text": ["attribute", "Number of dimensions of broadcasted result. Alias for nd.", "New in version 1.12.0."]}, {"name": "broadcast.numiter", "path": "reference/generated/numpy.broadcast.numiter", "type": "Standard array subclasses", "text": ["attribute", "Number of iterators possessed by the broadcasted result."]}, {"name": "broadcast.reset()", "path": "reference/generated/numpy.broadcast.reset", "type": "numpy.broadcast.reset", "text": ["method", "Reset the broadcasted result\u2019s iterator(s)."]}, {"name": "broadcast.size", "path": "reference/generated/numpy.broadcast.size", "type": "Standard array subclasses", "text": ["attribute", "Total size of broadcasted result."]}, {"name": "build_src", "path": "f2py/buildtools/distutils", "type": "Using via \n        \n         numpy.distutils", "text": ["numpy.distutils is part of NumPy, and extends the standard Python distutils module to deal with Fortran sources and F2PY signature files, e.g. compile Fortran sources, call F2PY to construct extension modules, etc.", "Example", "Consider the following setup_file.py for the fib and scalar examples from Three ways to wrap - getting started section:", "Running", "will build two extension modules scalar and fib2 to the build directory.", "numpy.distutils extends distutils with the following features:", "Extension class argument sources may contain Fortran source files. In addition, the list sources may contain at most one F2PY signature file, and in this case, the name of an Extension module must match with the <modulename> used in signature file. It is assumed that an F2PY signature file contains exactly one python\nmodule block.", "If sources do not contain a signature file, then F2PY is used to scan Fortran source files to construct wrappers to the Fortran codes.", "Additional options to the F2PY executable can be given using the Extension class argument f2py_options.", "The following new distutils commands are defined:", "to construct Fortran wrapper extension modules, among many other things.", "to change Fortran compiler options.", "Additionally, the build_ext and build_clib commands are also enhanced to support Fortran sources.", "Run", "to see available options for these commands.", "When building Python packages containing Fortran sources, one can choose different Fortran compilers by using the build_ext command option --fcompiler=<Vendor>. Here <Vendor> can be one of the following names (on linux systems):", "See numpy_distutils/fcompiler.py for an up-to-date list of supported compilers for different platforms, or run"]}, {"name": "Building from source", "path": "user/building", "type": "User Guide", "text": ["There are two options for building NumPy- building with Gitpod or locally from source. Your choice depends on your operating system and familiarity with the command line.", "Gitpod is an open-source platform that automatically creates the correct development environment right in your browser, reducing the need to install local development environments and deal with incompatible dependencies.", "If you are a Windows user, unfamiliar with using the command line or building NumPy for the first time, it is often faster to build with Gitpod. Here are the in-depth instructions for building NumPy with building NumPy with Gitpod.", "Building locally on your machine gives you more granular control. If you are a MacOS or Linux user familiar with using the command line, you can continue with building NumPy locally by following the instructions below.", "Building NumPy requires the following software installed:", "Python 3.6.x or newer", "Please note that the Python development headers also need to be installed, e.g., on Debian/Ubuntu one needs to install both python3 and python3-dev. On Windows and macOS this is normally not an issue.", "Compilers", "Much of NumPy is written in C. You will need a C compiler that complies with the C99 standard.", "While a FORTRAN 77 compiler is not necessary for building NumPy, it is needed to run the numpy.f2py tests. These tests are skipped if the compiler is not auto-detected.", "Note that NumPy is developed mainly using GNU compilers and tested on MSVC and Clang compilers. Compilers from other vendors such as Intel, Absoft, Sun, NAG, Compaq, Vast, Portland, Lahey, HP, IBM are only supported in the form of community feedback, and may not work out of the box. GCC 4.x (and later) compilers are recommended. On ARM64 (aarch64) GCC 8.x (and later) are recommended.", "Linear Algebra libraries", "NumPy does not require any external linear algebra libraries to be installed. However, if these are available, NumPy\u2019s setup script can detect them and use them for building. A number of different LAPACK library setups can be used, including optimized LAPACK libraries such as OpenBLAS or MKL. The choice and location of these libraries as well as include paths and other such build options can be specified in a site.cfg file located in the NumPy root repository or a .numpy-site.cfg file in your home directory. See the site.cfg.example example file included in the NumPy repository or sdist for documentation, and below for specifying search priority from environmental variables.", "Cython", "For building NumPy, you\u2019ll need a recent version of Cython.", "To install NumPy, run:", "To perform an in-place build that can be run from the source folder run:", "Note: for build instructions to do development work on NumPy itself, see Setting up and using your development environment.", "Make sure to test your builds. To ensure everything stays in shape, see if all tests pass:", "For detailed info on testing, see Testing builds.", "It\u2019s possible to do a parallel build with:", "This will compile numpy on 4 CPUs and install it into the specified prefix. to perform a parallel in-place build, run:", "The number of build jobs can also be specified via the environment variable NPY_NUM_BUILD_JOBS.", "Compilers are auto-detected; building with a particular compiler can be done with --fcompiler. E.g. to select gfortran:", "For more information see:", "One relatively simple and reliable way to check for the compiler used to build a library is to use ldd on the library. If libg2c.so is a dependency, this means that g77 has been used (note: g77 is no longer supported for building NumPy). If libgfortran.so is a dependency, gfortran has been used. If both are dependencies, this means both have been used, which is almost always a very bad idea.", "NumPy searches for optimized linear algebra libraries such as BLAS and LAPACK. There are specific orders for searching these libraries, as described below and in the site.cfg.example file.", "Note that both BLAS and CBLAS interfaces are needed for a properly optimized build of NumPy.", "The default order for the libraries are:", "The detection of BLAS libraries may be bypassed by defining the environment variable NPY_BLAS_LIBS , which should contain the exact linker flags you want to use (interface is assumed to be Fortran 77). Also define NPY_CBLAS_LIBS (even empty if CBLAS is contained in your BLAS library) to trigger use of CBLAS and avoid slow fallback code for matrix calculations.", "If you wish to build against OpenBLAS but you also have BLIS available one may predefine the order of searching via the environment variable NPY_BLAS_ORDER which is a comma-separated list of the above names which is used to determine what to search for, for instance:", "will prefer to use ATLAS, then BLIS, then OpenBLAS and as a last resort MKL. If neither of these exists the build will fail (names are compared lower case).", "Alternatively one may use ! or ^ to negate all items:", "will allow using anything but NetLIB BLAS and ATLAS libraries, the order of the above list is retained.", "One cannot mix negation and positives, nor have multiple negations, such cases will raise an error.", "The default order for the libraries are:", "The detection of LAPACK libraries may be bypassed by defining the environment variable NPY_LAPACK_LIBS, which should contain the exact linker flags you want to use (language is assumed to be Fortran 77).", "If you wish to build against OpenBLAS but you also have MKL available one may predefine the order of searching via the environment variable NPY_LAPACK_ORDER which is a comma-separated list of the above names, for instance:", "will prefer to use ATLAS, then OpenBLAS and as a last resort MKL. If neither of these exists the build will fail (names are compared lower case).", "Alternatively one may use ! or ^ to negate all items:", "will allow using anything but the NetLIB LAPACK library, the order of the above list is retained.", "One cannot mix negation and positives, nor have multiple negations, such cases will raise an error.", "Deprecated since version 1.20: The native libraries on macOS, provided by Accelerate, are not fit for use in NumPy since they have bugs that cause wrong output under easily reproducible conditions. If the vendor fixes those bugs, the library could be reinstated, but until then users compiling for themselves should use another linear algebra library or use the built-in (but slower) default, see the next section.", "Usage of ATLAS and other accelerated libraries in NumPy can be disabled via:", "or:", "You can tell Numpy to use 64-bit BLAS/LAPACK libraries by setting the environment variable:", "when building Numpy. The following 64-bit BLAS/LAPACK libraries are supported:", "The order in which they are preferred is determined by NPY_BLAS_ILP64_ORDER and NPY_LAPACK_ILP64_ORDER environment variables. The default value is openblas64_,openblas_ilp64.", "Note", "Using non-symbol-suffixed 64-bit BLAS/LAPACK in a program that also uses 32-bit BLAS/LAPACK can cause crashes under certain conditions (e.g. with embedded Python interpreters on Linux).", "The 64-bit OpenBLAS with 64_ symbol suffix is obtained by compiling OpenBLAS with settings:", "The symbol suffix avoids the symbol name clashes between 32-bit and 64-bit BLAS/LAPACK libraries.", "Additional compiler flags can be supplied by setting the OPT, FOPT (for Fortran), and CC environment variables. When providing options that should improve the performance of the code ensure that you also set -DNDEBUG so that debugging code is not executed."]}, {"name": "Building the NumPy API and reference docs", "path": "dev/howto_build_docs", "type": "Development", "text": ["If you only want to get the documentation, note that pre-built versions can be found at", "https://numpy.org/doc/", "in several different formats.", "Before proceeding further it should be noted that the documentation is built with the make tool, which is not natively available on Windows. MacOS or Linux users can jump to Prerequisites. It is recommended for Windows users to set up their development environment on Gitpod or Windows Subsystem for Linux (WSL). WSL is a good option for a persistent local set-up.", "Gitpod is an open-source platform that automatically creates the correct development environment right in your browser, reducing the need to install local development environments and deal with incompatible dependencies.", "If you have good internet connectivity and want a temporary set-up, it is often faster to build with Gitpod. Here are the in-depth instructions for building NumPy with Gitpod.", "Building the NumPy documentation and API reference requires the following:", "Since large parts of the main documentation are obtained from NumPy via import numpy and examining the docstrings, you will need to first build and install it so that the correct version is imported. NumPy has to be re-built and re-installed every time you fetch the latest version of the repository, before generating the documentation. This ensures that the NumPy version and the git repository version are in sync.", "Note that you can e.g. install NumPy to a temporary location and set the PYTHONPATH environment variable appropriately. Alternatively, if using Python virtual environments (via e.g. conda, virtualenv or the venv module), installing NumPy into a new virtual environment is recommended.", "All of the necessary dependencies for building the NumPy docs except for Doxygen can be installed with:", "We currently use Sphinx along with Doxygen for generating the API and reference documentation for NumPy. In addition, building the documentation requires the Sphinx extension plot_directive, which is shipped with Matplotlib. We also use numpydoc to render docstrings in the generated API documentation. SciPy is installed since some parts of the documentation require SciPy functions.", "For installing Doxygen, please check the official download and installation pages, or if you are using Linux then you can install it through your distribution package manager.", "Note", "Try to install a newer version of Doxygen > 1.8.10 otherwise you may get some warnings during the build.", "If you obtained NumPy via git, also get the git submodules that contain additional parts required for building the documentation:", "Now you are ready to generate the docs, so write:", "If all goes well, this will generate a build/html subdirectory in the /doc directory, containing the built documentation. If you get a message about installed numpy != current repo git version, you must either override the check by setting GITVER or re-install NumPy.", "If you have built NumPy into a virtual environment and get an error that says numpy not found, cannot build documentation without..., you need to override the makefile PYTHON variable at the command line, so instead of writing make\u00a0 html write:", "To build the PDF documentation, do instead:", "You will need to have LaTeX installed for this, inclusive of support for Greek letters. For example, on Ubuntu xenial texlive-lang-greek and cm-super are needed. Also, latexmk is needed on non-Windows systems.", "Instead of the above, you can also do:", "which will rebuild NumPy, install it to a temporary location, and build the documentation in all formats. This will most likely again only work on Unix platforms.", "The documentation for NumPy distributed at https://numpy.org/doc in html and pdf format is also built with make dist. See HOWTO RELEASE for details on how to update https://numpy.org/doc."]}, {"name": "busdaycalendar.holidays", "path": "reference/generated/numpy.busdaycalendar.holidays", "type": "Datetime support functions", "text": ["attribute", "A copy of the holiday array indicating additional invalid days."]}, {"name": "busdaycalendar.weekmask", "path": "reference/generated/numpy.busdaycalendar.weekmask", "type": "Datetime support functions", "text": ["attribute", "A copy of the seven-element boolean mask indicating valid days."]}, {"name": "Byte-swapping", "path": "user/basics.byteswapping", "type": "User Guide", "text": ["The ndarray is an object that provide a python array interface to data in memory.", "It often happens that the memory that you want to view with an array is not of the same byte ordering as the computer on which you are running Python.", "For example, I might be working on a computer with a little-endian CPU - such as an Intel Pentium, but I have loaded some data from a file written by a computer that is big-endian. Let\u2019s say I have loaded 4 bytes from a file written by a Sun (big-endian) computer. I know that these 4 bytes represent two 16-bit integers. On a big-endian machine, a two-byte integer is stored with the Most Significant Byte (MSB) first, and then the Least Significant Byte (LSB). Thus the bytes are, in memory order:", "Let\u2019s say the two integers were in fact 1 and 770. Because 770 = 256 * 3 + 2, the 4 bytes in memory would contain respectively: 0, 1, 3, 2. The bytes I have loaded from the file would have these contents:", "We might want to use an ndarray to access these integers. In that case, we can create an array around this memory, and tell numpy that there are two integers, and that they are 16 bit and big-endian:", "Note the array dtype above of >i2. The > means \u2018big-endian\u2019 (< is little-endian) and i2 means \u2018signed 2-byte integer\u2019. For example, if our data represented a single unsigned 4-byte little-endian integer, the dtype string would be <u4.", "In fact, why don\u2019t we try that?", "Returning to our big_end_arr - in this case our underlying data is big-endian (data endianness) and we\u2019ve set the dtype to match (the dtype is also big-endian). However, sometimes you need to flip these around.", "Warning", "Scalars currently do not include byte order information, so extracting a scalar from an array will return an integer in native byte order. Hence:", "As you can imagine from the introduction, there are two ways you can affect the relationship between the byte ordering of the array and the underlying memory it is looking at:", "The common situations in which you need to change byte ordering are:", "We make something where they don\u2019t match:", "The obvious fix for this situation is to change the dtype so it gives the correct endianness:", "Note the array has not changed in memory:", "You might want to do this if you need the data in memory to be a certain ordering. For example you might be writing the memory out to a file that needs a certain byte ordering.", "Now the array has changed in memory:", "You may have a correctly specified array dtype, but you need the array to have the opposite byte order in memory, and you want the dtype to match so the array values make sense. In this case you just do both of the previous operations:", "An easier way of casting the data to a specific dtype and byte ordering can be achieved with the ndarray astype method:"]}, {"name": "C API Deprecations", "path": "reference/c-api/deprecations", "type": "C API Deprecations", "text": ["The API exposed by NumPy for third-party extensions has grown over years of releases, and has allowed programmers to directly access NumPy functionality from C. This API can be best described as \u201corganic\u201d. It has emerged from multiple competing desires and from multiple points of view over the years, strongly influenced by the desire to make it easy for users to move to NumPy from Numeric and Numarray. The core API originated with Numeric in 1995 and there are patterns such as the heavy use of macros written to mimic Python\u2019s C-API as well as account for compiler technology of the late 90\u2019s. There is also only a small group of volunteers who have had very little time to spend on improving this API.", "There is an ongoing effort to improve the API. It is important in this effort to ensure that code that compiles for NumPy 1.X continues to compile for NumPy 1.X. At the same time, certain API\u2019s will be marked as deprecated so that future-looking code can avoid these API\u2019s and follow better practices.", "Another important role played by deprecation markings in the C API is to move towards hiding internal details of the NumPy implementation. For those needing direct, easy, access to the data of ndarrays, this will not remove this ability. Rather, there are many potential performance optimizations which require changing the implementation details, and NumPy developers have been unable to try them because of the high value of preserving ABI compatibility. By deprecating this direct access, we will in the future be able to improve NumPy\u2019s performance in ways we cannot presently.", "In C, there is no equivalent to the deprecation warnings that Python supports. One way to do deprecations is to flag them in the documentation and release notes, then remove or change the deprecated features in a future major version (NumPy 2.0 and beyond). Minor versions of NumPy should not have major C-API changes, however, that prevent code that worked on a previous minor release. For example, we will do our best to ensure that code that compiled and worked on NumPy 1.4 should continue to work on NumPy 1.7 (but perhaps with compiler warnings).", "To use the NPY_NO_DEPRECATED_API mechanism, you need to #define it to the target API version of NumPy before #including any NumPy headers. If you want to confirm that your code is clean against 1.7, use:", "On compilers which support a #warning mechanism, NumPy issues a compiler warning if you do not define the symbol NPY_NO_DEPRECATED_API. This way, the fact that there are deprecations will be flagged for third-party developers who may not have read the release notes closely."]}, {"name": "char **NpyIter_GetDataPtrArray()", "path": "reference/c-api/iterator#c.NpyIter_GetDataPtrArray", "type": "Array Iterator API", "text": ["This gives back a pointer to the nop data pointers. If NPY_ITER_EXTERNAL_LOOP was not specified, each data pointer points to the current data item of the iterator. If no inner iteration was specified, it points to the first data item of the inner loop.", "This pointer may be cached before the iteration loop, calling iternext will not change it. This function may be safely called without holding the Python GIL."]}, {"name": "char **NpyIter_GetInitialDataPtrArray()", "path": "reference/c-api/iterator#c.NpyIter_GetInitialDataPtrArray", "type": "Array Iterator API", "text": ["Gets the array of data pointers directly into the arrays (never into the buffers), corresponding to iteration index 0.", "These pointers are different from the pointers accepted by NpyIter_ResetBasePointers, because the direction along some axes may have been reversed.", "This function may be safely called without holding the Python GIL."]}, {"name": "char *core_signature", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.core_signature", "type": "Python Types and C-Structures", "text": ["Core signature string"]}, {"name": "char *data", "path": "reference/c-api/types-and-structures#c.NPY_AO.data", "type": "Python Types and C-Structures", "text": ["Accessible via PyArray_DATA, this data member is a pointer to the first element of the array. This pointer can (and normally should) be recast to the data type of the array."]}, {"name": "char *dataptr", "path": "reference/c-api/types-and-structures#c.PyArrayIterObject.dataptr", "type": "Python Types and C-Structures", "text": ["This member points to an element in the ndarray indicated by the index."]}, {"name": "char *doc", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.doc", "type": "Python Types and C-Structures", "text": ["Documentation for the ufunc. Should not contain the function signature as this is generated dynamically when __doc__ is retrieved."]}, {"name": "char *name", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.name", "type": "Python Types and C-Structures", "text": ["A string name for the ufunc. This is used dynamically to build the __doc__ attribute of ufuncs."]}, {"name": "char *PyArray_BYTES()", "path": "reference/c-api/array#c.PyArray_BYTES", "type": "Array API", "text": ["These two macros are similar and obtain the pointer to the data-buffer for the array. The first macro can (and should be) assigned to a particular pointer where the second is for generic processing. If you have not guaranteed a contiguous and/or aligned array then be sure you understand how to access the data in the array to avoid memory and/or alignment problems."]}, {"name": "char *PyArray_One()", "path": "reference/c-api/array#c.PyArray_One", "type": "Array API", "text": ["A pointer to newly created memory of size arr ->itemsize that holds the representation of 1 for that type. The returned pointer, ret, must be freed using PyDataMem_FREE (ret) when it is not needed anymore."]}, {"name": "char *PyArray_Zero()", "path": "reference/c-api/array#c.PyArray_Zero", "type": "Array API", "text": ["A pointer to newly created memory of size arr ->itemsize that holds the representation of 0 for that type. The returned pointer, ret, must be freed using PyDataMem_FREE (ret) when it is not needed anymore."]}, {"name": "char *PyDataMem_RENEW()", "path": "reference/c-api/array#c.PyDataMem_RENEW", "type": "Array API", "text": ["Macros to allocate, free, and reallocate memory. These macros are used internally to create arrays."]}, {"name": "char *types", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.types", "type": "Python Types and C-Structures", "text": ["An array of \\(nargs \\times ntypes\\) 8-bit type_numbers which contains the type signature for the function for each of the supported (builtin) data types. For each of the ntypes functions, the corresponding set of type numbers in this array shows how the args argument should be interpreted in the 1-d vector loop. These type numbers do not have to be the same type and mixed-type ufuncs are supported."]}, {"name": "char byteorder", "path": "reference/c-api/types-and-structures#c.PyArray_Descr.byteorder", "type": "Python Types and C-Structures", "text": ["A character indicating the byte-order: \u2018>\u2019 (big-endian), \u2018<\u2019 (little- endian), \u2018=\u2019 (native), \u2018|\u2019 (irrelevant, ignore). All builtin data- types have byteorder \u2018=\u2019."]}, {"name": "char flags", "path": "reference/c-api/types-and-structures#c.PyArray_Descr.flags", "type": "Python Types and C-Structures", "text": ["A data-type bit-flag that determines if the data-type exhibits object- array like behavior. Each bit in this member is a flag which are named as:"]}, {"name": "char kind", "path": "reference/c-api/types-and-structures#c.PyArray_Descr.kind", "type": "Python Types and C-Structures", "text": ["A character code indicating the kind of array (using the array interface typestring notation). A \u2018b\u2019 represents Boolean, a \u2018i\u2019 represents signed integer, a \u2018u\u2019 represents unsigned integer, \u2018f\u2019 represents floating point, \u2018c\u2019 represents complex floating point, \u2018S\u2019 represents 8-bit zero-terminated bytes, \u2018U\u2019 represents 32-bit/character unicode string, and \u2018V\u2019 represents arbitrary."]}, {"name": "char type", "path": "reference/c-api/types-and-structures#c.PyArray_Descr.type", "type": "Python Types and C-Structures", "text": ["A traditional character code indicating the data type."]}, {"name": "char typekind", "path": "reference/c-api/types-and-structures#c.PyArrayInterface.typekind", "type": "Python Types and C-Structures", "text": ["A character indicating what kind of array is present according to the typestring convention with \u2018t\u2019 -> bitfield, \u2018b\u2019 -> Boolean, \u2018i\u2019 -> signed integer, \u2018u\u2019 -> unsigned integer, \u2018f\u2019 -> floating point, \u2018c\u2019 -> complex floating point, \u2018O\u2019 -> object, \u2018S\u2019 -> (byte-)string, \u2018U\u2019 -> unicode, \u2018V\u2019 -> void."]}, {"name": "char.add()", "path": "reference/generated/numpy.char.add", "type": "numpy.char.add", "text": ["Return element-wise string concatenation for two arrays of str or unicode.", "Arrays x1 and x2 must have the same shape.", "Input array.", "Input array.", "Output array of string_ or unicode_, depending on input types of the same shape as x1 and x2."]}, {"name": "char.array()", "path": "reference/generated/numpy.char.array", "type": "numpy.char.array", "text": ["Create a chararray.", "Note", "This class is provided for numarray backward-compatibility. New code (not concerned with numarray compatibility) should use arrays of type string_ or unicode_ and use the free functions in numpy.char for fast vectorized string operations instead.", "Versus a regular NumPy array of type str or unicode, this class adds the following functionality:", "itemsize is the number of characters per scalar in the resulting array. If itemsize is None, and obj is an object array or a Python list, the itemsize will be automatically determined. If itemsize is provided and obj is of type str or unicode, then the obj string will be chunked into itemsize pieces.", "If true (default), then the object is copied. Otherwise, a copy will only be made if __array__ returns a copy, if obj is a nested sequence, or if a copy is needed to satisfy any of the other requirements (itemsize, unicode, order, etc.).", "When true, the resulting chararray can contain Unicode characters, when false only 8-bit characters. If unicode is None and obj is one of the following:", "then the unicode setting of the output array will be automatically determined.", "Specify the order of the array. If order is \u2018C\u2019 (default), then the array will be in C-contiguous order (last-index varies the fastest). If order is \u2018F\u2019, then the returned array will be in Fortran-contiguous order (first-index varies the fastest). If order is \u2018A\u2019, then the returned array may be in any order (either C-, Fortran-contiguous, or even discontiguous)."]}, {"name": "char.asarray()", "path": "reference/generated/numpy.char.asarray", "type": "numpy.char.asarray", "text": ["Convert the input to a chararray, copying the data only if necessary.", "Versus a regular NumPy array of type str or unicode, this class adds the following functionality:", "itemsize is the number of characters per scalar in the resulting array. If itemsize is None, and obj is an object array or a Python list, the itemsize will be automatically determined. If itemsize is provided and obj is of type str or unicode, then the obj string will be chunked into itemsize pieces.", "When true, the resulting chararray can contain Unicode characters, when false only 8-bit characters. If unicode is None and obj is one of the following:", "then the unicode setting of the output array will be automatically determined.", "Specify the order of the array. If order is \u2018C\u2019 (default), then the array will be in C-contiguous order (last-index varies the fastest). If order is \u2018F\u2019, then the returned array will be in Fortran-contiguous order (first-index varies the fastest)."]}, {"name": "char.capitalize()", "path": "reference/generated/numpy.char.capitalize", "type": "numpy.char.capitalize", "text": ["Return a copy of a with only the first character of each element capitalized.", "Calls str.capitalize element-wise.", "For 8-bit strings, this method is locale-dependent.", "Input array of strings to capitalize.", "Output array of str or unicode, depending on input types", "See also"]}, {"name": "char.center()", "path": "reference/generated/numpy.char.center", "type": "numpy.char.center", "text": ["Return a copy of a with its elements centered in a string of length width.", "Calls str.center element-wise.", "The length of the resulting strings", "The padding character to use (default is space).", "Output array of str or unicode, depending on input types", "See also"]}, {"name": "char.chararray.argsort()", "path": "reference/generated/numpy.char.chararray.argsort", "type": "numpy.char.chararray.argsort", "text": ["method", "Returns the indices that would sort this array.", "Refer to numpy.argsort for full documentation.", "See also", "equivalent function"]}, {"name": "char.chararray.astype()", "path": "reference/generated/numpy.char.chararray.astype", "type": "numpy.char.chararray.astype", "text": ["method", "Copy of the array, cast to a specified type.", "Typecode or data-type to which the array is cast.", "Controls the memory layout order of the result. \u2018C\u2019 means C order, \u2018F\u2019 means Fortran order, \u2018A\u2019 means \u2018F\u2019 order if all the arrays are Fortran contiguous, \u2018C\u2019 order otherwise, and \u2018K\u2019 means as close to the order the array elements appear in memory as possible. Default is \u2018K\u2019.", "Controls what kind of data casting may occur. Defaults to \u2018unsafe\u2019 for backwards compatibility.", "If True, then sub-classes will be passed-through (default), otherwise the returned array will be forced to be a base-class array.", "By default, astype always returns a newly allocated array. If this is set to false, and the dtype, order, and subok requirements are satisfied, the input array is returned instead of a copy.", "Unless copy is False and the other conditions for returning the input array are satisfied (see description for copy input parameter), arr_t is a new array of the same shape as the input array, with dtype, order given by dtype, order.", "When casting from complex to float or int. To avoid this, one should use a.real.astype(t).", "Changed in version 1.17.0: Casting between a simple data type and a structured one is possible only for \u201cunsafe\u201d casting. Casting to multiple fields is allowed, but casting from multiple fields is not.", "Changed in version 1.9.0: Casting from numeric to string types in \u2018safe\u2019 casting mode requires that the string dtype length is long enough to store the max integer/float value converted."]}, {"name": "char.chararray.base", "path": "reference/generated/numpy.char.chararray.base", "type": "String operations", "text": ["attribute", "Base object if memory is from some other object.", "The base of an array that owns its memory is None:", "Slicing creates a view, whose memory is shared with x:"]}, {"name": "char.chararray.copy()", "path": "reference/generated/numpy.char.chararray.copy", "type": "numpy.char.chararray.copy", "text": ["method", "Return a copy of the array.", "Controls the memory layout of the copy. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of a as closely as possible. (Note that this function and numpy.copy are very similar but have different default values for their order= arguments, and this function always passes sub-classes through.)", "See also", "Similar function with different default behavior", "This function is the preferred method for creating an array copy. The function numpy.copy is similar, but it defaults to using order \u2018K\u2019, and will not pass sub-classes through by default."]}, {"name": "char.chararray.count()", "path": "reference/generated/numpy.char.chararray.count", "type": "numpy.char.chararray.count", "text": ["method", "Returns an array with the number of non-overlapping occurrences of substring sub in the range [start, end].", "See also"]}, {"name": "char.chararray.ctypes", "path": "reference/generated/numpy.char.chararray.ctypes", "type": "String operations", "text": ["attribute", "An object to simplify the interaction of the array with the ctypes module.", "This attribute creates an object that makes it easier to use arrays when calling shared libraries with the ctypes module. The returned object has, among others, data, shape, and strides attributes (see Notes below) which themselves return ctypes objects that can be used as arguments to a shared library.", "Possessing attributes data, shape, strides, etc.", "See also", "Below are the public attributes of this object which were documented in \u201cGuide to NumPy\u201d (we have omitted undocumented public attributes, as well as documented private attributes):", "A pointer to the memory area of the array as a Python integer. This memory area may contain data that is not aligned, or not in correct byte-order. The memory area may not even be writeable. The array flags and data-type of this array should be respected when passing this attribute to arbitrary C-code to avoid trouble that can include Python crashing. User Beware! The value of this attribute is exactly the same as self._array_interface_['data'][0].", "Note that unlike data_as, a reference will not be kept to the array: code like ctypes.c_void_p((a + b).ctypes.data) will result in a pointer to a deallocated array, and should be spelt (a + b).ctypes.data_as(ctypes.c_void_p)", "(c_intp*self.ndim): A ctypes array of length self.ndim where the basetype is the C-integer corresponding to dtype('p') on this platform (see c_intp). This base-type could be ctypes.c_int, ctypes.c_long, or ctypes.c_longlong depending on the platform. The ctypes array contains the shape of the underlying array.", "(c_intp*self.ndim): A ctypes array of length self.ndim where the basetype is the same as for the shape attribute. This ctypes array contains the strides information from the underlying array. This strides information is important for showing how many bytes must be jumped to get to the next element in the array.", "Return the data pointer cast to a particular c-types object. For example, calling self._as_parameter_ is equivalent to self.data_as(ctypes.c_void_p). Perhaps you want to use the data as a pointer to a ctypes array of floating-point data: self.data_as(ctypes.POINTER(ctypes.c_double)).", "The returned pointer will keep a reference to the array.", "Return the shape tuple as an array of some other c-types type. For example: self.shape_as(ctypes.c_short).", "Return the strides tuple as an array of some other c-types type. For example: self.strides_as(ctypes.c_longlong).", "If the ctypes module is not available, then the ctypes attribute of array objects still returns something useful, but ctypes objects are not returned and errors may be raised instead. In particular, the object will still have the as_parameter attribute which will return an integer equal to the data attribute."]}, {"name": "char.chararray.data", "path": "reference/generated/numpy.char.chararray.data", "type": "String operations", "text": ["attribute", "Python buffer object pointing to the start of the array\u2019s data."]}, {"name": "char.chararray.decode()", "path": "reference/generated/numpy.char.chararray.decode", "type": "numpy.char.chararray.decode", "text": ["method", "Calls str.decode element-wise.", "See also"]}, {"name": "char.chararray.dtype", "path": "reference/generated/numpy.char.chararray.dtype", "type": "String operations", "text": ["attribute", "Data-type of the array\u2019s elements.", "See also"]}, {"name": "char.chararray.dump()", "path": "reference/generated/numpy.char.chararray.dump", "type": "numpy.char.chararray.dump", "text": ["method", "Dump a pickle of the array to the specified file. The array can be read back with pickle.load or numpy.load.", "A string naming the dump file.", "Changed in version 1.17.0: pathlib.Path objects are now accepted."]}, {"name": "char.chararray.dumps()", "path": "reference/generated/numpy.char.chararray.dumps", "type": "numpy.char.chararray.dumps", "text": ["method", "Returns the pickle of the array as a string. pickle.loads will convert the string back to an array."]}, {"name": "char.chararray.encode()", "path": "reference/generated/numpy.char.chararray.encode", "type": "numpy.char.chararray.encode", "text": ["method", "Calls str.encode element-wise.", "See also"]}, {"name": "char.chararray.endswith()", "path": "reference/generated/numpy.char.chararray.endswith", "type": "numpy.char.chararray.endswith", "text": ["method", "Returns a boolean array which is True where the string element in self ends with suffix, otherwise False.", "See also"]}, {"name": "char.chararray.expandtabs()", "path": "reference/generated/numpy.char.chararray.expandtabs", "type": "numpy.char.chararray.expandtabs", "text": ["method", "Return a copy of each string element where all tab characters are replaced by one or more spaces.", "See also"]}, {"name": "char.chararray.fill()", "path": "reference/generated/numpy.char.chararray.fill", "type": "numpy.char.chararray.fill", "text": ["method", "Fill the array with a scalar value.", "All elements of a will be assigned this value."]}, {"name": "char.chararray.find()", "path": "reference/generated/numpy.char.chararray.find", "type": "numpy.char.chararray.find", "text": ["method", "For each element, return the lowest index in the string where substring sub is found.", "See also"]}, {"name": "char.chararray.flags", "path": "reference/generated/numpy.char.chararray.flags", "type": "String operations", "text": ["attribute", "Information about the memory layout of the array.", "The flags object can be accessed dictionary-like (as in a.flags['WRITEABLE']), or by using lowercased attribute names (as in a.flags.writeable). Short flag names are only supported in dictionary access.", "Only the WRITEBACKIFCOPY, UPDATEIFCOPY, WRITEABLE, and ALIGNED flags can be changed by the user, via direct assignment to the attribute or dictionary entry, or by calling ndarray.setflags.", "The array flags cannot be set arbitrarily:", "Arrays can be both C-style and Fortran-style contiguous simultaneously. This is clear for 1-dimensional arrays, but can also be true for higher dimensional arrays.", "Even for contiguous arrays a stride for a given dimension arr.strides[dim] may be arbitrary if arr.shape[dim] == 1 or the array has no elements. It does not generally hold that self.strides[-1] == self.itemsize for C-style contiguous arrays or self.strides[0] == self.itemsize for Fortran-style contiguous arrays is true.", "The data is in a single, C-style contiguous segment.", "The data is in a single, Fortran-style contiguous segment.", "The array owns the memory it uses or borrows it from another object.", "The data area can be written to. Setting this to False locks the data, making it read-only. A view (slice, etc.) inherits WRITEABLE from its base array at creation time, but a view of a writeable array may be subsequently locked while the base array remains writeable. (The opposite is not true, in that a view of a locked array may not be made writeable. However, currently, locking a base object does not lock any views that already reference it, so under that circumstance it is possible to alter the contents of a locked array via a previously created writeable view onto it.) Attempting to change a non-writeable array raises a RuntimeError exception.", "The data and all elements are aligned appropriately for the hardware.", "This array is a copy of some other array. The C-API function PyArray_ResolveWritebackIfCopy must be called before deallocating to the base array will be updated with the contents of this array.", "(Deprecated, use WRITEBACKIFCOPY) This array is a copy of some other array. When this array is deallocated, the base array will be updated with the contents of this array.", "F_CONTIGUOUS and not C_CONTIGUOUS.", "F_CONTIGUOUS or C_CONTIGUOUS (one-segment test).", "ALIGNED and WRITEABLE.", "BEHAVED and C_CONTIGUOUS.", "BEHAVED and F_CONTIGUOUS and not C_CONTIGUOUS."]}, {"name": "char.chararray.flat", "path": "reference/generated/numpy.char.chararray.flat", "type": "String operations", "text": ["attribute", "A 1-D iterator over the array.", "This is a numpy.flatiter instance, which acts similarly to, but is not a subclass of, Python\u2019s built-in iterator object.", "See also", "Return a copy of the array collapsed into one dimension.", "An assignment example:"]}, {"name": "char.chararray.flatten()", "path": "reference/generated/numpy.char.chararray.flatten", "type": "numpy.char.chararray.flatten", "text": ["method", "Return a copy of the array collapsed into one dimension.", "\u2018C\u2019 means to flatten in row-major (C-style) order. \u2018F\u2019 means to flatten in column-major (Fortran- style) order. \u2018A\u2019 means to flatten in column-major order if a is Fortran contiguous in memory, row-major order otherwise. \u2018K\u2019 means to flatten a in the order the elements occur in memory. The default is \u2018C\u2019.", "A copy of the input array, flattened to one dimension.", "See also", "Return a flattened array.", "A 1-D flat iterator over the array."]}, {"name": "char.chararray.getfield()", "path": "reference/generated/numpy.char.chararray.getfield", "type": "numpy.char.chararray.getfield", "text": ["method", "Returns a field of the given array as a certain type.", "A field is a view of the array data with a given data-type. The values in the view are determined by the given type and the offset into the current array in bytes. The offset needs to be such that the view dtype fits in the array dtype; for example an array of dtype complex128 has 16-byte elements. If taking a view with a 32-bit integer (4 bytes), the offset needs to be between 0 and 12 bytes.", "The data type of the view. The dtype size of the view can not be larger than that of the array itself.", "Number of bytes to skip before beginning the element view.", "By choosing an offset of 8 bytes we can select the complex part of the array for our view:"]}, {"name": "char.chararray.imag", "path": "reference/generated/numpy.char.chararray.imag", "type": "String operations", "text": ["attribute", "The imaginary part of the array."]}, {"name": "char.chararray.index()", "path": "reference/generated/numpy.char.chararray.index", "type": "numpy.char.chararray.index", "text": ["method", "Like find, but raises ValueError when the substring is not found.", "See also"]}, {"name": "char.chararray.isalnum()", "path": "reference/generated/numpy.char.chararray.isalnum", "type": "numpy.char.chararray.isalnum", "text": ["method", "Returns true for each element if all characters in the string are alphanumeric and there is at least one character, false otherwise.", "See also"]}, {"name": "char.chararray.isalpha()", "path": "reference/generated/numpy.char.chararray.isalpha", "type": "numpy.char.chararray.isalpha", "text": ["method", "Returns true for each element if all characters in the string are alphabetic and there is at least one character, false otherwise.", "See also"]}, {"name": "char.chararray.isdecimal()", "path": "reference/generated/numpy.char.chararray.isdecimal", "type": "numpy.char.chararray.isdecimal", "text": ["method", "For each element in self, return True if there are only decimal characters in the element.", "See also"]}, {"name": "char.chararray.isdigit()", "path": "reference/generated/numpy.char.chararray.isdigit", "type": "numpy.char.chararray.isdigit", "text": ["method", "Returns true for each element if all characters in the string are digits and there is at least one character, false otherwise.", "See also"]}, {"name": "char.chararray.islower()", "path": "reference/generated/numpy.char.chararray.islower", "type": "numpy.char.chararray.islower", "text": ["method", "Returns true for each element if all cased characters in the string are lowercase and there is at least one cased character, false otherwise.", "See also"]}, {"name": "char.chararray.isnumeric()", "path": "reference/generated/numpy.char.chararray.isnumeric", "type": "numpy.char.chararray.isnumeric", "text": ["method", "For each element in self, return True if there are only numeric characters in the element.", "See also"]}, {"name": "char.chararray.isspace()", "path": "reference/generated/numpy.char.chararray.isspace", "type": "numpy.char.chararray.isspace", "text": ["method", "Returns true for each element if there are only whitespace characters in the string and there is at least one character, false otherwise.", "See also"]}, {"name": "char.chararray.istitle()", "path": "reference/generated/numpy.char.chararray.istitle", "type": "numpy.char.chararray.istitle", "text": ["method", "Returns true for each element if the element is a titlecased string and there is at least one character, false otherwise.", "See also"]}, {"name": "char.chararray.isupper()", "path": "reference/generated/numpy.char.chararray.isupper", "type": "numpy.char.chararray.isupper", "text": ["method", "Returns true for each element if all cased characters in the string are uppercase and there is at least one character, false otherwise.", "See also"]}, {"name": "char.chararray.item()", "path": "reference/generated/numpy.char.chararray.item", "type": "numpy.char.chararray.item", "text": ["method", "Copy an element of an array to a standard Python scalar and return it.", "A copy of the specified element of the array as a suitable Python scalar", "When the data type of a is longdouble or clongdouble, item() returns a scalar array object because there is no available Python scalar that would not lose information. Void arrays return a buffer object for item(), unless fields are defined, in which case a tuple is returned.", "item is very similar to a[args], except, instead of an array scalar, a standard Python scalar is returned. This can be useful for speeding up access to elements of the array and doing arithmetic on elements of the array using Python\u2019s optimized math."]}, {"name": "char.chararray.itemsize", "path": "reference/generated/numpy.char.chararray.itemsize", "type": "String operations", "text": ["attribute", "Length of one array element in bytes."]}, {"name": "char.chararray.join()", "path": "reference/generated/numpy.char.chararray.join", "type": "numpy.char.chararray.join", "text": ["method", "Return a string which is the concatenation of the strings in the sequence seq.", "See also"]}, {"name": "char.chararray.ljust()", "path": "reference/generated/numpy.char.chararray.ljust", "type": "numpy.char.chararray.ljust", "text": ["method", "Return an array with the elements of self left-justified in a string of length width.", "See also"]}, {"name": "char.chararray.lower()", "path": "reference/generated/numpy.char.chararray.lower", "type": "numpy.char.chararray.lower", "text": ["method", "Return an array with the elements of self converted to lowercase.", "See also"]}, {"name": "char.chararray.lstrip()", "path": "reference/generated/numpy.char.chararray.lstrip", "type": "numpy.char.chararray.lstrip", "text": ["method", "For each element in self, return a copy with the leading characters removed.", "See also"]}, {"name": "char.chararray.nbytes", "path": "reference/generated/numpy.char.chararray.nbytes", "type": "String operations", "text": ["attribute", "Total bytes consumed by the elements of the array.", "Does not include memory consumed by non-element attributes of the array object."]}, {"name": "char.chararray.ndim", "path": "reference/generated/numpy.char.chararray.ndim", "type": "String operations", "text": ["attribute", "Number of array dimensions."]}, {"name": "char.chararray.nonzero()", "path": "reference/generated/numpy.char.chararray.nonzero", "type": "numpy.char.chararray.nonzero", "text": ["method", "Return the indices of the elements that are non-zero.", "Refer to numpy.nonzero for full documentation.", "See also", "equivalent function"]}, {"name": "char.chararray.put()", "path": "reference/generated/numpy.char.chararray.put", "type": "numpy.char.chararray.put", "text": ["method", "Set a.flat[n] = values[n] for all n in indices.", "Refer to numpy.put for full documentation.", "See also", "equivalent function"]}, {"name": "char.chararray.ravel()", "path": "reference/generated/numpy.char.chararray.ravel", "type": "numpy.char.chararray.ravel", "text": ["method", "Return a flattened array.", "Refer to numpy.ravel for full documentation.", "See also", "equivalent function", "a flat iterator on the array."]}, {"name": "char.chararray.real", "path": "reference/generated/numpy.char.chararray.real", "type": "String operations", "text": ["attribute", "The real part of the array.", "See also", "equivalent function"]}, {"name": "char.chararray.repeat()", "path": "reference/generated/numpy.char.chararray.repeat", "type": "numpy.char.chararray.repeat", "text": ["method", "Repeat elements of an array.", "Refer to numpy.repeat for full documentation.", "See also", "equivalent function"]}, {"name": "char.chararray.replace()", "path": "reference/generated/numpy.char.chararray.replace", "type": "numpy.char.chararray.replace", "text": ["method", "For each element in self, return a copy of the string with all occurrences of substring old replaced by new.", "See also"]}, {"name": "char.chararray.reshape()", "path": "reference/generated/numpy.char.chararray.reshape", "type": "numpy.char.chararray.reshape", "text": ["method", "Returns an array containing the same data with a new shape.", "Refer to numpy.reshape for full documentation.", "See also", "equivalent function", "Unlike the free function numpy.reshape, this method on ndarray allows the elements of the shape parameter to be passed in as separate arguments. For example, a.reshape(10, 11) is equivalent to a.reshape((10, 11))."]}, {"name": "char.chararray.resize()", "path": "reference/generated/numpy.char.chararray.resize", "type": "numpy.char.chararray.resize", "text": ["method", "Change shape and size of array in-place.", "Shape of resized array.", "If False, reference count will not be checked. Default is True.", "If a does not own its own data or references or views to it exist, and the data memory must be changed. PyPy only: will always raise if the data memory must be changed, since there is no reliable way to determine if references or views to it exist.", "If the order keyword argument is specified. This behaviour is a bug in NumPy.", "See also", "Return a new array with the specified shape.", "This reallocates space for the data area if necessary.", "Only contiguous arrays (data elements consecutive in memory) can be resized.", "The purpose of the reference count check is to make sure you do not use this array as a buffer for another Python object and then reallocate the memory. However, reference counts can increase in other ways so if you are sure that you have not shared the memory for this array with another Python object, then you may safely set refcheck to False.", "Shrinking an array: array is flattened (in the order that the data are stored in memory), resized, and reshaped:", "Enlarging an array: as above, but missing entries are filled with zeros:", "Referencing an array prevents resizing\u2026", "Unless refcheck is False:"]}, {"name": "char.chararray.rfind()", "path": "reference/generated/numpy.char.chararray.rfind", "type": "numpy.char.chararray.rfind", "text": ["method", "For each element in self, return the highest index in the string where substring sub is found, such that sub is contained within [start, end].", "See also"]}, {"name": "char.chararray.rindex()", "path": "reference/generated/numpy.char.chararray.rindex", "type": "numpy.char.chararray.rindex", "text": ["method", "Like rfind, but raises ValueError when the substring sub is not found.", "See also"]}, {"name": "char.chararray.rjust()", "path": "reference/generated/numpy.char.chararray.rjust", "type": "numpy.char.chararray.rjust", "text": ["method", "Return an array with the elements of self right-justified in a string of length width.", "See also"]}, {"name": "char.chararray.rsplit()", "path": "reference/generated/numpy.char.chararray.rsplit", "type": "numpy.char.chararray.rsplit", "text": ["method", "For each element in self, return a list of the words in the string, using sep as the delimiter string.", "See also"]}, {"name": "char.chararray.rstrip()", "path": "reference/generated/numpy.char.chararray.rstrip", "type": "numpy.char.chararray.rstrip", "text": ["method", "For each element in self, return a copy with the trailing characters removed.", "See also"]}, {"name": "char.chararray.searchsorted()", "path": "reference/generated/numpy.char.chararray.searchsorted", "type": "numpy.char.chararray.searchsorted", "text": ["method", "Find indices where elements of v should be inserted in a to maintain order.", "For full documentation, see numpy.searchsorted", "See also", "equivalent function"]}, {"name": "char.chararray.setfield()", "path": "reference/generated/numpy.char.chararray.setfield", "type": "numpy.char.chararray.setfield", "text": ["method", "Put a value into a specified place in a field defined by a data-type.", "Place val into a\u2019s field defined by dtype and beginning offset bytes into the field.", "Value to be placed in field.", "Data-type of the field in which to place val.", "The number of bytes into the field at which to place val.", "See also"]}, {"name": "char.chararray.setflags()", "path": "reference/generated/numpy.char.chararray.setflags", "type": "numpy.char.chararray.setflags", "text": ["method", "Set array flags WRITEABLE, ALIGNED, (WRITEBACKIFCOPY and UPDATEIFCOPY), respectively.", "These Boolean-valued flags affect how numpy interprets the memory area used by a (see Notes below). The ALIGNED flag can only be set to True if the data is actually aligned according to the type. The WRITEBACKIFCOPY and (deprecated) UPDATEIFCOPY flags can never be set to True. The flag WRITEABLE can only be set to True if the array owns its own memory, or the ultimate owner of the memory exposes a writeable buffer interface, or is a string. (The exception for string is made so that unpickling can be done without copying memory.)", "Describes whether or not a can be written to.", "Describes whether or not a is aligned properly for its type.", "Describes whether or not a is a copy of another \u201cbase\u201d array.", "Array flags provide information about how the memory area used for the array is to be interpreted. There are 7 Boolean flags in use, only four of which can be changed by the user: WRITEBACKIFCOPY, UPDATEIFCOPY, WRITEABLE, and ALIGNED.", "WRITEABLE (W) the data area can be written to;", "ALIGNED (A) the data and strides are aligned appropriately for the hardware (as determined by the compiler);", "UPDATEIFCOPY (U) (deprecated), replaced by WRITEBACKIFCOPY;", "WRITEBACKIFCOPY (X) this array is a copy of some other array (referenced by .base). When the C-API function PyArray_ResolveWritebackIfCopy is called, the base array will be updated with the contents of this array.", "All flags can be accessed using the single (upper case) letter as well as the full name."]}, {"name": "char.chararray.shape", "path": "reference/generated/numpy.char.chararray.shape", "type": "String operations", "text": ["attribute", "Tuple of array dimensions.", "The shape property is usually used to get the current shape of an array, but may also be used to reshape the array in-place by assigning a tuple of array dimensions to it. As with numpy.reshape, one of the new shape dimensions can be -1, in which case its value is inferred from the size of the array and the remaining dimensions. Reshaping an array in-place will fail if a copy is required.", "See also", "similar function", "similar method"]}, {"name": "char.chararray.size", "path": "reference/generated/numpy.char.chararray.size", "type": "String operations", "text": ["attribute", "Number of elements in the array.", "Equal to np.prod(a.shape), i.e., the product of the array\u2019s dimensions.", "a.size returns a standard arbitrary precision Python integer. This may not be the case with other methods of obtaining the same value (like the suggested np.prod(a.shape), which returns an instance of np.int_), and may be relevant if the value is used further in calculations that may overflow a fixed size integer type."]}, {"name": "char.chararray.sort()", "path": "reference/generated/numpy.char.chararray.sort", "type": "numpy.char.chararray.sort", "text": ["method", "Sort an array in-place. Refer to numpy.sort for full documentation.", "Axis along which to sort. Default is -1, which means sort along the last axis.", "Sorting algorithm. The default is \u2018quicksort\u2019. Note that both \u2018stable\u2019 and \u2018mergesort\u2019 use timsort under the covers and, in general, the actual implementation will vary with datatype. The \u2018mergesort\u2019 option is retained for backwards compatibility.", "Changed in version 1.15.0: The \u2018stable\u2019 option was added.", "When a is an array with fields defined, this argument specifies which fields to compare first, second, etc. A single field can be specified as a string, and not all fields need be specified, but unspecified fields will still be used, in the order in which they come up in the dtype, to break ties.", "See also", "Return a sorted copy of an array.", "Indirect sort.", "Indirect stable sort on multiple keys.", "Find elements in sorted array.", "Partial sort.", "See numpy.sort for notes on the different sorting algorithms.", "Use the order keyword to specify a field to use when sorting a structured array:"]}, {"name": "char.chararray.split()", "path": "reference/generated/numpy.char.chararray.split", "type": "numpy.char.chararray.split", "text": ["method", "For each element in self, return a list of the words in the string, using sep as the delimiter string.", "See also"]}, {"name": "char.chararray.splitlines()", "path": "reference/generated/numpy.char.chararray.splitlines", "type": "numpy.char.chararray.splitlines", "text": ["method", "For each element in self, return a list of the lines in the element, breaking at line boundaries.", "See also"]}, {"name": "char.chararray.squeeze()", "path": "reference/generated/numpy.char.chararray.squeeze", "type": "numpy.char.chararray.squeeze", "text": ["method", "Remove axes of length one from a.", "Refer to numpy.squeeze for full documentation.", "See also", "equivalent function"]}, {"name": "char.chararray.startswith()", "path": "reference/generated/numpy.char.chararray.startswith", "type": "numpy.char.chararray.startswith", "text": ["method", "Returns a boolean array which is True where the string element in self starts with prefix, otherwise False.", "See also"]}, {"name": "char.chararray.strides", "path": "reference/generated/numpy.char.chararray.strides", "type": "String operations", "text": ["attribute", "Tuple of bytes to step in each dimension when traversing an array.", "The byte offset of element (i[0], i[1], ..., i[n]) in an array a is:", "A more detailed explanation of strides can be found in the \u201cndarray.rst\u201d file in the NumPy reference guide.", "See also", "Imagine an array of 32-bit integers (each 4 bytes):", "This array is stored in memory as 40 bytes, one after the other (known as a contiguous block of memory). The strides of an array tell us how many bytes we have to skip in memory to move to the next position along a certain axis. For example, we have to skip 4 bytes (1 value) to move to the next column, but 20 bytes (5 values) to get to the same position in the next row. As such, the strides for the array x will be (20, 4)."]}, {"name": "char.chararray.strip()", "path": "reference/generated/numpy.char.chararray.strip", "type": "numpy.char.chararray.strip", "text": ["method", "For each element in self, return a copy with the leading and trailing characters removed.", "See also"]}, {"name": "char.chararray.swapaxes()", "path": "reference/generated/numpy.char.chararray.swapaxes", "type": "numpy.char.chararray.swapaxes", "text": ["method", "Return a view of the array with axis1 and axis2 interchanged.", "Refer to numpy.swapaxes for full documentation.", "See also", "equivalent function"]}, {"name": "char.chararray.swapcase()", "path": "reference/generated/numpy.char.chararray.swapcase", "type": "numpy.char.chararray.swapcase", "text": ["method", "For each element in self, return a copy of the string with uppercase characters converted to lowercase and vice versa.", "See also"]}, {"name": "char.chararray.T", "path": "reference/generated/numpy.char.chararray.t", "type": "String operations", "text": ["attribute", "The transposed array.", "Same as self.transpose().", "See also"]}, {"name": "char.chararray.take()", "path": "reference/generated/numpy.char.chararray.take", "type": "numpy.char.chararray.take", "text": ["method", "Return an array formed from the elements of a at the given indices.", "Refer to numpy.take for full documentation.", "See also", "equivalent function"]}, {"name": "char.chararray.title()", "path": "reference/generated/numpy.char.chararray.title", "type": "numpy.char.chararray.title", "text": ["method", "For each element in self, return a titlecased version of the string: words start with uppercase characters, all remaining cased characters are lowercase.", "See also"]}, {"name": "char.chararray.tobytes()", "path": "reference/generated/numpy.char.chararray.tobytes", "type": "String operations", "text": ["method", "Construct Python bytes containing the raw data bytes in the array.", "Constructs Python bytes showing a copy of the raw contents of data memory. The bytes object is produced in C-order by default. This behavior is controlled by the order parameter.", "New in version 1.9.0.", "Controls the memory layout of the bytes object. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 (short for Any) means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. Default is \u2018C\u2019.", "Python bytes exhibiting a copy of a\u2019s raw data."]}, {"name": "char.chararray.tofile()", "path": "reference/generated/numpy.char.chararray.tofile", "type": "numpy.char.chararray.tofile", "text": ["method", "Write array to a file as text or binary (default).", "Data is always written in \u2018C\u2019 order, independent of the order of a. The data produced by this method can be recovered using the function fromfile().", "An open file object, or a string containing a filename.", "Changed in version 1.17.0: pathlib.Path objects are now accepted.", "Separator between array items for text output. If \u201c\u201d (empty), a binary file is written, equivalent to file.write(a.tobytes()).", "Format string for text file output. Each entry in the array is formatted to text by first converting it to the closest Python type, and then using \u201cformat\u201d % item.", "This is a convenience function for quick storage of array data. Information on endianness and precision is lost, so this method is not a good choice for files intended to archive data or transport data between machines with different endianness. Some of these problems can be overcome by outputting the data as text files, at the expense of speed and file size.", "When fid is a file object, array contents are directly written to the file, bypassing the file object\u2019s write method. As a result, tofile cannot be used with files objects supporting compression (e.g., GzipFile) or file-like objects that do not support fileno() (e.g., BytesIO)."]}, {"name": "char.chararray.tolist()", "path": "reference/generated/numpy.char.chararray.tolist", "type": "numpy.char.chararray.tolist", "text": ["method", "Return the array as an a.ndim-levels deep nested list of Python scalars.", "Return a copy of the array data as a (nested) Python list. Data items are converted to the nearest compatible builtin Python type, via the item function.", "If a.ndim is 0, then since the depth of the nested list is 0, it will not be a list at all, but a simple Python scalar.", "The possibly nested list of array elements.", "The array may be recreated via a = np.array(a.tolist()), although this may sometimes lose precision.", "For a 1D array, a.tolist() is almost the same as list(a), except that tolist changes numpy scalars to Python scalars:", "Additionally, for a 2D array, tolist applies recursively:", "The base case for this recursion is a 0D array:"]}, {"name": "char.chararray.tostring()", "path": "reference/generated/numpy.char.chararray.tostring", "type": "numpy.char.chararray.tostring", "text": ["method", "A compatibility alias for tobytes, with exactly the same behavior.", "Despite its name, it returns bytes not strs.", "Deprecated since version 1.19.0."]}, {"name": "char.chararray.translate()", "path": "reference/generated/numpy.char.chararray.translate", "type": "numpy.char.chararray.translate", "text": ["method", "For each element in self, return a copy of the string where all characters occurring in the optional argument deletechars are removed, and the remaining characters have been mapped through the given translation table.", "See also"]}, {"name": "char.chararray.transpose()", "path": "reference/generated/numpy.char.chararray.transpose", "type": "numpy.char.chararray.transpose", "text": ["method", "Returns a view of the array with axes transposed.", "For a 1-D array this has no effect, as a transposed vector is simply the same vector. To convert a 1-D array into a 2D column vector, an additional dimension must be added. np.atleast2d(a).T achieves this, as does a[:, np.newaxis]. For a 2-D array, this is a standard matrix transpose. For an n-D array, if axes are given, their order indicates how the axes are permuted (see Examples). If axes are not provided and a.shape = (i[0], i[1], ... i[n-2], i[n-1]), then a.transpose().shape = (i[n-1], i[n-2], ... i[1], i[0]).", "View of a, with axes suitably permuted.", "See also", "Equivalent function", "Array property returning the array transposed.", "Give a new shape to an array without changing its data."]}, {"name": "char.chararray.upper()", "path": "reference/generated/numpy.char.chararray.upper", "type": "numpy.char.chararray.upper", "text": ["method", "Return an array with the elements of self converted to uppercase.", "See also"]}, {"name": "char.chararray.view()", "path": "reference/generated/numpy.char.chararray.view", "type": "numpy.char.chararray.view", "text": ["method", "New view of array with the same data.", "Note", "Passing None for dtype is different from omitting the parameter, since the former invokes dtype(None) which is an alias for dtype('float_').", "Data-type descriptor of the returned view, e.g., float32 or int16. Omitting it results in the view having the same data-type as a. This argument can also be specified as an ndarray sub-class, which then specifies the type of the returned object (this is equivalent to setting the type parameter).", "Type of the returned view, e.g., ndarray or matrix. Again, omission of the parameter results in type preservation.", "a.view() is used two different ways:", "a.view(some_dtype) or a.view(dtype=some_dtype) constructs a view of the array\u2019s memory with a different data-type. This can cause a reinterpretation of the bytes of memory.", "a.view(ndarray_subclass) or a.view(type=ndarray_subclass) just returns an instance of ndarray_subclass that looks at the same array (same shape, dtype, etc.) This does not cause a reinterpretation of the memory.", "For a.view(some_dtype), if some_dtype has a different number of bytes per entry than the previous dtype (for example, converting a regular array to a structured array), then the behavior of the view cannot be predicted just from the superficial appearance of a (shown by print(a)). It also depends on exactly how a is stored in memory. Therefore if a is C-ordered versus fortran-ordered, versus defined as a slice or transpose, etc., the view may give different results.", "Viewing array data using a different type and dtype:", "Creating a view on a structured array so it can be used in calculations", "Making changes to the view changes the underlying array", "Using a view to convert an array to a recarray:", "Views share data:", "Views that change the dtype size (bytes per entry) should normally be avoided on arrays defined by slices, transposes, fortran-ordering, etc.:"]}, {"name": "char.chararray.zfill()", "path": "reference/generated/numpy.char.chararray.zfill", "type": "numpy.char.chararray.zfill", "text": ["method", "Return the numeric string left-filled with zeros in a string of length width.", "See also"]}, {"name": "char.compare_chararrays()", "path": "reference/generated/numpy.char.compare_chararrays", "type": "numpy.char.compare_chararrays", "text": ["Performs element-wise comparison of two string arrays using the comparison operator specified by cmp_op.", "Arrays to be compared.", "Type of comparison.", "If True, the spaces at the end of Strings are removed before the comparison.", "The output array of type Boolean with the same shape as a and b.", "If cmp_op is not valid.", "If at least one of a or b is a non-string array"]}, {"name": "char.count()", "path": "reference/generated/numpy.char.count", "type": "numpy.char.count", "text": ["Returns an array with the number of non-overlapping occurrences of substring sub in the range [start, end].", "Calls str.count element-wise.", "The substring to search for.", "Optional arguments start and end are interpreted as slice notation to specify the range in which to count.", "Output array of ints.", "See also"]}, {"name": "char.decode()", "path": "reference/generated/numpy.char.decode", "type": "numpy.char.decode", "text": ["Calls str.decode element-wise.", "The set of available codecs comes from the Python standard library, and may be extended at runtime. For more information, see the codecs module.", "The name of an encoding", "Specifies how to handle encoding errors", "See also", "The type of the result will depend on the encoding specified."]}, {"name": "char.encode()", "path": "reference/generated/numpy.char.encode", "type": "numpy.char.encode", "text": ["Calls str.encode element-wise.", "The set of available codecs comes from the Python standard library, and may be extended at runtime. For more information, see the codecs module.", "The name of an encoding", "Specifies how to handle encoding errors", "See also", "The type of the result will depend on the encoding specified."]}, {"name": "char.endswith()", "path": "reference/generated/numpy.char.endswith", "type": "numpy.char.endswith", "text": ["Returns a boolean array which is True where the string element in a ends with suffix, otherwise False.", "Calls str.endswith element-wise.", "With optional start, test beginning at that position. With optional end, stop comparing at that position.", "Outputs an array of bools.", "See also"]}, {"name": "char.equal()", "path": "reference/generated/numpy.char.equal", "type": "numpy.char.equal", "text": ["Return (x1 == x2) element-wise.", "Unlike numpy.equal, this comparison is performed by first stripping whitespace characters from the end of the string. This behavior is provided for backward-compatibility with numarray.", "Input arrays of the same shape.", "Output array of bools.", "See also"]}, {"name": "char.expandtabs()", "path": "reference/generated/numpy.char.expandtabs", "type": "numpy.char.expandtabs", "text": ["Return a copy of each string element where all tab characters are replaced by one or more spaces.", "Calls str.expandtabs element-wise.", "Return a copy of each string element where all tab characters are replaced by one or more spaces, depending on the current column and the given tabsize. The column number is reset to zero after each newline occurring in the string. This doesn\u2019t understand other non-printing characters or escape sequences.", "Input array", "Replace tabs with tabsize number of spaces. If not given defaults to 8 spaces.", "Output array of str or unicode, depending on input type", "See also"]}, {"name": "char.find()", "path": "reference/generated/numpy.char.find", "type": "numpy.char.find", "text": ["For each element, return the lowest index in the string where substring sub is found.", "Calls str.find element-wise.", "For each element, return the lowest index in the string where substring sub is found, such that sub is contained in the range [start, end].", "Optional arguments start and end are interpreted as in slice notation.", "Output array of ints. Returns -1 if sub is not found.", "See also"]}, {"name": "char.greater()", "path": "reference/generated/numpy.char.greater", "type": "numpy.char.greater", "text": ["Return (x1 > x2) element-wise.", "Unlike numpy.greater, this comparison is performed by first stripping whitespace characters from the end of the string. This behavior is provided for backward-compatibility with numarray.", "Input arrays of the same shape.", "Output array of bools.", "See also"]}, {"name": "char.greater_equal()", "path": "reference/generated/numpy.char.greater_equal", "type": "numpy.char.greater_equal", "text": ["Return (x1 >= x2) element-wise.", "Unlike numpy.greater_equal, this comparison is performed by first stripping whitespace characters from the end of the string. This behavior is provided for backward-compatibility with numarray.", "Input arrays of the same shape.", "Output array of bools.", "See also"]}, {"name": "char.index()", "path": "reference/generated/numpy.char.index", "type": "numpy.char.index", "text": ["Like find, but raises ValueError when the substring is not found.", "Calls str.index element-wise.", "Output array of ints. Returns -1 if sub is not found.", "See also"]}, {"name": "char.isalnum()", "path": "reference/generated/numpy.char.isalnum", "type": "numpy.char.isalnum", "text": ["Returns true for each element if all characters in the string are alphanumeric and there is at least one character, false otherwise.", "Calls str.isalnum element-wise.", "For 8-bit strings, this method is locale-dependent.", "Output array of str or unicode, depending on input type", "See also"]}, {"name": "char.isalpha()", "path": "reference/generated/numpy.char.isalpha", "type": "numpy.char.isalpha", "text": ["Returns true for each element if all characters in the string are alphabetic and there is at least one character, false otherwise.", "Calls str.isalpha element-wise.", "For 8-bit strings, this method is locale-dependent.", "Output array of bools", "See also"]}, {"name": "char.isdecimal()", "path": "reference/generated/numpy.char.isdecimal", "type": "numpy.char.isdecimal", "text": ["For each element, return True if there are only decimal characters in the element.", "Calls unicode.isdecimal element-wise.", "Decimal characters include digit characters, and all characters that can be used to form decimal-radix numbers, e.g. U+0660, ARABIC-INDIC DIGIT ZERO.", "Input array.", "Array of booleans identical in shape to a.", "See also"]}, {"name": "char.isdigit()", "path": "reference/generated/numpy.char.isdigit", "type": "numpy.char.isdigit", "text": ["Returns true for each element if all characters in the string are digits and there is at least one character, false otherwise.", "Calls str.isdigit element-wise.", "For 8-bit strings, this method is locale-dependent.", "Output array of bools", "See also"]}, {"name": "char.islower()", "path": "reference/generated/numpy.char.islower", "type": "numpy.char.islower", "text": ["Returns true for each element if all cased characters in the string are lowercase and there is at least one cased character, false otherwise.", "Calls str.islower element-wise.", "For 8-bit strings, this method is locale-dependent.", "Output array of bools", "See also"]}, {"name": "char.isnumeric()", "path": "reference/generated/numpy.char.isnumeric", "type": "numpy.char.isnumeric", "text": ["For each element, return True if there are only numeric characters in the element.", "Calls unicode.isnumeric element-wise.", "Numeric characters include digit characters, and all characters that have the Unicode numeric value property, e.g. U+2155,\nVULGAR FRACTION ONE FIFTH.", "Input array.", "Array of booleans of same shape as a.", "See also"]}, {"name": "char.isspace()", "path": "reference/generated/numpy.char.isspace", "type": "numpy.char.isspace", "text": ["Returns true for each element if there are only whitespace characters in the string and there is at least one character, false otherwise.", "Calls str.isspace element-wise.", "For 8-bit strings, this method is locale-dependent.", "Output array of bools", "See also"]}, {"name": "char.istitle()", "path": "reference/generated/numpy.char.istitle", "type": "numpy.char.istitle", "text": ["Returns true for each element if the element is a titlecased string and there is at least one character, false otherwise.", "Call str.istitle element-wise.", "For 8-bit strings, this method is locale-dependent.", "Output array of bools", "See also"]}, {"name": "char.isupper()", "path": "reference/generated/numpy.char.isupper", "type": "numpy.char.isupper", "text": ["Returns true for each element if all cased characters in the string are uppercase and there is at least one character, false otherwise.", "Call str.isupper element-wise.", "For 8-bit strings, this method is locale-dependent.", "Output array of bools", "See also"]}, {"name": "char.join()", "path": "reference/generated/numpy.char.join", "type": "numpy.char.join", "text": ["Return a string which is the concatenation of the strings in the sequence seq.", "Calls str.join element-wise.", "Output array of str or unicode, depending on input types", "See also"]}, {"name": "char.less()", "path": "reference/generated/numpy.char.less", "type": "numpy.char.less", "text": ["Return (x1 < x2) element-wise.", "Unlike numpy.greater, this comparison is performed by first stripping whitespace characters from the end of the string. This behavior is provided for backward-compatibility with numarray.", "Input arrays of the same shape.", "Output array of bools.", "See also"]}, {"name": "char.less_equal()", "path": "reference/generated/numpy.char.less_equal", "type": "numpy.char.less_equal", "text": ["Return (x1 <= x2) element-wise.", "Unlike numpy.less_equal, this comparison is performed by first stripping whitespace characters from the end of the string. This behavior is provided for backward-compatibility with numarray.", "Input arrays of the same shape.", "Output array of bools.", "See also"]}, {"name": "char.ljust()", "path": "reference/generated/numpy.char.ljust", "type": "numpy.char.ljust", "text": ["Return an array with the elements of a left-justified in a string of length width.", "Calls str.ljust element-wise.", "The length of the resulting strings", "The character to use for padding", "Output array of str or unicode, depending on input type", "See also"]}, {"name": "char.lower()", "path": "reference/generated/numpy.char.lower", "type": "numpy.char.lower", "text": ["Return an array with the elements converted to lowercase.", "Call str.lower element-wise.", "For 8-bit strings, this method is locale-dependent.", "Input array.", "Output array of str or unicode, depending on input type", "See also"]}, {"name": "char.lstrip()", "path": "reference/generated/numpy.char.lstrip", "type": "numpy.char.lstrip", "text": ["For each element in a, return a copy with the leading characters removed.", "Calls str.lstrip element-wise.", "Input array.", "The chars argument is a string specifying the set of characters to be removed. If omitted or None, the chars argument defaults to removing whitespace. The chars argument is not a prefix; rather, all combinations of its values are stripped.", "Output array of str or unicode, depending on input type", "See also", "The \u2018a\u2019 variable is unstripped from c[1] because whitespace leading."]}, {"name": "char.mod()", "path": "reference/generated/numpy.char.mod", "type": "numpy.char.mod", "text": ["Return (a % i), that is pre-Python 2.6 string formatting (interpolation), element-wise for a pair of array_likes of str or unicode.", "These values will be element-wise interpolated into the string.", "Output array of str or unicode, depending on input types", "See also"]}, {"name": "char.multiply()", "path": "reference/generated/numpy.char.multiply", "type": "numpy.char.multiply", "text": ["Return (a * i), that is string multiple concatenation, element-wise.", "Values in i of less than 0 are treated as 0 (which yields an empty string).", "Output array of str or unicode, depending on input types"]}, {"name": "char.not_equal()", "path": "reference/generated/numpy.char.not_equal", "type": "numpy.char.not_equal", "text": ["Return (x1 != x2) element-wise.", "Unlike numpy.not_equal, this comparison is performed by first stripping whitespace characters from the end of the string. This behavior is provided for backward-compatibility with numarray.", "Input arrays of the same shape.", "Output array of bools.", "See also"]}, {"name": "char.partition()", "path": "reference/generated/numpy.char.partition", "type": "numpy.char.partition", "text": ["Partition each element in a around sep.", "Calls str.partition element-wise.", "For each element in a, split the element as the first occurrence of sep, and return 3 strings containing the part before the separator, the separator itself, and the part after the separator. If the separator is not found, return 3 strings containing the string itself, followed by two empty strings.", "Input array", "Separator to split each string element in a.", "Output array of str or unicode, depending on input type. The output array will have an extra dimension with 3 elements per input element.", "See also"]}, {"name": "char.replace()", "path": "reference/generated/numpy.char.replace", "type": "numpy.char.replace", "text": ["For each element in a, return a copy of the string with all occurrences of substring old replaced by new.", "Calls str.replace element-wise.", "If the optional argument count is given, only the first count occurrences are replaced.", "Output array of str or unicode, depending on input type", "See also"]}, {"name": "char.rfind()", "path": "reference/generated/numpy.char.rfind", "type": "numpy.char.rfind", "text": ["For each element in a, return the highest index in the string where substring sub is found, such that sub is contained within [start, end].", "Calls str.rfind element-wise.", "Optional arguments start and end are interpreted as in slice notation.", "Output array of ints. Return -1 on failure.", "See also"]}, {"name": "char.rindex()", "path": "reference/generated/numpy.char.rindex", "type": "numpy.char.rindex", "text": ["Like rfind, but raises ValueError when the substring sub is not found.", "Calls str.rindex element-wise.", "Output array of ints.", "See also"]}, {"name": "char.rjust()", "path": "reference/generated/numpy.char.rjust", "type": "numpy.char.rjust", "text": ["Return an array with the elements of a right-justified in a string of length width.", "Calls str.rjust element-wise.", "The length of the resulting strings", "The character to use for padding", "Output array of str or unicode, depending on input type", "See also"]}, {"name": "char.rpartition()", "path": "reference/generated/numpy.char.rpartition", "type": "numpy.char.rpartition", "text": ["Partition (split) each element around the right-most separator.", "Calls str.rpartition element-wise.", "For each element in a, split the element as the last occurrence of sep, and return 3 strings containing the part before the separator, the separator itself, and the part after the separator. If the separator is not found, return 3 strings containing the string itself, followed by two empty strings.", "Input array", "Right-most separator to split each element in array.", "Output array of string or unicode, depending on input type. The output array will have an extra dimension with 3 elements per input element.", "See also"]}, {"name": "char.rsplit()", "path": "reference/generated/numpy.char.rsplit", "type": "numpy.char.rsplit", "text": ["For each element in a, return a list of the words in the string, using sep as the delimiter string.", "Calls str.rsplit element-wise.", "Except for splitting from the right, rsplit behaves like split.", "If sep is not specified or None, any whitespace string is a separator.", "If maxsplit is given, at most maxsplit splits are done, the rightmost ones.", "Array of list objects", "See also"]}, {"name": "char.rstrip()", "path": "reference/generated/numpy.char.rstrip", "type": "numpy.char.rstrip", "text": ["For each element in a, return a copy with the trailing characters removed.", "Calls str.rstrip element-wise.", "The chars argument is a string specifying the set of characters to be removed. If omitted or None, the chars argument defaults to removing whitespace. The chars argument is not a suffix; rather, all combinations of its values are stripped.", "Output array of str or unicode, depending on input type", "See also"]}, {"name": "char.split()", "path": "reference/generated/numpy.char.split", "type": "numpy.char.split", "text": ["For each element in a, return a list of the words in the string, using sep as the delimiter string.", "Calls str.split element-wise.", "If sep is not specified or None, any whitespace string is a separator.", "If maxsplit is given, at most maxsplit splits are done.", "Array of list objects", "See also"]}, {"name": "char.splitlines()", "path": "reference/generated/numpy.char.splitlines", "type": "numpy.char.splitlines", "text": ["For each element in a, return a list of the lines in the element, breaking at line boundaries.", "Calls str.splitlines element-wise.", "Line breaks are not included in the resulting list unless keepends is given and true.", "Array of list objects", "See also"]}, {"name": "char.startswith()", "path": "reference/generated/numpy.char.startswith", "type": "numpy.char.startswith", "text": ["Returns a boolean array which is True where the string element in a starts with prefix, otherwise False.", "Calls str.startswith element-wise.", "With optional start, test beginning at that position. With optional end, stop comparing at that position.", "Array of booleans", "See also"]}, {"name": "char.str_len()", "path": "reference/generated/numpy.char.str_len", "type": "numpy.char.str_len", "text": ["Return len(a) element-wise.", "Output array of integers", "See also"]}, {"name": "char.strip()", "path": "reference/generated/numpy.char.strip", "type": "numpy.char.strip", "text": ["For each element in a, return a copy with the leading and trailing characters removed.", "Calls str.strip element-wise.", "The chars argument is a string specifying the set of characters to be removed. If omitted or None, the chars argument defaults to removing whitespace. The chars argument is not a prefix or suffix; rather, all combinations of its values are stripped.", "Output array of str or unicode, depending on input type", "See also"]}, {"name": "char.swapcase()", "path": "reference/generated/numpy.char.swapcase", "type": "numpy.char.swapcase", "text": ["Return element-wise a copy of the string with uppercase characters converted to lowercase and vice versa.", "Calls str.swapcase element-wise.", "For 8-bit strings, this method is locale-dependent.", "Input array.", "Output array of str or unicode, depending on input type", "See also"]}, {"name": "char.title()", "path": "reference/generated/numpy.char.title", "type": "numpy.char.title", "text": ["Return element-wise title cased version of string or unicode.", "Title case words start with uppercase characters, all remaining cased characters are lowercase.", "Calls str.title element-wise.", "For 8-bit strings, this method is locale-dependent.", "Input array.", "Output array of str or unicode, depending on input type", "See also"]}, {"name": "char.translate()", "path": "reference/generated/numpy.char.translate", "type": "numpy.char.translate", "text": ["For each element in a, return a copy of the string where all characters occurring in the optional argument deletechars are removed, and the remaining characters have been mapped through the given translation table.", "Calls str.translate element-wise.", "Output array of str or unicode, depending on input type", "See also"]}, {"name": "char.upper()", "path": "reference/generated/numpy.char.upper", "type": "numpy.char.upper", "text": ["Return an array with the elements converted to uppercase.", "Calls str.upper element-wise.", "For 8-bit strings, this method is locale-dependent.", "Input array.", "Output array of str or unicode, depending on input type", "See also"]}, {"name": "char.zfill()", "path": "reference/generated/numpy.char.zfill", "type": "numpy.char.zfill", "text": ["Return the numeric string left-filled with zeros", "Calls str.zfill element-wise.", "Input array.", "Width of string to left-fill elements in a.", "Output array of str or unicode, depending on input type", "See also"]}, {"name": "chararray.argsort()", "path": "reference/generated/numpy.chararray.argsort", "type": "numpy.chararray.argsort", "text": ["method", "Returns the indices that would sort this array.", "Refer to numpy.argsort for full documentation.", "See also", "equivalent function"]}, {"name": "chararray.astype()", "path": "reference/generated/numpy.chararray.astype", "type": "numpy.chararray.astype", "text": ["method", "Copy of the array, cast to a specified type.", "Typecode or data-type to which the array is cast.", "Controls the memory layout order of the result. \u2018C\u2019 means C order, \u2018F\u2019 means Fortran order, \u2018A\u2019 means \u2018F\u2019 order if all the arrays are Fortran contiguous, \u2018C\u2019 order otherwise, and \u2018K\u2019 means as close to the order the array elements appear in memory as possible. Default is \u2018K\u2019.", "Controls what kind of data casting may occur. Defaults to \u2018unsafe\u2019 for backwards compatibility.", "If True, then sub-classes will be passed-through (default), otherwise the returned array will be forced to be a base-class array.", "By default, astype always returns a newly allocated array. If this is set to false, and the dtype, order, and subok requirements are satisfied, the input array is returned instead of a copy.", "Unless copy is False and the other conditions for returning the input array are satisfied (see description for copy input parameter), arr_t is a new array of the same shape as the input array, with dtype, order given by dtype, order.", "When casting from complex to float or int. To avoid this, one should use a.real.astype(t).", "Changed in version 1.17.0: Casting between a simple data type and a structured one is possible only for \u201cunsafe\u201d casting. Casting to multiple fields is allowed, but casting from multiple fields is not.", "Changed in version 1.9.0: Casting from numeric to string types in \u2018safe\u2019 casting mode requires that the string dtype length is long enough to store the max integer/float value converted."]}, {"name": "chararray.base", "path": "reference/generated/numpy.chararray.base", "type": "String operations", "text": ["attribute", "Base object if memory is from some other object.", "The base of an array that owns its memory is None:", "Slicing creates a view, whose memory is shared with x:"]}, {"name": "chararray.copy()", "path": "reference/generated/numpy.chararray.copy", "type": "numpy.chararray.copy", "text": ["method", "Return a copy of the array.", "Controls the memory layout of the copy. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of a as closely as possible. (Note that this function and numpy.copy are very similar but have different default values for their order= arguments, and this function always passes sub-classes through.)", "See also", "Similar function with different default behavior", "This function is the preferred method for creating an array copy. The function numpy.copy is similar, but it defaults to using order \u2018K\u2019, and will not pass sub-classes through by default."]}, {"name": "chararray.count()", "path": "reference/generated/numpy.chararray.count", "type": "numpy.chararray.count", "text": ["method", "Returns an array with the number of non-overlapping occurrences of substring sub in the range [start, end].", "See also"]}, {"name": "chararray.ctypes", "path": "reference/generated/numpy.chararray.ctypes", "type": "String operations", "text": ["attribute", "An object to simplify the interaction of the array with the ctypes module.", "This attribute creates an object that makes it easier to use arrays when calling shared libraries with the ctypes module. The returned object has, among others, data, shape, and strides attributes (see Notes below) which themselves return ctypes objects that can be used as arguments to a shared library.", "Possessing attributes data, shape, strides, etc.", "See also", "Below are the public attributes of this object which were documented in \u201cGuide to NumPy\u201d (we have omitted undocumented public attributes, as well as documented private attributes):", "A pointer to the memory area of the array as a Python integer. This memory area may contain data that is not aligned, or not in correct byte-order. The memory area may not even be writeable. The array flags and data-type of this array should be respected when passing this attribute to arbitrary C-code to avoid trouble that can include Python crashing. User Beware! The value of this attribute is exactly the same as self._array_interface_['data'][0].", "Note that unlike data_as, a reference will not be kept to the array: code like ctypes.c_void_p((a + b).ctypes.data) will result in a pointer to a deallocated array, and should be spelt (a + b).ctypes.data_as(ctypes.c_void_p)", "(c_intp*self.ndim): A ctypes array of length self.ndim where the basetype is the C-integer corresponding to dtype('p') on this platform (see c_intp). This base-type could be ctypes.c_int, ctypes.c_long, or ctypes.c_longlong depending on the platform. The ctypes array contains the shape of the underlying array.", "(c_intp*self.ndim): A ctypes array of length self.ndim where the basetype is the same as for the shape attribute. This ctypes array contains the strides information from the underlying array. This strides information is important for showing how many bytes must be jumped to get to the next element in the array.", "Return the data pointer cast to a particular c-types object. For example, calling self._as_parameter_ is equivalent to self.data_as(ctypes.c_void_p). Perhaps you want to use the data as a pointer to a ctypes array of floating-point data: self.data_as(ctypes.POINTER(ctypes.c_double)).", "The returned pointer will keep a reference to the array.", "Return the shape tuple as an array of some other c-types type. For example: self.shape_as(ctypes.c_short).", "Return the strides tuple as an array of some other c-types type. For example: self.strides_as(ctypes.c_longlong).", "If the ctypes module is not available, then the ctypes attribute of array objects still returns something useful, but ctypes objects are not returned and errors may be raised instead. In particular, the object will still have the as_parameter attribute which will return an integer equal to the data attribute."]}, {"name": "chararray.data", "path": "reference/generated/numpy.chararray.data", "type": "String operations", "text": ["attribute", "Python buffer object pointing to the start of the array\u2019s data."]}, {"name": "chararray.decode()", "path": "reference/generated/numpy.chararray.decode", "type": "numpy.chararray.decode", "text": ["method", "Calls str.decode element-wise.", "See also"]}, {"name": "chararray.dump()", "path": "reference/generated/numpy.chararray.dump", "type": "numpy.chararray.dump", "text": ["method", "Dump a pickle of the array to the specified file. The array can be read back with pickle.load or numpy.load.", "A string naming the dump file.", "Changed in version 1.17.0: pathlib.Path objects are now accepted."]}, {"name": "chararray.dumps()", "path": "reference/generated/numpy.chararray.dumps", "type": "numpy.chararray.dumps", "text": ["method", "Returns the pickle of the array as a string. pickle.loads will convert the string back to an array."]}, {"name": "chararray.encode()", "path": "reference/generated/numpy.chararray.encode", "type": "numpy.chararray.encode", "text": ["method", "Calls str.encode element-wise.", "See also"]}, {"name": "chararray.endswith()", "path": "reference/generated/numpy.chararray.endswith", "type": "numpy.chararray.endswith", "text": ["method", "Returns a boolean array which is True where the string element in self ends with suffix, otherwise False.", "See also"]}, {"name": "chararray.expandtabs()", "path": "reference/generated/numpy.chararray.expandtabs", "type": "numpy.chararray.expandtabs", "text": ["method", "Return a copy of each string element where all tab characters are replaced by one or more spaces.", "See also"]}, {"name": "chararray.fill()", "path": "reference/generated/numpy.chararray.fill", "type": "numpy.chararray.fill", "text": ["method", "Fill the array with a scalar value.", "All elements of a will be assigned this value."]}, {"name": "chararray.find()", "path": "reference/generated/numpy.chararray.find", "type": "numpy.chararray.find", "text": ["method", "For each element, return the lowest index in the string where substring sub is found.", "See also"]}, {"name": "chararray.flags", "path": "reference/generated/numpy.chararray.flags", "type": "String operations", "text": ["attribute", "Information about the memory layout of the array.", "The flags object can be accessed dictionary-like (as in a.flags['WRITEABLE']), or by using lowercased attribute names (as in a.flags.writeable). Short flag names are only supported in dictionary access.", "Only the WRITEBACKIFCOPY, UPDATEIFCOPY, WRITEABLE, and ALIGNED flags can be changed by the user, via direct assignment to the attribute or dictionary entry, or by calling ndarray.setflags.", "The array flags cannot be set arbitrarily:", "Arrays can be both C-style and Fortran-style contiguous simultaneously. This is clear for 1-dimensional arrays, but can also be true for higher dimensional arrays.", "Even for contiguous arrays a stride for a given dimension arr.strides[dim] may be arbitrary if arr.shape[dim] == 1 or the array has no elements. It does not generally hold that self.strides[-1] == self.itemsize for C-style contiguous arrays or self.strides[0] == self.itemsize for Fortran-style contiguous arrays is true.", "The data is in a single, C-style contiguous segment.", "The data is in a single, Fortran-style contiguous segment.", "The array owns the memory it uses or borrows it from another object.", "The data area can be written to. Setting this to False locks the data, making it read-only. A view (slice, etc.) inherits WRITEABLE from its base array at creation time, but a view of a writeable array may be subsequently locked while the base array remains writeable. (The opposite is not true, in that a view of a locked array may not be made writeable. However, currently, locking a base object does not lock any views that already reference it, so under that circumstance it is possible to alter the contents of a locked array via a previously created writeable view onto it.) Attempting to change a non-writeable array raises a RuntimeError exception.", "The data and all elements are aligned appropriately for the hardware.", "This array is a copy of some other array. The C-API function PyArray_ResolveWritebackIfCopy must be called before deallocating to the base array will be updated with the contents of this array.", "(Deprecated, use WRITEBACKIFCOPY) This array is a copy of some other array. When this array is deallocated, the base array will be updated with the contents of this array.", "F_CONTIGUOUS and not C_CONTIGUOUS.", "F_CONTIGUOUS or C_CONTIGUOUS (one-segment test).", "ALIGNED and WRITEABLE.", "BEHAVED and C_CONTIGUOUS.", "BEHAVED and F_CONTIGUOUS and not C_CONTIGUOUS."]}, {"name": "chararray.flat", "path": "reference/generated/numpy.chararray.flat", "type": "String operations", "text": ["attribute", "A 1-D iterator over the array.", "This is a numpy.flatiter instance, which acts similarly to, but is not a subclass of, Python\u2019s built-in iterator object.", "See also", "Return a copy of the array collapsed into one dimension.", "An assignment example:"]}, {"name": "chararray.flatten()", "path": "reference/generated/numpy.chararray.flatten", "type": "numpy.chararray.flatten", "text": ["method", "Return a copy of the array collapsed into one dimension.", "\u2018C\u2019 means to flatten in row-major (C-style) order. \u2018F\u2019 means to flatten in column-major (Fortran- style) order. \u2018A\u2019 means to flatten in column-major order if a is Fortran contiguous in memory, row-major order otherwise. \u2018K\u2019 means to flatten a in the order the elements occur in memory. The default is \u2018C\u2019.", "A copy of the input array, flattened to one dimension.", "See also", "Return a flattened array.", "A 1-D flat iterator over the array."]}, {"name": "chararray.getfield()", "path": "reference/generated/numpy.chararray.getfield", "type": "numpy.chararray.getfield", "text": ["method", "Returns a field of the given array as a certain type.", "A field is a view of the array data with a given data-type. The values in the view are determined by the given type and the offset into the current array in bytes. The offset needs to be such that the view dtype fits in the array dtype; for example an array of dtype complex128 has 16-byte elements. If taking a view with a 32-bit integer (4 bytes), the offset needs to be between 0 and 12 bytes.", "The data type of the view. The dtype size of the view can not be larger than that of the array itself.", "Number of bytes to skip before beginning the element view.", "By choosing an offset of 8 bytes we can select the complex part of the array for our view:"]}, {"name": "chararray.index()", "path": "reference/generated/numpy.chararray.index", "type": "numpy.chararray.index", "text": ["method", "Like find, but raises ValueError when the substring is not found.", "See also"]}, {"name": "chararray.isalnum()", "path": "reference/generated/numpy.chararray.isalnum", "type": "numpy.chararray.isalnum", "text": ["method", "Returns true for each element if all characters in the string are alphanumeric and there is at least one character, false otherwise.", "See also"]}, {"name": "chararray.isalpha()", "path": "reference/generated/numpy.chararray.isalpha", "type": "numpy.chararray.isalpha", "text": ["method", "Returns true for each element if all characters in the string are alphabetic and there is at least one character, false otherwise.", "See also"]}, {"name": "chararray.isdecimal()", "path": "reference/generated/numpy.chararray.isdecimal", "type": "numpy.chararray.isdecimal", "text": ["method", "For each element in self, return True if there are only decimal characters in the element.", "See also"]}, {"name": "chararray.isdigit()", "path": "reference/generated/numpy.chararray.isdigit", "type": "numpy.chararray.isdigit", "text": ["method", "Returns true for each element if all characters in the string are digits and there is at least one character, false otherwise.", "See also"]}, {"name": "chararray.islower()", "path": "reference/generated/numpy.chararray.islower", "type": "numpy.chararray.islower", "text": ["method", "Returns true for each element if all cased characters in the string are lowercase and there is at least one cased character, false otherwise.", "See also"]}, {"name": "chararray.isnumeric()", "path": "reference/generated/numpy.chararray.isnumeric", "type": "numpy.chararray.isnumeric", "text": ["method", "For each element in self, return True if there are only numeric characters in the element.", "See also"]}, {"name": "chararray.isspace()", "path": "reference/generated/numpy.chararray.isspace", "type": "numpy.chararray.isspace", "text": ["method", "Returns true for each element if there are only whitespace characters in the string and there is at least one character, false otherwise.", "See also"]}, {"name": "chararray.istitle()", "path": "reference/generated/numpy.chararray.istitle", "type": "numpy.chararray.istitle", "text": ["method", "Returns true for each element if the element is a titlecased string and there is at least one character, false otherwise.", "See also"]}, {"name": "chararray.isupper()", "path": "reference/generated/numpy.chararray.isupper", "type": "numpy.chararray.isupper", "text": ["method", "Returns true for each element if all cased characters in the string are uppercase and there is at least one character, false otherwise.", "See also"]}, {"name": "chararray.item()", "path": "reference/generated/numpy.chararray.item", "type": "numpy.chararray.item", "text": ["method", "Copy an element of an array to a standard Python scalar and return it.", "A copy of the specified element of the array as a suitable Python scalar", "When the data type of a is longdouble or clongdouble, item() returns a scalar array object because there is no available Python scalar that would not lose information. Void arrays return a buffer object for item(), unless fields are defined, in which case a tuple is returned.", "item is very similar to a[args], except, instead of an array scalar, a standard Python scalar is returned. This can be useful for speeding up access to elements of the array and doing arithmetic on elements of the array using Python\u2019s optimized math."]}, {"name": "chararray.itemsize", "path": "reference/generated/numpy.chararray.itemsize", "type": "String operations", "text": ["attribute", "Length of one array element in bytes."]}, {"name": "chararray.join()", "path": "reference/generated/numpy.chararray.join", "type": "numpy.chararray.join", "text": ["method", "Return a string which is the concatenation of the strings in the sequence seq.", "See also"]}, {"name": "chararray.ljust()", "path": "reference/generated/numpy.chararray.ljust", "type": "numpy.chararray.ljust", "text": ["method", "Return an array with the elements of self left-justified in a string of length width.", "See also"]}, {"name": "chararray.lower()", "path": "reference/generated/numpy.chararray.lower", "type": "numpy.chararray.lower", "text": ["method", "Return an array with the elements of self converted to lowercase.", "See also"]}, {"name": "chararray.lstrip()", "path": "reference/generated/numpy.chararray.lstrip", "type": "numpy.chararray.lstrip", "text": ["method", "For each element in self, return a copy with the leading characters removed.", "See also"]}, {"name": "chararray.nbytes", "path": "reference/generated/numpy.chararray.nbytes", "type": "String operations", "text": ["attribute", "Total bytes consumed by the elements of the array.", "Does not include memory consumed by non-element attributes of the array object."]}, {"name": "chararray.ndim", "path": "reference/generated/numpy.chararray.ndim", "type": "String operations", "text": ["attribute", "Number of array dimensions."]}, {"name": "chararray.nonzero()", "path": "reference/generated/numpy.chararray.nonzero", "type": "numpy.chararray.nonzero", "text": ["method", "Return the indices of the elements that are non-zero.", "Refer to numpy.nonzero for full documentation.", "See also", "equivalent function"]}, {"name": "chararray.put()", "path": "reference/generated/numpy.chararray.put", "type": "numpy.chararray.put", "text": ["method", "Set a.flat[n] = values[n] for all n in indices.", "Refer to numpy.put for full documentation.", "See also", "equivalent function"]}, {"name": "chararray.ravel()", "path": "reference/generated/numpy.chararray.ravel", "type": "numpy.chararray.ravel", "text": ["method", "Return a flattened array.", "Refer to numpy.ravel for full documentation.", "See also", "equivalent function", "a flat iterator on the array."]}, {"name": "chararray.repeat()", "path": "reference/generated/numpy.chararray.repeat", "type": "numpy.chararray.repeat", "text": ["method", "Repeat elements of an array.", "Refer to numpy.repeat for full documentation.", "See also", "equivalent function"]}, {"name": "chararray.replace()", "path": "reference/generated/numpy.chararray.replace", "type": "numpy.chararray.replace", "text": ["method", "For each element in self, return a copy of the string with all occurrences of substring old replaced by new.", "See also"]}, {"name": "chararray.reshape()", "path": "reference/generated/numpy.chararray.reshape", "type": "numpy.chararray.reshape", "text": ["method", "Returns an array containing the same data with a new shape.", "Refer to numpy.reshape for full documentation.", "See also", "equivalent function", "Unlike the free function numpy.reshape, this method on ndarray allows the elements of the shape parameter to be passed in as separate arguments. For example, a.reshape(10, 11) is equivalent to a.reshape((10, 11))."]}, {"name": "chararray.resize()", "path": "reference/generated/numpy.chararray.resize", "type": "numpy.chararray.resize", "text": ["method", "Change shape and size of array in-place.", "Shape of resized array.", "If False, reference count will not be checked. Default is True.", "If a does not own its own data or references or views to it exist, and the data memory must be changed. PyPy only: will always raise if the data memory must be changed, since there is no reliable way to determine if references or views to it exist.", "If the order keyword argument is specified. This behaviour is a bug in NumPy.", "See also", "Return a new array with the specified shape.", "This reallocates space for the data area if necessary.", "Only contiguous arrays (data elements consecutive in memory) can be resized.", "The purpose of the reference count check is to make sure you do not use this array as a buffer for another Python object and then reallocate the memory. However, reference counts can increase in other ways so if you are sure that you have not shared the memory for this array with another Python object, then you may safely set refcheck to False.", "Shrinking an array: array is flattened (in the order that the data are stored in memory), resized, and reshaped:", "Enlarging an array: as above, but missing entries are filled with zeros:", "Referencing an array prevents resizing\u2026", "Unless refcheck is False:"]}, {"name": "chararray.rfind()", "path": "reference/generated/numpy.chararray.rfind", "type": "numpy.chararray.rfind", "text": ["method", "For each element in self, return the highest index in the string where substring sub is found, such that sub is contained within [start, end].", "See also"]}, {"name": "chararray.rindex()", "path": "reference/generated/numpy.chararray.rindex", "type": "numpy.chararray.rindex", "text": ["method", "Like rfind, but raises ValueError when the substring sub is not found.", "See also"]}, {"name": "chararray.rjust()", "path": "reference/generated/numpy.chararray.rjust", "type": "numpy.chararray.rjust", "text": ["method", "Return an array with the elements of self right-justified in a string of length width.", "See also"]}, {"name": "chararray.rsplit()", "path": "reference/generated/numpy.chararray.rsplit", "type": "numpy.chararray.rsplit", "text": ["method", "For each element in self, return a list of the words in the string, using sep as the delimiter string.", "See also"]}, {"name": "chararray.rstrip()", "path": "reference/generated/numpy.chararray.rstrip", "type": "numpy.chararray.rstrip", "text": ["method", "For each element in self, return a copy with the trailing characters removed.", "See also"]}, {"name": "chararray.searchsorted()", "path": "reference/generated/numpy.chararray.searchsorted", "type": "numpy.chararray.searchsorted", "text": ["method", "Find indices where elements of v should be inserted in a to maintain order.", "For full documentation, see numpy.searchsorted", "See also", "equivalent function"]}, {"name": "chararray.setfield()", "path": "reference/generated/numpy.chararray.setfield", "type": "numpy.chararray.setfield", "text": ["method", "Put a value into a specified place in a field defined by a data-type.", "Place val into a\u2019s field defined by dtype and beginning offset bytes into the field.", "Value to be placed in field.", "Data-type of the field in which to place val.", "The number of bytes into the field at which to place val.", "See also"]}, {"name": "chararray.setflags()", "path": "reference/generated/numpy.chararray.setflags", "type": "numpy.chararray.setflags", "text": ["method", "Set array flags WRITEABLE, ALIGNED, (WRITEBACKIFCOPY and UPDATEIFCOPY), respectively.", "These Boolean-valued flags affect how numpy interprets the memory area used by a (see Notes below). The ALIGNED flag can only be set to True if the data is actually aligned according to the type. The WRITEBACKIFCOPY and (deprecated) UPDATEIFCOPY flags can never be set to True. The flag WRITEABLE can only be set to True if the array owns its own memory, or the ultimate owner of the memory exposes a writeable buffer interface, or is a string. (The exception for string is made so that unpickling can be done without copying memory.)", "Describes whether or not a can be written to.", "Describes whether or not a is aligned properly for its type.", "Describes whether or not a is a copy of another \u201cbase\u201d array.", "Array flags provide information about how the memory area used for the array is to be interpreted. There are 7 Boolean flags in use, only four of which can be changed by the user: WRITEBACKIFCOPY, UPDATEIFCOPY, WRITEABLE, and ALIGNED.", "WRITEABLE (W) the data area can be written to;", "ALIGNED (A) the data and strides are aligned appropriately for the hardware (as determined by the compiler);", "UPDATEIFCOPY (U) (deprecated), replaced by WRITEBACKIFCOPY;", "WRITEBACKIFCOPY (X) this array is a copy of some other array (referenced by .base). When the C-API function PyArray_ResolveWritebackIfCopy is called, the base array will be updated with the contents of this array.", "All flags can be accessed using the single (upper case) letter as well as the full name."]}, {"name": "chararray.size", "path": "reference/generated/numpy.chararray.size", "type": "String operations", "text": ["attribute", "Number of elements in the array.", "Equal to np.prod(a.shape), i.e., the product of the array\u2019s dimensions.", "a.size returns a standard arbitrary precision Python integer. This may not be the case with other methods of obtaining the same value (like the suggested np.prod(a.shape), which returns an instance of np.int_), and may be relevant if the value is used further in calculations that may overflow a fixed size integer type."]}, {"name": "chararray.sort()", "path": "reference/generated/numpy.chararray.sort", "type": "numpy.chararray.sort", "text": ["method", "Sort an array in-place. Refer to numpy.sort for full documentation.", "Axis along which to sort. Default is -1, which means sort along the last axis.", "Sorting algorithm. The default is \u2018quicksort\u2019. Note that both \u2018stable\u2019 and \u2018mergesort\u2019 use timsort under the covers and, in general, the actual implementation will vary with datatype. The \u2018mergesort\u2019 option is retained for backwards compatibility.", "Changed in version 1.15.0: The \u2018stable\u2019 option was added.", "When a is an array with fields defined, this argument specifies which fields to compare first, second, etc. A single field can be specified as a string, and not all fields need be specified, but unspecified fields will still be used, in the order in which they come up in the dtype, to break ties.", "See also", "Return a sorted copy of an array.", "Indirect sort.", "Indirect stable sort on multiple keys.", "Find elements in sorted array.", "Partial sort.", "See numpy.sort for notes on the different sorting algorithms.", "Use the order keyword to specify a field to use when sorting a structured array:"]}, {"name": "chararray.split()", "path": "reference/generated/numpy.chararray.split", "type": "numpy.chararray.split", "text": ["method", "For each element in self, return a list of the words in the string, using sep as the delimiter string.", "See also"]}, {"name": "chararray.splitlines()", "path": "reference/generated/numpy.chararray.splitlines", "type": "numpy.chararray.splitlines", "text": ["method", "For each element in self, return a list of the lines in the element, breaking at line boundaries.", "See also"]}, {"name": "chararray.squeeze()", "path": "reference/generated/numpy.chararray.squeeze", "type": "numpy.chararray.squeeze", "text": ["method", "Remove axes of length one from a.", "Refer to numpy.squeeze for full documentation.", "See also", "equivalent function"]}, {"name": "chararray.startswith()", "path": "reference/generated/numpy.chararray.startswith", "type": "numpy.chararray.startswith", "text": ["method", "Returns a boolean array which is True where the string element in self starts with prefix, otherwise False.", "See also"]}, {"name": "chararray.strides", "path": "reference/generated/numpy.chararray.strides", "type": "String operations", "text": ["attribute", "Tuple of bytes to step in each dimension when traversing an array.", "The byte offset of element (i[0], i[1], ..., i[n]) in an array a is:", "A more detailed explanation of strides can be found in the \u201cndarray.rst\u201d file in the NumPy reference guide.", "See also", "Imagine an array of 32-bit integers (each 4 bytes):", "This array is stored in memory as 40 bytes, one after the other (known as a contiguous block of memory). The strides of an array tell us how many bytes we have to skip in memory to move to the next position along a certain axis. For example, we have to skip 4 bytes (1 value) to move to the next column, but 20 bytes (5 values) to get to the same position in the next row. As such, the strides for the array x will be (20, 4)."]}, {"name": "chararray.strip()", "path": "reference/generated/numpy.chararray.strip", "type": "numpy.chararray.strip", "text": ["method", "For each element in self, return a copy with the leading and trailing characters removed.", "See also"]}, {"name": "chararray.swapaxes()", "path": "reference/generated/numpy.chararray.swapaxes", "type": "numpy.chararray.swapaxes", "text": ["method", "Return a view of the array with axis1 and axis2 interchanged.", "Refer to numpy.swapaxes for full documentation.", "See also", "equivalent function"]}, {"name": "chararray.swapcase()", "path": "reference/generated/numpy.chararray.swapcase", "type": "numpy.chararray.swapcase", "text": ["method", "For each element in self, return a copy of the string with uppercase characters converted to lowercase and vice versa.", "See also"]}, {"name": "chararray.T", "path": "reference/generated/numpy.chararray.t", "type": "String operations", "text": ["attribute", "The transposed array.", "Same as self.transpose().", "See also"]}, {"name": "chararray.take()", "path": "reference/generated/numpy.chararray.take", "type": "numpy.chararray.take", "text": ["method", "Return an array formed from the elements of a at the given indices.", "Refer to numpy.take for full documentation.", "See also", "equivalent function"]}, {"name": "chararray.title()", "path": "reference/generated/numpy.chararray.title", "type": "numpy.chararray.title", "text": ["method", "For each element in self, return a titlecased version of the string: words start with uppercase characters, all remaining cased characters are lowercase.", "See also"]}, {"name": "chararray.tobytes()", "path": "reference/generated/numpy.chararray.tobytes", "type": "String operations", "text": ["method", "Construct Python bytes containing the raw data bytes in the array.", "Constructs Python bytes showing a copy of the raw contents of data memory. The bytes object is produced in C-order by default. This behavior is controlled by the order parameter.", "New in version 1.9.0.", "Controls the memory layout of the bytes object. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 (short for Any) means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. Default is \u2018C\u2019.", "Python bytes exhibiting a copy of a\u2019s raw data."]}, {"name": "chararray.tofile()", "path": "reference/generated/numpy.chararray.tofile", "type": "numpy.chararray.tofile", "text": ["method", "Write array to a file as text or binary (default).", "Data is always written in \u2018C\u2019 order, independent of the order of a. The data produced by this method can be recovered using the function fromfile().", "An open file object, or a string containing a filename.", "Changed in version 1.17.0: pathlib.Path objects are now accepted.", "Separator between array items for text output. If \u201c\u201d (empty), a binary file is written, equivalent to file.write(a.tobytes()).", "Format string for text file output. Each entry in the array is formatted to text by first converting it to the closest Python type, and then using \u201cformat\u201d % item.", "This is a convenience function for quick storage of array data. Information on endianness and precision is lost, so this method is not a good choice for files intended to archive data or transport data between machines with different endianness. Some of these problems can be overcome by outputting the data as text files, at the expense of speed and file size.", "When fid is a file object, array contents are directly written to the file, bypassing the file object\u2019s write method. As a result, tofile cannot be used with files objects supporting compression (e.g., GzipFile) or file-like objects that do not support fileno() (e.g., BytesIO)."]}, {"name": "chararray.tolist()", "path": "reference/generated/numpy.chararray.tolist", "type": "numpy.chararray.tolist", "text": ["method", "Return the array as an a.ndim-levels deep nested list of Python scalars.", "Return a copy of the array data as a (nested) Python list. Data items are converted to the nearest compatible builtin Python type, via the item function.", "If a.ndim is 0, then since the depth of the nested list is 0, it will not be a list at all, but a simple Python scalar.", "The possibly nested list of array elements.", "The array may be recreated via a = np.array(a.tolist()), although this may sometimes lose precision.", "For a 1D array, a.tolist() is almost the same as list(a), except that tolist changes numpy scalars to Python scalars:", "Additionally, for a 2D array, tolist applies recursively:", "The base case for this recursion is a 0D array:"]}, {"name": "chararray.tostring()", "path": "reference/generated/numpy.chararray.tostring", "type": "numpy.chararray.tostring", "text": ["method", "A compatibility alias for tobytes, with exactly the same behavior.", "Despite its name, it returns bytes not strs.", "Deprecated since version 1.19.0."]}, {"name": "chararray.translate()", "path": "reference/generated/numpy.chararray.translate", "type": "numpy.chararray.translate", "text": ["method", "For each element in self, return a copy of the string where all characters occurring in the optional argument deletechars are removed, and the remaining characters have been mapped through the given translation table.", "See also"]}, {"name": "chararray.transpose()", "path": "reference/generated/numpy.chararray.transpose", "type": "numpy.chararray.transpose", "text": ["method", "Returns a view of the array with axes transposed.", "For a 1-D array this has no effect, as a transposed vector is simply the same vector. To convert a 1-D array into a 2D column vector, an additional dimension must be added. np.atleast2d(a).T achieves this, as does a[:, np.newaxis]. For a 2-D array, this is a standard matrix transpose. For an n-D array, if axes are given, their order indicates how the axes are permuted (see Examples). If axes are not provided and a.shape = (i[0], i[1], ... i[n-2], i[n-1]), then a.transpose().shape = (i[n-1], i[n-2], ... i[1], i[0]).", "View of a, with axes suitably permuted.", "See also", "Equivalent function", "Array property returning the array transposed.", "Give a new shape to an array without changing its data."]}, {"name": "chararray.upper()", "path": "reference/generated/numpy.chararray.upper", "type": "numpy.chararray.upper", "text": ["method", "Return an array with the elements of self converted to uppercase.", "See also"]}, {"name": "chararray.view()", "path": "reference/generated/numpy.chararray.view", "type": "numpy.chararray.view", "text": ["method", "New view of array with the same data.", "Note", "Passing None for dtype is different from omitting the parameter, since the former invokes dtype(None) which is an alias for dtype('float_').", "Data-type descriptor of the returned view, e.g., float32 or int16. Omitting it results in the view having the same data-type as a. This argument can also be specified as an ndarray sub-class, which then specifies the type of the returned object (this is equivalent to setting the type parameter).", "Type of the returned view, e.g., ndarray or matrix. Again, omission of the parameter results in type preservation.", "a.view() is used two different ways:", "a.view(some_dtype) or a.view(dtype=some_dtype) constructs a view of the array\u2019s memory with a different data-type. This can cause a reinterpretation of the bytes of memory.", "a.view(ndarray_subclass) or a.view(type=ndarray_subclass) just returns an instance of ndarray_subclass that looks at the same array (same shape, dtype, etc.) This does not cause a reinterpretation of the memory.", "For a.view(some_dtype), if some_dtype has a different number of bytes per entry than the previous dtype (for example, converting a regular array to a structured array), then the behavior of the view cannot be predicted just from the superficial appearance of a (shown by print(a)). It also depends on exactly how a is stored in memory. Therefore if a is C-ordered versus fortran-ordered, versus defined as a slice or transpose, etc., the view may give different results.", "Viewing array data using a different type and dtype:", "Creating a view on a structured array so it can be used in calculations", "Making changes to the view changes the underlying array", "Using a view to convert an array to a recarray:", "Views share data:", "Views that change the dtype size (bytes per entry) should normally be avoided on arrays defined by slices, transposes, fortran-ordering, etc.:"]}, {"name": "chararray.zfill()", "path": "reference/generated/numpy.chararray.zfill", "type": "numpy.chararray.zfill", "text": ["method", "Return the numeric string left-filled with zeros in a string of length width.", "See also"]}, {"name": "class.__array__()", "path": "reference/arrays.classes#numpy.class.__array__", "type": "Standard array subclasses", "text": ["If a class (ndarray subclass or not) having the __array__ method is used as the output object of an ufunc, results will not be written to the object returned by __array__. This practice will return TypeError."]}, {"name": "class.__array_finalize__()", "path": "reference/arrays.classes#numpy.class.__array_finalize__", "type": "Standard array subclasses", "text": ["This method is called whenever the system internally allocates a new array from obj, where obj is a subclass (subtype) of the ndarray. It can be used to change attributes of self after construction (so as to ensure a 2-d matrix for example), or to update meta-information from the \u201cparent.\u201d Subclasses inherit a default implementation of this method that does nothing."]}, {"name": "class.__array_function__()", "path": "reference/arrays.classes#numpy.class.__array_function__", "type": "Standard array subclasses", "text": ["New in version 1.16.", "Note", "As a convenience for __array_function__ implementors, types provides all argument types with an '__array_function__' attribute. This allows implementors to quickly identify cases where they should defer to __array_function__ implementations on other arguments. Implementations should not rely on the iteration order of types.", "Most implementations of __array_function__ will start with two checks:", "If these conditions hold, __array_function__ should return the result from calling its implementation for func(*args, **kwargs). Otherwise, it should return the sentinel value NotImplemented, indicating that the function is not implemented by these types.", "There are no general requirements on the return value from __array_function__, although most sensible implementations should probably return array(s) with the same type as one of the function\u2019s arguments.", "It may also be convenient to define a custom decorators (implements below) for registering __array_function__ implementations.", "Note that it is not required for __array_function__ implementations to include all of the corresponding NumPy function\u2019s optional arguments (e.g., broadcast_to above omits the irrelevant subok argument). Optional arguments are only passed in to __array_function__ if they were explicitly used in the NumPy function call.", "Just like the case for builtin special methods like __add__, properly written __array_function__ methods should always return NotImplemented when an unknown type is encountered. Otherwise, it will be impossible to correctly override NumPy functions from another object if the operation also includes one of your objects.", "For the most part, the rules for dispatch with __array_function__ match those for __array_ufunc__. In particular:", "If no __array_function__ methods exists, NumPy will default to calling its own implementation, intended for use on NumPy arrays. This case arises, for example, when all array-like arguments are Python numbers or lists. (NumPy arrays do have a __array_function__ method, given below, but it always returns NotImplemented if any argument other than a NumPy array subclass implements __array_function__.)", "One deviation from the current behavior of __array_ufunc__ is that NumPy will only call __array_function__ on the first argument of each unique type. This matches Python\u2019s rule for calling reflected methods, and this ensures that checking overloads has acceptable performance even when there are a large number of overloaded arguments."]}, {"name": "class.__array_prepare__()", "path": "reference/arrays.classes#numpy.class.__array_prepare__", "type": "Standard array subclasses", "text": ["At the beginning of every ufunc, this method is called on the input object with the highest array priority, or the output object if one was specified. The output array is passed in and whatever is returned is passed to the ufunc. Subclasses inherit a default implementation of this method which simply returns the output array unmodified. Subclasses may opt to use this method to transform the output array into an instance of the subclass and update metadata before returning the array to the ufunc for computation.", "Note", "For ufuncs, it is hoped to eventually deprecate this method in favour of __array_ufunc__."]}, {"name": "class.__array_priority__", "path": "reference/arrays.classes#numpy.class.__array_priority__", "type": "Standard array subclasses", "text": ["The value of this attribute is used to determine what type of object to return in situations where there is more than one possibility for the Python type of the returned object. Subclasses inherit a default value of 0.0 for this attribute.", "Note", "For ufuncs, it is hoped to eventually deprecate this method in favour of __array_ufunc__."]}, {"name": "class.__array_ufunc__()", "path": "reference/arrays.classes", "type": "Standard array subclasses", "text": ["Note", "Subclassing a numpy.ndarray is possible but if your goal is to create an array with modified behavior, as do dask arrays for distributed computation and cupy arrays for GPU-based computation, subclassing is discouraged. Instead, using numpy\u2019s dispatch mechanism is recommended.", "The ndarray can be inherited from (in Python or in C) if desired. Therefore, it can form a foundation for many useful classes. Often whether to sub-class the array object or to simply use the core array component as an internal part of a new class is a difficult decision, and can be simply a matter of choice. NumPy has several tools for simplifying how your new object interacts with other array objects, and so the choice may not be significant in the end. One way to simplify the question is by asking yourself if the object you are interested in can be replaced as a single array or does it really require two or more arrays at its core.", "Note that asarray always returns the base-class ndarray. If you are confident that your use of the array object can handle any subclass of an ndarray, then asanyarray can be used to allow subclasses to propagate more cleanly through your subroutine. In principal a subclass could redefine any aspect of the array and therefore, under strict guidelines, asanyarray would rarely be useful. However, most subclasses of the array object will not redefine certain aspects of the array object such as the buffer interface, or the attributes of the array. One important example, however, of why your subroutine may not be able to handle an arbitrary subclass of an array is that matrices redefine the \u201c*\u201d operator to be matrix-multiplication, rather than element-by-element multiplication.", "See also", "Subclassing ndarray", "NumPy provides several hooks that classes can customize:", "New in version 1.13.", "Any class, ndarray subclass or not, can define this method or set it to None in order to override the behavior of NumPy\u2019s ufuncs. This works quite similarly to Python\u2019s __mul__ and other binary operation routines.", "The method should return either the result of the operation, or NotImplemented if the operation requested is not implemented.", "If one of the input or output arguments has a __array_ufunc__ method, it is executed instead of the ufunc. If more than one of the arguments implements __array_ufunc__, they are tried in the order: subclasses before superclasses, inputs before outputs, otherwise left to right. The first routine returning something other than NotImplemented determines the result. If all of the __array_ufunc__ operations return NotImplemented, a TypeError is raised.", "Note", "We intend to re-implement numpy functions as (generalized) Ufunc, in which case it will become possible for them to be overridden by the __array_ufunc__ method. A prime candidate is matmul, which currently is not a Ufunc, but could be relatively easily be rewritten as a (set of) generalized Ufuncs. The same may happen with functions such as median, amin, and argsort.", "Like with some other special methods in python, such as __hash__ and __iter__, it is possible to indicate that your class does not support ufuncs by setting __array_ufunc__ = None. Ufuncs always raise TypeError when called on an object that sets __array_ufunc__ = None.", "The presence of __array_ufunc__ also influences how ndarray handles binary operations like arr + obj and arr\n< obj when arr is an ndarray and obj is an instance of a custom class. There are two possibilities. If obj.__array_ufunc__ is present and not None, then ndarray.__add__ and friends will delegate to the ufunc machinery, meaning that arr + obj becomes np.add(arr, obj), and then add invokes obj.__array_ufunc__. This is useful if you want to define an object that acts like an array.", "Alternatively, if obj.__array_ufunc__ is set to None, then as a special case, special methods like ndarray.__add__ will notice this and unconditionally raise TypeError. This is useful if you want to create objects that interact with arrays via binary operations, but are not themselves arrays. For example, a units handling system might have an object m representing the \u201cmeters\u201d unit, and want to support the syntax arr * m to represent that the array has units of \u201cmeters\u201d, but not want to otherwise interact with arrays via ufuncs or otherwise. This can be done by setting __array_ufunc__ = None and defining __mul__ and __rmul__ methods. (Note that this means that writing an __array_ufunc__ that always returns NotImplemented is not quite the same as setting __array_ufunc__ = None: in the former case, arr + obj will raise TypeError, while in the latter case it is possible to define a __radd__ method to prevent this.)", "The above does not hold for in-place operators, for which ndarray never returns NotImplemented. Hence, arr += obj would always lead to a TypeError. This is because for arrays in-place operations cannot generically be replaced by a simple reverse operation. (For instance, by default, arr += obj would be translated to arr =\narr + obj, i.e., arr would be replaced, contrary to what is expected for in-place array operations.)", "Note", "If you define __array_ufunc__:", "Note", "If a class defines the __array_ufunc__ method, this disables the __array_wrap__, __array_prepare__, __array_priority__ mechanism described below for ufuncs (which may eventually be deprecated).", "New in version 1.16.", "Note", "As a convenience for __array_function__ implementors, types provides all argument types with an '__array_function__' attribute. This allows implementors to quickly identify cases where they should defer to __array_function__ implementations on other arguments. Implementations should not rely on the iteration order of types.", "Most implementations of __array_function__ will start with two checks:", "If these conditions hold, __array_function__ should return the result from calling its implementation for func(*args, **kwargs). Otherwise, it should return the sentinel value NotImplemented, indicating that the function is not implemented by these types.", "There are no general requirements on the return value from __array_function__, although most sensible implementations should probably return array(s) with the same type as one of the function\u2019s arguments.", "It may also be convenient to define a custom decorators (implements below) for registering __array_function__ implementations.", "Note that it is not required for __array_function__ implementations to include all of the corresponding NumPy function\u2019s optional arguments (e.g., broadcast_to above omits the irrelevant subok argument). Optional arguments are only passed in to __array_function__ if they were explicitly used in the NumPy function call.", "Just like the case for builtin special methods like __add__, properly written __array_function__ methods should always return NotImplemented when an unknown type is encountered. Otherwise, it will be impossible to correctly override NumPy functions from another object if the operation also includes one of your objects.", "For the most part, the rules for dispatch with __array_function__ match those for __array_ufunc__. In particular:", "If no __array_function__ methods exists, NumPy will default to calling its own implementation, intended for use on NumPy arrays. This case arises, for example, when all array-like arguments are Python numbers or lists. (NumPy arrays do have a __array_function__ method, given below, but it always returns NotImplemented if any argument other than a NumPy array subclass implements __array_function__.)", "One deviation from the current behavior of __array_ufunc__ is that NumPy will only call __array_function__ on the first argument of each unique type. This matches Python\u2019s rule for calling reflected methods, and this ensures that checking overloads has acceptable performance even when there are a large number of overloaded arguments.", "This method is called whenever the system internally allocates a new array from obj, where obj is a subclass (subtype) of the ndarray. It can be used to change attributes of self after construction (so as to ensure a 2-d matrix for example), or to update meta-information from the \u201cparent.\u201d Subclasses inherit a default implementation of this method that does nothing.", "At the beginning of every ufunc, this method is called on the input object with the highest array priority, or the output object if one was specified. The output array is passed in and whatever is returned is passed to the ufunc. Subclasses inherit a default implementation of this method which simply returns the output array unmodified. Subclasses may opt to use this method to transform the output array into an instance of the subclass and update metadata before returning the array to the ufunc for computation.", "Note", "For ufuncs, it is hoped to eventually deprecate this method in favour of __array_ufunc__.", "At the end of every ufunc, this method is called on the input object with the highest array priority, or the output object if one was specified. The ufunc-computed array is passed in and whatever is returned is passed to the user. Subclasses inherit a default implementation of this method, which transforms the array into a new instance of the object\u2019s class. Subclasses may opt to use this method to transform the output array into an instance of the subclass and update metadata before returning the array to the user.", "Note", "For ufuncs, it is hoped to eventually deprecate this method in favour of __array_ufunc__.", "The value of this attribute is used to determine what type of object to return in situations where there is more than one possibility for the Python type of the returned object. Subclasses inherit a default value of 0.0 for this attribute.", "Note", "For ufuncs, it is hoped to eventually deprecate this method in favour of __array_ufunc__.", "If a class (ndarray subclass or not) having the __array__ method is used as the output object of an ufunc, results will not be written to the object returned by __array__. This practice will return TypeError.", "Note", "It is strongly advised not to use the matrix subclass. As described below, it makes writing functions that deal consistently with matrices and regular arrays very difficult. Currently, they are mainly used for interacting with scipy.sparse. We hope to provide an alternative for this use, however, and eventually remove the matrix subclass.", "matrix objects inherit from the ndarray and therefore, they have the same attributes and methods of ndarrays. There are six important differences of matrix objects, however, that may lead to unexpected results when you use matrices but expect them to act like arrays:", "Matrices have special attributes which make calculations easier. These are", "matrix.T", "Returns the transpose of the matrix.", "matrix.H", "Returns the (complex) conjugate transpose of self.", "matrix.I", "Returns the (multiplicative) inverse of invertible self.", "matrix.A", "Return self as an ndarray object.", "Warning", "Matrix objects over-ride multiplication, \u2018*\u2019, and power, \u2018**\u2019, to be matrix-multiplication and matrix power, respectively. If your subroutine can accept sub-classes and you do not convert to base- class arrays, then you must use the ufuncs multiply and power to be sure that you are performing the correct operation for all inputs.", "The matrix class is a Python subclass of the ndarray and can be used as a reference for how to construct your own subclass of the ndarray. Matrices can be created from other matrices, strings, and anything else that can be converted to an ndarray . The name \u201cmat \u201cis an alias for \u201cmatrix \u201cin NumPy.", "matrix(data[, dtype, copy])", "Note", "It is no longer recommended to use this class, even for linear", "asmatrix(data[, dtype])", "Interpret the input as a matrix.", "bmat(obj[, ldict, gdict])", "Build a matrix object from a string, nested sequence, or array.", "Example 1: Matrix creation from a string", "Example 2: Matrix creation from nested sequence", "Example 3: Matrix creation from an array", "Memory-mapped files are useful for reading and/or modifying small segments of a large file with regular layout, without reading the entire file into memory. A simple subclass of the ndarray uses a memory-mapped file for the data buffer of the array. For small files, the over-head of reading the entire file into memory is typically not significant, however for large files using memory mapping can save considerable resources.", "Memory-mapped-file arrays have one additional method (besides those they inherit from the ndarray): .flush() which must be called manually by the user to ensure that any changes to the array actually get written to disk.", "memmap(filename[, dtype, mode, offset, ...])", "Create a memory-map to an array stored in a binary file on disk.", "memmap.flush()", "Write any changes in the array to the file on disk.", "Example:", "See also", "Creating character arrays (numpy.char)", "Note", "The chararray class exists for backwards compatibility with Numarray, it is not recommended for new development. Starting from numpy 1.4, if one needs arrays of strings, it is recommended to use arrays of dtype object_, bytes_ or str_, and use the free functions in the numpy.char module for fast vectorized string operations.", "These are enhanced arrays of either str_ type or bytes_ type. These arrays inherit from the ndarray, but specially-define the operations +, *, and % on a (broadcasting) element-by-element basis. These operations are not available on the standard ndarray of character type. In addition, the chararray has all of the standard str (and bytes) methods, executing them on an element-by-element basis. Perhaps the easiest way to create a chararray is to use self.view(chararray) where self is an ndarray of str or unicode data-type. However, a chararray can also be created using the numpy.chararray constructor, or via the numpy.char.array function:", "chararray(shape[, itemsize, unicode, ...])", "Provides a convenient view on arrays of string and unicode values.", "core.defchararray.array(obj[, itemsize, ...])", "Create a chararray.", "Another difference with the standard ndarray of str data-type is that the chararray inherits the feature introduced by Numarray that white-space at the end of any element in the array will be ignored on item retrieval and comparison operations.", "See also", "Creating record arrays (numpy.rec), Data type routines, Data type objects (dtype).", "NumPy provides the recarray class which allows accessing the fields of a structured array as attributes, and a corresponding scalar data type object record.", "recarray(shape[, dtype, buf, offset, ...])", "Construct an ndarray that allows field access using attributes.", "record", "A data-type scalar that allows field access as attribute lookup.", "See also", "Masked arrays", "For backward compatibility and as a standard \u201ccontainer \u201cclass, the UserArray from Numeric has been brought over to NumPy and named numpy.lib.user_array.container The container class is a Python class whose self.array attribute is an ndarray. Multiple inheritance is probably easier with numpy.lib.user_array.container than with the ndarray itself and so it is included by default. It is not documented here beyond mentioning its existence because you are encouraged to use the ndarray class directly if you can.", "numpy.lib.user_array.container(data[, ...])", "Standard container-class for easy multiple-inheritance.", "Iterators are a powerful concept for array processing. Essentially, iterators implement a generalized for-loop. If myiter is an iterator object, then the Python code:", "calls val = next(myiter) repeatedly until StopIteration is raised by the iterator. There are several ways to iterate over an array that may be useful: default iteration, flat iteration, and \\(N\\)-dimensional enumeration.", "The default iterator of an ndarray object is the default Python iterator of a sequence type. Thus, when the array object itself is used as an iterator. The default behavior is equivalent to:", "This default iterator selects a sub-array of dimension \\(N-1\\) from the array. This can be a useful construct for defining recursive algorithms. To loop over the entire array requires \\(N\\) for-loops.", "ndarray.flat", "A 1-D iterator over the array.", "As mentioned previously, the flat attribute of ndarray objects returns an iterator that will cycle over the entire array in C-style contiguous order.", "Here, I\u2019ve used the built-in enumerate iterator to return the iterator index as well as the value.", "ndenumerate(arr)", "Multidimensional index iterator.", "Sometimes it may be useful to get the N-dimensional index while iterating. The ndenumerate iterator can achieve this.", "broadcast", "Produce an object that mimics broadcasting.", "The general concept of broadcasting is also available from Python using the broadcast iterator. This object takes \\(N\\) objects as inputs and returns an iterator that returns tuples providing each of the input sequence elements in the broadcasted result."]}, {"name": "class.__array_wrap__()", "path": "reference/arrays.classes#numpy.class.__array_wrap__", "type": "Standard array subclasses", "text": ["At the end of every ufunc, this method is called on the input object with the highest array priority, or the output object if one was specified. The ufunc-computed array is passed in and whatever is returned is passed to the user. Subclasses inherit a default implementation of this method, which transforms the array into a new instance of the object\u2019s class. Subclasses may opt to use this method to transform the output array into an instance of the subclass and update metadata before returning the array to the user.", "Note", "For ufuncs, it is hoped to eventually deprecate this method in favour of __array_ufunc__."]}, {"name": "config.add_library()", "path": "reference/distutils_guide", "type": "NumPy Distutils - Users Guide", "text": ["Currently SciPy project consists of two packages:", "NumPy \u2014 it provides packages like:", "The aim of this document is to describe how to add new tools to SciPy.", "SciPy consists of Python packages, called SciPy packages, that are available to Python users via the scipy namespace. Each SciPy package may contain other SciPy packages. And so on. Therefore, the SciPy directory tree is a tree of packages with arbitrary depth and width. Any SciPy package may depend on NumPy packages but the dependence on other SciPy packages should be kept minimal or zero.", "A SciPy package contains, in addition to its sources, the following files and directories:", "Their contents are described below.", "In order to add a Python package to SciPy, its build script (setup.py) must meet certain requirements. The most important requirement is that the package define a configuration(parent_package='',top_path=None) function which returns a dictionary suitable for passing to numpy.distutils.core.setup(..). To simplify the construction of this dictionary, numpy.distutils.misc_util provides the Configuration class, described below.", "Below is an example of a minimal setup.py file for a pure SciPy package:", "The arguments of the configuration function specify the name of parent SciPy package (parent_package) and the directory location of the main setup.py script (top_path). These arguments, along with the name of the current package, should be passed to the Configuration constructor.", "The Configuration constructor has a fourth optional argument, package_path, that can be used when package files are located in a different location than the directory of the setup.py file.", "Remaining Configuration arguments are all keyword arguments that will be used to initialize attributes of Configuration instance. Usually, these keywords are the same as the ones that setup(..) function would expect, for example, packages, ext_modules, data_files, include_dirs, libraries, headers, scripts, package_dir, etc. However, the direct specification of these keywords is not recommended as the content of these keyword arguments will not be processed or checked for the consistency of SciPy building system.", "Finally, Configuration has .todict() method that returns all the configuration data as a dictionary suitable for passing on to the setup(..) function.", "In addition to attributes that can be specified via keyword arguments to Configuration constructor, Configuration instance (let us denote as config) has the following attributes that can be useful in writing setup scripts:", "config.add_data_files(*files) \u2014 prepend files to data_files list. If files item is a tuple then its first element defines the suffix of where data files are copied relative to package installation directory and the second element specifies the path to data files. By default data files are copied under package installation directory. For example,", "will install data files to the following locations", "Path to data files can be a function taking no arguments and returning path(s) to data files \u2013 this is a useful when data files are generated while building the package. (XXX: explain the step when this function are called exactly)", "config.add_data_dir(data_path) \u2014 add directory data_path recursively to data_files. The whole directory tree starting at data_path will be copied under package installation directory. If data_path is a tuple then its first element defines the suffix of where data files are copied relative to package installation directory and the second element specifies the path to data directory. By default, data directory are copied under package installation directory under the basename of data_path. For example,", "will install data files to the following locations", "config.add_extension(name,sources,**kw) \u2014 create and add an Extension instance to ext_modules list. The first argument name defines the name of the extension module that will be installed under config.name package. The second argument is a list of sources. add_extension method takes also keyword arguments that are passed on to the Extension constructor. The list of allowed keywords is the following: include_dirs, define_macros, undef_macros, library_dirs, libraries, runtime_library_dirs, extra_objects, extra_compile_args, extra_link_args, export_symbols, swig_opts, depends, language, f2py_options, module_dirs, extra_info, extra_f77_compile_args, extra_f90_compile_args.", "Note that config.paths method is applied to all lists that may contain paths. extra_info is a dictionary or a list of dictionaries that content will be appended to keyword arguments. The list depends contains paths to files or directories that the sources of the extension module depend on. If any path in the depends list is newer than the extension module, then the module will be rebuilt.", "The list of sources may contain functions (\u2018source generators\u2019) with a pattern def <funcname>(ext, build_dir): return\n<source(s) or None>. If funcname returns None, no sources are generated. And if the Extension instance has no sources after processing all source generators, no extension module will be built. This is the recommended way to conditionally define extension modules. Source generator functions are called by the build_src sub-command of numpy.distutils.", "For example, here is a typical source generator function:", "The first argument contains the Extension instance that can be useful to access its attributes like depends, sources, etc. lists and modify them during the building process. The second argument gives a path to a build directory that must be used when creating files to a disk.", "NumPy distutils supports automatic conversion of source files named <somefile>.src. This facility can be used to maintain very similar code blocks requiring only simple changes between blocks. During the build phase of setup, if a template file named <somefile>.src is encountered, a new file named <somefile> is constructed from the template and placed in the build directory to be used instead. Two forms of template conversion are supported. The first form occurs for files named <file>.ext.src where ext is a recognized Fortran extension (f, f90, f95, f77, for, ftn, pyf). The second form is used for all other cases.", "This template converter will replicate all function and subroutine blocks in the file with names that contain \u2018<\u2026>\u2019 according to the rules in \u2018<\u2026>\u2019. The number of comma-separated words in \u2018<\u2026>\u2019 determines the number of times the block is repeated. What these words are indicates what that repeat rule, \u2018<\u2026>\u2019, should be replaced with in each block. All of the repeat rules in a block must contain the same number of comma-separated words indicating the number of times that block should be repeated. If the word in the repeat rule needs a comma, leftarrow, or rightarrow, then prepend it with a backslash \u2018 '. If a word in the repeat rule matches \u2018 \\<index>\u2019 then it will be replaced with the <index>-th word in the same repeat specification. There are two forms for the repeat rule: named and short.", "A named repeat rule is useful when the same set of repeats must be used several times in a block. It is specified using <rule1=item1, item2, item3,\u2026, itemN>, where N is the number of times the block should be repeated. On each repeat of the block, the entire expression, \u2018<\u2026>\u2019 will be replaced first with item1, and then with item2, and so forth until N repeats are accomplished. Once a named repeat specification has been introduced, the same repeat rule may be used in the current block by referring only to the name (i.e. <rule1>).", "A short repeat rule looks like <item1, item2, item3, \u2026, itemN>. The rule specifies that the entire expression, \u2018<\u2026>\u2019 should be replaced first with item1, and then with item2, and so forth until N repeats are accomplished.", "The following predefined named repeat rules are available:", "Non-Fortran files use a separate syntax for defining template blocks that should be repeated using a variable expansion similar to the named repeat rules of the Fortran-specific repeats.", "NumPy Distutils preprocesses C source files (extension: .c.src) written in a custom templating language to generate C code. The @ symbol is used to wrap macro-style variables to empower a string substitution mechanism that might describe (for instance) a set of data types.", "The template language blocks are delimited by /**begin repeat and /**end repeat**/ lines, which may also be nested using consecutively numbered delimiting lines such as /**begin repeat1 and /**end repeat1**/:", "The above rules may be clearer in the following template source example:", "The preprocessing of generically-typed C source files (whether in NumPy proper or in any third party package using NumPy Distutils) is performed by conv_template.py. The type-specific C files generated (extension: .c) by these modules during the build process are ready to be compiled. This form of generic typing is also supported for C header files (preprocessed to produce .h files).", "The header of a typical SciPy __init__.py is:", "It is possible to specify config_fc options in setup.py scripts. For example, using", "sources=[\u2026], config_fc={\u2018noopt\u2019:(__file__,1)})", "will compile the library sources without optimization flags.", "It\u2019s recommended to specify only those config_fc options in such a way that are compiler independent.", "Some old Fortran codes need special compiler options in order to work correctly. In order to specify compiler options per source file, numpy.distutils Fortran compiler looks for the following pattern:", "in the first 20 lines of the source and use the f77flags for specified type of the fcompiler (the first character C is optional).", "TODO: This feature can be easily extended for Fortran 90 codes as well. Let us know if you would need such a feature."]}, {"name": "const Tp *data()", "path": "dev/howto-docs#_CPPv4N9DoxyLimbo4dataEv", "type": "Development", "text": ["Returns the raw data for the limbo. "]}, {"name": "Contributing to NumPy", "path": "dev/index", "type": "Development", "text": ["Not a coder? Not a problem! NumPy is multi-faceted, and we can use a lot of help. These are all activities we\u2019d like to get help with (they\u2019re all important, so we list them in alphabetical order):", "The rest of this document discusses working on the NumPy code base and documentation. We\u2019re in the process of updating our descriptions of other activities and roles. If you are interested in these other activities, please contact us! You can do this via the numpy-discussion mailing list, or on GitHub (open an issue or comment on a relevant issue). These are our preferred communication channels (open source is open by nature!), however if you prefer to discuss in private first, please reach out to our community coordinators at numpy-team@googlegroups.com or numpy-team.slack.com (send an email to numpy-team@googlegroups.com for an invite the first time).", "Here\u2019s the short summary, complete TOC links are below:", "If you are a first-time contributor:", "Clone the project to your local computer:", "Change the directory:", "Add the upstream repository:", "Now, git remote -v will show two remote repositories named:", "Develop your contribution:", "Pull the latest changes from upstream:", "Create a branch for the feature you want to work on. Since the branch name will appear in the merge message, use a sensible name such as \u2018linspace-speedups\u2019:", "To submit your contribution:", "Push your changes back to your fork on GitHub:", "Review process:", "Document changes", "Beyond changes to a functions docstring and possible description in the general documentation, if your change introduces any user-facing modifications they may need to be mentioned in the release notes. To add your change to the release notes, you need to create a short file with a summary and place it in doc/release/upcoming_changes. The file doc/release/upcoming_changes/README.rst details the format and filename conventions.", "If your change introduces a deprecation, make sure to discuss this first on GitHub or the mailing list first. If agreement on the deprecation is reached, follow NEP 23 deprecation policy to add the deprecation.", "Cross referencing issues", "If the PR relates to any issues, you can add the text xref gh-xxxx where xxxx is the number of the issue to github comments. Likewise, if the PR solves an issue, replace the xref with closes, fixes or any of the other flavors github accepts.", "In the source code, be sure to preface any issue or PR reference with gh-xxxx.", "For a more detailed discussion, read on and follow the links at the bottom of this page.", "If GitHub indicates that the branch of your Pull Request can no longer be merged automatically, you have to incorporate changes that have been made since you started into your branch. Our recommended way to do this is to rebase on main.", "Use the following import conventions:", "Pull requests (PRs) that modify code should either have new tests, or modify existing tests to fail before the PR and pass afterwards. You should run the tests before pushing a PR.", "Running NumPy\u2019s test suite locally requires some additional packages, such as pytest and hypothesis. The additional testing dependencies are listed in test_requirements.txt in the top-level directory, and can conveniently be installed with:", "Tests for a module should ideally cover all code in that module, i.e., statement coverage should be at 100%.", "To measure the test coverage, install pytest-cov and then run:", "This will create a report in build/coverage, which can be viewed with:", "To build docs, run make from the doc directory. make help lists all targets. For example, to build the HTML documentation, you can run:", "To get the appropriate dependencies and other requirements, see Building the NumPy API and reference docs.", "The rest of the story", "NumPy-specific workflow is in numpy-development-workflow."]}, {"name": "Convenience Classes", "path": "reference/routines.polynomials.package", "type": "Convenience classes", "text": ["The following lists the various constants and methods common to all of the classes representing the various kinds of polynomials. In the following, the term Poly represents any one of the convenience classes (e.g. Polynomial, Chebyshev, Hermite, etc.) while the lowercase p represents an instance of a polynomial class.", "Methods for creating polynomial instances.", "Methods for converting a polynomial instance of one kind to another."]}, {"name": "Copies and views", "path": "user/basics.copies", "type": "User Guide", "text": ["When operating on NumPy arrays, it is possible to access the internal data buffer directly using a view without copying data around. This ensures good performance but can also cause unwanted problems if the user is not aware of how this works. Hence, it is important to know the difference between these two terms and to know which operations return copies and which return views.", "The NumPy array is a data structure consisting of two parts: the contiguous data buffer with the actual data elements and the metadata that contains information about the data buffer. The metadata includes data type, strides, and other important information that helps manipulate the ndarray easily. See the Internal organization of NumPy arrays section for a detailed look.", "It is possible to access the array differently by just changing certain metadata like stride and dtype without changing the data buffer. This creates a new way of looking at the data and these new arrays are called views. The data buffer remains the same, so any changes made to a view reflects in the original copy. A view can be forced through the ndarray.view method.", "When a new array is created by duplicating the data buffer as well as the metadata, it is called a copy. Changes made to the copy do not reflect on the original array. Making a copy is slower and memory-consuming but sometimes necessary. A copy can be forced by using ndarray.copy.", "See also", "Indexing on ndarrays", "Views are created when elements can be addressed with offsets and strides in the original array. Hence, basic indexing always creates views. For example:", "Here, y gets changed when x is changed because it is a view.", "Advanced indexing, on the other hand, always creates copies. For example:", "Here, y is a copy, as signified by the base attribute. We can also confirm this by assigning new values to x[[1, 2]] which in turn will not affect y at all:", "It must be noted here that during the assignment of x[[1, 2]] no view or copy is created as the assignment happens in-place.", "The numpy.reshape function creates a view where possible or a copy otherwise. In most cases, the strides can be modified to reshape the array with a view. However, in some cases where the array becomes non-contiguous (perhaps after a ndarray.transpose operation), the reshaping cannot be done by modifying strides and requires a copy. In these cases, we can raise an error by assigning the new shape to the shape attribute of the array. For example:", "Taking the example of another operation, ravel returns a contiguous flattened view of the array wherever possible. On the other hand, ndarray.flatten always returns a flattened copy of the array. However, to guarantee a view in most cases, x.reshape(-1) may be preferable.", "The base attribute of the ndarray makes it easy to tell if an array is a view or a copy. The base attribute of a view returns the original array while it returns None for a copy.", "Note that the base attribute should not be used to determine if an ndarray object is new; only if it is a view or a copy of another ndarray."]}, {"name": "core.defchararray.array()", "path": "reference/generated/numpy.core.defchararray.array", "type": "numpy.core.defchararray.array", "text": ["Create a chararray.", "Note", "This class is provided for numarray backward-compatibility. New code (not concerned with numarray compatibility) should use arrays of type string_ or unicode_ and use the free functions in numpy.char for fast vectorized string operations instead.", "Versus a regular NumPy array of type str or unicode, this class adds the following functionality:", "itemsize is the number of characters per scalar in the resulting array. If itemsize is None, and obj is an object array or a Python list, the itemsize will be automatically determined. If itemsize is provided and obj is of type str or unicode, then the obj string will be chunked into itemsize pieces.", "If true (default), then the object is copied. Otherwise, a copy will only be made if __array__ returns a copy, if obj is a nested sequence, or if a copy is needed to satisfy any of the other requirements (itemsize, unicode, order, etc.).", "When true, the resulting chararray can contain Unicode characters, when false only 8-bit characters. If unicode is None and obj is one of the following:", "then the unicode setting of the output array will be automatically determined.", "Specify the order of the array. If order is \u2018C\u2019 (default), then the array will be in C-contiguous order (last-index varies the fastest). If order is \u2018F\u2019, then the returned array will be in Fortran-contiguous order (first-index varies the fastest). If order is \u2018A\u2019, then the returned array may be in any order (either C-, Fortran-contiguous, or even discontiguous)."]}, {"name": "core.defchararray.asarray()", "path": "reference/generated/numpy.core.defchararray.asarray", "type": "numpy.core.defchararray.asarray", "text": ["Convert the input to a chararray, copying the data only if necessary.", "Versus a regular NumPy array of type str or unicode, this class adds the following functionality:", "itemsize is the number of characters per scalar in the resulting array. If itemsize is None, and obj is an object array or a Python list, the itemsize will be automatically determined. If itemsize is provided and obj is of type str or unicode, then the obj string will be chunked into itemsize pieces.", "When true, the resulting chararray can contain Unicode characters, when false only 8-bit characters. If unicode is None and obj is one of the following:", "then the unicode setting of the output array will be automatically determined.", "Specify the order of the array. If order is \u2018C\u2019 (default), then the array will be in C-contiguous order (last-index varies the fastest). If order is \u2018F\u2019, then the returned array will be in Fortran-contiguous order (first-index varies the fastest)."]}, {"name": "core.records.array()", "path": "reference/generated/numpy.core.records.array", "type": "numpy.core.records.array", "text": ["Construct a record array from a wide-variety of objects.", "A general-purpose record array constructor that dispatches to the appropriate recarray creation function based on the inputs (see Notes).", "Input object. See Notes for details on how various input types are treated.", "Valid dtype for array.", "Shape of each array.", "Position in the file or buffer to start reading from.", "Buffer (buf) is interpreted according to these strides (strides define how many bytes each array element, row, column, etc. occupy in memory).", "If dtype is None, these arguments are passed to numpy.format_parser to construct a dtype. See that function for detailed documentation.", "Whether to copy the input object (True), or to use a reference instead. This option only applies when the input is an ndarray or recarray. Defaults to True.", "Record array created from the specified object.", "If obj is None, then call the recarray constructor. If obj is a string, then call the fromstring constructor. If obj is a list or a tuple, then if the first object is an ndarray, call fromarrays, otherwise call fromrecords. If obj is a recarray, then make a copy of the data in the recarray (if copy=True) and use the new formats, names, and titles. If obj is a file, then call fromfile. Finally, if obj is an ndarray, then return obj.view(recarray), making a copy of the data if copy=True."]}, {"name": "core.records.fromarrays()", "path": "reference/generated/numpy.core.records.fromarrays", "type": "numpy.core.records.fromarrays", "text": ["Create a record array from a (flat) list of arrays", "List of array-like objects (such as lists, tuples, and ndarrays).", "valid dtype for all arrays", "Shape of the resulting array. If not provided, inferred from arrayList[0].", "If dtype is None, these arguments are passed to numpy.format_parser to construct a dtype. See that function for detailed documentation.", "Record array consisting of given arrayList columns."]}, {"name": "core.records.fromfile()", "path": "reference/generated/numpy.core.records.fromfile", "type": "numpy.core.records.fromfile", "text": ["Create an array from binary file data", "If file is a string or a path-like object then that file is opened, else it is assumed to be a file object. The file object must support random access (i.e. it must have tell and seek methods).", "valid dtype for all arrays", "shape of each array.", "Position in the file to start reading from.", "If dtype is None, these arguments are passed to numpy.format_parser to construct a dtype. See that function for detailed documentation", "record array consisting of data enclosed in file."]}, {"name": "core.records.fromrecords()", "path": "reference/generated/numpy.core.records.fromrecords", "type": "numpy.core.records.fromrecords", "text": ["Create a recarray from a list of records in text form.", "data in the same field may be heterogeneous - they will be promoted to the highest data type.", "valid dtype for all arrays", "shape of each array.", "If dtype is None, these arguments are passed to numpy.format_parser to construct a dtype. See that function for detailed documentation.", "If both formats and dtype are None, then this will auto-detect formats. Use list of tuples rather than list of lists for faster processing.", "record array consisting of given recList rows."]}, {"name": "core.records.fromstring()", "path": "reference/generated/numpy.core.records.fromstring", "type": "numpy.core.records.fromstring", "text": ["Create a record array from binary data", "Note that despite the name of this function it does not accept str instances.", "Buffer of binary data", "Valid dtype for all arrays", "Shape of each array.", "Position in the buffer to start reading from.", "If dtype is None, these arguments are passed to numpy.format_parser to construct a dtype. See that function for detailed documentation.", "Record array view into the data in datastring. This will be readonly if datastring is readonly.", "See also"]}, {"name": "CT", "path": "reference/routines.fft", "type": "Discrete Fourier Transform ( \n      \n       numpy.fft\n      \n      )", "text": ["The SciPy module scipy.fft is a more comprehensive superset of numpy.fft, which includes only a basic set of routines.", "fft(a[, n, axis, norm])", "Compute the one-dimensional discrete Fourier Transform.", "ifft(a[, n, axis, norm])", "Compute the one-dimensional inverse discrete Fourier Transform.", "fft2(a[, s, axes, norm])", "Compute the 2-dimensional discrete Fourier Transform.", "ifft2(a[, s, axes, norm])", "Compute the 2-dimensional inverse discrete Fourier Transform.", "fftn(a[, s, axes, norm])", "Compute the N-dimensional discrete Fourier Transform.", "ifftn(a[, s, axes, norm])", "Compute the N-dimensional inverse discrete Fourier Transform.", "rfft(a[, n, axis, norm])", "Compute the one-dimensional discrete Fourier Transform for real input.", "irfft(a[, n, axis, norm])", "Computes the inverse of rfft.", "rfft2(a[, s, axes, norm])", "Compute the 2-dimensional FFT of a real array.", "irfft2(a[, s, axes, norm])", "Computes the inverse of rfft2.", "rfftn(a[, s, axes, norm])", "Compute the N-dimensional discrete Fourier Transform for real input.", "irfftn(a[, s, axes, norm])", "Computes the inverse of rfftn.", "hfft(a[, n, axis, norm])", "Compute the FFT of a signal that has Hermitian symmetry, i.e., a real spectrum.", "ihfft(a[, n, axis, norm])", "Compute the inverse FFT of a signal that has Hermitian symmetry.", "fftfreq(n[, d])", "Return the Discrete Fourier Transform sample frequencies.", "rfftfreq(n[, d])", "Return the Discrete Fourier Transform sample frequencies (for usage with rfft, irfft).", "fftshift(x[, axes])", "Shift the zero-frequency component to the center of the spectrum.", "ifftshift(x[, axes])", "The inverse of fftshift.", "Fourier analysis is fundamentally a method for expressing a function as a sum of periodic components, and for recovering the function from those components. When both the function and its Fourier transform are replaced with discretized counterparts, it is called the discrete Fourier transform (DFT). The DFT has become a mainstay of numerical computing in part because of a very fast algorithm for computing it, called the Fast Fourier Transform (FFT), which was known to Gauss (1805) and was brought to light in its current form by Cooley and Tukey [CT]. Press et al. [NR] provide an accessible introduction to Fourier analysis and its applications.", "Because the discrete Fourier transform separates its input into components that contribute at discrete frequencies, it has a great number of applications in digital signal processing, e.g., for filtering, and in this context the discretized input to the transform is customarily referred to as a signal, which exists in the time domain. The output is called a spectrum or transform and exists in the frequency domain.", "There are many ways to define the DFT, varying in the sign of the exponent, normalization, etc. In this implementation, the DFT is defined as", "The DFT is in general defined for complex inputs and outputs, and a single-frequency component at linear frequency \\(f\\) is represented by a complex exponential \\(a_m = \\exp\\{2\\pi i\\,f m\\Delta t\\}\\), where \\(\\Delta t\\) is the sampling interval.", "The values in the result follow so-called \u201cstandard\u201d order: If A =\nfft(a, n), then A[0] contains the zero-frequency term (the sum of the signal), which is always purely real for real inputs. Then A[1:n/2] contains the positive-frequency terms, and A[n/2+1:] contains the negative-frequency terms, in order of decreasingly negative frequency. For an even number of input points, A[n/2] represents both positive and negative Nyquist frequency, and is also purely real for real input. For an odd number of input points, A[(n-1)/2] contains the largest positive frequency, while A[(n+1)/2] contains the largest negative frequency. The routine np.fft.fftfreq(n) returns an array giving the frequencies of corresponding elements in the output. The routine np.fft.fftshift(A) shifts transforms and their frequencies to put the zero-frequency components in the middle, and np.fft.ifftshift(A) undoes that shift.", "When the input a is a time-domain signal and A = fft(a), np.abs(A) is its amplitude spectrum and np.abs(A)**2 is its power spectrum. The phase spectrum is obtained by np.angle(A).", "The inverse DFT is defined as", "It differs from the forward transform by the sign of the exponential argument and the default normalization by \\(1/n\\).", "numpy.fft promotes float32 and complex64 arrays to float64 and complex128 arrays respectively. For an FFT implementation that does not promote input arrays, see scipy.fftpack.", "The argument norm indicates which direction of the pair of direct/inverse transforms is scaled and with what normalization factor. The default normalization (\"backward\") has the direct (forward) transforms unscaled and the inverse (backward) transforms scaled by \\(1/n\\). It is possible to obtain unitary transforms by setting the keyword argument norm to \"ortho\" so that both direct and inverse transforms are scaled by \\(1/\\sqrt{n}\\). Finally, setting the keyword argument norm to \"forward\" has the direct transforms scaled by \\(1/n\\) and the inverse transforms unscaled (i.e. exactly opposite to the default \"backward\"). None is an alias of the default option \"backward\" for backward compatibility.", "When the input is purely real, its transform is Hermitian, i.e., the component at frequency \\(f_k\\) is the complex conjugate of the component at frequency \\(-f_k\\), which means that for real inputs there is no information in the negative frequency components that is not already available from the positive frequency components. The family of rfft functions is designed to operate on real inputs, and exploits this symmetry by computing only the positive frequency components, up to and including the Nyquist frequency. Thus, n input points produce n/2+1 complex output points. The inverses of this family assumes the same symmetry of its input, and for an output of n points uses n/2+1 input points.", "Correspondingly, when the spectrum is purely real, the signal is Hermitian. The hfft family of functions exploits this symmetry by using n/2+1 complex points in the input (time) domain for n real points in the frequency domain.", "In higher dimensions, FFTs are used, e.g., for image analysis and filtering. The computational efficiency of the FFT means that it can also be a faster way to compute large convolutions, using the property that a convolution in the time domain is equivalent to a point-by-point multiplication in the frequency domain.", "In two dimensions, the DFT is defined as", "which extends in the obvious way to higher dimensions, and the inverses in higher dimensions also extend in the same way.", "Cooley, James W., and John W. Tukey, 1965, \u201cAn algorithm for the machine calculation of complex Fourier series,\u201d Math. Comput. 19: 297-301.", "Press, W., Teukolsky, S., Vetterline, W.T., and Flannery, B.P., 2007, Numerical Recipes: The Art of Scientific Computing, ch. 12-13. Cambridge Univ. Press, Cambridge, UK.", "For examples, see the various functions."]}, {"name": "Data type routines", "path": "reference/routines.dtype", "type": "Data type routines", "text": ["can_cast(from_, to[, casting])", "Returns True if cast between data types can occur according to the casting rule.", "promote_types(type1, type2)", "Returns the data type with the smallest size and smallest scalar kind to which both type1 and type2 may be safely cast.", "min_scalar_type(a, /)", "For scalar a, returns the data type with the smallest size and smallest scalar kind which can hold its value.", "result_type(*arrays_and_dtypes)", "Returns the type that results from applying the NumPy type promotion rules to the arguments.", "common_type(*arrays)", "Return a scalar type which is common to the input arrays.", "obj2sctype(rep[, default])", "Return the scalar dtype or NumPy equivalent of Python type of an object.", "dtype(dtype[, align, copy])", "Create a data type object.", "format_parser(formats, names, titles[, ...])", "Class to convert formats, names, titles description to a dtype.", "finfo(dtype)", "Machine limits for floating point types.", "iinfo(type)", "Machine limits for integer types.", "MachAr([float_conv, int_conv, ...])", "Diagnosing machine parameters.", "issctype(rep)", "Determines whether the given object represents a scalar data-type.", "issubdtype(arg1, arg2)", "Returns True if first argument is a typecode lower/equal in type hierarchy.", "issubsctype(arg1, arg2)", "Determine if the first argument is a subclass of the second argument.", "issubclass_(arg1, arg2)", "Determine if a class is a subclass of a second class.", "find_common_type(array_types, scalar_types)", "Determine common type following standard coercion rules.", "typename(char)", "Return a description for the given data type code.", "sctype2char(sctype)", "Return the string representation of a scalar dtype.", "mintypecode(typechars[, typeset, default])", "Return the character for the minimum-size type to which given types can be safely cast.", "maximum_sctype(t)", "Return the scalar type of highest precision of the same kind as the input."]}, {"name": "Data types", "path": "user/basics.types", "type": "User Guide", "text": ["See also", "Data type objects", "NumPy supports a much greater variety of numerical types than Python does. This section shows which are available, and how to modify an array\u2019s data-type.", "The primitive types supported are tied closely to those in C:", "Numpy type", "C type", "Description", "numpy.bool_", "bool", "Boolean (True or False) stored as a byte", "numpy.byte", "signed char", "Platform-defined", "numpy.ubyte", "unsigned char", "Platform-defined", "numpy.short", "short", "Platform-defined", "numpy.ushort", "unsigned short", "Platform-defined", "numpy.intc", "int", "Platform-defined", "numpy.uintc", "unsigned int", "Platform-defined", "numpy.int_", "long", "Platform-defined", "numpy.uint", "unsigned long", "Platform-defined", "numpy.longlong", "long long", "Platform-defined", "numpy.ulonglong", "unsigned long long", "Platform-defined", "numpy.half / numpy.float16", "Half precision float: sign bit, 5 bits exponent, 10 bits mantissa", "numpy.single", "float", "Platform-defined single precision float: typically sign bit, 8 bits exponent, 23 bits mantissa", "numpy.double", "double", "Platform-defined double precision float: typically sign bit, 11 bits exponent, 52 bits mantissa.", "numpy.longdouble", "long double", "Platform-defined extended-precision float", "numpy.csingle", "float complex", "Complex number, represented by two single-precision floats (real and imaginary components)", "numpy.cdouble", "double complex", "Complex number, represented by two double-precision floats (real and imaginary components).", "numpy.clongdouble", "long double complex", "Complex number, represented by two extended-precision floats (real and imaginary components).", "Since many of these have platform-dependent definitions, a set of fixed-size aliases are provided (See Sized aliases).", "NumPy numerical types are instances of dtype (data-type) objects, each having unique characteristics. Once you have imported NumPy using", "the dtypes are available as np.bool_, np.float32, etc.", "Advanced types, not listed above, are explored in section Structured arrays.", "There are 5 basic numerical types representing booleans (bool), integers (int), unsigned integers (uint) floating point (float) and complex. Those with numbers in their name indicate the bitsize of the type (i.e. how many bits are needed to represent a single value in memory). Some types, such as int and intp, have differing bitsizes, dependent on the platforms (e.g. 32-bit vs. 64-bit machines). This should be taken into account when interfacing with low-level code (such as C or Fortran) where the raw memory is addressed.", "Data-types can be used as functions to convert python numbers to array scalars (see the array scalar section for an explanation), python sequences of numbers to arrays of that type, or as arguments to the dtype keyword that many numpy functions or methods accept. Some examples:", "Array types can also be referred to by character codes, mostly to retain backward compatibility with older packages such as Numeric. Some documentation may still refer to these, for example:", "We recommend using dtype objects instead.", "To convert the type of an array, use the .astype() method (preferred) or the type itself as a function. For example:", "Note that, above, we use the Python float object as a dtype. NumPy knows that int refers to np.int_, bool means np.bool_, that float is np.float_ and complex is np.complex_. The other data-types do not have Python equivalents.", "To determine the type of an array, look at the dtype attribute:", "dtype objects also contain information about the type, such as its bit-width and its byte-order. The data type can also be used indirectly to query properties of the type, such as whether it is an integer:", "NumPy generally returns elements of arrays as array scalars (a scalar with an associated dtype). Array scalars differ from Python scalars, but for the most part they can be used interchangeably (the primary exception is for versions of Python older than v2.x, where integer array scalars cannot act as indices for lists and tuples). There are some exceptions, such as when code requires very specific attributes of a scalar or when it checks specifically whether a value is a Python scalar. Generally, problems are easily fixed by explicitly converting array scalars to Python scalars, using the corresponding Python type function (e.g., int, float, complex, str, unicode).", "The primary advantage of using array scalars is that they preserve the array type (Python may not have a matching scalar type available, e.g. int16). Therefore, the use of array scalars ensures identical behaviour between arrays and scalars, irrespective of whether the value is inside an array or not. NumPy scalars also have many of the same methods arrays do.", "The fixed size of NumPy numeric types may cause overflow errors when a value requires more memory than available in the data type. For example, numpy.power evaluates 100 ** 8 correctly for 64-bit integers, but gives 1874919424 (incorrect) for a 32-bit integer.", "The behaviour of NumPy and Python integer types differs significantly for integer overflows and may confuse users expecting NumPy integers to behave similar to Python\u2019s int. Unlike NumPy, the size of Python\u2019s int is flexible. This means Python integers may expand to accommodate any integer and will not overflow.", "NumPy provides numpy.iinfo and numpy.finfo to verify the minimum or maximum values of NumPy integer and floating point values respectively", "If 64-bit integers are still too small the result may be cast to a floating point number. Floating point numbers offer a larger, but inexact, range of possible values.", "Python\u2019s floating-point numbers are usually 64-bit floating-point numbers, nearly equivalent to np.float64. In some unusual situations it may be useful to use floating-point numbers with more precision. Whether this is possible in numpy depends on the hardware and on the development environment: specifically, x86 machines provide hardware floating-point with 80-bit precision, and while most C compilers provide this as their long double type, MSVC (standard for Windows builds) makes long double identical to double (64 bits). NumPy makes the compiler\u2019s long double available as np.longdouble (and np.clongdouble for the complex numbers). You can find out what your numpy provides with np.finfo(np.longdouble).", "NumPy does not provide a dtype with more precision than C\u2019s long double\\; in particular, the 128-bit IEEE quad precision data type (FORTRAN\u2019s REAL*16\\) is not available.", "For efficient memory alignment, np.longdouble is usually stored padded with zero bits, either to 96 or 128 bits. Which is more efficient depends on hardware and development environment; typically on 32-bit systems they are padded to 96 bits, while on 64-bit systems they are typically padded to 128 bits. np.longdouble is padded to the system default; np.float96 and np.float128 are provided for users who want specific padding. In spite of the names, np.float96 and np.float128 provide only as much precision as np.longdouble, that is, 80 bits on most x86 machines and 64 bits in standard Windows builds.", "Be warned that even if np.longdouble offers more precision than python float, it is easy to lose that extra precision, since python often forces values to pass through float. For example, the % formatting operator requires its arguments to be converted to standard python types, and it is therefore impossible to preserve extended precision even if many decimal places are requested. It can be useful to test your code with the value 1 + np.finfo(np.longdouble).eps."]}, {"name": "DataSource.abspath()", "path": "reference/generated/numpy.datasource.abspath", "type": "numpy.DataSource.abspath", "text": ["method", "Return absolute path of file in the DataSource directory.", "If path is an URL, then abspath will return either the location the file exists locally or the location it would exist when opened using the open method.", "Can be a local file or a remote URL.", "Complete path, including the DataSource destination directory.", "The functionality is based on os.path.abspath."]}, {"name": "DataSource.exists()", "path": "reference/generated/numpy.datasource.exists", "type": "numpy.DataSource.exists", "text": ["method", "Test if path exists.", "Test if path exists as (and in this order):", "Can be a local file or a remote URL.", "True if path exists.", "When path is an URL, exists will return True if it\u2019s either stored locally in the DataSource directory, or is a valid remote URL. DataSource does not discriminate between the two, the file is accessible if it exists in either location."]}, {"name": "DataSource.open()", "path": "reference/generated/numpy.datasource.open", "type": "numpy.DataSource.open", "text": ["method", "Open and return file-like object.", "If path is an URL, it will be downloaded, stored in the DataSource directory and opened from there.", "Local file path or URL to open.", "Mode to open path. Mode \u2018r\u2019 for reading, \u2018w\u2019 for writing, \u2018a\u2019 to append. Available modes depend on the type of object specified by path. Default is \u2018r\u2019.", "Open text file with given encoding. The default encoding will be what io.open uses.", "Newline to use when reading text file.", "File object."]}, {"name": "Datetime Support Functions", "path": "reference/routines.datetime", "type": "Datetime Support Functions", "text": ["datetime_as_string(arr[, unit, timezone, ...])", "Convert an array of datetimes into an array of strings.", "datetime_data(dtype, /)", "Get information about the step size of a date or time type.", "busdaycalendar([weekmask, holidays])", "A business day calendar object that efficiently stores information defining valid days for the busday family of functions.", "is_busday(dates[, weekmask, holidays, ...])", "Calculates which of the given dates are valid days, and which are not.", "busday_offset(dates, offsets[, roll, ...])", "First adjusts the date to fall on a valid day according to the roll rule, then applies offsets to the given dates counted in valid days.", "busday_count(begindates, enddates[, ...])", "Counts the number of valid days between begindates and enddates, not including the day of enddates."]}, {"name": "Datetimes and Timedeltas", "path": "reference/arrays.datetime", "type": "Datetimes and Timedeltas", "text": ["New in version 1.7.0.", "Starting in NumPy 1.7, there are core array data types which natively support datetime functionality. The data type is called \u201cdatetime64\u201d, so named because \u201cdatetime\u201d is already taken by the datetime library included in Python.", "The most basic way to create datetimes is from strings in ISO 8601 date or datetime format. It is also possible to create datetimes from an integer by offset relative to the Unix epoch (00:00:00 UTC on 1 January 1970). The unit for internal storage is automatically selected from the form of the string, and can be either a date unit or a time unit. The date units are years (\u2018Y\u2019), months (\u2018M\u2019), weeks (\u2018W\u2019), and days (\u2018D\u2019), while the time units are hours (\u2018h\u2019), minutes (\u2018m\u2019), seconds (\u2018s\u2019), milliseconds (\u2018ms\u2019), and some additional SI-prefix seconds-based units. The datetime64 data type also accepts the string \u201cNAT\u201d, in any combination of lowercase/uppercase letters, for a \u201cNot A Time\u201d value.", "A simple ISO date:", "From an integer and a date unit, 1 year since the UNIX epoch:", "Using months for the unit:", "Specifying just the month, but forcing a \u2018days\u2019 unit:", "From a date and time:", "NAT (not a time):", "When creating an array of datetimes from a string, it is still possible to automatically select the unit from the inputs, by using the datetime type with generic units.", "An array of datetimes can be constructed from integers representing POSIX timestamps with the given unit.", "The datetime type works with many common NumPy functions, for example arange can be used to generate ranges of dates.", "All the dates for one month:", "The datetime object represents a single moment in time. If two datetimes have different units, they may still be representing the same moment of time, and converting from a bigger unit like months to a smaller unit like days is considered a \u2018safe\u2019 cast because the moment of time is still being represented exactly.", "Deprecated since version 1.11.0: NumPy does not store timezone information. For backwards compatibility, datetime64 still parses timezone offsets, which it handles by converting to UTC. This behaviour is deprecated and will raise an error in the future.", "NumPy allows the subtraction of two Datetime values, an operation which produces a number with a time unit. Because NumPy doesn\u2019t have a physical quantities system in its core, the timedelta64 data type was created to complement datetime64. The arguments for timedelta64 are a number, to represent the number of units, and a date/time unit, such as (D)ay, (M)onth, (Y)ear, (h)ours, (m)inutes, or (s)econds. The timedelta64 data type also accepts the string \u201cNAT\u201d in place of the number for a \u201cNot A Time\u201d value.", "Datetimes and Timedeltas work together to provide ways for simple datetime calculations.", "There are two Timedelta units (\u2018Y\u2019, years and \u2018M\u2019, months) which are treated specially, because how much time they represent changes depending on when they are used. While a timedelta day unit is equivalent to 24 hours, there is no way to convert a month unit into days, because different months have different numbers of days.", "The Datetime and Timedelta data types support a large number of time units, as well as generic units which can be coerced into any of the other units based on input data.", "Datetimes are always stored based on POSIX time (though having a TAI mode which allows for accounting of leap-seconds is proposed), with an epoch of 1970-01-01T00:00Z. This means the supported dates are always a symmetric interval around the epoch, called \u201ctime span\u201d in the table below.", "The length of the span is the range of a 64-bit integer times the length of the date or unit. For example, the time span for \u2018W\u2019 (week) is exactly 7 times longer than the time span for \u2018D\u2019 (day), and the time span for \u2018D\u2019 (day) is exactly 24 times longer than the time span for \u2018h\u2019 (hour).", "Here are the date units:", "Code", "Meaning", "Time span (relative)", "Time span (absolute)", "Y", "year", "+/- 9.2e18 years", "[9.2e18 BC, 9.2e18 AD]", "M", "month", "+/- 7.6e17 years", "[7.6e17 BC, 7.6e17 AD]", "W", "week", "+/- 1.7e17 years", "[1.7e17 BC, 1.7e17 AD]", "D", "day", "+/- 2.5e16 years", "[2.5e16 BC, 2.5e16 AD]", "And here are the time units:", "Code", "Meaning", "Time span (relative)", "Time span (absolute)", "h", "hour", "+/- 1.0e15 years", "[1.0e15 BC, 1.0e15 AD]", "m", "minute", "+/- 1.7e13 years", "[1.7e13 BC, 1.7e13 AD]", "s", "second", "+/- 2.9e11 years", "[2.9e11 BC, 2.9e11 AD]", "ms", "millisecond", "+/- 2.9e8 years", "[ 2.9e8 BC, 2.9e8 AD]", "us / \u03bcs", "microsecond", "+/- 2.9e5 years", "[290301 BC, 294241 AD]", "ns", "nanosecond", "+/- 292 years", "[ 1678 AD, 2262 AD]", "ps", "picosecond", "+/- 106 days", "[ 1969 AD, 1970 AD]", "fs", "femtosecond", "+/- 2.6 hours", "[ 1969 AD, 1970 AD]", "as", "attosecond", "+/- 9.2 seconds", "[ 1969 AD, 1970 AD]", "To allow the datetime to be used in contexts where only certain days of the week are valid, NumPy includes a set of \u201cbusday\u201d (business day) functions.", "The default for busday functions is that the only valid days are Monday through Friday (the usual business days). The implementation is based on a \u201cweekmask\u201d containing 7 Boolean flags to indicate valid days; custom weekmasks are possible that specify other sets of valid days.", "The \u201cbusday\u201d functions can additionally check a list of \u201choliday\u201d dates, specific dates that are not valid days.", "The function busday_offset allows you to apply offsets specified in business days to datetimes with a unit of \u2018D\u2019 (day).", "When an input date falls on the weekend or a holiday, busday_offset first applies a rule to roll the date to a valid business day, then applies the offset. The default rule is \u2018raise\u2019, which simply raises an exception. The rules most typically used are \u2018forward\u2019 and \u2018backward\u2019.", "In some cases, an appropriate use of the roll and the offset is necessary to get a desired answer.", "The first business day on or after a date:", "The first business day strictly after a date:", "The function is also useful for computing some kinds of days like holidays. In Canada and the U.S., Mother\u2019s day is on the second Sunday in May, which can be computed with a custom weekmask.", "When performance is important for manipulating many business dates with one particular choice of weekmask and holidays, there is an object busdaycalendar which stores the data necessary in an optimized form.", "To test a datetime64 value to see if it is a valid day, use is_busday.", "To find how many valid days there are in a specified range of datetime64 dates, use busday_count:", "If you have an array of datetime64 day values, and you want a count of how many of them are valid dates, you can do this:", "Here are several examples of custom weekmask values. These examples specify the \u201cbusday\u201d default of Monday through Friday being valid days.", "Some examples:"]}, {"name": "deletechars", "path": "user/basics.io.genfromtxt", "type": "User Guide", "text": ["NumPy provides several functions to create arrays from tabular data. We focus here on the genfromtxt function.", "In a nutshell, genfromtxt runs two main loops. The first loop converts each line of the file in a sequence of strings. The second loop converts each string to the appropriate data type. This mechanism is slower than a single loop, but gives more flexibility. In particular, genfromtxt is able to take missing data into account, when other faster and simpler functions like loadtxt cannot.", "Note", "When giving examples, we will use the following conventions:", "The only mandatory argument of genfromtxt is the source of the data. It can be a string, a list of strings, a generator or an open file-like object with a read method, for example, a file or io.StringIO object. If a single string is provided, it is assumed to be the name of a local or remote file. If a list of strings or a generator returning strings is provided, each string is treated as one line in a file. When the URL of a remote file is passed, the file is automatically downloaded to the current directory and opened.", "Recognized file types are text files and archives. Currently, the function recognizes gzip and bz2 (bzip2) archives. The type of the archive is determined from the extension of the file: if the filename ends with '.gz', a gzip archive is expected; if it ends with 'bz2', a bzip2 archive is assumed.", "Once the file is defined and open for reading, genfromtxt splits each non-empty line into a sequence of strings. Empty or commented lines are just skipped. The delimiter keyword is used to define how the splitting should take place.", "Quite often, a single character marks the separation between columns. For example, comma-separated files (CSV) use a comma (,) or a semicolon (;) as delimiter:", "Another common separator is \"\\t\", the tabulation character. However, we are not limited to a single character, any string will do. By default, genfromtxt assumes delimiter=None, meaning that the line is split along white spaces (including tabs) and that consecutive white spaces are considered as a single white space.", "Alternatively, we may be dealing with a fixed-width file, where columns are defined as a given number of characters. In that case, we need to set delimiter to a single integer (if all the columns have the same size) or to a sequence of integers (if columns can have different sizes):", "By default, when a line is decomposed into a series of strings, the individual entries are not stripped of leading nor trailing white spaces. This behavior can be overwritten by setting the optional argument autostrip to a value of True:", "The optional argument comments is used to define a character string that marks the beginning of a comment. By default, genfromtxt assumes comments='#'. The comment marker may occur anywhere on the line. Any character present after the comment marker(s) is simply ignored:", "New in version 1.7.0: When comments is set to None, no lines are treated as comments.", "Note", "There is one notable exception to this behavior: if the optional argument names=True, the first commented line will be examined for names.", "The presence of a header in the file can hinder data processing. In that case, we need to use the skip_header optional argument. The values of this argument must be an integer which corresponds to the number of lines to skip at the beginning of the file, before any other action is performed. Similarly, we can skip the last n lines of the file by using the skip_footer attribute and giving it a value of n:", "By default, skip_header=0 and skip_footer=0, meaning that no lines are skipped.", "In some cases, we are not interested in all the columns of the data but only a few of them. We can select which columns to import with the usecols argument. This argument accepts a single integer or a sequence of integers corresponding to the indices of the columns to import. Remember that by convention, the first column has an index of 0. Negative integers behave the same as regular Python negative indexes.", "For example, if we want to import only the first and the last columns, we can use usecols=(0, -1):", "If the columns have names, we can also select which columns to import by giving their name to the usecols argument, either as a sequence of strings or a comma-separated string:", "The main way to control how the sequences of strings we have read from the file are converted to other types is to set the dtype argument. Acceptable values for this argument are:", "In all the cases but the first one, the output will be a 1D array with a structured dtype. This dtype has as many fields as items in the sequence. The field names are defined with the names keyword.", "When dtype=None, the type of each column is determined iteratively from its data. We start by checking whether a string can be converted to a boolean (that is, if the string matches true or false in lower cases); then whether it can be converted to an integer, then to a float, then to a complex and eventually to a string. This behavior may be changed by modifying the default mapper of the StringConverter class.", "The option dtype=None is provided for convenience. However, it is significantly slower than setting the dtype explicitly.", "A natural approach when dealing with tabular data is to allocate a name to each column. A first possibility is to use an explicit structured dtype, as mentioned previously:", "Another simpler possibility is to use the names keyword with a sequence of strings or a comma-separated string:", "In the example above, we used the fact that by default, dtype=float. By giving a sequence of names, we are forcing the output to a structured dtype.", "We may sometimes need to define the column names from the data itself. In that case, we must use the names keyword with a value of True. The names will then be read from the first line (after the skip_header ones), even if the line is commented out:", "The default value of names is None. If we give any other value to the keyword, the new names will overwrite the field names we may have defined with the dtype:", "If names=None but a structured dtype is expected, names are defined with the standard NumPy default of \"f%i\", yielding names like f0, f1 and so forth:", "In the same way, if we don\u2019t give enough names to match the length of the dtype, the missing names will be defined with this default template:", "We can overwrite this default with the defaultfmt argument, that takes any format string:", "Note", "We need to keep in mind that defaultfmt is used only if some names are expected but not defined.", "NumPy arrays with a structured dtype can also be viewed as recarray, where a field can be accessed as if it were an attribute. For that reason, we may need to make sure that the field name doesn\u2019t contain any space or invalid character, or that it does not correspond to the name of a standard attribute (like size or shape), which would confuse the interpreter. genfromtxt accepts three optional arguments that provide a finer control on the names:", "Gives a string combining all the characters that must be deleted from the name. By default, invalid characters are ~!@#$%^&*()-=+~\\|]}[{';:\n/?.>,<.", "Gives a list of the names to exclude, such as return, file, print\u2026 If one of the input name is part of this list, an underscore character ('_') will be appended to it.", "Whether the names should be case-sensitive (case_sensitive=True), converted to upper case (case_sensitive=False or case_sensitive='upper') or to lower case (case_sensitive='lower').", "Usually, defining a dtype is sufficient to define how the sequence of strings must be converted. However, some additional control may sometimes be required. For example, we may want to make sure that a date in a format YYYY/MM/DD is converted to a datetime object, or that a string like xx% is properly converted to a float between 0 and 1. In such cases, we should define conversion functions with the converters arguments.", "The value of this argument is typically a dictionary with column indices or column names as keys and a conversion functions as values. These conversion functions can either be actual functions or lambda functions. In any case, they should accept only a string as input and output only a single element of the wanted type.", "In the following example, the second column is converted from as string representing a percentage to a float between 0 and 1:", "We need to keep in mind that by default, dtype=float. A float is therefore expected for the second column. However, the strings ' 2.3%' and ' 78.9%' cannot be converted to float and we end up having np.nan instead. Let\u2019s now use a converter:", "The same results can be obtained by using the name of the second column (\"p\") as key instead of its index (1):", "Converters can also be used to provide a default for missing entries. In the following example, the converter convert transforms a stripped string into the corresponding float or into -999 if the string is empty. We need to explicitly strip the string from white spaces as it is not done by default:", "Some entries may be missing in the dataset we are trying to import. In a previous example, we used a converter to transform an empty string into a float. However, user-defined converters may rapidly become cumbersome to manage.", "The genfromtxt function provides two other complementary mechanisms: the missing_values argument is used to recognize missing data and a second argument, filling_values, is used to process these missing data.", "By default, any empty string is marked as missing. We can also consider more complex strings, such as \"N/A\" or \"???\" to represent missing or invalid data. The missing_values argument accepts three kinds of values:", "This string will be used as the marker for missing data for all the columns", "In that case, each item is associated to a column, in order.", "Values of the dictionary are strings or sequence of strings. The corresponding keys can be column indices (integers) or column names (strings). In addition, the special key None can be used to define a default applicable to all columns.", "We know how to recognize missing data, but we still need to provide a value for these missing entries. By default, this value is determined from the expected dtype according to this table:", "Expected type", "Default", "bool", "False", "int", "-1", "float", "np.nan", "complex", "np.nan+0j", "string", "'???'", "We can get a finer control on the conversion of missing values with the filling_values optional argument. Like missing_values, this argument accepts different kind of values:", "This will be the default for all columns", "Each entry will be the default for the corresponding column", "Each key can be a column index or a column name, and the corresponding value should be a single object. We can use the special key None to define a default for all columns.", "In the following example, we suppose that the missing values are flagged with \"N/A\" in the first column and by \"???\" in the third column. We wish to transform these missing values to 0 if they occur in the first and second column, and to -999 if they occur in the last column:", "We may also want to keep track of the occurrence of missing data by constructing a boolean mask, with True entries where data was missing and False otherwise. To do that, we just have to set the optional argument usemask to True (the default is False). The output array will then be a MaskedArray.", "In addition to genfromtxt, the numpy.lib.npyio module provides several convenience functions derived from genfromtxt. These functions work the same way as the original, but they have different default values.", "Returns a standard numpy.recarray (if usemask=False) or a MaskedRecords array (if usemaske=True). The default dtype is dtype=None, meaning that the types of each column will be automatically determined.", "Like recfromtxt, but with a default delimiter=\",\"."]}, {"name": "Development workflow", "path": "dev/development_workflow", "type": "Development", "text": ["You already have your own forked copy of the NumPy repository, by following Create a NumPy fork, Make the local copy, you have configured git by following Git configuration, and have linked the upstream repository as explained in Linking your repository to the upstream repo.", "What is described below is a recommended workflow with Git.", "In short:", "When finished:", "This way of working helps to keep work well organized and the history as clear as possible.", "See also", "There are many online tutorials to help you learn git. For discussions of specific git workflows, see these discussions on linux git workflow, and ipython git workflow.", "First, fetch new commits from the upstream repository:", "Then, create a new branch based on the main branch of the upstream repository:", "Optional: Check which files have changed with git status (see git status). You\u2019ll see a listing like this one:", "To commit the staged files into the local copy of your repo, do git\ncommit. At this point, a text editor will open up to allow you to write a commit message. Read the commit message section to be sure that you are writing a properly formatted and sufficiently detailed commit message. After saving your message and closing the editor, your commit will be saved. For trivial commits, a short commit message can be passed in through the command line using the -m flag. For example, git commit -am \"ENH: Some message\".", "In some cases, you will see this form of the commit command: git commit\n-a. The extra -a flag automatically commits all modified files and removes all deleted files. This can save you some typing of numerous git\nadd commands; however, it can add unwanted changes to a commit if you\u2019re not careful. For more information, see why the -a flag? - and the helpful use-case description in the tangled working copy problem.", "Push the changes to your forked repo on github:", "For more information, see git push.", "Note", "Assuming you have followed the instructions in these pages, git will create a default link to your github repo called origin. In git >= 1.7 you can ensure that the link to origin is permanently set by using the --set-upstream option:", "From now on git will know that my-new-feature is related to the my-new-feature branch in your own github repo. Subsequent push calls are then simplified to the following:", "You have to use --set-upstream for each new branch that you create.", "It may be the case that while you were working on your edits, new commits have been added to upstream that affect your work. In this case, follow the Rebasing on main section of this document to apply those changes to your branch.", "Commit messages should be clear and follow a few basic rules. Example:", "Describing the motivation for a change, the nature of a bug for bug fixes or some details on what an enhancement does are also good to include in a commit message. Messages should be understandable without looking at the code changes. A commit message like MAINT: fixed another one is an example of what not to do; the reader has to go look for context elsewhere.", "Standard acronyms to start the commit message with are:", "If you plan a new feature or API change, it\u2019s wisest to first email the NumPy mailing list asking for comment. If you haven\u2019t heard back in a week, it\u2019s OK to ping the list again.", "When you feel your work is finished, you can create a pull request (PR). Github has a nice help page that outlines the process for filing pull requests.", "If your changes involve modifications to the API or addition/modification of a function, add a release note to the doc/release/upcoming_changes/ directory, following the instructions and format in the doc/release/upcoming_changes/README.rst file.", "We review pull requests as soon as we can, typically within a week. If you get no review comments within two weeks, feel free to ask for feedback by adding a comment on your PR (this will notify maintainers).", "If your PR is large or complicated, asking for input on the numpy-discussion mailing list may also be useful.", "This updates your feature branch with changes from the upstream NumPy github repo. If you do not absolutely need to do this, try to avoid doing it, except perhaps when you are finished. The first step will be to update the remote repository with new commits from upstream:", "Next, you need to update the feature branch:", "If you have made changes to files that have changed also upstream, this may generate merge conflicts that you need to resolve. See below for help in this case.", "Finally, remove the backup branch upon a successful rebase:", "Note", "Rebasing on main is preferred over merging upstream back to your branch. Using git merge and git pull is discouraged when working on feature branches.", "Sometimes, you mess up merges or rebases. Luckily, in Git it is relatively straightforward to recover from such mistakes.", "If you mess up during a rebase:", "If you notice you messed up after the rebase:", "If you forgot to make a backup branch:", "If you didn\u2019t actually mess up but there are merge conflicts, you need to resolve those. This can be one of the trickier things to get right. For a good description of how to do this, see this article on merging conflicts.", "Note", "Do this only for your own feature branches.", "There\u2019s an embarrassing typo in a commit you made? Or perhaps you made several false starts you would like the posterity not to see.", "This can be done via interactive rebasing.", "Suppose that the commit history looks like this:", "and 6ad92e5 is the last commit in the main branch. Suppose we want to make the following changes:", "We do as follows:", "This will open an editor with the following text in it:", "To achieve what we want, we will make the following changes to it:", "This means that (i) we want to edit the commit message for 13d7934, and (ii) collapse the last three commits into one. Now we save and quit the editor.", "Git will then immediately bring up an editor for editing the commit message. After revising it, we get the output:", "and the history looks now like this:", "If it went wrong, recovery is again possible as explained above.", "See also: https://stackoverflow.com/questions/2003505/how-do-i-delete-a-git-branch-locally-and-remotely", "If you want to work on some stuff with other people, where you are all committing into the same repository, or even the same branch, then just share it via github.", "First fork NumPy into your account, as from Create a NumPy fork.", "Then, go to your forked repository github page, say https://github.com/your-user-name/numpy", "Click on the \u2018Admin\u2019 button, and add anyone else to the repo as a collaborator:", "Now all those people can do:", "Remember that links starting with git@ use the ssh protocol and are read-write; links starting with git:// are read-only.", "Your collaborators can then commit directly into that repo with the usual:", "To see a graphical representation of the repository branches and commits:", "To see a linear list of commits for this branch:", "You can also look at the network graph visualizer for your github repo.", "Backporting is the process of copying new feature/fixes committed in numpy/main back to stable release branches. To do this you make a branch off the branch you are backporting to, cherry pick the commits you want from numpy/main, and then submit a pull request for the branch containing the backport.", "First, you need to make the branch you will work on. This needs to be based on the older version of NumPy (not main):", "Now you need to apply the changes from main to this branch using git cherry-pick:", "Push the new branch to your Github repository:", "Requires commit rights to the main NumPy repo.", "When you have a set of \u201cready\u201d changes in a feature branch ready for NumPy\u2019s main or maintenance branches, you can push them to upstream as follows:", "First, merge or rebase on the target branch.", "Only a few, unrelated commits then prefer rebasing:", "See Rebasing on main.", "If all of the commits are related, create a merge commit:", "Check that what you are going to push looks sensible:", "Push to upstream:", "Note", "It\u2019s usually a good idea to use the -n flag to git push to check first that you\u2019re about to push the changes you want to the place you want."]}, {"name": "distutils.ccompiler.CCompiler_compile()", "path": "reference/generated/numpy.distutils.ccompiler.ccompiler_compile", "type": "numpy.distutils.ccompiler.CCompiler_compile", "text": ["Compile one or more source files.", "Please refer to the Python distutils API reference for more details.", "A list of filenames", "Path to the output directory.", "A list of macro definitions.", "The directories to add to the default include file search path for this compilation only.", "Whether or not to output debug symbols in or alongside the object file(s).", "Extra pre- and post-arguments.", "A list of file names that all targets depend on.", "A list of object file names, one per source file sources.", "If compilation fails."]}, {"name": "distutils.ccompiler.CCompiler_customize()", "path": "reference/generated/numpy.distutils.ccompiler.ccompiler_customize", "type": "numpy.distutils.ccompiler.CCompiler_customize", "text": ["Do any platform-specific customization of a compiler instance.", "This method calls distutils.sysconfig.customize_compiler for platform-specific customization, as well as optionally remove a flag to suppress spurious warnings in case C++ code is being compiled.", "This parameter is not used for anything.", "Whether or not C++ has to be compiled. If so (True), the \"-Wstrict-prototypes\" option is removed to prevent spurious warnings. Default is False.", "All the default options used by distutils can be extracted with:"]}, {"name": "distutils.ccompiler.CCompiler_customize_cmd()", "path": "reference/generated/numpy.distutils.ccompiler.ccompiler_customize_cmd", "type": "numpy.distutils.ccompiler.CCompiler_customize_cmd", "text": ["Customize compiler using distutils command.", "An instance inheriting from distutils.cmd.Command.", "List of CCompiler commands (without 'set_') that should not be altered. Strings that are checked for are: ('include_dirs', 'define', 'undef', 'libraries', 'library_dirs',\n'rpath', 'link_objects')."]}, {"name": "distutils.ccompiler.CCompiler_cxx_compiler()", "path": "reference/generated/numpy.distutils.ccompiler.ccompiler_cxx_compiler", "type": "numpy.distutils.ccompiler.CCompiler_cxx_compiler", "text": ["Return the C++ compiler.", "The C++ compiler, as a CCompiler instance."]}, {"name": "distutils.ccompiler.CCompiler_find_executables()", "path": "reference/generated/numpy.distutils.ccompiler.ccompiler_find_executables", "type": "numpy.distutils.ccompiler.CCompiler_find_executables", "text": ["Does nothing here, but is called by the get_version method and can be overridden by subclasses. In particular it is redefined in the FCompiler class where more documentation can be found."]}, {"name": "distutils.ccompiler.CCompiler_get_version()", "path": "reference/generated/numpy.distutils.ccompiler.ccompiler_get_version", "type": "numpy.distutils.ccompiler.CCompiler_get_version", "text": ["Return compiler version, or None if compiler is not available.", "If True, force a new determination of the version, even if the compiler already has a version attribute. Default is False.", "The list of status values returned by the version look-up process for which a version string is returned. If the status value is not in ok_status, None is returned. Default is [0].", "Version string, in the format of distutils.version.LooseVersion."]}, {"name": "distutils.ccompiler.CCompiler_object_filenames()", "path": "reference/generated/numpy.distutils.ccompiler.ccompiler_object_filenames", "type": "numpy.distutils.ccompiler.CCompiler_object_filenames", "text": ["Return the name of the object files for the given source files.", "The list of paths to source files. Paths can be either relative or absolute, this is handled transparently.", "Whether to strip the directory from the returned paths. If True, the file name prepended by output_dir is returned. Default is False.", "If given, this path is prepended to the returned paths to the object files.", "The list of paths to the object files corresponding to the source files in source_filenames."]}, {"name": "distutils.ccompiler.CCompiler_show_customization()", "path": "reference/generated/numpy.distutils.ccompiler.ccompiler_show_customization", "type": "numpy.distutils.ccompiler.CCompiler_show_customization", "text": ["Print the compiler customizations to stdout.", "Printing is only done if the distutils log threshold is < 2."]}, {"name": "distutils.ccompiler.CCompiler_spawn()", "path": "reference/generated/numpy.distutils.ccompiler.ccompiler_spawn", "type": "numpy.distutils.ccompiler.CCompiler_spawn", "text": ["Execute a command in a sub-process.", "The command to execute.", "The text to add to the log file kept by numpy.distutils. If not given, display is equal to cmd.", "If the command failed, i.e. the exit status was not 0."]}, {"name": "distutils.ccompiler.gen_lib_options()", "path": "reference/generated/numpy.distutils.ccompiler.gen_lib_options", "type": "numpy.distutils.ccompiler.gen_lib_options", "text": []}, {"name": "distutils.ccompiler.new_compiler()", "path": "reference/generated/numpy.distutils.ccompiler.new_compiler", "type": "numpy.distutils.ccompiler.new_compiler", "text": []}, {"name": "distutils.ccompiler.replace_method()", "path": "reference/generated/numpy.distutils.ccompiler.replace_method", "type": "numpy.distutils.ccompiler.replace_method", "text": []}, {"name": "distutils.ccompiler.simple_version_match()", "path": "reference/generated/numpy.distutils.ccompiler.simple_version_match", "type": "numpy.distutils.ccompiler.simple_version_match", "text": ["Simple matching of version numbers, for use in CCompiler and FCompiler.", "A regular expression matching version numbers. Default is r'[-.\\d]+'.", "A regular expression matching patterns to skip. Default is '', in which case nothing is skipped.", "A regular expression matching the start of where to start looking for version numbers. Default is '', in which case searching is started at the beginning of the version string given to matcher.", "A function that is appropriate to use as the .version_match attribute of a CCompiler class. matcher takes a single parameter, a version string."]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.cache_flush()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.cache_flush", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.cache_flush", "text": ["method", "Force update the cache."]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.cc_normalize_flags()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.cc_normalize_flags", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.cc_normalize_flags", "text": ["method", "Remove the conflicts that caused due gathering implied features flags.", "flags should be sorted from the lowest to the highest interest."]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.conf_features", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.conf_features", "type": "NumPy.distutils.ccompiler_opt.ccompileropt.conf_features", "text": ["attribute"]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.conf_features_partial()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.conf_features_partial", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.conf_features_partial", "text": ["method", "Return a dictionary of supported CPU features by the platform, and accumulate the rest of undefined options in conf_features, the returned dict has same rules and notes in class attribute conf_features, also its override any options that been set in \u2018conf_features\u2019."]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.cpu_baseline_flags()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.cpu_baseline_flags", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.cpu_baseline_flags", "text": ["method", "Returns a list of final CPU baseline compiler flags"]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.cpu_baseline_names()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.cpu_baseline_names", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.cpu_baseline_names", "text": ["method", "return a list of final CPU baseline feature names"]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.cpu_dispatch_names()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.cpu_dispatch_names", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.cpu_dispatch_names", "text": ["method", "return a list of final CPU dispatch feature names"]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.dist_compile()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.dist_compile", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.dist_compile", "text": ["method", "Wrap CCompiler.compile()"]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.dist_info()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.dist_info", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.dist_info", "text": ["method", "Return a tuple containing info about (platform, compiler, extra_args), required by the abstract class \u2018_CCompiler\u2019 for discovering the platform environment. This is also used as a cache factor in order to detect any changes happening from outside."]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.dist_test()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.dist_test", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.dist_test", "text": ["method", "Return True if \u2018CCompiler.compile()\u2019 able to compile a source file with certain flags."]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.feature_ahead()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.feature_ahead", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_ahead", "text": ["method", "Return list of features in \u2018names\u2019 after remove any implied features and keep the origins.", "sequence of CPU feature names in uppercase."]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.feature_c_preprocessor()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.feature_c_preprocessor", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_c_preprocessor", "text": ["method", "Generate C preprocessor definitions and include headers of a CPU feature.", "CPU feature name in uppercase.", "if > 0, align the generated strings to the right depend on number of tabs."]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.feature_detect()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.feature_detect", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_detect", "text": ["method", "Return a list of CPU features that required to be detected sorted from the lowest to highest interest."]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.feature_get_til()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.feature_get_til", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_get_til", "text": ["method", "same as feature_implies_c() but stop collecting implied features when feature\u2019s option that provided through parameter \u2018keyisfalse\u2019 is False, also sorting the returned features."]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.feature_implies()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.feature_implies", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_implies", "text": ["method", "Return a set of CPU features that implied by \u2018names\u2019", "CPU feature name(s) in uppercase.", "if False(default) then the returned set will not contain any features from \u2018names\u2019. This case happens only when two features imply each other."]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.feature_implies_c()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.feature_implies_c", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_implies_c", "text": ["method", "same as feature_implies() but combining \u2018names\u2019"]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.feature_is_exist()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.feature_is_exist", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_is_exist", "text": ["method", "Returns True if a certain feature is exist and covered within _Config.conf_features.", "feature name in uppercase."]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.feature_names()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.feature_names", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_names", "text": ["method", "Returns a set of CPU feature names that supported by platform and the C compiler.", "Specify certain CPU features to test it against the C compiler. if None(default), it will test all current supported features. Note: feature names must be in upper-case.", "If None(default), default compiler flags for every CPU feature will be used during the test.", "A list of C macro definitions."]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.feature_sorted()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.feature_sorted", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_sorted", "text": ["method", "Sort a list of CPU features ordered by the lowest interest.", "sequence of supported feature names in uppercase.", "If true, the sorted features is reversed. (highest interest)"]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.feature_untied()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.feature_untied", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.feature_untied", "text": ["method", "same as \u2018feature_ahead()\u2019 but if both features implied each other and keep the highest interest.", "sequence of CPU feature names in uppercase."]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.generate_dispatch_header()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.generate_dispatch_header", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.generate_dispatch_header", "text": ["method", "Generate the dispatch header which contains the #definitions and headers for platform-specific instruction-sets for the enabled CPU baseline and dispatch-able features.", "Its highly recommended to take a look at the generated header also the generated source files via try_dispatch() in order to get the full picture."]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.is_cached()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.is_cached", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.is_cached", "text": ["method", "Returns True if the class loaded from the cache file"]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.parse_targets()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.parse_targets", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.parse_targets", "text": ["method", "Fetch and parse configuration statements that required for defining the targeted CPU features, statements should be declared in the top of source in between C comment and start with a special mark @targets.", "Configuration statements are sort of keywords representing CPU features names, group of statements and policies, combined together to determine the required optimization.", "the path of C source file."]}, {"name": "distutils.ccompiler_opt.CCompilerOpt.try_dispatch()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.try_dispatch", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.try_dispatch", "text": ["method", "Compile one or more dispatch-able sources and generates object files, also generates abstract C config headers and macros that used later for the final runtime dispatching process.", "The mechanism behind it is to takes each source file that specified in \u2018sources\u2019 and branching it into several files depend on special configuration statements that must be declared in the top of each source which contains targeted CPU features, then it compiles every branched source with the proper compiler flags.", "Must be a list of dispatch-able sources file paths, and configuration statements must be declared inside each file.", "Path of parent directory for the generated headers and wrapped sources. If None(default) the files will generated in-place.", "Distutils CCompiler instance to be used for compilation. If None (default), the provided instance during the initialization will be used instead.", "Arguments to pass on to the CCompiler.compile()", "Raises by CCompiler.compile() on compiling failure.", "Some errors during checking the sanity of configuration statements.", "See also", "Parsing the configuration statements of dispatch-able sources."]}, {"name": "distutils.ccompiler_opt.new_ccompiler_opt()", "path": "reference/generated/numpy.distutils.ccompiler_opt.new_ccompiler_opt", "type": "numpy.distutils.ccompiler_opt.new_ccompiler_opt", "text": ["Create a new instance of \u2018CCompilerOpt\u2019 and generate the dispatch header which contains the #definitions and headers of platform-specific instruction-sets for the enabled CPU baseline and dispatch-able features.", "path of the dispatch header"]}, {"name": "distutils.cpuinfo.cpu", "path": "reference/generated/numpy.distutils.cpuinfo.cpu", "type": "numpy.distutils.cpuinfo.cpu", "text": []}, {"name": "distutils.exec_command.exec_command()", "path": "reference/generated/numpy.distutils.exec_command.exec_command", "type": "numpy.distutils.exec_command.exec_command", "text": ["Return (status,output) of executed command.", "Deprecated since version 1.17: Use subprocess.Popen instead", "A concatenated string of executable and arguments.", "Before running command cd execute_in and after cd -.", "If True, execute sh -c command. Default None (True)", "If True use tee. Default None (True)", "Both stdout and stderr messages.", "On NT, DOS systems the returned status is correct for external commands. Wild cards will not work for non-posix systems or when use_shell=0."]}, {"name": "distutils.exec_command.filepath_from_subprocess_output()", "path": "reference/generated/numpy.distutils.exec_command.filepath_from_subprocess_output", "type": "numpy.distutils.exec_command.filepath_from_subprocess_output", "text": ["Convert bytes in the encoding used by a subprocess into a filesystem-appropriate str.", "Inherited from exec_command, and possibly incorrect."]}, {"name": "distutils.exec_command.find_executable()", "path": "reference/generated/numpy.distutils.exec_command.find_executable", "type": "numpy.distutils.exec_command.find_executable", "text": ["Return full path of a executable or None.", "Symbolic links are not followed."]}, {"name": "distutils.exec_command.forward_bytes_to_stdout()", "path": "reference/generated/numpy.distutils.exec_command.forward_bytes_to_stdout", "type": "numpy.distutils.exec_command.forward_bytes_to_stdout", "text": ["Forward bytes from a subprocess call to the console, without attempting to decode them.", "The assumption is that the subprocess call already returned bytes in a suitable encoding."]}, {"name": "distutils.exec_command.get_pythonexe()", "path": "reference/generated/numpy.distutils.exec_command.get_pythonexe", "type": "numpy.distutils.exec_command.get_pythonexe", "text": []}, {"name": "distutils.exec_command.temp_file_name()", "path": "reference/generated/numpy.distutils.exec_command.temp_file_name", "type": "numpy.distutils.exec_command.temp_file_name", "text": []}, {"name": "distutils.log.set_verbosity()", "path": "reference/generated/numpy.distutils.log.set_verbosity", "type": "numpy.distutils.log.set_verbosity", "text": []}, {"name": "distutils.system_info.get_info()", "path": "reference/generated/numpy.distutils.system_info.get_info", "type": "numpy.distutils.system_info.get_info", "text": ["0 - do nothing 1 - display warning message 2 - raise error"]}, {"name": "distutils.system_info.get_standard_file()", "path": "reference/generated/numpy.distutils.system_info.get_standard_file", "type": "numpy.distutils.system_info.get_standard_file", "text": ["Returns a list of files named \u2018fname\u2019 from 1) System-wide directory (directory-location of this module) 2) Users HOME directory (os.environ[\u2018HOME\u2019]) 3) Local directory"]}, {"name": "double npy_half_to_double()", "path": "reference/c-api/coremath#c.npy_half_to_double", "type": "NumPy core libraries", "text": ["Converts a half-precision float to a double-precision float."]}, {"name": "double npy_spacing()", "path": "reference/c-api/coremath#c.npy_spacing", "type": "NumPy core libraries", "text": ["This is a function equivalent to Fortran intrinsic. Return distance between x and next representable floating point value from x, e.g. spacing(1) == eps. spacing of nan and +/- inf return nan. Single and extended precisions are available with suffix f and l.", "New in version 1.4.0."]}, {"name": "double PyArray_GetPriority()", "path": "reference/c-api/array#c.PyArray_GetPriority", "type": "Array API", "text": ["Return the __array_priority__ attribute (converted to a double) of obj or def if no attribute of that name exists. Fast returns that avoid the attribute lookup are provided for objects of type PyArray_Type."]}, {"name": "double random_beta()", "path": "reference/random/c-api#c.random_beta", "type": "C API for random", "text": []}, {"name": "double random_chisquare()", "path": "reference/random/c-api#c.random_chisquare", "type": "C API for random", "text": []}, {"name": "double random_exponential()", "path": "reference/random/c-api#c.random_exponential", "type": "C API for random", "text": []}, {"name": "double random_f()", "path": "reference/random/c-api#c.random_f", "type": "C API for random", "text": []}, {"name": "double random_gamma()", "path": "reference/random/c-api#c.random_gamma", "type": "C API for random", "text": []}, {"name": "double random_gumbel()", "path": "reference/random/c-api#c.random_gumbel", "type": "C API for random", "text": []}, {"name": "double random_laplace()", "path": "reference/random/c-api#c.random_laplace", "type": "C API for random", "text": []}, {"name": "double random_logistic()", "path": "reference/random/c-api#c.random_logistic", "type": "C API for random", "text": []}, {"name": "double random_lognormal()", "path": "reference/random/c-api#c.random_lognormal", "type": "C API for random", "text": []}, {"name": "double random_noncentral_chisquare()", "path": "reference/random/c-api#c.random_noncentral_chisquare", "type": "C API for random", "text": []}, {"name": "double random_noncentral_f()", "path": "reference/random/c-api#c.random_noncentral_f", "type": "C API for random", "text": []}, {"name": "double random_normal()", "path": "reference/random/c-api#c.random_normal", "type": "C API for random", "text": []}, {"name": "double random_pareto()", "path": "reference/random/c-api#c.random_pareto", "type": "C API for random", "text": []}, {"name": "double random_power()", "path": "reference/random/c-api#c.random_power", "type": "C API for random", "text": []}, {"name": "double random_rayleigh()", "path": "reference/random/c-api#c.random_rayleigh", "type": "C API for random", "text": []}, {"name": "double random_standard_cauchy()", "path": "reference/random/c-api#c.random_standard_cauchy", "type": "C API for random", "text": []}, {"name": "double random_standard_exponential()", "path": "reference/random/c-api#c.random_standard_exponential", "type": "C API for random", "text": []}, {"name": "double random_standard_gamma()", "path": "reference/random/c-api#c.random_standard_gamma", "type": "C API for random", "text": []}, {"name": "double random_standard_normal()", "path": "reference/random/c-api#c.random_standard_normal", "type": "C API for random", "text": []}, {"name": "double random_standard_t()", "path": "reference/random/c-api#c.random_standard_t", "type": "C API for random", "text": []}, {"name": "double random_standard_uniform()", "path": "reference/random/c-api#c.random_standard_uniform", "type": "C API for random", "text": []}, {"name": "double random_triangular()", "path": "reference/random/c-api#c.random_triangular", "type": "C API for random", "text": []}, {"name": "double random_uniform()", "path": "reference/random/c-api#c.random_uniform", "type": "C API for random", "text": []}, {"name": "double random_vonmises()", "path": "reference/random/c-api#c.random_vonmises", "type": "C API for random", "text": []}, {"name": "double random_wald()", "path": "reference/random/c-api#c.random_wald", "type": "C API for random", "text": []}, {"name": "double random_weibull()", "path": "reference/random/c-api#c.random_weibull", "type": "C API for random", "text": []}, {"name": "DoxyLimbo()", "path": "dev/howto-docs#_CPPv4N9DoxyLimbo9DoxyLimboERK9DoxyLimboI2Tp1NE", "type": "Development", "text": ["Set Default behavior for copy the limbo. "]}, {"name": "dtype object", "path": "reference/arrays.dtypes", "type": "Data type objects ( \n      \n       dtype\n      \n      )", "text": ["A data type object (an instance of numpy.dtype class) describes how the bytes in the fixed-size block of memory corresponding to an array item should be interpreted. It describes the following aspects of the data:", "If the data type is structured data type, an aggregate of other data types, (e.g., describing an array item consisting of an integer and a float),", "To describe the type of scalar data, there are several built-in scalar types in NumPy for various precision of integers, floating-point numbers, etc. An item extracted from an array, e.g., by indexing, will be a Python object whose type is the scalar type associated with the data type of the array.", "Note that the scalar types are not dtype objects, even though they can be used in place of one whenever a data type specification is needed in NumPy.", "Structured data types are formed by creating a data type whose field contain other data types. Each field has a name by which it can be accessed. The parent data type should be of sufficient size to contain all its fields; the parent is nearly always based on the void type which allows an arbitrary item size. Structured data types may also contain nested structured sub-array data types in their fields.", "Finally, a data type can describe items that are themselves arrays of items of another data type. These sub-arrays must, however, be of a fixed size.", "If an array is created using a data-type describing a sub-array, the dimensions of the sub-array are appended to the shape of the array when the array is created. Sub-arrays in a field of a structured type behave differently, see Field access.", "Sub-arrays always have a C-contiguous memory layout.", "A simple data type containing a 32-bit big-endian integer: (see Specifying and constructing data types for details on construction)", "The corresponding array scalar type is int32.", "A structured data type containing a 16-character string (in field \u2018name\u2019) and a sub-array of two 64-bit floating-point number (in field \u2018grades\u2019):", "Items of an array of this data type are wrapped in an array scalar type that also has two fields:", "Whenever a data-type is required in a NumPy function or method, either a dtype object or something that can be converted to one can be supplied. Such conversions are done by the dtype constructor:", "dtype(dtype[, align, copy])", "Create a data type object.", "What can be converted to a data-type object is described below:", "Used as-is.", "The default data type: float_.", "The 24 built-in array scalar type objects all convert to an associated data-type object. This is true for their sub-classes as well.", "Note that not all data-type information can be supplied with a type-object: for example, flexible data-types have a default itemsize of 0, and require an explicitly given size to be useful.", "The generic hierarchical type objects convert to corresponding type objects according to the associations:", "number, inexact, floating", "float", "complexfloating", "cfloat", "integer, signedinteger", "int_", "unsignedinteger", "uint", "character", "string", "generic, flexible", "void", "Deprecated since version 1.19: This conversion of generic scalar types is deprecated. This is because it can be unexpected in a context such as arr.astype(dtype=np.floating), which casts an array of float32 to an array of float64, even though float32 is a subdtype of np.floating.", "Several python types are equivalent to a corresponding array scalar when used to generate a dtype object:", "int", "int_", "bool", "bool_", "float", "float_", "complex", "cfloat", "bytes", "bytes_", "str", "str_", "buffer", "void", "(all others)", "object_", "Note that str refers to either null terminated bytes or unicode strings depending on the Python version. In code targeting both Python 2 and 3 np.unicode_ should be used as a dtype for strings. See Note on string types.", "Note", "All other types map to object_ for convenience. Code should expect that such types may map to a specific (new) dtype in the future.", "Any type object with a dtype attribute: The attribute will be accessed and used directly. The attribute must return something that is convertible into a dtype object.", "Several kinds of strings can be converted. Recognized strings can be prepended with '>' (big-endian), '<' (little-endian), or '=' (hardware-native, the default), to specify the byte order.", "Each built-in data-type has a character code (the updated Numeric typecodes), that uniquely identifies it.", "The first character specifies the kind of data and the remaining characters specify the number of bytes per item, except for Unicode, where it is interpreted as the number of characters. The item size must correspond to an existing type, or an error will be raised. The supported kinds are", "'?'", "boolean", "'b'", "(signed) byte", "'B'", "unsigned byte", "'i'", "(signed) integer", "'u'", "unsigned integer", "'f'", "floating-point", "'c'", "complex-floating point", "'m'", "timedelta", "'M'", "datetime", "'O'", "(Python) objects", "'S', 'a'", "zero-terminated bytes (not recommended)", "'U'", "Unicode string", "'V'", "raw data (void)", "Note on string types", "For backward compatibility with Python 2 the S and a typestrings remain zero-terminated bytes and numpy.string_ continues to alias numpy.bytes_. To use actual strings in Python 3 use U or numpy.str_. For signed bytes that do not need zero-termination b or i1 can be used.", "A short-hand notation for specifying the format of a structured data type is a comma-separated string of basic formats.", "A basic format in this context is an optional shape specifier followed by an array-protocol type string. Parenthesis are required on the shape if it has more than one dimension. NumPy allows a modification on the format in that any string that can uniquely identify the type can be used to specify the data-type in a field. The generated data-type fields are named 'f0', 'f1', \u2026, 'f<N-1>' where N (>1) is the number of comma-separated basic formats in the string. If the optional shape specifier is provided, then the data-type for the corresponding field describes a sub-array.", "Any string in numpy.sctypeDict.keys():", "The first argument must be an object that is converted to a zero-sized flexible data-type object, the second argument is an integer providing the desired itemsize.", "The first argument is any object that can be converted into a fixed-size data-type object. The second argument is the desired shape of this type. If the shape parameter is 1, then the data-type object used to be equivalent to fixed dtype. This behaviour is deprecated since NumPy 1.17 and will raise an error in the future. If shape is a tuple, then the new dtype defines a sub-array of the given shape.", "obj should be a list of fields where each field is described by a tuple of length 2 or 3. (Equivalent to the descr item in the __array_interface__ attribute.)", "The first element, field_name, is the field name (if this is '' then a standard field name, 'f#', is assigned). The field name may also be a 2-tuple of strings where the first string is either a \u201ctitle\u201d (which may be any string or unicode string) or meta-data for the field which can be any object, and the second string is the \u201cname\u201d which must be a valid Python identifier.", "The second element, field_dtype, can be anything that can be interpreted as a data-type.", "The optional third element field_shape contains the shape if this field represents an array of the data-type in the second element. Note that a 3-tuple with a third argument equal to 1 is equivalent to a 2-tuple.", "This style does not accept align in the dtype constructor as it is assumed that all of the memory is accounted for by the array interface description.", "Data-type with fields big (big-endian 32-bit integer) and little (little-endian 32-bit integer):", "Data-type with fields R, G, B, A, each being an unsigned 8-bit integer:", "This style has two required and three optional keys. The names and formats keys are required. Their respective values are equal-length lists with the field names and the field formats. The field names must be strings and the field formats can be any object accepted by dtype constructor.", "When the optional keys offsets and titles are provided, their values must each be lists of the same length as the names and formats lists. The offsets value is a list of byte offsets (limited to ctypes.c_int) for each field, while the titles value is a list of titles for each field (None can be used if no title is desired for that field). The titles can be any object, but when a str object will add another entry to the fields dictionary keyed by the title and referencing the same field tuple which will contain the title as an additional tuple member.", "The itemsize key allows the total size of the dtype to be set, and must be an integer large enough so all the fields are within the dtype. If the dtype being constructed is aligned, the itemsize must also be divisible by the struct alignment. Total dtype itemsize is limited to ctypes.c_int.", "Data type with fields r, g, b, a, each being an 8-bit unsigned integer:", "Data type with fields r and b (with the given titles), both being 8-bit unsigned integers, the first at byte position 0 from the start of the field and the second at position 2:", "This usage is discouraged, because it is ambiguous with the other dict-based construction method. If you have a field called \u2018names\u2019 and a field called \u2018formats\u2019 there will be a conflict.", "This style allows passing in the fields attribute of a data-type object.", "obj should contain string or unicode keys that refer to (data-type, offset) or (data-type, offset, title) tuples.", "Data type containing field col1 (10-character string at byte position 0), col2 (32-bit float at byte position 10), and col3 (integers at byte position 14):", "In NumPy 1.7 and later, this form allows base_dtype to be interpreted as a structured dtype. Arrays created with this dtype will have underlying dtype base_dtype but will have fields and flags taken from new_dtype. This is useful for creating custom structured dtypes, as done in record arrays.", "This form also makes it possible to specify struct dtypes with overlapping fields, functioning like the \u2018union\u2019 type in C. This usage is discouraged, however, and the union mechanism is preferred.", "Both arguments must be convertible to data-type objects with the same total size.", "32-bit integer, whose first two bytes are interpreted as an integer via field real, and the following two bytes via field imag.", "32-bit integer, which is interpreted as consisting of a sub-array of shape (4,) containing 8-bit integers:", "32-bit integer, containing fields r, g, b, a that interpret the 4 bytes in the integer as four unsigned integers:", "NumPy data type descriptions are instances of the dtype class.", "The type of the data is described by the following dtype attributes:", "dtype.type", "dtype.kind", "A character code (one of 'biufcmMOSUV') identifying the general kind of data.", "dtype.char", "A unique character code for each of the 21 different built-in types.", "dtype.num", "A unique number for each of the 21 different built-in types.", "dtype.str", "The array-protocol typestring of this data-type object.", "Size of the data is in turn described by:", "dtype.name", "A bit-width name for this data-type.", "dtype.itemsize", "The element size of this data-type object.", "Endianness of this data:", "dtype.byteorder", "A character indicating the byte-order of this data-type object.", "Information about sub-data-types in a structured data type:", "dtype.fields", "Dictionary of named fields defined for this data type, or None.", "dtype.names", "Ordered list of field names, or None if there are no fields.", "For data types that describe sub-arrays:", "dtype.subdtype", "Tuple (item_dtype, shape) if this dtype describes a sub-array, and None otherwise.", "dtype.shape", "Shape tuple of the sub-array if this data type describes a sub-array, and () otherwise.", "Attributes providing additional information:", "dtype.hasobject", "Boolean indicating whether this dtype contains any reference-counted objects in any fields or sub-dtypes.", "dtype.flags", "Bit-flags describing how this data type is to be interpreted.", "dtype.isbuiltin", "Integer indicating how this dtype relates to the built-in dtypes.", "dtype.isnative", "Boolean indicating whether the byte order of this dtype is native to the platform.", "dtype.descr", "__array_interface__ description of the data-type.", "dtype.alignment", "The required alignment (bytes) of this data-type according to the compiler.", "dtype.base", "Returns dtype for the base element of the subarrays, regardless of their dimension or shape.", "Metadata attached by the user:", "dtype.metadata", "Either None or a readonly dictionary of metadata (mappingproxy).", "Data types have the following method for changing the byte order:", "dtype.newbyteorder([new_order])", "Return a new dtype with a different byte order.", "The following methods implement the pickle protocol:", "dtype.__reduce__", "Helper for pickle.", "dtype.__setstate__", "Utility method for typing:", "dtype.__class_getitem__(item, /)", "Return a parametrized wrapper around the dtype type.", "Comparison operations:", "dtype.__ge__(value, /)", "Return self>=value.", "dtype.__gt__(value, /)", "Return self>value.", "dtype.__le__(value, /)", "Return self<=value.", "dtype.__lt__(value, /)", "Return self<value."]}, {"name": "dtype.__class_getitem__()", "path": "reference/generated/numpy.dtype.__class_getitem__", "type": "numpy.dtype.__class_getitem__", "text": ["method", "Return a parametrized wrapper around the dtype type.", "New in version 1.22.", "A parametrized dtype type.", "See also", "Type hinting generics in standard collections.", "This method is only available for python 3.9 and later."]}, {"name": "dtype.__ge__()", "path": "reference/generated/numpy.dtype.__ge__", "type": "numpy.dtype.__ge__", "text": ["method", "Return self>=value."]}, {"name": "dtype.__gt__()", "path": "reference/generated/numpy.dtype.__gt__", "type": "numpy.dtype.__gt__", "text": ["method", "Return self>value."]}, {"name": "dtype.__le__()", "path": "reference/generated/numpy.dtype.__le__", "type": "numpy.dtype.__le__", "text": ["method", "Return self<=value."]}, {"name": "dtype.__lt__()", "path": "reference/generated/numpy.dtype.__lt__", "type": "numpy.dtype.__lt__", "text": ["method", "Return self<value."]}, {"name": "dtype.__reduce__()", "path": "reference/generated/numpy.dtype.__reduce__", "type": "numpy.dtype.__reduce__", "text": ["method", "Helper for pickle."]}, {"name": "dtype.__setstate__()", "path": "reference/generated/numpy.dtype.__setstate__", "type": "numpy.dtype.__setstate__", "text": ["method"]}, {"name": "dtype.alignment", "path": "reference/generated/numpy.dtype.alignment", "type": "numpy.dtype.alignment", "text": ["attribute", "The required alignment (bytes) of this data-type according to the compiler.", "More information is available in the C-API section of the manual."]}, {"name": "dtype.base", "path": "reference/generated/numpy.dtype.base", "type": "numpy.dtype.base", "text": ["attribute", "Returns dtype for the base element of the subarrays, regardless of their dimension or shape.", "See also"]}, {"name": "dtype.byteorder", "path": "reference/generated/numpy.dtype.byteorder", "type": "numpy.dtype.byteorder", "text": ["attribute", "A character indicating the byte-order of this data-type object.", "One of:", "\u2018=\u2019", "native", "\u2018<\u2019", "little-endian", "\u2018>\u2019", "big-endian", "\u2018|\u2019", "not applicable", "All built-in data-type objects have byteorder either \u2018=\u2019 or \u2018|\u2019."]}, {"name": "dtype.char", "path": "reference/generated/numpy.dtype.char", "type": "numpy.dtype.char", "text": ["attribute", "A unique character code for each of the 21 different built-in types."]}, {"name": "dtype.descr", "path": "reference/generated/numpy.dtype.descr", "type": "numpy.dtype.descr", "text": ["attribute", "__array_interface__ description of the data-type.", "The format is that required by the \u2018descr\u2019 key in the __array_interface__ attribute.", "Warning: This attribute exists specifically for __array_interface__, and passing it directly to np.dtype will not accurately reconstruct some dtypes (e.g., scalar and subarray dtypes)."]}, {"name": "dtype.fields", "path": "reference/generated/numpy.dtype.fields", "type": "numpy.dtype.fields", "text": ["attribute", "Dictionary of named fields defined for this data type, or None.", "The dictionary is indexed by keys that are the names of the fields. Each entry in the dictionary is a tuple fully describing the field:", "Offset is limited to C int, which is signed and usually 32 bits. If present, the optional title can be any object (if it is a string or unicode then it will also be a key in the fields dictionary, otherwise it\u2019s meta-data). Notice also that the first two elements of the tuple can be passed directly as arguments to the ndarray.getfield and ndarray.setfield methods.", "See also"]}, {"name": "dtype.flags", "path": "reference/generated/numpy.dtype.flags", "type": "numpy.dtype.flags", "text": ["attribute", "Bit-flags describing how this data type is to be interpreted.", "Bit-masks are in numpy.core.multiarray as the constants ITEM_HASOBJECT, LIST_PICKLE, ITEM_IS_POINTER, NEEDS_INIT, NEEDS_PYAPI, USE_GETITEM, USE_SETITEM. A full explanation of these flags is in C-API documentation; they are largely useful for user-defined data-types.", "The following example demonstrates that operations on this particular dtype requires Python C-API."]}, {"name": "dtype.hasobject", "path": "reference/generated/numpy.dtype.hasobject", "type": "numpy.dtype.hasobject", "text": ["attribute", "Boolean indicating whether this dtype contains any reference-counted objects in any fields or sub-dtypes.", "Recall that what is actually in the ndarray memory representing the Python object is the memory address of that object (a pointer). Special handling may be required, and this attribute is useful for distinguishing data types that may contain arbitrary Python objects and data-types that won\u2019t."]}, {"name": "dtype.isalignedstruct", "path": "reference/generated/numpy.dtype.isalignedstruct", "type": "Data type objects", "text": ["attribute", "Boolean indicating whether the dtype is a struct which maintains field alignment. This flag is sticky, so when combining multiple structs together, it is preserved and produces new dtypes which are also aligned."]}, {"name": "dtype.isbuiltin", "path": "reference/generated/numpy.dtype.isbuiltin", "type": "numpy.dtype.isbuiltin", "text": ["attribute", "Integer indicating how this dtype relates to the built-in dtypes.", "Read-only.", "0", "if this is a structured array type, with fields", "1", "if this is a dtype compiled into numpy (such as ints, floats etc)", "2", "if the dtype is for a user-defined numpy type A user-defined type uses the numpy C-API machinery to extend numpy to handle a new array type. See User-defined data-types in the NumPy manual."]}, {"name": "dtype.isnative", "path": "reference/generated/numpy.dtype.isnative", "type": "numpy.dtype.isnative", "text": ["attribute", "Boolean indicating whether the byte order of this dtype is native to the platform."]}, {"name": "dtype.itemsize", "path": "reference/generated/numpy.dtype.itemsize", "type": "numpy.dtype.itemsize", "text": ["attribute", "The element size of this data-type object.", "For 18 of the 21 types this number is fixed by the data-type. For the flexible data-types, this number can be anything."]}, {"name": "dtype.kind", "path": "reference/generated/numpy.dtype.kind", "type": "numpy.dtype.kind", "text": ["attribute", "A character code (one of \u2018biufcmMOSUV\u2019) identifying the general kind of data.", "b", "boolean", "i", "signed integer", "u", "unsigned integer", "f", "floating-point", "c", "complex floating-point", "m", "timedelta", "M", "datetime", "O", "object", "S", "(byte-)string", "U", "Unicode", "V", "void"]}, {"name": "dtype.metadata", "path": "reference/generated/numpy.dtype.metadata", "type": "numpy.dtype.metadata", "text": ["attribute", "Either None or a readonly dictionary of metadata (mappingproxy).", "The metadata field can be set using any dictionary at data-type creation. NumPy currently has no uniform approach to propagating metadata; although some array operations preserve it, there is no guarantee that others will.", "Warning", "Although used in certain projects, this feature was long undocumented and is not well supported. Some aspects of metadata propagation are expected to change in the future.", "Adding arrays with identical datatypes currently preserves the metadata:", "But if the arrays have different dtype metadata, the metadata may be dropped:"]}, {"name": "dtype.name", "path": "reference/generated/numpy.dtype.name", "type": "numpy.dtype.name", "text": ["attribute", "A bit-width name for this data-type.", "Un-sized flexible data-type objects do not have this attribute."]}, {"name": "dtype.names", "path": "reference/generated/numpy.dtype.names", "type": "numpy.dtype.names", "text": ["attribute", "Ordered list of field names, or None if there are no fields.", "The names are ordered according to increasing byte offset. This can be used, for example, to walk through all of the named fields in offset order."]}, {"name": "dtype.ndim", "path": "reference/generated/numpy.dtype.ndim", "type": "Data type objects", "text": ["attribute", "Number of dimensions of the sub-array if this data type describes a sub-array, and 0 otherwise.", "New in version 1.13.0."]}, {"name": "dtype.newbyteorder()", "path": "reference/generated/numpy.dtype.newbyteorder", "type": "numpy.dtype.newbyteorder", "text": ["method", "Return a new dtype with a different byte order.", "Changes are also made in all fields and sub-arrays of the data type.", "Byte order to force; a value from the byte order specifications below. The default value (\u2018S\u2019) results in swapping the current byte order. new_order codes can be any of:", "New dtype object with the given change to the byte order.", "Changes are also made in all fields and sub-arrays of the data type."]}, {"name": "dtype.num", "path": "reference/generated/numpy.dtype.num", "type": "numpy.dtype.num", "text": ["attribute", "A unique number for each of the 21 different built-in types.", "These are roughly ordered from least-to-most precision."]}, {"name": "dtype.shape", "path": "reference/generated/numpy.dtype.shape", "type": "numpy.dtype.shape", "text": ["attribute", "Shape tuple of the sub-array if this data type describes a sub-array, and () otherwise."]}, {"name": "dtype.str", "path": "reference/generated/numpy.dtype.str", "type": "numpy.dtype.str", "text": ["attribute", "The array-protocol typestring of this data-type object."]}, {"name": "dtype.subdtype", "path": "reference/generated/numpy.dtype.subdtype", "type": "numpy.dtype.subdtype", "text": ["attribute", "Tuple (item_dtype, shape) if this dtype describes a sub-array, and None otherwise.", "The shape is the fixed shape of the sub-array described by this data type, and item_dtype the data type of the array.", "If a field whose dtype object has this attribute is retrieved, then the extra dimensions implied by shape are tacked on to the end of the retrieved array.", "See also"]}, {"name": "dtype.type", "path": "reference/generated/numpy.dtype.type", "type": "numpy.dtype.type", "text": ["attribute"]}, {"name": "Elementary Function", "path": "reference/c-api/generalized-ufuncs", "type": "Generalized Universal Function API", "text": ["There is a general need for looping over not only functions on scalars but also over functions on vectors (or arrays). This concept is realized in NumPy by generalizing the universal functions (ufuncs). In regular ufuncs, the elementary function is limited to element-by-element operations, whereas the generalized version (gufuncs) supports \u201csub-array\u201d by \u201csub-array\u201d operations. The Perl vector library PDL provides a similar functionality and its terms are re-used in the following.", "Each generalized ufunc has information associated with it that states what the \u201ccore\u201d dimensionality of the inputs is, as well as the corresponding dimensionality of the outputs (the element-wise ufuncs have zero core dimensions). The list of the core dimensions for all arguments is called the \u201csignature\u201d of a ufunc. For example, the ufunc numpy.add has signature (),()->() defining two scalar inputs and one scalar output.", "Another example is the function inner1d(a, b) with a signature of (i),(i)->(). This applies the inner product along the last axis of each input, but keeps the remaining indices intact. For example, where a is of shape (3, 5, N) and b is of shape (5, N), this will return an output of shape (3,5). The underlying elementary function is called 3 * 5 times. In the signature, we specify one core dimension (i) for each input and zero core dimensions () for the output, since it takes two 1-d arrays and returns a scalar. By using the same name i, we specify that the two corresponding dimensions should be of the same size.", "The dimensions beyond the core dimensions are called \u201cloop\u201d dimensions. In the above example, this corresponds to (3, 5).", "The signature determines how the dimensions of each input/output array are split into core and loop dimensions:", "Typically, the size of all core dimensions in an output will be determined by the size of a core dimension with the same label in an input array. This is not a requirement, and it is possible to define a signature where a label comes up for the first time in an output, although some precautions must be taken when calling such a function. An example would be the function euclidean_pdist(a), with signature (n,d)->(p), that given an array of n d-dimensional vectors, computes all unique pairwise Euclidean distances among them. The output dimension p must therefore be equal to n * (n - 1) / 2, but it is the caller\u2019s responsibility to pass in an output array of the right size. If the size of a core dimension of an output cannot be determined from a passed in input or output array, an error will be raised.", "Note: Prior to NumPy 1.10.0, less strict checks were in place: missing core dimensions were created by prepending 1\u2019s to the shape as necessary, core dimensions with the same label were broadcast together, and undetermined dimensions were created with size 1.", "Each ufunc consists of an elementary function that performs the most basic operation on the smallest portion of array arguments (e.g. adding two numbers is the most basic operation in adding two arrays). The ufunc applies the elementary function multiple times on different parts of the arrays. The input/output of elementary functions can be vectors; e.g., the elementary function of inner1d takes two vectors as input.", "A signature is a string describing the input/output dimensions of the elementary function of a ufunc. See section below for more details.", "The dimensionality of each input/output of an elementary function is defined by its core dimensions (zero core dimensions correspond to a scalar input/output). The core dimensions are mapped to the last dimensions of the input/output arrays.", "A dimension name represents a core dimension in the signature. Different dimensions may share a name, indicating that they are of the same size.", "A dimension index is an integer representing a dimension name. It enumerates the dimension names according to the order of the first occurrence of each name in the signature.", "The signature defines \u201ccore\u201d dimensionality of input and output variables, and thereby also defines the contraction of the dimensions. The signature is represented by a string of the following format:", "The formal syntax of signatures is as follows:", "Notes:", "Here are some examples of signatures:", "name", "signature", "common usage", "add", "(),()->()", "binary ufunc", "sum1d", "(i)->()", "reduction", "inner1d", "(i),(i)->()", "vector-vector multiplication", "matmat", "(m,n),(n,p)->(m,p)", "matrix multiplication", "vecmat", "(n),(n,p)->(p)", "vector-matrix multiplication", "matvec", "(m,n),(n)->(m)", "matrix-vector multiplication", "matmul", "(m?,n),(n,p?)->(m?,p?)", "combination of the four above", "outer_inner", "(i,t),(j,t)->(i,j)", "inner over the last dimension, outer over the second to last, and loop/broadcast over the rest.", "cross1d", "(3),(3)->(3)", "cross product where the last dimension is frozen and must be 3", "The last is an instance of freezing a core dimension and can be used to improve ufunc performance", "The current interface remains unchanged, and PyUFunc_FromFuncAndData can still be used to implement (specialized) ufuncs, consisting of scalar elementary functions.", "One can use PyUFunc_FromFuncAndDataAndSignature to declare a more general ufunc. The argument list is the same as PyUFunc_FromFuncAndData, with an additional argument specifying the signature as C string.", "Furthermore, the callback function is of the same type as before, void (*foo)(char **args, intp *dimensions, intp *steps, void *func). When invoked, args is a list of length nargs containing the data of all input/output arguments. For a scalar elementary function, steps is also of length nargs, denoting the strides used for the arguments. dimensions is a pointer to a single integer defining the size of the axis to be looped over.", "For a non-trivial signature, dimensions will also contain the sizes of the core dimensions as well, starting at the second entry. Only one size is provided for each unique dimension name and the sizes are given according to the first occurrence of a dimension name in the signature.", "The first nargs elements of steps remain the same as for scalar ufuncs. The following elements contain the strides of all core dimensions for all arguments in order.", "For example, consider a ufunc with signature (i,j),(i)->(). In this case, args will contain three pointers to the data of the input/output arrays a, b, c. Furthermore, dimensions will be [N, I, J] to define the size of N of the loop and the sizes I and J for the core dimensions i and j. Finally, steps will be [a_N, b_N, c_N, a_i, a_j, b_i], containing all necessary strides."]}, {"name": "enum NPY_CASTING", "path": "reference/c-api/array#c.NPY_CASTING", "type": "Array API", "text": ["New in version 1.6.", "An enumeration type indicating how permissive data conversions should be. This is used by the iterator added in NumPy 1.6, and is intended to be used more broadly in a future version.", "Only allow identical types.", "Allow identical and casts involving byte swapping.", "Only allow casts which will not cause values to be rounded, truncated, or otherwise changed.", "Allow any safe casts, and casts between types of the same kind. For example, float64 -> float32 is permitted with this rule.", "Allow any cast, no matter what kind of data loss may occur."]}, {"name": "enum NPY_CLIPMODE", "path": "reference/c-api/array#c.NPY_CLIPMODE", "type": "Array API", "text": ["A variable type indicating the kind of clipping that should be applied in certain functions.", "The default for most operations, raises an exception if an index is out of bounds.", "Clips an index to the valid range if it is out of bounds.", "Wraps an index to the valid range if it is out of bounds."]}, {"name": "enum NPY_ORDER", "path": "reference/c-api/array#c.NPY_ORDER", "type": "Array API", "text": ["An enumeration type indicating the element order that an array should be interpreted in. When a brand new array is created, generally only NPY_CORDER and NPY_FORTRANORDER are used, whereas when one or more inputs are provided, the order can be based on them.", "Fortran order if all the inputs are Fortran, C otherwise.", "C order.", "Fortran order.", "An order as close to the order of the inputs as possible, even if the input is in neither C nor Fortran order."]}, {"name": "enum NPY_SCALARKIND", "path": "reference/c-api/array#c.NPY_SCALARKIND", "type": "Array API", "text": ["A special variable type indicating the number of \u201ckinds\u201d of scalars distinguished in determining scalar-coercion rules. This variable can take on the values:", "Defined to be the number of scalar kinds (not including NPY_NOSCALAR)."]}, {"name": "enum NPY_SEARCHSIDE", "path": "reference/c-api/array#c.NPY_SEARCHSIDE", "type": "Array API", "text": ["A variable type indicating whether the index returned should be that of the first suitable location (if NPY_SEARCHLEFT) or of the last (if NPY_SEARCHRIGHT)."]}, {"name": "enum NPY_SELECTKIND", "path": "reference/c-api/array#c.NPY_SELECTKIND", "type": "Array API", "text": ["A variable type indicating the selection algorithm being used."]}, {"name": "enumerator NPY_BOOL", "path": "reference/c-api/dtype#c.NPY_BOOL", "type": "Data Type API", "text": ["The enumeration value for the boolean type, stored as one byte. It may only be set to the values 0 and 1."]}, {"name": "enumerator NPY_BOOL_SCALAR", "path": "reference/c-api/array#c.NPY_SCALARKIND.NPY_BOOL_SCALAR", "type": "Array API", "text": []}, {"name": "enumerator NPY_BYTE", "path": "reference/c-api/dtype#c.NPY_BYTE", "type": "Data Type API", "text": []}, {"name": "enumerator NPY_CDOUBLE", "path": "reference/c-api/dtype#c.NPY_CDOUBLE", "type": "Data Type API", "text": []}, {"name": "enumerator NPY_CFLOAT", "path": "reference/c-api/dtype#c.NPY_CFLOAT", "type": "Data Type API", "text": []}, {"name": "enumerator NPY_CLIP", "path": "reference/c-api/array#c.NPY_CLIPMODE.NPY_CLIP", "type": "Array API", "text": ["Clips an index to the valid range if it is out of bounds."]}, {"name": "enumerator NPY_CLONGDOUBLE", "path": "reference/c-api/dtype#c.NPY_CLONGDOUBLE", "type": "Data Type API", "text": ["The enumeration value for a platform-specific complex floating point type which is made up of two NPY_LONGDOUBLE values."]}, {"name": "enumerator NPY_COMPLEX128", "path": "reference/c-api/dtype#c.NPY_COMPLEX128", "type": "Data Type API", "text": ["The enumeration value for a 128-bit/16-byte complex type made up of two NPY_DOUBLE values."]}, {"name": "enumerator NPY_COMPLEX64", "path": "reference/c-api/dtype#c.NPY_COMPLEX64", "type": "Data Type API", "text": ["The enumeration value for a 64-bit/8-byte complex type made up of two NPY_FLOAT values."]}, {"name": "enumerator NPY_COMPLEX_SCALAR", "path": "reference/c-api/array#c.NPY_SCALARKIND.NPY_COMPLEX_SCALAR", "type": "Array API", "text": []}, {"name": "enumerator NPY_CORDER", "path": "reference/c-api/array#c.NPY_ORDER.NPY_CORDER", "type": "Array API", "text": ["C order."]}, {"name": "enumerator NPY_DATETIME", "path": "reference/c-api/dtype#c.NPY_DATETIME", "type": "Data Type API", "text": ["The enumeration value for a data type which holds dates or datetimes with a precision based on selectable date or time units."]}, {"name": "enumerator NPY_DEFAULT_TYPE", "path": "reference/c-api/dtype#c.NPY_DEFAULT_TYPE", "type": "Data Type API", "text": ["The default type to use when no dtype is explicitly specified, for example when calling np.zero(shape). This is equivalent to NPY_DOUBLE."]}, {"name": "enumerator NPY_DOUBLE", "path": "reference/c-api/dtype#c.NPY_DOUBLE", "type": "Data Type API", "text": []}, {"name": "enumerator NPY_EQUIV_CASTING", "path": "reference/c-api/array#c.NPY_CASTING.NPY_EQUIV_CASTING", "type": "Array API", "text": ["Allow identical and casts involving byte swapping."]}, {"name": "enumerator NPY_FLOAT", "path": "reference/c-api/dtype#c.NPY_FLOAT", "type": "Data Type API", "text": []}, {"name": "enumerator NPY_FLOAT16", "path": "reference/c-api/dtype#c.NPY_FLOAT16", "type": "Data Type API", "text": ["The enumeration value for a 16-bit/2-byte IEEE 754-2008 compatible floating point type."]}, {"name": "enumerator NPY_FLOAT32", "path": "reference/c-api/dtype#c.NPY_FLOAT32", "type": "Data Type API", "text": ["The enumeration value for a 32-bit/4-byte IEEE 754 compatible floating point type."]}, {"name": "enumerator NPY_FLOAT64", "path": "reference/c-api/dtype#c.NPY_FLOAT64", "type": "Data Type API", "text": ["The enumeration value for a 64-bit/8-byte IEEE 754 compatible floating point type."]}, {"name": "enumerator NPY_FLOAT_SCALAR", "path": "reference/c-api/array#c.NPY_SCALARKIND.NPY_FLOAT_SCALAR", "type": "Array API", "text": []}, {"name": "enumerator NPY_FORTRANORDER", "path": "reference/c-api/array#c.NPY_ORDER.NPY_FORTRANORDER", "type": "Array API", "text": ["Fortran order."]}, {"name": "enumerator NPY_HALF", "path": "reference/c-api/dtype#c.NPY_HALF", "type": "Data Type API", "text": []}, {"name": "enumerator NPY_HEAPSORT", "path": "reference/c-api/array#c.NPY_SORTKIND.NPY_HEAPSORT", "type": "Array API", "text": []}, {"name": "enumerator NPY_INT", "path": "reference/c-api/dtype#c.NPY_INT", "type": "Data Type API", "text": []}, {"name": "enumerator NPY_INT16", "path": "reference/c-api/dtype#c.NPY_INT16", "type": "Data Type API", "text": ["The enumeration value for a 16-bit/2-byte signed integer."]}, {"name": "enumerator NPY_INT32", "path": "reference/c-api/dtype#c.NPY_INT32", "type": "Data Type API", "text": ["The enumeration value for a 32-bit/4-byte signed integer."]}, {"name": "enumerator NPY_INT64", "path": "reference/c-api/dtype#c.NPY_INT64", "type": "Data Type API", "text": ["The enumeration value for a 64-bit/8-byte signed integer."]}, {"name": "enumerator NPY_INT8", "path": "reference/c-api/dtype#c.NPY_INT8", "type": "Data Type API", "text": ["The enumeration value for an 8-bit/1-byte signed integer."]}, {"name": "enumerator NPY_INTNEG_SCALAR", "path": "reference/c-api/array#c.NPY_SCALARKIND.NPY_INTNEG_SCALAR", "type": "Array API", "text": []}, {"name": "enumerator NPY_INTP", "path": "reference/c-api/dtype#c.NPY_INTP", "type": "Data Type API", "text": ["The enumeration value for a signed integer type which is the same size as a (void *) pointer. This is the type used by all arrays of indices."]}, {"name": "enumerator NPY_INTPOS_SCALAR", "path": "reference/c-api/array#c.NPY_SCALARKIND.NPY_INTPOS_SCALAR", "type": "Array API", "text": []}, {"name": "enumerator NPY_KEEPORDER", "path": "reference/c-api/array#c.NPY_ORDER.NPY_KEEPORDER", "type": "Array API", "text": ["An order as close to the order of the inputs as possible, even if the input is in neither C nor Fortran order."]}, {"name": "enumerator NPY_LONG", "path": "reference/c-api/dtype#c.NPY_LONG", "type": "Data Type API", "text": ["Equivalent to either NPY_INT or NPY_LONGLONG, depending on the platform."]}, {"name": "enumerator NPY_LONGDOUBLE", "path": "reference/c-api/dtype#c.NPY_LONGDOUBLE", "type": "Data Type API", "text": ["The enumeration value for a platform-specific floating point type which is at least as large as NPY_DOUBLE, but larger on many platforms."]}, {"name": "enumerator NPY_LONGLONG", "path": "reference/c-api/dtype#c.NPY_LONGLONG", "type": "Data Type API", "text": []}, {"name": "enumerator NPY_MASK", "path": "reference/c-api/dtype#c.NPY_MASK", "type": "Data Type API", "text": ["The enumeration value of the type used for masks, such as with the NPY_ITER_ARRAYMASK iterator flag. This is equivalent to NPY_UINT8."]}, {"name": "enumerator NPY_MERGESORT", "path": "reference/c-api/array#c.NPY_SORTKIND.NPY_MERGESORT", "type": "Array API", "text": []}, {"name": "enumerator NPY_NSCALARKINDS", "path": "reference/c-api/array#c.NPY_SCALARKIND.NPY_NSCALARKINDS", "type": "Array API", "text": ["Defined to be the number of scalar kinds (not including NPY_NOSCALAR)."]}, {"name": "enumerator NPY_NSORTS", "path": "reference/c-api/array#c.NPY_SORTKIND.NPY_NSORTS", "type": "Array API", "text": ["Defined to be the number of sorts. It is fixed at three by the need for backwards compatibility, and consequently NPY_MERGESORT and NPY_STABLESORT are aliased to each other and may refer to one of several stable sorting algorithms depending on the data type."]}, {"name": "enumerator NPY_OBJECT", "path": "reference/c-api/dtype#c.NPY_OBJECT", "type": "Data Type API", "text": ["The enumeration value for references to arbitrary Python objects."]}, {"name": "enumerator NPY_OBJECT_SCALAR", "path": "reference/c-api/array#c.NPY_SCALARKIND.NPY_OBJECT_SCALAR", "type": "Array API", "text": []}, {"name": "enumerator NPY_SAFE_CASTING", "path": "reference/c-api/array#c.NPY_CASTING.NPY_SAFE_CASTING", "type": "Array API", "text": ["Only allow casts which will not cause values to be rounded, truncated, or otherwise changed."]}, {"name": "enumerator NPY_SAME_KIND_CASTING", "path": "reference/c-api/array#c.NPY_CASTING.NPY_SAME_KIND_CASTING", "type": "Array API", "text": ["Allow any safe casts, and casts between types of the same kind. For example, float64 -> float32 is permitted with this rule."]}, {"name": "enumerator NPY_SEARCHRIGHT", "path": "reference/c-api/array#c.NPY_SEARCHSIDE.NPY_SEARCHRIGHT", "type": "Array API", "text": []}, {"name": "enumerator NPY_SHORT", "path": "reference/c-api/dtype#c.NPY_SHORT", "type": "Data Type API", "text": []}, {"name": "enumerator NPY_STABLESORT", "path": "reference/c-api/array#c.NPY_SORTKIND.NPY_STABLESORT", "type": "Array API", "text": ["Used as an alias of NPY_MERGESORT and vica versa."]}, {"name": "enumerator NPY_STRING", "path": "reference/c-api/dtype#c.NPY_STRING", "type": "Data Type API", "text": ["The enumeration value for ASCII strings of a selectable size. The strings have a fixed maximum size within a given array."]}, {"name": "enumerator NPY_TIMEDELTA", "path": "reference/c-api/dtype#c.NPY_TIMEDELTA", "type": "Data Type API", "text": ["The enumeration value for a data type which holds lengths of times in integers of selectable date or time units."]}, {"name": "enumerator NPY_TYPES", "path": "reference/c-api/dtype", "type": "Data Type API", "text": ["The standard array can have 24 different data types (and has some support for adding your own types). These data types all have an enumerated type, an enumerated type-character, and a corresponding array scalar Python type object (placed in a hierarchy). There are also standard C typedefs to make it easier to manipulate elements of the given data type. For the numeric types, there are also bit-width equivalent C typedefs and named typenumbers that make it easier to select the precision desired.", "Warning", "The names for the types in c code follows c naming conventions more closely. The Python names for these types follow Python conventions. Thus, NPY_FLOAT picks up a 32-bit float in C, but numpy.float_ in Python corresponds to a 64-bit double. The bit-width names can be used in both Python and C for clarity.", "There is a list of enumerated types defined providing the basic 24 data types plus some useful generic names. Whenever the code requires a type number, one of these enumerated types is requested. The types are all called NPY_{NAME}:", "The enumeration value for the boolean type, stored as one byte. It may only be set to the values 0 and 1.", "The enumeration value for an 8-bit/1-byte signed integer.", "The enumeration value for a 16-bit/2-byte signed integer.", "The enumeration value for a 32-bit/4-byte signed integer.", "Equivalent to either NPY_INT or NPY_LONGLONG, depending on the platform.", "The enumeration value for a 64-bit/8-byte signed integer.", "The enumeration value for an 8-bit/1-byte unsigned integer.", "The enumeration value for a 16-bit/2-byte unsigned integer.", "The enumeration value for a 32-bit/4-byte unsigned integer.", "Equivalent to either NPY_UINT or NPY_ULONGLONG, depending on the platform.", "The enumeration value for a 64-bit/8-byte unsigned integer.", "The enumeration value for a 16-bit/2-byte IEEE 754-2008 compatible floating point type.", "The enumeration value for a 32-bit/4-byte IEEE 754 compatible floating point type.", "The enumeration value for a 64-bit/8-byte IEEE 754 compatible floating point type.", "The enumeration value for a platform-specific floating point type which is at least as large as NPY_DOUBLE, but larger on many platforms.", "The enumeration value for a 64-bit/8-byte complex type made up of two NPY_FLOAT values.", "The enumeration value for a 128-bit/16-byte complex type made up of two NPY_DOUBLE values.", "The enumeration value for a platform-specific complex floating point type which is made up of two NPY_LONGDOUBLE values.", "The enumeration value for a data type which holds dates or datetimes with a precision based on selectable date or time units.", "The enumeration value for a data type which holds lengths of times in integers of selectable date or time units.", "The enumeration value for ASCII strings of a selectable size. The strings have a fixed maximum size within a given array.", "The enumeration value for UCS4 strings of a selectable size. The strings have a fixed maximum size within a given array.", "The enumeration value for references to arbitrary Python objects.", "Primarily used to hold struct dtypes, but can contain arbitrary binary data.", "Some useful aliases of the above types are", "The enumeration value for a signed integer type which is the same size as a (void *) pointer. This is the type used by all arrays of indices.", "The enumeration value for an unsigned integer type which is the same size as a (void *) pointer.", "The enumeration value of the type used for masks, such as with the NPY_ITER_ARRAYMASK iterator flag. This is equivalent to NPY_UINT8.", "The default type to use when no dtype is explicitly specified, for example when calling np.zero(shape). This is equivalent to NPY_DOUBLE.", "Other useful related constants are", "The total number of built-in NumPy types. The enumeration covers the range from 0 to NPY_NTYPES-1.", "A signal value guaranteed not to be a valid type enumeration number.", "The start of type numbers used for Custom Data types.", "The various character codes indicating certain types are also part of an enumerated list. References to type characters (should they be needed at all) should always use these enumerations. The form of them is NPY_{NAME}LTR where {NAME} can be", "BOOL, BYTE, UBYTE, SHORT, USHORT, INT, UINT, LONG, ULONG, LONGLONG, ULONGLONG, HALF, FLOAT, DOUBLE, LONGDOUBLE, CFLOAT, CDOUBLE, CLONGDOUBLE, DATETIME, TIMEDELTA, OBJECT, STRING, VOID", "INTP, UINTP", "GENBOOL, SIGNED, UNSIGNED, FLOATING, COMPLEX", "The latter group of {NAME}s corresponds to letters used in the array interface typestring specification.", "These are defined for {bits} = 8, 16, 32, 64, 128, and 256 and provide the maximum (minimum) value of the corresponding (unsigned) integer type. Note: the actual integer type may not be available on all platforms (i.e. 128-bit and 256-bit integers are rare).", "This is defined for {type} = BYTE, SHORT, INT, LONG, LONGLONG, INTP", "This is defined for all defined for {type} = BYTE, UBYTE, SHORT, USHORT, INT, UINT, LONG, ULONG, LONGLONG, ULONGLONG, INTP, UINTP", "All NPY_SIZEOF_{CTYPE} constants have corresponding NPY_BITSOF_{CTYPE} constants defined. The NPY_BITSOF_{CTYPE} constants provide the number of bits in the data type. Specifically, the available {CTYPE}s are", "BOOL, CHAR, SHORT, INT, LONG, LONGLONG, FLOAT, DOUBLE, LONGDOUBLE", "All of the numeric data types (integer, floating point, and complex) have constants that are defined to be a specific enumerated type number. Exactly which enumerated type a bit-width type refers to is platform dependent. In particular, the constants available are PyArray_{NAME}{BITS} where {NAME} is INT, UINT, FLOAT, COMPLEX and {BITS} can be 8, 16, 32, 64, 80, 96, 128, 160, 192, 256, and 512. Obviously not all bit-widths are available on all platforms for all the kinds of numeric types. Commonly 8-, 16-, 32-, 64-bit integers; 32-, 64-bit floats; and 64-, 128-bit complex types are available.", "The constants NPY_INTP and NPY_UINTP refer to an enumerated integer type that is large enough to hold a pointer on the platform. Index arrays should always be converted to NPY_INTP , because the dimension of the array is of type npy_intp.", "There are standard variable types for each of the numeric data types and the bool data type. Some of these are already available in the C-specification. You can create variables in extension code with these types.", "unsigned char; The constants NPY_FALSE and NPY_TRUE are also defined.", "Unsigned versions of the integers can be defined by pre-pending a \u2018u\u2019 to the front of the integer name.", "char", "unsigned char", "short", "unsigned short", "int", "unsigned int", "16-bit integer", "16-bit unsigned integer", "32-bit integer", "32-bit unsigned integer", "64-bit integer", "64-bit unsigned integer", "long int", "unsigned long int", "long long int", "unsigned long long int", "Py_intptr_t (an integer that is the size of a pointer on the platform).", "unsigned Py_intptr_t (an integer that is the size of a pointer on the platform).", "16-bit float", "32-bit float", "32-bit complex float", "64-bit double", "64-bit complex double", "long double", "long complex double", "complex types are structures with .real and .imag members (in that order).", "There are also typedefs for signed integers, unsigned integers, floating point, and complex floating point types of specific bit- widths. The available type names are", "npy_int{bits}, npy_uint{bits}, npy_float{bits}, and npy_complex{bits}", "where {bits} is the number of bits in the type and can be 8, 16, 32, 64, 128, and 256 for integer types; 16, 32 , 64, 80, 96, 128, and 256 for floating-point types; and 32, 64, 128, 160, 192, and 512 for complex-valued types. Which bit-widths are available is platform dependent. The bolded bit-widths are usually available on all platforms.", "For help in printing, the following strings are defined as the correct format specifier in printf and related commands."]}, {"name": "enumerator NPY_UBYTE", "path": "reference/c-api/dtype#c.NPY_UBYTE", "type": "Data Type API", "text": []}, {"name": "enumerator NPY_UINT", "path": "reference/c-api/dtype#c.NPY_UINT", "type": "Data Type API", "text": []}, {"name": "enumerator NPY_UINT16", "path": "reference/c-api/dtype#c.NPY_UINT16", "type": "Data Type API", "text": ["The enumeration value for a 16-bit/2-byte unsigned integer."]}, {"name": "enumerator NPY_UINT32", "path": "reference/c-api/dtype#c.NPY_UINT32", "type": "Data Type API", "text": ["The enumeration value for a 32-bit/4-byte unsigned integer."]}, {"name": "enumerator NPY_UINT64", "path": "reference/c-api/dtype#c.NPY_UINT64", "type": "Data Type API", "text": ["The enumeration value for a 64-bit/8-byte unsigned integer."]}, {"name": "enumerator NPY_UINT8", "path": "reference/c-api/dtype#c.NPY_UINT8", "type": "Data Type API", "text": ["The enumeration value for an 8-bit/1-byte unsigned integer."]}, {"name": "enumerator NPY_UINTP", "path": "reference/c-api/dtype#c.NPY_UINTP", "type": "Data Type API", "text": ["The enumeration value for an unsigned integer type which is the same size as a (void *) pointer."]}, {"name": "enumerator NPY_ULONG", "path": "reference/c-api/dtype#c.NPY_ULONG", "type": "Data Type API", "text": ["Equivalent to either NPY_UINT or NPY_ULONGLONG, depending on the platform."]}, {"name": "enumerator NPY_ULONGLONG", "path": "reference/c-api/dtype#c.NPY_ULONGLONG", "type": "Data Type API", "text": []}, {"name": "enumerator NPY_UNICODE", "path": "reference/c-api/dtype#c.NPY_UNICODE", "type": "Data Type API", "text": ["The enumeration value for UCS4 strings of a selectable size. The strings have a fixed maximum size within a given array."]}, {"name": "enumerator NPY_UNSAFE_CASTING", "path": "reference/c-api/array#c.NPY_CASTING.NPY_UNSAFE_CASTING", "type": "Array API", "text": ["Allow any cast, no matter what kind of data loss may occur."]}, {"name": "enumerator NPY_USHORT", "path": "reference/c-api/dtype#c.NPY_USHORT", "type": "Data Type API", "text": []}, {"name": "enumerator NPY_VOID", "path": "reference/c-api/dtype#c.NPY_VOID", "type": "Data Type API", "text": ["Primarily used to hold struct dtypes, but can contain arbitrary binary data."]}, {"name": "enumerator NPY_WRAP", "path": "reference/c-api/array#c.NPY_CLIPMODE.NPY_WRAP", "type": "Array API", "text": ["Wraps an index to the valid range if it is out of bounds."]}, {"name": "errstate.__call__()", "path": "reference/generated/numpy.errstate.__call__", "type": "numpy.errstate.__call__", "text": ["method", "Call self as a function."]}, {"name": "exec_command", "path": "reference/generated/numpy.distutils.exec_command", "type": "numpy.distutils.exec_command", "text": ["exec_command", "Implements exec_command function that is (almost) equivalent to commands.getstatusoutput function but on NT, DOS systems the returned status is actually correct (though, the returned status values may be different by a factor). In addition, exec_command takes keyword arguments for (re-)defining environment variables.", "Provides functions:", "in the modified environment.", "variable PATH. Equivalent to posix which command.", "Author: Pearu Peterson <pearu@cens.ioc.ee> Created: 11 January 2003", "Requires: Python 2.x", "Successfully tested on:", "os.name", "sys.platform", "comments", "posix", "linux2", "Debian (sid) Linux, Python 2.1.3+, 2.2.3+, 2.3.3 PyCrust 0.9.3, Idle 1.0.2", "posix", "linux2", "Red Hat 9 Linux, Python 2.1.3, 2.2.2, 2.3.2", "posix", "sunos5", "SunOS 5.9, Python 2.2, 2.3.2", "posix", "darwin", "Darwin 7.2.0, Python 2.3", "nt", "win32", "Windows Me Python 2.3(EE), Idle 1.0, PyCrust 0.7.2 Python 2.1.1 Idle 0.8", "nt", "win32", "Windows 98, Python 2.1.1. Idle 0.8", "nt", "win32", "Cygwin 98-4.10, Python 2.1.1(MSC) - echo tests fail i.e. redefining environment variables may not work. FIXED: don\u2019t use cygwin echo! Comment: also cmd /c echo will not work but redefining environment variables do work.", "posix", "cygwin", "Cygwin 98-4.10, Python 2.3.3(cygming special)", "nt", "win32", "Windows XP, Python 2.3.3", "Known bugs:", "exec_command(command[, execute_in, ...])", "Return (status,output) of executed command.", "filepath_from_subprocess_output(output)", "Convert bytes in the encoding used by a subprocess into a filesystem-appropriate str.", "find_executable(exe[, path, _cache])", "Return full path of a executable or None.", "forward_bytes_to_stdout(val)", "Forward bytes from a subprocess call to the console, without attempting to decode them.", "get_pythonexe()", "temp_file_name()"]}, {"name": "Extending", "path": "reference/random/extending", "type": "Examples of using Numba, Cython, CFFI", "text": ["The BitGenerators have been designed to be extendable using standard tools for high-performance Python \u2013 numba and Cython. The Generator object can also be used with user-provided BitGenerators as long as these export a small set of required functions.", "Numba can be used with either CTypes or CFFI. The current iteration of the BitGenerators all export a small set of functions through both interfaces.", "This example shows how numba can be used to produce gaussian samples using a pure Python implementation which is then compiled. The random numbers are provided by ctypes.next_double.", "Both CTypes and CFFI allow the more complicated distributions to be used directly in Numba after compiling the file distributions.c into a DLL or so. An example showing the use of a more complicated distribution is in the examples section below.", "Cython can be used to unpack the PyCapsule provided by a BitGenerator. This example uses PCG64 and the example from above. The usual caveats for writing high-performance code using Cython \u2013 removing bounds checks and wrap around, providing array alignment information \u2013 still apply.", "The BitGenerator can also be directly accessed using the members of the bitgen_t struct.", "Cython can be used to directly access the functions in numpy/random/c_distributions.pxd. This requires linking with the npyrandom library located in numpy/random/lib.", "See Extending numpy.random via Cython for the complete listings of these examples and a minimal setup.py to build the c-extension modules.", "CFFI can be used to directly access the functions in include/numpy/random/distributions.h. Some \u201cmassaging\u201d of the header file is required:", "Once the header is parsed by ffi.cdef, the functions can be accessed directly from the _generator shared object, using the BitGenerator.cffi interface.", "Generator can be used with user-provided BitGenerators. The simplest way to write a new BitGenerator is to examine the pyx file of one of the existing BitGenerators. The key structure that must be provided is the capsule which contains a PyCapsule to a struct pointer of type bitgen_t,", "which provides 5 pointers. The first is an opaque pointer to the data structure used by the BitGenerators. The next three are function pointers which return the next 64- and 32-bit unsigned integers, the next random double and the next raw value. This final function is used for testing and so can be set to the next 64-bit unsigned integer function if not needed. Functions inside Generator use this structure as in"]}, {"name": "Extending numpy.random via Cython", "path": "reference/random/examples/cython/index", "type": "Cython", "text": []}, {"name": "Extending via CFFI", "path": "reference/random/examples/cffi", "type": "CFFI", "text": []}, {"name": "Extending via Numba", "path": "reference/random/examples/numba", "type": "Numba", "text": []}, {"name": "Extending via Numba and CFFI", "path": "reference/random/examples/numba_cffi", "type": "CFFI + Numba", "text": []}, {"name": "extending.pyx", "path": "reference/random/examples/cython/extending.pyx", "type": "Cython", "text": []}, {"name": "extending_distributions.pyx", "path": "reference/random/examples/cython/extending_distributions.pyx", "type": "Cython", "text": []}, {"name": "F2PY user guide and reference manual", "path": "f2py/index", "type": "F2PY user guide and reference manual", "text": ["The purpose of the F2PY \u2013Fortran to Python interface generator\u2013 utility is to provide a connection between Python and Fortran languages. F2PY is a part of NumPy (numpy.f2py) and also available as a standalone command line tool f2py when numpy is installed that facilitates creating/building Python C/API extension modules that make it possible", "from Python."]}, {"name": "fft.fft()", "path": "reference/generated/numpy.fft.fft", "type": "numpy.fft.fft", "text": ["Compute the one-dimensional discrete Fourier Transform.", "This function computes the one-dimensional n-point discrete Fourier Transform (DFT) with the efficient Fast Fourier Transform (FFT) algorithm [CT].", "Input array, can be complex.", "Length of the transformed axis of the output. If n is smaller than the length of the input, the input is cropped. If it is larger, the input is padded with zeros. If n is not given, the length of the input along the axis specified by axis is used.", "Axis over which to compute the FFT. If not given, the last axis is used.", "New in version 1.10.0.", "Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.", "New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.", "The truncated or zero-padded input, transformed along the axis indicated by axis, or the last one if axis is not specified.", "If axis is not a valid axis of a.", "See also", "for definition of the DFT and conventions used.", "The inverse of fft.", "The two-dimensional FFT.", "The n-dimensional FFT.", "The n-dimensional FFT of real input.", "Frequency bins for given FFT parameters.", "FFT (Fast Fourier Transform) refers to a way the discrete Fourier Transform (DFT) can be calculated efficiently, by using symmetries in the calculated terms. The symmetry is highest when n is a power of 2, and the transform is therefore most efficient for these sizes.", "The DFT is defined, with the conventions used in this implementation, in the documentation for the numpy.fft module.", "Cooley, James W., and John W. Tukey, 1965, \u201cAn algorithm for the machine calculation of complex Fourier series,\u201d Math. Comput. 19: 297-301.", "In this example, real input has an FFT which is Hermitian, i.e., symmetric in the real part and anti-symmetric in the imaginary part, as described in the numpy.fft documentation:"]}, {"name": "fft.fft2()", "path": "reference/generated/numpy.fft.fft2", "type": "numpy.fft.fft2", "text": ["Compute the 2-dimensional discrete Fourier Transform.", "This function computes the n-dimensional discrete Fourier Transform over any axes in an M-dimensional array by means of the Fast Fourier Transform (FFT). By default, the transform is computed over the last two axes of the input array, i.e., a 2-dimensional FFT.", "Input array, can be complex", "Shape (length of each transformed axis) of the output (s[0] refers to axis 0, s[1] to axis 1, etc.). This corresponds to n for fft(x, n). Along each axis, if the given shape is smaller than that of the input, the input is cropped. If it is larger, the input is padded with zeros. if s is not given, the shape of the input along the axes specified by axes is used.", "Axes over which to compute the FFT. If not given, the last two axes are used. A repeated index in axes means the transform over that axis is performed multiple times. A one-element sequence means that a one-dimensional FFT is performed.", "New in version 1.10.0.", "Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.", "New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.", "The truncated or zero-padded input, transformed along the axes indicated by axes, or the last two axes if axes is not given.", "If s and axes have different length, or axes not given and len(s) != 2.", "If an element of axes is larger than than the number of axes of a.", "See also", "Overall view of discrete Fourier transforms, with definitions and conventions used.", "The inverse two-dimensional FFT.", "The one-dimensional FFT.", "The n-dimensional FFT.", "Shifts zero-frequency terms to the center of the array. For two-dimensional input, swaps first and third quadrants, and second and fourth quadrants.", "fft2 is just fftn with a different default for axes.", "The output, analogously to fft, contains the term for zero frequency in the low-order corner of the transformed axes, the positive frequency terms in the first half of these axes, the term for the Nyquist frequency in the middle of the axes and the negative frequency terms in the second half of the axes, in order of decreasingly negative frequency.", "See fftn for details and a plotting example, and numpy.fft for definitions and conventions used."]}, {"name": "fft.fftfreq()", "path": "reference/generated/numpy.fft.fftfreq", "type": "numpy.fft.fftfreq", "text": ["Return the Discrete Fourier Transform sample frequencies.", "The returned float array f contains the frequency bin centers in cycles per unit of the sample spacing (with zero at the start). For instance, if the sample spacing is in seconds, then the frequency unit is cycles/second.", "Given a window length n and a sample spacing d:", "Window length.", "Sample spacing (inverse of the sampling rate). Defaults to 1.", "Array of length n containing the sample frequencies."]}, {"name": "fft.fftn()", "path": "reference/generated/numpy.fft.fftn", "type": "numpy.fft.fftn", "text": ["Compute the N-dimensional discrete Fourier Transform.", "This function computes the N-dimensional discrete Fourier Transform over any number of axes in an M-dimensional array by means of the Fast Fourier Transform (FFT).", "Input array, can be complex.", "Shape (length of each transformed axis) of the output (s[0] refers to axis 0, s[1] to axis 1, etc.). This corresponds to n for fft(x, n). Along any axis, if the given shape is smaller than that of the input, the input is cropped. If it is larger, the input is padded with zeros. if s is not given, the shape of the input along the axes specified by axes is used.", "Axes over which to compute the FFT. If not given, the last len(s) axes are used, or all axes if s is also not specified. Repeated indices in axes means that the transform over that axis is performed multiple times.", "New in version 1.10.0.", "Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.", "New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.", "The truncated or zero-padded input, transformed along the axes indicated by axes, or by a combination of s and a, as explained in the parameters section above.", "If s and axes have different length.", "If an element of axes is larger than than the number of axes of a.", "See also", "Overall view of discrete Fourier transforms, with definitions and conventions used.", "The inverse of fftn, the inverse n-dimensional FFT.", "The one-dimensional FFT, with definitions and conventions used.", "The n-dimensional FFT of real input.", "The two-dimensional FFT.", "Shifts zero-frequency terms to centre of array", "The output, analogously to fft, contains the term for zero frequency in the low-order corner of all axes, the positive frequency terms in the first half of all axes, the term for the Nyquist frequency in the middle of all axes and the negative frequency terms in the second half of all axes, in order of decreasingly negative frequency.", "See numpy.fft for details, definitions and conventions used."]}, {"name": "fft.fftshift()", "path": "reference/generated/numpy.fft.fftshift", "type": "numpy.fft.fftshift", "text": ["Shift the zero-frequency component to the center of the spectrum.", "This function swaps half-spaces for all axes listed (defaults to all). Note that y[0] is the Nyquist component only if len(x) is even.", "Input array.", "Axes over which to shift. Default is None, which shifts all axes.", "The shifted array.", "See also", "The inverse of fftshift.", "Shift the zero-frequency component only along the second axis:"]}, {"name": "fft.hfft()", "path": "reference/generated/numpy.fft.hfft", "type": "numpy.fft.hfft", "text": ["Compute the FFT of a signal that has Hermitian symmetry, i.e., a real spectrum.", "The input array.", "Length of the transformed axis of the output. For n output points, n//2 + 1 input points are necessary. If the input is longer than this, it is cropped. If it is shorter than this, it is padded with zeros. If n is not given, it is taken to be 2*(m-1) where m is the length of the input along the axis specified by axis.", "Axis over which to compute the FFT. If not given, the last axis is used.", "New in version 1.10.0.", "Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.", "New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.", "The truncated or zero-padded input, transformed along the axis indicated by axis, or the last one if axis is not specified. The length of the transformed axis is n, or, if n is not given, 2*m - 2 where m is the length of the transformed axis of the input. To get an odd number of output points, n must be specified, for instance as 2*m - 1 in the typical case,", "If axis is not a valid axis of a.", "See also", "Compute the one-dimensional FFT for real input.", "The inverse of hfft.", "hfft/ihfft are a pair analogous to rfft/irfft, but for the opposite case: here the signal has Hermitian symmetry in the time domain and is real in the frequency domain. So here it\u2019s hfft for which you must supply the length of the result if it is to be odd.", "The correct interpretation of the hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, hfft assumes an even output length which puts the last entry at the Nyquist frequency; aliasing with its symmetric counterpart. By Hermitian symmetry, the value is thus treated as purely real. To avoid losing information, the shape of the full signal must be given."]}, {"name": "fft.ifft()", "path": "reference/generated/numpy.fft.ifft", "type": "numpy.fft.ifft", "text": ["Compute the one-dimensional inverse discrete Fourier Transform.", "This function computes the inverse of the one-dimensional n-point discrete Fourier transform computed by fft. In other words, ifft(fft(a)) == a to within numerical accuracy. For a general description of the algorithm and definitions, see numpy.fft.", "The input should be ordered in the same way as is returned by fft, i.e.,", "For an even number of input points, A[n//2] represents the sum of the values at the positive and negative Nyquist frequencies, as the two are aliased together. See numpy.fft for details.", "Input array, can be complex.", "Length of the transformed axis of the output. If n is smaller than the length of the input, the input is cropped. If it is larger, the input is padded with zeros. If n is not given, the length of the input along the axis specified by axis is used. See notes about padding issues.", "Axis over which to compute the inverse DFT. If not given, the last axis is used.", "New in version 1.10.0.", "Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.", "New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.", "The truncated or zero-padded input, transformed along the axis indicated by axis, or the last one if axis is not specified.", "If axis is not a valid axis of a.", "See also", "An introduction, with definitions and general explanations.", "The one-dimensional (forward) FFT, of which ifft is the inverse", "The two-dimensional inverse FFT.", "The n-dimensional inverse FFT.", "If the input parameter n is larger than the size of the input, the input is padded by appending zeros at the end. Even though this is the common approach, it might lead to surprising results. If a different padding is desired, it must be performed before calling ifft.", "Create and plot a band-limited signal with random phases:"]}, {"name": "fft.ifft2()", "path": "reference/generated/numpy.fft.ifft2", "type": "numpy.fft.ifft2", "text": ["Compute the 2-dimensional inverse discrete Fourier Transform.", "This function computes the inverse of the 2-dimensional discrete Fourier Transform over any number of axes in an M-dimensional array by means of the Fast Fourier Transform (FFT). In other words, ifft2(fft2(a)) == a to within numerical accuracy. By default, the inverse transform is computed over the last two axes of the input array.", "The input, analogously to ifft, should be ordered in the same way as is returned by fft2, i.e. it should have the term for zero frequency in the low-order corner of the two axes, the positive frequency terms in the first half of these axes, the term for the Nyquist frequency in the middle of the axes and the negative frequency terms in the second half of both axes, in order of decreasingly negative frequency.", "Input array, can be complex.", "Shape (length of each axis) of the output (s[0] refers to axis 0, s[1] to axis 1, etc.). This corresponds to n for ifft(x, n). Along each axis, if the given shape is smaller than that of the input, the input is cropped. If it is larger, the input is padded with zeros. if s is not given, the shape of the input along the axes specified by axes is used. See notes for issue on ifft zero padding.", "Axes over which to compute the FFT. If not given, the last two axes are used. A repeated index in axes means the transform over that axis is performed multiple times. A one-element sequence means that a one-dimensional FFT is performed.", "New in version 1.10.0.", "Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.", "New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.", "The truncated or zero-padded input, transformed along the axes indicated by axes, or the last two axes if axes is not given.", "If s and axes have different length, or axes not given and len(s) != 2.", "If an element of axes is larger than than the number of axes of a.", "See also", "Overall view of discrete Fourier transforms, with definitions and conventions used.", "The forward 2-dimensional FFT, of which ifft2 is the inverse.", "The inverse of the n-dimensional FFT.", "The one-dimensional FFT.", "The one-dimensional inverse FFT.", "ifft2 is just ifftn with a different default for axes.", "See ifftn for details and a plotting example, and numpy.fft for definition and conventions used.", "Zero-padding, analogously with ifft, is performed by appending zeros to the input along the specified dimension. Although this is the common approach, it might lead to surprising results. If another form of zero padding is desired, it must be performed before ifft2 is called."]}, {"name": "fft.ifftn()", "path": "reference/generated/numpy.fft.ifftn", "type": "numpy.fft.ifftn", "text": ["Compute the N-dimensional inverse discrete Fourier Transform.", "This function computes the inverse of the N-dimensional discrete Fourier Transform over any number of axes in an M-dimensional array by means of the Fast Fourier Transform (FFT). In other words, ifftn(fftn(a)) == a to within numerical accuracy. For a description of the definitions and conventions used, see numpy.fft.", "The input, analogously to ifft, should be ordered in the same way as is returned by fftn, i.e. it should have the term for zero frequency in all axes in the low-order corner, the positive frequency terms in the first half of all axes, the term for the Nyquist frequency in the middle of all axes and the negative frequency terms in the second half of all axes, in order of decreasingly negative frequency.", "Input array, can be complex.", "Shape (length of each transformed axis) of the output (s[0] refers to axis 0, s[1] to axis 1, etc.). This corresponds to n for ifft(x, n). Along any axis, if the given shape is smaller than that of the input, the input is cropped. If it is larger, the input is padded with zeros. if s is not given, the shape of the input along the axes specified by axes is used. See notes for issue on ifft zero padding.", "Axes over which to compute the IFFT. If not given, the last len(s) axes are used, or all axes if s is also not specified. Repeated indices in axes means that the inverse transform over that axis is performed multiple times.", "New in version 1.10.0.", "Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.", "New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.", "The truncated or zero-padded input, transformed along the axes indicated by axes, or by a combination of s or a, as explained in the parameters section above.", "If s and axes have different length.", "If an element of axes is larger than than the number of axes of a.", "See also", "Overall view of discrete Fourier transforms, with definitions and conventions used.", "The forward n-dimensional FFT, of which ifftn is the inverse.", "The one-dimensional inverse FFT.", "The two-dimensional inverse FFT.", "Undoes fftshift, shifts zero-frequency terms to beginning of array.", "See numpy.fft for definitions and conventions used.", "Zero-padding, analogously with ifft, is performed by appending zeros to the input along the specified dimension. Although this is the common approach, it might lead to surprising results. If another form of zero padding is desired, it must be performed before ifftn is called.", "Create and plot an image with band-limited frequency content:"]}, {"name": "fft.ifftshift()", "path": "reference/generated/numpy.fft.ifftshift", "type": "numpy.fft.ifftshift", "text": ["The inverse of fftshift. Although identical for even-length x, the functions differ by one sample for odd-length x.", "Input array.", "Axes over which to calculate. Defaults to None, which shifts all axes.", "The shifted array.", "See also", "Shift zero-frequency component to the center of the spectrum."]}, {"name": "fft.ihfft()", "path": "reference/generated/numpy.fft.ihfft", "type": "numpy.fft.ihfft", "text": ["Compute the inverse FFT of a signal that has Hermitian symmetry.", "Input array.", "Length of the inverse FFT, the number of points along transformation axis in the input to use. If n is smaller than the length of the input, the input is cropped. If it is larger, the input is padded with zeros. If n is not given, the length of the input along the axis specified by axis is used.", "Axis over which to compute the inverse FFT. If not given, the last axis is used.", "New in version 1.10.0.", "Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.", "New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.", "The truncated or zero-padded input, transformed along the axis indicated by axis, or the last one if axis is not specified. The length of the transformed axis is n//2 + 1.", "See also", "hfft/ihfft are a pair analogous to rfft/irfft, but for the opposite case: here the signal has Hermitian symmetry in the time domain and is real in the frequency domain. So here it\u2019s hfft for which you must supply the length of the result if it is to be odd:"]}, {"name": "fft.irfft()", "path": "reference/generated/numpy.fft.irfft", "type": "numpy.fft.irfft", "text": ["Computes the inverse of rfft.", "This function computes the inverse of the one-dimensional n-point discrete Fourier Transform of real input computed by rfft. In other words, irfft(rfft(a), len(a)) == a to within numerical accuracy. (See Notes below for why len(a) is necessary here.)", "The input is expected to be in the form returned by rfft, i.e. the real zero-frequency term followed by the complex positive frequency terms in order of increasing frequency. Since the discrete Fourier Transform of real input is Hermitian-symmetric, the negative frequency terms are taken to be the complex conjugates of the corresponding positive frequency terms.", "The input array.", "Length of the transformed axis of the output. For n output points, n//2+1 input points are necessary. If the input is longer than this, it is cropped. If it is shorter than this, it is padded with zeros. If n is not given, it is taken to be 2*(m-1) where m is the length of the input along the axis specified by axis.", "Axis over which to compute the inverse FFT. If not given, the last axis is used.", "New in version 1.10.0.", "Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.", "New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.", "The truncated or zero-padded input, transformed along the axis indicated by axis, or the last one if axis is not specified. The length of the transformed axis is n, or, if n is not given, 2*(m-1) where m is the length of the transformed axis of the input. To get an odd number of output points, n must be specified.", "If axis is not a valid axis of a.", "See also", "For definition of the DFT and conventions used.", "The one-dimensional FFT of real input, of which irfft is inverse.", "The one-dimensional FFT.", "The inverse of the two-dimensional FFT of real input.", "The inverse of the n-dimensional FFT of real input.", "Returns the real valued n-point inverse discrete Fourier transform of a, where a contains the non-negative frequency terms of a Hermitian-symmetric sequence. n is the length of the result, not the input.", "If you specify an n such that a must be zero-padded or truncated, the extra/removed values will be added/removed at high frequencies. One can thus resample a series to m points via Fourier interpolation by: a_resamp = irfft(rfft(a), m).", "The correct interpretation of the hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, irfft assumes an even output length which puts the last entry at the Nyquist frequency; aliasing with its symmetric counterpart. By Hermitian symmetry, the value is thus treated as purely real. To avoid losing information, the correct length of the real input must be given.", "Notice how the last term in the input to the ordinary ifft is the complex conjugate of the second term, and the output has zero imaginary part everywhere. When calling irfft, the negative frequencies are not specified, and the output array is purely real."]}, {"name": "fft.irfft2()", "path": "reference/generated/numpy.fft.irfft2", "type": "numpy.fft.irfft2", "text": ["Computes the inverse of rfft2.", "The input array", "Shape of the real output to the inverse FFT.", "The axes over which to compute the inverse fft. Default is the last two axes.", "New in version 1.10.0.", "Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.", "New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.", "The result of the inverse real 2-D FFT.", "See also", "The forward two-dimensional FFT of real input, of which irfft2 is the inverse.", "The one-dimensional FFT for real input.", "The inverse of the one-dimensional FFT of real input.", "Compute the inverse of the N-dimensional FFT of real input.", "This is really irfftn with different defaults. For more details see irfftn."]}, {"name": "fft.irfftn()", "path": "reference/generated/numpy.fft.irfftn", "type": "numpy.fft.irfftn", "text": ["Computes the inverse of rfftn.", "This function computes the inverse of the N-dimensional discrete Fourier Transform for real input over any number of axes in an M-dimensional array by means of the Fast Fourier Transform (FFT). In other words, irfftn(rfftn(a), a.shape) == a to within numerical accuracy. (The a.shape is necessary like len(a) is for irfft, and for the same reason.)", "The input should be ordered in the same way as is returned by rfftn, i.e. as for irfft for the final transformation axis, and as for ifftn along all the other axes.", "Input array.", "Shape (length of each transformed axis) of the output (s[0] refers to axis 0, s[1] to axis 1, etc.). s is also the number of input points used along this axis, except for the last axis, where s[-1]//2+1 points of the input are used. Along any axis, if the shape indicated by s is smaller than that of the input, the input is cropped. If it is larger, the input is padded with zeros. If s is not given, the shape of the input along the axes specified by axes is used. Except for the last axis which is taken to be 2*(m-1) where m is the length of the input along that axis.", "Axes over which to compute the inverse FFT. If not given, the last len(s) axes are used, or all axes if s is also not specified. Repeated indices in axes means that the inverse transform over that axis is performed multiple times.", "New in version 1.10.0.", "Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.", "New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.", "The truncated or zero-padded input, transformed along the axes indicated by axes, or by a combination of s or a, as explained in the parameters section above. The length of each transformed axis is as given by the corresponding element of s, or the length of the input in every axis except for the last one if s is not given. In the final transformed axis the length of the output when s is not given is 2*(m-1) where m is the length of the final transformed axis of the input. To get an odd number of output points in the final axis, s must be specified.", "If s and axes have different length.", "If an element of axes is larger than than the number of axes of a.", "See also", "The forward n-dimensional FFT of real input, of which ifftn is the inverse.", "The one-dimensional FFT, with definitions and conventions used.", "The inverse of the one-dimensional FFT of real input.", "The inverse of the two-dimensional FFT of real input.", "See fft for definitions and conventions used.", "See rfft for definitions and conventions used for real input.", "The correct interpretation of the hermitian input depends on the shape of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, irfftn assumes an even output length which puts the last entry at the Nyquist frequency; aliasing with its symmetric counterpart. When performing the final complex to real transform, the last value is thus treated as purely real. To avoid losing information, the correct shape of the real input must be given."]}, {"name": "fft.rfft()", "path": "reference/generated/numpy.fft.rfft", "type": "numpy.fft.rfft", "text": ["Compute the one-dimensional discrete Fourier Transform for real input.", "This function computes the one-dimensional n-point discrete Fourier Transform (DFT) of a real-valued array by means of an efficient algorithm called the Fast Fourier Transform (FFT).", "Input array", "Number of points along transformation axis in the input to use. If n is smaller than the length of the input, the input is cropped. If it is larger, the input is padded with zeros. If n is not given, the length of the input along the axis specified by axis is used.", "Axis over which to compute the FFT. If not given, the last axis is used.", "New in version 1.10.0.", "Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.", "New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.", "The truncated or zero-padded input, transformed along the axis indicated by axis, or the last one if axis is not specified. If n is even, the length of the transformed axis is (n/2)+1. If n is odd, the length is (n+1)/2.", "If axis is not a valid axis of a.", "See also", "For definition of the DFT and conventions used.", "The inverse of rfft.", "The one-dimensional FFT of general (complex) input.", "The n-dimensional FFT.", "The n-dimensional FFT of real input.", "When the DFT is computed for purely real input, the output is Hermitian-symmetric, i.e. the negative frequency terms are just the complex conjugates of the corresponding positive-frequency terms, and the negative-frequency terms are therefore redundant. This function does not compute the negative frequency terms, and the length of the transformed axis of the output is therefore n//2 + 1.", "When A = rfft(a) and fs is the sampling frequency, A[0] contains the zero-frequency term 0*fs, which is real due to Hermitian symmetry.", "If n is even, A[-1] contains the term representing both positive and negative Nyquist frequency (+fs/2 and -fs/2), and must also be purely real. If n is odd, there is no term at fs/2; A[-1] contains the largest positive frequency (fs/2*(n-1)/n), and is complex in the general case.", "If the input a contains an imaginary part, it is silently discarded.", "Notice how the final element of the fft output is the complex conjugate of the second element, for real input. For rfft, this symmetry is exploited to compute only the non-negative frequency terms."]}, {"name": "fft.rfft2()", "path": "reference/generated/numpy.fft.rfft2", "type": "numpy.fft.rfft2", "text": ["Compute the 2-dimensional FFT of a real array.", "Input array, taken to be real.", "Shape of the FFT.", "Axes over which to compute the FFT.", "New in version 1.10.0.", "Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.", "New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.", "The result of the real 2-D FFT.", "See also", "Compute the N-dimensional discrete Fourier Transform for real input.", "This is really just rfftn with different default behavior. For more details see rfftn."]}, {"name": "fft.rfftfreq()", "path": "reference/generated/numpy.fft.rfftfreq", "type": "numpy.fft.rfftfreq", "text": ["Return the Discrete Fourier Transform sample frequencies (for usage with rfft, irfft).", "The returned float array f contains the frequency bin centers in cycles per unit of the sample spacing (with zero at the start). For instance, if the sample spacing is in seconds, then the frequency unit is cycles/second.", "Given a window length n and a sample spacing d:", "Unlike fftfreq (but like scipy.fftpack.rfftfreq) the Nyquist frequency component is considered to be positive.", "Window length.", "Sample spacing (inverse of the sampling rate). Defaults to 1.", "Array of length n//2 + 1 containing the sample frequencies."]}, {"name": "fft.rfftn()", "path": "reference/generated/numpy.fft.rfftn", "type": "numpy.fft.rfftn", "text": ["Compute the N-dimensional discrete Fourier Transform for real input.", "This function computes the N-dimensional discrete Fourier Transform over any number of axes in an M-dimensional real array by means of the Fast Fourier Transform (FFT). By default, all axes are transformed, with the real transform performed over the last axis, while the remaining transforms are complex.", "Input array, taken to be real.", "Shape (length along each transformed axis) to use from the input. (s[0] refers to axis 0, s[1] to axis 1, etc.). The final element of s corresponds to n for rfft(x, n), while for the remaining axes, it corresponds to n for fft(x, n). Along any axis, if the given shape is smaller than that of the input, the input is cropped. If it is larger, the input is padded with zeros. if s is not given, the shape of the input along the axes specified by axes is used.", "Axes over which to compute the FFT. If not given, the last len(s) axes are used, or all axes if s is also not specified.", "New in version 1.10.0.", "Normalization mode (see numpy.fft). Default is \u201cbackward\u201d. Indicates which direction of the forward/backward pair of transforms is scaled and with what normalization factor.", "New in version 1.20.0: The \u201cbackward\u201d, \u201cforward\u201d values were added.", "The truncated or zero-padded input, transformed along the axes indicated by axes, or by a combination of s and a, as explained in the parameters section above. The length of the last axis transformed will be s[-1]//2+1, while the remaining transformed axes will have lengths according to s, or unchanged from the input.", "If s and axes have different length.", "If an element of axes is larger than than the number of axes of a.", "See also", "The inverse of rfftn, i.e. the inverse of the n-dimensional FFT of real input.", "The one-dimensional FFT, with definitions and conventions used.", "The one-dimensional FFT of real input.", "The n-dimensional FFT.", "The two-dimensional FFT of real input.", "The transform for real input is performed over the last transformation axis, as by rfft, then the transform over the remaining axes is performed as by fftn. The order of the output is as for rfft for the final transformation axis, and as for fftn for the remaining transformation axes.", "See fft for details, definitions and conventions used."]}, {"name": "final class numpy.typing.NBitBase", "path": "reference/typing#numpy.typing.NBitBase", "type": "Typing ( \n    \n     numpy.typing\n    \n    )", "text": ["A type representing numpy.number precision during static type checking.", "Used exclusively for the purpose static type checking, NBitBase represents the base of a hierarchical set of subclasses. Each subsequent subclass is herein used for representing a lower level of precision, e.g. 64Bit > 32Bit > 16Bit.", "New in version 1.20.", "Below is a typical usage example: NBitBase is herein used for annotating a function that takes a float and integer of arbitrary precision as arguments and returns a new float of whichever precision is largest (e.g. np.float16 + np.int64 -> np.float64)."]}, {"name": "flatiter.base", "path": "reference/generated/numpy.flatiter.base", "type": "Indexing routines", "text": ["attribute", "A reference to the array that is iterated over."]}, {"name": "flatiter.coords", "path": "reference/generated/numpy.flatiter.coords", "type": "Indexing routines", "text": ["attribute", "An N-dimensional tuple of current coordinates."]}, {"name": "flatiter.copy()", "path": "reference/generated/numpy.flatiter.copy", "type": "numpy.flatiter.copy", "text": ["method", "Get a copy of the iterator as a 1-D array."]}, {"name": "flatiter.index", "path": "reference/generated/numpy.flatiter.index", "type": "Indexing routines", "text": ["attribute", "Current flat index into the array."]}, {"name": "float npy_half_to_float()", "path": "reference/c-api/coremath#c.npy_half_to_float", "type": "NumPy core libraries", "text": ["Converts a half-precision float to a single-precision float."]}, {"name": "float random_gamma_f()", "path": "reference/random/c-api#c.random_gamma_f", "type": "C API for random", "text": []}, {"name": "float random_standard_exponential_f()", "path": "reference/random/c-api#c.random_standard_exponential_f", "type": "C API for random", "text": []}, {"name": "float random_standard_gamma_f()", "path": "reference/random/c-api#c.random_standard_gamma_f", "type": "C API for random", "text": []}, {"name": "float random_standard_normal_f()", "path": "reference/random/c-api#c.random_standard_normal_f", "type": "C API for random", "text": []}, {"name": "float random_standard_uniform_f()", "path": "reference/random/c-api#c.random_standard_uniform_f", "type": "C API for random", "text": []}, {"name": "Floating point error handling", "path": "reference/routines.err", "type": "Floating point error handling", "text": ["seterr([all, divide, over, under, invalid])", "Set how floating-point errors are handled.", "geterr()", "Get the current way of handling floating-point errors.", "seterrcall(func)", "Set the floating-point error callback function or log object.", "geterrcall()", "Return the current callback function used on floating-point errors.", "errstate(**kwargs)", "Context manager for floating-point error handling.", "seterrobj(errobj, /)", "Set the object that defines floating-point error handling.", "geterrobj()", "Return the current object that defines floating-point error handling."]}, {"name": "For downstream package authors", "path": "user/depending_on_numpy", "type": "User Guide", "text": ["This document aims to explain some best practices for authoring a package that depends on NumPy.", "NumPy uses a standard, PEP 440 compliant, versioning scheme: major.minor.bugfix. A major release is highly unusual (NumPy is still at version 1.xx) and if it happens it will likely indicate an ABI break. Minor versions are released regularly, typically every 6 months. Minor versions contain new features, deprecations, and removals of previously deprecated code. Bugfix releases are made even more frequently; they do not contain any new features or deprecations.", "It is important to know that NumPy, like Python itself and most other well known scientific Python projects, does not use semantic versioning. Instead, backwards incompatible API changes require deprecation warnings for at least two releases. For more details, see NEP 23 \u2014 Backwards compatibility and deprecation policy.", "NumPy has both a Python API and a C API. The C API can be used directly or via Cython, f2py, or other such tools. If your package uses the C API, then ABI (application binary interface) stability of NumPy is important. NumPy\u2019s ABI is forward but not backward compatible. This means: binaries compiled against a given version of NumPy will still run correctly with newer NumPy versions, but not with older versions.", "For large, actively maintained packages that depend on NumPy, we recommend testing against the development version of NumPy in CI. To make this easy, nightly builds are provided as wheels at https://anaconda.org/scipy-wheels-nightly/. This helps detect regressions in NumPy that need fixing before the next NumPy release. Furthermore, we recommend to raise errors on warnings in CI for this job, either all warnings or otherwise at least DeprecationWarning and FutureWarning. This gives you an early warning about changes in NumPy to adapt your code.", "If a package either uses the NumPy C API directly or it uses some other tool that depends on it like Cython or Pythran, NumPy is a build-time dependency of the package. Because the NumPy ABI is only forward compatible, you must build your own binaries (wheels or other package formats) against the lowest NumPy version that you support (or an even older version).", "Picking the correct NumPy version to build against for each Python version and platform can get complicated. There are a couple of ways to do this. Build-time dependencies are specified in pyproject.toml (see PEP 517), which is the file used to build wheels by PEP 517 compliant tools (e.g., when using pip wheel).", "You can specify everything manually in pyproject.toml, or you can instead rely on the oldest-supported-numpy metapackage. oldest-supported-numpy will specify the correct NumPy version at build time for wheels, taking into account Python version, Python implementation (CPython or PyPy), operating system and hardware platform. It will specify the oldest NumPy version that supports that combination of characteristics. Note: for platforms for which NumPy provides wheels on PyPI, it will be the first version with wheels (even if some older NumPy version happens to build).", "For conda-forge it\u2019s a little less complicated: there\u2019s dedicated handling for NumPy in build-time and runtime dependencies, so typically this is enough (see here for docs):", "Note", "pip has --no-use-pep517 and --no-build-isolation flags that may ignore pyproject.toml or treat it differently - if users use those flags, they are responsible for installing the correct build dependencies themselves.", "conda will always use -no-build-isolation; dependencies for conda builds are given in the conda recipe (meta.yaml), the ones in pyproject.toml have no effect.", "Please do not use setup_requires (it is deprecated and may invoke easy_install).", "Because for NumPy you have to care about ABI compatibility, you specify the version with == to the lowest supported version. For your other build dependencies you can probably be looser, however it\u2019s still important to set lower and upper bounds for each dependency. It\u2019s fine to specify either a range or a specific version for a dependency like wheel or setuptools. It\u2019s recommended to set the upper bound of the range to the latest already released version of wheel and setuptools - this prevents future releases from breaking your packages on PyPI.", "NumPy itself and many core scientific Python packages have agreed on a schedule for dropping support for old Python and NumPy versions: NEP 29 \u2014 Recommend Python and NumPy version support as a community policy standard. We recommend all packages depending on NumPy to follow the recommendations in NEP 29.", "For run-time dependencies, you specify the range of versions in install_requires in setup.py (assuming you use numpy.distutils or setuptools to build). Getting the upper bound right for NumPy is slightly tricky. If we don\u2019t set any bound, a too-new version will be pulled in a few years down the line, and NumPy may have deprecated and removed some API that your package depended on by then. On the other hand if you set the upper bound to the newest already-released version, then as soon as a new NumPy version is released there will be no matching version of your package that works with it.", "What to do here depends on your release frequency. Given that NumPy releases come in a 6-monthly cadence and that features that get deprecated in NumPy should stay around for another two releases, a good upper bound is <1.(xx+3).0 - where xx is the minor version of the latest already-released NumPy. This is safe to do if you release at least once a year. If your own releases are much less frequent, you may set the upper bound a little further into the future - this is a trade-off between a future NumPy version _maybe_ removing something you rely on, and the upper bound being exceeded which _may_ lead to your package being hard to install in combination with other packages relying on the latest NumPy.", "Note", "SciPy has more documentation on how it builds wheels and deals with its build-time and runtime dependencies here.", "NumPy and SciPy wheel build CI may also be useful as a reference, it can be found here for NumPy and here for SciPy."]}, {"name": "Fortran 77 programs", "path": "f2py/buildtools/index", "type": "F2PY and Build Systems", "text": ["In this section we will cover the various popular build systems and their usage with f2py.", "Note", "As of November 2021", "The default build system for F2PY has traditionally been the through the enhanced numpy.distutils module. This module is based on distutils which will be removed in Python 3.12.0 in October 2023; setuptools does not have support for Fortran or F2PY and it is unclear if it will be supported in the future. Alternative methods are thus increasingly more important.", "Building an extension module which includes Python and Fortran consists of:", "One or more generated files from f2py", "fortranobject.{c,h}", "NumPy headers", "Broadly speaking there are three cases which arise when considering the outputs of f2py:", "Generates", "When no COMMON blocks are present only a C wrapper file is generated. Wrappers are also generated to rewrite assumed shape arrays as automatic arrays.", "Generates:", "The secondary wrapper is used to handle code which is subdivided into modules. It rewrites assumed shape arrays as automatic arrays.", "Generates:", "Signature files .pyf do not signal their language standard via the file extension, they may generate the F90 and F77 specific wrappers depending on their contents; which shifts the burden of checking for generated files onto the build system.", "Note", "The signature file output situation is being reconsidered in issue 20385 .", "In theory keeping the above requirements in hand, any build system can be adapted to generate f2py extension modules. Here we will cover a subset of the more popular systems.", "Note", "make has no place in a modern multi-language setup, and so is not discussed further."]}, {"name": "Functional programming", "path": "reference/routines.functional", "type": "Functional programming", "text": ["apply_along_axis(func1d, axis, arr, *args, ...)", "Apply a function to 1-D slices along the given axis.", "apply_over_axes(func, a, axes)", "Apply a function repeatedly over multiple axes.", "vectorize(pyfunc[, otypes, doc, excluded, ...])", "Generalized function class.", "frompyfunc(func, /, nin, nout, *[, identity])", "Takes an arbitrary Python function and returns a NumPy ufunc.", "piecewise(x, condlist, funclist, *args, **kw)", "Evaluate a piecewise-defined function."]}, {"name": "generic.__array__()", "path": "reference/generated/numpy.generic.__array__", "type": "numpy.generic.__array__", "text": ["method", "sc.__array__(dtype) return 0-dim array from scalar with specified dtype"]}, {"name": "generic.__array_interface__", "path": "reference/generated/numpy.generic.__array_interface__", "type": "numpy.generic.__array_interface__", "text": ["attribute", "Array protocol: Python side"]}, {"name": "generic.__array_priority__", "path": "reference/generated/numpy.generic.__array_priority__", "type": "numpy.generic.__array_priority__", "text": ["attribute", "Array priority."]}, {"name": "generic.__array_struct__", "path": "reference/generated/numpy.generic.__array_struct__", "type": "numpy.generic.__array_struct__", "text": ["attribute", "Array protocol: struct"]}, {"name": "generic.__array_wrap__()", "path": "reference/generated/numpy.generic.__array_wrap__", "type": "numpy.generic.__array_wrap__", "text": ["method", "sc.__array_wrap__(obj) return scalar from array"]}, {"name": "generic.__reduce__()", "path": "reference/generated/numpy.generic.__reduce__", "type": "numpy.generic.__reduce__", "text": ["method", "Helper for pickle."]}, {"name": "generic.__setstate__()", "path": "reference/generated/numpy.generic.__setstate__", "type": "numpy.generic.__setstate__", "text": ["method"]}, {"name": "generic.base", "path": "reference/generated/numpy.generic.base", "type": "numpy.generic.base", "text": ["attribute", "Scalar attribute identical to the corresponding array attribute.", "Please see ndarray.base."]}, {"name": "generic.byteswap()", "path": "reference/generated/numpy.generic.byteswap", "type": "numpy.generic.byteswap", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.byteswap."]}, {"name": "generic.data", "path": "reference/generated/numpy.generic.data", "type": "numpy.generic.data", "text": ["attribute", "Pointer to start of data."]}, {"name": "generic.dtype", "path": "reference/generated/numpy.generic.dtype", "type": "numpy.generic.dtype", "text": ["attribute", "Get array data-descriptor."]}, {"name": "generic.flags", "path": "reference/generated/numpy.generic.flags", "type": "numpy.generic.flags", "text": ["attribute", "The integer value of flags."]}, {"name": "generic.flat", "path": "reference/generated/numpy.generic.flat", "type": "numpy.generic.flat", "text": ["attribute", "A 1-D view of the scalar."]}, {"name": "generic.imag", "path": "reference/generated/numpy.generic.imag", "type": "numpy.generic.imag", "text": ["attribute", "The imaginary part of the scalar."]}, {"name": "generic.itemsize", "path": "reference/generated/numpy.generic.itemsize", "type": "numpy.generic.itemsize", "text": ["attribute", "The length of one element in bytes."]}, {"name": "generic.ndim", "path": "reference/generated/numpy.generic.ndim", "type": "numpy.generic.ndim", "text": ["attribute", "The number of array dimensions."]}, {"name": "generic.real", "path": "reference/generated/numpy.generic.real", "type": "numpy.generic.real", "text": ["attribute", "The real part of the scalar."]}, {"name": "generic.setflags()", "path": "reference/generated/numpy.generic.setflags", "type": "numpy.generic.setflags", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.setflags."]}, {"name": "generic.shape", "path": "reference/generated/numpy.generic.shape", "type": "numpy.generic.shape", "text": ["attribute", "Tuple of array dimensions."]}, {"name": "generic.size", "path": "reference/generated/numpy.generic.size", "type": "numpy.generic.size", "text": ["attribute", "The number of elements in the gentype."]}, {"name": "generic.squeeze()", "path": "reference/generated/numpy.generic.squeeze", "type": "numpy.generic.squeeze", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.squeeze."]}, {"name": "generic.strides", "path": "reference/generated/numpy.generic.strides", "type": "numpy.generic.strides", "text": ["attribute", "Tuple of bytes steps in each dimension."]}, {"name": "generic.T", "path": "reference/generated/numpy.generic.t", "type": "numpy.generic.T", "text": ["attribute", "Scalar attribute identical to the corresponding array attribute.", "Please see ndarray.T."]}, {"name": "Get the local copy of the code", "path": "dev/gitwash/following_latest", "type": "Development", "text": ["From the command line:", "You now have a copy of the code tree in the new numpy directory. If this doesn\u2019t work you can try the alternative read-only url:"]}, {"name": "get_build_temp_dir()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.get_build_temp_dir", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": ["Return a path to a temporary directory where temporary files should be placed."]}, {"name": "get_config_cmd()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.get_config_cmd", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": ["Returns the numpy.distutils config command instance."]}, {"name": "get_distribution()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.get_distribution", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": ["Return the distutils distribution object for self."]}, {"name": "get_info()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.get_info", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": ["Get resources information.", "Return information (from system_info.get_info) for all of the names in the argument list in a single dictionary."]}, {"name": "get_subpackage()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.get_subpackage", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": ["Return list of subpackage configurations.", "Name of the subpackage to get the configuration. \u2018*\u2019 in subpackage_name is handled as a wildcard.", "If None, then the path is assumed to be the local path plus the subpackage_name. If a setup.py file is not found in the subpackage_path, then a default configuration is used.", "Parent name."]}, {"name": "get_version()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.get_version", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": ["Try to get version string of a package.", "Return a version string of the current package or None if the version information could not be detected.", "This method scans files named __version__.py, <packagename>_version.py, version.py, and __svn_version__.py for string variables version, __version__, and <packagename>_version, until a version number is found."]}, {"name": "Git configuration", "path": "dev/gitwash/configure_git", "type": "Development", "text": ["Your personal git configurations are saved in the .gitconfig file in your home directory. Here is an example .gitconfig file:", "You can edit this file directly or you can use the git config --global command:", "To set up on another computer, you can copy your ~/.gitconfig file, or run the commands above.", "It is good practice to tell git who you are, for labeling any changes you make to the code. The simplest way to do this is from the command line:", "This will write the settings into your git configuration file, which should now contain a user section with your name and email:", "Of course you\u2019ll need to replace Your Name and you@yourdomain.example.com with your actual name and email address.", "You might well benefit from some aliases to common commands.", "For example, you might well want to be able to shorten git checkout to git co. Or you may want to alias git diff --color-words (which gives a nicely formatted output of the diff) to git wdiff", "The following git config --global commands:", "will create an alias section in your .gitconfig file with contents like this:", "You may also want to make sure that your editor of choice is used", "To enforce summaries when doing merges (~/.gitconfig file again):", "Or from the command line:"]}, {"name": "Git for development", "path": "dev/gitwash/index", "type": "Development", "text": ["These pages describe a general git and github workflow.", "This is not a comprehensive git reference. It\u2019s tailored to the github hosting service. You may well find better or quicker ways of getting stuff done with git, but these should get you started.", "For general resources for learning git see Additional Git Resources.", "Have a look at the github install help pages available from github help", "Contents:"]}, {"name": "Global State", "path": "reference/global_state", "type": "Global State", "text": ["NumPy has a few import-time, compile-time, or runtime options which change the global behaviour. Most of these are related to performance or for debugging purposes and will not be interesting to the vast majority of users.", "NumPy itself is normally intentionally limited to a single thread during function calls, however it does support multiple Python threads running at the same time. Note that for performant linear algebra NumPy uses a BLAS backend such as OpenBLAS or MKL, which may use multiple threads that may be controlled by environment variables such as OMP_NUM_THREADS depending on what is used. One way to control the number of threads is the package threadpoolctl", "When working with very large arrays on modern Linux kernels, you can experience a significant speedup when transparent hugepage is used. The current system policy for transparent hugepages can be seen by:", "When set to madvise NumPy will typically use hugepages for a performance boost. This behaviour can be modified by setting the environment variable:", "or setting it to 1 to always enable it. When not set, the default is to use madvise on Kernels 4.6 and newer. These kernels presumably experience a large speedup with hugepage support. This flag is checked at import time.", "The array function protocol which allows array-like objects to hook into the NumPy API is currently enabled by default. This option exists since NumPy 1.16 and is enabled by default since NumPy 1.17. It can be disabled using:", "See also numpy.class.__array_function__ for more information. This flag is checked at import time.", "The compile-time environment variables:", "control how NumPy reports contiguity for arrays. The default that it is enabled and the debug mode is disabled. This setting should always be enabled. Setting the debug option can be interesting for testing code written in C which iterates through arrays that may or may not be contiguous in memory. Most users will have no reason to change these; for details see the memory layout documentation.", "Some users might pass ownership of the data pointer to the ndarray by setting the OWNDATA flag. If they do this without setting (manually) a memory allocation policy, the default will be to call free. If NUMPY_WARN_IF_NO_MEM_POLICY is set to \"1\", a RuntimeWarning will be emitted. A better alternative is to use a PyCapsule with a deallocator and set the ndarray.base."]}, {"name": "have_f77c()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.have_f77c", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": ["Check for availability of Fortran 77 compiler.", "Use it inside source generating function to ensure that setup distribution instance has been initialized.", "True if a Fortran 77 compiler is available (because a simple Fortran 77 code was able to be compiled successfully)."]}, {"name": "have_f90c()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.have_f90c", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": ["Check for availability of Fortran 90 compiler.", "Use it inside source generating function to ensure that setup distribution instance has been initialized.", "True if a Fortran 90 compiler is available (because a simple Fortran 90 code was able to be compiled successfully)"]}, {"name": "Hermite Series, \u201cPhysicists\u201d (numpy.polynomial.hermite)", "path": "reference/routines.polynomials.hermite", "type": "Hermite Series, \u201cPhysicists\u201d ( \n        \n         numpy.polynomial.hermite\n        \n        )", "text": ["This module provides a number of objects (mostly functions) useful for dealing with Hermite series, including a Hermite class that encapsulates the usual arithmetic operations. (General information on how this module represents and works with such polynomials is in the docstring for its \u201cparent\u201d sub-package, numpy.polynomial).", "Hermite(coef[, domain, window])", "An Hermite series class.", "hermdomain", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "hermzero", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "hermone", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "hermx", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "hermadd(c1, c2)", "Add one Hermite series to another.", "hermsub(c1, c2)", "Subtract one Hermite series from another.", "hermmulx(c)", "Multiply a Hermite series by x.", "hermmul(c1, c2)", "Multiply one Hermite series by another.", "hermdiv(c1, c2)", "Divide one Hermite series by another.", "hermpow(c, pow[, maxpower])", "Raise a Hermite series to a power.", "hermval(x, c[, tensor])", "Evaluate an Hermite series at points x.", "hermval2d(x, y, c)", "Evaluate a 2-D Hermite series at points (x, y).", "hermval3d(x, y, z, c)", "Evaluate a 3-D Hermite series at points (x, y, z).", "hermgrid2d(x, y, c)", "Evaluate a 2-D Hermite series on the Cartesian product of x and y.", "hermgrid3d(x, y, z, c)", "Evaluate a 3-D Hermite series on the Cartesian product of x, y, and z.", "hermder(c[, m, scl, axis])", "Differentiate a Hermite series.", "hermint(c[, m, k, lbnd, scl, axis])", "Integrate a Hermite series.", "hermfromroots(roots)", "Generate a Hermite series with given roots.", "hermroots(c)", "Compute the roots of a Hermite series.", "hermvander(x, deg)", "Pseudo-Vandermonde matrix of given degree.", "hermvander2d(x, y, deg)", "Pseudo-Vandermonde matrix of given degrees.", "hermvander3d(x, y, z, deg)", "Pseudo-Vandermonde matrix of given degrees.", "hermgauss(deg)", "Gauss-Hermite quadrature.", "hermweight(x)", "Weight function of the Hermite polynomials.", "hermcompanion(c)", "Return the scaled companion matrix of c.", "hermfit(x, y, deg[, rcond, full, w])", "Least squares fit of Hermite series to data.", "hermtrim(c[, tol])", "Remove \"small\" \"trailing\" coefficients from a polynomial.", "hermline(off, scl)", "Hermite series whose graph is a straight line.", "herm2poly(c)", "Convert a Hermite series to a polynomial.", "poly2herm(pol)", "Convert a polynomial to a Hermite series.", "numpy.polynomial"]}, {"name": "HermiteE Series, \u201cProbabilists\u201d (numpy.polynomial.hermite_e)", "path": "reference/routines.polynomials.hermite_e", "type": "HermiteE Series, \u201cProbabilists\u201d ( \n        \n         numpy.polynomial.hermite_e\n        \n        )", "text": ["This module provides a number of objects (mostly functions) useful for dealing with Hermite_e series, including a HermiteE class that encapsulates the usual arithmetic operations. (General information on how this module represents and works with such polynomials is in the docstring for its \u201cparent\u201d sub-package, numpy.polynomial).", "HermiteE(coef[, domain, window])", "An HermiteE series class.", "hermedomain", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "hermezero", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "hermeone", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "hermex", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "hermeadd(c1, c2)", "Add one Hermite series to another.", "hermesub(c1, c2)", "Subtract one Hermite series from another.", "hermemulx(c)", "Multiply a Hermite series by x.", "hermemul(c1, c2)", "Multiply one Hermite series by another.", "hermediv(c1, c2)", "Divide one Hermite series by another.", "hermepow(c, pow[, maxpower])", "Raise a Hermite series to a power.", "hermeval(x, c[, tensor])", "Evaluate an HermiteE series at points x.", "hermeval2d(x, y, c)", "Evaluate a 2-D HermiteE series at points (x, y).", "hermeval3d(x, y, z, c)", "Evaluate a 3-D Hermite_e series at points (x, y, z).", "hermegrid2d(x, y, c)", "Evaluate a 2-D HermiteE series on the Cartesian product of x and y.", "hermegrid3d(x, y, z, c)", "Evaluate a 3-D HermiteE series on the Cartesian product of x, y, and z.", "hermeder(c[, m, scl, axis])", "Differentiate a Hermite_e series.", "hermeint(c[, m, k, lbnd, scl, axis])", "Integrate a Hermite_e series.", "hermefromroots(roots)", "Generate a HermiteE series with given roots.", "hermeroots(c)", "Compute the roots of a HermiteE series.", "hermevander(x, deg)", "Pseudo-Vandermonde matrix of given degree.", "hermevander2d(x, y, deg)", "Pseudo-Vandermonde matrix of given degrees.", "hermevander3d(x, y, z, deg)", "Pseudo-Vandermonde matrix of given degrees.", "hermegauss(deg)", "Gauss-HermiteE quadrature.", "hermeweight(x)", "Weight function of the Hermite_e polynomials.", "hermecompanion(c)", "Return the scaled companion matrix of c.", "hermefit(x, y, deg[, rcond, full, w])", "Least squares fit of Hermite series to data.", "hermetrim(c[, tol])", "Remove \"small\" \"trailing\" coefficients from a polynomial.", "hermeline(off, scl)", "Hermite series whose graph is a straight line.", "herme2poly(c)", "Convert a Hermite series to a polynomial.", "poly2herme(pol)", "Convert a polynomial to a Hermite series.", "numpy.polynomial"]}, {"name": "How to write a NumPy how-to", "path": "user/how-to-how-to", "type": "User Guide", "text": ["How-tos get straight to the point \u2013 they", "\u201cI need to refuel my car.\u201d", "Add helpful details for newcomers (\u201cHayseed Road\u201d, even though it\u2019s the only turnoff at three km/mi). But not irrelevant ones:", "If there\u2019s related background (tutorial, explanation, reference, alternative approach), bring it to the user\u2019s attention with a link (\u201cDirections from Route 7,\u201d \u201cWhy so few filling stations?\u201d).", "If the information is already documented and succinct enough for a how-to, just link to it, possibly after an introduction (\u201cThree km/mi, take a right\u201d).", "\u201cI want to see the sights.\u201d", "The See the sights how-to should link to a set of narrower how-tos:", "and these might in turn link to still narrower how-tos \u2013 so the town center page might link to", "By organizing how-tos this way, you not only display the options for people who need to narrow their question, you also have provided answers for users who start with narrower questions (\u201cI want to see historic buildings,\u201d \u201cWhich way to city hall?\u201d).", "If a how-to has many steps:", "People use the terms \u201chow-to\u201d and \u201ctutorial\u201d interchangeably, but we draw a distinction, following Daniele Procida\u2019s taxonomy of documentation.", "Documentation needs to meet users where they are. How-tos offer get-it-done information; the user wants steps to copy and doesn\u2019t necessarily want to understand NumPy. Tutorials are warm-fuzzy information; the user wants a feel for some aspect of NumPy (and again, may or may not care about deeper knowledge).", "We distinguish both tutorials and how-tos from Explanations, which are deep dives intended to give understanding rather than immediate assistance, and References, which give complete, authoritative data on some concrete part of NumPy (like its API) but aren\u2019t obligated to paint a broader picture.", "For more on tutorials, see Learn to write a NumPy tutorial", "Yes \u2013 until the sections with question-mark headings; they explain rather than giving directions. In a how-to, those would be links."]}, {"name": "I/O with NumPy", "path": "user/basics.io", "type": "User Guide", "text": []}, {"name": "include statements", "path": "f2py/signature-file", "type": "Signature file", "text": ["The syntax specification for signature files (.pyf files) is modeled on the Fortran 90/95 language specification. Almost all Fortran 90/95 standard constructs are understood, both in free and fixed format (recall that Fortran 77 is a subset of Fortran 90/95). F2PY introduces some extensions to the Fortran 90/95 language specification that help in the design of the Fortran to Python interface, making it more \u201cPythonic\u201d.", "Signature files may contain arbitrary Fortran code so that any Fortran 90/95 codes can be treated as signature files. F2PY silently ignores Fortran constructs that are irrelevant for creating the interface. However, this also means that syntax errors are not caught by F2PY and will only be caught when the library is built.", "In general, the contents of the signature files are case-sensitive. When scanning Fortran codes to generate a signature file, F2PY lowers all cases automatically except in multi-line blocks or when the --no-lower option is used.", "The syntax of signature files is presented below.", "A signature file may contain one (recommended) or more python\nmodule blocks. The python module block describes the contents of a Python/C extension module <modulename>module.c that F2PY generates.", "Warning", "Exception: if <modulename> contains a substring __user__, then the corresponding python module block describes the signatures of call-back functions (see Call-back arguments).", "A python module block has the following structure:", "Here brackets [] indicate an optional section, dots ... indicate one or more of a previous section. So, []... is to be read as zero or more of a previous section.", "The signature of a Fortran routine has the following structure:", "From a Fortran routine signature F2PY generates a Python/C extension function that has the following signature:", "The signature of a Fortran block data has the following structure:", "The definition of the <argument/variable type declaration> part is", "where", "and", "If an argument has no <argument type declaration>, its type is determined by applying implicit rules to its name.", "The definition of the <use statement> part is", "where", "The definition of the <common block statement> part is", "where", "The <other statement> part refers to any other Fortran language constructs that are not described above. F2PY ignores most of them except the following:", "If a file <filename> does not exist, the include statement is ignored. Otherwise, the file <filename> is included to a signature file. include statements can be used in any part of a signature file, also outside the Fortran/C routine signature blocks.", "where", "Implicit rules are used to determine the type specification of a variable (from the first-letter of its name) if the variable is not defined using <variable type declaration>. Default implicit rules are given by:", "F2PY generates wrappers for all entry names using the signature of the routine block.", "Note", "The entry statement can be used to describe the signature of an arbitrary subroutine or function allowing F2PY to generate a number of wrappers from only one routine block signature. There are few restrictions while doing this: fortranname cannot be used, callstatement and callprotoargument can be used only if they are valid for all entry routines, etc.", "In addition, F2PY introduces the following statements:", "Uses a Py_BEGIN_ALLOW_THREADS .. Py_END_ALLOW_THREADS block around the call to Fortran/C function.", "Replaces the F2PY generated call statement to Fortran/C function with <C-expr|multi-line block>. The wrapped Fortran/C function is available as (*f2py_func).", "To raise an exception, set f2py_success = 0 in <C-expr|multi-line\nblock>.", "When the callstatement statement is used then F2PY may not generate proper prototypes for Fortran/C functions (because <C-expr> may contain any function calls and F2PY has no way to determine what should be the proper prototype).", "With this statement you can explicitly specify the arguments of the corresponding prototype:", "F2PY allows for the use of an arbitrary <routine name> for a given Fortran/C function. Then this statement is used for the <actual\nFortran/C routine name>.", "If fortranname statement is used without <actual Fortran/C routine name> then a dummy wrapper is generated.", "When this is used inside a python module block, the given C code will be inserted to generated C/API source just before wrapper function definitions.", "Here you can define arbitrary C functions to be used for the initialization of optional arguments.", "For example, if usercode is used twice inside python module block then the second multi-line block is inserted after the definition of the external routines.", "When used inside <routine signature>, then the given C code will be inserted into the corresponding wrapper function just after the declaration of variables but before any C statements. So, the usercode follow-up can contain both declarations and C statements.", "When used inside the first interface block, then the given C code will be inserted at the end of the initialization function of the extension module. This is how the extension modules dictionary can be modified and has many use-cases; for example, to define additional variables.", "This is a multi-line block which will be inserted into the definition of a module methods PyMethodDef-array. It must be a comma-separated list of C arrays (see Extending and Embedding Python documentation for details). pymethoddef statement can be used only inside python\nmodule block.", "The following attributes are used by F2PY:", "The corresponding argument is moved to the end of <optional\narguments> list. A default value for an optional argument can be specified via <init_expr>, see the entitydecl definition.", "Note", "The corresponding argument with this attribute considered mandatory. This is the default. required should only be specified if there is a need to disable the automatic optional setting when <init_expr> is used.", "If a Python None object is used as a required argument, the argument is treated as optional. That is, in the case of array argument, the memory is allocated. If <init_expr> is given, then the corresponding initialization is carried out.", "The corresponding variable is considered as an array with dimensions given in <arrayspec>.", "This specifies the \u201cintention\u201d of the corresponding argument. <intentspec> is a comma separated list of the following keys:", "The corresponding argument is considered to be input-only. This means that the value of the argument is passed to a Fortran/C function and that the function is expected to not change the value of this argument.", "The corresponding argument is marked for input/output or as an in situ output argument. intent(inout) arguments can be only \u201ccontiguous\u201d NumPy arrays with proper type and size. Here \u201ccontiguous\u201d can be either in the Fortran or C sense. The latter coincides with the default contiguous concept used in NumPy and is effective only if intent(c) is used. F2PY assumes Fortran contiguous arguments by default.", "Note", "Using intent(inout) is generally not recommended, use intent(in,out) instead.", "See also the intent(inplace) attribute.", "The corresponding argument is considered to be an input/output or in situ output argument. intent(inplace) arguments must be NumPy arrays of a proper size. If the type of an array is not \u201cproper\u201d or the array is non-contiguous then the array will be modified in-place to fix the type and make it contiguous.", "Note", "Using intent(inplace) is generally not recommended either.", "For example, when slices have been taken from an intent(inplace) argument then after in-place changes, the data pointers for the slices may point to an unallocated memory area.", "The corresponding argument is considered to be a return variable. It is appended to the <returned variables> list. Using intent(out) sets intent(hide) automatically, unless intent(in) or intent(inout) are specified as well.", "By default, returned multidimensional arrays are Fortran-contiguous. If intent(c) attribute is used, then the returned multidimensional arrays are C-contiguous.", "The corresponding argument is removed from the list of required or optional arguments. Typically intent(hide) is used with intent(out) or when <init_expr> completely determines the value of the argument like in the following example:", "The corresponding argument is treated as a C scalar or C array argument. For the case of a scalar argument, its value is passed to a C function as a C scalar argument (recall that Fortran scalar arguments are actually C pointer arguments). For array arguments, the wrapper function is assumed to treat multidimensional arrays as C-contiguous arrays.", "There is no need to use intent(c) for one-dimensional arrays, irrespective of whether the wrapped function is in Fortran or C. This is because the concepts of Fortran- and C contiguity overlap in one-dimensional cases.", "If intent(c) is used as a statement but without an entity declaration list, then F2PY adds the intent(c) attribute to all arguments.", "Also, when wrapping C functions, one must use intent(c) attribute for <routine name> in order to disable Fortran specific F_FUNC(..,..) macros.", "The corresponding argument is treated as junk memory. No Fortran nor C contiguity checks are carried out. Using intent(cache) makes sense only for array arguments, also in conjunction with intent(hide) or optional attributes.", "Ensures that the original contents of intent(in) argument is preserved. Typically used with the intent(in,out) attribute. F2PY creates an optional argument overwrite_<argument name> with the default value 0.", "This indicates that the original contents of the intent(in) argument may be altered by the Fortran/C function. F2PY creates an optional argument overwrite_<argument name> with the default value 1.", "Replaces the returned name with <new name> in the __doc__ string of the wrapper function.", "Constructs an external function suitable for calling Python functions from Fortran. intent(callback) must be specified before the corresponding external statement. If the \u2018argument\u2019 is not in the argument list then it will be added to Python wrapper but only by initializing an external function.", "Note", "Use intent(callback) in situations where the Fortran/C code assumes that the user implemented a function with a given prototype and linked it to an executable. Don\u2019t use intent(callback) if the function appears in the argument list of a Fortran routine.", "With intent(hide) or optional attributes specified and using a wrapper function without specifying the callback argument in the argument list; then the call-back function is assumed to be found in the namespace of the F2PY generated extension module where it can be set as a module attribute by a user.", "Defines an auxiliary C variable in the F2PY generated wrapper function. Useful to save parameter values so that they can be accessed in initialization expressions for other variables.", "Note", "intent(aux) silently implies intent(c).", "The following rules apply:", "If none of intent(in | inout | out | hide) are specified, intent(in) is assumed.", "If intent(copy) or intent(overwrite) is used, then an additional optional argument is introduced with a name overwrite_<argument name> and a default value 0 or 1, respectively.", "Performs a consistency check on the arguments by evaluating <C-booleanexpr>; if <C-booleanexpr> returns 0, an exception is raised.", "Note", "If check(..) is not used then F2PY automatically generates a few standard checks (e.g. in a case of an array argument, it checks for the proper shape and size). Use check() to disable checks generated by F2PY.", "This declares that the corresponding argument depends on the values of variables in the <names> list. For example, <init_expr> may use the values of other arguments. Using information given by depend(..) attributes, F2PY ensures that arguments are initialized in a proper order. If the depend(..) attribute is not used then F2PY determines dependence relations automatically. Use depend() to disable the dependence relations generated by F2PY.", "When you edit dependence relations that were initially generated by F2PY, be careful not to break the dependence relations of other relevant variables. Another thing to watch out for is cyclic dependencies. F2PY is able to detect cyclic dependencies when constructing wrappers and it complains if any are found.", "The corresponding variable is a Fortran 90 allocatable array defined as Fortran 90 module data.", "The corresponding argument is a function provided by user. The signature of this call-back function can be defined", "For example, F2PY generates from:", "the following call-back signatures:", "The corresponding user-provided Python function are then:", "See also the intent(callback) attribute.", "This indicates that the corresponding variable is a parameter and it must have a fixed value. F2PY replaces all parameter occurrences by their corresponding values.", "The F2PY directives allow using F2PY signature file constructs in Fortran 77/90 source codes. With this feature one can (almost) completely skip the intermediate signature file generation and apply F2PY directly to Fortran source codes.", "F2PY directives have the following form:", "where allowed comment characters for fixed and free format Fortran codes are cC*!# and !, respectively. Everything that follows <comment char>f2py is ignored by a compiler but read by F2PY as a normal non-comment Fortran line:", "Note", "When F2PY finds a line with F2PY directive, the directive is first replaced by 5 spaces and then the line is reread.", "For fixed format Fortran codes, <comment char> must be at the first column of a file, of course. For free format Fortran codes, the F2PY directives can appear anywhere in a file.", "C expressions are used in the following parts of signature files:", "A C expression may contain:", "the following CPP macros:", "For initializing an array <array name>, F2PY generates a loop over all indices and dimensions that executes the following pseudo-statement:", "where _i[<i>] refers to the <i>-th index value and that runs from 0 to shape(<array name>,<i>)-1.", "For example, a function myrange(n) generated from the following signature", "is equivalent to numpy.arange(n,dtype=float).", "Warning", "F2PY may lower cases also in C expressions when scanning Fortran codes (see --[no]-lower option).", "A multi-line block starts with ''' (triple single-quotes) and ends with ''' in some strictly subsequent line. Multi-line blocks can be used only within .pyf files. The contents of a multi-line block can be arbitrary (except that it cannot contain ''') and no transformations (e.g. lowering cases) are applied to it.", "Currently, multi-line blocks can be used in the following constructs:"]}, {"name": "Indexing on ndarrays", "path": "user/basics.indexing", "type": "User Guide", "text": ["See also", "Indexing routines", "ndarrays can be indexed using the standard Python x[obj] syntax, where x is the array and obj the selection. There are different kinds of indexing available depending on obj: basic indexing, advanced indexing and field access.", "Most of the following examples show the use of indexing when referencing data in an array. The examples work just as well when assigning to an array. See Assigning values to indexed arrays for specific examples and explanations on how assignments work.", "Note that in Python, x[(exp1, exp2, ..., expN)] is equivalent to x[exp1, exp2, ..., expN]; the latter is just syntactic sugar for the former.", "Single element indexing works exactly like that for other standard Python sequences. It is 0-based, and accepts negative indices for indexing from the end of the array.", "It is not necessary to separate each dimension\u2019s index into its own set of square brackets.", "Note that if one indexes a multidimensional array with fewer indices than dimensions, one gets a subdimensional array. For example:", "That is, each index specified selects the array corresponding to the rest of the dimensions selected. In the above example, choosing 0 means that the remaining dimension of length 5 is being left unspecified, and that what is returned is an array of that dimensionality and size. It must be noted that the returned array is a view, i.e., it is not a copy of the original, but points to the same values in memory as does the original array. In this case, the 1-D array at the first position (0) is returned. So using a single index on the returned array, results in a single element being returned. That is:", "So note that x[0, 2] == x[0][2] though the second case is more inefficient as a new temporary array is created after the first index that is subsequently indexed by 2.", "Note", "NumPy uses C-order indexing. That means that the last index usually represents the most rapidly changing memory location, unlike Fortran or IDL, where the first index represents the most rapidly changing location in memory. This difference represents a great potential for confusion.", "Basic slicing extends Python\u2019s basic concept of slicing to N dimensions. Basic slicing occurs when obj is a slice object (constructed by start:stop:step notation inside of brackets), an integer, or a tuple of slice objects and integers. Ellipsis and newaxis objects can be interspersed with these as well.", "Deprecated since version 1.15.0: In order to remain backward compatible with a common usage in Numeric, basic slicing is also initiated if the selection object is any non-ndarray and non-tuple sequence (such as a list) containing slice objects, the Ellipsis object, or the newaxis object, but not for integer arrays or other embedded sequences.", "The simplest case of indexing with N integers returns an array scalar representing the corresponding item. As in Python, all indices are zero-based: for the i-th index \\(n_i\\), the valid range is \\(0 \\le n_i < d_i\\) where \\(d_i\\) is the i-th element of the shape of the array. Negative indices are interpreted as counting from the end of the array (i.e., if \\(n_i < 0\\), it means \\(n_i + d_i\\)).", "All arrays generated by basic slicing are always views of the original array.", "Note", "NumPy slicing creates a view instead of a copy as in the case of built-in Python sequences such as string, tuple and list. Care must be taken when extracting a small portion from a large array which becomes useless after the extraction, because the small portion extracted contains a reference to the large original array whose memory will not be released until all arrays derived from it are garbage-collected. In such cases an explicit copy() is recommended.", "The standard rules of sequence slicing apply to basic slicing on a per-dimension basis (including using a step index). Some useful concepts to remember include:", "The basic slice syntax is i:j:k where i is the starting index, j is the stopping index, and k is the step (\\(k\\neq0\\)). This selects the m elements (in the corresponding dimension) with index values i, i + k, \u2026, i + (m - 1) k where \\(m = q + (r\\neq0)\\) and q and r are the quotient and remainder obtained by dividing j - i by k: j - i = q k + r, so that i + (m - 1) k < j. For example:", "Negative i and j are interpreted as n + i and n + j where n is the number of elements in the corresponding dimension. Negative k makes stepping go towards smaller indices. From the above example:", "Assume n is the number of elements in the dimension being sliced. Then, if i is not given it defaults to 0 for k > 0 and n - 1 for k < 0 . If j is not given it defaults to n for k > 0 and -n-1 for k < 0 . If k is not given it defaults to 1. Note that :: is the same as : and means select all indices along this axis. From the above example:", "If the number of objects in the selection tuple is less than N, then : is assumed for any subsequent dimensions. For example:", "Basic slicing with more than one non-: entry in the slicing tuple, acts like repeated application of slicing using a single non-: entry, where the non-: entries are successively taken (with all other non-: entries replaced by :). Thus, x[ind1, ..., ind2,:] acts like x[ind1][..., ind2, :] under basic slicing.", "Warning", "The above is not true for advanced indexing.", "There are some tools to facilitate the easy matching of array shapes with expressions and in assignments.", "Ellipsis expands to the number of : objects needed for the selection tuple to index all dimensions. In most cases, this means that the length of the expanded selection tuple is x.ndim. There may only be a single ellipsis present. From the above example:", "This is equivalent to:", "Each newaxis object in the selection tuple serves to expand the dimensions of the resulting selection by one unit-length dimension. The added dimension is the position of the newaxis object in the selection tuple. newaxis is an alias for None, and None can be used in place of this with the same result. From the above example:", "This can be handy to combine two arrays in a way that otherwise would require explicit reshaping operations. For example:", "Advanced indexing is triggered when the selection object, obj, is a non-tuple sequence object, an ndarray (of data type integer or bool), or a tuple with at least one sequence object or ndarray (of data type integer or bool). There are two types of advanced indexing: integer and Boolean.", "Advanced indexing always returns a copy of the data (contrast with basic slicing that returns a view).", "Warning", "The definition of advanced indexing means that x[(1, 2, 3),] is fundamentally different than x[(1, 2, 3)]. The latter is equivalent to x[1, 2, 3] which will trigger basic selection while the former will trigger advanced indexing. Be sure to understand why this occurs.", "Also recognize that x[[1, 2, 3]] will trigger advanced indexing, whereas due to the deprecated Numeric compatibility mentioned above, x[[1, 2, slice(None)]] will trigger basic slicing.", "Integer array indexing allows selection of arbitrary items in the array based on their N-dimensional index. Each integer array represents a number of indices into that dimension.", "Negative values are permitted in the index arrays and work as they do with single indices or slices:", "If the index values are out of bounds then an IndexError is thrown:", "When the index consists of as many integer arrays as dimensions of the array being indexed, the indexing is straightforward, but different from slicing.", "Advanced indices always are broadcast and iterated as one:", "Note that the resulting shape is identical to the (broadcast) indexing array shapes ind_1, ..., ind_N. If the indices cannot be broadcast to the same shape, an exception IndexError: shape mismatch: indexing arrays could\nnot be broadcast together with shapes... is raised.", "Indexing with multidimensional index arrays tend to be more unusual uses, but they are permitted, and they are useful for some problems. We\u2019ll start with the simplest multidimensional case:", "In this case, if the index arrays have a matching shape, and there is an index array for each dimension of the array being indexed, the resultant array has the same shape as the index arrays, and the values correspond to the index set for each position in the index arrays. In this example, the first index value is 0 for both index arrays, and thus the first value of the resultant array is y[0, 0]. The next value is y[2, 1], and the last is y[4, 2].", "If the index arrays do not have the same shape, there is an attempt to broadcast them to the same shape. If they cannot be broadcast to the same shape, an exception is raised:", "The broadcasting mechanism permits index arrays to be combined with scalars for other indices. The effect is that the scalar value is used for all the corresponding values of the index arrays:", "Jumping to the next level of complexity, it is possible to only partially index an array with index arrays. It takes a bit of thought to understand what happens in such cases. For example if we just use one index array with y:", "It results in the construction of a new array where each value of the index array selects one row from the array being indexed and the resultant array has the resulting shape (number of index elements, size of row).", "In general, the shape of the resultant array will be the concatenation of the shape of the index array (or the shape that all the index arrays were broadcast to) with the shape of any unused dimensions (those not indexed) in the array being indexed.", "From each row, a specific element should be selected. The row index is just [0, 1, 2] and the column index specifies the element to choose for the corresponding row, here [0, 1, 0]. Using both together the task can be solved using advanced indexing:", "To achieve a behaviour similar to the basic slicing above, broadcasting can be used. The function ix_ can help with this broadcasting. This is best understood with an example.", "From a 4x3 array the corner elements should be selected using advanced indexing. Thus all elements for which the column is one of [0, 2] and the row is one of [0, 3] need to be selected. To use advanced indexing one needs to select all elements explicitly. Using the method explained previously one could write:", "However, since the indexing arrays above just repeat themselves, broadcasting can be used (compare operations such as rows[:, np.newaxis] + columns) to simplify this:", "This broadcasting can also be achieved using the function ix_:", "Note that without the np.ix_ call, only the diagonal elements would be selected:", "This difference is the most important thing to remember about indexing with multiple advanced indices.", "A real-life example of where advanced indexing may be useful is for a color lookup table where we want to map the values of an image into RGB triples for display. The lookup table could have a shape (nlookup, 3). Indexing such an array with an image with shape (ny, nx) with dtype=np.uint8 (or any integer type so long as values are with the bounds of the lookup table) will result in an array of shape (ny, nx, 3) where a triple of RGB values is associated with each pixel location.", "This advanced indexing occurs when obj is an array object of Boolean type, such as may be returned from comparison operators. A single boolean index array is practically identical to x[obj.nonzero()] where, as described above, obj.nonzero() returns a tuple (of length obj.ndim) of integer index arrays showing the True elements of obj. However, it is faster when obj.shape == x.shape.", "If obj.ndim == x.ndim, x[obj] returns a 1-dimensional array filled with the elements of x corresponding to the True values of obj. The search order will be row-major, C-style. If obj has True values at entries that are outside of the bounds of x, then an index error will be raised. If obj is smaller than x it is identical to filling it with False.", "A common use case for this is filtering for desired element values. For example, one may wish to select all entries from an array which are not NaN:", "Or wish to add a constant to all negative elements:", "In general if an index includes a Boolean array, the result will be identical to inserting obj.nonzero() into the same position and using the integer array indexing mechanism described above. x[ind_1, boolean_array, ind_2] is equivalent to x[(ind_1,) + boolean_array.nonzero() + (ind_2,)].", "If there is only one Boolean array and no integer indexing array present, this is straightforward. Care must only be taken to make sure that the boolean index has exactly as many dimensions as it is supposed to work with.", "In general, when the boolean array has fewer dimensions than the array being indexed, this is equivalent to x[b, ...], which means x is indexed by b followed by as many : as are needed to fill out the rank of x. Thus the shape of the result is one dimension containing the number of True elements of the boolean array, followed by the remaining dimensions of the array being indexed:", "Here the 4th and 5th rows are selected from the indexed array and combined to make a 2-D array.", "From an array, select all rows which sum up to less or equal two:", "Combining multiple Boolean indexing arrays or a Boolean with an integer indexing array can best be understood with the obj.nonzero() analogy. The function ix_ also supports boolean arrays and will work without any surprises.", "Use boolean indexing to select all rows adding up to an even number. At the same time columns 0 and 2 should be selected with an advanced integer index. Using the ix_ function this can be done with:", "Without the np.ix_ call, only the diagonal elements would be selected.", "Or without np.ix_ (compare the integer array examples):", "Use a 2-D boolean array of shape (2, 3) with four True elements to select rows from a 3-D array of shape (2, 3, 5) results in a 2-D result of shape (4, 5):", "When there is at least one slice (:), ellipsis (...) or newaxis in the index (or the array has more dimensions than there are advanced indices), then the behaviour can be more complicated. It is like concatenating the indexing result for each advanced index element.", "In the simplest case, there is only a single advanced index combined with a slice. For example:", "In effect, the slice and index array operation are independent. The slice operation extracts columns with index 1 and 2, (i.e. the 2nd and 3rd columns), followed by the index array operation which extracts rows with index 0, 2 and 4 (i.e the first, third and fifth rows). This is equivalent to:", "A single advanced index can, for example, replace a slice and the result array will be the same. However, it is a copy and may have a different memory layout. A slice is preferable when it is possible. For example:", "The easiest way to understand a combination of multiple advanced indices may be to think in terms of the resulting shape. There are two parts to the indexing operation, the subspace defined by the basic indexing (excluding integers) and the subspace from the advanced indexing part. Two cases of index combination need to be distinguished:", "In the first case, the dimensions resulting from the advanced indexing operation come first in the result array, and the subspace dimensions after that. In the second case, the dimensions from the advanced indexing operations are inserted into the result array at the same spot as they were in the initial array (the latter logic is what makes simple advanced indexing behave just like slicing).", "Suppose x.shape is (10, 20, 30) and ind is a (2, 3, 4)-shaped indexing intp array, then result = x[..., ind, :] has shape (10, 2, 3, 4, 30) because the (20,)-shaped subspace has been replaced with a (2, 3, 4)-shaped broadcasted indexing subspace. If we let i, j, k loop over the (2, 3, 4)-shaped subspace then result[..., i, j, k, :] = x[..., ind[i, j, k], :]. This example produces the same result as x.take(ind, axis=-2).", "Let x.shape be (10, 20, 30, 40, 50) and suppose ind_1 and ind_2 can be broadcast to the shape (2, 3, 4). Then x[:, ind_1, ind_2] has shape (10, 2, 3, 4, 40, 50) because the (20, 30)-shaped subspace from X has been replaced with the (2, 3, 4) subspace from the indices. However, x[:, ind_1, :, ind_2] has shape (2, 3, 4, 10, 30, 50) because there is no unambiguous place to drop in the indexing subspace, thus it is tacked-on to the beginning. It is always possible to use .transpose() to move the subspace anywhere desired. Note that this example cannot be replicated using take.", "Slicing can be combined with broadcasted boolean indices:", "See also", "Structured arrays", "If the ndarray object is a structured array the fields of the array can be accessed by indexing the array with strings, dictionary-like.", "Indexing x['field-name'] returns a new view to the array, which is of the same shape as x (except when the field is a sub-array) but of data type x.dtype['field-name'] and contains only the part of the data in the specified field. Also, record array scalars can be \u201cindexed\u201d this way.", "Indexing into a structured array can also be done with a list of field names, e.g. x[['field-name1', 'field-name2']]. As of NumPy 1.16, this returns a view containing only those fields. In older versions of NumPy, it returned a copy. See the user guide section on Structured arrays for more information on multifield indexing.", "If the accessed field is a sub-array, the dimensions of the sub-array are appended to the shape of the result. For example:", "x.flat returns an iterator that will iterate over the entire array (in C-contiguous style with the last index varying the fastest). This iterator object can also be indexed using basic slicing or advanced indexing as long as the selection object is not a tuple. This should be clear from the fact that x.flat is a 1-dimensional view. It can be used for integer indexing with 1-dimensional C-style-flat indices. The shape of any returned array is therefore the shape of the integer indexing object.", "As mentioned, one can select a subset of an array to assign to using a single index, slices, and index and mask arrays. The value being assigned to the indexed array must be shape consistent (the same shape or broadcastable to the shape the index produces). For example, it is permitted to assign a constant to a slice:", "or an array of the right size:", "Note that assignments may result in changes if assigning higher types to lower types (like floats to ints) or even exceptions (assigning complex to floats or ints):", "Unlike some of the references (such as array and mask indices) assignments are always made to the original data in the array (indeed, nothing else would make sense!). Note though, that some actions may not work as one may naively expect. This particular example is often surprising to people:", "Where people expect that the 1st location will be incremented by 3. In fact, it will only be incremented by 1. The reason is that a new array is extracted from the original (as a temporary) containing the values at 1, 1, 3, 1, then the value 1 is added to the temporary, and then the temporary is assigned back to the original array. Thus the value of the array at x[1] + 1 is assigned to x[1] three times, rather than being incremented 3 times.", "The indexing syntax is very powerful but limiting when dealing with a variable number of indices. For example, if you want to write a function that can handle arguments with various numbers of dimensions without having to write special case code for each number of possible dimensions, how can that be done? If one supplies to the index a tuple, the tuple will be interpreted as a list of indices. For example:", "So one can use code to construct tuples of any number of indices and then use these within an index.", "Slices can be specified within programs by using the slice() function in Python. For example:", "Likewise, ellipsis can be specified by code by using the Ellipsis object:", "For this reason, it is possible to use the output from the np.nonzero() function directly as an index since it always returns a tuple of index arrays.", "Because the special treatment of tuples, they are not automatically converted to an array as a list would be. As an example:", "These are some detailed notes, which are not of importance for day to day indexing (in no particular order):"]}, {"name": "Indexing routines", "path": "reference/arrays.indexing", "type": "Indexing routines", "text": ["See also", "Indexing on ndarrays", "c_", "Translates slice objects to concatenation along the second axis.", "r_", "Translates slice objects to concatenation along the first axis.", "s_", "A nicer way to build up index tuples for arrays.", "nonzero(a)", "Return the indices of the elements that are non-zero.", "where(condition, [x, y], /)", "Return elements chosen from x or y depending on condition.", "indices(dimensions[, dtype, sparse])", "Return an array representing the indices of a grid.", "ix_(*args)", "Construct an open mesh from multiple sequences.", "ogrid", "nd_grid instance which returns an open multi-dimensional \"meshgrid\".", "ravel_multi_index(multi_index, dims[, mode, ...])", "Converts a tuple of index arrays into an array of flat indices, applying boundary modes to the multi-index.", "unravel_index(indices, shape[, order])", "Converts a flat index or array of flat indices into a tuple of coordinate arrays.", "diag_indices(n[, ndim])", "Return the indices to access the main diagonal of an array.", "diag_indices_from(arr)", "Return the indices to access the main diagonal of an n-dimensional array.", "mask_indices(n, mask_func[, k])", "Return the indices to access (n, n) arrays, given a masking function.", "tril_indices(n[, k, m])", "Return the indices for the lower-triangle of an (n, m) array.", "tril_indices_from(arr[, k])", "Return the indices for the lower-triangle of arr.", "triu_indices(n[, k, m])", "Return the indices for the upper-triangle of an (n, m) array.", "triu_indices_from(arr[, k])", "Return the indices for the upper-triangle of arr.", "take(a, indices[, axis, out, mode])", "Take elements from an array along an axis.", "take_along_axis(arr, indices, axis)", "Take values from the input array by matching 1d index and data slices.", "choose(a, choices[, out, mode])", "Construct an array from an index array and a list of arrays to choose from.", "compress(condition, a[, axis, out])", "Return selected slices of an array along given axis.", "diag(v[, k])", "Extract a diagonal or construct a diagonal array.", "diagonal(a[, offset, axis1, axis2])", "Return specified diagonals.", "select(condlist, choicelist[, default])", "Return an array drawn from elements in choicelist, depending on conditions.", "lib.stride_tricks.sliding_window_view(x, ...)", "Create a sliding window view into the array with the given window shape.", "lib.stride_tricks.as_strided(x[, shape, ...])", "Create a view into the array with the given shape and strides.", "place(arr, mask, vals)", "Change elements of an array based on conditional and input values.", "put(a, ind, v[, mode])", "Replaces specified elements of an array with given values.", "put_along_axis(arr, indices, values, axis)", "Put values into the destination array by matching 1d index and data slices.", "putmask(a, mask, values)", "Changes elements of an array based on conditional and input values.", "fill_diagonal(a, val[, wrap])", "Fill the main diagonal of the given array of any dimensionality.", "nditer(op[, flags, op_flags, op_dtypes, ...])", "Efficient multi-dimensional iterator object to iterate over arrays.", "ndenumerate(arr)", "Multidimensional index iterator.", "ndindex(*shape)", "An N-dimensional iterator object to index arrays.", "nested_iters(op, axes[, flags, op_flags, ...])", "Create nditers for use in nested loops", "flatiter()", "Flat iterator object to iterate over arrays.", "lib.Arrayterator(var[, buf_size])", "Buffered iterator for big arrays."]}, {"name": "Input and output", "path": "reference/routines.io", "type": "Input and output", "text": ["load(file[, mmap_mode, allow_pickle, ...])", "Load arrays or pickled objects from .npy, .npz or pickled files.", "save(file, arr[, allow_pickle, fix_imports])", "Save an array to a binary file in NumPy .npy format.", "savez(file, *args, **kwds)", "Save several arrays into a single file in uncompressed .npz format.", "savez_compressed(file, *args, **kwds)", "Save several arrays into a single file in compressed .npz format.", "The format of these binary file types is documented in numpy.lib.format", "loadtxt(fname[, dtype, comments, delimiter, ...])", "Load data from a text file.", "savetxt(fname, X[, fmt, delimiter, newline, ...])", "Save an array to a text file.", "genfromtxt(fname[, dtype, comments, ...])", "Load data from a text file, with missing values handled as specified.", "fromregex(file, regexp, dtype[, encoding])", "Construct an array from a text file, using regular expression parsing.", "fromstring(string[, dtype, count, like])", "A new 1-D array initialized from text data in a string.", "ndarray.tofile(fid[, sep, format])", "Write array to a file as text or binary (default).", "ndarray.tolist()", "Return the array as an a.ndim-levels deep nested list of Python scalars.", "fromfile(file[, dtype, count, sep, offset, like])", "Construct an array from data in a text or binary file.", "ndarray.tofile(fid[, sep, format])", "Write array to a file as text or binary (default).", "array2string(a[, max_line_width, precision, ...])", "Return a string representation of an array.", "array_repr(arr[, max_line_width, precision, ...])", "Return the string representation of an array.", "array_str(a[, max_line_width, precision, ...])", "Return a string representation of the data in an array.", "format_float_positional(x[, precision, ...])", "Format a floating-point scalar as a decimal string in positional notation.", "format_float_scientific(x[, precision, ...])", "Format a floating-point scalar as a decimal string in scientific notation.", "memmap(filename[, dtype, mode, offset, ...])", "Create a memory-map to an array stored in a binary file on disk.", "lib.format.open_memmap(filename[, mode, ...])", "Open a .npy file as a memory-mapped array.", "set_printoptions([precision, threshold, ...])", "Set printing options.", "get_printoptions()", "Return the current print options.", "set_string_function(f[, repr])", "Set a Python function to be used when pretty printing arrays.", "printoptions(*args, **kwargs)", "Context manager for setting print options.", "binary_repr(num[, width])", "Return the binary representation of the input number as a string.", "base_repr(number[, base, padding])", "Return a string representation of a number in the given base system.", "DataSource([destpath])", "A generic data source file (file, http, ftp, ...).", "lib.format", "Binary serialization"]}, {"name": "Install git", "path": "dev/gitwash/git_intro", "type": "Development", "text": ["Developing with git can be done entirely without github. Git is a distributed version control system. In order to use git on your machine you must install it."]}, {"name": "int **cancastscalarkindto", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.cancastscalarkindto", "type": "Python Types and C-Structures", "text": ["Either NULL or an array of NPY_NSCALARKINDS pointers. These pointers should each be either NULL or a pointer to an array of integers (terminated by NPY_NOTYPE) indicating data-types that a scalar of this data-type of the specified kind can be cast to safely (this usually means without losing precision)."]}, {"name": "int *cancastto", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.cancastto", "type": "Python Types and C-Structures", "text": ["Either NULL or an array of integers (terminated by NPY_NOTYPE ) indicated data-types that this data-type can be cast to safely (this usually means without losing precision)."]}, {"name": "int *core_dim_ixs", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.core_dim_ixs", "type": "Python Types and C-Structures", "text": ["Dimension indices in a flattened form; indices of argument k are stored in core_dim_ixs[core_offsets[k] : core_offsets[k] +\ncore_numdims[k]]"]}, {"name": "int *core_num_dims", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.core_num_dims", "type": "Python Types and C-Structures", "text": ["Number of core dimensions of each argument"]}, {"name": "int *core_offsets", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.core_offsets", "type": "Python Types and C-Structures", "text": ["Position of 1st core dimension of each argument in core_dim_ixs, equivalent to cumsum(core_num_dims)"]}, {"name": "int alignment", "path": "reference/c-api/types-and-structures#c.NPY_USE_SETITEM.alignment", "type": "Python Types and C-Structures", "text": ["A number providing alignment information for this data type. Specifically, it shows how far from the start of a 2-element structure (whose first element is a char ), the compiler places an item of this type: offsetof(struct {char c; type v;},\nv)"]}, {"name": "int argmax()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.argmax", "type": "Python Types and C-Structures", "text": ["A pointer to a function that retrieves the index of the largest of n elements in arr beginning at the element pointed to by data. This function requires that the memory segment be contiguous and behaved. The return value is always 0. The index of the largest element is returned in max_ind."]}, {"name": "int argmin()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.argmin", "type": "Python Types and C-Structures", "text": ["A pointer to a function that retrieves the index of the smallest of n elements in arr beginning at the element pointed to by data. This function requires that the memory segment be contiguous and behaved. The return value is always 0. The index of the smallest element is returned in min_ind."]}, {"name": "int argsort()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.argsort", "type": "Python Types and C-Structures", "text": ["An array of function pointers to sorting algorithms for this data type. The same sorting algorithms as for sort are available. The indices producing the sort are returned in result (which must be initialized with indices 0 to length-1 inclusive)."]}, {"name": "int compare()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.compare", "type": "Python Types and C-Structures", "text": ["A pointer to a function that compares two elements of the array, arr, pointed to by d1 and d2. This function requires behaved (aligned and not swapped) arrays. The return value is 1 if * d1 > * d2, 0 if * d1 == * d2, and -1 if * d1 < * d2. The array object arr is used to retrieve itemsize and field information for flexible arrays."]}, {"name": "int core_enabled", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.core_enabled", "type": "Python Types and C-Structures", "text": ["0 for scalar ufuncs; 1 for generalized ufuncs"]}, {"name": "int core_num_dim_ix", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.core_num_dim_ix", "type": "Python Types and C-Structures", "text": ["Number of distinct core dimension names in the signature"]}, {"name": "int doxy_javadoc_example()", "path": "dev/howto-docs", "type": "Development", "text": ["This guide will help you decide what to contribute and how to submit it to the official NumPy documentation.", "The NumPy community has set a firm goal of improving its documentation. We hold regular documentation meetings on Zoom (dates are announced on the numpy-discussion mailing list), and everyone is welcome. Reach out if you have questions or need someone to guide you through your first steps \u2013 we\u2019re happy to help. Minutes are taken on hackmd.io and stored in the NumPy Archive repository.", "The NumPy Documentation has the details covered. API reference documentation is generated directly from docstrings in the code when the documentation is built. Although we have mostly complete reference documentation for each function and class exposed to users, there is a lack of usage examples for some of them.", "What we lack are docs with broader scope \u2013 tutorials, how-tos, and explanations. Reporting defects is another way to contribute. We discuss both.", "We\u2019re eager to hear about and fix doc defects. But to attack the biggest problems we end up having to defer or overlook some bug reports. Here are the best defects to go after.", "Top priority goes to technical inaccuracies \u2013 a docstring missing a parameter, a faulty description of a function/parameter/method, and so on. Other \u201cstructural\u201d defects like broken links also get priority. All these fixes are easy to confirm and put in place. You can submit a pull request (PR) with the fix, if you know how to do that; otherwise please open an issue.", "Typos and misspellings fall on a lower rung; we welcome hearing about them but may not be able to fix them promptly. These too can be handled as pull requests or issues.", "Obvious wording mistakes (like leaving out a \u201cnot\u201d) fall into the typo category, but other rewordings \u2013 even for grammar \u2013 require a judgment call, which raises the bar. Test the waters by first presenting the fix as an issue.", "Some functions/objects like numpy.ndarray.transpose, numpy.array etc. defined in C-extension modules have their docstrings defined separately in _add_newdocs.py", "Your frustrations using our documents are our best guide to what needs fixing.", "If you write a missing doc you join the front line of open source, but it\u2019s a meaningful contribution just to let us know what\u2019s missing. If you want to compose a doc, run your thoughts by the mailing list for further ideas and feedback. If you want to alert us to a gap, open an issue. See this issue for an example.", "If you\u2019re looking for subjects, our formal roadmap for documentation is a NumPy Enhancement Proposal (NEP), NEP 44 - Restructuring the NumPy Documentation. It identifies areas where our docs need help and lists several additions we\u2019d like to see, including Jupyter notebooks.", "There are formulas for writing useful documents, and four formulas cover nearly everything. There are four formulas because there are four categories of document \u2013 tutorial, how-to guide, explanation, and reference. The insight that docs divide up this way belongs to Daniele Procida and his Di\u00e1taxis Framework. When you begin a document or propose one, have in mind which of these types it will be.", "In addition to the documentation that is part of the NumPy source tree, you can submit content in Jupyter Notebook format to the NumPy Tutorials page. This set of tutorials and educational materials is meant to provide high-quality resources by the NumPy project, both for self-learning and for teaching classes with. These resources are developed in a separate GitHub repository, numpy-tutorials, where you can check out existing notebooks, open issues to suggest new topics or submit your own tutorials as pull requests.", "Don\u2019t worry if English is not your first language, or if you can only come up with a rough draft. Open source is a community effort. Do your best \u2013 we\u2019ll help fix issues.", "Images and real-life data make text more engaging and powerful, but be sure what you use is appropriately licensed and available. Here again, even a rough idea for artwork can be polished by others.", "For now, the only data formats accepted by NumPy are those also used by other Python scientific libraries like pandas, SciPy, or Matplotlib. We\u2019re developing a package to accept more formats; contact us for details.", "NumPy documentation is kept in the source code tree. To get your document into the docbase you must download the tree, build it, and submit a pull request. If GitHub and pull requests are new to you, check our Contributor Guide.", "Our markup language is reStructuredText (rST), which is more elaborate than Markdown. Sphinx, the tool many Python projects use to build and link project documentation, converts the rST into HTML and other formats. For more on rST, see the Quick reStructuredText Guide or the reStructuredText Primer", "If you run across outside material that would be a useful addition to the NumPy docs, let us know by opening an issue.", "You don\u2019t have to contribute here to contribute to NumPy. You\u2019ve contributed if you write a tutorial on your blog, create a YouTube video, or answer questions on Stack Overflow and other sites.", "NumPy style governs cases where:", "Our current rules:", "When using Sphinx in combination with the NumPy conventions, you should use the numpydoc extension so that your docstrings will be handled correctly. For example, Sphinx will extract the Parameters section from your docstring and convert it into a field list. Using numpydoc will also avoid the reStructuredText errors produced by plain Sphinx when it encounters NumPy docstring conventions like section headers (e.g. -------------) that sphinx does not expect to find in docstrings.", "It is available from:", "Note that for documentation within NumPy, it is not necessary to do import numpy as np at the beginning of an example.", "Please use the numpydoc formatting standard as shown in their example.", "NumPy uses Doxygen to parse specially-formatted C/C++ comment blocks. This generates XML files, which are converted by Breathe into RST, which is used by Sphinx.", "It takes three steps to complete the documentation process:", "Although there is still no commenting style set to follow, the Javadoc is more preferable than the others due to the similarities with the current existing non-indexed comment blocks.", "Note", "Please see \u201cDocumenting the code\u201d.", "This is what Javadoc style looks like:", "And here is how it is rendered:", "This a simple brief. ", "And the details goes here. Multi lines are welcome.", "leave a comment for the returned value. ", "For line comment, you can use a triple forward slash. For example:", "And here is how it is rendered:", "Template to represent limbo numbers. ", "Specializations for integer types that are part of nowhere. It doesn\u2019t support with any real types.", "Type of the integer. Required to be an integer type. ", "Number of elements. ", "Default constructor. Initialize nothing. ", "Set Default behavior for copy the limbo. ", "Returns the raw data for the limbo. ", "Example for inline comment. ", "Note", "For more tags/commands, please take a look at https://www.doxygen.nl/manual/commands.html", "@brief", "Starts a paragraph that serves as a brief description. By default the first sentence of the documentation block is automatically treated as a brief description, since option JAVADOC_AUTOBRIEF is enabled within doxygen configurations.", "@details", "Just like @brief starts a brief description, @details starts the detailed description. You can also start a new paragraph (blank line) then the @details command is not needed.", "@param", "Starts a parameter description for a function parameter with name <parameter-name>, followed by a description of the parameter. The existence of the parameter is checked and a warning is given if the documentation of this (or any other) parameter is missing or not present in the function declaration or definition.", "@return", "Starts a return value description for a function. Multiple adjacent @return commands will be joined into a single paragraph. The @return description ends when a blank line or some other sectioning command is encountered.", "@code/@endcode", "Starts/Ends a block of code. A code block is treated differently from ordinary text. It is interpreted as source code.", "@rst/@endrst", "Starts/Ends a block of reST markup.", "Take a look at the following example:", "And here is how it is rendered:", "A comment block contains reST markup. ", "Some code example:", "Note", "Thanks to Breathe, we were able to bring it to Doxygen", "Not all headers files are collected automatically. You have to add the desired C/C++ header paths within the sub-config files of Doxygen.", "Sub-config files have the unique name .doxyfile, which you can usually find near directories that contain documented headers. You need to create a new config file if there\u2019s not one located in a path close(2-depth) to the headers you want to add.", "Sub-config files can accept any of Doxygen configuration options, but do not override or re-initialize any configuration option, rather only use the concatenation operator \u201c+=\u201d. For example:", "Note", "@CUR_DIR is a template constant returns the current dir path of the sub-config file.", "Breathe provides a wide range of custom directives to allow converting the documents generated by Doxygen into reST files.", "Note", "For more information, please check out \u201cDirectives & Config Variables\u201d", "doxygenfunction", "This directive generates the appropriate output for a single function. The function name is required to be unique in the project.", "Checkout the example to see it in action.", "doxygenclass", "This directive generates the appropriate output for a single class. It takes the standard project, path, outline and no-link options and additionally the members, protected-members, private-members, undoc-members, membergroups and members-only options:", "Checkout the doxygenclass documentation <https://breathe.readthedocs.io/en/latest/class.html#class-example>_ for more details and to see it in action.", "doxygennamespace", "This directive generates the appropriate output for the contents of a namespace. It takes the standard project, path, outline and no-link options and additionally the content-only, members, protected-members, private-members and undoc-members options. To reference a nested namespace, the full namespaced path must be provided, e.g. foo::bar for the bar namespace inside the foo namespace.", "Checkout the doxygennamespace documentation for more details and to see it in action.", "doxygengroup", "This directive generates the appropriate output for the contents of a doxygen group. A doxygen group can be declared with specific doxygen markup in the source comments as covered in the doxygen grouping documentation.", "It takes the standard project, path, outline and no-link options and additionally the content-only, members, protected-members, private-members and undoc-members options.", "Checkout the doxygengroup documentation for more details and to see it in action."]}, {"name": "int elsize", "path": "reference/c-api/types-and-structures#c.NPY_USE_SETITEM.elsize", "type": "Python Types and C-Structures", "text": ["For data types that are always the same size (such as long), this holds the size of the data type. For flexible data types where different arrays can have a different elementsize, this should be 0."]}, {"name": "int flags", "path": "reference/c-api/types-and-structures#c.PyArray_Chunk.flags", "type": "Python Types and C-Structures", "text": ["Any data flags (e.g. NPY_ARRAY_WRITEABLE ) that should be used to interpret the memory."]}, {"name": "int flags", "path": "reference/c-api/types-and-structures#c.NPY_AO.flags", "type": "Python Types and C-Structures", "text": ["Pointed to by the macro PyArray_FLAGS, this data member represents the flags indicating how the memory pointed to by data is to be interpreted. Possible flags are NPY_ARRAY_C_CONTIGUOUS, NPY_ARRAY_F_CONTIGUOUS, NPY_ARRAY_OWNDATA, NPY_ARRAY_ALIGNED, NPY_ARRAY_WRITEABLE, NPY_ARRAY_WRITEBACKIFCOPY, and NPY_ARRAY_UPDATEIFCOPY."]}, {"name": "int flags", "path": "reference/c-api/types-and-structures#c.PyArrayInterface.flags", "type": "Python Types and C-Structures", "text": ["Any of the bits NPY_ARRAY_C_CONTIGUOUS (1), NPY_ARRAY_F_CONTIGUOUS (2), NPY_ARRAY_ALIGNED (0x100), NPY_ARRAY_NOTSWAPPED (0x200), or NPY_ARRAY_WRITEABLE (0x400) to indicate something about the data. The NPY_ARRAY_ALIGNED, NPY_ARRAY_C_CONTIGUOUS, and NPY_ARRAY_F_CONTIGUOUS flags can actually be determined from the other parameters. The flag NPY_ARR_HAS_DESCR (0x800) can also be set to indicate to objects consuming the version 3 array interface that the descr member of the structure is present (it will be ignored by objects consuming version 2 of the array interface)."]}, {"name": "int fromstr()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.fromstr", "type": "Python Types and C-Structures", "text": ["A pointer to a function that converts the string pointed to by str to one element of the corresponding type and places it in the memory location pointed to by ip. After the conversion is completed, *endptr points to the rest of the string. The last argument arr is the array into which ip points (needed for variable-size data- types). Returns 0 on success or -1 on failure. Requires a behaved array. This function should be called without holding the Python GIL, and has to grab it for error reporting."]}, {"name": "int identity", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.identity", "type": "Python Types and C-Structures", "text": ["Either PyUFunc_One, PyUFunc_Zero, PyUFunc_MinusOne, PyUFunc_None, PyUFunc_ReorderableNone, or PyUFunc_IdentityValue to indicate the identity for this operation. It is only used for a reduce-like call on an empty array."]}, {"name": "int itemsize", "path": "reference/c-api/types-and-structures#c.PyArrayInterface.itemsize", "type": "Python Types and C-Structures", "text": ["The number of bytes each item in the array requires."]}, {"name": "int len", "path": "reference/c-api/types-and-structures#c.PyArray_Dims.len", "type": "Python Types and C-Structures", "text": ["The length of the list of integers. It is assumed safe to access ptr [0] to ptr [len-1]."]}, {"name": "int nargs", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.nargs", "type": "Python Types and C-Structures", "text": ["The total number of arguments (nin + nout). This must be less than NPY_MAXARGS."]}, {"name": "int nd", "path": "reference/c-api/types-and-structures#c.NPY_AO.nd", "type": "Python Types and C-Structures", "text": ["An integer providing the number of dimensions for this array. When nd is 0, the array is sometimes called a rank-0 array. Such arrays have undefined dimensions and strides and cannot be accessed. Macro PyArray_NDIM defined in ndarraytypes.h points to this data member. NPY_MAXDIMS is the largest number of dimensions for any array."]}, {"name": "int nd", "path": "reference/c-api/types-and-structures#c.PyArrayMultiIterObject.nd", "type": "Python Types and C-Structures", "text": ["The number of dimensions in the broadcasted result."]}, {"name": "int nd", "path": "reference/c-api/types-and-structures#c.PyArrayInterface.nd", "type": "Python Types and C-Structures", "text": ["the number of dimensions in the array."]}, {"name": "int nout", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.nout", "type": "Python Types and C-Structures", "text": ["The number of output arguments."]}, {"name": "int npy_clear_floatstatus()", "path": "reference/c-api/coremath#c.npy_clear_floatstatus", "type": "NumPy core libraries", "text": ["Clears the floating point status. Returns the previous status mask.", "Note that npy_clear_floatstatus_barrier is preferable as it prevents aggressive compiler optimizations reordering the call relative to the code setting the status, which could lead to incorrect results.", "New in version 1.9.0."]}, {"name": "int npy_clear_floatstatus_barrier()", "path": "reference/c-api/coremath#c.npy_clear_floatstatus_barrier", "type": "NumPy core libraries", "text": ["Clears the floating point status. A pointer to a local variable is passed in to prevent aggressive compiler optimizations from reordering this function call. Returns the previous status mask.", "New in version 1.15.0."]}, {"name": "int npy_get_floatstatus()", "path": "reference/c-api/coremath#c.npy_get_floatstatus", "type": "NumPy core libraries", "text": ["Get floating point status. Returns a bitmask with following possible flags:", "Note that npy_get_floatstatus_barrier is preferable as it prevents aggressive compiler optimizations reordering the call relative to the code setting the status, which could lead to incorrect results.", "New in version 1.9.0."]}, {"name": "int npy_get_floatstatus_barrier()", "path": "reference/c-api/coremath#c.npy_get_floatstatus_barrier", "type": "NumPy core libraries", "text": ["Get floating point status. A pointer to a local variable is passed in to prevent aggressive compiler optimizations from reordering this function call relative to the code setting the status, which could lead to incorrect results.", "Returns a bitmask with following possible flags:", "New in version 1.15.0."]}, {"name": "int npy_half_eq()", "path": "reference/c-api/coremath#c.npy_half_eq", "type": "NumPy core libraries", "text": ["Compares two half-precision floats (h1 == h2)."]}, {"name": "int npy_half_eq_nonan()", "path": "reference/c-api/coremath#c.npy_half_eq_nonan", "type": "NumPy core libraries", "text": ["Compares two half-precision floats that are known to not be NaN (h1 == h2). If a value is NaN, the result is undefined."]}, {"name": "int npy_half_ge()", "path": "reference/c-api/coremath#c.npy_half_ge", "type": "NumPy core libraries", "text": ["Compares two half-precision floats (h1 >= h2)."]}, {"name": "int npy_half_gt()", "path": "reference/c-api/coremath#c.npy_half_gt", "type": "NumPy core libraries", "text": ["Compares two half-precision floats (h1 > h2)."]}, {"name": "int npy_half_isfinite()", "path": "reference/c-api/coremath#c.npy_half_isfinite", "type": "NumPy core libraries", "text": ["Tests whether the half-precision float is finite (not NaN or Inf)."]}, {"name": "int npy_half_isinf()", "path": "reference/c-api/coremath#c.npy_half_isinf", "type": "NumPy core libraries", "text": ["Tests whether the half-precision float is plus or minus Inf."]}, {"name": "int npy_half_isnan()", "path": "reference/c-api/coremath#c.npy_half_isnan", "type": "NumPy core libraries", "text": ["Tests whether the half-precision float is a NaN."]}, {"name": "int npy_half_iszero()", "path": "reference/c-api/coremath#c.npy_half_iszero", "type": "NumPy core libraries", "text": ["Tests whether the half-precision float has a value equal to zero. This may be slightly faster than calling npy_half_eq(h, NPY_ZERO)."]}, {"name": "int npy_half_le()", "path": "reference/c-api/coremath#c.npy_half_le", "type": "NumPy core libraries", "text": ["Compares two half-precision floats (h1 <= h2)."]}, {"name": "int npy_half_le_nonan()", "path": "reference/c-api/coremath#c.npy_half_le_nonan", "type": "NumPy core libraries", "text": ["Compares two half-precision floats that are known to not be NaN (h1 <= h2). If a value is NaN, the result is undefined."]}, {"name": "int npy_half_lt()", "path": "reference/c-api/coremath#c.npy_half_lt", "type": "NumPy core libraries", "text": ["Compares two half-precision floats (h1 < h2)."]}, {"name": "int npy_half_lt_nonan()", "path": "reference/c-api/coremath#c.npy_half_lt_nonan", "type": "NumPy core libraries", "text": ["Compares two half-precision floats that are known to not be NaN (h1 < h2). If a value is NaN, the result is undefined."]}, {"name": "int npy_half_ne()", "path": "reference/c-api/coremath#c.npy_half_ne", "type": "NumPy core libraries", "text": ["Compares two half-precision floats (h1 != h2)."]}, {"name": "int npy_half_signbit()", "path": "reference/c-api/coremath#c.npy_half_signbit", "type": "NumPy core libraries", "text": ["Returns 1 is h is negative, 0 otherwise."]}, {"name": "int NpyIter_CreateCompatibleStrides()", "path": "reference/c-api/iterator#c.NpyIter_CreateCompatibleStrides", "type": "Array Iterator API", "text": ["Builds a set of strides which are the same as the strides of an output array created using the NPY_ITER_ALLOCATE flag, where NULL was passed for op_axes. This is for data packed contiguously, but not necessarily in C or Fortran order. This should be used together with NpyIter_GetShape and NpyIter_GetNDim with the flag NPY_ITER_MULTI_INDEX passed into the constructor.", "A use case for this function is to match the shape and layout of the iterator and tack on one or more dimensions. For example, in order to generate a vector per input value for a numerical gradient, you pass in ndim*itemsize for itemsize, then add another dimension to the end with size ndim and stride itemsize. To do the Hessian matrix, you do the same thing but add two dimensions, or take advantage of the symmetry and pack it into 1 dimension with a particular encoding.", "This function may only be called if the iterator is tracking a multi-index and if NPY_ITER_DONT_NEGATE_STRIDES was used to prevent an axis from being iterated in reverse order.", "If an array is created with this method, simply adding \u2018itemsize\u2019 for each iteration will traverse the new array matching the iterator.", "Returns NPY_SUCCEED or NPY_FAIL."]}, {"name": "int NpyIter_Deallocate()", "path": "reference/c-api/iterator#c.NpyIter_Deallocate", "type": "Array Iterator API", "text": ["Deallocates the iterator object and resolves any needed writebacks.", "Returns NPY_SUCCEED or NPY_FAIL."]}, {"name": "int NpyIter_EnableExternalLoop()", "path": "reference/c-api/iterator#c.NpyIter_EnableExternalLoop", "type": "Array Iterator API", "text": ["If NpyIter_RemoveMultiIndex was called, you may want to enable the flag NPY_ITER_EXTERNAL_LOOP. This flag is not permitted together with NPY_ITER_MULTI_INDEX, so this function is provided to enable the feature after NpyIter_RemoveMultiIndex is called. This function also resets the iterator to its initial state.", "WARNING: This function changes the internal logic of the iterator. Any cached functions or pointers from the iterator must be retrieved again!", "Returns NPY_SUCCEED or NPY_FAIL."]}, {"name": "int NpyIter_GetNDim()", "path": "reference/c-api/iterator#c.NpyIter_GetNDim", "type": "Array Iterator API", "text": ["Returns the number of dimensions being iterated. If a multi-index was not requested in the iterator constructor, this value may be smaller than the number of dimensions in the original objects."]}, {"name": "int NpyIter_GetNOp()", "path": "reference/c-api/iterator#c.NpyIter_GetNOp", "type": "Array Iterator API", "text": ["Returns the number of operands in the iterator."]}, {"name": "int NpyIter_GetShape()", "path": "reference/c-api/iterator#c.NpyIter_GetShape", "type": "Array Iterator API", "text": ["Returns the broadcast shape of the iterator in outshape. This can only be called on an iterator which is tracking a multi-index.", "Returns NPY_SUCCEED or NPY_FAIL."]}, {"name": "int NpyIter_GotoIndex()", "path": "reference/c-api/iterator#c.NpyIter_GotoIndex", "type": "Array Iterator API", "text": ["Adjusts the iterator to point to the index specified. If the iterator was constructed with the flag NPY_ITER_C_INDEX, index is the C-order index, and if the iterator was constructed with the flag NPY_ITER_F_INDEX, index is the Fortran-order index. Returns an error if there is no index being tracked, the index is out of bounds, or inner loop iteration is disabled.", "Returns NPY_SUCCEED or NPY_FAIL."]}, {"name": "int NpyIter_GotoIterIndex()", "path": "reference/c-api/iterator#c.NpyIter_GotoIterIndex", "type": "Array Iterator API", "text": ["Adjusts the iterator to point to the iterindex specified. The IterIndex is an index matching the iteration order of the iterator. Returns an error if the iterindex is out of bounds, buffering is enabled, or inner loop iteration is disabled.", "Returns NPY_SUCCEED or NPY_FAIL."]}, {"name": "int NpyIter_GotoMultiIndex()", "path": "reference/c-api/iterator#c.NpyIter_GotoMultiIndex", "type": "Array Iterator API", "text": ["Adjusts the iterator to point to the ndim indices pointed to by multi_index. Returns an error if a multi-index is not being tracked, the indices are out of bounds, or inner loop iteration is disabled.", "Returns NPY_SUCCEED or NPY_FAIL."]}, {"name": "int NpyIter_RemoveAxis()", "path": "reference/c-api/iterator#c.NpyIter_RemoveAxis", "type": "Array Iterator API", "text": ["Removes an axis from iteration. This requires that NPY_ITER_MULTI_INDEX was set for iterator creation, and does not work if buffering is enabled or an index is being tracked. This function also resets the iterator to its initial state.", "This is useful for setting up an accumulation loop, for example. The iterator can first be created with all the dimensions, including the accumulation axis, so that the output gets created correctly. Then, the accumulation axis can be removed, and the calculation done in a nested fashion.", "WARNING: This function may change the internal memory layout of the iterator. Any cached functions or pointers from the iterator must be retrieved again! The iterator range will be reset as well.", "Returns NPY_SUCCEED or NPY_FAIL."]}, {"name": "int NpyIter_RemoveMultiIndex()", "path": "reference/c-api/iterator#c.NpyIter_RemoveMultiIndex", "type": "Array Iterator API", "text": ["If the iterator is tracking a multi-index, this strips support for them, and does further iterator optimizations that are possible if multi-indices are not needed. This function also resets the iterator to its initial state.", "WARNING: This function may change the internal memory layout of the iterator. Any cached functions or pointers from the iterator must be retrieved again!", "After calling this function, NpyIter_HasMultiIndex(iter) will return false.", "Returns NPY_SUCCEED or NPY_FAIL."]}, {"name": "int NpyIter_Reset()", "path": "reference/c-api/iterator#c.NpyIter_Reset", "type": "Array Iterator API", "text": ["Resets the iterator back to its initial state, at the beginning of the iteration range.", "Returns NPY_SUCCEED or NPY_FAIL. If errmsg is non-NULL, no Python exception is set when NPY_FAIL is returned. Instead, *errmsg is set to an error message. When errmsg is non-NULL, the function may be safely called without holding the Python GIL."]}, {"name": "int NpyIter_ResetBasePointers()", "path": "reference/c-api/iterator#c.NpyIter_ResetBasePointers", "type": "Array Iterator API", "text": ["Resets the iterator back to its initial state, but using the values in baseptrs for the data instead of the pointers from the arrays being iterated. This functions is intended to be used, together with the op_axes parameter, by nested iteration code with two or more iterators.", "Returns NPY_SUCCEED or NPY_FAIL. If errmsg is non-NULL, no Python exception is set when NPY_FAIL is returned. Instead, *errmsg is set to an error message. When errmsg is non-NULL, the function may be safely called without holding the Python GIL.", "TODO: Move the following into a special section on nested iterators.", "Creating iterators for nested iteration requires some care. All the iterator operands must match exactly, or the calls to NpyIter_ResetBasePointers will be invalid. This means that automatic copies and output allocation should not be used haphazardly. It is possible to still use the automatic data conversion and casting features of the iterator by creating one of the iterators with all the conversion parameters enabled, then grabbing the allocated operands with the NpyIter_GetOperandArray function and passing them into the constructors for the rest of the iterators.", "WARNING: When creating iterators for nested iteration, the code must not use a dimension more than once in the different iterators. If this is done, nested iteration will produce out-of-bounds pointers during iteration.", "WARNING: When creating iterators for nested iteration, buffering can only be applied to the innermost iterator. If a buffered iterator is used as the source for baseptrs, it will point into a small buffer instead of the array and the inner iteration will be invalid.", "The pattern for using nested iterators is as follows."]}, {"name": "int NpyIter_ResetToIterIndexRange()", "path": "reference/c-api/iterator#c.NpyIter_ResetToIterIndexRange", "type": "Array Iterator API", "text": ["Resets the iterator and restricts it to the iterindex range [istart, iend). See NpyIter_Copy for an explanation of how to use this for multi-threaded iteration. This requires that the flag NPY_ITER_RANGED was passed to the iterator constructor.", "If you want to reset both the iterindex range and the base pointers at the same time, you can do the following to avoid extra buffer copying (be sure to add the return code error checks when you copy this code).", "Returns NPY_SUCCEED or NPY_FAIL. If errmsg is non-NULL, no Python exception is set when NPY_FAIL is returned. Instead, *errmsg is set to an error message. When errmsg is non-NULL, the function may be safely called without holding the Python GIL."]}, {"name": "int ntypes", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.ntypes", "type": "Python Types and C-Structures", "text": ["The number of supported data types for the ufunc. This number specifies how many different 1-d loops (of the builtin data types) are available."]}, {"name": "int PyArray_AxisConverter()", "path": "reference/c-api/array#c.PyArray_AxisConverter", "type": "Array API", "text": ["Convert a Python object, obj, representing an axis argument to the proper value for passing to the functions that take an integer axis. Specifically, if obj is None, axis is set to NPY_MAXDIMS which is interpreted correctly by the C-API functions that take axis arguments."]}, {"name": "int PyArray_BoolConverter()", "path": "reference/c-api/array#c.PyArray_BoolConverter", "type": "Array API", "text": ["Convert any Python object, obj, to NPY_TRUE or NPY_FALSE, and place the result in value."]}, {"name": "int PyArray_Broadcast()", "path": "reference/c-api/array#c.PyArray_Broadcast", "type": "Array API", "text": ["This function encapsulates the broadcasting rules. The mit container should already contain iterators for all the arrays that need to be broadcast. On return, these iterators will be adjusted so that iteration over each simultaneously will accomplish the broadcasting. A negative number is returned if an error occurs."]}, {"name": "int PyArray_BufferConverter()", "path": "reference/c-api/array#c.PyArray_BufferConverter", "type": "Array API", "text": ["Convert any Python object, obj, with a (single-segment) buffer interface to a variable with members that detail the object\u2019s use of its chunk of memory. The buf variable is a pointer to a structure with base, ptr, len, and flags members. The PyArray_Chunk structure is binary compatible with the Python\u2019s buffer object (through its len member on 32-bit platforms and its ptr member on 64-bit platforms or in Python 2.5). On return, the base member is set to obj (or its base if obj is already a buffer object pointing to another object). If you need to hold on to the memory be sure to INCREF the base member. The chunk of memory is pointed to by buf ->ptr member and has length buf ->len. The flags member of buf is NPY_ARRAY_ALIGNED with the NPY_ARRAY_WRITEABLE flag set if obj has a writeable buffer interface."]}, {"name": "int PyArray_ByteorderConverter()", "path": "reference/c-api/array#c.PyArray_ByteorderConverter", "type": "Array API", "text": ["Convert Python strings into the corresponding byte-order character: \u2018>\u2019, \u2018<\u2019, \u2018s\u2019, \u2018=\u2019, or \u2018|\u2019."]}, {"name": "int PyArray_CanCastArrayTo()", "path": "reference/c-api/array#c.PyArray_CanCastArrayTo", "type": "Array API", "text": ["New in version 1.6.", "Returns non-zero if arr can be cast to totype according to the casting rule given in casting. If arr is an array scalar, its value is taken into account, and non-zero is also returned when the value will not overflow or be truncated to an integer when converting to a smaller type.", "This is almost the same as the result of PyArray_CanCastTypeTo(PyArray_MinScalarType(arr), totype, casting), but it also handles a special case arising because the set of uint values is not a subset of the int values for types with the same number of bits."]}, {"name": "int PyArray_CanCastSafely()", "path": "reference/c-api/array#c.PyArray_CanCastSafely", "type": "Array API", "text": ["Returns non-zero if an array of data type fromtype can be cast to an array of data type totype without losing information. An exception is that 64-bit integers are allowed to be cast to 64-bit floating point values even though this can lose precision on large integers so as not to proliferate the use of long doubles without explicit requests. Flexible array types are not checked according to their lengths with this function."]}, {"name": "int PyArray_CanCastTo()", "path": "reference/c-api/array#c.PyArray_CanCastTo", "type": "Array API", "text": ["PyArray_CanCastTypeTo supersedes this function in NumPy 1.6 and later.", "Equivalent to PyArray_CanCastTypeTo(fromtype, totype, NPY_SAFE_CASTING)."]}, {"name": "int PyArray_CanCastTypeTo()", "path": "reference/c-api/array#c.PyArray_CanCastTypeTo", "type": "Array API", "text": ["New in version 1.6.", "Returns non-zero if an array of data type fromtype (which can include flexible types) can be cast safely to an array of data type totype (which can include flexible types) according to the casting rule casting. For simple types with NPY_SAFE_CASTING, this is basically a wrapper around PyArray_CanCastSafely, but for flexible types such as strings or unicode, it produces results taking into account their sizes. Integer and float types can only be cast to a string or unicode type using NPY_SAFE_CASTING if the string or unicode type is big enough to hold the max value of the integer/float type being cast from."]}, {"name": "int PyArray_CanCoerceScalar()", "path": "reference/c-api/array#c.PyArray_CanCoerceScalar", "type": "Array API", "text": ["See the function PyArray_ResultType for details of NumPy type promotion, updated in NumPy 1.6.0.", "Implements the rules for scalar coercion. Scalars are only silently coerced from thistype to neededtype if this function returns nonzero. If scalar is NPY_NOSCALAR, then this function is equivalent to PyArray_CanCastSafely. The rule is that scalars of the same KIND can be coerced into arrays of the same KIND. This rule means that high-precision scalars will never cause low-precision arrays of the same KIND to be upcast."]}, {"name": "int PyArray_CastingConverter()", "path": "reference/c-api/array#c.PyArray_CastingConverter", "type": "Array API", "text": ["Convert the Python strings \u2018no\u2019, \u2018equiv\u2019, \u2018safe\u2019, \u2018same_kind\u2019, and \u2018unsafe\u2019 into the NPY_CASTING enumeration NPY_NO_CASTING, NPY_EQUIV_CASTING, NPY_SAFE_CASTING, NPY_SAME_KIND_CASTING, and NPY_UNSAFE_CASTING."]}, {"name": "int PyArray_CastTo()", "path": "reference/c-api/array#c.PyArray_CastTo", "type": "Array API", "text": ["As of 1.6, this function simply calls PyArray_CopyInto, which handles the casting.", "Cast the elements of the array in into the array out. The output array should be writeable, have an integer-multiple of the number of elements in the input array (more than one copy can be placed in out), and have a data type that is one of the builtin types. Returns 0 on success and -1 if an error occurs."]}, {"name": "int PyArray_CheckAnyScalar()", "path": "reference/c-api/array#c.PyArray_CheckAnyScalar", "type": "Array API", "text": ["Evaluates true if op is a Python scalar object (see PyArray_IsPythonScalar), an array scalar (an instance of a sub-type of PyGenericArr_Type) or an instance of a sub-type of PyArray_Type whose dimensionality is 0."]}, {"name": "int PyArray_CheckExact()", "path": "reference/c-api/array#c.PyArray_CheckExact", "type": "Array API", "text": ["Evaluates true if op is a Python object with type PyArray_Type."]}, {"name": "int PyArray_CheckScalar()", "path": "reference/c-api/array#c.PyArray_CheckScalar", "type": "Array API", "text": ["Evaluates true if op is either an array scalar (an instance of a sub-type of PyGenericArr_Type ), or an instance of (a sub-class of) PyArray_Type whose dimensionality is 0."]}, {"name": "int PyArray_ClipmodeConverter()", "path": "reference/c-api/array#c.PyArray_ClipmodeConverter", "type": "Array API", "text": ["Convert the Python strings \u2018clip\u2019, \u2018wrap\u2019, and \u2018raise\u2019 into the NPY_CLIPMODE enumeration NPY_CLIP, NPY_WRAP, and NPY_RAISE."]}, {"name": "int PyArray_CompareLists()", "path": "reference/c-api/array#c.PyArray_CompareLists", "type": "Array API", "text": ["Given two n -length arrays of integers, l1, and l2, return 1 if the lists are identical; otherwise, return 0."]}, {"name": "int PyArray_ConvertClipmodeSequence()", "path": "reference/c-api/array#c.PyArray_ConvertClipmodeSequence", "type": "Array API", "text": ["Converts either a sequence of clipmodes or a single clipmode into a C array of NPY_CLIPMODE values. The number of clipmodes n must be known before calling this function. This function is provided to help functions allow a different clipmode for each dimension."]}, {"name": "int PyArray_CopyInto()", "path": "reference/c-api/array#c.PyArray_CopyInto", "type": "Array API", "text": ["Copy from the source array, src, into the destination array, dest, performing a data-type conversion if necessary. If an error occurs return -1 (otherwise 0). The shape of src must be broadcastable to the shape of dest. The data areas of dest and src must not overlap."]}, {"name": "int PyArray_CopyObject()", "path": "reference/c-api/array#c.PyArray_CopyObject", "type": "Array API", "text": ["Assign an object src to a NumPy array dest according to array-coercion rules. This is basically identical to PyArray_FromAny, but assigns directly to the output array. Returns 0 on success and -1 on failures."]}, {"name": "int Pyarray_DescrAlignConverter()", "path": "reference/c-api/array#c.Pyarray_DescrAlignConverter", "type": "Array API", "text": ["Like PyArray_DescrConverter except it aligns C-struct-like objects on word-boundaries as the compiler would."]}, {"name": "int Pyarray_DescrAlignConverter2()", "path": "reference/c-api/array#c.Pyarray_DescrAlignConverter2", "type": "Array API", "text": ["Like PyArray_DescrConverter2 except it aligns C-struct-like objects on word-boundaries as the compiler would."]}, {"name": "int PyArray_DescrConverter()", "path": "reference/c-api/array#c.PyArray_DescrConverter", "type": "Array API", "text": ["Convert any compatible Python object, obj, to a data-type object in dtype. A large number of Python objects can be converted to data-type objects. See Data type objects (dtype) for a complete description. This version of the converter converts None objects to a NPY_DEFAULT_TYPE data-type object. This function can be used with the \u201cO&\u201d character code in PyArg_ParseTuple processing."]}, {"name": "int PyArray_DescrConverter2()", "path": "reference/c-api/array#c.PyArray_DescrConverter2", "type": "Array API", "text": ["Convert any compatible Python object, obj, to a data-type object in dtype. This version of the converter converts None objects so that the returned data-type is NULL. This function can also be used with the \u201cO&\u201d character in PyArg_ParseTuple processing."]}, {"name": "int PyArray_Dump()", "path": "reference/c-api/array#c.PyArray_Dump", "type": "Array API", "text": ["Pickle the object in self to the given file (either a string or a Python file object). If file is a Python string it is considered to be the name of a file which is then opened in binary mode. The given protocol is used (if protocol is negative, or the highest available is used). This is a simple wrapper around cPickle.dump(self, file, protocol)."]}, {"name": "int PyArray_EquivByteorders()", "path": "reference/c-api/array#c.PyArray_EquivByteorders", "type": "Array API", "text": ["True if byteorder characters b1 and b2 ( NPY_LITTLE, NPY_BIG, NPY_NATIVE, NPY_IGNORE ) are either equal or equivalent as to their specification of a native byte order. Thus, on a little-endian machine NPY_LITTLE and NPY_NATIVE are equivalent where they are not equivalent on a big-endian machine."]}, {"name": "int PyArray_FillWithScalar()", "path": "reference/c-api/array#c.PyArray_FillWithScalar", "type": "Array API", "text": ["Fill the array, arr, with the given scalar object, obj. The object is first converted to the data type of arr, and then copied into every location. A -1 is returned if an error occurs, otherwise 0 is returned."]}, {"name": "int PyArray_FinalizeFunc()", "path": "reference/c-api/array#c.PyArray_FinalizeFunc", "type": "Array API", "text": ["The function pointed to by the CObject __array_finalize__. The first argument is the newly created sub-type. The second argument (if not NULL) is the \u201cparent\u201d array (if the array was created using slicing or some other operation where a clearly-distinguishable parent is present). This routine can do anything it wants to. It should return a -1 on error and 0 otherwise."]}, {"name": "int PyArray_FLAGS()", "path": "reference/c-api/array#c.PyArray_FLAGS", "type": "Array API", "text": ["Returns an integer representing the array-flags."]}, {"name": "int PyArray_Free()", "path": "reference/c-api/array#c.PyArray_Free", "type": "Array API", "text": ["Must be called with the same objects and memory locations returned from PyArray_AsCArray (\u2026). This function cleans up memory that otherwise would get leaked."]}, {"name": "int PyArray_GetArrayParamsFromObject()", "path": "reference/c-api/array#c.PyArray_GetArrayParamsFromObject", "type": "Array API", "text": ["Deprecated since version NumPy: 1.19", "Unless NumPy is made aware of an issue with this, this function is scheduled for rapid removal without replacement.", "Changed in version NumPy: 1.19", "context is never used. Its use results in an error.", "New in version 1.6."]}, {"name": "int PyArray_GetEndianness()", "path": "reference/c-api/config#c.PyArray_GetEndianness", "type": "System configuration", "text": ["New in version 1.3.0.", "Returns the endianness of the current platform. One of NPY_CPU_BIG, NPY_CPU_LITTLE, or NPY_CPU_UNKNOWN_ENDIAN."]}, {"name": "int PyArray_HasArrayInterface()", "path": "reference/c-api/array#c.PyArray_HasArrayInterface", "type": "Array API", "text": ["If op implements any part of the array interface, then out will contain a new reference to the newly created ndarray using the interface or out will contain NULL if an error during conversion occurs. Otherwise, out will contain a borrowed reference to Py_NotImplemented and no error condition is set."]}, {"name": "int PyArray_HasArrayInterfaceType()", "path": "reference/c-api/array#c.PyArray_HasArrayInterfaceType", "type": "Array API", "text": ["If op implements any part of the array interface, then out will contain a new reference to the newly created ndarray using the interface or out will contain NULL if an error during conversion occurs. Otherwise, out will contain a borrowed reference to Py_NotImplemented and no error condition is set. This version allows setting of the dtype in the part of the array interface that looks for the __array__ attribute. context is unused."]}, {"name": "int PyArray_HASFIELDS()", "path": "reference/c-api/array#c.PyArray_HASFIELDS", "type": "Array API", "text": ["Type has fields associated with it."]}, {"name": "int PyArray_IntpConverter()", "path": "reference/c-api/array#c.PyArray_IntpConverter", "type": "Array API", "text": ["Convert any Python sequence, obj, smaller than NPY_MAXDIMS to a C-array of npy_intp. The Python object could also be a single number. The seq variable is a pointer to a structure with members ptr and len. On successful return, seq ->ptr contains a pointer to memory that must be freed, by calling PyDimMem_FREE, to avoid a memory leak. The restriction on memory size allows this converter to be conveniently used for sequences intended to be interpreted as array shapes."]}, {"name": "int PyArray_IntpFromSequence()", "path": "reference/c-api/array#c.PyArray_IntpFromSequence", "type": "Array API", "text": ["Convert any Python sequence (or single Python number) passed in as seq to (up to) maxvals pointer-sized integers and place them in the vals array. The sequence can be smaller then maxvals as the number of converted objects is returned."]}, {"name": "int PyArray_IS_C_CONTIGUOUS()", "path": "reference/c-api/array#c.PyArray_IS_C_CONTIGUOUS", "type": "Array API", "text": ["Evaluates true if arr is C-style contiguous."]}, {"name": "int PyArray_IS_F_CONTIGUOUS()", "path": "reference/c-api/array#c.PyArray_IS_F_CONTIGUOUS", "type": "Array API", "text": ["Evaluates true if arr is Fortran-style contiguous."]}, {"name": "int PyArray_ISALIGNED()", "path": "reference/c-api/array#c.PyArray_ISALIGNED", "type": "Array API", "text": ["Evaluates true if the data area of arr is properly aligned on the machine."]}, {"name": "int PyArray_IsAnyScalar()", "path": "reference/c-api/array#c.PyArray_IsAnyScalar", "type": "Array API", "text": ["Evaluates true if op is either a Python scalar object (see PyArray_IsPythonScalar) or an array scalar (an instance of a sub- type of PyGenericArr_Type )."]}, {"name": "int PyArray_ISBEHAVED()", "path": "reference/c-api/array#c.PyArray_ISBEHAVED", "type": "Array API", "text": ["Evaluates true if the data area of arr is aligned and writeable and in machine byte-order according to its descriptor."]}, {"name": "int PyArray_ISBEHAVED_RO()", "path": "reference/c-api/array#c.PyArray_ISBEHAVED_RO", "type": "Array API", "text": ["Evaluates true if the data area of arr is aligned and in machine byte-order."]}, {"name": "int PyArray_ISBOOL()", "path": "reference/c-api/array#c.PyArray_ISBOOL", "type": "Array API", "text": ["Type represents Boolean data type."]}, {"name": "int PyArray_ISBYTESWAPPED()", "path": "reference/c-api/array#c.PyArray_ISBYTESWAPPED", "type": "Array API", "text": ["Evaluates true if the data area of the ndarray m is not in machine byte-order according to the array\u2019s data-type descriptor."]}, {"name": "int PyArray_ISCARRAY()", "path": "reference/c-api/array#c.PyArray_ISCARRAY", "type": "Array API", "text": ["Evaluates true if the data area of arr is C-style contiguous, and PyArray_ISBEHAVED (arr) is true."]}, {"name": "int PyArray_ISCARRAY_RO()", "path": "reference/c-api/array#c.PyArray_ISCARRAY_RO", "type": "Array API", "text": ["Evaluates true if the data area of arr is C-style contiguous, aligned, and in machine byte-order."]}, {"name": "int PyArray_ISCOMPLEX()", "path": "reference/c-api/array#c.PyArray_ISCOMPLEX", "type": "Array API", "text": ["Type represents any complex floating point number."]}, {"name": "int PyArray_ISEXTENDED()", "path": "reference/c-api/array#c.PyArray_ISEXTENDED", "type": "Array API", "text": ["Type is either flexible or user-defined."]}, {"name": "int PyArray_ISFARRAY()", "path": "reference/c-api/array#c.PyArray_ISFARRAY", "type": "Array API", "text": ["Evaluates true if the data area of arr is Fortran-style contiguous and PyArray_ISBEHAVED (arr) is true."]}, {"name": "int PyArray_ISFARRAY_RO()", "path": "reference/c-api/array#c.PyArray_ISFARRAY_RO", "type": "Array API", "text": ["Evaluates true if the data area of arr is Fortran-style contiguous, aligned, and in machine byte-order ."]}, {"name": "int PyArray_ISFLEXIBLE()", "path": "reference/c-api/array#c.PyArray_ISFLEXIBLE", "type": "Array API", "text": ["Type represents one of the flexible array types ( NPY_STRING, NPY_UNICODE, or NPY_VOID )."]}, {"name": "int PyArray_ISFLOAT()", "path": "reference/c-api/array#c.PyArray_ISFLOAT", "type": "Array API", "text": ["Type represents any floating point number."]}, {"name": "int PyArray_ISFORTRAN()", "path": "reference/c-api/array#c.PyArray_ISFORTRAN", "type": "Array API", "text": ["Evaluates true if arr is Fortran-style contiguous and not C-style contiguous. PyArray_IS_F_CONTIGUOUS is the correct way to test for Fortran-style contiguity."]}, {"name": "int PyArray_ISINTEGER()", "path": "reference/c-api/array#c.PyArray_ISINTEGER", "type": "Array API", "text": ["Type represents any integer."]}, {"name": "int PyArray_ISNOTSWAPPED()", "path": "reference/c-api/array#c.PyArray_ISNOTSWAPPED", "type": "Array API", "text": ["Evaluates true if the data area of the ndarray m is in machine byte-order according to the array\u2019s data-type descriptor."]}, {"name": "int PyArray_ISNUMBER()", "path": "reference/c-api/array#c.PyArray_ISNUMBER", "type": "Array API", "text": ["Type represents any integer, floating point, or complex floating point number."]}, {"name": "int PyArray_ISOBJECT()", "path": "reference/c-api/array#c.PyArray_ISOBJECT", "type": "Array API", "text": ["Type represents object data type."]}, {"name": "int PyArray_ISONESEGMENT()", "path": "reference/c-api/array#c.PyArray_ISONESEGMENT", "type": "Array API", "text": ["Evaluates true if the data area of arr consists of a single (C-style or Fortran-style) contiguous segment."]}, {"name": "int PyArray_ISPYTHON()", "path": "reference/c-api/array#c.PyArray_ISPYTHON", "type": "Array API", "text": ["Type represents an enumerated type corresponding to one of the standard Python scalar (bool, int, float, or complex)."]}, {"name": "int PyArray_IsPythonNumber()", "path": "reference/c-api/array#c.PyArray_IsPythonNumber", "type": "Array API", "text": ["Evaluates true if op is an instance of a builtin numeric type (int, float, complex, long, bool)"]}, {"name": "int PyArray_IsPythonScalar()", "path": "reference/c-api/array#c.PyArray_IsPythonScalar", "type": "Array API", "text": ["Evaluates true if op is a builtin Python scalar object (int, float, complex, bytes, str, long, bool)."]}, {"name": "int PyArray_ISSIGNED()", "path": "reference/c-api/array#c.PyArray_ISSIGNED", "type": "Array API", "text": ["Type represents a signed integer."]}, {"name": "int PyArray_ISSTRING()", "path": "reference/c-api/array#c.PyArray_ISSTRING", "type": "Array API", "text": ["Type represents a string data type."]}, {"name": "int PyArray_ISUNSIGNED()", "path": "reference/c-api/array#c.PyArray_ISUNSIGNED", "type": "Array API", "text": ["Type represents an unsigned integer."]}, {"name": "int PyArray_ISUSERDEF()", "path": "reference/c-api/array#c.PyArray_ISUSERDEF", "type": "Array API", "text": ["Type represents a user-defined type."]}, {"name": "int PyArray_ISWRITEABLE()", "path": "reference/c-api/array#c.PyArray_ISWRITEABLE", "type": "Array API", "text": ["Evaluates true if the data area of arr can be written to"]}, {"name": "int PyArray_IsZeroDim()", "path": "reference/c-api/array#c.PyArray_IsZeroDim", "type": "Array API", "text": ["Evaluates true if op is an instance of (a subclass of) PyArray_Type and has 0 dimensions."]}, {"name": "int PyArray_ITER_NOTDONE()", "path": "reference/c-api/array#c.PyArray_ITER_NOTDONE", "type": "Array API", "text": ["Evaluates TRUE as long as the iterator has not looped through all of the elements, otherwise it evaluates FALSE."]}, {"name": "int PyArray_MoveInto()", "path": "reference/c-api/array#c.PyArray_MoveInto", "type": "Array API", "text": ["Move data from the source array, src, into the destination array, dest, performing a data-type conversion if necessary. If an error occurs return -1 (otherwise 0). The shape of src must be broadcastable to the shape of dest. The data areas of dest and src may overlap."]}, {"name": "int PyArray_MultiIter_NOTDONE()", "path": "reference/c-api/array#c.PyArray_MultiIter_NOTDONE", "type": "Array API", "text": ["Evaluates TRUE as long as the multi-iterator has not looped through all of the elements (of the broadcasted result), otherwise it evaluates FALSE."]}, {"name": "int PyArray_MultiplyIntList()", "path": "reference/c-api/array#c.PyArray_MultiplyIntList", "type": "Array API", "text": ["Both of these routines multiply an n -length array, seq, of integers and return the result. No overflow checking is performed."]}, {"name": "int PyArray_NDIM()", "path": "reference/c-api/array", "type": "Array API", "text": ["These macros access the PyArrayObject structure members and are defined in ndarraytypes.h. The input argument, arr, can be any PyObject* that is directly interpretable as a PyArrayObject* (any instance of the PyArray_Type and its sub-types).", "The number of dimensions in the array.", "Returns an integer representing the array-flags.", "Return the (builtin) typenumber for the elements of this array.", "Convert obj and place it in the ndarray, arr, at the place pointed to by itemptr. Return -1 if an error occurs or 0 on success.", "New in version 1.7.", "Enables the specified array flags. This function does no validation, and assumes that you know what you\u2019re doing.", "New in version 1.7.", "Clears the specified array flags. This function does no validation, and assumes that you know what you\u2019re doing.", "These two macros are similar and obtain the pointer to the data-buffer for the array. The first macro can (and should be) assigned to a particular pointer where the second is for generic processing. If you have not guaranteed a contiguous and/or aligned array then be sure you understand how to access the data in the array to avoid memory and/or alignment problems.", "Returns a pointer to the dimensions/shape of the array. The number of elements matches the number of dimensions of the array. Can return NULL for 0-dimensional arrays.", "New in version 1.7.", "A synonym for PyArray_DIMS, named to be consistent with the shape usage within Python.", "Returns a pointer to the strides of the array. The number of elements matches the number of dimensions of the array.", "Return the shape in the n \\(^{\\textrm{th}}\\) dimension.", "Return the stride in the n \\(^{\\textrm{th}}\\) dimension.", "Return the itemsize for the elements of this array.", "Note that, in the old API that was deprecated in version 1.7, this function had the return type int.", "Returns the total size (in number of elements) of the array.", "Returns 0 if obj is not a sub-class of ndarray. Otherwise, returns the total number of elements in the array. Safer version of PyArray_SIZE (obj).", "Returns the total number of bytes consumed by the array.", "This returns the base object of the array. In most cases, this means the object which owns the memory the array is pointing at.", "If you are constructing an array using the C API, and specifying your own memory, you should use the function PyArray_SetBaseObject to set the base to an object which owns the memory.", "If the (deprecated) NPY_ARRAY_UPDATEIFCOPY or the NPY_ARRAY_WRITEBACKIFCOPY flags are set, it has a different meaning, namely base is the array into which the current array will be copied upon copy resolution. This overloading of the base property for two functions is likely to change in a future version of NumPy.", "Returns a borrowed reference to the dtype property of the array.", "New in version 1.7.", "A synonym for PyArray_DESCR, named to be consistent with the \u2018dtype\u2019 usage within Python.", "Get a Python object of a builtin type from the ndarray, arr, at the location pointed to by itemptr. Return NULL on failure.", "numpy.ndarray.item is identical to PyArray_GETITEM.", "The function pointed to by the CObject __array_finalize__. The first argument is the newly created sub-type. The second argument (if not NULL) is the \u201cparent\u201d array (if the array was created using slicing or some other operation where a clearly-distinguishable parent is present). This routine can do anything it wants to. It should return a -1 on error and 0 otherwise.", "These functions and macros provide easy access to elements of the ndarray from C. These work for all arrays. You may need to take care when accessing the data in the array, however, if it is not in machine byte-order, misaligned, or not writeable. In other words, be sure to respect the state of the flags unless you know what you are doing, or have previously guaranteed an array that is writeable, aligned, and in machine byte-order using PyArray_FromAny. If you wish to handle all types of arrays, the copyswap function for each type is useful for handling misbehaved arrays. Some platforms (e.g. Solaris) do not like misaligned data and will crash if you de-reference a misaligned pointer. Other platforms (e.g. x86 Linux) will just work more slowly with misaligned data.", "Return a pointer to the data of the ndarray, aobj, at the N-dimensional index given by the c-array, ind, (which must be at least aobj ->nd in size). You may want to typecast the returned pointer to the data type of the ndarray.", "Quick, inline access to the element at the given coordinates in the ndarray, obj, which must have respectively 1, 2, 3, or 4 dimensions (this is not checked). The corresponding i, j, k, and l coordinates can be any integer but will be interpreted as npy_intp. You may want to typecast the returned pointer to the data type of the ndarray.", "This function steals a reference to descr. The easiest way to get one is using PyArray_DescrFromType.", "This is the main array creation function. Most new arrays are created with this flexible function.", "The returned object is an object of Python-type subtype, which must be a subtype of PyArray_Type. The array has nd dimensions, described by dims. The data-type descriptor of the new array is descr.", "If subtype is of an array subclass instead of the base &PyArray_Type, then obj is the object to pass to the __array_finalize__ method of the subclass.", "If data is NULL, then new unitinialized memory will be allocated and flags can be non-zero to indicate a Fortran-style contiguous array. Use PyArray_FILLWBYTE to initialize the memory.", "If data is not NULL, then it is assumed to point to the memory to be used for the array and the flags argument is used as the new flags for the array (except the state of NPY_ARRAY_OWNDATA, NPY_ARRAY_WRITEBACKIFCOPY and NPY_ARRAY_UPDATEIFCOPY flags of the new array will be reset).", "In addition, if data is non-NULL, then strides can also be provided. If strides is NULL, then the array strides are computed as C-style contiguous (default) or Fortran-style contiguous (flags is nonzero for data = NULL or flags & NPY_ARRAY_F_CONTIGUOUS is nonzero non-NULL data). Any provided dims and strides are copied into newly allocated dimension and strides arrays for the new array object.", "PyArray_CheckStrides can help verify non- NULL stride information.", "If data is provided, it must stay alive for the life of the array. One way to manage this is through PyArray_SetBaseObject", "New in version 1.6.", "This function steals a reference to descr if it is not NULL. This array creation routine allows for the convenient creation of a new array matching an existing array\u2019s shapes and memory layout, possibly changing the layout and/or data type.", "When order is NPY_ANYORDER, the result order is NPY_FORTRANORDER if prototype is a fortran array, NPY_CORDER otherwise. When order is NPY_KEEPORDER, the result order matches that of prototype, even when the axes of prototype aren\u2019t in C or Fortran order.", "If descr is NULL, the data type of prototype is used.", "If subok is 1, the newly created array will use the sub-type of prototype to create the new array, otherwise it will create a base-class array.", "This is similar to PyArray_NewFromDescr (\u2026) except you specify the data-type descriptor with type_num and itemsize, where type_num corresponds to a builtin (or user-defined) type. If the type always has the same number of bytes, then itemsize is ignored. Otherwise, itemsize specifies the particular size of this array.", "Warning", "If data is passed to PyArray_NewFromDescr or PyArray_New, this memory must not be deallocated until the new array is deleted. If this data came from another Python object, this can be accomplished using Py_INCREF on that object and setting the base member of the new array to point to that object. If strides are passed in they must be consistent with the dimensions, the itemsize, and the data of the array.", "Create a new uninitialized array of type, typenum, whose size in each of nd dimensions is given by the integer array, dims.The memory for the array is uninitialized (unless typenum is NPY_OBJECT in which case each element in the array is set to NULL). The typenum argument allows specification of any of the builtin data-types such as NPY_FLOAT or NPY_LONG. The memory for the array can be set to zero if desired using PyArray_FILLWBYTE (return_object, 0).This function cannot be used to create a flexible-type array (no itemsize given).", "Create an array wrapper around data pointed to by the given pointer. The array flags will have a default that the data area is well-behaved and C-style contiguous. The shape of the array is given by the dims c-array of length nd. The data-type of the array is indicated by typenum. If data comes from another reference-counted Python object, the reference count on this object should be increased after the pointer is passed in, and the base member of the returned ndarray should point to the Python object that owns the data. This will ensure that the provided memory is not freed while the returned array is in existence.", "This function steals a reference to descr.", "Create a new array with the provided data-type descriptor, descr, of the shape determined by nd and dims.", "Fill the array pointed to by obj \u2014which must be a (subclass of) ndarray\u2014with the contents of val (evaluated as a byte). This macro calls memset, so obj must be contiguous.", "Construct a new nd -dimensional array with shape given by dims and data type given by dtype. If fortran is non-zero, then a Fortran-order array is created, otherwise a C-order array is created. Fill the memory with zeros (or the 0 object if dtype corresponds to NPY_OBJECT ).", "Macro form of PyArray_Zeros which takes a type-number instead of a data-type object.", "Construct a new nd -dimensional array with shape given by dims and data type given by dtype. If fortran is non-zero, then a Fortran-order array is created, otherwise a C-order array is created. The array is uninitialized unless the data type corresponds to NPY_OBJECT in which case the array is filled with Py_None.", "Macro form of PyArray_Empty which takes a type-number, typenum, instead of a data-type object.", "Construct a new 1-dimensional array of data-type, typenum, that ranges from start to stop (exclusive) in increments of step . Equivalent to arange (start, stop, step, dtype).", "Construct a new 1-dimensional array of data-type determined by descr, that ranges from start to stop (exclusive) in increments of step. Equivalent to arange( start, stop, step, typenum ).", "New in version 1.7.", "This function steals a reference to obj and sets it as the base property of arr.", "If you construct an array by passing in your own memory buffer as a parameter, you need to set the array\u2019s base property to ensure the lifetime of the memory buffer is appropriate.", "The return value is 0 on success, -1 on failure.", "If the object provided is an array, this function traverses the chain of base pointers so that each array points to the owner of the memory directly. Once the base is set, it may not be changed to another value.", "This is the main function used to obtain an array from any nested sequence, or object that exposes the array interface, op. The parameters allow specification of the required dtype, the minimum (min_depth) and maximum (max_depth) number of dimensions acceptable, and other requirements for the array. This function steals a reference to the dtype argument, which needs to be a PyArray_Descr structure indicating the desired data-type (including required byteorder). The dtype argument may be NULL, indicating that any data-type (and byteorder) is acceptable. Unless NPY_ARRAY_FORCECAST is present in flags, this call will generate an error if the data type cannot be safely obtained from the object. If you want to use NULL for the dtype and ensure the array is notswapped then use PyArray_CheckFromAny. A value of 0 for either of the depth parameters causes the parameter to be ignored. Any of the following array flags can be added (e.g. using |) to get the requirements argument. If your code can handle general (e.g. strided, byte-swapped, or unaligned arrays) then requirements may be 0. Also, if op is not already an array (or does not expose the array interface), then a new array will be created (and filled from op using the sequence protocol). The new array will have NPY_ARRAY_DEFAULT as its flags member. The context argument is unused.", "Make sure the returned array is C-style contiguous", "Make sure the returned array is Fortran-style contiguous.", "Make sure the returned array is aligned on proper boundaries for its data type. An aligned array has the data pointer and every strides factor as a multiple of the alignment factor for the data-type- descriptor.", "Make sure the returned array can be written to.", "Make sure a copy is made of op. If this flag is not present, data is not copied if it can be avoided.", "Make sure the result is a base-class ndarray. By default, if op is an instance of a subclass of ndarray, an instance of that same subclass is returned. If this flag is set, an ndarray object will be returned instead.", "Force a cast to the output type even if it cannot be done safely. Without this flag, a data cast will occur only if it can be done safely, otherwise an error is raised.", "If op is already an array, but does not satisfy the requirements, then a copy is made (which will satisfy the requirements). If this flag is present and a copy (of an object that is already an array) must be made, then the corresponding NPY_ARRAY_WRITEBACKIFCOPY flag is set in the returned copy and op is made to be read-only. You must be sure to call PyArray_ResolveWritebackIfCopy to copy the contents back into op and the op array will be made writeable again. If op is not writeable to begin with, or if it is not already an array, then an error is raised.", "Deprecated. Use NPY_ARRAY_WRITEBACKIFCOPY, which is similar. This flag \u201cautomatically\u201d copies the data back when the returned array is deallocated, which is not supported in all python implementations.", "NPY_ARRAY_ALIGNED | NPY_ARRAY_WRITEABLE", "NPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_BEHAVED", "NPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_ALIGNED", "NPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_BEHAVED", "NPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_ALIGNED", "NPY_ARRAY_CARRAY", "NPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_ALIGNED", "NPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_ALIGNED", "NPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_WRITEABLE | NPY_ARRAY_ALIGNED", "NPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_ALIGNED | NPY_ARRAY_WRITEABLE", "NPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_WRITEABLE | NPY_ARRAY_ALIGNED", "NPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_WRITEABLE | NPY_ARRAY_ALIGNED | NPY_ARRAY_WRITEBACKIFCOPY | NPY_ARRAY_UPDATEIFCOPY", "NPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_WRITEABLE | NPY_ARRAY_ALIGNED | NPY_ARRAY_WRITEBACKIFCOPY | NPY_ARRAY_UPDATEIFCOPY", "Deprecated since version NumPy: 1.19", "Unless NumPy is made aware of an issue with this, this function is scheduled for rapid removal without replacement.", "Changed in version NumPy: 1.19", "context is never used. Its use results in an error.", "New in version 1.6.", "Nearly identical to PyArray_FromAny (\u2026) except requirements can contain NPY_ARRAY_NOTSWAPPED (over-riding the specification in dtype) and NPY_ARRAY_ELEMENTSTRIDES which indicates that the array should be aligned in the sense that the strides are multiples of the element size.", "In versions 1.6 and earlier of NumPy, the following flags did not have the _ARRAY_ macro namespace in them. That form of the constant names is deprecated in 1.7.", "Make sure the returned array has a data-type descriptor that is in machine byte-order, over-riding any specification in the dtype argument. Normally, the byte-order requirement is determined by the dtype argument. If this flag is set and the dtype argument does not indicate a machine byte-order descriptor (or is NULL and the object is already an array with a data-type descriptor that is not in machine byte- order), then a new data-type descriptor is created and used with its byte-order field set to native.", "NPY_ARRAY_ALIGNED | NPY_ARRAY_WRITEABLE | NPY_ARRAY_NOTSWAPPED", "Make sure the returned array has strides that are multiples of the element size.", "Special case of PyArray_FromAny for when op is already an array but it needs to be of a specific newtype (including byte-order) or has certain requirements.", "Returns an ndarray object from a Python object that exposes the __array_struct__ attribute and follows the array interface protocol. If the object does not contain this attribute then a borrowed reference to Py_NotImplemented is returned.", "Returns an ndarray object from a Python object that exposes the __array_interface__ attribute following the array interface protocol. If the object does not contain this attribute then a borrowed reference to Py_NotImplemented is returned.", "Return an ndarray object from a Python object that exposes the __array__ method. The __array__ method can take 0, or 1 argument ([dtype]). context is unused.", "This function returns a (C-style) contiguous and behaved function array from any nested sequence or array interface exporting object, op, of (non-flexible) type given by the enumerated typenum, of minimum depth min_depth, and of maximum depth max_depth. Equivalent to a call to PyArray_FromAny with requirements set to NPY_ARRAY_DEFAULT and the type_num member of the type argument set to typenum.", "This function returns a well-behaved C-style contiguous array from any nested sequence or array-interface exporting object. The minimum number of dimensions the array can have is given by min_depth while the maximum is max_depth. This is equivalent to call PyArray_FromAny with requirements NPY_ARRAY_DEFAULT and NPY_ARRAY_ENSUREARRAY.", "Return an aligned and in native-byteorder array from any nested sequence or array-interface exporting object, op, of a type given by the enumerated typenum. The minimum number of dimensions the array can have is given by min_depth while the maximum is max_depth. This is equivalent to a call to PyArray_FromAny with requirements set to BEHAVED.", "This function steals a reference to op and makes sure that op is a base-class ndarray. It special cases array scalars, but otherwise calls PyArray_FromAny ( op, NULL, 0, 0, NPY_ARRAY_ENSUREARRAY, NULL).", "Construct a one-dimensional ndarray of a single type from a binary or (ASCII) text string of length slen. The data-type of the array to-be-created is given by dtype. If num is -1, then copy the entire string and return an appropriately sized array, otherwise, num is the number of items to copy from the string. If sep is NULL (or \u201c\u201d), then interpret the string as bytes of binary data, otherwise convert the sub-strings separated by sep to items of data-type dtype. Some data-types may not be readable in text mode and an error will be raised if that occurs. All errors return NULL.", "Construct a one-dimensional ndarray of a single type from a binary or text file. The open file pointer is fp, the data-type of the array to be created is given by dtype. This must match the data in the file. If num is -1, then read until the end of the file and return an appropriately sized array, otherwise, num is the number of items to read. If sep is NULL (or \u201c\u201d), then read from the file in binary mode, otherwise read from the file in text mode with sep providing the item separator. Some array types cannot be read in text mode in which case an error is raised.", "Construct a one-dimensional ndarray of a single type from an object, buf, that exports the (single-segment) buffer protocol (or has an attribute __buffer__ that returns an object that exports the buffer protocol). A writeable buffer will be tried first followed by a read- only buffer. The NPY_ARRAY_WRITEABLE flag of the returned array will reflect which one was successful. The data is assumed to start at offset bytes from the start of the memory location for the object. The type of the data in the buffer will be interpreted depending on the data- type descriptor, dtype. If count is negative then it will be determined from the size of the buffer and the requested itemsize, otherwise, count represents how many elements should be converted from the buffer.", "Copy from the source array, src, into the destination array, dest, performing a data-type conversion if necessary. If an error occurs return -1 (otherwise 0). The shape of src must be broadcastable to the shape of dest. The data areas of dest and src must not overlap.", "Assign an object src to a NumPy array dest according to array-coercion rules. This is basically identical to PyArray_FromAny, but assigns directly to the output array. Returns 0 on success and -1 on failures.", "Move data from the source array, src, into the destination array, dest, performing a data-type conversion if necessary. If an error occurs return -1 (otherwise 0). The shape of src must be broadcastable to the shape of dest. The data areas of dest and src may overlap.", "If op is already (C-style) contiguous and well-behaved then just return a reference, otherwise return a (contiguous and well-behaved) copy of the array. The parameter op must be a (sub-class of an) ndarray and no checking for that is done.", "Convert obj to an ndarray. The argument can be any nested sequence or object that exports the array interface. This is a macro form of PyArray_FromAny using NULL, 0, 0, 0 for the other arguments. Your code must be able to handle any data-type descriptor and any combination of data-flags to use this macro.", "Similar to PyArray_FROM_O except it can take an argument of requirements indicating properties the resulting array must have. Available requirements that can be enforced are NPY_ARRAY_C_CONTIGUOUS, NPY_ARRAY_F_CONTIGUOUS, NPY_ARRAY_ALIGNED, NPY_ARRAY_WRITEABLE, NPY_ARRAY_NOTSWAPPED, NPY_ARRAY_ENSURECOPY, NPY_ARRAY_WRITEBACKIFCOPY, NPY_ARRAY_UPDATEIFCOPY, NPY_ARRAY_FORCECAST, and NPY_ARRAY_ENSUREARRAY. Standard combinations of flags can also be used:", "Similar to PyArray_FROM_O except it can take an argument of typenum specifying the type-number the returned array.", "Combination of PyArray_FROM_OF and PyArray_FROM_OT allowing both a typenum and a flags argument to be provided.", "Similar to PyArray_FromAny except the data-type is specified using a typenumber. PyArray_DescrFromType (typenum) is passed directly to PyArray_FromAny. This macro also adds NPY_ARRAY_DEFAULT to requirements if NPY_ARRAY_ENSURECOPY is passed in as requirements.", "Encapsulate the functionality of functions and methods that take the axis= keyword and work properly with None as the axis argument. The input array is obj, while *axis is a converted integer (so that >=MAXDIMS is the None value), and requirements gives the needed properties of obj. The output is a converted version of the input so that requirements are met and if needed a flattening has occurred. On output negative values of *axis are converted and the new value is checked to ensure consistency with the shape of obj.", "Evaluates true if op is a Python object whose type is a sub-type of PyArray_Type.", "Evaluates true if op is a Python object with type PyArray_Type.", "If op implements any part of the array interface, then out will contain a new reference to the newly created ndarray using the interface or out will contain NULL if an error during conversion occurs. Otherwise, out will contain a borrowed reference to Py_NotImplemented and no error condition is set.", "If op implements any part of the array interface, then out will contain a new reference to the newly created ndarray using the interface or out will contain NULL if an error during conversion occurs. Otherwise, out will contain a borrowed reference to Py_NotImplemented and no error condition is set. This version allows setting of the dtype in the part of the array interface that looks for the __array__ attribute. context is unused.", "Evaluates true if op is an instance of (a subclass of) PyArray_Type and has 0 dimensions.", "Evaluates true if op is an instance of Py{cls}ArrType_Type.", "Evaluates true if op is either an array scalar (an instance of a sub-type of PyGenericArr_Type ), or an instance of (a sub-class of) PyArray_Type whose dimensionality is 0.", "Evaluates true if op is an instance of a builtin numeric type (int, float, complex, long, bool)", "Evaluates true if op is a builtin Python scalar object (int, float, complex, bytes, str, long, bool).", "Evaluates true if op is either a Python scalar object (see PyArray_IsPythonScalar) or an array scalar (an instance of a sub- type of PyGenericArr_Type ).", "Evaluates true if op is a Python scalar object (see PyArray_IsPythonScalar), an array scalar (an instance of a sub-type of PyGenericArr_Type) or an instance of a sub-type of PyArray_Type whose dimensionality is 0.", "For the typenum macros, the argument is an integer representing an enumerated array data type. For the array type checking macros the argument must be a PyObject* that can be directly interpreted as a PyArrayObject*.", "Type represents an unsigned integer.", "Type represents a signed integer.", "Type represents any integer.", "Type represents any floating point number.", "Type represents any complex floating point number.", "Type represents any integer, floating point, or complex floating point number.", "Type represents a string data type.", "Type represents an enumerated type corresponding to one of the standard Python scalar (bool, int, float, or complex).", "Type represents one of the flexible array types ( NPY_STRING, NPY_UNICODE, or NPY_VOID ).", "Type has no size information attached, and can be resized. Should only be called on flexible dtypes. Types that are attached to an array will always be sized, hence the array form of this macro not existing.", "Changed in version 1.18.", "For structured datatypes with no fields this function now returns False.", "Type represents a user-defined type.", "Type is either flexible or user-defined.", "Type represents object data type.", "Type represents Boolean data type.", "Type has fields associated with it.", "Evaluates true if the data area of the ndarray m is in machine byte-order according to the array\u2019s data-type descriptor.", "Evaluates true if the data area of the ndarray m is not in machine byte-order according to the array\u2019s data-type descriptor.", "Return NPY_TRUE if type1 and type2 actually represent equivalent types for this platform (the fortran member of each type is ignored). For example, on 32-bit platforms, NPY_LONG and NPY_INT are equivalent. Otherwise return NPY_FALSE.", "Return NPY_TRUE if a1 and a2 are arrays with equivalent types for this platform.", "Special case of PyArray_EquivTypes (\u2026) that does not accept flexible data types but may be easier to call.", "True if byteorder characters b1 and b2 ( NPY_LITTLE, NPY_BIG, NPY_NATIVE, NPY_IGNORE ) are either equal or equivalent as to their specification of a native byte order. Thus, on a little-endian machine NPY_LITTLE and NPY_NATIVE are equivalent where they are not equivalent on a big-endian machine.", "Mainly for backwards compatibility to the Numeric C-API and for simple casts to non-flexible types. Return a new array object with the elements of arr cast to the data-type typenum which must be one of the enumerated types and not a flexible type.", "Return a new array of the type specified, casting the elements of arr as appropriate. The fortran argument specifies the ordering of the output array.", "As of 1.6, this function simply calls PyArray_CopyInto, which handles the casting.", "Cast the elements of the array in into the array out. The output array should be writeable, have an integer-multiple of the number of elements in the input array (more than one copy can be placed in out), and have a data type that is one of the builtin types. Returns 0 on success and -1 if an error occurs.", "Return the low-level casting function to cast from the given descriptor to the builtin type number. If no casting function exists return NULL and set an error. Using this function instead of direct access to from ->f->cast will allow support of any user-defined casting functions added to a descriptors casting dictionary.", "Returns non-zero if an array of data type fromtype can be cast to an array of data type totype without losing information. An exception is that 64-bit integers are allowed to be cast to 64-bit floating point values even though this can lose precision on large integers so as not to proliferate the use of long doubles without explicit requests. Flexible array types are not checked according to their lengths with this function.", "PyArray_CanCastTypeTo supersedes this function in NumPy 1.6 and later.", "Equivalent to PyArray_CanCastTypeTo(fromtype, totype, NPY_SAFE_CASTING).", "New in version 1.6.", "Returns non-zero if an array of data type fromtype (which can include flexible types) can be cast safely to an array of data type totype (which can include flexible types) according to the casting rule casting. For simple types with NPY_SAFE_CASTING, this is basically a wrapper around PyArray_CanCastSafely, but for flexible types such as strings or unicode, it produces results taking into account their sizes. Integer and float types can only be cast to a string or unicode type using NPY_SAFE_CASTING if the string or unicode type is big enough to hold the max value of the integer/float type being cast from.", "New in version 1.6.", "Returns non-zero if arr can be cast to totype according to the casting rule given in casting. If arr is an array scalar, its value is taken into account, and non-zero is also returned when the value will not overflow or be truncated to an integer when converting to a smaller type.", "This is almost the same as the result of PyArray_CanCastTypeTo(PyArray_MinScalarType(arr), totype, casting), but it also handles a special case arising because the set of uint values is not a subset of the int values for types with the same number of bits.", "New in version 1.6.", "If arr is an array, returns its data type descriptor, but if arr is an array scalar (has 0 dimensions), it finds the data type of smallest size to which the value may be converted without overflow or truncation to an integer.", "This function will not demote complex to float or anything to boolean, but will demote a signed integer to an unsigned integer when the scalar value is positive.", "New in version 1.6.", "Finds the data type of smallest size and kind to which type1 and type2 may be safely converted. This function is symmetric and associative. A string or unicode result will be the proper size for storing the max value of the input types converted to a string or unicode.", "New in version 1.6.", "This applies type promotion to all the inputs, using the NumPy rules for combining scalars and arrays, to determine the output type of a set of operands. This is the same result type that ufuncs produce. The specific algorithm used is as follows.", "Categories are determined by first checking which of boolean, integer (int/uint), or floating point (float/complex) the maximum kind of all the arrays and the scalars are.", "If there are only scalars or the maximum category of the scalars is higher than the maximum category of the arrays, the data types are combined with PyArray_PromoteTypes to produce the return value.", "Otherwise, PyArray_MinScalarType is called on each array, and the resulting data types are all combined with PyArray_PromoteTypes to produce the return value.", "The set of int values is not a subset of the uint values for types with the same number of bits, something not reflected in PyArray_MinScalarType, but handled as a special case in PyArray_ResultType.", "This function is superseded by PyArray_MinScalarType and/or PyArray_ResultType.", "This function is useful for determining a common type that two or more arrays can be converted to. It only works for non-flexible array types as no itemsize information is passed. The mintype argument represents the minimum type acceptable, and op represents the object that will be converted to an array. The return value is the enumerated typenumber that represents the data-type that op should have.", "This function is superseded by PyArray_ResultType.", "This function works similarly to PyArray_ObjectType (\u2026) except it handles flexible arrays. The mintype argument can have an itemsize member and the outtype argument will have an itemsize member at least as big but perhaps bigger depending on the object op.", "The functionality this provides is largely superseded by iterator NpyIter introduced in 1.6, with flag NPY_ITER_COMMON_DTYPE or with the same dtype parameter for all operands.", "Convert a sequence of Python objects contained in op to an array of ndarrays each having the same data type. The type is selected in the same way as PyArray_ResultType. The length of the sequence is returned in n, and an n -length array of PyArrayObject pointers is the return value (or NULL if an error occurs). The returned array must be freed by the caller of this routine (using PyDataMem_FREE ) and all the array objects in it DECREF \u2018d or a memory-leak will occur. The example template-code below shows a typically usage:", "Changed in version 1.18.0: A mix of scalars and zero-dimensional arrays now produces a type capable of holding the scalar value. Previously priority was given to the dtype of the arrays.", "A pointer to newly created memory of size arr ->itemsize that holds the representation of 0 for that type. The returned pointer, ret, must be freed using PyDataMem_FREE (ret) when it is not needed anymore.", "A pointer to newly created memory of size arr ->itemsize that holds the representation of 1 for that type. The returned pointer, ret, must be freed using PyDataMem_FREE (ret) when it is not needed anymore.", "Returns NPY_TRUE if typenum represents a valid type-number (builtin or user-defined or character code). Otherwise, this function returns NPY_FALSE.", "Initialize all function pointers and members to NULL.", "Register a data-type as a new user-defined data type for arrays. The type must have most of its entries filled in. This is not always checked and errors can produce segfaults. In particular, the typeobj member of the dtype structure must be filled with a Python type that has a fixed-size element-size that corresponds to the elsize member of dtype. Also the f member must have the required functions: nonzero, copyswap, copyswapn, getitem, setitem, and cast (some of the cast functions may be NULL if no support is desired). To avoid confusion, you should choose a unique character typecode but this is not enforced and not relied on internally.", "A user-defined type number is returned that uniquely identifies the type. A pointer to the new structure can then be obtained from PyArray_DescrFromType using the returned type number. A -1 is returned if an error occurs. If this dtype has already been registered (checked only by the address of the pointer), then return the previously-assigned type-number.", "Register a low-level casting function, castfunc, to convert from the data-type, descr, to the given data-type number, totype. Any old casting function is over-written. A 0 is returned on success or a -1 on failure.", "Register the data-type number, totype, as castable from data-type object, descr, of the given scalar kind. Use scalar = NPY_NOSCALAR to register that an array of data-type descr can be cast safely to a data-type whose type_number is totype. The return value is 0 on success or -1 on failure.", "Given a string return the type-number for the data-type with that string as the type-object name. Returns NPY_NOTYPE without setting an error if no type can be found. Only works for user-defined data-types.", "Used for an array, op, that contains any Python objects. It increments the reference count of every object in the array according to the data-type of op. A -1 is returned if an error occurs, otherwise 0 is returned.", "A function to INCREF all the objects at the location ptr according to the data-type dtype. If ptr is the start of a structured type with an object at any offset, then this will (recursively) increment the reference count of all object-like items in the structured type.", "Used for an array, op, that contains any Python objects. It decrements the reference count of every object in the array according to the data-type of op. Normal return value is 0. A -1 is returned if an error occurs.", "A function to XDECREF all the object-like items at the location ptr as recorded in the data-type, dtype. This works recursively so that if dtype itself has fields with data-types that contain object-like items, all the object-like fields will be XDECREF 'd.", "Fill a newly created array with a single value obj at all locations in the structure with object data-types. No checking is performed but arr must be of data-type NPY_OBJECT and be single-segment and uninitialized (no previous objects in position). Use PyArray_XDECREF (arr) if you need to decrement all the items in the object array prior to calling this function.", "Precondition: arr is a copy of base (though possibly with different strides, ordering, etc.) Set the UPDATEIFCOPY flag and arr->base so that when arr is destructed, it will copy any changes back to base. DEPRECATED, use PyArray_SetWritebackIfCopyBase.", "Returns 0 for success, -1 for failure.", "Precondition: arr is a copy of base (though possibly with different strides, ordering, etc.) Sets the NPY_ARRAY_WRITEBACKIFCOPY flag and arr->base, and set base to READONLY. Call PyArray_ResolveWritebackIfCopy before calling Py_DECREF in order copy any changes back to base and reset the READONLY flag.", "Returns 0 for success, -1 for failure.", "The flags attribute of the PyArrayObject structure contains important information about the memory used by the array (pointed to by the data member) This flag information must be kept accurate or strange results and even segfaults may result.", "There are 6 (binary) flags that describe the memory area used by the data buffer. These constants are defined in arrayobject.h and determine the bit-position of the flag. Python exposes a nice attribute- based interface as well as a dictionary-like interface for getting (and, if appropriate, setting) these flags.", "Memory areas of all kinds can be pointed to by an ndarray, necessitating these flags. If you get an arbitrary PyArrayObject in C-code, you need to be aware of the flags that are set. If you need to guarantee a certain kind of array (like NPY_ARRAY_C_CONTIGUOUS and NPY_ARRAY_BEHAVED), then pass these requirements into the PyArray_FromAny function.", "An ndarray can have a data segment that is not a simple contiguous chunk of well-behaved memory you can manipulate. It may not be aligned with word boundaries (very important on some platforms). It might have its data in a different byte-order than the machine recognizes. It might not be writeable. It might be in Fortran-contiguous order. The array flags are used to indicate what can be said about data associated with an array.", "In versions 1.6 and earlier of NumPy, the following flags did not have the _ARRAY_ macro namespace in them. That form of the constant names is deprecated in 1.7.", "The data area is in C-style contiguous order (last index varies the fastest).", "The data area is in Fortran-style contiguous order (first index varies the fastest).", "Note", "Arrays can be both C-style and Fortran-style contiguous simultaneously. This is clear for 1-dimensional arrays, but can also be true for higher dimensional arrays.", "Even for contiguous arrays a stride for a given dimension arr.strides[dim] may be arbitrary if arr.shape[dim] == 1 or the array has no elements. It does not generally hold that self.strides[-1] == self.itemsize for C-style contiguous arrays or self.strides[0] == self.itemsize for Fortran-style contiguous arrays is true. The correct way to access the itemsize of an array from the C API is PyArray_ITEMSIZE(arr).", "See also", "Internal memory layout of an ndarray", "The data area is owned by this array. Should never be set manually, instead create a PyObject wrapping the data and set the array\u2019s base to that object. For an example, see the test in test_mem_policy.", "The data area and all array elements are aligned appropriately.", "The data area can be written to.", "Notice that the above 3 flags are defined so that a new, well- behaved array has these flags defined as true.", "The data area represents a (well-behaved) copy whose information should be transferred back to the original when PyArray_ResolveWritebackIfCopy is called.", "This is a special flag that is set if this array represents a copy made because a user required certain flags in PyArray_FromAny and a copy had to be made of some other array (and the user asked for this flag to be set in such a situation). The base attribute then points to the \u201cmisbehaved\u201d array (which is set read_only). :c:func`PyArray_ResolveWritebackIfCopy` will copy its contents back to the \u201cmisbehaved\u201d array (casting if necessary) and will reset the \u201cmisbehaved\u201d array to NPY_ARRAY_WRITEABLE. If the \u201cmisbehaved\u201d array was not NPY_ARRAY_WRITEABLE to begin with then PyArray_FromAny would have returned an error because NPY_ARRAY_WRITEBACKIFCOPY would not have been possible.", "A deprecated version of NPY_ARRAY_WRITEBACKIFCOPY which depends upon dealloc to trigger the writeback. For backwards compatibility, PyArray_ResolveWritebackIfCopy is called at dealloc but relying on that behavior is deprecated and not supported in PyPy.", "PyArray_UpdateFlags (obj, flags) will update the obj->flags for flags which can be any of NPY_ARRAY_C_CONTIGUOUS, NPY_ARRAY_F_CONTIGUOUS, NPY_ARRAY_ALIGNED, or NPY_ARRAY_WRITEABLE.", "NPY_ARRAY_ALIGNED | NPY_ARRAY_WRITEABLE", "NPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_BEHAVED", "NPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_ALIGNED", "NPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_BEHAVED", "NPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_ALIGNED", "NPY_ARRAY_CARRAY", "NPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_ALIGNED", "These constants are used in PyArray_FromAny (and its macro forms) to specify desired properties of the new array.", "Cast to the desired type, even if it can\u2019t be done without losing information.", "Make sure the resulting array is a copy of the original.", "Make sure the resulting object is an actual ndarray, and not a sub-class.", "For all of these macros arr must be an instance of a (subclass of) PyArray_Type.", "The first parameter, arr, must be an ndarray or subclass. The parameter, flags, should be an integer consisting of bitwise combinations of the possible flags an array can have: NPY_ARRAY_C_CONTIGUOUS, NPY_ARRAY_F_CONTIGUOUS, NPY_ARRAY_OWNDATA, NPY_ARRAY_ALIGNED, NPY_ARRAY_WRITEABLE, NPY_ARRAY_WRITEBACKIFCOPY, NPY_ARRAY_UPDATEIFCOPY.", "Evaluates true if arr is C-style contiguous.", "Evaluates true if arr is Fortran-style contiguous.", "Evaluates true if arr is Fortran-style contiguous and not C-style contiguous. PyArray_IS_F_CONTIGUOUS is the correct way to test for Fortran-style contiguity.", "Evaluates true if the data area of arr can be written to", "Evaluates true if the data area of arr is properly aligned on the machine.", "Evaluates true if the data area of arr is aligned and writeable and in machine byte-order according to its descriptor.", "Evaluates true if the data area of arr is aligned and in machine byte-order.", "Evaluates true if the data area of arr is C-style contiguous, and PyArray_ISBEHAVED (arr) is true.", "Evaluates true if the data area of arr is Fortran-style contiguous and PyArray_ISBEHAVED (arr) is true.", "Evaluates true if the data area of arr is C-style contiguous, aligned, and in machine byte-order.", "Evaluates true if the data area of arr is Fortran-style contiguous, aligned, and in machine byte-order .", "Evaluates true if the data area of arr consists of a single (C-style or Fortran-style) contiguous segment.", "The NPY_ARRAY_C_CONTIGUOUS, NPY_ARRAY_ALIGNED, and NPY_ARRAY_F_CONTIGUOUS array flags can be \u201ccalculated\u201d from the array object itself. This routine updates one or more of these flags of arr as specified in flagmask by performing the required calculation.", "Warning", "It is important to keep the flags updated (using PyArray_UpdateFlags can help) whenever a manipulation with an array is performed that might cause them to change. Later calculations in NumPy that rely on the state of these flags do not repeat the calculation to update them.", "Equivalent to ndarray.getfield (self, dtype, offset). This function steals a reference to PyArray_Descr and returns a new array of the given dtype using the data in the current array at a specified offset in bytes. The offset plus the itemsize of the new array type must be less than self\n->descr->elsize or an error is raised. The same shape and strides as the original array are used. Therefore, this function has the effect of returning a field from a structured array. But, it can also be used to select specific bytes or groups of bytes from any array type.", "Equivalent to ndarray.setfield (self, val, dtype, offset ). Set the field starting at offset in bytes and of the given dtype to val. The offset plus dtype ->elsize must be less than self ->descr->elsize or an error is raised. Otherwise, the val argument is converted to an array and copied into the field pointed to. If necessary, the elements of val are repeated to fill the destination array, But, the number of elements in the destination must be an integer multiple of the number of elements in val.", "Equivalent to ndarray.byteswap (self, inplace). Return an array whose data area is byteswapped. If inplace is non-zero, then do the byteswap inplace and return a reference to self. Otherwise, create a byteswapped copy and leave self unchanged.", "Equivalent to ndarray.copy (self, fortran). Make a copy of the old array. The returned array is always aligned and writeable with data interpreted the same as the old array. If order is NPY_CORDER, then a C-style contiguous array is returned. If order is NPY_FORTRANORDER, then a Fortran-style contiguous array is returned. If order is NPY_ANYORDER, then the array returned is Fortran-style contiguous only if the old one is; otherwise, it is C-style contiguous.", "Equivalent to ndarray.tolist (self). Return a nested Python list from self.", "Equivalent to ndarray.tobytes (self, order). Return the bytes of this array in a Python string.", "Write the contents of self to the file pointer fp in C-style contiguous fashion. Write the data as binary bytes if sep is the string \u201c\u201dor NULL. Otherwise, write the contents of self as text using the sep string as the item separator. Each item will be printed to the file. If the format string is not NULL or \u201c\u201d, then it is a Python print statement format string showing how the items are to be written.", "Pickle the object in self to the given file (either a string or a Python file object). If file is a Python string it is considered to be the name of a file which is then opened in binary mode. The given protocol is used (if protocol is negative, or the highest available is used). This is a simple wrapper around cPickle.dump(self, file, protocol).", "Pickle the object in self to a Python string and return it. Use the Pickle protocol provided (or the highest available if protocol is negative).", "Fill the array, arr, with the given scalar object, obj. The object is first converted to the data type of arr, and then copied into every location. A -1 is returned if an error occurs, otherwise 0 is returned.", "Equivalent to ndarray.view (self, dtype). Return a new view of the array self as possibly a different data-type, dtype, and different array subclass ptype.", "If dtype is NULL, then the returned array will have the same data type as self. The new data-type must be consistent with the size of self. Either the itemsizes must be identical, or self must be single-segment and the total number of bytes must be the same. In the latter case the dimensions of the returned array will be altered in the last (or first for Fortran-style contiguous arrays) dimension. The data area of the returned array and self is exactly the same.", "Result will be a new array (pointing to the same memory location as self if possible), but having a shape given by newshape. If the new shape is not compatible with the strides of self, then a copy of the array with the new specified shape will be returned.", "Equivalent to ndarray.reshape (self, shape) where shape is a sequence. Converts shape to a PyArray_Dims structure and calls PyArray_Newshape internally. For back-ward compatibility \u2013 Not recommended", "Equivalent to ndarray.squeeze (self). Return a new view of self with all of the dimensions of length 1 removed from the shape.", "Warning", "matrix objects are always 2-dimensional. Therefore, PyArray_Squeeze has no effect on arrays of matrix sub-class.", "Equivalent to ndarray.swapaxes (self, a1, a2). The returned array is a new view of the data in self with the given axes, a1 and a2, swapped.", "Equivalent to ndarray.resize (self, newshape, refcheck = refcheck, order= fortran ). This function only works on single-segment arrays. It changes the shape of self inplace and will reallocate the memory for self if newshape has a different total number of elements then the old shape. If reallocation is necessary, then self must own its data, have self - >base==NULL, have self - >weakrefs==NULL, and (unless refcheck is 0) not be referenced by any other array. The fortran argument can be NPY_ANYORDER, NPY_CORDER, or NPY_FORTRANORDER. It currently has no effect. Eventually it could be used to determine how the resize operation should view the data when constructing a differently-dimensioned array. Returns None on success and NULL on error.", "Equivalent to ndarray.transpose (self, permute). Permute the axes of the ndarray object self according to the data structure permute and return the result. If permute is NULL, then the resulting array has its axes reversed. For example if self has shape \\(10\\times20\\times30\\), and permute .ptr is (0,2,1) the shape of the result is \\(10\\times30\\times20.\\) If permute is NULL, the shape of the result is \\(30\\times20\\times10.\\)", "Equivalent to ndarray.flatten (self, order). Return a 1-d copy of the array. If order is NPY_FORTRANORDER the elements are scanned out in Fortran order (first-dimension varies the fastest). If order is NPY_CORDER, the elements of self are scanned in C-order (last dimension varies the fastest). If order NPY_ANYORDER, then the result of PyArray_ISFORTRAN (self) is used to determine which order to flatten.", "Equivalent to self.ravel(order). Same basic functionality as PyArray_Flatten (self, order) except if order is 0 and self is C-style contiguous, the shape is altered but no copy is performed.", "Equivalent to ndarray.take (self, indices, axis, ret, clipmode) except axis =None in Python is obtained by setting axis = NPY_MAXDIMS in C. Extract the items from self indicated by the integer-valued indices along the given axis. The clipmode argument can be NPY_RAISE, NPY_WRAP, or NPY_CLIP to indicate what to do with out-of-bound indices. The ret argument can specify an output array rather than having one created internally.", "Equivalent to self.put(values, indices, clipmode ). Put values into self at the corresponding (flattened) indices. If values is too small it will be repeated as necessary.", "Place the values in self wherever corresponding positions (using a flattened context) in mask are true. The mask and self arrays must have the same total number of elements. If values is too small, it will be repeated as necessary.", "Equivalent to ndarray.repeat (self, op, axis). Copy the elements of self, op times along the given axis. Either op is a scalar integer or a sequence of length self ->dimensions[ axis ] indicating how many times to repeat each item along the axis.", "Equivalent to ndarray.choose (self, op, ret, clipmode). Create a new array by selecting elements from the sequence of arrays in op based on the integer values in self. The arrays must all be broadcastable to the same shape and the entries in self should be between 0 and len(op). The output is placed in ret unless it is NULL in which case a new output is created. The clipmode argument determines behavior for when entries in self are not between 0 and len(op).", "raise a ValueError;", "wrap values < 0 by adding len(op) and values >=len(op) by subtracting len(op) until they are in range;", "all values are clipped to the region [0, len(op) ).", "Equivalent to ndarray.sort (self, axis, kind). Return an array with the items of self sorted along axis. The array is sorted using the algorithm denoted by kind, which is an integer/enum pointing to the type of sorting algorithms used.", "Equivalent to ndarray.argsort (self, axis). Return an array of indices such that selection of these indices along the given axis would return a sorted version of self. If self ->descr is a data-type with fields defined, then self->descr->names is used to determine the sort order. A comparison where the first field is equal will use the second field and so on. To alter the sort order of a structured array, create a new data-type with a different order of names and construct a view of the array with that new data-type.", "Given a sequence of arrays (sort_keys) of the same shape, return an array of indices (similar to PyArray_ArgSort (\u2026)) that would sort the arrays lexicographically. A lexicographic sort specifies that when two keys are found to be equal, the order is based on comparison of subsequent keys. A merge sort (which leaves equal entries unmoved) is required to be defined for the types. The sort is accomplished by sorting the indices first using the first sort_key and then using the second sort_key and so forth. This is equivalent to the lexsort(sort_keys, axis) Python command. Because of the way the merge-sort works, be sure to understand the order the sort_keys must be in (reversed from the order you would use when comparing two elements).", "If these arrays are all collected in a structured array, then PyArray_Sort (\u2026) can also be used to sort the array directly.", "Equivalent to ndarray.searchsorted (self, values, side, perm). Assuming self is a 1-d array in ascending order, then the output is an array of indices the same shape as values such that, if the elements in values were inserted before the indices, the order of self would be preserved. No checking is done on whether or not self is in ascending order.", "The side argument indicates whether the index returned should be that of the first suitable location (if NPY_SEARCHLEFT) or of the last (if NPY_SEARCHRIGHT).", "The sorter argument, if not NULL, must be a 1D array of integer indices the same length as self, that sorts it into ascending order. This is typically the result of a call to PyArray_ArgSort (\u2026) Binary search is used to find the required insertion points.", "Equivalent to ndarray.partition (self, ktharray, axis, kind). Partitions the array so that the values of the element indexed by ktharray are in the positions they would be if the array is fully sorted and places all elements smaller than the kth before and all elements equal or greater after the kth element. The ordering of all elements within the partitions is undefined. If self->descr is a data-type with fields defined, then self->descr->names is used to determine the sort order. A comparison where the first field is equal will use the second field and so on. To alter the sort order of a structured array, create a new data-type with a different order of names and construct a view of the array with that new data-type. Returns zero on success and -1 on failure.", "Equivalent to ndarray.argpartition (self, ktharray, axis, kind). Return an array of indices such that selection of these indices along the given axis would return a partitioned version of self.", "Equivalent to ndarray.diagonal (self, offset, axis1, axis2 ). Return the offset diagonals of the 2-d arrays defined by axis1 and axis2.", "New in version 1.6.", "Counts the number of non-zero elements in the array object self.", "Equivalent to ndarray.nonzero (self). Returns a tuple of index arrays that select elements of self that are nonzero. If (nd= PyArray_NDIM ( self ))==1, then a single index array is returned. The index arrays have data type NPY_INTP. If a tuple is returned (nd \\(\\neq\\) 1), then its length is nd.", "Equivalent to ndarray.compress (self, condition, axis ). Return the elements along axis corresponding to elements of condition that are true.", "Tip", "Pass in NPY_MAXDIMS for axis in order to achieve the same effect that is obtained by passing in axis=None in Python (treating the array as a 1-d array).", "Note", "The out argument specifies where to place the result. If out is NULL, then the output array is created, otherwise the output is placed in out which must be the correct size and type. A new reference to the output array is always returned even when out is not NULL. The caller of the routine has the responsibility to Py_DECREF out if not NULL or a memory-leak will occur.", "Equivalent to ndarray.argmax (self, axis). Return the index of the largest element of self along axis.", "Equivalent to ndarray.argmin (self, axis). Return the index of the smallest element of self along axis.", "Equivalent to ndarray.max (self, axis). Returns the largest element of self along the given axis. When the result is a single element, returns a numpy scalar instead of an ndarray.", "Equivalent to ndarray.min (self, axis). Return the smallest element of self along the given axis. When the result is a single element, returns a numpy scalar instead of an ndarray.", "Equivalent to ndarray.ptp (self, axis). Return the difference between the largest element of self along axis and the smallest element of self along axis. When the result is a single element, returns a numpy scalar instead of an ndarray.", "Note", "The rtype argument specifies the data-type the reduction should take place over. This is important if the data-type of the array is not \u201clarge\u201d enough to handle the output. By default, all integer data-types are made at least as large as NPY_LONG for the \u201cadd\u201d and \u201cmultiply\u201d ufuncs (which form the basis for mean, sum, cumsum, prod, and cumprod functions).", "Equivalent to ndarray.mean (self, axis, rtype). Returns the mean of the elements along the given axis, using the enumerated type rtype as the data type to sum in. Default sum behavior is obtained using NPY_NOTYPE for rtype.", "Equivalent to ndarray.trace (self, offset, axis1, axis2, rtype). Return the sum (using rtype as the data type of summation) over the offset diagonal elements of the 2-d arrays defined by axis1 and axis2 variables. A positive offset chooses diagonals above the main diagonal. A negative offset selects diagonals below the main diagonal.", "Equivalent to ndarray.clip (self, min, max). Clip an array, self, so that values larger than max are fixed to max and values less than min are fixed to min.", "Equivalent to ndarray.conjugate (self). Return the complex conjugate of self. If self is not of complex data type, then return self with a reference.", "Equivalent to ndarray.round (self, decimals, out). Returns the array with elements rounded to the nearest decimal place. The decimal place is defined as the \\(10^{-\\textrm{decimals}}\\) digit so that negative decimals cause rounding to the nearest 10\u2019s, 100\u2019s, etc. If out is NULL, then the output array is created, otherwise the output is placed in out which must be the correct size and type.", "Equivalent to ndarray.std (self, axis, rtype). Return the standard deviation using data along axis converted to data type rtype.", "Equivalent to ndarray.sum (self, axis, rtype). Return 1-d vector sums of elements in self along axis. Perform the sum after converting data to data type rtype.", "Equivalent to ndarray.cumsum (self, axis, rtype). Return cumulative 1-d sums of elements in self along axis. Perform the sum after converting data to data type rtype.", "Equivalent to ndarray.prod (self, axis, rtype). Return 1-d products of elements in self along axis. Perform the product after converting data to data type rtype.", "Equivalent to ndarray.cumprod (self, axis, rtype). Return 1-d cumulative products of elements in self along axis. Perform the product after converting data to data type rtype.", "Equivalent to ndarray.all (self, axis). Return an array with True elements for every 1-d sub-array of self defined by axis in which all the elements are True.", "Equivalent to ndarray.any (self, axis). Return an array with True elements for every 1-d sub-array of self defined by axis in which any of the elements are True.", "Sometimes it is useful to access a multidimensional array as a C-style multi-dimensional array so that algorithms can be implemented using C\u2019s a[i][j][k] syntax. This routine returns a pointer, ptr, that simulates this kind of C-style array, for 1-, 2-, and 3-d ndarrays.", "Note", "The simulation of a C-style array is not complete for 2-d and 3-d arrays. For example, the simulated arrays of pointers cannot be passed to subroutines expecting specific, statically-defined 2-d and 3-d arrays. To pass to functions requiring those kind of inputs, you must statically define the required array and copy data.", "Must be called with the same objects and memory locations returned from PyArray_AsCArray (\u2026). This function cleans up memory that otherwise would get leaked.", "Join the sequence of objects in obj together along axis into a single array. If the dimensions or types are not compatible an error is raised.", "Compute a product-sum over the last dimensions of obj1 and obj2. Neither array is conjugated.", "Compute a product-sum over the last dimension of obj1 and the second-to-last dimension of obj2. For 2-d arrays this is a matrix-product. Neither array is conjugated.", "New in version 1.6.", "Same as PyArray_MatrixProduct, but store the result in out. The output array must have the correct shape, type, and be C-contiguous, or an exception is raised.", "New in version 1.6.", "Applies the Einstein summation convention to the array operands provided, returning a new array or placing the result in out. The string in subscripts is a comma separated list of index letters. The number of operands is in nop, and op_in is an array containing those operands. The data type of the output can be forced with dtype, the output order can be forced with order (NPY_KEEPORDER is recommended), and when dtype is specified, casting indicates how permissive the data conversion should be.", "See the einsum function for more details.", "A specialized copy and transpose function that works only for 2-d arrays. The returned array is a transposed copy of op.", "Compute the 1-d correlation of the 1-d arrays op1 and op2 . The correlation is computed at each output point by multiplying op1 by a shifted version of op2 and summing the result. As a result of the shift, needed values outside of the defined range of op1 and op2 are interpreted as zero. The mode determines how many shifts to return: 0 - return only shifts that did not need to assume zero- values; 1 - return an object that is the same size as op1, 2 - return all possible shifts (any overlap at all is accepted).", "This does not compute the usual correlation: if op2 is larger than op1, the arguments are swapped, and the conjugate is never taken for complex arrays. See PyArray_Correlate2 for the usual signal processing correlation.", "Updated version of PyArray_Correlate, which uses the usual definition of correlation for 1d arrays. The correlation is computed at each output point by multiplying op1 by a shifted version of op2 and summing the result. As a result of the shift, needed values outside of the defined range of op1 and op2 are interpreted as zero. The mode determines how many shifts to return: 0 - return only shifts that did not need to assume zero- values; 1 - return an object that is the same size as op1, 2 - return all possible shifts (any overlap at all is accepted).", "Compute z as follows:", "If both x and y are NULL, then return PyArray_Nonzero (condition). Otherwise, both x and y must be given and the object returned is shaped like condition and has elements of x and y where condition is respectively True or False.", "Determine if newstrides is a strides array consistent with the memory of an nd -dimensional array with shape dims and element-size, elsize. The newstrides array is checked to see if jumping by the provided number of bytes in each direction will ever mean jumping more than numbytes which is the assumed size of the available memory segment. If numbytes is 0, then an equivalent numbytes is computed assuming nd, dims, and elsize refer to a single-segment array. Return NPY_TRUE if newstrides is acceptable, otherwise return NPY_FALSE.", "Both of these routines multiply an n -length array, seq, of integers and return the result. No overflow checking is performed.", "Given two n -length arrays of integers, l1, and l2, return 1 if the lists are identical; otherwise, return 0.", "New in version 1.7.0.", "When working with more complex dtypes which are composed of other dtypes, such as the struct dtype, creating inner loops that manipulate the dtypes requires carrying along additional data. NumPy supports this idea through a struct NpyAuxData, mandating a few conventions so that it is possible to do this.", "Defining an NpyAuxData is similar to defining a class in C++, but the object semantics have to be tracked manually since the API is in C. Here\u2019s an example for a function which doubles up an element using an element copier function as a primitive.", "The function pointer type for NpyAuxData free functions.", "The function pointer type for NpyAuxData clone functions. These functions should never set the Python exception on error, because they may be called from a multi-threaded context.", "A macro which calls the auxdata\u2019s free function appropriately, does nothing if auxdata is NULL.", "A macro which calls the auxdata\u2019s clone function appropriately, returning a deep copy of the auxiliary data.", "As of NumPy 1.6.0, these array iterators are superseded by the new array iterator, NpyIter.", "An array iterator is a simple way to access the elements of an N-dimensional array quickly and efficiently. Section 2 provides more description and examples of this useful approach to looping over an array.", "Return an array iterator object from the array, arr. This is equivalent to arr. flat. The array iterator object makes it easy to loop over an N-dimensional non-contiguous array in C-style contiguous fashion.", "Return an array iterator that will iterate over all axes but the one provided in *axis. The returned iterator cannot be used with PyArray_ITER_GOTO1D. This iterator could be used to write something similar to what ufuncs do wherein the loop over the largest axis is done by a separate sub-routine. If *axis is negative then *axis will be set to the axis having the smallest stride and that axis will be used.", "Return an array iterator that is broadcast to iterate as an array of the shape provided by dimensions and nd.", "Evaluates true if op is an array iterator (or instance of a subclass of the array iterator type).", "Reset an iterator to the beginning of the array.", "Incremement the index and the dataptr members of the iterator to point to the next element of the array. If the array is not (C-style) contiguous, also increment the N-dimensional coordinates array.", "A pointer to the current element of the array.", "Set the iterator index, dataptr, and coordinates members to the location in the array indicated by the N-dimensional c-array, destination, which must have size at least iterator ->nd_m1+1.", "Set the iterator index and dataptr to the location in the array indicated by the integer index which points to an element in the C-styled flattened array.", "Evaluates TRUE as long as the iterator has not looped through all of the elements, otherwise it evaluates FALSE.", "A simplified interface to broadcasting. This function takes the number of arrays to broadcast and then num extra ( PyObject * ) arguments. These arguments are converted to arrays and iterators are created. PyArray_Broadcast is then called on the resulting multi-iterator object. The resulting, broadcasted mult-iterator object is then returned. A broadcasted operation can then be performed using a single loop and using PyArray_MultiIter_NEXT (..)", "Reset all the iterators to the beginning in a multi-iterator object, multi.", "Advance each iterator in a multi-iterator object, multi, to its next (broadcasted) element.", "Return the data-pointer of the i \\(^{\\textrm{th}}\\) iterator in a multi-iterator object.", "Advance the pointer of only the i \\(^{\\textrm{th}}\\) iterator.", "Advance each iterator in a multi-iterator object, multi, to the given \\(N\\) -dimensional destination where \\(N\\) is the number of dimensions in the broadcasted array.", "Advance each iterator in a multi-iterator object, multi, to the corresponding location of the index into the flattened broadcasted array.", "Evaluates TRUE as long as the multi-iterator has not looped through all of the elements (of the broadcasted result), otherwise it evaluates FALSE.", "This function encapsulates the broadcasting rules. The mit container should already contain iterators for all the arrays that need to be broadcast. On return, these iterators will be adjusted so that iteration over each simultaneously will accomplish the broadcasting. A negative number is returned if an error occurs.", "This function takes a multi-iterator object that has been previously \u201cbroadcasted,\u201d finds the dimension with the smallest \u201csum of strides\u201d in the broadcasted result and adapts all the iterators so as not to iterate over that dimension (by effectively making them of length-1 in that dimension). The corresponding dimension is returned unless mit ->nd is 0, then -1 is returned. This function is useful for constructing ufunc-like routines that broadcast their inputs correctly and then call a strided 1-d version of the routine as the inner-loop. This 1-d version is usually optimized for speed and for this reason the loop should be performed over the axis that won\u2019t require large stride jumps.", "New in version 1.4.0.", "Neighborhood iterators are subclasses of the iterator object, and can be used to iter over a neighborhood of a point. For example, you may want to iterate over every voxel of a 3d image, and for every such voxel, iterate over an hypercube. Neighborhood iterator automatically handle boundaries, thus making this kind of code much easier to write than manual boundaries handling, at the cost of a slight overhead.", "This function creates a new neighborhood iterator from an existing iterator. The neighborhood will be computed relatively to the position currently pointed by iter, the bounds define the shape of the neighborhood iterator, and the mode argument the boundaries handling mode.", "The bounds argument is expected to be a (2 * iter->ao->nd) arrays, such as the range bound[2*i]->bounds[2*i+1] defines the range where to walk for dimension i (both bounds are included in the walked coordinates). The bounds should be ordered for each dimension (bounds[2*i] <= bounds[2*i+1]).", "The mode should be one of:", "Zero padding. Outside bounds values will be 0.", "One padding, Outside bounds values will be 1.", "Constant padding. Outside bounds values will be the same as the first item in fill_value.", "Mirror padding. Outside bounds values will be as if the array items were mirrored. For example, for the array [1, 2, 3, 4], x[-2] will be 2, x[-2] will be 1, x[4] will be 4, x[5] will be 1, etc\u2026", "Circular padding. Outside bounds values will be as if the array was repeated. For example, for the array [1, 2, 3, 4], x[-2] will be 3, x[-2] will be 4, x[4] will be 1, x[5] will be 2, etc\u2026", "If the mode is constant filling (NPY_NEIGHBORHOOD_ITER_CONSTANT_PADDING), fill_value should point to an array object which holds the filling value (the first item will be the filling value if the array contains more than one item). For other cases, fill_value may be NULL.", "Reset the iterator position to the first point of the neighborhood. This should be called whenever the iter argument given at PyArray_NeighborhoodIterObject is changed (see example)", "After this call, iter->dataptr points to the next point of the neighborhood. Calling this function after every point of the neighborhood has been visited is undefined.", "Array mapping is the machinery behind advanced indexing.", "Use advanced indexing to iterate an array.", "Swap the axes to or from their inserted form. MapIter always puts the advanced (array) indices first in the iteration. But if they are consecutive, it will insert/transpose them back before returning. This is stored as mit->consec != 0 (the place where they are inserted). For assignments, the opposite happens: the values to be assigned are transposed (getmap=1 instead of getmap=0). getmap=0 and getmap=1 undo the other operation.", "This function needs to update the state of the map iterator and point mit->dataptr to the memory-location of the next object.", "Note that this function never handles an extra operand but provides compatibility for an old (exposed) API.", "Similar to PyArray_MapIterArray but with an additional copy_if_overlap argument. If copy_if_overlap != 0, checks if a has memory overlap with any of the arrays in index and with extra_op, and make copies as appropriate to avoid problems if the input is modified during the iteration. iter->array may contain a copied array (UPDATEIFCOPY/WRITEBACKIFCOPY set).", "This function steals a reference to arr.", "This function checks to see if arr is a 0-dimensional array and, if so, returns the appropriate array scalar. It should be used whenever 0-dimensional arrays could be returned to Python.", "Return an array scalar object of the given dtype by copying from memory pointed to by data. base is expected to be the array object that is the owner of the data. base is required if dtype is a void scalar, or if the NPY_USE_GETITEM flag is set and it is known that the getitem method uses the arr argument without checking if it is NULL. Otherwise base may be NULL.", "If the data is not in native byte order (as indicated by dtype->byteorder) then this function will byteswap the data, because array scalars are always in correct machine-byte order.", "Return an array scalar object of the type and itemsize indicated by the array object arr copied from the memory pointed to by data and swapping if the data in arr is not in machine byte-order.", "Return a 0-dimensional array of type determined by outcode from scalar which should be an array-scalar object. If outcode is NULL, then the type is determined from scalar.", "Return in ctypeptr a pointer to the actual value in an array scalar. There is no error checking so scalar must be an array-scalar object, and ctypeptr must have enough space to hold the correct type. For flexible-sized types, a pointer to the data is copied into the memory of ctypeptr, for all other types, the actual data is copied into the address pointed to by ctypeptr.", "Return the data (cast to the data type indicated by outcode) from the array-scalar, scalar, into the memory pointed to by ctypeptr (which must be large enough to handle the incoming memory).", "Returns a scalar type-object from a type-number, type . Equivalent to PyArray_DescrFromType (type)->typeobj except for reference counting and error-checking. Returns a new reference to the typeobject on success or NULL on failure.", "See the function PyArray_MinScalarType for an alternative mechanism introduced in NumPy 1.6.0.", "Return the kind of scalar represented by typenum and the array in *arr (if arr is not NULL ). The array is assumed to be rank-0 and only used if typenum represents a signed integer. If arr is not NULL and the first element is negative then NPY_INTNEG_SCALAR is returned, otherwise NPY_INTPOS_SCALAR is returned. The possible return values are the enumerated values in NPY_SCALARKIND.", "See the function PyArray_ResultType for details of NumPy type promotion, updated in NumPy 1.6.0.", "Implements the rules for scalar coercion. Scalars are only silently coerced from thistype to neededtype if this function returns nonzero. If scalar is NPY_NOSCALAR, then this function is equivalent to PyArray_CanCastSafely. The rule is that scalars of the same KIND can be coerced into arrays of the same KIND. This rule means that high-precision scalars will never cause low-precision arrays of the same KIND to be upcast.", "Warning", "Data-type objects must be reference counted so be aware of the action on the data-type reference of different C-API calls. The standard rule is that when a data-type object is returned it is a new reference. Functions that take PyArray_Descr* objects and return arrays steal references to the data-type their inputs unless otherwise noted. Therefore, you must own a reference to any data-type object used as input to such a function.", "Evaluates as true if obj is a data-type object ( PyArray_Descr* ).", "Return a new data-type object copied from obj (the fields reference is just updated so that the new object points to the same fields dictionary if any).", "Create a new data-type object from the built-in (or user-registered) data-type indicated by typenum. All builtin types should not have any of their fields changed. This creates a new copy of the PyArray_Descr structure so that you can fill it in as appropriate. This function is especially needed for flexible data-types which need to have a new elsize member in order to be meaningful in array construction.", "Create a new data-type object with the byteorder set according to newendian. All referenced data-type objects (in subdescr and fields members of the data-type object) are also changed (recursively).", "The value of newendian is one of these macros:", "If a byteorder of NPY_IGNORE is encountered it is left alone. If newendian is NPY_SWAP, then all byte-orders are swapped. Other valid newendian values are NPY_NATIVE, NPY_LITTLE, and NPY_BIG which all cause the returned data-typed descriptor (and all it\u2019s referenced data-type descriptors) to have the corresponding byte- order.", "Determine an appropriate data-type object from the object op (which should be a \u201cnested\u201d sequence object) and the minimum data-type descriptor mintype (which can be NULL ). Similar in behavior to array(op).dtype. Don\u2019t confuse this function with PyArray_DescrConverter. This function essentially looks at all the objects in the (nested) sequence and determines the data-type from the elements it finds.", "Return a data-type object from an array-scalar object. No checking is done to be sure that scalar is an array scalar. If no suitable data-type can be determined, then a data-type of NPY_OBJECT is returned by default.", "Returns a data-type object corresponding to typenum. The typenum can be one of the enumerated types, a character code for one of the enumerated types, or a user-defined type. If you want to use a flexible size array, then you need to flexible typenum and set the results elsize parameter to the desired size. The typenum is one of the NPY_TYPES.", "Convert any compatible Python object, obj, to a data-type object in dtype. A large number of Python objects can be converted to data-type objects. See Data type objects (dtype) for a complete description. This version of the converter converts None objects to a NPY_DEFAULT_TYPE data-type object. This function can be used with the \u201cO&\u201d character code in PyArg_ParseTuple processing.", "Convert any compatible Python object, obj, to a data-type object in dtype. This version of the converter converts None objects so that the returned data-type is NULL. This function can also be used with the \u201cO&\u201d character in PyArg_ParseTuple processing.", "Like PyArray_DescrConverter except it aligns C-struct-like objects on word-boundaries as the compiler would.", "Like PyArray_DescrConverter2 except it aligns C-struct-like objects on word-boundaries as the compiler would.", "Take the fields dictionary, dict, such as the one attached to a data-type object and construct an ordered-list of field names such as is stored in the names field of the PyArray_Descr object.", "All of these functions can be used in PyArg_ParseTuple (\u2026) with the \u201cO&\u201d format specifier to automatically convert any Python object to the required C-object. All of these functions return NPY_SUCCEED if successful and NPY_FAIL if not. The first argument to all of these function is a Python object. The second argument is the address of the C-type to convert the Python object to.", "Warning", "Be sure to understand what steps you should take to manage the memory when using these conversion functions. These functions can require freeing memory, and/or altering the reference counts of specific objects based on your use.", "Convert any Python object to a PyArrayObject. If PyArray_Check (obj) is TRUE then its reference count is incremented and a reference placed in address. If obj is not an array, then convert it to an array using PyArray_FromAny . No matter what is returned, you must DECREF the object returned by this routine in address when you are done with it.", "This is a default converter for output arrays given to functions. If obj is Py_None or NULL, then *address will be NULL but the call will succeed. If PyArray_Check ( obj) is TRUE then it is returned in *address without incrementing its reference count.", "Convert any Python sequence, obj, smaller than NPY_MAXDIMS to a C-array of npy_intp. The Python object could also be a single number. The seq variable is a pointer to a structure with members ptr and len. On successful return, seq ->ptr contains a pointer to memory that must be freed, by calling PyDimMem_FREE, to avoid a memory leak. The restriction on memory size allows this converter to be conveniently used for sequences intended to be interpreted as array shapes.", "Convert any Python object, obj, with a (single-segment) buffer interface to a variable with members that detail the object\u2019s use of its chunk of memory. The buf variable is a pointer to a structure with base, ptr, len, and flags members. The PyArray_Chunk structure is binary compatible with the Python\u2019s buffer object (through its len member on 32-bit platforms and its ptr member on 64-bit platforms or in Python 2.5). On return, the base member is set to obj (or its base if obj is already a buffer object pointing to another object). If you need to hold on to the memory be sure to INCREF the base member. The chunk of memory is pointed to by buf ->ptr member and has length buf ->len. The flags member of buf is NPY_ARRAY_ALIGNED with the NPY_ARRAY_WRITEABLE flag set if obj has a writeable buffer interface.", "Convert a Python object, obj, representing an axis argument to the proper value for passing to the functions that take an integer axis. Specifically, if obj is None, axis is set to NPY_MAXDIMS which is interpreted correctly by the C-API functions that take axis arguments.", "Convert any Python object, obj, to NPY_TRUE or NPY_FALSE, and place the result in value.", "Convert Python strings into the corresponding byte-order character: \u2018>\u2019, \u2018<\u2019, \u2018s\u2019, \u2018=\u2019, or \u2018|\u2019.", "Convert Python strings into one of NPY_QUICKSORT (starts with \u2018q\u2019 or \u2018Q\u2019), NPY_HEAPSORT (starts with \u2018h\u2019 or \u2018H\u2019), NPY_MERGESORT (starts with \u2018m\u2019 or \u2018M\u2019) or NPY_STABLESORT (starts with \u2018t\u2019 or \u2018T\u2019). NPY_MERGESORT and NPY_STABLESORT are aliased to each other for backwards compatibility and may refer to one of several stable sorting algorithms depending on the data type.", "Convert Python strings into one of NPY_SEARCHLEFT (starts with \u2018l\u2019 or \u2018L\u2019), or NPY_SEARCHRIGHT (starts with \u2018r\u2019 or \u2018R\u2019).", "Convert the Python strings \u2018C\u2019, \u2018F\u2019, \u2018A\u2019, and \u2018K\u2019 into the NPY_ORDER enumeration NPY_CORDER, NPY_FORTRANORDER, NPY_ANYORDER, and NPY_KEEPORDER.", "Convert the Python strings \u2018no\u2019, \u2018equiv\u2019, \u2018safe\u2019, \u2018same_kind\u2019, and \u2018unsafe\u2019 into the NPY_CASTING enumeration NPY_NO_CASTING, NPY_EQUIV_CASTING, NPY_SAFE_CASTING, NPY_SAME_KIND_CASTING, and NPY_UNSAFE_CASTING.", "Convert the Python strings \u2018clip\u2019, \u2018wrap\u2019, and \u2018raise\u2019 into the NPY_CLIPMODE enumeration NPY_CLIP, NPY_WRAP, and NPY_RAISE.", "Converts either a sequence of clipmodes or a single clipmode into a C array of NPY_CLIPMODE values. The number of clipmodes n must be known before calling this function. This function is provided to help functions allow a different clipmode for each dimension.", "Convert all kinds of Python objects (including arrays and array scalars) to a standard integer. On error, -1 is returned and an exception set. You may find useful the macro:", "Convert all kinds of Python objects (including arrays and array scalars) to a (platform-pointer-sized) integer. On error, -1 is returned and an exception set.", "Convert any Python sequence (or single Python number) passed in as seq to (up to) maxvals pointer-sized integers and place them in the vals array. The sequence can be smaller then maxvals as the number of converted objects is returned.", "Convert typestring characters (with itemsize) to basic enumerated data types. The typestring character corresponding to signed and unsigned integers, floating point numbers, and complex-floating point numbers are recognized and converted. Other values of gentype are returned. This function can be used to convert, for example, the string \u2018f4\u2019 to NPY_FLOAT32.", "In order to make use of the C-API from another extension module, the import_array function must be called. If the extension module is self-contained in a single .c file, then that is all that needs to be done. If, however, the extension module involves multiple files where the C-API is needed then some additional steps must be taken.", "This function must be called in the initialization section of a module that will make use of the C-API. It imports the module where the function-pointer table is stored and points the correct variable to it.", "Using these #defines you can use the C-API in multiple files for a single extension module. In each file you must define PY_ARRAY_UNIQUE_SYMBOL to some name that will hold the C-API (e.g. myextension_ARRAY_API). This must be done before including the numpy/arrayobject.h file. In the module initialization routine you call import_array. In addition, in the files that do not have the module initialization sub_routine define NO_IMPORT_ARRAY prior to including numpy/arrayobject.h.", "Suppose I have two files coolmodule.c and coolhelper.c which need to be compiled and linked into a single extension module. Suppose coolmodule.c contains the required initcool module initialization function (with the import_array() function called). Then, coolmodule.c would have at the top:", "On the other hand, coolhelper.c would contain at the top:", "You can also put the common two last lines into an extension-local header file as long as you make sure that NO_IMPORT_ARRAY is #defined before #including that file.", "Internally, these #defines work as follows:", "Because python extensions are not used in the same way as usual libraries on most platforms, some errors cannot be automatically detected at build time or even runtime. For example, if you build an extension using a function available only for numpy >= 1.3.0, and you import the extension later with numpy 1.2, you will not get an import error (but almost certainly a segmentation fault when calling the function). That\u2019s why several functions are provided to check for numpy versions. The macros NPY_VERSION and NPY_FEATURE_VERSION corresponds to the numpy version used to build the extension, whereas the versions returned by the functions PyArray_GetNDArrayCVersion and PyArray_GetNDArrayCFeatureVersion corresponds to the runtime numpy\u2019s version.", "The rules for ABI and API compatibilities can be summarized as follows:", "ABI incompatibility is automatically detected in every numpy\u2019s version. API incompatibility detection was added in numpy 1.4.0. If you want to supported many different numpy versions with one extension binary, you have to build your extension with the lowest NPY_FEATURE_VERSION as possible.", "The current version of the ndarray object (check to see if this variable is defined to guarantee the numpy/arrayobject.h header is being used).", "The current version of the C-API.", "This just returns the value NPY_VERSION. NPY_VERSION changes whenever a backward incompatible change at the ABI level. Because it is in the C-API, however, comparing the output of this function from the value defined in the current header gives a way to test if the C-API has changed thus requiring a re-compilation of extension modules that use the C-API. This is automatically checked in the function import_array.", "New in version 1.4.0.", "This just returns the value NPY_FEATURE_VERSION. NPY_FEATURE_VERSION changes whenever the API changes (e.g. a function is added). A changed value does not always require a recompile.", "NumPy stores an internal table of Python callable objects that are used to implement arithmetic operations for arrays as well as certain array calculation methods. This function allows the user to replace any or all of these Python objects with their own versions. The keys of the dictionary, dict, are the named functions to replace and the paired value is the Python callable object to use. Care should be taken that the function used to replace an internal array operation does not itself call back to that internal array operation (unless you have designed the function to handle that), or an unchecked infinite recursion can result (possibly causing program crash). The key names that represent operations that can be replaced are:", "add, subtract, multiply, divide, remainder, power, square, reciprocal, ones_like, sqrt, negative, positive, absolute, invert, left_shift, right_shift, bitwise_and, bitwise_xor, bitwise_or, less, less_equal, equal, not_equal, greater, greater_equal, floor_divide, true_divide, logical_or, logical_and, floor, ceil, maximum, minimum, rint.", "These functions are included here because they are used at least once in the array object\u2019s methods. The function returns -1 (without setting a Python Error) if one of the objects being assigned is not callable.", "Deprecated since version 1.16.", "Return a Python dictionary containing the callable Python objects stored in the internal arithmetic operation table. The keys of this dictionary are given in the explanation for PyArray_SetNumericOps.", "Deprecated since version 1.16.", "This function allows you to alter the tp_str and tp_repr methods of the array object to any Python function. Thus you can alter what happens for all arrays when str(arr) or repr(arr) is called from Python. The function to be called is passed in as op. If repr is non-zero, then this function will be called in response to repr(arr), otherwise the function will be called in response to str(arr). No check on whether or not op is callable is performed. The callable passed in to op should expect an array argument and should return a string to be printed.", "Macros to allocate, free, and reallocate memory. These macros are used internally to create arrays.", "Macros to allocate, free, and reallocate dimension and strides memory.", "These macros use different memory allocators, depending on the constant NPY_USE_PYMEM. The system malloc is used when NPY_USE_PYMEM is 0, if NPY_USE_PYMEM is 1, then the Python memory allocator is used.", "If obj.flags has NPY_ARRAY_WRITEBACKIFCOPY or (deprecated) NPY_ARRAY_UPDATEIFCOPY, this function clears the flags, DECREF s obj->base and makes it writeable, and sets obj->base to NULL. It then copies obj->data to obj->base->data, and returns the error state of the copy operation. This is the opposite of PyArray_SetWritebackIfCopyBase. Usually this is called once you are finished with obj, just before Py_DECREF(obj). It may be called multiple times, or with NULL input. See also PyArray_DiscardWritebackIfCopy.", "Returns 0 if nothing was done, -1 on error, and 1 if action was taken.", "These macros are only meaningful if NPY_ALLOW_THREADS evaluates True during compilation of the extension module. Otherwise, these macros are equivalent to whitespace. Python uses a single Global Interpreter Lock (GIL) for each Python process so that only a single thread may execute at a time (even on multi-cpu machines). When calling out to a compiled function that may take time to compute (and does not have side-effects for other threads like updated global variables), the GIL should be released so that other Python threads can run while the time-consuming calculations are performed. This can be accomplished using two groups of macros. Typically, if one macro in a group is used in a code block, all of them must be used in the same code block. Currently, NPY_ALLOW_THREADS is defined to the python-defined WITH_THREADS constant unless the environment variable NPY_NOSMP is set in which case NPY_ALLOW_THREADS is defined to be 0.", "This group is used to call code that may take some time but does not use any Python C-API calls. Thus, the GIL should be released during its calculation.", "Equivalent to Py_BEGIN_ALLOW_THREADS except it uses NPY_ALLOW_THREADS to determine if the macro if replaced with white-space or not.", "Equivalent to Py_END_ALLOW_THREADS except it uses NPY_ALLOW_THREADS to determine if the macro if replaced with white-space or not.", "Place in the variable declaration area. This macro sets up the variable needed for storing the Python state.", "Place right before code that does not need the Python interpreter (no Python C-API calls). This macro saves the Python state and releases the GIL.", "Place right after code that does not need the Python interpreter. This macro acquires the GIL and restores the Python state from the saved variable.", "Useful to release the GIL only if dtype does not contain arbitrary Python objects which may need the Python interpreter during execution of the loop.", "Useful to regain the GIL in situations where it was released using the BEGIN form of this macro.", "Useful to release the GIL only if loop_size exceeds a minimum threshold, currently set to 500. Should be matched with a NPY_END_THREADS to regain the GIL.", "This group is used to re-acquire the Python GIL after it has been released. For example, suppose the GIL has been released (using the previous calls), and then some path in the code (perhaps in a different subroutine) requires use of the Python C-API, then these macros are useful to acquire the GIL. These macros accomplish essentially a reverse of the previous three (acquire the LOCK saving what state it had) and then re-release it with the saved state.", "Place in the variable declaration area to set up the necessary variable.", "Place before code that needs to call the Python C-API (when it is known that the GIL has already been released).", "Place after code that needs to call the Python C-API (to re-release the GIL).", "Tip", "Never use semicolons after the threading support macros.", "Default priority for arrays.", "Default subtype priority.", "Default scalar priority (very small)", "Return the __array_priority__ attribute (converted to a double) of obj or def if no attribute of that name exists. Fast returns that avoid the attribute lookup are provided for objects of type PyArray_Type.", "Default size of the user-settable internal buffers.", "Smallest size of user-settable internal buffers.", "Largest size allowed for the user-settable buffers.", "The number of floating-point types", "The maximum number of dimensions allowed in arrays.", "The maximum number of array arguments that can be used in functions.", "Defined as 0 for use with Bool.", "Defined as 1 for use with Bool.", "The return value of failed converter functions which are called using the \u201cO&\u201d syntax in PyArg_ParseTuple-like functions.", "The return value of successful converter functions which are called using the \u201cO&\u201d syntax in PyArg_ParseTuple-like functions.", "Evaluates as True if arrays a1 and a2 have the same shape.", "Returns the maximum of a and b. If (a) or (b) are expressions they are evaluated twice.", "Returns the minimum of a and b. If (a) or (b) are expressions they are evaluated twice.", "Implements the complex comparisons between two complex numbers (structures with a real and imag member) using NumPy\u2019s definition of the ordering which is lexicographic: comparing the real parts first and then the complex parts if the real parts are equal.", "Returns the reference count of any Python object.", "If obj.flags has NPY_ARRAY_WRITEBACKIFCOPY or (deprecated) NPY_ARRAY_UPDATEIFCOPY, this function clears the flags, DECREF s obj->base and makes it writeable, and sets obj->base to NULL. In contrast to PyArray_DiscardWritebackIfCopy it makes no attempt to copy the data from obj->base This undoes PyArray_SetWritebackIfCopyBase. Usually this is called after an error when you are finished with obj, just before Py_DECREF(obj). It may be called multiple times, or with NULL input.", "Deprecated in 1.14, use PyArray_DiscardWritebackIfCopy followed by Py_XDECREF", "DECREF\u2019s an array object which may have the (deprecated) NPY_ARRAY_UPDATEIFCOPY or NPY_ARRAY_WRITEBACKIFCOPY flag set without causing the contents to be copied back into the original array. Resets the NPY_ARRAY_WRITEABLE flag on the base object. This is useful for recovering from an error condition when writeback semantics are used, but will lead to wrong results.", "A special variable-type which can take on different values to indicate the sorting algorithm being used.", "Used as an alias of NPY_MERGESORT and vica versa.", "Defined to be the number of sorts. It is fixed at three by the need for backwards compatibility, and consequently NPY_MERGESORT and NPY_STABLESORT are aliased to each other and may refer to one of several stable sorting algorithms depending on the data type.", "A special variable type indicating the number of \u201ckinds\u201d of scalars distinguished in determining scalar-coercion rules. This variable can take on the values:", "Defined to be the number of scalar kinds (not including NPY_NOSCALAR).", "An enumeration type indicating the element order that an array should be interpreted in. When a brand new array is created, generally only NPY_CORDER and NPY_FORTRANORDER are used, whereas when one or more inputs are provided, the order can be based on them.", "Fortran order if all the inputs are Fortran, C otherwise.", "C order.", "Fortran order.", "An order as close to the order of the inputs as possible, even if the input is in neither C nor Fortran order.", "A variable type indicating the kind of clipping that should be applied in certain functions.", "The default for most operations, raises an exception if an index is out of bounds.", "Clips an index to the valid range if it is out of bounds.", "Wraps an index to the valid range if it is out of bounds.", "A variable type indicating whether the index returned should be that of the first suitable location (if NPY_SEARCHLEFT) or of the last (if NPY_SEARCHRIGHT).", "A variable type indicating the selection algorithm being used.", "New in version 1.6.", "An enumeration type indicating how permissive data conversions should be. This is used by the iterator added in NumPy 1.6, and is intended to be used more broadly in a future version.", "Only allow identical types.", "Allow identical and casts involving byte swapping.", "Only allow casts which will not cause values to be rounded, truncated, or otherwise changed.", "Allow any safe casts, and casts between types of the same kind. For example, float64 -> float32 is permitted with this rule.", "Allow any cast, no matter what kind of data loss may occur."]}, {"name": "int PyArray_ObjectType()", "path": "reference/c-api/array#c.PyArray_ObjectType", "type": "Array API", "text": ["This function is superseded by PyArray_MinScalarType and/or PyArray_ResultType.", "This function is useful for determining a common type that two or more arrays can be converted to. It only works for non-flexible array types as no itemsize information is passed. The mintype argument represents the minimum type acceptable, and op represents the object that will be converted to an array. The return value is the enumerated typenumber that represents the data-type that op should have."]}, {"name": "int PyArray_OrderConverter()", "path": "reference/c-api/array#c.PyArray_OrderConverter", "type": "Array API", "text": ["Convert the Python strings \u2018C\u2019, \u2018F\u2019, \u2018A\u2019, and \u2018K\u2019 into the NPY_ORDER enumeration NPY_CORDER, NPY_FORTRANORDER, NPY_ANYORDER, and NPY_KEEPORDER."]}, {"name": "int PyArray_OutputConverter()", "path": "reference/c-api/array#c.PyArray_OutputConverter", "type": "Array API", "text": ["This is a default converter for output arrays given to functions. If obj is Py_None or NULL, then *address will be NULL but the call will succeed. If PyArray_Check ( obj) is TRUE then it is returned in *address without incrementing its reference count."]}, {"name": "int PyArray_Partition()", "path": "reference/c-api/array#c.PyArray_Partition", "type": "Array API", "text": ["Equivalent to ndarray.partition (self, ktharray, axis, kind). Partitions the array so that the values of the element indexed by ktharray are in the positions they would be if the array is fully sorted and places all elements smaller than the kth before and all elements equal or greater after the kth element. The ordering of all elements within the partitions is undefined. If self->descr is a data-type with fields defined, then self->descr->names is used to determine the sort order. A comparison where the first field is equal will use the second field and so on. To alter the sort order of a structured array, create a new data-type with a different order of names and construct a view of the array with that new data-type. Returns zero on success and -1 on failure."]}, {"name": "int PyArray_RegisterCanCast()", "path": "reference/c-api/array#c.PyArray_RegisterCanCast", "type": "Array API", "text": ["Register the data-type number, totype, as castable from data-type object, descr, of the given scalar kind. Use scalar = NPY_NOSCALAR to register that an array of data-type descr can be cast safely to a data-type whose type_number is totype. The return value is 0 on success or -1 on failure."]}, {"name": "int PyArray_RegisterCastFunc()", "path": "reference/c-api/array#c.PyArray_RegisterCastFunc", "type": "Array API", "text": ["Register a low-level casting function, castfunc, to convert from the data-type, descr, to the given data-type number, totype. Any old casting function is over-written. A 0 is returned on success or a -1 on failure."]}, {"name": "int PyArray_RegisterDataType()", "path": "reference/c-api/array#c.PyArray_RegisterDataType", "type": "Array API", "text": ["Register a data-type as a new user-defined data type for arrays. The type must have most of its entries filled in. This is not always checked and errors can produce segfaults. In particular, the typeobj member of the dtype structure must be filled with a Python type that has a fixed-size element-size that corresponds to the elsize member of dtype. Also the f member must have the required functions: nonzero, copyswap, copyswapn, getitem, setitem, and cast (some of the cast functions may be NULL if no support is desired). To avoid confusion, you should choose a unique character typecode but this is not enforced and not relied on internally.", "A user-defined type number is returned that uniquely identifies the type. A pointer to the new structure can then be obtained from PyArray_DescrFromType using the returned type number. A -1 is returned if an error occurs. If this dtype has already been registered (checked only by the address of the pointer), then return the previously-assigned type-number."]}, {"name": "int PyArray_RemoveSmallest()", "path": "reference/c-api/array#c.PyArray_RemoveSmallest", "type": "Array API", "text": ["This function takes a multi-iterator object that has been previously \u201cbroadcasted,\u201d finds the dimension with the smallest \u201csum of strides\u201d in the broadcasted result and adapts all the iterators so as not to iterate over that dimension (by effectively making them of length-1 in that dimension). The corresponding dimension is returned unless mit ->nd is 0, then -1 is returned. This function is useful for constructing ufunc-like routines that broadcast their inputs correctly and then call a strided 1-d version of the routine as the inner-loop. This 1-d version is usually optimized for speed and for this reason the loop should be performed over the axis that won\u2019t require large stride jumps."]}, {"name": "int PyArray_ResolveWritebackIfCopy()", "path": "reference/c-api/array#c.PyArray_ResolveWritebackIfCopy", "type": "Array API", "text": ["If obj.flags has NPY_ARRAY_WRITEBACKIFCOPY or (deprecated) NPY_ARRAY_UPDATEIFCOPY, this function clears the flags, DECREF s obj->base and makes it writeable, and sets obj->base to NULL. It then copies obj->data to obj->base->data, and returns the error state of the copy operation. This is the opposite of PyArray_SetWritebackIfCopyBase. Usually this is called once you are finished with obj, just before Py_DECREF(obj). It may be called multiple times, or with NULL input. See also PyArray_DiscardWritebackIfCopy.", "Returns 0 if nothing was done, -1 on error, and 1 if action was taken."]}, {"name": "int PyArray_SearchsideConverter()", "path": "reference/c-api/array#c.PyArray_SearchsideConverter", "type": "Array API", "text": ["Convert Python strings into one of NPY_SEARCHLEFT (starts with \u2018l\u2019 or \u2018L\u2019), or NPY_SEARCHRIGHT (starts with \u2018r\u2019 or \u2018R\u2019)."]}, {"name": "int PyArray_SetBaseObject()", "path": "reference/c-api/array#c.PyArray_SetBaseObject", "type": "Array API", "text": ["New in version 1.7.", "This function steals a reference to obj and sets it as the base property of arr.", "If you construct an array by passing in your own memory buffer as a parameter, you need to set the array\u2019s base property to ensure the lifetime of the memory buffer is appropriate.", "The return value is 0 on success, -1 on failure.", "If the object provided is an array, this function traverses the chain of base pointers so that each array points to the owner of the memory directly. Once the base is set, it may not be changed to another value."]}, {"name": "int PyArray_SetField()", "path": "reference/c-api/array#c.PyArray_SetField", "type": "Array API", "text": ["Equivalent to ndarray.setfield (self, val, dtype, offset ). Set the field starting at offset in bytes and of the given dtype to val. The offset plus dtype ->elsize must be less than self ->descr->elsize or an error is raised. Otherwise, the val argument is converted to an array and copied into the field pointed to. If necessary, the elements of val are repeated to fill the destination array, But, the number of elements in the destination must be an integer multiple of the number of elements in val."]}, {"name": "int PyArray_SETITEM()", "path": "reference/c-api/array#c.PyArray_SETITEM", "type": "Array API", "text": ["Convert obj and place it in the ndarray, arr, at the place pointed to by itemptr. Return -1 if an error occurs or 0 on success."]}, {"name": "int PyArray_SetUpdateIfCopyBase()", "path": "reference/c-api/array#c.PyArray_SetUpdateIfCopyBase", "type": "Array API", "text": ["Precondition: arr is a copy of base (though possibly with different strides, ordering, etc.) Set the UPDATEIFCOPY flag and arr->base so that when arr is destructed, it will copy any changes back to base. DEPRECATED, use PyArray_SetWritebackIfCopyBase.", "Returns 0 for success, -1 for failure."]}, {"name": "int PyArray_SetWritebackIfCopyBase()", "path": "reference/c-api/array#c.PyArray_SetWritebackIfCopyBase", "type": "Array API", "text": ["Precondition: arr is a copy of base (though possibly with different strides, ordering, etc.) Sets the NPY_ARRAY_WRITEBACKIFCOPY flag and arr->base, and set base to READONLY. Call PyArray_ResolveWritebackIfCopy before calling Py_DECREF in order copy any changes back to base and reset the READONLY flag.", "Returns 0 for success, -1 for failure."]}, {"name": "int PyArray_SortkindConverter()", "path": "reference/c-api/array#c.PyArray_SortkindConverter", "type": "Array API", "text": ["Convert Python strings into one of NPY_QUICKSORT (starts with \u2018q\u2019 or \u2018Q\u2019), NPY_HEAPSORT (starts with \u2018h\u2019 or \u2018H\u2019), NPY_MERGESORT (starts with \u2018m\u2019 or \u2018M\u2019) or NPY_STABLESORT (starts with \u2018t\u2019 or \u2018T\u2019). NPY_MERGESORT and NPY_STABLESORT are aliased to each other for backwards compatibility and may refer to one of several stable sorting algorithms depending on the data type."]}, {"name": "int PyArray_TYPE()", "path": "reference/c-api/array#c.PyArray_TYPE", "type": "Array API", "text": ["Return the (builtin) typenumber for the elements of this array."]}, {"name": "int PyArray_TypeNumFromName()", "path": "reference/c-api/array#c.PyArray_TypeNumFromName", "type": "Array API", "text": ["Given a string return the type-number for the data-type with that string as the type-object name. Returns NPY_NOTYPE without setting an error if no type can be found. Only works for user-defined data-types."]}, {"name": "int PyArray_TypestrConvert()", "path": "reference/c-api/array#c.PyArray_TypestrConvert", "type": "Array API", "text": ["Convert typestring characters (with itemsize) to basic enumerated data types. The typestring character corresponding to signed and unsigned integers, floating point numbers, and complex-floating point numbers are recognized and converted. Other values of gentype are returned. This function can be used to convert, for example, the string \u2018f4\u2019 to NPY_FLOAT32."]}, {"name": "int PyArray_ValidType()", "path": "reference/c-api/array#c.PyArray_ValidType", "type": "Array API", "text": ["Returns NPY_TRUE if typenum represents a valid type-number (builtin or user-defined or character code). Otherwise, this function returns NPY_FALSE."]}, {"name": "int PyArray_XDECREF()", "path": "reference/c-api/array#c.PyArray_XDECREF", "type": "Array API", "text": ["Used for an array, op, that contains any Python objects. It decrements the reference count of every object in the array according to the data-type of op. Normal return value is 0. A -1 is returned if an error occurs."]}, {"name": "int PyArrayIter_Check()", "path": "reference/c-api/array#c.PyArrayIter_Check", "type": "Array API", "text": ["Evaluates true if op is an array iterator (or instance of a subclass of the array iterator type)."]}, {"name": "int PyArrayNeighborhoodIter_Next()", "path": "reference/c-api/array#c.PyArrayNeighborhoodIter_Next", "type": "Array API", "text": ["After this call, iter->dataptr points to the next point of the neighborhood. Calling this function after every point of the neighborhood has been visited is undefined."]}, {"name": "int PyArrayNeighborhoodIter_Reset()", "path": "reference/c-api/array#c.PyArrayNeighborhoodIter_Reset", "type": "Array API", "text": ["Reset the iterator position to the first point of the neighborhood. This should be called whenever the iter argument given at PyArray_NeighborhoodIterObject is changed (see example)"]}, {"name": "int PyDataType_FLAGCHK()", "path": "reference/c-api/types-and-structures#c.NPY_USE_SETITEM.PyDataType_FLAGCHK", "type": "Python Types and C-Structures", "text": ["Return true if all the given flags are set for the data-type object."]}, {"name": "int PyDataType_HASFIELDS()", "path": "reference/c-api/array#c.PyDataType_HASFIELDS", "type": "Array API", "text": []}, {"name": "int PyDataType_ISBOOL()", "path": "reference/c-api/array#c.PyDataType_ISBOOL", "type": "Array API", "text": []}, {"name": "int PyDataType_ISCOMPLEX()", "path": "reference/c-api/array#c.PyDataType_ISCOMPLEX", "type": "Array API", "text": []}, {"name": "int PyDataType_ISEXTENDED()", "path": "reference/c-api/array#c.PyDataType_ISEXTENDED", "type": "Array API", "text": []}, {"name": "int PyDataType_ISFLEXIBLE()", "path": "reference/c-api/array#c.PyDataType_ISFLEXIBLE", "type": "Array API", "text": []}, {"name": "int PyDataType_ISFLOAT()", "path": "reference/c-api/array#c.PyDataType_ISFLOAT", "type": "Array API", "text": []}, {"name": "int PyDataType_ISINTEGER()", "path": "reference/c-api/array#c.PyDataType_ISINTEGER", "type": "Array API", "text": []}, {"name": "int PyDataType_ISNUMBER()", "path": "reference/c-api/array#c.PyDataType_ISNUMBER", "type": "Array API", "text": []}, {"name": "int PyDataType_ISOBJECT()", "path": "reference/c-api/array#c.PyDataType_ISOBJECT", "type": "Array API", "text": []}, {"name": "int PyDataType_ISPYTHON()", "path": "reference/c-api/array#c.PyDataType_ISPYTHON", "type": "Array API", "text": []}, {"name": "int PyDataType_ISSIGNED()", "path": "reference/c-api/array#c.PyDataType_ISSIGNED", "type": "Array API", "text": []}, {"name": "int PyDataType_ISSTRING()", "path": "reference/c-api/array#c.PyDataType_ISSTRING", "type": "Array API", "text": []}, {"name": "int PyDataType_ISUNSIGNED()", "path": "reference/c-api/array#c.PyDataType_ISUNSIGNED", "type": "Array API", "text": []}, {"name": "int PyDataType_ISUNSIZED()", "path": "reference/c-api/array#c.PyDataType_ISUNSIZED", "type": "Array API", "text": ["Type has no size information attached, and can be resized. Should only be called on flexible dtypes. Types that are attached to an array will always be sized, hence the array form of this macro not existing.", "Changed in version 1.18.", "For structured datatypes with no fields this function now returns False."]}, {"name": "int PyDataType_ISUSERDEF()", "path": "reference/c-api/array#c.PyDataType_ISUSERDEF", "type": "Array API", "text": []}, {"name": "int PyDataType_REFCHK()", "path": "reference/c-api/types-and-structures#c.NPY_USE_SETITEM.PyDataType_REFCHK", "type": "Python Types and C-Structures", "text": ["Equivalent to PyDataType_FLAGCHK (dtype, NPY_ITEM_REFCOUNT)."]}, {"name": "int PyModule_AddIntConstant()", "path": "user/c-info.how-to-extend#c.PyModule_AddIntConstant", "type": "User Guide", "text": []}, {"name": "int PyModule_AddObject()", "path": "user/c-info.how-to-extend", "type": "User Guide", "text": ["While the ndarray object is designed to allow rapid computation in Python, it is also designed to be general-purpose and satisfy a wide- variety of computational needs. As a result, if absolute speed is essential, there is no replacement for a well-crafted, compiled loop specific to your application and hardware. This is one of the reasons that numpy includes f2py so that an easy-to-use mechanisms for linking (simple) C/C++ and (arbitrary) Fortran code directly into Python are available. You are encouraged to use and improve this mechanism. The purpose of this section is not to document this tool but to document the more basic steps to writing an extension module that this tool depends on.", "When an extension module is written, compiled, and installed to somewhere in the Python path (sys.path), the code can then be imported into Python as if it were a standard python file. It will contain objects and methods that have been defined and compiled in C code. The basic steps for doing this in Python are well-documented and you can find more information in the documentation for Python itself available online at www.python.org .", "In addition to the Python C-API, there is a full and rich C-API for NumPy allowing sophisticated manipulations on a C-level. However, for most applications, only a few API calls will typically be used. For example, if you need to just extract a pointer to memory along with some shape information to pass to another calculation routine, then you will use very different calls than if you are trying to create a new array-like type or add a new data type for ndarrays. This chapter documents the API calls and macros that are most commonly used.", "There is exactly one function that must be defined in your C-code in order for Python to use it as an extension module. The function must be called init{name} where {name} is the name of the module from Python. This function must be declared so that it is visible to code outside of the routine. Besides adding the methods and constants you desire, this subroutine must also contain calls like import_array() and/or import_ufunc() depending on which C-API is needed. Forgetting to place these commands will show itself as an ugly segmentation fault (crash) as soon as any C-API subroutine is actually called. It is actually possible to have multiple init{name} functions in a single file in which case multiple modules will be defined by that file. However, there are some tricks to get that to work correctly and it is not covered here.", "A minimal init{name} method looks like:", "The mymethods must be an array (usually statically declared) of PyMethodDef structures which contain method names, actual C-functions, a variable indicating whether the method uses keyword arguments or not, and docstrings. These are explained in the next section. If you want to add constants to the module, then you store the returned value from Py_InitModule which is a module object. The most general way to add items to the module is to get the module dictionary using PyModule_GetDict(module). With the module dictionary, you can add whatever you like to the module manually. An easier way to add objects to the module is to use one of three additional Python C-API calls that do not require a separate extraction of the module dictionary. These are documented in the Python documentation, but repeated here for convenience:", "All three of these functions require the module object (the return value of Py_InitModule). The name is a string that labels the value in the module. Depending on which function is called, the value argument is either a general object (PyModule_AddObject steals a reference to it), an integer constant, or a string constant.", "The second argument passed in to the Py_InitModule function is a structure that makes it easy to to define functions in the module. In the example given above, the mymethods structure would have been defined earlier in the file (usually right before the init{name} subroutine) to:", "Each entry in the mymethods array is a PyMethodDef structure containing 1) the Python name, 2) the C-function that implements the function, 3) flags indicating whether or not keywords are accepted for this function, and 4) The docstring for the function. Any number of functions may be defined for a single module by adding more entries to this table. The last entry must be all NULL as shown to act as a sentinel. Python looks for this entry to know that all of the functions for the module have been defined.", "The last thing that must be done to finish the extension module is to actually write the code that performs the desired functions. There are two kinds of functions: those that don\u2019t accept keyword arguments, and those that do.", "Functions that don\u2019t accept keyword arguments should be written as:", "The dummy argument is not used in this context and can be safely ignored. The args argument contains all of the arguments passed in to the function as a tuple. You can do anything you want at this point, but usually the easiest way to manage the input arguments is to call PyArg_ParseTuple (args, format_string, addresses_to_C_variables\u2026) or PyArg_UnpackTuple (tuple, \u201cname\u201d, min, max, \u2026). A good description of how to use the first function is contained in the Python C-API reference manual under section 5.5 (Parsing arguments and building values). You should pay particular attention to the \u201cO&\u201d format which uses converter functions to go between the Python object and the C object. All of the other format functions can be (mostly) thought of as special cases of this general rule. There are several converter functions defined in the NumPy C-API that may be of use. In particular, the PyArray_DescrConverter function is very useful to support arbitrary data-type specification. This function transforms any valid data-type Python object into a PyArray_Descr* object. Remember to pass in the address of the C-variables that should be filled in.", "There are lots of examples of how to use PyArg_ParseTuple throughout the NumPy source code. The standard usage is like this:", "It is important to keep in mind that you get a borrowed reference to the object when using the \u201cO\u201d format string. However, the converter functions usually require some form of memory handling. In this example, if the conversion is successful, dtype will hold a new reference to a PyArray_Descr* object, while input will hold a borrowed reference. Therefore, if this conversion were mixed with another conversion (say to an integer) and the data-type conversion was successful but the integer conversion failed, then you would need to release the reference count to the data-type object before returning. A typical way to do this is to set dtype to NULL before calling PyArg_ParseTuple and then use Py_XDECREF on dtype before returning.", "After the input arguments are processed, the code that actually does the work is written (likely calling other functions as needed). The final step of the C-function is to return something. If an error is encountered then NULL should be returned (making sure an error has actually been set). If nothing should be returned then increment Py_None and return it. If a single object should be returned then it is returned (ensuring that you own a reference to it first). If multiple objects should be returned then you need to return a tuple. The Py_BuildValue (format_string, c_variables\u2026) function makes it easy to build tuples of Python objects from C variables. Pay special attention to the difference between \u2018N\u2019 and \u2018O\u2019 in the format string or you can easily create memory leaks. The \u2018O\u2019 format string increments the reference count of the PyObject* C-variable it corresponds to, while the \u2018N\u2019 format string steals a reference to the corresponding PyObject* C-variable. You should use \u2018N\u2019 if you have already created a reference for the object and just want to give that reference to the tuple. You should use \u2018O\u2019 if you only have a borrowed reference to an object and need to create one to provide for the tuple.", "These functions are very similar to functions without keyword arguments. The only difference is that the function signature is:", "The kwds argument holds a Python dictionary whose keys are the names of the keyword arguments and whose values are the corresponding keyword-argument values. This dictionary can be processed however you see fit. The easiest way to handle it, however, is to replace the PyArg_ParseTuple (args, format_string, addresses\u2026) function with a call to PyArg_ParseTupleAndKeywords (args, kwds, format_string, char *kwlist[], addresses\u2026). The kwlist parameter to this function is a NULL -terminated array of strings providing the expected keyword arguments. There should be one string for each entry in the format_string. Using this function will raise a TypeError if invalid keyword arguments are passed in.", "For more help on this function please see section 1.8 (Keyword Parameters for Extension Functions) of the Extending and Embedding tutorial in the Python documentation.", "The biggest difficulty when writing extension modules is reference counting. It is an important reason for the popularity of f2py, weave, Cython, ctypes, etc\u2026. If you mis-handle reference counts you can get problems from memory-leaks to segmentation faults. The only strategy I know of to handle reference counts correctly is blood, sweat, and tears. First, you force it into your head that every Python variable has a reference count. Then, you understand exactly what each function does to the reference count of your objects, so that you can properly use DECREF and INCREF when you need them. Reference counting can really test the amount of patience and diligence you have towards your programming craft. Despite the grim depiction, most cases of reference counting are quite straightforward with the most common difficulty being not using DECREF on objects before exiting early from a routine due to some error. In second place, is the common error of not owning the reference on an object that is passed to a function or macro that is going to steal the reference ( e.g. PyTuple_SET_ITEM, and most functions that take PyArray_Descr objects).", "Typically you get a new reference to a variable when it is created or is the return value of some function (there are some prominent exceptions, however \u2014 such as getting an item out of a tuple or a dictionary). When you own the reference, you are responsible to make sure that Py_DECREF (var) is called when the variable is no longer necessary (and no other function has \u201cstolen\u201d its reference). Also, if you are passing a Python object to a function that will \u201csteal\u201d the reference, then you need to make sure you own it (or use Py_INCREF to get your own reference). You will also encounter the notion of borrowing a reference. A function that borrows a reference does not alter the reference count of the object and does not expect to \u201chold on \u201cto the reference. It\u2019s just going to use the object temporarily. When you use PyArg_ParseTuple or PyArg_UnpackTuple you receive a borrowed reference to the objects in the tuple and should not alter their reference count inside your function. With practice, you can learn to get reference counting right, but it can be frustrating at first.", "One common source of reference-count errors is the Py_BuildValue function. Pay careful attention to the difference between the \u2018N\u2019 format character and the \u2018O\u2019 format character. If you create a new object in your subroutine (such as an output array), and you are passing it back in a tuple of return values, then you should most- likely use the \u2018N\u2019 format character in Py_BuildValue. The \u2018O\u2019 character will increase the reference count by one. This will leave the caller with two reference counts for a brand-new array. When the variable is deleted and the reference count decremented by one, there will still be that extra reference count, and the array will never be deallocated. You will have a reference-counting induced memory leak. Using the \u2018N\u2019 character will avoid this situation as it will return to the caller an object (inside the tuple) with a single reference count.", "Most extension modules for NumPy will need to access the memory for an ndarray object (or one of it\u2019s sub-classes). The easiest way to do this doesn\u2019t require you to know much about the internals of NumPy. The method is to", "Ensure you are dealing with a well-behaved array (aligned, in machine byte-order and single-segment) of the correct type and number of dimensions.", "Each of these sub-topics is covered in the following sub-sections.", "The main routine for obtaining an array from any Python object that can be converted to an array is PyArray_FromAny. This function is very flexible with many input arguments. Several macros make it easier to use the basic function. PyArray_FROM_OTF is arguably the most useful of these macros for the most common uses. It allows you to convert an arbitrary Python object to an array of a specific builtin data-type ( e.g. float), while specifying a particular set of requirements ( e.g. contiguous, aligned, and writeable). The syntax is", "Return an ndarray from any Python object, obj, that can be converted to an array. The number of dimensions in the returned array is determined by the object. The desired data-type of the returned array is provided in typenum which should be one of the enumerated types. The requirements for the returned array can be any combination of standard array flags. Each of these arguments is explained in more detail below. You receive a new reference to the array on success. On failure, NULL is returned and an exception is set.", "The object can be any Python object convertible to an ndarray. If the object is already (a subclass of) the ndarray that satisfies the requirements then a new reference is returned. Otherwise, a new array is constructed. The contents of obj are copied to the new array unless the array interface is used so that data does not have to be copied. Objects that can be converted to an array include: 1) any nested sequence object, 2) any object exposing the array interface, 3) any object with an __array__ method (which should return an ndarray), and 4) any scalar object (becomes a zero-dimensional array). Sub-classes of the ndarray that otherwise fit the requirements will be passed through. If you want to ensure a base-class ndarray, then use NPY_ARRAY_ENSUREARRAY in the requirements flag. A copy is made only if necessary. If you want to guarantee a copy, then pass in NPY_ARRAY_ENSURECOPY to the requirements flag.", "One of the enumerated types or NPY_NOTYPE if the data-type should be determined from the object itself. The C-based names can be used:", "NPY_BOOL, NPY_BYTE, NPY_UBYTE, NPY_SHORT, NPY_USHORT, NPY_INT, NPY_UINT, NPY_LONG, NPY_ULONG, NPY_LONGLONG, NPY_ULONGLONG, NPY_DOUBLE, NPY_LONGDOUBLE, NPY_CFLOAT, NPY_CDOUBLE, NPY_CLONGDOUBLE, NPY_OBJECT.", "Alternatively, the bit-width names can be used as supported on the platform. For example:", "NPY_INT8, NPY_INT16, NPY_INT32, NPY_INT64, NPY_UINT8, NPY_UINT16, NPY_UINT32, NPY_UINT64, NPY_FLOAT32, NPY_FLOAT64, NPY_COMPLEX64, NPY_COMPLEX128.", "The object will be converted to the desired type only if it can be done without losing precision. Otherwise NULL will be returned and an error raised. Use NPY_ARRAY_FORCECAST in the requirements flag to override this behavior.", "The memory model for an ndarray admits arbitrary strides in each dimension to advance to the next element of the array. Often, however, you need to interface with code that expects a C-contiguous or a Fortran-contiguous memory layout. In addition, an ndarray can be misaligned (the address of an element is not at an integral multiple of the size of the element) which can cause your program to crash (or at least work more slowly) if you try and dereference a pointer into the array data. Both of these problems can be solved by converting the Python object into an array that is more \u201cwell-behaved\u201d for your specific usage.", "The requirements flag allows specification of what kind of array is acceptable. If the object passed in does not satisfy this requirements then a copy is made so that the returned object will satisfy the requirements. these ndarray can use a very generic pointer to memory. This flag allows specification of the desired properties of the returned array object. All of the flags are explained in the detailed API chapter. The flags most commonly needed are NPY_ARRAY_IN_ARRAY, NPY_OUT_ARRAY, and NPY_ARRAY_INOUT_ARRAY:", "This flag is useful for arrays that must be in C-contiguous order and aligned. These kinds of arrays are usually input arrays for some algorithm.", "This flag is useful to specify an array that is in C-contiguous order, is aligned, and can be written to as well. Such an array is usually returned as output (although normally such output arrays are created from scratch).", "This flag is useful to specify an array that will be used for both input and output. PyArray_ResolveWritebackIfCopy must be called before Py_DECREF at the end of the interface routine to write back the temporary data into the original array passed in. Use of the NPY_ARRAY_WRITEBACKIFCOPY or NPY_ARRAY_UPDATEIFCOPY flags requires that the input object is already an array (because other objects cannot be automatically updated in this fashion). If an error occurs use PyArray_DiscardWritebackIfCopy (obj) on an array with these flags set. This will set the underlying base array writable without causing the contents to be copied back into the original array.", "Other useful flags that can be OR\u2019d as additional requirements are:", "Cast to the desired type, even if it can\u2019t be done without losing information.", "Make sure the resulting array is a copy of the original.", "Make sure the resulting object is an actual ndarray and not a sub- class.", "Note", "Whether or not an array is byte-swapped is determined by the data-type of the array. Native byte-order arrays are always requested by PyArray_FROM_OTF and so there is no need for a NPY_ARRAY_NOTSWAPPED flag in the requirements argument. There is also no way to get a byte-swapped array from this routine.", "Quite often, new arrays must be created from within extension-module code. Perhaps an output array is needed and you don\u2019t want the caller to have to supply it. Perhaps only a temporary array is needed to hold an intermediate calculation. Whatever the need there are simple ways to get an ndarray object of whatever data-type is needed. The most general function for doing this is PyArray_NewFromDescr. All array creation functions go through this heavily re-used code. Because of its flexibility, it can be somewhat confusing to use. As a result, simpler forms exist that are easier to use. These forms are part of the PyArray_SimpleNew family of functions, which simplify the interface by providing default values for common use cases.", "If obj is an ndarray (PyArrayObject*), then the data-area of the ndarray is pointed to by the void* pointer PyArray_DATA (obj) or the char* pointer PyArray_BYTES (obj). Remember that (in general) this data-area may not be aligned according to the data-type, it may represent byte-swapped data, and/or it may not be writeable. If the data area is aligned and in native byte-order, then how to get at a specific element of the array is determined only by the array of npy_intp variables, PyArray_STRIDES (obj). In particular, this c-array of integers shows how many bytes must be added to the current element pointer to get to the next element in each dimension. For arrays less than 4-dimensions there are PyArray_GETPTR{k} (obj, \u2026) macros where {k} is the integer 1, 2, 3, or 4 that make using the array strides easier. The arguments \u2026. represent {k} non- negative integer indices into the array. For example, suppose E is a 3-dimensional ndarray. A (void*) pointer to the element E[i,j,k] is obtained as PyArray_GETPTR3 (E, i, j, k).", "As explained previously, C-style contiguous arrays and Fortran-style contiguous arrays have particular striding patterns. Two array flags (NPY_ARRAY_C_CONTIGUOUS and NPY_ARRAY_F_CONTIGUOUS) indicate whether or not the striding pattern of a particular array matches the C-style contiguous or Fortran-style contiguous or neither. Whether or not the striding pattern matches a standard C or Fortran one can be tested Using PyArray_IS_C_CONTIGUOUS (obj) and PyArray_ISFORTRAN (obj) respectively. Most third-party libraries expect contiguous arrays. But, often it is not difficult to support general-purpose striding. I encourage you to use the striding information in your own code whenever possible, and reserve single-segment requirements for wrapping third-party code. Using the striding information provided with the ndarray rather than requiring a contiguous striding reduces copying that otherwise must be made.", "The following example shows how you might write a wrapper that accepts two input arguments (that will be converted to an array) and an output argument (that must be an array). The function returns None and updates the output array. Note the updated use of WRITEBACKIFCOPY semantics for NumPy v1.14 and above"]}, {"name": "int PyModule_AddStringConstant()", "path": "user/c-info.how-to-extend#c.PyModule_AddStringConstant", "type": "User Guide", "text": ["All three of these functions require the module object (the return value of Py_InitModule). The name is a string that labels the value in the module. Depending on which function is called, the value argument is either a general object (PyModule_AddObject steals a reference to it), an integer constant, or a string constant."]}, {"name": "int PyTypeNum_ISBOOL()", "path": "reference/c-api/array#c.PyTypeNum_ISBOOL", "type": "Array API", "text": []}, {"name": "int PyTypeNum_ISCOMPLEX()", "path": "reference/c-api/array#c.PyTypeNum_ISCOMPLEX", "type": "Array API", "text": []}, {"name": "int PyTypeNum_ISEXTENDED()", "path": "reference/c-api/array#c.PyTypeNum_ISEXTENDED", "type": "Array API", "text": []}, {"name": "int PyTypeNum_ISFLEXIBLE()", "path": "reference/c-api/array#c.PyTypeNum_ISFLEXIBLE", "type": "Array API", "text": []}, {"name": "int PyTypeNum_ISFLOAT()", "path": "reference/c-api/array#c.PyTypeNum_ISFLOAT", "type": "Array API", "text": []}, {"name": "int PyTypeNum_ISINTEGER()", "path": "reference/c-api/array#c.PyTypeNum_ISINTEGER", "type": "Array API", "text": []}, {"name": "int PyTypeNum_ISNUMBER()", "path": "reference/c-api/array#c.PyTypeNum_ISNUMBER", "type": "Array API", "text": []}, {"name": "int PyTypeNum_ISOBJECT()", "path": "reference/c-api/array#c.PyTypeNum_ISOBJECT", "type": "Array API", "text": []}, {"name": "int PyTypeNum_ISPYTHON()", "path": "reference/c-api/array#c.PyTypeNum_ISPYTHON", "type": "Array API", "text": []}, {"name": "int PyTypeNum_ISSIGNED()", "path": "reference/c-api/array#c.PyTypeNum_ISSIGNED", "type": "Array API", "text": []}, {"name": "int PyTypeNum_ISSTRING()", "path": "reference/c-api/array#c.PyTypeNum_ISSTRING", "type": "Array API", "text": []}, {"name": "int PyTypeNum_ISUSERDEF()", "path": "reference/c-api/array#c.PyTypeNum_ISUSERDEF", "type": "Array API", "text": []}, {"name": "int PyUFunc_checkfperr()", "path": "reference/c-api/ufunc#c.PyUFunc_checkfperr", "type": "UFunc API", "text": ["A simple interface to the IEEE error-flag checking support. The errmask argument is a mask of UFUNC_MASK_{ERR} bitmasks indicating which errors to check for (and how to check for them). The errobj must be a Python tuple with two elements: a string containing the name which will be used in any communication of error and either a callable Python object (call-back function) or Py_None. The callable object will only be used if UFUNC_ERR_CALL is set as the desired error checking method. This routine manages the GIL and is safe to call even after releasing the GIL. If an error in the IEEE-compatible hardware is determined a -1 is returned, otherwise a 0 is returned."]}, {"name": "int PyUFunc_RegisterLoopForDescr()", "path": "reference/c-api/ufunc#c.PyUFunc_RegisterLoopForDescr", "type": "UFunc API", "text": ["This function behaves like PyUFunc_RegisterLoopForType above, except that it allows the user to register a 1-d loop using PyArray_Descr objects instead of dtype type num values. This allows a 1-d loop to be registered for structured array data-dtypes and custom data-types instead of scalar data-types."]}, {"name": "int PyUFunc_RegisterLoopForType()", "path": "reference/c-api/ufunc#c.PyUFunc_RegisterLoopForType", "type": "UFunc API", "text": ["This function allows the user to register a 1-d loop with an already- created ufunc to be used whenever the ufunc is called with any of its input arguments as the user-defined data-type. This is needed in order to make ufuncs work with built-in data-types. The data-type must have been previously registered with the numpy system. The loop is passed in as function. This loop can take arbitrary data which should be passed in as data. The data-types the loop requires are passed in as arg_types which must be a pointer to memory at least as large as ufunc->nargs."]}, {"name": "int PyUFunc_ReplaceLoopBySignature()", "path": "reference/c-api/ufunc#c.PyUFunc_ReplaceLoopBySignature", "type": "UFunc API", "text": ["Replace a 1-d loop matching the given signature in the already-created ufunc with the new 1-d loop newfunc. Return the old 1-d loop function in oldfunc. Return 0 on success and -1 on failure. This function works only with built-in types (use PyUFunc_RegisterLoopForType for user-defined types). A signature is an array of data-type numbers indicating the inputs followed by the outputs assumed by the 1-d loop."]}, {"name": "int random_multivariate_hypergeometric_count()", "path": "reference/random/c-api#c.random_multivariate_hypergeometric_count", "type": "C API for random", "text": []}, {"name": "int reserved1", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.reserved1", "type": "Python Types and C-Structures", "text": ["Unused."]}, {"name": "int scanfunc()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.scanfunc", "type": "Python Types and C-Structures", "text": ["A pointer to a function that scans (scanf style) one element of the corresponding type from the file descriptor fd into the array memory pointed to by ip. The array is assumed to be behaved. The last argument arr is the array to be scanned into. Returns number of receiving arguments successfully assigned (which may be zero in case a matching failure occurred before the first receiving argument was assigned), or EOF if input failure occurs before the first receiving argument was assigned. This function should be called without holding the Python GIL, and has to grab it for error reporting."]}, {"name": "int setitem()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.setitem", "type": "Python Types and C-Structures", "text": ["A pointer to a function that sets the Python object item into the array, arr, at the position pointed to by data . This function deals with \u201cmisbehaved\u201d arrays. If successful, a zero is returned, otherwise, a negative one is returned (and a Python error set)."]}, {"name": "int sort()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.sort", "type": "Python Types and C-Structures", "text": ["An array of function pointers to a particular sorting algorithms. A particular sorting algorithm is obtained using a key (so far NPY_QUICKSORT, NPY_HEAPSORT, and NPY_MERGESORT are defined). These sorts are done in-place assuming contiguous and aligned data."]}, {"name": "Internal organization of NumPy arrays", "path": "dev/internals", "type": "Development", "text": ["It helps to understand a bit about how NumPy arrays are handled under the covers to help understand NumPy better. This section will not go into great detail. Those wishing to understand the full details are requested to refer to Travis Oliphant\u2019s book Guide to NumPy.", "NumPy arrays consist of two major components: the raw array data (from now on, referred to as the data buffer), and the information about the raw array data. The data buffer is typically what people think of as arrays in C or Fortran, a contiguous (and fixed) block of memory containing fixed-sized data items. NumPy also contains a significant set of data that describes how to interpret the data in the data buffer. This extra information contains (among other things):", "This arrangement allows for the very flexible use of arrays. One thing that it allows is simple changes to the metadata to change the interpretation of the array buffer. Changing the byteorder of the array is a simple change involving no rearrangement of the data. The shape of the array can be changed very easily without changing anything in the data buffer or any data copying at all.", "Among other things that are made possible is one can create a new array metadata object that uses the same data buffer to create a new view of that data buffer that has a different interpretation of the buffer (e.g., different shape, offset, byte order, strides, etc) but shares the same data bytes. Many operations in NumPy do just this such as slicing. Other operations, such as transpose, don\u2019t move data elements around in the array, but rather change the information about the shape and strides so that the indexing of the array changes, but the data in the doesn\u2019t move.", "Typically these new versions of the array metadata but the same data buffer are new views into the data buffer. There is a different ndarray object, but it uses the same data buffer. This is why it is necessary to force copies through the use of the copy method if one really wants to make a new and independent copy of the data buffer.", "New views into arrays mean the object reference counts for the data buffer increase. Simply doing away with the original array object will not remove the data buffer if other views of it still exist.", "See also", "Indexing on ndarrays", "What is the right way to index multi-dimensional arrays? Before you jump to conclusions about the one and true way to index multi-dimensional arrays, it pays to understand why this is a confusing issue. This section will try to explain in detail how NumPy indexing works and why we adopt the convention we do for images, and when it may be appropriate to adopt other conventions.", "The first thing to understand is that there are two conflicting conventions for indexing 2-dimensional arrays. Matrix notation uses the first index to indicate which row is being selected and the second index to indicate which column is selected. This is opposite the geometrically oriented-convention for images where people generally think the first index represents x position (i.e., column) and the second represents y position (i.e., row). This alone is the source of much confusion; matrix-oriented users and image-oriented users expect two different things with regard to indexing.", "The second issue to understand is how indices correspond to the order in which the array is stored in memory. In Fortran, the first index is the most rapidly varying index when moving through the elements of a two-dimensional array as it is stored in memory. If you adopt the matrix convention for indexing, then this means the matrix is stored one column at a time (since the first index moves to the next row as it changes). Thus Fortran is considered a Column-major language. C has just the opposite convention. In C, the last index changes most rapidly as one moves through the array as stored in memory. Thus C is a Row-major language. The matrix is stored by rows. Note that in both cases it presumes that the matrix convention for indexing is being used, i.e., for both Fortran and C, the first index is the row. Note this convention implies that the indexing convention is invariant and that the data order changes to keep that so.", "But that\u2019s not the only way to look at it. Suppose one has large two-dimensional arrays (images or matrices) stored in data files. Suppose the data are stored by rows rather than by columns. If we are to preserve our index convention (whether matrix or image) that means that depending on the language we use, we may be forced to reorder the data if it is read into memory to preserve our indexing convention. For example, if we read row-ordered data into memory without reordering, it will match the matrix indexing convention for C, but not for Fortran. Conversely, it will match the image indexing convention for Fortran, but not for C. For C, if one is using data stored in row order, and one wants to preserve the image index convention, the data must be reordered when reading into memory.", "In the end, what you do for Fortran or C depends on which is more important, not reordering data or preserving the indexing convention. For large images, reordering data is potentially expensive, and often the indexing convention is inverted to avoid that.", "The situation with NumPy makes this issue yet more complicated. The internal machinery of NumPy arrays is flexible enough to accept any ordering of indices. One can simply reorder indices by manipulating the internal stride information for arrays without reordering the data at all. NumPy will know how to map the new index order to the data without moving the data.", "So if this is true, why not choose the index order that matches what you most expect? In particular, why not define row-ordered images to use the image convention? (This is sometimes referred to as the Fortran convention vs the C convention, thus the \u2018C\u2019 and \u2018FORTRAN\u2019 order options for array ordering in NumPy.) The drawback of doing this is potential performance penalties. It\u2019s common to access the data sequentially, either implicitly in array operations or explicitly by looping over rows of an image. When that is done, then the data will be accessed in non-optimal order. As the first index is incremented, what is actually happening is that elements spaced far apart in memory are being sequentially accessed, with usually poor memory access speeds. For example, for a two-dimensional image im defined so that im[0, 10] represents the value at x = 0, y = 10. To be consistent with usual Python behavior then im[0] would represent a column at x = 0. Yet that data would be spread over the whole array since the data are stored in row order. Despite the flexibility of NumPy\u2019s indexing, it can\u2019t really paper over the fact basic operations are rendered inefficient because of data order or that getting contiguous subarrays is still awkward (e.g., im[:, 0] for the first row, vs im[0]). Thus one can\u2019t use an idiom such as for row in im; for col in im does work, but doesn\u2019t yield contiguous column data.", "As it turns out, NumPy is smart enough when dealing with ufuncs to determine which index is the most rapidly varying one in memory and uses that for the innermost loop. Thus for ufuncs, there is no large intrinsic advantage to either approach in most cases. On the other hand, use of ndarray.flat with a FORTRAN ordered array will lead to non-optimal memory access as adjacent elements in the flattened array (iterator, actually) are not contiguous in memory.", "Indeed, the fact is that Python indexing on lists and other sequences naturally leads to an outside-to-inside ordering (the first index gets the largest grouping, the next largest, and the last gets the smallest element). Since image data are normally stored in rows, this corresponds to the position within rows being the last item indexed.", "If you do want to use Fortran ordering realize that there are two approaches to consider: 1) accept that the first index is just not the most rapidly changing in memory and have all your I/O routines reorder your data when going from memory to disk or visa versa, or use NumPy\u2019s mechanism for mapping the first index to the most rapidly varying data. We recommend the former if possible. The disadvantage of the latter is that many of NumPy\u2019s functions will yield arrays without Fortran ordering unless you are careful to use the order keyword. Doing this would be highly inconvenient.", "Otherwise, we recommend simply learning to reverse the usual order of indices when accessing elements of an array. Granted, it goes against the grain, but it is more in line with Python semantics and the natural order of the data."]}, {"name": "Is the intended behavior clear under all conditions? Some things to watch:", "path": "dev/reviewer_guidelines", "type": "Development", "text": ["Reviewing open pull requests (PRs) helps move the project forward. We encourage people outside the project to get involved as well; it\u2019s a great way to get familiar with the codebase.", "Reviews can come from outside the NumPy team \u2013 we welcome contributions from domain experts (for instance, linalg or fft) or maintainers of other projects. You do not need to be a NumPy maintainer (a NumPy team member with permission to merge a PR) to review.", "If we do not know you yet, consider introducing yourself in the mailing list or Slack before you start reviewing pull requests.", "When reviewing pull requests, please use workflow tracking features on GitHub as appropriate:", "It may be helpful to have a copy of the pull request code checked out on your own machine so that you can play with it locally. You can use the GitHub CLI to do this by clicking the Open with button in the upper right-hand corner of the PR page.", "Assuming you have your development environment set up, you can now build the code and test it.", "It may be helpful to store some of these in GitHub\u2019s saved replies for reviewing:"]}, {"name": "is_array()", "path": "reference/swig.interface-file", "type": "numpy.i: a SWIG Interface File for NumPy", "text": ["The Simple Wrapper and Interface Generator (or SWIG) is a powerful tool for generating wrapper code for interfacing to a wide variety of scripting languages. SWIG can parse header files, and using only the code prototypes, create an interface to the target language. But SWIG is not omnipotent. For example, it cannot know from the prototype:", "what exactly seq is. Is it a single value to be altered in-place? Is it an array, and if so what is its length? Is it input-only? Output-only? Input-output? SWIG cannot determine these details, and does not attempt to do so.", "If we designed rms, we probably made it a routine that takes an input-only array of length n of double values called seq and returns the root mean square. The default behavior of SWIG, however, will be to create a wrapper function that compiles, but is nearly impossible to use from the scripting language in the way the C routine was intended.", "For Python, the preferred way of handling contiguous (or technically, strided) blocks of homogeneous data is with NumPy, which provides full object-oriented access to multidimensial arrays of data. Therefore, the most logical Python interface for the rms function would be (including doc string):", "where seq would be a NumPy array of double values, and its length n would be extracted from seq internally before being passed to the C routine. Even better, since NumPy supports construction of arrays from arbitrary Python sequences, seq itself could be a nearly arbitrary sequence (so long as each element can be converted to a double) and the wrapper code would internally convert it to a NumPy array before extracting its data and length.", "SWIG allows these types of conversions to be defined via a mechanism called typemaps. This document provides information on how to use numpy.i, a SWIG interface file that defines a series of typemaps intended to make the type of array-related conversions described above relatively simple to implement. For example, suppose that the rms function prototype defined above was in a header file named rms.h. To obtain the Python interface discussed above, your SWIG interface file would need the following:", "Typemaps are keyed off a list of one or more function arguments, either by type or by type and name. We will refer to such lists as signatures. One of the many typemaps defined by numpy.i is used above and has the signature (double* IN_ARRAY1, int DIM1). The argument names are intended to suggest that the double* argument is an input array of one dimension and that the int represents the size of that dimension. This is precisely the pattern in the rms prototype.", "Most likely, no actual prototypes to be wrapped will have the argument names IN_ARRAY1 and DIM1. We use the SWIG %apply directive to apply the typemap for one-dimensional input arrays of type double to the actual prototype used by rms. Using numpy.i effectively, therefore, requires knowing what typemaps are available and what they do.", "A SWIG interface file that includes the SWIG directives given above will produce wrapper code that looks something like:", "The typemaps from numpy.i are responsible for the following lines of code: 12\u201320, 25 and 30. Line 10 parses the input to the rms function. From the format string \"O:rms\", we can see that the argument list is expected to be a single Python object (specified by the O before the colon) and whose pointer is stored in obj0. A number of functions, supplied by numpy.i, are called to make and check the (possible) conversion from a generic Python object to a NumPy array. These functions are explained in the section Helper Functions, but hopefully their names are self-explanatory. At line 12 we use obj0 to construct a NumPy array. At line 17, we check the validity of the result: that it is non-null and that it has a single dimension of arbitrary length. Once these states are verified, we extract the data buffer and length in lines 19 and 20 so that we can call the underlying C function at line 22. Line 25 performs memory management for the case where we have created a new array that is no longer needed.", "This code has a significant amount of error handling. Note the SWIG_fail is a macro for goto fail, referring to the label at line 28. If the user provides the wrong number of arguments, this will be caught at line 10. If construction of the NumPy array fails or produces an array with the wrong number of dimensions, these errors are caught at line 17. And finally, if an error is detected, memory is still managed correctly at line 30.", "Note that if the C function signature was in a different order:", "that SWIG would not match the typemap signature given above with the argument list for rms. Fortunately, numpy.i has a set of typemaps with the data pointer given last:", "This simply has the effect of switching the definitions of arg1 and arg2 in lines 3 and 4 of the generated code above, and their assignments in lines 19 and 20.", "The numpy.i file is currently located in the tools/swig sub-directory under the numpy installation directory. Typically, you will want to copy it to the directory where you are developing your wrappers.", "A simple module that only uses a single SWIG interface file should include the following:", "Within a compiled Python module, import_array() should only get called once. This could be in a C/C++ file that you have written and is linked to the module. If this is the case, then none of your interface files should #define SWIG_FILE_WITH_INIT or call import_array(). Or, this initialization call could be in a wrapper file generated by SWIG from an interface file that has the %init block as above. If this is the case, and you have more than one SWIG interface file, then only one interface file should #define SWIG_FILE_WITH_INIT and call import_array().", "The typemap directives provided by numpy.i for arrays of different data types, say double and int, and dimensions of different types, say int or long, are identical to one another except for the C and NumPy type specifications. The typemaps are therefore implemented (typically behind the scenes) via a macro:", "that can be invoked for appropriate (DATA_TYPE, DATA_TYPECODE,\nDIM_TYPE) triplets. For example:", "The numpy.i interface file uses the %numpy_typemaps macro to implement typemaps for the following C data types and int dimension types:", "In the following descriptions, we reference a generic DATA_TYPE, which could be any of the C data types listed above, and DIM_TYPE which should be one of the many types of integers.", "The typemap signatures are largely differentiated on the name given to the buffer pointer. Names with FARRAY are for Fortran-ordered arrays, and names with ARRAY are for C-ordered (or 1D arrays).", "Input arrays are defined as arrays of data that are passed into a routine but are not altered in-place or returned to the user. The Python input array is therefore allowed to be almost any Python sequence (such as a list) that can be converted to the requested type of array. The input array signatures are", "1D:", "2D:", "3D:", "4D:", "The first signature listed, ( DATA_TYPE IN_ARRAY[ANY] ) is for one-dimensional arrays with hard-coded dimensions. Likewise, ( DATA_TYPE IN_ARRAY2[ANY][ANY] ) is for two-dimensional arrays with hard-coded dimensions, and similarly for three-dimensional.", "In-place arrays are defined as arrays that are modified in-place. The input values may or may not be used, but the values at the time the function returns are significant. The provided Python argument must therefore be a NumPy array of the required type. The in-place signatures are", "1D:", "2D:", "3D:", "4D:", "These typemaps now check to make sure that the INPLACE_ARRAY arguments use native byte ordering. If not, an exception is raised.", "There is also a \u201cflat\u201d in-place array for situations in which you would like to modify or process each element, regardless of the number of dimensions. One example is a \u201cquantization\u201d function that quantizes each element of an array in-place, be it 1D, 2D or whatever. This form checks for continuity but allows either C or Fortran ordering.", "ND:", "Argout arrays are arrays that appear in the input arguments in C, but are in fact output arrays. This pattern occurs often when there is more than one output variable and the single return argument is therefore not sufficient. In Python, the conventional way to return multiple arguments is to pack them into a sequence (tuple, list, etc.) and return the sequence. This is what the argout typemaps do. If a wrapped function that uses these argout typemaps has more than one return argument, they are packed into a tuple or list, depending on the version of Python. The Python user does not pass these arrays in, they simply get returned. For the case where a dimension is specified, the python user must provide that dimension as an argument. The argout signatures are", "1D:", "2D:", "3D:", "4D:", "These are typically used in situations where in C/C++, you would allocate a(n) array(s) on the heap, and call the function to fill the array(s) values. In Python, the arrays are allocated for you and returned as new array objects.", "Note that we support DATA_TYPE* argout typemaps in 1D, but not 2D or 3D. This is because of a quirk with the SWIG typemap syntax and cannot be avoided. Note that for these types of 1D typemaps, the Python function will take a single argument representing DIM1.", "Argoutview arrays are for when your C code provides you with a view of its internal data and does not require any memory to be allocated by the user. This can be dangerous. There is almost no way to guarantee that the internal data from the C code will remain in existence for the entire lifetime of the NumPy array that encapsulates it. If the user destroys the object that provides the view of the data before destroying the NumPy array, then using that array may result in bad memory references or segmentation faults. Nevertheless, there are situations, working with large data sets, where you simply have no other choice.", "The C code to be wrapped for argoutview arrays are characterized by pointers: pointers to the dimensions and double pointers to the data, so that these values can be passed back to the user. The argoutview typemap signatures are therefore", "1D:", "2D:", "3D:", "4D:", "Note that arrays with hard-coded dimensions are not supported. These cannot follow the double pointer signatures of these typemaps.", "A recent addition to numpy.i are typemaps that permit argout arrays with views into memory that is managed. See the discussion here.", "1D:", "2D:", "3D:", "4D:", "The numpy.i interface file does not support typemaps for output arrays, for several reasons. First, C/C++ return arguments are limited to a single value. This prevents obtaining dimension information in a general way. Second, arrays with hard-coded lengths are not permitted as return arguments. In other words:", "is not legal C/C++ syntax. Therefore, we cannot provide typemaps of the form:", "If you run into a situation where a function or method is returning a pointer to an array, your best bet is to write your own version of the function to be wrapped, either with %extend for the case of class methods or %ignore and %rename for the case of functions.", "Note that C++ type bool is not supported in the list in the Available Typemaps section. NumPy bools are a single byte, while the C++ bool is four bytes (at least on my system). Therefore:", "will result in typemaps that will produce code that reference improper data lengths. You can implement the following macro expansion:", "to fix the data length problem, and Input Arrays will work fine, but In-Place Arrays might fail type-checking.", "Typemap conversions for complex floating-point types is also not supported automatically. This is because Python and NumPy are written in C, which does not have native complex types. Both Python and NumPy implement their own (essentially equivalent) struct definitions for complex variables:", "We could have implemented:", "which would have provided automatic type conversions for arrays of type Py_complex, npy_cfloat and npy_cdouble. However, it seemed unlikely that there would be any independent (non-Python, non-NumPy) application code that people would be using SWIG to generate a Python interface to, that also used these definitions for complex types. More likely, these application codes will define their own complex types, or in the case of C++, use std::complex. Assuming these data structures are compatible with Python and NumPy complex types, %numpy_typemap expansions as above (with the user\u2019s complex type substituted for the first argument) should work.", "SWIG has sophisticated type checking for numerical types. For example, if your C/C++ routine expects an integer as input, the code generated by SWIG will check for both Python integers and Python long integers, and raise an overflow error if the provided Python integer is too big to cast down to a C integer. With the introduction of NumPy scalar arrays into your Python code, you might conceivably extract an integer from a NumPy array and attempt to pass this to a SWIG-wrapped C/C++ function that expects an int, but the SWIG type checking will not recognize the NumPy array scalar as an integer. (Often, this does in fact work \u2013 it depends on whether NumPy recognizes the integer type you are using as inheriting from the Python integer type on the platform you are using. Sometimes, this means that code that works on a 32-bit machine will fail on a 64-bit machine.)", "If you get a Python error that looks like the following:", "and the argument you are passing is an integer extracted from a NumPy array, then you have stumbled upon this problem. The solution is to modify the SWIG type conversion system to accept NumPy array scalars in addition to the standard integer types. Fortunately, this capability has been provided for you. Simply copy the file:", "to the working build directory for you project, and this problem will be fixed. It is suggested that you do this anyway, as it only increases the capabilities of your Python interface.", "The SWIG type checking and conversion system is a complicated combination of C macros, SWIG macros, SWIG typemaps and SWIG fragments. Fragments are a way to conditionally insert code into your wrapper file if it is needed, and not insert it if not needed. If multiple typemaps require the same fragment, the fragment only gets inserted into your wrapper code once.", "There is a fragment for converting a Python integer to a C long. There is a different fragment that converts a Python integer to a C int, that calls the routine defined in the long fragment. We can make the changes we want here by changing the definition for the long fragment. SWIG determines the active definition for a fragment using a \u201cfirst come, first served\u201d system. That is, we need to define the fragment for long conversions prior to SWIG doing it internally. SWIG allows us to do this by putting our fragment definitions in the file pyfragments.swg. If we were to put the new fragment definitions in numpy.i, they would be ignored.", "The numpy.i file contains several macros and routines that it uses internally to build its typemaps. However, these functions may be useful elsewhere in your interface file. These macros and routines are implemented as fragments, which are described briefly in the previous section. If you try to use one or more of the following macros or functions, but your compiler complains that it does not recognize the symbol, then you need to force these fragments to appear in your code using:", "in your SWIG interface file.", "Evaluates as true if a is non-NULL and can be cast to a PyArrayObject*.", "Evaluates to the integer data type code of a, assuming a can be cast to a PyArrayObject*.", "Evaluates to the integer number of dimensions of a, assuming a can be cast to a PyArrayObject*.", "Evaluates to an array of type npy_intp and length array_numdims(a), giving the lengths of all of the dimensions of a, assuming a can be cast to a PyArrayObject*.", "Evaluates to the i-th dimension size of a, assuming a can be cast to a PyArrayObject*.", "Evaluates to an array of type npy_intp and length array_numdims(a), giving the stridess of all of the dimensions of a, assuming a can be cast to a PyArrayObject*. A stride is the distance in bytes between an element and its immediate neighbor along the same axis.", "Evaluates to the i-th stride of a, assuming a can be cast to a PyArrayObject*.", "Evaluates to a pointer of type void* that points to the data buffer of a, assuming a can be cast to a PyArrayObject*.", "Returns a borrowed reference to the dtype property (PyArray_Descr*) of a, assuming a can be cast to a PyArrayObject*.", "Returns an integer representing the flags of a, assuming a can be cast to a PyArrayObject*.", "Sets the flag represented by f of a, assuming a can be cast to a PyArrayObject*.", "Evaluates as true if a is a contiguous array. Equivalent to (PyArray_ISCONTIGUOUS(a)).", "Evaluates as true if the data buffer of a uses native byte order. Equivalent to (PyArray_ISNOTSWAPPED(a)).", "Evaluates as true if a is FORTRAN ordered.", "pytype_string()", "Return type: const char*", "Arguments:", "Return a string describing the type of py_obj.", "typecode_string()", "Return type: const char*", "Arguments:", "Return a string describing the type corresponding to the NumPy typecode.", "type_match()", "Return type: int", "Arguments:", "Make sure that actual_type is compatible with desired_type. For example, this allows character and byte types, or int and long types, to match. This is now equivalent to PyArray_EquivTypenums().", "obj_to_array_no_conversion()", "Return type: PyArrayObject*", "Arguments:", "Cast input to a PyArrayObject* if legal, and ensure that it is of type typecode. If input cannot be cast, or the typecode is wrong, set a Python error and return NULL.", "obj_to_array_allow_conversion()", "Return type: PyArrayObject*", "Arguments:", "Convert input to a NumPy array with the given typecode. On success, return a valid PyArrayObject* with the correct type. On failure, the Python error string will be set and the routine returns NULL.", "make_contiguous()", "Return type: PyArrayObject*", "Arguments:", "Check to see if ary is contiguous. If so, return the input pointer and flag it as not a new object. If it is not contiguous, create a new PyArrayObject* using the original data, flag it as a new object and return the pointer.", "make_fortran()", "Return type: PyArrayObject*", "Arguments", "Check to see if ary is Fortran contiguous. If so, return the input pointer and flag it as not a new object. If it is not Fortran contiguous, create a new PyArrayObject* using the original data, flag it as a new object and return the pointer.", "obj_to_array_contiguous_allow_conversion()", "Return type: PyArrayObject*", "Arguments:", "Convert input to a contiguous PyArrayObject* of the specified type. If the input object is not a contiguous PyArrayObject*, a new one will be created and the new object flag will be set.", "obj_to_array_fortran_allow_conversion()", "Return type: PyArrayObject*", "Arguments:", "Convert input to a Fortran contiguous PyArrayObject* of the specified type. If the input object is not a Fortran contiguous PyArrayObject*, a new one will be created and the new object flag will be set.", "require_contiguous()", "Return type: int", "Arguments:", "Test whether ary is contiguous. If so, return 1. Otherwise, set a Python error and return 0.", "require_native()", "Return type: int", "Arguments:", "Require that ary is not byte-swapped. If the array is not byte-swapped, return 1. Otherwise, set a Python error and return 0.", "require_dimensions()", "Return type: int", "Arguments:", "Require ary to have a specified number of dimensions. If the array has the specified number of dimensions, return 1. Otherwise, set a Python error and return 0.", "require_dimensions_n()", "Return type: int", "Arguments:", "Require ary to have one of a list of specified number of dimensions. If the array has one of the specified number of dimensions, return 1. Otherwise, set the Python error string and return 0.", "require_size()", "Return type: int", "Arguments:", "Require ary to have a specified shape. If the array has the specified shape, return 1. Otherwise, set the Python error string and return 0.", "require_fortran()", "Return type: int", "Arguments:", "Require the given PyArrayObject to to be Fortran ordered. If the PyArrayObject is already Fortran ordered, do nothing. Else, set the Fortran ordering flag and recompute the strides.", "There are many C or C++ array/NumPy array situations not covered by a simple %include \"numpy.i\" and subsequent %apply directives.", "Consider a reasonable prototype for a dot product function:", "The Python interface that we want is:", "The problem here is that there is one dimension argument and two array arguments, and our typemaps are set up for dimensions that apply to a single array (in fact, SWIG does not provide a mechanism for associating len with vec2 that takes two Python input arguments). The recommended solution is the following:", "If the header file that contains the prototype for double dot() also contains other prototypes that you want to wrap, so that you need to %include this header file, then you will also need a %ignore\ndot; directive, placed after the %rename and before the %include directives. Or, if the function in question is a class method, you will want to use %extend rather than %inline in addition to %ignore.", "A note on error handling: Note that my_dot returns a double but that it can also raise a Python error. The resulting wrapper function will return a Python float representation of 0.0 when the vector lengths do not match. Since this is not NULL, the Python interpreter will not know to check for an error. For this reason, we add the %exception directive above for my_dot to get the behavior we want (note that $action is a macro that gets expanded to a valid call to my_dot). In general, you will probably want to write a SWIG macro to perform this task.", "There are other wrapping situations in which numpy.i may be helpful when you encounter them.", "In some situations, it is possible that you could use the %numpy_typemaps macro to implement typemaps for your own types. See the Other Common Types: bool or Other Common Types: complex sections for examples. Another situation is if your dimensions are of a type other than int (say long for example):", "When you use the %apply directive, as is usually necessary to use numpy.i, it will remain in effect until you tell SWIG that it shouldn\u2019t be. If the arguments to the functions or methods that you are wrapping have common names, such as length or vector, these typemaps may get applied in situations you do not expect or want. Therefore, it is always a good idea to add a %clear directive after you are done with a specific typemap:", "In general, you should target these typemap signatures specifically where you want them, and then clear them after you are done.", "Out of the box, numpy.i provides typemaps that support conversion between NumPy arrays and C arrays:", "That support 74 different argument signatures for each data type, including:", "The numpy.i interface file also provides additional tools for wrapper developers, including:"]}, {"name": "Iterating Over Arrays", "path": "reference/arrays.nditer", "type": "Iterating Over Arrays", "text": ["Note", "Arrays support the iterator protocol and can be iterated over like Python lists. See the Indexing, Slicing and Iterating section in the Quickstart guide for basic usage and examples. The remainder of this document presents the nditer object and covers more advanced usage.", "The iterator object nditer, introduced in NumPy 1.6, provides many flexible ways to visit all the elements of one or more arrays in a systematic fashion. This page introduces some basic ways to use the object for computations on arrays in Python, then concludes with how one can accelerate the inner loop in Cython. Since the Python exposure of nditer is a relatively straightforward mapping of the C array iterator API, these ideas will also provide help working with array iteration from C or C++.", "The most basic task that can be done with the nditer is to visit every element of an array. Each element is provided one by one using the standard Python iterator interface.", "An important thing to be aware of for this iteration is that the order is chosen to match the memory layout of the array instead of using a standard C or Fortran ordering. This is done for access efficiency, reflecting the idea that by default one simply wants to visit each element without concern for a particular ordering. We can see this by iterating over the transpose of our previous array, compared to taking a copy of that transpose in C order.", "The elements of both a and a.T get traversed in the same order, namely the order they are stored in memory, whereas the elements of a.T.copy(order=\u2019C\u2019) get visited in a different order because they have been put into a different memory layout.", "There are times when it is important to visit the elements of an array in a specific order, irrespective of the layout of the elements in memory. The nditer object provides an order parameter to control this aspect of iteration. The default, having the behavior described above, is order=\u2019K\u2019 to keep the existing order. This can be overridden with order=\u2019C\u2019 for C order and order=\u2019F\u2019 for Fortran order.", "By default, the nditer treats the input operand as a read-only object. To be able to modify the array elements, you must specify either read-write or write-only mode using the \u2018readwrite\u2019 or \u2018writeonly\u2019 per-operand flags.", "The nditer will then yield writeable buffer arrays which you may modify. However, because the nditer must copy this buffer data back to the original array once iteration is finished, you must signal when the iteration is ended, by one of two methods. You may either:", "The nditer can no longer be iterated once either close is called or its context is exited.", "If you are writing code that needs to support older versions of numpy, note that prior to 1.15, nditer was not a context manager and did not have a close method. Instead it relied on the destructor to initiate the writeback of the buffer.", "In all the examples so far, the elements of a are provided by the iterator one at a time, because all the looping logic is internal to the iterator. While this is simple and convenient, it is not very efficient. A better approach is to move the one-dimensional innermost loop into your code, external to the iterator. This way, NumPy\u2019s vectorized operations can be used on larger chunks of the elements being visited.", "The nditer will try to provide chunks that are as large as possible to the inner loop. By forcing \u2018C\u2019 and \u2018F\u2019 order, we get different external loop sizes. This mode is enabled by specifying an iterator flag.", "Observe that with the default of keeping native memory order, the iterator is able to provide a single one-dimensional chunk, whereas when forcing Fortran order, it has to provide three chunks of two elements each.", "During iteration, you may want to use the index of the current element in a computation. For example, you may want to visit the elements of an array in memory order, but use a C-order, Fortran-order, or multidimensional index to look up values in a different array.", "The index is tracked by the iterator object itself, and accessible through the index or multi_index properties, depending on what was requested. The examples below show printouts demonstrating the progression of the index:", "Tracking an index or multi-index is incompatible with using an external loop, because it requires a different index value per element. If you try to combine these flags, the nditer object will raise an exception.", "To make its properties more readily accessible during iteration, nditer has an alternative syntax for iterating, which works explicitly with the iterator object itself. With this looping construct, the current value is accessible by indexing into the iterator. Other properties, such as tracked indices remain as before. The examples below produce identical results to the ones in the previous section.", "When forcing an iteration order, we observed that the external loop option may provide the elements in smaller chunks because the elements can\u2019t be visited in the appropriate order with a constant stride. When writing C code, this is generally fine, however in pure Python code this can cause a significant reduction in performance.", "By enabling buffering mode, the chunks provided by the iterator to the inner loop can be made larger, significantly reducing the overhead of the Python interpreter. In the example forcing Fortran iteration order, the inner loop gets to see all the elements in one go when buffering is enabled.", "There are times when it is necessary to treat an array as a different data type than it is stored as. For instance, one may want to do all computations on 64-bit floats, even if the arrays being manipulated are 32-bit floats. Except when writing low-level C code, it\u2019s generally better to let the iterator handle the copying or buffering instead of casting the data type yourself in the inner loop.", "There are two mechanisms which allow this to be done, temporary copies and buffering mode. With temporary copies, a copy of the entire array is made with the new data type, then iteration is done in the copy. Write access is permitted through a mode which updates the original array after all the iteration is complete. The major drawback of temporary copies is that the temporary copy may consume a large amount of memory, particularly if the iteration data type has a larger itemsize than the original one.", "Buffering mode mitigates the memory usage issue and is more cache-friendly than making temporary copies. Except for special cases, where the whole array is needed at once outside the iterator, buffering is recommended over temporary copying. Within NumPy, buffering is used by the ufuncs and other functions to support flexible inputs with minimal memory overhead.", "In our examples, we will treat the input array with a complex data type, so that we can take square roots of negative numbers. Without enabling copies or buffering mode, the iterator will raise an exception if the data type doesn\u2019t match precisely.", "In copying mode, \u2018copy\u2019 is specified as a per-operand flag. This is done to provide control in a per-operand fashion. Buffering mode is specified as an iterator flag.", "The iterator uses NumPy\u2019s casting rules to determine whether a specific conversion is permitted. By default, it enforces \u2018safe\u2019 casting. This means, for example, that it will raise an exception if you try to treat a 64-bit float array as a 32-bit float array. In many cases, the rule \u2018same_kind\u2019 is the most reasonable rule to use, since it will allow conversion from 64 to 32-bit float, but not from float to int or from complex to float.", "One thing to watch out for is conversions back to the original data type when using a read-write or write-only operand. A common case is to implement the inner loop in terms of 64-bit floats, and use \u2018same_kind\u2019 casting to allow the other floating-point types to be processed as well. While in read-only mode, an integer array could be provided, read-write mode will raise an exception because conversion back to the array would violate the casting rule.", "NumPy has a set of rules for dealing with arrays that have differing shapes which are applied whenever functions take multiple operands which combine element-wise. This is called broadcasting. The nditer object can apply these rules for you when you need to write such a function.", "As an example, we print out the result of broadcasting a one and a two dimensional array together.", "When a broadcasting error occurs, the iterator raises an exception which includes the input shapes to help diagnose the problem.", "A common case in NumPy functions is to have outputs allocated based on the broadcasting of the input, and additionally have an optional parameter called \u2018out\u2019 where the result will be placed when it is provided. The nditer object provides a convenient idiom that makes it very easy to support this mechanism.", "We\u2019ll show how this works by creating a function square which squares its input. Let\u2019s start with a minimal function definition excluding \u2018out\u2019 parameter support.", "By default, the nditer uses the flags \u2018allocate\u2019 and \u2018writeonly\u2019 for operands that are passed in as None. This means we were able to provide just the two operands to the iterator, and it handled the rest.", "When adding the \u2018out\u2019 parameter, we have to explicitly provide those flags, because if someone passes in an array as \u2018out\u2019, the iterator will default to \u2018readonly\u2019, and our inner loop would fail. The reason \u2018readonly\u2019 is the default for input arrays is to prevent confusion about unintentionally triggering a reduction operation. If the default were \u2018readwrite\u2019, any broadcasting operation would also trigger a reduction, a topic which is covered later in this document.", "While we\u2019re at it, let\u2019s also introduce the \u2018no_broadcast\u2019 flag, which will prevent the output from being broadcast. This is important, because we only want one input value for each output. Aggregating more than one input value is a reduction operation which requires special handling. It would already raise an error because reductions must be explicitly enabled in an iterator flag, but the error message that results from disabling broadcasting is much more understandable for end-users. To see how to generalize the square function to a reduction, look at the sum of squares function in the section about Cython.", "For completeness, we\u2019ll also add the \u2018external_loop\u2019 and \u2018buffered\u2019 flags, as these are what you will typically want for performance reasons.", "Any binary operation can be extended to an array operation in an outer product fashion like in outer, and the nditer object provides a way to accomplish this by explicitly mapping the axes of the operands. It is also possible to do this with newaxis indexing, but we will show you how to directly use the nditer op_axes parameter to accomplish this with no intermediate views.", "We\u2019ll do a simple outer product, placing the dimensions of the first operand before the dimensions of the second operand. The op_axes parameter needs one list of axes for each operand, and provides a mapping from the iterator\u2019s axes to the axes of the operand.", "Suppose the first operand is one dimensional and the second operand is two dimensional. The iterator will have three dimensions, so op_axes will have two 3-element lists. The first list picks out the one axis of the first operand, and is -1 for the rest of the iterator axes, with a final result of [0, -1, -1]. The second list picks out the two axes of the second operand, but shouldn\u2019t overlap with the axes picked out in the first operand. Its list is [-1, 0, 1]. The output operand maps onto the iterator axes in the standard manner, so we can provide None instead of constructing another list.", "The operation in the inner loop is a straightforward multiplication. Everything to do with the outer product is handled by the iterator setup.", "Note that once the iterator is closed we can not access operands and must use a reference created inside the context manager.", "Whenever a writeable operand has fewer elements than the full iteration space, that operand is undergoing a reduction. The nditer object requires that any reduction operand be flagged as read-write, and only allows reductions when \u2018reduce_ok\u2019 is provided as an iterator flag.", "For a simple example, consider taking the sum of all elements in an array.", "Things are a little bit more tricky when combining reduction and allocated operands. Before iteration is started, any reduction operand must be initialized to its starting values. Here\u2019s how we can do this, taking sums along the last axis of a.", "To do buffered reduction requires yet another adjustment during the setup. Normally the iterator construction involves copying the first buffer of data from the readable arrays into the buffer. Any reduction operand is readable, so it may be read into a buffer. Unfortunately, initialization of the operand after this buffering operation is complete will not be reflected in the buffer that the iteration starts with, and garbage results will be produced.", "The iterator flag \u201cdelay_bufalloc\u201d is there to allow iterator-allocated reduction operands to exist together with buffering. When this flag is set, the iterator will leave its buffers uninitialized until it receives a reset, after which it will be ready for regular iteration. Here\u2019s how the previous example looks if we also enable buffering.", "Those who want really good performance out of their low level operations should strongly consider directly using the iteration API provided in C, but for those who are not comfortable with C or C++, Cython is a good middle ground with reasonable performance tradeoffs. For the nditer object, this means letting the iterator take care of broadcasting, dtype conversion, and buffering, while giving the inner loop to Cython.", "For our example, we\u2019ll create a sum of squares function. To start, let\u2019s implement this function in straightforward Python. We want to support an \u2018axis\u2019 parameter similar to the numpy sum function, so we will need to construct a list for the op_axes parameter. Here\u2019s how this looks.", "To Cython-ize this function, we replace the inner loop (y[\u2026] += x*x) with Cython code that\u2019s specialized for the float64 dtype. With the \u2018external_loop\u2019 flag enabled, the arrays provided to the inner loop will always be one-dimensional, so very little checking needs to be done.", "Here\u2019s the listing of sum_squares.pyx:", "On this machine, building the .pyx file into a module looked like the following, but you may have to find some Cython tutorials to tell you the specifics for your system configuration.:", "Running this from the Python interpreter produces the same answers as our native Python/NumPy code did.", "Doing a little timing in IPython shows that the reduced overhead and memory allocation of the Cython inner loop is providing a very nice speedup over both the straightforward Python code and an expression using NumPy\u2019s built-in sum function.:"]}, {"name": "Laguerre Series (numpy.polynomial.laguerre)", "path": "reference/routines.polynomials.laguerre", "type": "Laguerre Series ( \n        \n         numpy.polynomial.laguerre\n        \n        )", "text": ["This module provides a number of objects (mostly functions) useful for dealing with Laguerre series, including a Laguerre class that encapsulates the usual arithmetic operations. (General information on how this module represents and works with such polynomials is in the docstring for its \u201cparent\u201d sub-package, numpy.polynomial).", "Laguerre(coef[, domain, window])", "A Laguerre series class.", "lagdomain", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "lagzero", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "lagone", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "lagx", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "lagadd(c1, c2)", "Add one Laguerre series to another.", "lagsub(c1, c2)", "Subtract one Laguerre series from another.", "lagmulx(c)", "Multiply a Laguerre series by x.", "lagmul(c1, c2)", "Multiply one Laguerre series by another.", "lagdiv(c1, c2)", "Divide one Laguerre series by another.", "lagpow(c, pow[, maxpower])", "Raise a Laguerre series to a power.", "lagval(x, c[, tensor])", "Evaluate a Laguerre series at points x.", "lagval2d(x, y, c)", "Evaluate a 2-D Laguerre series at points (x, y).", "lagval3d(x, y, z, c)", "Evaluate a 3-D Laguerre series at points (x, y, z).", "laggrid2d(x, y, c)", "Evaluate a 2-D Laguerre series on the Cartesian product of x and y.", "laggrid3d(x, y, z, c)", "Evaluate a 3-D Laguerre series on the Cartesian product of x, y, and z.", "lagder(c[, m, scl, axis])", "Differentiate a Laguerre series.", "lagint(c[, m, k, lbnd, scl, axis])", "Integrate a Laguerre series.", "lagfromroots(roots)", "Generate a Laguerre series with given roots.", "lagroots(c)", "Compute the roots of a Laguerre series.", "lagvander(x, deg)", "Pseudo-Vandermonde matrix of given degree.", "lagvander2d(x, y, deg)", "Pseudo-Vandermonde matrix of given degrees.", "lagvander3d(x, y, z, deg)", "Pseudo-Vandermonde matrix of given degrees.", "laggauss(deg)", "Gauss-Laguerre quadrature.", "lagweight(x)", "Weight function of the Laguerre polynomials.", "lagcompanion(c)", "Return the companion matrix of c.", "lagfit(x, y, deg[, rcond, full, w])", "Least squares fit of Laguerre series to data.", "lagtrim(c[, tol])", "Remove \"small\" \"trailing\" coefficients from a polynomial.", "lagline(off, scl)", "Laguerre series whose graph is a straight line.", "lag2poly(c)", "Convert a Laguerre series to a polynomial.", "poly2lag(pol)", "Convert a polynomial to a Laguerre series.", "numpy.polynomial"]}, {"name": "Legendre Series (numpy.polynomial.legendre)", "path": "reference/routines.polynomials.legendre", "type": "Legendre Series ( \n        \n         numpy.polynomial.legendre\n        \n        )", "text": ["This module provides a number of objects (mostly functions) useful for dealing with Legendre series, including a Legendre class that encapsulates the usual arithmetic operations. (General information on how this module represents and works with such polynomials is in the docstring for its \u201cparent\u201d sub-package, numpy.polynomial).", "Legendre(coef[, domain, window])", "A Legendre series class.", "legdomain", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "legzero", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "legone", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "legx", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "legadd(c1, c2)", "Add one Legendre series to another.", "legsub(c1, c2)", "Subtract one Legendre series from another.", "legmulx(c)", "Multiply a Legendre series by x.", "legmul(c1, c2)", "Multiply one Legendre series by another.", "legdiv(c1, c2)", "Divide one Legendre series by another.", "legpow(c, pow[, maxpower])", "Raise a Legendre series to a power.", "legval(x, c[, tensor])", "Evaluate a Legendre series at points x.", "legval2d(x, y, c)", "Evaluate a 2-D Legendre series at points (x, y).", "legval3d(x, y, z, c)", "Evaluate a 3-D Legendre series at points (x, y, z).", "leggrid2d(x, y, c)", "Evaluate a 2-D Legendre series on the Cartesian product of x and y.", "leggrid3d(x, y, z, c)", "Evaluate a 3-D Legendre series on the Cartesian product of x, y, and z.", "legder(c[, m, scl, axis])", "Differentiate a Legendre series.", "legint(c[, m, k, lbnd, scl, axis])", "Integrate a Legendre series.", "legfromroots(roots)", "Generate a Legendre series with given roots.", "legroots(c)", "Compute the roots of a Legendre series.", "legvander(x, deg)", "Pseudo-Vandermonde matrix of given degree.", "legvander2d(x, y, deg)", "Pseudo-Vandermonde matrix of given degrees.", "legvander3d(x, y, z, deg)", "Pseudo-Vandermonde matrix of given degrees.", "leggauss(deg)", "Gauss-Legendre quadrature.", "legweight(x)", "Weight function of the Legendre polynomials.", "legcompanion(c)", "Return the scaled companion matrix of c.", "legfit(x, y, deg[, rcond, full, w])", "Least squares fit of Legendre series to data.", "legtrim(c[, tol])", "Remove \"small\" \"trailing\" coefficients from a polynomial.", "legline(off, scl)", "Legendre series whose graph is a straight line.", "leg2poly(c)", "Convert a Legendre series to a polynomial.", "poly2leg(pol)", "Convert a polynomial to a Legendre series.", "numpy.polynomial"]}, {"name": "lib.format.descr_to_dtype()", "path": "reference/generated/numpy.lib.format.descr_to_dtype", "type": "numpy.lib.format.descr_to_dtype", "text": ["Returns a dtype based off the given description.", "This is essentially the reverse of dtype_to_descr(). It will remove the valueless padding fields created by, i.e. simple fields like dtype(\u2018float32\u2019), and then convert the description to its corresponding dtype.", "The object retrieved by dtype.descr. Can be passed to numpy.dtype() in order to replicate the input dtype.", "The dtype constructed by the description."]}, {"name": "lib.format.dtype_to_descr()", "path": "reference/generated/numpy.lib.format.dtype_to_descr", "type": "numpy.lib.format.dtype_to_descr", "text": ["Get a serializable descriptor from the dtype.", "The .descr attribute of a dtype object cannot be round-tripped through the dtype() constructor. Simple types, like dtype(\u2018float32\u2019), have a descr which looks like a record array with one field with \u2018\u2019 as a name. The dtype() constructor interprets this as a request to give a default name. Instead, we construct descriptor that can be passed to dtype().", "The dtype of the array that will be written to disk.", "An object that can be passed to numpy.dtype() in order to replicate the input dtype."]}, {"name": "lib.format.header_data_from_array_1_0()", "path": "reference/generated/numpy.lib.format.header_data_from_array_1_0", "type": "numpy.lib.format.header_data_from_array_1_0", "text": ["Get the dictionary of header metadata from a numpy.ndarray.", "This has the appropriate entries for writing its string representation to the header of the file."]}, {"name": "lib.format.magic()", "path": "reference/generated/numpy.lib.format.magic", "type": "numpy.lib.format.magic", "text": ["Return the magic string for the given file format version."]}, {"name": "lib.format.open_memmap()", "path": "reference/generated/numpy.lib.format.open_memmap", "type": "numpy.lib.format.open_memmap", "text": ["Open a .npy file as a memory-mapped array.", "This may be used to read an existing file or create a new one.", "The name of the file on disk. This may not be a file-like object.", "The mode in which to open the file; the default is \u2018r+\u2019. In addition to the standard file modes, \u2018c\u2019 is also accepted to mean \u201ccopy on write.\u201d See memmap for the available mode strings.", "The data type of the array if we are creating a new file in \u201cwrite\u201d mode, if not, dtype is ignored. The default value is None, which results in a data-type of float64.", "The shape of the array if we are creating a new file in \u201cwrite\u201d mode, in which case this parameter is required. Otherwise, this parameter is ignored and is thus optional.", "Whether the array should be Fortran-contiguous (True) or C-contiguous (False, the default) if we are creating a new file in \u201cwrite\u201d mode.", "If the mode is a \u201cwrite\u201d mode, then this is the version of the file format used to create the file. None means use the oldest supported version that is able to store the data. Default: None", "The memory-mapped array.", "If the data or the mode is invalid.", "If the file is not found or cannot be opened correctly.", "See also"]}, {"name": "lib.format.read_array()", "path": "reference/generated/numpy.lib.format.read_array", "type": "numpy.lib.format.read_array", "text": ["Read an array from an NPY file.", "If this is not a real file object, then this may take extra memory and time.", "Whether to allow writing pickled data. Default: False", "Changed in version 1.16.3: Made default False in response to CVE-2019-6446.", "Additional keyword arguments to pass to pickle.load. These are only useful when loading object arrays saved on Python 2 when using Python 3.", "The array from the data on disk.", "If the data is invalid, or allow_pickle=False and the file contains an object array."]}, {"name": "lib.format.read_array_header_1_0()", "path": "reference/generated/numpy.lib.format.read_array_header_1_0", "type": "numpy.lib.format.read_array_header_1_0", "text": ["Read an array header from a filelike object using the 1.0 file format version.", "This will leave the file object located just after the header.", "A file object or something with a read() method like a file.", "The shape of the array.", "The array data will be written out directly if it is either C-contiguous or Fortran-contiguous. Otherwise, it will be made contiguous before writing it out.", "The dtype of the file\u2019s data.", "If the data is invalid."]}, {"name": "lib.format.read_array_header_2_0()", "path": "reference/generated/numpy.lib.format.read_array_header_2_0", "type": "numpy.lib.format.read_array_header_2_0", "text": ["Read an array header from a filelike object using the 2.0 file format version.", "This will leave the file object located just after the header.", "New in version 1.9.0.", "A file object or something with a read() method like a file.", "The shape of the array.", "The array data will be written out directly if it is either C-contiguous or Fortran-contiguous. Otherwise, it will be made contiguous before writing it out.", "The dtype of the file\u2019s data.", "If the data is invalid."]}, {"name": "lib.format.read_magic()", "path": "reference/generated/numpy.lib.format.read_magic", "type": "numpy.lib.format.read_magic", "text": ["Read the magic string to get the version of the file format."]}, {"name": "lib.format.write_array()", "path": "reference/generated/numpy.lib.format.write_array", "type": "numpy.lib.format.write_array", "text": ["Write an array to an NPY file, including a header.", "If the array is neither C-contiguous nor Fortran-contiguous AND the file_like object is not a real file object, this function will have to copy data in memory.", "An open, writable file object, or similar object with a .write() method.", "The array to write to disk.", "The version number of the format. None means use the oldest supported version that is able to store the data. Default: None", "Whether to allow writing pickled data. Default: True", "Additional keyword arguments to pass to pickle.dump, excluding \u2018protocol\u2019. These are only useful when pickling objects in object arrays on Python 3 to Python 2 compatible format.", "If the array cannot be persisted. This includes the case of allow_pickle=False and array being an object array.", "If the array contains Python objects as part of its dtype, the process of pickling them may raise various errors if the objects are not picklable."]}, {"name": "lib.format.write_array_header_1_0()", "path": "reference/generated/numpy.lib.format.write_array_header_1_0", "type": "numpy.lib.format.write_array_header_1_0", "text": ["Write the header for an array using the 1.0 format.", "This has the appropriate entries for writing its string representation to the header of the file."]}, {"name": "lib.format.write_array_header_2_0()", "path": "reference/generated/numpy.lib.format.write_array_header_2_0", "type": "numpy.lib.format.write_array_header_2_0", "text": ["The 2.0 format allows storing very large structured arrays.", "New in version 1.9.0.", "This has the appropriate entries for writing its string representation to the header of the file."]}, {"name": "lib.scimath.arccos()", "path": "reference/generated/numpy.lib.scimath.arccos", "type": "numpy.lib.scimath.arccos", "text": ["Compute the inverse cosine of x.", "Return the \u201cprincipal value\u201d (for a description of this, see numpy.arccos) of the inverse cosine of x. For real x such that abs(x) <= 1, this is a real number in the closed interval \\([0, \\pi]\\). Otherwise, the complex principle value is returned.", "The value(s) whose arccos is (are) required.", "The inverse cosine(s) of the x value(s). If x was a scalar, so is out, otherwise an array object is returned.", "See also", "For an arccos() that returns NAN when real x is not in the interval [-1,1], use numpy.arccos."]}, {"name": "lib.scimath.arcsin()", "path": "reference/generated/numpy.lib.scimath.arcsin", "type": "numpy.lib.scimath.arcsin", "text": ["Compute the inverse sine of x.", "Return the \u201cprincipal value\u201d (for a description of this, see numpy.arcsin) of the inverse sine of x. For real x such that abs(x) <= 1, this is a real number in the closed interval \\([-\\pi/2, \\pi/2]\\). Otherwise, the complex principle value is returned.", "The value(s) whose arcsin is (are) required.", "The inverse sine(s) of the x value(s). If x was a scalar, so is out, otherwise an array object is returned.", "See also", "For an arcsin() that returns NAN when real x is not in the interval [-1,1], use numpy.arcsin."]}, {"name": "lib.scimath.arctanh()", "path": "reference/generated/numpy.lib.scimath.arctanh", "type": "numpy.lib.scimath.arctanh", "text": ["Compute the inverse hyperbolic tangent of x.", "Return the \u201cprincipal value\u201d (for a description of this, see numpy.arctanh) of arctanh(x). For real x such that abs(x) < 1, this is a real number. If abs(x) > 1, or if x is complex, the result is complex. Finally, x = 1 returns``inf`` and x=-1 returns -inf.", "The value(s) whose arctanh is (are) required.", "The inverse hyperbolic tangent(s) of the x value(s). If x was a scalar so is out, otherwise an array is returned.", "See also", "For an arctanh() that returns NAN when real x is not in the interval (-1,1), use numpy.arctanh (this latter, however, does return +/-inf for x = +/-1)."]}, {"name": "lib.scimath.log()", "path": "reference/generated/numpy.lib.scimath.log", "type": "numpy.lib.scimath.log", "text": ["Compute the natural logarithm of x.", "Return the \u201cprincipal value\u201d (for a description of this, see numpy.log) of \\(log_e(x)\\). For real x > 0, this is a real number (log(0) returns -inf and log(np.inf) returns inf). Otherwise, the complex principle value is returned.", "The value(s) whose log is (are) required.", "The log of the x value(s). If x was a scalar, so is out, otherwise an array is returned.", "See also", "For a log() that returns NAN when real x < 0, use numpy.log (note, however, that otherwise numpy.log and this log are identical, i.e., both return -inf for x = 0, inf for x = inf, and, notably, the complex principle value if x.imag != 0).", "Negative arguments are handled \u201ccorrectly\u201d (recall that exp(log(x)) == x does not hold for real x < 0):"]}, {"name": "lib.scimath.log10()", "path": "reference/generated/numpy.lib.scimath.log10", "type": "numpy.lib.scimath.log10", "text": ["Compute the logarithm base 10 of x.", "Return the \u201cprincipal value\u201d (for a description of this, see numpy.log10) of \\(log_{10}(x)\\). For real x > 0, this is a real number (log10(0) returns -inf and log10(np.inf) returns inf). Otherwise, the complex principle value is returned.", "The value(s) whose log base 10 is (are) required.", "The log base 10 of the x value(s). If x was a scalar, so is out, otherwise an array object is returned.", "See also", "For a log10() that returns NAN when real x < 0, use numpy.log10 (note, however, that otherwise numpy.log10 and this log10 are identical, i.e., both return -inf for x = 0, inf for x = inf, and, notably, the complex principle value if x.imag != 0).", "(We set the printing precision so the example can be auto-tested)"]}, {"name": "lib.scimath.log2()", "path": "reference/generated/numpy.lib.scimath.log2", "type": "numpy.lib.scimath.log2", "text": ["Compute the logarithm base 2 of x.", "Return the \u201cprincipal value\u201d (for a description of this, see numpy.log2) of \\(log_2(x)\\). For real x > 0, this is a real number (log2(0) returns -inf and log2(np.inf) returns inf). Otherwise, the complex principle value is returned.", "The value(s) whose log base 2 is (are) required.", "The log base 2 of the x value(s). If x was a scalar, so is out, otherwise an array is returned.", "See also", "For a log2() that returns NAN when real x < 0, use numpy.log2 (note, however, that otherwise numpy.log2 and this log2 are identical, i.e., both return -inf for x = 0, inf for x = inf, and, notably, the complex principle value if x.imag != 0).", "We set the printing precision so the example can be auto-tested:"]}, {"name": "lib.scimath.logn()", "path": "reference/generated/numpy.lib.scimath.logn", "type": "numpy.lib.scimath.logn", "text": ["Take log base n of x.", "If x contains negative inputs, the answer is computed and returned in the complex domain.", "The integer base(s) in which the log is taken.", "The value(s) whose log base n is (are) required.", "The log base n of the x value(s). If x was a scalar, so is out, otherwise an array is returned."]}, {"name": "lib.scimath.power()", "path": "reference/generated/numpy.lib.scimath.power", "type": "numpy.lib.scimath.power", "text": ["Return x to the power p, (x**p).", "If x contains negative values, the output is converted to the complex domain.", "The input value(s).", "The power(s) to which x is raised. If x contains multiple values, p has to either be a scalar, or contain the same number of values as x. In the latter case, the result is x[0]**p[0], x[1]**p[1], ....", "The result of x**p. If x and p are scalars, so is out, otherwise an array is returned.", "See also"]}, {"name": "lib.scimath.sqrt()", "path": "reference/generated/numpy.lib.scimath.sqrt", "type": "numpy.lib.scimath.sqrt", "text": ["Compute the square root of x.", "For negative input elements, a complex value is returned (unlike numpy.sqrt which returns NaN).", "The input value(s).", "The square root of x. If x was a scalar, so is out, otherwise an array is returned.", "See also", "For real, non-negative inputs this works just like numpy.sqrt:", "But it automatically handles negative inputs:"]}, {"name": "lib.stride_tricks.as_strided()", "path": "reference/generated/numpy.lib.stride_tricks.as_strided", "type": "numpy.lib.stride_tricks.as_strided", "text": ["Create a view into the array with the given shape and strides.", "Warning", "This function has to be used with extreme care, see notes.", "Array to create a new.", "The shape of the new array. Defaults to x.shape.", "The strides of the new array. Defaults to x.strides.", "New in version 1.10.", "If True, subclasses are preserved.", "New in version 1.12.", "If set to False, the returned array will always be readonly. Otherwise it will be writable if the original array was. It is advisable to set this to False if possible (see Notes).", "See also", "broadcast an array to a given shape.", "reshape an array.", "userfriendly and safe function for the creation of sliding window views.", "as_strided creates a view into the array given the exact strides and shape. This means it manipulates the internal data structure of ndarray and, if done incorrectly, the array elements can point to invalid memory and can corrupt results or crash your program. It is advisable to always use the original x.strides when calculating new strides to avoid reliance on a contiguous memory layout.", "Furthermore, arrays created with this function often contain self overlapping memory, so that two elements are identical. Vectorized write operations on such arrays will typically be unpredictable. They may even give different results for small, large, or transposed arrays. Since writing to these arrays has to be tested and done with great care, you may want to use writeable=False to avoid accidental write operations.", "For these reasons it is advisable to avoid as_strided when possible."]}, {"name": "lib.stride_tricks.sliding_window_view()", "path": "reference/generated/numpy.lib.stride_tricks.sliding_window_view", "type": "numpy.lib.stride_tricks.sliding_window_view", "text": ["Create a sliding window view into the array with the given window shape.", "Also known as rolling or moving window, the window slides across all dimensions of the array and extracts subsets of the array at all window positions.", "New in version 1.20.0.", "Array to create the sliding window view from.", "Size of window over each axis that takes part in the sliding window. If axis is not present, must have same length as the number of input array dimensions. Single integers i are treated as if they were the tuple (i,).", "Axis or axes along which the sliding window is applied. By default, the sliding window is applied to all axes and window_shape[i] will refer to axis i of x. If axis is given as a tuple of int, window_shape[i] will refer to the axis axis[i] of x. Single integers i are treated as if they were the tuple (i,).", "If True, sub-classes will be passed-through, otherwise the returned array will be forced to be a base-class array (default).", "When true, allow writing to the returned view. The default is false, as this should be used with caution: the returned view contains the same memory location multiple times, so writing to one location will cause others to change.", "Sliding window view of the array. The sliding window dimensions are inserted at the end, and the original dimensions are trimmed as required by the size of the sliding window. That is, view.shape = x_shape_trimmed + window_shape, where x_shape_trimmed is x.shape with every entry reduced by one less than the corresponding window size.", "See also", "A lower-level and less safe routine for creating arbitrary views from custom shape and strides.", "broadcast an array to a given shape.", "For many applications using a sliding window view can be convenient, but potentially very slow. Often specialized solutions exist, for example:", "As a rough estimate, a sliding window approach with an input size of N and a window size of W will scale as O(N*W) where frequently a special algorithm can achieve O(N). That means that the sliding window variant for a window size of 100 can be a 100 times slower than a more specialized version.", "Nevertheless, for small window sizes, when no custom algorithm exists, or as a prototyping and developing tool, this function can be a good solution.", "This also works in more dimensions, e.g.", "The axis can be specified explicitly:", "The same axis can be used several times. In that case, every use reduces the corresponding original dimension:", "Combining with stepped slicing (::step), this can be used to take sliding views which skip elements:", "or views which move by multiple elements", "A common application of sliding_window_view is the calculation of running statistics. The simplest example is the moving average:", "Note that a sliding window approach is often not optimal (see Notes)."]}, {"name": "linalg.cholesky()", "path": "reference/generated/numpy.linalg.cholesky", "type": "numpy.linalg.cholesky", "text": ["Cholesky decomposition.", "Return the Cholesky decomposition, L * L.H, of the square matrix a, where L is lower-triangular and .H is the conjugate transpose operator (which is the ordinary transpose if a is real-valued). a must be Hermitian (symmetric if real-valued) and positive-definite. No checking is performed to verify whether a is Hermitian or not. In addition, only the lower-triangular and diagonal elements of a are used. Only L is actually returned.", "Hermitian (symmetric if all elements are real), positive-definite input matrix.", "Upper or lower-triangular Cholesky factor of a. Returns a matrix object if a is a matrix object.", "If the decomposition fails, for example, if a is not positive-definite.", "See also", "Similar function in SciPy.", "Cholesky decompose a banded Hermitian positive-definite matrix.", "Cholesky decomposition of a matrix, to use in scipy.linalg.cho_solve.", "New in version 1.8.0.", "Broadcasting rules apply, see the numpy.linalg documentation for details.", "The Cholesky decomposition is often used as a fast way of solving", "(when A is both Hermitian/symmetric and positive-definite).", "First, we solve for \\(\\mathbf{y}\\) in", "and then for \\(\\mathbf{x}\\) in"]}, {"name": "linalg.cond()", "path": "reference/generated/numpy.linalg.cond", "type": "numpy.linalg.cond", "text": ["Compute the condition number of a matrix.", "This function is capable of returning the condition number using one of seven different norms, depending on the value of p (see Parameters below).", "The matrix whose condition number is sought.", "Order of the norm used in the condition number computation:", "p", "norm for matrices", "None", "2-norm, computed directly using the SVD", "\u2018fro\u2019", "Frobenius norm", "inf", "max(sum(abs(x), axis=1))", "-inf", "min(sum(abs(x), axis=1))", "1", "max(sum(abs(x), axis=0))", "-1", "min(sum(abs(x), axis=0))", "2", "2-norm (largest sing. value)", "-2", "smallest singular value", "inf means the numpy.inf object, and the Frobenius norm is the root-of-sum-of-squares norm.", "The condition number of the matrix. May be infinite.", "See also", "The condition number of x is defined as the norm of x times the norm of the inverse of x [1]; the norm can be the usual L2-norm (root-of-sum-of-squares) or one of a number of other matrix norms.", "G. Strang, Linear Algebra and Its Applications, Orlando, FL, Academic Press, Inc., 1980, pg. 285."]}, {"name": "linalg.det()", "path": "reference/generated/numpy.linalg.det", "type": "numpy.linalg.det", "text": ["Compute the determinant of an array.", "Input array to compute determinants for.", "Determinant of a.", "See also", "Another way to represent the determinant, more suitable for large matrices where underflow/overflow may occur.", "Similar function in SciPy.", "New in version 1.8.0.", "Broadcasting rules apply, see the numpy.linalg documentation for details.", "The determinant is computed via LU factorization using the LAPACK routine z/dgetrf.", "The determinant of a 2-D array [[a, b], [c, d]] is ad - bc:", "Computing determinants for a stack of matrices:"]}, {"name": "linalg.eig()", "path": "reference/generated/numpy.linalg.eig", "type": "numpy.linalg.eig", "text": ["Compute the eigenvalues and right eigenvectors of a square array.", "Matrices for which the eigenvalues and right eigenvectors will be computed", "The eigenvalues, each repeated according to its multiplicity. The eigenvalues are not necessarily ordered. The resulting array will be of complex type, unless the imaginary part is zero in which case it will be cast to a real type. When a is real the resulting eigenvalues will be real (0 imaginary part) or occur in conjugate pairs", "The normalized (unit \u201clength\u201d) eigenvectors, such that the column v[:,i] is the eigenvector corresponding to the eigenvalue w[i].", "If the eigenvalue computation does not converge.", "See also", "eigenvalues of a non-symmetric array.", "eigenvalues and eigenvectors of a real symmetric or complex Hermitian (conjugate symmetric) array.", "eigenvalues of a real symmetric or complex Hermitian (conjugate symmetric) array.", "Similar function in SciPy that also solves the generalized eigenvalue problem.", "Best choice for unitary and other non-Hermitian normal matrices.", "New in version 1.8.0.", "Broadcasting rules apply, see the numpy.linalg documentation for details.", "This is implemented using the _geev LAPACK routines which compute the eigenvalues and eigenvectors of general square arrays.", "The number w is an eigenvalue of a if there exists a vector v such that a @ v = w * v. Thus, the arrays a, w, and v satisfy the equations a @ v[:,i] = w[i] * v[:,i] for \\(i \\in \\{0,...,M-1\\}\\).", "The array v of eigenvectors may not be of maximum rank, that is, some of the columns may be linearly dependent, although round-off error may obscure that fact. If the eigenvalues are all different, then theoretically the eigenvectors are linearly independent and a can be diagonalized by a similarity transformation using v, i.e, inv(v) @ a @ v is diagonal.", "For non-Hermitian normal matrices the SciPy function scipy.linalg.schur is preferred because the matrix v is guaranteed to be unitary, which is not the case when using eig. The Schur factorization produces an upper triangular matrix rather than a diagonal matrix, but for normal matrices only the diagonal of the upper triangular matrix is needed, the rest is roundoff error.", "Finally, it is emphasized that v consists of the right (as in right-hand side) eigenvectors of a. A vector y satisfying y.T @ a = z * y.T for some number z is called a left eigenvector of a, and, in general, the left and right eigenvectors of a matrix are not necessarily the (perhaps conjugate) transposes of each other.", "G. Strang, Linear Algebra and Its Applications, 2nd Ed., Orlando, FL, Academic Press, Inc., 1980, Various pp.", "(Almost) trivial example with real e-values and e-vectors.", "Real matrix possessing complex e-values and e-vectors; note that the e-values are complex conjugates of each other.", "Complex-valued matrix with real e-values (but complex-valued e-vectors); note that a.conj().T == a, i.e., a is Hermitian.", "Be careful about round-off error!"]}, {"name": "linalg.eigh()", "path": "reference/generated/numpy.linalg.eigh", "type": "numpy.linalg.eigh", "text": ["Return the eigenvalues and eigenvectors of a complex Hermitian (conjugate symmetric) or a real symmetric matrix.", "Returns two objects, a 1-D array containing the eigenvalues of a, and a 2-D square array or matrix (depending on the input type) of the corresponding eigenvectors (in columns).", "Hermitian or real symmetric matrices whose eigenvalues and eigenvectors are to be computed.", "Specifies whether the calculation is done with the lower triangular part of a (\u2018L\u2019, default) or the upper triangular part (\u2018U\u2019). Irrespective of this value only the real parts of the diagonal will be considered in the computation to preserve the notion of a Hermitian matrix. It therefore follows that the imaginary part of the diagonal will always be treated as zero.", "The eigenvalues in ascending order, each repeated according to its multiplicity.", "The column v[:, i] is the normalized eigenvector corresponding to the eigenvalue w[i]. Will return a matrix object if a is a matrix object.", "If the eigenvalue computation does not converge.", "See also", "eigenvalues of real symmetric or complex Hermitian (conjugate symmetric) arrays.", "eigenvalues and right eigenvectors for non-symmetric arrays.", "eigenvalues of non-symmetric arrays.", "Similar function in SciPy (but also solves the generalized eigenvalue problem).", "New in version 1.8.0.", "Broadcasting rules apply, see the numpy.linalg documentation for details.", "The eigenvalues/eigenvectors are computed using LAPACK routines _syevd, _heevd.", "The eigenvalues of real symmetric or complex Hermitian matrices are always real. [1] The array v of (column) eigenvectors is unitary and a, w, and v satisfy the equations dot(a, v[:, i]) = w[i] * v[:, i].", "G. Strang, Linear Algebra and Its Applications, 2nd Ed., Orlando, FL, Academic Press, Inc., 1980, pg. 222."]}, {"name": "linalg.eigvals()", "path": "reference/generated/numpy.linalg.eigvals", "type": "numpy.linalg.eigvals", "text": ["Compute the eigenvalues of a general matrix.", "Main difference between eigvals and eig: the eigenvectors aren\u2019t returned.", "A complex- or real-valued matrix whose eigenvalues will be computed.", "The eigenvalues, each repeated according to its multiplicity. They are not necessarily ordered, nor are they necessarily real for real matrices.", "If the eigenvalue computation does not converge.", "See also", "eigenvalues and right eigenvectors of general arrays", "eigenvalues of real symmetric or complex Hermitian (conjugate symmetric) arrays.", "eigenvalues and eigenvectors of real symmetric or complex Hermitian (conjugate symmetric) arrays.", "Similar function in SciPy.", "New in version 1.8.0.", "Broadcasting rules apply, see the numpy.linalg documentation for details.", "This is implemented using the _geev LAPACK routines which compute the eigenvalues and eigenvectors of general square arrays.", "Illustration, using the fact that the eigenvalues of a diagonal matrix are its diagonal elements, that multiplying a matrix on the left by an orthogonal matrix, Q, and on the right by Q.T (the transpose of Q), preserves the eigenvalues of the \u201cmiddle\u201d matrix. In other words, if Q is orthogonal, then Q * A * Q.T has the same eigenvalues as A:", "Now multiply a diagonal matrix by Q on one side and by Q.T on the other:"]}, {"name": "linalg.eigvalsh()", "path": "reference/generated/numpy.linalg.eigvalsh", "type": "numpy.linalg.eigvalsh", "text": ["Compute the eigenvalues of a complex Hermitian or real symmetric matrix.", "Main difference from eigh: the eigenvectors are not computed.", "A complex- or real-valued matrix whose eigenvalues are to be computed.", "Specifies whether the calculation is done with the lower triangular part of a (\u2018L\u2019, default) or the upper triangular part (\u2018U\u2019). Irrespective of this value only the real parts of the diagonal will be considered in the computation to preserve the notion of a Hermitian matrix. It therefore follows that the imaginary part of the diagonal will always be treated as zero.", "The eigenvalues in ascending order, each repeated according to its multiplicity.", "If the eigenvalue computation does not converge.", "See also", "eigenvalues and eigenvectors of real symmetric or complex Hermitian (conjugate symmetric) arrays.", "eigenvalues of general real or complex arrays.", "eigenvalues and right eigenvectors of general real or complex arrays.", "Similar function in SciPy.", "New in version 1.8.0.", "Broadcasting rules apply, see the numpy.linalg documentation for details.", "The eigenvalues are computed using LAPACK routines _syevd, _heevd."]}, {"name": "linalg.inv()", "path": "reference/generated/numpy.linalg.inv", "type": "numpy.linalg.inv", "text": ["Compute the (multiplicative) inverse of a matrix.", "Given a square matrix a, return the matrix ainv satisfying dot(a, ainv) = dot(ainv, a) = eye(a.shape[0]).", "Matrix to be inverted.", "(Multiplicative) inverse of the matrix a.", "If a is not square or inversion fails.", "See also", "Similar function in SciPy.", "New in version 1.8.0.", "Broadcasting rules apply, see the numpy.linalg documentation for details.", "If a is a matrix object, then the return value is a matrix as well:", "Inverses of several matrices can be computed at once:"]}, {"name": "linalg.LinAlgError", "path": "reference/generated/numpy.linalg.linalgerror", "type": "numpy.linalg.LinAlgError", "text": ["Generic Python-exception-derived object raised by linalg functions.", "General purpose exception class, derived from Python\u2019s exception.Exception class, programmatically raised in linalg functions when a Linear Algebra-related condition would prevent further correct execution of the function."]}, {"name": "linalg.lstsq()", "path": "reference/generated/numpy.linalg.lstsq", "type": "numpy.linalg.lstsq", "text": ["Return the least-squares solution to a linear matrix equation.", "Computes the vector x that approximately solves the equation a @ x = b. The equation may be under-, well-, or over-determined (i.e., the number of linearly independent rows of a can be less than, equal to, or greater than its number of linearly independent columns). If a is square and of full rank, then x (but for round-off error) is the \u201cexact\u201d solution of the equation. Else, x minimizes the Euclidean 2-norm \\(||b - ax||\\). If there are multiple minimizing solutions, the one with the smallest 2-norm \\(||x||\\) is returned.", "\u201cCoefficient\u201d matrix.", "Ordinate or \u201cdependent variable\u201d values. If b is two-dimensional, the least-squares solution is calculated for each of the K columns of b.", "Cut-off ratio for small singular values of a. For the purposes of rank determination, singular values are treated as zero if they are smaller than rcond times the largest singular value of a.", "Changed in version 1.14.0: If not set, a FutureWarning is given. The previous default of -1 will use the machine precision as rcond parameter, the new default will use the machine precision times max(M, N). To silence the warning and use the new default, use rcond=None, to keep using the old behavior, use rcond=-1.", "Least-squares solution. If b is two-dimensional, the solutions are in the K columns of x.", "Sums of squared residuals: Squared Euclidean 2-norm for each column in b - a @ x. If the rank of a is < N or M <= N, this is an empty array. If b is 1-dimensional, this is a (1,) shape array. Otherwise the shape is (K,).", "Rank of matrix a.", "Singular values of a.", "If computation does not converge.", "See also", "Similar function in SciPy.", "If b is a matrix, then all array results are returned as matrices.", "Fit a line, y = mx + c, through some noisy data-points:", "By examining the coefficients, we see that the line should have a gradient of roughly 1 and cut the y-axis at, more or less, -1.", "We can rewrite the line equation as y = Ap, where A = [[x 1]] and p = [[m], [c]]. Now use lstsq to solve for p:", "Plot the data along with the fitted line:"]}, {"name": "linalg.matrix_power()", "path": "reference/generated/numpy.linalg.matrix_power", "type": "numpy.linalg.matrix_power", "text": ["Raise a square matrix to the (integer) power n.", "For positive integers n, the power is computed by repeated matrix squarings and matrix multiplications. If n == 0, the identity matrix of the same shape as M is returned. If n < 0, the inverse is computed and then raised to the abs(n).", "Note", "Stacks of object matrices are not currently supported.", "Matrix to be \u201cpowered\u201d.", "The exponent can be any integer or long integer, positive, negative, or zero.", "The return value is the same shape and type as M; if the exponent is positive or zero then the type of the elements is the same as those of M. If the exponent is negative the elements are floating-point.", "For matrices that are not square or that (for negative powers) cannot be inverted numerically.", "Somewhat more sophisticated example"]}, {"name": "linalg.matrix_rank()", "path": "reference/generated/numpy.linalg.matrix_rank", "type": "numpy.linalg.matrix_rank", "text": ["Return matrix rank of array using SVD method", "Rank of the array is the number of singular values of the array that are greater than tol.", "Changed in version 1.14: Can now operate on stacks of matrices", "Input vector or stack of matrices.", "Threshold below which SVD values are considered zero. If tol is None, and S is an array with singular values for M, and eps is the epsilon value for datatype of S, then tol is set to S.max() * max(M, N) * eps.", "Changed in version 1.14: Broadcasted against the stack of matrices", "If True, A is assumed to be Hermitian (symmetric if real-valued), enabling a more efficient method for finding singular values. Defaults to False.", "New in version 1.14.", "Rank of A.", "The default threshold to detect rank deficiency is a test on the magnitude of the singular values of A. By default, we identify singular values less than S.max() * max(M, N) * eps as indicating rank deficiency (with the symbols defined above). This is the algorithm MATLAB uses [1]. It also appears in Numerical recipes in the discussion of SVD solutions for linear least squares [2].", "This default threshold is designed to detect rank deficiency accounting for the numerical errors of the SVD computation. Imagine that there is a column in A that is an exact (in floating point) linear combination of other columns in A. Computing the SVD on A will not produce a singular value exactly equal to 0 in general: any difference of the smallest SVD value from 0 will be caused by numerical imprecision in the calculation of the SVD. Our threshold for small SVD values takes this numerical imprecision into account, and the default threshold will detect such numerical rank deficiency. The threshold may declare a matrix A rank deficient even if the linear combination of some columns of A is not exactly equal to another column of A but only numerically very close to another column of A.", "We chose our default threshold because it is in wide use. Other thresholds are possible. For example, elsewhere in the 2007 edition of Numerical recipes there is an alternative threshold of S.max() *\nnp.finfo(A.dtype).eps / 2. * np.sqrt(m + n + 1.). The authors describe this threshold as being based on \u201cexpected roundoff error\u201d (p 71).", "The thresholds above deal with floating point roundoff error in the calculation of the SVD. However, you may have more information about the sources of error in A that would make you consider other tolerance values to detect effective rank deficiency. The most useful measure of the tolerance depends on the operations you intend to use on your matrix. For example, if your data come from uncertain measurements with uncertainties greater than floating point epsilon, choosing a tolerance near that uncertainty may be preferable. The tolerance may be absolute if the uncertainties are absolute rather than relative.", "MATLAB reference documentation, \u201cRank\u201d https://www.mathworks.com/help/techdoc/ref/rank.html", "W. H. Press, S. A. Teukolsky, W. T. Vetterling and B. P. Flannery, \u201cNumerical Recipes (3rd edition)\u201d, Cambridge University Press, 2007, page 795."]}, {"name": "linalg.multi_dot()", "path": "reference/generated/numpy.linalg.multi_dot", "type": "numpy.linalg.multi_dot", "text": ["Compute the dot product of two or more arrays in a single function call, while automatically selecting the fastest evaluation order.", "multi_dot chains numpy.dot and uses optimal parenthesization of the matrices [1] [2]. Depending on the shapes of the matrices, this can speed up the multiplication a lot.", "If the first argument is 1-D it is treated as a row vector. If the last argument is 1-D it is treated as a column vector. The other arguments must be 2-D.", "Think of multi_dot as:", "If the first argument is 1-D it is treated as row vector. If the last argument is 1-D it is treated as column vector. The other arguments must be 2-D.", "Output argument. This must have the exact kind that would be returned if it was not used. In particular, it must have the right type, must be C-contiguous, and its dtype must be the dtype that would be returned for dot(a, b). This is a performance feature. Therefore, if these conditions are not met, an exception is raised, instead of attempting to be flexible.", "New in version 1.19.0.", "Returns the dot product of the supplied arrays.", "See also", "dot multiplication with two arguments.", "The cost for a matrix multiplication can be calculated with the following function:", "Assume we have three matrices \\(A_{10x100}, B_{100x5}, C_{5x50}\\).", "The costs for the two different parenthesizations are as follows:", "Cormen, \u201cIntroduction to Algorithms\u201d, Chapter 15.2, p. 370-378", "https://en.wikipedia.org/wiki/Matrix_chain_multiplication", "multi_dot allows you to write:", "instead of:"]}, {"name": "linalg.norm()", "path": "reference/generated/numpy.linalg.norm", "type": "numpy.linalg.norm", "text": ["Matrix or vector norm.", "This function is able to return one of eight different matrix norms, or one of an infinite number of vector norms (described below), depending on the value of the ord parameter.", "Input array. If axis is None, x must be 1-D or 2-D, unless ord is None. If both axis and ord are None, the 2-norm of x.ravel will be returned.", "Order of the norm (see table under Notes). inf means numpy\u2019s inf object. The default is None.", "If axis is an integer, it specifies the axis of x along which to compute the vector norms. If axis is a 2-tuple, it specifies the axes that hold 2-D matrices, and the matrix norms of these matrices are computed. If axis is None then either a vector norm (when x is 1-D) or a matrix norm (when x is 2-D) is returned. The default is None.", "New in version 1.8.0.", "If this is set to True, the axes which are normed over are left in the result as dimensions with size one. With this option the result will broadcast correctly against the original x.", "New in version 1.10.0.", "Norm of the matrix or vector(s).", "See also", "Similar function in SciPy.", "For values of ord < 1, the result is, strictly speaking, not a mathematical \u2018norm\u2019, but it may still be useful for various numerical purposes.", "The following norms can be calculated:", "ord", "norm for matrices", "norm for vectors", "None", "Frobenius norm", "2-norm", "\u2018fro\u2019", "Frobenius norm", "\u2013", "\u2018nuc\u2019", "nuclear norm", "\u2013", "inf", "max(sum(abs(x), axis=1))", "max(abs(x))", "-inf", "min(sum(abs(x), axis=1))", "min(abs(x))", "0", "\u2013", "sum(x != 0)", "1", "max(sum(abs(x), axis=0))", "as below", "-1", "min(sum(abs(x), axis=0))", "as below", "2", "2-norm (largest sing. value)", "as below", "-2", "smallest singular value", "as below", "other", "\u2013", "sum(abs(x)**ord)**(1./ord)", "The Frobenius norm is given by [1]:", "\\(||A||_F = [\\sum_{i,j} abs(a_{i,j})^2]^{1/2}\\)", "The nuclear norm is the sum of the singular values.", "Both the Frobenius and nuclear norm orders are only defined for matrices and raise a ValueError when x.ndim != 2.", "G. H. Golub and C. F. Van Loan, Matrix Computations, Baltimore, MD, Johns Hopkins University Press, 1985, pg. 15", "Using the axis argument to compute vector norms:", "Using the axis argument to compute matrix norms:"]}, {"name": "linalg.pinv()", "path": "reference/generated/numpy.linalg.pinv", "type": "numpy.linalg.pinv", "text": ["Compute the (Moore-Penrose) pseudo-inverse of a matrix.", "Calculate the generalized inverse of a matrix using its singular-value decomposition (SVD) and including all large singular values.", "Changed in version 1.14: Can now operate on stacks of matrices", "Matrix or stack of matrices to be pseudo-inverted.", "Cutoff for small singular values. Singular values less than or equal to rcond * largest_singular_value are set to zero. Broadcasts against the stack of matrices.", "If True, a is assumed to be Hermitian (symmetric if real-valued), enabling a more efficient method for finding singular values. Defaults to False.", "New in version 1.17.0.", "The pseudo-inverse of a. If a is a matrix instance, then so is B.", "If the SVD computation does not converge.", "See also", "Similar function in SciPy.", "Similar function in SciPy (SVD-based).", "Compute the (Moore-Penrose) pseudo-inverse of a Hermitian matrix.", "The pseudo-inverse of a matrix A, denoted \\(A^+\\), is defined as: \u201cthe matrix that \u2018solves\u2019 [the least-squares problem] \\(Ax = b\\),\u201d i.e., if \\(\\bar{x}\\) is said solution, then \\(A^+\\) is that matrix such that \\(\\bar{x} = A^+b\\).", "It can be shown that if \\(Q_1 \\Sigma Q_2^T = A\\) is the singular value decomposition of A, then \\(A^+ = Q_2 \\Sigma^+ Q_1^T\\), where \\(Q_{1,2}\\) are orthogonal matrices, \\(\\Sigma\\) is a diagonal matrix consisting of A\u2019s so-called singular values, (followed, typically, by zeros), and then \\(\\Sigma^+\\) is simply the diagonal matrix consisting of the reciprocals of A\u2019s singular values (again, followed by zeros). [1]", "G. Strang, Linear Algebra and Its Applications, 2nd Ed., Orlando, FL, Academic Press, Inc., 1980, pp. 139-142.", "The following example checks that a * a+ * a == a and a+ * a * a+ == a+:"]}, {"name": "linalg.qr()", "path": "reference/generated/numpy.linalg.qr", "type": "numpy.linalg.qr", "text": ["Compute the qr factorization of a matrix.", "Factor the matrix a as qr, where q is orthonormal and r is upper-triangular.", "An array-like object with the dimensionality of at least 2.", "If K = min(M, N), then", "(\u2026, M, K), (\u2026, K, N) (default)", "The options \u2018reduced\u2019, \u2018complete, and \u2018raw\u2019 are new in numpy 1.8, see the notes for more information. The default is \u2018reduced\u2019, and to maintain backward compatibility with earlier versions of numpy both it and the old default \u2018full\u2019 can be omitted. Note that array h returned in \u2018raw\u2019 mode is transposed for calling Fortran. The \u2018economic\u2019 mode is deprecated. The modes \u2018full\u2019 and \u2018economic\u2019 may be passed using only the first letter for backwards compatibility, but all others must be spelled out. See the Notes for more explanation.", "A matrix with orthonormal columns. When mode = \u2018complete\u2019 the result is an orthogonal/unitary matrix depending on whether or not a is real/complex. The determinant may be either +/- 1 in that case. In case the number of dimensions in the input array is greater than 2 then a stack of the matrices with above properties is returned.", "The upper-triangular matrix or a stack of upper-triangular matrices if the number of dimensions in the input array is greater than 2.", "The array h contains the Householder reflectors that generate q along with r. The tau array contains scaling factors for the reflectors. In the deprecated \u2018economic\u2019 mode only h is returned.", "If factoring fails.", "See also", "Similar function in SciPy.", "Compute RQ decomposition of a matrix.", "This is an interface to the LAPACK routines dgeqrf, zgeqrf, dorgqr, and zungqr.", "For more information on the qr factorization, see for example: https://en.wikipedia.org/wiki/QR_factorization", "Subclasses of ndarray are preserved except for the \u2018raw\u2019 mode. So if a is of type matrix, all the return values will be matrices too.", "New \u2018reduced\u2019, \u2018complete\u2019, and \u2018raw\u2019 options for mode were added in NumPy 1.8.0 and the old option \u2018full\u2019 was made an alias of \u2018reduced\u2019. In addition the options \u2018full\u2019 and \u2018economic\u2019 were deprecated. Because \u2018full\u2019 was the previous default and \u2018reduced\u2019 is the new default, backward compatibility can be maintained by letting mode default. The \u2018raw\u2019 option was added so that LAPACK routines that can multiply arrays by q using the Householder reflectors can be used. Note that in this case the returned arrays are of type np.double or np.cdouble and the h array is transposed to be FORTRAN compatible. No routines using the \u2018raw\u2019 return are currently exposed by numpy, but some are available in lapack_lite and just await the necessary work.", "Example illustrating a common use of qr: solving of least squares problems", "What are the least-squares-best m and y0 in y = y0 + mx for the following data: {(0,1), (1,0), (1,2), (2,1)}. (Graph the points and you\u2019ll see that it should be y0 = 0, m = 1.) The answer is provided by solving the over-determined matrix equation Ax = b, where:", "If A = qr such that q is orthonormal (which is always possible via Gram-Schmidt), then x = inv(r) * (q.T) * b. (In numpy practice, however, we simply use lstsq.)"]}, {"name": "linalg.slogdet()", "path": "reference/generated/numpy.linalg.slogdet", "type": "numpy.linalg.slogdet", "text": ["Compute the sign and (natural) logarithm of the determinant of an array.", "If an array has a very small or very large determinant, then a call to det may overflow or underflow. This routine is more robust against such issues, because it computes the logarithm of the determinant rather than the determinant itself.", "Input array, has to be a square 2-D array.", "A number representing the sign of the determinant. For a real matrix, this is 1, 0, or -1. For a complex matrix, this is a complex number with absolute value 1 (i.e., it is on the unit circle), or else 0.", "The natural log of the absolute value of the determinant.", "See also", "New in version 1.8.0.", "Broadcasting rules apply, see the numpy.linalg documentation for details.", "New in version 1.6.0.", "The determinant is computed via LU factorization using the LAPACK routine z/dgetrf.", "The determinant of a 2-D array [[a, b], [c, d]] is ad - bc:", "Computing log-determinants for a stack of matrices:", "This routine succeeds where ordinary det does not:"]}, {"name": "linalg.solve()", "path": "reference/generated/numpy.linalg.solve", "type": "numpy.linalg.solve", "text": ["Solve a linear matrix equation, or system of linear scalar equations.", "Computes the \u201cexact\u201d solution, x, of the well-determined, i.e., full rank, linear matrix equation ax = b.", "Coefficient matrix.", "Ordinate or \u201cdependent variable\u201d values.", "Solution to the system a x = b. Returned shape is identical to b.", "If a is singular or not square.", "See also", "Similar function in SciPy.", "New in version 1.8.0.", "Broadcasting rules apply, see the numpy.linalg documentation for details.", "The solutions are computed using LAPACK routine _gesv.", "a must be square and of full-rank, i.e., all rows (or, equivalently, columns) must be linearly independent; if either is not true, use lstsq for the least-squares best \u201csolution\u201d of the system/equation.", "G. Strang, Linear Algebra and Its Applications, 2nd Ed., Orlando, FL, Academic Press, Inc., 1980, pg. 22.", "Solve the system of equations x0 + 2 * x1 = 1 and 3 * x0 + 5 * x1 = 2:", "Check that the solution is correct:"]}, {"name": "linalg.svd()", "path": "reference/generated/numpy.linalg.svd", "type": "numpy.linalg.svd", "text": ["Singular Value Decomposition.", "When a is a 2D array, it is factorized as u @ np.diag(s) @ vh\n= (u * s) @ vh, where u and vh are 2D unitary arrays and s is a 1D array of a\u2019s singular values. When a is higher-dimensional, SVD is applied in stacked mode as explained below.", "A real or complex array with a.ndim >= 2.", "If True (default), u and vh have the shapes (..., M, M) and (..., N, N), respectively. Otherwise, the shapes are (..., M, K) and (..., K, N), respectively, where K = min(M, N).", "Whether or not to compute u and vh in addition to s. True by default.", "If True, a is assumed to be Hermitian (symmetric if real-valued), enabling a more efficient method for finding singular values. Defaults to False.", "New in version 1.17.0.", "Unitary array(s). The first a.ndim - 2 dimensions have the same size as those of the input a. The size of the last two dimensions depends on the value of full_matrices. Only returned when compute_uv is True.", "Vector(s) with the singular values, within each vector sorted in descending order. The first a.ndim - 2 dimensions have the same size as those of the input a.", "Unitary array(s). The first a.ndim - 2 dimensions have the same size as those of the input a. The size of the last two dimensions depends on the value of full_matrices. Only returned when compute_uv is True.", "If SVD computation does not converge.", "See also", "Similar function in SciPy.", "Compute singular values of a matrix.", "Changed in version 1.8.0: Broadcasting rules apply, see the numpy.linalg documentation for details.", "The decomposition is performed using LAPACK routine _gesdd.", "SVD is usually described for the factorization of a 2D matrix \\(A\\). The higher-dimensional case will be discussed below. In the 2D case, SVD is written as \\(A = U S V^H\\), where \\(A = a\\), \\(U= u\\), \\(S= \\mathtt{np.diag}(s)\\) and \\(V^H = vh\\). The 1D array s contains the singular values of a and u and vh are unitary. The rows of vh are the eigenvectors of \\(A^H A\\) and the columns of u are the eigenvectors of \\(A A^H\\). In both cases the corresponding (possibly non-zero) eigenvalues are given by s**2.", "If a has more than two dimensions, then broadcasting rules apply, as explained in Linear algebra on several matrices at once. This means that SVD is working in \u201cstacked\u201d mode: it iterates over all indices of the first a.ndim - 2 dimensions and for each combination SVD is applied to the last two indices. The matrix a can be reconstructed from the decomposition with either (u * s[..., None, :]) @ vh or u @ (s[..., None] * vh). (The @ operator can be replaced by the function np.matmul for python versions below 3.5.)", "If a is a matrix object (as opposed to an ndarray), then so are all the return values.", "Reconstruction based on full SVD, 2D case:", "Reconstruction based on reduced SVD, 2D case:", "Reconstruction based on full SVD, 4D case:", "Reconstruction based on reduced SVD, 4D case:"]}, {"name": "linalg.tensorinv()", "path": "reference/generated/numpy.linalg.tensorinv", "type": "numpy.linalg.tensorinv", "text": ["Compute the \u2018inverse\u2019 of an N-dimensional array.", "The result is an inverse for a relative to the tensordot operation tensordot(a, b, ind), i. e., up to floating-point accuracy, tensordot(tensorinv(a), a, ind) is the \u201cidentity\u201d tensor for the tensordot operation.", "Tensor to \u2018invert\u2019. Its shape must be \u2018square\u2019, i. e., prod(a.shape[:ind]) == prod(a.shape[ind:]).", "Number of first indices that are involved in the inverse sum. Must be a positive integer, default is 2.", "a\u2019s tensordot inverse, shape a.shape[ind:] + a.shape[:ind].", "If a is singular or not \u2018square\u2019 (in the above sense).", "See also"]}, {"name": "linalg.tensorsolve()", "path": "reference/generated/numpy.linalg.tensorsolve", "type": "numpy.linalg.tensorsolve", "text": ["Solve the tensor equation a x = b for x.", "It is assumed that all indices of x are summed over in the product, together with the rightmost indices of a, as is done in, for example, tensordot(a, x, axes=b.ndim).", "Coefficient tensor, of shape b.shape + Q. Q, a tuple, equals the shape of that sub-tensor of a consisting of the appropriate number of its rightmost indices, and must be such that prod(Q) == prod(b.shape) (in which sense a is said to be \u2018square\u2019).", "Right-hand tensor, which can be of any shape.", "Axes in a to reorder to the right, before inversion. If None (default), no reordering is done.", "If a is singular or not \u2018square\u2019 (in the above sense).", "See also"]}, {"name": "Linear algebra (numpy.linalg)", "path": "reference/routines.linalg", "type": "Linear algebra ( \n      \n       numpy.linalg\n      \n      )", "text": ["The NumPy linear algebra functions rely on BLAS and LAPACK to provide efficient low level implementations of standard linear algebra algorithms. Those libraries may be provided by NumPy itself using C versions of a subset of their reference implementations but, when possible, highly optimized libraries that take advantage of specialized processor functionality are preferred. Examples of such libraries are OpenBLAS, MKL (TM), and ATLAS. Because those libraries are multithreaded and processor dependent, environmental variables and external packages such as threadpoolctl may be needed to control the number of threads or specify the processor architecture.", "The SciPy library also contains a linalg submodule, and there is overlap in the functionality provided by the SciPy and NumPy submodules. SciPy contains functions not found in numpy.linalg, such as functions related to LU decomposition and the Schur decomposition, multiple ways of calculating the pseudoinverse, and matrix transcendentals such as the matrix logarithm. Some functions that exist in both have augmented functionality in scipy.linalg. For example, scipy.linalg.eig can take a second matrix argument for solving generalized eigenvalue problems. Some functions in NumPy, however, have more flexible broadcasting options. For example, numpy.linalg.solve can handle \u201cstacked\u201d arrays, while scipy.linalg.solve accepts only a single square array as its first argument.", "Note", "The term matrix as it is used on this page indicates a 2d numpy.array object, and not a numpy.matrix object. The latter is no longer recommended, even for linear algebra. See the matrix object documentation for more information.", "Introduced in NumPy 1.10.0, the @ operator is preferable to other methods when computing the matrix product between 2d arrays. The numpy.matmul function implements the @ operator.", "dot(a, b[, out])", "Dot product of two arrays.", "linalg.multi_dot(arrays, *[, out])", "Compute the dot product of two or more arrays in a single function call, while automatically selecting the fastest evaluation order.", "vdot(a, b, /)", "Return the dot product of two vectors.", "inner(a, b, /)", "Inner product of two arrays.", "outer(a, b[, out])", "Compute the outer product of two vectors.", "matmul(x1, x2, /[, out, casting, order, ...])", "Matrix product of two arrays.", "tensordot(a, b[, axes])", "Compute tensor dot product along specified axes.", "einsum(subscripts, *operands[, out, dtype, ...])", "Evaluates the Einstein summation convention on the operands.", "einsum_path(subscripts, *operands[, optimize])", "Evaluates the lowest cost contraction order for an einsum expression by considering the creation of intermediate arrays.", "linalg.matrix_power(a, n)", "Raise a square matrix to the (integer) power n.", "kron(a, b)", "Kronecker product of two arrays.", "linalg.cholesky(a)", "Cholesky decomposition.", "linalg.qr(a[, mode])", "Compute the qr factorization of a matrix.", "linalg.svd(a[, full_matrices, compute_uv, ...])", "Singular Value Decomposition.", "linalg.eig(a)", "Compute the eigenvalues and right eigenvectors of a square array.", "linalg.eigh(a[, UPLO])", "Return the eigenvalues and eigenvectors of a complex Hermitian (conjugate symmetric) or a real symmetric matrix.", "linalg.eigvals(a)", "Compute the eigenvalues of a general matrix.", "linalg.eigvalsh(a[, UPLO])", "Compute the eigenvalues of a complex Hermitian or real symmetric matrix.", "linalg.norm(x[, ord, axis, keepdims])", "Matrix or vector norm.", "linalg.cond(x[, p])", "Compute the condition number of a matrix.", "linalg.det(a)", "Compute the determinant of an array.", "linalg.matrix_rank(A[, tol, hermitian])", "Return matrix rank of array using SVD method", "linalg.slogdet(a)", "Compute the sign and (natural) logarithm of the determinant of an array.", "trace(a[, offset, axis1, axis2, dtype, out])", "Return the sum along diagonals of the array.", "linalg.solve(a, b)", "Solve a linear matrix equation, or system of linear scalar equations.", "linalg.tensorsolve(a, b[, axes])", "Solve the tensor equation a x = b for x.", "linalg.lstsq(a, b[, rcond])", "Return the least-squares solution to a linear matrix equation.", "linalg.inv(a)", "Compute the (multiplicative) inverse of a matrix.", "linalg.pinv(a[, rcond, hermitian])", "Compute the (Moore-Penrose) pseudo-inverse of a matrix.", "linalg.tensorinv(a[, ind])", "Compute the 'inverse' of an N-dimensional array.", "linalg.LinAlgError", "Generic Python-exception-derived object raised by linalg functions.", "New in version 1.8.0.", "Several of the linear algebra routines listed above are able to compute results for several matrices at once, if they are stacked into the same array.", "This is indicated in the documentation via input parameter specifications such as a : (..., M, M) array_like. This means that if for instance given an input array a.shape == (N, M, M), it is interpreted as a \u201cstack\u201d of N matrices, each of size M-by-M. Similar specification applies to return values, for instance the determinant has det : (...) and will in this case return an array of shape det(a).shape == (N,). This generalizes to linear algebra operations on higher-dimensional arrays: the last 1 or 2 dimensions of a multidimensional array are interpreted as vectors or matrices, as appropriate for each operation."]}, {"name": "Logic functions", "path": "reference/routines.logic", "type": "Logic functions", "text": ["all(a[, axis, out, keepdims, where])", "Test whether all array elements along a given axis evaluate to True.", "any(a[, axis, out, keepdims, where])", "Test whether any array element along a given axis evaluates to True.", "isfinite(x, /[, out, where, casting, order, ...])", "Test element-wise for finiteness (not infinity and not Not a Number).", "isinf(x, /[, out, where, casting, order, ...])", "Test element-wise for positive or negative infinity.", "isnan(x, /[, out, where, casting, order, ...])", "Test element-wise for NaN and return result as a boolean array.", "isnat(x, /[, out, where, casting, order, ...])", "Test element-wise for NaT (not a time) and return result as a boolean array.", "isneginf(x[, out])", "Test element-wise for negative infinity, return result as bool array.", "isposinf(x[, out])", "Test element-wise for positive infinity, return result as bool array.", "iscomplex(x)", "Returns a bool array, where True if input element is complex.", "iscomplexobj(x)", "Check for a complex type or an array of complex numbers.", "isfortran(a)", "Check if the array is Fortran contiguous but not C contiguous.", "isreal(x)", "Returns a bool array, where True if input element is real.", "isrealobj(x)", "Return True if x is a not complex type or an array of complex numbers.", "isscalar(element)", "Returns True if the type of element is a scalar type.", "logical_and(x1, x2, /[, out, where, ...])", "Compute the truth value of x1 AND x2 element-wise.", "logical_or(x1, x2, /[, out, where, casting, ...])", "Compute the truth value of x1 OR x2 element-wise.", "logical_not(x, /[, out, where, casting, ...])", "Compute the truth value of NOT x element-wise.", "logical_xor(x1, x2, /[, out, where, ...])", "Compute the truth value of x1 XOR x2, element-wise.", "allclose(a, b[, rtol, atol, equal_nan])", "Returns True if two arrays are element-wise equal within a tolerance.", "isclose(a, b[, rtol, atol, equal_nan])", "Returns a boolean array where two arrays are element-wise equal within a tolerance.", "array_equal(a1, a2[, equal_nan])", "True if two arrays have the same shape and elements, False otherwise.", "array_equiv(a1, a2)", "Returns True if input arrays are shape consistent and all elements equal.", "greater(x1, x2, /[, out, where, casting, ...])", "Return the truth value of (x1 > x2) element-wise.", "greater_equal(x1, x2, /[, out, where, ...])", "Return the truth value of (x1 >= x2) element-wise.", "less(x1, x2, /[, out, where, casting, ...])", "Return the truth value of (x1 < x2) element-wise.", "less_equal(x1, x2, /[, out, where, casting, ...])", "Return the truth value of (x1 <= x2) element-wise.", "equal(x1, x2, /[, out, where, casting, ...])", "Return (x1 == x2) element-wise.", "not_equal(x1, x2, /[, out, where, casting, ...])", "Return (x1 != x2) element-wise."]}, {"name": "ma.all()", "path": "reference/generated/numpy.ma.all", "type": "numpy.ma.all", "text": ["Returns True if all elements evaluate to True.", "The output array is masked where all the values along the given axis are masked: if the output would have been a scalar and that all the values are masked, then the output is masked.", "Refer to numpy.all for full documentation.", "See also", "corresponding function for ndarrays", "equivalent function"]}, {"name": "ma.allclose()", "path": "reference/generated/numpy.ma.allclose", "type": "numpy.ma.allclose", "text": ["Returns True if two arrays are element-wise equal within a tolerance.", "This function is equivalent to allclose except that masked values are treated as equal (default) or unequal, depending on the masked_equal argument.", "Input arrays to compare.", "Whether masked values in a and b are considered equal (True) or not (False). They are considered equal by default.", "Relative tolerance. The relative difference is equal to rtol * b. Default is 1e-5.", "Absolute tolerance. The absolute difference is equal to atol. Default is 1e-8.", "Returns True if the two arrays are equal within the given tolerance, False otherwise. If either array contains NaN, then False is returned.", "See also", "the non-masked allclose.", "If the following equation is element-wise True, then allclose returns True:", "Return True if all elements of a and b are equal subject to given tolerances.", "Masked values are not compared directly."]}, {"name": "ma.allequal()", "path": "reference/generated/numpy.ma.allequal", "type": "numpy.ma.allequal", "text": ["Return True if all entries of a and b are equal, using fill_value as a truth value where either or both are masked.", "Input arrays to compare.", "Whether masked values in a or b are considered equal (True) or not (False).", "Returns True if the two arrays are equal within the given tolerance, False otherwise. If either array contains NaN, then False is returned.", "See also"]}, {"name": "ma.anom()", "path": "reference/generated/numpy.ma.anom", "type": "numpy.ma.anom", "text": ["Compute the anomalies (deviations from the arithmetic mean) along the given axis.", "Returns an array of anomalies, with the same shape as the input and where the arithmetic mean is computed along the given axis.", "Axis over which the anomalies are taken. The default is to use the mean of the flattened array as reference.", "the default is float32; for arrays of float types it is the same as the array type.", "See also", "Compute the mean of the array."]}, {"name": "ma.anomalies()", "path": "reference/generated/numpy.ma.anomalies", "type": "numpy.ma.anomalies", "text": ["Compute the anomalies (deviations from the arithmetic mean) along the given axis.", "Returns an array of anomalies, with the same shape as the input and where the arithmetic mean is computed along the given axis.", "Axis over which the anomalies are taken. The default is to use the mean of the flattened array as reference.", "the default is float32; for arrays of float types it is the same as the array type.", "See also", "Compute the mean of the array."]}, {"name": "ma.any()", "path": "reference/generated/numpy.ma.any", "type": "numpy.ma.any", "text": ["Returns True if any of the elements of a evaluate to True.", "Masked values are considered as False during computation.", "Refer to numpy.any for full documentation.", "See also", "corresponding function for ndarrays", "equivalent function"]}, {"name": "ma.append()", "path": "reference/generated/numpy.ma.append", "type": "numpy.ma.append", "text": ["Append values to the end of an array.", "New in version 1.9.0.", "Values are appended to a copy of this array.", "These values are appended to a copy of a. It must be of the correct shape (the same shape as a, excluding axis). If axis is not specified, b can be any shape and will be flattened before use.", "The axis along which v are appended. If axis is not given, both a and b are flattened before use.", "A copy of a with b appended to axis. Note that append does not occur in-place: a new array is allocated and filled. If axis is None, the result is a flattened array.", "See also", "Equivalent function in the top-level NumPy module."]}, {"name": "ma.apply_along_axis()", "path": "reference/generated/numpy.ma.apply_along_axis", "type": "numpy.ma.apply_along_axis", "text": ["Apply a function to 1-D slices along the given axis.", "Execute func1d(a, *args, **kwargs) where func1d operates on 1-D arrays and a is a 1-D slice of arr along axis.", "This is equivalent to (but faster than) the following use of ndindex and s_, which sets each of ii, jj, and kk to a tuple of indices:", "Equivalently, eliminating the inner loop, this can be expressed as:", "This function should accept 1-D arrays. It is applied to 1-D slices of arr along the specified axis.", "Axis along which arr is sliced.", "Input array.", "Additional arguments to func1d.", "Additional named arguments to func1d.", "New in version 1.9.0.", "The output array. The shape of out is identical to the shape of arr, except along the axis dimension. This axis is removed, and replaced with new dimensions equal to the shape of the return value of func1d. So if func1d returns a scalar out will have one fewer dimensions than arr.", "See also", "Apply a function repeatedly over multiple axes.", "For a function that returns a 1D array, the number of dimensions in outarr is the same as arr.", "For a function that returns a higher dimensional array, those dimensions are inserted in place of the axis dimension."]}, {"name": "ma.apply_over_axes()", "path": "reference/generated/numpy.ma.apply_over_axes", "type": "numpy.ma.apply_over_axes", "text": ["Apply a function repeatedly over multiple axes.", "func is called as res = func(a, axis), where axis is the first element of axes. The result res of the function call must have either the same dimensions as a or one less dimension. If res has one less dimension than a, a dimension is inserted before axis. The call to func is then repeated for each axis in axes, with res as the first argument.", "This function must take two arguments, func(a, axis).", "Input array.", "Axes over which func is applied; the elements must be integers.", "The output array. The number of dimensions is the same as a, but the shape can be different. This depends on whether func changes the shape of its output with respect to its input.", "See also", "Apply a function to 1-D slices of an array along the given axis.", "Tuple axis arguments to ufuncs are equivalent:"]}, {"name": "ma.arange()", "path": "reference/generated/numpy.ma.arange", "type": "numpy.ma.arange", "text": ["Return evenly spaced values within a given interval.", "Values are generated within the half-open interval [start, stop) (in other words, the interval including start but excluding stop). For integer arguments the function is equivalent to the Python built-in range function, but returns an ndarray rather than a list.", "When using a non-integer step, such as 0.1, it is often better to use numpy.linspace. See the warnings section below for more information.", "Start of interval. The interval includes this value. The default start value is 0.", "End of interval. The interval does not include this value, except in some cases where step is not an integer and floating point round-off affects the length of out.", "Spacing between values. For any output out, this is the distance between two adjacent values, out[i+1] - out[i]. The default step size is 1. If step is specified as a position argument, start must also be given.", "The type of the output array. If dtype is not given, infer the data type from the other input arguments.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "Array of evenly spaced values.", "For floating point arguments, the length of the result is ceil((stop - start)/step). Because of floating point overflow, this rule may result in the last element of out being greater than stop.", "Warning", "The length of the output might not be numerically stable.", "Another stability issue is due to the internal implementation of numpy.arange. The actual step value used to populate the array is dtype(start + step) - dtype(start) and not step. Precision loss can occur here, due to casting or due to using floating points when start is much larger than step. This can lead to unexpected behaviour. For example:", "In such cases, the use of numpy.linspace should be preferred.", "See also", "Evenly spaced numbers with careful handling of endpoints.", "Arrays of evenly spaced numbers in N-dimensions.", "Grid-shaped arrays of evenly spaced numbers in N-dimensions."]}, {"name": "ma.argmax()", "path": "reference/generated/numpy.ma.argmax", "type": "numpy.ma.argmax", "text": ["Returns array of indices of the maximum values along the given axis. Masked values are treated as if they had the value fill_value.", "If None, the index is into the flattened array, otherwise along the specified axis", "Value used to fill in the masked values. If None, the output of maximum_fill_value(self._data) is used instead.", "Array into which the result can be placed. Its type is preserved and it must be of the right shape to hold the output."]}, {"name": "ma.argmin()", "path": "reference/generated/numpy.ma.argmin", "type": "numpy.ma.argmin", "text": ["Return array of indices to the minimum values along the given axis.", "If None, the index is into the flattened array, otherwise along the specified axis", "Value used to fill in the masked values. If None, the output of minimum_fill_value(self._data) is used instead.", "Array into which the result can be placed. Its type is preserved and it must be of the right shape to hold the output.", "If multi-dimension input, returns a new ndarray of indices to the minimum values along the given axis. Otherwise, returns a scalar of index to the minimum values along the given axis."]}, {"name": "ma.argsort()", "path": "reference/generated/numpy.ma.argsort", "type": "numpy.ma.argsort", "text": ["Return an ndarray of indices that sort the array along the specified axis. Masked values are filled beforehand to fill_value.", "Axis along which to sort. If None, the default, the flattened array is used.", "Changed in version 1.13.0: Previously, the default was documented to be -1, but that was in error. At some future date, the default will change to -1, as originally intended. Until then, the axis should be given explicitly when arr.ndim > 1, to avoid a FutureWarning.", "The sorting algorithm used.", "When a is an array with fields defined, this argument specifies which fields to compare first, second, etc. Not all fields need be specified.", "Whether missing values (if any) should be treated as the largest values (True) or the smallest values (False) When the array contains unmasked values at the same extremes of the datatype, the ordering of these values and the masked values is undefined.", "Value used internally for the masked values. If fill_value is not None, it supersedes endwith.", "Array of indices that sort a along the specified axis. In other words, a[index_array] yields a sorted a.", "See also", "Describes sorting algorithms used.", "Indirect stable sort with multiple keys.", "Inplace sort.", "See sort for notes on the different sorting algorithms."]}, {"name": "ma.around", "path": "reference/generated/numpy.ma.around", "type": "numpy.ma.around", "text": ["Round an array to the given number of decimals.", "See also", "equivalent function; see for details."]}, {"name": "ma.array()", "path": "reference/generated/numpy.ma.array", "type": "numpy.ma.array", "text": ["An array class with possibly masked values.", "Masked values of True exclude the corresponding element from any computation.", "Construction:", "Input data.", "Mask. Must be convertible to an array of booleans with the same shape as data. True indicates a masked (i.e. invalid) data.", "Data type of the output. If dtype is None, the type of the data argument (data.dtype) is used. If dtype is not None and different from data.dtype, a copy is performed.", "Whether to copy the input data (True), or to use a reference instead. Default is False.", "Whether to return a subclass of MaskedArray if possible (True) or a plain MaskedArray. Default is True.", "Minimum number of dimensions. Default is 0.", "Value used to fill in the masked values when necessary. If None, a default based on the data-type is used.", "Whether to combine mask with the mask of the input data, if any (True), or to use only mask for the output (False). Default is True.", "Whether to use a hard mask or not. With a hard mask, masked values cannot be unmasked. Default is False.", "Whether to force compression of an empty mask. Default is True.", "Specify the order of the array. If order is \u2018C\u2019, then the array will be in C-contiguous order (last-index varies the fastest). If order is \u2018F\u2019, then the returned array will be in Fortran-contiguous order (first-index varies the fastest). If order is \u2018A\u2019 (default), then the returned array may be in any order (either C-, Fortran-contiguous, or even discontiguous), unless a copy is required, in which case it will be C-contiguous.", "The mask can be initialized with an array of boolean values with the same shape as data.", "Alternatively, the mask can be initialized to homogeneous boolean array with the same shape as data by passing in a scalar boolean value:", "Note", "The recommended practice for initializing mask with a scalar boolean value is to use True/False rather than np.True_/np.False_. The reason is nomask is represented internally as np.False_."]}, {"name": "ma.asanyarray()", "path": "reference/generated/numpy.ma.asanyarray", "type": "numpy.ma.asanyarray", "text": ["Convert the input to a masked array, conserving subclasses.", "If a is a subclass of MaskedArray, its class is conserved. No copy is performed if the input is already an ndarray.", "Input data, in any form that can be converted to an array.", "By default, the data-type is inferred from the input data.", "Whether to use row-major (\u2018C\u2019) or column-major (\u2018FORTRAN\u2019) memory representation. Default is \u2018C\u2019.", "MaskedArray interpretation of a.", "See also", "Similar to asanyarray, but does not conserve subclass."]}, {"name": "ma.asarray()", "path": "reference/generated/numpy.ma.asarray", "type": "numpy.ma.asarray", "text": ["Convert the input to a masked array of the given data-type.", "No copy is performed if the input is already an ndarray. If a is a subclass of MaskedArray, a base class MaskedArray is returned.", "Input data, in any form that can be converted to a masked array. This includes lists, lists of tuples, tuples, tuples of tuples, tuples of lists, ndarrays and masked arrays.", "By default, the data-type is inferred from the input data.", "Whether to use row-major (\u2018C\u2019) or column-major (\u2018FORTRAN\u2019) memory representation. Default is \u2018C\u2019.", "Masked array interpretation of a.", "See also", "Similar to asarray, but conserves subclasses."]}, {"name": "ma.atleast_1d()", "path": "reference/generated/numpy.ma.atleast_1d", "type": "numpy.ma.atleast_1d", "text": ["Convert inputs to arrays with at least one dimension.", "Scalar inputs are converted to 1-dimensional arrays, whilst higher-dimensional inputs are preserved.", "One or more input arrays.", "An array, or list of arrays, each with a.ndim >= 1. Copies are made only if necessary.", "See also", "The function is applied to both the _data and the _mask, if any."]}, {"name": "ma.atleast_2d()", "path": "reference/generated/numpy.ma.atleast_2d", "type": "numpy.ma.atleast_2d", "text": ["View inputs as arrays with at least two dimensions.", "One or more array-like sequences. Non-array inputs are converted to arrays. Arrays that already have two or more dimensions are preserved.", "An array, or list of arrays, each with a.ndim >= 2. Copies are avoided where possible, and views with two or more dimensions are returned.", "See also", "The function is applied to both the _data and the _mask, if any."]}, {"name": "ma.atleast_3d()", "path": "reference/generated/numpy.ma.atleast_3d", "type": "numpy.ma.atleast_3d", "text": ["View inputs as arrays with at least three dimensions.", "One or more array-like sequences. Non-array inputs are converted to arrays. Arrays that already have three or more dimensions are preserved.", "An array, or list of arrays, each with a.ndim >= 3. Copies are avoided where possible, and views with three or more dimensions are returned. For example, a 1-D array of shape (N,) becomes a view of shape (1, N, 1), and a 2-D array of shape (M, N) becomes a view of shape (M, N, 1).", "See also", "The function is applied to both the _data and the _mask, if any."]}, {"name": "ma.average()", "path": "reference/generated/numpy.ma.average", "type": "numpy.ma.average", "text": ["Return the weighted average of array over the given axis.", "Data to be averaged. Masked entries are not taken into account in the computation.", "Axis along which to average a. If None, averaging is done over the flattened array.", "The importance that each element has in the computation of the average. The weights array can either be 1-D (in which case its length must be the size of a along the given axis) or of the same shape as a. If weights=None, then all data in a are assumed to have a weight equal to one. The 1-D calculation is:", "The only constraint on weights is that sum(weights) must not be 0.", "Flag indicating whether a tuple (result, sum of weights) should be returned as output (True), or just the result (False). Default is False.", "The average along the specified axis. When returned is True, return a tuple with the average as the first element and the sum of the weights as the second element. The return type is np.float64 if a is of integer type and floats smaller than float64, or the input data-type, otherwise. If returned, sum_of_weights is always float64."]}, {"name": "ma.choose()", "path": "reference/generated/numpy.ma.choose", "type": "numpy.ma.choose", "text": ["Use an index array to construct a new array from a list of choices.", "Given an array of integers and a list of n choice arrays, this method will create a new array that merges each of the choice arrays. Where a value in index is i, the new array will have the value that choices[i] contains in the same place.", "This array must contain integers in [0, n-1], where n is the number of choices.", "Choice arrays. The index array and all of the choices should be broadcastable to the same shape.", "If provided, the result will be inserted into this array. It should be of the appropriate shape and dtype.", "Specifies how out-of-bounds indices will behave.", "See also", "equivalent function"]}, {"name": "ma.clip()", "path": "reference/generated/numpy.ma.clip", "type": "numpy.ma.clip", "text": ["Clip (limit) the values in an array.", "Given an interval, values outside the interval are clipped to the interval edges. For example, if an interval of [0, 1] is specified, values smaller than 0 become 0, and values larger than 1 become 1.", "Equivalent to but faster than np.minimum(a_max, np.maximum(a, a_min)).", "No check is performed to ensure a_min < a_max.", "Array containing elements to clip.", "Minimum and maximum value. If None, clipping is not performed on the corresponding edge. Only one of a_min and a_max may be None. Both are broadcast against a.", "The results will be placed in this array. It may be the input array for in-place clipping. out must be of the right shape to hold the output. Its type is preserved.", "For other keyword-only arguments, see the ufunc docs.", "New in version 1.17.0.", "An array with the elements of a, but where values < a_min are replaced with a_min, and those > a_max with a_max.", "See also", "When a_min is greater than a_max, clip returns an array in which all values are equal to a_max, as shown in the second example."]}, {"name": "ma.clump_masked()", "path": "reference/generated/numpy.ma.clump_masked", "type": "numpy.ma.clump_masked", "text": ["Returns a list of slices corresponding to the masked clumps of a 1-D array. (A \u201cclump\u201d is defined as a contiguous region of the array).", "A one-dimensional masked array.", "The list of slices, one for each continuous region of masked elements in a.", "See also", "New in version 1.4.0."]}, {"name": "ma.clump_unmasked()", "path": "reference/generated/numpy.ma.clump_unmasked", "type": "numpy.ma.clump_unmasked", "text": ["Return list of slices corresponding to the unmasked clumps of a 1-D array. (A \u201cclump\u201d is defined as a contiguous region of the array).", "A one-dimensional masked array.", "The list of slices, one for each continuous region of unmasked elements in a.", "See also", "New in version 1.4.0."]}, {"name": "ma.column_stack()", "path": "reference/generated/numpy.ma.column_stack", "type": "numpy.ma.column_stack", "text": ["Stack 1-D arrays as columns into a 2-D array.", "Take a sequence of 1-D arrays and stack them as columns to make a single 2-D array. 2-D arrays are stacked as-is, just like with hstack. 1-D arrays are turned into 2-D columns first.", "Arrays to stack. All of them must have the same first dimension.", "The array formed by stacking the given arrays.", "See also", "The function is applied to both the _data and the _mask, if any."]}, {"name": "ma.common_fill_value()", "path": "reference/generated/numpy.ma.common_fill_value", "type": "numpy.ma.common_fill_value", "text": ["Return the common filling value of two masked arrays, if any.", "If a.fill_value == b.fill_value, return the fill value, otherwise return None.", "The masked arrays for which to compare fill values.", "The common fill value, or None."]}, {"name": "ma.compress_cols()", "path": "reference/generated/numpy.ma.compress_cols", "type": "numpy.ma.compress_cols", "text": ["Suppress whole columns of a 2-D array that contain masked values.", "This is equivalent to np.ma.compress_rowcols(a, 1), see compress_rowcols for details.", "See also"]}, {"name": "ma.compress_rowcols()", "path": "reference/generated/numpy.ma.compress_rowcols", "type": "numpy.ma.compress_rowcols", "text": ["Suppress the rows and/or columns of a 2-D array that contain masked values.", "The suppression behavior is selected with the axis parameter.", "The array to operate on. If not a MaskedArray instance (or if no array elements are masked), x is interpreted as a MaskedArray with mask set to nomask. Must be a 2D array.", "Axis along which to perform the operation. Default is None.", "The compressed array."]}, {"name": "ma.compress_rows()", "path": "reference/generated/numpy.ma.compress_rows", "type": "numpy.ma.compress_rows", "text": ["Suppress whole rows of a 2-D array that contain masked values.", "This is equivalent to np.ma.compress_rowcols(a, 0), see compress_rowcols for details.", "See also"]}, {"name": "ma.compressed()", "path": "reference/generated/numpy.ma.compressed", "type": "numpy.ma.compressed", "text": ["Return all the non-masked data as a 1-D array.", "This function is equivalent to calling the \u201ccompressed\u201d method of a ma.MaskedArray, see ma.MaskedArray.compressed for details.", "See also", "Equivalent method."]}, {"name": "ma.concatenate()", "path": "reference/generated/numpy.ma.concatenate", "type": "numpy.ma.concatenate", "text": ["Concatenate a sequence of arrays along the given axis.", "The arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).", "The axis along which the arrays will be joined. Default is 0.", "The concatenated array with any masked entries preserved.", "See also", "Equivalent function in the top-level NumPy module."]}, {"name": "ma.conjugate()", "path": "reference/generated/numpy.ma.conjugate", "type": "numpy.ma.conjugate", "text": ["Return the complex conjugate, element-wise.", "The complex conjugate of a complex number is obtained by changing the sign of its imaginary part.", "Input value.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The complex conjugate of x, with same dtype as y. This is a scalar if x is a scalar.", "conj is an alias for conjugate:"]}, {"name": "ma.copy()", "path": "reference/generated/numpy.ma.copy", "type": "numpy.ma.copy", "text": ["Return a copy of the array.", "Controls the memory layout of the copy. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of a as closely as possible. (Note that this function and numpy.copy are very similar but have different default values for their order= arguments, and this function always passes sub-classes through.)", "See also", "Similar function with different default behavior numpy.copyto", "This function is the preferred method for creating an array copy. The function numpy.copy is similar, but it defaults to using order \u2018K\u2019, and will not pass sub-classes through by default."]}, {"name": "ma.corrcoef()", "path": "reference/generated/numpy.ma.corrcoef", "type": "numpy.ma.corrcoef", "text": ["Return Pearson product-moment correlation coefficients.", "Except for the handling of missing data this function does the same as numpy.corrcoef. For more details and examples, see numpy.corrcoef.", "A 1-D or 2-D array containing multiple variables and observations. Each row of x represents a variable, and each column a single observation of all those variables. Also see rowvar below.", "An additional set of variables and observations. y has the same shape as x.", "If rowvar is True (default), then each row represents a variable, with observations in the columns. Otherwise, the relationship is transposed: each column represents a variable, while the rows contain observations.", "Has no effect, do not use.", "Deprecated since version 1.10.0.", "If True, masked values are propagated pair-wise: if a value is masked in x, the corresponding value is masked in y. If False, raises an exception. Because bias is deprecated, this argument needs to be treated as keyword only to avoid a warning.", "Has no effect, do not use.", "Deprecated since version 1.10.0.", "See also", "Equivalent function in top-level NumPy module.", "Estimate the covariance matrix.", "This function accepts but discards arguments bias and ddof. This is for backwards compatibility with previous versions of this function. These arguments had no effect on the return values of the function and can be safely ignored in this and previous versions of numpy."]}, {"name": "ma.count()", "path": "reference/generated/numpy.ma.count", "type": "numpy.ma.count", "text": ["Count the non-masked elements of the array along the given axis.", "Axis or axes along which the count is performed. The default, None, performs the count over all the dimensions of the input array. axis may be negative, in which case it counts from the last to the first axis.", "New in version 1.10.0.", "If this is a tuple of ints, the count is performed on multiple axes, instead of a single axis or all the axes as before.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the array.", "An array with the same shape as the input array, with the specified axis removed. If the array is a 0-d array, or if axis is None, a scalar is returned.", "See also", "Count masked elements in array or along a given axis.", "When the axis keyword is specified an array of appropriate size is returned."]}, {"name": "ma.count_masked()", "path": "reference/generated/numpy.ma.count_masked", "type": "numpy.ma.count_masked", "text": ["Count the number of masked elements along the given axis.", "An array with (possibly) masked elements.", "Axis along which to count. If None (default), a flattened version of the array is used.", "The total number of masked elements (axis=None) or the number of masked elements along each slice of the given axis.", "See also", "Count non-masked elements.", "When the axis keyword is used an array is returned."]}, {"name": "ma.cov()", "path": "reference/generated/numpy.ma.cov", "type": "numpy.ma.cov", "text": ["Estimate the covariance matrix.", "Except for the handling of missing data this function does the same as numpy.cov. For more details and examples, see numpy.cov.", "By default, masked values are recognized as such. If x and y have the same shape, a common mask is allocated: if x[i,j] is masked, then y[i,j] will also be masked. Setting allow_masked to False will raise an exception if values are missing in either of the input arrays.", "A 1-D or 2-D array containing multiple variables and observations. Each row of x represents a variable, and each column a single observation of all those variables. Also see rowvar below.", "An additional set of variables and observations. y has the same shape as x.", "If rowvar is True (default), then each row represents a variable, with observations in the columns. Otherwise, the relationship is transposed: each column represents a variable, while the rows contain observations.", "Default normalization (False) is by (N-1), where N is the number of observations given (unbiased estimate). If bias is True, then normalization is by N. This keyword can be overridden by the keyword ddof in numpy versions >= 1.5.", "If True, masked values are propagated pair-wise: if a value is masked in x, the corresponding value is masked in y. If False, raises a ValueError exception when some values are missing.", "If not None normalization is by (N - ddof), where N is the number of observations; this overrides the value implied by bias. The default value is None.", "New in version 1.5.", "Raised if some values are missing and allow_masked is False.", "See also"]}, {"name": "ma.cumprod()", "path": "reference/generated/numpy.ma.cumprod", "type": "numpy.ma.cumprod", "text": ["Return the cumulative product of the array elements over the given axis.", "Masked values are set to 1 internally during the computation. However, their position is saved, and the result will be masked at the same locations.", "Refer to numpy.cumprod for full documentation.", "See also", "corresponding function for ndarrays", "equivalent function", "The mask is lost if out is not a valid MaskedArray !", "Arithmetic is modular when using integer types, and no error is raised on overflow."]}, {"name": "ma.cumsum()", "path": "reference/generated/numpy.ma.cumsum", "type": "numpy.ma.cumsum", "text": ["Return the cumulative sum of the array elements over the given axis.", "Masked values are set to 0 internally during the computation. However, their position is saved, and the result will be masked at the same locations.", "Refer to numpy.cumsum for full documentation.", "See also", "corresponding function for ndarrays", "equivalent function", "The mask is lost if out is not a valid ma.MaskedArray !", "Arithmetic is modular when using integer types, and no error is raised on overflow."]}, {"name": "ma.default_fill_value()", "path": "reference/generated/numpy.ma.default_fill_value", "type": "numpy.ma.default_fill_value", "text": ["Return the default fill value for the argument object.", "The default filling value depends on the datatype of the input array or the type of the input scalar:", "datatype", "default", "bool", "True", "int", "999999", "float", "1.e20", "complex", "1.e20+0j", "object", "\u2018?\u2019", "string", "\u2018N/A\u2019", "For structured types, a structured scalar is returned, with each field the default fill value for its type.", "For subarray types, the fill value is an array of the same size containing the default scalar fill value.", "The array data-type or scalar for which the default fill value is returned.", "The default fill value."]}, {"name": "ma.diag()", "path": "reference/generated/numpy.ma.diag", "type": "numpy.ma.diag", "text": ["Extract a diagonal or construct a diagonal array.", "This function is the equivalent of numpy.diag that takes masked values into account, see numpy.diag for details.", "See also", "Equivalent function for ndarrays."]}, {"name": "ma.diff()", "path": "reference/generated/numpy.ma.diff", "type": "numpy.ma.diff", "text": ["Calculate the n-th discrete difference along the given axis.", "The first difference is given by out[i] = a[i+1] - a[i] along the given axis, higher differences are calculated by using diff recursively.", "Input array", "The number of times values are differenced. If zero, the input is returned as-is.", "The axis along which the difference is taken, default is the last axis.", "Values to prepend or append to a along axis prior to performing the difference. Scalar values are expanded to arrays with length 1 in the direction of axis and the shape of the input array in along all other axes. Otherwise the dimension and shape must match a except along axis.", "New in version 1.16.0.", "The n-th differences. The shape of the output is the same as a except along axis where the dimension is smaller by n. The type of the output is the same as the type of the difference between any two elements of a. This is the same as the type of a in most cases. A notable exception is datetime64, which results in a timedelta64 output array.", "See also", "Type is preserved for boolean arrays, so the result will contain False when consecutive elements are the same and True when they differ.", "For unsigned integer arrays, the results will also be unsigned. This should not be surprising, as the result is consistent with calculating the difference directly:", "If this is not desirable, then the array should be cast to a larger integer type first:"]}, {"name": "ma.dot()", "path": "reference/generated/numpy.ma.dot", "type": "numpy.ma.dot", "text": ["Return the dot product of two arrays.", "This function is the equivalent of numpy.dot that takes masked values into account. Note that strict and out are in different position than in the method version. In order to maintain compatibility with the corresponding method, it is recommended that the optional arguments be treated as keyword only. At some point that may be mandatory.", "Note", "Works only with 2-D arrays at the moment.", "Inputs arrays.", "Whether masked data are propagated (True) or set to 0 (False) for the computation. Default is False. Propagating the mask means that if a masked value appears in a row or column, the whole row or column is considered masked.", "Output argument. This must have the exact kind that would be returned if it was not used. In particular, it must have the right type, must be C-contiguous, and its dtype must be the dtype that would be returned for dot(a,b). This is a performance feature. Therefore, if these conditions are not met, an exception is raised, instead of attempting to be flexible.", "New in version 1.10.2.", "See also", "Equivalent function for ndarrays."]}, {"name": "ma.dstack()", "path": "reference/generated/numpy.ma.dstack", "type": "numpy.ma.dstack", "text": ["Stack arrays in sequence depth wise (along third axis).", "This is equivalent to concatenation along the third axis after 2-D arrays of shape (M,N) have been reshaped to (M,N,1) and 1-D arrays of shape (N,) have been reshaped to (1,N,1). Rebuilds arrays divided by dsplit.", "This function makes most sense for arrays with up to 3 dimensions. For instance, for pixel-data with a height (first axis), width (second axis), and r/g/b channels (third axis). The functions concatenate, stack and block provide more general stacking and concatenation operations.", "The arrays must have the same shape along all but the third axis. 1-D or 2-D arrays must have the same shape.", "The array formed by stacking the given arrays, will be at least 3-D.", "See also", "Join a sequence of arrays along an existing axis.", "Join a sequence of arrays along a new axis.", "Assemble an nd-array from nested lists of blocks.", "Stack arrays in sequence vertically (row wise).", "Stack arrays in sequence horizontally (column wise).", "Stack 1-D arrays as columns into a 2-D array.", "Split array along third axis.", "The function is applied to both the _data and the _mask, if any."]}, {"name": "ma.ediff1d()", "path": "reference/generated/numpy.ma.ediff1d", "type": "numpy.ma.ediff1d", "text": ["Compute the differences between consecutive elements of an array.", "This function is the equivalent of numpy.ediff1d that takes masked values into account, see numpy.ediff1d for details.", "See also", "Equivalent function for ndarrays."]}, {"name": "ma.empty()", "path": "reference/generated/numpy.ma.empty", "type": "numpy.ma.empty", "text": ["Return a new array of given shape and type, without initializing entries.", "Shape of the empty array, e.g., (2, 3) or 2.", "Desired output data-type for the array, e.g, numpy.int8. Default is numpy.float64.", "Whether to store multi-dimensional data in row-major (C-style) or column-major (Fortran-style) order in memory.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "Array of uninitialized (arbitrary) data of the given shape, dtype, and order. Object arrays will be initialized to None.", "See also", "Return an empty array with shape and type of input.", "Return a new array setting values to one.", "Return a new array setting values to zero.", "Return a new array of given shape filled with value.", "empty, unlike zeros, does not set the array values to zero, and may therefore be marginally faster. On the other hand, it requires the user to manually set all the values in the array, and should be used with caution."]}, {"name": "ma.empty_like()", "path": "reference/generated/numpy.ma.empty_like", "type": "numpy.ma.empty_like", "text": ["Return a new array with the same shape and type as a given array.", "The shape and data-type of prototype define these same attributes of the returned array.", "Overrides the data type of the result.", "New in version 1.6.0.", "Overrides the memory layout of the result. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if prototype is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of prototype as closely as possible.", "New in version 1.6.0.", "If True, then the newly created array will use the sub-class type of prototype, otherwise it will be a base-class array. Defaults to True.", "Overrides the shape of the result. If order=\u2019K\u2019 and the number of dimensions is unchanged, will try to keep order, otherwise, order=\u2019C\u2019 is implied.", "New in version 1.17.0.", "Array of uninitialized (arbitrary) data with the same shape and type as prototype.", "See also", "Return an array of ones with shape and type of input.", "Return an array of zeros with shape and type of input.", "Return a new array with shape of input filled with value.", "Return a new uninitialized array.", "This function does not initialize the returned array; to do that use zeros_like or ones_like instead. It may be marginally faster than the functions that do set the array values."]}, {"name": "ma.expand_dims()", "path": "reference/generated/numpy.ma.expand_dims", "type": "numpy.ma.expand_dims", "text": ["Expand the shape of an array.", "Insert a new axis that will appear at the axis position in the expanded array shape.", "Input array.", "Position in the expanded axes where the new axis (or axes) is placed.", "Deprecated since version 1.13.0: Passing an axis where axis > a.ndim will be treated as axis == a.ndim, and passing axis < -a.ndim - 1 will be treated as axis == 0. This behavior is deprecated.", "Changed in version 1.18.0: A tuple of axes is now supported. Out of range axes as described above are now forbidden and raise an AxisError.", "View of a with the number of dimensions increased.", "See also", "The inverse operation, removing singleton dimensions", "Insert, remove, and combine dimensions, and resize existing ones", "The following is equivalent to x[np.newaxis, :] or x[np.newaxis]:", "The following is equivalent to x[:, np.newaxis]:", "axis may also be a tuple:", "Note that some examples may use None instead of np.newaxis. These are the same objects:"]}, {"name": "ma.filled()", "path": "reference/generated/numpy.ma.filled", "type": "numpy.ma.filled", "text": ["Return input as an array with masked data replaced by a fill value.", "If a is not a MaskedArray, a itself is returned. If a is a MaskedArray and fill_value is None, fill_value is set to a.fill_value.", "An input object.", "Can be scalar or non-scalar. If non-scalar, the resulting filled array should be broadcastable over input array. Default is None.", "The filled array.", "See also"]}, {"name": "ma.fix_invalid()", "path": "reference/generated/numpy.ma.fix_invalid", "type": "numpy.ma.fix_invalid", "text": ["Return input with invalid data masked and replaced by a fill value.", "Invalid data means values of nan, inf, etc.", "Input array, a (subclass of) ndarray.", "Mask. Must be convertible to an array of booleans with the same shape as data. True indicates a masked (i.e. invalid) data.", "Whether to use a copy of a (True) or to fix a in place (False). Default is True.", "Value used for fixing invalid data. Default is None, in which case the a.fill_value is used.", "The input array with invalid entries fixed.", "A copy is performed by default."]}, {"name": "ma.flatnotmasked_contiguous()", "path": "reference/generated/numpy.ma.flatnotmasked_contiguous", "type": "numpy.ma.flatnotmasked_contiguous", "text": ["Find contiguous unmasked data in a masked array along the given axis.", "The input array.", "A sorted sequence of slice objects (start index, end index).", "Changed in version 1.15.0: Now returns an empty list instead of None for a fully masked array", "See also", "Only accepts 2-D arrays at most."]}, {"name": "ma.flatnotmasked_edges()", "path": "reference/generated/numpy.ma.flatnotmasked_edges", "type": "numpy.ma.flatnotmasked_edges", "text": ["Find the indices of the first and last unmasked values.", "Expects a 1-D MaskedArray, returns None if all values are masked.", "Input 1-D MaskedArray", "The indices of first and last non-masked value in the array. Returns None if all values are masked.", "See also", "Only accepts 1-D arrays."]}, {"name": "ma.frombuffer()", "path": "reference/generated/numpy.ma.frombuffer", "type": "numpy.ma.frombuffer", "text": ["Interpret a buffer as a 1-dimensional array.", "An object that exposes the buffer interface.", "Data-type of the returned array; default: float.", "Number of items to read. -1 means all data in the buffer.", "Start reading the buffer from this offset (in bytes); default: 0.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "If the buffer has data that is not in machine byte-order, this should be specified as part of the data-type, e.g.:", "The data of the resulting array will not be byteswapped, but will be interpreted correctly."]}, {"name": "ma.fromfunction()", "path": "reference/generated/numpy.ma.fromfunction", "type": "numpy.ma.fromfunction", "text": ["Construct an array by executing a function over each coordinate.", "The resulting array therefore has a value fn(x, y, z) at coordinate (x, y, z).", "The function is called with N parameters, where N is the rank of shape. Each parameter represents the coordinates of the array varying along a specific axis. For example, if shape were (2, 2), then the parameters would be array([[0, 0], [1, 1]]) and array([[0, 1], [0, 1]])", "Shape of the output array, which also determines the shape of the coordinate arrays passed to function.", "Data-type of the coordinate arrays passed to function. By default, dtype is float.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "The result of the call to function is passed back directly. Therefore the shape of fromfunction is completely determined by function. If function returns a scalar value, the shape of fromfunction would not match the shape parameter.", "See also", "Keywords other than dtype are passed to function."]}, {"name": "ma.getdata()", "path": "reference/generated/numpy.ma.getdata", "type": "numpy.ma.getdata", "text": ["Return the data of a masked array as an ndarray.", "Return the data of a (if any) as an ndarray if a is a MaskedArray, else return a as a ndarray or subclass (depending on subok) if not.", "Input MaskedArray, alternatively a ndarray or a subclass thereof.", "Whether to force the output to be a pure ndarray (False) or to return a subclass of ndarray if appropriate (True, default).", "See also", "Return the mask of a masked array, or nomask.", "Return the mask of a masked array, or full array of False.", "Equivalently use the MaskedArray data attribute."]}, {"name": "ma.getmask()", "path": "reference/generated/numpy.ma.getmask", "type": "numpy.ma.getmask", "text": ["Return the mask of a masked array, or nomask.", "Return the mask of a as an ndarray if a is a MaskedArray and the mask is not nomask, else return nomask. To guarantee a full array of booleans of the same shape as a, use getmaskarray.", "Input MaskedArray for which the mask is required.", "See also", "Return the data of a masked array as an ndarray.", "Return the mask of a masked array, or full array of False.", "Equivalently use the MaskedArray mask attribute.", "Result when mask == nomask"]}, {"name": "ma.getmaskarray()", "path": "reference/generated/numpy.ma.getmaskarray", "type": "numpy.ma.getmaskarray", "text": ["Return the mask of a masked array, or full boolean array of False.", "Return the mask of arr as an ndarray if arr is a MaskedArray and the mask is not nomask, else return a full boolean array of False of the same shape as arr.", "Input MaskedArray for which the mask is required.", "See also", "Return the mask of a masked array, or nomask.", "Return the data of a masked array as an ndarray.", "Result when mask == nomask"]}, {"name": "ma.harden_mask()", "path": "reference/generated/numpy.ma.harden_mask", "type": "numpy.ma.harden_mask", "text": ["Force the mask to hard.", "Whether the mask of a masked array is hard or soft is determined by its hardmask property. harden_mask sets hardmask to True.", "See also"]}, {"name": "ma.hsplit()", "path": "reference/generated/numpy.ma.hsplit", "type": "numpy.ma.hsplit", "text": ["Split an array into multiple sub-arrays horizontally (column-wise).", "Please refer to the split documentation. hsplit is equivalent to split with axis=1, the array is always split along the second axis regardless of the array dimension.", "See also", "Split an array into multiple sub-arrays of equal size.", "The function is applied to both the _data and the _mask, if any.", "With a higher dimensional array the split is still along the second axis."]}, {"name": "ma.hstack()", "path": "reference/generated/numpy.ma.hstack", "type": "numpy.ma.hstack", "text": ["Stack arrays in sequence horizontally (column wise).", "This is equivalent to concatenation along the second axis, except for 1-D arrays where it concatenates along the first axis. Rebuilds arrays divided by hsplit.", "This function makes most sense for arrays with up to 3 dimensions. For instance, for pixel-data with a height (first axis), width (second axis), and r/g/b channels (third axis). The functions concatenate, stack and block provide more general stacking and concatenation operations.", "The arrays must have the same shape along all but the second axis, except 1-D arrays which can be any length.", "The array formed by stacking the given arrays.", "See also", "Join a sequence of arrays along an existing axis.", "Join a sequence of arrays along a new axis.", "Assemble an nd-array from nested lists of blocks.", "Stack arrays in sequence vertically (row wise).", "Stack arrays in sequence depth wise (along third axis).", "Stack 1-D arrays as columns into a 2-D array.", "Split an array into multiple sub-arrays horizontally (column-wise).", "The function is applied to both the _data and the _mask, if any."]}, {"name": "ma.identity()", "path": "reference/generated/numpy.ma.identity", "type": "numpy.ma.identity", "text": ["Return the identity array.", "The identity array is a square array with ones on the main diagonal.", "Number of rows (and columns) in n x n output.", "Data-type of the output. Defaults to float.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "n x n array with its main diagonal set to one, and all other elements 0."]}, {"name": "ma.indices()", "path": "reference/generated/numpy.ma.indices", "type": "numpy.ma.indices", "text": ["Return an array representing the indices of a grid.", "Compute an array where the subarrays contain index values 0, 1, \u2026 varying only along the corresponding axis.", "The shape of the grid.", "Data type of the result.", "Return a sparse representation of the grid instead of a dense representation. Default is False.", "New in version 1.17.", "Returns one array of grid indices, grid.shape = (len(dimensions),) + tuple(dimensions).", "Returns a tuple of arrays, with grid[i].shape = (1, ..., 1, dimensions[i], 1, ..., 1) with dimensions[i] in the ith place", "See also", "The output shape in the dense case is obtained by prepending the number of dimensions in front of the tuple of dimensions, i.e. if dimensions is a tuple (r0, ..., rN-1) of length N, the output shape is (N, r0, ..., rN-1).", "The subarrays grid[k] contains the N-D array of indices along the k-th axis. Explicitly:", "The indices can be used as an index into an array.", "Note that it would be more straightforward in the above example to extract the required elements directly with x[:2, :3].", "If sparse is set to true, the grid will be returned in a sparse representation."]}, {"name": "ma.inner()", "path": "reference/generated/numpy.ma.inner", "type": "numpy.ma.inner", "text": ["Inner product of two arrays.", "Ordinary inner product of vectors for 1-D arrays (without complex conjugation), in higher dimensions a sum product over the last axes.", "If a and b are nonscalar, their last dimensions must match.", "If a and b are both scalars or both 1-D arrays then a scalar is returned; otherwise an array is returned. out.shape = (*a.shape[:-1], *b.shape[:-1])", "If both a and b are nonscalar and their last dimensions have different sizes.", "See also", "Sum products over arbitrary axes.", "Generalised matrix product, using second last dimension of b.", "Einstein summation convention.", "Masked values are replaced by 0.", "For vectors (1-D arrays) it computes the ordinary inner-product:", "More generally, if ndim(a) = r > 0 and ndim(b) = s > 0:", "or explicitly:", "In addition a or b may be scalars, in which case:", "Ordinary inner product for vectors:", "Some multidimensional examples:", "An example where b is a scalar:"]}, {"name": "ma.innerproduct()", "path": "reference/generated/numpy.ma.innerproduct", "type": "numpy.ma.innerproduct", "text": ["Inner product of two arrays.", "Ordinary inner product of vectors for 1-D arrays (without complex conjugation), in higher dimensions a sum product over the last axes.", "If a and b are nonscalar, their last dimensions must match.", "If a and b are both scalars or both 1-D arrays then a scalar is returned; otherwise an array is returned. out.shape = (*a.shape[:-1], *b.shape[:-1])", "If both a and b are nonscalar and their last dimensions have different sizes.", "See also", "Sum products over arbitrary axes.", "Generalised matrix product, using second last dimension of b.", "Einstein summation convention.", "Masked values are replaced by 0.", "For vectors (1-D arrays) it computes the ordinary inner-product:", "More generally, if ndim(a) = r > 0 and ndim(b) = s > 0:", "or explicitly:", "In addition a or b may be scalars, in which case:", "Ordinary inner product for vectors:", "Some multidimensional examples:", "An example where b is a scalar:"]}, {"name": "ma.is_mask()", "path": "reference/generated/numpy.ma.is_mask", "type": "numpy.ma.is_mask", "text": ["Return True if m is a valid, standard mask.", "This function does not check the contents of the input, only that the type is MaskType. In particular, this function returns False if the mask has a flexible dtype.", "Array to test.", "True if m.dtype.type is MaskType, False otherwise.", "See also", "Test whether input is an instance of MaskedArray.", "Input must be an ndarray (or have similar attributes) for it to be considered a valid mask.", "Arrays with complex dtypes don\u2019t return True."]}, {"name": "ma.is_masked()", "path": "reference/generated/numpy.ma.is_masked", "type": "numpy.ma.is_masked", "text": ["Determine whether input has masked values.", "Accepts any object as input, but always returns False unless the input is a MaskedArray containing masked values.", "Array to check for masked values.", "True if x is a MaskedArray with masked values, False otherwise.", "Always returns False if x isn\u2019t a MaskedArray."]}, {"name": "ma.isarray()", "path": "reference/generated/numpy.ma.isarray", "type": "numpy.ma.isarray", "text": ["Test whether input is an instance of MaskedArray.", "This function returns True if x is an instance of MaskedArray and returns False otherwise. Any object is accepted as input.", "Object to test.", "True if x is a MaskedArray.", "See also", "Alias to isMaskedArray.", "Alias to isMaskedArray."]}, {"name": "ma.isMA()", "path": "reference/generated/numpy.ma.isma", "type": "numpy.ma.isMA", "text": ["Test whether input is an instance of MaskedArray.", "This function returns True if x is an instance of MaskedArray and returns False otherwise. Any object is accepted as input.", "Object to test.", "True if x is a MaskedArray.", "See also", "Alias to isMaskedArray.", "Alias to isMaskedArray."]}, {"name": "ma.isMaskedArray()", "path": "reference/generated/numpy.ma.ismaskedarray", "type": "numpy.ma.isMaskedArray", "text": ["Test whether input is an instance of MaskedArray.", "This function returns True if x is an instance of MaskedArray and returns False otherwise. Any object is accepted as input.", "Object to test.", "True if x is a MaskedArray.", "See also", "Alias to isMaskedArray.", "Alias to isMaskedArray."]}, {"name": "ma.make_mask()", "path": "reference/generated/numpy.ma.make_mask", "type": "numpy.ma.make_mask", "text": ["Create a boolean mask from an array.", "Return m as a boolean mask, creating a copy if necessary or requested. The function can accept any sequence that is convertible to integers, or nomask. Does not require that contents must be 0s and 1s, values of 0 are interpreted as False, everything else as True.", "Potential mask.", "Whether to return a copy of m (True) or m itself (False).", "Whether to shrink m to nomask if all its values are False.", "Data-type of the output mask. By default, the output mask has a dtype of MaskType (bool). If the dtype is flexible, each field has a boolean dtype. This is ignored when m is nomask, in which case nomask is always returned.", "A boolean mask derived from m.", "Effect of the shrink parameter.", "Using a flexible dtype."]}, {"name": "ma.make_mask_descr()", "path": "reference/generated/numpy.ma.make_mask_descr", "type": "numpy.ma.make_mask_descr", "text": ["Construct a dtype description list from a given dtype.", "Returns a new dtype object, with the type of all fields in ndtype to a boolean type. Field names are not altered.", "The dtype to convert.", "A dtype that looks like ndtype, the type of all fields is boolean."]}, {"name": "ma.make_mask_none()", "path": "reference/generated/numpy.ma.make_mask_none", "type": "numpy.ma.make_mask_none", "text": ["Return a boolean mask of the given shape, filled with False.", "This function returns a boolean ndarray with all entries False, that can be used in common mask manipulations. If a complex dtype is specified, the type of each field is converted to a boolean type.", "A tuple indicating the shape of the mask.", "If None, use a MaskType instance. Otherwise, use a new datatype with the same fields as dtype, converted to boolean types.", "An ndarray of appropriate shape and dtype, filled with False.", "See also", "Create a boolean mask from an array.", "Construct a dtype description list from a given dtype.", "Defining a more complex dtype."]}, {"name": "ma.mask_cols()", "path": "reference/generated/numpy.ma.mask_cols", "type": "numpy.ma.mask_cols", "text": ["Mask columns of a 2D array that contain masked values.", "This function is a shortcut to mask_rowcols with axis equal to 1.", "See also", "Mask rows and/or columns of a 2D array.", "Mask where a condition is met."]}, {"name": "ma.mask_or()", "path": "reference/generated/numpy.ma.mask_or", "type": "numpy.ma.mask_or", "text": ["Combine two masks with the logical_or operator.", "The result may be a view on m1 or m2 if the other is nomask (i.e. False).", "Input masks.", "If copy is False and one of the inputs is nomask, return a view of the other input mask. Defaults to False.", "Whether to shrink the output to nomask if all its values are False. Defaults to True.", "The result masks values that are masked in either m1 or m2.", "If m1 and m2 have different flexible dtypes."]}, {"name": "ma.mask_rowcols()", "path": "reference/generated/numpy.ma.mask_rowcols", "type": "numpy.ma.mask_rowcols", "text": ["Mask rows and/or columns of a 2D array that contain masked values.", "Mask whole rows and/or columns of a 2D array that contain masked values. The masking behavior is selected using the axis parameter.", "The array to mask. If not a MaskedArray instance (or if no array elements are masked). The result is a MaskedArray with mask set to nomask (False). Must be a 2D array.", "Axis along which to perform the operation. If None, applies to a flattened version of the array.", "A modified version of the input array, masked depending on the value of the axis parameter.", "If input array a is not 2D.", "See also", "Mask rows of a 2D array that contain masked values.", "Mask cols of a 2D array that contain masked values.", "Mask where a condition is met.", "The input array\u2019s mask is modified by this function."]}, {"name": "ma.mask_rows()", "path": "reference/generated/numpy.ma.mask_rows", "type": "numpy.ma.mask_rows", "text": ["Mask rows of a 2D array that contain masked values.", "This function is a shortcut to mask_rowcols with axis equal to 0.", "See also", "Mask rows and/or columns of a 2D array.", "Mask where a condition is met."]}, {"name": "ma.masked_all()", "path": "reference/generated/numpy.ma.masked_all", "type": "numpy.ma.masked_all", "text": ["Empty masked array with all elements masked.", "Return an empty masked array of the given shape and dtype, where all the data are masked.", "Shape of the required MaskedArray.", "Data type of the output.", "A masked array with all data masked.", "See also", "Empty masked array modelled on an existing array.", "The dtype parameter defines the underlying data type."]}, {"name": "ma.masked_all_like()", "path": "reference/generated/numpy.ma.masked_all_like", "type": "numpy.ma.masked_all_like", "text": ["Empty masked array with the properties of an existing array.", "Return an empty masked array of the same shape and dtype as the array arr, where all the data are masked.", "An array describing the shape and dtype of the required MaskedArray.", "A masked array with all data masked.", "If arr doesn\u2019t have a shape attribute (i.e. not an ndarray)", "See also", "Empty masked array with all elements masked.", "The dtype of the masked array matches the dtype of arr."]}, {"name": "ma.masked_equal()", "path": "reference/generated/numpy.ma.masked_equal", "type": "numpy.ma.masked_equal", "text": ["Mask an array where equal to a given value.", "This function is a shortcut to masked_where, with condition = (x == value). For floating point arrays, consider using masked_values(x, value).", "See also", "Mask where a condition is met.", "Mask using floating point equality."]}, {"name": "ma.masked_greater()", "path": "reference/generated/numpy.ma.masked_greater", "type": "numpy.ma.masked_greater", "text": ["Mask an array where greater than a given value.", "This function is a shortcut to masked_where, with condition = (x > value).", "See also", "Mask where a condition is met."]}, {"name": "ma.masked_greater_equal()", "path": "reference/generated/numpy.ma.masked_greater_equal", "type": "numpy.ma.masked_greater_equal", "text": ["Mask an array where greater than or equal to a given value.", "This function is a shortcut to masked_where, with condition = (x >= value).", "See also", "Mask where a condition is met."]}, {"name": "ma.masked_inside()", "path": "reference/generated/numpy.ma.masked_inside", "type": "numpy.ma.masked_inside", "text": ["Mask an array inside a given interval.", "Shortcut to masked_where, where condition is True for x inside the interval [v1,v2] (v1 <= x <= v2). The boundaries v1 and v2 can be given in either order.", "See also", "Mask where a condition is met.", "The array x is prefilled with its filling value.", "The order of v1 and v2 doesn\u2019t matter."]}, {"name": "ma.masked_invalid()", "path": "reference/generated/numpy.ma.masked_invalid", "type": "numpy.ma.masked_invalid", "text": ["Mask an array where invalid values occur (NaNs or infs).", "This function is a shortcut to masked_where, with condition = ~(np.isfinite(a)). Any pre-existing mask is conserved. Only applies to arrays with a dtype where NaNs or infs make sense (i.e. floating point types), but accepts any array_like object.", "See also", "Mask where a condition is met."]}, {"name": "ma.masked_less()", "path": "reference/generated/numpy.ma.masked_less", "type": "numpy.ma.masked_less", "text": ["Mask an array where less than a given value.", "This function is a shortcut to masked_where, with condition = (x < value).", "See also", "Mask where a condition is met."]}, {"name": "ma.masked_less_equal()", "path": "reference/generated/numpy.ma.masked_less_equal", "type": "numpy.ma.masked_less_equal", "text": ["Mask an array where less than or equal to a given value.", "This function is a shortcut to masked_where, with condition = (x <= value).", "See also", "Mask where a condition is met."]}, {"name": "ma.masked_not_equal()", "path": "reference/generated/numpy.ma.masked_not_equal", "type": "numpy.ma.masked_not_equal", "text": ["Mask an array where not equal to a given value.", "This function is a shortcut to masked_where, with condition = (x != value).", "See also", "Mask where a condition is met."]}, {"name": "ma.masked_object()", "path": "reference/generated/numpy.ma.masked_object", "type": "numpy.ma.masked_object", "text": ["Mask the array x where the data are exactly equal to value.", "This function is similar to masked_values, but only suitable for object arrays: for floating point, use masked_values instead.", "Array to mask", "Comparison value", "Whether to return a copy of x.", "Whether to collapse a mask full of False to nomask", "The result of masking x where equal to value.", "See also", "Mask where a condition is met.", "Mask where equal to a given value (integers).", "Mask using floating point equality.", "Note that mask is set to nomask if possible."]}, {"name": "ma.masked_outside()", "path": "reference/generated/numpy.ma.masked_outside", "type": "numpy.ma.masked_outside", "text": ["Mask an array outside a given interval.", "Shortcut to masked_where, where condition is True for x outside the interval [v1,v2] (x < v1)|(x > v2). The boundaries v1 and v2 can be given in either order.", "See also", "Mask where a condition is met.", "The array x is prefilled with its filling value.", "The order of v1 and v2 doesn\u2019t matter."]}, {"name": "ma.masked_values()", "path": "reference/generated/numpy.ma.masked_values", "type": "numpy.ma.masked_values", "text": ["Mask using floating point equality.", "Return a MaskedArray, masked where the data in array x are approximately equal to value, determined using isclose. The default tolerances for masked_values are the same as those for isclose.", "For integer types, exact equality is used, in the same way as masked_equal.", "The fill_value is set to value and the mask is set to nomask if possible.", "Array to mask.", "Masking value.", "Tolerance parameters passed on to isclose", "Whether to return a copy of x.", "Whether to collapse a mask full of False to nomask.", "The result of masking x where approximately equal to value.", "See also", "Mask where a condition is met.", "Mask where equal to a given value (integers).", "Note that mask is set to nomask if possible.", "For integers, the fill value will be different in general to the result of masked_equal."]}, {"name": "ma.masked_where()", "path": "reference/generated/numpy.ma.masked_where", "type": "numpy.ma.masked_where", "text": ["Mask an array where a condition is met.", "Return a as an array masked where condition is True. Any masked values of a or condition are also masked in the output.", "Masking condition. When condition tests floating point values for equality, consider using masked_values instead.", "Array to mask.", "If True (default) make a copy of a in the result. If False modify a in place and return a view.", "The result of masking a where condition is True.", "See also", "Mask using floating point equality.", "Mask where equal to a given value.", "Mask where not equal to a given value.", "Mask where less than or equal to a given value.", "Mask where greater than or equal to a given value.", "Mask where less than a given value.", "Mask where greater than a given value.", "Mask inside a given interval.", "Mask outside a given interval.", "Mask invalid values (NaNs or infs).", "Mask array b conditional on a.", "Effect of the copy argument.", "When condition or a contain masked values."]}, {"name": "ma.MaskedArray.__abs__()", "path": "reference/generated/numpy.ma.maskedarray.__abs__", "type": "Masked arrays", "text": ["method"]}, {"name": "ma.MaskedArray.__add__()", "path": "reference/generated/numpy.ma.maskedarray.__add__", "type": "Masked arrays", "text": ["method", "Add self to other, and return a new masked array."]}, {"name": "ma.MaskedArray.__and__()", "path": "reference/generated/numpy.ma.maskedarray.__and__", "type": "Masked arrays", "text": ["method", "Return self&value."]}, {"name": "ma.MaskedArray.__array__()", "path": "reference/generated/numpy.ma.maskedarray.__array__", "type": "Masked arrays", "text": ["method", "Returns either a new reference to self if dtype is not given or a new array of provided data type if dtype is different from the current dtype of the array."]}, {"name": "ma.MaskedArray.__array_priority__", "path": "reference/generated/numpy.ma.maskedarray.__array_priority__", "type": "Masked arrays", "text": ["attribute"]}, {"name": "ma.MaskedArray.__array_wrap__()", "path": "reference/generated/numpy.ma.maskedarray.__array_wrap__", "type": "Masked arrays", "text": ["method", "Special hook for ufuncs.", "Wraps the numpy array and sets the mask according to context."]}, {"name": "ma.MaskedArray.__bool__()", "path": "reference/generated/numpy.ma.maskedarray.__bool__", "type": "Masked arrays", "text": ["method", "self != 0"]}, {"name": "ma.MaskedArray.__contains__()", "path": "reference/generated/numpy.ma.maskedarray.__contains__", "type": "Masked arrays", "text": ["method", "Return key in self."]}, {"name": "ma.MaskedArray.__copy__()", "path": "reference/generated/numpy.ma.maskedarray.__copy__", "type": "Masked arrays", "text": ["method", "Used if copy.copy is called on an array. Returns a copy of the array.", "Equivalent to a.copy(order='K')."]}, {"name": "ma.MaskedArray.__deepcopy__()", "path": "reference/generated/numpy.ma.maskedarray.__deepcopy__", "type": "Masked arrays", "text": ["method", "Used if copy.deepcopy is called on an array."]}, {"name": "ma.MaskedArray.__delitem__()", "path": "reference/generated/numpy.ma.maskedarray.__delitem__", "type": "Masked arrays", "text": ["method", "Delete self[key]."]}, {"name": "ma.MaskedArray.__div__()", "path": "reference/generated/numpy.ma.maskedarray.__div__", "type": "Masked arrays", "text": ["method", "Divide other into self, and return a new masked array."]}, {"name": "ma.MaskedArray.__divmod__()", "path": "reference/generated/numpy.ma.maskedarray.__divmod__", "type": "Masked arrays", "text": ["method", "Return divmod(self, value)."]}, {"name": "ma.MaskedArray.__eq__()", "path": "reference/generated/numpy.ma.maskedarray.__eq__", "type": "Masked arrays", "text": ["method", "Check whether other equals self elementwise.", "When either of the elements is masked, the result is masked as well, but the underlying boolean data are still set, with self and other considered equal if both are masked, and unequal otherwise.", "For structured arrays, all fields are combined, with masked values ignored. The result is masked if all fields were masked, with self and other considered equal only if both were fully masked."]}, {"name": "ma.MaskedArray.__float__()", "path": "reference/generated/numpy.ma.maskedarray.__float__", "type": "Masked arrays", "text": ["method", "Convert to float."]}, {"name": "ma.MaskedArray.__floordiv__()", "path": "reference/generated/numpy.ma.maskedarray.__floordiv__", "type": "Masked arrays", "text": ["method", "Divide other into self, and return a new masked array."]}, {"name": "ma.MaskedArray.__ge__()", "path": "reference/generated/numpy.ma.maskedarray.__ge__", "type": "Masked arrays", "text": ["method", "Return self>=value."]}, {"name": "ma.MaskedArray.__getitem__()", "path": "reference/generated/numpy.ma.maskedarray.__getitem__", "type": "Masked arrays", "text": ["method", "x.__getitem__(y) <==> x[y]", "Return the item described by i, as a masked array."]}, {"name": "ma.MaskedArray.__getstate__()", "path": "reference/generated/numpy.ma.maskedarray.__getstate__", "type": "Masked arrays", "text": ["method", "Return the internal state of the masked array, for pickling purposes."]}, {"name": "ma.MaskedArray.__gt__()", "path": "reference/generated/numpy.ma.maskedarray.__gt__", "type": "Masked arrays", "text": ["method", "Return self>value."]}, {"name": "ma.MaskedArray.__iadd__()", "path": "reference/generated/numpy.ma.maskedarray.__iadd__", "type": "Masked arrays", "text": ["method", "Add other to self in-place."]}, {"name": "ma.MaskedArray.__iand__()", "path": "reference/generated/numpy.ma.maskedarray.__iand__", "type": "Masked arrays", "text": ["method", "Return self&=value."]}, {"name": "ma.MaskedArray.__idiv__()", "path": "reference/generated/numpy.ma.maskedarray.__idiv__", "type": "Masked arrays", "text": ["method", "Divide self by other in-place."]}, {"name": "ma.MaskedArray.__ifloordiv__()", "path": "reference/generated/numpy.ma.maskedarray.__ifloordiv__", "type": "Masked arrays", "text": ["method", "Floor divide self by other in-place."]}, {"name": "ma.MaskedArray.__ilshift__()", "path": "reference/generated/numpy.ma.maskedarray.__ilshift__", "type": "Masked arrays", "text": ["method", "Return self<<=value."]}, {"name": "ma.MaskedArray.__imod__()", "path": "reference/generated/numpy.ma.maskedarray.__imod__", "type": "Masked arrays", "text": ["method", "Return self%=value."]}, {"name": "ma.MaskedArray.__imul__()", "path": "reference/generated/numpy.ma.maskedarray.__imul__", "type": "Masked arrays", "text": ["method", "Multiply self by other in-place."]}, {"name": "ma.MaskedArray.__int__()", "path": "reference/generated/numpy.ma.maskedarray.__int__", "type": "Masked arrays", "text": ["method", "Convert to int."]}, {"name": "ma.MaskedArray.__ior__()", "path": "reference/generated/numpy.ma.maskedarray.__ior__", "type": "Masked arrays", "text": ["method", "Return self|=value."]}, {"name": "ma.MaskedArray.__ipow__()", "path": "reference/generated/numpy.ma.maskedarray.__ipow__", "type": "Masked arrays", "text": ["method", "Raise self to the power other, in place."]}, {"name": "ma.MaskedArray.__irshift__()", "path": "reference/generated/numpy.ma.maskedarray.__irshift__", "type": "Masked arrays", "text": ["method", "Return self>>=value."]}, {"name": "ma.MaskedArray.__isub__()", "path": "reference/generated/numpy.ma.maskedarray.__isub__", "type": "Masked arrays", "text": ["method", "Subtract other from self in-place."]}, {"name": "ma.MaskedArray.__itruediv__()", "path": "reference/generated/numpy.ma.maskedarray.__itruediv__", "type": "Masked arrays", "text": ["method", "True divide self by other in-place."]}, {"name": "ma.MaskedArray.__ixor__()", "path": "reference/generated/numpy.ma.maskedarray.__ixor__", "type": "Masked arrays", "text": ["method", "Return self^=value."]}, {"name": "ma.MaskedArray.__le__()", "path": "reference/generated/numpy.ma.maskedarray.__le__", "type": "Masked arrays", "text": ["method", "Return self<=value."]}, {"name": "ma.MaskedArray.__len__()", "path": "reference/generated/numpy.ma.maskedarray.__len__", "type": "Masked arrays", "text": ["method", "Return len(self)."]}, {"name": "ma.MaskedArray.__lshift__()", "path": "reference/generated/numpy.ma.maskedarray.__lshift__", "type": "Masked arrays", "text": ["method", "Return self<<value."]}, {"name": "ma.MaskedArray.__lt__()", "path": "reference/generated/numpy.ma.maskedarray.__lt__", "type": "Masked arrays", "text": ["method", "Return self<value."]}, {"name": "ma.MaskedArray.__mod__()", "path": "reference/generated/numpy.ma.maskedarray.__mod__", "type": "Masked arrays", "text": ["method", "Return self%value."]}, {"name": "ma.MaskedArray.__mul__()", "path": "reference/generated/numpy.ma.maskedarray.__mul__", "type": "Masked arrays", "text": ["method", "Multiply self by other, and return a new masked array."]}, {"name": "ma.MaskedArray.__ne__()", "path": "reference/generated/numpy.ma.maskedarray.__ne__", "type": "Masked arrays", "text": ["method", "Check whether other does not equal self elementwise.", "When either of the elements is masked, the result is masked as well, but the underlying boolean data are still set, with self and other considered equal if both are masked, and unequal otherwise.", "For structured arrays, all fields are combined, with masked values ignored. The result is masked if all fields were masked, with self and other considered equal only if both were fully masked."]}, {"name": "ma.MaskedArray.__or__()", "path": "reference/generated/numpy.ma.maskedarray.__or__", "type": "Masked arrays", "text": ["method", "Return self|value."]}, {"name": "ma.MaskedArray.__pow__()", "path": "reference/generated/numpy.ma.maskedarray.__pow__", "type": "Masked arrays", "text": ["method", "Raise self to the power other, masking the potential NaNs/Infs"]}, {"name": "ma.MaskedArray.__radd__()", "path": "reference/generated/numpy.ma.maskedarray.__radd__", "type": "Masked arrays", "text": ["method", "Add other to self, and return a new masked array."]}, {"name": "ma.MaskedArray.__rand__()", "path": "reference/generated/numpy.ma.maskedarray.__rand__", "type": "Masked arrays", "text": ["method", "Return value&self."]}, {"name": "ma.MaskedArray.__rdivmod__()", "path": "reference/generated/numpy.ma.maskedarray.__rdivmod__", "type": "Masked arrays", "text": ["method", "Return divmod(value, self)."]}, {"name": "ma.MaskedArray.__reduce__()", "path": "reference/generated/numpy.ma.maskedarray.__reduce__", "type": "Masked arrays", "text": ["method", "Return a 3-tuple for pickling a MaskedArray."]}, {"name": "ma.MaskedArray.__repr__()", "path": "reference/generated/numpy.ma.maskedarray.__repr__", "type": "Masked arrays", "text": ["method", "Literal string representation."]}, {"name": "ma.MaskedArray.__rfloordiv__()", "path": "reference/generated/numpy.ma.maskedarray.__rfloordiv__", "type": "Masked arrays", "text": ["method", "Divide self into other, and return a new masked array."]}, {"name": "ma.MaskedArray.__rlshift__()", "path": "reference/generated/numpy.ma.maskedarray.__rlshift__", "type": "Masked arrays", "text": ["method", "Return value<<self."]}, {"name": "ma.MaskedArray.__rmod__()", "path": "reference/generated/numpy.ma.maskedarray.__rmod__", "type": "Masked arrays", "text": ["method", "Return value%self."]}, {"name": "ma.MaskedArray.__rmul__()", "path": "reference/generated/numpy.ma.maskedarray.__rmul__", "type": "Masked arrays", "text": ["method", "Multiply other by self, and return a new masked array."]}, {"name": "ma.MaskedArray.__ror__()", "path": "reference/generated/numpy.ma.maskedarray.__ror__", "type": "Masked arrays", "text": ["method", "Return value|self."]}, {"name": "ma.MaskedArray.__rpow__()", "path": "reference/generated/numpy.ma.maskedarray.__rpow__", "type": "Masked arrays", "text": ["method", "Raise other to the power self, masking the potential NaNs/Infs"]}, {"name": "ma.MaskedArray.__rrshift__()", "path": "reference/generated/numpy.ma.maskedarray.__rrshift__", "type": "Masked arrays", "text": ["method", "Return value>>self."]}, {"name": "ma.MaskedArray.__rshift__()", "path": "reference/generated/numpy.ma.maskedarray.__rshift__", "type": "Masked arrays", "text": ["method", "Return self>>value."]}, {"name": "ma.MaskedArray.__rsub__()", "path": "reference/generated/numpy.ma.maskedarray.__rsub__", "type": "Masked arrays", "text": ["method", "Subtract self from other, and return a new masked array."]}, {"name": "ma.MaskedArray.__rtruediv__()", "path": "reference/generated/numpy.ma.maskedarray.__rtruediv__", "type": "Masked arrays", "text": ["method", "Divide self into other, and return a new masked array."]}, {"name": "ma.MaskedArray.__rxor__()", "path": "reference/generated/numpy.ma.maskedarray.__rxor__", "type": "Masked arrays", "text": ["method", "Return value^self."]}, {"name": "ma.MaskedArray.__setitem__()", "path": "reference/generated/numpy.ma.maskedarray.__setitem__", "type": "Masked arrays", "text": ["method", "x.__setitem__(i, y) <==> x[i]=y", "Set item described by index. If value is masked, masks those locations."]}, {"name": "ma.MaskedArray.__setmask__()", "path": "reference/generated/numpy.ma.maskedarray.__setmask__", "type": "Masked arrays", "text": ["method", "Set the mask."]}, {"name": "ma.MaskedArray.__setstate__()", "path": "reference/generated/numpy.ma.maskedarray.__setstate__", "type": "Masked arrays", "text": ["method", "Restore the internal state of the masked array, for pickling purposes. state is typically the output of the __getstate__ output, and is a 5-tuple:"]}, {"name": "ma.MaskedArray.__str__()", "path": "reference/generated/numpy.ma.maskedarray.__str__", "type": "Masked arrays", "text": ["method", "Return str(self)."]}, {"name": "ma.MaskedArray.__sub__()", "path": "reference/generated/numpy.ma.maskedarray.__sub__", "type": "Masked arrays", "text": ["method", "Subtract other from self, and return a new masked array."]}, {"name": "ma.MaskedArray.__truediv__()", "path": "reference/generated/numpy.ma.maskedarray.__truediv__", "type": "Masked arrays", "text": ["method", "Divide other into self, and return a new masked array."]}, {"name": "ma.MaskedArray.__xor__()", "path": "reference/generated/numpy.ma.maskedarray.__xor__", "type": "Masked arrays", "text": ["method", "Return self^value."]}, {"name": "ma.MaskedArray.all()", "path": "reference/generated/numpy.ma.maskedarray.all", "type": "numpy.ma.MaskedArray.all", "text": ["method", "Returns True if all elements evaluate to True.", "The output array is masked where all the values along the given axis are masked: if the output would have been a scalar and that all the values are masked, then the output is masked.", "Refer to numpy.all for full documentation.", "See also", "corresponding function for ndarrays", "equivalent function"]}, {"name": "ma.MaskedArray.anom()", "path": "reference/generated/numpy.ma.maskedarray.anom", "type": "numpy.ma.MaskedArray.anom", "text": ["method", "Compute the anomalies (deviations from the arithmetic mean) along the given axis.", "Returns an array of anomalies, with the same shape as the input and where the arithmetic mean is computed along the given axis.", "Axis over which the anomalies are taken. The default is to use the mean of the flattened array as reference.", "the default is float32; for arrays of float types it is the same as the array type.", "See also", "Compute the mean of the array."]}, {"name": "ma.MaskedArray.any()", "path": "reference/generated/numpy.ma.maskedarray.any", "type": "numpy.ma.MaskedArray.any", "text": ["method", "Returns True if any of the elements of a evaluate to True.", "Masked values are considered as False during computation.", "Refer to numpy.any for full documentation.", "See also", "corresponding function for ndarrays", "equivalent function"]}, {"name": "ma.MaskedArray.argmax()", "path": "reference/generated/numpy.ma.maskedarray.argmax", "type": "numpy.ma.MaskedArray.argmax", "text": ["method", "Returns array of indices of the maximum values along the given axis. Masked values are treated as if they had the value fill_value.", "If None, the index is into the flattened array, otherwise along the specified axis", "Value used to fill in the masked values. If None, the output of maximum_fill_value(self._data) is used instead.", "Array into which the result can be placed. Its type is preserved and it must be of the right shape to hold the output."]}, {"name": "ma.MaskedArray.argmin()", "path": "reference/generated/numpy.ma.maskedarray.argmin", "type": "numpy.ma.MaskedArray.argmin", "text": ["method", "Return array of indices to the minimum values along the given axis.", "If None, the index is into the flattened array, otherwise along the specified axis", "Value used to fill in the masked values. If None, the output of minimum_fill_value(self._data) is used instead.", "Array into which the result can be placed. Its type is preserved and it must be of the right shape to hold the output.", "If multi-dimension input, returns a new ndarray of indices to the minimum values along the given axis. Otherwise, returns a scalar of index to the minimum values along the given axis."]}, {"name": "ma.MaskedArray.argsort()", "path": "reference/generated/numpy.ma.maskedarray.argsort", "type": "numpy.ma.MaskedArray.argsort", "text": ["method", "Return an ndarray of indices that sort the array along the specified axis. Masked values are filled beforehand to fill_value.", "Axis along which to sort. If None, the default, the flattened array is used.", "Changed in version 1.13.0: Previously, the default was documented to be -1, but that was in error. At some future date, the default will change to -1, as originally intended. Until then, the axis should be given explicitly when arr.ndim > 1, to avoid a FutureWarning.", "The sorting algorithm used.", "When a is an array with fields defined, this argument specifies which fields to compare first, second, etc. Not all fields need be specified.", "Whether missing values (if any) should be treated as the largest values (True) or the smallest values (False) When the array contains unmasked values at the same extremes of the datatype, the ordering of these values and the masked values is undefined.", "Value used internally for the masked values. If fill_value is not None, it supersedes endwith.", "Array of indices that sort a along the specified axis. In other words, a[index_array] yields a sorted a.", "See also", "Describes sorting algorithms used.", "Indirect stable sort with multiple keys.", "Inplace sort.", "See sort for notes on the different sorting algorithms."]}, {"name": "ma.MaskedArray.astype()", "path": "reference/generated/numpy.ma.maskedarray.astype", "type": "Masked arrays", "text": ["method", "Copy of the array, cast to a specified type.", "Typecode or data-type to which the array is cast.", "Controls the memory layout order of the result. \u2018C\u2019 means C order, \u2018F\u2019 means Fortran order, \u2018A\u2019 means \u2018F\u2019 order if all the arrays are Fortran contiguous, \u2018C\u2019 order otherwise, and \u2018K\u2019 means as close to the order the array elements appear in memory as possible. Default is \u2018K\u2019.", "Controls what kind of data casting may occur. Defaults to \u2018unsafe\u2019 for backwards compatibility.", "If True, then sub-classes will be passed-through (default), otherwise the returned array will be forced to be a base-class array.", "By default, astype always returns a newly allocated array. If this is set to false, and the dtype, order, and subok requirements are satisfied, the input array is returned instead of a copy.", "Unless copy is False and the other conditions for returning the input array are satisfied (see description for copy input parameter), arr_t is a new array of the same shape as the input array, with dtype, order given by dtype, order.", "When casting from complex to float or int. To avoid this, one should use a.real.astype(t).", "Changed in version 1.17.0: Casting between a simple data type and a structured one is possible only for \u201cunsafe\u201d casting. Casting to multiple fields is allowed, but casting from multiple fields is not.", "Changed in version 1.9.0: Casting from numeric to string types in \u2018safe\u2019 casting mode requires that the string dtype length is long enough to store the max integer/float value converted."]}, {"name": "ma.MaskedArray.base", "path": "reference/generated/numpy.ma.maskedarray.base", "type": "Masked arrays", "text": ["attribute", "Base object if memory is from some other object.", "The base of an array that owns its memory is None:", "Slicing creates a view, whose memory is shared with x:"]}, {"name": "ma.MaskedArray.byteswap()", "path": "reference/generated/numpy.ma.maskedarray.byteswap", "type": "Masked arrays", "text": ["method", "Swap the bytes of the array elements", "Toggle between low-endian and big-endian data representation by returning a byteswapped array, optionally swapped in-place. Arrays of byte-strings are not swapped. The real and imaginary parts of a complex number are swapped individually.", "If True, swap bytes in-place, default is False.", "The byteswapped array. If inplace is True, this is a view to self.", "Arrays of byte-strings are not swapped", "but different representation in memory"]}, {"name": "ma.MaskedArray.choose()", "path": "reference/generated/numpy.ma.maskedarray.choose", "type": "Masked arrays", "text": ["method", "Use an index array to construct a new array from a set of choices.", "Refer to numpy.choose for full documentation.", "See also", "equivalent function"]}, {"name": "ma.MaskedArray.clip()", "path": "reference/generated/numpy.ma.maskedarray.clip", "type": "numpy.ma.MaskedArray.clip", "text": ["method", "Return an array whose values are limited to [min, max]. One of max or min must be given.", "Refer to numpy.clip for full documentation.", "See also", "equivalent function"]}, {"name": "ma.MaskedArray.compress()", "path": "reference/generated/numpy.ma.maskedarray.compress", "type": "Masked arrays", "text": ["method", "Return a where condition is True.", "If condition is a MaskedArray, missing values are considered as False.", "Boolean 1-d array selecting which entries to return. If len(condition) is less than the size of a along the axis, then output is truncated to length of condition array.", "Axis along which the operation must be performed.", "Alternative output array in which to place the result. It must have the same shape as the expected output but the type will be cast if necessary.", "A MaskedArray object.", "Please note the difference with compressed ! The output of compress has a mask, the output of compressed does not."]}, {"name": "ma.MaskedArray.compressed()", "path": "reference/generated/numpy.ma.maskedarray.compressed", "type": "numpy.ma.MaskedArray.compressed", "text": ["method", "Return all the non-masked data as a 1-D array.", "A new ndarray holding the non-masked data is returned.", "The result is not a MaskedArray!"]}, {"name": "ma.MaskedArray.conj()", "path": "reference/generated/numpy.ma.maskedarray.conj", "type": "Masked arrays", "text": ["method", "Complex-conjugate all elements.", "Refer to numpy.conjugate for full documentation.", "See also", "equivalent function"]}, {"name": "ma.MaskedArray.conjugate()", "path": "reference/generated/numpy.ma.maskedarray.conjugate", "type": "Masked arrays", "text": ["method", "Return the complex conjugate, element-wise.", "Refer to numpy.conjugate for full documentation.", "See also", "equivalent function"]}, {"name": "ma.MaskedArray.copy()", "path": "reference/generated/numpy.ma.maskedarray.copy", "type": "numpy.ma.MaskedArray.copy", "text": ["method", "Return a copy of the array.", "Controls the memory layout of the copy. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of a as closely as possible. (Note that this function and numpy.copy are very similar but have different default values for their order= arguments, and this function always passes sub-classes through.)", "See also", "Similar function with different default behavior", "This function is the preferred method for creating an array copy. The function numpy.copy is similar, but it defaults to using order \u2018K\u2019, and will not pass sub-classes through by default."]}, {"name": "ma.MaskedArray.count()", "path": "reference/generated/numpy.ma.maskedarray.count", "type": "numpy.ma.MaskedArray.count", "text": ["method", "Count the non-masked elements of the array along the given axis.", "Axis or axes along which the count is performed. The default, None, performs the count over all the dimensions of the input array. axis may be negative, in which case it counts from the last to the first axis.", "New in version 1.10.0.", "If this is a tuple of ints, the count is performed on multiple axes, instead of a single axis or all the axes as before.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the array.", "An array with the same shape as the input array, with the specified axis removed. If the array is a 0-d array, or if axis is None, a scalar is returned.", "See also", "Count masked elements in array or along a given axis.", "When the axis keyword is specified an array of appropriate size is returned."]}, {"name": "ma.MaskedArray.ctypes", "path": "reference/generated/numpy.ma.maskedarray.ctypes", "type": "Masked arrays", "text": ["attribute", "An object to simplify the interaction of the array with the ctypes module.", "This attribute creates an object that makes it easier to use arrays when calling shared libraries with the ctypes module. The returned object has, among others, data, shape, and strides attributes (see Notes below) which themselves return ctypes objects that can be used as arguments to a shared library.", "Possessing attributes data, shape, strides, etc.", "See also", "Below are the public attributes of this object which were documented in \u201cGuide to NumPy\u201d (we have omitted undocumented public attributes, as well as documented private attributes):", "A pointer to the memory area of the array as a Python integer. This memory area may contain data that is not aligned, or not in correct byte-order. The memory area may not even be writeable. The array flags and data-type of this array should be respected when passing this attribute to arbitrary C-code to avoid trouble that can include Python crashing. User Beware! The value of this attribute is exactly the same as self._array_interface_['data'][0].", "Note that unlike data_as, a reference will not be kept to the array: code like ctypes.c_void_p((a + b).ctypes.data) will result in a pointer to a deallocated array, and should be spelt (a + b).ctypes.data_as(ctypes.c_void_p)", "(c_intp*self.ndim): A ctypes array of length self.ndim where the basetype is the C-integer corresponding to dtype('p') on this platform (see c_intp). This base-type could be ctypes.c_int, ctypes.c_long, or ctypes.c_longlong depending on the platform. The ctypes array contains the shape of the underlying array.", "(c_intp*self.ndim): A ctypes array of length self.ndim where the basetype is the same as for the shape attribute. This ctypes array contains the strides information from the underlying array. This strides information is important for showing how many bytes must be jumped to get to the next element in the array.", "Return the data pointer cast to a particular c-types object. For example, calling self._as_parameter_ is equivalent to self.data_as(ctypes.c_void_p). Perhaps you want to use the data as a pointer to a ctypes array of floating-point data: self.data_as(ctypes.POINTER(ctypes.c_double)).", "The returned pointer will keep a reference to the array.", "Return the shape tuple as an array of some other c-types type. For example: self.shape_as(ctypes.c_short).", "Return the strides tuple as an array of some other c-types type. For example: self.strides_as(ctypes.c_longlong).", "If the ctypes module is not available, then the ctypes attribute of array objects still returns something useful, but ctypes objects are not returned and errors may be raised instead. In particular, the object will still have the as_parameter attribute which will return an integer equal to the data attribute."]}, {"name": "ma.MaskedArray.cumprod()", "path": "reference/generated/numpy.ma.maskedarray.cumprod", "type": "numpy.ma.MaskedArray.cumprod", "text": ["method", "Return the cumulative product of the array elements over the given axis.", "Masked values are set to 1 internally during the computation. However, their position is saved, and the result will be masked at the same locations.", "Refer to numpy.cumprod for full documentation.", "See also", "corresponding function for ndarrays", "equivalent function", "The mask is lost if out is not a valid MaskedArray !", "Arithmetic is modular when using integer types, and no error is raised on overflow."]}, {"name": "ma.MaskedArray.cumsum()", "path": "reference/generated/numpy.ma.maskedarray.cumsum", "type": "numpy.ma.MaskedArray.cumsum", "text": ["method", "Return the cumulative sum of the array elements over the given axis.", "Masked values are set to 0 internally during the computation. However, their position is saved, and the result will be masked at the same locations.", "Refer to numpy.cumsum for full documentation.", "See also", "corresponding function for ndarrays", "equivalent function", "The mask is lost if out is not a valid ma.MaskedArray !", "Arithmetic is modular when using integer types, and no error is raised on overflow."]}, {"name": "ma.MaskedArray.diagonal()", "path": "reference/generated/numpy.ma.maskedarray.diagonal", "type": "Masked arrays", "text": ["method", "Return specified diagonals. In NumPy 1.9 the returned array is a read-only view instead of a copy as in previous NumPy versions. In a future version the read-only restriction will be removed.", "Refer to numpy.diagonal for full documentation.", "See also", "equivalent function"]}, {"name": "ma.MaskedArray.dump()", "path": "reference/generated/numpy.ma.maskedarray.dump", "type": "Masked arrays", "text": ["method", "Dump a pickle of the array to the specified file. The array can be read back with pickle.load or numpy.load.", "A string naming the dump file.", "Changed in version 1.17.0: pathlib.Path objects are now accepted."]}, {"name": "ma.MaskedArray.dumps()", "path": "reference/generated/numpy.ma.maskedarray.dumps", "type": "Masked arrays", "text": ["method", "Returns the pickle of the array as a string. pickle.loads will convert the string back to an array."]}, {"name": "ma.MaskedArray.fill()", "path": "reference/generated/numpy.ma.maskedarray.fill", "type": "Masked arrays", "text": ["method", "Fill the array with a scalar value.", "All elements of a will be assigned this value."]}, {"name": "ma.MaskedArray.filled()", "path": "reference/generated/numpy.ma.maskedarray.filled", "type": "numpy.ma.MaskedArray.filled", "text": ["method", "Return a copy of self, with masked values filled with a given value. However, if there are no masked values to fill, self will be returned instead as an ndarray.", "The value to use for invalid entries. Can be scalar or non-scalar. If non-scalar, the resulting ndarray must be broadcastable over input array. Default is None, in which case, the fill_value attribute of the array is used instead.", "A copy of self with invalid entries replaced by fill_value (be it the function argument or the attribute of self), or self itself as an ndarray if there are no invalid entries to be replaced.", "The result is not a MaskedArray!", "Subclassing is preserved. This means that if, e.g., the data part of the masked array is a recarray, filled returns a recarray:"]}, {"name": "ma.MaskedArray.flags", "path": "reference/generated/numpy.ma.maskedarray.flags", "type": "Masked arrays", "text": ["attribute", "Information about the memory layout of the array.", "The flags object can be accessed dictionary-like (as in a.flags['WRITEABLE']), or by using lowercased attribute names (as in a.flags.writeable). Short flag names are only supported in dictionary access.", "Only the WRITEBACKIFCOPY, UPDATEIFCOPY, WRITEABLE, and ALIGNED flags can be changed by the user, via direct assignment to the attribute or dictionary entry, or by calling ndarray.setflags.", "The array flags cannot be set arbitrarily:", "Arrays can be both C-style and Fortran-style contiguous simultaneously. This is clear for 1-dimensional arrays, but can also be true for higher dimensional arrays.", "Even for contiguous arrays a stride for a given dimension arr.strides[dim] may be arbitrary if arr.shape[dim] == 1 or the array has no elements. It does not generally hold that self.strides[-1] == self.itemsize for C-style contiguous arrays or self.strides[0] == self.itemsize for Fortran-style contiguous arrays is true.", "The data is in a single, C-style contiguous segment.", "The data is in a single, Fortran-style contiguous segment.", "The array owns the memory it uses or borrows it from another object.", "The data area can be written to. Setting this to False locks the data, making it read-only. A view (slice, etc.) inherits WRITEABLE from its base array at creation time, but a view of a writeable array may be subsequently locked while the base array remains writeable. (The opposite is not true, in that a view of a locked array may not be made writeable. However, currently, locking a base object does not lock any views that already reference it, so under that circumstance it is possible to alter the contents of a locked array via a previously created writeable view onto it.) Attempting to change a non-writeable array raises a RuntimeError exception.", "The data and all elements are aligned appropriately for the hardware.", "This array is a copy of some other array. The C-API function PyArray_ResolveWritebackIfCopy must be called before deallocating to the base array will be updated with the contents of this array.", "(Deprecated, use WRITEBACKIFCOPY) This array is a copy of some other array. When this array is deallocated, the base array will be updated with the contents of this array.", "F_CONTIGUOUS and not C_CONTIGUOUS.", "F_CONTIGUOUS or C_CONTIGUOUS (one-segment test).", "ALIGNED and WRITEABLE.", "BEHAVED and C_CONTIGUOUS.", "BEHAVED and F_CONTIGUOUS and not C_CONTIGUOUS."]}, {"name": "ma.MaskedArray.flatten()", "path": "reference/generated/numpy.ma.maskedarray.flatten", "type": "numpy.ma.MaskedArray.flatten", "text": ["method", "Return a copy of the array collapsed into one dimension.", "\u2018C\u2019 means to flatten in row-major (C-style) order. \u2018F\u2019 means to flatten in column-major (Fortran- style) order. \u2018A\u2019 means to flatten in column-major order if a is Fortran contiguous in memory, row-major order otherwise. \u2018K\u2019 means to flatten a in the order the elements occur in memory. The default is \u2018C\u2019.", "A copy of the input array, flattened to one dimension.", "See also", "Return a flattened array.", "A 1-D flat iterator over the array."]}, {"name": "ma.MaskedArray.get_fill_value()", "path": "reference/generated/numpy.ma.maskedarray.get_fill_value", "type": "numpy.ma.MaskedArray.get_fill_value", "text": ["method", "The filling value of the masked array is a scalar. When setting, None will set to a default based on the data type.", "Reset to default:"]}, {"name": "ma.MaskedArray.harden_mask()", "path": "reference/generated/numpy.ma.maskedarray.harden_mask", "type": "numpy.ma.MaskedArray.harden_mask", "text": ["method", "Force the mask to hard.", "Whether the mask of a masked array is hard or soft is determined by its hardmask property. harden_mask sets hardmask to True.", "See also"]}, {"name": "ma.MaskedArray.ids()", "path": "reference/generated/numpy.ma.maskedarray.ids", "type": "Masked arrays", "text": ["method", "Return the addresses of the data and mask areas.", "If the array has no mask, the address of nomask is returned. This address is typically not close to the data in memory:"]}, {"name": "ma.MaskedArray.iscontiguous()", "path": "reference/generated/numpy.ma.maskedarray.iscontiguous", "type": "Masked arrays", "text": ["method", "Return a boolean indicating whether the data is contiguous.", "iscontiguous returns one of the flags of the masked array:"]}, {"name": "ma.MaskedArray.item()", "path": "reference/generated/numpy.ma.maskedarray.item", "type": "Masked arrays", "text": ["method", "Copy an element of an array to a standard Python scalar and return it.", "A copy of the specified element of the array as a suitable Python scalar", "When the data type of a is longdouble or clongdouble, item() returns a scalar array object because there is no available Python scalar that would not lose information. Void arrays return a buffer object for item(), unless fields are defined, in which case a tuple is returned.", "item is very similar to a[args], except, instead of an array scalar, a standard Python scalar is returned. This can be useful for speeding up access to elements of the array and doing arithmetic on elements of the array using Python\u2019s optimized math."]}, {"name": "ma.MaskedArray.itemsize", "path": "reference/generated/numpy.ma.maskedarray.itemsize", "type": "Masked arrays", "text": ["attribute", "Length of one array element in bytes."]}, {"name": "ma.MaskedArray.max()", "path": "reference/generated/numpy.ma.maskedarray.max", "type": "numpy.ma.MaskedArray.max", "text": ["method", "Return the maximum along a given axis.", "Axis along which to operate. By default, axis is None and the flattened input is used.", "Alternative output array in which to place the result. Must be of the same shape and buffer length as the expected output.", "Value used to fill in the masked values. If None, use the output of maximum_fill_value().", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the array.", "New array holding the result. If out was specified, out is returned.", "See also", "Returns the maximum filling value for a given datatype."]}, {"name": "ma.MaskedArray.mean()", "path": "reference/generated/numpy.ma.maskedarray.mean", "type": "numpy.ma.MaskedArray.mean", "text": ["method", "Returns the average of the array elements along given axis.", "Masked entries are ignored, and result elements which are not finite will be masked.", "Refer to numpy.mean for full documentation.", "See also", "corresponding function for ndarrays", "Equivalent function", "Weighted average."]}, {"name": "ma.MaskedArray.min()", "path": "reference/generated/numpy.ma.maskedarray.min", "type": "numpy.ma.MaskedArray.min", "text": ["method", "Return the minimum along a given axis.", "Axis along which to operate. By default, axis is None and the flattened input is used.", "Alternative output array in which to place the result. Must be of the same shape and buffer length as the expected output.", "Value used to fill in the masked values. If None, use the output of minimum_fill_value.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the array.", "New array holding the result. If out was specified, out is returned.", "See also", "Returns the minimum filling value for a given datatype."]}, {"name": "ma.MaskedArray.nbytes", "path": "reference/generated/numpy.ma.maskedarray.nbytes", "type": "Masked arrays", "text": ["attribute", "Total bytes consumed by the elements of the array.", "Does not include memory consumed by non-element attributes of the array object."]}, {"name": "ma.MaskedArray.ndim", "path": "reference/generated/numpy.ma.maskedarray.ndim", "type": "Masked arrays", "text": ["attribute", "Number of array dimensions."]}, {"name": "ma.MaskedArray.nonzero()", "path": "reference/generated/numpy.ma.maskedarray.nonzero", "type": "numpy.ma.MaskedArray.nonzero", "text": ["method", "Return the indices of unmasked elements that are not zero.", "Returns a tuple of arrays, one for each dimension, containing the indices of the non-zero elements in that dimension. The corresponding non-zero values can be obtained with:", "To group the indices by element, rather than dimension, use instead:", "The result of this is always a 2d array, with a row for each non-zero element.", "Indices of elements that are non-zero.", "See also", "Function operating on ndarrays.", "Return indices that are non-zero in the flattened version of the input array.", "Equivalent ndarray method.", "Counts the number of non-zero elements in the input array.", "Masked elements are ignored.", "Indices can also be grouped by element.", "A common use for nonzero is to find the indices of an array, where a condition is True. Given an array a, the condition a > 3 is a boolean array and since False is interpreted as 0, ma.nonzero(a > 3) yields the indices of the a where the condition is true.", "The nonzero method of the condition array can also be called."]}, {"name": "ma.MaskedArray.prod()", "path": "reference/generated/numpy.ma.maskedarray.prod", "type": "numpy.ma.MaskedArray.prod", "text": ["method", "Return the product of the array elements over the given axis.", "Masked elements are set to 1 internally for computation.", "Refer to numpy.prod for full documentation.", "See also", "corresponding function for ndarrays", "equivalent function", "Arithmetic is modular when using integer types, and no error is raised on overflow."]}, {"name": "ma.MaskedArray.product()", "path": "reference/generated/numpy.ma.maskedarray.product", "type": "Masked arrays", "text": ["method", "Return the product of the array elements over the given axis.", "Masked elements are set to 1 internally for computation.", "Refer to numpy.prod for full documentation.", "See also", "corresponding function for ndarrays", "equivalent function", "Arithmetic is modular when using integer types, and no error is raised on overflow."]}, {"name": "ma.MaskedArray.ptp()", "path": "reference/generated/numpy.ma.maskedarray.ptp", "type": "numpy.ma.MaskedArray.ptp", "text": ["method", "Return (maximum - minimum) along the given dimension (i.e. peak-to-peak value).", "Warning", "ptp preserves the data type of the array. This means the return value for an input of signed integers with n bits (e.g. np.int8, np.int16, etc) is also a signed integer with n bits. In that case, peak-to-peak values greater than 2**(n-1)-1 will be returned as negative values. An example with a work-around is shown below.", "Axis along which to find the peaks. If None (default) the flattened array is used.", "Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output but the type will be cast if necessary.", "Value used to fill in the masked values.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the array.", "A new array holding the result, unless out was specified, in which case a reference to out is returned.", "This example shows that a negative value can be returned when the input is an array of signed integers.", "A work-around is to use the view() method to view the result as unsigned integers with the same bit width:"]}, {"name": "ma.MaskedArray.put()", "path": "reference/generated/numpy.ma.maskedarray.put", "type": "Masked arrays", "text": ["method", "Set storage-indexed locations to corresponding values.", "Sets self._data.flat[n] = values[n] for each n in indices. If values is shorter than indices then it will repeat. If values has some masked values, the initial mask is updated in consequence, else the corresponding values are unmasked.", "Target indices, interpreted as integers.", "Values to place in self._data copy at target indices.", "Specifies how out-of-bounds indices will behave. \u2018raise\u2019 : raise an error. \u2018wrap\u2019 : wrap around. \u2018clip\u2019 : clip to the range.", "values can be a scalar or length 1 array."]}, {"name": "ma.MaskedArray.ravel()", "path": "reference/generated/numpy.ma.maskedarray.ravel", "type": "numpy.ma.MaskedArray.ravel", "text": ["method", "Returns a 1D version of self, as a view.", "The elements of a are read using this index order. \u2018C\u2019 means to index the elements in C-like order, with the last axis index changing fastest, back to the first axis index changing slowest. \u2018F\u2019 means to index the elements in Fortran-like index order, with the first index changing fastest, and the last index changing slowest. Note that the \u2018C\u2019 and \u2018F\u2019 options take no account of the memory layout of the underlying array, and only refer to the order of axis indexing. \u2018A\u2019 means to read the elements in Fortran-like index order if m is Fortran contiguous in memory, C-like order otherwise. \u2018K\u2019 means to read the elements in the order they occur in memory, except for reversing the data when strides are negative. By default, \u2018C\u2019 index order is used.", "Output view is of shape (self.size,) (or (np.ma.product(self.shape),))."]}, {"name": "ma.MaskedArray.repeat()", "path": "reference/generated/numpy.ma.maskedarray.repeat", "type": "Masked arrays", "text": ["method", "Repeat elements of an array.", "Refer to numpy.repeat for full documentation.", "See also", "equivalent function"]}, {"name": "ma.MaskedArray.reshape()", "path": "reference/generated/numpy.ma.maskedarray.reshape", "type": "numpy.ma.MaskedArray.reshape", "text": ["method", "Give a new shape to the array without changing its data.", "Returns a masked array containing the same data, but with a new shape. The result is a view on the original array; if this is not possible, a ValueError is raised.", "The new shape should be compatible with the original shape. If an integer is supplied, then the result will be a 1-D array of that length.", "Determines whether the array data should be viewed as in C (row-major) or FORTRAN (column-major) order.", "A new view on the array.", "See also", "Equivalent function in the masked array module.", "Equivalent method on ndarray object.", "Equivalent function in the NumPy module.", "The reshaping operation cannot guarantee that a copy will not be made, to modify the shape in place, use a.shape = s"]}, {"name": "ma.MaskedArray.resize()", "path": "reference/generated/numpy.ma.maskedarray.resize", "type": "numpy.ma.MaskedArray.resize", "text": ["method", "Warning", "This method does nothing, except raise a ValueError exception. A masked array does not own its data and therefore cannot safely be resized in place. Use the numpy.ma.resize function instead.", "This method is difficult to implement safely and may be deprecated in future releases of NumPy."]}, {"name": "ma.MaskedArray.round()", "path": "reference/generated/numpy.ma.maskedarray.round", "type": "numpy.ma.MaskedArray.round", "text": ["method", "Return each element rounded to the given number of decimals.", "Refer to numpy.around for full documentation.", "See also", "corresponding function for ndarrays", "equivalent function"]}, {"name": "ma.MaskedArray.searchsorted()", "path": "reference/generated/numpy.ma.maskedarray.searchsorted", "type": "Masked arrays", "text": ["method", "Find indices where elements of v should be inserted in a to maintain order.", "For full documentation, see numpy.searchsorted", "See also", "equivalent function"]}, {"name": "ma.MaskedArray.set_fill_value()", "path": "reference/generated/numpy.ma.maskedarray.set_fill_value", "type": "numpy.ma.MaskedArray.set_fill_value", "text": ["method"]}, {"name": "ma.MaskedArray.shrink_mask()", "path": "reference/generated/numpy.ma.maskedarray.shrink_mask", "type": "numpy.ma.MaskedArray.shrink_mask", "text": ["method", "Reduce a mask to nomask when possible."]}, {"name": "ma.MaskedArray.size", "path": "reference/generated/numpy.ma.maskedarray.size", "type": "Masked arrays", "text": ["attribute", "Number of elements in the array.", "Equal to np.prod(a.shape), i.e., the product of the array\u2019s dimensions.", "a.size returns a standard arbitrary precision Python integer. This may not be the case with other methods of obtaining the same value (like the suggested np.prod(a.shape), which returns an instance of np.int_), and may be relevant if the value is used further in calculations that may overflow a fixed size integer type."]}, {"name": "ma.MaskedArray.soften_mask()", "path": "reference/generated/numpy.ma.maskedarray.soften_mask", "type": "numpy.ma.MaskedArray.soften_mask", "text": ["method", "Force the mask to soft.", "Whether the mask of a masked array is hard or soft is determined by its hardmask property. soften_mask sets hardmask to False.", "See also"]}, {"name": "ma.MaskedArray.sort()", "path": "reference/generated/numpy.ma.maskedarray.sort", "type": "numpy.ma.MaskedArray.sort", "text": ["method", "Sort the array, in-place", "Array to be sorted.", "Axis along which to sort. If None, the array is flattened before sorting. The default is -1, which sorts along the last axis.", "The sorting algorithm used.", "When a is a structured array, this argument specifies which fields to compare first, second, and so on. This list does not need to include all of the fields.", "Whether missing values (if any) should be treated as the largest values (True) or the smallest values (False) When the array contains unmasked values sorting at the same extremes of the datatype, the ordering of these values and the masked values is undefined.", "Value used internally for the masked values. If fill_value is not None, it supersedes endwith.", "Array of the same type and shape as a.", "See also", "Method to sort an array in-place.", "Indirect sort.", "Indirect stable sort on multiple keys.", "Find elements in a sorted array.", "See sort for notes on the different sorting algorithms."]}, {"name": "ma.MaskedArray.squeeze()", "path": "reference/generated/numpy.ma.maskedarray.squeeze", "type": "numpy.ma.MaskedArray.squeeze", "text": ["method", "Remove axes of length one from a.", "Refer to numpy.squeeze for full documentation.", "See also", "equivalent function"]}, {"name": "ma.MaskedArray.std()", "path": "reference/generated/numpy.ma.maskedarray.std", "type": "numpy.ma.MaskedArray.std", "text": ["method", "Returns the standard deviation of the array elements along given axis.", "Masked entries are ignored.", "Refer to numpy.std for full documentation.", "See also", "corresponding function for ndarrays", "Equivalent function"]}, {"name": "ma.MaskedArray.strides", "path": "reference/generated/numpy.ma.maskedarray.strides", "type": "Masked arrays", "text": ["attribute", "Tuple of bytes to step in each dimension when traversing an array.", "The byte offset of element (i[0], i[1], ..., i[n]) in an array a is:", "A more detailed explanation of strides can be found in the \u201cndarray.rst\u201d file in the NumPy reference guide.", "See also", "Imagine an array of 32-bit integers (each 4 bytes):", "This array is stored in memory as 40 bytes, one after the other (known as a contiguous block of memory). The strides of an array tell us how many bytes we have to skip in memory to move to the next position along a certain axis. For example, we have to skip 4 bytes (1 value) to move to the next column, but 20 bytes (5 values) to get to the same position in the next row. As such, the strides for the array x will be (20, 4)."]}, {"name": "ma.MaskedArray.sum()", "path": "reference/generated/numpy.ma.maskedarray.sum", "type": "numpy.ma.MaskedArray.sum", "text": ["method", "Return the sum of the array elements over the given axis.", "Masked elements are set to 0 internally.", "Refer to numpy.sum for full documentation.", "See also", "corresponding function for ndarrays", "equivalent function"]}, {"name": "ma.MaskedArray.swapaxes()", "path": "reference/generated/numpy.ma.maskedarray.swapaxes", "type": "numpy.ma.MaskedArray.swapaxes", "text": ["method", "Return a view of the array with axis1 and axis2 interchanged.", "Refer to numpy.swapaxes for full documentation.", "See also", "equivalent function"]}, {"name": "ma.MaskedArray.take()", "path": "reference/generated/numpy.ma.maskedarray.take", "type": "Masked arrays", "text": ["method"]}, {"name": "ma.MaskedArray.tobytes()", "path": "reference/generated/numpy.ma.maskedarray.tobytes", "type": "numpy.ma.MaskedArray.tobytes", "text": ["method", "Return the array data as a string containing the raw bytes in the array.", "The array is filled with a fill value before the string conversion.", "New in version 1.9.0.", "Value used to fill in the masked values. Default is None, in which case MaskedArray.fill_value is used.", "Order of the data item in the copy. Default is \u2018C\u2019.", "See also", "As for ndarray.tobytes, information about the shape, dtype, etc., but also about fill_value, will be lost."]}, {"name": "ma.MaskedArray.tofile()", "path": "reference/generated/numpy.ma.maskedarray.tofile", "type": "numpy.ma.MaskedArray.tofile", "text": ["method", "Save a masked array to a file in binary format.", "Warning", "This function is not implemented yet.", "When tofile is called."]}, {"name": "ma.MaskedArray.toflex()", "path": "reference/generated/numpy.ma.maskedarray.toflex", "type": "Masked arrays", "text": ["method", "Transforms a masked array into a flexible-type array.", "The flexible type array that is returned will have two fields:", "A new flexible-type ndarray with two fields: the first element containing a value, the second element containing the corresponding mask boolean. The returned record shape matches self.shape.", "A side-effect of transforming a masked array into a flexible ndarray is that meta information (fill_value, \u2026) will be lost."]}, {"name": "ma.MaskedArray.tolist()", "path": "reference/generated/numpy.ma.maskedarray.tolist", "type": "numpy.ma.MaskedArray.tolist", "text": ["method", "Return the data portion of the masked array as a hierarchical Python list.", "Data items are converted to the nearest compatible Python type. Masked values are converted to fill_value. If fill_value is None, the corresponding entries in the output list will be None.", "The value to use for invalid entries. Default is None.", "The Python list representation of the masked array."]}, {"name": "ma.MaskedArray.torecords()", "path": "reference/generated/numpy.ma.maskedarray.torecords", "type": "numpy.ma.MaskedArray.torecords", "text": ["method", "Transforms a masked array into a flexible-type array.", "The flexible type array that is returned will have two fields:", "A new flexible-type ndarray with two fields: the first element containing a value, the second element containing the corresponding mask boolean. The returned record shape matches self.shape.", "A side-effect of transforming a masked array into a flexible ndarray is that meta information (fill_value, \u2026) will be lost."]}, {"name": "ma.MaskedArray.tostring()", "path": "reference/generated/numpy.ma.maskedarray.tostring", "type": "Masked arrays", "text": ["method", "A compatibility alias for tobytes, with exactly the same behavior.", "Despite its name, it returns bytes not strs.", "Deprecated since version 1.19.0."]}, {"name": "ma.MaskedArray.trace()", "path": "reference/generated/numpy.ma.maskedarray.trace", "type": "numpy.ma.MaskedArray.trace", "text": ["method", "Return the sum along diagonals of the array.", "Refer to numpy.trace for full documentation.", "See also", "equivalent function"]}, {"name": "ma.MaskedArray.transpose()", "path": "reference/generated/numpy.ma.maskedarray.transpose", "type": "numpy.ma.MaskedArray.transpose", "text": ["method", "Returns a view of the array with axes transposed.", "For a 1-D array this has no effect, as a transposed vector is simply the same vector. To convert a 1-D array into a 2D column vector, an additional dimension must be added. np.atleast2d(a).T achieves this, as does a[:, np.newaxis]. For a 2-D array, this is a standard matrix transpose. For an n-D array, if axes are given, their order indicates how the axes are permuted (see Examples). If axes are not provided and a.shape = (i[0], i[1], ... i[n-2], i[n-1]), then a.transpose().shape = (i[n-1], i[n-2], ... i[1], i[0]).", "View of a, with axes suitably permuted.", "See also", "Equivalent function", "Array property returning the array transposed.", "Give a new shape to an array without changing its data."]}, {"name": "ma.MaskedArray.unshare_mask()", "path": "reference/generated/numpy.ma.maskedarray.unshare_mask", "type": "numpy.ma.MaskedArray.unshare_mask", "text": ["method", "Copy the mask and set the sharedmask flag to False.", "Whether the mask is shared between masked arrays can be seen from the sharedmask property. unshare_mask ensures the mask is not shared. A copy of the mask is only made if it was shared.", "See also"]}, {"name": "ma.MaskedArray.var()", "path": "reference/generated/numpy.ma.maskedarray.var", "type": "numpy.ma.MaskedArray.var", "text": ["method", "Compute the variance along the specified axis.", "Returns the variance of the array elements, a measure of the spread of a distribution. The variance is computed for the flattened array by default, otherwise over the specified axis.", "Array containing numbers whose variance is desired. If a is not an array, a conversion is attempted.", "Axis or axes along which the variance is computed. The default is to compute the variance of the flattened array.", "New in version 1.7.0.", "If this is a tuple of ints, a variance is performed over multiple axes, instead of a single axis or all the axes as before.", "Type to use in computing the variance. For arrays of integer type the default is float64; for arrays of float types it is the same as the array type.", "Alternate output array in which to place the result. It must have the same shape as the expected output, but the type is cast if necessary.", "\u201cDelta Degrees of Freedom\u201d: the divisor used in the calculation is N - ddof, where N represents the number of elements. By default ddof is zero.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.", "If the default value is passed, then keepdims will not be passed through to the var method of sub-classes of ndarray, however any non-default value will be. If the sub-class\u2019 method does not implement keepdims any exceptions will be raised.", "Elements to include in the variance. See reduce for details.", "New in version 1.20.0.", "If out=None, returns a new array containing the variance; otherwise, a reference to the output array is returned.", "See also", "The variance is the average of the squared deviations from the mean, i.e., var = mean(x), where x = abs(a - a.mean())**2.", "The mean is typically calculated as x.sum() / N, where N = len(x). If, however, ddof is specified, the divisor N - ddof is used instead. In standard statistical practice, ddof=1 provides an unbiased estimator of the variance of a hypothetical infinite population. ddof=0 provides a maximum likelihood estimate of the variance for normally distributed variables.", "Note that for complex numbers, the absolute value is taken before squaring, so that the result is always real and nonnegative.", "For floating-point input, the variance is computed using the same precision the input has. Depending on the input data, this can cause the results to be inaccurate, especially for float32 (see example below). Specifying a higher-accuracy accumulator using the dtype keyword can alleviate this issue.", "In single precision, var() can be inaccurate:", "Computing the variance in float64 is more accurate:", "Specifying a where argument:"]}, {"name": "ma.MaskedArray.view()", "path": "reference/generated/numpy.ma.maskedarray.view", "type": "Masked arrays", "text": ["method", "Return a view of the MaskedArray data.", "Data-type descriptor of the returned view, e.g., float32 or int16. The default, None, results in the view having the same data-type as a. As with ndarray.view, dtype can also be specified as an ndarray sub-class, which then specifies the type of the returned object (this is equivalent to setting the type parameter).", "Type of the returned view, either ndarray or a subclass. The default None results in type preservation.", "The value to use for invalid entries (None by default). If None, then this argument is inferred from the passed dtype, or in its absence the original array, as discussed in the notes below.", "See also", "Equivalent method on ndarray object.", "a.view() is used two different ways:", "a.view(some_dtype) or a.view(dtype=some_dtype) constructs a view of the array\u2019s memory with a different data-type. This can cause a reinterpretation of the bytes of memory.", "a.view(ndarray_subclass) or a.view(type=ndarray_subclass) just returns an instance of ndarray_subclass that looks at the same array (same shape, dtype, etc.) This does not cause a reinterpretation of the memory.", "If fill_value is not specified, but dtype is specified (and is not an ndarray sub-class), the fill_value of the MaskedArray will be reset. If neither fill_value nor dtype are specified (or if dtype is an ndarray sub-class), then the fill value is preserved. Finally, if fill_value is specified, but dtype is not, the fill value is set to the specified value.", "For a.view(some_dtype), if some_dtype has a different number of bytes per entry than the previous dtype (for example, converting a regular array to a structured array), then the behavior of the view cannot be predicted just from the superficial appearance of a (shown by print(a)). It also depends on exactly how a is stored in memory. Therefore if a is C-ordered versus fortran-ordered, versus defined as a slice or transpose, etc., the view may give different results."]}, {"name": "ma.max()", "path": "reference/generated/numpy.ma.max", "type": "numpy.ma.max", "text": ["Return the maximum along a given axis.", "Axis along which to operate. By default, axis is None and the flattened input is used.", "Alternative output array in which to place the result. Must be of the same shape and buffer length as the expected output.", "Value used to fill in the masked values. If None, use the output of maximum_fill_value().", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the array.", "New array holding the result. If out was specified, out is returned.", "See also", "Returns the maximum filling value for a given datatype."]}, {"name": "ma.maximum_fill_value()", "path": "reference/generated/numpy.ma.maximum_fill_value", "type": "numpy.ma.maximum_fill_value", "text": ["Return the minimum value that can be represented by the dtype of an object.", "This function is useful for calculating a fill value suitable for taking the maximum of an array with a given dtype.", "An object that can be queried for it\u2019s numeric type.", "The minimum representable value.", "If obj isn\u2019t a suitable numeric type.", "See also", "The inverse function.", "Set the filling value of a masked array.", "Return current fill value.", "An array of numeric data can also be passed."]}, {"name": "ma.mean()", "path": "reference/generated/numpy.ma.mean", "type": "numpy.ma.mean", "text": ["Returns the average of the array elements along given axis.", "Masked entries are ignored, and result elements which are not finite will be masked.", "Refer to numpy.mean for full documentation.", "See also", "corresponding function for ndarrays", "Equivalent function", "Weighted average."]}, {"name": "ma.median()", "path": "reference/generated/numpy.ma.median", "type": "numpy.ma.median", "text": ["Compute the median along the specified axis.", "Returns the median of the array elements.", "Input array or object that can be converted to an array.", "Axis along which the medians are computed. The default (None) is to compute the median along a flattened version of the array.", "Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output but the type will be cast if necessary.", "If True, then allow use of memory of input array (a) for calculations. The input array will be modified by the call to median. This will save memory when you do not need to preserve the contents of the input array. Treat the input as undefined, but it will probably be fully or partially sorted. Default is False. Note that, if overwrite_input is True, and the input is not already an ndarray, an error will be raised.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.", "New in version 1.10.0.", "A new array holding the result is returned unless out is specified, in which case a reference to out is returned. Return data-type is float64 for integers and floats smaller than float64, or the input data-type, otherwise.", "See also", "Given a vector V with N non masked values, the median of V is the middle value of a sorted copy of V (Vs) - i.e. Vs[(N-1)/2], when N is odd, or {Vs[N/2 - 1] + Vs[N/2]}/2 when N is even."]}, {"name": "ma.min()", "path": "reference/generated/numpy.ma.min", "type": "numpy.ma.min", "text": ["Return the minimum along a given axis.", "Axis along which to operate. By default, axis is None and the flattened input is used.", "Alternative output array in which to place the result. Must be of the same shape and buffer length as the expected output.", "Value used to fill in the masked values. If None, use the output of minimum_fill_value.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the array.", "New array holding the result. If out was specified, out is returned.", "See also", "Returns the minimum filling value for a given datatype."]}, {"name": "ma.minimum_fill_value()", "path": "reference/generated/numpy.ma.minimum_fill_value", "type": "numpy.ma.minimum_fill_value", "text": ["Return the maximum value that can be represented by the dtype of an object.", "This function is useful for calculating a fill value suitable for taking the minimum of an array with a given dtype.", "An object that can be queried for it\u2019s numeric type.", "The maximum representable value.", "If obj isn\u2019t a suitable numeric type.", "See also", "The inverse function.", "Set the filling value of a masked array.", "Return current fill value.", "An array of numeric data can also be passed."]}, {"name": "ma.mr_", "path": "reference/generated/numpy.ma.mr_", "type": "numpy.ma.mr_", "text": ["Translate slice objects to concatenation along the first axis.", "This is the masked array version of lib.index_tricks.RClass.", "See also"]}, {"name": "ma.nonzero()", "path": "reference/generated/numpy.ma.nonzero", "type": "numpy.ma.nonzero", "text": ["Return the indices of unmasked elements that are not zero.", "Returns a tuple of arrays, one for each dimension, containing the indices of the non-zero elements in that dimension. The corresponding non-zero values can be obtained with:", "To group the indices by element, rather than dimension, use instead:", "The result of this is always a 2d array, with a row for each non-zero element.", "Indices of elements that are non-zero.", "See also", "Function operating on ndarrays.", "Return indices that are non-zero in the flattened version of the input array.", "Equivalent ndarray method.", "Counts the number of non-zero elements in the input array.", "Masked elements are ignored.", "Indices can also be grouped by element.", "A common use for nonzero is to find the indices of an array, where a condition is True. Given an array a, the condition a > 3 is a boolean array and since False is interpreted as 0, ma.nonzero(a > 3) yields the indices of the a where the condition is true.", "The nonzero method of the condition array can also be called."]}, {"name": "ma.notmasked_contiguous()", "path": "reference/generated/numpy.ma.notmasked_contiguous", "type": "numpy.ma.notmasked_contiguous", "text": ["Find contiguous unmasked data in a masked array along the given axis.", "The input array.", "Axis along which to perform the operation. If None (default), applies to a flattened version of the array, and this is the same as flatnotmasked_contiguous.", "A list of slices (start and end indexes) of unmasked indexes in the array.", "If the input is 2d and axis is specified, the result is a list of lists.", "See also", "Only accepts 2-D arrays at most."]}, {"name": "ma.notmasked_edges()", "path": "reference/generated/numpy.ma.notmasked_edges", "type": "numpy.ma.notmasked_edges", "text": ["Find the indices of the first and last unmasked values along an axis.", "If all values are masked, return None. Otherwise, return a list of two tuples, corresponding to the indices of the first and last unmasked values respectively.", "The input array.", "Axis along which to perform the operation. If None (default), applies to a flattened version of the array.", "An array of start and end indexes if there are any masked data in the array. If there are no masked data in the array, edges is a list of the first and last index.", "See also"]}, {"name": "ma.ones()", "path": "reference/generated/numpy.ma.ones", "type": "numpy.ma.ones", "text": ["Return a new array of given shape and type, filled with ones.", "Shape of the new array, e.g., (2, 3) or 2.", "The desired data-type for the array, e.g., numpy.int8. Default is numpy.float64.", "Whether to store multi-dimensional data in row-major (C-style) or column-major (Fortran-style) order in memory.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "Array of ones with the given shape, dtype, and order.", "See also", "Return an array of ones with shape and type of input.", "Return a new uninitialized array.", "Return a new array setting values to zero.", "Return a new array of given shape filled with value."]}, {"name": "ma.ones_like()", "path": "reference/generated/numpy.ma.ones_like", "type": "numpy.ma.ones_like", "text": ["Return an array of ones with the same shape and type as a given array.", "The shape and data-type of a define these same attributes of the returned array.", "Overrides the data type of the result.", "New in version 1.6.0.", "Overrides the memory layout of the result. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of a as closely as possible.", "New in version 1.6.0.", "If True, then the newly created array will use the sub-class type of a, otherwise it will be a base-class array. Defaults to True.", "Overrides the shape of the result. If order=\u2019K\u2019 and the number of dimensions is unchanged, will try to keep order, otherwise, order=\u2019C\u2019 is implied.", "New in version 1.17.0.", "Array of ones with the same shape and type as a.", "See also", "Return an empty array with shape and type of input.", "Return an array of zeros with shape and type of input.", "Return a new array with shape of input filled with value.", "Return a new array setting values to one."]}, {"name": "ma.outer()", "path": "reference/generated/numpy.ma.outer", "type": "numpy.ma.outer", "text": ["Compute the outer product of two vectors.", "Given two vectors, a = [a0, a1, ..., aM] and b = [b0, b1, ..., bN], the outer product [1] is:", "First input vector. Input is flattened if not already 1-dimensional.", "Second input vector. Input is flattened if not already 1-dimensional.", "A location where the result is stored", "New in version 1.9.0.", "out[i, j] = a[i] * b[j]", "See also", "einsum('i,j->ij', a.ravel(), b.ravel()) is the equivalent.", "A generalization to dimensions other than 1D and other operations. np.multiply.outer(a.ravel(), b.ravel()) is the equivalent.", "np.tensordot(a.ravel(), b.ravel(), axes=((), ())) is the equivalent.", "Masked values are replaced by 0.", ": G. H. Golub and C. F. Van Loan, Matrix Computations, 3rd ed., Baltimore, MD, Johns Hopkins University Press, 1996, pg. 8.", "Make a (very coarse) grid for computing a Mandelbrot set:", "An example using a \u201cvector\u201d of letters:"]}, {"name": "ma.outerproduct()", "path": "reference/generated/numpy.ma.outerproduct", "type": "numpy.ma.outerproduct", "text": ["Compute the outer product of two vectors.", "Given two vectors, a = [a0, a1, ..., aM] and b = [b0, b1, ..., bN], the outer product [1] is:", "First input vector. Input is flattened if not already 1-dimensional.", "Second input vector. Input is flattened if not already 1-dimensional.", "A location where the result is stored", "New in version 1.9.0.", "out[i, j] = a[i] * b[j]", "See also", "einsum('i,j->ij', a.ravel(), b.ravel()) is the equivalent.", "A generalization to dimensions other than 1D and other operations. np.multiply.outer(a.ravel(), b.ravel()) is the equivalent.", "np.tensordot(a.ravel(), b.ravel(), axes=((), ())) is the equivalent.", "Masked values are replaced by 0.", ": G. H. Golub and C. F. Van Loan, Matrix Computations, 3rd ed., Baltimore, MD, Johns Hopkins University Press, 1996, pg. 8.", "Make a (very coarse) grid for computing a Mandelbrot set:", "An example using a \u201cvector\u201d of letters:"]}, {"name": "ma.polyfit()", "path": "reference/generated/numpy.ma.polyfit", "type": "numpy.ma.polyfit", "text": ["Least squares polynomial fit.", "Note", "This forms part of the old polynomial API. Since version 1.4, the new polynomial API defined in numpy.polynomial is preferred. A summary of the differences can be found in the transition guide.", "Fit a polynomial p(x) = p[0] * x**deg + ... + p[deg] of degree deg to points (x, y). Returns a vector of coefficients p that minimises the squared error in the order deg, deg-1, \u2026 0.", "The Polynomial.fit class method is recommended for new code as it is more stable numerically. See the documentation of the method for more information.", "x-coordinates of the M sample points (x[i], y[i]).", "y-coordinates of the sample points. Several data sets of sample points sharing the same x-coordinates can be fitted at once by passing in a 2D-array that contains one dataset per column.", "Degree of the fitting polynomial", "Relative condition number of the fit. Singular values smaller than this relative to the largest singular value will be ignored. The default value is len(x)*eps, where eps is the relative precision of the float type, about 2e-16 in most cases.", "Switch determining nature of return value. When it is False (the default) just the coefficients are returned, when True diagnostic information from the singular value decomposition is also returned.", "Weights. If not None, the weight w[i] applies to the unsquared residual y[i] - y_hat[i] at x[i]. Ideally the weights are chosen so that the errors of the products w[i]*y[i] all have the same variance. When using inverse-variance weighting, use w[i] = 1/sigma(y[i]). The default value is None.", "If given and not False, return not just the estimate but also its covariance matrix. By default, the covariance are scaled by chi2/dof, where dof = M - (deg + 1), i.e., the weights are presumed to be unreliable except in a relative sense and everything is scaled such that the reduced chi2 is unity. This scaling is omitted if cov='unscaled', as is relevant for the case that the weights are w = 1/sigma, with sigma known to be a reliable estimate of the uncertainty.", "Polynomial coefficients, highest power first. If y was 2-D, the coefficients for k-th data set are in p[:,k].", "These values are only returned if full == True", "coefficient matrix", "coefficient matrix", "For more details, see numpy.linalg.lstsq.", "Present only if full == False and cov == True. The covariance matrix of the polynomial coefficient estimates. The diagonal of this matrix are the variance estimates for each coefficient. If y is a 2-D array, then the covariance matrix for the k-th data set are in V[:,:,k]", "The rank of the coefficient matrix in the least-squares fit is deficient. The warning is only raised if full == False.", "The warnings can be turned off by", "See also", "Compute polynomial values.", "Computes a least-squares fit.", "Computes spline fits.", "Any masked values in x is propagated in y, and vice-versa.", "The solution minimizes the squared error", "in the equations:", "The coefficient matrix of the coefficients p is a Vandermonde matrix.", "polyfit issues a RankWarning when the least-squares fit is badly conditioned. This implies that the best fit is not well-defined due to numerical error. The results may be improved by lowering the polynomial degree or by replacing x by x - x.mean(). The rcond parameter can also be set to a value smaller than its default, but the resulting fit may be spurious: including contributions from the small singular values can add numerical noise to the result.", "Note that fitting polynomial coefficients is inherently badly conditioned when the degree of the polynomial is large or the interval of sample points is badly centered. The quality of the fit should always be checked in these cases. When polynomial fits are not satisfactory, splines may be a good alternative.", "Wikipedia, \u201cCurve fitting\u201d, https://en.wikipedia.org/wiki/Curve_fitting", "Wikipedia, \u201cPolynomial interpolation\u201d, https://en.wikipedia.org/wiki/Polynomial_interpolation", "It is convenient to use poly1d objects for dealing with polynomials:", "High-order polynomials may oscillate wildly:", "Illustration:"]}, {"name": "ma.power()", "path": "reference/generated/numpy.ma.power", "type": "numpy.ma.power", "text": ["Returns element-wise base array raised to power from second array.", "This is the masked array version of numpy.power. For details see numpy.power.", "See also", "The out argument to numpy.power is not supported, third has to be None."]}, {"name": "ma.prod()", "path": "reference/generated/numpy.ma.prod", "type": "numpy.ma.prod", "text": ["Return the product of the array elements over the given axis.", "Masked elements are set to 1 internally for computation.", "Refer to numpy.prod for full documentation.", "See also", "corresponding function for ndarrays", "equivalent function", "Arithmetic is modular when using integer types, and no error is raised on overflow."]}, {"name": "ma.ptp()", "path": "reference/generated/numpy.ma.ptp", "type": "numpy.ma.ptp", "text": ["Return (maximum - minimum) along the given dimension (i.e. peak-to-peak value).", "Warning", "ptp preserves the data type of the array. This means the return value for an input of signed integers with n bits (e.g. np.int8, np.int16, etc) is also a signed integer with n bits. In that case, peak-to-peak values greater than 2**(n-1)-1 will be returned as negative values. An example with a work-around is shown below.", "Axis along which to find the peaks. If None (default) the flattened array is used.", "Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output but the type will be cast if necessary.", "Value used to fill in the masked values.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the array.", "A new array holding the result, unless out was specified, in which case a reference to out is returned.", "This example shows that a negative value can be returned when the input is an array of signed integers.", "A work-around is to use the view() method to view the result as unsigned integers with the same bit width:"]}, {"name": "ma.ravel()", "path": "reference/generated/numpy.ma.ravel", "type": "numpy.ma.ravel", "text": ["Returns a 1D version of self, as a view.", "The elements of a are read using this index order. \u2018C\u2019 means to index the elements in C-like order, with the last axis index changing fastest, back to the first axis index changing slowest. \u2018F\u2019 means to index the elements in Fortran-like index order, with the first index changing fastest, and the last index changing slowest. Note that the \u2018C\u2019 and \u2018F\u2019 options take no account of the memory layout of the underlying array, and only refer to the order of axis indexing. \u2018A\u2019 means to read the elements in Fortran-like index order if m is Fortran contiguous in memory, C-like order otherwise. \u2018K\u2019 means to read the elements in the order they occur in memory, except for reversing the data when strides are negative. By default, \u2018C\u2019 index order is used.", "Output view is of shape (self.size,) (or (np.ma.product(self.shape),))."]}, {"name": "ma.reshape()", "path": "reference/generated/numpy.ma.reshape", "type": "numpy.ma.reshape", "text": ["Returns an array containing the same data with a new shape.", "Refer to MaskedArray.reshape for full documentation.", "See also", "equivalent function"]}, {"name": "ma.resize()", "path": "reference/generated/numpy.ma.resize", "type": "numpy.ma.resize", "text": ["Return a new masked array with the specified size and shape.", "This is the masked equivalent of the numpy.resize function. The new array is filled with repeated copies of x (in the order that the data are stored in memory). If x is masked, the new array will be masked, and the new mask will be a repetition of the old one.", "See also", "Equivalent function in the top level NumPy module.", "A MaskedArray is always returned, regardless of the input type."]}, {"name": "ma.round()", "path": "reference/generated/numpy.ma.round", "type": "numpy.ma.round", "text": ["Return a copy of a, rounded to \u2018decimals\u2019 places.", "When \u2018decimals\u2019 is negative, it specifies the number of positions to the left of the decimal point. The real and imaginary parts of complex numbers are rounded separately. Nothing is done if the array is not of float type and \u2018decimals\u2019 is greater than or equal to 0.", "Number of decimals to round to. May be negative.", "Existing array to use for output. If not given, returns a default copy of a.", "If out is given and does not have a mask attribute, the mask of a is lost!"]}, {"name": "ma.row_stack()", "path": "reference/generated/numpy.ma.row_stack", "type": "numpy.ma.row_stack", "text": ["Stack arrays in sequence vertically (row wise).", "This is equivalent to concatenation along the first axis after 1-D arrays of shape (N,) have been reshaped to (1,N). Rebuilds arrays divided by vsplit.", "This function makes most sense for arrays with up to 3 dimensions. For instance, for pixel-data with a height (first axis), width (second axis), and r/g/b channels (third axis). The functions concatenate, stack and block provide more general stacking and concatenation operations.", "The arrays must have the same shape along all but the first axis. 1-D arrays must have the same length.", "The array formed by stacking the given arrays, will be at least 2-D.", "See also", "Join a sequence of arrays along an existing axis.", "Join a sequence of arrays along a new axis.", "Assemble an nd-array from nested lists of blocks.", "Stack arrays in sequence horizontally (column wise).", "Stack arrays in sequence depth wise (along third axis).", "Stack 1-D arrays as columns into a 2-D array.", "Split an array into multiple sub-arrays vertically (row-wise).", "The function is applied to both the _data and the _mask, if any."]}, {"name": "ma.set_fill_value()", "path": "reference/generated/numpy.ma.set_fill_value", "type": "numpy.ma.set_fill_value", "text": ["Set the filling value of a, if a is a masked array.", "This function changes the fill value of the masked array a in place. If a is not a masked array, the function returns silently, without doing anything.", "Input array.", "Filling value. A consistency test is performed to make sure the value is compatible with the dtype of a.", "Nothing returned by this function.", "See also", "Return the default fill value for a dtype.", "Return current fill value.", "Equivalent method.", "Nothing happens if a is not a masked array."]}, {"name": "ma.shape()", "path": "reference/generated/numpy.ma.shape", "type": "numpy.ma.shape", "text": ["Return the shape of an array.", "Input array.", "The elements of the shape tuple give the lengths of the corresponding array dimensions.", "See also", "Equivalent array method."]}, {"name": "ma.size()", "path": "reference/generated/numpy.ma.size", "type": "numpy.ma.size", "text": ["Return the number of elements along a given axis.", "Input data.", "Axis along which the elements are counted. By default, give the total number of elements.", "Number of elements along the specified axis.", "See also", "dimensions of array", "dimensions of array", "number of elements in array"]}, {"name": "ma.soften_mask()", "path": "reference/generated/numpy.ma.soften_mask", "type": "numpy.ma.soften_mask", "text": ["Force the mask to soft.", "Whether the mask of a masked array is hard or soft is determined by its hardmask property. soften_mask sets hardmask to False.", "See also"]}, {"name": "ma.sort()", "path": "reference/generated/numpy.ma.sort", "type": "numpy.ma.sort", "text": ["Return a sorted copy of the masked array.", "Equivalent to creating a copy of the array and applying the MaskedArray sort() method.", "Refer to MaskedArray.sort for the full documentation", "See also", "equivalent method"]}, {"name": "ma.squeeze()", "path": "reference/generated/numpy.ma.squeeze", "type": "numpy.ma.squeeze", "text": ["Remove axes of length one from a.", "Input data.", "New in version 1.7.0.", "Selects a subset of the entries of length one in the shape. If an axis is selected with shape entry greater than one, an error is raised.", "The input array, but with all or a subset of the dimensions of length 1 removed. This is always a itself or a view into a. Note that if all axes are squeezed, the result is a 0d array and not a scalar.", "If axis is not None, and an axis being squeezed is not of length 1", "See also", "The inverse operation, adding entries of length one", "Insert, remove, and combine dimensions, and resize existing ones"]}, {"name": "ma.stack()", "path": "reference/generated/numpy.ma.stack", "type": "numpy.ma.stack", "text": ["Join a sequence of arrays along a new axis.", "The axis parameter specifies the index of the new axis in the dimensions of the result. For example, if axis=0 it will be the first dimension and if axis=-1 it will be the last dimension.", "New in version 1.10.0.", "Each array must have the same shape.", "The axis in the result array along which the input arrays are stacked.", "If provided, the destination to place the result. The shape must be correct, matching that of what stack would have returned if no out argument were specified.", "The stacked array has one more dimension than the input arrays.", "See also", "Join a sequence of arrays along an existing axis.", "Assemble an nd-array from nested lists of blocks.", "Split array into a list of multiple sub-arrays of equal size.", "The function is applied to both the _data and the _mask, if any."]}, {"name": "ma.std()", "path": "reference/generated/numpy.ma.std", "type": "numpy.ma.std", "text": ["Returns the standard deviation of the array elements along given axis.", "Masked entries are ignored.", "Refer to numpy.std for full documentation.", "See also", "corresponding function for ndarrays", "Equivalent function"]}, {"name": "ma.sum()", "path": "reference/generated/numpy.ma.sum", "type": "numpy.ma.sum", "text": ["Return the sum of the array elements over the given axis.", "Masked elements are set to 0 internally.", "Refer to numpy.sum for full documentation.", "See also", "corresponding function for ndarrays", "equivalent function"]}, {"name": "ma.swapaxes()", "path": "reference/generated/numpy.ma.swapaxes", "type": "numpy.ma.swapaxes", "text": ["Return a view of the array with axis1 and axis2 interchanged.", "Refer to numpy.swapaxes for full documentation.", "See also", "equivalent function"]}, {"name": "ma.trace()", "path": "reference/generated/numpy.ma.trace", "type": "numpy.ma.trace", "text": ["Return the sum along diagonals of the array.", "Refer to numpy.trace for full documentation.", "See also", "equivalent function"]}, {"name": "ma.transpose()", "path": "reference/generated/numpy.ma.transpose", "type": "numpy.ma.transpose", "text": ["Permute the dimensions of an array.", "This function is exactly equivalent to numpy.transpose.", "See also", "Equivalent function in top-level NumPy module."]}, {"name": "ma.vander()", "path": "reference/generated/numpy.ma.vander", "type": "numpy.ma.vander", "text": ["Generate a Vandermonde matrix.", "The columns of the output matrix are powers of the input vector. The order of the powers is determined by the increasing boolean argument. Specifically, when increasing is False, the i-th output column is the input vector raised element-wise to the power of N - i - 1. Such a matrix with a geometric progression in each row is named for Alexandre- Theophile Vandermonde.", "1-D input array.", "Number of columns in the output. If N is not specified, a square array is returned (N = len(x)).", "Order of the powers of the columns. If True, the powers increase from left to right, if False (the default) they are reversed.", "New in version 1.9.0.", "Vandermonde matrix. If increasing is False, the first column is x^(N-1), the second x^(N-2) and so forth. If increasing is True, the columns are x^0, x^1, ..., x^(N-1).", "See also", "Masked values in the input array result in rows of zeros.", "The determinant of a square Vandermonde matrix is the product of the differences between the values of the input vector:"]}, {"name": "ma.var()", "path": "reference/generated/numpy.ma.var", "type": "numpy.ma.var", "text": ["Compute the variance along the specified axis.", "Returns the variance of the array elements, a measure of the spread of a distribution. The variance is computed for the flattened array by default, otherwise over the specified axis.", "Array containing numbers whose variance is desired. If a is not an array, a conversion is attempted.", "Axis or axes along which the variance is computed. The default is to compute the variance of the flattened array.", "New in version 1.7.0.", "If this is a tuple of ints, a variance is performed over multiple axes, instead of a single axis or all the axes as before.", "Type to use in computing the variance. For arrays of integer type the default is float64; for arrays of float types it is the same as the array type.", "Alternate output array in which to place the result. It must have the same shape as the expected output, but the type is cast if necessary.", "\u201cDelta Degrees of Freedom\u201d: the divisor used in the calculation is N - ddof, where N represents the number of elements. By default ddof is zero.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.", "If the default value is passed, then keepdims will not be passed through to the var method of sub-classes of ndarray, however any non-default value will be. If the sub-class\u2019 method does not implement keepdims any exceptions will be raised.", "Elements to include in the variance. See reduce for details.", "New in version 1.20.0.", "If out=None, returns a new array containing the variance; otherwise, a reference to the output array is returned.", "See also", "The variance is the average of the squared deviations from the mean, i.e., var = mean(x), where x = abs(a - a.mean())**2.", "The mean is typically calculated as x.sum() / N, where N = len(x). If, however, ddof is specified, the divisor N - ddof is used instead. In standard statistical practice, ddof=1 provides an unbiased estimator of the variance of a hypothetical infinite population. ddof=0 provides a maximum likelihood estimate of the variance for normally distributed variables.", "Note that for complex numbers, the absolute value is taken before squaring, so that the result is always real and nonnegative.", "For floating-point input, the variance is computed using the same precision the input has. Depending on the input data, this can cause the results to be inaccurate, especially for float32 (see example below). Specifying a higher-accuracy accumulator using the dtype keyword can alleviate this issue.", "In single precision, var() can be inaccurate:", "Computing the variance in float64 is more accurate:", "Specifying a where argument:"]}, {"name": "ma.vstack()", "path": "reference/generated/numpy.ma.vstack", "type": "numpy.ma.vstack", "text": ["Stack arrays in sequence vertically (row wise).", "This is equivalent to concatenation along the first axis after 1-D arrays of shape (N,) have been reshaped to (1,N). Rebuilds arrays divided by vsplit.", "This function makes most sense for arrays with up to 3 dimensions. For instance, for pixel-data with a height (first axis), width (second axis), and r/g/b channels (third axis). The functions concatenate, stack and block provide more general stacking and concatenation operations.", "The arrays must have the same shape along all but the first axis. 1-D arrays must have the same length.", "The array formed by stacking the given arrays, will be at least 2-D.", "See also", "Join a sequence of arrays along an existing axis.", "Join a sequence of arrays along a new axis.", "Assemble an nd-array from nested lists of blocks.", "Stack arrays in sequence horizontally (column wise).", "Stack arrays in sequence depth wise (along third axis).", "Stack 1-D arrays as columns into a 2-D array.", "Split an array into multiple sub-arrays vertically (row-wise).", "The function is applied to both the _data and the _mask, if any."]}, {"name": "ma.where()", "path": "reference/generated/numpy.ma.where", "type": "numpy.ma.where", "text": ["Return a masked array with elements from x or y, depending on condition.", "Note", "When only condition is provided, this function is identical to nonzero. The rest of this documentation covers only the case where all three arguments are provided.", "Where True, yield x, otherwise yield y.", "Values from which to choose. x, y and condition need to be broadcastable to some shape.", "An masked array with masked elements where the condition is masked, elements from x where condition is True, and elements from y elsewhere.", "See also", "Equivalent function in the top-level NumPy module.", "The function that is called when x and y are omitted"]}, {"name": "ma.zeros()", "path": "reference/generated/numpy.ma.zeros", "type": "numpy.ma.zeros", "text": ["Return a new array of given shape and type, filled with zeros.", "Shape of the new array, e.g., (2, 3) or 2.", "The desired data-type for the array, e.g., numpy.int8. Default is numpy.float64.", "Whether to store multi-dimensional data in row-major (C-style) or column-major (Fortran-style) order in memory.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "Array of zeros with the given shape, dtype, and order.", "See also", "Return an array of zeros with shape and type of input.", "Return a new uninitialized array.", "Return a new array setting values to one.", "Return a new array of given shape filled with value."]}, {"name": "ma.zeros_like()", "path": "reference/generated/numpy.ma.zeros_like", "type": "numpy.ma.zeros_like", "text": ["Return an array of zeros with the same shape and type as a given array.", "The shape and data-type of a define these same attributes of the returned array.", "Overrides the data type of the result.", "New in version 1.6.0.", "Overrides the memory layout of the result. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of a as closely as possible.", "New in version 1.6.0.", "If True, then the newly created array will use the sub-class type of a, otherwise it will be a base-class array. Defaults to True.", "Overrides the shape of the result. If order=\u2019K\u2019 and the number of dimensions is unchanged, will try to keep order, otherwise, order=\u2019C\u2019 is implied.", "New in version 1.17.0.", "Array of zeros with the same shape and type as a.", "See also", "Return an empty array with shape and type of input.", "Return an array of ones with shape and type of input.", "Return a new array with shape of input filled with value.", "Return a new array setting values to zero."]}, {"name": "make_config_py()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.make_config_py", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": ["Generate package __config__.py file containing system_info information used during building the package.", "This file is installed to the package installation directory."]}, {"name": "make_svn_version_py()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.make_svn_version_py", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": ["Appends a data function to the data_files list that will generate __svn_version__.py file to the current package directory.", "Generate package __svn_version__.py file from SVN revision number, it will be removed after python exits but will be available when sdist, etc commands are executed.", "If __svn_version__.py existed before, nothing is done.", "This is intended for working with source directories that are in an SVN repository."]}, {"name": "Masked array operations", "path": "reference/routines.ma", "type": "Masked array operations", "text": ["ma.MaskType", "alias of numpy.bool_", "ma.masked_array", "alias of numpy.ma.core.MaskedArray", "ma.array(data[, dtype, copy, order, mask, ...])", "An array class with possibly masked values.", "ma.copy(self, *args, **params) a.copy(order=)", "Return a copy of the array.", "ma.frombuffer(buffer[, dtype, count, ...])", "Interpret a buffer as a 1-dimensional array.", "ma.fromfunction(function, shape, **dtype)", "Construct an array by executing a function over each coordinate.", "ma.MaskedArray.copy([order])", "Return a copy of the array.", "ma.empty(shape[, dtype, order, like])", "Return a new array of given shape and type, without initializing entries.", "ma.empty_like(prototype[, dtype, order, ...])", "Return a new array with the same shape and type as a given array.", "ma.masked_all(shape[, dtype])", "Empty masked array with all elements masked.", "ma.masked_all_like(arr)", "Empty masked array with the properties of an existing array.", "ma.ones(shape[, dtype, order])", "Return a new array of given shape and type, filled with ones.", "ma.ones_like(*args, **kwargs)", "Return an array of ones with the same shape and type as a given array.", "ma.zeros(shape[, dtype, order, like])", "Return a new array of given shape and type, filled with zeros.", "ma.zeros_like(*args, **kwargs)", "Return an array of zeros with the same shape and type as a given array.", "ma.all(self[, axis, out, keepdims])", "Returns True if all elements evaluate to True.", "ma.any(self[, axis, out, keepdims])", "Returns True if any of the elements of a evaluate to True.", "ma.count(self[, axis, keepdims])", "Count the non-masked elements of the array along the given axis.", "ma.count_masked(arr[, axis])", "Count the number of masked elements along the given axis.", "ma.getmask(a)", "Return the mask of a masked array, or nomask.", "ma.getmaskarray(arr)", "Return the mask of a masked array, or full boolean array of False.", "ma.getdata(a[, subok])", "Return the data of a masked array as an ndarray.", "ma.nonzero(self)", "Return the indices of unmasked elements that are not zero.", "ma.shape(obj)", "Return the shape of an array.", "ma.size(obj[, axis])", "Return the number of elements along a given axis.", "ma.is_masked(x)", "Determine whether input has masked values.", "ma.is_mask(m)", "Return True if m is a valid, standard mask.", "ma.isMaskedArray(x)", "Test whether input is an instance of MaskedArray.", "ma.isMA(x)", "Test whether input is an instance of MaskedArray.", "ma.isarray(x)", "Test whether input is an instance of MaskedArray.", "ma.MaskedArray.all([axis, out, keepdims])", "Returns True if all elements evaluate to True.", "ma.MaskedArray.any([axis, out, keepdims])", "Returns True if any of the elements of a evaluate to True.", "ma.MaskedArray.count([axis, keepdims])", "Count the non-masked elements of the array along the given axis.", "ma.MaskedArray.nonzero()", "Return the indices of unmasked elements that are not zero.", "ma.shape(obj)", "Return the shape of an array.", "ma.size(obj[, axis])", "Return the number of elements along a given axis.", "ma.MaskedArray.data", "Returns the underlying data, as a view of the masked array.", "ma.MaskedArray.mask", "Current mask.", "ma.MaskedArray.recordmask", "Get or set the mask of the array if it has no named fields.", "ma.ravel(self[, order])", "Returns a 1D version of self, as a view.", "ma.reshape(a, new_shape[, order])", "Returns an array containing the same data with a new shape.", "ma.resize(x, new_shape)", "Return a new masked array with the specified size and shape.", "ma.MaskedArray.flatten([order])", "Return a copy of the array collapsed into one dimension.", "ma.MaskedArray.ravel([order])", "Returns a 1D version of self, as a view.", "ma.MaskedArray.reshape(*s, **kwargs)", "Give a new shape to the array without changing its data.", "ma.MaskedArray.resize(newshape[, refcheck, ...])", "ma.swapaxes(self, *args, ...)", "Return a view of the array with axis1 and axis2 interchanged.", "ma.transpose(a[, axes])", "Permute the dimensions of an array.", "ma.MaskedArray.swapaxes(axis1, axis2)", "Return a view of the array with axis1 and axis2 interchanged.", "ma.MaskedArray.transpose(*axes)", "Returns a view of the array with axes transposed.", "ma.atleast_1d(*args, **kwargs)", "Convert inputs to arrays with at least one dimension.", "ma.atleast_2d(*args, **kwargs)", "View inputs as arrays with at least two dimensions.", "ma.atleast_3d(*args, **kwargs)", "View inputs as arrays with at least three dimensions.", "ma.expand_dims(a, axis)", "Expand the shape of an array.", "ma.squeeze(*args, **kwargs)", "Remove axes of length one from a.", "ma.MaskedArray.squeeze([axis])", "Remove axes of length one from a.", "ma.stack(*args, **kwargs)", "Join a sequence of arrays along a new axis.", "ma.column_stack(*args, **kwargs)", "Stack 1-D arrays as columns into a 2-D array.", "ma.concatenate(arrays[, axis])", "Concatenate a sequence of arrays along the given axis.", "ma.dstack(*args, **kwargs)", "Stack arrays in sequence depth wise (along third axis).", "ma.hstack(*args, **kwargs)", "Stack arrays in sequence horizontally (column wise).", "ma.hsplit(*args, **kwargs)", "Split an array into multiple sub-arrays horizontally (column-wise).", "ma.mr_", "Translate slice objects to concatenation along the first axis.", "ma.row_stack(*args, **kwargs)", "Stack arrays in sequence vertically (row wise).", "ma.vstack(*args, **kwargs)", "Stack arrays in sequence vertically (row wise).", "ma.concatenate(arrays[, axis])", "Concatenate a sequence of arrays along the given axis.", "ma.stack(*args, **kwargs)", "Join a sequence of arrays along a new axis.", "ma.vstack(*args, **kwargs)", "Stack arrays in sequence vertically (row wise).", "ma.hstack(*args, **kwargs)", "Stack arrays in sequence horizontally (column wise).", "ma.dstack(*args, **kwargs)", "Stack arrays in sequence depth wise (along third axis).", "ma.column_stack(*args, **kwargs)", "Stack 1-D arrays as columns into a 2-D array.", "ma.append(a, b[, axis])", "Append values to the end of an array.", "ma.make_mask(m[, copy, shrink, dtype])", "Create a boolean mask from an array.", "ma.make_mask_none(newshape[, dtype])", "Return a boolean mask of the given shape, filled with False.", "ma.mask_or(m1, m2[, copy, shrink])", "Combine two masks with the logical_or operator.", "ma.make_mask_descr(ndtype)", "Construct a dtype description list from a given dtype.", "ma.getmask(a)", "Return the mask of a masked array, or nomask.", "ma.getmaskarray(arr)", "Return the mask of a masked array, or full boolean array of False.", "ma.masked_array.mask", "Current mask.", "ma.flatnotmasked_contiguous(a)", "Find contiguous unmasked data in a masked array along the given axis.", "ma.flatnotmasked_edges(a)", "Find the indices of the first and last unmasked values.", "ma.notmasked_contiguous(a[, axis])", "Find contiguous unmasked data in a masked array along the given axis.", "ma.notmasked_edges(a[, axis])", "Find the indices of the first and last unmasked values along an axis.", "ma.clump_masked(a)", "Returns a list of slices corresponding to the masked clumps of a 1-D array.", "ma.clump_unmasked(a)", "Return list of slices corresponding to the unmasked clumps of a 1-D array.", "ma.mask_cols(a[, axis])", "Mask columns of a 2D array that contain masked values.", "ma.mask_or(m1, m2[, copy, shrink])", "Combine two masks with the logical_or operator.", "ma.mask_rowcols(a[, axis])", "Mask rows and/or columns of a 2D array that contain masked values.", "ma.mask_rows(a[, axis])", "Mask rows of a 2D array that contain masked values.", "ma.harden_mask(self)", "Force the mask to hard.", "ma.soften_mask(self)", "Force the mask to soft.", "ma.MaskedArray.harden_mask()", "Force the mask to hard.", "ma.MaskedArray.soften_mask()", "Force the mask to soft.", "ma.MaskedArray.shrink_mask()", "Reduce a mask to nomask when possible.", "ma.MaskedArray.unshare_mask()", "Copy the mask and set the sharedmask flag to False.", "ma.asarray(a[, dtype, order])", "Convert the input to a masked array of the given data-type.", "ma.asanyarray(a[, dtype])", "Convert the input to a masked array, conserving subclasses.", "ma.fix_invalid(a[, mask, copy, fill_value])", "Return input with invalid data masked and replaced by a fill value.", "ma.masked_equal(x, value[, copy])", "Mask an array where equal to a given value.", "ma.masked_greater(x, value[, copy])", "Mask an array where greater than a given value.", "ma.masked_greater_equal(x, value[, copy])", "Mask an array where greater than or equal to a given value.", "ma.masked_inside(x, v1, v2[, copy])", "Mask an array inside a given interval.", "ma.masked_invalid(a[, copy])", "Mask an array where invalid values occur (NaNs or infs).", "ma.masked_less(x, value[, copy])", "Mask an array where less than a given value.", "ma.masked_less_equal(x, value[, copy])", "Mask an array where less than or equal to a given value.", "ma.masked_not_equal(x, value[, copy])", "Mask an array where not equal to a given value.", "ma.masked_object(x, value[, copy, shrink])", "Mask the array x where the data are exactly equal to value.", "ma.masked_outside(x, v1, v2[, copy])", "Mask an array outside a given interval.", "ma.masked_values(x, value[, rtol, atol, ...])", "Mask using floating point equality.", "ma.masked_where(condition, a[, copy])", "Mask an array where a condition is met.", "ma.compress_cols(a)", "Suppress whole columns of a 2-D array that contain masked values.", "ma.compress_rowcols(x[, axis])", "Suppress the rows and/or columns of a 2-D array that contain masked values.", "ma.compress_rows(a)", "Suppress whole rows of a 2-D array that contain masked values.", "ma.compressed(x)", "Return all the non-masked data as a 1-D array.", "ma.filled(a[, fill_value])", "Return input as an array with masked data replaced by a fill value.", "ma.MaskedArray.compressed()", "Return all the non-masked data as a 1-D array.", "ma.MaskedArray.filled([fill_value])", "Return a copy of self, with masked values filled with a given value.", "ma.MaskedArray.tofile(fid[, sep, format])", "Save a masked array to a file in binary format.", "ma.MaskedArray.tolist([fill_value])", "Return the data portion of the masked array as a hierarchical Python list.", "ma.MaskedArray.torecords()", "Transforms a masked array into a flexible-type array.", "ma.MaskedArray.tobytes([fill_value, order])", "Return the array data as a string containing the raw bytes in the array.", "ma.common_fill_value(a, b)", "Return the common filling value of two masked arrays, if any.", "ma.default_fill_value(obj)", "Return the default fill value for the argument object.", "ma.maximum_fill_value(obj)", "Return the minimum value that can be represented by the dtype of an object.", "ma.minimum_fill_value(obj)", "Return the maximum value that can be represented by the dtype of an object.", "ma.set_fill_value(a, fill_value)", "Set the filling value of a, if a is a masked array.", "ma.MaskedArray.get_fill_value()", "The filling value of the masked array is a scalar.", "ma.MaskedArray.set_fill_value([value])", "ma.MaskedArray.fill_value", "The filling value of the masked array is a scalar.", "ma.anom(self[, axis, dtype])", "Compute the anomalies (deviations from the arithmetic mean) along the given axis.", "ma.anomalies(self[, axis, dtype])", "Compute the anomalies (deviations from the arithmetic mean) along the given axis.", "ma.average(a[, axis, weights, returned])", "Return the weighted average of array over the given axis.", "ma.conjugate(x, /[, out, where, casting, ...])", "Return the complex conjugate, element-wise.", "ma.corrcoef(x[, y, rowvar, bias, ...])", "Return Pearson product-moment correlation coefficients.", "ma.cov(x[, y, rowvar, bias, allow_masked, ddof])", "Estimate the covariance matrix.", "ma.cumsum(self[, axis, dtype, out])", "Return the cumulative sum of the array elements over the given axis.", "ma.cumprod(self[, axis, dtype, out])", "Return the cumulative product of the array elements over the given axis.", "ma.mean(self[, axis, dtype, out, keepdims])", "Returns the average of the array elements along given axis.", "ma.median(a[, axis, out, overwrite_input, ...])", "Compute the median along the specified axis.", "ma.power(a, b[, third])", "Returns element-wise base array raised to power from second array.", "ma.prod(self[, axis, dtype, out, keepdims])", "Return the product of the array elements over the given axis.", "ma.std(self[, axis, dtype, out, ddof, keepdims])", "Returns the standard deviation of the array elements along given axis.", "ma.sum(self[, axis, dtype, out, keepdims])", "Return the sum of the array elements over the given axis.", "ma.var(self[, axis, dtype, out, ddof, keepdims])", "Compute the variance along the specified axis.", "ma.MaskedArray.anom([axis, dtype])", "Compute the anomalies (deviations from the arithmetic mean) along the given axis.", "ma.MaskedArray.cumprod([axis, dtype, out])", "Return the cumulative product of the array elements over the given axis.", "ma.MaskedArray.cumsum([axis, dtype, out])", "Return the cumulative sum of the array elements over the given axis.", "ma.MaskedArray.mean([axis, dtype, out, keepdims])", "Returns the average of the array elements along given axis.", "ma.MaskedArray.prod([axis, dtype, out, keepdims])", "Return the product of the array elements over the given axis.", "ma.MaskedArray.std([axis, dtype, out, ddof, ...])", "Returns the standard deviation of the array elements along given axis.", "ma.MaskedArray.sum([axis, dtype, out, keepdims])", "Return the sum of the array elements over the given axis.", "ma.MaskedArray.var([axis, dtype, out, ddof, ...])", "Compute the variance along the specified axis.", "ma.argmax(self[, axis, fill_value, out])", "Returns array of indices of the maximum values along the given axis.", "ma.argmin(self[, axis, fill_value, out])", "Return array of indices to the minimum values along the given axis.", "ma.max(obj[, axis, out, fill_value, keepdims])", "Return the maximum along a given axis.", "ma.min(obj[, axis, out, fill_value, keepdims])", "Return the minimum along a given axis.", "ma.ptp(obj[, axis, out, fill_value, keepdims])", "Return (maximum - minimum) along the given dimension (i.e.", "ma.diff(*args, **kwargs)", "Calculate the n-th discrete difference along the given axis.", "ma.MaskedArray.argmax([axis, fill_value, ...])", "Returns array of indices of the maximum values along the given axis.", "ma.MaskedArray.argmin([axis, fill_value, ...])", "Return array of indices to the minimum values along the given axis.", "ma.MaskedArray.max([axis, out, fill_value, ...])", "Return the maximum along a given axis.", "ma.MaskedArray.min([axis, out, fill_value, ...])", "Return the minimum along a given axis.", "ma.MaskedArray.ptp([axis, out, fill_value, ...])", "Return (maximum - minimum) along the given dimension (i.e.", "ma.argsort(a[, axis, kind, order, endwith, ...])", "Return an ndarray of indices that sort the array along the specified axis.", "ma.sort(a[, axis, kind, order, endwith, ...])", "Return a sorted copy of the masked array.", "ma.MaskedArray.argsort([axis, kind, order, ...])", "Return an ndarray of indices that sort the array along the specified axis.", "ma.MaskedArray.sort([axis, kind, order, ...])", "Sort the array, in-place", "ma.diag(v[, k])", "Extract a diagonal or construct a diagonal array.", "ma.dot(a, b[, strict, out])", "Return the dot product of two arrays.", "ma.identity(n[, dtype])", "Return the identity array.", "ma.inner(a, b, /)", "Inner product of two arrays.", "ma.innerproduct(a, b, /)", "Inner product of two arrays.", "ma.outer(a, b)", "Compute the outer product of two vectors.", "ma.outerproduct(a, b)", "Compute the outer product of two vectors.", "ma.trace(self[, offset, axis1, axis2, ...])", "Return the sum along diagonals of the array.", "ma.transpose(a[, axes])", "Permute the dimensions of an array.", "ma.MaskedArray.trace([offset, axis1, axis2, ...])", "Return the sum along diagonals of the array.", "ma.MaskedArray.transpose(*axes)", "Returns a view of the array with axes transposed.", "ma.vander(x[, n])", "Generate a Vandermonde matrix.", "ma.polyfit(x, y, deg[, rcond, full, w, cov])", "Least squares polynomial fit.", "ma.around", "Round an array to the given number of decimals.", "ma.clip(*args, **kwargs)", "Clip (limit) the values in an array.", "ma.round(a[, decimals, out])", "Return a copy of a, rounded to 'decimals' places.", "ma.MaskedArray.clip([min, max, out])", "Return an array whose values are limited to [min, max].", "ma.MaskedArray.round([decimals, out])", "Return each element rounded to the given number of decimals.", "ma.allequal(a, b[, fill_value])", "Return True if all entries of a and b are equal, using fill_value as a truth value where either or both are masked.", "ma.allclose(a, b[, masked_equal, rtol, atol])", "Returns True if two arrays are element-wise equal within a tolerance.", "ma.apply_along_axis(func1d, axis, arr, ...)", "Apply a function to 1-D slices along the given axis.", "ma.apply_over_axes(func, a, axes)", "Apply a function repeatedly over multiple axes.", "ma.arange([start,] stop[, step,][, dtype, like])", "Return evenly spaced values within a given interval.", "ma.choose(indices, choices[, out, mode])", "Use an index array to construct a new array from a list of choices.", "ma.ediff1d(arr[, to_end, to_begin])", "Compute the differences between consecutive elements of an array.", "ma.indices(dimensions[, dtype, sparse])", "Return an array representing the indices of a grid.", "ma.where(condition[, x, y])", "Return a masked array with elements from x or y, depending on condition."]}, {"name": "Masked arrays", "path": "reference/maskedarray", "type": "Masked arrays", "text": ["Masked arrays are arrays that may have missing or invalid entries. The numpy.ma module provides a nearly work-alike replacement for numpy that supports data arrays with masks."]}, {"name": "MaskedArray.baseclass", "path": "reference/maskedarray.baseclass#numpy.ma.MaskedArray.baseclass", "type": "Constants of the \n        \n         numpy.ma\n        \n        module", "text": ["In addition to the MaskedArray class, the numpy.ma module defines several constants.", "The masked constant is a special case of MaskedArray, with a float datatype and a null shape. It is used to test whether a specific entry of a masked array is masked, or to mask one or several entries of a masked array:", "Value indicating that a masked array has no invalid entry. nomask is used internally to speed up computations when the mask is not needed. It is represented internally as np.False_.", "String used in lieu of missing data when a masked array is printed. By default, this string is '--'."]}, {"name": "MaskedArray.fill_value", "path": "reference/maskedarray.baseclass#numpy.ma.MaskedArray.fill_value", "type": "Constants of the \n        \n         numpy.ma\n        \n        module", "text": ["In addition to the MaskedArray class, the numpy.ma module defines several constants.", "The masked constant is a special case of MaskedArray, with a float datatype and a null shape. It is used to test whether a specific entry of a masked array is masked, or to mask one or several entries of a masked array:", "Value indicating that a masked array has no invalid entry. nomask is used internally to speed up computations when the mask is not needed. It is represented internally as np.False_.", "String used in lieu of missing data when a masked array is printed. By default, this string is '--'."]}, {"name": "MaskedArray.hardmask", "path": "reference/maskedarray.baseclass#numpy.ma.MaskedArray.hardmask", "type": "Constants of the \n        \n         numpy.ma\n        \n        module", "text": ["In addition to the MaskedArray class, the numpy.ma module defines several constants.", "The masked constant is a special case of MaskedArray, with a float datatype and a null shape. It is used to test whether a specific entry of a masked array is masked, or to mask one or several entries of a masked array:", "Value indicating that a masked array has no invalid entry. nomask is used internally to speed up computations when the mask is not needed. It is represented internally as np.False_.", "String used in lieu of missing data when a masked array is printed. By default, this string is '--'."]}, {"name": "MaskedArray.mask", "path": "reference/maskedarray.baseclass#numpy.ma.MaskedArray.mask", "type": "Constants of the \n        \n         numpy.ma\n        \n        module", "text": ["In addition to the MaskedArray class, the numpy.ma module defines several constants.", "The masked constant is a special case of MaskedArray, with a float datatype and a null shape. It is used to test whether a specific entry of a masked array is masked, or to mask one or several entries of a masked array:", "Value indicating that a masked array has no invalid entry. nomask is used internally to speed up computations when the mask is not needed. It is represented internally as np.False_.", "String used in lieu of missing data when a masked array is printed. By default, this string is '--'."]}, {"name": "MaskedArray.recordmask", "path": "reference/maskedarray.baseclass#numpy.ma.MaskedArray.recordmask", "type": "Constants of the \n        \n         numpy.ma\n        \n        module", "text": ["In addition to the MaskedArray class, the numpy.ma module defines several constants.", "The masked constant is a special case of MaskedArray, with a float datatype and a null shape. It is used to test whether a specific entry of a masked array is masked, or to mask one or several entries of a masked array:", "Value indicating that a masked array has no invalid entry. nomask is used internally to speed up computations when the mask is not needed. It is represented internally as np.False_.", "String used in lieu of missing data when a masked array is printed. By default, this string is '--'."]}, {"name": "MaskedArray.sharedmask", "path": "reference/maskedarray.baseclass#numpy.ma.MaskedArray.sharedmask", "type": "Constants of the \n        \n         numpy.ma\n        \n        module", "text": ["In addition to the MaskedArray class, the numpy.ma module defines several constants.", "The masked constant is a special case of MaskedArray, with a float datatype and a null shape. It is used to test whether a specific entry of a masked array is masked, or to mask one or several entries of a masked array:", "Value indicating that a masked array has no invalid entry. nomask is used internally to speed up computations when the mask is not needed. It is represented internally as np.False_.", "String used in lieu of missing data when a masked array is printed. By default, this string is '--'."]}, {"name": "Mathematical functions", "path": "reference/routines.math", "type": "Mathematical functions", "text": ["sin(x, /[, out, where, casting, order, ...])", "Trigonometric sine, element-wise.", "cos(x, /[, out, where, casting, order, ...])", "Cosine element-wise.", "tan(x, /[, out, where, casting, order, ...])", "Compute tangent element-wise.", "arcsin(x, /[, out, where, casting, order, ...])", "Inverse sine, element-wise.", "arccos(x, /[, out, where, casting, order, ...])", "Trigonometric inverse cosine, element-wise.", "arctan(x, /[, out, where, casting, order, ...])", "Trigonometric inverse tangent, element-wise.", "hypot(x1, x2, /[, out, where, casting, ...])", "Given the \"legs\" of a right triangle, return its hypotenuse.", "arctan2(x1, x2, /[, out, where, casting, ...])", "Element-wise arc tangent of x1/x2 choosing the quadrant correctly.", "degrees(x, /[, out, where, casting, order, ...])", "Convert angles from radians to degrees.", "radians(x, /[, out, where, casting, order, ...])", "Convert angles from degrees to radians.", "unwrap(p[, discont, axis, period])", "Unwrap by taking the complement of large deltas with respect to the period.", "deg2rad(x, /[, out, where, casting, order, ...])", "Convert angles from degrees to radians.", "rad2deg(x, /[, out, where, casting, order, ...])", "Convert angles from radians to degrees.", "sinh(x, /[, out, where, casting, order, ...])", "Hyperbolic sine, element-wise.", "cosh(x, /[, out, where, casting, order, ...])", "Hyperbolic cosine, element-wise.", "tanh(x, /[, out, where, casting, order, ...])", "Compute hyperbolic tangent element-wise.", "arcsinh(x, /[, out, where, casting, order, ...])", "Inverse hyperbolic sine element-wise.", "arccosh(x, /[, out, where, casting, order, ...])", "Inverse hyperbolic cosine, element-wise.", "arctanh(x, /[, out, where, casting, order, ...])", "Inverse hyperbolic tangent element-wise.", "around(a[, decimals, out])", "Evenly round to the given number of decimals.", "round_(a[, decimals, out])", "Round an array to the given number of decimals.", "rint(x, /[, out, where, casting, order, ...])", "Round elements of the array to the nearest integer.", "fix(x[, out])", "Round to nearest integer towards zero.", "floor(x, /[, out, where, casting, order, ...])", "Return the floor of the input, element-wise.", "ceil(x, /[, out, where, casting, order, ...])", "Return the ceiling of the input, element-wise.", "trunc(x, /[, out, where, casting, order, ...])", "Return the truncated value of the input, element-wise.", "prod(a[, axis, dtype, out, keepdims, ...])", "Return the product of array elements over a given axis.", "sum(a[, axis, dtype, out, keepdims, ...])", "Sum of array elements over a given axis.", "nanprod(a[, axis, dtype, out, keepdims, ...])", "Return the product of array elements over a given axis treating Not a Numbers (NaNs) as ones.", "nansum(a[, axis, dtype, out, keepdims, ...])", "Return the sum of array elements over a given axis treating Not a Numbers (NaNs) as zero.", "cumprod(a[, axis, dtype, out])", "Return the cumulative product of elements along a given axis.", "cumsum(a[, axis, dtype, out])", "Return the cumulative sum of the elements along a given axis.", "nancumprod(a[, axis, dtype, out])", "Return the cumulative product of array elements over a given axis treating Not a Numbers (NaNs) as one.", "nancumsum(a[, axis, dtype, out])", "Return the cumulative sum of array elements over a given axis treating Not a Numbers (NaNs) as zero.", "diff(a[, n, axis, prepend, append])", "Calculate the n-th discrete difference along the given axis.", "ediff1d(ary[, to_end, to_begin])", "The differences between consecutive elements of an array.", "gradient(f, *varargs[, axis, edge_order])", "Return the gradient of an N-dimensional array.", "cross(a, b[, axisa, axisb, axisc, axis])", "Return the cross product of two (arrays of) vectors.", "trapz(y[, x, dx, axis])", "Integrate along the given axis using the composite trapezoidal rule.", "exp(x, /[, out, where, casting, order, ...])", "Calculate the exponential of all elements in the input array.", "expm1(x, /[, out, where, casting, order, ...])", "Calculate exp(x) - 1 for all elements in the array.", "exp2(x, /[, out, where, casting, order, ...])", "Calculate 2**p for all p in the input array.", "log(x, /[, out, where, casting, order, ...])", "Natural logarithm, element-wise.", "log10(x, /[, out, where, casting, order, ...])", "Return the base 10 logarithm of the input array, element-wise.", "log2(x, /[, out, where, casting, order, ...])", "Base-2 logarithm of x.", "log1p(x, /[, out, where, casting, order, ...])", "Return the natural logarithm of one plus the input array, element-wise.", "logaddexp(x1, x2, /[, out, where, casting, ...])", "Logarithm of the sum of exponentiations of the inputs.", "logaddexp2(x1, x2, /[, out, where, casting, ...])", "Logarithm of the sum of exponentiations of the inputs in base-2.", "i0(x)", "Modified Bessel function of the first kind, order 0.", "sinc(x)", "Return the normalized sinc function.", "signbit(x, /[, out, where, casting, order, ...])", "Returns element-wise True where signbit is set (less than zero).", "copysign(x1, x2, /[, out, where, casting, ...])", "Change the sign of x1 to that of x2, element-wise.", "frexp(x[, out1, out2], / [[, out, where, ...])", "Decompose the elements of x into mantissa and twos exponent.", "ldexp(x1, x2, /[, out, where, casting, ...])", "Returns x1 * 2**x2, element-wise.", "nextafter(x1, x2, /[, out, where, casting, ...])", "Return the next floating-point value after x1 towards x2, element-wise.", "spacing(x, /[, out, where, casting, order, ...])", "Return the distance between x and the nearest adjacent number.", "lcm(x1, x2, /[, out, where, casting, order, ...])", "Returns the lowest common multiple of |x1| and |x2|", "gcd(x1, x2, /[, out, where, casting, order, ...])", "Returns the greatest common divisor of |x1| and |x2|", "add(x1, x2, /[, out, where, casting, order, ...])", "Add arguments element-wise.", "reciprocal(x, /[, out, where, casting, ...])", "Return the reciprocal of the argument, element-wise.", "positive(x, /[, out, where, casting, order, ...])", "Numerical positive, element-wise.", "negative(x, /[, out, where, casting, order, ...])", "Numerical negative, element-wise.", "multiply(x1, x2, /[, out, where, casting, ...])", "Multiply arguments element-wise.", "divide(x1, x2, /[, out, where, casting, ...])", "Returns a true division of the inputs, element-wise.", "power(x1, x2, /[, out, where, casting, ...])", "First array elements raised to powers from second array, element-wise.", "subtract(x1, x2, /[, out, where, casting, ...])", "Subtract arguments, element-wise.", "true_divide(x1, x2, /[, out, where, ...])", "Returns a true division of the inputs, element-wise.", "floor_divide(x1, x2, /[, out, where, ...])", "Return the largest integer smaller or equal to the division of the inputs.", "float_power(x1, x2, /[, out, where, ...])", "First array elements raised to powers from second array, element-wise.", "fmod(x1, x2, /[, out, where, casting, ...])", "Returns the element-wise remainder of division.", "mod(x1, x2, /[, out, where, casting, order, ...])", "Returns the element-wise remainder of division.", "modf(x[, out1, out2], / [[, out, where, ...])", "Return the fractional and integral parts of an array, element-wise.", "remainder(x1, x2, /[, out, where, casting, ...])", "Returns the element-wise remainder of division.", "divmod(x1, x2[, out1, out2], / [[, out, ...])", "Return element-wise quotient and remainder simultaneously.", "angle(z[, deg])", "Return the angle of the complex argument.", "real(val)", "Return the real part of the complex argument.", "imag(val)", "Return the imaginary part of the complex argument.", "conj(x, /[, out, where, casting, order, ...])", "Return the complex conjugate, element-wise.", "conjugate(x, /[, out, where, casting, ...])", "Return the complex conjugate, element-wise.", "maximum(x1, x2, /[, out, where, casting, ...])", "Element-wise maximum of array elements.", "fmax(x1, x2, /[, out, where, casting, ...])", "Element-wise maximum of array elements.", "amax(a[, axis, out, keepdims, initial, where])", "Return the maximum of an array or maximum along an axis.", "nanmax(a[, axis, out, keepdims, initial, where])", "Return the maximum of an array or maximum along an axis, ignoring any NaNs.", "minimum(x1, x2, /[, out, where, casting, ...])", "Element-wise minimum of array elements.", "fmin(x1, x2, /[, out, where, casting, ...])", "Element-wise minimum of array elements.", "amin(a[, axis, out, keepdims, initial, where])", "Return the minimum of an array or minimum along an axis.", "nanmin(a[, axis, out, keepdims, initial, where])", "Return minimum of an array or minimum along an axis, ignoring any NaNs.", "convolve(a, v[, mode])", "Returns the discrete, linear convolution of two one-dimensional sequences.", "clip(a, a_min, a_max[, out])", "Clip (limit) the values in an array.", "sqrt(x, /[, out, where, casting, order, ...])", "Return the non-negative square-root of an array, element-wise.", "cbrt(x, /[, out, where, casting, order, ...])", "Return the cube-root of an array, element-wise.", "square(x, /[, out, where, casting, order, ...])", "Return the element-wise square of the input.", "absolute(x, /[, out, where, casting, order, ...])", "Calculate the absolute value element-wise.", "fabs(x, /[, out, where, casting, order, ...])", "Compute the absolute values element-wise.", "sign(x, /[, out, where, casting, order, ...])", "Returns an element-wise indication of the sign of a number.", "heaviside(x1, x2, /[, out, where, casting, ...])", "Compute the Heaviside step function.", "nan_to_num(x[, copy, nan, posinf, neginf])", "Replace NaN with zero and infinity with large finite numbers (default behaviour) or with the numbers defined by the user using the nan, posinf and/or neginf keywords.", "real_if_close(a[, tol])", "If input is complex with all imaginary parts close to zero, return real parts.", "interp(x, xp, fp[, left, right, period])", "One-dimensional linear interpolation for monotonically increasing sample points."]}, {"name": "Mathematical functions with automatic domain (numpy.emath)", "path": "reference/routines.emath", "type": "Mathematical functions with automatic domain ( \n      \n       numpy.emath\n      \n      )", "text": ["Note", "numpy.emath is a preferred alias for numpy.lib.scimath, available after numpy is imported.", "Wrapper functions to more user-friendly calling of certain math functions whose output data-type is different than the input data-type in certain domains of the input.", "For example, for functions like log with branch cuts, the versions in this module provide the mathematically valid answers in the complex plane:", "Similarly, sqrt, other base logarithms, power and trig functions are correctly handled. See their respective docstrings for specific examples.", "sqrt(x)", "Compute the square root of x.", "log(x)", "Compute the natural logarithm of x.", "log2(x)", "Compute the logarithm base 2 of x.", "logn(n, x)", "Take log base n of x.", "log10(x)", "Compute the logarithm base 10 of x.", "power(x, p)", "Return x to the power p, (x**p).", "arccos(x)", "Compute the inverse cosine of x.", "arcsin(x)", "Compute the inverse sine of x.", "arctanh(x)", "Compute the inverse hyperbolic tangent of x."]}, {"name": "matlib.empty()", "path": "reference/generated/numpy.matlib.empty", "type": "numpy.matlib.empty", "text": ["Return a new matrix of given shape and type, without initializing entries.", "Shape of the empty matrix.", "Desired output data-type.", "Whether to store multi-dimensional data in row-major (C-style) or column-major (Fortran-style) order in memory.", "See also", "empty, unlike zeros, does not set the matrix values to zero, and may therefore be marginally faster. On the other hand, it requires the user to manually set all the values in the array, and should be used with caution."]}, {"name": "matlib.eye()", "path": "reference/generated/numpy.matlib.eye", "type": "numpy.matlib.eye", "text": ["Return a matrix with ones on the diagonal and zeros elsewhere.", "Number of rows in the output.", "Number of columns in the output, defaults to n.", "Index of the diagonal: 0 refers to the main diagonal, a positive value refers to an upper diagonal, and a negative value to a lower diagonal.", "Data-type of the returned matrix.", "Whether the output should be stored in row-major (C-style) or column-major (Fortran-style) order in memory.", "New in version 1.14.0.", "A n x M matrix where all elements are equal to zero, except for the k-th diagonal, whose values are equal to one.", "See also", "Equivalent array function.", "Square identity matrix."]}, {"name": "matlib.identity()", "path": "reference/generated/numpy.matlib.identity", "type": "numpy.matlib.identity", "text": ["Returns the square identity matrix of given size.", "Size of the returned identity matrix.", "Data-type of the output. Defaults to float.", "n x n matrix with its main diagonal set to one, and all other elements zero.", "See also", "Equivalent array function.", "More general matrix identity function."]}, {"name": "matlib.ones()", "path": "reference/generated/numpy.matlib.ones", "type": "numpy.matlib.ones", "text": ["Matrix of ones.", "Return a matrix of given shape and type, filled with ones.", "Shape of the matrix", "The desired data-type for the matrix, default is np.float64.", "Whether to store matrix in C- or Fortran-contiguous order, default is \u2018C\u2019.", "Matrix of ones of given shape, dtype, and order.", "See also", "Array of ones.", "Zero matrix.", "If shape has length one i.e. (N,), or is a scalar N, out becomes a single row matrix of shape (1,N)."]}, {"name": "matlib.rand()", "path": "reference/generated/numpy.matlib.rand", "type": "numpy.matlib.rand", "text": ["Return a matrix of random values with given shape.", "Create a matrix of the given shape and propagate it with random samples from a uniform distribution over [0, 1).", "Shape of the output. If given as N integers, each integer specifies the size of one dimension. If given as a tuple, this tuple gives the complete shape.", "The matrix of random values with shape given by *args.", "See also", "If the first argument is a tuple, other arguments are ignored:"]}, {"name": "matlib.randn()", "path": "reference/generated/numpy.matlib.randn", "type": "numpy.matlib.randn", "text": ["Return a random matrix with data from the \u201cstandard normal\u201d distribution.", "randn generates a matrix filled with random floats sampled from a univariate \u201cnormal\u201d (Gaussian) distribution of mean 0 and variance 1.", "Shape of the output. If given as N integers, each integer specifies the size of one dimension. If given as a tuple, this tuple gives the complete shape.", "A matrix of floating-point samples drawn from the standard normal distribution.", "See also", "For random samples from \\(N(\\mu, \\sigma^2)\\), use:", "sigma * np.matlib.randn(...) + mu", "Two-by-four matrix of samples from \\(N(3, 6.25)\\):"]}, {"name": "matlib.repmat()", "path": "reference/generated/numpy.matlib.repmat", "type": "numpy.matlib.repmat", "text": ["Repeat a 0-D to 2-D array or matrix MxN times.", "The array or matrix to be repeated.", "The number of times a is repeated along the first and second axes.", "The result of repeating a."]}, {"name": "matlib.zeros()", "path": "reference/generated/numpy.matlib.zeros", "type": "numpy.matlib.zeros", "text": ["Return a matrix of given shape and type, filled with zeros.", "Shape of the matrix", "The desired data-type for the matrix, default is float.", "Whether to store the result in C- or Fortran-contiguous order, default is \u2018C\u2019.", "Zero matrix of given shape, dtype, and order.", "See also", "Equivalent array function.", "Return a matrix of ones.", "If shape has length one i.e. (N,), or is a scalar N, out becomes a single row matrix of shape (1,N)."]}, {"name": "Matrix library (numpy.matlib)", "path": "reference/routines.matlib", "type": "Matrix library ( \n      \n       numpy.matlib\n      \n      )", "text": ["This module contains all functions in the numpy namespace, with the following replacement functions that return matrices instead of ndarrays.", "Functions that are also in the numpy namespace and return matrices", "mat(data[, dtype])", "Interpret the input as a matrix.", "matrix(data[, dtype, copy])", "Note", "It is no longer recommended to use this class, even for linear", "asmatrix(data[, dtype])", "Interpret the input as a matrix.", "bmat(obj[, ldict, gdict])", "Build a matrix object from a string, nested sequence, or array.", "Replacement functions in matlib", "empty(shape[, dtype, order])", "Return a new matrix of given shape and type, without initializing entries.", "zeros(shape[, dtype, order])", "Return a matrix of given shape and type, filled with zeros.", "ones(shape[, dtype, order])", "Matrix of ones.", "eye(n[, M, k, dtype, order])", "Return a matrix with ones on the diagonal and zeros elsewhere.", "identity(n[, dtype])", "Returns the square identity matrix of given size.", "repmat(a, m, n)", "Repeat a 0-D to 2-D array or matrix MxN times.", "rand(*args)", "Return a matrix of random values with given shape.", "randn(*args)", "Return a random matrix with data from the \"standard normal\" distribution."]}, {"name": "matrix.all()", "path": "reference/generated/numpy.matrix.all", "type": "numpy.matrix.all", "text": ["method", "Test whether all matrix elements along a given axis evaluate to True.", "See also", "This is the same as ndarray.all, but it returns a matrix object."]}, {"name": "matrix.any()", "path": "reference/generated/numpy.matrix.any", "type": "numpy.matrix.any", "text": ["method", "Test whether any array element along a given axis evaluates to True.", "Refer to numpy.any for full documentation.", "Axis along which logical OR is performed", "Output to existing array instead of creating new one, must have same shape as expected output", "Returns a single bool if axis is None; otherwise, returns ndarray"]}, {"name": "matrix.argmax()", "path": "reference/generated/numpy.matrix.argmax", "type": "numpy.matrix.argmax", "text": ["method", "Indexes of the maximum values along an axis.", "Return the indexes of the first occurrences of the maximum values along the specified axis. If axis is None, the index is for the flattened matrix.", "See also", "This is the same as ndarray.argmax, but returns a matrix object where ndarray.argmax would return an ndarray."]}, {"name": "matrix.argmin()", "path": "reference/generated/numpy.matrix.argmin", "type": "numpy.matrix.argmin", "text": ["method", "Indexes of the minimum values along an axis.", "Return the indexes of the first occurrences of the minimum values along the specified axis. If axis is None, the index is for the flattened matrix.", "See also", "This is the same as ndarray.argmin, but returns a matrix object where ndarray.argmin would return an ndarray."]}, {"name": "matrix.argpartition()", "path": "reference/generated/numpy.matrix.argpartition", "type": "numpy.matrix.argpartition", "text": ["method", "Returns the indices that would partition this array.", "Refer to numpy.argpartition for full documentation.", "New in version 1.8.0.", "See also", "equivalent function"]}, {"name": "matrix.argsort()", "path": "reference/generated/numpy.matrix.argsort", "type": "numpy.matrix.argsort", "text": ["method", "Returns the indices that would sort this array.", "Refer to numpy.argsort for full documentation.", "See also", "equivalent function"]}, {"name": "matrix.astype()", "path": "reference/generated/numpy.matrix.astype", "type": "numpy.matrix.astype", "text": ["method", "Copy of the array, cast to a specified type.", "Typecode or data-type to which the array is cast.", "Controls the memory layout order of the result. \u2018C\u2019 means C order, \u2018F\u2019 means Fortran order, \u2018A\u2019 means \u2018F\u2019 order if all the arrays are Fortran contiguous, \u2018C\u2019 order otherwise, and \u2018K\u2019 means as close to the order the array elements appear in memory as possible. Default is \u2018K\u2019.", "Controls what kind of data casting may occur. Defaults to \u2018unsafe\u2019 for backwards compatibility.", "If True, then sub-classes will be passed-through (default), otherwise the returned array will be forced to be a base-class array.", "By default, astype always returns a newly allocated array. If this is set to false, and the dtype, order, and subok requirements are satisfied, the input array is returned instead of a copy.", "Unless copy is False and the other conditions for returning the input array are satisfied (see description for copy input parameter), arr_t is a new array of the same shape as the input array, with dtype, order given by dtype, order.", "When casting from complex to float or int. To avoid this, one should use a.real.astype(t).", "Changed in version 1.17.0: Casting between a simple data type and a structured one is possible only for \u201cunsafe\u201d casting. Casting to multiple fields is allowed, but casting from multiple fields is not.", "Changed in version 1.9.0: Casting from numeric to string types in \u2018safe\u2019 casting mode requires that the string dtype length is long enough to store the max integer/float value converted."]}, {"name": "matrix.base", "path": "reference/generated/numpy.matrix.base", "type": "Standard array subclasses", "text": ["attribute", "Base object if memory is from some other object.", "The base of an array that owns its memory is None:", "Slicing creates a view, whose memory is shared with x:"]}, {"name": "matrix.byteswap()", "path": "reference/generated/numpy.matrix.byteswap", "type": "numpy.matrix.byteswap", "text": ["method", "Swap the bytes of the array elements", "Toggle between low-endian and big-endian data representation by returning a byteswapped array, optionally swapped in-place. Arrays of byte-strings are not swapped. The real and imaginary parts of a complex number are swapped individually.", "If True, swap bytes in-place, default is False.", "The byteswapped array. If inplace is True, this is a view to self.", "Arrays of byte-strings are not swapped", "but different representation in memory"]}, {"name": "matrix.choose()", "path": "reference/generated/numpy.matrix.choose", "type": "numpy.matrix.choose", "text": ["method", "Use an index array to construct a new array from a set of choices.", "Refer to numpy.choose for full documentation.", "See also", "equivalent function"]}, {"name": "matrix.clip()", "path": "reference/generated/numpy.matrix.clip", "type": "numpy.matrix.clip", "text": ["method", "Return an array whose values are limited to [min, max]. One of max or min must be given.", "Refer to numpy.clip for full documentation.", "See also", "equivalent function"]}, {"name": "matrix.compress()", "path": "reference/generated/numpy.matrix.compress", "type": "numpy.matrix.compress", "text": ["method", "Return selected slices of this array along given axis.", "Refer to numpy.compress for full documentation.", "See also", "equivalent function"]}, {"name": "matrix.conj()", "path": "reference/generated/numpy.matrix.conj", "type": "numpy.matrix.conj", "text": ["method", "Complex-conjugate all elements.", "Refer to numpy.conjugate for full documentation.", "See also", "equivalent function"]}, {"name": "matrix.conjugate()", "path": "reference/generated/numpy.matrix.conjugate", "type": "numpy.matrix.conjugate", "text": ["method", "Return the complex conjugate, element-wise.", "Refer to numpy.conjugate for full documentation.", "See also", "equivalent function"]}, {"name": "matrix.copy()", "path": "reference/generated/numpy.matrix.copy", "type": "numpy.matrix.copy", "text": ["method", "Return a copy of the array.", "Controls the memory layout of the copy. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of a as closely as possible. (Note that this function and numpy.copy are very similar but have different default values for their order= arguments, and this function always passes sub-classes through.)", "See also", "Similar function with different default behavior", "This function is the preferred method for creating an array copy. The function numpy.copy is similar, but it defaults to using order \u2018K\u2019, and will not pass sub-classes through by default."]}, {"name": "matrix.ctypes", "path": "reference/generated/numpy.matrix.ctypes", "type": "Standard array subclasses", "text": ["attribute", "An object to simplify the interaction of the array with the ctypes module.", "This attribute creates an object that makes it easier to use arrays when calling shared libraries with the ctypes module. The returned object has, among others, data, shape, and strides attributes (see Notes below) which themselves return ctypes objects that can be used as arguments to a shared library.", "Possessing attributes data, shape, strides, etc.", "See also", "Below are the public attributes of this object which were documented in \u201cGuide to NumPy\u201d (we have omitted undocumented public attributes, as well as documented private attributes):", "A pointer to the memory area of the array as a Python integer. This memory area may contain data that is not aligned, or not in correct byte-order. The memory area may not even be writeable. The array flags and data-type of this array should be respected when passing this attribute to arbitrary C-code to avoid trouble that can include Python crashing. User Beware! The value of this attribute is exactly the same as self._array_interface_['data'][0].", "Note that unlike data_as, a reference will not be kept to the array: code like ctypes.c_void_p((a + b).ctypes.data) will result in a pointer to a deallocated array, and should be spelt (a + b).ctypes.data_as(ctypes.c_void_p)", "(c_intp*self.ndim): A ctypes array of length self.ndim where the basetype is the C-integer corresponding to dtype('p') on this platform (see c_intp). This base-type could be ctypes.c_int, ctypes.c_long, or ctypes.c_longlong depending on the platform. The ctypes array contains the shape of the underlying array.", "(c_intp*self.ndim): A ctypes array of length self.ndim where the basetype is the same as for the shape attribute. This ctypes array contains the strides information from the underlying array. This strides information is important for showing how many bytes must be jumped to get to the next element in the array.", "Return the data pointer cast to a particular c-types object. For example, calling self._as_parameter_ is equivalent to self.data_as(ctypes.c_void_p). Perhaps you want to use the data as a pointer to a ctypes array of floating-point data: self.data_as(ctypes.POINTER(ctypes.c_double)).", "The returned pointer will keep a reference to the array.", "Return the shape tuple as an array of some other c-types type. For example: self.shape_as(ctypes.c_short).", "Return the strides tuple as an array of some other c-types type. For example: self.strides_as(ctypes.c_longlong).", "If the ctypes module is not available, then the ctypes attribute of array objects still returns something useful, but ctypes objects are not returned and errors may be raised instead. In particular, the object will still have the as_parameter attribute which will return an integer equal to the data attribute."]}, {"name": "matrix.cumprod()", "path": "reference/generated/numpy.matrix.cumprod", "type": "numpy.matrix.cumprod", "text": ["method", "Return the cumulative product of the elements along the given axis.", "Refer to numpy.cumprod for full documentation.", "See also", "equivalent function"]}, {"name": "matrix.cumsum()", "path": "reference/generated/numpy.matrix.cumsum", "type": "numpy.matrix.cumsum", "text": ["method", "Return the cumulative sum of the elements along the given axis.", "Refer to numpy.cumsum for full documentation.", "See also", "equivalent function"]}, {"name": "matrix.data", "path": "reference/generated/numpy.matrix.data", "type": "Standard array subclasses", "text": ["attribute", "Python buffer object pointing to the start of the array\u2019s data."]}, {"name": "matrix.diagonal()", "path": "reference/generated/numpy.matrix.diagonal", "type": "numpy.matrix.diagonal", "text": ["method", "Return specified diagonals. In NumPy 1.9 the returned array is a read-only view instead of a copy as in previous NumPy versions. In a future version the read-only restriction will be removed.", "Refer to numpy.diagonal for full documentation.", "See also", "equivalent function"]}, {"name": "matrix.dump()", "path": "reference/generated/numpy.matrix.dump", "type": "numpy.matrix.dump", "text": ["method", "Dump a pickle of the array to the specified file. The array can be read back with pickle.load or numpy.load.", "A string naming the dump file.", "Changed in version 1.17.0: pathlib.Path objects are now accepted."]}, {"name": "matrix.dumps()", "path": "reference/generated/numpy.matrix.dumps", "type": "numpy.matrix.dumps", "text": ["method", "Returns the pickle of the array as a string. pickle.loads will convert the string back to an array."]}, {"name": "matrix.fill()", "path": "reference/generated/numpy.matrix.fill", "type": "numpy.matrix.fill", "text": ["method", "Fill the array with a scalar value.", "All elements of a will be assigned this value."]}, {"name": "matrix.flags", "path": "reference/generated/numpy.matrix.flags", "type": "Standard array subclasses", "text": ["attribute", "Information about the memory layout of the array.", "The flags object can be accessed dictionary-like (as in a.flags['WRITEABLE']), or by using lowercased attribute names (as in a.flags.writeable). Short flag names are only supported in dictionary access.", "Only the WRITEBACKIFCOPY, UPDATEIFCOPY, WRITEABLE, and ALIGNED flags can be changed by the user, via direct assignment to the attribute or dictionary entry, or by calling ndarray.setflags.", "The array flags cannot be set arbitrarily:", "Arrays can be both C-style and Fortran-style contiguous simultaneously. This is clear for 1-dimensional arrays, but can also be true for higher dimensional arrays.", "Even for contiguous arrays a stride for a given dimension arr.strides[dim] may be arbitrary if arr.shape[dim] == 1 or the array has no elements. It does not generally hold that self.strides[-1] == self.itemsize for C-style contiguous arrays or self.strides[0] == self.itemsize for Fortran-style contiguous arrays is true.", "The data is in a single, C-style contiguous segment.", "The data is in a single, Fortran-style contiguous segment.", "The array owns the memory it uses or borrows it from another object.", "The data area can be written to. Setting this to False locks the data, making it read-only. A view (slice, etc.) inherits WRITEABLE from its base array at creation time, but a view of a writeable array may be subsequently locked while the base array remains writeable. (The opposite is not true, in that a view of a locked array may not be made writeable. However, currently, locking a base object does not lock any views that already reference it, so under that circumstance it is possible to alter the contents of a locked array via a previously created writeable view onto it.) Attempting to change a non-writeable array raises a RuntimeError exception.", "The data and all elements are aligned appropriately for the hardware.", "This array is a copy of some other array. The C-API function PyArray_ResolveWritebackIfCopy must be called before deallocating to the base array will be updated with the contents of this array.", "(Deprecated, use WRITEBACKIFCOPY) This array is a copy of some other array. When this array is deallocated, the base array will be updated with the contents of this array.", "F_CONTIGUOUS and not C_CONTIGUOUS.", "F_CONTIGUOUS or C_CONTIGUOUS (one-segment test).", "ALIGNED and WRITEABLE.", "BEHAVED and C_CONTIGUOUS.", "BEHAVED and F_CONTIGUOUS and not C_CONTIGUOUS."]}, {"name": "matrix.flat", "path": "reference/generated/numpy.matrix.flat", "type": "Standard array subclasses", "text": ["attribute", "A 1-D iterator over the array.", "This is a numpy.flatiter instance, which acts similarly to, but is not a subclass of, Python\u2019s built-in iterator object.", "See also", "Return a copy of the array collapsed into one dimension.", "An assignment example:"]}, {"name": "matrix.flatten()", "path": "reference/generated/numpy.matrix.flatten", "type": "numpy.matrix.flatten", "text": ["method", "Return a flattened copy of the matrix.", "All N elements of the matrix are placed into a single row.", "\u2018C\u2019 means to flatten in row-major (C-style) order. \u2018F\u2019 means to flatten in column-major (Fortran-style) order. \u2018A\u2019 means to flatten in column-major order if m is Fortran contiguous in memory, row-major order otherwise. \u2018K\u2019 means to flatten m in the order the elements occur in memory. The default is \u2018C\u2019.", "A copy of the matrix, flattened to a (1, N) matrix where N is the number of elements in the original matrix.", "See also", "Return a flattened array.", "A 1-D flat iterator over the matrix."]}, {"name": "matrix.getA()", "path": "reference/generated/numpy.matrix.geta", "type": "numpy.matrix.getA", "text": ["method", "Return self as an ndarray object.", "Equivalent to np.asarray(self).", "self as an ndarray"]}, {"name": "matrix.getA1()", "path": "reference/generated/numpy.matrix.geta1", "type": "numpy.matrix.getA1", "text": ["method", "Return self as a flattened ndarray.", "Equivalent to np.asarray(x).ravel()", "self, 1-D, as an ndarray"]}, {"name": "matrix.getfield()", "path": "reference/generated/numpy.matrix.getfield", "type": "numpy.matrix.getfield", "text": ["method", "Returns a field of the given array as a certain type.", "A field is a view of the array data with a given data-type. The values in the view are determined by the given type and the offset into the current array in bytes. The offset needs to be such that the view dtype fits in the array dtype; for example an array of dtype complex128 has 16-byte elements. If taking a view with a 32-bit integer (4 bytes), the offset needs to be between 0 and 12 bytes.", "The data type of the view. The dtype size of the view can not be larger than that of the array itself.", "Number of bytes to skip before beginning the element view.", "By choosing an offset of 8 bytes we can select the complex part of the array for our view:"]}, {"name": "matrix.getH()", "path": "reference/generated/numpy.matrix.geth", "type": "numpy.matrix.getH", "text": ["method", "Returns the (complex) conjugate transpose of self.", "Equivalent to np.transpose(self) if self is real-valued.", "complex conjugate transpose of self"]}, {"name": "matrix.getI()", "path": "reference/generated/numpy.matrix.geti", "type": "numpy.matrix.getI", "text": ["method", "Returns the (multiplicative) inverse of invertible self.", "If self is non-singular, ret is such that ret * self == self * ret == np.matrix(np.eye(self[0,:].size)) all return True.", "If self is singular.", "See also"]}, {"name": "matrix.getT()", "path": "reference/generated/numpy.matrix.gett", "type": "numpy.matrix.getT", "text": ["method", "Returns the transpose of the matrix.", "Does not conjugate! For the complex conjugate transpose, use .H.", "The (non-conjugated) transpose of the matrix.", "See also"]}, {"name": "matrix.item()", "path": "reference/generated/numpy.matrix.item", "type": "numpy.matrix.item", "text": ["method", "Copy an element of an array to a standard Python scalar and return it.", "A copy of the specified element of the array as a suitable Python scalar", "When the data type of a is longdouble or clongdouble, item() returns a scalar array object because there is no available Python scalar that would not lose information. Void arrays return a buffer object for item(), unless fields are defined, in which case a tuple is returned.", "item is very similar to a[args], except, instead of an array scalar, a standard Python scalar is returned. This can be useful for speeding up access to elements of the array and doing arithmetic on elements of the array using Python\u2019s optimized math."]}, {"name": "matrix.itemset()", "path": "reference/generated/numpy.matrix.itemset", "type": "numpy.matrix.itemset", "text": ["method", "Insert scalar into an array (scalar is cast to array\u2019s dtype, if possible)", "There must be at least 1 argument, and define the last argument as item. Then, a.itemset(*args) is equivalent to but faster than a[args] = item. The item should be a scalar value and args must select a single item in the array a.", "If one argument: a scalar, only used in case a is of size 1. If two arguments: the last argument is the value to be set and must be a scalar, the first argument specifies a single array element location. It is either an int or a tuple.", "Compared to indexing syntax, itemset provides some speed increase for placing a scalar into a particular location in an ndarray, if you must do this. However, generally this is discouraged: among other problems, it complicates the appearance of the code. Also, when using itemset (and item) inside a loop, be sure to assign the methods to a local variable to avoid the attribute look-up at each loop iteration."]}, {"name": "matrix.itemsize", "path": "reference/generated/numpy.matrix.itemsize", "type": "Standard array subclasses", "text": ["attribute", "Length of one array element in bytes."]}, {"name": "matrix.max()", "path": "reference/generated/numpy.matrix.max", "type": "numpy.matrix.max", "text": ["method", "Return the maximum value along an axis.", "See also", "This is the same as ndarray.max, but returns a matrix object where ndarray.max would return an ndarray."]}, {"name": "matrix.mean()", "path": "reference/generated/numpy.matrix.mean", "type": "numpy.matrix.mean", "text": ["method", "Returns the average of the matrix elements along the given axis.", "Refer to numpy.mean for full documentation.", "See also", "Same as ndarray.mean except that, where that returns an ndarray, this returns a matrix object."]}, {"name": "matrix.min()", "path": "reference/generated/numpy.matrix.min", "type": "numpy.matrix.min", "text": ["method", "Return the minimum value along an axis.", "See also", "This is the same as ndarray.min, but returns a matrix object where ndarray.min would return an ndarray."]}, {"name": "matrix.nbytes", "path": "reference/generated/numpy.matrix.nbytes", "type": "Standard array subclasses", "text": ["attribute", "Total bytes consumed by the elements of the array.", "Does not include memory consumed by non-element attributes of the array object."]}, {"name": "matrix.ndim", "path": "reference/generated/numpy.matrix.ndim", "type": "Standard array subclasses", "text": ["attribute", "Number of array dimensions."]}, {"name": "matrix.newbyteorder()", "path": "reference/generated/numpy.matrix.newbyteorder", "type": "numpy.matrix.newbyteorder", "text": ["method", "Return the array with the same data viewed with a different byte order.", "Equivalent to:", "Changes are also made in all fields and sub-arrays of the array data type.", "Byte order to force; a value from the byte order specifications below. new_order codes can be any of:", "The default value (\u2018S\u2019) results in swapping the current byte order.", "New array object with the dtype reflecting given change to the byte order."]}, {"name": "matrix.nonzero()", "path": "reference/generated/numpy.matrix.nonzero", "type": "numpy.matrix.nonzero", "text": ["method", "Return the indices of the elements that are non-zero.", "Refer to numpy.nonzero for full documentation.", "See also", "equivalent function"]}, {"name": "matrix.partition()", "path": "reference/generated/numpy.matrix.partition", "type": "numpy.matrix.partition", "text": ["method", "Rearranges the elements in the array in such a way that the value of the element in kth position is in the position it would be in a sorted array. All elements smaller than the kth element are moved before this element and all equal or greater are moved behind it. The ordering of the elements in the two partitions is undefined.", "New in version 1.8.0.", "Element index to partition by. The kth element value will be in its final sorted position and all smaller elements will be moved before it and all equal or greater elements behind it. The order of all elements in the partitions is undefined. If provided with a sequence of kth it will partition all elements indexed by kth of them into their sorted position at once.", "Deprecated since version 1.22.0: Passing booleans as index is deprecated.", "Axis along which to sort. Default is -1, which means sort along the last axis.", "Selection algorithm. Default is \u2018introselect\u2019.", "When a is an array with fields defined, this argument specifies which fields to compare first, second, etc. A single field can be specified as a string, and not all fields need to be specified, but unspecified fields will still be used, in the order in which they come up in the dtype, to break ties.", "See also", "Return a parititioned copy of an array.", "Indirect partition.", "Full sort.", "See np.partition for notes on the different algorithms."]}, {"name": "matrix.prod()", "path": "reference/generated/numpy.matrix.prod", "type": "numpy.matrix.prod", "text": ["method", "Return the product of the array elements over the given axis.", "Refer to prod for full documentation.", "See also", "Same as ndarray.prod, except, where that returns an ndarray, this returns a matrix object instead."]}, {"name": "matrix.ptp()", "path": "reference/generated/numpy.matrix.ptp", "type": "numpy.matrix.ptp", "text": ["method", "Peak-to-peak (maximum - minimum) value along the given axis.", "Refer to numpy.ptp for full documentation.", "See also", "Same as ndarray.ptp, except, where that would return an ndarray object, this returns a matrix object."]}, {"name": "matrix.put()", "path": "reference/generated/numpy.matrix.put", "type": "numpy.matrix.put", "text": ["method", "Set a.flat[n] = values[n] for all n in indices.", "Refer to numpy.put for full documentation.", "See also", "equivalent function"]}, {"name": "matrix.ravel()", "path": "reference/generated/numpy.matrix.ravel", "type": "numpy.matrix.ravel", "text": ["method", "Return a flattened matrix.", "Refer to numpy.ravel for more documentation.", "The elements of m are read using this index order. \u2018C\u2019 means to index the elements in C-like order, with the last axis index changing fastest, back to the first axis index changing slowest. \u2018F\u2019 means to index the elements in Fortran-like index order, with the first index changing fastest, and the last index changing slowest. Note that the \u2018C\u2019 and \u2018F\u2019 options take no account of the memory layout of the underlying array, and only refer to the order of axis indexing. \u2018A\u2019 means to read the elements in Fortran-like index order if m is Fortran contiguous in memory, C-like order otherwise. \u2018K\u2019 means to read the elements in the order they occur in memory, except for reversing the data when strides are negative. By default, \u2018C\u2019 index order is used.", "Return the matrix flattened to shape (1, N) where N is the number of elements in the original matrix. A copy is made only if necessary.", "See also", "returns a similar output matrix but always a copy", "a flat iterator on the array.", "related function which returns an ndarray"]}, {"name": "matrix.repeat()", "path": "reference/generated/numpy.matrix.repeat", "type": "numpy.matrix.repeat", "text": ["method", "Repeat elements of an array.", "Refer to numpy.repeat for full documentation.", "See also", "equivalent function"]}, {"name": "matrix.reshape()", "path": "reference/generated/numpy.matrix.reshape", "type": "numpy.matrix.reshape", "text": ["method", "Returns an array containing the same data with a new shape.", "Refer to numpy.reshape for full documentation.", "See also", "equivalent function", "Unlike the free function numpy.reshape, this method on ndarray allows the elements of the shape parameter to be passed in as separate arguments. For example, a.reshape(10, 11) is equivalent to a.reshape((10, 11))."]}, {"name": "matrix.resize()", "path": "reference/generated/numpy.matrix.resize", "type": "numpy.matrix.resize", "text": ["method", "Change shape and size of array in-place.", "Shape of resized array.", "If False, reference count will not be checked. Default is True.", "If a does not own its own data or references or views to it exist, and the data memory must be changed. PyPy only: will always raise if the data memory must be changed, since there is no reliable way to determine if references or views to it exist.", "If the order keyword argument is specified. This behaviour is a bug in NumPy.", "See also", "Return a new array with the specified shape.", "This reallocates space for the data area if necessary.", "Only contiguous arrays (data elements consecutive in memory) can be resized.", "The purpose of the reference count check is to make sure you do not use this array as a buffer for another Python object and then reallocate the memory. However, reference counts can increase in other ways so if you are sure that you have not shared the memory for this array with another Python object, then you may safely set refcheck to False.", "Shrinking an array: array is flattened (in the order that the data are stored in memory), resized, and reshaped:", "Enlarging an array: as above, but missing entries are filled with zeros:", "Referencing an array prevents resizing\u2026", "Unless refcheck is False:"]}, {"name": "matrix.round()", "path": "reference/generated/numpy.matrix.round", "type": "numpy.matrix.round", "text": ["method", "Return a with each element rounded to the given number of decimals.", "Refer to numpy.around for full documentation.", "See also", "equivalent function"]}, {"name": "matrix.searchsorted()", "path": "reference/generated/numpy.matrix.searchsorted", "type": "numpy.matrix.searchsorted", "text": ["method", "Find indices where elements of v should be inserted in a to maintain order.", "For full documentation, see numpy.searchsorted", "See also", "equivalent function"]}, {"name": "matrix.setfield()", "path": "reference/generated/numpy.matrix.setfield", "type": "numpy.matrix.setfield", "text": ["method", "Put a value into a specified place in a field defined by a data-type.", "Place val into a\u2019s field defined by dtype and beginning offset bytes into the field.", "Value to be placed in field.", "Data-type of the field in which to place val.", "The number of bytes into the field at which to place val.", "See also"]}, {"name": "matrix.setflags()", "path": "reference/generated/numpy.matrix.setflags", "type": "numpy.matrix.setflags", "text": ["method", "Set array flags WRITEABLE, ALIGNED, (WRITEBACKIFCOPY and UPDATEIFCOPY), respectively.", "These Boolean-valued flags affect how numpy interprets the memory area used by a (see Notes below). The ALIGNED flag can only be set to True if the data is actually aligned according to the type. The WRITEBACKIFCOPY and (deprecated) UPDATEIFCOPY flags can never be set to True. The flag WRITEABLE can only be set to True if the array owns its own memory, or the ultimate owner of the memory exposes a writeable buffer interface, or is a string. (The exception for string is made so that unpickling can be done without copying memory.)", "Describes whether or not a can be written to.", "Describes whether or not a is aligned properly for its type.", "Describes whether or not a is a copy of another \u201cbase\u201d array.", "Array flags provide information about how the memory area used for the array is to be interpreted. There are 7 Boolean flags in use, only four of which can be changed by the user: WRITEBACKIFCOPY, UPDATEIFCOPY, WRITEABLE, and ALIGNED.", "WRITEABLE (W) the data area can be written to;", "ALIGNED (A) the data and strides are aligned appropriately for the hardware (as determined by the compiler);", "UPDATEIFCOPY (U) (deprecated), replaced by WRITEBACKIFCOPY;", "WRITEBACKIFCOPY (X) this array is a copy of some other array (referenced by .base). When the C-API function PyArray_ResolveWritebackIfCopy is called, the base array will be updated with the contents of this array.", "All flags can be accessed using the single (upper case) letter as well as the full name."]}, {"name": "matrix.size", "path": "reference/generated/numpy.matrix.size", "type": "Standard array subclasses", "text": ["attribute", "Number of elements in the array.", "Equal to np.prod(a.shape), i.e., the product of the array\u2019s dimensions.", "a.size returns a standard arbitrary precision Python integer. This may not be the case with other methods of obtaining the same value (like the suggested np.prod(a.shape), which returns an instance of np.int_), and may be relevant if the value is used further in calculations that may overflow a fixed size integer type."]}, {"name": "matrix.sort()", "path": "reference/generated/numpy.matrix.sort", "type": "numpy.matrix.sort", "text": ["method", "Sort an array in-place. Refer to numpy.sort for full documentation.", "Axis along which to sort. Default is -1, which means sort along the last axis.", "Sorting algorithm. The default is \u2018quicksort\u2019. Note that both \u2018stable\u2019 and \u2018mergesort\u2019 use timsort under the covers and, in general, the actual implementation will vary with datatype. The \u2018mergesort\u2019 option is retained for backwards compatibility.", "Changed in version 1.15.0: The \u2018stable\u2019 option was added.", "When a is an array with fields defined, this argument specifies which fields to compare first, second, etc. A single field can be specified as a string, and not all fields need be specified, but unspecified fields will still be used, in the order in which they come up in the dtype, to break ties.", "See also", "Return a sorted copy of an array.", "Indirect sort.", "Indirect stable sort on multiple keys.", "Find elements in sorted array.", "Partial sort.", "See numpy.sort for notes on the different sorting algorithms.", "Use the order keyword to specify a field to use when sorting a structured array:"]}, {"name": "matrix.squeeze()", "path": "reference/generated/numpy.matrix.squeeze", "type": "numpy.matrix.squeeze", "text": ["method", "Return a possibly reshaped matrix.", "Refer to numpy.squeeze for more documentation.", "Selects a subset of the axes of length one in the shape. If an axis is selected with shape entry greater than one, an error is raised.", "The matrix, but as a (1, N) matrix if it had shape (N, 1).", "See also", "related function", "If m has a single column then that column is returned as the single row of a matrix. Otherwise m is returned. The returned matrix is always either m itself or a view into m. Supplying an axis keyword argument will not affect the returned matrix but it may cause an error to be raised."]}, {"name": "matrix.std()", "path": "reference/generated/numpy.matrix.std", "type": "numpy.matrix.std", "text": ["method", "Return the standard deviation of the array elements along the given axis.", "Refer to numpy.std for full documentation.", "See also", "This is the same as ndarray.std, except that where an ndarray would be returned, a matrix object is returned instead."]}, {"name": "matrix.strides", "path": "reference/generated/numpy.matrix.strides", "type": "Standard array subclasses", "text": ["attribute", "Tuple of bytes to step in each dimension when traversing an array.", "The byte offset of element (i[0], i[1], ..., i[n]) in an array a is:", "A more detailed explanation of strides can be found in the \u201cndarray.rst\u201d file in the NumPy reference guide.", "See also", "Imagine an array of 32-bit integers (each 4 bytes):", "This array is stored in memory as 40 bytes, one after the other (known as a contiguous block of memory). The strides of an array tell us how many bytes we have to skip in memory to move to the next position along a certain axis. For example, we have to skip 4 bytes (1 value) to move to the next column, but 20 bytes (5 values) to get to the same position in the next row. As such, the strides for the array x will be (20, 4)."]}, {"name": "matrix.sum()", "path": "reference/generated/numpy.matrix.sum", "type": "numpy.matrix.sum", "text": ["method", "Returns the sum of the matrix elements, along the given axis.", "Refer to numpy.sum for full documentation.", "See also", "This is the same as ndarray.sum, except that where an ndarray would be returned, a matrix object is returned instead."]}, {"name": "matrix.swapaxes()", "path": "reference/generated/numpy.matrix.swapaxes", "type": "numpy.matrix.swapaxes", "text": ["method", "Return a view of the array with axis1 and axis2 interchanged.", "Refer to numpy.swapaxes for full documentation.", "See also", "equivalent function"]}, {"name": "matrix.take()", "path": "reference/generated/numpy.matrix.take", "type": "numpy.matrix.take", "text": ["method", "Return an array formed from the elements of a at the given indices.", "Refer to numpy.take for full documentation.", "See also", "equivalent function"]}, {"name": "matrix.tobytes()", "path": "reference/generated/numpy.matrix.tobytes", "type": "numpy.matrix.tobytes", "text": ["method", "Construct Python bytes containing the raw data bytes in the array.", "Constructs Python bytes showing a copy of the raw contents of data memory. The bytes object is produced in C-order by default. This behavior is controlled by the order parameter.", "New in version 1.9.0.", "Controls the memory layout of the bytes object. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 (short for Any) means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. Default is \u2018C\u2019.", "Python bytes exhibiting a copy of a\u2019s raw data."]}, {"name": "matrix.tofile()", "path": "reference/generated/numpy.matrix.tofile", "type": "numpy.matrix.tofile", "text": ["method", "Write array to a file as text or binary (default).", "Data is always written in \u2018C\u2019 order, independent of the order of a. The data produced by this method can be recovered using the function fromfile().", "An open file object, or a string containing a filename.", "Changed in version 1.17.0: pathlib.Path objects are now accepted.", "Separator between array items for text output. If \u201c\u201d (empty), a binary file is written, equivalent to file.write(a.tobytes()).", "Format string for text file output. Each entry in the array is formatted to text by first converting it to the closest Python type, and then using \u201cformat\u201d % item.", "This is a convenience function for quick storage of array data. Information on endianness and precision is lost, so this method is not a good choice for files intended to archive data or transport data between machines with different endianness. Some of these problems can be overcome by outputting the data as text files, at the expense of speed and file size.", "When fid is a file object, array contents are directly written to the file, bypassing the file object\u2019s write method. As a result, tofile cannot be used with files objects supporting compression (e.g., GzipFile) or file-like objects that do not support fileno() (e.g., BytesIO)."]}, {"name": "matrix.tolist()", "path": "reference/generated/numpy.matrix.tolist", "type": "numpy.matrix.tolist", "text": ["method", "Return the matrix as a (possibly nested) list.", "See ndarray.tolist for full documentation.", "See also"]}, {"name": "matrix.tostring()", "path": "reference/generated/numpy.matrix.tostring", "type": "numpy.matrix.tostring", "text": ["method", "A compatibility alias for tobytes, with exactly the same behavior.", "Despite its name, it returns bytes not strs.", "Deprecated since version 1.19.0."]}, {"name": "matrix.trace()", "path": "reference/generated/numpy.matrix.trace", "type": "numpy.matrix.trace", "text": ["method", "Return the sum along diagonals of the array.", "Refer to numpy.trace for full documentation.", "See also", "equivalent function"]}, {"name": "matrix.transpose()", "path": "reference/generated/numpy.matrix.transpose", "type": "numpy.matrix.transpose", "text": ["method", "Returns a view of the array with axes transposed.", "For a 1-D array this has no effect, as a transposed vector is simply the same vector. To convert a 1-D array into a 2D column vector, an additional dimension must be added. np.atleast2d(a).T achieves this, as does a[:, np.newaxis]. For a 2-D array, this is a standard matrix transpose. For an n-D array, if axes are given, their order indicates how the axes are permuted (see Examples). If axes are not provided and a.shape = (i[0], i[1], ... i[n-2], i[n-1]), then a.transpose().shape = (i[n-1], i[n-2], ... i[1], i[0]).", "View of a, with axes suitably permuted.", "See also", "Equivalent function", "Array property returning the array transposed.", "Give a new shape to an array without changing its data."]}, {"name": "matrix.var()", "path": "reference/generated/numpy.matrix.var", "type": "numpy.matrix.var", "text": ["method", "Returns the variance of the matrix elements, along the given axis.", "Refer to numpy.var for full documentation.", "See also", "This is the same as ndarray.var, except that where an ndarray would be returned, a matrix object is returned instead."]}, {"name": "matrix.view()", "path": "reference/generated/numpy.matrix.view", "type": "numpy.matrix.view", "text": ["method", "New view of array with the same data.", "Note", "Passing None for dtype is different from omitting the parameter, since the former invokes dtype(None) which is an alias for dtype('float_').", "Data-type descriptor of the returned view, e.g., float32 or int16. Omitting it results in the view having the same data-type as a. This argument can also be specified as an ndarray sub-class, which then specifies the type of the returned object (this is equivalent to setting the type parameter).", "Type of the returned view, e.g., ndarray or matrix. Again, omission of the parameter results in type preservation.", "a.view() is used two different ways:", "a.view(some_dtype) or a.view(dtype=some_dtype) constructs a view of the array\u2019s memory with a different data-type. This can cause a reinterpretation of the bytes of memory.", "a.view(ndarray_subclass) or a.view(type=ndarray_subclass) just returns an instance of ndarray_subclass that looks at the same array (same shape, dtype, etc.) This does not cause a reinterpretation of the memory.", "For a.view(some_dtype), if some_dtype has a different number of bytes per entry than the previous dtype (for example, converting a regular array to a structured array), then the behavior of the view cannot be predicted just from the superficial appearance of a (shown by print(a)). It also depends on exactly how a is stored in memory. Therefore if a is C-ordered versus fortran-ordered, versus defined as a slice or transpose, etc., the view may give different results.", "Viewing array data using a different type and dtype:", "Creating a view on a structured array so it can be used in calculations", "Making changes to the view changes the underlying array", "Using a view to convert an array to a recarray:", "Views share data:", "Views that change the dtype size (bytes per entry) should normally be avoided on arrays defined by slices, transposes, fortran-ordering, etc.:"]}, {"name": "memmap.flush()", "path": "reference/generated/numpy.memmap.flush", "type": "numpy.memmap.flush", "text": ["method", "Write any changes in the array to the file on disk.", "For further information, see memmap.", "See also"]}, {"name": "Miscellaneous", "path": "user/misc", "type": "User Guide", "text": ["Special values defined in numpy: nan, inf,", "NaNs can be used as a poor-man\u2019s mask (if you don\u2019t care what the original value was)", "Note: cannot use equality to test NaNs. E.g.:", "Other related special value functions:", "The following corresponds to the usual functions except that nans are excluded from the results:", "The default is to 'warn' for invalid, divide, and overflow and 'ignore' for underflow. But this can be changed, and it can be set individually for different kinds of exceptions. The different behaviors are:", "These behaviors can be set for all kinds of errors or specific ones:", "Note that integer divide-by-zero is handled by the same machinery. These behaviors are set on a per-thread basis.", "Only a survey of the choices. Little detail on how each works.", "Plusses:", "Minuses:", "Lots of learning overhead:", "Reference counting often difficult to get right.", "Plusses:", "Minuses:", "Plusses:", "good numpy support: arrays have all these in their ctypes attribute:", "Minuses:", "Plusses:", "Minuses:", "Plusses:", "Minuses:", "Plusses:", "Minuses:", "The clear choice to wrap Fortran code is f2py.", "Pyfort is an older alternative, but not supported any longer. Fwrap is a newer project that looked promising but isn\u2019t being developed any longer."]}, {"name": "Miscellaneous routines", "path": "reference/routines.other", "type": "Miscellaneous routines", "text": ["setbufsize(size)", "Set the size of the buffer used in ufuncs.", "getbufsize()", "Return the size of the buffer used in ufuncs.", "shares_memory(a, b, /[, max_work])", "Determine if two arrays share memory.", "may_share_memory(a, b, /[, max_work])", "Determine if two arrays might share memory", "byte_bounds(a)", "Returns pointers to the end-points of an array.", "lib.mixins.NDArrayOperatorsMixin()", "Mixin defining all operator special methods using __array_ufunc__.", "lib.NumpyVersion(vstring)", "Parse and compare numpy version strings.", "get_include()", "Return the directory that contains the NumPy *.h header files.", "show_config()", "Show libraries in the system on which NumPy was built.", "deprecate(*args, **kwargs)", "Issues a DeprecationWarning, adds warning to old_name's docstring, rebinds old_name.__name__ and returns the new function object.", "deprecate_with_doc(msg)", "Deprecates a function and includes the deprecation in its docstring.", "broadcast_shapes(*args)", "Broadcast the input shapes into a single shape.", "who([vardict])", "Print the NumPy arrays in the given dictionary.", "disp(mesg[, device, linefeed])", "Display a message on a device.", "AxisError(axis[, ndim, msg_prefix])", "Axis supplied was invalid."]}, {"name": "Multithreaded Generation", "path": "reference/random/multithreading", "type": "Multithreaded Generation", "text": ["The four core distributions (random, standard_normal, standard_exponential, and standard_gamma) all allow existing arrays to be filled using the out keyword argument. Existing arrays need to be contiguous and well-behaved (writable and aligned). Under normal circumstances, arrays created using the common constructors such as numpy.empty will satisfy these requirements.", "This example makes use of Python 3 concurrent.futures to fill an array using multiple threads. Threads are long-lived so that repeated calls do not require any additional overheads from thread creation.", "The random numbers generated are reproducible in the sense that the same seed will produce the same outputs, given that the number of threads does not change.", "The multithreaded random number generator can be used to fill an array. The values attributes shows the zero-value before the fill and the random value after.", "The time required to produce using multiple threads can be compared to the time required to generate using a single thread.", "The single threaded call directly uses the BitGenerator.", "The gains are substantial and the scaling is reasonable even for arrays that are only moderately large. The gains are even larger when compared to a call that does not use an existing array due to array creation overhead.", "Note that if threads is not set by the user, it will be determined by multiprocessing.cpu_count()."]}, {"name": "NATIVE: Enables all CPU features that supported by the current", "path": "reference/simd/simd-optimizations", "type": "SIMD Optimizations", "text": ["NumPy provides a set of macros that define Universal Intrinsics to abstract out typical platform-specific intrinsics so SIMD code needs to be written only once. There are three layers:", "The command arguments are available in build, build_clib, and build_ext. if build_clib or build_ext are not specified by the user, the arguments of build will be used instead, which also holds the default values.", "Optimization names can be CPU features or groups of features that gather several features or special options to perform a series of procedures.", "The following tables show the current supported optimizations sorted from the lowest to the highest interest.", "Name", "Implies", "SSE", "SSE2", "SSE2", "SSE", "SSE3", "SSE SSE2", "SSSE3", "SSE SSE2 SSE3", "SSE41", "SSE SSE2 SSE3 SSSE3", "POPCNT", "SSE SSE2 SSE3 SSSE3 SSE41", "SSE42", "SSE SSE2 SSE3 SSSE3 SSE41 POPCNT", "AVX", "SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42", "XOP", "SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX", "FMA4", "SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX", "F16C", "SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX", "FMA3", "SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C", "AVX2", "SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C", "AVX512F", "SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2", "AVX512CD", "SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 AVX512F", "Name", "Gather", "Implies", "AVX512_KNL", "AVX512ER AVX512PF", "SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 AVX512F AVX512CD", "AVX512_KNM", "AVX5124FMAPS AVX5124VNNIW AVX512VPOPCNTDQ", "SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 AVX512F AVX512CD AVX512_KNL", "AVX512_SKX", "AVX512VL AVX512BW AVX512DQ", "SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 AVX512F AVX512CD", "AVX512_CLX", "AVX512VNNI", "SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 AVX512F AVX512CD AVX512_SKX", "AVX512_CNL", "AVX512IFMA AVX512VBMI", "SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 AVX512F AVX512CD AVX512_SKX", "AVX512_ICL", "AVX512VBMI2 AVX512BITALG AVX512VPOPCNTDQ", "SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 AVX512F AVX512CD AVX512_SKX AVX512_CLX AVX512_CNL", "Name", "Implies", "VSX", "VSX2", "VSX", "VSX3", "VSX VSX2", "Name", "Implies", "VSX", "VSX2", "VSX2", "VSX", "VSX3", "VSX VSX2", "Name", "Implies", "NEON", "NEON_FP16", "NEON", "NEON_VFPV4", "NEON NEON_FP16", "ASIMD", "NEON NEON_FP16 NEON_VFPV4", "ASIMDHP", "NEON NEON_FP16 NEON_VFPV4 ASIMD", "ASIMDDP", "NEON NEON_FP16 NEON_VFPV4 ASIMD", "ASIMDFHM", "NEON NEON_FP16 NEON_VFPV4 ASIMD ASIMDHP", "Name", "Implies", "NEON", "NEON_FP16 NEON_VFPV4 ASIMD", "NEON_FP16", "NEON NEON_VFPV4 ASIMD", "NEON_VFPV4", "NEON NEON_FP16 ASIMD", "ASIMD", "NEON NEON_FP16 NEON_VFPV4", "ASIMDHP", "NEON NEON_FP16 NEON_VFPV4 ASIMD", "ASIMDDP", "NEON NEON_FP16 NEON_VFPV4 ASIMD", "ASIMDFHM", "NEON NEON_FP16 NEON_VFPV4 ASIMD ASIMDHP", "While the above tables are based on the GCC Compiler, the following tables showing the differences in the other compilers:", "Name", "Implies", "FMA3", "SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C AVX2", "AVX2", "SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3", "AVX512F", "SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 AVX512CD", "Note", "The following features aren\u2019t supported by x86::Intel Compiler: XOP FMA4", "Name", "Implies", "FMA3", "SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C AVX2", "AVX2", "SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3", "AVX512F", "SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 AVX512CD AVX512_SKX", "AVX512CD", "SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 AVX512F AVX512_SKX", "Note", "The following features aren\u2019t supported by x86::Microsoft Visual C/C++: AVX512_KNL AVX512_KNM", "machine, this operation is based on the compiler flags (-march=native, -xHost, /QxHost)", "MIN: Enables the minimum CPU features that can safely run on a wide range of platforms:", "For Arch", "Returns", "x86", "SSE SSE2", "x86 64-bit mode", "SSE SSE2 SSE3", "IBM/POWER big-endian mode", "NONE", "IBM/POWER little-endian mode", "VSX VSX2", "ARMHF", "NONE", "ARM64 AARCH64", "NEON NEON_FP16 NEON_VFPV4 ASIMD", "Interrelated CPU features: Some exceptional conditions force us to link some features together when it come to certain compilers or architectures, resulting in the impossibility of building them separately. These conditions can be divided into two parts, as follows:", "Architectural compatibility: The need to align certain CPU features that are assured to be supported by successive generations of the same architecture, for example:", "NumPy dispatcher is based on multi-source compiling, which means taking a certain source and compiling it multiple times with different compiler flags and also with different C definitions that affect the code paths to enable certain instruction-sets for each compiled object depending on the required optimizations, then combining the returned objects together.", "This mechanism should support all compilers and it doesn\u2019t require any compiler-specific extension, but at the same time it is adds a few steps to normal compilation that are explained as follows:", "Configuring the required optimization by the user before starting to build the source files via the two command arguments as explained above:", "In this part, we check the compiler and platform architecture and cache some of the intermediary results to speed up rebuilding.", "By testing them against the compiler, and seeing what the compiler can support according to the requested optimizations.", "The generated header _cpu_dispatch.h contains all the definitions and headers of instruction-sets for the required optimizations that have been validated during the previous step.", "It also contains extra C definitions that are used for defining NumPy\u2019s Python-level module attributes __cpu_baseline__ and __cpu_dispa\u064dtch__.", "What is in this header?", "The example header was dynamically generated by gcc on an X86 machine. The compiler supports --cpu-baseline=\"sse sse2 sse3\" and --cpu-dispatch=\"ssse3 sse41\", and the result is below.", "Baseline features are the minimal set of required optimizations configured via --cpu-baseline. They have no preprocessor guards and they\u2019re always on, which means they can be used in any source.", "Does this mean NumPy\u2019s infrastructure passes the compiler\u2019s flags of baseline features to all sources?", "Definitely, yes. But the dispatch-able sources are treated differently.", "What if the user specifies certain baseline features during the build but at runtime the machine doesn\u2019t support even these features? Will the compiled code be called via one of these definitions, or maybe the compiler itself auto-generated/vectorized certain piece of code based on the provided command line compiler flags?", "During the loading of the NumPy module, there\u2019s a validation step which detects this behavior. It will raise a Python runtime error to inform the user. This is to prevent the CPU reaching an illegal instruction error causing a segfault.", "Dispatch-able features are our dispatched set of additional optimizations that were configured via --cpu-dispatch. They are not activated by default and are always guarded by other C definitions prefixed with NPY__CPU_TARGET_. C definitions NPY__CPU_TARGET_ are only enabled within dispatch-able sources.", "Dispatch-able sources are special C files that can be compiled multiple times with different compiler flags and also with different C definitions. These affect code paths to enable certain instruction-sets for each compiled object according to \u201cthe configuration statements\u201d that must be declared between a C comment(/**/) and start with a special mark @targets at the top of each dispatch-able source. At the same time, dispatch-able sources will be treated as normal C sources if the optimization was disabled by the command argument --disable-optimization .", "What are configuration statements?", "Configuration statements are sort of keywords combined together to determine the required optimization for the dispatch-able source.", "Example:", "The keywords mainly represent the additional optimizations configured through --cpu-dispatch, but it can also represent other options such as:", "Numpy\u2019s infrastructure handles dispatch-able sources in four steps:", "(C) Wrapping: This is the approach taken by NumPy\u2019s infrastructure, which has proved to be sufficiently flexible in order to compile a single source multiple times with different C definitions and flags that affect the code paths. The process is achieved by creating a temporary C source for each required optimization that related to the additional optimization, which contains the declarations of the C definitions and includes the involved source via the C directive #include. For more clarification take a look at the following code for AVX512F :", "(D) Dispatch-able configuration header: The infrastructure generates a config header for each dispatch-able source, this header mainly contains two abstract C macros used for identifying the generated objects, so they can be used for runtime dispatching certain symbols from the generated objects by any C source. It is also used for forward declarations.", "The generated header takes the name of the dispatch-able source after excluding the extension and replace it with \u2018.h\u2019, for example assume we have a dispatch-able source called hello.dispatch.c and contains the following:", "Now assume you attached hello.dispatch.c to the source tree, then the infrastructure should generate a temporary config header called hello.dispatch.h that can be reached by any source in the source tree, and it should contain the following code :", "An example of using the config header in light of the above:"]}, {"name": "ndarray.__abs__()", "path": "reference/generated/numpy.ndarray.__abs__", "type": "numpy.ndarray.__abs__", "text": ["method"]}, {"name": "ndarray.__add__()", "path": "reference/generated/numpy.ndarray.__add__", "type": "numpy.ndarray.__add__", "text": ["method", "Return self+value."]}, {"name": "ndarray.__and__()", "path": "reference/generated/numpy.ndarray.__and__", "type": "numpy.ndarray.__and__", "text": ["method", "Return self&value."]}, {"name": "ndarray.__array__()", "path": "reference/generated/numpy.ndarray.__array__", "type": "numpy.ndarray.__array__", "text": ["method", "Returns either a new reference to self if dtype is not given or a new array of provided data type if dtype is different from the current dtype of the array."]}, {"name": "ndarray.__array_wrap__()", "path": "reference/generated/numpy.ndarray.__array_wrap__", "type": "numpy.ndarray.__array_wrap__", "text": ["method", "Returns a view of array with the same type as self."]}, {"name": "ndarray.__bool__()", "path": "reference/generated/numpy.ndarray.__bool__", "type": "numpy.ndarray.__bool__", "text": ["method", "self != 0"]}, {"name": "ndarray.__class_getitem__()", "path": "reference/generated/numpy.ndarray.__class_getitem__", "type": "numpy.ndarray.__class_getitem__", "text": ["method", "Return a parametrized wrapper around the ndarray type.", "New in version 1.22.", "A parametrized ndarray type.", "See also", "Type hinting generics in standard collections.", "An ndarray alias generic w.r.t. its dtype.type.", "This method is only available for python 3.9 and later."]}, {"name": "ndarray.__complex__()", "path": "reference/generated/numpy.ndarray.__complex__", "type": "numpy.ndarray.__complex__", "text": ["method"]}, {"name": "ndarray.__contains__()", "path": "reference/generated/numpy.ndarray.__contains__", "type": "numpy.ndarray.__contains__", "text": ["method", "Return key in self."]}, {"name": "ndarray.__copy__()", "path": "reference/generated/numpy.ndarray.__copy__", "type": "numpy.ndarray.__copy__", "text": ["method", "Used if copy.copy is called on an array. Returns a copy of the array.", "Equivalent to a.copy(order='K')."]}, {"name": "ndarray.__deepcopy__()", "path": "reference/generated/numpy.ndarray.__deepcopy__", "type": "numpy.ndarray.__deepcopy__", "text": ["method", "Used if copy.deepcopy is called on an array."]}, {"name": "ndarray.__divmod__()", "path": "reference/generated/numpy.ndarray.__divmod__", "type": "numpy.ndarray.__divmod__", "text": ["method", "Return divmod(self, value)."]}, {"name": "ndarray.__eq__()", "path": "reference/generated/numpy.ndarray.__eq__", "type": "numpy.ndarray.__eq__", "text": ["method", "Return self==value."]}, {"name": "ndarray.__float__()", "path": "reference/generated/numpy.ndarray.__float__", "type": "numpy.ndarray.__float__", "text": ["method"]}, {"name": "ndarray.__floordiv__()", "path": "reference/generated/numpy.ndarray.__floordiv__", "type": "numpy.ndarray.__floordiv__", "text": ["method", "Return self//value."]}, {"name": "ndarray.__ge__()", "path": "reference/generated/numpy.ndarray.__ge__", "type": "numpy.ndarray.__ge__", "text": ["method", "Return self>=value."]}, {"name": "ndarray.__getitem__()", "path": "reference/generated/numpy.ndarray.__getitem__", "type": "numpy.ndarray.__getitem__", "text": ["method", "Return self[key]."]}, {"name": "ndarray.__gt__()", "path": "reference/generated/numpy.ndarray.__gt__", "type": "numpy.ndarray.__gt__", "text": ["method", "Return self>value."]}, {"name": "ndarray.__iadd__()", "path": "reference/generated/numpy.ndarray.__iadd__", "type": "numpy.ndarray.__iadd__", "text": ["method", "Return self+=value."]}, {"name": "ndarray.__iand__()", "path": "reference/generated/numpy.ndarray.__iand__", "type": "numpy.ndarray.__iand__", "text": ["method", "Return self&=value."]}, {"name": "ndarray.__ifloordiv__()", "path": "reference/generated/numpy.ndarray.__ifloordiv__", "type": "numpy.ndarray.__ifloordiv__", "text": ["method", "Return self//=value."]}, {"name": "ndarray.__ilshift__()", "path": "reference/generated/numpy.ndarray.__ilshift__", "type": "numpy.ndarray.__ilshift__", "text": ["method", "Return self<<=value."]}, {"name": "ndarray.__imod__()", "path": "reference/generated/numpy.ndarray.__imod__", "type": "numpy.ndarray.__imod__", "text": ["method", "Return self%=value."]}, {"name": "ndarray.__imul__()", "path": "reference/generated/numpy.ndarray.__imul__", "type": "numpy.ndarray.__imul__", "text": ["method", "Return self*=value."]}, {"name": "ndarray.__int__()", "path": "reference/generated/numpy.ndarray.__int__", "type": "numpy.ndarray.__int__", "text": ["method"]}, {"name": "ndarray.__invert__()", "path": "reference/generated/numpy.ndarray.__invert__", "type": "numpy.ndarray.__invert__", "text": ["method", "~self"]}, {"name": "ndarray.__ior__()", "path": "reference/generated/numpy.ndarray.__ior__", "type": "numpy.ndarray.__ior__", "text": ["method", "Return self|=value."]}, {"name": "ndarray.__ipow__()", "path": "reference/generated/numpy.ndarray.__ipow__", "type": "numpy.ndarray.__ipow__", "text": ["method", "Return self**=value."]}, {"name": "ndarray.__irshift__()", "path": "reference/generated/numpy.ndarray.__irshift__", "type": "numpy.ndarray.__irshift__", "text": ["method", "Return self>>=value."]}, {"name": "ndarray.__isub__()", "path": "reference/generated/numpy.ndarray.__isub__", "type": "numpy.ndarray.__isub__", "text": ["method", "Return self-=value."]}, {"name": "ndarray.__itruediv__()", "path": "reference/generated/numpy.ndarray.__itruediv__", "type": "numpy.ndarray.__itruediv__", "text": ["method", "Return self/=value."]}, {"name": "ndarray.__ixor__()", "path": "reference/generated/numpy.ndarray.__ixor__", "type": "numpy.ndarray.__ixor__", "text": ["method", "Return self^=value."]}, {"name": "ndarray.__le__()", "path": "reference/generated/numpy.ndarray.__le__", "type": "numpy.ndarray.__le__", "text": ["method", "Return self<=value."]}, {"name": "ndarray.__len__()", "path": "reference/generated/numpy.ndarray.__len__", "type": "numpy.ndarray.__len__", "text": ["method", "Return len(self)."]}, {"name": "ndarray.__lshift__()", "path": "reference/generated/numpy.ndarray.__lshift__", "type": "numpy.ndarray.__lshift__", "text": ["method", "Return self<<value."]}, {"name": "ndarray.__lt__()", "path": "reference/generated/numpy.ndarray.__lt__", "type": "numpy.ndarray.__lt__", "text": ["method", "Return self<value."]}, {"name": "ndarray.__matmul__()", "path": "reference/generated/numpy.ndarray.__matmul__", "type": "numpy.ndarray.__matmul__", "text": ["method", "Return self@value."]}, {"name": "ndarray.__mod__()", "path": "reference/generated/numpy.ndarray.__mod__", "type": "numpy.ndarray.__mod__", "text": ["method", "Return self%value."]}, {"name": "ndarray.__mul__()", "path": "reference/generated/numpy.ndarray.__mul__", "type": "numpy.ndarray.__mul__", "text": ["method", "Return self*value."]}, {"name": "ndarray.__ne__()", "path": "reference/generated/numpy.ndarray.__ne__", "type": "numpy.ndarray.__ne__", "text": ["method", "Return self!=value."]}, {"name": "ndarray.__neg__()", "path": "reference/generated/numpy.ndarray.__neg__", "type": "numpy.ndarray.__neg__", "text": ["method", "-self"]}, {"name": "ndarray.__new__()", "path": "reference/generated/numpy.ndarray.__new__", "type": "numpy.ndarray.__new__", "text": ["method"]}, {"name": "ndarray.__or__()", "path": "reference/generated/numpy.ndarray.__or__", "type": "numpy.ndarray.__or__", "text": ["method", "Return self|value."]}, {"name": "ndarray.__pos__()", "path": "reference/generated/numpy.ndarray.__pos__", "type": "numpy.ndarray.__pos__", "text": ["method", "+self"]}, {"name": "ndarray.__pow__()", "path": "reference/generated/numpy.ndarray.__pow__", "type": "numpy.ndarray.__pow__", "text": ["method", "Return pow(self, value, mod)."]}, {"name": "ndarray.__reduce__()", "path": "reference/generated/numpy.ndarray.__reduce__", "type": "numpy.ndarray.__reduce__", "text": ["method", "For pickling."]}, {"name": "ndarray.__repr__()", "path": "reference/generated/numpy.ndarray.__repr__", "type": "numpy.ndarray.__repr__", "text": ["method", "Return repr(self)."]}, {"name": "ndarray.__rshift__()", "path": "reference/generated/numpy.ndarray.__rshift__", "type": "numpy.ndarray.__rshift__", "text": ["method", "Return self>>value."]}, {"name": "ndarray.__setitem__()", "path": "reference/generated/numpy.ndarray.__setitem__", "type": "numpy.ndarray.__setitem__", "text": ["method", "Set self[key] to value."]}, {"name": "ndarray.__setstate__()", "path": "reference/generated/numpy.ndarray.__setstate__", "type": "numpy.ndarray.__setstate__", "text": ["method", "For unpickling.", "The state argument must be a sequence that contains the following elements:", "optional pickle version. If omitted defaults to 0.", "a binary string with the data (or a list if \u2018a\u2019 is an object array)"]}, {"name": "ndarray.__str__()", "path": "reference/generated/numpy.ndarray.__str__", "type": "numpy.ndarray.__str__", "text": ["method", "Return str(self)."]}, {"name": "ndarray.__sub__()", "path": "reference/generated/numpy.ndarray.__sub__", "type": "numpy.ndarray.__sub__", "text": ["method", "Return self-value."]}, {"name": "ndarray.__truediv__()", "path": "reference/generated/numpy.ndarray.__truediv__", "type": "numpy.ndarray.__truediv__", "text": ["method", "Return self/value."]}, {"name": "ndarray.__xor__()", "path": "reference/generated/numpy.ndarray.__xor__", "type": "numpy.ndarray.__xor__", "text": ["method", "Return self^value."]}, {"name": "ndarray.all()", "path": "reference/generated/numpy.ndarray.all", "type": "numpy.ndarray.all", "text": ["method", "Returns True if all elements evaluate to True.", "Refer to numpy.all for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.any()", "path": "reference/generated/numpy.ndarray.any", "type": "numpy.ndarray.any", "text": ["method", "Returns True if any of the elements of a evaluate to True.", "Refer to numpy.any for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.argmax()", "path": "reference/generated/numpy.ndarray.argmax", "type": "numpy.ndarray.argmax", "text": ["method", "Return indices of the maximum values along the given axis.", "Refer to numpy.argmax for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.argmin()", "path": "reference/generated/numpy.ndarray.argmin", "type": "numpy.ndarray.argmin", "text": ["method", "Return indices of the minimum values along the given axis.", "Refer to numpy.argmin for detailed documentation.", "See also", "equivalent function"]}, {"name": "ndarray.argpartition()", "path": "reference/generated/numpy.ndarray.argpartition", "type": "numpy.ndarray.argpartition", "text": ["method", "Returns the indices that would partition this array.", "Refer to numpy.argpartition for full documentation.", "New in version 1.8.0.", "See also", "equivalent function"]}, {"name": "ndarray.argsort()", "path": "reference/generated/numpy.ndarray.argsort", "type": "numpy.ndarray.argsort", "text": ["method", "Returns the indices that would sort this array.", "Refer to numpy.argsort for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.astype()", "path": "reference/generated/numpy.ndarray.astype", "type": "numpy.ndarray.astype", "text": ["method", "Copy of the array, cast to a specified type.", "Typecode or data-type to which the array is cast.", "Controls the memory layout order of the result. \u2018C\u2019 means C order, \u2018F\u2019 means Fortran order, \u2018A\u2019 means \u2018F\u2019 order if all the arrays are Fortran contiguous, \u2018C\u2019 order otherwise, and \u2018K\u2019 means as close to the order the array elements appear in memory as possible. Default is \u2018K\u2019.", "Controls what kind of data casting may occur. Defaults to \u2018unsafe\u2019 for backwards compatibility.", "If True, then sub-classes will be passed-through (default), otherwise the returned array will be forced to be a base-class array.", "By default, astype always returns a newly allocated array. If this is set to false, and the dtype, order, and subok requirements are satisfied, the input array is returned instead of a copy.", "Unless copy is False and the other conditions for returning the input array are satisfied (see description for copy input parameter), arr_t is a new array of the same shape as the input array, with dtype, order given by dtype, order.", "When casting from complex to float or int. To avoid this, one should use a.real.astype(t).", "Changed in version 1.17.0: Casting between a simple data type and a structured one is possible only for \u201cunsafe\u201d casting. Casting to multiple fields is allowed, but casting from multiple fields is not.", "Changed in version 1.9.0: Casting from numeric to string types in \u2018safe\u2019 casting mode requires that the string dtype length is long enough to store the max integer/float value converted."]}, {"name": "ndarray.base", "path": "reference/generated/numpy.ndarray.base", "type": "numpy.ndarray.base", "text": ["attribute", "Base object if memory is from some other object.", "The base of an array that owns its memory is None:", "Slicing creates a view, whose memory is shared with x:"]}, {"name": "ndarray.byteswap()", "path": "reference/generated/numpy.ndarray.byteswap", "type": "numpy.ndarray.byteswap", "text": ["method", "Swap the bytes of the array elements", "Toggle between low-endian and big-endian data representation by returning a byteswapped array, optionally swapped in-place. Arrays of byte-strings are not swapped. The real and imaginary parts of a complex number are swapped individually.", "If True, swap bytes in-place, default is False.", "The byteswapped array. If inplace is True, this is a view to self.", "Arrays of byte-strings are not swapped", "but different representation in memory"]}, {"name": "ndarray.choose()", "path": "reference/generated/numpy.ndarray.choose", "type": "numpy.ndarray.choose", "text": ["method", "Use an index array to construct a new array from a set of choices.", "Refer to numpy.choose for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.clip()", "path": "reference/generated/numpy.ndarray.clip", "type": "numpy.ndarray.clip", "text": ["method", "Return an array whose values are limited to [min, max]. One of max or min must be given.", "Refer to numpy.clip for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.compress()", "path": "reference/generated/numpy.ndarray.compress", "type": "numpy.ndarray.compress", "text": ["method", "Return selected slices of this array along given axis.", "Refer to numpy.compress for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.conj()", "path": "reference/generated/numpy.ndarray.conj", "type": "numpy.ndarray.conj", "text": ["method", "Complex-conjugate all elements.", "Refer to numpy.conjugate for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.conjugate()", "path": "reference/generated/numpy.ndarray.conjugate", "type": "numpy.ndarray.conjugate", "text": ["method", "Return the complex conjugate, element-wise.", "Refer to numpy.conjugate for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.copy()", "path": "reference/generated/numpy.ndarray.copy", "type": "numpy.ndarray.copy", "text": ["method", "Return a copy of the array.", "Controls the memory layout of the copy. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of a as closely as possible. (Note that this function and numpy.copy are very similar but have different default values for their order= arguments, and this function always passes sub-classes through.)", "See also", "Similar function with different default behavior", "This function is the preferred method for creating an array copy. The function numpy.copy is similar, but it defaults to using order \u2018K\u2019, and will not pass sub-classes through by default."]}, {"name": "ndarray.ctypes", "path": "reference/generated/numpy.ndarray.ctypes", "type": "numpy.ndarray.ctypes", "text": ["attribute", "An object to simplify the interaction of the array with the ctypes module.", "This attribute creates an object that makes it easier to use arrays when calling shared libraries with the ctypes module. The returned object has, among others, data, shape, and strides attributes (see Notes below) which themselves return ctypes objects that can be used as arguments to a shared library.", "Possessing attributes data, shape, strides, etc.", "See also", "Below are the public attributes of this object which were documented in \u201cGuide to NumPy\u201d (we have omitted undocumented public attributes, as well as documented private attributes):", "A pointer to the memory area of the array as a Python integer. This memory area may contain data that is not aligned, or not in correct byte-order. The memory area may not even be writeable. The array flags and data-type of this array should be respected when passing this attribute to arbitrary C-code to avoid trouble that can include Python crashing. User Beware! The value of this attribute is exactly the same as self._array_interface_['data'][0].", "Note that unlike data_as, a reference will not be kept to the array: code like ctypes.c_void_p((a + b).ctypes.data) will result in a pointer to a deallocated array, and should be spelt (a + b).ctypes.data_as(ctypes.c_void_p)", "(c_intp*self.ndim): A ctypes array of length self.ndim where the basetype is the C-integer corresponding to dtype('p') on this platform (see c_intp). This base-type could be ctypes.c_int, ctypes.c_long, or ctypes.c_longlong depending on the platform. The ctypes array contains the shape of the underlying array.", "(c_intp*self.ndim): A ctypes array of length self.ndim where the basetype is the same as for the shape attribute. This ctypes array contains the strides information from the underlying array. This strides information is important for showing how many bytes must be jumped to get to the next element in the array.", "Return the data pointer cast to a particular c-types object. For example, calling self._as_parameter_ is equivalent to self.data_as(ctypes.c_void_p). Perhaps you want to use the data as a pointer to a ctypes array of floating-point data: self.data_as(ctypes.POINTER(ctypes.c_double)).", "The returned pointer will keep a reference to the array.", "Return the shape tuple as an array of some other c-types type. For example: self.shape_as(ctypes.c_short).", "Return the strides tuple as an array of some other c-types type. For example: self.strides_as(ctypes.c_longlong).", "If the ctypes module is not available, then the ctypes attribute of array objects still returns something useful, but ctypes objects are not returned and errors may be raised instead. In particular, the object will still have the as_parameter attribute which will return an integer equal to the data attribute."]}, {"name": "ndarray.cumprod()", "path": "reference/generated/numpy.ndarray.cumprod", "type": "numpy.ndarray.cumprod", "text": ["method", "Return the cumulative product of the elements along the given axis.", "Refer to numpy.cumprod for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.cumsum()", "path": "reference/generated/numpy.ndarray.cumsum", "type": "numpy.ndarray.cumsum", "text": ["method", "Return the cumulative sum of the elements along the given axis.", "Refer to numpy.cumsum for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.data", "path": "reference/generated/numpy.ndarray.data", "type": "numpy.ndarray.data", "text": ["attribute", "Python buffer object pointing to the start of the array\u2019s data."]}, {"name": "ndarray.diagonal()", "path": "reference/generated/numpy.ndarray.diagonal", "type": "numpy.ndarray.diagonal", "text": ["method", "Return specified diagonals. In NumPy 1.9 the returned array is a read-only view instead of a copy as in previous NumPy versions. In a future version the read-only restriction will be removed.", "Refer to numpy.diagonal for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.dtype", "path": "reference/generated/numpy.ndarray.dtype", "type": "numpy.ndarray.dtype", "text": ["attribute", "Data-type of the array\u2019s elements.", "See also"]}, {"name": "ndarray.dump()", "path": "reference/generated/numpy.ndarray.dump", "type": "numpy.ndarray.dump", "text": ["method", "Dump a pickle of the array to the specified file. The array can be read back with pickle.load or numpy.load.", "A string naming the dump file.", "Changed in version 1.17.0: pathlib.Path objects are now accepted."]}, {"name": "ndarray.dumps()", "path": "reference/generated/numpy.ndarray.dumps", "type": "numpy.ndarray.dumps", "text": ["method", "Returns the pickle of the array as a string. pickle.loads will convert the string back to an array."]}, {"name": "ndarray.fill()", "path": "reference/generated/numpy.ndarray.fill", "type": "numpy.ndarray.fill", "text": ["method", "Fill the array with a scalar value.", "All elements of a will be assigned this value."]}, {"name": "ndarray.flags", "path": "reference/generated/numpy.ndarray.flags", "type": "numpy.ndarray.flags", "text": ["attribute", "Information about the memory layout of the array.", "The flags object can be accessed dictionary-like (as in a.flags['WRITEABLE']), or by using lowercased attribute names (as in a.flags.writeable). Short flag names are only supported in dictionary access.", "Only the WRITEBACKIFCOPY, UPDATEIFCOPY, WRITEABLE, and ALIGNED flags can be changed by the user, via direct assignment to the attribute or dictionary entry, or by calling ndarray.setflags.", "The array flags cannot be set arbitrarily:", "Arrays can be both C-style and Fortran-style contiguous simultaneously. This is clear for 1-dimensional arrays, but can also be true for higher dimensional arrays.", "Even for contiguous arrays a stride for a given dimension arr.strides[dim] may be arbitrary if arr.shape[dim] == 1 or the array has no elements. It does not generally hold that self.strides[-1] == self.itemsize for C-style contiguous arrays or self.strides[0] == self.itemsize for Fortran-style contiguous arrays is true.", "The data is in a single, C-style contiguous segment.", "The data is in a single, Fortran-style contiguous segment.", "The array owns the memory it uses or borrows it from another object.", "The data area can be written to. Setting this to False locks the data, making it read-only. A view (slice, etc.) inherits WRITEABLE from its base array at creation time, but a view of a writeable array may be subsequently locked while the base array remains writeable. (The opposite is not true, in that a view of a locked array may not be made writeable. However, currently, locking a base object does not lock any views that already reference it, so under that circumstance it is possible to alter the contents of a locked array via a previously created writeable view onto it.) Attempting to change a non-writeable array raises a RuntimeError exception.", "The data and all elements are aligned appropriately for the hardware.", "This array is a copy of some other array. The C-API function PyArray_ResolveWritebackIfCopy must be called before deallocating to the base array will be updated with the contents of this array.", "(Deprecated, use WRITEBACKIFCOPY) This array is a copy of some other array. When this array is deallocated, the base array will be updated with the contents of this array.", "F_CONTIGUOUS and not C_CONTIGUOUS.", "F_CONTIGUOUS or C_CONTIGUOUS (one-segment test).", "ALIGNED and WRITEABLE.", "BEHAVED and C_CONTIGUOUS.", "BEHAVED and F_CONTIGUOUS and not C_CONTIGUOUS."]}, {"name": "ndarray.flat", "path": "reference/generated/numpy.ndarray.flat", "type": "numpy.ndarray.flat", "text": ["attribute", "A 1-D iterator over the array.", "This is a numpy.flatiter instance, which acts similarly to, but is not a subclass of, Python\u2019s built-in iterator object.", "See also", "Return a copy of the array collapsed into one dimension.", "An assignment example:"]}, {"name": "ndarray.flatten()", "path": "reference/generated/numpy.ndarray.flatten", "type": "numpy.ndarray.flatten", "text": ["method", "Return a copy of the array collapsed into one dimension.", "\u2018C\u2019 means to flatten in row-major (C-style) order. \u2018F\u2019 means to flatten in column-major (Fortran- style) order. \u2018A\u2019 means to flatten in column-major order if a is Fortran contiguous in memory, row-major order otherwise. \u2018K\u2019 means to flatten a in the order the elements occur in memory. The default is \u2018C\u2019.", "A copy of the input array, flattened to one dimension.", "See also", "Return a flattened array.", "A 1-D flat iterator over the array."]}, {"name": "ndarray.getfield()", "path": "reference/generated/numpy.ndarray.getfield", "type": "numpy.ndarray.getfield", "text": ["method", "Returns a field of the given array as a certain type.", "A field is a view of the array data with a given data-type. The values in the view are determined by the given type and the offset into the current array in bytes. The offset needs to be such that the view dtype fits in the array dtype; for example an array of dtype complex128 has 16-byte elements. If taking a view with a 32-bit integer (4 bytes), the offset needs to be between 0 and 12 bytes.", "The data type of the view. The dtype size of the view can not be larger than that of the array itself.", "Number of bytes to skip before beginning the element view.", "By choosing an offset of 8 bytes we can select the complex part of the array for our view:"]}, {"name": "ndarray.imag", "path": "reference/generated/numpy.ndarray.imag", "type": "numpy.ndarray.imag", "text": ["attribute", "The imaginary part of the array."]}, {"name": "ndarray.item()", "path": "reference/generated/numpy.ndarray.item", "type": "numpy.ndarray.item", "text": ["method", "Copy an element of an array to a standard Python scalar and return it.", "A copy of the specified element of the array as a suitable Python scalar", "When the data type of a is longdouble or clongdouble, item() returns a scalar array object because there is no available Python scalar that would not lose information. Void arrays return a buffer object for item(), unless fields are defined, in which case a tuple is returned.", "item is very similar to a[args], except, instead of an array scalar, a standard Python scalar is returned. This can be useful for speeding up access to elements of the array and doing arithmetic on elements of the array using Python\u2019s optimized math."]}, {"name": "ndarray.itemset()", "path": "reference/generated/numpy.ndarray.itemset", "type": "numpy.ndarray.itemset", "text": ["method", "Insert scalar into an array (scalar is cast to array\u2019s dtype, if possible)", "There must be at least 1 argument, and define the last argument as item. Then, a.itemset(*args) is equivalent to but faster than a[args] = item. The item should be a scalar value and args must select a single item in the array a.", "If one argument: a scalar, only used in case a is of size 1. If two arguments: the last argument is the value to be set and must be a scalar, the first argument specifies a single array element location. It is either an int or a tuple.", "Compared to indexing syntax, itemset provides some speed increase for placing a scalar into a particular location in an ndarray, if you must do this. However, generally this is discouraged: among other problems, it complicates the appearance of the code. Also, when using itemset (and item) inside a loop, be sure to assign the methods to a local variable to avoid the attribute look-up at each loop iteration."]}, {"name": "ndarray.itemsize", "path": "reference/generated/numpy.ndarray.itemsize", "type": "numpy.ndarray.itemsize", "text": ["attribute", "Length of one array element in bytes."]}, {"name": "ndarray.max()", "path": "reference/generated/numpy.ndarray.max", "type": "numpy.ndarray.max", "text": ["method", "Return the maximum along a given axis.", "Refer to numpy.amax for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.mean()", "path": "reference/generated/numpy.ndarray.mean", "type": "numpy.ndarray.mean", "text": ["method", "Returns the average of the array elements along given axis.", "Refer to numpy.mean for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.min()", "path": "reference/generated/numpy.ndarray.min", "type": "numpy.ndarray.min", "text": ["method", "Return the minimum along a given axis.", "Refer to numpy.amin for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.nbytes", "path": "reference/generated/numpy.ndarray.nbytes", "type": "numpy.ndarray.nbytes", "text": ["attribute", "Total bytes consumed by the elements of the array.", "Does not include memory consumed by non-element attributes of the array object."]}, {"name": "ndarray.ndim", "path": "reference/generated/numpy.ndarray.ndim", "type": "numpy.ndarray.ndim", "text": ["attribute", "Number of array dimensions."]}, {"name": "ndarray.ndim", "path": "user/quickstart", "type": "User Guide", "text": ["You\u2019ll need to know a bit of Python. For a refresher, see the Python tutorial.", "To work the examples, you\u2019ll need matplotlib installed in addition to NumPy.", "Learner profile", "This is a quick overview of arrays in NumPy. It demonstrates how n-dimensional (\\(n>=2\\)) arrays are represented and can be manipulated. In particular, if you don\u2019t know how to apply common functions to n-dimensional arrays (without using for-loops), or if you want to understand axis and shape properties for n-dimensional arrays, this article might be of help.", "Learning Objectives", "After reading, you should be able to:", "NumPy\u2019s main object is the homogeneous multidimensional array. It is a table of elements (usually numbers), all of the same type, indexed by a tuple of non-negative integers. In NumPy dimensions are called axes.", "For example, the array for the coordinates of a point in 3D space, [1, 2, 1], has one axis. That axis has 3 elements in it, so we say it has a length of 3. In the example pictured below, the array has 2 axes. The first axis has a length of 2, the second axis has a length of 3.", "NumPy\u2019s array class is called ndarray. It is also known by the alias array. Note that numpy.array is not the same as the Standard Python Library class array.array, which only handles one-dimensional arrays and offers less functionality. The more important attributes of an ndarray object are:", "the number of axes (dimensions) of the array.", "the dimensions of the array. This is a tuple of integers indicating the size of the array in each dimension. For a matrix with n rows and m columns, shape will be (n,m). The length of the shape tuple is therefore the number of axes, ndim.", "the total number of elements of the array. This is equal to the product of the elements of shape.", "an object describing the type of the elements in the array. One can create or specify dtype\u2019s using standard Python types. Additionally NumPy provides types of its own. numpy.int32, numpy.int16, and numpy.float64 are some examples.", "the size in bytes of each element of the array. For example, an array of elements of type float64 has itemsize 8 (=64/8), while one of type complex32 has itemsize 4 (=32/8). It is equivalent to ndarray.dtype.itemsize.", "the buffer containing the actual elements of the array. Normally, we won\u2019t need to use this attribute because we will access the elements in an array using indexing facilities.", "There are several ways to create arrays.", "For example, you can create an array from a regular Python list or tuple using the array function. The type of the resulting array is deduced from the type of the elements in the sequences.", "A frequent error consists in calling array with multiple arguments, rather than providing a single sequence as an argument.", "array transforms sequences of sequences into two-dimensional arrays, sequences of sequences of sequences into three-dimensional arrays, and so on.", "The type of the array can also be explicitly specified at creation time:", "Often, the elements of an array are originally unknown, but its size is known. Hence, NumPy offers several functions to create arrays with initial placeholder content. These minimize the necessity of growing arrays, an expensive operation.", "The function zeros creates an array full of zeros, the function ones creates an array full of ones, and the function empty creates an array whose initial content is random and depends on the state of the memory. By default, the dtype of the created array is float64, but it can be specified via the key word argument dtype.", "To create sequences of numbers, NumPy provides the arange function which is analogous to the Python built-in range, but returns an array.", "When arange is used with floating point arguments, it is generally not possible to predict the number of elements obtained, due to the finite floating point precision. For this reason, it is usually better to use the function linspace that receives as an argument the number of elements that we want, instead of the step:", "See also", "array, zeros, zeros_like, ones, ones_like, empty, empty_like, arange, linspace, numpy.random.Generator.rand, numpy.random.Generator.randn, fromfunction, fromfile", "When you print an array, NumPy displays it in a similar way to nested lists, but with the following layout:", "One-dimensional arrays are then printed as rows, bidimensionals as matrices and tridimensionals as lists of matrices.", "See below to get more details on reshape.", "If an array is too large to be printed, NumPy automatically skips the central part of the array and only prints the corners:", "To disable this behaviour and force NumPy to print the entire array, you can change the printing options using set_printoptions.", "Arithmetic operators on arrays apply elementwise. A new array is created and filled with the result.", "Unlike in many matrix languages, the product operator * operates elementwise in NumPy arrays. The matrix product can be performed using the @ operator (in python >=3.5) or the dot function or method:", "Some operations, such as += and *=, act in place to modify an existing array rather than create a new one.", "When operating with arrays of different types, the type of the resulting array corresponds to the more general or precise one (a behavior known as upcasting).", "Many unary operations, such as computing the sum of all the elements in the array, are implemented as methods of the ndarray class.", "By default, these operations apply to the array as though it were a list of numbers, regardless of its shape. However, by specifying the axis parameter you can apply an operation along the specified axis of an array:", "NumPy provides familiar mathematical functions such as sin, cos, and exp. In NumPy, these are called \u201cuniversal functions\u201d (ufunc). Within NumPy, these functions operate elementwise on an array, producing an array as output.", "See also", "all, any, apply_along_axis, argmax, argmin, argsort, average, bincount, ceil, clip, conj, corrcoef, cov, cross, cumprod, cumsum, diff, dot, floor, inner, invert, lexsort, max, maximum, mean, median, min, minimum, nonzero, outer, prod, re, round, sort, std, sum, trace, transpose, var, vdot, vectorize, where", "One-dimensional arrays can be indexed, sliced and iterated over, much like lists and other Python sequences.", "Multidimensional arrays can have one index per axis. These indices are given in a tuple separated by commas:", "When fewer indices are provided than the number of axes, the missing indices are considered complete slices:", "The expression within brackets in b[i] is treated as an i followed by as many instances of : as needed to represent the remaining axes. NumPy also allows you to write this using dots as b[i, ...].", "The dots (...) represent as many colons as needed to produce a complete indexing tuple. For example, if x is an array with 5 axes, then", "Iterating over multidimensional arrays is done with respect to the first axis:", "However, if one wants to perform an operation on each element in the array, one can use the flat attribute which is an iterator over all the elements of the array:", "See also", "Indexing on ndarrays, Indexing routines (reference), newaxis, ndenumerate, indices", "An array has a shape given by the number of elements along each axis:", "The shape of an array can be changed with various commands. Note that the following three commands all return a modified array, but do not change the original array:", "The order of the elements in the array resulting from ravel is normally \u201cC-style\u201d, that is, the rightmost index \u201cchanges the fastest\u201d, so the element after a[0, 0] is a[0, 1]. If the array is reshaped to some other shape, again the array is treated as \u201cC-style\u201d. NumPy normally creates arrays stored in this order, so ravel will usually not need to copy its argument, but if the array was made by taking slices of another array or created with unusual options, it may need to be copied. The functions ravel and reshape can also be instructed, using an optional argument, to use FORTRAN-style arrays, in which the leftmost index changes the fastest.", "The reshape function returns its argument with a modified shape, whereas the ndarray.resize method modifies the array itself:", "If a dimension is given as -1 in a reshaping operation, the other dimensions are automatically calculated:", "See also", "ndarray.shape, reshape, resize, ravel", "Several arrays can be stacked together along different axes:", "The function column_stack stacks 1D arrays as columns into a 2D array. It is equivalent to hstack only for 2D arrays:", "On the other hand, the function row_stack is equivalent to vstack for any input arrays. In fact, row_stack is an alias for vstack:", "In general, for arrays with more than two dimensions, hstack stacks along their second axes, vstack stacks along their first axes, and concatenate allows for an optional arguments giving the number of the axis along which the concatenation should happen.", "Note", "In complex cases, r_ and c_ are useful for creating arrays by stacking numbers along one axis. They allow the use of range literals :.", "When used with arrays as arguments, r_ and c_ are similar to vstack and hstack in their default behavior, but allow for an optional argument giving the number of the axis along which to concatenate.", "See also", "hstack, vstack, column_stack, concatenate, c_, r_", "Using hsplit, you can split an array along its horizontal axis, either by specifying the number of equally shaped arrays to return, or by specifying the columns after which the division should occur:", "vsplit splits along the vertical axis, and array_split allows one to specify along which axis to split.", "When operating and manipulating arrays, their data is sometimes copied into a new array and sometimes not. This is often a source of confusion for beginners. There are three cases:", "Simple assignments make no copy of objects or their data.", "Python passes mutable objects as references, so function calls make no copy.", "Different array objects can share the same data. The view method creates a new array object that looks at the same data.", "Slicing an array returns a view of it:", "The copy method makes a complete copy of the array and its data.", "Sometimes copy should be called after slicing if the original array is not required anymore. For example, suppose a is a huge intermediate result and the final result b only contains a small fraction of a, a deep copy should be made when constructing b with slicing:", "If b = a[:100] is used instead, a is referenced by b and will persist in memory even if del a is executed.", "Here is a list of some useful NumPy functions and methods names ordered in categories. See Routines for the full list.", "arange, array, copy, empty, empty_like, eye, fromfile, fromfunction, identity, linspace, logspace, mgrid, ogrid, ones, ones_like, r_, zeros, zeros_like", "ndarray.astype, atleast_1d, atleast_2d, atleast_3d, mat", "array_split, column_stack, concatenate, diagonal, dsplit, dstack, hsplit, hstack, ndarray.item, newaxis, ravel, repeat, reshape, resize, squeeze, swapaxes, take, transpose, vsplit, vstack", "all, any, nonzero, where", "argmax, argmin, argsort, max, min, ptp, searchsorted, sort", "choose, compress, cumprod, cumsum, inner, ndarray.fill, imag, prod, put, putmask, real, sum", "cov, mean, std, var", "cross, dot, outer, linalg.svd, vdot", "Broadcasting allows universal functions to deal in a meaningful way with inputs that do not have exactly the same shape.", "The first rule of broadcasting is that if all input arrays do not have the same number of dimensions, a \u201c1\u201d will be repeatedly prepended to the shapes of the smaller arrays until all the arrays have the same number of dimensions.", "The second rule of broadcasting ensures that arrays with a size of 1 along a particular dimension act as if they had the size of the array with the largest shape along that dimension. The value of the array element is assumed to be the same along that dimension for the \u201cbroadcast\u201d array.", "After application of the broadcasting rules, the sizes of all arrays must match. More details can be found in Broadcasting.", "NumPy offers more indexing facilities than regular Python sequences. In addition to indexing by integers and slices, as we saw before, arrays can be indexed by arrays of integers and arrays of booleans.", "When the indexed array a is multidimensional, a single array of indices refers to the first dimension of a. The following example shows this behavior by converting an image of labels into a color image using a palette.", "We can also give indexes for more than one dimension. The arrays of indices for each dimension must have the same shape.", "In Python, arr[i, j] is exactly the same as arr[(i, j)]\u2014so we can put i and j in a tuple and then do the indexing with that.", "However, we can not do this by putting i and j into an array, because this array will be interpreted as indexing the first dimension of a.", "Another common use of indexing with arrays is the search of the maximum value of time-dependent series:", "You can also use indexing with arrays as a target to assign to:", "However, when the list of indices contains repetitions, the assignment is done several times, leaving behind the last value:", "This is reasonable enough, but watch out if you want to use Python\u2019s += construct, as it may not do what you expect:", "Even though 0 occurs twice in the list of indices, the 0th element is only incremented once. This is because Python requires a += 1 to be equivalent to a = a + 1.", "When we index arrays with arrays of (integer) indices we are providing the list of indices to pick. With boolean indices the approach is different; we explicitly choose which items in the array we want and which ones we don\u2019t.", "The most natural way one can think of for boolean indexing is to use boolean arrays that have the same shape as the original array:", "This property can be very useful in assignments:", "You can look at the following example to see how to use boolean indexing to generate an image of the Mandelbrot set:", "The second way of indexing with booleans is more similar to integer indexing; for each dimension of the array we give a 1D boolean array selecting the slices we want:", "Note that the length of the 1D boolean array must coincide with the length of the dimension (or axis) you want to slice. In the previous example, b1 has length 3 (the number of rows in a), and b2 (of length 4) is suitable to index the 2nd axis (columns) of a.", "The ix_ function can be used to combine different vectors so as to obtain the result for each n-uplet. For example, if you want to compute all the a+b*c for all the triplets taken from each of the vectors a, b and c:", "You could also implement the reduce as follows:", "and then use it as:", "The advantage of this version of reduce compared to the normal ufunc.reduce is that it makes use of the broadcasting rules in order to avoid creating an argument array the size of the output times the number of vectors.", "See Structured arrays.", "Here we give a list of short and useful tips.", "To change the dimensions of an array, you can omit one of the sizes which will then be deduced automatically:", "How do we construct a 2D array from a list of equally-sized row vectors? In MATLAB this is quite easy: if x and y are two vectors of the same length you only need do m=[x;y]. In NumPy this works via the functions column_stack, dstack, hstack and vstack, depending on the dimension in which the stacking is to be done. For example:", "The logic behind those functions in more than two dimensions can be strange.", "See also", "NumPy for MATLAB users", "The NumPy histogram function applied to an array returns a pair of vectors: the histogram of the array and a vector of the bin edges. Beware: matplotlib also has a function to build histograms (called hist, as in Matlab) that differs from the one in NumPy. The main difference is that pylab.hist plots the histogram automatically, while numpy.histogram only generates the data.", "With Matplotlib >=3.4 you can also use plt.stairs(n, bins)."]}, {"name": "ndarray.newbyteorder()", "path": "reference/generated/numpy.ndarray.newbyteorder", "type": "numpy.ndarray.newbyteorder", "text": ["method", "Return the array with the same data viewed with a different byte order.", "Equivalent to:", "Changes are also made in all fields and sub-arrays of the array data type.", "Byte order to force; a value from the byte order specifications below. new_order codes can be any of:", "The default value (\u2018S\u2019) results in swapping the current byte order.", "New array object with the dtype reflecting given change to the byte order."]}, {"name": "ndarray.nonzero()", "path": "reference/generated/numpy.ndarray.nonzero", "type": "numpy.ndarray.nonzero", "text": ["method", "Return the indices of the elements that are non-zero.", "Refer to numpy.nonzero for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.partition()", "path": "reference/generated/numpy.ndarray.partition", "type": "numpy.ndarray.partition", "text": ["method", "Rearranges the elements in the array in such a way that the value of the element in kth position is in the position it would be in a sorted array. All elements smaller than the kth element are moved before this element and all equal or greater are moved behind it. The ordering of the elements in the two partitions is undefined.", "New in version 1.8.0.", "Element index to partition by. The kth element value will be in its final sorted position and all smaller elements will be moved before it and all equal or greater elements behind it. The order of all elements in the partitions is undefined. If provided with a sequence of kth it will partition all elements indexed by kth of them into their sorted position at once.", "Deprecated since version 1.22.0: Passing booleans as index is deprecated.", "Axis along which to sort. Default is -1, which means sort along the last axis.", "Selection algorithm. Default is \u2018introselect\u2019.", "When a is an array with fields defined, this argument specifies which fields to compare first, second, etc. A single field can be specified as a string, and not all fields need to be specified, but unspecified fields will still be used, in the order in which they come up in the dtype, to break ties.", "See also", "Return a parititioned copy of an array.", "Indirect partition.", "Full sort.", "See np.partition for notes on the different algorithms."]}, {"name": "ndarray.prod()", "path": "reference/generated/numpy.ndarray.prod", "type": "numpy.ndarray.prod", "text": ["method", "Return the product of the array elements over the given axis", "Refer to numpy.prod for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.ptp()", "path": "reference/generated/numpy.ndarray.ptp", "type": "numpy.ndarray.ptp", "text": ["method", "Peak to peak (maximum - minimum) value along a given axis.", "Refer to numpy.ptp for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.put()", "path": "reference/generated/numpy.ndarray.put", "type": "numpy.ndarray.put", "text": ["method", "Set a.flat[n] = values[n] for all n in indices.", "Refer to numpy.put for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.ravel()", "path": "reference/generated/numpy.ndarray.ravel", "type": "numpy.ndarray.ravel", "text": ["method", "Return a flattened array.", "Refer to numpy.ravel for full documentation.", "See also", "equivalent function", "a flat iterator on the array."]}, {"name": "ndarray.real", "path": "reference/generated/numpy.ndarray.real", "type": "numpy.ndarray.real", "text": ["attribute", "The real part of the array.", "See also", "equivalent function"]}, {"name": "ndarray.repeat()", "path": "reference/generated/numpy.ndarray.repeat", "type": "numpy.ndarray.repeat", "text": ["method", "Repeat elements of an array.", "Refer to numpy.repeat for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.reshape()", "path": "reference/generated/numpy.ndarray.reshape", "type": "numpy.ndarray.reshape", "text": ["method", "Returns an array containing the same data with a new shape.", "Refer to numpy.reshape for full documentation.", "See also", "equivalent function", "Unlike the free function numpy.reshape, this method on ndarray allows the elements of the shape parameter to be passed in as separate arguments. For example, a.reshape(10, 11) is equivalent to a.reshape((10, 11))."]}, {"name": "ndarray.resize()", "path": "reference/generated/numpy.ndarray.resize", "type": "numpy.ndarray.resize", "text": ["method", "Change shape and size of array in-place.", "Shape of resized array.", "If False, reference count will not be checked. Default is True.", "If a does not own its own data or references or views to it exist, and the data memory must be changed. PyPy only: will always raise if the data memory must be changed, since there is no reliable way to determine if references or views to it exist.", "If the order keyword argument is specified. This behaviour is a bug in NumPy.", "See also", "Return a new array with the specified shape.", "This reallocates space for the data area if necessary.", "Only contiguous arrays (data elements consecutive in memory) can be resized.", "The purpose of the reference count check is to make sure you do not use this array as a buffer for another Python object and then reallocate the memory. However, reference counts can increase in other ways so if you are sure that you have not shared the memory for this array with another Python object, then you may safely set refcheck to False.", "Shrinking an array: array is flattened (in the order that the data are stored in memory), resized, and reshaped:", "Enlarging an array: as above, but missing entries are filled with zeros:", "Referencing an array prevents resizing\u2026", "Unless refcheck is False:"]}, {"name": "ndarray.round()", "path": "reference/generated/numpy.ndarray.round", "type": "numpy.ndarray.round", "text": ["method", "Return a with each element rounded to the given number of decimals.", "Refer to numpy.around for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.searchsorted()", "path": "reference/generated/numpy.ndarray.searchsorted", "type": "numpy.ndarray.searchsorted", "text": ["method", "Find indices where elements of v should be inserted in a to maintain order.", "For full documentation, see numpy.searchsorted", "See also", "equivalent function"]}, {"name": "ndarray.setfield()", "path": "reference/generated/numpy.ndarray.setfield", "type": "numpy.ndarray.setfield", "text": ["method", "Put a value into a specified place in a field defined by a data-type.", "Place val into a\u2019s field defined by dtype and beginning offset bytes into the field.", "Value to be placed in field.", "Data-type of the field in which to place val.", "The number of bytes into the field at which to place val.", "See also"]}, {"name": "ndarray.setflags()", "path": "reference/generated/numpy.ndarray.setflags", "type": "numpy.ndarray.setflags", "text": ["method", "Set array flags WRITEABLE, ALIGNED, (WRITEBACKIFCOPY and UPDATEIFCOPY), respectively.", "These Boolean-valued flags affect how numpy interprets the memory area used by a (see Notes below). The ALIGNED flag can only be set to True if the data is actually aligned according to the type. The WRITEBACKIFCOPY and (deprecated) UPDATEIFCOPY flags can never be set to True. The flag WRITEABLE can only be set to True if the array owns its own memory, or the ultimate owner of the memory exposes a writeable buffer interface, or is a string. (The exception for string is made so that unpickling can be done without copying memory.)", "Describes whether or not a can be written to.", "Describes whether or not a is aligned properly for its type.", "Describes whether or not a is a copy of another \u201cbase\u201d array.", "Array flags provide information about how the memory area used for the array is to be interpreted. There are 7 Boolean flags in use, only four of which can be changed by the user: WRITEBACKIFCOPY, UPDATEIFCOPY, WRITEABLE, and ALIGNED.", "WRITEABLE (W) the data area can be written to;", "ALIGNED (A) the data and strides are aligned appropriately for the hardware (as determined by the compiler);", "UPDATEIFCOPY (U) (deprecated), replaced by WRITEBACKIFCOPY;", "WRITEBACKIFCOPY (X) this array is a copy of some other array (referenced by .base). When the C-API function PyArray_ResolveWritebackIfCopy is called, the base array will be updated with the contents of this array.", "All flags can be accessed using the single (upper case) letter as well as the full name."]}, {"name": "ndarray.shape", "path": "reference/generated/numpy.ndarray.shape", "type": "numpy.ndarray.shape", "text": ["attribute", "Tuple of array dimensions.", "The shape property is usually used to get the current shape of an array, but may also be used to reshape the array in-place by assigning a tuple of array dimensions to it. As with numpy.reshape, one of the new shape dimensions can be -1, in which case its value is inferred from the size of the array and the remaining dimensions. Reshaping an array in-place will fail if a copy is required.", "See also", "similar function", "similar method"]}, {"name": "ndarray.size", "path": "reference/generated/numpy.ndarray.size", "type": "numpy.ndarray.size", "text": ["attribute", "Number of elements in the array.", "Equal to np.prod(a.shape), i.e., the product of the array\u2019s dimensions.", "a.size returns a standard arbitrary precision Python integer. This may not be the case with other methods of obtaining the same value (like the suggested np.prod(a.shape), which returns an instance of np.int_), and may be relevant if the value is used further in calculations that may overflow a fixed size integer type."]}, {"name": "ndarray.sort()", "path": "reference/generated/numpy.ndarray.sort", "type": "numpy.ndarray.sort", "text": ["method", "Sort an array in-place. Refer to numpy.sort for full documentation.", "Axis along which to sort. Default is -1, which means sort along the last axis.", "Sorting algorithm. The default is \u2018quicksort\u2019. Note that both \u2018stable\u2019 and \u2018mergesort\u2019 use timsort under the covers and, in general, the actual implementation will vary with datatype. The \u2018mergesort\u2019 option is retained for backwards compatibility.", "Changed in version 1.15.0: The \u2018stable\u2019 option was added.", "When a is an array with fields defined, this argument specifies which fields to compare first, second, etc. A single field can be specified as a string, and not all fields need be specified, but unspecified fields will still be used, in the order in which they come up in the dtype, to break ties.", "See also", "Return a sorted copy of an array.", "Indirect sort.", "Indirect stable sort on multiple keys.", "Find elements in sorted array.", "Partial sort.", "See numpy.sort for notes on the different sorting algorithms.", "Use the order keyword to specify a field to use when sorting a structured array:"]}, {"name": "ndarray.squeeze()", "path": "reference/generated/numpy.ndarray.squeeze", "type": "numpy.ndarray.squeeze", "text": ["method", "Remove axes of length one from a.", "Refer to numpy.squeeze for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.std()", "path": "reference/generated/numpy.ndarray.std", "type": "numpy.ndarray.std", "text": ["method", "Returns the standard deviation of the array elements along given axis.", "Refer to numpy.std for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.strides", "path": "reference/generated/numpy.ndarray.strides", "type": "numpy.ndarray.strides", "text": ["attribute", "Tuple of bytes to step in each dimension when traversing an array.", "The byte offset of element (i[0], i[1], ..., i[n]) in an array a is:", "A more detailed explanation of strides can be found in the \u201cndarray.rst\u201d file in the NumPy reference guide.", "See also", "Imagine an array of 32-bit integers (each 4 bytes):", "This array is stored in memory as 40 bytes, one after the other (known as a contiguous block of memory). The strides of an array tell us how many bytes we have to skip in memory to move to the next position along a certain axis. For example, we have to skip 4 bytes (1 value) to move to the next column, but 20 bytes (5 values) to get to the same position in the next row. As such, the strides for the array x will be (20, 4)."]}, {"name": "ndarray.sum()", "path": "reference/generated/numpy.ndarray.sum", "type": "numpy.ndarray.sum", "text": ["method", "Return the sum of the array elements over the given axis.", "Refer to numpy.sum for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.swapaxes()", "path": "reference/generated/numpy.ndarray.swapaxes", "type": "numpy.ndarray.swapaxes", "text": ["method", "Return a view of the array with axis1 and axis2 interchanged.", "Refer to numpy.swapaxes for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.T", "path": "reference/generated/numpy.ndarray.t", "type": "numpy.ndarray.T", "text": ["attribute", "The transposed array.", "Same as self.transpose().", "See also"]}, {"name": "ndarray.take()", "path": "reference/generated/numpy.ndarray.take", "type": "numpy.ndarray.take", "text": ["method", "Return an array formed from the elements of a at the given indices.", "Refer to numpy.take for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.tobytes()", "path": "reference/generated/numpy.ndarray.tobytes", "type": "numpy.ndarray.tobytes", "text": ["method", "Construct Python bytes containing the raw data bytes in the array.", "Constructs Python bytes showing a copy of the raw contents of data memory. The bytes object is produced in C-order by default. This behavior is controlled by the order parameter.", "New in version 1.9.0.", "Controls the memory layout of the bytes object. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 (short for Any) means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. Default is \u2018C\u2019.", "Python bytes exhibiting a copy of a\u2019s raw data."]}, {"name": "ndarray.tofile()", "path": "reference/generated/numpy.ndarray.tofile", "type": "numpy.ndarray.tofile", "text": ["method", "Write array to a file as text or binary (default).", "Data is always written in \u2018C\u2019 order, independent of the order of a. The data produced by this method can be recovered using the function fromfile().", "An open file object, or a string containing a filename.", "Changed in version 1.17.0: pathlib.Path objects are now accepted.", "Separator between array items for text output. If \u201c\u201d (empty), a binary file is written, equivalent to file.write(a.tobytes()).", "Format string for text file output. Each entry in the array is formatted to text by first converting it to the closest Python type, and then using \u201cformat\u201d % item.", "This is a convenience function for quick storage of array data. Information on endianness and precision is lost, so this method is not a good choice for files intended to archive data or transport data between machines with different endianness. Some of these problems can be overcome by outputting the data as text files, at the expense of speed and file size.", "When fid is a file object, array contents are directly written to the file, bypassing the file object\u2019s write method. As a result, tofile cannot be used with files objects supporting compression (e.g., GzipFile) or file-like objects that do not support fileno() (e.g., BytesIO)."]}, {"name": "ndarray.tolist()", "path": "reference/generated/numpy.ndarray.tolist", "type": "numpy.ndarray.tolist", "text": ["method", "Return the array as an a.ndim-levels deep nested list of Python scalars.", "Return a copy of the array data as a (nested) Python list. Data items are converted to the nearest compatible builtin Python type, via the item function.", "If a.ndim is 0, then since the depth of the nested list is 0, it will not be a list at all, but a simple Python scalar.", "The possibly nested list of array elements.", "The array may be recreated via a = np.array(a.tolist()), although this may sometimes lose precision.", "For a 1D array, a.tolist() is almost the same as list(a), except that tolist changes numpy scalars to Python scalars:", "Additionally, for a 2D array, tolist applies recursively:", "The base case for this recursion is a 0D array:"]}, {"name": "ndarray.tostring()", "path": "reference/generated/numpy.ndarray.tostring", "type": "numpy.ndarray.tostring", "text": ["method", "A compatibility alias for tobytes, with exactly the same behavior.", "Despite its name, it returns bytes not strs.", "Deprecated since version 1.19.0."]}, {"name": "ndarray.trace()", "path": "reference/generated/numpy.ndarray.trace", "type": "numpy.ndarray.trace", "text": ["method", "Return the sum along diagonals of the array.", "Refer to numpy.trace for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.transpose()", "path": "reference/generated/numpy.ndarray.transpose", "type": "numpy.ndarray.transpose", "text": ["method", "Returns a view of the array with axes transposed.", "For a 1-D array this has no effect, as a transposed vector is simply the same vector. To convert a 1-D array into a 2D column vector, an additional dimension must be added. np.atleast2d(a).T achieves this, as does a[:, np.newaxis]. For a 2-D array, this is a standard matrix transpose. For an n-D array, if axes are given, their order indicates how the axes are permuted (see Examples). If axes are not provided and a.shape = (i[0], i[1], ... i[n-2], i[n-1]), then a.transpose().shape = (i[n-1], i[n-2], ... i[1], i[0]).", "View of a, with axes suitably permuted.", "See also", "Equivalent function", "Array property returning the array transposed.", "Give a new shape to an array without changing its data."]}, {"name": "ndarray.var()", "path": "reference/generated/numpy.ndarray.var", "type": "numpy.ndarray.var", "text": ["method", "Returns the variance of the array elements, along given axis.", "Refer to numpy.var for full documentation.", "See also", "equivalent function"]}, {"name": "ndarray.view()", "path": "reference/generated/numpy.ndarray.view", "type": "numpy.ndarray.view", "text": ["method", "New view of array with the same data.", "Note", "Passing None for dtype is different from omitting the parameter, since the former invokes dtype(None) which is an alias for dtype('float_').", "Data-type descriptor of the returned view, e.g., float32 or int16. Omitting it results in the view having the same data-type as a. This argument can also be specified as an ndarray sub-class, which then specifies the type of the returned object (this is equivalent to setting the type parameter).", "Type of the returned view, e.g., ndarray or matrix. Again, omission of the parameter results in type preservation.", "a.view() is used two different ways:", "a.view(some_dtype) or a.view(dtype=some_dtype) constructs a view of the array\u2019s memory with a different data-type. This can cause a reinterpretation of the bytes of memory.", "a.view(ndarray_subclass) or a.view(type=ndarray_subclass) just returns an instance of ndarray_subclass that looks at the same array (same shape, dtype, etc.) This does not cause a reinterpretation of the memory.", "For a.view(some_dtype), if some_dtype has a different number of bytes per entry than the previous dtype (for example, converting a regular array to a structured array), then the behavior of the view cannot be predicted just from the superficial appearance of a (shown by print(a)). It also depends on exactly how a is stored in memory. Therefore if a is C-ordered versus fortran-ordered, versus defined as a slice or transpose, etc., the view may give different results.", "Viewing array data using a different type and dtype:", "Creating a view on a structured array so it can be used in calculations", "Making changes to the view changes the underlying array", "Using a view to convert an array to a recarray:", "Views share data:", "Views that change the dtype size (bytes per entry) should normally be avoided on arrays defined by slices, transposes, fortran-ordering, etc.:"]}, {"name": "ndindex.ndincr()", "path": "reference/generated/numpy.ndindex.ndincr", "type": "numpy.ndindex.ndincr", "text": ["method", "Increment the multi-dimensional index by one.", "This method is for backward compatibility only: do not use.", "Deprecated since version 1.20.0: This method has been advised against since numpy 1.8.0, but only started emitting DeprecationWarning as of this version."]}, {"name": "nditer.close()", "path": "reference/generated/numpy.nditer.close", "type": "numpy.nditer.close", "text": ["method", "Resolve all writeback semantics in writeable operands.", "New in version 1.15.0.", "See also"]}, {"name": "nditer.copy()", "path": "reference/generated/numpy.nditer.copy", "type": "numpy.nditer.copy", "text": ["method", "Get a copy of the iterator in its current state."]}, {"name": "nditer.debug_print()", "path": "reference/generated/numpy.nditer.debug_print", "type": "numpy.nditer.debug_print", "text": ["method", "Print the current state of the nditer instance and debug info to stdout."]}, {"name": "nditer.enable_external_loop()", "path": "reference/generated/numpy.nditer.enable_external_loop", "type": "numpy.nditer.enable_external_loop", "text": ["method", "When the \u201cexternal_loop\u201d was not used during construction, but is desired, this modifies the iterator to behave as if the flag was specified."]}, {"name": "nditer.index", "path": "reference/generated/numpy.nditer.index", "type": "Indexing routines", "text": ["attribute"]}, {"name": "nditer.iternext()", "path": "reference/generated/numpy.nditer.iternext", "type": "numpy.nditer.iternext", "text": ["method", "Check whether iterations are left, and perform a single internal iteration without returning the result. Used in the C-style pattern do-while pattern. For an example, see nditer.", "Whether or not there are iterations left."]}, {"name": "nditer.itersize", "path": "reference/generated/numpy.nditer.itersize", "type": "Indexing routines", "text": ["attribute"]}, {"name": "nditer.multi_index", "path": "reference/generated/numpy.nditer.multi_index", "type": "Indexing routines", "text": ["attribute"]}, {"name": "nditer.operands", "path": "reference/generated/numpy.nditer.operands", "type": "Indexing routines", "text": ["attribute", "operands[Slice]", "The array(s) to be iterated over. Valid only before the iterator is closed."]}, {"name": "nditer.remove_axis()", "path": "reference/generated/numpy.nditer.remove_axis", "type": "numpy.nditer.remove_axis", "text": ["method", "Removes axis i from the iterator. Requires that the flag \u201cmulti_index\u201d be enabled."]}, {"name": "nditer.remove_multi_index()", "path": "reference/generated/numpy.nditer.remove_multi_index", "type": "numpy.nditer.remove_multi_index", "text": ["method", "When the \u201cmulti_index\u201d flag was specified, this removes it, allowing the internal iteration structure to be optimized further."]}, {"name": "nditer.reset()", "path": "reference/generated/numpy.nditer.reset", "type": "numpy.nditer.reset", "text": ["method", "Reset the iterator to its initial state."]}, {"name": "nditer.value", "path": "reference/generated/numpy.nditer.value", "type": "Indexing routines", "text": ["attribute"]}, {"name": "ndpointer()", "path": "user/c-info.python-as-glue", "type": "User Guide", "text": ["Many people like to say that Python is a fantastic glue language. Hopefully, this Chapter will convince you that this is true. The first adopters of Python for science were typically people who used it to glue together large application codes running on super-computers. Not only was it much nicer to code in Python than in a shell script or Perl, in addition, the ability to easily extend Python made it relatively easy to create new classes and types specifically adapted to the problems being solved. From the interactions of these early contributors, Numeric emerged as an array-like object that could be used to pass data between these applications.", "As Numeric has matured and developed into NumPy, people have been able to write more code directly in NumPy. Often this code is fast-enough for production use, but there are still times that there is a need to access compiled code. Either to get that last bit of efficiency out of the algorithm or to make it easier to access widely-available codes written in C/C++ or Fortran.", "This chapter will review many of the tools that are available for the purpose of accessing code written in other compiled languages. There are many resources available for learning to call other compiled libraries from Python and the purpose of this Chapter is not to make you an expert. The main goal is to make you aware of some of the possibilities so that you will know what to \u201cGoogle\u201d in order to learn more.", "While Python is a great language and a pleasure to code in, its dynamic nature results in overhead that can cause some code ( i.e. raw computations inside of for loops) to be up 10-100 times slower than equivalent code written in a static compiled language. In addition, it can cause memory usage to be larger than necessary as temporary arrays are created and destroyed during computation. For many types of computing needs, the extra slow-down and memory consumption can often not be spared (at least for time- or memory- critical portions of your code). Therefore one of the most common needs is to call out from Python code to a fast, machine-code routine (e.g. compiled using C/C++ or Fortran). The fact that this is relatively easy to do is a big reason why Python is such an excellent high-level language for scientific and engineering programming.", "Their are two basic approaches to calling compiled code: writing an extension module that is then imported to Python using the import command, or calling a shared-library subroutine directly from Python using the ctypes module. Writing an extension module is the most common method.", "Warning", "Calling C-code from Python can result in Python crashes if you are not careful. None of the approaches in this chapter are immune. You have to know something about the way data is handled by both NumPy and by the third-party library being used.", "Extension modules were discussed in Writing an extension module. The most basic way to interface with compiled code is to write an extension module and construct a module method that calls the compiled code. For improved readability, your method should take advantage of the PyArg_ParseTuple call to convert between Python objects and C data-types. For standard C data-types there is probably already a built-in converter. For others you may need to write your own converter and use the \"O&\" format string which allows you to specify a function that will be used to perform the conversion from the Python object to whatever C-structures are needed.", "Once the conversions to the appropriate C-structures and C data-types have been performed, the next step in the wrapper is to call the underlying function. This is straightforward if the underlying function is in C or C++. However, in order to call Fortran code you must be familiar with how Fortran subroutines are called from C/C++ using your compiler and platform. This can vary somewhat platforms and compilers (which is another reason f2py makes life much simpler for interfacing Fortran code) but generally involves underscore mangling of the name and the fact that all variables are passed by reference (i.e. all arguments are pointers).", "The advantage of the hand-generated wrapper is that you have complete control over how the C-library gets used and called which can lead to a lean and tight interface with minimal over-head. The disadvantage is that you have to write, debug, and maintain C-code, although most of it can be adapted using the time-honored technique of \u201ccutting-pasting-and-modifying\u201d from other extension modules. Because, the procedure of calling out to additional C-code is fairly regimented, code-generation procedures have been developed to make this process easier. One of these code-generation techniques is distributed with NumPy and allows easy integration with Fortran and (simple) C code. This package, f2py, will be covered briefly in the next section.", "F2py allows you to automatically construct an extension module that interfaces to routines in Fortran 77/90/95 code. It has the ability to parse Fortran 77/90/95 code and automatically generate Python signatures for the subroutines it encounters, or you can guide how the subroutine interfaces with Python by constructing an interface-definition-file (or modifying the f2py-produced one).", "Probably the easiest way to introduce f2py is to offer a simple example. Here is one of the subroutines contained in a file named add.f", "This routine simply adds the elements in two contiguous arrays and places the result in a third. The memory for all three arrays must be provided by the calling routine. A very basic interface to this routine can be automatically generated by f2py:", "You should be able to run this command assuming your search-path is set-up properly. This command will produce an extension module named addmodule.c in the current directory. This extension module can now be compiled and used from Python just like any other extension module.", "You can also get f2py to both compile add.f along with the produced extension module leaving only a shared-library extension file that can be imported from Python:", "This command leaves a file named add.{ext} in the current directory (where {ext} is the appropriate extension for a Python extension module on your platform \u2014 so, pyd, etc. ). This module may then be imported from Python. It will contain a method for each subroutine in add (zadd, cadd, dadd, sadd). The docstring of each method contains information about how the module method may be called:", "The default interface is a very literal translation of the Fortran code into Python. The Fortran array arguments must now be NumPy arrays and the integer argument should be an integer. The interface will attempt to convert all arguments to their required types (and shapes) and issue an error if unsuccessful. However, because it knows nothing about the semantics of the arguments (such that C is an output and n should really match the array sizes), it is possible to abuse this function in ways that can cause Python to crash. For example:", "will cause a program crash on most systems. Under the covers, the lists are being converted to proper arrays but then the underlying add loop is told to cycle way beyond the borders of the allocated memory.", "In order to improve the interface, directives should be provided. This is accomplished by constructing an interface definition file. It is usually best to start from the interface file that f2py can produce (where it gets its default behavior from). To get f2py to generate the interface file use the -h option:", "This command leaves the file add.pyf in the current directory. The section of this file corresponding to zadd is:", "By placing intent directives and checking code, the interface can be cleaned up quite a bit until the Python module method is both easier to use and more robust.", "The intent directive, intent(out) is used to tell f2py that c is an output variable and should be created by the interface before being passed to the underlying code. The intent(hide) directive tells f2py to not allow the user to specify the variable, n, but instead to get it from the size of a. The depend( a ) directive is necessary to tell f2py that the value of n depends on the input a (so that it won\u2019t try to create the variable n until the variable a is created).", "After modifying add.pyf, the new Python module file can be generated by compiling both add.f and add.pyf:", "The new interface has docstring:", "Now, the function can be called in a much more robust way:", "Notice the automatic conversion to the correct format that occurred.", "The nice interface can also be generated automatically by placing the variable directives as special comments in the original Fortran code. Thus, if the source code is modified to contain:", "Then, one can compile the extension module using:", "The resulting signature for the function add.zadd is exactly the same one that was created previously. If the original source code had contained A(N) instead of A(*) and so forth with B and C, then nearly the same interface can be obtained by placing the INTENT(OUT) :: C comment line in the source code. The only difference is that N would be an optional input that would default to the length of A.", "For comparison with the other methods to be discussed. Here is another example of a function that filters a two-dimensional array of double precision floating-point numbers using a fixed averaging filter. The advantage of using Fortran to index into multi-dimensional arrays should be clear from this example.", "This code can be compiled and linked into an extension module named filter using:", "This will produce an extension module named filter.so in the current directory with a method named dfilter2d that returns a filtered version of the input.", "The f2py program is written in Python and can be run from inside your code to compile Fortran code at runtime, as follows:", "The source string can be any valid Fortran code. If you want to save the extension-module source code then a suitable file-name can be provided by the source_fn keyword to the compile function.", "If you want to distribute your f2py extension module, then you only need to include the .pyf file and the Fortran code. The distutils extensions in NumPy allow you to define an extension module entirely in terms of this interface file. A valid setup.py file allowing distribution of the add.f module (as part of the package f2py_examples so that it would be loaded as f2py_examples.add) is:", "Installation of the new package is easy using:", "assuming you have the proper permissions to write to the main site- packages directory for the version of Python you are using. For the resulting package to work, you need to create a file named __init__.py (in the same directory as add.pyf). Notice the extension module is defined entirely in terms of the add.pyf and add.f files. The conversion of the .pyf file to a .c file is handled by numpy.disutils.", "The interface definition file (.pyf) is how you can fine-tune the interface between Python and Fortran. There is decent documentation for f2py at F2PY user guide and reference manual. There is also more information on using f2py (including how to use it to wrap C codes) at the \u201cInterfacing With Other Languages\u201d heading of the SciPy Cookbook.", "The f2py method of linking compiled code is currently the most sophisticated and integrated approach. It allows clean separation of Python with compiled code while still allowing for separate distribution of the extension module. The only draw-back is that it requires the existence of a Fortran compiler in order for a user to install the code. However, with the existence of the free-compilers g77, gfortran, and g95, as well as high-quality commercial compilers, this restriction is not particularly onerous. In our opinion, Fortran is still the easiest way to write fast and clear code for scientific computing. It handles complex numbers, and multi-dimensional indexing in the most straightforward way. Be aware, however, that some Fortran compilers will not be able to optimize code as well as good hand- written C-code.", "Cython is a compiler for a Python dialect that adds (optional) static typing for speed, and allows mixing C or C++ code into your modules. It produces C or C++ extensions that can be compiled and imported in Python code.", "If you are writing an extension module that will include quite a bit of your own algorithmic code as well, then Cython is a good match. Among its features is the ability to easily and quickly work with multidimensional arrays.", "Notice that Cython is an extension-module generator only. Unlike f2py, it includes no automatic facility for compiling and linking the extension module (which must be done in the usual fashion). It does provide a modified distutils class called build_ext which lets you build an extension module from a .pyx source. Thus, you could write in a setup.py file:", "Adding the NumPy include directory is, of course, only necessary if you are using NumPy arrays in the extension module (which is what we assume you are using Cython for). The distutils extensions in NumPy also include support for automatically producing the extension-module and linking it from a .pyx file. It works so that if the user does not have Cython installed, then it looks for a file with the same file-name but a .c extension which it then uses instead of trying to produce the .c file again.", "If you just use Cython to compile a standard Python module, then you will get a C extension module that typically runs a bit faster than the equivalent Python module. Further speed increases can be gained by using the cdef keyword to statically define C variables.", "Let\u2019s look at two examples we\u2019ve seen before to see how they might be implemented using Cython. These examples were compiled into extension modules using Cython 0.21.1.", "Here is part of a Cython module named add.pyx which implements the complex addition functions we previously implemented using f2py:", "This module shows use of the cimport statement to load the definitions from the numpy.pxd header that ships with Cython. It looks like NumPy is imported twice; cimport only makes the NumPy C-API available, while the regular import causes a Python-style import at runtime and makes it possible to call into the familiar NumPy Python API.", "The example also demonstrates Cython\u2019s \u201ctyped memoryviews\u201d, which are like NumPy arrays at the C level, in the sense that they are shaped and strided arrays that know their own extent (unlike a C array addressed through a bare pointer). The syntax double complex[:] denotes a one-dimensional array (vector) of doubles, with arbitrary strides. A contiguous array of ints would be int[::1], while a matrix of floats would be float[:, :].", "Shown commented is the cython.boundscheck decorator, which turns bounds-checking for memory view accesses on or off on a per-function basis. We can use this to further speed up our code, at the expense of safety (or a manual check prior to entering the loop).", "Other than the view syntax, the function is immediately readable to a Python programmer. Static typing of the variable i is implicit. Instead of the view syntax, we could also have used Cython\u2019s special NumPy array syntax, but the view syntax is preferred.", "The two-dimensional example we created using Fortran is just as easy to write in Cython:", "This 2-d averaging filter runs quickly because the loop is in C and the pointer computations are done only as needed. If the code above is compiled as a module image, then a 2-d image, img, can be filtered using this code very quickly using:", "Regarding the code, two things are of note: firstly, it is impossible to return a memory view to Python. Instead, a NumPy array out is first created, and then a view b onto this array is used for the computation. Secondly, the view b is typed double[:, ::1]. This means 2-d array with contiguous rows, i.e., C matrix order. Specifying the order explicitly can speed up some algorithms since they can skip stride computations.", "Cython is the extension mechanism of choice for several scientific Python libraries, including Scipy, Pandas, SAGE, scikit-image and scikit-learn, as well as the XML processing library LXML. The language and compiler are well-maintained.", "There are several disadvantages of using Cython:", "One big advantage of Cython-generated extension modules is that they are easy to distribute. In summary, Cython is a very capable tool for either gluing C code or generating an extension module quickly and should not be over-looked. It is especially useful for people that can\u2019t or won\u2019t write C or Fortran code.", "Ctypes is a Python extension module, included in the stdlib, that allows you to call an arbitrary function in a shared library directly from Python. This approach allows you to interface with C-code directly from Python. This opens up an enormous number of libraries for use from Python. The drawback, however, is that coding mistakes can lead to ugly program crashes very easily (just as can happen in C) because there is little type or bounds checking done on the parameters. This is especially true when array data is passed in as a pointer to a raw memory location. The responsibility is then on you that the subroutine will not access memory outside the actual array area. But, if you don\u2019t mind living a little dangerously ctypes can be an effective tool for quickly taking advantage of a large shared library (or writing extended functionality in your own shared library).", "Because the ctypes approach exposes a raw interface to the compiled code it is not always tolerant of user mistakes. Robust use of the ctypes module typically involves an additional layer of Python code in order to check the data types and array bounds of objects passed to the underlying subroutine. This additional layer of checking (not to mention the conversion from ctypes objects to C-data-types that ctypes itself performs), will make the interface slower than a hand-written extension-module interface. However, this overhead should be negligible if the C-routine being called is doing any significant amount of work. If you are a great Python programmer with weak C skills, ctypes is an easy way to write a useful interface to a (shared) library of compiled code.", "To use ctypes you must", "There are several requirements for a shared library that can be used with ctypes that are platform specific. This guide assumes you have some familiarity with making a shared library on your system (or simply have a shared library available to you). Items to remember are:", "On some platforms (e.g. Windows), a shared library requires a .def file that specifies the functions to be exported. For example a mylib.def file might contain:", "Alternatively, you may be able to use the storage-class specifier __declspec(dllexport) in the C-definition of the function to avoid the need for this .def file.", "There is no standard way in Python distutils to create a standard shared library (an extension module is a \u201cspecial\u201d shared library Python understands) in a cross-platform manner. Thus, a big disadvantage of ctypes at the time of writing this book is that it is difficult to distribute in a cross-platform manner a Python extension that uses ctypes and includes your own code which should be compiled as a shared library on the users system.", "A simple, but robust way to load the shared library is to get the absolute path name and load it using the cdll object of ctypes:", "However, on Windows accessing an attribute of the cdll method will load the first DLL by that name found in the current directory or on the PATH. Loading the absolute path name requires a little finesse for cross-platform work since the extension of shared libraries varies. There is a ctypes.util.find_library utility available that can simplify the process of finding the library to load but it is not foolproof. Complicating matters, different platforms have different default extensions used by shared libraries (e.g. .dll \u2013 Windows, .so \u2013 Linux, .dylib \u2013 Mac OS X). This must also be taken into account if you are using ctypes to wrap code that needs to work on several platforms.", "NumPy provides a convenience function called ctypeslib.load_library (name, path). This function takes the name of the shared library (including any prefix like \u2018lib\u2019 but excluding the extension) and a path where the shared library can be located. It returns a ctypes library object or raises an OSError if the library cannot be found or raises an ImportError if the ctypes module is not available. (Windows users: the ctypes library object loaded using load_library is always loaded assuming cdecl calling convention. See the ctypes documentation under ctypes.windll and/or ctypes.oledll for ways to load libraries under other calling conventions).", "The functions in the shared library are available as attributes of the ctypes library object (returned from ctypeslib.load_library) or as items using lib['func_name'] syntax. The latter method for retrieving a function name is particularly useful if the function name contains characters that are not allowable in Python variable names.", "Python ints/longs, strings, and unicode objects are automatically converted as needed to equivalent ctypes arguments The None object is also converted automatically to a NULL pointer. All other Python objects must be converted to ctypes-specific types. There are two ways around this restriction that allow ctypes to integrate with other objects.", "NumPy uses both methods with a preference for the second method because it can be safer. The ctypes attribute of the ndarray returns an object that has an _as_parameter_ attribute which returns an integer representing the address of the ndarray to which it is associated. As a result, one can pass this ctypes attribute object directly to a function expecting a pointer to the data in your ndarray. The caller must be sure that the ndarray object is of the correct type, shape, and has the correct flags set or risk nasty crashes if the data-pointer to inappropriate arrays are passed in.", "To implement the second method, NumPy provides the class-factory function ndpointer in the numpy.ctypeslib module. This class-factory function produces an appropriate class that can be placed in an argtypes attribute entry of a ctypes function. The class will contain a from_param method which ctypes will use to convert any ndarray passed in to the function to a ctypes-recognized object. In the process, the conversion will perform checking on any properties of the ndarray that were specified by the user in the call to ndpointer. Aspects of the ndarray that can be checked include the data-type, the number-of-dimensions, the shape, and/or the state of the flags on any array passed. The return value of the from_param method is the ctypes attribute of the array which (because it contains the _as_parameter_ attribute pointing to the array data area) can be used by ctypes directly.", "The ctypes attribute of an ndarray is also endowed with additional attributes that may be convenient when passing additional information about the array into a ctypes function. The attributes data, shape, and strides can provide ctypes compatible types corresponding to the data-area, the shape, and the strides of the array. The data attribute returns a c_void_p representing a pointer to the data area. The shape and strides attributes each return an array of ctypes integers (or None representing a NULL pointer, if a 0-d array). The base ctype of the array is a ctype integer of the same size as a pointer on the platform. There are also methods data_as({ctype}), shape_as(<base ctype>), and strides_as(<base\nctype>). These return the data as a ctype object of your choice and the shape/strides arrays using an underlying base type of your choice. For convenience, the ctypeslib module also contains c_intp as a ctypes integer data-type whose size is the same as the size of c_void_p on the platform (its value is None if ctypes is not installed).", "The function is accessed as an attribute of or an item from the loaded shared-library. Thus, if ./mylib.so has a function named cool_function1, it may be accessed either as:", "In ctypes, the return-value of a function is set to be \u2018int\u2019 by default. This behavior can be changed by setting the restype attribute of the function. Use None for the restype if the function has no return value (\u2018void\u2019):", "As previously discussed, you can also set the argtypes attribute of the function in order to have ctypes check the types of the input arguments when the function is called. Use the ndpointer factory function to generate a ready-made class for data-type, shape, and flags checking on your new function. The ndpointer function has the signature", "Keyword arguments with the value None are not checked. Specifying a keyword enforces checking of that aspect of the ndarray on conversion to a ctypes-compatible object. The dtype keyword can be any object understood as a data-type object. The ndim keyword should be an integer, and the shape keyword should be an integer or a sequence of integers. The flags keyword specifies the minimal flags that are required on any array passed in. This can be specified as a string of comma separated requirements, an integer indicating the requirement bits OR\u2019d together, or a flags object returned from the flags attribute of an array with the necessary requirements.", "Using an ndpointer class in the argtypes method can make it significantly safer to call a C function using ctypes and the data- area of an ndarray. You may still want to wrap the function in an additional Python wrapper to make it user-friendly (hiding some obvious arguments and making some arguments output arguments). In this process, the requires function in NumPy may be useful to return the right kind of array from a given input.", "In this example, we will demonstrate how the addition function and the filter function implemented previously using the other approaches can be implemented using ctypes. First, the C code which implements the algorithms contains the functions zadd, dadd, sadd, cadd, and dfilter2d. The zadd function is:", "with similar code for cadd, dadd, and sadd that handles complex float, double, and float data-types, respectively:", "The code.c file also contains the function dfilter2d:", "A possible advantage this code has over the Fortran-equivalent code is that it takes arbitrarily strided (i.e. non-contiguous arrays) and may also run faster depending on the optimization capability of your compiler. But, it is an obviously more complicated than the simple code in filter.f. This code must be compiled into a shared library. On my Linux system this is accomplished using:", "Which creates a shared_library named code.so in the current directory. On Windows don\u2019t forget to either add __declspec(dllexport) in front of void on the line preceding each function definition, or write a code.def file that lists the names of the functions to be exported.", "A suitable Python interface to this shared library should be constructed. To do this create a file named interface.py with the following lines at the top:", "This code loads the shared library named code.{ext} located in the same path as this file. It then adds a return type of void to the functions contained in the library. It also adds argument checking to the functions in the library so that ndarrays can be passed as the first three arguments along with an integer (large enough to hold a pointer on the platform) as the fourth argument.", "Setting up the filtering function is similar and allows the filtering function to be called with ndarray arguments as the first two arguments and with pointers to integers (large enough to handle the strides and shape of an ndarray) as the last two arguments.:", "Next, define a simple selection function that chooses which addition function to call in the shared library based on the data-type:", "Finally, the two functions to be exported by the interface can be written simply as:", "and:", "Using ctypes is a powerful way to connect Python with arbitrary C-code. Its advantages for extending Python include", "clean separation of C code from Python code", "Its disadvantages include", "Because of the difficulty in distributing an extension module made using ctypes, f2py and Cython are still the easiest ways to extend Python for package creation. However, ctypes is in some cases a useful alternative. This should bring more features to ctypes that should eliminate the difficulty in extending Python and distributing the extension using ctypes.", "These tools have been found useful by others using Python and so are included here. They are discussed separately because they are either older ways to do things now handled by f2py, Cython, or ctypes (SWIG, PyFort) or because of a lack of reasonable documentation (SIP, Boost). Links to these methods are not included since the most relevant can be found using Google or some other search engine, and any links provided here would be quickly dated. Do not assume that inclusion in this list means that the package deserves attention. Information about these packages are collected here because many people have found them useful and we\u2019d like to give you as many options as possible for tackling the problem of easily integrating your code.", "Simplified Wrapper and Interface Generator (SWIG) is an old and fairly stable method for wrapping C/C++-libraries to a large variety of other languages. It does not specifically understand NumPy arrays but can be made usable with NumPy through the use of typemaps. There are some sample typemaps in the numpy/tools/swig directory under numpy.i together with an example module that makes use of them. SWIG excels at wrapping large C/C++ libraries because it can (almost) parse their headers and auto-produce an interface. Technically, you need to generate a .i file that defines the interface. Often, however, this .i file can be parts of the header itself. The interface usually needs a bit of tweaking to be very useful. This ability to parse C/C++ headers and auto-generate the interface still makes SWIG a useful approach to adding functionalilty from C/C++ into Python, despite the other methods that have emerged that are more targeted to Python. SWIG can actually target extensions for several languages, but the typemaps usually have to be language-specific. Nonetheless, with modifications to the Python-specific typemaps, SWIG can be used to interface a library with other languages such as Perl, Tcl, and Ruby.", "My experience with SWIG has been generally positive in that it is relatively easy to use and quite powerful. It has been used often before becoming more proficient at writing C-extensions. However, writing custom interfaces with SWIG is often troublesome because it must be done using the concept of typemaps which are not Python specific and are written in a C-like syntax. Therefore, other gluing strategies are preferred and SWIG would be probably considered only to wrap a very-large C/C++ library. Nonetheless, there are others who use SWIG quite happily.", "SIP is another tool for wrapping C/C++ libraries that is Python specific and appears to have very good support for C++. Riverbank Computing developed SIP in order to create Python bindings to the QT library. An interface file must be written to generate the binding, but the interface file looks a lot like a C/C++ header file. While SIP is not a full C++ parser, it understands quite a bit of C++ syntax as well as its own special directives that allow modification of how the Python binding is accomplished. It also allows the user to define mappings between Python types and C/C++ structures and classes.", "Boost is a repository of C++ libraries and Boost.Python is one of those libraries which provides a concise interface for binding C++ classes and functions to Python. The amazing part of the Boost.Python approach is that it works entirely in pure C++ without introducing a new syntax. Many users of C++ report that Boost.Python makes it possible to combine the best of both worlds in a seamless fashion. Using Boost to wrap simple C-subroutines is usually over-kill. Its primary purpose is to make C++ classes available in Python. So, if you have a set of C++ classes that need to be integrated cleanly into Python, consider learning about and using Boost.Python.", "PyFort is a nice tool for wrapping Fortran and Fortran-like C-code into Python with support for Numeric arrays. It was written by Paul Dubois, a distinguished computer scientist and the very first maintainer of Numeric (now retired). It is worth mentioning in the hopes that somebody will update PyFort to work with NumPy arrays as well which now support either Fortran or C-style contiguous arrays."]}, {"name": "NO_IMPORT_ARRAY", "path": "reference/c-api/array#c.NO_IMPORT_ARRAY", "type": "Array API", "text": ["Using these #defines you can use the C-API in multiple files for a single extension module. In each file you must define PY_ARRAY_UNIQUE_SYMBOL to some name that will hold the C-API (e.g. myextension_ARRAY_API). This must be done before including the numpy/arrayobject.h file. In the module initialization routine you call import_array. In addition, in the files that do not have the module initialization sub_routine define NO_IMPORT_ARRAY prior to including numpy/arrayobject.h.", "Suppose I have two files coolmodule.c and coolhelper.c which need to be compiled and linked into a single extension module. Suppose coolmodule.c contains the required initcool module initialization function (with the import_array() function called). Then, coolmodule.c would have at the top:", "On the other hand, coolhelper.c would contain at the top:", "You can also put the common two last lines into an extension-local header file as long as you make sure that NO_IMPORT_ARRAY is #defined before #including that file.", "Internally, these #defines work as follows:"]}, {"name": "NO_IMPORT_UFUNC", "path": "reference/c-api/ufunc#c.NO_IMPORT_UFUNC", "type": "UFunc API", "text": []}, {"name": "NPY_1_PI", "path": "reference/c-api/coremath#c.NPY_1_PI", "type": "NumPy core libraries", "text": ["Reciprocal of pi (\\(\\frac{1}{\\pi}\\))"]}, {"name": "NPY_2_PI", "path": "reference/c-api/coremath#c.NPY_2_PI", "type": "NumPy core libraries", "text": ["Two times the reciprocal of pi (\\(\\frac{2}{\\pi}\\))"]}, {"name": "NPY_ALLOW_C_API", "path": "reference/c-api/array#c.NPY_ALLOW_C_API", "type": "Array API", "text": ["Place before code that needs to call the Python C-API (when it is known that the GIL has already been released)."]}, {"name": "NPY_ARRAY_ALIGNED", "path": "reference/c-api/array#c.NPY_ARRAY_ALIGNED", "type": "Array API", "text": ["The data area and all array elements are aligned appropriately."]}, {"name": "NPY_ARRAY_ALIGNED", "path": "reference/c-api/array#c.PyArray_FromAny.NPY_ARRAY_ALIGNED", "type": "Array API", "text": ["Make sure the returned array is aligned on proper boundaries for its data type. An aligned array has the data pointer and every strides factor as a multiple of the alignment factor for the data-type- descriptor."]}, {"name": "NPY_ARRAY_BEHAVED", "path": "reference/c-api/array#c.PyArray_FromAny.NPY_ARRAY_BEHAVED", "type": "Array API", "text": ["NPY_ARRAY_ALIGNED | NPY_ARRAY_WRITEABLE"]}, {"name": "NPY_ARRAY_CARRAY", "path": "reference/c-api/array#c.PyArray_FromAny.NPY_ARRAY_CARRAY", "type": "Array API", "text": ["NPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_BEHAVED"]}, {"name": "NPY_ARRAY_CARRAY", "path": "reference/c-api/array#c.NPY_ARRAY_CARRAY", "type": "Array API", "text": ["NPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_BEHAVED"]}, {"name": "NPY_ARRAY_CARRAY_RO", "path": "reference/c-api/array#c.PyArray_FromAny.NPY_ARRAY_CARRAY_RO", "type": "Array API", "text": ["NPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_ALIGNED"]}, {"name": "NPY_ARRAY_CARRAY_RO", "path": "reference/c-api/array#c.NPY_ARRAY_CARRAY_RO", "type": "Array API", "text": ["NPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_ALIGNED"]}, {"name": "NPY_ARRAY_DEFAULT", "path": "reference/c-api/array#c.PyArray_FromAny.NPY_ARRAY_DEFAULT", "type": "Array API", "text": ["NPY_ARRAY_CARRAY"]}, {"name": "NPY_ARRAY_DEFAULT", "path": "reference/c-api/array#c.NPY_ARRAY_DEFAULT", "type": "Array API", "text": ["NPY_ARRAY_CARRAY"]}, {"name": "NPY_ARRAY_ELEMENTSTRIDES", "path": "reference/c-api/array#c.NPY_ARRAY_ELEMENTSTRIDES", "type": "Array API", "text": ["Make sure the returned array has strides that are multiples of the element size."]}, {"name": "NPY_ARRAY_ENSUREARRAY", "path": "reference/c-api/array#c.PyArray_FromAny.NPY_ARRAY_ENSUREARRAY", "type": "Array API", "text": ["Make sure the result is a base-class ndarray. By default, if op is an instance of a subclass of ndarray, an instance of that same subclass is returned. If this flag is set, an ndarray object will be returned instead."]}, {"name": "NPY_ARRAY_ENSUREARRAY", "path": "reference/c-api/array#c.NPY_ARRAY_ENSUREARRAY", "type": "Array API", "text": ["Make sure the resulting object is an actual ndarray, and not a sub-class."]}, {"name": "NPY_ARRAY_ENSURECOPY", "path": "reference/c-api/array#c.NPY_ARRAY_ENSURECOPY", "type": "Array API", "text": ["Make sure the resulting array is a copy of the original."]}, {"name": "NPY_ARRAY_ENSURECOPY", "path": "reference/c-api/array#c.PyArray_FromAny.NPY_ARRAY_ENSURECOPY", "type": "Array API", "text": ["Make sure a copy is made of op. If this flag is not present, data is not copied if it can be avoided."]}, {"name": "NPY_ARRAY_F_CONTIGUOUS", "path": "reference/c-api/array#c.PyArray_FromAny.NPY_ARRAY_F_CONTIGUOUS", "type": "Array API", "text": ["Make sure the returned array is Fortran-style contiguous."]}, {"name": "NPY_ARRAY_F_CONTIGUOUS", "path": "reference/c-api/array#c.NPY_ARRAY_F_CONTIGUOUS", "type": "Array API", "text": ["The data area is in Fortran-style contiguous order (first index varies the fastest)."]}, {"name": "NPY_ARRAY_FARRAY", "path": "reference/c-api/array#c.PyArray_FromAny.NPY_ARRAY_FARRAY", "type": "Array API", "text": ["NPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_BEHAVED"]}, {"name": "NPY_ARRAY_FARRAY", "path": "reference/c-api/array#c.NPY_ARRAY_FARRAY", "type": "Array API", "text": ["NPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_BEHAVED"]}, {"name": "NPY_ARRAY_FARRAY_RO", "path": "reference/c-api/array#c.PyArray_FromAny.NPY_ARRAY_FARRAY_RO", "type": "Array API", "text": ["NPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_ALIGNED"]}, {"name": "NPY_ARRAY_FARRAY_RO", "path": "reference/c-api/array#c.NPY_ARRAY_FARRAY_RO", "type": "Array API", "text": ["NPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_ALIGNED"]}, {"name": "NPY_ARRAY_FORCECAST", "path": "reference/c-api/array#c.PyArray_FromAny.NPY_ARRAY_FORCECAST", "type": "Array API", "text": ["Force a cast to the output type even if it cannot be done safely. Without this flag, a data cast will occur only if it can be done safely, otherwise an error is raised."]}, {"name": "NPY_ARRAY_IN_ARRAY", "path": "reference/c-api/array#c.NPY_ARRAY_IN_ARRAY", "type": "Array API", "text": ["NPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_ALIGNED", "NPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_ALIGNED"]}, {"name": "NPY_ARRAY_INOUT_ARRAY", "path": "reference/c-api/array#c.NPY_ARRAY_INOUT_ARRAY", "type": "Array API", "text": ["NPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_WRITEABLE | NPY_ARRAY_ALIGNED | NPY_ARRAY_WRITEBACKIFCOPY | NPY_ARRAY_UPDATEIFCOPY", "NPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_WRITEABLE | NPY_ARRAY_ALIGNED | NPY_ARRAY_WRITEBACKIFCOPY | NPY_ARRAY_UPDATEIFCOPY"]}, {"name": "NPY_ARRAY_NOTSWAPPED", "path": "reference/c-api/array#c.NPY_ARRAY_NOTSWAPPED", "type": "Array API", "text": ["Make sure the returned array has a data-type descriptor that is in machine byte-order, over-riding any specification in the dtype argument. Normally, the byte-order requirement is determined by the dtype argument. If this flag is set and the dtype argument does not indicate a machine byte-order descriptor (or is NULL and the object is already an array with a data-type descriptor that is not in machine byte- order), then a new data-type descriptor is created and used with its byte-order field set to native.", "NPY_ARRAY_ALIGNED | NPY_ARRAY_WRITEABLE | NPY_ARRAY_NOTSWAPPED"]}, {"name": "NPY_ARRAY_OUT_ARRAY", "path": "reference/c-api/array#c.NPY_ARRAY_OUT_ARRAY", "type": "Array API", "text": ["NPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_ALIGNED | NPY_ARRAY_WRITEABLE", "NPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_WRITEABLE | NPY_ARRAY_ALIGNED"]}, {"name": "NPY_ARRAY_OWNDATA", "path": "reference/c-api/array#c.NPY_ARRAY_OWNDATA", "type": "Array API", "text": ["The data area is owned by this array. Should never be set manually, instead create a PyObject wrapping the data and set the array\u2019s base to that object. For an example, see the test in test_mem_policy."]}, {"name": "NPY_ARRAY_UPDATE_ALL", "path": "reference/c-api/array#c.NPY_ARRAY_UPDATE_ALL", "type": "Array API", "text": ["NPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_F_CONTIGUOUS | NPY_ARRAY_ALIGNED"]}, {"name": "NPY_ARRAY_UPDATEIFCOPY", "path": "reference/c-api/array#c.NPY_ARRAY_UPDATEIFCOPY", "type": "Array API", "text": ["A deprecated version of NPY_ARRAY_WRITEBACKIFCOPY which depends upon dealloc to trigger the writeback. For backwards compatibility, PyArray_ResolveWritebackIfCopy is called at dealloc but relying on that behavior is deprecated and not supported in PyPy."]}, {"name": "NPY_ARRAY_UPDATEIFCOPY", "path": "reference/c-api/array#c.PyArray_FromAny.NPY_ARRAY_UPDATEIFCOPY", "type": "Array API", "text": ["Deprecated. Use NPY_ARRAY_WRITEBACKIFCOPY, which is similar. This flag \u201cautomatically\u201d copies the data back when the returned array is deallocated, which is not supported in all python implementations."]}, {"name": "NPY_ARRAY_WRITEABLE", "path": "reference/c-api/array#c.PyArray_FromAny.NPY_ARRAY_WRITEABLE", "type": "Array API", "text": ["Make sure the returned array can be written to."]}, {"name": "NPY_ARRAY_WRITEABLE", "path": "reference/c-api/array#c.NPY_ARRAY_WRITEABLE", "type": "Array API", "text": ["The data area can be written to.", "Notice that the above 3 flags are defined so that a new, well- behaved array has these flags defined as true."]}, {"name": "NPY_ARRAY_WRITEBACKIFCOPY", "path": "reference/c-api/array#c.NPY_ARRAY_WRITEBACKIFCOPY", "type": "Array API", "text": ["The data area represents a (well-behaved) copy whose information should be transferred back to the original when PyArray_ResolveWritebackIfCopy is called.", "This is a special flag that is set if this array represents a copy made because a user required certain flags in PyArray_FromAny and a copy had to be made of some other array (and the user asked for this flag to be set in such a situation). The base attribute then points to the \u201cmisbehaved\u201d array (which is set read_only). :c:func`PyArray_ResolveWritebackIfCopy` will copy its contents back to the \u201cmisbehaved\u201d array (casting if necessary) and will reset the \u201cmisbehaved\u201d array to NPY_ARRAY_WRITEABLE. If the \u201cmisbehaved\u201d array was not NPY_ARRAY_WRITEABLE to begin with then PyArray_FromAny would have returned an error because NPY_ARRAY_WRITEBACKIFCOPY would not have been possible."]}, {"name": "NPY_ARRAY_WRITEBACKIFCOPY", "path": "reference/c-api/array#c.PyArray_FromAny.NPY_ARRAY_WRITEBACKIFCOPY", "type": "Array API", "text": ["If op is already an array, but does not satisfy the requirements, then a copy is made (which will satisfy the requirements). If this flag is present and a copy (of an object that is already an array) must be made, then the corresponding NPY_ARRAY_WRITEBACKIFCOPY flag is set in the returned copy and op is made to be read-only. You must be sure to call PyArray_ResolveWritebackIfCopy to copy the contents back into op and the op array will be made writeable again. If op is not writeable to begin with, or if it is not already an array, then an error is raised."]}, {"name": "NPY_BEGIN_THREADS", "path": "reference/c-api/array#c.NPY_BEGIN_THREADS", "type": "Array API", "text": ["Place right before code that does not need the Python interpreter (no Python C-API calls). This macro saves the Python state and releases the GIL."]}, {"name": "NPY_BEGIN_THREADS_DEF", "path": "reference/c-api/array#c.NPY_BEGIN_THREADS_DEF", "type": "Array API", "text": ["Place in the variable declaration area. This macro sets up the variable needed for storing the Python state."]}, {"name": "NPY_BIG", "path": "reference/c-api/array#c.NPY_BIG", "type": "Array API", "text": ["If a byteorder of NPY_IGNORE is encountered it is left alone. If newendian is NPY_SWAP, then all byte-orders are swapped. Other valid newendian values are NPY_NATIVE, NPY_LITTLE, and NPY_BIG which all cause the returned data-typed descriptor (and all it\u2019s referenced data-type descriptors) to have the corresponding byte- order."]}, {"name": "NPY_BIG_ENDIAN", "path": "reference/c-api/config#c.NPY_BIG_ENDIAN", "type": "System configuration", "text": []}, {"name": "npy_bool contiguous", "path": "reference/c-api/types-and-structures#c.PyArrayIterObject.contiguous", "type": "Python Types and C-Structures", "text": ["This flag is true if the underlying array is NPY_ARRAY_C_CONTIGUOUS. It is used to simplify calculations when possible."]}, {"name": "npy_bool nonzero()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.nonzero", "type": "Python Types and C-Structures", "text": ["A pointer to a function that returns TRUE if the item of arr pointed to by data is nonzero. This function can deal with misbehaved arrays."]}, {"name": "npy_bool NpyIter_HasDelayedBufAlloc()", "path": "reference/c-api/iterator#c.NpyIter_HasDelayedBufAlloc", "type": "Array Iterator API", "text": ["Returns 1 if the flag NPY_ITER_DELAY_BUFALLOC was passed to the iterator constructor, and no call to one of the Reset functions has been done yet, 0 otherwise."]}, {"name": "npy_bool NpyIter_HasExternalLoop()", "path": "reference/c-api/iterator#c.NpyIter_HasExternalLoop", "type": "Array Iterator API", "text": ["Returns 1 if the caller needs to handle the inner-most 1-dimensional loop, or 0 if the iterator handles all looping. This is controlled by the constructor flag NPY_ITER_EXTERNAL_LOOP or NpyIter_EnableExternalLoop."]}, {"name": "npy_bool NpyIter_HasIndex()", "path": "reference/c-api/iterator#c.NpyIter_HasIndex", "type": "Array Iterator API", "text": ["Returns 1 if the iterator was created with the NPY_ITER_C_INDEX or NPY_ITER_F_INDEX flag, 0 otherwise."]}, {"name": "npy_bool NpyIter_HasMultiIndex()", "path": "reference/c-api/iterator#c.NpyIter_HasMultiIndex", "type": "Array Iterator API", "text": ["Returns 1 if the iterator was created with the NPY_ITER_MULTI_INDEX flag, 0 otherwise."]}, {"name": "npy_bool NpyIter_IsBuffered()", "path": "reference/c-api/iterator#c.NpyIter_IsBuffered", "type": "Array Iterator API", "text": ["Returns 1 if the iterator was created with the NPY_ITER_BUFFERED flag, 0 otherwise."]}, {"name": "npy_bool NpyIter_IsFirstVisit()", "path": "reference/c-api/iterator#c.NpyIter_IsFirstVisit", "type": "Array Iterator API", "text": ["New in version 1.7.", "Checks to see whether this is the first time the elements of the specified reduction operand which the iterator points at are being seen for the first time. The function returns a reasonable answer for reduction operands and when buffering is disabled. The answer may be incorrect for buffered non-reduction operands.", "This function is intended to be used in EXTERNAL_LOOP mode only, and will produce some wrong answers when that mode is not enabled.", "If this function returns true, the caller should also check the inner loop stride of the operand, because if that stride is 0, then only the first element of the innermost external loop is being visited for the first time.", "WARNING: For performance reasons, \u2018iop\u2019 is not bounds-checked, it is not confirmed that \u2018iop\u2019 is actually a reduction operand, and it is not confirmed that EXTERNAL_LOOP mode is enabled. These checks are the responsibility of the caller, and should be done outside of any inner loops."]}, {"name": "npy_bool NpyIter_IsGrowInner()", "path": "reference/c-api/iterator#c.NpyIter_IsGrowInner", "type": "Array Iterator API", "text": ["Returns 1 if the iterator was created with the NPY_ITER_GROWINNER flag, 0 otherwise."]}, {"name": "npy_bool NpyIter_RequiresBuffering()", "path": "reference/c-api/iterator#c.NpyIter_RequiresBuffering", "type": "Array Iterator API", "text": ["Returns 1 if the iterator requires buffering, which occurs when an operand needs conversion or alignment and so cannot be used directly."]}, {"name": "npy_bool PyArray_EquivArrTypes()", "path": "reference/c-api/array#c.PyArray_EquivArrTypes", "type": "Array API", "text": ["Return NPY_TRUE if a1 and a2 are arrays with equivalent types for this platform."]}, {"name": "npy_bool PyArray_EquivTypenums()", "path": "reference/c-api/array#c.PyArray_EquivTypenums", "type": "Array API", "text": ["Special case of PyArray_EquivTypes (\u2026) that does not accept flexible data types but may be easier to call."]}, {"name": "npy_bool PyArray_EquivTypes()", "path": "reference/c-api/array#c.PyArray_EquivTypes", "type": "Array API", "text": ["Return NPY_TRUE if type1 and type2 actually represent equivalent types for this platform (the fortran member of each type is ignored). For example, on 32-bit platforms, NPY_LONG and NPY_INT are equivalent. Otherwise return NPY_FALSE."]}, {"name": "NPY_BYTE_ORDER", "path": "reference/c-api/config#c.NPY_BYTE_ORDER", "type": "System configuration", "text": ["New in version 1.3.0.", "Portable alternatives to the endian.h macros of GNU Libc. If big endian, NPY_BYTE_ORDER == NPY_BIG_ENDIAN, and similarly for little endian architectures.", "Defined in numpy/npy_endian.h."]}, {"name": "NPY_CLIP", "path": "reference/c-api/array#c.PyArray_Choose.NPY_CLIP", "type": "Array API", "text": ["all values are clipped to the region [0, len(op) )."]}, {"name": "npy_copysign()", "path": "reference/c-api/coremath#c.npy_copysign", "type": "NumPy core libraries", "text": ["This is a function equivalent to C99 copysign: return x with the same sign as y. Works for any value, including inf and nan. Single and extended precisions are available with suffix f and l.", "New in version 1.4.0."]}, {"name": "NPY_CPU_AMD64", "path": "reference/c-api/config#c.NPY_CPU_AMD64", "type": "System configuration", "text": []}, {"name": "NPY_CPU_IA64", "path": "reference/c-api/config#c.NPY_CPU_IA64", "type": "System configuration", "text": []}, {"name": "NPY_CPU_LITTLE", "path": "reference/c-api/config#c.PyArray_GetEndianness.NPY_CPU_LITTLE", "type": "System configuration", "text": []}, {"name": "NPY_CPU_PARISC", "path": "reference/c-api/config#c.NPY_CPU_PARISC", "type": "System configuration", "text": ["New in version 1.3.0.", "CPU architecture of the platform; only one of the above is defined.", "Defined in numpy/npy_cpu.h"]}, {"name": "NPY_CPU_PPC", "path": "reference/c-api/config#c.NPY_CPU_PPC", "type": "System configuration", "text": []}, {"name": "NPY_CPU_PPC64", "path": "reference/c-api/config#c.NPY_CPU_PPC64", "type": "System configuration", "text": []}, {"name": "NPY_CPU_S390", "path": "reference/c-api/config#c.NPY_CPU_S390", "type": "System configuration", "text": []}, {"name": "NPY_CPU_SPARC", "path": "reference/c-api/config#c.NPY_CPU_SPARC", "type": "System configuration", "text": []}, {"name": "NPY_CPU_SPARC64", "path": "reference/c-api/config#c.NPY_CPU_SPARC64", "type": "System configuration", "text": []}, {"name": "NPY_CPU_UNKNOWN_ENDIAN", "path": "reference/c-api/config#c.PyArray_GetEndianness.NPY_CPU_UNKNOWN_ENDIAN", "type": "System configuration", "text": []}, {"name": "NPY_DISABLE_C_API", "path": "reference/c-api/array#c.NPY_DISABLE_C_API", "type": "Array API", "text": ["Place after code that needs to call the Python C-API (to re-release the GIL)."]}, {"name": "NPY_END_ALLOW_THREADS", "path": "reference/c-api/array#c.NPY_END_ALLOW_THREADS", "type": "Array API", "text": ["Equivalent to Py_END_ALLOW_THREADS except it uses NPY_ALLOW_THREADS to determine if the macro if replaced with white-space or not."]}, {"name": "NPY_END_THREADS", "path": "reference/c-api/array#c.NPY_END_THREADS", "type": "Array API", "text": ["Place right after code that does not need the Python interpreter. This macro acquires the GIL and restores the Python state from the saved variable."]}, {"name": "NPY_EULER", "path": "reference/c-api/coremath#c.NPY_EULER", "type": "NumPy core libraries", "text": ["\\(\\lim_{n\\rightarrow\\infty}({\\sum_{k=1}^n{\\frac{1}{k}}-\\ln n})\\)"]}, {"name": "NPY_FAIL", "path": "reference/c-api/array#c.NPY_FAIL", "type": "Array API", "text": ["The return value of failed converter functions which are called using the \u201cO&\u201d syntax in PyArg_ParseTuple-like functions."]}, {"name": "NPY_FALSE", "path": "reference/c-api/array#c.NPY_FALSE", "type": "Array API", "text": ["Defined as 0 for use with Bool."]}, {"name": "NPY_FEATURE_VERSION", "path": "reference/c-api/array#c.NPY_FEATURE_VERSION", "type": "Array API", "text": ["The current version of the C-API."]}, {"name": "npy_half npy_double_to_half()", "path": "reference/c-api/coremath#c.npy_double_to_half", "type": "NumPy core libraries", "text": ["Converts a double-precision float to a half-precision float. The value is rounded to the nearest representable half, with ties going to the nearest even. If the value is too small or too big, the system\u2019s floating point underflow or overflow bit will be set."]}, {"name": "npy_half npy_float_to_half()", "path": "reference/c-api/coremath#c.npy_float_to_half", "type": "NumPy core libraries", "text": ["Converts a single-precision float to a half-precision float. The value is rounded to the nearest representable half, with ties going to the nearest even. If the value is too small or too big, the system\u2019s floating point underflow or overflow bit will be set."]}, {"name": "npy_half npy_half_copysign()", "path": "reference/c-api/coremath#c.npy_half_copysign", "type": "NumPy core libraries", "text": ["Returns the value of x with the sign bit copied from y. Works for any value, including Inf and NaN."]}, {"name": "npy_half npy_half_nextafter()", "path": "reference/c-api/coremath#c.npy_half_nextafter", "type": "NumPy core libraries", "text": ["This is the same for half-precision float as npy_nextafter and npy_nextafterf described in the low-level floating point section."]}, {"name": "npy_half npy_half_spacing()", "path": "reference/c-api/coremath#c.npy_half_spacing", "type": "NumPy core libraries", "text": ["This is the same for half-precision float as npy_spacing and npy_spacingf described in the low-level floating point section."]}, {"name": "NPY_HALF_NAN", "path": "reference/c-api/coremath#c.NPY_HALF_NAN", "type": "NumPy core libraries", "text": ["This macro is defined to a NaN value, guaranteed to have its sign bit unset."]}, {"name": "NPY_HALF_NEGONE", "path": "reference/c-api/coremath#c.NPY_HALF_NEGONE", "type": "NumPy core libraries", "text": ["This macro is defined to -1.0."]}, {"name": "NPY_HALF_NINF", "path": "reference/c-api/coremath#c.NPY_HALF_NINF", "type": "NumPy core libraries", "text": ["This macro is defined to -inf."]}, {"name": "NPY_HALF_NZERO", "path": "reference/c-api/coremath#c.NPY_HALF_NZERO", "type": "NumPy core libraries", "text": ["This macro is defined to negative zero."]}, {"name": "NPY_HALF_ONE", "path": "reference/c-api/coremath#c.NPY_HALF_ONE", "type": "NumPy core libraries", "text": ["This macro is defined to 1.0."]}, {"name": "NPY_HALF_PINF", "path": "reference/c-api/coremath#c.NPY_HALF_PINF", "type": "NumPy core libraries", "text": ["This macro is defined to +inf."]}, {"name": "NPY_HALF_PZERO", "path": "reference/c-api/coremath#c.NPY_HALF_PZERO", "type": "NumPy core libraries", "text": ["This macro is defined to positive zero."]}, {"name": "npy_hash_t *hash", "path": "reference/c-api/types-and-structures#c.NPY_USE_SETITEM.hash", "type": "Python Types and C-Structures", "text": ["Currently unused. Reserved for future use in caching hash values."]}, {"name": "NPY_IGNORE", "path": "reference/c-api/array#c.NPY_IGNORE", "type": "Array API", "text": ["If a byteorder of NPY_IGNORE is encountered it is left alone. If newendian is NPY_SWAP, then all byte-orders are swapped. Other valid newendian values are NPY_NATIVE, NPY_LITTLE, and NPY_BIG which all cause the returned data-typed descriptor (and all it\u2019s referenced data-type descriptors) to have the corresponding byte- order."]}, {"name": "NPY_INFINITY", "path": "reference/c-api/coremath#c.NPY_INFINITY", "type": "NumPy core libraries", "text": ["This macro is defined to a positive inf. The corresponding single and extension precision macro are available with the suffix F and L."]}, {"name": "npy_int32 random_positive_int32()", "path": "reference/random/c-api#c.random_positive_int32", "type": "C API for random", "text": []}, {"name": "npy_int64 random_binomial()", "path": "reference/random/c-api#c.random_binomial", "type": "C API for random", "text": []}, {"name": "npy_int64 random_geometric()", "path": "reference/random/c-api#c.random_geometric", "type": "C API for random", "text": []}, {"name": "npy_int64 random_geometric_inversion()", "path": "reference/random/c-api#c.random_geometric_inversion", "type": "C API for random", "text": []}, {"name": "npy_int64 random_geometric_search()", "path": "reference/random/c-api#c.random_geometric_search", "type": "C API for random", "text": []}, {"name": "npy_int64 random_hypergeometric()", "path": "reference/random/c-api#c.random_hypergeometric", "type": "C API for random", "text": []}, {"name": "npy_int64 random_logseries()", "path": "reference/random/c-api#c.random_logseries", "type": "C API for random", "text": []}, {"name": "npy_int64 random_negative_binomial()", "path": "reference/random/c-api#c.random_negative_binomial", "type": "C API for random", "text": []}, {"name": "npy_int64 random_poisson()", "path": "reference/random/c-api#c.random_poisson", "type": "C API for random", "text": []}, {"name": "npy_int64 random_positive_int()", "path": "reference/random/c-api#c.random_positive_int", "type": "C API for random", "text": []}, {"name": "npy_int64 random_positive_int64()", "path": "reference/random/c-api#c.random_positive_int64", "type": "C API for random", "text": []}, {"name": "npy_int64 random_zipf()", "path": "reference/random/c-api#c.random_zipf", "type": "C API for random", "text": []}, {"name": "npy_intp *backstrides", "path": "reference/c-api/types-and-structures#c.PyArrayIterObject.backstrides", "type": "Python Types and C-Structures", "text": ["How many bytes needed to jump from the end of a dimension back to its beginning. Note that backstrides[k] == strides[k] *\ndims_m1[k], but it is stored here as an optimization."]}, {"name": "npy_intp *coordinates", "path": "reference/c-api/types-and-structures#c.PyArrayIterObject.coordinates", "type": "Python Types and C-Structures", "text": ["An \\(N\\) -dimensional index into the array."]}, {"name": "npy_intp *core_dim_sizes", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.core_dim_sizes", "type": "Python Types and C-Structures", "text": ["For each distinct core dimension, the possible frozen size if UFUNC_CORE_DIM_SIZE_INFERRED is 0"]}, {"name": "npy_intp *dimensions", "path": "reference/c-api/types-and-structures#c.PyArrayMultiIterObject.dimensions", "type": "Python Types and C-Structures", "text": ["The shape of the broadcasted result (only nd slots are used)."]}, {"name": "npy_intp *dims_m1", "path": "reference/c-api/types-and-structures#c.PyArrayIterObject.dims_m1", "type": "Python Types and C-Structures", "text": ["The size of the array minus 1 in each dimension."]}, {"name": "npy_intp *factors", "path": "reference/c-api/types-and-structures#c.PyArrayIterObject.factors", "type": "Python Types and C-Structures", "text": ["This array is used in computing an N-d index from a 1-d index. It contains needed products of the dimensions."]}, {"name": "npy_intp *NpyIter_GetAxisStrideArray()", "path": "reference/c-api/iterator#c.NpyIter_GetAxisStrideArray", "type": "Array Iterator API", "text": ["Gets the array of strides for the specified axis. Requires that the iterator be tracking a multi-index, and that buffering not be enabled.", "This may be used when you want to match up operand axes in some fashion, then remove them with NpyIter_RemoveAxis to handle their processing manually. By calling this function before removing the axes, you can get the strides for the manual processing.", "Returns NULL on error."]}, {"name": "npy_intp *NpyIter_GetIndexPtr()", "path": "reference/c-api/iterator#c.NpyIter_GetIndexPtr", "type": "Array Iterator API", "text": ["This gives back a pointer to the index being tracked, or NULL if no index is being tracked. It is only usable if one of the flags NPY_ITER_C_INDEX or NPY_ITER_F_INDEX were specified during construction."]}, {"name": "npy_intp *NpyIter_GetInnerLoopSizePtr()", "path": "reference/c-api/iterator#c.NpyIter_GetInnerLoopSizePtr", "type": "Array Iterator API", "text": ["Returns a pointer to the number of iterations the inner loop should execute.", "This address may be cached before the iteration loop, calling iternext will not change it. The value itself may change during iteration, in particular if buffering is enabled. This function may be safely called without holding the Python GIL."]}, {"name": "npy_intp *NpyIter_GetInnerStrideArray()", "path": "reference/c-api/iterator#c.NpyIter_GetInnerStrideArray", "type": "Array Iterator API", "text": ["Returns a pointer to an array of the nop strides, one for each iterated object, to be used by the inner loop.", "This pointer may be cached before the iteration loop, calling iternext will not change it. This function may be safely called without holding the Python GIL.", "WARNING: While the pointer may be cached, its values may change if the iterator is buffered."]}, {"name": "npy_intp *PyArray_DIMS()", "path": "reference/c-api/array#c.PyArray_DIMS", "type": "Array API", "text": ["Returns a pointer to the dimensions/shape of the array. The number of elements matches the number of dimensions of the array. Can return NULL for 0-dimensional arrays."]}, {"name": "npy_intp *PyArray_SHAPE()", "path": "reference/c-api/array#c.PyArray_SHAPE", "type": "Array API", "text": ["New in version 1.7.", "A synonym for PyArray_DIMS, named to be consistent with the shape usage within Python."]}, {"name": "npy_intp *PyArray_STRIDES()", "path": "reference/c-api/array#c.PyArray_STRIDES", "type": "Array API", "text": ["Returns a pointer to the strides of the array. The number of elements matches the number of dimensions of the array."]}, {"name": "npy_intp *PyDimMem_NEW()", "path": "reference/c-api/array#c.PyDimMem_NEW", "type": "Array API", "text": []}, {"name": "npy_intp *PyDimMem_RENEW()", "path": "reference/c-api/array#c.PyDimMem_RENEW", "type": "Array API", "text": ["Macros to allocate, free, and reallocate dimension and strides memory."]}, {"name": "npy_intp *shape", "path": "reference/c-api/types-and-structures#c.PyArrayInterface.shape", "type": "Python Types and C-Structures", "text": ["An array containing the size of the array in each dimension."]}, {"name": "npy_intp *strides", "path": "reference/c-api/types-and-structures#c.NPY_AO.strides", "type": "Python Types and C-Structures", "text": ["An array of integers providing for each dimension the number of bytes that must be skipped to get to the next element in that dimension. Associated with macro PyArray_STRIDES."]}, {"name": "npy_intp *strides", "path": "reference/c-api/types-and-structures#c.PyArrayIterObject.strides", "type": "Python Types and C-Structures", "text": ["The strides of the array. How many bytes needed to jump to the next element in each dimension."]}, {"name": "npy_intp *strides", "path": "reference/c-api/types-and-structures#c.PyArrayInterface.strides", "type": "Python Types and C-Structures", "text": ["An array containing the number of bytes to jump to get to the next element in each dimension."]}, {"name": "npy_intp dimensions", "path": "reference/c-api/types-and-structures#c.NPY_AO.dimensions", "type": "Python Types and C-Structures", "text": ["An array of integers providing the shape in each dimension as long as nd \\(\\geq\\) 1. The integer is always large enough to hold a pointer on the platform, so the dimension size is only limited by memory. PyArray_DIMS is the macro associated with this data member."]}, {"name": "npy_intp index", "path": "reference/c-api/types-and-structures#c.PyArrayIterObject.index", "type": "Python Types and C-Structures", "text": ["The current 1-d index into the array."]}, {"name": "npy_intp index", "path": "reference/c-api/types-and-structures#c.PyArrayMultiIterObject.index", "type": "Python Types and C-Structures", "text": ["The current (1-d) index into the broadcasted result."]}, {"name": "npy_intp len", "path": "reference/c-api/types-and-structures#c.PyArray_Chunk.len", "type": "Python Types and C-Structures", "text": ["The length of the segment in bytes."]}, {"name": "npy_intp NpyIter_GetBufferSize()", "path": "reference/c-api/iterator#c.NpyIter_GetBufferSize", "type": "Array Iterator API", "text": ["If the iterator is buffered, returns the size of the buffer being used, otherwise returns 0."]}, {"name": "npy_intp NpyIter_GetIterIndex()", "path": "reference/c-api/iterator#c.NpyIter_GetIterIndex", "type": "Array Iterator API", "text": ["Gets the iterindex of the iterator, which is an index matching the iteration order of the iterator."]}, {"name": "npy_intp NpyIter_GetIterSize()", "path": "reference/c-api/iterator#c.NpyIter_GetIterSize", "type": "Array Iterator API", "text": ["Returns the number of elements being iterated. This is the product of all the dimensions in the shape. When a multi index is being tracked (and NpyIter_RemoveAxis may be called) the size may be -1 to indicate an iterator is too large. Such an iterator is invalid, but may become valid after NpyIter_RemoveAxis is called. It is not necessary to check for this case."]}, {"name": "npy_intp PyArray_CountNonzero()", "path": "reference/c-api/array#c.PyArray_CountNonzero", "type": "Array API", "text": ["New in version 1.6.", "Counts the number of non-zero elements in the array object self."]}, {"name": "npy_intp PyArray_DIM()", "path": "reference/c-api/array#c.PyArray_DIM", "type": "Array API", "text": ["Return the shape in the n \\(^{\\textrm{th}}\\) dimension."]}, {"name": "npy_intp PyArray_ITEMSIZE()", "path": "reference/c-api/array#c.PyArray_ITEMSIZE", "type": "Array API", "text": ["Return the itemsize for the elements of this array.", "Note that, in the old API that was deprecated in version 1.7, this function had the return type int."]}, {"name": "npy_intp PyArray_MultiplyList()", "path": "reference/c-api/array#c.PyArray_MultiplyList", "type": "Array API", "text": []}, {"name": "npy_intp PyArray_NBYTES()", "path": "reference/c-api/array#c.PyArray_NBYTES", "type": "Array API", "text": ["Returns the total number of bytes consumed by the array."]}, {"name": "npy_intp PyArray_PyIntAsIntp()", "path": "reference/c-api/array#c.PyArray_PyIntAsIntp", "type": "Array API", "text": ["Convert all kinds of Python objects (including arrays and array scalars) to a (platform-pointer-sized) integer. On error, -1 is returned and an exception set."]}, {"name": "npy_intp PyArray_REFCOUNT()", "path": "reference/c-api/array#c.PyArray_REFCOUNT", "type": "Array API", "text": ["Returns the reference count of any Python object."]}, {"name": "npy_intp PyArray_Size()", "path": "reference/c-api/array#c.PyArray_Size", "type": "Array API", "text": ["Returns 0 if obj is not a sub-class of ndarray. Otherwise, returns the total number of elements in the array. Safer version of PyArray_SIZE (obj)."]}, {"name": "npy_intp PyArray_SIZE()", "path": "reference/c-api/array#c.PyArray_SIZE", "type": "Array API", "text": ["Returns the total size (in number of elements) of the array."]}, {"name": "npy_intp PyArray_STRIDE()", "path": "reference/c-api/array#c.PyArray_STRIDE", "type": "Array API", "text": ["Return the stride in the n \\(^{\\textrm{th}}\\) dimension."]}, {"name": "npy_intp size", "path": "reference/c-api/types-and-structures#c.PyArrayIterObject.size", "type": "Python Types and C-Structures", "text": ["The total size of the underlying array."]}, {"name": "npy_intp size", "path": "reference/c-api/types-and-structures#c.PyArrayMultiIterObject.size", "type": "Python Types and C-Structures", "text": ["The total broadcasted size."]}, {"name": "NPY_INTP_FMT", "path": "reference/c-api/dtype#c.NPY_INTP_FMT", "type": "Data Type API", "text": []}, {"name": "npy_isfinite()", "path": "reference/c-api/coremath#c.npy_isfinite", "type": "NumPy core libraries", "text": ["This is a macro, and is equivalent to C99 isfinite: works for single, double and extended precision, and return a non 0 value if x is neither a NaN nor an infinity."]}, {"name": "npy_isinf()", "path": "reference/c-api/coremath#c.npy_isinf", "type": "NumPy core libraries", "text": ["This is a macro, and is equivalent to C99 isinf: works for single, double and extended precision, and return a non 0 value if x is infinite (positive and negative)."]}, {"name": "npy_isnan()", "path": "reference/c-api/coremath#c.npy_isnan", "type": "NumPy core libraries", "text": ["This is a macro, and is equivalent to C99 isnan: works for single, double and extended precision, and return a non 0 value if x is a NaN."]}, {"name": "NPY_ITEM_IS_POINTER", "path": "reference/c-api/types-and-structures#c.NPY_ITEM_IS_POINTER", "type": "Python Types and C-Structures", "text": ["Indicates the item is a pointer to some other data-type"]}, {"name": "NPY_ITEM_REFCOUNT", "path": "reference/c-api/types-and-structures#c.NPY_ITEM_REFCOUNT", "type": "Python Types and C-Structures", "text": ["Indicates that items of this data-type must be reference counted (using Py_INCREF and Py_DECREF ).", "Same as NPY_ITEM_REFCOUNT."]}, {"name": "NPY_ITER_ALIGNED", "path": "reference/c-api/iterator#c.NPY_ITER_ALIGNED", "type": "Array Iterator API", "text": []}, {"name": "NPY_ITER_ALLOCATE", "path": "reference/c-api/iterator#c.NPY_ITER_ALLOCATE", "type": "Array Iterator API", "text": ["This is for output arrays, and requires that the flag NPY_ITER_WRITEONLY or NPY_ITER_READWRITE be set. If op[i] is NULL, creates a new array with the final broadcast dimensions, and a layout matching the iteration order of the iterator.", "When op[i] is NULL, the requested data type op_dtypes[i] may be NULL as well, in which case it is automatically generated from the dtypes of the arrays which are flagged as readable. The rules for generating the dtype are the same is for UFuncs. Of special note is handling of byte order in the selected dtype. If there is exactly one input, the input\u2019s dtype is used as is. Otherwise, if more than one input dtypes are combined together, the output will be in native byte order.", "After being allocated with this flag, the caller may retrieve the new array by calling NpyIter_GetOperandArray and getting the i-th object in the returned C array. The caller must call Py_INCREF on it to claim a reference to the array."]}, {"name": "NPY_ITER_ARRAYMASK", "path": "reference/c-api/iterator#c.NPY_ITER_ARRAYMASK", "type": "Array Iterator API", "text": ["New in version 1.7.", "Indicates that this operand is the mask to use for selecting elements when writing to operands which have the NPY_ITER_WRITEMASKED flag applied to them. Only one operand may have NPY_ITER_ARRAYMASK flag applied to it.", "The data type of an operand with this flag should be either NPY_BOOL, NPY_MASK, or a struct dtype whose fields are all valid mask dtypes. In the latter case, it must match up with a struct operand being WRITEMASKED, as it is specifying a mask for each field of that array.", "This flag only affects writing from the buffer back to the array. This means that if the operand is also NPY_ITER_READWRITE or NPY_ITER_WRITEONLY, code doing iteration can write to this operand to control which elements will be untouched and which ones will be modified. This is useful when the mask should be a combination of input masks."]}, {"name": "NPY_ITER_BUFFERED", "path": "reference/c-api/iterator#c.NPY_ITER_BUFFERED", "type": "Array Iterator API", "text": ["Causes the iterator to store buffering data, and use buffering to satisfy data type, alignment, and byte-order requirements. To buffer an operand, do not specify the NPY_ITER_COPY or NPY_ITER_UPDATEIFCOPY flags, because they will override buffering. Buffering is especially useful for Python code using the iterator, allowing for larger chunks of data at once to amortize the Python interpreter overhead.", "If used with NPY_ITER_EXTERNAL_LOOP, the inner loop for the caller may get larger chunks than would be possible without buffering, because of how the strides are laid out.", "Note that if an operand is given the flag NPY_ITER_COPY or NPY_ITER_UPDATEIFCOPY, a copy will be made in preference to buffering. Buffering will still occur when the array was broadcast so elements need to be duplicated to get a constant stride.", "In normal buffering, the size of each inner loop is equal to the buffer size, or possibly larger if NPY_ITER_GROWINNER is specified. If NPY_ITER_REDUCE_OK is enabled and a reduction occurs, the inner loops may become smaller depending on the structure of the reduction."]}, {"name": "NPY_ITER_C_INDEX", "path": "reference/c-api/iterator#c.NPY_ITER_C_INDEX", "type": "Array Iterator API", "text": ["Causes the iterator to track a raveled flat index matching C order. This option cannot be used with NPY_ITER_F_INDEX."]}, {"name": "NPY_ITER_COMMON_DTYPE", "path": "reference/c-api/iterator#c.NPY_ITER_COMMON_DTYPE", "type": "Array Iterator API", "text": ["Causes the iterator to convert all the operands to a common data type, calculated based on the ufunc type promotion rules. Copying or buffering must be enabled.", "If the common data type is known ahead of time, don\u2019t use this flag. Instead, set the requested dtype for all the operands."]}, {"name": "NPY_ITER_CONTIG", "path": "reference/c-api/iterator#c.NPY_ITER_CONTIG", "type": "Array Iterator API", "text": ["Causes the iterator to provide data for op[i] that is in native byte order, aligned according to the dtype requirements, contiguous, or any combination.", "By default, the iterator produces pointers into the arrays provided, which may be aligned or unaligned, and with any byte order. If copying or buffering is not enabled and the operand data doesn\u2019t satisfy the constraints, an error will be raised.", "The contiguous constraint applies only to the inner loop, successive inner loops may have arbitrary pointer changes.", "If the requested data type is in non-native byte order, the NBO flag overrides it and the requested data type is converted to be in native byte order."]}, {"name": "NPY_ITER_COPY", "path": "reference/c-api/iterator#c.NPY_ITER_COPY", "type": "Array Iterator API", "text": ["Allow a copy of op[i] to be made if it does not meet the data type or alignment requirements as specified by the constructor flags and parameters."]}, {"name": "NPY_ITER_COPY_IF_OVERLAP", "path": "reference/c-api/iterator#c.NPY_ITER_COPY_IF_OVERLAP", "type": "Array Iterator API", "text": ["If any write operand has overlap with any read operand, eliminate all overlap by making temporary copies (enabling UPDATEIFCOPY for write operands, if necessary). A pair of operands has overlap if there is a memory address that contains data common to both arrays.", "Because exact overlap detection has exponential runtime in the number of dimensions, the decision is made based on heuristics, which has false positives (needless copies in unusual cases) but has no false negatives.", "If any read/write overlap exists, this flag ensures the result of the operation is the same as if all operands were copied. In cases where copies would need to be made, the result of the computation may be undefined without this flag!", "Flags that may be passed in op_flags[i], where 0 <= i < nop:"]}, {"name": "NPY_ITER_DELAY_BUFALLOC", "path": "reference/c-api/iterator#c.NPY_ITER_DELAY_BUFALLOC", "type": "Array Iterator API", "text": ["When buffering is enabled, this delays allocation of the buffers until NpyIter_Reset or another reset function is called. This flag exists to avoid wasteful copying of buffer data when making multiple copies of a buffered iterator for multi-threaded iteration.", "Another use of this flag is for setting up reduction operations. After the iterator is created, and a reduction output is allocated automatically by the iterator (be sure to use READWRITE access), its value may be initialized to the reduction unit. Use NpyIter_GetOperandArray to get the object. Then, call NpyIter_Reset to allocate and fill the buffers with their initial values."]}, {"name": "NPY_ITER_DONT_NEGATE_STRIDES", "path": "reference/c-api/iterator#c.NPY_ITER_DONT_NEGATE_STRIDES", "type": "Array Iterator API", "text": ["This only affects the iterator when NPY_KEEPORDER is specified for the order parameter. By default with NPY_KEEPORDER, the iterator reverses axes which have negative strides, so that memory is traversed in a forward direction. This disables this step. Use this flag if you want to use the underlying memory-ordering of the axes, but don\u2019t want an axis reversed. This is the behavior of numpy.ravel(a, order='K'), for instance."]}, {"name": "NPY_ITER_EXTERNAL_LOOP", "path": "reference/c-api/iterator#c.NPY_ITER_EXTERNAL_LOOP", "type": "Array Iterator API", "text": ["Causes the iterator to skip iteration of the innermost loop, requiring the user of the iterator to handle it.", "This flag is incompatible with NPY_ITER_C_INDEX, NPY_ITER_F_INDEX, and NPY_ITER_MULTI_INDEX."]}, {"name": "NPY_ITER_F_INDEX", "path": "reference/c-api/iterator#c.NPY_ITER_F_INDEX", "type": "Array Iterator API", "text": ["Causes the iterator to track a raveled flat index matching Fortran order. This option cannot be used with NPY_ITER_C_INDEX."]}, {"name": "NPY_ITER_GROWINNER", "path": "reference/c-api/iterator#c.NPY_ITER_GROWINNER", "type": "Array Iterator API", "text": ["When buffering is enabled, this allows the size of the inner loop to grow when buffering isn\u2019t necessary. This option is best used if you\u2019re doing a straight pass through all the data, rather than anything with small cache-friendly arrays of temporary values for each inner loop."]}, {"name": "NPY_ITER_MULTI_INDEX", "path": "reference/c-api/iterator#c.NPY_ITER_MULTI_INDEX", "type": "Array Iterator API", "text": ["Causes the iterator to track a multi-index. This prevents the iterator from coalescing axes to produce bigger inner loops. If the loop is also not buffered and no index is being tracked (NpyIter_RemoveAxis can be called), then the iterator size can be -1 to indicate that the iterator is too large. This can happen due to complex broadcasting and will result in errors being created when the setting the iterator range, removing the multi index, or getting the next function. However, it is possible to remove axes again and use the iterator normally if the size is small enough after removal."]}, {"name": "NPY_ITER_NBO", "path": "reference/c-api/iterator#c.NPY_ITER_NBO", "type": "Array Iterator API", "text": []}, {"name": "NPY_ITER_NO_BROADCAST", "path": "reference/c-api/iterator#c.NPY_ITER_NO_BROADCAST", "type": "Array Iterator API", "text": ["Ensures that the input or output matches the iteration dimensions exactly."]}, {"name": "NPY_ITER_NO_SUBTYPE", "path": "reference/c-api/iterator#c.NPY_ITER_NO_SUBTYPE", "type": "Array Iterator API", "text": ["For use with NPY_ITER_ALLOCATE, this flag disables allocating an array subtype for the output, forcing it to be a straight ndarray.", "TODO: Maybe it would be better to introduce a function NpyIter_GetWrappedOutput and remove this flag?"]}, {"name": "NPY_ITER_OVERLAP_ASSUME_ELEMENTWISE", "path": "reference/c-api/iterator#c.NPY_ITER_OVERLAP_ASSUME_ELEMENTWISE", "type": "Array Iterator API", "text": ["In memory overlap checks, assume that operands with NPY_ITER_OVERLAP_ASSUME_ELEMENTWISE enabled are accessed only in the iterator order.", "This enables the iterator to reason about data dependency, possibly avoiding unnecessary copies.", "This flag has effect only if NPY_ITER_COPY_IF_OVERLAP is enabled on the iterator."]}, {"name": "NPY_ITER_RANGED", "path": "reference/c-api/iterator#c.NPY_ITER_RANGED", "type": "Array Iterator API", "text": ["Enables support for iteration of sub-ranges of the full iterindex range [0, NpyIter_IterSize(iter)). Use the function NpyIter_ResetToIterIndexRange to specify a range for iteration.", "This flag can only be used with NPY_ITER_EXTERNAL_LOOP when NPY_ITER_BUFFERED is enabled. This is because without buffering, the inner loop is always the size of the innermost iteration dimension, and allowing it to get cut up would require special handling, effectively making it more like the buffered version."]}, {"name": "NPY_ITER_READONLY", "path": "reference/c-api/iterator#c.NPY_ITER_READONLY", "type": "Array Iterator API", "text": []}, {"name": "NPY_ITER_READWRITE", "path": "reference/c-api/iterator#c.NPY_ITER_READWRITE", "type": "Array Iterator API", "text": []}, {"name": "NPY_ITER_REDUCE_OK", "path": "reference/c-api/iterator#c.NPY_ITER_REDUCE_OK", "type": "Array Iterator API", "text": ["Permits writeable operands with a dimension with zero stride and size greater than one. Note that such operands must be read/write.", "When buffering is enabled, this also switches to a special buffering mode which reduces the loop length as necessary to not trample on values being reduced.", "Note that if you want to do a reduction on an automatically allocated output, you must use NpyIter_GetOperandArray to get its reference, then set every value to the reduction unit before doing the iteration loop. In the case of a buffered reduction, this means you must also specify the flag NPY_ITER_DELAY_BUFALLOC, then reset the iterator after initializing the allocated operand to prepare the buffers."]}, {"name": "NPY_ITER_REFS_OK", "path": "reference/c-api/iterator#c.NPY_ITER_REFS_OK", "type": "Array Iterator API", "text": ["Indicates that arrays with reference types (object arrays or structured arrays containing an object type) may be accepted and used in the iterator. If this flag is enabled, the caller must be sure to check whether NpyIter_IterationNeedsAPI(iter) is true, in which case it may not release the GIL during iteration."]}, {"name": "NPY_ITER_UPDATEIFCOPY", "path": "reference/c-api/iterator#c.NPY_ITER_UPDATEIFCOPY", "type": "Array Iterator API", "text": ["Triggers NPY_ITER_COPY, and when an array operand is flagged for writing and is copied, causes the data in a copy to be copied back to op[i] when NpyIter_Deallocate is called.", "If the operand is flagged as write-only and a copy is needed, an uninitialized temporary array will be created and then copied to back to op[i] on calling NpyIter_Deallocate, instead of doing the unnecessary copy operation."]}, {"name": "NPY_ITER_WRITEMASKED", "path": "reference/c-api/iterator#c.NPY_ITER_WRITEMASKED", "type": "Array Iterator API", "text": ["New in version 1.7.", "This array is the mask for all writemasked operands. Code uses the writemasked flag which indicates that only elements where the chosen ARRAYMASK operand is True will be written to. In general, the iterator does not enforce this, it is up to the code doing the iteration to follow that promise.", "When writemasked flag is used, and this operand is buffered, this changes how data is copied from the buffer into the array. A masked copying routine is used, which only copies the elements in the buffer for which writemasked returns true from the corresponding element in the ARRAYMASK operand."]}, {"name": "NPY_ITER_WRITEONLY", "path": "reference/c-api/iterator#c.NPY_ITER_WRITEONLY", "type": "Array Iterator API", "text": ["Indicate how the user of the iterator will read or write to op[i]. Exactly one of these flags must be specified per operand. Using NPY_ITER_READWRITE or NPY_ITER_WRITEONLY for a user-provided operand may trigger WRITEBACKIFCOPY` semantics. The data will be written back to the original array when NpyIter_Deallocate is called."]}, {"name": "NPY_ITER_ZEROSIZE_OK", "path": "reference/c-api/iterator#c.NPY_ITER_ZEROSIZE_OK", "type": "Array Iterator API", "text": ["Indicates that arrays with a size of zero should be permitted. Since the typical iteration loop does not naturally work with zero-sized arrays, you must check that the IterSize is larger than zero before entering the iteration loop. Currently only the operands are checked, not a forced shape."]}, {"name": "NPY_LIST_PICKLE", "path": "reference/c-api/types-and-structures#c.NPY_LIST_PICKLE", "type": "Python Types and C-Structures", "text": ["Indicates arrays of this data-type must be converted to a list before pickling."]}, {"name": "NPY_LITTLE", "path": "reference/c-api/array#c.NPY_LITTLE", "type": "Array API", "text": ["If a byteorder of NPY_IGNORE is encountered it is left alone. If newendian is NPY_SWAP, then all byte-orders are swapped. Other valid newendian values are NPY_NATIVE, NPY_LITTLE, and NPY_BIG which all cause the returned data-typed descriptor (and all it\u2019s referenced data-type descriptors) to have the corresponding byte- order."]}, {"name": "NPY_LITTLE_ENDIAN", "path": "reference/c-api/config#c.NPY_LITTLE_ENDIAN", "type": "System configuration", "text": []}, {"name": "NPY_LOG10E", "path": "reference/c-api/coremath#c.NPY_LOG10E", "type": "NumPy core libraries", "text": ["Logarithm to base 10 of the Euler constant (\\(\\frac{\\ln(e)}{\\ln(10)}\\))"]}, {"name": "NPY_LOG2E", "path": "reference/c-api/coremath#c.NPY_LOG2E", "type": "NumPy core libraries", "text": ["Logarithm to base 2 of the Euler constant (\\(\\frac{\\ln(e)}{\\ln(2)}\\))"]}, {"name": "NPY_LOGE10", "path": "reference/c-api/coremath#c.NPY_LOGE10", "type": "NumPy core libraries", "text": ["Natural logarithm of 10 (\\(\\ln(10)\\))"]}, {"name": "NPY_LOGE2", "path": "reference/c-api/coremath#c.NPY_LOGE2", "type": "NumPy core libraries", "text": ["Natural logarithm of 2 (\\(\\ln(2)\\))"]}, {"name": "NPY_LONGDOUBLE_FMT", "path": "reference/c-api/dtype#c.NPY_LONGDOUBLE_FMT", "type": "Data Type API", "text": []}, {"name": "NPY_LOOP_END_THREADS", "path": "reference/c-api/ufunc#c.NPY_LOOP_END_THREADS", "type": "UFunc API", "text": ["Used in universal function code to re-acquire the Python GIL if it was released (because loop->obj was not true)."]}, {"name": "NPY_MAX_BUFSIZE", "path": "reference/c-api/array#c.NPY_MAX_BUFSIZE", "type": "Array API", "text": ["Largest size allowed for the user-settable buffers."]}, {"name": "NPY_MAXARGS", "path": "reference/c-api/array#c.NPY_MAXARGS", "type": "Array API", "text": ["The maximum number of array arguments that can be used in functions."]}, {"name": "NPY_MAXDIMS", "path": "reference/c-api/array#c.NPY_MAXDIMS", "type": "Array API", "text": ["The maximum number of dimensions allowed in arrays."]}, {"name": "NPY_MIN_BUFSIZE", "path": "reference/c-api/array#c.NPY_MIN_BUFSIZE", "type": "Array API", "text": ["Smallest size of user-settable internal buffers."]}, {"name": "NPY_NAN", "path": "reference/c-api/coremath", "type": "NumPy core libraries", "text": ["New in version 1.3.0.", "Starting from numpy 1.3.0, we are working on separating the pure C, \u201ccomputational\u201d code from the python dependent code. The goal is twofolds: making the code cleaner, and enabling code reuse by other extensions outside numpy (scipy, etc\u2026).", "The numpy core math library (\u2018npymath\u2019) is a first step in this direction. This library contains most math-related C99 functionality, which can be used on platforms where C99 is not well supported. The core math functions have the same API as the C99 ones, except for the npy_* prefix.", "The available functions are defined in <numpy/npy_math.h> - please refer to this header when in doubt.", "This macro is defined to a NaN (Not a Number), and is guaranteed to have the signbit unset (\u2018positive\u2019 NaN). The corresponding single and extension precision macro are available with the suffix F and L.", "This macro is defined to a positive inf. The corresponding single and extension precision macro are available with the suffix F and L.", "This macro is defined to positive zero. The corresponding single and extension precision macro are available with the suffix F and L.", "This macro is defined to negative zero (that is with the sign bit set). The corresponding single and extension precision macro are available with the suffix F and L.", "This is a macro, and is equivalent to C99 isnan: works for single, double and extended precision, and return a non 0 value if x is a NaN.", "This is a macro, and is equivalent to C99 isfinite: works for single, double and extended precision, and return a non 0 value if x is neither a NaN nor an infinity.", "This is a macro, and is equivalent to C99 isinf: works for single, double and extended precision, and return a non 0 value if x is infinite (positive and negative).", "This is a macro, and is equivalent to C99 signbit: works for single, double and extended precision, and return a non 0 value if x has the signbit set (that is the number is negative).", "This is a function equivalent to C99 copysign: return x with the same sign as y. Works for any value, including inf and nan. Single and extended precisions are available with suffix f and l.", "New in version 1.4.0.", "The following math constants are available in npy_math.h. Single and extended precision are also available by adding the f and l suffixes respectively.", "Base of natural logarithm (\\(e\\))", "Logarithm to base 2 of the Euler constant (\\(\\frac{\\ln(e)}{\\ln(2)}\\))", "Logarithm to base 10 of the Euler constant (\\(\\frac{\\ln(e)}{\\ln(10)}\\))", "Natural logarithm of 2 (\\(\\ln(2)\\))", "Natural logarithm of 10 (\\(\\ln(10)\\))", "Pi (\\(\\pi\\))", "Pi divided by 2 (\\(\\frac{\\pi}{2}\\))", "Pi divided by 4 (\\(\\frac{\\pi}{4}\\))", "Reciprocal of pi (\\(\\frac{1}{\\pi}\\))", "Two times the reciprocal of pi (\\(\\frac{2}{\\pi}\\))", "\\(\\lim_{n\\rightarrow\\infty}({\\sum_{k=1}^n{\\frac{1}{k}}-\\ln n})\\)", "Those can be useful for precise floating point comparison.", "This is a function equivalent to C99 nextafter: return next representable floating point value from x in the direction of y. Single and extended precisions are available with suffix f and l.", "New in version 1.4.0.", "This is a function equivalent to Fortran intrinsic. Return distance between x and next representable floating point value from x, e.g. spacing(1) == eps. spacing of nan and +/- inf return nan. Single and extended precisions are available with suffix f and l.", "New in version 1.4.0.", "Set the divide by zero floating point exception", "New in version 1.6.0.", "Set the overflow floating point exception", "New in version 1.6.0.", "Set the underflow floating point exception", "New in version 1.6.0.", "Set the invalid floating point exception", "New in version 1.6.0.", "Get floating point status. Returns a bitmask with following possible flags:", "Note that npy_get_floatstatus_barrier is preferable as it prevents aggressive compiler optimizations reordering the call relative to the code setting the status, which could lead to incorrect results.", "New in version 1.9.0.", "Get floating point status. A pointer to a local variable is passed in to prevent aggressive compiler optimizations from reordering this function call relative to the code setting the status, which could lead to incorrect results.", "Returns a bitmask with following possible flags:", "New in version 1.15.0.", "Clears the floating point status. Returns the previous status mask.", "Note that npy_clear_floatstatus_barrier is preferable as it prevents aggressive compiler optimizations reordering the call relative to the code setting the status, which could lead to incorrect results.", "New in version 1.9.0.", "Clears the floating point status. A pointer to a local variable is passed in to prevent aggressive compiler optimizations from reordering this function call. Returns the previous status mask.", "New in version 1.15.0.", "New in version 1.4.0.", "C99-like complex functions have been added. Those can be used if you wish to implement portable C extensions. Since we still support platforms without C99 complex type, you need to restrict to C90-compatible syntax, e.g.:", "New in version 1.4.0.", "To use the core math library in your own extension, you need to add the npymath compile and link options to your extension in your setup.py:", "In other words, the usage of info is exactly the same as when using blas_info and co.", "New in version 1.6.0.", "The header file <numpy/halffloat.h> provides functions to work with IEEE 754-2008 16-bit floating point values. While this format is not typically used for numerical computations, it is useful for storing values which require floating point but do not need much precision. It can also be used as an educational tool to understand the nature of floating point round-off error.", "Like for other types, NumPy includes a typedef npy_half for the 16 bit float. Unlike for most of the other types, you cannot use this as a normal type in C, since it is a typedef for npy_uint16. For example, 1.0 looks like 0x3c00 to C, and if you do an equality comparison between the different signed zeros, you will get -0.0 != 0.0 (0x8000 != 0x0000), which is incorrect.", "For these reasons, NumPy provides an API to work with npy_half values accessible by including <numpy/halffloat.h> and linking to \u2018npymath\u2019. For functions that are not provided directly, such as the arithmetic operations, the preferred method is to convert to float or double and back again, as in the following example.", "External Links:", "This macro is defined to positive zero.", "This macro is defined to positive zero.", "This macro is defined to negative zero.", "This macro is defined to 1.0.", "This macro is defined to -1.0.", "This macro is defined to +inf.", "This macro is defined to -inf.", "This macro is defined to a NaN value, guaranteed to have its sign bit unset.", "Converts a half-precision float to a single-precision float.", "Converts a half-precision float to a double-precision float.", "Converts a single-precision float to a half-precision float. The value is rounded to the nearest representable half, with ties going to the nearest even. If the value is too small or too big, the system\u2019s floating point underflow or overflow bit will be set.", "Converts a double-precision float to a half-precision float. The value is rounded to the nearest representable half, with ties going to the nearest even. If the value is too small or too big, the system\u2019s floating point underflow or overflow bit will be set.", "Compares two half-precision floats (h1 == h2).", "Compares two half-precision floats (h1 != h2).", "Compares two half-precision floats (h1 <= h2).", "Compares two half-precision floats (h1 < h2).", "Compares two half-precision floats (h1 >= h2).", "Compares two half-precision floats (h1 > h2).", "Compares two half-precision floats that are known to not be NaN (h1 == h2). If a value is NaN, the result is undefined.", "Compares two half-precision floats that are known to not be NaN (h1 < h2). If a value is NaN, the result is undefined.", "Compares two half-precision floats that are known to not be NaN (h1 <= h2). If a value is NaN, the result is undefined.", "Tests whether the half-precision float has a value equal to zero. This may be slightly faster than calling npy_half_eq(h, NPY_ZERO).", "Tests whether the half-precision float is a NaN.", "Tests whether the half-precision float is plus or minus Inf.", "Tests whether the half-precision float is finite (not NaN or Inf).", "Returns 1 is h is negative, 0 otherwise.", "Returns the value of x with the sign bit copied from y. Works for any value, including Inf and NaN.", "This is the same for half-precision float as npy_spacing and npy_spacingf described in the low-level floating point section.", "This is the same for half-precision float as npy_nextafter and npy_nextafterf described in the low-level floating point section.", "Low-level function which converts a 32-bit single-precision float, stored as a uint32, into a 16-bit half-precision float.", "Low-level function which converts a 64-bit double-precision float, stored as a uint64, into a 16-bit half-precision float.", "Low-level function which converts a 16-bit half-precision float into a 32-bit single-precision float, stored as a uint32.", "Low-level function which converts a 16-bit half-precision float into a 64-bit double-precision float, stored as a uint64."]}, {"name": "NPY_NATIVE", "path": "reference/c-api/array#c.NPY_NATIVE", "type": "Array API", "text": ["If a byteorder of NPY_IGNORE is encountered it is left alone. If newendian is NPY_SWAP, then all byte-orders are swapped. Other valid newendian values are NPY_NATIVE, NPY_LITTLE, and NPY_BIG which all cause the returned data-typed descriptor (and all it\u2019s referenced data-type descriptors) to have the corresponding byte- order."]}, {"name": "NPY_NEEDS_INIT", "path": "reference/c-api/types-and-structures#c.NPY_NEEDS_INIT", "type": "Python Types and C-Structures", "text": ["Indicates memory for this data-type must be initialized (set to 0) on creation."]}, {"name": "NPY_NEEDS_PYAPI", "path": "reference/c-api/types-and-structures#c.NPY_NEEDS_PYAPI", "type": "Python Types and C-Structures", "text": ["Indicates this data-type requires the Python C-API during access (so don\u2019t give up the GIL if array access is going to be needed)."]}, {"name": "NPY_NEIGHBORHOOD_ITER_CIRCULAR_PADDING", "path": "reference/c-api/array#c.PyArray_NeighborhoodIterNew.NPY_NEIGHBORHOOD_ITER_CIRCULAR_PADDING", "type": "Array API", "text": ["Circular padding. Outside bounds values will be as if the array was repeated. For example, for the array [1, 2, 3, 4], x[-2] will be 3, x[-2] will be 4, x[4] will be 1, x[5] will be 2, etc\u2026"]}, {"name": "NPY_NEIGHBORHOOD_ITER_CONSTANT_PADDING", "path": "reference/c-api/array#c.PyArray_NeighborhoodIterNew.NPY_NEIGHBORHOOD_ITER_CONSTANT_PADDING", "type": "Array API", "text": ["Constant padding. Outside bounds values will be the same as the first item in fill_value."]}, {"name": "NPY_NEIGHBORHOOD_ITER_MIRROR_PADDING", "path": "reference/c-api/array#c.PyArray_NeighborhoodIterNew.NPY_NEIGHBORHOOD_ITER_MIRROR_PADDING", "type": "Array API", "text": ["Mirror padding. Outside bounds values will be as if the array items were mirrored. For example, for the array [1, 2, 3, 4], x[-2] will be 2, x[-2] will be 1, x[4] will be 4, x[5] will be 1, etc\u2026"]}, {"name": "NPY_NEIGHBORHOOD_ITER_ONE_PADDING", "path": "reference/c-api/array#c.PyArray_NeighborhoodIterNew.NPY_NEIGHBORHOOD_ITER_ONE_PADDING", "type": "Array API", "text": ["One padding, Outside bounds values will be 1."]}, {"name": "NPY_NOTYPE", "path": "reference/c-api/dtype#c.NPY_NOTYPE", "type": "Data Type API", "text": ["A signal value guaranteed not to be a valid type enumeration number."]}, {"name": "NPY_NTYPES", "path": "reference/c-api/dtype#c.NPY_NTYPES", "type": "Data Type API", "text": ["The total number of built-in NumPy types. The enumeration covers the range from 0 to NPY_NTYPES-1."]}, {"name": "NPY_NZERO", "path": "reference/c-api/coremath#c.NPY_NZERO", "type": "NumPy core libraries", "text": ["This macro is defined to negative zero (that is with the sign bit set). The corresponding single and extension precision macro are available with the suffix F and L."]}, {"name": "NPY_OBJECT_DTYPE_FLAGS", "path": "reference/c-api/types-and-structures#c.NPY_USE_SETITEM.NPY_OBJECT_DTYPE_FLAGS", "type": "Python Types and C-Structures", "text": ["Bits set for the object data-type: ( NPY_LIST_PICKLE | NPY_USE_GETITEM | NPY_ITEM_IS_POINTER | NPY_ITEM_REFCOUNT | NPY_NEEDS_INIT | NPY_NEEDS_PYAPI)."]}, {"name": "NPY_OUT_ARRAY", "path": "reference/c-api/array#c.NPY_OUT_ARRAY", "type": "Array API", "text": ["NPY_ARRAY_C_CONTIGUOUS | NPY_ARRAY_WRITEABLE | NPY_ARRAY_ALIGNED"]}, {"name": "NPY_PI", "path": "reference/c-api/coremath#c.NPY_PI", "type": "NumPy core libraries", "text": ["Pi (\\(\\pi\\))"]}, {"name": "NPY_PI_2", "path": "reference/c-api/coremath#c.NPY_PI_2", "type": "NumPy core libraries", "text": ["Pi divided by 2 (\\(\\frac{\\pi}{2}\\))"]}, {"name": "NPY_PI_4", "path": "reference/c-api/coremath#c.NPY_PI_4", "type": "NumPy core libraries", "text": ["Pi divided by 4 (\\(\\frac{\\pi}{4}\\))"]}, {"name": "NPY_PZERO", "path": "reference/c-api/coremath#c.NPY_PZERO", "type": "NumPy core libraries", "text": ["This macro is defined to positive zero. The corresponding single and extension precision macro are available with the suffix F and L."]}, {"name": "NPY_SCALAR_PRIORITY", "path": "reference/c-api/array#c.NPY_SCALAR_PRIORITY", "type": "Array API", "text": ["Default scalar priority (very small)"]}, {"name": "NPY_SCALARKIND PyArray_ScalarKind()", "path": "reference/c-api/array#c.PyArray_ScalarKind", "type": "Array API", "text": ["See the function PyArray_MinScalarType for an alternative mechanism introduced in NumPy 1.6.0.", "Return the kind of scalar represented by typenum and the array in *arr (if arr is not NULL ). The array is assumed to be rank-0 and only used if typenum represents a signed integer. If arr is not NULL and the first element is negative then NPY_INTNEG_SCALAR is returned, otherwise NPY_INTPOS_SCALAR is returned. The possible return values are the enumerated values in NPY_SCALARKIND."]}, {"name": "NPY_SCALARKIND scalarkind()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.scalarkind", "type": "Python Types and C-Structures", "text": ["A function to determine how scalars of this type should be interpreted. The argument is NULL or a 0-dimensional array containing the data (if that is needed to determine the kind of scalar). The return value must be of type NPY_SCALARKIND."]}, {"name": "NPY_SIGINT_OFF", "path": "reference/c-api/config#c.NPY_SIGINT_OFF", "type": "System configuration", "text": []}, {"name": "NPY_SIGINT_ON", "path": "reference/c-api/config#c.NPY_SIGINT_ON", "type": "System configuration", "text": []}, {"name": "NPY_SIGJMP_BUF", "path": "reference/c-api/config#c.NPY_SIGJMP_BUF", "type": "System configuration", "text": []}, {"name": "NPY_SIGLONGJMP", "path": "reference/c-api/config#c.NPY_SIGLONGJMP", "type": "System configuration", "text": []}, {"name": "npy_signbit()", "path": "reference/c-api/coremath#c.npy_signbit", "type": "NumPy core libraries", "text": ["This is a macro, and is equivalent to C99 signbit: works for single, double and extended precision, and return a non 0 value if x has the signbit set (that is the number is negative)."]}, {"name": "NPY_SIGSETJMP", "path": "reference/c-api/config#c.NPY_SIGSETJMP", "type": "System configuration", "text": []}, {"name": "NPY_SIZEOF_DOUBLE", "path": "reference/c-api/config#c.NPY_SIZEOF_DOUBLE", "type": "System configuration", "text": ["sizeof(double)"]}, {"name": "NPY_SIZEOF_FLOAT", "path": "reference/c-api/config#c.NPY_SIZEOF_FLOAT", "type": "System configuration", "text": ["sizeof(float)"]}, {"name": "NPY_SIZEOF_INT", "path": "reference/c-api/config#c.NPY_SIZEOF_INT", "type": "System configuration", "text": ["sizeof(int)"]}, {"name": "NPY_SIZEOF_INTP", "path": "reference/c-api/config#c.NPY_SIZEOF_INTP", "type": "System configuration", "text": ["Size of a pointer on this platform (sizeof(void *))"]}, {"name": "NPY_SIZEOF_LONG", "path": "reference/c-api/config#c.NPY_SIZEOF_LONG", "type": "System configuration", "text": ["sizeof(long)"]}, {"name": "NPY_SIZEOF_LONG_DOUBLE", "path": "reference/c-api/config#c.NPY_SIZEOF_LONG_DOUBLE", "type": "System configuration", "text": []}, {"name": "NPY_SIZEOF_LONGDOUBLE", "path": "reference/c-api/config#c.NPY_SIZEOF_LONGDOUBLE", "type": "System configuration", "text": ["sizeof(longdouble)"]}, {"name": "NPY_SIZEOF_LONGLONG", "path": "reference/c-api/config#c.NPY_SIZEOF_LONGLONG", "type": "System configuration", "text": ["sizeof(longlong) where longlong is defined appropriately on the platform."]}, {"name": "NPY_SIZEOF_PY_INTPTR_T", "path": "reference/c-api/config#c.NPY_SIZEOF_PY_INTPTR_T", "type": "System configuration", "text": []}, {"name": "NPY_SIZEOF_PY_LONG_LONG", "path": "reference/c-api/config#c.NPY_SIZEOF_PY_LONG_LONG", "type": "System configuration", "text": []}, {"name": "NPY_SIZEOF_SHORT", "path": "reference/c-api/config", "type": "System configuration", "text": ["When NumPy is built, information about system configuration is recorded, and is made available for extension modules using NumPy\u2019s C API. These are mostly defined in numpyconfig.h (included in ndarrayobject.h). The public symbols are prefixed by NPY_*. NumPy also offers some functions for querying information about the platform in use.", "For private use, NumPy also constructs a config.h in the NumPy include directory, which is not exported by NumPy (that is a python extension which use the numpy C API will not see those symbols), to avoid namespace pollution.", "The NPY_SIZEOF_{CTYPE} constants are defined so that sizeof information is available to the pre-processor.", "sizeof(short)", "sizeof(int)", "sizeof(long)", "sizeof(longlong) where longlong is defined appropriately on the platform.", "sizeof(float)", "sizeof(double)", "sizeof(longdouble)", "Size of a pointer on this platform (sizeof(void *))", "New in version 1.3.0.", "CPU architecture of the platform; only one of the above is defined.", "Defined in numpy/npy_cpu.h", "New in version 1.3.0.", "Portable alternatives to the endian.h macros of GNU Libc. If big endian, NPY_BYTE_ORDER == NPY_BIG_ENDIAN, and similarly for little endian architectures.", "Defined in numpy/npy_endian.h.", "New in version 1.3.0.", "Returns the endianness of the current platform. One of NPY_CPU_BIG, NPY_CPU_LITTLE, or NPY_CPU_UNKNOWN_ENDIAN."]}, {"name": "NPY_SUBTYPE_PRIORITY", "path": "reference/c-api/array#c.NPY_SUBTYPE_PRIORITY", "type": "Array API", "text": ["Default subtype priority."]}, {"name": "NPY_SUCCEED", "path": "reference/c-api/array#c.NPY_SUCCEED", "type": "Array API", "text": ["The return value of successful converter functions which are called using the \u201cO&\u201d syntax in PyArg_ParseTuple-like functions."]}, {"name": "NPY_SWAP", "path": "reference/c-api/array#c.NPY_SWAP", "type": "Array API", "text": ["If a byteorder of NPY_IGNORE is encountered it is left alone. If newendian is NPY_SWAP, then all byte-orders are swapped. Other valid newendian values are NPY_NATIVE, NPY_LITTLE, and NPY_BIG which all cause the returned data-typed descriptor (and all it\u2019s referenced data-type descriptors) to have the corresponding byte- order."]}, {"name": "NPY_TRUE", "path": "reference/c-api/array#c.NPY_TRUE", "type": "Array API", "text": ["Defined as 1 for use with Bool."]}, {"name": "npy_uint16 npy_doublebits_to_halfbits()", "path": "reference/c-api/coremath#c.npy_doublebits_to_halfbits", "type": "NumPy core libraries", "text": ["Low-level function which converts a 64-bit double-precision float, stored as a uint64, into a 16-bit half-precision float."]}, {"name": "npy_uint16 npy_floatbits_to_halfbits()", "path": "reference/c-api/coremath#c.npy_floatbits_to_halfbits", "type": "NumPy core libraries", "text": ["Low-level function which converts a 32-bit single-precision float, stored as a uint32, into a 16-bit half-precision float."]}, {"name": "npy_uint32 *core_dim_flags", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.core_dim_flags", "type": "Python Types and C-Structures", "text": ["For each distinct core dimension, a set of UFUNC_CORE_DIM* flags"]}, {"name": "npy_uint32 iter_flags", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.iter_flags", "type": "Python Types and C-Structures", "text": ["Override the default nditer flags for the ufunc."]}, {"name": "npy_uint32 npy_halfbits_to_floatbits()", "path": "reference/c-api/coremath#c.npy_halfbits_to_floatbits", "type": "NumPy core libraries", "text": ["Low-level function which converts a 16-bit half-precision float into a 32-bit single-precision float, stored as a uint32."]}, {"name": "npy_uint32 op_flags", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.op_flags", "type": "Python Types and C-Structures", "text": ["Override the default operand flags for each ufunc operand."]}, {"name": "npy_uint64 npy_halfbits_to_doublebits()", "path": "reference/c-api/coremath#c.npy_halfbits_to_doublebits", "type": "NumPy core libraries", "text": ["Low-level function which converts a 16-bit half-precision float into a 64-bit double-precision float, stored as a uint64."]}, {"name": "npy_uint64 random_bounded_uint64()", "path": "reference/random/c-api#c.random_bounded_uint64", "type": "C API for random", "text": []}, {"name": "npy_uint64 random_interval()", "path": "reference/random/c-api#c.random_interval", "type": "C API for random", "text": []}, {"name": "npy_uint64 random_uint()", "path": "reference/random/c-api#c.random_uint", "type": "C API for random", "text": []}, {"name": "NPY_UINTP_FMT", "path": "reference/c-api/dtype#c.NPY_UINTP_FMT", "type": "Data Type API", "text": []}, {"name": "NPY_ULONGLONG_FMT", "path": "reference/c-api/dtype#c.NPY_ULONGLONG_FMT", "type": "Data Type API", "text": []}, {"name": "NPY_UNLIKELY", "path": "reference/c-api/config#c.NPY_UNLIKELY", "type": "System configuration", "text": []}, {"name": "NPY_UNUSED", "path": "reference/c-api/config#c.NPY_UNUSED", "type": "System configuration", "text": []}, {"name": "NPY_USE_GETITEM", "path": "reference/c-api/types-and-structures#c.NPY_USE_GETITEM", "type": "Python Types and C-Structures", "text": ["On array access use the f->getitem function pointer instead of the standard conversion to an array scalar. Must use if you don\u2019t define an array scalar to go along with the data-type."]}, {"name": "NPY_USE_SETITEM", "path": "reference/c-api/types-and-structures#c.NPY_USE_SETITEM", "type": "Python Types and C-Structures", "text": ["When creating a 0-d array from an array scalar use f->setitem instead of the standard copy from an array scalar. Must use if you don\u2019t define an array scalar to go along with the data-type.", "The bits that are inherited for the parent data-type if these bits are set in any field of the data-type. Currently ( NPY_NEEDS_INIT | NPY_LIST_PICKLE | NPY_ITEM_REFCOUNT | NPY_NEEDS_PYAPI ).", "Bits set for the object data-type: ( NPY_LIST_PICKLE | NPY_USE_GETITEM | NPY_ITEM_IS_POINTER | NPY_ITEM_REFCOUNT | NPY_NEEDS_INIT | NPY_NEEDS_PYAPI).", "Return true if all the given flags are set for the data-type object.", "Equivalent to PyDataType_FLAGCHK (dtype, NPY_ITEM_REFCOUNT).", "A number that uniquely identifies the data type. For new data-types, this number is assigned when the data-type is registered.", "For data types that are always the same size (such as long), this holds the size of the data type. For flexible data types where different arrays can have a different elementsize, this should be 0.", "A number providing alignment information for this data type. Specifically, it shows how far from the start of a 2-element structure (whose first element is a char ), the compiler places an item of this type: offsetof(struct {char c; type v;},\nv)", "If this is non- NULL, then this data-type descriptor is a C-style contiguous array of another data-type descriptor. In other-words, each element that this descriptor describes is actually an array of some other base descriptor. This is most useful as the data-type descriptor for a field in another data-type descriptor. The fields member should be NULL if this is non- NULL (the fields member of the base descriptor can be non- NULL however).", "The data-type-descriptor object of the base-type.", "The shape (always C-style contiguous) of the sub-array as a Python tuple.", "If this is non-NULL, then this data-type-descriptor has fields described by a Python dictionary whose keys are names (and also titles if given) and whose values are tuples that describe the fields. Recall that a data-type-descriptor always describes a fixed-length set of bytes. A field is a named sub-region of that total, fixed-length collection. A field is described by a tuple composed of another data- type-descriptor and a byte offset. Optionally, the tuple may contain a title which is normally a Python string. These tuples are placed in this dictionary keyed by name (and also title if given).", "An ordered tuple of field names. It is NULL if no field is defined.", "A pointer to a structure containing functions that the type needs to implement internal features. These functions are not the same thing as the universal functions (ufuncs) described later. Their signatures can vary arbitrarily.", "Metadata about this dtype.", "Metadata specific to the C implementation of the particular dtype. Added for NumPy 1.7.0.", "Currently unused. Reserved for future use in caching hash values."]}, {"name": "NPY_USERDEF", "path": "reference/c-api/dtype#c.NPY_USERDEF", "type": "Data Type API", "text": ["The start of type numbers used for Custom Data types."]}, {"name": "NPY_WRAP", "path": "reference/c-api/array#c.PyArray_Choose.NPY_WRAP", "type": "Array API", "text": ["wrap values < 0 by adding len(op) and values >=len(op) by subtracting len(op) until they are in range;"]}, {"name": "NpyAuxData *c_metadata", "path": "reference/c-api/types-and-structures#c.NPY_USE_SETITEM.c_metadata", "type": "Python Types and C-Structures", "text": ["Metadata specific to the C implementation of the particular dtype. Added for NumPy 1.7.0."]}, {"name": "NpyAuxData *NPY_AUXDATA_CLONE()", "path": "reference/c-api/array#c.NPY_AUXDATA_CLONE", "type": "Array API", "text": ["A macro which calls the auxdata\u2019s clone function appropriately, returning a deep copy of the auxiliary data."]}, {"name": "NpyIter *NpyIter_AdvancedNew()", "path": "reference/c-api/iterator#c.NpyIter_AdvancedNew", "type": "Array Iterator API", "text": ["Extends NpyIter_MultiNew with several advanced options providing more control over broadcasting and buffering.", "If -1/NULL values are passed to oa_ndim, op_axes, itershape, and buffersize, it is equivalent to NpyIter_MultiNew.", "The parameter oa_ndim, when not zero or -1, specifies the number of dimensions that will be iterated with customized broadcasting. If it is provided, op_axes must and itershape can also be provided. The op_axes parameter let you control in detail how the axes of the operand arrays get matched together and iterated. In op_axes, you must provide an array of nop pointers to oa_ndim-sized arrays of type npy_intp. If an entry in op_axes is NULL, normal broadcasting rules will apply. In op_axes[j][i] is stored either a valid axis of op[j], or -1 which means newaxis. Within each op_axes[j] array, axes may not be repeated. The following example is how normal broadcasting applies to a 3-D array, a 2-D array, a 1-D array and a scalar.", "Note: Before NumPy 1.8 oa_ndim == 0` was used for signalling that\nthat ``op_axes and itershape are unused. This is deprecated and should be replaced with -1. Better backward compatibility may be achieved by using NpyIter_MultiNew for this case.", "The itershape parameter allows you to force the iterator to have a specific iteration shape. It is an array of length oa_ndim. When an entry is negative, its value is determined from the operands. This parameter allows automatically allocated outputs to get additional dimensions which don\u2019t match up with any dimension of an input.", "If buffersize is zero, a default buffer size is used, otherwise it specifies how big of a buffer to use. Buffers which are powers of 2 such as 4096 or 8192 are recommended.", "Returns NULL if there is an error, otherwise returns the allocated iterator."]}, {"name": "NpyIter *NpyIter_Copy()", "path": "reference/c-api/iterator#c.NpyIter_Copy", "type": "Array Iterator API", "text": ["Makes a copy of the given iterator. This function is provided primarily to enable multi-threaded iteration of the data.", "TODO: Move this to a section about multithreaded iteration.", "The recommended approach to multithreaded iteration is to first create an iterator with the flags NPY_ITER_EXTERNAL_LOOP, NPY_ITER_RANGED, NPY_ITER_BUFFERED, NPY_ITER_DELAY_BUFALLOC, and possibly NPY_ITER_GROWINNER. Create a copy of this iterator for each thread (minus one for the first iterator). Then, take the iteration index range [0, NpyIter_GetIterSize(iter)) and split it up into tasks, for example using a TBB parallel_for loop. When a thread gets a task to execute, it then uses its copy of the iterator by calling NpyIter_ResetToIterIndexRange and iterating over the full range.", "When using the iterator in multi-threaded code or in code not holding the Python GIL, care must be taken to only call functions which are safe in that context. NpyIter_Copy cannot be safely called without the Python GIL, because it increments Python references. The Reset* and some other functions may be safely called by passing in the errmsg parameter as non-NULL, so that the functions will pass back errors through it instead of setting a Python exception.", "NpyIter_Deallocate must be called for each copy."]}, {"name": "NpyIter *NpyIter_MultiNew()", "path": "reference/c-api/iterator#c.NpyIter_MultiNew", "type": "Array Iterator API", "text": ["Creates an iterator for broadcasting the nop array objects provided in op, using regular NumPy broadcasting rules.", "Any of the NPY_ORDER enum values may be passed to order. For efficient iteration, NPY_KEEPORDER is the best option, and the other orders enforce the particular iteration pattern. When using NPY_KEEPORDER, if you also want to ensure that the iteration is not reversed along an axis, you should pass the flag NPY_ITER_DONT_NEGATE_STRIDES.", "Any of the NPY_CASTING enum values may be passed to casting. The values include NPY_NO_CASTING, NPY_EQUIV_CASTING, NPY_SAFE_CASTING, NPY_SAME_KIND_CASTING, and NPY_UNSAFE_CASTING. To allow the casts to occur, copying or buffering must also be enabled.", "If op_dtypes isn\u2019t NULL, it specifies a data type or NULL for each op[i].", "Returns NULL if there is an error, otherwise returns the allocated iterator.", "Flags that may be passed in flags, applying to the whole iterator, are:"]}, {"name": "NpyIter_GetMultiIndexFunc *NpyIter_GetGetMultiIndex()", "path": "reference/c-api/iterator#c.NpyIter_GetGetMultiIndex", "type": "Array Iterator API", "text": ["Returns a function pointer for getting the current multi-index of the iterator. Returns NULL if the iterator is not tracking a multi-index. It is recommended that this function pointer be cached in a local variable before the iteration loop.", "Returns NULL if there is an error. If errmsg is non-NULL, no Python exception is set when NPY_FAIL is returned. Instead, *errmsg is set to an error message. When errmsg is non-NULL, the function may be safely called without holding the Python GIL."]}, {"name": "number.__class_getitem__()", "path": "reference/generated/numpy.number.__class_getitem__", "type": "numpy.number.__class_getitem__", "text": ["method", "Return a parametrized wrapper around the number type.", "New in version 1.22.", "A parametrized number type.", "See also", "Type hinting generics in standard collections.", "This method is only available for python 3.9 and later."]}, {"name": "NumPy and SWIG", "path": "reference/swig", "type": "NumPy and SWIG", "text": []}, {"name": "NumPy benchmarks", "path": "benchmarking", "type": "NumPy benchmarks", "text": ["Benchmarking NumPy with Airspeed Velocity.", "Airspeed Velocity manages building and Python virtualenvs by itself, unless told otherwise. Some of the benchmarking features in runtests.py also tell ASV to use the NumPy compiled by runtests.py. To run the benchmarks, you do not need to install a development version of NumPy to your current Python environment.", "Before beginning, ensure that airspeed velocity is installed. By default, asv ships with support for anaconda and virtualenv:", "After contributing new benchmarks, you should test them locally before submitting a pull request.", "To run all benchmarks, navigate to the root NumPy directory at the command line and execute:", "where --bench activates the benchmark suite instead of the test suite. This builds NumPy and runs all available benchmarks defined in benchmarks/. (Note: this could take a while. Each benchmark is run multiple times to measure the distribution in execution times.)", "To run benchmarks from a particular benchmark module, such as bench_core.py, simply append the filename without the extension:", "To run a benchmark defined in a class, such as Mandelbrot from bench_avx.py:", "Compare change in benchmark results to another version/commit/branch:", "All of the commands above display the results in plain text in the console, and the results are not saved for comparison with future commits. For greater control, a graphical view, and to have results saved for future comparison you can run ASV commands (record results and generate HTML):", "More on how to use asv can be found in ASV documentation Command-line help is available as usual via asv --help and asv run --help.", "See ASV documentation for basics on how to write benchmarks.", "Some things to consider:"]}, {"name": "NumPy C code explanations", "path": "dev/internals.code-explanations", "type": "Development", "text": ["Fanaticism consists of redoubling your efforts when you have forgotten your aim. \u2014 George Santayana", "An authority is a person who can tell you more about something than you really care to know. \u2014 Unknown", "This page attempts to explain the logic behind some of the new pieces of code. The purpose behind these explanations is to enable somebody to be able to understand the ideas behind the implementation somewhat more easily than just staring at the code. Perhaps in this way, the algorithms can be improved on, borrowed from, and/or optimized by more people.", "One fundamental aspect of the ndarray is that an array is seen as a \u201cchunk\u201d of memory starting at some location. The interpretation of this memory depends on the stride information. For each dimension in an \\(N\\)-dimensional array, an integer (stride) dictates how many bytes must be skipped to get to the next element in that dimension. Unless you have a single-segment array, this stride information must be consulted when traversing through an array. It is not difficult to write code that accepts strides, you just have to use char* pointers because strides are in units of bytes. Keep in mind also that strides do not have to be unit-multiples of the element size. Also, remember that if the number of dimensions of the array is 0 (sometimes called a rank-0 array), then the strides and dimensions variables are NULL.", "Besides the structural information contained in the strides and dimensions members of the PyArrayObject, the flags contain important information about how the data may be accessed. In particular, the NPY_ARRAY_ALIGNED flag is set when the memory is on a suitable boundary according to the datatype array. Even if you have a contiguous chunk of memory, you cannot just assume it is safe to dereference a datatype-specific pointer to an element. Only if the NPY_ARRAY_ALIGNED flag is set, this is a safe operation. On some platforms it will work but on others, like Solaris, it will cause a bus error. The NPY_ARRAY_WRITEABLE should also be ensured if you plan on writing to the memory area of the array. It is also possible to obtain a pointer to an unwritable memory area. Sometimes, writing to the memory area when the NPY_ARRAY_WRITEABLE flag is not set will just be rude. Other times it can cause program crashes (e.g. a data-area that is a read-only memory-mapped file).", "See also", "Data type objects (dtype)", "The datatype is an important abstraction of the ndarray. Operations will look to the datatype to provide the key functionality that is needed to operate on the array. This functionality is provided in the list of function pointers pointed to by the f member of the PyArray_Descr structure. In this way, the number of datatypes can be extended simply by providing a PyArray_Descr structure with suitable function pointers in the f member. For built-in types, there are some optimizations that bypass this mechanism, but the point of the datatype abstraction is to allow new datatypes to be added.", "One of the built-in datatypes, the void datatype allows for arbitrary structured types containing 1 or more fields as elements of the array. A field is simply another datatype object along with an offset into the current structured type. In order to support arbitrarily nested fields, several recursive implementations of datatype access are implemented for the void type. A common idiom is to cycle through the elements of the dictionary and perform a specific operation based on the datatype object stored at the given offset. These offsets can be arbitrary numbers. Therefore, the possibility of encountering misaligned data must be recognized and taken into account if necessary.", "See also", "Iterating Over Arrays", "A very common operation in much of NumPy code is the need to iterate over all the elements of a general, strided, N-dimensional array. This operation of a general-purpose N-dimensional loop is abstracted in the notion of an iterator object. To write an N-dimensional loop, you only have to create an iterator object from an ndarray, work with the dataptr member of the iterator object structure and call the macro PyArray_ITER_NEXT on the iterator object to move to the next element. The next element is always in C-contiguous order. The macro works by first special-casing the C-contiguous, 1-D, and 2-D cases which work very simply.", "For the general case, the iteration works by keeping track of a list of coordinate counters in the iterator object. At each iteration, the last coordinate counter is increased (starting from 0). If this counter is smaller than one less than the size of the array in that dimension (a pre-computed and stored value), then the counter is increased and the dataptr member is increased by the strides in that dimension and the macro ends. If the end of a dimension is reached, the counter for the last dimension is reset to zero and the dataptr is moved back to the beginning of that dimension by subtracting the strides value times one less than the number of elements in that dimension (this is also pre-computed and stored in the backstrides member of the iterator object). In this case, the macro does not end, but a local dimension counter is decremented so that the next-to-last dimension replaces the role that the last dimension played and the previously-described tests are executed again on the next-to-last dimension. In this way, the dataptr is adjusted appropriately for arbitrary striding.", "The coordinates member of the PyArrayIterObject structure maintains the current N-d counter unless the underlying array is C-contiguous in which case the coordinate counting is bypassed. The index member of the PyArrayIterObject keeps track of the current flat index of the iterator. It is updated by the PyArray_ITER_NEXT macro.", "See also", "Broadcasting", "In Numeric, the ancestor of NumPy, broadcasting was implemented in several lines of code buried deep in ufuncobject.c. In NumPy, the notion of broadcasting has been abstracted so that it can be performed in multiple places. Broadcasting is handled by the function PyArray_Broadcast. This function requires a PyArrayMultiIterObject (or something that is a binary equivalent) to be passed in. The PyArrayMultiIterObject keeps track of the broadcast number of dimensions and size in each dimension along with the total size of the broadcast result. It also keeps track of the number of arrays being broadcast and a pointer to an iterator for each of the arrays being broadcast.", "The PyArray_Broadcast function takes the iterators that have already been defined and uses them to determine the broadcast shape in each dimension (to create the iterators at the same time that broadcasting occurs then use the PyArray_MultiIterNew function). Then, the iterators are adjusted so that each iterator thinks it is iterating over an array with the broadcast size. This is done by adjusting the iterators number of dimensions, and the shape in each dimension. This works because the iterator strides are also adjusted. Broadcasting only adjusts (or adds) length-1 dimensions. For these dimensions, the strides variable is simply set to 0 so that the data-pointer for the iterator over that array doesn\u2019t move as the broadcasting operation operates over the extended dimension.", "Broadcasting was always implemented in Numeric using 0-valued strides for the extended dimensions. It is done in exactly the same way in NumPy. The big difference is that now the array of strides is kept track of in a PyArrayIterObject, the iterators involved in a broadcast result are kept track of in a PyArrayMultiIterObject, and the PyArray_Broadcast call implements the General Broadcasting Rules.", "See also", "Scalars", "The array scalars offer a hierarchy of Python types that allow a one-to-one correspondence between the datatype stored in an array and the Python-type that is returned when an element is extracted from the array. An exception to this rule was made with object arrays. Object arrays are heterogeneous collections of arbitrary Python objects. When you select an item from an object array, you get back the original Python object (and not an object array scalar which does exist but is rarely used for practical purposes).", "The array scalars also offer the same methods and attributes as arrays with the intent that the same code can be used to support arbitrary dimensions (including 0-dimensions). The array scalars are read-only (immutable) with the exception of the void scalar which can also be written to so that structured array field setting works more naturally (a[0]['f1'] = value).", "See also", "Indexing on ndarrays, Indexing routines", "All Python indexing operations arr[index] are organized by first preparing the index and finding the index type. The supported index types are:", "As well as the scalar array special case signaling that an integer array was interpreted as an integer index, which is important because an integer array index forces a copy but is ignored if a scalar is returned (full integer index). The prepared index is guaranteed to be valid with the exception of out of bound values and broadcasting errors for advanced indexing. This includes that an Ellipsis is added for incomplete indices for example when a two-dimensional array is indexed with a single integer.", "The next step depends on the type of index which was found. If all dimensions are indexed with an integer a scalar is returned or set. A single boolean indexing array will call specialized boolean functions. Indices containing an Ellipsis or slice but no advanced indexing will always create a view into the old array by calculating the new strides and memory offset. This view can then either be returned or, for assignments, filled using PyArray_CopyObject. Note that PyArray_CopyObject may also be called on temporary arrays in other branches to support complicated assignments when the array is of object dtype.", "By far the most complex case is advanced indexing, which may or may not be combined with typical view-based indexing. Here integer indices are interpreted as view-based. Before trying to understand this, you may want to make yourself familiar with its subtleties. The advanced indexing code has three different branches and one special case:", "Deciding what case applies, checking broadcasting, and determining the kind of transposition needed are all done in PyArray_MapIterNew. After setting up, there are two cases. If there is no subarray or it only has one element, no subarray iteration is necessary and an iterator is prepared which iterates all indexing arrays as well as the result or value array. If there is a subarray, there are three iterators prepared. One for the indexing arrays, one for the result or value array (minus its subarray), and one for the subarrays of the original and the result/assignment array. The first two iterators give (or allow calculation) of the pointers into the start of the subarray, which then allows restarting the subarray iteration.", "When advanced indices are next to each other transposing may be necessary. All necessary transposing is handled by PyArray_MapIterSwapAxes and has to be handled by the caller unless PyArray_MapIterNew is asked to allocate the result.", "After preparation, getting and setting are relatively straightforward, although the different modes of iteration need to be considered. Unless there is only a single indexing array during item getting, the validity of the indices is checked beforehand. Otherwise, it is handled in the inner loop itself for optimization.", "See also", "Universal functions (ufunc), Universal functions (ufunc) basics", "Universal functions are callable objects that take \\(N\\) inputs and produce \\(M\\) outputs by wrapping basic 1-D loops that work element-by-element into full easy-to-use functions that seamlessly implement broadcasting, type-checking, buffered coercion, and output-argument handling. New universal functions are normally created in C, although there is a mechanism for creating ufuncs from Python functions (frompyfunc). The user must supply a 1-D loop that implements the basic function taking the input scalar values and placing the resulting scalars into the appropriate output slots as explained in implementation.", "Every ufunc calculation involves some overhead related to setting up the calculation. The practical significance of this overhead is that even though the actual calculation of the ufunc is very fast, you will be able to write array and type-specific code that will work faster for small arrays than the ufunc. In particular, using ufuncs to perform many calculations on 0-D arrays will be slower than other Python-based solutions (the silently-imported scalarmath module exists precisely to give array scalars the look-and-feel of ufunc based calculations with significantly reduced overhead).", "When a ufunc is called, many things must be done. The information collected from these setup operations is stored in a loop object. This loop object is a C-structure (that could become a Python object but is not initialized as such because it is only used internally). This loop object has the layout needed to be used with PyArray_Broadcast so that the broadcasting can be handled in the same way as it is handled in other sections of code.", "The first thing done is to look up in the thread-specific global dictionary the current values for the buffer-size, the error mask, and the associated error object. The state of the error mask controls what happens when an error condition is found. It should be noted that checking of the hardware error flags is only performed after each 1-D loop is executed. This means that if the input and output arrays are contiguous and of the correct type so that a single 1-D loop is performed, then the flags may not be checked until all elements of the array have been calculated. Looking up these values in a thread-specific dictionary takes time which is easily ignored for all but very small arrays.", "After checking, the thread-specific global variables, the inputs are evaluated to determine how the ufunc should proceed and the input and output arrays are constructed if necessary. Any inputs which are not arrays are converted to arrays (using context if necessary). Which of the inputs are scalars (and therefore converted to 0-D arrays) is noted.", "Next, an appropriate 1-D loop is selected from the 1-D loops available to the ufunc based on the input array types. This 1-D loop is selected by trying to match the signature of the datatypes of the inputs against the available signatures. The signatures corresponding to built-in types are stored in the ufunc.types member of the ufunc structure. The signatures corresponding to user-defined types are stored in a linked list of function information with the head element stored as a CObject in the userloops dictionary keyed by the datatype number (the first user-defined type in the argument list is used as the key). The signatures are searched until a signature is found to which the input arrays can all be cast safely (ignoring any scalar arguments which are not allowed to determine the type of the result). The implication of this search procedure is that \u201clesser types\u201d should be placed below \u201clarger types\u201d when the signatures are stored. If no 1-D loop is found, then an error is reported. Otherwise, the argument_list is updated with the stored signature \u2014 in case casting is necessary and to fix the output types assumed by the 1-D loop.", "If the ufunc has 2 inputs and 1 output and the second input is an Object array then a special-case check is performed so that NotImplemented is returned if the second input is not an ndarray, has the __array_priority__ attribute, and has an __r{op}__ special method. In this way, Python is signaled to give the other object a chance to complete the operation instead of using generic object-array calculations. This allows (for example) sparse matrices to override the multiplication operator 1-D loop.", "For input arrays that are smaller than the specified buffer size, copies are made of all non-contiguous, misaligned, or out-of-byteorder arrays to ensure that for small arrays, a single loop is used. Then, array iterators are created for all the input arrays and the resulting collection of iterators is broadcast to a single shape.", "The output arguments (if any) are then processed and any missing return arrays are constructed. If any provided output array doesn\u2019t have the correct type (or is misaligned) and is smaller than the buffer size, then a new output array is constructed with the special NPY_ARRAY_WRITEBACKIFCOPY flag set. At the end of the function, PyArray_ResolveWritebackIfCopy is called so that its contents will be copied back into the output array. Iterators for the output arguments are then processed.", "Finally, the decision is made about how to execute the looping mechanism to ensure that all elements of the input arrays are combined to produce the output arrays of the correct type. The options for loop execution are one-loop (for :term`contiguous`, aligned, and correct data type), strided-loop (for non-contiguous but still aligned and correct data type), and a buffered loop (for misaligned or incorrect data type situations). Depending on which execution method is called for, the loop is then set up and computed.", "This section describes how the basic universal function computation loop is set up and executed for each of the three different kinds of execution. If NPY_ALLOW_THREADS is defined during compilation, then as long as no object arrays are involved, the Python Global Interpreter Lock (GIL) is released prior to calling the loops. It is re-acquired if necessary to handle error conditions. The hardware error flags are checked only after the 1-D loop is completed.", "This is the simplest case of all. The ufunc is executed by calling the underlying 1-D loop exactly once. This is possible only when we have aligned data of the correct type (including byteorder) for both input and output and all arrays have uniform strides (either contiguous, 0-D, or 1-D). In this case, the 1-D computational loop is called once to compute the calculation for the entire array. Note that the hardware error flags are only checked after the entire calculation is complete.", "When the input and output arrays are aligned and of the correct type, but the striding is not uniform (non-contiguous and 2-D or larger), then a second looping structure is employed for the calculation. This approach converts all of the iterators for the input and output arguments to iterate over all but the largest dimension. The inner loop is then handled by the underlying 1-D computational loop. The outer loop is a standard iterator loop on the converted iterators. The hardware error flags are checked after each 1-D loop is completed.", "This is the code that handles the situation whenever the input and/or output arrays are either misaligned or of the wrong datatype (including being byteswapped) from what the underlying 1-D loop expects. The arrays are also assumed to be non-contiguous. The code works very much like the strided-loop except for the inner 1-D loop is modified so that pre-processing is performed on the inputs and post-processing is performed on the outputs in bufsize chunks (where bufsize is a user-settable parameter). The underlying 1-D computational loop is called on data that is copied over (if it needs to be). The setup code and the loop code is considerably more complicated in this case because it has to handle:", "Again, the hardware error flags are checked at the end of each 1-D loop.", "Ufuncs allow other array-like classes to be passed seamlessly through the interface in that inputs of a particular class will induce the outputs to be of that same class. The mechanism by which this works is the following. If any of the inputs are not ndarrays and define the __array_wrap__ method, then the class with the largest __array_priority__ attribute determines the type of all the outputs (with the exception of any output arrays passed in). The __array_wrap__ method of the input array will be called with the ndarray being returned from the ufunc as its input. There are two calling styles of the __array_wrap__ function supported. The first takes the ndarray as the first argument and a tuple of \u201ccontext\u201d as the second argument. The context is (ufunc, arguments, output argument number). This is the first call tried. If a TypeError occurs, then the function is called with just the ndarray as the first argument.", "There are three methods of ufuncs that require calculation similar to the general-purpose ufuncs. These are ufunc.reduce, ufunc.accumulate, and ufunc.reduceat. Each of these methods requires a setup command followed by a loop. There are four loop styles possible for the methods corresponding to no-elements, one-element, strided-loop, and buffered-loop. These are the same basic loop styles as implemented for the general-purpose function call except for the no-element and one-element cases which are special-cases occurring when the input array objects have 0 and 1 elements respectively.", "The setup function for all three methods is construct_reduce. This function creates a reducing loop object and fills it with the parameters needed to complete the loop. All of the methods only work on ufuncs that take 2-inputs and return 1 output. Therefore, the underlying 1-D loop is selected assuming a signature of [otype,\notype, otype] where otype is the requested reduction datatype. The buffer size and error handling are then retrieved from (per-thread) global storage. For small arrays that are misaligned or have incorrect datatype, a copy is made so that the un-buffered section of code is used. Then, the looping strategy is selected. If there is 1 element or 0 elements in the array, then a simple looping method is selected. If the array is not misaligned and has the correct datatype, then strided looping is selected. Otherwise, buffered looping must be performed. Looping parameters are then established, and the return array is constructed. The output array is of a different shape depending on whether the method is reduce, accumulate, or reduceat. If an output array is already provided, then its shape is checked. If the output array is not C-contiguous, aligned, and of the correct data type, then a temporary copy is made with the NPY_ARRAY_WRITEBACKIFCOPY flag set. In this way, the methods will be able to work with a well-behaved output array but the result will be copied back into the true output array when PyArray_ResolveWritebackIfCopy is called at function completion. Finally, iterators are set up to loop over the correct axis (depending on the value of axis provided to the method) and the setup routine returns to the actual computation routine.", "All of the ufunc methods use the same underlying 1-D computational loops with input and output arguments adjusted so that the appropriate reduction takes place. For example, the key to the functioning of reduce is that the 1-D loop is called with the output and the second input pointing to the same position in memory and both having a step-size of 0. The first input is pointing to the input array with a step-size given by the appropriate stride for the selected axis. In this way, the operation performed is", "where \\(N+1\\) is the number of elements in the input, \\(i\\), \\(o\\) is the output, and \\(i[k]\\) is the \\(k^{\\textrm{th}}\\) element of \\(i\\) along the selected axis. This basic operation is repeated for arrays with greater than 1 dimension so that the reduction takes place for every 1-D sub-array along the selected axis. An iterator with the selected dimension removed handles this looping.", "For buffered loops, care must be taken to copy and cast data before the loop function is called because the underlying loop expects aligned data of the correct datatype (including byteorder). The buffered loop must handle this copying and casting prior to calling the loop function on chunks no greater than the user-specified bufsize.", "The accumulate method is very similar to the reduce method in that the output and the second input both point to the output. The difference is that the second input points to memory one stride behind the current output pointer. Thus, the operation performed is", "The output has the same shape as the input and each 1-D loop operates over \\(N\\) elements when the shape in the selected axis is \\(N+1\\). Again, buffered loops take care to copy and cast the data before calling the underlying 1-D computational loop.", "The reduceat function is a generalization of both the reduce and accumulate functions. It implements a reduce over ranges of the input array specified by indices. The extra indices argument is checked to be sure that every input is not too large for the input array along the selected dimension before the loop calculations take place. The loop implementation is handled using code that is very similar to the reduce code repeated as many times as there are elements in the indices input. In particular: the first input pointer passed to the underlying 1-D computational loop points to the input array at the correct location indicated by the index array. In addition, the output pointer and the second input pointer passed to the underlying 1-D loop point to the same position in memory. The size of the 1-D computational loop is fixed to be the difference between the current index and the next index (when the current index is the last index, then the next index is assumed to be the length of the array along the selected dimension). In this way, the 1-D loop will implement a reduce over the specified indices.", "Misaligned or a loop datatype that does not match the input and/or output datatype is handled using buffered code wherein data is copied to a temporary buffer and cast to the correct datatype if necessary prior to calling the underlying 1-D function. The temporary buffers are created in (element) sizes no bigger than the user settable buffer-size value. Thus, the loop must be flexible enough to call the underlying 1-D computational loop enough times to complete the total calculation in chunks no bigger than the buffer-size."]}, {"name": "NumPy C-API", "path": "reference/c-api/index", "type": "NumPy C-API", "text": ["NumPy provides a C-API to enable users to extend the system and get access to the array object for use in other routines. The best way to truly understand the C-API is to read the source code. If you are unfamiliar with (C) source code, however, this can be a daunting experience at first. Be assured that the task becomes easier with practice, and you may be surprised at how simple the C-code can be to understand. Even if you don\u2019t think you can write C-code from scratch, it is much easier to understand and modify already-written source code than create it de novo.", "Python extensions are especially straightforward to understand because they all have a very similar structure. Admittedly, NumPy is not a trivial extension to Python, and may take a little more snooping to grasp. This is especially true because of the code-generation techniques, which simplify maintenance of very similar code, but can make the code a little less readable to beginners. Still, with a little persistence, the code can be opened to your understanding. It is my hope, that this guide to the C-API can assist in the process of becoming familiar with the compiled-level work that can be done with NumPy in order to squeeze that last bit of necessary speed out of your code."]}, {"name": "NumPy for MATLAB users", "path": "user/numpy-for-matlab-users", "type": "User Guide", "text": ["MATLAB\u00ae and NumPy have a lot in common, but NumPy was created to work with Python, not to be a MATLAB clone. This guide will help MATLAB users get started with NumPy.", "In MATLAB, the basic type, even for scalars, is a multidimensional array. Array assignments in MATLAB are stored as 2D arrays of double precision floating point numbers, unless you specify the number of dimensions and type. Operations on the 2D instances of these arrays are modeled on matrix operations in linear algebra.", "In NumPy, the basic type is a multidimensional array. Array assignments in NumPy are usually stored as n-dimensional arrays with the minimum type required to hold the objects in sequence, unless you specify the number of dimensions and type. NumPy performs operations element-by-element, so multiplying 2D arrays with * is not a matrix multiplication \u2013 it\u2019s an element-by-element multiplication. (The @ operator, available since Python 3.5, can be used for conventional matrix multiplication.)", "MATLAB numbers indices from 1; a(1) is the first element. See note INDEXING", "NumPy, like Python, numbers indices from 0; a[0] is the first element.", "MATLAB\u2019s scripting language was created for linear algebra so the syntax for some array manipulations is more compact than NumPy\u2019s. On the other hand, the API for adding GUIs and creating full-fledged applications is more or less an afterthought.", "NumPy is based on Python, a general-purpose language. The advantage to NumPy is access to Python libraries including: SciPy, Matplotlib, Pandas, OpenCV, and more. In addition, Python is often embedded as a scripting language in other software, allowing NumPy to be used there too.", "MATLAB array slicing uses pass-by-value semantics, with a lazy copy-on-write scheme to prevent creating copies until they are needed. Slicing operations copy parts of the array.", "NumPy array slicing uses pass-by-reference, that does not copy the arguments. Slicing operations are views into an array.", "The table below gives rough equivalents for some common MATLAB expressions. These are similar expressions, not equivalents. For details, see the documentation.", "In the table below, it is assumed that you have executed the following commands in Python:", "Also assume below that if the Notes talk about \u201cmatrix\u201d that the arguments are two-dimensional entities.", "MATLAB", "NumPy", "Notes", "help func", "info(func) or help(func) or func? (in IPython)", "get help on the function func", "which func", "see note HELP", "find out where func is defined", "type func", "np.source(func) or func?? (in IPython)", "print source for func (if not a native function)", "% comment", "# comment", "comment a line of code with the text comment", "use a for-loop to print the numbers 1, 2, and 3 using range", "a && b", "a and b", "short-circuiting logical AND operator (Python native operator); scalar arguments only", "a || b", "a or b", "short-circuiting logical OR operator (Python native operator); scalar arguments only", "The boolean objects in Python are True and False, as opposed to MATLAB logical types of 1 and 0.", "create an if-else statement to check if a is 4 or 5 and print result", "1*i, 1*j, 1i, 1j", "1j", "complex numbers", "eps", "np.finfo(float).eps or np.spacing(1)", "Upper bound to relative error due to rounding in 64-bit floating point arithmetic.", "load data.mat", "io.loadmat('data.mat')", "Load MATLAB variables saved to the file data.mat. (Note: When saving arrays to data.mat in MATLAB/Octave, use a recent binary format. scipy.io.loadmat will create a dictionary with the saved arrays and further information.)", "ode45", "integrate.solve_ivp(f)", "integrate an ODE with Runge-Kutta 4,5", "ode15s", "integrate.solve_ivp(f, method='BDF')", "integrate an ODE with BDF method", "MATLAB", "NumPy", "Notes", "ndims(a)", "np.ndim(a) or a.ndim", "number of dimensions of array a", "numel(a)", "np.size(a) or a.size", "number of elements of array a", "size(a)", "np.shape(a) or a.shape", "\u201csize\u201d of array a", "size(a,n)", "a.shape[n-1]", "get the number of elements of the n-th dimension of array a. (Note that MATLAB uses 1 based indexing while Python uses 0 based indexing, See note INDEXING)", "[ 1 2 3; 4 5 6 ]", "np.array([[1. ,2. ,3.], [4. ,5. ,6.]])", "define a 2x3 2D array", "[ a b; c d ]", "np.block([[a, b], [c, d]])", "construct a matrix from blocks a, b, c, and d", "a(end)", "a[-1]", "access last element in MATLAB vector (1xn or nx1) or 1D NumPy array a (length n)", "a(2,5)", "a[1, 4]", "access element in second row, fifth column in 2D array a", "a(2,:)", "a[1] or a[1, :]", "entire second row of 2D array a", "a(1:5,:)", "a[0:5] or a[:5] or a[0:5, :]", "first 5 rows of 2D array a", "a(end-4:end,:)", "a[-5:]", "last 5 rows of 2D array a", "a(1:3,5:9)", "a[0:3, 4:9]", "The first through third rows and fifth through ninth columns of a 2D array, a.", "a([2,4,5],[1,3])", "a[np.ix_([1, 3, 4], [0, 2])]", "rows 2,4 and 5 and columns 1 and 3. This allows the matrix to be modified, and doesn\u2019t require a regular slice.", "a(3:2:21,:)", "a[2:21:2,:]", "every other row of a, starting with the third and going to the twenty-first", "a(1:2:end,:)", "a[ ::2,:]", "every other row of a, starting with the first", "a(end:-1:1,:) or flipud(a)", "a[::-1,:]", "a with rows in reverse order", "a([1:end 1],:)", "a[np.r_[:len(a),0]]", "a with copy of the first row appended to the end", "a.'", "a.transpose() or a.T", "transpose of a", "a'", "a.conj().transpose() or a.conj().T", "conjugate transpose of a", "a * b", "a @ b", "matrix multiply", "a .* b", "a * b", "element-wise multiply", "a./b", "a/b", "element-wise divide", "a.^3", "a**3", "element-wise exponentiation", "(a > 0.5)", "(a > 0.5)", "matrix whose i,jth element is (a_ij > 0.5). The MATLAB result is an array of logical values 0 and 1. The NumPy result is an array of the boolean values False and True.", "find(a > 0.5)", "np.nonzero(a > 0.5)", "find the indices where (a > 0.5)", "a(:,find(v > 0.5))", "a[:,np.nonzero(v > 0.5)[0]]", "extract the columns of a where vector v > 0.5", "a(:,find(v>0.5))", "a[:, v.T > 0.5]", "extract the columns of a where column vector v > 0.5", "a(a<0.5)=0", "a[a < 0.5]=0", "a with elements less than 0.5 zeroed out", "a .* (a>0.5)", "a * (a > 0.5)", "a with elements less than 0.5 zeroed out", "a(:) = 3", "a[:] = 3", "set all values to the same scalar value", "y=x", "y = x.copy()", "NumPy assigns by reference", "y=x(2,:)", "y = x[1, :].copy()", "NumPy slices are by reference", "y=x(:)", "y = x.flatten()", "turn array into vector (note that this forces a copy). To obtain the same data ordering as in MATLAB, use x.flatten('F').", "1:10", "np.arange(1., 11.) or np.r_[1.:11.] or np.r_[1:10:10j]", "create an increasing vector (see note RANGES)", "0:9", "np.arange(10.) or np.r_[:10.] or np.r_[:9:10j]", "create an increasing vector (see note RANGES)", "[1:10]'", "np.arange(1.,11.)[:, np.newaxis]", "create a column vector", "zeros(3,4)", "np.zeros((3, 4))", "3x4 two-dimensional array full of 64-bit floating point zeros", "zeros(3,4,5)", "np.zeros((3, 4, 5))", "3x4x5 three-dimensional array full of 64-bit floating point zeros", "ones(3,4)", "np.ones((3, 4))", "3x4 two-dimensional array full of 64-bit floating point ones", "eye(3)", "np.eye(3)", "3x3 identity matrix", "diag(a)", "np.diag(a)", "returns a vector of the diagonal elements of 2D array, a", "diag(v,0)", "np.diag(v, 0)", "returns a square diagonal matrix whose nonzero values are the elements of vector, v", "or older version: random.rand((3, 4))", "generate a random 3x4 array with default random number generator and seed = 42", "linspace(1,3,4)", "np.linspace(1,3,4)", "4 equally spaced samples between 1 and 3, inclusive", "[x,y]=meshgrid(0:8,0:5)", "np.mgrid[0:9.,0:6.] or np.meshgrid(r_[0:9.],r_[0:6.]", "two 2D arrays: one of x values, the other of y values", "ogrid[0:9.,0:6.] or np.ix_(np.r_[0:9.],np.r_[0:6.]", "the best way to eval functions on a grid", "[x,y]=meshgrid([1,2,4],[2,4,5])", "np.meshgrid([1,2,4],[2,4,5])", "ix_([1,2,4],[2,4,5])", "the best way to eval functions on a grid", "repmat(a, m, n)", "np.tile(a, (m, n))", "create m by n copies of a", "[a b]", "np.concatenate((a,b),1) or np.hstack((a,b)) or np.column_stack((a,b)) or np.c_[a,b]", "concatenate columns of a and b", "[a; b]", "np.concatenate((a,b)) or np.vstack((a,b)) or np.r_[a,b]", "concatenate rows of a and b", "max(max(a))", "a.max() or np.nanmax(a)", "maximum element of a (with ndims(a)<=2 for MATLAB, if there are NaN\u2019s, nanmax will ignore these and return largest value)", "max(a)", "a.max(0)", "maximum element of each column of array a", "max(a,[],2)", "a.max(1)", "maximum element of each row of array a", "max(a,b)", "np.maximum(a, b)", "compares a and b element-wise, and returns the maximum value from each pair", "norm(v)", "np.sqrt(v @ v) or np.linalg.norm(v)", "L2 norm of vector v", "a & b", "logical_and(a,b)", "element-by-element AND operator (NumPy ufunc) See note LOGICOPS", "a | b", "np.logical_or(a,b)", "element-by-element OR operator (NumPy ufunc) See note LOGICOPS", "bitand(a,b)", "a & b", "bitwise AND operator (Python native and NumPy ufunc)", "bitor(a,b)", "a | b", "bitwise OR operator (Python native and NumPy ufunc)", "inv(a)", "linalg.inv(a)", "inverse of square 2D array a", "pinv(a)", "linalg.pinv(a)", "pseudo-inverse of 2D array a", "rank(a)", "linalg.matrix_rank(a)", "matrix rank of a 2D array a", "a\\b", "linalg.solve(a, b) if a is square; linalg.lstsq(a, b) otherwise", "solution of a x = b for x", "b/a", "Solve a.T x.T = b.T instead", "solution of x a = b for x", "[U,S,V]=svd(a)", "U, S, Vh = linalg.svd(a), V = Vh.T", "singular value decomposition of a", "c=chol(a) where a==c'*c", "c = linalg.cholesky(a) where a == c@c.T", "Cholesky factorization of a 2D array (chol(a) in MATLAB returns an upper triangular 2D array, but cholesky returns a lower triangular 2D array)", "[V,D]=eig(a)", "D,V = linalg.eig(a)", "eigenvalues \\(\\lambda\\) and eigenvectors \\(\\bar{v}\\) of a, where \\(\\lambda\\bar{v}=\\mathbf{a}\\bar{v}\\)", "[V,D]=eig(a,b)", "D,V = linalg.eig(a, b)", "eigenvalues \\(\\lambda\\) and eigenvectors \\(\\bar{v}\\) of a, b where \\(\\lambda\\mathbf{b}\\bar{v}=\\mathbf{a}\\bar{v}\\)", "[V,D]=eigs(a,3)", "D,V = eigs(a, k = 3)", "find the k=3 largest eigenvalues and eigenvectors of 2D array, a", "[Q,R,P]=qr(a,0)", "Q,R = linalg.qr(a)", "QR decomposition", "[L,U,P]=lu(a) where a==P'*L*U", "P,L,U = linalg.lu(a) where a == P@L@U", "LU decomposition (note: P(MATLAB) == transpose(P(NumPy)))", "conjgrad", "cg", "Conjugate gradients solver", "fft(a)", "np.fft(a)", "Fourier transform of a", "ifft(a)", "np.ifft(a)", "inverse Fourier transform of a", "sort(a)", "np.sort(a) or a.sort(axis=0)", "sort each column of a 2D array, a", "sort(a, 2)", "np.sort(a, axis = 1) or a.sort(axis = 1)", "sort the each row of 2D array, a", "[b,I]=sortrows(a,1)", "I = np.argsort(a[:, 0]); b = a[I,:]", "save the array a as array b with rows sorted by the first column", "x = Z\\y", "x = linalg.lstsq(Z, y)", "perform a linear regression of the form \\(\\mathbf{Zx}=\\mathbf{y}\\)", "decimate(x, q)", "signal.resample(x, np.ceil(len(x)/q))", "downsample with low-pass filtering", "unique(a)", "np.unique(a)", "a vector of unique values in array a", "squeeze(a)", "a.squeeze()", "remove singleton dimensions of array a. Note that MATLAB will always return arrays of 2D or higher while NumPy will return arrays of 0D or higher", "Submatrix: Assignment to a submatrix can be done with lists of indices using the ix_ command. E.g., for 2D array a, one might do: ind=[1, 3];\u00a0a[np.ix_(ind, ind)] += 100.", "HELP: There is no direct equivalent of MATLAB\u2019s which command, but the commands help and numpy.source will usually list the filename where the function is located. Python also has an inspect module (do import\u00a0inspect) which provides a getfile that often works.", "INDEXING: MATLAB uses one based indexing, so the initial element of a sequence has index 1. Python uses zero based indexing, so the initial element of a sequence has index 0. Confusion and flamewars arise because each has advantages and disadvantages. One based indexing is consistent with common human language usage, where the \u201cfirst\u201d element of a sequence has index 1. Zero based indexing simplifies indexing. See also a text by prof.dr. Edsger W. Dijkstra.", "RANGES: In MATLAB, 0:5 can be used as both a range literal and a \u2018slice\u2019 index (inside parentheses); however, in Python, constructs like 0:5 can only be used as a slice index (inside square brackets). Thus the somewhat quirky r_ object was created to allow NumPy to have a similarly terse range construction mechanism. Note that r_ is not called like a function or a constructor, but rather indexed using square brackets, which allows the use of Python\u2019s slice syntax in the arguments.", "LOGICOPS: & or | in NumPy is bitwise AND/OR, while in MATLAB & and | are logical AND/OR. The two can appear to work the same, but there are important differences. If you would have used MATLAB\u2019s & or | operators, you should use the NumPy ufuncs logical_and/logical_or. The notable differences between MATLAB\u2019s and NumPy\u2019s & and | operators are:", "If you know you have boolean arguments, you can get away with using NumPy\u2019s bitwise operators, but be careful with parentheses, like this: z\n= (x > 1) & (x < 2). The absence of NumPy operator forms of logical_and and logical_or is an unfortunate consequence of Python\u2019s design.", "RESHAPE and LINEAR INDEXING: MATLAB always allows multi-dimensional arrays to be accessed using scalar or linear indices, NumPy does not. Linear indices are common in MATLAB programs, e.g. find() on a matrix returns them, whereas NumPy\u2019s find behaves differently. When converting MATLAB code it might be necessary to first reshape a matrix to a linear sequence, perform some indexing operations and then reshape back. As reshape (usually) produces views onto the same storage, it should be possible to do this fairly efficiently. Note that the scan order used by reshape in NumPy defaults to the \u2018C\u2019 order, whereas MATLAB uses the Fortran order. If you are simply converting to a linear sequence and back this doesn\u2019t matter. But if you are converting reshapes from MATLAB code which relies on the scan order, then this MATLAB code: z =\nreshape(x,3,4); should become z = x.reshape(3,4,order='F').copy() in NumPy.", "Historically, NumPy has provided a special matrix type, np.matrix, which is a subclass of ndarray which makes binary operations linear algebra operations. You may see it used in some existing code instead of np.array. So, which one to use?", "Use arrays.", "Until Python 3.5 the only disadvantage of using the array type was that you had to use dot instead of * to multiply (reduce) two tensors (scalar product, matrix vector multiplication etc.). Since Python 3.5 you can use the matrix multiplication @ operator.", "Given the above, we intend to deprecate matrix eventually.", "NumPy contains both an array class and a matrix class. The array class is intended to be a general-purpose n-dimensional array for many kinds of numerical computing, while matrix is intended to facilitate linear algebra computations specifically. In practice there are only a handful of key differences between the two.", "Operators * and @, functions dot(), and multiply():", "Handling of vectors (one-dimensional arrays)", "Handling of higher-dimensional arrays (ndim > 2)", "Convenience attributes", "Convenience constructor", "There are pros and cons to using both:", "array", "matrix", "The array is thus much more advisable to use. Indeed, we intend to deprecate matrix eventually.", "In MATLAB the main tool available to you for customizing the environment is to modify the search path with the locations of your favorite functions. You can put such customizations into a startup script that MATLAB will run on startup.", "NumPy, or rather Python, has similar facilities.", "Unlike MATLAB, where anything on your path can be called immediately, with Python you need to first do an \u2018import\u2019 statement to make functions in a particular file accessible.", "For example you might make a startup script that looks like this (Note: this is just an example, not a statement of \u201cbest practices\u201d):", "To use the deprecated matrix and other matlib functions:", "Another somewhat outdated MATLAB/NumPy cross-reference can be found at http://mathesaurus.sf.net/", "An extensive list of tools for scientific work with Python can be found in the topical software page.", "See List of Python software: scripting for a list of software that use Python as a scripting language", "MATLAB\u00ae and SimuLink\u00ae are registered trademarks of The MathWorks, Inc."]}, {"name": "NumPy fundamentals", "path": "user/basics", "type": "User Guide", "text": ["These documents clarify concepts, design decisions, and technical constraints in NumPy. This is a great place to understand the fundamental NumPy ideas and philosophy."]}, {"name": "NumPy governance", "path": "dev/governance/index", "type": "Development", "text": []}, {"name": "NumPy How Tos", "path": "user/howtos_index", "type": "User Guide", "text": ["These documents are intended as recipes to common tasks using NumPy. For detailed reference documentation of the functions and classes contained in the package, see the API reference."]}, {"name": "NumPy license", "path": "license", "type": "NumPy license", "text": []}, {"name": "NumPy project governance and decision-making", "path": "dev/governance/governance", "type": "Development", "text": ["The purpose of this document is to formalize the governance process used by the NumPy project in both ordinary and extraordinary situations, and to clarify how decisions are made and how the various elements of our community interact, including the relationship between open source collaborative development and work that may be funded by for-profit or non-profit entities.", "NumPy is a community-owned and community-run project. To the maximum extent possible, decisions about project direction are made by community consensus (but note that \u201cconsensus\u201d here has a somewhat technical meaning that might not match everyone\u2019s expectations \u2013 see below). Some members of the community additionally contribute by serving on the NumPy steering council, where they are responsible for facilitating the establishment of community consensus, for stewarding project resources, and \u2013 in extreme cases \u2013 for making project decisions if the normal community-based process breaks down.", "The NumPy Project (The Project) is an open source software project affiliated with the 501(c)3 NumFOCUS Foundation. The goal of The Project is to develop open source software for array-based computing in Python, and in particular the numpy package, along with related software such as f2py and the NumPy Sphinx extensions. The Software developed by The Project is released under the BSD (or similar) open source license, developed openly and hosted on public GitHub repositories under the numpy GitHub organization.", "The Project is developed by a team of distributed developers, called Contributors. Contributors are individuals who have contributed code, documentation, designs or other work to the Project. Anyone can be a Contributor. Contributors can be affiliated with any legal entity or none. Contributors participate in the project by submitting, reviewing and discussing GitHub Pull Requests and Issues and participating in open and public Project discussions on GitHub, mailing lists, and other channels. The foundation of Project participation is openness and transparency.", "The Project Community consists of all Contributors and Users of the Project. Contributors work on behalf of and are responsible to the larger Project Community and we strive to keep the barrier between Contributors and Users as low as possible.", "The Project is formally affiliated with the 501(c)3 NumFOCUS Foundation (http://numfocus.org), which serves as its fiscal sponsor, may hold project trademarks and other intellectual property, helps manage project donations and acts as a parent legal entity. NumFOCUS is the only legal entity that has a formal relationship with the project (see Institutional Partners section below).", "This section describes the governance and leadership model of The Project.", "The foundations of Project governance are:", "Normally, all project decisions will be made by consensus of all interested Contributors. The primary goal of this approach is to ensure that the people who are most affected by and involved in any given change can contribute their knowledge in the confidence that their voices will be heard, because thoughtful review from a broad community is the best mechanism we know of for creating high-quality software.", "The mechanism we use to accomplish this goal may be unfamiliar for those who are not experienced with the cultural norms around free/open-source software development. We provide a summary here, and highly recommend that all Contributors additionally read Chapter 4: Social and Political Infrastructure of Karl Fogel\u2019s classic Producing Open Source Software, and in particular the section on Consensus-based Democracy, for a more detailed discussion.", "In this context, consensus does not require:", "For us, what consensus means is that we entrust everyone with the right to veto any change if they feel it necessary. While this may sound like a recipe for obstruction and pain, this is not what happens. Instead, we find that most people take this responsibility seriously, and only invoke their veto when they judge that a serious problem is being ignored, and that their veto is necessary to protect the project. And in practice, it turns out that such vetoes are almost never formally invoked, because their mere possibility ensures that Contributors are motivated from the start to find some solution that everyone can live with \u2013 thus accomplishing our goal of ensuring that all interested perspectives are taken into account.", "How do we know when consensus has been achieved? In principle, this is rather difficult, since consensus is defined by the absence of vetos, which requires us to somehow prove a negative. In practice, we use a combination of our best judgement (e.g., a simple and uncontroversial bug fix posted on GitHub and reviewed by a core developer is probably fine) and best efforts (e.g., all substantive API changes must be posted to the mailing list in order to give the broader community a chance to catch any problems and suggest improvements; we assume that anyone who cares enough about NumPy to invoke their veto right should be on the mailing list). If no-one bothers to comment on the mailing list after a few days, then it\u2019s probably fine. And worst case, if a change is more controversial than expected, or a crucial critique is delayed because someone was on vacation, then it\u2019s no big deal: we apologize for misjudging the situation, back up, and sort things out.", "If one does need to invoke a formal veto, then it should consist of:", "If all proposals for resolving some issue are vetoed, then the status quo wins by default.", "In the worst case, if a Contributor is genuinely misusing their veto in an obstructive fashion to the detriment of the project, then they can be ejected from the project by consensus of the Steering Council \u2013 see below.", "The Project will have a Steering Council that consists of Project Contributors who have produced contributions that are substantial in quality and quantity, and sustained over at least one year. The overall role of the Council is to ensure, with input from the Community, the long-term well-being of the project, both technically and as a community.", "During the everyday project activities, council members participate in all discussions, code review and other project activities as peers with all other Contributors and the Community. In these everyday activities, Council Members do not have any special power or privilege through their membership on the Council. However, it is expected that because of the quality and quantity of their contributions and their expert knowledge of the Project Software and Services that Council Members will provide useful guidance, both technical and in terms of project direction, to potentially less experienced contributors.", "The Steering Council and its Members play a special role in certain situations. In particular, the Council may, if necessary:", "However, the Council\u2019s primary responsibility is to facilitate the ordinary community-based decision making procedure described above. If we ever have to step in and formally override the community for the health of the Project, then we will do so, but we will consider reaching this point to indicate a failure in our leadership.", "If it becomes necessary for the Steering Council to produce a formal decision, then they will use a form of the Apache Foundation voting process. This is a formalized version of consensus, in which +1 votes indicate agreement, -1 votes are vetoes (and must be accompanied with a rationale, as above), and one can also vote fractionally (e.g. -0.5, +0.5) if one wishes to express an opinion without registering a full veto. These numeric votes are also often used informally as a way of getting a general sense of people\u2019s feelings on some issue, and should not normally be taken as formal votes. A formal vote only occurs if explicitly declared, and if this does occur then the vote should be held open for long enough to give all interested Council Members a chance to respond \u2013 at least one week.", "In practice, we anticipate that for most Steering Council decisions (e.g., voting in new members) a more informal process will suffice.", "A list of current Steering Council Members is maintained at the page About Us.", "To become eligible to join the Steering Council, an individual must be a Project Contributor who has produced contributions that are substantial in quality and quantity, and sustained over at least one year. Potential Council Members are nominated by existing Council members, and become members following consensus of the existing Council members, and confirmation that the potential Member is interested and willing to serve in that capacity. The Council will be initially formed from the set of existing Core Developers who, as of late 2015, have been significantly active over the last year.", "When considering potential Members, the Council will look at candidates with a comprehensive view of their contributions. This will include but is not limited to code, code review, infrastructure work, mailing list and chat participation, community help/building, education and outreach, design work, etc. We are deliberately not setting arbitrary quantitative metrics (like \u201c100 commits in this repo\u201d) to avoid encouraging behavior that plays to the metrics rather than the project\u2019s overall well-being. We want to encourage a diverse array of backgrounds, viewpoints and talents in our team, which is why we explicitly do not define code as the sole metric on which council membership will be evaluated.", "If a Council member becomes inactive in the project for a period of one year, they will be considered for removal from the Council. Before removal, inactive Member will be approached to see if they plan on returning to active participation. If not they will be removed immediately upon a Council vote. If they plan on returning to active participation soon, they will be given a grace period of one year. If they don\u2019t return to active participation within that time period they will be removed by vote of the Council without further grace period. All former Council members can be considered for membership again at any time in the future, like any other Project Contributor. Retired Council members will be listed on the project website, acknowledging the period during which they were active in the Council.", "The Council reserves the right to eject current Members, if they are deemed to be actively harmful to the project\u2019s well-being, and attempts at communication and conflict resolution have failed. This requires the consensus of the remaining Members.", "It is expected that the Council Members will be employed at a wide range of companies, universities and non-profit organizations. Because of this, it is possible that Members will have conflict of interests. Such conflict of interests include, but are not limited to:", "All members of the Council shall disclose to the rest of the Council any conflict of interest they may have. Members with a conflict of interest in a particular issue may participate in Council discussions on that issue, but must recuse themselves from voting on the issue.", "To the maximum extent possible, Council discussions and activities will be public and done in collaboration and discussion with the Project Contributors and Community. The Council will have a private mailing list that will be used sparingly and only when a specific matter requires privacy. When private communications and decisions are needed, the Council will do its best to summarize those to the Community after eliding personal/private/sensitive information that should not be posted to the public internet.", "The Council can create subcommittees that provide leadership and guidance for specific aspects of the project. Like the Council as a whole, subcommittees should conduct their business in an open and public manner unless privacy is specifically called for. Private subcommittee communications should happen on the main private mailing list of the Council unless specifically called for.", "The Council will maintain one narrowly focused subcommittee to manage its interactions with NumFOCUS.", "The current membership of the NumFOCUS Subcommittee is listed at the page About Us.", "The Steering Council are the primary leadership for the project. No outside institution, individual or legal entity has the ability to own, control, usurp or influence the project other than by participating in the Project as Contributors and Council Members. However, because institutions can be an important funding mechanism for the project, it is important to formally acknowledge institutional participation in the project. These are Institutional Partners.", "An Institutional Contributor is any individual Project Contributor who contributes to the project as part of their official duties at an Institutional Partner. Likewise, an Institutional Council Member is any Project Steering Council Member who contributes to the project as part of their official duties at an Institutional Partner.", "With these definitions, an Institutional Partner is any recognized legal entity in the United States or elsewhere that employs at least 1 Institutional Contributor of Institutional Council Member. Institutional Partners can be for-profit or non-profit entities.", "Institutions become eligible to become an Institutional Partner by employing individuals who actively contribute to The Project as part of their official duties. To state this another way, the only way for a Partner to influence the project is by actively contributing to the open development of the project, in equal terms to any other member of the community of Contributors and Council Members. Merely using Project Software in institutional context does not allow an entity to become an Institutional Partner. Financial gifts do not enable an entity to become an Institutional Partner. Once an institution becomes eligible for Institutional Partnership, the Steering Council must nominate and approve the Partnership.", "If at some point an existing Institutional Partner stops having any contributing employees, then a one year grace period commences. If at the end of this one year period they continue not to have any contributing employees, then their Institutional Partnership will lapse, and resuming it will require going through the normal process for new Partnerships.", "An Institutional Partner is free to pursue funding for their work on The Project through any legal means. This could involve a non-profit organization raising money from private foundations and donors or a for-profit company building proprietary products and services that leverage Project Software and Services. Funding acquired by Institutional Partners to work on The Project is called Institutional Funding. However, no funding obtained by an Institutional Partner can override the Steering Council. If a Partner has funding to do NumPy work and the Council decides to not pursue that work as a project, the Partner is free to pursue it on their own. However in this situation, that part of the Partner\u2019s work will not be under the NumPy umbrella and cannot use the Project trademarks in a way that suggests a formal relationship.", "Institutional Partner benefits are:", "A list of current Institutional Partners is maintained at the page About Us.", "https://github.com/numpy/numpy/commits/main/doc/source/dev/governance/governance.rst", "Substantial portions of this document were adapted from the Jupyter/IPython project\u2019s governance document", "To the extent possible under law, the authors have waived all copyright and related or neighboring rights to the NumPy project governance and decision-making document, as per the CC-0 public domain dedication / license."]}, {"name": "NumPy user guide", "path": "user/index", "type": "User Guide", "text": ["This guide is an overview and explains the important features; details are found in Command Reference."]}, {"name": "NumPy-specific help functions", "path": "reference/routines.help", "type": "NumPy-specific help functions", "text": ["lookfor(what[, module, import_modules, ...])", "Do a keyword search on docstrings.", "info([object, maxwidth, output, toplevel])", "Get help information for a function, class, or module.", "source(object[, output])", "Print or write to a file the source code for a NumPy object."]}, {"name": "numpy.absolute()", "path": "reference/generated/numpy.absolute", "type": "numpy.absolute", "text": ["Calculate the absolute value element-wise.", "np.abs is a shorthand for this function.", "Input array.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "An ndarray containing the absolute value of each element in x. For complex input, a + ib, the absolute value is \\(\\sqrt{ a^2 + b^2 }\\). This is a scalar if x is a scalar.", "Plot the function over [-10, 10]:", "Plot the function over the complex plane:", "The abs function can be used as a shorthand for np.absolute on ndarrays."]}, {"name": "numpy.add()", "path": "reference/generated/numpy.add", "type": "numpy.add", "text": ["Add arguments element-wise.", "The arrays to be added. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The sum of x1 and x2, element-wise. This is a scalar if both x1 and x2 are scalars.", "Equivalent to x1 + x2 in terms of array broadcasting.", "The + operator can be used as a shorthand for np.add on ndarrays."]}, {"name": "numpy.all()", "path": "reference/generated/numpy.all", "type": "numpy.all", "text": ["Test whether all array elements along a given axis evaluate to True.", "Input array or object that can be converted to an array.", "Axis or axes along which a logical AND reduction is performed. The default (axis=None) is to perform a logical AND over all the dimensions of the input array. axis may be negative, in which case it counts from the last to the first axis.", "New in version 1.7.0.", "If this is a tuple of ints, a reduction is performed on multiple axes, instead of a single axis or all the axes as before.", "Alternate output array in which to place the result. It must have the same shape as the expected output and its type is preserved (e.g., if dtype(out) is float, the result will consist of 0.0\u2019s and 1.0\u2019s). See Output type determination for more details.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.", "If the default value is passed, then keepdims will not be passed through to the all method of sub-classes of ndarray, however any non-default value will be. If the sub-class\u2019 method does not implement keepdims any exceptions will be raised.", "Elements to include in checking for all True values. See reduce for details.", "New in version 1.20.0.", "A new boolean or array is returned unless out is specified, in which case a reference to out is returned.", "See also", "equivalent method", "Test whether any element along a given axis evaluates to True.", "Not a Number (NaN), positive infinity and negative infinity evaluate to True because these are not equal to zero."]}, {"name": "numpy.allclose()", "path": "reference/generated/numpy.allclose", "type": "numpy.allclose", "text": ["Returns True if two arrays are element-wise equal within a tolerance.", "The tolerance values are positive, typically very small numbers. The relative difference (rtol * abs(b)) and the absolute difference atol are added together to compare against the absolute difference between a and b.", "NaNs are treated as equal if they are in the same place and if equal_nan=True. Infs are treated as equal if they are in the same place and of the same sign in both arrays.", "Input arrays to compare.", "The relative tolerance parameter (see Notes).", "The absolute tolerance parameter (see Notes).", "Whether to compare NaN\u2019s as equal. If True, NaN\u2019s in a will be considered equal to NaN\u2019s in b in the output array.", "New in version 1.10.0.", "Returns True if the two arrays are equal within the given tolerance; False otherwise.", "See also", "If the following equation is element-wise True, then allclose returns True.", "absolute(a - b) <= (atol + rtol * absolute(b))", "The above equation is not symmetric in a and b, so that allclose(a, b) might be different from allclose(b, a) in some rare cases.", "The comparison of a and b uses standard broadcasting, which means that a and b need not have the same shape in order for allclose(a, b) to evaluate to True. The same is true for equal but not array_equal.", "allclose is not defined for non-numeric data types. bool is considered a numeric data-type for this purpose."]}, {"name": "numpy.amax()", "path": "reference/generated/numpy.amax", "type": "numpy.amax", "text": ["Return the maximum of an array or maximum along an axis.", "Input data.", "Axis or axes along which to operate. By default, flattened input is used.", "New in version 1.7.0.", "If this is a tuple of ints, the maximum is selected over multiple axes, instead of a single axis or all the axes as before.", "Alternative output array in which to place the result. Must be of the same shape and buffer length as the expected output. See Output type determination for more details.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.", "If the default value is passed, then keepdims will not be passed through to the amax method of sub-classes of ndarray, however any non-default value will be. If the sub-class\u2019 method does not implement keepdims any exceptions will be raised.", "The minimum value of an output element. Must be present to allow computation on empty slice. See reduce for details.", "New in version 1.15.0.", "Elements to compare for the maximum. See reduce for details.", "New in version 1.17.0.", "Maximum of a. If axis is None, the result is a scalar value. If axis is given, the result is an array of dimension a.ndim - 1.", "See also", "The minimum value of an array along a given axis, propagating any NaNs.", "The maximum value of an array along a given axis, ignoring any NaNs.", "Element-wise maximum of two arrays, propagating any NaNs.", "Element-wise maximum of two arrays, ignoring any NaNs.", "Return the indices of the maximum values.", "NaN values are propagated, that is if at least one item is NaN, the corresponding max value will be NaN as well. To ignore NaN values (MATLAB behavior), please use nanmax.", "Don\u2019t use amax for element-wise comparison of 2 arrays; when a.shape[0] is 2, maximum(a[0], a[1]) is faster than amax(a, axis=0).", "You can use an initial value to compute the maximum of an empty slice, or to initialize it to a different value:", "Notice that the initial value is used as one of the elements for which the maximum is determined, unlike for the default argument Python\u2019s max function, which is only used for empty iterables."]}, {"name": "numpy.amin()", "path": "reference/generated/numpy.amin", "type": "numpy.amin", "text": ["Return the minimum of an array or minimum along an axis.", "Input data.", "Axis or axes along which to operate. By default, flattened input is used.", "New in version 1.7.0.", "If this is a tuple of ints, the minimum is selected over multiple axes, instead of a single axis or all the axes as before.", "Alternative output array in which to place the result. Must be of the same shape and buffer length as the expected output. See Output type determination for more details.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.", "If the default value is passed, then keepdims will not be passed through to the amin method of sub-classes of ndarray, however any non-default value will be. If the sub-class\u2019 method does not implement keepdims any exceptions will be raised.", "The maximum value of an output element. Must be present to allow computation on empty slice. See reduce for details.", "New in version 1.15.0.", "Elements to compare for the minimum. See reduce for details.", "New in version 1.17.0.", "Minimum of a. If axis is None, the result is a scalar value. If axis is given, the result is an array of dimension a.ndim - 1.", "See also", "The maximum value of an array along a given axis, propagating any NaNs.", "The minimum value of an array along a given axis, ignoring any NaNs.", "Element-wise minimum of two arrays, propagating any NaNs.", "Element-wise minimum of two arrays, ignoring any NaNs.", "Return the indices of the minimum values.", "NaN values are propagated, that is if at least one item is NaN, the corresponding min value will be NaN as well. To ignore NaN values (MATLAB behavior), please use nanmin.", "Don\u2019t use amin for element-wise comparison of 2 arrays; when a.shape[0] is 2, minimum(a[0], a[1]) is faster than amin(a, axis=0).", "Notice that the initial value is used as one of the elements for which the minimum is determined, unlike for the default argument Python\u2019s max function, which is only used for empty iterables.", "Notice that this isn\u2019t the same as Python\u2019s default argument."]}, {"name": "numpy.angle()", "path": "reference/generated/numpy.angle", "type": "numpy.angle", "text": ["Return the angle of the complex argument.", "A complex number or sequence of complex numbers.", "Return angle in degrees if True, radians if False (default).", "The counterclockwise angle from the positive real axis on the complex plane in the range (-pi, pi], with dtype as numpy.float64.", "Changed in version 1.16.0: This function works on subclasses of ndarray like ma.array.", "See also", "Although the angle of the complex number 0 is undefined, numpy.angle(0) returns the value 0."]}, {"name": "numpy.any()", "path": "reference/generated/numpy.any", "type": "numpy.any", "text": ["Test whether any array element along a given axis evaluates to True.", "Returns single boolean unless axis is not None", "Input array or object that can be converted to an array.", "Axis or axes along which a logical OR reduction is performed. The default (axis=None) is to perform a logical OR over all the dimensions of the input array. axis may be negative, in which case it counts from the last to the first axis.", "New in version 1.7.0.", "If this is a tuple of ints, a reduction is performed on multiple axes, instead of a single axis or all the axes as before.", "Alternate output array in which to place the result. It must have the same shape as the expected output and its type is preserved (e.g., if it is of type float, then it will remain so, returning 1.0 for True and 0.0 for False, regardless of the type of a). See Output type determination for more details.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.", "If the default value is passed, then keepdims will not be passed through to the any method of sub-classes of ndarray, however any non-default value will be. If the sub-class\u2019 method does not implement keepdims any exceptions will be raised.", "Elements to include in checking for any True values. See reduce for details.", "New in version 1.20.0.", "A new boolean or ndarray is returned unless out is specified, in which case a reference to out is returned.", "See also", "equivalent method", "Test whether all elements along a given axis evaluate to True.", "Not a Number (NaN), positive infinity and negative infinity evaluate to True because these are not equal to zero."]}, {"name": "numpy.append()", "path": "reference/generated/numpy.append", "type": "numpy.append", "text": ["Append values to the end of an array.", "Values are appended to a copy of this array.", "These values are appended to a copy of arr. It must be of the correct shape (the same shape as arr, excluding axis). If axis is not specified, values can be any shape and will be flattened before use.", "The axis along which values are appended. If axis is not given, both arr and values are flattened before use.", "A copy of arr with values appended to axis. Note that append does not occur in-place: a new array is allocated and filled. If axis is None, out is a flattened array.", "See also", "Insert elements into an array.", "Delete elements from an array.", "When axis is specified, values must have the correct shape."]}, {"name": "numpy.apply_along_axis()", "path": "reference/generated/numpy.apply_along_axis", "type": "numpy.apply_along_axis", "text": ["Apply a function to 1-D slices along the given axis.", "Execute func1d(a, *args, **kwargs) where func1d operates on 1-D arrays and a is a 1-D slice of arr along axis.", "This is equivalent to (but faster than) the following use of ndindex and s_, which sets each of ii, jj, and kk to a tuple of indices:", "Equivalently, eliminating the inner loop, this can be expressed as:", "This function should accept 1-D arrays. It is applied to 1-D slices of arr along the specified axis.", "Axis along which arr is sliced.", "Input array.", "Additional arguments to func1d.", "Additional named arguments to func1d.", "New in version 1.9.0.", "The output array. The shape of out is identical to the shape of arr, except along the axis dimension. This axis is removed, and replaced with new dimensions equal to the shape of the return value of func1d. So if func1d returns a scalar out will have one fewer dimensions than arr.", "See also", "Apply a function repeatedly over multiple axes.", "For a function that returns a 1D array, the number of dimensions in outarr is the same as arr.", "For a function that returns a higher dimensional array, those dimensions are inserted in place of the axis dimension."]}, {"name": "numpy.apply_over_axes()", "path": "reference/generated/numpy.apply_over_axes", "type": "numpy.apply_over_axes", "text": ["Apply a function repeatedly over multiple axes.", "func is called as res = func(a, axis), where axis is the first element of axes. The result res of the function call must have either the same dimensions as a or one less dimension. If res has one less dimension than a, a dimension is inserted before axis. The call to func is then repeated for each axis in axes, with res as the first argument.", "This function must take two arguments, func(a, axis).", "Input array.", "Axes over which func is applied; the elements must be integers.", "The output array. The number of dimensions is the same as a, but the shape can be different. This depends on whether func changes the shape of its output with respect to its input.", "See also", "Apply a function to 1-D slices of an array along the given axis.", "This function is equivalent to tuple axis arguments to reorderable ufuncs with keepdims=True. Tuple axis arguments to ufuncs have been available since version 1.7.0.", "Sum over axes 0 and 2. The result has same number of dimensions as the original array:", "Tuple axis arguments to ufuncs are equivalent:"]}, {"name": "numpy.arange()", "path": "reference/generated/numpy.arange", "type": "numpy.arange", "text": ["Return evenly spaced values within a given interval.", "Values are generated within the half-open interval [start, stop) (in other words, the interval including start but excluding stop). For integer arguments the function is equivalent to the Python built-in range function, but returns an ndarray rather than a list.", "When using a non-integer step, such as 0.1, it is often better to use numpy.linspace. See the warnings section below for more information.", "Start of interval. The interval includes this value. The default start value is 0.", "End of interval. The interval does not include this value, except in some cases where step is not an integer and floating point round-off affects the length of out.", "Spacing between values. For any output out, this is the distance between two adjacent values, out[i+1] - out[i]. The default step size is 1. If step is specified as a position argument, start must also be given.", "The type of the output array. If dtype is not given, infer the data type from the other input arguments.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "Array of evenly spaced values.", "For floating point arguments, the length of the result is ceil((stop - start)/step). Because of floating point overflow, this rule may result in the last element of out being greater than stop.", "Warning", "The length of the output might not be numerically stable.", "Another stability issue is due to the internal implementation of numpy.arange. The actual step value used to populate the array is dtype(start + step) - dtype(start) and not step. Precision loss can occur here, due to casting or due to using floating points when start is much larger than step. This can lead to unexpected behaviour. For example:", "In such cases, the use of numpy.linspace should be preferred.", "See also", "Evenly spaced numbers with careful handling of endpoints.", "Arrays of evenly spaced numbers in N-dimensions.", "Grid-shaped arrays of evenly spaced numbers in N-dimensions."]}, {"name": "numpy.arccos()", "path": "reference/generated/numpy.arccos", "type": "numpy.arccos", "text": ["Trigonometric inverse cosine, element-wise.", "The inverse of cos so that, if y = cos(x), then x = arccos(y).", "x-coordinate on the unit circle. For real arguments, the domain is [-1, 1].", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The angle of the ray intersecting the unit circle at the given x-coordinate in radians [0, pi]. This is a scalar if x is a scalar.", "See also", "arccos is a multivalued function: for each x there are infinitely many numbers z such that cos(z) = x. The convention is to return the angle z whose real part lies in [0, pi].", "For real-valued input data types, arccos always returns real output. For each value that cannot be expressed as a real number or infinity, it yields nan and sets the invalid floating point error flag.", "For complex-valued input, arccos is a complex analytic function that has branch cuts [-inf, -1] and [1, inf] and is continuous from above on the former and from below on the latter.", "The inverse cos is also known as acos or cos^-1.", "M. Abramowitz and I.A. Stegun, \u201cHandbook of Mathematical Functions\u201d, 10th printing, 1964, pp. 79. https://personal.math.ubc.ca/~cbm/aands/page_79.htm", "We expect the arccos of 1 to be 0, and of -1 to be pi:", "Plot arccos:"]}, {"name": "numpy.arccosh()", "path": "reference/generated/numpy.arccosh", "type": "numpy.arccosh", "text": ["Inverse hyperbolic cosine, element-wise.", "Input array.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Array of the same shape as x. This is a scalar if x is a scalar.", "See also", "arccosh is a multivalued function: for each x there are infinitely many numbers z such that cosh(z) = x. The convention is to return the z whose imaginary part lies in [-pi, pi] and the real part in [0, inf].", "For real-valued input data types, arccosh always returns real output. For each value that cannot be expressed as a real number or infinity, it yields nan and sets the invalid floating point error flag.", "For complex-valued input, arccosh is a complex analytical function that has a branch cut [-inf, 1] and is continuous from above on it.", "M. Abramowitz and I.A. Stegun, \u201cHandbook of Mathematical Functions\u201d, 10th printing, 1964, pp. 86. https://personal.math.ubc.ca/~cbm/aands/page_86.htm", "Wikipedia, \u201cInverse hyperbolic function\u201d, https://en.wikipedia.org/wiki/Arccosh"]}, {"name": "numpy.arcsin()", "path": "reference/generated/numpy.arcsin", "type": "numpy.arcsin", "text": ["Inverse sine, element-wise.", "y-coordinate on the unit circle.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The inverse sine of each element in x, in radians and in the closed interval [-pi/2, pi/2]. This is a scalar if x is a scalar.", "See also", "arcsin is a multivalued function: for each x there are infinitely many numbers z such that \\(sin(z) = x\\). The convention is to return the angle z whose real part lies in [-pi/2, pi/2].", "For real-valued input data types, arcsin always returns real output. For each value that cannot be expressed as a real number or infinity, it yields nan and sets the invalid floating point error flag.", "For complex-valued input, arcsin is a complex analytic function that has, by convention, the branch cuts [-inf, -1] and [1, inf] and is continuous from above on the former and from below on the latter.", "The inverse sine is also known as asin or sin^{-1}.", "Abramowitz, M. and Stegun, I. A., Handbook of Mathematical Functions, 10th printing, New York: Dover, 1964, pp. 79ff. https://personal.math.ubc.ca/~cbm/aands/page_79.htm"]}, {"name": "numpy.arcsinh()", "path": "reference/generated/numpy.arcsinh", "type": "numpy.arcsinh", "text": ["Inverse hyperbolic sine element-wise.", "Input array.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Array of the same shape as x. This is a scalar if x is a scalar.", "arcsinh is a multivalued function: for each x there are infinitely many numbers z such that sinh(z) = x. The convention is to return the z whose imaginary part lies in [-pi/2, pi/2].", "For real-valued input data types, arcsinh always returns real output. For each value that cannot be expressed as a real number or infinity, it returns nan and sets the invalid floating point error flag.", "For complex-valued input, arccos is a complex analytical function that has branch cuts [1j, infj] and [-1j, -infj] and is continuous from the right on the former and from the left on the latter.", "The inverse hyperbolic sine is also known as asinh or sinh^-1.", "M. Abramowitz and I.A. Stegun, \u201cHandbook of Mathematical Functions\u201d, 10th printing, 1964, pp. 86. https://personal.math.ubc.ca/~cbm/aands/page_86.htm", "Wikipedia, \u201cInverse hyperbolic function\u201d, https://en.wikipedia.org/wiki/Arcsinh"]}, {"name": "numpy.arctan()", "path": "reference/generated/numpy.arctan", "type": "numpy.arctan", "text": ["Trigonometric inverse tangent, element-wise.", "The inverse of tan, so that if y = tan(x) then x = arctan(y).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Out has the same shape as x. Its real part is in [-pi/2, pi/2] (arctan(+/-inf) returns +/-pi/2). This is a scalar if x is a scalar.", "See also", "The \u201cfour quadrant\u201d arctan of the angle formed by (x, y) and the positive x-axis.", "Argument of complex values.", "arctan is a multi-valued function: for each x there are infinitely many numbers z such that tan(z) = x. The convention is to return the angle z whose real part lies in [-pi/2, pi/2].", "For real-valued input data types, arctan always returns real output. For each value that cannot be expressed as a real number or infinity, it yields nan and sets the invalid floating point error flag.", "For complex-valued input, arctan is a complex analytic function that has [1j, infj] and [-1j, -infj] as branch cuts, and is continuous from the left on the former and from the right on the latter.", "The inverse tangent is also known as atan or tan^{-1}.", "Abramowitz, M. and Stegun, I. A., Handbook of Mathematical Functions, 10th printing, New York: Dover, 1964, pp. 79. https://personal.math.ubc.ca/~cbm/aands/page_79.htm", "We expect the arctan of 0 to be 0, and of 1 to be pi/4:", "Plot arctan:"]}, {"name": "numpy.arctan2()", "path": "reference/generated/numpy.arctan2", "type": "numpy.arctan2", "text": ["Element-wise arc tangent of x1/x2 choosing the quadrant correctly.", "The quadrant (i.e., branch) is chosen so that arctan2(x1, x2) is the signed angle in radians between the ray ending at the origin and passing through the point (1,0), and the ray ending at the origin and passing through the point (x2, x1). (Note the role reversal: the \u201cy-coordinate\u201d is the first function parameter, the \u201cx-coordinate\u201d is the second.) By IEEE convention, this function is defined for x2 = +/-0 and for either or both of x1 and x2 = +/-inf (see Notes for specific values).", "This function is not defined for complex-valued arguments; for the so-called argument of complex values, use angle.", "y-coordinates.", "x-coordinates. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Array of angles in radians, in the range [-pi, pi]. This is a scalar if both x1 and x2 are scalars.", "See also", "arctan2 is identical to the atan2 function of the underlying C library. The following special values are defined in the C standard: [1]", "x1", "x2", "arctan2(x1,x2)", "+/- 0", "+0", "+/- 0", "+/- 0", "-0", "+/- pi", "> 0", "+/-inf", "+0 / +pi", "< 0", "+/-inf", "-0 / -pi", "+/-inf", "+inf", "+/- (pi/4)", "+/-inf", "-inf", "+/- (3*pi/4)", "Note that +0 and -0 are distinct floating point numbers, as are +inf and -inf.", "ISO/IEC standard 9899:1999, \u201cProgramming language C.\u201d", "Consider four points in different quadrants:", "Note the order of the parameters. arctan2 is defined also when x2 = 0 and at several other special points, obtaining values in the range [-pi, pi]:"]}, {"name": "numpy.arctanh()", "path": "reference/generated/numpy.arctanh", "type": "numpy.arctanh", "text": ["Inverse hyperbolic tangent element-wise.", "Input array.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Array of the same shape as x. This is a scalar if x is a scalar.", "See also", "arctanh is a multivalued function: for each x there are infinitely many numbers z such that tanh(z) = x. The convention is to return the z whose imaginary part lies in [-pi/2, pi/2].", "For real-valued input data types, arctanh always returns real output. For each value that cannot be expressed as a real number or infinity, it yields nan and sets the invalid floating point error flag.", "For complex-valued input, arctanh is a complex analytical function that has branch cuts [-1, -inf] and [1, inf] and is continuous from above on the former and from below on the latter.", "The inverse hyperbolic tangent is also known as atanh or tanh^-1.", "M. Abramowitz and I.A. Stegun, \u201cHandbook of Mathematical Functions\u201d, 10th printing, 1964, pp. 86. https://personal.math.ubc.ca/~cbm/aands/page_86.htm", "Wikipedia, \u201cInverse hyperbolic function\u201d, https://en.wikipedia.org/wiki/Arctanh"]}, {"name": "numpy.argmax()", "path": "reference/generated/numpy.argmax", "type": "numpy.argmax", "text": ["Returns the indices of the maximum values along an axis.", "Input array.", "By default, the index is into the flattened array, otherwise along the specified axis.", "If provided, the result will be inserted into this array. It should be of the appropriate shape and dtype.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the array.", "New in version 1.22.0.", "Array of indices into the array. It has the same shape as a.shape with the dimension along axis removed. If keepdims is set to True, then the size of axis will be 1 with the resulting array having same shape as a.shape.", "See also", "The maximum value along a given axis.", "Convert a flat index into an index tuple.", "Apply np.expand_dims(index_array, axis) from argmax to an array as if by calling max.", "In case of multiple occurrences of the maximum values, the indices corresponding to the first occurrence are returned.", "Indexes of the maximal elements of a N-dimensional array:", "Setting keepdims to True,"]}, {"name": "numpy.argmin()", "path": "reference/generated/numpy.argmin", "type": "numpy.argmin", "text": ["Returns the indices of the minimum values along an axis.", "Input array.", "By default, the index is into the flattened array, otherwise along the specified axis.", "If provided, the result will be inserted into this array. It should be of the appropriate shape and dtype.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the array.", "New in version 1.22.0.", "Array of indices into the array. It has the same shape as a.shape with the dimension along axis removed. If keepdims is set to True, then the size of axis will be 1 with the resulting array having same shape as a.shape.", "See also", "The minimum value along a given axis.", "Convert a flat index into an index tuple.", "Apply np.expand_dims(index_array, axis) from argmin to an array as if by calling min.", "In case of multiple occurrences of the minimum values, the indices corresponding to the first occurrence are returned.", "Indices of the minimum elements of a N-dimensional array:", "Setting keepdims to True,"]}, {"name": "numpy.argpartition()", "path": "reference/generated/numpy.argpartition", "type": "numpy.argpartition", "text": ["Perform an indirect partition along the given axis using the algorithm specified by the kind keyword. It returns an array of indices of the same shape as a that index data along the given axis in partitioned order.", "New in version 1.8.0.", "Array to sort.", "Element index to partition by. The k-th element will be in its final sorted position and all smaller elements will be moved before it and all larger elements behind it. The order all elements in the partitions is undefined. If provided with a sequence of k-th it will partition all of them into their sorted position at once.", "Deprecated since version 1.22.0: Passing booleans as index is deprecated.", "Axis along which to sort. The default is -1 (the last axis). If None, the flattened array is used.", "Selection algorithm. Default is \u2018introselect\u2019", "When a is an array with fields defined, this argument specifies which fields to compare first, second, etc. A single field can be specified as a string, and not all fields need be specified, but unspecified fields will still be used, in the order in which they come up in the dtype, to break ties.", "Array of indices that partition a along the specified axis. If a is one-dimensional, a[index_array] yields a partitioned a. More generally, np.take_along_axis(a, index_array, axis=a) always yields the partitioned a, irrespective of dimensionality.", "See also", "Describes partition algorithms used.", "Inplace partition.", "Full indirect sort.", "Apply index_array from argpartition to an array as if by calling partition.", "See partition for notes on the different selection algorithms.", "One dimensional array:", "Multi-dimensional array:"]}, {"name": "numpy.argsort()", "path": "reference/generated/numpy.argsort", "type": "numpy.argsort", "text": ["Returns the indices that would sort an array.", "Perform an indirect sort along the given axis using the algorithm specified by the kind keyword. It returns an array of indices of the same shape as a that index data along the given axis in sorted order.", "Array to sort.", "Axis along which to sort. The default is -1 (the last axis). If None, the flattened array is used.", "Sorting algorithm. The default is \u2018quicksort\u2019. Note that both \u2018stable\u2019 and \u2018mergesort\u2019 use timsort under the covers and, in general, the actual implementation will vary with data type. The \u2018mergesort\u2019 option is retained for backwards compatibility.", "Changed in version 1.15.0.: The \u2018stable\u2019 option was added.", "When a is an array with fields defined, this argument specifies which fields to compare first, second, etc. A single field can be specified as a string, and not all fields need be specified, but unspecified fields will still be used, in the order in which they come up in the dtype, to break ties.", "Array of indices that sort a along the specified axis. If a is one-dimensional, a[index_array] yields a sorted a. More generally, np.take_along_axis(a, index_array, axis=axis) always yields the sorted a, irrespective of dimensionality.", "See also", "Describes sorting algorithms used.", "Indirect stable sort with multiple keys.", "Inplace sort.", "Indirect partial sort.", "Apply index_array from argsort to an array as if by calling sort.", "See sort for notes on the different sorting algorithms.", "As of NumPy 1.4.0 argsort works with real/complex arrays containing nan values. The enhanced sort order is documented in sort.", "One dimensional array:", "Two-dimensional array:", "Indices of the sorted elements of a N-dimensional array:", "Sorting with keys:"]}, {"name": "numpy.argwhere()", "path": "reference/generated/numpy.argwhere", "type": "numpy.argwhere", "text": ["Find the indices of array elements that are non-zero, grouped by element.", "Input data.", "Indices of elements that are non-zero. Indices are grouped by element. This array will have shape (N, a.ndim) where N is the number of non-zero items.", "See also", "np.argwhere(a) is almost the same as np.transpose(np.nonzero(a)), but produces a result of the correct shape for a 0D array.", "The output of argwhere is not suitable for indexing arrays. For this purpose use nonzero(a) instead."]}, {"name": "numpy.around()", "path": "reference/generated/numpy.around", "type": "numpy.around", "text": ["Evenly round to the given number of decimals.", "Input data.", "Number of decimal places to round to (default: 0). If decimals is negative, it specifies the number of positions to the left of the decimal point.", "Alternative output array in which to place the result. It must have the same shape as the expected output, but the type of the output values will be cast if necessary. See Output type determination for more details.", "An array of the same type as a, containing the rounded values. Unless out was specified, a new array is created. A reference to the result is returned.", "The real and imaginary parts of complex numbers are rounded separately. The result of rounding a float is a float.", "See also", "equivalent method", "For values exactly halfway between rounded decimal values, NumPy rounds to the nearest even value. Thus 1.5 and 2.5 round to 2.0, -0.5 and 0.5 round to 0.0, etc.", "np.around uses a fast but sometimes inexact algorithm to round floating-point datatypes. For positive decimals it is equivalent to np.true_divide(np.rint(a * 10**decimals), 10**decimals), which has error due to the inexact representation of decimal fractions in the IEEE floating point standard [1] and errors introduced when scaling by powers of ten. For instance, note the extra \u201c1\u201d in the following:", "If your goal is to print such values with a fixed number of decimals, it is preferable to use numpy\u2019s float printing routines to limit the number of printed decimals:", "The float printing routines use an accurate but much more computationally demanding algorithm to compute the number of digits after the decimal point.", "Alternatively, Python\u2019s builtin round function uses a more accurate but slower algorithm for 64-bit floating point values:", "\u201cLecture Notes on the Status of IEEE 754\u201d, William Kahan, https://people.eecs.berkeley.edu/~wkahan/ieee754status/IEEE754.PDF"]}, {"name": "numpy.array()", "path": "reference/generated/numpy.array", "type": "numpy.array", "text": ["Create an array.", "An array, any object exposing the array interface, an object whose __array__ method returns an array, or any (nested) sequence. If object is a scalar, a 0-dimensional array containing object is returned.", "The desired data-type for the array. If not given, then the type will be determined as the minimum type required to hold the objects in the sequence.", "If true (default), then the object is copied. Otherwise, a copy will only be made if __array__ returns a copy, if obj is a nested sequence, or if a copy is needed to satisfy any of the other requirements (dtype, order, etc.).", "Specify the memory layout of the array. If object is not an array, the newly created array will be in C order (row major) unless \u2018F\u2019 is specified, in which case it will be in Fortran order (column major). If object is an array the following holds.", "order", "no copy", "copy=True", "\u2018K\u2019", "unchanged", "F & C order preserved, otherwise most similar order", "\u2018A\u2019", "unchanged", "F order if input is F and not C, otherwise C order", "\u2018C\u2019", "C order", "C order", "\u2018F\u2019", "F order", "F order", "When copy=False and a copy is made for other reasons, the result is the same as if copy=True, with some exceptions for \u2018A\u2019, see the Notes section. The default order is \u2018K\u2019.", "If True, then sub-classes will be passed-through, otherwise the returned array will be forced to be a base-class array (default).", "Specifies the minimum number of dimensions that the resulting array should have. Ones will be pre-pended to the shape as needed to meet this requirement.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "An array object satisfying the specified requirements.", "See also", "Return an empty array with shape and type of input.", "Return an array of ones with shape and type of input.", "Return an array of zeros with shape and type of input.", "Return a new array with shape of input filled with value.", "Return a new uninitialized array.", "Return a new array setting values to one.", "Return a new array setting values to zero.", "Return a new array of given shape filled with value.", "When order is \u2018A\u2019 and object is an array in neither \u2018C\u2019 nor \u2018F\u2019 order, and a copy is forced by a change in dtype, then the order of the result is not necessarily \u2018C\u2019 as expected. This is likely a bug.", "Upcasting:", "More than one dimension:", "Minimum dimensions 2:", "Type provided:", "Data-type consisting of more than one element:", "Creating an array from sub-classes:"]}, {"name": "numpy.array2string()", "path": "reference/generated/numpy.array2string", "type": "numpy.array2string", "text": ["Return a string representation of an array.", "Input array.", "Inserts newlines if text is longer than max_line_width. Defaults to numpy.get_printoptions()['linewidth'].", "Floating point precision. Defaults to numpy.get_printoptions()['precision'].", "Represent numbers \u201cvery close\u201d to zero as zero; default is False. Very close is defined by precision: if the precision is 8, e.g., numbers smaller (in absolute value) than 5e-9 are represented as zero. Defaults to numpy.get_printoptions()['suppress'].", "Inserted between elements.", "The length of the prefix and suffix strings are used to respectively align and wrap the output. An array is typically printed as:", "The output is left-padded by the length of the prefix string, and wrapping is forced at the column max_line_width - len(suffix). It should be noted that the content of prefix and suffix strings are not included in the output.", "Has no effect, do not use.", "Deprecated since version 1.14.0.", "If not None, the keys should indicate the type(s) that the respective formatting function applies to. Callables should return a string. Types that are not specified (by their corresponding keys) are handled by the default formatters. Individual types for which a formatter can be set are:", "Other keys that can be used to set a group of types at once are:", "Total number of array elements which trigger summarization rather than full repr. Defaults to numpy.get_printoptions()['threshold'].", "Number of array items in summary at beginning and end of each dimension. Defaults to numpy.get_printoptions()['edgeitems'].", "Controls printing of the sign of floating-point types. If \u2018+\u2019, always print the sign of positive values. If \u2018 \u2018, always prints a space (whitespace character) in the sign position of positive values. If \u2018-\u2019, omit the sign character of positive values. Defaults to numpy.get_printoptions()['sign'].", "Controls the interpretation of the precision option for floating-point types. Defaults to numpy.get_printoptions()['floatmode']. Can take the following values:", "If set to the string \u20181.13\u2019 enables 1.13 legacy printing mode. This approximates numpy 1.13 print output by including a space in the sign position of floats and different behavior for 0d arrays. If set to False, disables legacy mode. Unrecognized strings will be ignored with a warning for forward compatibility.", "New in version 1.14.0.", "String representation of the array.", "if a callable in formatter does not return a string.", "See also", "If a formatter is specified for a certain type, the precision keyword is ignored for that type.", "This is a very flexible function; array_repr and array_str are using array2string internally so keywords with the same name should work identically in all three functions."]}, {"name": "numpy.array_equal()", "path": "reference/generated/numpy.array_equal", "type": "numpy.array_equal", "text": ["True if two arrays have the same shape and elements, False otherwise.", "Input arrays.", "Whether to compare NaN\u2019s as equal. If the dtype of a1 and a2 is complex, values will be considered equal if either the real or the imaginary component of a given value is nan.", "New in version 1.19.0.", "Returns True if the arrays are equal.", "See also", "Returns True if two arrays are element-wise equal within a tolerance.", "Returns True if input arrays are shape consistent and all elements equal.", "When equal_nan is True, complex values with nan components are considered equal if either the real or the imaginary components are nan."]}, {"name": "numpy.array_equiv()", "path": "reference/generated/numpy.array_equiv", "type": "numpy.array_equiv", "text": ["Returns True if input arrays are shape consistent and all elements equal.", "Shape consistent means they are either the same shape, or one input array can be broadcasted to create the same shape as the other one.", "Input arrays.", "True if equivalent, False otherwise.", "Showing the shape equivalence:"]}, {"name": "numpy.array_repr()", "path": "reference/generated/numpy.array_repr", "type": "numpy.array_repr", "text": ["Return the string representation of an array.", "Input array.", "Inserts newlines if text is longer than max_line_width. Defaults to numpy.get_printoptions()['linewidth'].", "Floating point precision. Defaults to numpy.get_printoptions()['precision'].", "Represent numbers \u201cvery close\u201d to zero as zero; default is False. Very close is defined by precision: if the precision is 8, e.g., numbers smaller (in absolute value) than 5e-9 are represented as zero. Defaults to numpy.get_printoptions()['suppress'].", "The string representation of an array.", "See also"]}, {"name": "numpy.array_split()", "path": "reference/generated/numpy.array_split", "type": "numpy.array_split", "text": ["Split an array into multiple sub-arrays.", "Please refer to the split documentation. The only difference between these functions is that array_split allows indices_or_sections to be an integer that does not equally divide the axis. For an array of length l that should be split into n sections, it returns l % n sub-arrays of size l//n + 1 and the rest of size l//n.", "See also", "Split array into multiple sub-arrays of equal size."]}, {"name": "numpy.array_str()", "path": "reference/generated/numpy.array_str", "type": "numpy.array_str", "text": ["Return a string representation of the data in an array.", "The data in the array is returned as a single string. This function is similar to array_repr, the difference being that array_repr also returns information on the kind of array and its data type.", "Input array.", "Inserts newlines if text is longer than max_line_width. Defaults to numpy.get_printoptions()['linewidth'].", "Floating point precision. Defaults to numpy.get_printoptions()['precision'].", "Represent numbers \u201cvery close\u201d to zero as zero; default is False. Very close is defined by precision: if the precision is 8, e.g., numbers smaller (in absolute value) than 5e-9 are represented as zero. Defaults to numpy.get_printoptions()['suppress'].", "See also"]}, {"name": "numpy.asanyarray()", "path": "reference/generated/numpy.asanyarray", "type": "numpy.asanyarray", "text": ["Convert the input to an ndarray, but pass ndarray subclasses through.", "Input data, in any form that can be converted to an array. This includes scalars, lists, lists of tuples, tuples, tuples of tuples, tuples of lists, and ndarrays.", "By default, the data-type is inferred from the input data.", "Memory layout. \u2018A\u2019 and \u2018K\u2019 depend on the order of input array a. \u2018C\u2019 row-major (C-style), \u2018F\u2019 column-major (Fortran-style) memory representation. \u2018A\u2019 (any) means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise \u2018K\u2019 (keep) preserve input order Defaults to \u2018C\u2019.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "Array interpretation of a. If a is an ndarray or a subclass of ndarray, it is returned as-is and no copy is performed.", "See also", "Similar function which always returns ndarrays.", "Convert input to a contiguous array.", "Convert input to a floating point ndarray.", "Convert input to an ndarray with column-major memory order.", "Similar function which checks input for NaNs and Infs.", "Create an array from an iterator.", "Construct an array by executing a function on grid positions.", "Convert a list into an array:", "Instances of ndarray subclasses are passed through as-is:"]}, {"name": "numpy.asarray()", "path": "reference/generated/numpy.asarray", "type": "numpy.asarray", "text": ["Convert the input to an array.", "Input data, in any form that can be converted to an array. This includes lists, lists of tuples, tuples, tuples of tuples, tuples of lists and ndarrays.", "By default, the data-type is inferred from the input data.", "Memory layout. \u2018A\u2019 and \u2018K\u2019 depend on the order of input array a. \u2018C\u2019 row-major (C-style), \u2018F\u2019 column-major (Fortran-style) memory representation. \u2018A\u2019 (any) means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise \u2018K\u2019 (keep) preserve input order Defaults to \u2018K\u2019.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "Array interpretation of a. No copy is performed if the input is already an ndarray with matching dtype and order. If a is a subclass of ndarray, a base class ndarray is returned.", "See also", "Similar function which passes through subclasses.", "Convert input to a contiguous array.", "Convert input to a floating point ndarray.", "Convert input to an ndarray with column-major memory order.", "Similar function which checks input for NaNs and Infs.", "Create an array from an iterator.", "Construct an array by executing a function on grid positions.", "Convert a list into an array:", "Existing arrays are not copied:", "If dtype is set, array is copied only if dtype does not match:", "Contrary to asanyarray, ndarray subclasses are not passed through:"]}, {"name": "numpy.asarray_chkfinite()", "path": "reference/generated/numpy.asarray_chkfinite", "type": "numpy.asarray_chkfinite", "text": ["Convert the input to an array, checking for NaNs or Infs.", "Input data, in any form that can be converted to an array. This includes lists, lists of tuples, tuples, tuples of tuples, tuples of lists and ndarrays. Success requires no NaNs or Infs.", "By default, the data-type is inferred from the input data.", "Memory layout. \u2018A\u2019 and \u2018K\u2019 depend on the order of input array a. \u2018C\u2019 row-major (C-style), \u2018F\u2019 column-major (Fortran-style) memory representation. \u2018A\u2019 (any) means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise \u2018K\u2019 (keep) preserve input order Defaults to \u2018C\u2019.", "Array interpretation of a. No copy is performed if the input is already an ndarray. If a is a subclass of ndarray, a base class ndarray is returned.", "Raises ValueError if a contains NaN (Not a Number) or Inf (Infinity).", "See also", "Create and array.", "Similar function which passes through subclasses.", "Convert input to a contiguous array.", "Convert input to a floating point ndarray.", "Convert input to an ndarray with column-major memory order.", "Create an array from an iterator.", "Construct an array by executing a function on grid positions.", "Convert a list into an array. If all elements are finite asarray_chkfinite is identical to asarray.", "Raises ValueError if array_like contains Nans or Infs."]}, {"name": "numpy.ascontiguousarray()", "path": "reference/generated/numpy.ascontiguousarray", "type": "numpy.ascontiguousarray", "text": ["Return a contiguous array (ndim >= 1) in memory (C order).", "Input array.", "Data-type of returned array.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "Contiguous array of same shape and content as a, with type dtype if specified.", "See also", "Convert input to an ndarray with column-major memory order.", "Return an ndarray that satisfies requirements.", "Information about the memory layout of the array.", "Note: This function returns an array with at least one-dimension (1-d) so it will not preserve 0-d arrays."]}, {"name": "numpy.asfarray()", "path": "reference/generated/numpy.asfarray", "type": "numpy.asfarray", "text": ["Return an array converted to a float type.", "The input array.", "Float type code to coerce input array a. If dtype is one of the \u2018int\u2019 dtypes, it is replaced with float64.", "The input a as a float ndarray."]}, {"name": "numpy.asfortranarray()", "path": "reference/generated/numpy.asfortranarray", "type": "numpy.asfortranarray", "text": ["Return an array (ndim >= 1) laid out in Fortran order in memory.", "Input array.", "By default, the data-type is inferred from the input data.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "The input a in Fortran, or column-major, order.", "See also", "Convert input to a contiguous (C order) array.", "Convert input to an ndarray with either row or column-major memory order.", "Return an ndarray that satisfies requirements.", "Information about the memory layout of the array.", "Note: This function returns an array with at least one-dimension (1-d) so it will not preserve 0-d arrays."]}, {"name": "numpy.asmatrix()", "path": "reference/generated/numpy.asmatrix", "type": "numpy.asmatrix", "text": ["Interpret the input as a matrix.", "Unlike matrix, asmatrix does not make a copy if the input is already a matrix or an ndarray. Equivalent to matrix(data, copy=False).", "Input data.", "Data-type of the output matrix.", "data interpreted as a matrix."]}, {"name": "numpy.asscalar()", "path": "reference/generated/numpy.asscalar", "type": "numpy.asscalar", "text": ["Convert an array of size 1 to its scalar equivalent.", "Deprecated since version 1.16: Deprecated, use numpy.ndarray.item() instead.", "Input array of size 1.", "Scalar representation of a. The output data type is the same type returned by the input\u2019s item method."]}, {"name": "numpy.atleast_1d()", "path": "reference/generated/numpy.atleast_1d", "type": "numpy.atleast_1d", "text": ["Convert inputs to arrays with at least one dimension.", "Scalar inputs are converted to 1-dimensional arrays, whilst higher-dimensional inputs are preserved.", "One or more input arrays.", "An array, or list of arrays, each with a.ndim >= 1. Copies are made only if necessary.", "See also"]}, {"name": "numpy.atleast_2d()", "path": "reference/generated/numpy.atleast_2d", "type": "numpy.atleast_2d", "text": ["View inputs as arrays with at least two dimensions.", "One or more array-like sequences. Non-array inputs are converted to arrays. Arrays that already have two or more dimensions are preserved.", "An array, or list of arrays, each with a.ndim >= 2. Copies are avoided where possible, and views with two or more dimensions are returned.", "See also"]}, {"name": "numpy.atleast_3d()", "path": "reference/generated/numpy.atleast_3d", "type": "numpy.atleast_3d", "text": ["View inputs as arrays with at least three dimensions.", "One or more array-like sequences. Non-array inputs are converted to arrays. Arrays that already have three or more dimensions are preserved.", "An array, or list of arrays, each with a.ndim >= 3. Copies are avoided where possible, and views with three or more dimensions are returned. For example, a 1-D array of shape (N,) becomes a view of shape (1, N, 1), and a 2-D array of shape (M, N) becomes a view of shape (M, N, 1).", "See also"]}, {"name": "numpy.average()", "path": "reference/generated/numpy.average", "type": "numpy.average", "text": ["Compute the weighted average along the specified axis.", "Array containing data to be averaged. If a is not an array, a conversion is attempted.", "Axis or axes along which to average a. The default, axis=None, will average over all of the elements of the input array. If axis is negative it counts from the last to the first axis.", "New in version 1.7.0.", "If axis is a tuple of ints, averaging is performed on all of the axes specified in the tuple instead of a single axis or all the axes as before.", "An array of weights associated with the values in a. Each value in a contributes to the average according to its associated weight. The weights array can either be 1-D (in which case its length must be the size of a along the given axis) or of the same shape as a. If weights=None, then all data in a are assumed to have a weight equal to one. The 1-D calculation is:", "The only constraint on weights is that sum(weights) must not be 0.", "Default is False. If True, the tuple (average, sum_of_weights) is returned, otherwise only the average is returned. If weights=None, sum_of_weights is equivalent to the number of elements over which the average is taken.", "Return the average along the specified axis. When returned is True, return a tuple with the average as the first element and the sum of the weights as the second element. sum_of_weights is of the same type as retval. The result dtype follows a genereal pattern. If weights is None, the result dtype will be that of a , or float64 if a is integral. Otherwise, if weights is not None and a is non- integral, the result type will be the type of lowest precision capable of representing values of both a and weights. If a happens to be integral, the previous rules still applies but the result dtype will at least be float64.", "When all weights along axis are zero. See numpy.ma.average for a version robust to this type of error.", "When the length of 1D weights is not the same as the shape of a along axis.", "See also", "average for masked arrays \u2013 useful if your data contains \u201cmissing\u201d values", "Returns the type that results from applying the numpy type promotion rules to the arguments."]}, {"name": "numpy.AxisError()", "path": "reference/generated/numpy.axiserror", "type": "numpy.AxisError", "text": ["Axis supplied was invalid.", "This is raised whenever an axis parameter is specified that is larger than the number of array dimensions. For compatibility with code written against older numpy versions, which raised a mixture of ValueError and IndexError for this situation, this exception subclasses both to ensure that except ValueError and except IndexError statements continue to catch AxisError.", "New in version 1.13.", "The out of bounds axis or a custom exception message. If an axis is provided, then ndim should be specified as well.", "The number of array dimensions.", "A prefix for the exception message.", "Negative axes are preserved:", "The class constructor generally takes the axis and arrays\u2019 dimensionality as arguments:", "Alternatively, a custom exception message can be passed:", "The out of bounds axis or None if a custom exception message was provided. This should be the axis as passed by the user, before any normalization to resolve negative indices.", "New in version 1.22.", "The number of array dimensions or None if a custom exception message was provided.", "New in version 1.22."]}, {"name": "numpy.bartlett()", "path": "reference/generated/numpy.bartlett", "type": "numpy.bartlett", "text": ["Return the Bartlett window.", "The Bartlett window is very similar to a triangular window, except that the end points are at zero. It is often used in signal processing for tapering a signal, without generating too much ripple in the frequency domain.", "Number of points in the output window. If zero or less, an empty array is returned.", "The triangular window, with the maximum value normalized to one (the value one appears only if the number of samples is odd), with the first and last samples equal to zero.", "See also", "The Bartlett window is defined as", "Most references to the Bartlett window come from the signal processing literature, where it is used as one of many windowing functions for smoothing values. Note that convolution with this window produces linear interpolation. It is also known as an apodization (which means\u201dremoving the foot\u201d, i.e. smoothing discontinuities at the beginning and end of the sampled signal) or tapering function. The fourier transform of the Bartlett is the product of two sinc functions. Note the excellent discussion in Kanasewich.", "M.S. Bartlett, \u201cPeriodogram Analysis and Continuous Spectra\u201d, Biometrika 37, 1-16, 1950.", "E.R. Kanasewich, \u201cTime Sequence Analysis in Geophysics\u201d, The University of Alberta Press, 1975, pp. 109-110.", "A.V. Oppenheim and R.W. Schafer, \u201cDiscrete-Time Signal Processing\u201d, Prentice-Hall, 1999, pp. 468-471.", "Wikipedia, \u201cWindow function\u201d, https://en.wikipedia.org/wiki/Window_function", "W.H. Press, B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling, \u201cNumerical Recipes\u201d, Cambridge University Press, 1986, page 429.", "Plot the window and its frequency response (requires SciPy and matplotlib):"]}, {"name": "numpy.base_repr()", "path": "reference/generated/numpy.base_repr", "type": "numpy.base_repr", "text": ["Return a string representation of a number in the given base system.", "The value to convert. Positive and negative values are handled.", "Convert number to the base number system. The valid range is 2-36, the default value is 2.", "Number of zeros padded on the left. Default is 0 (no padding).", "String representation of number in base system.", "See also", "Faster version of base_repr for base 2."]}, {"name": "numpy.binary_repr()", "path": "reference/generated/numpy.binary_repr", "type": "numpy.binary_repr", "text": ["Return the binary representation of the input number as a string.", "For negative numbers, if width is not given, a minus sign is added to the front. If width is given, the two\u2019s complement of the number is returned, with respect to that width.", "In a two\u2019s-complement system negative numbers are represented by the two\u2019s complement of the absolute value. This is the most common method of representing signed integers on computers [1]. A N-bit two\u2019s-complement system can represent every integer in the range \\(-2^{N-1}\\) to \\(+2^{N-1}-1\\).", "Only an integer decimal number can be used.", "The length of the returned string if num is positive, or the length of the two\u2019s complement if num is negative, provided that width is at least a sufficient number of bits for num to be represented in the designated form.", "If the width value is insufficient, it will be ignored, and num will be returned in binary (num > 0) or two\u2019s complement (num < 0) form with its width equal to the minimum number of bits needed to represent the number in the designated form. This behavior is deprecated and will later raise an error.", "Deprecated since version 1.12.0.", "Binary representation of num or two\u2019s complement of num.", "See also", "Return a string representation of a number in the given base system.", "Python\u2019s built-in binary representation generator of an integer.", "binary_repr is equivalent to using base_repr with base 2, but about 25x faster.", "Wikipedia, \u201cTwo\u2019s complement\u201d, https://en.wikipedia.org/wiki/Two\u2019s_complement", "The two\u2019s complement is returned when the input number is negative and width is specified:"]}, {"name": "numpy.bincount()", "path": "reference/generated/numpy.bincount", "type": "numpy.bincount", "text": ["Count number of occurrences of each value in array of non-negative ints.", "The number of bins (of size 1) is one larger than the largest value in x. If minlength is specified, there will be at least this number of bins in the output array (though it will be longer if necessary, depending on the contents of x). Each bin gives the number of occurrences of its index value in x. If weights is specified the input array is weighted by it, i.e. if a value n is found at position i, out[n] += weight[i] instead of out[n] += 1.", "Input array.", "Weights, array of the same shape as x.", "A minimum number of bins for the output array.", "New in version 1.6.0.", "The result of binning the input array. The length of out is equal to np.amax(x)+1.", "If the input is not 1-dimensional, or contains elements with negative values, or if minlength is negative.", "If the type of the input is float or complex.", "See also", "The input array needs to be of integer dtype, otherwise a TypeError is raised:", "A possible use of bincount is to perform sums over variable-size chunks of an array, using the weights keyword."]}, {"name": "numpy.bitwise_and()", "path": "reference/generated/numpy.bitwise_and", "type": "numpy.bitwise_and", "text": ["Compute the bit-wise AND of two arrays element-wise.", "Computes the bit-wise AND of the underlying binary representation of the integers in the input arrays. This ufunc implements the C/Python operator &.", "Only integer and boolean types are handled. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Result. This is a scalar if both x1 and x2 are scalars.", "See also", "Return the binary representation of the input number as a string.", "The number 13 is represented by 00001101. Likewise, 17 is represented by 00010001. The bit-wise AND of 13 and 17 is therefore 000000001, or 1:", "The & operator can be used as a shorthand for np.bitwise_and on ndarrays."]}, {"name": "numpy.bitwise_or()", "path": "reference/generated/numpy.bitwise_or", "type": "numpy.bitwise_or", "text": ["Compute the bit-wise OR of two arrays element-wise.", "Computes the bit-wise OR of the underlying binary representation of the integers in the input arrays. This ufunc implements the C/Python operator |.", "Only integer and boolean types are handled. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Result. This is a scalar if both x1 and x2 are scalars.", "See also", "Return the binary representation of the input number as a string.", "The number 13 has the binary representation 00001101. Likewise, 16 is represented by 00010000. The bit-wise OR of 13 and 16 is then 000111011, or 29:", "The | operator can be used as a shorthand for np.bitwise_or on ndarrays."]}, {"name": "numpy.bitwise_xor()", "path": "reference/generated/numpy.bitwise_xor", "type": "numpy.bitwise_xor", "text": ["Compute the bit-wise XOR of two arrays element-wise.", "Computes the bit-wise XOR of the underlying binary representation of the integers in the input arrays. This ufunc implements the C/Python operator ^.", "Only integer and boolean types are handled. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Result. This is a scalar if both x1 and x2 are scalars.", "See also", "Return the binary representation of the input number as a string.", "The number 13 is represented by 00001101. Likewise, 17 is represented by 00010001. The bit-wise XOR of 13 and 17 is therefore 00011100, or 28:", "The ^ operator can be used as a shorthand for np.bitwise_xor on ndarrays."]}, {"name": "numpy.blackman()", "path": "reference/generated/numpy.blackman", "type": "numpy.blackman", "text": ["Return the Blackman window.", "The Blackman window is a taper formed by using the first three terms of a summation of cosines. It was designed to have close to the minimal leakage possible. It is close to optimal, only slightly worse than a Kaiser window.", "Number of points in the output window. If zero or less, an empty array is returned.", "The window, with the maximum value normalized to one (the value one appears only if the number of samples is odd).", "See also", "The Blackman window is defined as", "Most references to the Blackman window come from the signal processing literature, where it is used as one of many windowing functions for smoothing values. It is also known as an apodization (which means \u201cremoving the foot\u201d, i.e. smoothing discontinuities at the beginning and end of the sampled signal) or tapering function. It is known as a \u201cnear optimal\u201d tapering function, almost as good (by some measures) as the kaiser window.", "Blackman, R.B. and Tukey, J.W., (1958) The measurement of power spectra, Dover Publications, New York.", "Oppenheim, A.V., and R.W. Schafer. Discrete-Time Signal Processing. Upper Saddle River, NJ: Prentice-Hall, 1999, pp. 468-471.", "Plot the window and the frequency response:"]}, {"name": "numpy.block()", "path": "reference/generated/numpy.block", "type": "numpy.block", "text": ["Assemble an nd-array from nested lists of blocks.", "Blocks in the innermost lists are concatenated (see concatenate) along the last dimension (-1), then these are concatenated along the second-last dimension (-2), and so on until the outermost list is reached.", "Blocks can be of any dimension, but will not be broadcasted using the normal rules. Instead, leading axes of size 1 are inserted, to make block.ndim the same for all blocks. This is primarily useful for working with scalars, and means that code like np.block([v, 1]) is valid, where v.ndim == 1.", "When the nested list is two levels deep, this allows block matrices to be constructed from their components.", "New in version 1.13.0.", "If passed a single ndarray or scalar (a nested list of depth 0), this is returned unmodified (and not copied).", "Elements shapes must match along the appropriate axes (without broadcasting), but leading 1s will be prepended to the shape as necessary to make the dimensions match.", "The array assembled from the given blocks.", "The dimensionality of the output is equal to the greatest of: * the dimensionality of all the inputs * the depth to which the input list is nested", "See also", "Join a sequence of arrays along an existing axis.", "Join a sequence of arrays along a new axis.", "Stack arrays in sequence vertically (row wise).", "Stack arrays in sequence horizontally (column wise).", "Stack arrays in sequence depth wise (along third axis).", "Stack 1-D arrays as columns into a 2-D array.", "Split an array into multiple sub-arrays vertically (row-wise).", "When called with only scalars, np.block is equivalent to an ndarray call. So np.block([[1, 2], [3, 4]]) is equivalent to np.array([[1, 2], [3, 4]]).", "This function does not enforce that the blocks lie on a fixed grid. np.block([[a, b], [c, d]]) is not restricted to arrays of the form:", "But is also allowed to produce, for some a, b, c, d:", "Since concatenation happens along the last axis first, block is _not_ capable of producing the following directly:", "Matlab\u2019s \u201csquare bracket stacking\u201d, [A, B, ...; p, q, ...], is equivalent to np.block([[A, B, ...], [p, q, ...]]).", "The most common use of this function is to build a block matrix", "With a list of depth 1, block can be used as hstack", "With a list of depth 2, block can be used in place of vstack:", "It can also be used in places of atleast_1d and atleast_2d"]}, {"name": "numpy.bmat()", "path": "reference/generated/numpy.bmat", "type": "numpy.bmat", "text": ["Build a matrix object from a string, nested sequence, or array.", "Input data. If a string, variables in the current scope may be referenced by name.", "A dictionary that replaces local operands in current frame. Ignored if obj is not a string or gdict is None.", "A dictionary that replaces global operands in current frame. Ignored if obj is not a string.", "Returns a matrix object, which is a specialized 2-D array.", "See also", "A generalization of this function for N-d arrays, that returns normal ndarrays.", "All the following expressions construct the same block matrix:"]}, {"name": "numpy.broadcast", "path": "reference/generated/numpy.broadcast", "type": "numpy.broadcast", "text": ["Produce an object that mimics broadcasting.", "Input parameters.", "Broadcast the input parameters against one another, and return an object that encapsulates the result. Amongst others, it has shape and nd properties, and may be used as an iterator.", "See also", "Manually adding two vectors, using broadcasting:", "Compare against built-in broadcasting:", "current index in broadcasted result", "tuple of iterators along self\u2019s \u201ccomponents.\u201d", "Number of dimensions of broadcasted result.", "Number of dimensions of broadcasted result.", "Number of iterators possessed by the broadcasted result.", "Shape of broadcasted result.", "Total size of broadcasted result.", "reset()", "Reset the broadcasted result's iterator(s)."]}, {"name": "numpy.broadcast_arrays()", "path": "reference/generated/numpy.broadcast_arrays", "type": "numpy.broadcast_arrays", "text": ["Broadcast any number of arrays against each other.", "The arrays to broadcast.", "If True, then sub-classes will be passed-through, otherwise the returned arrays will be forced to be a base-class array (default).", "These arrays are views on the original arrays. They are typically not contiguous. Furthermore, more than one element of a broadcasted array may refer to a single memory location. If you need to write to the arrays, make copies first. While you can set the writable flag True, writing to a single output value may end up changing more than one location in the output array.", "Deprecated since version 1.17: The output is currently marked so that if written to, a deprecation warning will be emitted. A future version will set the writable flag False so writing to it will raise an error.", "See also", "Here is a useful idiom for getting contiguous copies instead of non-contiguous views."]}, {"name": "numpy.broadcast_shapes()", "path": "reference/generated/numpy.broadcast_shapes", "type": "numpy.broadcast_shapes", "text": ["Broadcast the input shapes into a single shape.", "Learn more about broadcasting here.", "New in version 1.20.0.", "The shapes to be broadcast against each other.", "Broadcasted shape.", "If the shapes are not compatible and cannot be broadcast according to NumPy\u2019s broadcasting rules.", "See also"]}, {"name": "numpy.broadcast_to()", "path": "reference/generated/numpy.broadcast_to", "type": "numpy.broadcast_to", "text": ["Broadcast an array to a new shape.", "The array to broadcast.", "The shape of the desired array. A single integer i is interpreted as (i,).", "If True, then sub-classes will be passed-through, otherwise the returned array will be forced to be a base-class array (default).", "A readonly view on the original array with the given shape. It is typically not contiguous. Furthermore, more than one element of a broadcasted array may refer to a single memory location.", "If the array is not compatible with the new shape according to NumPy\u2019s broadcasting rules.", "See also", "New in version 1.10.0."]}, {"name": "numpy.busday_count()", "path": "reference/generated/numpy.busday_count", "type": "numpy.busday_count", "text": ["Counts the number of valid days between begindates and enddates, not including the day of enddates.", "If enddates specifies a date value that is earlier than the corresponding begindates date value, the count will be negative.", "New in version 1.7.0.", "The array of the first dates for counting.", "The array of the end dates for counting, which are excluded from the count themselves.", "A seven-element array indicating which of Monday through Sunday are valid days. May be specified as a length-seven list or array, like [1,1,1,1,1,0,0]; a length-seven string, like \u20181111100\u2019; or a string like \u201cMon Tue Wed Thu Fri\u201d, made up of 3-character abbreviations for weekdays, optionally separated by white space. Valid abbreviations are: Mon Tue Wed Thu Fri Sat Sun", "An array of dates to consider as invalid dates. They may be specified in any order, and NaT (not-a-time) dates are ignored. This list is saved in a normalized form that is suited for fast calculations of valid days.", "A busdaycalendar object which specifies the valid days. If this parameter is provided, neither weekmask nor holidays may be provided.", "If provided, this array is filled with the result.", "An array with a shape from broadcasting begindates and enddates together, containing the number of valid days between the begin and end dates.", "See also", "An object that specifies a custom set of valid days.", "Returns a boolean array indicating valid days.", "Applies an offset counted in valid days."]}, {"name": "numpy.busday_offset()", "path": "reference/generated/numpy.busday_offset", "type": "numpy.busday_offset", "text": ["First adjusts the date to fall on a valid day according to the roll rule, then applies offsets to the given dates counted in valid days.", "New in version 1.7.0.", "The array of dates to process.", "The array of offsets, which is broadcast with dates.", "How to treat dates that do not fall on a valid day. The default is \u2018raise\u2019.", "A seven-element array indicating which of Monday through Sunday are valid days. May be specified as a length-seven list or array, like [1,1,1,1,1,0,0]; a length-seven string, like \u20181111100\u2019; or a string like \u201cMon Tue Wed Thu Fri\u201d, made up of 3-character abbreviations for weekdays, optionally separated by white space. Valid abbreviations are: Mon Tue Wed Thu Fri Sat Sun", "An array of dates to consider as invalid dates. They may be specified in any order, and NaT (not-a-time) dates are ignored. This list is saved in a normalized form that is suited for fast calculations of valid days.", "A busdaycalendar object which specifies the valid days. If this parameter is provided, neither weekmask nor holidays may be provided.", "If provided, this array is filled with the result.", "An array with a shape from broadcasting dates and offsets together, containing the dates with offsets applied.", "See also", "An object that specifies a custom set of valid days.", "Returns a boolean array indicating valid days.", "Counts how many valid days are in a half-open date range."]}, {"name": "numpy.busdaycalendar()", "path": "reference/generated/numpy.busdaycalendar", "type": "numpy.busdaycalendar", "text": ["A business day calendar object that efficiently stores information defining valid days for the busday family of functions.", "The default valid days are Monday through Friday (\u201cbusiness days\u201d). A busdaycalendar object can be specified with any set of weekly valid days, plus an optional \u201choliday\u201d dates that always will be invalid.", "Once a busdaycalendar object is created, the weekmask and holidays cannot be modified.", "New in version 1.7.0.", "A seven-element array indicating which of Monday through Sunday are valid days. May be specified as a length-seven list or array, like [1,1,1,1,1,0,0]; a length-seven string, like \u20181111100\u2019; or a string like \u201cMon Tue Wed Thu Fri\u201d, made up of 3-character abbreviations for weekdays, optionally separated by white space. Valid abbreviations are: Mon Tue Wed Thu Fri Sat Sun", "An array of dates to consider as invalid dates, no matter which weekday they fall upon. Holiday dates may be specified in any order, and NaT (not-a-time) dates are ignored. This list is saved in a normalized form that is suited for fast calculations of valid days.", "A business day calendar object containing the specified weekmask and holidays values.", "See also", "Returns a boolean array indicating valid days.", "Applies an offset counted in valid days.", "Counts how many valid days are in a half-open date range.", "A copy of the seven-element boolean mask indicating valid days.", "A copy of the holiday array indicating additional invalid days."]}, {"name": "numpy.byte", "path": "reference/arrays.scalars#numpy.byte", "type": "Scalars", "text": ["Signed integer type, compatible with C char.", "'b'", "numpy.int8: 8-bit signed integer (-128 to 127)."]}, {"name": "numpy.byte_bounds()", "path": "reference/generated/numpy.byte_bounds", "type": "numpy.byte_bounds", "text": ["Returns pointers to the end-points of an array.", "Input array. It must conform to the Python-side of the array interface.", "The first integer is the first byte of the array, the second integer is just past the last byte of the array. If a is not contiguous it will not use every byte between the (low, high) values."]}, {"name": "numpy.bytes_", "path": "reference/arrays.scalars#numpy.bytes_", "type": "Scalars", "text": ["A byte string.", "When used in arrays, this type strips trailing null bytes.", "'S'", "numpy.string_"]}, {"name": "numpy.c_", "path": "reference/generated/numpy.c_", "type": "numpy.c_", "text": ["Translates slice objects to concatenation along the second axis.", "This is short-hand for np.r_['-1,2,0', index expression], which is useful because of its common occurrence. In particular, arrays will be stacked along their last axis after being upgraded to at least 2-D with 1\u2019s post-pended to the shape (column vectors made out of 1-D arrays).", "See also", "Stack 1-D arrays as columns into a 2-D array.", "For more detailed documentation."]}, {"name": "numpy.can_cast()", "path": "reference/generated/numpy.can_cast", "type": "numpy.can_cast", "text": ["Returns True if cast between data types can occur according to the casting rule. If from is a scalar or array scalar, also returns True if the scalar value can be cast without overflow or truncation to an integer.", "Data type, scalar, or array to cast from.", "Data type to cast to.", "Controls what kind of data casting may occur.", "True if cast can occur according to the casting rule.", "See also", "Changed in version 1.17.0: Casting between a simple data type and a structured one is possible only for \u201cunsafe\u201d casting. Casting to multiple fields is allowed, but casting from multiple fields is not.", "Changed in version 1.9.0: Casting from numeric to string types in \u2018safe\u2019 casting mode requires that the string dtype length is long enough to store the maximum integer/float value converted.", "Basic examples", "Casting scalars", "Array scalar checks the value, array does not", "Using the casting rules"]}, {"name": "numpy.cbrt()", "path": "reference/generated/numpy.cbrt", "type": "numpy.cbrt", "text": ["Return the cube-root of an array, element-wise.", "New in version 1.10.0.", "The values whose cube-roots are required.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "An array of the same shape as x, containing the cube cube-root of each element in x. If out was provided, y is a reference to it. This is a scalar if x is a scalar."]}, {"name": "numpy.cdouble()", "path": "reference/arrays.scalars#numpy.cdouble", "type": "Scalars", "text": ["Complex number type composed of two double-precision floating-point numbers, compatible with Python complex.", "'D'", "numpy.cfloat", "numpy.complex_", "numpy.complex128: Complex number type composed of 2 64-bit-precision floating-point numbers."]}, {"name": "numpy.ceil()", "path": "reference/generated/numpy.ceil", "type": "numpy.ceil", "text": ["Return the ceiling of the input, element-wise.", "The ceil of the scalar x is the smallest integer i, such that i >= x. It is often denoted as \\(\\lceil x \\rceil\\).", "Input data.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The ceiling of each element in x, with float dtype. This is a scalar if x is a scalar.", "See also"]}, {"name": "numpy.cfloat", "path": "reference/arrays.scalars#numpy.cfloat", "type": "Scalars", "text": ["alias of numpy.cdouble"]}, {"name": "numpy.char.chararray()", "path": "reference/generated/numpy.char.chararray", "type": "numpy.char.chararray", "text": ["Provides a convenient view on arrays of string and unicode values.", "Note", "The chararray class exists for backwards compatibility with Numarray, it is not recommended for new development. Starting from numpy 1.4, if one needs arrays of strings, it is recommended to use arrays of dtype object_, string_ or unicode_, and use the free functions in the numpy.char module for fast vectorized string operations.", "Versus a regular NumPy array of type str or unicode, this class adds the following functionality:", "chararrays should be created using numpy.char.array or numpy.char.asarray, rather than this constructor directly.", "This constructor creates the array, using buffer (with offset and strides) if it is not None. If buffer is None, then constructs a new array with strides in \u201cC order\u201d, unless both len(shape) >= 2 and order='F', in which case strides is in \u201cFortran order\u201d.", "Shape of the array.", "Length of each array element, in number of characters. Default is 1.", "Are the array elements of type unicode (True) or string (False). Default is False.", "Memory address of the start of the array data. Default is None, in which case a new array is created.", "Fixed stride displacement from the beginning of an axis? Default is 0. Needs to be >=0.", "Strides for the array (see ndarray.strides for full description). Default is None.", "The order in which the array data is stored in memory: \u2018C\u2019 -> \u201crow major\u201d order (the default), \u2018F\u2019 -> \u201ccolumn major\u201d (Fortran) order.", "The transposed array.", "Base object if memory is from some other object.", "An object to simplify the interaction of the array with the ctypes module.", "Python buffer object pointing to the start of the array\u2019s data.", "Data-type of the array\u2019s elements.", "Information about the memory layout of the array.", "A 1-D iterator over the array.", "The imaginary part of the array.", "Length of one array element in bytes.", "Total bytes consumed by the elements of the array.", "Number of array dimensions.", "The real part of the array.", "Tuple of array dimensions.", "Number of elements in the array.", "Tuple of bytes to step in each dimension when traversing an array.", "astype(dtype[, order, casting, subok, copy])", "Copy of the array, cast to a specified type.", "argsort([axis, kind, order])", "Returns the indices that would sort this array.", "copy([order])", "Return a copy of the array.", "count(sub[, start, end])", "Returns an array with the number of non-overlapping occurrences of substring sub in the range [start, end].", "decode([encoding, errors])", "Calls str.decode element-wise.", "dump(file)", "Dump a pickle of the array to the specified file.", "dumps()", "Returns the pickle of the array as a string.", "encode([encoding, errors])", "Calls str.encode element-wise.", "endswith(suffix[, start, end])", "Returns a boolean array which is True where the string element in self ends with suffix, otherwise False.", "expandtabs([tabsize])", "Return a copy of each string element where all tab characters are replaced by one or more spaces.", "fill(value)", "Fill the array with a scalar value.", "find(sub[, start, end])", "For each element, return the lowest index in the string where substring sub is found.", "flatten([order])", "Return a copy of the array collapsed into one dimension.", "getfield(dtype[, offset])", "Returns a field of the given array as a certain type.", "index(sub[, start, end])", "Like find, but raises ValueError when the substring is not found.", "isalnum()", "Returns true for each element if all characters in the string are alphanumeric and there is at least one character, false otherwise.", "isalpha()", "Returns true for each element if all characters in the string are alphabetic and there is at least one character, false otherwise.", "isdecimal()", "For each element in self, return True if there are only decimal characters in the element.", "isdigit()", "Returns true for each element if all characters in the string are digits and there is at least one character, false otherwise.", "islower()", "Returns true for each element if all cased characters in the string are lowercase and there is at least one cased character, false otherwise.", "isnumeric()", "For each element in self, return True if there are only numeric characters in the element.", "isspace()", "Returns true for each element if there are only whitespace characters in the string and there is at least one character, false otherwise.", "istitle()", "Returns true for each element if the element is a titlecased string and there is at least one character, false otherwise.", "isupper()", "Returns true for each element if all cased characters in the string are uppercase and there is at least one character, false otherwise.", "item(*args)", "Copy an element of an array to a standard Python scalar and return it.", "join(seq)", "Return a string which is the concatenation of the strings in the sequence seq.", "ljust(width[, fillchar])", "Return an array with the elements of self left-justified in a string of length width.", "lower()", "Return an array with the elements of self converted to lowercase.", "lstrip([chars])", "For each element in self, return a copy with the leading characters removed.", "nonzero()", "Return the indices of the elements that are non-zero.", "put(indices, values[, mode])", "Set a.flat[n] = values[n] for all n in indices.", "ravel([order])", "Return a flattened array.", "repeat(repeats[, axis])", "Repeat elements of an array.", "replace(old, new[, count])", "For each element in self, return a copy of the string with all occurrences of substring old replaced by new.", "reshape(shape[, order])", "Returns an array containing the same data with a new shape.", "resize(new_shape[, refcheck])", "Change shape and size of array in-place.", "rfind(sub[, start, end])", "For each element in self, return the highest index in the string where substring sub is found, such that sub is contained within [start, end].", "rindex(sub[, start, end])", "Like rfind, but raises ValueError when the substring sub is not found.", "rjust(width[, fillchar])", "Return an array with the elements of self right-justified in a string of length width.", "rsplit([sep, maxsplit])", "For each element in self, return a list of the words in the string, using sep as the delimiter string.", "rstrip([chars])", "For each element in self, return a copy with the trailing characters removed.", "searchsorted(v[, side, sorter])", "Find indices where elements of v should be inserted in a to maintain order.", "setfield(val, dtype[, offset])", "Put a value into a specified place in a field defined by a data-type.", "setflags([write, align, uic])", "Set array flags WRITEABLE, ALIGNED, (WRITEBACKIFCOPY and UPDATEIFCOPY), respectively.", "sort([axis, kind, order])", "Sort an array in-place.", "split([sep, maxsplit])", "For each element in self, return a list of the words in the string, using sep as the delimiter string.", "splitlines([keepends])", "For each element in self, return a list of the lines in the element, breaking at line boundaries.", "squeeze([axis])", "Remove axes of length one from a.", "startswith(prefix[, start, end])", "Returns a boolean array which is True where the string element in self starts with prefix, otherwise False.", "strip([chars])", "For each element in self, return a copy with the leading and trailing characters removed.", "swapaxes(axis1, axis2)", "Return a view of the array with axis1 and axis2 interchanged.", "swapcase()", "For each element in self, return a copy of the string with uppercase characters converted to lowercase and vice versa.", "take(indices[, axis, out, mode])", "Return an array formed from the elements of a at the given indices.", "title()", "For each element in self, return a titlecased version of the string: words start with uppercase characters, all remaining cased characters are lowercase.", "tofile(fid[, sep, format])", "Write array to a file as text or binary (default).", "tolist()", "Return the array as an a.ndim-levels deep nested list of Python scalars.", "tostring([order])", "A compatibility alias for tobytes, with exactly the same behavior.", "translate(table[, deletechars])", "For each element in self, return a copy of the string where all characters occurring in the optional argument deletechars are removed, and the remaining characters have been mapped through the given translation table.", "transpose(*axes)", "Returns a view of the array with axes transposed.", "upper()", "Return an array with the elements of self converted to uppercase.", "view([dtype][, type])", "New view of array with the same data.", "zfill(width)", "Return the numeric string left-filled with zeros in a string of length width."]}, {"name": "numpy.chararray()", "path": "reference/generated/numpy.chararray", "type": "numpy.chararray", "text": ["Provides a convenient view on arrays of string and unicode values.", "Note", "The chararray class exists for backwards compatibility with Numarray, it is not recommended for new development. Starting from numpy 1.4, if one needs arrays of strings, it is recommended to use arrays of dtype object_, string_ or unicode_, and use the free functions in the numpy.char module for fast vectorized string operations.", "Versus a regular NumPy array of type str or unicode, this class adds the following functionality:", "chararrays should be created using numpy.char.array or numpy.char.asarray, rather than this constructor directly.", "This constructor creates the array, using buffer (with offset and strides) if it is not None. If buffer is None, then constructs a new array with strides in \u201cC order\u201d, unless both len(shape) >= 2 and order='F', in which case strides is in \u201cFortran order\u201d.", "Shape of the array.", "Length of each array element, in number of characters. Default is 1.", "Are the array elements of type unicode (True) or string (False). Default is False.", "Memory address of the start of the array data. Default is None, in which case a new array is created.", "Fixed stride displacement from the beginning of an axis? Default is 0. Needs to be >=0.", "Strides for the array (see ndarray.strides for full description). Default is None.", "The order in which the array data is stored in memory: \u2018C\u2019 -> \u201crow major\u201d order (the default), \u2018F\u2019 -> \u201ccolumn major\u201d (Fortran) order.", "The transposed array.", "Base object if memory is from some other object.", "An object to simplify the interaction of the array with the ctypes module.", "Python buffer object pointing to the start of the array\u2019s data.", "Data-type of the array\u2019s elements.", "Information about the memory layout of the array.", "A 1-D iterator over the array.", "The imaginary part of the array.", "Length of one array element in bytes.", "Total bytes consumed by the elements of the array.", "Number of array dimensions.", "The real part of the array.", "Tuple of array dimensions.", "Number of elements in the array.", "Tuple of bytes to step in each dimension when traversing an array.", "astype(dtype[, order, casting, subok, copy])", "Copy of the array, cast to a specified type.", "argsort([axis, kind, order])", "Returns the indices that would sort this array.", "copy([order])", "Return a copy of the array.", "count(sub[, start, end])", "Returns an array with the number of non-overlapping occurrences of substring sub in the range [start, end].", "decode([encoding, errors])", "Calls str.decode element-wise.", "dump(file)", "Dump a pickle of the array to the specified file.", "dumps()", "Returns the pickle of the array as a string.", "encode([encoding, errors])", "Calls str.encode element-wise.", "endswith(suffix[, start, end])", "Returns a boolean array which is True where the string element in self ends with suffix, otherwise False.", "expandtabs([tabsize])", "Return a copy of each string element where all tab characters are replaced by one or more spaces.", "fill(value)", "Fill the array with a scalar value.", "find(sub[, start, end])", "For each element, return the lowest index in the string where substring sub is found.", "flatten([order])", "Return a copy of the array collapsed into one dimension.", "getfield(dtype[, offset])", "Returns a field of the given array as a certain type.", "index(sub[, start, end])", "Like find, but raises ValueError when the substring is not found.", "isalnum()", "Returns true for each element if all characters in the string are alphanumeric and there is at least one character, false otherwise.", "isalpha()", "Returns true for each element if all characters in the string are alphabetic and there is at least one character, false otherwise.", "isdecimal()", "For each element in self, return True if there are only decimal characters in the element.", "isdigit()", "Returns true for each element if all characters in the string are digits and there is at least one character, false otherwise.", "islower()", "Returns true for each element if all cased characters in the string are lowercase and there is at least one cased character, false otherwise.", "isnumeric()", "For each element in self, return True if there are only numeric characters in the element.", "isspace()", "Returns true for each element if there are only whitespace characters in the string and there is at least one character, false otherwise.", "istitle()", "Returns true for each element if the element is a titlecased string and there is at least one character, false otherwise.", "isupper()", "Returns true for each element if all cased characters in the string are uppercase and there is at least one character, false otherwise.", "item(*args)", "Copy an element of an array to a standard Python scalar and return it.", "join(seq)", "Return a string which is the concatenation of the strings in the sequence seq.", "ljust(width[, fillchar])", "Return an array with the elements of self left-justified in a string of length width.", "lower()", "Return an array with the elements of self converted to lowercase.", "lstrip([chars])", "For each element in self, return a copy with the leading characters removed.", "nonzero()", "Return the indices of the elements that are non-zero.", "put(indices, values[, mode])", "Set a.flat[n] = values[n] for all n in indices.", "ravel([order])", "Return a flattened array.", "repeat(repeats[, axis])", "Repeat elements of an array.", "replace(old, new[, count])", "For each element in self, return a copy of the string with all occurrences of substring old replaced by new.", "reshape(shape[, order])", "Returns an array containing the same data with a new shape.", "resize(new_shape[, refcheck])", "Change shape and size of array in-place.", "rfind(sub[, start, end])", "For each element in self, return the highest index in the string where substring sub is found, such that sub is contained within [start, end].", "rindex(sub[, start, end])", "Like rfind, but raises ValueError when the substring sub is not found.", "rjust(width[, fillchar])", "Return an array with the elements of self right-justified in a string of length width.", "rsplit([sep, maxsplit])", "For each element in self, return a list of the words in the string, using sep as the delimiter string.", "rstrip([chars])", "For each element in self, return a copy with the trailing characters removed.", "searchsorted(v[, side, sorter])", "Find indices where elements of v should be inserted in a to maintain order.", "setfield(val, dtype[, offset])", "Put a value into a specified place in a field defined by a data-type.", "setflags([write, align, uic])", "Set array flags WRITEABLE, ALIGNED, (WRITEBACKIFCOPY and UPDATEIFCOPY), respectively.", "sort([axis, kind, order])", "Sort an array in-place.", "split([sep, maxsplit])", "For each element in self, return a list of the words in the string, using sep as the delimiter string.", "splitlines([keepends])", "For each element in self, return a list of the lines in the element, breaking at line boundaries.", "squeeze([axis])", "Remove axes of length one from a.", "startswith(prefix[, start, end])", "Returns a boolean array which is True where the string element in self starts with prefix, otherwise False.", "strip([chars])", "For each element in self, return a copy with the leading and trailing characters removed.", "swapaxes(axis1, axis2)", "Return a view of the array with axis1 and axis2 interchanged.", "swapcase()", "For each element in self, return a copy of the string with uppercase characters converted to lowercase and vice versa.", "take(indices[, axis, out, mode])", "Return an array formed from the elements of a at the given indices.", "title()", "For each element in self, return a titlecased version of the string: words start with uppercase characters, all remaining cased characters are lowercase.", "tofile(fid[, sep, format])", "Write array to a file as text or binary (default).", "tolist()", "Return the array as an a.ndim-levels deep nested list of Python scalars.", "tostring([order])", "A compatibility alias for tobytes, with exactly the same behavior.", "translate(table[, deletechars])", "For each element in self, return a copy of the string where all characters occurring in the optional argument deletechars are removed, and the remaining characters have been mapped through the given translation table.", "transpose(*axes)", "Returns a view of the array with axes transposed.", "upper()", "Return an array with the elements of self converted to uppercase.", "view([dtype][, type])", "New view of array with the same data.", "zfill(width)", "Return the numeric string left-filled with zeros in a string of length width."]}, {"name": "numpy.choose()", "path": "reference/generated/numpy.choose", "type": "numpy.choose", "text": ["Construct an array from an index array and a list of arrays to choose from.", "First of all, if confused or uncertain, definitely look at the Examples - in its full generality, this function is less simple than it might seem from the following code description (below ndi = numpy.lib.index_tricks):", "np.choose(a,c) == np.array([c[a[I]][I] for I in ndi.ndindex(a.shape)]).", "But this omits some subtleties. Here is a fully general summary:", "Given an \u201cindex\u201d array (a) of integers and a sequence of n arrays (choices), a and each choice array are first broadcast, as necessary, to arrays of a common shape; calling these Ba and Bchoices[i], i = 0,\u2026,n-1 we have that, necessarily, Ba.shape == Bchoices[i].shape for each i. Then, a new array with shape Ba.shape is created as follows:", "This array must contain integers in [0, n-1], where n is the number of choices, unless mode=wrap or mode=clip, in which cases any integers are permissible.", "Choice arrays. a and all of the choices must be broadcastable to the same shape. If choices is itself an array (not recommended), then its outermost dimension (i.e., the one corresponding to choices.shape[0]) is taken as defining the \u201csequence\u201d.", "If provided, the result will be inserted into this array. It should be of the appropriate shape and dtype. Note that out is always buffered if mode='raise'; use other modes for better performance.", "Specifies how indices outside [0, n-1] will be treated:", "The merged result.", "If a and each choice array are not all broadcastable to the same shape.", "See also", "equivalent method", "Preferable if choices is an array", "To reduce the chance of misinterpretation, even though the following \u201cabuse\u201d is nominally supported, choices should neither be, nor be thought of as, a single array, i.e., the outermost sequence-like container should be either a list or a tuple.", "A couple examples illustrating how choose broadcasts:"]}, {"name": "numpy.clip()", "path": "reference/generated/numpy.clip", "type": "numpy.clip", "text": ["Clip (limit) the values in an array.", "Given an interval, values outside the interval are clipped to the interval edges. For example, if an interval of [0, 1] is specified, values smaller than 0 become 0, and values larger than 1 become 1.", "Equivalent to but faster than np.minimum(a_max, np.maximum(a, a_min)).", "No check is performed to ensure a_min < a_max.", "Array containing elements to clip.", "Minimum and maximum value. If None, clipping is not performed on the corresponding edge. Only one of a_min and a_max may be None. Both are broadcast against a.", "The results will be placed in this array. It may be the input array for in-place clipping. out must be of the right shape to hold the output. Its type is preserved.", "For other keyword-only arguments, see the ufunc docs.", "New in version 1.17.0.", "An array with the elements of a, but where values < a_min are replaced with a_min, and those > a_max with a_max.", "See also", "When a_min is greater than a_max, clip returns an array in which all values are equal to a_max, as shown in the second example."]}, {"name": "numpy.clongdouble", "path": "reference/arrays.scalars#numpy.clongdouble", "type": "Scalars", "text": ["Complex number type composed of two extended-precision floating-point numbers.", "'G'", "numpy.clongfloat", "numpy.longcomplex", "numpy.complex256: Complex number type composed of 2 128-bit extended-precision floating-point numbers."]}, {"name": "numpy.clongfloat", "path": "reference/arrays.scalars#numpy.clongfloat", "type": "Scalars", "text": ["alias of numpy.clongdouble"]}, {"name": "numpy.column_stack()", "path": "reference/generated/numpy.column_stack", "type": "numpy.column_stack", "text": ["Stack 1-D arrays as columns into a 2-D array.", "Take a sequence of 1-D arrays and stack them as columns to make a single 2-D array. 2-D arrays are stacked as-is, just like with hstack. 1-D arrays are turned into 2-D columns first.", "Arrays to stack. All of them must have the same first dimension.", "The array formed by stacking the given arrays.", "See also"]}, {"name": "numpy.common_type()", "path": "reference/generated/numpy.common_type", "type": "numpy.common_type", "text": ["Return a scalar type which is common to the input arrays.", "The return type will always be an inexact (i.e. floating point) scalar type, even if all the arrays are integer arrays. If one of the inputs is an integer array, the minimum precision type that is returned is a 64-bit floating point dtype.", "All input arrays except int64 and uint64 can be safely cast to the returned dtype without loss of information.", "Input arrays.", "Data type code.", "See also"]}, {"name": "numpy.complex128", "path": "reference/arrays.scalars#numpy.complex128", "type": "Scalars", "text": ["alias of numpy.cdouble"]}, {"name": "numpy.complex192", "path": "reference/arrays.scalars#numpy.complex192", "type": "Scalars", "text": ["Alias for numpy.clongdouble, named after its size in bits. The existence of these aliases depends on the platform."]}, {"name": "numpy.complex256", "path": "reference/arrays.scalars#numpy.complex256", "type": "Scalars", "text": ["Alias for numpy.clongdouble, named after its size in bits. The existence of these aliases depends on the platform."]}, {"name": "numpy.complex64", "path": "reference/arrays.scalars#numpy.complex64", "type": "Scalars", "text": ["alias of numpy.csingle"]}, {"name": "numpy.complex_", "path": "reference/arrays.scalars#numpy.complex_", "type": "Scalars", "text": ["alias of numpy.cdouble"]}, {"name": "numpy.compress()", "path": "reference/generated/numpy.compress", "type": "numpy.compress", "text": ["Return selected slices of an array along given axis.", "When working along a given axis, a slice along that axis is returned in output for each index where condition evaluates to True. When working on a 1-D array, compress is equivalent to extract.", "Array that selects which entries to return. If len(condition) is less than the size of a along the given axis, then output is truncated to the length of the condition array.", "Array from which to extract a part.", "Axis along which to take slices. If None (default), work on the flattened array.", "Output array. Its type is preserved and it must be of the right shape to hold the output.", "A copy of a without the slices along axis for which condition is false.", "See also", "Equivalent method in ndarray", "Equivalent method when working on 1-D arrays", "Working on the flattened array does not return slices along an axis but selects elements."]}, {"name": "numpy.concatenate()", "path": "reference/generated/numpy.concatenate", "type": "numpy.concatenate", "text": ["Join a sequence of arrays along an existing axis.", "The arrays must have the same shape, except in the dimension corresponding to axis (the first, by default).", "The axis along which the arrays will be joined. If axis is None, arrays are flattened before use. Default is 0.", "If provided, the destination to place the result. The shape must be correct, matching that of what concatenate would have returned if no out argument were specified.", "If provided, the destination array will have this dtype. Cannot be provided together with out.", "New in version 1.20.0.", "Controls what kind of data casting may occur. Defaults to \u2018same_kind\u2019.", "New in version 1.20.0.", "The concatenated array.", "See also", "Concatenate function that preserves input masks.", "Split an array into multiple sub-arrays of equal or near-equal size.", "Split array into a list of multiple sub-arrays of equal size.", "Split array into multiple sub-arrays horizontally (column wise).", "Split array into multiple sub-arrays vertically (row wise).", "Split array into multiple sub-arrays along the 3rd axis (depth).", "Stack a sequence of arrays along a new axis.", "Assemble arrays from blocks.", "Stack arrays in sequence horizontally (column wise).", "Stack arrays in sequence vertically (row wise).", "Stack arrays in sequence depth wise (along third dimension).", "Stack 1-D arrays as columns into a 2-D array.", "When one or more of the arrays to be concatenated is a MaskedArray, this function will return a MaskedArray object instead of an ndarray, but the input masks are not preserved. In cases where a MaskedArray is expected as input, use the ma.concatenate function from the masked array module instead.", "This function will not preserve masking of MaskedArray inputs."]}, {"name": "numpy.conj()", "path": "reference/generated/numpy.conj", "type": "numpy.conj", "text": ["Return the complex conjugate, element-wise.", "The complex conjugate of a complex number is obtained by changing the sign of its imaginary part.", "Input value.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The complex conjugate of x, with same dtype as y. This is a scalar if x is a scalar.", "conj is an alias for conjugate:"]}, {"name": "numpy.conjugate()", "path": "reference/generated/numpy.conjugate", "type": "numpy.conjugate", "text": ["Return the complex conjugate, element-wise.", "The complex conjugate of a complex number is obtained by changing the sign of its imaginary part.", "Input value.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The complex conjugate of x, with same dtype as y. This is a scalar if x is a scalar.", "conj is an alias for conjugate:"]}, {"name": "numpy.convolve()", "path": "reference/generated/numpy.convolve", "type": "numpy.convolve", "text": ["Returns the discrete, linear convolution of two one-dimensional sequences.", "The convolution operator is often seen in signal processing, where it models the effect of a linear time-invariant system on a signal [1]. In probability theory, the sum of two independent random variables is distributed according to the convolution of their individual distributions.", "If v is longer than a, the arrays are swapped before computation.", "First one-dimensional input array.", "Second one-dimensional input array.", "By default, mode is \u2018full\u2019. This returns the convolution at each point of overlap, with an output shape of (N+M-1,). At the end-points of the convolution, the signals do not overlap completely, and boundary effects may be seen.", "Mode \u2018same\u2019 returns output of length max(M, N). Boundary effects are still visible.", "Mode \u2018valid\u2019 returns output of length max(M, N) - min(M, N) + 1. The convolution product is only given for points where the signals overlap completely. Values outside the signal boundary have no effect.", "Discrete, linear convolution of a and v.", "See also", "Convolve two arrays using the Fast Fourier Transform.", "Used to construct the convolution operator.", "Polynomial multiplication. Same output as convolve, but also accepts poly1d objects as input.", "The discrete convolution operation is defined as", "It can be shown that a convolution \\(x(t) * y(t)\\) in time/space is equivalent to the multiplication \\(X(f) Y(f)\\) in the Fourier domain, after appropriate padding (padding is necessary to prevent circular convolution). Since multiplication is more efficient (faster) than convolution, the function scipy.signal.fftconvolve exploits the FFT to calculate the convolution of large data-sets.", "Wikipedia, \u201cConvolution\u201d, https://en.wikipedia.org/wiki/Convolution", "Note how the convolution operator flips the second array before \u201csliding\u201d the two across one another:", "Only return the middle values of the convolution. Contains boundary effects, where zeros are taken into account:", "The two arrays are of the same length, so there is only one position where they completely overlap:"]}, {"name": "numpy.copy()", "path": "reference/generated/numpy.copy", "type": "numpy.copy", "text": ["Return an array copy of the given object.", "Input data.", "Controls the memory layout of the copy. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of a as closely as possible. (Note that this function and ndarray.copy are very similar, but have different default values for their order= arguments.)", "If True, then sub-classes will be passed-through, otherwise the returned array will be forced to be a base-class array (defaults to False).", "New in version 1.19.0.", "Array interpretation of a.", "See also", "Preferred method for creating an array copy", "This is equivalent to:", "Create an array x, with a reference y and a copy z:", "Note that, when we modify x, y changes, but not z:", "Note that, np.copy clears previously set WRITEABLE=False flag.", "Note that np.copy is a shallow copy and will not copy object elements within arrays. This is mainly important for arrays containing Python objects. The new array will contain the same object which may lead to surprises if that object can be modified (is mutable):", "To ensure all elements within an object array are copied, use copy.deepcopy:"]}, {"name": "numpy.copysign()", "path": "reference/generated/numpy.copysign", "type": "numpy.copysign", "text": ["Change the sign of x1 to that of x2, element-wise.", "If x2 is a scalar, its sign will be copied to all elements of x1.", "Values to change the sign of.", "The sign of x2 is copied to x1. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The values of x1 with the sign of x2. This is a scalar if both x1 and x2 are scalars."]}, {"name": "numpy.copyto()", "path": "reference/generated/numpy.copyto", "type": "numpy.copyto", "text": ["Copies values from one array to another, broadcasting as necessary.", "Raises a TypeError if the casting rule is violated, and if where is provided, it selects which elements to copy.", "New in version 1.7.0.", "The array into which values are copied.", "The array from which values are copied.", "Controls what kind of data casting may occur when copying.", "A boolean array which is broadcasted to match the dimensions of dst, and selects elements to copy from src to dst wherever it contains the value True."]}, {"name": "numpy.corrcoef()", "path": "reference/generated/numpy.corrcoef", "type": "numpy.corrcoef", "text": ["Return Pearson product-moment correlation coefficients.", "Please refer to the documentation for cov for more detail. The relationship between the correlation coefficient matrix, R, and the covariance matrix, C, is", "The values of R are between -1 and 1, inclusive.", "A 1-D or 2-D array containing multiple variables and observations. Each row of x represents a variable, and each column a single observation of all those variables. Also see rowvar below.", "An additional set of variables and observations. y has the same shape as x.", "If rowvar is True (default), then each row represents a variable, with observations in the columns. Otherwise, the relationship is transposed: each column represents a variable, while the rows contain observations.", "Has no effect, do not use.", "Deprecated since version 1.10.0.", "Has no effect, do not use.", "Deprecated since version 1.10.0.", "Data-type of the result. By default, the return data-type will have at least numpy.float64 precision.", "New in version 1.20.", "The correlation coefficient matrix of the variables.", "See also", "Covariance matrix", "Due to floating point rounding the resulting array may not be Hermitian, the diagonal elements may not be 1, and the elements may not satisfy the inequality abs(a) <= 1. The real and imaginary parts are clipped to the interval [-1, 1] in an attempt to improve on that situation but is not much help in the complex case.", "This function accepts but discards arguments bias and ddof. This is for backwards compatibility with previous versions of this function. These arguments had no effect on the return values of the function and can be safely ignored in this and previous versions of numpy.", "In this example we generate two random arrays, xarr and yarr, and compute the row-wise and column-wise Pearson correlation coefficients, R. Since rowvar is true by default, we first find the row-wise Pearson correlation coefficients between the variables of xarr.", "If we add another set of variables and observations yarr, we can compute the row-wise Pearson correlation coefficients between the variables in xarr and yarr.", "Finally if we use the option rowvar=False, the columns are now being treated as the variables and we will find the column-wise Pearson correlation coefficients between variables in xarr and yarr."]}, {"name": "numpy.correlate()", "path": "reference/generated/numpy.correlate", "type": "numpy.correlate", "text": ["Cross-correlation of two 1-dimensional sequences.", "This function computes the correlation as generally defined in signal processing texts:", "with a and v sequences being zero-padded where necessary and conj being the conjugate.", "Input sequences.", "Refer to the convolve docstring. Note that the default is \u2018valid\u2019, unlike convolve, which uses \u2018full\u2019.", "old_behavior was removed in NumPy 1.10. If you need the old behavior, use multiarray.correlate.", "Discrete cross-correlation of a and v.", "See also", "Discrete, linear convolution of two one-dimensional sequences.", "Old, no conjugate, version of correlate.", "uses FFT which has superior performance on large arrays.", "The definition of correlation above is not unique and sometimes correlation may be defined differently. Another common definition is:", "which is related to c_{av}[k] by c'_{av}[k] = c_{av}[-k].", "numpy.correlate may perform slowly in large arrays (i.e. n = 1e5) because it does not use the FFT to compute the convolution; in that case, scipy.signal.correlate might be preferable.", "Using complex sequences:", "Note that you get the time reversed, complex conjugated result when the two input sequences change places, i.e., c_{va}[k] = c^{*}_{av}[-k]:"]}, {"name": "numpy.cos()", "path": "reference/generated/numpy.cos", "type": "numpy.cos", "text": ["Cosine element-wise.", "Input array in radians.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The corresponding cosine values. This is a scalar if x is a scalar.", "If out is provided, the function writes the result into it, and returns a reference to out. (See Examples)", "M. Abramowitz and I. A. Stegun, Handbook of Mathematical Functions. New York, NY: Dover, 1972."]}, {"name": "numpy.cosh()", "path": "reference/generated/numpy.cosh", "type": "numpy.cosh", "text": ["Hyperbolic cosine, element-wise.", "Equivalent to 1/2 * (np.exp(x) + np.exp(-x)) and np.cos(1j*x).", "Input array.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Output array of same shape as x. This is a scalar if x is a scalar.", "The hyperbolic cosine describes the shape of a hanging cable:"]}, {"name": "numpy.count_nonzero()", "path": "reference/generated/numpy.count_nonzero", "type": "numpy.count_nonzero", "text": ["Counts the number of non-zero values in the array a.", "The word \u201cnon-zero\u201d is in reference to the Python 2.x built-in method __nonzero__() (renamed __bool__() in Python 3.x) of Python objects that tests an object\u2019s \u201ctruthfulness\u201d. For example, any number is considered truthful if it is nonzero, whereas any string is considered truthful if it is not the empty string. Thus, this function (recursively) counts how many elements in a (and in sub-arrays thereof) have their __nonzero__() or __bool__() method evaluated to True.", "The array for which to count non-zeros.", "Axis or tuple of axes along which to count non-zeros. Default is None, meaning that non-zeros will be counted along a flattened version of a.", "New in version 1.12.0.", "If this is set to True, the axes that are counted are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.", "New in version 1.19.0.", "Number of non-zero values in the array along a given axis. Otherwise, the total number of non-zero values in the array is returned.", "See also", "Return the coordinates of all the non-zero values."]}, {"name": "numpy.cov()", "path": "reference/generated/numpy.cov", "type": "numpy.cov", "text": ["Estimate a covariance matrix, given data and weights.", "Covariance indicates the level to which two variables vary together. If we examine N-dimensional samples, \\(X = [x_1, x_2, ... x_N]^T\\), then the covariance matrix element \\(C_{ij}\\) is the covariance of \\(x_i\\) and \\(x_j\\). The element \\(C_{ii}\\) is the variance of \\(x_i\\).", "See the notes for an outline of the algorithm.", "A 1-D or 2-D array containing multiple variables and observations. Each row of m represents a variable, and each column a single observation of all those variables. Also see rowvar below.", "An additional set of variables and observations. y has the same form as that of m.", "If rowvar is True (default), then each row represents a variable, with observations in the columns. Otherwise, the relationship is transposed: each column represents a variable, while the rows contain observations.", "Default normalization (False) is by (N - 1), where N is the number of observations given (unbiased estimate). If bias is True, then normalization is by N. These values can be overridden by using the keyword ddof in numpy versions >= 1.5.", "If not None the default value implied by bias is overridden. Note that ddof=1 will return the unbiased estimate, even if both fweights and aweights are specified, and ddof=0 will return the simple average. See the notes for the details. The default value is None.", "New in version 1.5.", "1-D array of integer frequency weights; the number of times each observation vector should be repeated.", "New in version 1.10.", "1-D array of observation vector weights. These relative weights are typically large for observations considered \u201cimportant\u201d and smaller for observations considered less \u201cimportant\u201d. If ddof=0 the array of weights can be used to assign probabilities to observation vectors.", "New in version 1.10.", "Data-type of the result. By default, the return data-type will have at least numpy.float64 precision.", "New in version 1.20.", "The covariance matrix of the variables.", "See also", "Normalized covariance matrix", "Assume that the observations are in the columns of the observation array m and let f = fweights and a = aweights for brevity. The steps to compute the weighted covariance are as follows:", "Note that when a == 1, the normalization factor v1 / (v1**2 - ddof * v2) goes over to 1 / (np.sum(f) - ddof) as it should.", "Consider two variables, \\(x_0\\) and \\(x_1\\), which correlate perfectly, but in opposite directions:", "Note how \\(x_0\\) increases while \\(x_1\\) decreases. The covariance matrix shows this clearly:", "Note that element \\(C_{0,1}\\), which shows the correlation between \\(x_0\\) and \\(x_1\\), is negative.", "Further, note how x and y are combined:"]}, {"name": "numpy.cross()", "path": "reference/generated/numpy.cross", "type": "numpy.cross", "text": ["Return the cross product of two (arrays of) vectors.", "The cross product of a and b in \\(R^3\\) is a vector perpendicular to both a and b. If a and b are arrays of vectors, the vectors are defined by the last axis of a and b by default, and these axes can have dimensions 2 or 3. Where the dimension of either a or b is 2, the third component of the input vector is assumed to be zero and the cross product calculated accordingly. In cases where both input vectors have dimension 2, the z-component of the cross product is returned.", "Components of the first vector(s).", "Components of the second vector(s).", "Axis of a that defines the vector(s). By default, the last axis.", "Axis of b that defines the vector(s). By default, the last axis.", "Axis of c containing the cross product vector(s). Ignored if both input vectors have dimension 2, as the return is scalar. By default, the last axis.", "If defined, the axis of a, b and c that defines the vector(s) and cross product(s). Overrides axisa, axisb and axisc.", "Vector cross product(s).", "When the dimension of the vector(s) in a and/or b does not equal 2 or 3.", "See also", "Inner product", "Outer product.", "Construct index arrays.", "New in version 1.9.0.", "Supports full broadcasting of the inputs.", "Vector cross-product.", "One vector with dimension 2.", "Equivalently:", "Both vectors with dimension 2.", "Multiple vector cross-products. Note that the direction of the cross product vector is defined by the right-hand rule.", "The orientation of c can be changed using the axisc keyword.", "Change the vector definition of x and y using axisa and axisb."]}, {"name": "numpy.csingle", "path": "reference/arrays.scalars#numpy.csingle", "type": "Scalars", "text": ["Complex number type composed of two single-precision floating-point numbers.", "'F'", "numpy.singlecomplex", "numpy.complex64: Complex number type composed of 2 32-bit-precision floating-point numbers."]}, {"name": "numpy.ctypeslib.as_array()", "path": "reference/routines.ctypeslib", "type": "C-Types Foreign Function Interface ( \n      \n       numpy.ctypeslib\n      \n      )", "text": ["Create a numpy array from a ctypes array or POINTER.", "The numpy array shares the memory with the ctypes object.", "The shape parameter must be given if converting from a ctypes POINTER. The shape parameter is ignored if converting from a ctypes array", "Create and return a ctypes object from a numpy array. Actually anything that exposes the __array_interface__ is accepted.", "Convert a dtype into a ctypes type.", "The dtype to convert", "A ctype scalar, union, array, or struct", "If the conversion is not possible", "This function does not losslessly round-trip in either direction.", "np.dtype(as_ctypes_type(dt)) will:", "as_ctypes_type(np.dtype(ctype)) will:", "It is possible to load a library using", "But there are cross-platform considerations, such as library file extensions, plus the fact Windows will just load the first library it finds with that name. NumPy supplies the load_library function as a convenience.", "Changed in version 1.20.0: Allow libname and loader_path to take any path-like object.", "Name of the library, which can have \u2018lib\u2019 as a prefix, but without an extension.", "Where the library can be found.", "A ctypes library object", "If there is no library with the expected extension, or the library is defective and cannot be loaded.", "Array-checking restype/argtypes.", "An ndpointer instance is used to describe an ndarray in restypes and argtypes specifications. This approach is more flexible than using, for example, POINTER(c_double), since several restrictions can be specified, which are verified upon calling the ctypes function. These include data type, number of dimensions, shape and flags. If a given array does not satisfy the specified restrictions, a TypeError is raised.", "Array data-type.", "Number of array dimensions.", "Array shape.", "Array flags; may be one or more of:", "A type object, which is an _ndtpr instance containing dtype, ndim, shape and flags information.", "If a given array does not satisfy the specified restrictions.", "A ctypes signed integer type of the same size as numpy.intp.", "Depending on the platform, it can be an alias for either c_int, c_long or c_longlong."]}, {"name": "numpy.ctypeslib.as_ctypes()", "path": "reference/routines.ctypeslib#numpy.ctypeslib.as_ctypes", "type": "C-Types Foreign Function Interface ( \n      \n       numpy.ctypeslib\n      \n      )", "text": ["Create and return a ctypes object from a numpy array. Actually anything that exposes the __array_interface__ is accepted."]}, {"name": "numpy.ctypeslib.as_ctypes_type()", "path": "reference/routines.ctypeslib#numpy.ctypeslib.as_ctypes_type", "type": "C-Types Foreign Function Interface ( \n      \n       numpy.ctypeslib\n      \n      )", "text": ["Convert a dtype into a ctypes type.", "The dtype to convert", "A ctype scalar, union, array, or struct", "If the conversion is not possible", "This function does not losslessly round-trip in either direction.", "np.dtype(as_ctypes_type(dt)) will:", "as_ctypes_type(np.dtype(ctype)) will:"]}, {"name": "numpy.ctypeslib.c_intp", "path": "reference/routines.ctypeslib#numpy.ctypeslib.c_intp", "type": "C-Types Foreign Function Interface ( \n      \n       numpy.ctypeslib\n      \n      )", "text": ["A ctypes signed integer type of the same size as numpy.intp.", "Depending on the platform, it can be an alias for either c_int, c_long or c_longlong."]}, {"name": "numpy.ctypeslib.load_library()", "path": "reference/routines.ctypeslib#numpy.ctypeslib.load_library", "type": "C-Types Foreign Function Interface ( \n      \n       numpy.ctypeslib\n      \n      )", "text": ["It is possible to load a library using", "But there are cross-platform considerations, such as library file extensions, plus the fact Windows will just load the first library it finds with that name. NumPy supplies the load_library function as a convenience.", "Changed in version 1.20.0: Allow libname and loader_path to take any path-like object.", "Name of the library, which can have \u2018lib\u2019 as a prefix, but without an extension.", "Where the library can be found.", "A ctypes library object", "If there is no library with the expected extension, or the library is defective and cannot be loaded."]}, {"name": "numpy.ctypeslib.ndpointer()", "path": "reference/routines.ctypeslib#numpy.ctypeslib.ndpointer", "type": "C-Types Foreign Function Interface ( \n      \n       numpy.ctypeslib\n      \n      )", "text": ["Array-checking restype/argtypes.", "An ndpointer instance is used to describe an ndarray in restypes and argtypes specifications. This approach is more flexible than using, for example, POINTER(c_double), since several restrictions can be specified, which are verified upon calling the ctypes function. These include data type, number of dimensions, shape and flags. If a given array does not satisfy the specified restrictions, a TypeError is raised.", "Array data-type.", "Number of array dimensions.", "Array shape.", "Array flags; may be one or more of:", "A type object, which is an _ndtpr instance containing dtype, ndim, shape and flags information.", "If a given array does not satisfy the specified restrictions."]}, {"name": "numpy.cumprod()", "path": "reference/generated/numpy.cumprod", "type": "numpy.cumprod", "text": ["Return the cumulative product of elements along a given axis.", "Input array.", "Axis along which the cumulative product is computed. By default the input is flattened.", "Type of the returned array, as well as of the accumulator in which the elements are multiplied. If dtype is not specified, it defaults to the dtype of a, unless a has an integer dtype with a precision less than that of the default platform integer. In that case, the default platform integer is used instead.", "Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output but the type of the resulting values will be cast if necessary.", "A new array holding the result is returned unless out is specified, in which case a reference to out is returned.", "See also", "Arithmetic is modular when using integer types, and no error is raised on overflow.", "The cumulative product for each column (i.e., over the rows) of a:", "The cumulative product for each row (i.e. over the columns) of a:"]}, {"name": "numpy.cumsum()", "path": "reference/generated/numpy.cumsum", "type": "numpy.cumsum", "text": ["Return the cumulative sum of the elements along a given axis.", "Input array.", "Axis along which the cumulative sum is computed. The default (None) is to compute the cumsum over the flattened array.", "Type of the returned array and of the accumulator in which the elements are summed. If dtype is not specified, it defaults to the dtype of a, unless a has an integer dtype with a precision less than that of the default platform integer. In that case, the default platform integer is used.", "Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output but the type will be cast if necessary. See Output type determination for more details.", "A new array holding the result is returned unless out is specified, in which case a reference to out is returned. The result has the same size as a, and the same shape as a if axis is not None or a is a 1-d array.", "See also", "Sum array elements.", "Integration of array values using the composite trapezoidal rule.", "Calculate the n-th discrete difference along given axis.", "Arithmetic is modular when using integer types, and no error is raised on overflow.", "cumsum(a)[-1] may not be equal to sum(a) for floating-point values since sum may use a pairwise summation routine, reducing the roundoff-error. See sum for more information.", "cumsum(b)[-1] may not be equal to sum(b)"]}, {"name": "numpy.DataSource()", "path": "reference/generated/numpy.datasource", "type": "numpy.DataSource", "text": ["A generic data source file (file, http, ftp, \u2026).", "DataSources can be local files or remote files/URLs. The files may also be compressed or uncompressed. DataSource hides some of the low-level details of downloading the file, allowing you to simply pass in a valid file path (or URL) and obtain a file object.", "Path to the directory where the source file gets downloaded to for use. If destpath is None, a temporary directory will be created. The default path is the current directory.", "URLs require a scheme string (http://) to be used, without it they will fail:", "Temporary directories are deleted when the DataSource is deleted.", "abspath(path)", "Return absolute path of file in the DataSource directory.", "exists(path)", "Test if path exists.", "open(path[, mode, encoding, newline])", "Open and return file-like object."]}, {"name": "numpy.datetime64", "path": "reference/arrays.scalars#numpy.datetime64", "type": "Scalars", "text": ["If created from a 64-bit integer, it represents an offset from 1970-01-01T00:00:00. If created from string, the string can be in ISO 8601 date or datetime format.", "See Datetimes and Timedeltas for more information.", "'M'"]}, {"name": "numpy.datetime_as_string()", "path": "reference/generated/numpy.datetime_as_string", "type": "numpy.datetime_as_string", "text": ["Convert an array of datetimes into an array of strings.", "The array of UTC timestamps to format.", "One of None, \u2018auto\u2019, or a datetime unit.", "Timezone information to use when displaying the datetime. If \u2018UTC\u2019, end with a Z to indicate UTC time. If \u2018local\u2019, convert to the local timezone first, and suffix with a +-#### timezone offset. If a tzinfo object, then do as with \u2018local\u2019, but use the specified timezone.", "Casting to allow when changing between datetime units.", "An array of strings the same shape as arr.", "Setting the timezone to UTC shows the same information, but with a Z suffix", "Note that we picked datetimes that cross a DST boundary. Passing in a pytz timezone object will print the appropriate offset", "Passing in a unit will change the precision", "\u2018casting\u2019 can be used to specify whether precision can be changed"]}, {"name": "numpy.datetime_data()", "path": "reference/generated/numpy.datetime_data", "type": "numpy.datetime_data", "text": ["Get information about the step size of a date or time type.", "The returned tuple can be passed as the second argument of numpy.datetime64 and numpy.timedelta64.", "The dtype object, which must be a datetime64 or timedelta64 type.", "The datetime unit on which this dtype is based.", "The number of base units in a step.", "The result can be used to construct a datetime that uses the same units as a timedelta"]}, {"name": "numpy.deg2rad()", "path": "reference/generated/numpy.deg2rad", "type": "numpy.deg2rad", "text": ["Convert angles from degrees to radians.", "Angles in degrees.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The corresponding angle in radians. This is a scalar if x is a scalar.", "See also", "Convert angles from radians to degrees.", "Remove large jumps in angle by wrapping.", "New in version 1.3.0.", "deg2rad(x) is x * pi / 180."]}, {"name": "numpy.degrees()", "path": "reference/generated/numpy.degrees", "type": "numpy.degrees", "text": ["Convert angles from radians to degrees.", "Input array in radians.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The corresponding degree values; if out was supplied this is a reference to it. This is a scalar if x is a scalar.", "See also", "equivalent function", "Convert a radian array to degrees"]}, {"name": "numpy.delete()", "path": "reference/generated/numpy.delete", "type": "numpy.delete", "text": ["Return a new array with sub-arrays along an axis deleted. For a one dimensional array, this returns those entries not returned by arr[obj].", "Input array.", "Indicate indices of sub-arrays to remove along the specified axis.", "Changed in version 1.19.0: Boolean indices are now treated as a mask of elements to remove, rather than being cast to the integers 0 and 1.", "The axis along which to delete the subarray defined by obj. If axis is None, obj is applied to the flattened array.", "A copy of arr with the elements specified by obj removed. Note that delete does not occur in-place. If axis is None, out is a flattened array.", "See also", "Insert elements into an array.", "Append elements at the end of an array.", "Often it is preferable to use a boolean mask. For example:", "Is equivalent to np.delete(arr, [0,2,4], axis=0), but allows further use of mask."]}, {"name": "numpy.deprecate()", "path": "reference/generated/numpy.deprecate", "type": "numpy.deprecate", "text": ["Issues a DeprecationWarning, adds warning to old_name\u2019s docstring, rebinds old_name.__name__ and returns the new function object.", "This function may also be used as a decorator.", "The function to be deprecated.", "The name of the function to be deprecated. Default is None, in which case the name of func is used.", "The new name for the function. Default is None, in which case the deprecation message is that old_name is deprecated. If given, the deprecation message is that old_name is deprecated and new_name should be used instead.", "Additional explanation of the deprecation. Displayed in the docstring after the warning.", "The deprecated function.", "Note that olduint returns a value after printing Deprecation Warning:"]}, {"name": "numpy.deprecate_with_doc()", "path": "reference/generated/numpy.deprecate_with_doc", "type": "numpy.deprecate_with_doc", "text": ["Deprecates a function and includes the deprecation in its docstring.", "This function is used as a decorator. It returns an object that can be used to issue a DeprecationWarning, by passing the to-be decorated function as argument, this adds warning to the to-be decorated function\u2019s docstring and returns the new function object.", "Additional explanation of the deprecation. Displayed in the docstring after the warning.", "See also", "Decorate a function such that it issues a DeprecationWarning"]}, {"name": "numpy.diag()", "path": "reference/generated/numpy.diag", "type": "numpy.diag", "text": ["Extract a diagonal or construct a diagonal array.", "See the more detailed documentation for numpy.diagonal if you use this function to extract a diagonal and wish to write to the resulting array; whether it returns a copy or a view depends on what version of numpy you are using.", "If v is a 2-D array, return a copy of its k-th diagonal. If v is a 1-D array, return a 2-D array with v on the k-th diagonal.", "Diagonal in question. The default is 0. Use k>0 for diagonals above the main diagonal, and k<0 for diagonals below the main diagonal.", "The extracted diagonal or constructed diagonal array.", "See also", "Return specified diagonals.", "Create a 2-D array with the flattened input as a diagonal.", "Sum along diagonals.", "Upper triangle of an array.", "Lower triangle of an array."]}, {"name": "numpy.diag_indices()", "path": "reference/generated/numpy.diag_indices", "type": "numpy.diag_indices", "text": ["Return the indices to access the main diagonal of an array.", "This returns a tuple of indices that can be used to access the main diagonal of an array a with a.ndim >= 2 dimensions and shape (n, n, \u2026, n). For a.ndim = 2 this is the usual diagonal, for a.ndim > 2 this is the set of indices to access a[i, i, ..., i] for i = [0..n-1].", "The size, along each dimension, of the arrays for which the returned indices can be used.", "The number of dimensions.", "See also", "New in version 1.4.0.", "Create a set of indices to access the diagonal of a (4, 4) array:", "Now, we create indices to manipulate a 3-D array:", "And use it to set the diagonal of an array of zeros to 1:"]}, {"name": "numpy.diag_indices_from()", "path": "reference/generated/numpy.diag_indices_from", "type": "numpy.diag_indices_from", "text": ["Return the indices to access the main diagonal of an n-dimensional array.", "See diag_indices for full details.", "See also", "New in version 1.4.0."]}, {"name": "numpy.diagflat()", "path": "reference/generated/numpy.diagflat", "type": "numpy.diagflat", "text": ["Create a two-dimensional array with the flattened input as a diagonal.", "Input data, which is flattened and set as the k-th diagonal of the output.", "Diagonal to set; 0, the default, corresponds to the \u201cmain\u201d diagonal, a positive (negative) k giving the number of the diagonal above (below) the main.", "The 2-D output array.", "See also", "MATLAB work-alike for 1-D and 2-D arrays.", "Return specified diagonals.", "Sum along diagonals."]}, {"name": "numpy.diagonal()", "path": "reference/generated/numpy.diagonal", "type": "numpy.diagonal", "text": ["Return specified diagonals.", "If a is 2-D, returns the diagonal of a with the given offset, i.e., the collection of elements of the form a[i, i+offset]. If a has more than two dimensions, then the axes specified by axis1 and axis2 are used to determine the 2-D sub-array whose diagonal is returned. The shape of the resulting array can be determined by removing axis1 and axis2 and appending an index to the right equal to the size of the resulting diagonals.", "In versions of NumPy prior to 1.7, this function always returned a new, independent array containing a copy of the values in the diagonal.", "In NumPy 1.7 and 1.8, it continues to return a copy of the diagonal, but depending on this fact is deprecated. Writing to the resulting array continues to work as it used to, but a FutureWarning is issued.", "Starting in NumPy 1.9 it returns a read-only view on the original array. Attempting to write to the resulting array will produce an error.", "In some future release, it will return a read/write view and writing to the returned array will alter your original array. The returned array will have the same type as the input array.", "If you don\u2019t write to the array returned by this function, then you can just ignore all of the above.", "If you depend on the current behavior, then we suggest copying the returned array explicitly, i.e., use np.diagonal(a).copy() instead of just np.diagonal(a). This will work with both past and future versions of NumPy.", "Array from which the diagonals are taken.", "Offset of the diagonal from the main diagonal. Can be positive or negative. Defaults to main diagonal (0).", "Axis to be used as the first axis of the 2-D sub-arrays from which the diagonals should be taken. Defaults to first axis (0).", "Axis to be used as the second axis of the 2-D sub-arrays from which the diagonals should be taken. Defaults to second axis (1).", "If a is 2-D, then a 1-D array containing the diagonal and of the same type as a is returned unless a is a matrix, in which case a 1-D array rather than a (2-D) matrix is returned in order to maintain backward compatibility.", "If a.ndim > 2, then the dimensions specified by axis1 and axis2 are removed, and a new axis inserted at the end corresponding to the diagonal.", "If the dimension of a is less than 2.", "See also", "MATLAB work-a-like for 1-D and 2-D arrays.", "Create diagonal arrays.", "Sum along diagonals.", "A 3-D example:", "The sub-arrays whose main diagonals we just obtained; note that each corresponds to fixing the right-most (column) axis, and that the diagonals are \u201cpacked\u201d in rows.", "The anti-diagonal can be obtained by reversing the order of elements using either numpy.flipud or numpy.fliplr.", "Note that the order in which the diagonal is retrieved varies depending on the flip function."]}, {"name": "numpy.diff()", "path": "reference/generated/numpy.diff", "type": "numpy.diff", "text": ["Calculate the n-th discrete difference along the given axis.", "The first difference is given by out[i] = a[i+1] - a[i] along the given axis, higher differences are calculated by using diff recursively.", "Input array", "The number of times values are differenced. If zero, the input is returned as-is.", "The axis along which the difference is taken, default is the last axis.", "Values to prepend or append to a along axis prior to performing the difference. Scalar values are expanded to arrays with length 1 in the direction of axis and the shape of the input array in along all other axes. Otherwise the dimension and shape must match a except along axis.", "New in version 1.16.0.", "The n-th differences. The shape of the output is the same as a except along axis where the dimension is smaller by n. The type of the output is the same as the type of the difference between any two elements of a. This is the same as the type of a in most cases. A notable exception is datetime64, which results in a timedelta64 output array.", "See also", "Type is preserved for boolean arrays, so the result will contain False when consecutive elements are the same and True when they differ.", "For unsigned integer arrays, the results will also be unsigned. This should not be surprising, as the result is consistent with calculating the difference directly:", "If this is not desirable, then the array should be cast to a larger integer type first:"]}, {"name": "numpy.digitize()", "path": "reference/generated/numpy.digitize", "type": "numpy.digitize", "text": ["Return the indices of the bins to which each value in input array belongs.", "right", "order of bins", "returned index i satisfies", "False", "increasing", "bins[i-1] <= x < bins[i]", "True", "increasing", "bins[i-1] < x <= bins[i]", "False", "decreasing", "bins[i-1] > x >= bins[i]", "True", "decreasing", "bins[i-1] >= x > bins[i]", "If values in x are beyond the bounds of bins, 0 or len(bins) is returned as appropriate.", "Input array to be binned. Prior to NumPy 1.10.0, this array had to be 1-dimensional, but can now have any shape.", "Array of bins. It has to be 1-dimensional and monotonic.", "Indicating whether the intervals include the right or the left bin edge. Default behavior is (right==False) indicating that the interval does not include the right edge. The left bin end is open in this case, i.e., bins[i-1] <= x < bins[i] is the default behavior for monotonically increasing bins.", "Output array of indices, of same shape as x.", "If bins is not monotonic.", "If the type of the input is complex.", "See also", "If values in x are such that they fall outside the bin range, attempting to index bins with the indices that digitize returns will result in an IndexError.", "New in version 1.10.0.", "np.digitize is implemented in terms of np.searchsorted. This means that a binary search is used to bin the values, which scales much better for larger number of bins than the previous linear search. It also removes the requirement for the input array to be 1-dimensional.", "For monotonically _increasing_ bins, the following are equivalent:", "Note that as the order of the arguments are reversed, the side must be too. The searchsorted call is marginally faster, as it does not do any monotonicity checks. Perhaps more importantly, it supports all dtypes."]}, {"name": "numpy.disp()", "path": "reference/generated/numpy.disp", "type": "numpy.disp", "text": ["Display a message on a device.", "Message to display.", "Device to write message. If None, defaults to sys.stdout which is very similar to print. device needs to have write() and flush() methods.", "Option whether to print a line feed or not. Defaults to True.", "If device does not have a write() or flush() method.", "Besides sys.stdout, a file-like object can also be used as it has both required methods:"]}, {"name": "numpy.distutils.ccompiler", "path": "reference/generated/numpy.distutils.ccompiler", "type": "numpy.distutils.ccompiler", "text": ["CCompiler_compile(self, sources[, ...])", "Compile one or more source files.", "CCompiler_customize(self, dist[, need_cxx])", "Do any platform-specific customization of a compiler instance.", "CCompiler_customize_cmd(self, cmd[, ignore])", "Customize compiler using distutils command.", "CCompiler_cxx_compiler(self)", "Return the C++ compiler.", "CCompiler_find_executables(self)", "Does nothing here, but is called by the get_version method and can be overridden by subclasses.", "CCompiler_get_version(self[, force, ok_status])", "Return compiler version, or None if compiler is not available.", "CCompiler_object_filenames(self, ...[, ...])", "Return the name of the object files for the given source files.", "CCompiler_show_customization(self)", "Print the compiler customizations to stdout.", "CCompiler_spawn(self, cmd[, display, env])", "Execute a command in a sub-process.", "gen_lib_options(compiler, library_dirs, ...)", "new_compiler([plat, compiler, verbose, ...])", "replace_method(klass, method_name, func)", "simple_version_match([pat, ignore, start])", "Simple matching of version numbers, for use in CCompiler and FCompiler."]}, {"name": "numpy.distutils.ccompiler_opt", "path": "reference/generated/numpy.distutils.ccompiler_opt", "type": "numpy.distutils.ccompiler_opt", "text": ["Provides the CCompilerOpt class, used for handling the CPU/hardware optimization, starting from parsing the command arguments, to managing the relation between the CPU baseline and dispatch-able features, also generating the required C headers and ending with compiling the sources with proper compiler\u2019s flags.", "CCompilerOpt doesn\u2019t provide runtime detection for the CPU features, instead only focuses on the compiler side, but it creates abstract C headers that can be used later for the final runtime dispatching process.", "new_ccompiler_opt(compiler, dispatch_hpath, ...)", "Create a new instance of 'CCompilerOpt' and generate the dispatch header which contains the #definitions and headers of platform-specific instruction-sets for the enabled CPU baseline and dispatch-able features.", "CCompilerOpt(ccompiler[, cpu_baseline, ...])", "A helper class for CCompiler aims to provide extra build options to effectively control of compiler optimizations that are directly related to CPU features."]}, {"name": "numpy.distutils.ccompiler_opt.CCompilerOpt()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt", "text": ["A helper class for CCompiler aims to provide extra build options to effectively control of compiler optimizations that are directly related to CPU features.", "cache_flush()", "Force update the cache.", "cc_normalize_flags(flags)", "Remove the conflicts that caused due gathering implied features flags.", "conf_features_partial()", "Return a dictionary of supported CPU features by the platform, and accumulate the rest of undefined options in conf_features, the returned dict has same rules and notes in class attribute conf_features, also its override any options that been set in 'conf_features'.", "cpu_baseline_flags()", "Returns a list of final CPU baseline compiler flags", "cpu_baseline_names()", "return a list of final CPU baseline feature names", "cpu_dispatch_names()", "return a list of final CPU dispatch feature names", "dist_compile(sources, flags[, ccompiler])", "Wrap CCompiler.compile()", "dist_error(*args)", "Raise a compiler error", "dist_fatal(*args)", "Raise a distutils error", "dist_info()", "Return a tuple containing info about (platform, compiler, extra_args), required by the abstract class '_CCompiler' for discovering the platform environment.", "dist_load_module(name, path)", "Load a module from file, required by the abstract class '_Cache'.", "dist_log(*args[, stderr])", "Print a console message", "dist_test(source, flags[, macros])", "Return True if 'CCompiler.compile()' able to compile a source file with certain flags.", "feature_ahead(names)", "Return list of features in 'names' after remove any implied features and keep the origins.", "feature_c_preprocessor(feature_name[, tabs])", "Generate C preprocessor definitions and include headers of a CPU feature.", "feature_detect(names)", "Return a list of CPU features that required to be detected sorted from the lowest to highest interest.", "feature_get_til(names, keyisfalse)", "same as feature_implies_c() but stop collecting implied features when feature's option that provided through parameter 'keyisfalse' is False, also sorting the returned features.", "feature_implies(names[, keep_origins])", "Return a set of CPU features that implied by 'names'", "feature_implies_c(names)", "same as feature_implies() but combining 'names'", "feature_is_exist(name)", "Returns True if a certain feature is exist and covered within _Config.conf_features.", "feature_names([names, force_flags, macros])", "Returns a set of CPU feature names that supported by platform and the C compiler.", "feature_sorted(names[, reverse])", "Sort a list of CPU features ordered by the lowest interest.", "feature_untied(names)", "same as 'feature_ahead()' but if both features implied each other and keep the highest interest.", "generate_dispatch_header(header_path)", "Generate the dispatch header which contains the #definitions and headers for platform-specific instruction-sets for the enabled CPU baseline and dispatch-able features.", "is_cached()", "Returns True if the class loaded from the cache file", "me(cb)", "A static method that can be treated as a decorator to dynamically cache certain methods.", "parse_targets(source)", "Fetch and parse configuration statements that required for defining the targeted CPU features, statements should be declared in the top of source in between C comment and start with a special mark @targets.", "try_dispatch(sources[, src_dir, ccompiler])", "Compile one or more dispatch-able sources and generates object files, also generates abstract C config headers and macros that used later for the final runtime dispatching process.", "cache_hash", "cc_test_flags", "feature_can_autovec", "feature_extra_checks", "feature_flags", "feature_is_supported", "feature_test", "report"]}, {"name": "numpy.distutils.core.Extension()", "path": "reference/generated/numpy.distutils.core.extension", "type": "numpy.distutils.core.Extension", "text": ["Extension name.", "List of source file locations relative to the top directory of the package.", "Extra command line arguments to pass to the compiler.", "Extra command line arguments to pass to the fortran77 compiler.", "Extra command line arguments to pass to the fortran90 compiler.", "has_cxx_sources", "has_f2py_sources"]}, {"name": "numpy.distutils.misc_util.all_strings()", "path": "reference/distutils/misc_util", "type": "distutils.misc_util", "text": ["Return True if all items in lst are string objects.", "Convert a /-separated pathname to one using the OS\u2019s path separator.", "Convert a path from Cygwin-native to Windows-native.", "Uses the cygpath utility (part of the Base install) to do the actual conversion. Falls back to returning the original path if this fails.", "Handles the default /cygdrive mount prefix as well as the /proc/cygdrive portable prefix, custom cygdrive prefixes such as / or /mnt, and absolute paths such as /usr/src/ or /home/username", "The path to convert", "The converted path", "Documentation for cygpath utility: https://cygwin.com/cygwin-ug-net/cygpath.html Documentation for the C function it wraps: https://cygwin.com/cygwin-api/func-cygwin-conv-path.html", "Return a configuration dictionary for usage in configuration() function defined in file setup_<name>.py.", "Use importlib machinery to import a module modname from the file modfile. Depending on the spec.loader, the module may not be registered in sys.modules.", "Return four lists of filenames containing C, C++, Fortran, and Fortran 90 module sources, respectively.", "Generate config.py file containing system_info information used during building the package.", "config[\u2018py_modules\u2019].append((packagename, \u2018__config__\u2019,generate_config_py))", "Return frame object from call stack with given level.", "Return an info dict for a given C library.", "The info dict contains the necessary options to use the C library.", "Name of the package (should match the name of the .ini file, without the extension, e.g. foo for the file foo.ini).", "If given, should be a sequence of additional directories where to look for npy-pkg-config files. Those directories are searched prior to the NumPy directory.", "The dictionary with build information.", "If the package is not found.", "See also", "To get the necessary information for the npymath library from NumPy:", "This info dict can then be used as input to a Configuration instance:", "Determine language value (c,f77,f90) from sources", "Return the MATHLIB line from numpyconfig.h", "Get number of parallel build jobs set by the \u2013parallel command line argument of setup.py If the command did not receive a setting the environment variable NPY_NUM_BUILD_JOBS is checked. If that is unset, return the number of processors on the system, with a maximum of 8 (to prevent overloading the system if there a lot of CPUs).", "number of parallel jobs that can be run", "Return library info for the given package.", "Name of the package (should match the name of the .ini file, without the extension, e.g. foo for the file foo.ini).", "If given, should be a sequence of additional directories where to look for npy-pkg-config files. Those directories are searched prior to the NumPy directory.", "The LibraryInfo instance containing the build information.", "If the package is not found.", "See also", "Apply glob to paths and prepend local_path if needed.", "Return True if sources contains C++ files", "Return True if sources contains Fortran files", "Return true if directory is local directory.", "Return true when using mingw32 environment.", "Resolve  and \u2018.\u2019 from path.", "Join two or more pathname components + - convert a /-separated pathname to one using the OS\u2019s path separator. - resolve  and  from path.", "Either passing n arguments as in njoin(\u2018a\u2019,\u2019b\u2019), or a sequence of n names as in njoin([\u2018a\u2019,\u2019b\u2019]) is handled, or a mixture of such arguments.", "Some flags are valid for C but not C++. Prune them."]}, {"name": "numpy.distutils.misc_util.allpath()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.allpath", "type": "distutils.misc_util", "text": ["Convert a /-separated pathname to one using the OS\u2019s path separator."]}, {"name": "numpy.distutils.misc_util.appendpath()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.appendpath", "type": "distutils.misc_util", "text": []}, {"name": "numpy.distutils.misc_util.as_list()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.as_list", "type": "distutils.misc_util", "text": []}, {"name": "numpy.distutils.misc_util.blue_text()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.blue_text", "type": "distutils.misc_util", "text": []}, {"name": "numpy.distutils.misc_util.cyan_text()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.cyan_text", "type": "distutils.misc_util", "text": []}, {"name": "numpy.distutils.misc_util.cyg2win32()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.cyg2win32", "type": "distutils.misc_util", "text": ["Convert a path from Cygwin-native to Windows-native.", "Uses the cygpath utility (part of the Base install) to do the actual conversion. Falls back to returning the original path if this fails.", "Handles the default /cygdrive mount prefix as well as the /proc/cygdrive portable prefix, custom cygdrive prefixes such as / or /mnt, and absolute paths such as /usr/src/ or /home/username", "The path to convert", "The converted path", "Documentation for cygpath utility: https://cygwin.com/cygwin-ug-net/cygpath.html Documentation for the C function it wraps: https://cygwin.com/cygwin-api/func-cygwin-conv-path.html"]}, {"name": "numpy.distutils.misc_util.default_config_dict()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.default_config_dict", "type": "distutils.misc_util", "text": ["Return a configuration dictionary for usage in configuration() function defined in file setup_<name>.py."]}, {"name": "numpy.distutils.misc_util.dict_append()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.dict_append", "type": "distutils.misc_util", "text": []}, {"name": "numpy.distutils.misc_util.dot_join()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.dot_join", "type": "distutils.misc_util", "text": []}, {"name": "numpy.distutils.misc_util.exec_mod_from_location()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.exec_mod_from_location", "type": "distutils.misc_util", "text": ["Use importlib machinery to import a module modname from the file modfile. Depending on the spec.loader, the module may not be registered in sys.modules."]}, {"name": "numpy.distutils.misc_util.filter_sources()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.filter_sources", "type": "distutils.misc_util", "text": ["Return four lists of filenames containing C, C++, Fortran, and Fortran 90 module sources, respectively."]}, {"name": "numpy.distutils.misc_util.generate_config_py()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.generate_config_py", "type": "distutils.misc_util", "text": ["Generate config.py file containing system_info information used during building the package.", "config[\u2018py_modules\u2019].append((packagename, \u2018__config__\u2019,generate_config_py))"]}, {"name": "numpy.distutils.misc_util.get_build_architecture()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.get_build_architecture", "type": "distutils.misc_util", "text": []}, {"name": "numpy.distutils.misc_util.get_cmd()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.get_cmd", "type": "distutils.misc_util", "text": []}, {"name": "numpy.distutils.misc_util.get_data_files()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.get_data_files", "type": "distutils.misc_util", "text": []}, {"name": "numpy.distutils.misc_util.get_dependencies()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.get_dependencies", "type": "distutils.misc_util", "text": []}, {"name": "numpy.distutils.misc_util.get_ext_source_files()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.get_ext_source_files", "type": "distutils.misc_util", "text": []}, {"name": "numpy.distutils.misc_util.get_frame()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.get_frame", "type": "distutils.misc_util", "text": ["Return frame object from call stack with given level."]}, {"name": "numpy.distutils.misc_util.get_info()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.get_info", "type": "distutils.misc_util", "text": ["Return an info dict for a given C library.", "The info dict contains the necessary options to use the C library.", "Name of the package (should match the name of the .ini file, without the extension, e.g. foo for the file foo.ini).", "If given, should be a sequence of additional directories where to look for npy-pkg-config files. Those directories are searched prior to the NumPy directory.", "The dictionary with build information.", "If the package is not found.", "See also", "To get the necessary information for the npymath library from NumPy:", "This info dict can then be used as input to a Configuration instance:"]}, {"name": "numpy.distutils.misc_util.get_language()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.get_language", "type": "distutils.misc_util", "text": ["Determine language value (c,f77,f90) from sources"]}, {"name": "numpy.distutils.misc_util.get_lib_source_files()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.get_lib_source_files", "type": "distutils.misc_util", "text": []}, {"name": "numpy.distutils.misc_util.get_mathlibs()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.get_mathlibs", "type": "distutils.misc_util", "text": ["Return the MATHLIB line from numpyconfig.h"]}, {"name": "numpy.distutils.misc_util.get_num_build_jobs()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.get_num_build_jobs", "type": "distutils.misc_util", "text": ["Get number of parallel build jobs set by the \u2013parallel command line argument of setup.py If the command did not receive a setting the environment variable NPY_NUM_BUILD_JOBS is checked. If that is unset, return the number of processors on the system, with a maximum of 8 (to prevent overloading the system if there a lot of CPUs).", "number of parallel jobs that can be run"]}, {"name": "numpy.distutils.misc_util.get_numpy_include_dirs()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.get_numpy_include_dirs", "type": "distutils.misc_util", "text": []}, {"name": "numpy.distutils.misc_util.get_pkg_info()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.get_pkg_info", "type": "distutils.misc_util", "text": ["Return library info for the given package.", "Name of the package (should match the name of the .ini file, without the extension, e.g. foo for the file foo.ini).", "If given, should be a sequence of additional directories where to look for npy-pkg-config files. Those directories are searched prior to the NumPy directory.", "The LibraryInfo instance containing the build information.", "If the package is not found.", "See also"]}, {"name": "numpy.distutils.misc_util.get_script_files()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.get_script_files", "type": "distutils.misc_util", "text": []}, {"name": "numpy.distutils.misc_util.gpaths()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.gpaths", "type": "distutils.misc_util", "text": ["Apply glob to paths and prepend local_path if needed."]}, {"name": "numpy.distutils.misc_util.green_text()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.green_text", "type": "distutils.misc_util", "text": []}, {"name": "numpy.distutils.misc_util.has_cxx_sources()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.has_cxx_sources", "type": "distutils.misc_util", "text": ["Return True if sources contains C++ files"]}, {"name": "numpy.distutils.misc_util.has_f_sources()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.has_f_sources", "type": "distutils.misc_util", "text": ["Return True if sources contains Fortran files"]}, {"name": "numpy.distutils.misc_util.is_local_src_dir()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.is_local_src_dir", "type": "distutils.misc_util", "text": ["Return true if directory is local directory."]}, {"name": "numpy.distutils.misc_util.is_sequence()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.is_sequence", "type": "distutils.misc_util", "text": []}, {"name": "numpy.distutils.misc_util.is_string()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.is_string", "type": "distutils.misc_util", "text": []}, {"name": "numpy.distutils.misc_util.mingw32()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.mingw32", "type": "distutils.misc_util", "text": ["Return true when using mingw32 environment."]}, {"name": "numpy.distutils.misc_util.minrelpath()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.minrelpath", "type": "distutils.misc_util", "text": ["Resolve  and \u2018.\u2019 from path."]}, {"name": "numpy.distutils.misc_util.njoin()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.njoin", "type": "distutils.misc_util", "text": ["Join two or more pathname components + - convert a /-separated pathname to one using the OS\u2019s path separator. - resolve  and  from path.", "Either passing n arguments as in njoin(\u2018a\u2019,\u2019b\u2019), or a sequence of n names as in njoin([\u2018a\u2019,\u2019b\u2019]) is handled, or a mixture of such arguments."]}, {"name": "numpy.distutils.misc_util.red_text()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.red_text", "type": "distutils.misc_util", "text": []}, {"name": "numpy.distutils.misc_util.sanitize_cxx_flags()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.sanitize_cxx_flags", "type": "distutils.misc_util", "text": ["Some flags are valid for C but not C++. Prune them."]}, {"name": "numpy.distutils.misc_util.terminal_has_colors()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.terminal_has_colors", "type": "distutils.misc_util", "text": []}, {"name": "numpy.distutils.misc_util.yellow_text()", "path": "reference/distutils/misc_util#numpy.distutils.misc_util.yellow_text", "type": "distutils.misc_util", "text": []}, {"name": "numpy.divide()", "path": "reference/generated/numpy.divide", "type": "numpy.divide", "text": ["Returns a true division of the inputs, element-wise.", "Unlike \u2018floor division\u2019, true division adjusts the output type to present the best answer, regardless of input types.", "Dividend array.", "Divisor array. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "This is a scalar if both x1 and x2 are scalars.", "In Python, // is the floor division operator and / the true division operator. The true_divide(x1, x2) function is equivalent to true division in Python.", "The / operator can be used as a shorthand for np.true_divide on ndarrays."]}, {"name": "numpy.divmod()", "path": "reference/generated/numpy.divmod", "type": "numpy.divmod", "text": ["Return element-wise quotient and remainder simultaneously.", "New in version 1.13.0.", "np.divmod(x, y) is equivalent to (x // y, x % y), but faster because it avoids redundant work. It is used to implement the Python built-in function divmod on NumPy arrays.", "Dividend array.", "Divisor array. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Element-wise quotient resulting from floor division. This is a scalar if both x1 and x2 are scalars.", "Element-wise remainder from floor division. This is a scalar if both x1 and x2 are scalars.", "See also", "Equivalent to Python\u2019s // operator.", "Equivalent to Python\u2019s % operator.", "Equivalent to divmod(x, 1) for positive x with the return values switched.", "The divmod function can be used as a shorthand for np.divmod on ndarrays."]}, {"name": "numpy.dot()", "path": "reference/generated/numpy.dot", "type": "numpy.dot", "text": ["Dot product of two arrays. Specifically,", "If a is an N-D array and b is an M-D array (where M>=2), it is a sum product over the last axis of a and the second-to-last axis of b:", "First argument.", "Second argument.", "Output argument. This must have the exact kind that would be returned if it was not used. In particular, it must have the right type, must be C-contiguous, and its dtype must be the dtype that would be returned for dot(a,b). This is a performance feature. Therefore, if these conditions are not met, an exception is raised, instead of attempting to be flexible.", "Returns the dot product of a and b. If a and b are both scalars or both 1-D arrays then a scalar is returned; otherwise an array is returned. If out is given, then it is returned.", "If the last dimension of a is not the same size as the second-to-last dimension of b.", "See also", "Complex-conjugating dot product.", "Sum products over arbitrary axes.", "Einstein summation convention.", "\u2018@\u2019 operator as method with out parameter.", "Chained dot product.", "Neither argument is complex-conjugated:", "For 2-D arrays it is the matrix product:"]}, {"name": "numpy.double()", "path": "reference/arrays.scalars#numpy.double", "type": "Scalars", "text": ["Double-precision floating-point number type, compatible with Python float and C double.", "'d'", "numpy.float_", "numpy.float64: 64-bit precision floating-point number type: sign bit, 11 bits exponent, 52 bits mantissa."]}, {"name": "numpy.dsplit()", "path": "reference/generated/numpy.dsplit", "type": "numpy.dsplit", "text": ["Split array into multiple sub-arrays along the 3rd axis (depth).", "Please refer to the split documentation. dsplit is equivalent to split with axis=2, the array is always split along the third axis provided the array dimension is greater than or equal to 3.", "See also", "Split an array into multiple sub-arrays of equal size."]}, {"name": "numpy.dstack()", "path": "reference/generated/numpy.dstack", "type": "numpy.dstack", "text": ["Stack arrays in sequence depth wise (along third axis).", "This is equivalent to concatenation along the third axis after 2-D arrays of shape (M,N) have been reshaped to (M,N,1) and 1-D arrays of shape (N,) have been reshaped to (1,N,1). Rebuilds arrays divided by dsplit.", "This function makes most sense for arrays with up to 3 dimensions. For instance, for pixel-data with a height (first axis), width (second axis), and r/g/b channels (third axis). The functions concatenate, stack and block provide more general stacking and concatenation operations.", "The arrays must have the same shape along all but the third axis. 1-D or 2-D arrays must have the same shape.", "The array formed by stacking the given arrays, will be at least 3-D.", "See also", "Join a sequence of arrays along an existing axis.", "Join a sequence of arrays along a new axis.", "Assemble an nd-array from nested lists of blocks.", "Stack arrays in sequence vertically (row wise).", "Stack arrays in sequence horizontally (column wise).", "Stack 1-D arrays as columns into a 2-D array.", "Split array along third axis."]}, {"name": "numpy.dtype()", "path": "reference/generated/numpy.dtype", "type": "numpy.dtype", "text": ["Create a data type object.", "A numpy array is homogeneous, and contains elements described by a dtype object. A dtype object can be constructed from different combinations of fundamental numeric types.", "Object to be converted to a data type object.", "Add padding to the fields to match what a C compiler would output for a similar C-struct. Can be True only if obj is a dictionary or a comma-separated string. If a struct dtype is being created, this also sets a sticky alignment flag isalignedstruct.", "Make a new copy of the data-type object. If False, the result may just be a reference to a built-in data-type object.", "See also", "Using array-scalar type:", "Structured type, one field name \u2018f1\u2019, containing int16:", "Structured type, one field named \u2018f1\u2019, in itself containing a structured type with one field:", "Structured type, two fields: the first field contains an unsigned int, the second an int32:", "Using array-protocol type strings:", "Using comma-separated field formats. The shape is (2,3):", "Using tuples. int is a fixed type, 3 the field\u2019s shape. void is a flexible type, here of size 10:", "Subdivide int16 into 2 int8\u2019s, called x and y. 0 and 1 are the offsets in bytes:", "Using dictionaries. Two fields named \u2018gender\u2019 and \u2018age\u2019:", "Offsets in bytes, here 0 and 25:", "The required alignment (bytes) of this data-type according to the compiler.", "Returns dtype for the base element of the subarrays, regardless of their dimension or shape.", "A character indicating the byte-order of this data-type object.", "A unique character code for each of the 21 different built-in types.", "__array_interface__ description of the data-type.", "Dictionary of named fields defined for this data type, or None.", "Bit-flags describing how this data type is to be interpreted.", "Boolean indicating whether this dtype contains any reference-counted objects in any fields or sub-dtypes.", "Boolean indicating whether the dtype is a struct which maintains field alignment.", "Integer indicating how this dtype relates to the built-in dtypes.", "Boolean indicating whether the byte order of this dtype is native to the platform.", "The element size of this data-type object.", "A character code (one of \u2018biufcmMOSUV\u2019) identifying the general kind of data.", "Either None or a readonly dictionary of metadata (mappingproxy).", "A bit-width name for this data-type.", "Ordered list of field names, or None if there are no fields.", "Number of dimensions of the sub-array if this data type describes a sub-array, and 0 otherwise.", "A unique number for each of the 21 different built-in types.", "Shape tuple of the sub-array if this data type describes a sub-array, and () otherwise.", "The array-protocol typestring of this data-type object.", "Tuple (item_dtype, shape) if this dtype describes a sub-array, and None otherwise.", "newbyteorder([new_order])", "Return a new dtype with a different byte order."]}, {"name": "numpy.e", "path": "reference/constants#numpy.e", "type": "Constants", "text": ["Euler\u2019s constant, base of natural logarithms, Napier\u2019s constant.", "e = 2.71828182845904523536028747135266249775724709369995...", "exp : Exponential function log : Natural logarithm", "https://en.wikipedia.org/wiki/E_%28mathematical_constant%29"]}, {"name": "numpy.ediff1d()", "path": "reference/generated/numpy.ediff1d", "type": "numpy.ediff1d", "text": ["The differences between consecutive elements of an array.", "If necessary, will be flattened before the differences are taken.", "Number(s) to append at the end of the returned differences.", "Number(s) to prepend at the beginning of the returned differences.", "The differences. Loosely, this is ary.flat[1:] - ary.flat[:-1].", "See also", "When applied to masked arrays, this function drops the mask information if the to_begin and/or to_end parameters are used.", "The returned array is always 1D."]}, {"name": "numpy.einsum()", "path": "reference/generated/numpy.einsum", "type": "numpy.einsum", "text": ["Evaluates the Einstein summation convention on the operands.", "Using the Einstein summation convention, many common multi-dimensional, linear algebraic array operations can be represented in a simple fashion. In implicit mode einsum computes these values.", "In explicit mode, einsum provides further flexibility to compute other array operations that might not be considered classical Einstein summation operations, by disabling, or forcing summation over specified subscript labels.", "See the notes and examples for clarification.", "Specifies the subscripts for summation as comma separated list of subscript labels. An implicit (classical Einstein summation) calculation is performed unless the explicit indicator \u2018->\u2019 is included as well as subscript labels of the precise output form.", "These are the arrays for the operation.", "If provided, the calculation is done into this array.", "If provided, forces the calculation to use the data type specified. Note that you may have to also give a more liberal casting parameter to allow the conversions. Default is None.", "Controls the memory layout of the output. \u2018C\u2019 means it should be C contiguous. \u2018F\u2019 means it should be Fortran contiguous, \u2018A\u2019 means it should be \u2018F\u2019 if the inputs are all \u2018F\u2019, \u2018C\u2019 otherwise. \u2018K\u2019 means it should be as close to the layout as the inputs as is possible, including arbitrarily permuted axes. Default is \u2018K\u2019.", "Controls what kind of data casting may occur. Setting this to \u2018unsafe\u2019 is not recommended, as it can adversely affect accumulations.", "Default is \u2018safe\u2019.", "Controls if intermediate optimization should occur. No optimization will occur if False and True will default to the \u2018greedy\u2019 algorithm. Also accepts an explicit contraction list from the np.einsum_path function. See np.einsum_path for more details. Defaults to False.", "The calculation based on the Einstein summation convention.", "See also", "similar verbose interface is provided by einops package to cover additional operations: transpose, reshape/flatten, repeat/tile, squeeze/unsqueeze and reductions.", "opt_einsum optimizes contraction order for einsum-like expressions in backend-agnostic manner.", "New in version 1.6.0.", "The Einstein summation convention can be used to compute many multi-dimensional, linear algebraic array operations. einsum provides a succinct way of representing these.", "A non-exhaustive list of these operations, which can be computed by einsum, is shown below along with examples:", "The subscripts string is a comma-separated list of subscript labels, where each label refers to a dimension of the corresponding operand. Whenever a label is repeated it is summed, so np.einsum('i,i', a, b) is equivalent to np.inner(a,b). If a label appears only once, it is not summed, so np.einsum('i', a) produces a view of a with no changes. A further example np.einsum('ij,jk', a, b) describes traditional matrix multiplication and is equivalent to np.matmul(a,b). Repeated subscript labels in one operand take the diagonal. For example, np.einsum('ii', a) is equivalent to np.trace(a).", "In implicit mode, the chosen subscripts are important since the axes of the output are reordered alphabetically. This means that np.einsum('ij', a) doesn\u2019t affect a 2D array, while np.einsum('ji', a) takes its transpose. Additionally, np.einsum('ij,jk', a, b) returns a matrix multiplication, while, np.einsum('ij,jh', a, b) returns the transpose of the multiplication since subscript \u2018h\u2019 precedes subscript \u2018i\u2019.", "In explicit mode the output can be directly controlled by specifying output subscript labels. This requires the identifier \u2018->\u2019 as well as the list of output subscript labels. This feature increases the flexibility of the function since summing can be disabled or forced when required. The call np.einsum('i->', a) is like np.sum(a, axis=-1), and np.einsum('ii->i', a) is like np.diag(a). The difference is that einsum does not allow broadcasting by default. Additionally np.einsum('ij,jh->ih', a, b) directly specifies the order of the output subscript labels and therefore returns matrix multiplication, unlike the example above in implicit mode.", "To enable and control broadcasting, use an ellipsis. Default NumPy-style broadcasting is done by adding an ellipsis to the left of each term, like np.einsum('...ii->...i', a). To take the trace along the first and last axes, you can do np.einsum('i...i', a), or to do a matrix-matrix product with the left-most indices instead of rightmost, one can do np.einsum('ij...,jk...->ik...', a, b).", "When there is only one operand, no axes are summed, and no output parameter is provided, a view into the operand is returned instead of a new array. Thus, taking the diagonal as np.einsum('ii->i', a) produces a view (changed in version 1.10.0).", "einsum also provides an alternative way to provide the subscripts and operands as einsum(op0, sublist0, op1, sublist1, ..., [sublistout]). If the output shape is not provided in this format einsum will be calculated in implicit mode, otherwise it will be performed explicitly. The examples below have corresponding einsum calls with the two parameter methods.", "New in version 1.10.0.", "Views returned from einsum are now writeable whenever the input array is writeable. For example, np.einsum('ijk...->kji...', a) will now have the same effect as np.swapaxes(a, 0, 2) and np.einsum('ii->i', a) will return a writeable view of the diagonal of a 2D array.", "New in version 1.12.0.", "Added the optimize argument which will optimize the contraction order of an einsum expression. For a contraction with three or more operands this can greatly increase the computational efficiency at the cost of a larger memory footprint during computation.", "Typically a \u2018greedy\u2019 algorithm is applied which empirical tests have shown returns the optimal path in the majority of cases. In some cases \u2018optimal\u2019 will return the superlative path through a more expensive, exhaustive search. For iterative calculations it may be advisable to calculate the optimal path once and reuse that path by supplying it as an argument. An example is given below.", "See numpy.einsum_path for more details.", "Trace of a matrix:", "Extract the diagonal (requires explicit form):", "Sum over an axis (requires explicit form):", "For higher dimensional arrays summing a single axis can be done with ellipsis:", "Compute a matrix transpose, or reorder any number of axes:", "Vector inner products:", "Matrix vector multiplication:", "Broadcasting and scalar multiplication:", "Vector outer product:", "Tensor contraction:", "Writeable returned arrays (since version 1.10.0):", "Example of ellipsis use:", "Chained array operations. For more complicated contractions, speed ups might be achieved by repeatedly computing a \u2018greedy\u2019 path or pre-computing the \u2018optimal\u2019 path and repeatedly applying it, using an einsum_path insertion (since version 1.12.0). Performance improvements can be particularly significant with larger arrays:", "Basic einsum: ~1520ms (benchmarked on 3.1GHz Intel i5.)", "Sub-optimal einsum (due to repeated path calculation time): ~330ms", "Greedy einsum (faster optimal path approximation): ~160ms", "Optimal einsum (best usage pattern in some use cases): ~110ms"]}, {"name": "numpy.einsum_path()", "path": "reference/generated/numpy.einsum_path", "type": "numpy.einsum_path", "text": ["Evaluates the lowest cost contraction order for an einsum expression by considering the creation of intermediate arrays.", "Specifies the subscripts for summation.", "These are the arrays for the operation.", "Choose the type of path. If a tuple is provided, the second argument is assumed to be the maximum intermediate size created. If only a single argument is provided the largest input or output array size is used as a maximum intermediate size.", "Default is \u2018greedy\u2019.", "A list representation of the einsum path.", "A printable representation of the einsum path.", "See also", "The resulting path indicates which terms of the input contraction should be contracted first, the result of this contraction is then appended to the end of the contraction list. This list can then be iterated over until all intermediate contractions are complete.", "We can begin with a chain dot example. In this case, it is optimal to contract the b and c tensors first as represented by the first element of the path (1, 2). The resulting tensor is added to the end of the contraction and the remaining contraction (0, 1) is then completed.", "A more complex index transformation example."]}, {"name": "numpy.empty()", "path": "reference/generated/numpy.empty", "type": "numpy.empty", "text": ["Return a new array of given shape and type, without initializing entries.", "Shape of the empty array, e.g., (2, 3) or 2.", "Desired output data-type for the array, e.g, numpy.int8. Default is numpy.float64.", "Whether to store multi-dimensional data in row-major (C-style) or column-major (Fortran-style) order in memory.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "Array of uninitialized (arbitrary) data of the given shape, dtype, and order. Object arrays will be initialized to None.", "See also", "Return an empty array with shape and type of input.", "Return a new array setting values to one.", "Return a new array setting values to zero.", "Return a new array of given shape filled with value.", "empty, unlike zeros, does not set the array values to zero, and may therefore be marginally faster. On the other hand, it requires the user to manually set all the values in the array, and should be used with caution."]}, {"name": "numpy.empty_like()", "path": "reference/generated/numpy.empty_like", "type": "numpy.empty_like", "text": ["Return a new array with the same shape and type as a given array.", "The shape and data-type of prototype define these same attributes of the returned array.", "Overrides the data type of the result.", "New in version 1.6.0.", "Overrides the memory layout of the result. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if prototype is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of prototype as closely as possible.", "New in version 1.6.0.", "If True, then the newly created array will use the sub-class type of prototype, otherwise it will be a base-class array. Defaults to True.", "Overrides the shape of the result. If order=\u2019K\u2019 and the number of dimensions is unchanged, will try to keep order, otherwise, order=\u2019C\u2019 is implied.", "New in version 1.17.0.", "Array of uninitialized (arbitrary) data with the same shape and type as prototype.", "See also", "Return an array of ones with shape and type of input.", "Return an array of zeros with shape and type of input.", "Return a new array with shape of input filled with value.", "Return a new uninitialized array.", "This function does not initialize the returned array; to do that use zeros_like or ones_like instead. It may be marginally faster than the functions that do set the array values."]}, {"name": "numpy.equal()", "path": "reference/generated/numpy.equal", "type": "numpy.equal", "text": ["Return (x1 == x2) element-wise.", "Input arrays. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Output array, element-wise comparison of x1 and x2. Typically of type bool, unless dtype=object is passed. This is a scalar if both x1 and x2 are scalars.", "See also", "What is compared are values, not types. So an int (1) and an array of length one can evaluate as True:", "The == operator can be used as a shorthand for np.equal on ndarrays."]}, {"name": "numpy.errstate()", "path": "reference/generated/numpy.errstate", "type": "numpy.errstate", "text": ["Context manager for floating-point error handling.", "Using an instance of errstate as a context manager allows statements in that context to execute with a known error handling behavior. Upon entering the context the error handling is set with seterr and seterrcall, and upon exiting it is reset to what it was before.", "Changed in version 1.17.0: errstate is also usable as a function decorator, saving a level of indentation if an entire function is wrapped. See contextlib.ContextDecorator for more information.", "Keyword arguments. The valid keywords are the possible floating-point exceptions. Each keyword should have a string value that defines the treatment for the particular error. Possible values are {\u2018ignore\u2019, \u2018warn\u2019, \u2018raise\u2019, \u2018call\u2019, \u2018print\u2019, \u2018log\u2019}.", "See also", "For complete documentation of the types of floating-point exceptions and treatment options, see seterr.", "Outside the context the error handling behavior has not changed:", "__call__(func)", "Call self as a function."]}, {"name": "numpy.euler_gamma", "path": "reference/constants#numpy.euler_gamma", "type": "Constants", "text": ["\u03b3 = 0.5772156649015328606065120900824024310421...", "https://en.wikipedia.org/wiki/Euler-Mascheroni_constant"]}, {"name": "numpy.exp()", "path": "reference/generated/numpy.exp", "type": "numpy.exp", "text": ["Calculate the exponential of all elements in the input array.", "Input values.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Output array, element-wise exponential of x. This is a scalar if x is a scalar.", "See also", "Calculate exp(x) - 1 for all elements in the array.", "Calculate 2**x for all elements in the array.", "The irrational number e is also known as Euler\u2019s number. It is approximately 2.718281, and is the base of the natural logarithm, ln (this means that, if \\(x = \\ln y = \\log_e y\\), then \\(e^x = y\\). For real input, exp(x) is always positive.", "For complex arguments, x = a + ib, we can write \\(e^x = e^a e^{ib}\\). The first term, \\(e^a\\), is already known (it is the real argument, described above). The second term, \\(e^{ib}\\), is \\(\\cos b + i \\sin b\\), a function with magnitude 1 and a periodic phase.", "Wikipedia, \u201cExponential function\u201d, https://en.wikipedia.org/wiki/Exponential_function", "M. Abramovitz and I. A. Stegun, \u201cHandbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables,\u201d Dover, 1964, p. 69, https://personal.math.ubc.ca/~cbm/aands/page_69.htm", "Plot the magnitude and phase of exp(x) in the complex plane:"]}, {"name": "numpy.exp2()", "path": "reference/generated/numpy.exp2", "type": "numpy.exp2", "text": ["Calculate 2**p for all p in the input array.", "Input values.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Element-wise 2 to the power x. This is a scalar if x is a scalar.", "See also", "New in version 1.3.0."]}, {"name": "numpy.expand_dims()", "path": "reference/generated/numpy.expand_dims", "type": "numpy.expand_dims", "text": ["Expand the shape of an array.", "Insert a new axis that will appear at the axis position in the expanded array shape.", "Input array.", "Position in the expanded axes where the new axis (or axes) is placed.", "Deprecated since version 1.13.0: Passing an axis where axis > a.ndim will be treated as axis == a.ndim, and passing axis < -a.ndim - 1 will be treated as axis == 0. This behavior is deprecated.", "Changed in version 1.18.0: A tuple of axes is now supported. Out of range axes as described above are now forbidden and raise an AxisError.", "View of a with the number of dimensions increased.", "See also", "The inverse operation, removing singleton dimensions", "Insert, remove, and combine dimensions, and resize existing ones", "The following is equivalent to x[np.newaxis, :] or x[np.newaxis]:", "The following is equivalent to x[:, np.newaxis]:", "axis may also be a tuple:", "Note that some examples may use None instead of np.newaxis. These are the same objects:"]}, {"name": "numpy.expm1()", "path": "reference/generated/numpy.expm1", "type": "numpy.expm1", "text": ["Calculate exp(x) - 1 for all elements in the array.", "Input values.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Element-wise exponential minus one: out = exp(x) - 1. This is a scalar if x is a scalar.", "See also", "log(1 + x), the inverse of expm1.", "This function provides greater precision than exp(x) - 1 for small values of x.", "The true value of exp(1e-10) - 1 is 1.00000000005e-10 to about 32 significant digits. This example shows the superiority of expm1 in this case."]}, {"name": "numpy.extract()", "path": "reference/generated/numpy.extract", "type": "numpy.extract", "text": ["Return the elements of an array that satisfy some condition.", "This is equivalent to np.compress(ravel(condition), ravel(arr)). If condition is boolean np.extract is equivalent to arr[condition].", "Note that place does the exact opposite of extract.", "An array whose nonzero or True entries indicate the elements of arr to extract.", "Input array of the same size as condition.", "Rank 1 array of values from arr where condition is True.", "See also", "If condition is boolean:"]}, {"name": "numpy.eye()", "path": "reference/generated/numpy.eye", "type": "numpy.eye", "text": ["Return a 2-D array with ones on the diagonal and zeros elsewhere.", "Number of rows in the output.", "Number of columns in the output. If None, defaults to N.", "Index of the diagonal: 0 (the default) refers to the main diagonal, a positive value refers to an upper diagonal, and a negative value to a lower diagonal.", "Data-type of the returned array.", "Whether the output should be stored in row-major (C-style) or column-major (Fortran-style) order in memory.", "New in version 1.14.0.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "An array where all elements are equal to zero, except for the k-th diagonal, whose values are equal to one.", "See also", "(almost) equivalent function", "diagonal 2-D array from a 1-D array specified by the user."]}, {"name": "numpy.f2py.get_include()", "path": "f2py/usage#numpy.f2py.get_include", "type": "Using F2PY", "text": ["Return the directory that contains the fortranobject.c and .h files.", "Note", "This function is not needed when building an extension with numpy.distutils directly from .f and/or .pyf files in one go.", "Python extension modules built with f2py-generated code need to use fortranobject.c as a source file, and include the fortranobject.h header. This function can be used to obtain the directory containing both of these files.", "Absolute path to the directory containing fortranobject.c and fortranobject.h.", "See also", "function that returns the numpy include directory", "New in version 1.22.0.", "Unless the build system you are using has specific support for f2py, building a Python extension using a .pyf signature file is a two-step process. For a module mymod:", "Step 2: build your Python extension module. This requires the following source files:"]}, {"name": "numpy.f2py.run_main()", "path": "f2py/usage#numpy.f2py.run_main", "type": "Using F2PY", "text": ["Equivalent to running:", "where <args>=string.join(<list>,' '), but in Python. Unless -h is used, this function returns a dictionary containing information on generated modules and their dependencies on source files. For example, the command f2py -m scalar scalar.f can be executed from Python as follows", "You cannot build extension modules with this function, that is, using -c is not allowed. Use compile command instead"]}, {"name": "numpy.fabs()", "path": "reference/generated/numpy.fabs", "type": "numpy.fabs", "text": ["Compute the absolute values element-wise.", "This function returns the absolute values (positive magnitude) of the data in x. Complex values are not handled, use absolute to find the absolute values of complex data.", "The array of numbers for which the absolute values are required. If x is a scalar, the result y will also be a scalar.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The absolute values of x, the returned values are always floats. This is a scalar if x is a scalar.", "See also", "Absolute values including complex types."]}, {"name": "numpy.fill_diagonal()", "path": "reference/generated/numpy.fill_diagonal", "type": "numpy.fill_diagonal", "text": ["Fill the main diagonal of the given array of any dimensionality.", "For an array a with a.ndim >= 2, the diagonal is the list of locations with indices a[i, ..., i] all identical. This function modifies the input array in-place, it does not return a value.", "Array whose diagonal is to be filled, it gets modified in-place.", "Value(s) to write on the diagonal. If val is scalar, the value is written along the diagonal. If array-like, the flattened val is written along the diagonal, repeating if necessary to fill all diagonal entries.", "For tall matrices in NumPy version up to 1.6.2, the diagonal \u201cwrapped\u201d after N columns. You can have this behavior with this option. This affects only tall matrices.", "See also", "New in version 1.4.0.", "This functionality can be obtained via diag_indices, but internally this version uses a much faster implementation that never constructs the indices and uses simple slicing.", "The same function can operate on a 4-D array:", "We only show a few blocks for clarity:", "The wrap option affects only tall matrices:", "The anti-diagonal can be filled by reversing the order of elements using either numpy.flipud or numpy.fliplr.", "Note that the order in which the diagonal is filled varies depending on the flip function."]}, {"name": "numpy.find_common_type()", "path": "reference/generated/numpy.find_common_type", "type": "numpy.find_common_type", "text": ["Determine common type following standard coercion rules.", "A list of dtypes or dtype convertible objects representing arrays.", "A list of dtypes or dtype convertible objects representing scalars.", "The common data type, which is the maximum of array_types ignoring scalar_types, unless the maximum of scalar_types is of a different kind (dtype.kind). If the kind is not understood, then None is returned.", "See also", "The standard casting rules ensure that a scalar cannot up-cast an array unless the scalar is of a fundamentally different kind of data (i.e. under a different hierarchy in the data type hierarchy) then the array:", "Complex is of a different type, so it up-casts the float in the array_types argument:", "Type specifier strings are convertible to dtypes and can therefore be used instead of dtypes:"]}, {"name": "numpy.finfo()", "path": "reference/generated/numpy.finfo", "type": "numpy.finfo", "text": ["Machine limits for floating point types.", "Kind of floating point data-type about which to get information.", "See also", "The implementation of the tests that produce this information.", "The equivalent for integer data types.", "The distance between a value and the nearest adjacent number", "The next floating point value after x1 towards x2", "For developers of NumPy: do not instantiate this at the module level. The initial calculation of these parameters is expensive and negatively impacts import times. These objects are cached, so calling finfo() repeatedly inside your functions is not a problem.", "Note that smallest_normal is not actually the smallest positive representable value in a NumPy floating point type. As in the IEEE-754 standard [1], NumPy floating point types make use of subnormal numbers to fill the gap between 0 and smallest_normal. However, subnormal numbers may have significantly reduced precision [2].", "IEEE Standard for Floating-Point Arithmetic, IEEE Std 754-2008, pp.1-70, 2008, http://www.doi.org/10.1109/IEEESTD.2008.4610935", "Wikipedia, \u201cDenormal Numbers\u201d, https://en.wikipedia.org/wiki/Denormal_number", "The number of bits occupied by the type.", "The difference between 1.0 and the next smallest representable float larger than 1.0. For example, for 64-bit binary floats in the IEEE-754 standard, eps = 2**-52, approximately 2.22e-16.", "The difference between 1.0 and the next smallest representable float less than 1.0. For example, for 64-bit binary floats in the IEEE-754 standard, epsneg = 2**-53, approximately 1.11e-16.", "The number of bits in the exponent portion of the floating point representation.", "The object which calculated these parameters and holds more detailed information.", "The exponent that yields eps.", "The largest representable number.", "The smallest positive power of the base (2) that causes overflow.", "The smallest representable number, typically -max.", "The most negative power of the base (2) consistent with there being no leading 0\u2019s in the mantissa.", "The exponent that yields epsneg.", "The number of bits in the exponent including its sign and bias.", "The number of bits in the mantissa.", "The approximate number of decimal digits to which this kind of float is precise.", "The approximate decimal resolution of this type, i.e., 10**-precision.", "Return the value for tiny, alias of smallest_normal.", "Return the value for the smallest normal.", "The smallest positive floating point number with 0 as leading bit in the mantissa following IEEE-754."]}, {"name": "numpy.fix()", "path": "reference/generated/numpy.fix", "type": "numpy.fix", "text": ["Round to nearest integer towards zero.", "Round an array of floats element-wise to nearest integer towards zero. The rounded values are returned as floats.", "An array of floats to be rounded", "A location into which the result is stored. If provided, it must have a shape that the input broadcasts to. If not provided or None, a freshly-allocated array is returned.", "A float array with the same dimensions as the input. If second argument is not supplied then a float array is returned with the rounded values.", "If a second argument is supplied the result is stored there. The return value out is then a reference to that array.", "See also", "Round to given number of decimals"]}, {"name": "numpy.flatiter", "path": "reference/generated/numpy.flatiter", "type": "numpy.flatiter", "text": ["Flat iterator object to iterate over arrays.", "A flatiter iterator is returned by x.flat for any array x. It allows iterating over the array as if it were a 1-D array, either in a for-loop or by calling its next method.", "Iteration is done in row-major, C-style order (the last index varying the fastest). The iterator can also be indexed using basic slicing or advanced indexing.", "See also", "Return a flat iterator over an array.", "Returns a flattened copy of an array.", "A flatiter iterator can not be constructed directly from Python code by calling the flatiter constructor.", "A reference to the array that is iterated over.", "An N-dimensional tuple of current coordinates.", "Current flat index into the array.", "copy()", "Get a copy of the iterator as a 1-D array."]}, {"name": "numpy.flatnonzero()", "path": "reference/generated/numpy.flatnonzero", "type": "numpy.flatnonzero", "text": ["Return indices that are non-zero in the flattened version of a.", "This is equivalent to np.nonzero(np.ravel(a))[0].", "Input data.", "Output array, containing the indices of the elements of a.ravel() that are non-zero.", "See also", "Return the indices of the non-zero elements of the input array.", "Return a 1-D array containing the elements of the input array.", "Use the indices of the non-zero elements as an index array to extract these elements:"]}, {"name": "numpy.flexible", "path": "reference/arrays.scalars#numpy.flexible", "type": "Scalars", "text": ["Abstract base class of all scalar types without predefined length. The actual size of these types depends on the specific np.dtype instantiation."]}, {"name": "numpy.flip()", "path": "reference/generated/numpy.flip", "type": "numpy.flip", "text": ["Reverse the order of elements in an array along the given axis.", "The shape of the array is preserved, but the elements are reordered.", "New in version 1.12.0.", "Input array.", "Axis or axes along which to flip over. The default, axis=None, will flip over all of the axes of the input array. If axis is negative it counts from the last to the first axis.", "If axis is a tuple of ints, flipping is performed on all of the axes specified in the tuple.", "Changed in version 1.15.0: None and tuples of axes are supported", "A view of m with the entries of axis reversed. Since a view is returned, this operation is done in constant time.", "See also", "Flip an array vertically (axis=0).", "Flip an array horizontally (axis=1).", "flip(m, 0) is equivalent to flipud(m).", "flip(m, 1) is equivalent to fliplr(m).", "flip(m, n) corresponds to m[...,::-1,...] with ::-1 at position n.", "flip(m) corresponds to m[::-1,::-1,...,::-1] with ::-1 at all positions.", "flip(m, (0, 1)) corresponds to m[::-1,::-1,...] with ::-1 at position 0 and position 1."]}, {"name": "numpy.fliplr()", "path": "reference/generated/numpy.fliplr", "type": "numpy.fliplr", "text": ["Reverse the order of elements along axis 1 (left/right).", "For a 2-D array, this flips the entries in each row in the left/right direction. Columns are preserved, but appear in a different order than before.", "Input array, must be at least 2-D.", "A view of m with the columns reversed. Since a view is returned, this operation is \\(\\mathcal O(1)\\).", "See also", "Flip array in the up/down direction.", "Flip array in one or more dimensions.", "Rotate array counterclockwise.", "Equivalent to m[:,::-1] or np.flip(m, axis=1). Requires the array to be at least 2-D."]}, {"name": "numpy.flipud()", "path": "reference/generated/numpy.flipud", "type": "numpy.flipud", "text": ["Reverse the order of elements along axis 0 (up/down).", "For a 2-D array, this flips the entries in each column in the up/down direction. Rows are preserved, but appear in a different order than before.", "Input array.", "A view of m with the rows reversed. Since a view is returned, this operation is \\(\\mathcal O(1)\\).", "See also", "Flip array in the left/right direction.", "Flip array in one or more dimensions.", "Rotate array counterclockwise.", "Equivalent to m[::-1, ...] or np.flip(m, axis=0). Requires the array to be at least 1-D."]}, {"name": "numpy.float128", "path": "reference/arrays.scalars#numpy.float128", "type": "Scalars", "text": ["Alias for numpy.longdouble, named after its size in bits. The existence of these aliases depends on the platform."]}, {"name": "numpy.float16", "path": "reference/arrays.scalars#numpy.float16", "type": "Scalars", "text": ["alias of numpy.half"]}, {"name": "numpy.float32", "path": "reference/arrays.scalars#numpy.float32", "type": "Scalars", "text": ["alias of numpy.single"]}, {"name": "numpy.float64", "path": "reference/arrays.scalars#numpy.float64", "type": "Scalars", "text": ["alias of numpy.double"]}, {"name": "numpy.float96", "path": "reference/arrays.scalars#numpy.float96", "type": "Scalars", "text": ["Alias for numpy.longdouble, named after its size in bits. The existence of these aliases depends on the platform."]}, {"name": "numpy.float_power()", "path": "reference/generated/numpy.float_power", "type": "numpy.float_power", "text": ["First array elements raised to powers from second array, element-wise.", "Raise each base in x1 to the positionally-corresponding power in x2. x1 and x2 must be broadcastable to the same shape. This differs from the power function in that integers, float16, and float32 are promoted to floats with a minimum precision of float64 so that the result is always inexact. The intent is that the function will return a usable result for negative powers and seldom overflow for positive powers.", "Negative values raised to a non-integral value will return nan. To get complex results, cast the input to complex, or specify the dtype to be complex (see the example below).", "New in version 1.12.0.", "The bases.", "The exponents. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The bases in x1 raised to the exponents in x2. This is a scalar if both x1 and x2 are scalars.", "See also", "power function that preserves type", "Cube each element in a list.", "Raise the bases to different exponents.", "The effect of broadcasting.", "Negative values raised to a non-integral value will result in nan (and a warning will be generated).", "To get complex results, give the argument dtype=complex."]}, {"name": "numpy.floor()", "path": "reference/generated/numpy.floor", "type": "numpy.floor", "text": ["Return the floor of the input, element-wise.", "The floor of the scalar x is the largest integer i, such that i <= x. It is often denoted as \\(\\lfloor x \\rfloor\\).", "Input data.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The floor of each element in x. This is a scalar if x is a scalar.", "See also", "Some spreadsheet programs calculate the \u201cfloor-towards-zero\u201d, where floor(-2.5) == -2. NumPy instead uses the definition of floor where floor(-2.5) == -3. The \u201cfloor-towards-zero\u201d function is called fix in NumPy."]}, {"name": "numpy.floor_divide()", "path": "reference/generated/numpy.floor_divide", "type": "numpy.floor_divide", "text": ["Return the largest integer smaller or equal to the division of the inputs. It is equivalent to the Python // operator and pairs with the Python % (remainder), function so that a = a % b + b * (a // b) up to roundoff.", "Numerator.", "Denominator. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "y = floor(x1/x2) This is a scalar if both x1 and x2 are scalars.", "See also", "Remainder complementary to floor_divide.", "Simultaneous floor division and remainder.", "Standard division.", "Round a number to the nearest integer toward minus infinity.", "Round a number to the nearest integer toward infinity.", "The // operator can be used as a shorthand for np.floor_divide on ndarrays."]}, {"name": "numpy.fmax()", "path": "reference/generated/numpy.fmax", "type": "numpy.fmax", "text": ["Element-wise maximum of array elements.", "Compare two arrays and returns a new array containing the element-wise maxima. If one of the elements being compared is a NaN, then the non-nan element is returned. If both elements are NaNs then the first is returned. The latter distinction is important for complex NaNs, which are defined as at least one of the real or imaginary parts being a NaN. The net effect is that NaNs are ignored when possible.", "The arrays holding the elements to be compared. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The maximum of x1 and x2, element-wise. This is a scalar if both x1 and x2 are scalars.", "See also", "Element-wise minimum of two arrays, ignores NaNs.", "Element-wise maximum of two arrays, propagates NaNs.", "The maximum value of an array along a given axis, propagates NaNs.", "The maximum value of an array along a given axis, ignores NaNs.", "New in version 1.3.0.", "The fmax is equivalent to np.where(x1 >= x2, x1, x2) when neither x1 nor x2 are NaNs, but it is faster and does proper broadcasting."]}, {"name": "numpy.fmin()", "path": "reference/generated/numpy.fmin", "type": "numpy.fmin", "text": ["Element-wise minimum of array elements.", "Compare two arrays and returns a new array containing the element-wise minima. If one of the elements being compared is a NaN, then the non-nan element is returned. If both elements are NaNs then the first is returned. The latter distinction is important for complex NaNs, which are defined as at least one of the real or imaginary parts being a NaN. The net effect is that NaNs are ignored when possible.", "The arrays holding the elements to be compared. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The minimum of x1 and x2, element-wise. This is a scalar if both x1 and x2 are scalars.", "See also", "Element-wise maximum of two arrays, ignores NaNs.", "Element-wise minimum of two arrays, propagates NaNs.", "The minimum value of an array along a given axis, propagates NaNs.", "The minimum value of an array along a given axis, ignores NaNs.", "New in version 1.3.0.", "The fmin is equivalent to np.where(x1 <= x2, x1, x2) when neither x1 nor x2 are NaNs, but it is faster and does proper broadcasting."]}, {"name": "numpy.fmod()", "path": "reference/generated/numpy.fmod", "type": "numpy.fmod", "text": ["Returns the element-wise remainder of division.", "This is the NumPy implementation of the C library function fmod, the remainder has the same sign as the dividend x1. It is equivalent to the Matlab(TM) rem function and should not be confused with the Python modulus operator x1 % x2.", "Dividend.", "Divisor. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The remainder of the division of x1 by x2. This is a scalar if both x1 and x2 are scalars.", "See also", "Equivalent to the Python % operator.", "The result of the modulo operation for negative dividend and divisors is bound by conventions. For fmod, the sign of result is the sign of the dividend, while for remainder the sign of the result is the sign of the divisor. The fmod function is equivalent to the Matlab(TM) rem function."]}, {"name": "numpy.format_float_positional()", "path": "reference/generated/numpy.format_float_positional", "type": "numpy.format_float_positional", "text": ["Format a floating-point scalar as a decimal string in positional notation.", "Provides control over rounding, trimming and padding. Uses and assumes IEEE unbiased rounding. Uses the \u201cDragon4\u201d algorithm.", "Value to format.", "Maximum number of digits to print. May be None if unique is True, but must be an integer if unique is False.", "If True, use a digit-generation strategy which gives the shortest representation which uniquely identifies the floating-point number from other values of the same type, by judicious rounding. If precision is given fewer digits than necessary can be printed, or if min_digits is given more can be printed, in which cases the last digit is rounded with unbiased rounding. If False, digits are generated as if printing an infinite-precision value and stopping after precision digits, rounding the remaining value with unbiased rounding", "If True, the cutoffs of precision and min_digits refer to the total number of digits after the decimal point, including leading zeros. If False, precision and min_digits refer to the total number of significant digits, before or after the decimal point, ignoring leading zeros.", "Controls post-processing trimming of trailing digits, as follows:", "Whether to show the sign for positive values.", "Pad the left side of the string with whitespace until at least that many characters are to the left of the decimal point.", "Pad the right side of the string with whitespace until at least that many characters are to the right of the decimal point.", "Minimum number of digits to print. Only has an effect if unique=True in which case additional digits past those necessary to uniquely identify the value may be printed, rounding the last additional digit.", "\u2013 versionadded:: 1.21.0", "The string representation of the floating point value", "See also"]}, {"name": "numpy.format_float_scientific()", "path": "reference/generated/numpy.format_float_scientific", "type": "numpy.format_float_scientific", "text": ["Format a floating-point scalar as a decimal string in scientific notation.", "Provides control over rounding, trimming and padding. Uses and assumes IEEE unbiased rounding. Uses the \u201cDragon4\u201d algorithm.", "Value to format.", "Maximum number of digits to print. May be None if unique is True, but must be an integer if unique is False.", "If True, use a digit-generation strategy which gives the shortest representation which uniquely identifies the floating-point number from other values of the same type, by judicious rounding. If precision is given fewer digits than necessary can be printed. If min_digits is given more can be printed, in which cases the last digit is rounded with unbiased rounding. If False, digits are generated as if printing an infinite-precision value and stopping after precision digits, rounding the remaining value with unbiased rounding", "Controls post-processing trimming of trailing digits, as follows:", "Whether to show the sign for positive values.", "Pad the left side of the string with whitespace until at least that many characters are to the left of the decimal point.", "Pad the exponent with zeros until it contains at least this many digits. If omitted, the exponent will be at least 2 digits.", "Minimum number of digits to print. This only has an effect for unique=True. In that case more digits than necessary to uniquely identify the value may be printed and rounded unbiased.", "\u2013 versionadded:: 1.21.0", "The string representation of the floating point value", "See also"]}, {"name": "numpy.format_parser()", "path": "reference/generated/numpy.format_parser", "type": "numpy.format_parser", "text": ["Class to convert formats, names, titles description to a dtype.", "After constructing the format_parser object, the dtype attribute is the converted data-type: dtype = format_parser(formats, names, titles).dtype", "The format description, either specified as a string with comma-separated format descriptions in the form 'f8, i4, a5', or a list of format description strings in the form ['f8', 'i4', 'a5'].", "The field names, either specified as a comma-separated string in the form 'col1, col2, col3', or as a list or tuple of strings in the form ['col1', 'col2', 'col3']. An empty list can be used, in that case default field names (\u2018f0\u2019, \u2018f1\u2019, \u2026) are used.", "Sequence of title strings. An empty list can be used to leave titles out.", "If True, align the fields by padding as the C-compiler would. Default is False.", "If specified, all the fields will be changed to the provided byte-order. Otherwise, the default byte-order is used. For all available string specifiers, see dtype.newbyteorder.", "See also", "names and/or titles can be empty lists. If titles is an empty list, titles will simply not appear. If names is empty, default field names will be used.", "The converted data-type."]}, {"name": "numpy.frexp()", "path": "reference/generated/numpy.frexp", "type": "numpy.frexp", "text": ["Decompose the elements of x into mantissa and twos exponent.", "Returns (mantissa, exponent), where x = mantissa * 2**exponent`. The mantissa lies in the open interval(-1, 1), while the twos exponent is a signed integer.", "Array of numbers to be decomposed.", "Output array for the mantissa. Must have the same shape as x.", "Output array for the exponent. Must have the same shape as x.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Floating values between -1 and 1. This is a scalar if x is a scalar.", "Integer exponents of 2. This is a scalar if x is a scalar.", "See also", "Compute y = x1 * 2**x2, the inverse of frexp.", "Complex dtypes are not supported, they will raise a TypeError."]}, {"name": "numpy.frombuffer()", "path": "reference/generated/numpy.frombuffer", "type": "numpy.frombuffer", "text": ["Interpret a buffer as a 1-dimensional array.", "An object that exposes the buffer interface.", "Data-type of the returned array; default: float.", "Number of items to read. -1 means all data in the buffer.", "Start reading the buffer from this offset (in bytes); default: 0.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "If the buffer has data that is not in machine byte-order, this should be specified as part of the data-type, e.g.:", "The data of the resulting array will not be byteswapped, but will be interpreted correctly."]}, {"name": "numpy.fromfile()", "path": "reference/generated/numpy.fromfile", "type": "numpy.fromfile", "text": ["Construct an array from data in a text or binary file.", "A highly efficient way of reading binary data with a known data-type, as well as parsing simply formatted text files. Data written using the tofile method can be read using this function.", "Open file object or filename.", "Changed in version 1.17.0: pathlib.Path objects are now accepted.", "Data type of the returned array. For binary files, it is used to determine the size and byte-order of the items in the file. Most builtin numeric types are supported and extension types may be supported.", "New in version 1.18.0: Complex dtypes.", "Number of items to read. -1 means all items (i.e., the complete file).", "Separator between items if file is a text file. Empty (\u201c\u201d) separator means the file should be treated as binary. Spaces (\u201d \u201c) in the separator match zero or more whitespace characters. A separator consisting only of spaces must match at least one whitespace.", "The offset (in bytes) from the file\u2019s current position. Defaults to 0. Only permitted for binary files.", "New in version 1.17.0.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "See also", "More flexible way of loading data from a text file.", "Do not rely on the combination of tofile and fromfile for data storage, as the binary files generated are not platform independent. In particular, no byte-order or data-type information is saved. Data can be stored in the platform independent .npy format using save and load instead.", "Construct an ndarray:", "Save the raw data to disk:", "Read the raw data from disk:", "The recommended way to store and load data:"]}, {"name": "numpy.fromfunction()", "path": "reference/generated/numpy.fromfunction", "type": "numpy.fromfunction", "text": ["Construct an array by executing a function over each coordinate.", "The resulting array therefore has a value fn(x, y, z) at coordinate (x, y, z).", "The function is called with N parameters, where N is the rank of shape. Each parameter represents the coordinates of the array varying along a specific axis. For example, if shape were (2, 2), then the parameters would be array([[0, 0], [1, 1]]) and array([[0, 1], [0, 1]])", "Shape of the output array, which also determines the shape of the coordinate arrays passed to function.", "Data-type of the coordinate arrays passed to function. By default, dtype is float.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "The result of the call to function is passed back directly. Therefore the shape of fromfunction is completely determined by function. If function returns a scalar value, the shape of fromfunction would not match the shape parameter.", "See also", "Keywords other than dtype are passed to function."]}, {"name": "numpy.fromiter()", "path": "reference/generated/numpy.fromiter", "type": "numpy.fromiter", "text": ["Create a new 1-dimensional array from an iterable object.", "An iterable object providing data for the array.", "The data-type of the returned array.", "The number of items to read from iterable. The default is -1, which means all data is read.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "The output array.", "Specify count to improve performance. It allows fromiter to pre-allocate the output array, instead of resizing it on demand."]}, {"name": "numpy.frompyfunc()", "path": "reference/generated/numpy.frompyfunc", "type": "numpy.frompyfunc", "text": ["Takes an arbitrary Python function and returns a NumPy ufunc.", "Can be used, for example, to add broadcasting to a built-in Python function (see Examples section).", "An arbitrary Python function.", "The number of input arguments.", "The number of objects returned by func.", "The value to use for the identity attribute of the resulting object. If specified, this is equivalent to setting the underlying C identity field to PyUFunc_IdentityValue. If omitted, the identity is set to PyUFunc_None. Note that this is _not_ equivalent to setting the identity to None, which implies the operation is reorderable.", "Returns a NumPy universal function (ufunc) object.", "See also", "Evaluates pyfunc over input arrays using broadcasting rules of numpy.", "The returned ufunc always returns PyObject arrays.", "Use frompyfunc to add broadcasting to the Python function oct:"]}, {"name": "numpy.fromregex()", "path": "reference/generated/numpy.fromregex", "type": "numpy.fromregex", "text": ["Construct an array from a text file, using regular expression parsing.", "The returned array is always a structured array, and is constructed from all matches of the regular expression in the file. Groups in the regular expression are converted to fields of the structured array.", "Filename or file object to read.", "Changed in version 1.22.0: Now accepts os.PathLike implementations.", "Regular expression used to parse the file. Groups in the regular expression correspond to fields in the dtype.", "Dtype for the structured array; must be a structured datatype.", "Encoding used to decode the inputfile. Does not apply to input streams.", "New in version 1.14.0.", "The output array, containing the part of the content of file that was matched by regexp. output is always a structured array.", "When dtype is not a valid dtype for a structured array.", "See also", "Dtypes for structured arrays can be specified in several forms, but all forms specify at least the data type and field name. For details see basics.rec."]}, {"name": "numpy.fromstring()", "path": "reference/generated/numpy.fromstring", "type": "numpy.fromstring", "text": ["A new 1-D array initialized from text data in a string.", "A string containing the data.", "The data type of the array; default: float. For binary input data, the data must be in exactly this format. Most builtin numeric types are supported and extension types may be supported.", "New in version 1.18.0: Complex dtypes.", "Read this number of dtype elements from the data. If this is negative (the default), the count will be determined from the length of the data.", "The string separating numbers in the data; extra whitespace between elements is also ignored.", "Deprecated since version 1.14: Passing sep='', the default, is deprecated since it will trigger the deprecated binary mode of this function. This mode interprets string as binary bytes, rather than ASCII text with decimal numbers, an operation which is better spelt frombuffer(string, dtype, count). If string contains unicode text, the binary mode of fromstring will first encode it into bytes using either utf-8 (python 3) or the default encoding (python 2), neither of which produce sane results.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "The constructed array.", "If the string is not the correct size to satisfy the requested dtype and count.", "See also"]}, {"name": "numpy.full()", "path": "reference/generated/numpy.full", "type": "numpy.full", "text": ["Return a new array of given shape and type, filled with fill_value.", "Shape of the new array, e.g., (2, 3) or 2.", "Fill value.", "np.array(fill_value).dtype.", "Whether to store multidimensional data in C- or Fortran-contiguous (row- or column-wise) order in memory.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "Array of fill_value with the given shape, dtype, and order.", "See also", "Return a new array with shape of input filled with value.", "Return a new uninitialized array.", "Return a new array setting values to one.", "Return a new array setting values to zero."]}, {"name": "numpy.full_like()", "path": "reference/generated/numpy.full_like", "type": "numpy.full_like", "text": ["Return a full array with the same shape and type as a given array.", "The shape and data-type of a define these same attributes of the returned array.", "Fill value.", "Overrides the data type of the result.", "Overrides the memory layout of the result. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of a as closely as possible.", "If True, then the newly created array will use the sub-class type of a, otherwise it will be a base-class array. Defaults to True.", "Overrides the shape of the result. If order=\u2019K\u2019 and the number of dimensions is unchanged, will try to keep order, otherwise, order=\u2019C\u2019 is implied.", "New in version 1.17.0.", "Array of fill_value with the same shape and type as a.", "See also", "Return an empty array with shape and type of input.", "Return an array of ones with shape and type of input.", "Return an array of zeros with shape and type of input.", "Return a new array of given shape filled with value."]}, {"name": "numpy.gcd()", "path": "reference/generated/numpy.gcd", "type": "numpy.gcd", "text": ["Returns the greatest common divisor of |x1| and |x2|", "Arrays of values. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "The greatest common divisor of the absolute value of the inputs This is a scalar if both x1 and x2 are scalars.", "See also", "The lowest common multiple"]}, {"name": "numpy.genfromtxt()", "path": "reference/generated/numpy.genfromtxt", "type": "numpy.genfromtxt", "text": ["Load data from a text file, with missing values handled as specified.", "Each line past the first skip_header lines is split at the delimiter character, and characters following the comments character are discarded.", "File, filename, list, or generator to read. If the filename extension is .gz or .bz2, the file is first decompressed. Note that generators must return bytes or strings. The strings in a list or produced by a generator are treated as lines.", "Data type of the resulting array. If None, the dtypes will be determined by the contents of each column, individually.", "The character used to indicate the start of a comment. All the characters occurring on a line after a comment are discarded.", "The string used to separate values. By default, any consecutive whitespaces act as delimiter. An integer or sequence of integers can also be provided as width(s) of each field.", "skiprows was removed in numpy 1.10. Please use skip_header instead.", "The number of lines to skip at the beginning of the file.", "The number of lines to skip at the end of the file.", "The set of functions that convert the data of a column to a value. The converters can also be used to provide a default value for missing data: converters = {3: lambda s: float(s or 0)}.", "missing was removed in numpy 1.10. Please use missing_values instead.", "The set of strings corresponding to missing data.", "The set of values to be used as default when the data are missing.", "Which columns to read, with 0 being the first. For example, usecols = (1, 4, 5) will extract the 2nd, 5th and 6th columns.", "If names is True, the field names are read from the first line after the first skip_header lines. This line can optionally be preceded by a comment delimiter. If names is a sequence or a single-string of comma-separated names, the names will be used to define the field names in a structured dtype. If names is None, the names of the dtype fields will be used, if any.", "A list of names to exclude. This list is appended to the default list [\u2018return\u2019,\u2019file\u2019,\u2019print\u2019]. Excluded names are appended with an underscore: for example, file would become file_.", "A string combining invalid characters that must be deleted from the names.", "A format used to define default field names, such as \u201cf%i\u201d or \u201cf_%02i\u201d.", "Whether to automatically strip white spaces from the variables.", "Character(s) used in replacement of white spaces in the variable names. By default, use a \u2018_\u2019.", "If True, field names are case sensitive. If False or \u2018upper\u2019, field names are converted to upper case. If \u2018lower\u2019, field names are converted to lower case.", "If True, the returned array is transposed, so that arguments may be unpacked using x, y, z = genfromtxt(...). When used with a structured data-type, arrays are returned for each field. Default is False.", "If True, return a masked array. If False, return a regular array.", "If True, do not raise errors for invalid values.", "If True, an exception is raised if an inconsistency is detected in the number of columns. If False, a warning is emitted and the offending lines are skipped.", "The maximum number of rows to read. Must not be used with skip_footer at the same time. If given, the value must be at least 1. Default is to read the entire file.", "New in version 1.10.0.", "Encoding used to decode the inputfile. Does not apply when fname is a file object. The special value \u2018bytes\u2019 enables backward compatibility workarounds that ensure that you receive byte arrays when possible and passes latin1 encoded strings to converters. Override this value to receive unicode arrays and pass strings as input to converters. If set to None the system default is used. The default value is \u2018bytes\u2019.", "New in version 1.14.0.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "Data read from the text file. If usemask is True, this is a masked array.", "See also", "equivalent function when no data is missing.", "NumPy User Guide, section I/O with NumPy.", "Comma delimited file with mixed dtype", "Using dtype = None", "Specifying dtype and names", "An example with fixed-width columns", "An example to show comments"]}, {"name": "numpy.geomspace()", "path": "reference/generated/numpy.geomspace", "type": "numpy.geomspace", "text": ["Return numbers spaced evenly on a log scale (a geometric progression).", "This is similar to logspace, but with endpoints specified directly. Each output sample is a constant multiple of the previous.", "Changed in version 1.16.0: Non-scalar start and stop are now supported.", "The starting value of the sequence.", "The final value of the sequence, unless endpoint is False. In that case, num + 1 values are spaced over the interval in log-space, of which all but the last (a sequence of length num) are returned.", "Number of samples to generate. Default is 50.", "If true, stop is the last sample. Otherwise, it is not included. Default is True.", "The type of the output array. If dtype is not given, the data type is inferred from start and stop. The inferred dtype will never be an integer; float is chosen even if the arguments would produce an array of integers.", "The axis in the result to store the samples. Relevant only if start or stop are array-like. By default (0), the samples will be along a new axis inserted at the beginning. Use -1 to get an axis at the end.", "New in version 1.16.0.", "num samples, equally spaced on a log scale.", "See also", "Similar to geomspace, but with endpoints specified using log and base.", "Similar to geomspace, but with arithmetic instead of geometric progression.", "Similar to linspace, with the step size specified instead of the number of samples.", "If the inputs or dtype are complex, the output will follow a logarithmic spiral in the complex plane. (There are an infinite number of spirals passing through two points; the output will follow the shortest such path.)", "Note that the above may not produce exact integers:", "Negative, decreasing, and complex inputs are allowed:", "Graphical illustration of endpoint parameter:"]}, {"name": "numpy.get_include()", "path": "reference/generated/numpy.get_include", "type": "numpy.get_include", "text": ["Return the directory that contains the NumPy *.h header files.", "Extension modules that need to compile against NumPy should use this function to locate the appropriate include directory.", "When using distutils, for example in setup.py."]}, {"name": "numpy.get_printoptions()", "path": "reference/generated/numpy.get_printoptions", "type": "numpy.get_printoptions", "text": ["Return the current print options.", "Dictionary of current print options with keys", "For a full description of these options, see set_printoptions.", "See also"]}, {"name": "numpy.getbufsize()", "path": "reference/generated/numpy.getbufsize", "type": "numpy.getbufsize", "text": ["Return the size of the buffer used in ufuncs.", "Size of ufunc buffer in bytes."]}, {"name": "numpy.geterr()", "path": "reference/generated/numpy.geterr", "type": "numpy.geterr", "text": ["Get the current way of handling floating-point errors.", "A dictionary with keys \u201cdivide\u201d, \u201cover\u201d, \u201cunder\u201d, and \u201cinvalid\u201d, whose values are from the strings \u201cignore\u201d, \u201cprint\u201d, \u201clog\u201d, \u201cwarn\u201d, \u201craise\u201d, and \u201ccall\u201d. The keys represent possible floating-point exceptions, and the values define how these exceptions are handled.", "See also", "For complete documentation of the types of floating-point exceptions and treatment options, see seterr."]}, {"name": "numpy.geterrcall()", "path": "reference/generated/numpy.geterrcall", "type": "numpy.geterrcall", "text": ["Return the current callback function used on floating-point errors.", "When the error handling for a floating-point error (one of \u201cdivide\u201d, \u201cover\u201d, \u201cunder\u201d, or \u201cinvalid\u201d) is set to \u2018call\u2019 or \u2018log\u2019, the function that is called or the log instance that is written to is returned by geterrcall. This function or log instance has been set with seterrcall.", "The current error handler. If no handler was set through seterrcall, None is returned.", "See also", "For complete documentation of the types of floating-point exceptions and treatment options, see seterr."]}, {"name": "numpy.geterrobj()", "path": "reference/generated/numpy.geterrobj", "type": "numpy.geterrobj", "text": ["Return the current object that defines floating-point error handling.", "The error object contains all information that defines the error handling behavior in NumPy. geterrobj is used internally by the other functions that get and set error handling behavior (geterr, seterr, geterrcall, seterrcall).", "The error object, a list containing three elements: [internal numpy buffer size, error mask, error callback function].", "The error mask is a single integer that holds the treatment information on all four floating point errors. The information for each error type is contained in three bits of the integer. If we print it in base 8, we can see what treatment is set for \u201cinvalid\u201d, \u201cunder\u201d, \u201cover\u201d, and \u201cdivide\u201d (in that order). The printed string can be interpreted with", "See also", "For complete documentation of the types of floating-point exceptions and treatment options, see seterr."]}, {"name": "numpy.gradient()", "path": "reference/generated/numpy.gradient", "type": "numpy.gradient", "text": ["Return the gradient of an N-dimensional array.", "The gradient is computed using second order accurate central differences in the interior points and either first or second order accurate one-sides (forward or backwards) differences at the boundaries. The returned gradient hence has the same shape as the input array.", "An N-dimensional array containing samples of a scalar function.", "Spacing between f values. Default unitary spacing for all dimensions. Spacing can be specified using:", "If axis is given, the number of varargs must equal the number of axes. Default: 1.", "Gradient is calculated using N-th order accurate differences at the boundaries. Default: 1.", "New in version 1.9.1.", "Gradient is calculated only along the given axis or axes The default (axis = None) is to calculate the gradient for all the axes of the input array. axis may be negative, in which case it counts from the last to the first axis.", "New in version 1.11.0.", "A list of ndarrays (or a single ndarray if there is only one dimension) corresponding to the derivatives of f with respect to each dimension. Each derivative has the same shape as f.", "Assuming that \\(f\\in C^{3}\\) (i.e., \\(f\\) has at least 3 continuous derivatives) and let \\(h_{*}\\) be a non-homogeneous stepsize, we minimize the \u201cconsistency error\u201d \\(\\eta_{i}\\) between the true gradient and its estimate from a linear combination of the neighboring grid-points:", "By substituting \\(f(x_{i} + h_{d})\\) and \\(f(x_{i} - h_{s})\\) with their Taylor series expansion, this translates into solving the following the linear system:", "The resulting approximation of \\(f_{i}^{(1)}\\) is the following:", "It is worth noting that if \\(h_{s}=h_{d}\\) (i.e., data are evenly spaced) we find the standard second order approximation:", "With a similar procedure the forward/backward approximations used for boundaries can be derived.", "Quarteroni A., Sacco R., Saleri F. (2007) Numerical Mathematics (Texts in Applied Mathematics). New York: Springer.", "Durran D. R. (1999) Numerical Methods for Wave Equations in Geophysical Fluid Dynamics. New York: Springer.", "Fornberg B. (1988) Generation of Finite Difference Formulas on Arbitrarily Spaced Grids, Mathematics of Computation 51, no. 184 : 699-706. PDF.", "Spacing can be also specified with an array that represents the coordinates of the values F along the dimensions. For instance a uniform spacing:", "Or a non uniform one:", "For two dimensional arrays, the return will be two arrays ordered by axis. In this example the first array stands for the gradient in rows and the second one in columns direction:", "In this example the spacing is also specified: uniform for axis=0 and non uniform for axis=1", "It is possible to specify how boundaries are treated using edge_order", "The axis keyword can be used to specify a subset of axes of which the gradient is calculated"]}, {"name": "numpy.greater()", "path": "reference/generated/numpy.greater", "type": "numpy.greater", "text": ["Return the truth value of (x1 > x2) element-wise.", "Input arrays. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Output array, element-wise comparison of x1 and x2. Typically of type bool, unless dtype=object is passed. This is a scalar if both x1 and x2 are scalars.", "See also", "The > operator can be used as a shorthand for np.greater on ndarrays."]}, {"name": "numpy.greater_equal()", "path": "reference/generated/numpy.greater_equal", "type": "numpy.greater_equal", "text": ["Return the truth value of (x1 >= x2) element-wise.", "Input arrays. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Output array, element-wise comparison of x1 and x2. Typically of type bool, unless dtype=object is passed. This is a scalar if both x1 and x2 are scalars.", "See also", "The >= operator can be used as a shorthand for np.greater_equal on ndarrays."]}, {"name": "numpy.half", "path": "reference/arrays.scalars#numpy.half", "type": "Scalars", "text": ["Half-precision floating-point number type.", "'e'", "numpy.float16: 16-bit-precision floating-point number type: sign bit, 5 bits exponent, 10 bits mantissa."]}, {"name": "numpy.hamming()", "path": "reference/generated/numpy.hamming", "type": "numpy.hamming", "text": ["Return the Hamming window.", "The Hamming window is a taper formed by using a weighted cosine.", "Number of points in the output window. If zero or less, an empty array is returned.", "The window, with the maximum value normalized to one (the value one appears only if the number of samples is odd).", "See also", "The Hamming window is defined as", "The Hamming was named for R. W. Hamming, an associate of J. W. Tukey and is described in Blackman and Tukey. It was recommended for smoothing the truncated autocovariance function in the time domain. Most references to the Hamming window come from the signal processing literature, where it is used as one of many windowing functions for smoothing values. It is also known as an apodization (which means \u201cremoving the foot\u201d, i.e. smoothing discontinuities at the beginning and end of the sampled signal) or tapering function.", "Blackman, R.B. and Tukey, J.W., (1958) The measurement of power spectra, Dover Publications, New York.", "E.R. Kanasewich, \u201cTime Sequence Analysis in Geophysics\u201d, The University of Alberta Press, 1975, pp. 109-110.", "Wikipedia, \u201cWindow function\u201d, https://en.wikipedia.org/wiki/Window_function", "W.H. Press, B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling, \u201cNumerical Recipes\u201d, Cambridge University Press, 1986, page 425.", "Plot the window and the frequency response:"]}, {"name": "numpy.hanning()", "path": "reference/generated/numpy.hanning", "type": "numpy.hanning", "text": ["Return the Hanning window.", "The Hanning window is a taper formed by using a weighted cosine.", "Number of points in the output window. If zero or less, an empty array is returned.", "The window, with the maximum value normalized to one (the value one appears only if M is odd).", "See also", "The Hanning window is defined as", "The Hanning was named for Julius von Hann, an Austrian meteorologist. It is also known as the Cosine Bell. Some authors prefer that it be called a Hann window, to help avoid confusion with the very similar Hamming window.", "Most references to the Hanning window come from the signal processing literature, where it is used as one of many windowing functions for smoothing values. It is also known as an apodization (which means \u201cremoving the foot\u201d, i.e. smoothing discontinuities at the beginning and end of the sampled signal) or tapering function.", "Blackman, R.B. and Tukey, J.W., (1958) The measurement of power spectra, Dover Publications, New York.", "E.R. Kanasewich, \u201cTime Sequence Analysis in Geophysics\u201d, The University of Alberta Press, 1975, pp. 106-108.", "Wikipedia, \u201cWindow function\u201d, https://en.wikipedia.org/wiki/Window_function", "W.H. Press, B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling, \u201cNumerical Recipes\u201d, Cambridge University Press, 1986, page 425.", "Plot the window and its frequency response:"]}, {"name": "numpy.heaviside()", "path": "reference/generated/numpy.heaviside", "type": "numpy.heaviside", "text": ["Compute the Heaviside step function.", "The Heaviside step function is defined as:", "where x2 is often taken to be 0.5, but 0 and 1 are also sometimes used.", "Input values.", "The value of the function when x1 is 0. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The output array, element-wise Heaviside step function of x1. This is a scalar if both x1 and x2 are scalars.", "New in version 1.13.0."]}, {"name": "numpy.histogram()", "path": "reference/generated/numpy.histogram", "type": "numpy.histogram", "text": ["Compute the histogram of a dataset.", "Input data. The histogram is computed over the flattened array.", "If bins is an int, it defines the number of equal-width bins in the given range (10, by default). If bins is a sequence, it defines a monotonically increasing array of bin edges, including the rightmost edge, allowing for non-uniform bin widths.", "New in version 1.11.0.", "If bins is a string, it defines the method used to calculate the optimal bin width, as defined by histogram_bin_edges.", "The lower and upper range of the bins. If not provided, range is simply (a.min(), a.max()). Values outside the range are ignored. The first element of the range must be less than or equal to the second. range affects the automatic bin computation as well. While bin width is computed to be optimal based on the actual data within range, the bin count will fill the entire range including portions containing no data.", "Deprecated since version 1.6.0.", "This is equivalent to the density argument, but produces incorrect results for unequal bin widths. It should not be used.", "Changed in version 1.15.0: DeprecationWarnings are actually emitted.", "An array of weights, of the same shape as a. Each value in a only contributes its associated weight towards the bin count (instead of 1). If density is True, the weights are normalized, so that the integral of the density over the range remains 1.", "If False, the result will contain the number of samples in each bin. If True, the result is the value of the probability density function at the bin, normalized such that the integral over the range is 1. Note that the sum of the histogram values will not be equal to 1 unless bins of unity width are chosen; it is not a probability mass function.", "Overrides the normed keyword if given.", "The values of the histogram. See density and weights for a description of the possible semantics.", "Return the bin edges (length(hist)+1).", "See also", "All but the last (righthand-most) bin is half-open. In other words, if bins is:", "then the first bin is [1, 2) (including 1, but excluding 2) and the second [2, 3). The last bin, however, is [3, 4], which includes 4.", "New in version 1.11.0.", "Automated Bin Selection Methods example, using 2 peak random data with 2000 points:"]}, {"name": "numpy.histogram2d()", "path": "reference/generated/numpy.histogram2d", "type": "numpy.histogram2d", "text": ["Compute the bi-dimensional histogram of two data samples.", "An array containing the x coordinates of the points to be histogrammed.", "An array containing the y coordinates of the points to be histogrammed.", "The bin specification:", "The leftmost and rightmost edges of the bins along each dimension (if not specified explicitly in the bins parameters): [[xmin, xmax], [ymin, ymax]]. All values outside of this range will be considered outliers and not tallied in the histogram.", "If False, the default, returns the number of samples in each bin. If True, returns the probability density function at the bin, bin_count / sample_count / bin_area.", "An alias for the density argument that behaves identically. To avoid confusion with the broken normed argument to histogram, density should be preferred.", "An array of values w_i weighing each sample (x_i, y_i). Weights are normalized to 1 if normed is True. If normed is False, the values of the returned histogram are equal to the sum of the weights belonging to the samples falling into each bin.", "The bi-dimensional histogram of samples x and y. Values in x are histogrammed along the first dimension and values in y are histogrammed along the second dimension.", "The bin edges along the first dimension.", "The bin edges along the second dimension.", "See also", "1D histogram", "Multidimensional histogram", "When normed is True, then the returned histogram is the sample density, defined such that the sum over bins of the product bin_value * bin_area is 1.", "Please note that the histogram does not follow the Cartesian convention where x values are on the abscissa and y values on the ordinate axis. Rather, x is histogrammed along the first dimension of the array (vertical), and y along the second dimension of the array (horizontal). This ensures compatibility with histogramdd.", "Construct a 2-D histogram with variable bin width. First define the bin edges:", "Next we create a histogram H with random bin content:", "imshow can only display square bins:", "pcolormesh can display actual edges:", "NonUniformImage can be used to display actual bin edges with interpolation:", "It is also possible to construct a 2-D histogram without specifying bin edges:", "Now we can plot the histogram using pcolormesh, and a hexbin for comparison."]}, {"name": "numpy.histogram_bin_edges()", "path": "reference/generated/numpy.histogram_bin_edges", "type": "numpy.histogram_bin_edges", "text": ["Function to calculate only the edges of the bins used by the histogram function.", "Input data. The histogram is computed over the flattened array.", "If bins is an int, it defines the number of equal-width bins in the given range (10, by default). If bins is a sequence, it defines the bin edges, including the rightmost edge, allowing for non-uniform bin widths.", "If bins is a string from the list below, histogram_bin_edges will use the method chosen to calculate the optimal bin width and consequently the number of bins (see Notes for more detail on the estimators) from the data that falls within the requested range. While the bin width will be optimal for the actual data in the range, the number of bins will be computed to fill the entire range, including the empty portions. For visualisation, using the \u2018auto\u2019 option is suggested. Weighted data is not supported for automated bin size selection.", "Maximum of the \u2018sturges\u2019 and \u2018fd\u2019 estimators. Provides good all around performance.", "Robust (resilient to outliers) estimator that takes into account data variability and data size.", "An improved version of Sturges\u2019 estimator that works better with non-normal datasets.", "Less robust estimator that that takes into account data variability and data size.", "Estimator based on leave-one-out cross-validation estimate of the integrated squared error. Can be regarded as a generalization of Scott\u2019s rule.", "Estimator does not take variability into account, only data size. Commonly overestimates number of bins required.", "R\u2019s default method, only accounts for data size. Only optimal for gaussian data and underestimates number of bins for large non-gaussian datasets.", "Square root (of data size) estimator, used by Excel and other programs for its speed and simplicity.", "The lower and upper range of the bins. If not provided, range is simply (a.min(), a.max()). Values outside the range are ignored. The first element of the range must be less than or equal to the second. range affects the automatic bin computation as well. While bin width is computed to be optimal based on the actual data within range, the bin count will fill the entire range including portions containing no data.", "An array of weights, of the same shape as a. Each value in a only contributes its associated weight towards the bin count (instead of 1). This is currently not used by any of the bin estimators, but may be in the future.", "The edges to pass into histogram", "See also", "The methods to estimate the optimal number of bins are well founded in literature, and are inspired by the choices R provides for histogram visualisation. Note that having the number of bins proportional to \\(n^{1/3}\\) is asymptotically optimal, which is why it appears in most estimators. These are simply plug-in methods that give good starting points for number of bins. In the equations below, \\(h\\) is the binwidth and \\(n_h\\) is the number of bins. All estimators that compute bin counts are recast to bin width using the ptp of the data. The final bin count is obtained from np.round(np.ceil(range / h)). The final bin width is often less than what is returned by the estimators below.", "A compromise to get a good value. For small datasets the Sturges value will usually be chosen, while larger datasets will usually default to FD. Avoids the overly conservative behaviour of FD and Sturges for small and large datasets respectively. Switchover point is usually \\(a.size \\approx 1000\\).", "The binwidth is proportional to the interquartile range (IQR) and inversely proportional to cube root of a.size. Can be too conservative for small datasets, but is quite good for large datasets. The IQR is very robust to outliers.", "The binwidth is proportional to the standard deviation of the data and inversely proportional to cube root of x.size. Can be too conservative for small datasets, but is quite good for large datasets. The standard deviation is not very robust to outliers. Values are very similar to the Freedman-Diaconis estimator in the absence of outliers.", "The number of bins is only proportional to cube root of a.size. It tends to overestimate the number of bins and it does not take into account data variability.", "The number of bins is the base 2 log of a.size. This estimator assumes normality of data and is too conservative for larger, non-normal datasets. This is the default method in R\u2019s hist method.", "An improved version of Sturges\u2019 formula that produces better estimates for non-normal datasets. This estimator attempts to account for the skew of the data.", "The simplest and fastest estimator. Only takes into account the data size.", "For consistency with histogram, an array of pre-computed bins is passed through unmodified:", "This function allows one set of bins to be computed, and reused across multiple histograms:", "Which gives more easily comparable results than using separate bins for each histogram:"]}, {"name": "numpy.histogramdd()", "path": "reference/generated/numpy.histogramdd", "type": "numpy.histogramdd", "text": ["Compute the multidimensional histogram of some data.", "The data to be histogrammed.", "Note the unusual interpretation of sample when an array_like:", "The first form should be preferred.", "The bin specification:", "A sequence of length D, each an optional (lower, upper) tuple giving the outer bin edges to be used if the edges are not given explicitly in bins. An entry of None in the sequence results in the minimum and maximum values being used for the corresponding dimension. The default, None, is equivalent to passing a tuple of D None values.", "If False, the default, returns the number of samples in each bin. If True, returns the probability density function at the bin, bin_count / sample_count / bin_volume.", "An alias for the density argument that behaves identically. To avoid confusion with the broken normed argument to histogram, density should be preferred.", "An array of values w_i weighing each sample (x_i, y_i, z_i, \u2026). Weights are normalized to 1 if normed is True. If normed is False, the values of the returned histogram are equal to the sum of the weights belonging to the samples falling into each bin.", "The multidimensional histogram of sample x. See normed and weights for the different possible semantics.", "A list of D arrays describing the bin edges for each dimension.", "See also", "1-D histogram", "2-D histogram"]}, {"name": "numpy.hsplit()", "path": "reference/generated/numpy.hsplit", "type": "numpy.hsplit", "text": ["Split an array into multiple sub-arrays horizontally (column-wise).", "Please refer to the split documentation. hsplit is equivalent to split with axis=1, the array is always split along the second axis regardless of the array dimension.", "See also", "Split an array into multiple sub-arrays of equal size.", "With a higher dimensional array the split is still along the second axis."]}, {"name": "numpy.hstack()", "path": "reference/generated/numpy.hstack", "type": "numpy.hstack", "text": ["Stack arrays in sequence horizontally (column wise).", "This is equivalent to concatenation along the second axis, except for 1-D arrays where it concatenates along the first axis. Rebuilds arrays divided by hsplit.", "This function makes most sense for arrays with up to 3 dimensions. For instance, for pixel-data with a height (first axis), width (second axis), and r/g/b channels (third axis). The functions concatenate, stack and block provide more general stacking and concatenation operations.", "The arrays must have the same shape along all but the second axis, except 1-D arrays which can be any length.", "The array formed by stacking the given arrays.", "See also", "Join a sequence of arrays along an existing axis.", "Join a sequence of arrays along a new axis.", "Assemble an nd-array from nested lists of blocks.", "Stack arrays in sequence vertically (row wise).", "Stack arrays in sequence depth wise (along third axis).", "Stack 1-D arrays as columns into a 2-D array.", "Split an array into multiple sub-arrays horizontally (column-wise)."]}, {"name": "numpy.hypot()", "path": "reference/generated/numpy.hypot", "type": "numpy.hypot", "text": ["Given the \u201clegs\u201d of a right triangle, return its hypotenuse.", "Equivalent to sqrt(x1**2 + x2**2), element-wise. If x1 or x2 is scalar_like (i.e., unambiguously cast-able to a scalar type), it is broadcast for use with each element of the other argument. (See Examples)", "Leg of the triangle(s). If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The hypotenuse of the triangle(s). This is a scalar if both x1 and x2 are scalars.", "Example showing broadcast of scalar_like argument:"]}, {"name": "numpy.i0()", "path": "reference/generated/numpy.i0", "type": "numpy.i0", "text": ["Modified Bessel function of the first kind, order 0.", "Usually denoted \\(I_0\\).", "Argument of the Bessel function.", "The modified Bessel function evaluated at each of the elements of x.", "See also", "The scipy implementation is recommended over this function: it is a proper ufunc written in C, and more than an order of magnitude faster.", "We use the algorithm published by Clenshaw [1] and referenced by Abramowitz and Stegun [2], for which the function domain is partitioned into the two intervals [0,8] and (8,inf), and Chebyshev polynomial expansions are employed in each interval. Relative error on the domain [0,30] using IEEE arithmetic is documented [3] as having a peak of 5.8e-16 with an rms of 1.4e-16 (n = 30000).", "C. W. Clenshaw, \u201cChebyshev series for mathematical functions\u201d, in National Physical Laboratory Mathematical Tables, vol. 5, London: Her Majesty\u2019s Stationery Office, 1962.", "M. Abramowitz and I. A. Stegun, Handbook of Mathematical Functions, 10th printing, New York: Dover, 1964, pp. 379. https://personal.math.ubc.ca/~cbm/aands/page_379.htm", "https://metacpan.org/pod/distribution/Math-Cephes/lib/Math/Cephes.pod#i0:-Modified-Bessel-function-of-order-zero"]}, {"name": "numpy.identity()", "path": "reference/generated/numpy.identity", "type": "numpy.identity", "text": ["Return the identity array.", "The identity array is a square array with ones on the main diagonal.", "Number of rows (and columns) in n x n output.", "Data-type of the output. Defaults to float.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "n x n array with its main diagonal set to one, and all other elements 0."]}, {"name": "numpy.iinfo()", "path": "reference/generated/numpy.iinfo", "type": "numpy.iinfo", "text": ["Machine limits for integer types.", "The kind of integer data type to get information about.", "See also", "The equivalent for floating point data types.", "With types:", "With instances:", "The number of bits occupied by the type.", "Minimum value of given dtype.", "Maximum value of given dtype."]}, {"name": "numpy.imag()", "path": "reference/generated/numpy.imag", "type": "numpy.imag", "text": ["Return the imaginary part of the complex argument.", "Input array.", "The imaginary component of the complex argument. If val is real, the type of val is used for the output. If val has complex elements, the returned type is float.", "See also"]}, {"name": "numpy.in1d()", "path": "reference/generated/numpy.in1d", "type": "numpy.in1d", "text": ["Test whether each element of a 1-D array is also present in a second array.", "Returns a boolean array the same length as ar1 that is True where an element of ar1 is in ar2 and False otherwise.", "We recommend using isin instead of in1d for new code.", "Input array.", "The values against which to test each value of ar1.", "If True, the input arrays are both assumed to be unique, which can speed up the calculation. Default is False.", "If True, the values in the returned array are inverted (that is, False where an element of ar1 is in ar2 and True otherwise). Default is False. np.in1d(a, b, invert=True) is equivalent to (but is faster than) np.invert(in1d(a, b)).", "New in version 1.8.0.", "The values ar1[in1d] are in ar2.", "See also", "Version of this function that preserves the shape of ar1.", "Module with a number of other functions for performing set operations on arrays.", "in1d can be considered as an element-wise function version of the python keyword in, for 1-D sequences. in1d(a, b) is roughly equivalent to np.array([item in b for item in a]). However, this idea fails if ar2 is a set, or similar (non-sequence) container: As ar2 is converted to an array, in those cases asarray(ar2) is an object array rather than the expected array of contained values.", "New in version 1.4.0."]}, {"name": "numpy.indices()", "path": "reference/generated/numpy.indices", "type": "numpy.indices", "text": ["Return an array representing the indices of a grid.", "Compute an array where the subarrays contain index values 0, 1, \u2026 varying only along the corresponding axis.", "The shape of the grid.", "Data type of the result.", "Return a sparse representation of the grid instead of a dense representation. Default is False.", "New in version 1.17.", "Returns one array of grid indices, grid.shape = (len(dimensions),) + tuple(dimensions).", "Returns a tuple of arrays, with grid[i].shape = (1, ..., 1, dimensions[i], 1, ..., 1) with dimensions[i] in the ith place", "See also", "The output shape in the dense case is obtained by prepending the number of dimensions in front of the tuple of dimensions, i.e. if dimensions is a tuple (r0, ..., rN-1) of length N, the output shape is (N, r0, ..., rN-1).", "The subarrays grid[k] contains the N-D array of indices along the k-th axis. Explicitly:", "The indices can be used as an index into an array.", "Note that it would be more straightforward in the above example to extract the required elements directly with x[:2, :3].", "If sparse is set to true, the grid will be returned in a sparse representation."]}, {"name": "numpy.Inf", "path": "reference/constants", "type": "Constants", "text": ["NumPy includes several constants:", "IEEE 754 floating point representation of (positive) infinity.", "Use inf because Inf, Infinity, PINF and infty are aliases for inf. For more details, see inf.", "inf", "IEEE 754 floating point representation of (positive) infinity.", "Use inf because Inf, Infinity, PINF and infty are aliases for inf. For more details, see inf.", "inf", "IEEE 754 floating point representation of Not a Number (NaN).", "NaN and NAN are equivalent definitions of nan. Please use nan instead of NAN.", "nan", "IEEE 754 floating point representation of negative infinity.", "A floating point representation of negative infinity.", "isinf : Shows which elements are positive or negative infinity", "isposinf : Shows which elements are positive infinity", "isneginf : Shows which elements are negative infinity", "isnan : Shows which elements are Not a Number", "isfinite : Shows which elements are finite (not one of Not a Number, positive infinity and negative infinity)", "NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic (IEEE 754). This means that Not a Number is not equivalent to infinity. Also that positive infinity is not equivalent to negative infinity. But infinity is equivalent to positive infinity.", "IEEE 754 floating point representation of negative zero.", "A floating point representation of negative zero.", "PZERO : Defines positive zero.", "isinf : Shows which elements are positive or negative infinity.", "isposinf : Shows which elements are positive infinity.", "isneginf : Shows which elements are negative infinity.", "isnan : Shows which elements are Not a Number.", "Not a Number, positive infinity and negative infinity.", "NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic (IEEE 754). Negative zero is considered to be a finite number.", "IEEE 754 floating point representation of Not a Number (NaN).", "NaN and NAN are equivalent definitions of nan. Please use nan instead of NaN.", "nan", "IEEE 754 floating point representation of (positive) infinity.", "Use inf because Inf, Infinity, PINF and infty are aliases for inf. For more details, see inf.", "inf", "IEEE 754 floating point representation of positive zero.", "A floating point representation of positive zero.", "NZERO : Defines negative zero.", "isinf : Shows which elements are positive or negative infinity.", "isposinf : Shows which elements are positive infinity.", "isneginf : Shows which elements are negative infinity.", "isnan : Shows which elements are Not a Number.", "Not a Number, positive infinity and negative infinity.", "NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic (IEEE 754). Positive zero is considered to be a finite number.", "Euler\u2019s constant, base of natural logarithms, Napier\u2019s constant.", "e = 2.71828182845904523536028747135266249775724709369995...", "exp : Exponential function log : Natural logarithm", "https://en.wikipedia.org/wiki/E_%28mathematical_constant%29", "\u03b3 = 0.5772156649015328606065120900824024310421...", "https://en.wikipedia.org/wiki/Euler-Mascheroni_constant", "IEEE 754 floating point representation of (positive) infinity.", "A floating point representation of positive infinity.", "isinf : Shows which elements are positive or negative infinity", "isposinf : Shows which elements are positive infinity", "isneginf : Shows which elements are negative infinity", "isnan : Shows which elements are Not a Number", "isfinite : Shows which elements are finite (not one of Not a Number, positive infinity and negative infinity)", "NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic (IEEE 754). This means that Not a Number is not equivalent to infinity. Also that positive infinity is not equivalent to negative infinity. But infinity is equivalent to positive infinity.", "Inf, Infinity, PINF and infty are aliases for inf.", "IEEE 754 floating point representation of (positive) infinity.", "Use inf because Inf, Infinity, PINF and infty are aliases for inf. For more details, see inf.", "inf", "IEEE 754 floating point representation of Not a Number (NaN).", "y : A floating point representation of Not a Number.", "isnan : Shows which elements are Not a Number.", "isfinite : Shows which elements are finite (not one of Not a Number, positive infinity and negative infinity)", "NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic (IEEE 754). This means that Not a Number is not equivalent to infinity.", "NaN and NAN are aliases of nan.", "A convenient alias for None, useful for indexing arrays.", "Outer product, same as outer(x, y):", "x[newaxis, :] is equivalent to x[newaxis] and x[None]:", "pi = 3.1415926535897932384626433...", "https://en.wikipedia.org/wiki/Pi"]}, {"name": "numpy.inf", "path": "reference/constants#numpy.inf", "type": "Constants", "text": ["IEEE 754 floating point representation of (positive) infinity.", "A floating point representation of positive infinity.", "isinf : Shows which elements are positive or negative infinity", "isposinf : Shows which elements are positive infinity", "isneginf : Shows which elements are negative infinity", "isnan : Shows which elements are Not a Number", "isfinite : Shows which elements are finite (not one of Not a Number, positive infinity and negative infinity)", "NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic (IEEE 754). This means that Not a Number is not equivalent to infinity. Also that positive infinity is not equivalent to negative infinity. But infinity is equivalent to positive infinity.", "Inf, Infinity, PINF and infty are aliases for inf."]}, {"name": "numpy.Infinity", "path": "reference/constants#numpy.Infinity", "type": "Constants", "text": ["IEEE 754 floating point representation of (positive) infinity.", "Use inf because Inf, Infinity, PINF and infty are aliases for inf. For more details, see inf.", "inf"]}, {"name": "numpy.info()", "path": "reference/generated/numpy.info", "type": "numpy.info", "text": ["Get help information for a function, class, or module.", "Input object or name to get information about. If object is a numpy object, its docstring is given. If it is a string, available modules are searched for matching objects. If None, information about info itself is returned.", "Printing width.", "File like object that the output is written to, default is stdout. The object has to be opened in \u2018w\u2019 or \u2018a\u2019 mode.", "Start search at this level.", "See also", "When used interactively with an object, np.info(obj) is equivalent to help(obj) on the Python prompt or obj? on the IPython prompt.", "When using a string for object it is possible to get multiple results."]}, {"name": "numpy.infty", "path": "reference/constants#numpy.infty", "type": "Constants", "text": ["IEEE 754 floating point representation of (positive) infinity.", "Use inf because Inf, Infinity, PINF and infty are aliases for inf. For more details, see inf.", "inf"]}, {"name": "numpy.inner()", "path": "reference/generated/numpy.inner", "type": "numpy.inner", "text": ["Inner product of two arrays.", "Ordinary inner product of vectors for 1-D arrays (without complex conjugation), in higher dimensions a sum product over the last axes.", "If a and b are nonscalar, their last dimensions must match.", "If a and b are both scalars or both 1-D arrays then a scalar is returned; otherwise an array is returned. out.shape = (*a.shape[:-1], *b.shape[:-1])", "If both a and b are nonscalar and their last dimensions have different sizes.", "See also", "Sum products over arbitrary axes.", "Generalised matrix product, using second last dimension of b.", "Einstein summation convention.", "For vectors (1-D arrays) it computes the ordinary inner-product:", "More generally, if ndim(a) = r > 0 and ndim(b) = s > 0:", "or explicitly:", "In addition a or b may be scalars, in which case:", "Ordinary inner product for vectors:", "Some multidimensional examples:", "An example where b is a scalar:"]}, {"name": "numpy.insert()", "path": "reference/generated/numpy.insert", "type": "numpy.insert", "text": ["Insert values along the given axis before the given indices.", "Input array.", "Object that defines the index or indices before which values is inserted.", "New in version 1.8.0.", "Support for multiple insertions when obj is a single scalar or a sequence with one element (similar to calling insert multiple times).", "Values to insert into arr. If the type of values is different from that of arr, values is converted to the type of arr. values should be shaped so that arr[...,obj,...] = values is legal.", "Axis along which to insert values. If axis is None then arr is flattened first.", "A copy of arr with values inserted. Note that insert does not occur in-place: a new array is returned. If axis is None, out is a flattened array.", "See also", "Append elements at the end of an array.", "Join a sequence of arrays along an existing axis.", "Delete elements from an array.", "Note that for higher dimensional inserts obj=0 behaves very different from obj=[0] just like arr[:,0,:] = values is different from arr[:,[0],:] = values.", "Difference between sequence and scalars:"]}, {"name": "numpy.int16", "path": "reference/arrays.scalars#numpy.int16", "type": "Scalars", "text": ["Aliases for the signed integer types (one of numpy.byte, numpy.short, numpy.intc, numpy.int_ and numpy.longlong) with the specified number of bits.", "Compatible with the C99 int8_t, int16_t, int32_t, and int64_t, respectively."]}, {"name": "numpy.int32", "path": "reference/arrays.scalars#numpy.int32", "type": "Scalars", "text": ["Aliases for the signed integer types (one of numpy.byte, numpy.short, numpy.intc, numpy.int_ and numpy.longlong) with the specified number of bits.", "Compatible with the C99 int8_t, int16_t, int32_t, and int64_t, respectively."]}, {"name": "numpy.int64", "path": "reference/arrays.scalars#numpy.int64", "type": "Scalars", "text": ["Aliases for the signed integer types (one of numpy.byte, numpy.short, numpy.intc, numpy.int_ and numpy.longlong) with the specified number of bits.", "Compatible with the C99 int8_t, int16_t, int32_t, and int64_t, respectively."]}, {"name": "numpy.int8", "path": "reference/arrays.scalars#numpy.int8", "type": "Scalars", "text": ["Aliases for the signed integer types (one of numpy.byte, numpy.short, numpy.intc, numpy.int_ and numpy.longlong) with the specified number of bits.", "Compatible with the C99 int8_t, int16_t, int32_t, and int64_t, respectively."]}, {"name": "numpy.int_", "path": "reference/arrays.scalars#numpy.int_", "type": "Scalars", "text": ["Signed integer type, compatible with Python int and C long.", "'l'", "numpy.int64: 64-bit signed integer (-9_223_372_036_854_775_808 to 9_223_372_036_854_775_807).", "numpy.intp: Signed integer large enough to fit pointer, compatible with C intptr_t."]}, {"name": "numpy.intc", "path": "reference/arrays.scalars#numpy.intc", "type": "Scalars", "text": ["Signed integer type, compatible with C int.", "'i'", "numpy.int32: 32-bit signed integer (-2_147_483_648 to 2_147_483_647)."]}, {"name": "numpy.interp()", "path": "reference/generated/numpy.interp", "type": "numpy.interp", "text": ["One-dimensional linear interpolation for monotonically increasing sample points.", "Returns the one-dimensional piecewise linear interpolant to a function with given discrete data points (xp, fp), evaluated at x.", "The x-coordinates at which to evaluate the interpolated values.", "The x-coordinates of the data points, must be increasing if argument period is not specified. Otherwise, xp is internally sorted after normalizing the periodic boundaries with xp = xp % period.", "The y-coordinates of the data points, same length as xp.", "Value to return for x < xp[0], default is fp[0].", "Value to return for x > xp[-1], default is fp[-1].", "A period for the x-coordinates. This parameter allows the proper interpolation of angular x-coordinates. Parameters left and right are ignored if period is specified.", "New in version 1.10.0.", "The interpolated values, same shape as x.", "If xp and fp have different length If xp or fp are not 1-D sequences If period == 0", "Warning", "The x-coordinate sequence is expected to be increasing, but this is not explicitly enforced. However, if the sequence xp is non-increasing, interpolation results are meaningless.", "Note that, since NaN is unsortable, xp also cannot contain NaNs.", "A simple check for xp being strictly increasing is:", "See also", "Plot an interpolant to the sine function:", "Interpolation with periodic x-coordinates:", "Complex interpolation:"]}, {"name": "numpy.intersect1d()", "path": "reference/generated/numpy.intersect1d", "type": "numpy.intersect1d", "text": ["Find the intersection of two arrays.", "Return the sorted, unique values that are in both of the input arrays.", "Input arrays. Will be flattened if not already 1D.", "If True, the input arrays are both assumed to be unique, which can speed up the calculation. If True but ar1 or ar2 are not unique, incorrect results and out-of-bounds indices could result. Default is False.", "If True, the indices which correspond to the intersection of the two arrays are returned. The first instance of a value is used if there are multiple. Default is False.", "New in version 1.15.0.", "Sorted 1D array of common and unique elements.", "The indices of the first occurrences of the common values in ar1. Only provided if return_indices is True.", "The indices of the first occurrences of the common values in ar2. Only provided if return_indices is True.", "See also", "Module with a number of other functions for performing set operations on arrays.", "To intersect more than two arrays, use functools.reduce:", "To return the indices of the values common to the input arrays along with the intersected values:"]}, {"name": "numpy.intp", "path": "reference/arrays.scalars#numpy.intp", "type": "Scalars", "text": ["Alias for the signed integer type (one of numpy.byte, numpy.short, numpy.intc, numpy.int_ and np.longlong) that is the same size as a pointer.", "Compatible with the C intptr_t.", "'p'"]}, {"name": "numpy.invert()", "path": "reference/generated/numpy.invert", "type": "numpy.invert", "text": ["Compute bit-wise inversion, or bit-wise NOT, element-wise.", "Computes the bit-wise NOT of the underlying binary representation of the integers in the input arrays. This ufunc implements the C/Python operator ~.", "For signed integer inputs, the two\u2019s complement is returned. In a two\u2019s-complement system negative numbers are represented by the two\u2019s complement of the absolute value. This is the most common method of representing signed integers on computers [1]. A N-bit two\u2019s-complement system can represent every integer in the range \\(-2^{N-1}\\) to \\(+2^{N-1}-1\\).", "Only integer and boolean types are handled.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Result. This is a scalar if x is a scalar.", "See also", "Return the binary representation of the input number as a string.", "bitwise_not is an alias for invert:", "Wikipedia, \u201cTwo\u2019s complement\u201d, https://en.wikipedia.org/wiki/Two\u2019s_complement", "We\u2019ve seen that 13 is represented by 00001101. The invert or bit-wise NOT of 13 is then:", "The result depends on the bit-width:", "When using signed integer types the result is the two\u2019s complement of the result for the unsigned type:", "Booleans are accepted as well:", "The ~ operator can be used as a shorthand for np.invert on ndarrays."]}, {"name": "numpy.is_busday()", "path": "reference/generated/numpy.is_busday", "type": "numpy.is_busday", "text": ["Calculates which of the given dates are valid days, and which are not.", "New in version 1.7.0.", "The array of dates to process.", "A seven-element array indicating which of Monday through Sunday are valid days. May be specified as a length-seven list or array, like [1,1,1,1,1,0,0]; a length-seven string, like \u20181111100\u2019; or a string like \u201cMon Tue Wed Thu Fri\u201d, made up of 3-character abbreviations for weekdays, optionally separated by white space. Valid abbreviations are: Mon Tue Wed Thu Fri Sat Sun", "An array of dates to consider as invalid dates. They may be specified in any order, and NaT (not-a-time) dates are ignored. This list is saved in a normalized form that is suited for fast calculations of valid days.", "A busdaycalendar object which specifies the valid days. If this parameter is provided, neither weekmask nor holidays may be provided.", "If provided, this array is filled with the result.", "An array with the same shape as dates, containing True for each valid day, and False for each invalid day.", "See also", "An object that specifies a custom set of valid days.", "Applies an offset counted in valid days.", "Counts how many valid days are in a half-open date range."]}, {"name": "numpy.isclose()", "path": "reference/generated/numpy.isclose", "type": "numpy.isclose", "text": ["Returns a boolean array where two arrays are element-wise equal within a tolerance.", "The tolerance values are positive, typically very small numbers. The relative difference (rtol * abs(b)) and the absolute difference atol are added together to compare against the absolute difference between a and b.", "Warning", "The default atol is not appropriate for comparing numbers that are much smaller than one (see Notes).", "Input arrays to compare.", "The relative tolerance parameter (see Notes).", "The absolute tolerance parameter (see Notes).", "Whether to compare NaN\u2019s as equal. If True, NaN\u2019s in a will be considered equal to NaN\u2019s in b in the output array.", "Returns a boolean array of where a and b are equal within the given tolerance. If both a and b are scalars, returns a single boolean value.", "See also", "New in version 1.7.0.", "For finite values, isclose uses the following equation to test whether two floating point values are equivalent.", "absolute(a - b) <= (atol + rtol * absolute(b))", "Unlike the built-in math.isclose, the above equation is not symmetric in a and b \u2013 it assumes b is the reference value \u2013 so that isclose(a, b) might be different from isclose(b, a). Furthermore, the default value of atol is not zero, and is used to determine what small values should be considered close to zero. The default value is appropriate for expected values of order unity: if the expected values are significantly smaller than one, it can result in false positives. atol should be carefully selected for the use case at hand. A zero value for atol will result in False if either a or b is zero.", "isclose is not defined for non-numeric data types. bool is considered a numeric data-type for this purpose."]}, {"name": "numpy.iscomplex()", "path": "reference/generated/numpy.iscomplex", "type": "numpy.iscomplex", "text": ["Returns a bool array, where True if input element is complex.", "What is tested is whether the input has a non-zero imaginary part, not if the input type is complex.", "Input array.", "Output array.", "See also", "Return True if x is a complex type or an array of complex numbers."]}, {"name": "numpy.iscomplexobj()", "path": "reference/generated/numpy.iscomplexobj", "type": "numpy.iscomplexobj", "text": ["Check for a complex type or an array of complex numbers.", "The type of the input is checked, not the value. Even if the input has an imaginary part equal to zero, iscomplexobj evaluates to True.", "The input can be of any type and shape.", "The return value, True if x is of a complex type or has at least one complex element.", "See also"]}, {"name": "numpy.isfinite()", "path": "reference/generated/numpy.isfinite", "type": "numpy.isfinite", "text": ["Test element-wise for finiteness (not infinity and not Not a Number).", "The result is returned as a boolean array.", "Input values.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "True where x is not positive infinity, negative infinity, or NaN; false otherwise. This is a scalar if x is a scalar.", "See also", "Not a Number, positive infinity and negative infinity are considered to be non-finite.", "NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic (IEEE 754). This means that Not a Number is not equivalent to infinity. Also that positive infinity is not equivalent to negative infinity. But infinity is equivalent to positive infinity. Errors result if the second argument is also supplied when x is a scalar input, or if first and second arguments have different shapes."]}, {"name": "numpy.isfortran()", "path": "reference/generated/numpy.isfortran", "type": "numpy.isfortran", "text": ["Check if the array is Fortran contiguous but not C contiguous.", "This function is obsolete and, because of changes due to relaxed stride checking, its return value for the same array may differ for versions of NumPy >= 1.10.0 and previous versions. If you only want to check if an array is Fortran contiguous use a.flags.f_contiguous instead.", "Input array.", "Returns True if the array is Fortran contiguous but not C contiguous.", "np.array allows to specify whether the array is written in C-contiguous order (last index varies the fastest), or FORTRAN-contiguous order in memory (first index varies the fastest).", "The transpose of a C-ordered array is a FORTRAN-ordered array.", "C-ordered arrays evaluate as False even if they are also FORTRAN-ordered."]}, {"name": "numpy.isin()", "path": "reference/generated/numpy.isin", "type": "numpy.isin", "text": ["Calculates element in test_elements, broadcasting over element only. Returns a boolean array of the same shape as element that is True where an element of element is in test_elements and False otherwise.", "Input array.", "The values against which to test each value of element. This argument is flattened if it is an array or array_like. See notes for behavior with non-array-like parameters.", "If True, the input arrays are both assumed to be unique, which can speed up the calculation. Default is False.", "If True, the values in the returned array are inverted, as if calculating element not in test_elements. Default is False. np.isin(a, b, invert=True) is equivalent to (but faster than) np.invert(np.isin(a, b)).", "Has the same shape as element. The values element[isin] are in test_elements.", "See also", "Flattened version of this function.", "Module with a number of other functions for performing set operations on arrays.", "isin is an element-wise function version of the python keyword in. isin(a, b) is roughly equivalent to np.array([item in b for item in a]) if a and b are 1-D sequences.", "element and test_elements are converted to arrays if they are not already. If test_elements is a set (or other non-sequence collection) it will be converted to an object array with one element, rather than an array of the values contained in test_elements. This is a consequence of the array constructor\u2019s way of handling non-sequence collections. Converting the set to a list usually gives the desired behavior.", "New in version 1.13.0.", "The indices of the matched values can be obtained with nonzero:", "The test can also be inverted:", "Because of how array handles sets, the following does not work as expected:", "Casting the set to a list gives the expected result:"]}, {"name": "numpy.isinf()", "path": "reference/generated/numpy.isinf", "type": "numpy.isinf", "text": ["Test element-wise for positive or negative infinity.", "Returns a boolean array of the same shape as x, True where x ==\n+/-inf, otherwise False.", "Input values", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "True where x is positive or negative infinity, false otherwise. This is a scalar if x is a scalar.", "See also", "NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic (IEEE 754).", "Errors result if the second argument is supplied when the first argument is a scalar, or if the first and second arguments have different shapes."]}, {"name": "numpy.isnan()", "path": "reference/generated/numpy.isnan", "type": "numpy.isnan", "text": ["Test element-wise for NaN and return result as a boolean array.", "Input array.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "True where x is NaN, false otherwise. This is a scalar if x is a scalar.", "See also", "NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic (IEEE 754). This means that Not a Number is not equivalent to infinity."]}, {"name": "numpy.isnat()", "path": "reference/generated/numpy.isnat", "type": "numpy.isnat", "text": ["Test element-wise for NaT (not a time) and return result as a boolean array.", "New in version 1.13.0.", "Input array with datetime or timedelta data type.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "True where x is NaT, false otherwise. This is a scalar if x is a scalar.", "See also"]}, {"name": "numpy.isneginf()", "path": "reference/generated/numpy.isneginf", "type": "numpy.isneginf", "text": ["Test element-wise for negative infinity, return result as bool array.", "The input array.", "A location into which the result is stored. If provided, it must have a shape that the input broadcasts to. If not provided or None, a freshly-allocated boolean array is returned.", "A boolean array with the same dimensions as the input. If second argument is not supplied then a numpy boolean array is returned with values True where the corresponding element of the input is negative infinity and values False where the element of the input is not negative infinity.", "If a second argument is supplied the result is stored there. If the type of that array is a numeric type the result is represented as zeros and ones, if the type is boolean then as False and True. The return value out is then a reference to that array.", "See also", "NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic (IEEE 754).", "Errors result if the second argument is also supplied when x is a scalar input, if first and second arguments have different shapes, or if the first argument has complex values."]}, {"name": "numpy.isposinf()", "path": "reference/generated/numpy.isposinf", "type": "numpy.isposinf", "text": ["Test element-wise for positive infinity, return result as bool array.", "The input array.", "A location into which the result is stored. If provided, it must have a shape that the input broadcasts to. If not provided or None, a freshly-allocated boolean array is returned.", "A boolean array with the same dimensions as the input. If second argument is not supplied then a boolean array is returned with values True where the corresponding element of the input is positive infinity and values False where the element of the input is not positive infinity.", "If a second argument is supplied the result is stored there. If the type of that array is a numeric type the result is represented as zeros and ones, if the type is boolean then as False and True. The return value out is then a reference to that array.", "See also", "NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic (IEEE 754).", "Errors result if the second argument is also supplied when x is a scalar input, if first and second arguments have different shapes, or if the first argument has complex values"]}, {"name": "numpy.isreal()", "path": "reference/generated/numpy.isreal", "type": "numpy.isreal", "text": ["Returns a bool array, where True if input element is real.", "If element has complex type with zero complex part, the return value for that element is True.", "Input array.", "Boolean array of same shape as x.", "See also", "Return True if x is not a complex type.", "isreal may behave unexpectedly for string or object arrays (see examples)", "The function does not work on string arrays.", "Returns True for all elements in input array of dtype=object even if any of the elements is complex.", "isreal should not be used with object arrays"]}, {"name": "numpy.isrealobj()", "path": "reference/generated/numpy.isrealobj", "type": "numpy.isrealobj", "text": ["Return True if x is a not complex type or an array of complex numbers.", "The type of the input is checked, not the value. So even if the input has an imaginary part equal to zero, isrealobj evaluates to False if the data type is complex.", "The input can be of any type and shape.", "The return value, False if x is of a complex type.", "See also", "The function is only meant for arrays with numerical values but it accepts all other objects. Since it assumes array input, the return value of other objects may be True."]}, {"name": "numpy.isscalar()", "path": "reference/generated/numpy.isscalar", "type": "numpy.isscalar", "text": ["Returns True if the type of element is a scalar type.", "Input argument, can be of any type and shape.", "True if element is a scalar type, False if it is not.", "See also", "Get the number of dimensions of an array", "If you need a stricter way to identify a numerical scalar, use isinstance(x, numbers.Number), as that returns False for most non-numerical elements such as strings.", "In most cases np.ndim(x) == 0 should be used instead of this function, as that will also return true for 0d arrays. This is how numpy overloads functions in the style of the dx arguments to gradient and the bins argument to histogram. Some key differences:", "x", "isscalar(x)", "np.ndim(x) == 0", "PEP 3141 numeric objects (including builtins)", "True", "True", "builtin string and buffer objects", "True", "True", "other builtin objects, like pathlib.Path, Exception, the result of re.compile", "False", "True", "third-party objects like matplotlib.figure.Figure", "False", "True", "zero-dimensional numpy arrays", "False", "True", "other numpy arrays", "False", "False", "list, tuple, and other sequence objects", "False", "False", "NumPy supports PEP 3141 numbers:"]}, {"name": "numpy.issctype()", "path": "reference/generated/numpy.issctype", "type": "numpy.issctype", "text": ["Determines whether the given object represents a scalar data-type.", "If rep is an instance of a scalar dtype, True is returned. If not, False is returned.", "Boolean result of check whether rep is a scalar dtype.", "See also", "Strings are also a scalar type:"]}, {"name": "numpy.issubclass_()", "path": "reference/generated/numpy.issubclass_", "type": "numpy.issubclass_", "text": ["Determine if a class is a subclass of a second class.", "issubclass_ is equivalent to the Python built-in issubclass, except that it returns False instead of raising a TypeError if one of the arguments is not a class.", "Input class. True is returned if arg1 is a subclass of arg2.", "Input class. If a tuple of classes, True is returned if arg1 is a subclass of any of the tuple elements.", "Whether arg1 is a subclass of arg2 or not.", "See also"]}, {"name": "numpy.issubdtype()", "path": "reference/generated/numpy.issubdtype", "type": "numpy.issubdtype", "text": ["Returns True if first argument is a typecode lower/equal in type hierarchy.", "This is like the builtin issubclass, but for dtypes.", "dtype or object coercible to one", "See also", "Overview of the numpy type hierarchy.", "issubdtype can be used to check the type of arrays:", "Similar types of different sizes are not subdtypes of each other:", "but both are subtypes of floating:", "For convenience, dtype-like objects are allowed too:"]}, {"name": "numpy.issubsctype()", "path": "reference/generated/numpy.issubsctype", "type": "numpy.issubsctype", "text": ["Determine if the first argument is a subclass of the second argument.", "Data-types.", "The result.", "See also"]}, {"name": "numpy.ix_()", "path": "reference/generated/numpy.ix_", "type": "numpy.ix_", "text": ["Construct an open mesh from multiple sequences.", "This function takes N 1-D sequences and returns N outputs with N dimensions each, such that the shape is 1 in all but one dimension and the dimension with the non-unit shape value cycles through all N dimensions.", "Using ix_ one can quickly construct index arrays that will index the cross product. a[np.ix_([1,3],[2,5])] returns the array [[a[1,2] a[1,5]], [a[3,2] a[3,5]]].", "Each sequence should be of integer or boolean type. Boolean sequences will be interpreted as boolean masks for the corresponding dimension (equivalent to passing in np.nonzero(boolean_sequence)).", "N arrays with N dimensions each, with N the number of input sequences. Together these arrays form an open mesh.", "See also"]}, {"name": "numpy.kaiser()", "path": "reference/generated/numpy.kaiser", "type": "numpy.kaiser", "text": ["Return the Kaiser window.", "The Kaiser window is a taper formed by using a Bessel function.", "Number of points in the output window. If zero or less, an empty array is returned.", "Shape parameter for window.", "The window, with the maximum value normalized to one (the value one appears only if the number of samples is odd).", "See also", "The Kaiser window is defined as", "with", "where \\(I_0\\) is the modified zeroth-order Bessel function.", "The Kaiser was named for Jim Kaiser, who discovered a simple approximation to the DPSS window based on Bessel functions. The Kaiser window is a very good approximation to the Digital Prolate Spheroidal Sequence, or Slepian window, which is the transform which maximizes the energy in the main lobe of the window relative to total energy.", "The Kaiser can approximate many other windows by varying the beta parameter.", "beta", "Window shape", "0", "Rectangular", "5", "Similar to a Hamming", "6", "Similar to a Hanning", "8.6", "Similar to a Blackman", "A beta value of 14 is probably a good starting point. Note that as beta gets large, the window narrows, and so the number of samples needs to be large enough to sample the increasingly narrow spike, otherwise NaNs will get returned.", "Most references to the Kaiser window come from the signal processing literature, where it is used as one of many windowing functions for smoothing values. It is also known as an apodization (which means \u201cremoving the foot\u201d, i.e. smoothing discontinuities at the beginning and end of the sampled signal) or tapering function.", "J. F. Kaiser, \u201cDigital Filters\u201d - Ch 7 in \u201cSystems analysis by digital computer\u201d, Editors: F.F. Kuo and J.F. Kaiser, p 218-285. John Wiley and Sons, New York, (1966).", "E.R. Kanasewich, \u201cTime Sequence Analysis in Geophysics\u201d, The University of Alberta Press, 1975, pp. 177-178.", "Wikipedia, \u201cWindow function\u201d, https://en.wikipedia.org/wiki/Window_function", "Plot the window and the frequency response:"]}, {"name": "numpy.kron()", "path": "reference/generated/numpy.kron", "type": "numpy.kron", "text": ["Kronecker product of two arrays.", "Computes the Kronecker product, a composite array made of blocks of the second array scaled by the first.", "See also", "The outer product", "The function assumes that the number of dimensions of a and b are the same, if necessary prepending the smallest with ones. If a.shape = (r0,r1,..,rN) and b.shape = (s0,s1,...,sN), the Kronecker product has shape (r0*s0, r1*s1, ..., rN*SN). The elements are products of elements from a and b, organized explicitly by:", "where:", "In the common 2-D case (N=1), the block structure can be visualized:"]}, {"name": "numpy.lcm()", "path": "reference/generated/numpy.lcm", "type": "numpy.lcm", "text": ["Returns the lowest common multiple of |x1| and |x2|", "Arrays of values. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "The lowest common multiple of the absolute value of the inputs This is a scalar if both x1 and x2 are scalars.", "See also", "The greatest common divisor"]}, {"name": "numpy.ldexp()", "path": "reference/generated/numpy.ldexp", "type": "numpy.ldexp", "text": ["Returns x1 * 2**x2, element-wise.", "The mantissas x1 and twos exponents x2 are used to construct floating point numbers x1 * 2**x2.", "Array of multipliers.", "Array of twos exponents. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The result of x1 * 2**x2. This is a scalar if both x1 and x2 are scalars.", "See also", "Return (y1, y2) from x = y1 * 2**y2, inverse to ldexp.", "Complex dtypes are not supported, they will raise a TypeError.", "ldexp is useful as the inverse of frexp, if used by itself it is more clear to simply use the expression x1 * 2**x2."]}, {"name": "numpy.left_shift()", "path": "reference/generated/numpy.left_shift", "type": "numpy.left_shift", "text": ["Shift the bits of an integer to the left.", "Bits are shifted to the left by appending x2 0s at the right of x1. Since the internal representation of numbers is in binary format, this operation is equivalent to multiplying x1 by 2**x2.", "Input values.", "Number of zeros to append to x1. Has to be non-negative. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Return x1 with bits shifted x2 times to the left. This is a scalar if both x1 and x2 are scalars.", "See also", "Shift the bits of an integer to the right.", "Return the binary representation of the input number as a string.", "Note that the dtype of the second argument may change the dtype of the result and can lead to unexpected results in some cases (see Casting Rules):", "The << operator can be used as a shorthand for np.left_shift on ndarrays."]}, {"name": "numpy.less()", "path": "reference/generated/numpy.less", "type": "numpy.less", "text": ["Return the truth value of (x1 < x2) element-wise.", "Input arrays. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Output array, element-wise comparison of x1 and x2. Typically of type bool, unless dtype=object is passed. This is a scalar if both x1 and x2 are scalars.", "See also", "The < operator can be used as a shorthand for np.less on ndarrays."]}, {"name": "numpy.less_equal()", "path": "reference/generated/numpy.less_equal", "type": "numpy.less_equal", "text": ["Return the truth value of (x1 <= x2) element-wise.", "Input arrays. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Output array, element-wise comparison of x1 and x2. Typically of type bool, unless dtype=object is passed. This is a scalar if both x1 and x2 are scalars.", "See also", "The <= operator can be used as a shorthand for np.less_equal on ndarrays."]}, {"name": "numpy.lexsort()", "path": "reference/generated/numpy.lexsort", "type": "numpy.lexsort", "text": ["Perform an indirect stable sort using a sequence of keys.", "Given multiple sorting keys, which can be interpreted as columns in a spreadsheet, lexsort returns an array of integer indices that describes the sort order by multiple columns. The last key in the sequence is used for the primary sort order, the second-to-last key for the secondary sort order, and so on. The keys argument must be a sequence of objects that can be converted to arrays of the same shape. If a 2D array is provided for the keys argument, its rows are interpreted as the sorting keys and sorting is according to the last row, second last row etc.", "The k different \u201ccolumns\u201d to be sorted. The last column (or row if keys is a 2D array) is the primary sort key.", "Axis to be indirectly sorted. By default, sort over the last axis.", "Array of indices that sort the keys along the specified axis.", "See also", "Indirect sort.", "In-place sort.", "Return a sorted copy of an array.", "Sort names: first by surname, then by name.", "Sort two columns of numbers:", "Note that sorting is first according to the elements of a. Secondary sorting is according to the elements of b.", "A normal argsort would have yielded:", "Structured arrays are sorted lexically by argsort:"]}, {"name": "numpy.lib.arraysetops", "path": "reference/generated/numpy.lib.arraysetops", "type": "numpy.lib.arraysetops", "text": ["Set operations for arrays based on sorting.", "For floating point arrays, inaccurate results may appear due to usual round-off and floating point comparison issues.", "Speed could be gained in some operations by an implementation of numpy.sort, that can provide directly the permutation vectors, thus avoiding calls to numpy.argsort.", "Original author: Robert Cimrman"]}, {"name": "numpy.lib.Arrayterator()", "path": "reference/generated/numpy.lib.arrayterator", "type": "numpy.lib.Arrayterator", "text": ["Buffered iterator for big arrays.", "Arrayterator creates a buffered iterator for reading big arrays in small contiguous blocks. The class is useful for objects stored in the file system. It allows iteration over the object without reading everything in memory; instead, small blocks are read and iterated over.", "Arrayterator can be used with any object that supports multidimensional slices. This includes NumPy arrays, but also variables from Scientific.IO.NetCDF or pynetcdf for example.", "The object to iterate over.", "The buffer size. If buf_size is supplied, the maximum amount of data that will be read into memory is buf_size elements. Default is None, which will read as many element as possible into memory.", "See also", "Multidimensional array iterator.", "Flat array iterator.", "Create a memory-map to an array stored in a binary file on disk.", "The algorithm works by first finding a \u201crunning dimension\u201d, along which the blocks will be extracted. Given an array of dimensions (d1, d2, ..., dn), e.g. if buf_size is smaller than d1, the first dimension will be used. If, on the other hand, d1 < buf_size < d1*d2 the second dimension will be used, and so on. Blocks are extracted along this dimension, and when the last block is returned the process continues from the next dimension, until all elements have been read.", "Now we can iterate over a_itor, and it will return arrays of size two. Since buf_size was smaller than any dimension, the first dimension will be iterated over first:", "The shape of the array to be iterated over.", "A 1-D flat iterator for Arrayterator objects."]}, {"name": "numpy.lib.mixins.NDArrayOperatorsMixin", "path": "reference/generated/numpy.lib.mixins.ndarrayoperatorsmixin", "type": "numpy.lib.mixins.NDArrayOperatorsMixin", "text": ["Mixin defining all operator special methods using __array_ufunc__.", "This class implements the special methods for almost all of Python\u2019s builtin operators defined in the operator module, including comparisons (==, >, etc.) and arithmetic (+, *, -, etc.), by deferring to the __array_ufunc__ method, which subclasses must implement.", "It is useful for writing classes that do not inherit from numpy.ndarray, but that should support arithmetic and numpy universal functions like arrays as described in A Mechanism for Overriding Ufuncs.", "As an trivial example, consider this implementation of an ArrayLike class that simply wraps a NumPy array and ensures that the result of any arithmetic operation is also an ArrayLike object:", "In interactions between ArrayLike objects and numbers or numpy arrays, the result is always another ArrayLike:", "Note that unlike numpy.ndarray, ArrayLike does not allow operations with arbitrary, unrecognized types. This ensures that interactions with ArrayLike preserve a well-defined casting hierarchy.", "New in version 1.13."]}, {"name": "numpy.lib.NumpyVersion()", "path": "reference/generated/numpy.lib.numpyversion", "type": "numpy.lib.NumpyVersion", "text": ["Parse and compare numpy version strings.", "NumPy has the following versioning scheme (numbers given are examples; they can be > 9 in principle):", "\u20181.8.0b2.dev-f1234afa\u2019, \u20181.8.1rc1.dev-f1234afa\u2019, etc.", "Comparing needs to be done against a valid version string or other NumpyVersion instance. Note that all development versions of the same (pre-)release compare equal.", "New in version 1.9.0.", "NumPy version string (np.__version__)."]}, {"name": "numpy.lib.recfunctions.append_fields()", "path": "user/basics.rec", "type": "User Guide", "text": ["Structured arrays are ndarrays whose datatype is a composition of simpler datatypes organized as a sequence of named fields. For example,", "Here x is a one-dimensional array of length two whose datatype is a structure with three fields: 1. A string of length 10 or less named \u2018name\u2019, 2. a 32-bit integer named \u2018age\u2019, and 3. a 32-bit float named \u2018weight\u2019.", "If you index x at position 1 you get a structure:", "You can access and modify individual fields of a structured array by indexing with the field name:", "Structured datatypes are designed to be able to mimic \u2018structs\u2019 in the C language, and share a similar memory layout. They are meant for interfacing with C code and for low-level manipulation of structured buffers, for example for interpreting binary blobs. For these purposes they support specialized features such as subarrays, nested datatypes, and unions, and allow control over the memory layout of the structure.", "Users looking to manipulate tabular data, such as stored in csv files, may find other pydata projects more suitable, such as xarray, pandas, or DataArray. These provide a high-level interface for tabular data analysis and are better optimized for that use. For instance, the C-struct-like memory layout of structured arrays in numpy can lead to poor cache behavior in comparison.", "A structured datatype can be thought of as a sequence of bytes of a certain length (the structure\u2019s itemsize) which is interpreted as a collection of fields. Each field has a name, a datatype, and a byte offset within the structure. The datatype of a field may be any numpy datatype including other structured datatypes, and it may also be a subarray data type which behaves like an ndarray of a specified shape. The offsets of the fields are arbitrary, and fields may even overlap. These offsets are usually determined automatically by numpy, but can also be specified.", "Structured datatypes may be created using the function numpy.dtype. There are 4 alternative forms of specification which vary in flexibility and conciseness. These are further documented in the Data Type Objects reference page, and in summary they are:", "A list of tuples, one tuple per field", "Each tuple has the form (fieldname, datatype, shape) where shape is optional. fieldname is a string (or tuple if titles are used, see Field Titles below), datatype may be any object convertible to a datatype, and shape is a tuple of integers specifying subarray shape.", "If fieldname is the empty string '', the field will be given a default name of the form f#, where # is the integer index of the field, counting from 0 from the left:", "The byte offsets of the fields within the structure and the total structure itemsize are determined automatically.", "A string of comma-separated dtype specifications", "In this shorthand notation any of the string dtype specifications may be used in a string and separated by commas. The itemsize and byte offsets of the fields are determined automatically, and the field names are given the default names f0, f1, etc.", "A dictionary of field parameter arrays", "This is the most flexible form of specification since it allows control over the byte-offsets of the fields and the itemsize of the structure.", "The dictionary has two required keys, \u2018names\u2019 and \u2018formats\u2019, and four optional keys, \u2018offsets\u2019, \u2018itemsize\u2019, \u2018aligned\u2019 and \u2018titles\u2019. The values for \u2018names\u2019 and \u2018formats\u2019 should respectively be a list of field names and a list of dtype specifications, of the same length. The optional \u2018offsets\u2019 value should be a list of integer byte-offsets, one for each field within the structure. If \u2018offsets\u2019 is not given the offsets are determined automatically. The optional \u2018itemsize\u2019 value should be an integer describing the total size in bytes of the dtype, which must be large enough to contain all the fields.", "Offsets may be chosen such that the fields overlap, though this will mean that assigning to one field may clobber any overlapping field\u2019s data. As an exception, fields of numpy.object_ type cannot overlap with other fields, because of the risk of clobbering the internal object pointer and then dereferencing it.", "The optional \u2018aligned\u2019 value can be set to True to make the automatic offset computation use aligned offsets (see Automatic Byte Offsets and Alignment), as if the \u2018align\u2019 keyword argument of numpy.dtype had been set to True.", "The optional \u2018titles\u2019 value should be a list of titles of the same length as \u2018names\u2019, see Field Titles below.", "A dictionary of field names", "The use of this form of specification is discouraged, but documented here because older numpy code may use it. The keys of the dictionary are the field names and the values are tuples specifying type and offset:", "This form is discouraged because Python dictionaries do not preserve order in Python versions before Python 3.6, and the order of the fields in a structured dtype has meaning. Field Titles may be specified by using a 3-tuple, see below.", "The list of field names of a structured datatype can be found in the names attribute of the dtype object:", "The field names may be modified by assigning to the names attribute using a sequence of strings of the same length.", "The dtype object also has a dictionary-like attribute, fields, whose keys are the field names (and Field Titles, see below) and whose values are tuples containing the dtype and byte offset of each field.", "Both the names and fields attributes will equal None for unstructured arrays. The recommended way to test if a dtype is structured is with if dt.names is not None rather than if dt.names, to account for dtypes with 0 fields.", "The string representation of a structured datatype is shown in the \u201clist of tuples\u201d form if possible, otherwise numpy falls back to using the more general dictionary form.", "Numpy uses one of two methods to automatically determine the field byte offsets and the overall itemsize of a structured datatype, depending on whether align=True was specified as a keyword argument to numpy.dtype.", "By default (align=False), numpy will pack the fields together such that each field starts at the byte offset the previous field ended, and the fields are contiguous in memory.", "If align=True is set, numpy will pad the structure in the same way many C compilers would pad a C-struct. Aligned structures can give a performance improvement in some cases, at the cost of increased datatype size. Padding bytes are inserted between fields such that each field\u2019s byte offset will be a multiple of that field\u2019s alignment, which is usually equal to the field\u2019s size in bytes for simple datatypes, see PyArray_Descr.alignment. The structure will also have trailing padding added so that its itemsize is a multiple of the largest field\u2019s alignment.", "Note that although almost all modern C compilers pad in this way by default, padding in C structs is C-implementation-dependent so this memory layout is not guaranteed to exactly match that of a corresponding struct in a C program. Some work may be needed, either on the numpy side or the C side, to obtain exact correspondence.", "If offsets were specified using the optional offsets key in the dictionary-based dtype specification, setting align=True will check that each field\u2019s offset is a multiple of its size and that the itemsize is a multiple of the largest field size, and raise an exception if not.", "If the offsets of the fields and itemsize of a structured array satisfy the alignment conditions, the array will have the ALIGNED flag set.", "A convenience function numpy.lib.recfunctions.repack_fields converts an aligned dtype or array to a packed one and vice versa. It takes either a dtype or structured ndarray as an argument, and returns a copy with fields re-packed, with or without padding bytes.", "In addition to field names, fields may also have an associated title, an alternate name, which is sometimes used as an additional description or alias for the field. The title may be used to index an array, just like a field name.", "To add titles when using the list-of-tuples form of dtype specification, the field name may be specified as a tuple of two strings instead of a single string, which will be the field\u2019s title and field name respectively. For example:", "When using the first form of dictionary-based specification, the titles may be supplied as an extra 'titles' key as described above. When using the second (discouraged) dictionary-based specification, the title can be supplied by providing a 3-element tuple (datatype, offset, title) instead of the usual 2-element tuple:", "The dtype.fields dictionary will contain titles as keys, if any titles are used. This means effectively that a field with a title will be represented twice in the fields dictionary. The tuple values for these fields will also have a third element, the field title. Because of this, and because the names attribute preserves the field order while the fields attribute may not, it is recommended to iterate through the fields of a dtype using the names attribute of the dtype, which will not list titles, as in:", "Structured datatypes are implemented in numpy to have base type numpy.void by default, but it is possible to interpret other numpy types as structured types using the (base_dtype, dtype) form of dtype specification described in Data Type Objects. Here, base_dtype is the desired underlying dtype, and fields and flags will be copied from dtype. This dtype is similar to a \u2018union\u2019 in C.", "There are a number of ways to assign values to a structured array: Using python tuples, using scalar values, or using other structured arrays.", "The simplest way to assign values to a structured array is using python tuples. Each assigned value should be a tuple of length equal to the number of fields in the array, and not a list or array as these will trigger numpy\u2019s broadcasting rules. The tuple\u2019s elements are assigned to the successive fields of the array, from left to right:", "A scalar assigned to a structured element will be assigned to all fields. This happens when a scalar is assigned to a structured array, or when an unstructured array is assigned to a structured array:", "Structured arrays can also be assigned to unstructured arrays, but only if the structured datatype has just a single field:", "Assignment between two structured arrays occurs as if the source elements had been converted to tuples and then assigned to the destination elements. That is, the first field of the source array is assigned to the first field of the destination array, and the second field likewise, and so on, regardless of field names. Structured arrays with a different number of fields cannot be assigned to each other. Bytes of the destination structure which are not included in any of the fields are unaffected.", "When assigning to fields which are subarrays, the assigned value will first be broadcast to the shape of the subarray.", "Individual fields of a structured array may be accessed and modified by indexing the array with the field name.", "The resulting array is a view into the original array. It shares the same memory locations and writing to the view will modify the original array.", "This view has the same dtype and itemsize as the indexed field, so it is typically a non-structured array, except in the case of nested structures.", "If the accessed field is a subarray, the dimensions of the subarray are appended to the shape of the result:", "One can index and assign to a structured array with a multi-field index, where the index is a list of field names.", "Warning", "The behavior of multi-field indexes changed from Numpy 1.15 to Numpy 1.16.", "The result of indexing with a multi-field index is a view into the original array, as follows:", "Assignment to the view modifies the original array. The view\u2019s fields will be in the order they were indexed. Note that unlike for single-field indexing, the dtype of the view has the same itemsize as the original array, and has fields at the same offsets as in the original array, and unindexed fields are merely missing.", "Warning", "In Numpy 1.15, indexing an array with a multi-field index returned a copy of the result above, but with fields packed together in memory as if passed through numpy.lib.recfunctions.repack_fields.", "The new behavior as of Numpy 1.16 leads to extra \u201cpadding\u201d bytes at the location of unindexed fields compared to 1.15. You will need to update any code which depends on the data having a \u201cpacked\u201d layout. For instance code such as:", "will need to be changed. This code has raised a FutureWarning since Numpy 1.12, and similar code has raised FutureWarning since 1.7.", "In 1.16 a number of functions have been introduced in the numpy.lib.recfunctions module to help users account for this change. These are numpy.lib.recfunctions.repack_fields. numpy.lib.recfunctions.structured_to_unstructured, numpy.lib.recfunctions.unstructured_to_structured, numpy.lib.recfunctions.apply_along_fields, numpy.lib.recfunctions.assign_fields_by_name, and numpy.lib.recfunctions.require_fields.", "The function numpy.lib.recfunctions.repack_fields can always be used to reproduce the old behavior, as it will return a packed copy of the structured array. The code above, for example, can be replaced with:", "Furthermore, numpy now provides a new function numpy.lib.recfunctions.structured_to_unstructured which is a safer and more efficient alternative for users who wish to convert structured arrays to unstructured arrays, as the view above is often indeded to do. This function allows safe conversion to an unstructured type taking into account padding, often avoids a copy, and also casts the datatypes as needed, unlike the view. Code such as:", "can be made safer by replacing with:", "Assignment to an array with a multi-field index modifies the original array:", "This obeys the structured array assignment rules described above. For example, this means that one can swap the values of two fields using appropriate multi-field indexes:", "Indexing a single element of a structured array (with an integer index) returns a structured scalar:", "Unlike other numpy scalars, structured scalars are mutable and act like views into the original array, such that modifying the scalar will modify the original array. Structured scalars also support access and assignment by field name:", "Similarly to tuples, structured scalars can also be indexed with an integer:", "Thus, tuples might be thought of as the native Python equivalent to numpy\u2019s structured types, much like native python integers are the equivalent to numpy\u2019s integer types. Structured scalars may be converted to a tuple by calling numpy.ndarray.item:", "In order to prevent clobbering object pointers in fields of object type, numpy currently does not allow views of structured arrays containing objects.", "If the dtypes of two void structured arrays are equal, testing the equality of the arrays will result in a boolean array with the dimensions of the original arrays, with elements set to True where all fields of the corresponding structures are equal. Structured dtypes are equal if the field names, dtypes and titles are the same, ignoring endianness, and the fields are in the same order:", "Currently, if the dtypes of two void structured arrays are not equivalent the comparison fails, returning the scalar value False. This behavior is deprecated as of numpy 1.10 and will raise an error or perform elementwise comparison in the future.", "The < and > operators always return False when comparing void structured arrays, and arithmetic and bitwise operations are not supported.", "As an optional convenience numpy provides an ndarray subclass, numpy.recarray that allows access to fields of structured arrays by attribute instead of only by index. Record arrays use a special datatype, numpy.record, that allows field access by attribute on the structured scalars obtained from the array. The numpy.rec module provides functions for creating recarrays from various objects. Additional helper functions for creating and manipulating structured arrays can be found in numpy.lib.recfunctions.", "The simplest way to create a record array is with numpy.rec.array:", "numpy.rec.array can convert a wide variety of arguments into record arrays, including structured arrays:", "The numpy.rec module provides a number of other convenience functions for creating record arrays, see record array creation routines.", "A record array representation of a structured array can be obtained using the appropriate view:", "For convenience, viewing an ndarray as type numpy.recarray will automatically convert to numpy.record datatype, so the dtype can be left out of the view:", "To get back to a plain ndarray both the dtype and type must be reset. The following view does so, taking into account the unusual case that the recordarr was not a structured type:", "Record array fields accessed by index or by attribute are returned as a record array if the field has a structured type but as a plain ndarray otherwise.", "Note that if a field has the same name as an ndarray attribute, the ndarray attribute takes precedence. Such fields will be inaccessible by attribute but will still be accessible by index.", "Collection of utilities to manipulate structured arrays.", "Most of these functions were initially implemented by John Hunter for matplotlib. They have been rewritten and extended for convenience.", "Add new fields to an existing array.", "The names of the fields are given with the names arguments, the corresponding values with the data arguments. If a single field is appended, names, data and dtypes do not have to be lists but just values.", "Input array to extend.", "String or sequence of strings corresponding to the names of the new fields.", "Array or sequence of arrays storing the fields to add to the base.", "Datatype or sequence of datatypes. If None, the datatypes are estimated from the data.", "Filling value used to pad missing data on the shorter arrays.", "Whether to return a masked array or not.", "Whether to return a recarray (MaskedRecords) or not.", "Apply function \u2018func\u2019 as a reduction across fields of a structured array.", "This is similar to apply_along_axis, but treats the fields of a structured array as an extra axis. The fields are all first cast to a common type following the type-promotion rules from numpy.result_type applied to the field\u2019s dtypes.", "Function to apply on the \u201cfield\u201d dimension. This function must support an axis argument, like np.mean, np.sum, etc.", "Structured array for which to apply func.", "Result of the recution operation", "Assigns values from one structured array to another by field name.", "Normally in numpy >= 1.14, assignment of one structured array to another copies fields \u201cby position\u201d, meaning that the first field from the src is copied to the first field of the dst, and so on, regardless of field name.", "This function instead copies \u201cby field name\u201d, such that fields in the dst are assigned from the identically named field in the src. This applies recursively for nested structures. This is how structure assignment worked in numpy >= 1.6 to <= 1.13.", "The source and destination arrays during assignment.", "If True, fields in the dst for which there was no matching field in the src are filled with the value 0 (zero). This was the behavior of numpy <= 1.13. If False, those fields are not modified.", "Return a new array with fields in drop_names dropped.", "Nested fields are supported.", "Changed in version 1.18.0: drop_fields returns an array with 0 fields if all fields are dropped, rather than returning None as it did previously.", "Input array", "String or sequence of strings corresponding to the names of the fields to drop.", "Whether to return a masked array or not.", "Whether to return a recarray or a mrecarray (asrecarray=True) or a plain ndarray or masked array with flexible dtype. The default is False.", "Find the duplicates in a structured array along a given key", "Input array", "Name of the fields along which to check the duplicates. If None, the search is performed by records", "Whether masked data should be discarded or considered as duplicates.", "Whether to return the indices of the duplicated values.", "Flatten a structured data-type description.", "Returns a dictionary with fields indexing lists of their parent fields.", "This function is used to simplify access to fields nested in other fields.", "Input datatype", "Last processed field name (used internally during recursion).", "Dictionary of parent fields (used interbally during recursion).", "Returns the field names of the input datatype as a tuple.", "Input datatype", "Returns the field names of the input datatype as a tuple. Nested structure are flattened beforehand.", "Input datatype", "Join arrays r1 and r2 on key key.", "The key should be either a string or a sequence of string corresponding to the fields used to join the array. An exception is raised if the key field cannot be found in the two input arrays. Neither r1 nor r2 should have any duplicates along key: the presence of duplicates will make the output quite unreliable. Note that duplicates are not looked for by the algorithm.", "A string or a sequence of strings corresponding to the fields used for comparison.", "Structured arrays.", "If \u2018inner\u2019, returns the elements common to both r1 and r2. If \u2018outer\u2019, returns the common elements as well as the elements of r1 not in r2 and the elements of not in r2. If \u2018leftouter\u2019, returns the common elements and the elements of r1 not in r2.", "String appended to the names of the fields of r1 that are present in r2 but absent of the key.", "String appended to the names of the fields of r2 that are present in r1 but absent of the key.", "Dictionary mapping field names to the corresponding default values.", "Whether to return a MaskedArray (or MaskedRecords is asrecarray==True) or a ndarray.", "Whether to return a recarray (or MaskedRecords if usemask==True) or just a flexible-type ndarray.", "Merge arrays field by field.", "Sequence of arrays", "Filling value used to pad missing data on the shorter arrays.", "Whether to collapse nested fields.", "Whether to return a masked array or not.", "Whether to return a recarray (MaskedRecords) or not.", "Without a mask, the missing value will be filled with something, depending on what its corresponding type:", "Add new fields to an existing array.", "The names of the fields are given with the names arguments, the corresponding values with the data arguments. If a single field is appended, names, data and dtypes do not have to be lists but just values.", "Input array to extend.", "String or sequence of strings corresponding to the names of the new fields.", "Array or sequence of arrays storing the fields to add to the base.", "Datatype or sequence of datatypes. If None, the datatypes are estimated from the data.", "See also", "Returns a new numpy.recarray with fields in drop_names dropped.", "Join arrays r1 and r2 on keys. Alternative to join_by, that always returns a np.recarray.", "See also", "equivalent function", "Fills fields from output with fields from input, with support for nested structures.", "Input array.", "Output array.", "Rename the fields from a flexible-datatype ndarray or recarray.", "Nested fields are supported.", "Input array whose fields must be modified.", "Dictionary mapping old field names to their new version.", "Re-pack the fields of a structured array or dtype in memory.", "The memory layout of structured datatypes allows fields at arbitrary byte offsets. This means the fields can be separated by padding bytes, their offsets can be non-monotonically increasing, and they can overlap.", "This method removes any overlaps and reorders the fields in memory so they have increasing byte offsets, and adds or removes padding bytes depending on the align option, which behaves like the align option to np.dtype.", "If align=False, this method produces a \u201cpacked\u201d memory layout in which each field starts at the byte the previous field ended, and any padding bytes are removed.", "If align=True, this methods produces an \u201caligned\u201d memory layout in which each field\u2019s offset is a multiple of its alignment, and the total itemsize is a multiple of the largest alignment, by adding padding bytes as needed.", "array or dtype for which to repack the fields.", "If true, use an \u201caligned\u201d memory layout, otherwise use a \u201cpacked\u201d layout.", "If True, also repack nested structures.", "Copy of a with fields repacked, or a itself if no repacking was needed.", "Casts a structured array to a new dtype using assignment by field-name.", "This function assigns from the old to the new array by name, so the value of a field in the output array is the value of the field with the same name in the source array. This has the effect of creating a new ndarray containing only the fields \u201crequired\u201d by the required_dtype.", "If a field name in the required_dtype does not exist in the input array, that field is created and set to 0 in the output array.", "array to cast", "datatype for output array", "array with the new dtype, with field values copied from the fields in the input array with the same name", "Superposes arrays fields by fields", "Sequence of input arrays.", "Dictionary mapping field names to the corresponding default values.", "Whether to return a MaskedArray (or MaskedRecords is asrecarray==True) or a ndarray.", "Whether to return a recarray (or MaskedRecords if usemask==True) or just a flexible-type ndarray.", "Whether automatically cast the type of the field to the maximum.", "Converts an n-D structured array into an (n+1)-D unstructured array.", "The new array will have a new last dimension equal in size to the number of field-elements of the input array. If not supplied, the output datatype is determined from the numpy type promotion rules applied to all the field datatypes.", "Nested fields, as well as each element of any subarray fields, all count as a single field-elements.", "Structured array or dtype to convert. Cannot contain object datatype.", "The dtype of the output unstructured array.", "See copy argument to ndarray.astype. If true, always return a copy. If false, and dtype requirements are satisfied, a view is returned.", "See casting argument of ndarray.astype. Controls what kind of data casting may occur.", "Unstructured array with one more dimension.", "Converts an n-D unstructured array into an (n-1)-D structured array.", "The last dimension of the input array is converted into a structure, with number of field-elements equal to the size of the last dimension of the input array. By default all output fields have the input array\u2019s dtype, but an output structured dtype with an equal number of fields-elements can be supplied instead.", "Nested fields, as well as each element of any subarray fields, all count towards the number of field-elements.", "Unstructured array or dtype to convert.", "The structured dtype of the output array", "If dtype is not supplied, this specifies the field names for the output dtype, in order. The field dtypes will be the same as the input array.", "Whether to create an aligned memory layout.", "See copy argument to ndarray.astype. If true, always return a copy. If false, and dtype requirements are satisfied, a view is returned.", "See casting argument of ndarray.astype. Controls what kind of data casting may occur.", "Structured array with fewer dimensions."]}, {"name": "numpy.lib.recfunctions.apply_along_fields()", "path": "user/basics.rec#numpy.lib.recfunctions.apply_along_fields", "type": "User Guide", "text": ["Apply function \u2018func\u2019 as a reduction across fields of a structured array.", "This is similar to apply_along_axis, but treats the fields of a structured array as an extra axis. The fields are all first cast to a common type following the type-promotion rules from numpy.result_type applied to the field\u2019s dtypes.", "Function to apply on the \u201cfield\u201d dimension. This function must support an axis argument, like np.mean, np.sum, etc.", "Structured array for which to apply func.", "Result of the recution operation"]}, {"name": "numpy.lib.recfunctions.assign_fields_by_name()", "path": "user/basics.rec#numpy.lib.recfunctions.assign_fields_by_name", "type": "User Guide", "text": ["Assigns values from one structured array to another by field name.", "Normally in numpy >= 1.14, assignment of one structured array to another copies fields \u201cby position\u201d, meaning that the first field from the src is copied to the first field of the dst, and so on, regardless of field name.", "This function instead copies \u201cby field name\u201d, such that fields in the dst are assigned from the identically named field in the src. This applies recursively for nested structures. This is how structure assignment worked in numpy >= 1.6 to <= 1.13.", "The source and destination arrays during assignment.", "If True, fields in the dst for which there was no matching field in the src are filled with the value 0 (zero). This was the behavior of numpy <= 1.13. If False, those fields are not modified."]}, {"name": "numpy.lib.recfunctions.drop_fields()", "path": "user/basics.rec#numpy.lib.recfunctions.drop_fields", "type": "User Guide", "text": ["Return a new array with fields in drop_names dropped.", "Nested fields are supported.", "Changed in version 1.18.0: drop_fields returns an array with 0 fields if all fields are dropped, rather than returning None as it did previously.", "Input array", "String or sequence of strings corresponding to the names of the fields to drop.", "Whether to return a masked array or not.", "Whether to return a recarray or a mrecarray (asrecarray=True) or a plain ndarray or masked array with flexible dtype. The default is False."]}, {"name": "numpy.lib.recfunctions.find_duplicates()", "path": "user/basics.rec#numpy.lib.recfunctions.find_duplicates", "type": "User Guide", "text": ["Find the duplicates in a structured array along a given key", "Input array", "Name of the fields along which to check the duplicates. If None, the search is performed by records", "Whether masked data should be discarded or considered as duplicates.", "Whether to return the indices of the duplicated values."]}, {"name": "numpy.lib.recfunctions.flatten_descr()", "path": "user/basics.rec#numpy.lib.recfunctions.flatten_descr", "type": "User Guide", "text": ["Flatten a structured data-type description."]}, {"name": "numpy.lib.recfunctions.get_fieldstructure()", "path": "user/basics.rec#numpy.lib.recfunctions.get_fieldstructure", "type": "User Guide", "text": ["Returns a dictionary with fields indexing lists of their parent fields.", "This function is used to simplify access to fields nested in other fields.", "Input datatype", "Last processed field name (used internally during recursion).", "Dictionary of parent fields (used interbally during recursion)."]}, {"name": "numpy.lib.recfunctions.get_names()", "path": "user/basics.rec#numpy.lib.recfunctions.get_names", "type": "User Guide", "text": ["Returns the field names of the input datatype as a tuple.", "Input datatype"]}, {"name": "numpy.lib.recfunctions.get_names_flat()", "path": "user/basics.rec#numpy.lib.recfunctions.get_names_flat", "type": "User Guide", "text": ["Returns the field names of the input datatype as a tuple. Nested structure are flattened beforehand.", "Input datatype"]}, {"name": "numpy.lib.recfunctions.join_by()", "path": "user/basics.rec#numpy.lib.recfunctions.join_by", "type": "User Guide", "text": ["Join arrays r1 and r2 on key key.", "The key should be either a string or a sequence of string corresponding to the fields used to join the array. An exception is raised if the key field cannot be found in the two input arrays. Neither r1 nor r2 should have any duplicates along key: the presence of duplicates will make the output quite unreliable. Note that duplicates are not looked for by the algorithm.", "A string or a sequence of strings corresponding to the fields used for comparison.", "Structured arrays.", "If \u2018inner\u2019, returns the elements common to both r1 and r2. If \u2018outer\u2019, returns the common elements as well as the elements of r1 not in r2 and the elements of not in r2. If \u2018leftouter\u2019, returns the common elements and the elements of r1 not in r2.", "String appended to the names of the fields of r1 that are present in r2 but absent of the key.", "String appended to the names of the fields of r2 that are present in r1 but absent of the key.", "Dictionary mapping field names to the corresponding default values.", "Whether to return a MaskedArray (or MaskedRecords is asrecarray==True) or a ndarray.", "Whether to return a recarray (or MaskedRecords if usemask==True) or just a flexible-type ndarray."]}, {"name": "numpy.lib.recfunctions.merge_arrays()", "path": "user/basics.rec#numpy.lib.recfunctions.merge_arrays", "type": "User Guide", "text": ["Merge arrays field by field.", "Sequence of arrays", "Filling value used to pad missing data on the shorter arrays.", "Whether to collapse nested fields.", "Whether to return a masked array or not.", "Whether to return a recarray (MaskedRecords) or not.", "Without a mask, the missing value will be filled with something, depending on what its corresponding type:"]}, {"name": "numpy.lib.recfunctions.rec_append_fields()", "path": "user/basics.rec#numpy.lib.recfunctions.rec_append_fields", "type": "User Guide", "text": ["Add new fields to an existing array.", "The names of the fields are given with the names arguments, the corresponding values with the data arguments. If a single field is appended, names, data and dtypes do not have to be lists but just values.", "Input array to extend.", "String or sequence of strings corresponding to the names of the new fields.", "Array or sequence of arrays storing the fields to add to the base.", "Datatype or sequence of datatypes. If None, the datatypes are estimated from the data.", "See also"]}, {"name": "numpy.lib.recfunctions.rec_drop_fields()", "path": "user/basics.rec#numpy.lib.recfunctions.rec_drop_fields", "type": "User Guide", "text": ["Returns a new numpy.recarray with fields in drop_names dropped."]}, {"name": "numpy.lib.recfunctions.rec_join()", "path": "user/basics.rec#numpy.lib.recfunctions.rec_join", "type": "User Guide", "text": ["Join arrays r1 and r2 on keys. Alternative to join_by, that always returns a np.recarray.", "See also", "equivalent function"]}, {"name": "numpy.lib.recfunctions.recursive_fill_fields()", "path": "user/basics.rec#numpy.lib.recfunctions.recursive_fill_fields", "type": "User Guide", "text": ["Fills fields from output with fields from input, with support for nested structures.", "Input array.", "Output array."]}, {"name": "numpy.lib.recfunctions.rename_fields()", "path": "user/basics.rec#numpy.lib.recfunctions.rename_fields", "type": "User Guide", "text": ["Rename the fields from a flexible-datatype ndarray or recarray.", "Nested fields are supported.", "Input array whose fields must be modified.", "Dictionary mapping old field names to their new version."]}, {"name": "numpy.lib.recfunctions.repack_fields()", "path": "user/basics.rec#numpy.lib.recfunctions.repack_fields", "type": "User Guide", "text": ["Re-pack the fields of a structured array or dtype in memory.", "The memory layout of structured datatypes allows fields at arbitrary byte offsets. This means the fields can be separated by padding bytes, their offsets can be non-monotonically increasing, and they can overlap.", "This method removes any overlaps and reorders the fields in memory so they have increasing byte offsets, and adds or removes padding bytes depending on the align option, which behaves like the align option to np.dtype.", "If align=False, this method produces a \u201cpacked\u201d memory layout in which each field starts at the byte the previous field ended, and any padding bytes are removed.", "If align=True, this methods produces an \u201caligned\u201d memory layout in which each field\u2019s offset is a multiple of its alignment, and the total itemsize is a multiple of the largest alignment, by adding padding bytes as needed.", "array or dtype for which to repack the fields.", "If true, use an \u201caligned\u201d memory layout, otherwise use a \u201cpacked\u201d layout.", "If True, also repack nested structures.", "Copy of a with fields repacked, or a itself if no repacking was needed."]}, {"name": "numpy.lib.recfunctions.require_fields()", "path": "user/basics.rec#numpy.lib.recfunctions.require_fields", "type": "User Guide", "text": ["Casts a structured array to a new dtype using assignment by field-name.", "This function assigns from the old to the new array by name, so the value of a field in the output array is the value of the field with the same name in the source array. This has the effect of creating a new ndarray containing only the fields \u201crequired\u201d by the required_dtype.", "If a field name in the required_dtype does not exist in the input array, that field is created and set to 0 in the output array.", "array to cast", "datatype for output array", "array with the new dtype, with field values copied from the fields in the input array with the same name"]}, {"name": "numpy.lib.recfunctions.stack_arrays()", "path": "user/basics.rec#numpy.lib.recfunctions.stack_arrays", "type": "User Guide", "text": ["Superposes arrays fields by fields", "Sequence of input arrays.", "Dictionary mapping field names to the corresponding default values.", "Whether to return a MaskedArray (or MaskedRecords is asrecarray==True) or a ndarray.", "Whether to return a recarray (or MaskedRecords if usemask==True) or just a flexible-type ndarray.", "Whether automatically cast the type of the field to the maximum."]}, {"name": "numpy.lib.recfunctions.structured_to_unstructured()", "path": "user/basics.rec#numpy.lib.recfunctions.structured_to_unstructured", "type": "User Guide", "text": ["Converts an n-D structured array into an (n+1)-D unstructured array.", "The new array will have a new last dimension equal in size to the number of field-elements of the input array. If not supplied, the output datatype is determined from the numpy type promotion rules applied to all the field datatypes.", "Nested fields, as well as each element of any subarray fields, all count as a single field-elements.", "Structured array or dtype to convert. Cannot contain object datatype.", "The dtype of the output unstructured array.", "See copy argument to ndarray.astype. If true, always return a copy. If false, and dtype requirements are satisfied, a view is returned.", "See casting argument of ndarray.astype. Controls what kind of data casting may occur.", "Unstructured array with one more dimension."]}, {"name": "numpy.lib.recfunctions.unstructured_to_structured()", "path": "user/basics.rec#numpy.lib.recfunctions.unstructured_to_structured", "type": "User Guide", "text": ["Converts an n-D unstructured array into an (n-1)-D structured array.", "The last dimension of the input array is converted into a structure, with number of field-elements equal to the size of the last dimension of the input array. By default all output fields have the input array\u2019s dtype, but an output structured dtype with an equal number of fields-elements can be supplied instead.", "Nested fields, as well as each element of any subarray fields, all count towards the number of field-elements.", "Unstructured array or dtype to convert.", "The structured dtype of the output array", "If dtype is not supplied, this specifies the field names for the output dtype, in order. The field dtypes will be the same as the input array.", "Whether to create an aligned memory layout.", "See copy argument to ndarray.astype. If true, always return a copy. If false, and dtype requirements are satisfied, a view is returned.", "See casting argument of ndarray.astype. Controls what kind of data casting may occur.", "Structured array with fewer dimensions."]}, {"name": "numpy.lib.user_array.container()", "path": "reference/generated/numpy.lib.user_array.container", "type": "numpy.lib.user_array.container", "text": ["Standard container-class for easy multiple-inheritance.", "copy", "tostring", "byteswap", "astype"]}, {"name": "numpy.linspace()", "path": "reference/generated/numpy.linspace", "type": "numpy.linspace", "text": ["Return evenly spaced numbers over a specified interval.", "Returns num evenly spaced samples, calculated over the interval [start, stop].", "The endpoint of the interval can optionally be excluded.", "Changed in version 1.16.0: Non-scalar start and stop are now supported.", "Changed in version 1.20.0: Values are rounded towards -inf instead of 0 when an integer dtype is specified. The old behavior can still be obtained with np.linspace(start, stop, num).astype(int)", "The starting value of the sequence.", "The end value of the sequence, unless endpoint is set to False. In that case, the sequence consists of all but the last of num + 1 evenly spaced samples, so that stop is excluded. Note that the step size changes when endpoint is False.", "Number of samples to generate. Default is 50. Must be non-negative.", "If True, stop is the last sample. Otherwise, it is not included. Default is True.", "If True, return (samples, step), where step is the spacing between samples.", "The type of the output array. If dtype is not given, the data type is inferred from start and stop. The inferred dtype will never be an integer; float is chosen even if the arguments would produce an array of integers.", "New in version 1.9.0.", "The axis in the result to store the samples. Relevant only if start or stop are array-like. By default (0), the samples will be along a new axis inserted at the beginning. Use -1 to get an axis at the end.", "New in version 1.16.0.", "There are num equally spaced samples in the closed interval [start, stop] or the half-open interval [start, stop) (depending on whether endpoint is True or False).", "Only returned if retstep is True", "Size of spacing between samples.", "See also", "Similar to linspace, but uses a step size (instead of the number of samples).", "Similar to linspace, but with numbers spaced evenly on a log scale (a geometric progression).", "Similar to geomspace, but with the end points specified as logarithms.", "Graphical illustration:"]}, {"name": "numpy.load()", "path": "reference/generated/numpy.load", "type": "numpy.load", "text": ["Load arrays or pickled objects from .npy, .npz or pickled files.", "Warning", "Loading files that contain object arrays uses the pickle module, which is not secure against erroneous or maliciously constructed data. Consider passing allow_pickle=False to load data that is known not to contain object arrays for the safer handling of untrusted sources.", "The file to read. File-like objects must support the seek() and read() methods. Pickled files require that the file-like object support the readline() method as well.", "If not None, then memory-map the file, using the given mode (see numpy.memmap for a detailed description of the modes). A memory-mapped array is kept on disk. However, it can be accessed and sliced like any ndarray. Memory mapping is especially useful for accessing small fragments of large files without reading the entire file into memory.", "Allow loading pickled object arrays stored in npy files. Reasons for disallowing pickles include security, as loading pickled data can execute arbitrary code. If pickles are disallowed, loading object arrays will fail. Default: False", "Changed in version 1.16.3: Made default False in response to CVE-2019-6446.", "Only useful when loading Python 2 generated pickled files on Python 3, which includes npy/npz files containing object arrays. If fix_imports is True, pickle will try to map the old Python 2 names to the new names used in Python 3.", "What encoding to use when reading Python 2 strings. Only useful when loading Python 2 generated pickled files in Python 3, which includes npy/npz files containing object arrays. Values other than \u2018latin1\u2019, \u2018ASCII\u2019, and \u2018bytes\u2019 are not allowed, as they can corrupt numerical data. Default: \u2018ASCII\u2019", "Data stored in the file. For .npz files, the returned instance of NpzFile class must be closed to avoid leaking file descriptors.", "If the input file does not exist or cannot be read.", "If allow_pickle=True, but the file cannot be loaded as a pickle.", "The file contains an object array, but allow_pickle=False given.", "See also", "Create a memory-map to an array stored in a file on disk.", "Create or load a memory-mapped .npy file.", "If the file is a .npz file, the returned value supports the context manager protocol in a similar fashion to the open function:", "The underlying file descriptor is closed when exiting the \u2018with\u2019 block.", "Store data to disk, and load it again:", "Store compressed data to disk, and load it again:", "Mem-map the stored array, and then access the second row directly from disk:"]}, {"name": "numpy.loadtxt()", "path": "reference/generated/numpy.loadtxt", "type": "numpy.loadtxt", "text": ["Load data from a text file.", "Each row in the text file must have the same number of values.", "File, filename, list, or generator to read. If the filename extension is .gz or .bz2, the file is first decompressed. Note that generators must return bytes or strings. The strings in a list or produced by a generator are treated as lines.", "Data-type of the resulting array; default: float. If this is a structured data-type, the resulting array will be 1-dimensional, and each row will be interpreted as an element of the array. In this case, the number of columns used must match the number of fields in the data-type.", "The characters or list of characters used to indicate the start of a comment. None implies no comments. For backwards compatibility, byte strings will be decoded as \u2018latin1\u2019. The default is \u2018#\u2019.", "The string used to separate values. For backwards compatibility, byte strings will be decoded as \u2018latin1\u2019. The default is whitespace.", "A dictionary mapping column number to a function that will parse the column string into the desired value. E.g., if column 0 is a date string: converters = {0: datestr2num}. Converters can also be used to provide a default value for missing data (but see also genfromtxt): converters = {3: lambda s: float(s.strip() or 0)}. Default: None.", "Skip the first skiprows lines, including comments; default: 0.", "Which columns to read, with 0 being the first. For example, usecols = (1,4,5) will extract the 2nd, 5th and 6th columns. The default, None, results in all columns being read.", "Changed in version 1.11.0: When a single column has to be read it is possible to use an integer instead of a tuple. E.g usecols = 3 reads the fourth column the same way as usecols = (3,) would.", "If True, the returned array is transposed, so that arguments may be unpacked using x, y, z = loadtxt(...). When used with a structured data-type, arrays are returned for each field. Default is False.", "The returned array will have at least ndmin dimensions. Otherwise mono-dimensional axes will be squeezed. Legal values: 0 (default), 1 or 2.", "New in version 1.6.0.", "Encoding used to decode the inputfile. Does not apply to input streams. The special value \u2018bytes\u2019 enables backward compatibility workarounds that ensures you receive byte arrays as results if possible and passes \u2018latin1\u2019 encoded strings to converters. Override this value to receive unicode arrays and pass strings as input to converters. If set to None the system default is used. The default value is \u2018bytes\u2019.", "New in version 1.14.0.", "Read max_rows lines of content after skiprows lines. The default is to read all the lines.", "New in version 1.16.0.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "Data read from the text file.", "See also", "Load data with missing values handled as specified.", "reads MATLAB data files", "This function aims to be a fast reader for simply formatted files. The genfromtxt function provides more sophisticated handling of, e.g., lines with missing values.", "New in version 1.10.0.", "The strings produced by the Python float.hex method can be used as input for floats.", "This example shows how converters can be used to convert a field with a trailing minus sign into a negative number."]}, {"name": "numpy.log()", "path": "reference/generated/numpy.log", "type": "numpy.log", "text": ["Natural logarithm, element-wise.", "The natural logarithm log is the inverse of the exponential function, so that log(exp(x)) = x. The natural logarithm is logarithm in base e.", "Input value.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The natural logarithm of x, element-wise. This is a scalar if x is a scalar.", "See also", "Logarithm is a multivalued function: for each x there is an infinite number of z such that exp(z) = x. The convention is to return the z whose imaginary part lies in [-pi, pi].", "For real-valued input data types, log always returns real output. For each value that cannot be expressed as a real number or infinity, it yields nan and sets the invalid floating point error flag.", "For complex-valued input, log is a complex analytical function that has a branch cut [-inf, 0] and is continuous from above on it. log handles the floating-point negative zero as an infinitesimal negative number, conforming to the C99 standard.", "M. Abramowitz and I.A. Stegun, \u201cHandbook of Mathematical Functions\u201d, 10th printing, 1964, pp. 67. https://personal.math.ubc.ca/~cbm/aands/page_67.htm", "Wikipedia, \u201cLogarithm\u201d. https://en.wikipedia.org/wiki/Logarithm"]}, {"name": "numpy.log10()", "path": "reference/generated/numpy.log10", "type": "numpy.log10", "text": ["Return the base 10 logarithm of the input array, element-wise.", "Input values.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The logarithm to the base 10 of x, element-wise. NaNs are returned where x is negative. This is a scalar if x is a scalar.", "See also", "Logarithm is a multivalued function: for each x there is an infinite number of z such that 10**z = x. The convention is to return the z whose imaginary part lies in [-pi, pi].", "For real-valued input data types, log10 always returns real output. For each value that cannot be expressed as a real number or infinity, it yields nan and sets the invalid floating point error flag.", "For complex-valued input, log10 is a complex analytical function that has a branch cut [-inf, 0] and is continuous from above on it. log10 handles the floating-point negative zero as an infinitesimal negative number, conforming to the C99 standard.", "M. Abramowitz and I.A. Stegun, \u201cHandbook of Mathematical Functions\u201d, 10th printing, 1964, pp. 67. https://personal.math.ubc.ca/~cbm/aands/page_67.htm", "Wikipedia, \u201cLogarithm\u201d. https://en.wikipedia.org/wiki/Logarithm"]}, {"name": "numpy.log1p()", "path": "reference/generated/numpy.log1p", "type": "numpy.log1p", "text": ["Return the natural logarithm of one plus the input array, element-wise.", "Calculates log(1 + x).", "Input values.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Natural logarithm of 1 + x, element-wise. This is a scalar if x is a scalar.", "See also", "exp(x) - 1, the inverse of log1p.", "For real-valued input, log1p is accurate also for x so small that 1 + x == 1 in floating-point accuracy.", "Logarithm is a multivalued function: for each x there is an infinite number of z such that exp(z) = 1 + x. The convention is to return the z whose imaginary part lies in [-pi, pi].", "For real-valued input data types, log1p always returns real output. For each value that cannot be expressed as a real number or infinity, it yields nan and sets the invalid floating point error flag.", "For complex-valued input, log1p is a complex analytical function that has a branch cut [-inf, -1] and is continuous from above on it. log1p handles the floating-point negative zero as an infinitesimal negative number, conforming to the C99 standard.", "M. Abramowitz and I.A. Stegun, \u201cHandbook of Mathematical Functions\u201d, 10th printing, 1964, pp. 67. https://personal.math.ubc.ca/~cbm/aands/page_67.htm", "Wikipedia, \u201cLogarithm\u201d. https://en.wikipedia.org/wiki/Logarithm"]}, {"name": "numpy.log2()", "path": "reference/generated/numpy.log2", "type": "numpy.log2", "text": ["Base-2 logarithm of x.", "Input values.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Base-2 logarithm of x. This is a scalar if x is a scalar.", "See also", "New in version 1.3.0.", "Logarithm is a multivalued function: for each x there is an infinite number of z such that 2**z = x. The convention is to return the z whose imaginary part lies in [-pi, pi].", "For real-valued input data types, log2 always returns real output. For each value that cannot be expressed as a real number or infinity, it yields nan and sets the invalid floating point error flag.", "For complex-valued input, log2 is a complex analytical function that has a branch cut [-inf, 0] and is continuous from above on it. log2 handles the floating-point negative zero as an infinitesimal negative number, conforming to the C99 standard."]}, {"name": "numpy.logaddexp()", "path": "reference/generated/numpy.logaddexp", "type": "numpy.logaddexp", "text": ["Logarithm of the sum of exponentiations of the inputs.", "Calculates log(exp(x1) + exp(x2)). This function is useful in statistics where the calculated probabilities of events may be so small as to exceed the range of normal floating point numbers. In such cases the logarithm of the calculated probability is stored. This function allows adding probabilities stored in such a fashion.", "Input values. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Logarithm of exp(x1) + exp(x2). This is a scalar if both x1 and x2 are scalars.", "See also", "Logarithm of the sum of exponentiations of inputs in base 2.", "New in version 1.3.0."]}, {"name": "numpy.logaddexp2()", "path": "reference/generated/numpy.logaddexp2", "type": "numpy.logaddexp2", "text": ["Logarithm of the sum of exponentiations of the inputs in base-2.", "Calculates log2(2**x1 + 2**x2). This function is useful in machine learning when the calculated probabilities of events may be so small as to exceed the range of normal floating point numbers. In such cases the base-2 logarithm of the calculated probability can be used instead. This function allows adding probabilities stored in such a fashion.", "Input values. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Base-2 logarithm of 2**x1 + 2**x2. This is a scalar if both x1 and x2 are scalars.", "See also", "Logarithm of the sum of exponentiations of the inputs.", "New in version 1.3.0."]}, {"name": "numpy.logical_and()", "path": "reference/generated/numpy.logical_and", "type": "numpy.logical_and", "text": ["Compute the truth value of x1 AND x2 element-wise.", "Input arrays. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Boolean result of the logical AND operation applied to the elements of x1 and x2; the shape is determined by broadcasting. This is a scalar if both x1 and x2 are scalars.", "See also", "The & operator can be used as a shorthand for np.logical_and on boolean ndarrays."]}, {"name": "numpy.logical_not()", "path": "reference/generated/numpy.logical_not", "type": "numpy.logical_not", "text": ["Compute the truth value of NOT x element-wise.", "Logical NOT is applied to the elements of x.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Boolean result with the same shape as x of the NOT operation on elements of x. This is a scalar if x is a scalar.", "See also"]}, {"name": "numpy.logical_or()", "path": "reference/generated/numpy.logical_or", "type": "numpy.logical_or", "text": ["Compute the truth value of x1 OR x2 element-wise.", "Logical OR is applied to the elements of x1 and x2. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Boolean result of the logical OR operation applied to the elements of x1 and x2; the shape is determined by broadcasting. This is a scalar if both x1 and x2 are scalars.", "See also", "The | operator can be used as a shorthand for np.logical_or on boolean ndarrays."]}, {"name": "numpy.logical_xor()", "path": "reference/generated/numpy.logical_xor", "type": "numpy.logical_xor", "text": ["Compute the truth value of x1 XOR x2, element-wise.", "Logical XOR is applied to the elements of x1 and x2. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Boolean result of the logical XOR operation applied to the elements of x1 and x2; the shape is determined by broadcasting. This is a scalar if both x1 and x2 are scalars.", "See also", "Simple example showing support of broadcasting"]}, {"name": "numpy.logspace()", "path": "reference/generated/numpy.logspace", "type": "numpy.logspace", "text": ["Return numbers spaced evenly on a log scale.", "In linear space, the sequence starts at base ** start (base to the power of start) and ends with base ** stop (see endpoint below).", "Changed in version 1.16.0: Non-scalar start and stop are now supported.", "base ** start is the starting value of the sequence.", "base ** stop is the final value of the sequence, unless endpoint is False. In that case, num + 1 values are spaced over the interval in log-space, of which all but the last (a sequence of length num) are returned.", "Number of samples to generate. Default is 50.", "If true, stop is the last sample. Otherwise, it is not included. Default is True.", "The base of the log space. The step size between the elements in ln(samples) / ln(base) (or log_base(samples)) is uniform. Default is 10.0.", "The type of the output array. If dtype is not given, the data type is inferred from start and stop. The inferred type will never be an integer; float is chosen even if the arguments would produce an array of integers.", "The axis in the result to store the samples. Relevant only if start or stop are array-like. By default (0), the samples will be along a new axis inserted at the beginning. Use -1 to get an axis at the end.", "New in version 1.16.0.", "num samples, equally spaced on a log scale.", "See also", "Similar to linspace, with the step size specified instead of the number of samples. Note that, when used with a float endpoint, the endpoint may or may not be included.", "Similar to logspace, but with the samples uniformly distributed in linear space, instead of log space.", "Similar to logspace, but with endpoints specified directly.", "Logspace is equivalent to the code", "Graphical illustration:"]}, {"name": "numpy.longcomplex", "path": "reference/arrays.scalars#numpy.longcomplex", "type": "Scalars", "text": ["alias of numpy.clongdouble"]}, {"name": "numpy.longdouble", "path": "reference/arrays.scalars#numpy.longdouble", "type": "Scalars", "text": ["Extended-precision floating-point number type, compatible with C long double but not necessarily with IEEE 754 quadruple-precision.", "'g'", "numpy.longfloat", "numpy.float128: 128-bit extended-precision floating-point number type."]}, {"name": "numpy.longfloat", "path": "reference/arrays.scalars#numpy.longfloat", "type": "Scalars", "text": ["alias of numpy.longdouble"]}, {"name": "numpy.longlong", "path": "reference/arrays.scalars#numpy.longlong", "type": "Scalars", "text": ["Signed integer type, compatible with C long long.", "'q'"]}, {"name": "numpy.lookfor()", "path": "reference/generated/numpy.lookfor", "type": "numpy.lookfor", "text": ["Do a keyword search on docstrings.", "A list of objects that matched the search is displayed, sorted by relevance. All given keywords need to be found in the docstring for it to be returned as a result, but the order does not matter.", "String containing words to look for.", "Name of module(s) whose docstrings to go through.", "Whether to import sub-modules in packages. Default is True.", "Whether to re-generate the docstring cache. Default is False.", "File-like object to write the output to. If omitted, use a pager.", "See also", "Relevance is determined only roughly, by checking if the keywords occur in the function name, at the start of a docstring, etc."]}, {"name": "numpy.ma.masked", "path": "reference/maskedarray.baseclass", "type": "Constants of the \n        \n         numpy.ma\n        \n        module", "text": ["In addition to the MaskedArray class, the numpy.ma module defines several constants.", "The masked constant is a special case of MaskedArray, with a float datatype and a null shape. It is used to test whether a specific entry of a masked array is masked, or to mask one or several entries of a masked array:", "Value indicating that a masked array has no invalid entry. nomask is used internally to speed up computations when the mask is not needed. It is represented internally as np.False_.", "String used in lieu of missing data when a masked array is printed. By default, this string is '--'."]}, {"name": "numpy.ma.masked_array", "path": "reference/generated/numpy.ma.masked_array", "type": "numpy.ma.masked_array", "text": ["alias of numpy.ma.core.MaskedArray"]}, {"name": "numpy.ma.masked_print_options", "path": "reference/maskedarray.baseclass#numpy.ma.masked_print_options", "type": "Constants of the \n        \n         numpy.ma\n        \n        module", "text": ["String used in lieu of missing data when a masked array is printed. By default, this string is '--'."]}, {"name": "numpy.ma.MaskType", "path": "reference/generated/numpy.ma.masktype", "type": "numpy.ma.MaskType", "text": ["alias of numpy.bool_"]}, {"name": "numpy.ma.nomask", "path": "reference/maskedarray.baseclass#numpy.ma.nomask", "type": "Constants of the \n        \n         numpy.ma\n        \n        module", "text": ["Value indicating that a masked array has no invalid entry. nomask is used internally to speed up computations when the mask is not needed. It is represented internally as np.False_."]}, {"name": "numpy.MachAr()", "path": "reference/generated/numpy.machar", "type": "numpy.MachAr", "text": ["Diagnosing machine parameters.", "Function that converts an integer or integer array to a float or float array. Default is float.", "Function that converts a float or float array to an integer or integer array. Default is int.", "Function that converts a float array to float. Default is float. Note that this does not seem to do anything useful in the current implementation.", "Function that converts a single float to a string. Default is lambda v:'%24.16e' %v.", "Title that is printed in the string representation of MachAr.", "See also", "Machine limits for floating point types.", "Machine limits for integer types.", "Press, Teukolsky, Vetterling and Flannery, \u201cNumerical Recipes in C++,\u201d 2nd ed, Cambridge University Press, 2002, p. 31.", "Radix in which numbers are represented.", "Number of base-ibeta digits in the floating point mantissa M.", "Exponent of the smallest (most negative) power of ibeta that, added to 1.0, gives something different from 1.0", "Floating-point number beta**machep (floating point precision)", "Exponent of the smallest power of ibeta that, subtracted from 1.0, gives something different from 1.0.", "Floating-point number beta**negep.", "Number of bits in the exponent (including its sign and bias).", "Smallest (most negative) power of ibeta consistent with there being no leading zeros in the mantissa.", "Floating-point number beta**minexp (the smallest [in magnitude] positive floating point number with full precision).", "Smallest (positive) power of ibeta that causes overflow.", "(1-epsneg) * beta**maxexp (the largest [in magnitude] usable floating value).", "In range(6), information on what kind of rounding is done in addition, and on how underflow is handled.", "Number of \u2018guard digits\u2019 used when truncating the product of two mantissas to fit the representation.", "Same as eps.", "An alias for smallest_normal, kept for backwards compatibility.", "Same as xmax.", "- int(-log10(eps))", "- 10**(-precision)", "The smallest positive floating point number with 1 as leading bit in the mantissa following IEEE-754. Same as xmin.", "The smallest positive floating point number with 0 as leading bit in the mantissa following IEEE-754."]}, {"name": "numpy.mask_indices()", "path": "reference/generated/numpy.mask_indices", "type": "numpy.mask_indices", "text": ["Return the indices to access (n, n) arrays, given a masking function.", "Assume mask_func is a function that, for a square array a of size (n, n) with a possible offset argument k, when called as mask_func(a, k) returns a new array with zeros in certain locations (functions like triu or tril do precisely this). Then this function returns the indices where the non-zero values would be located.", "The returned indices will be valid to access arrays of shape (n, n).", "A function whose call signature is similar to that of triu, tril. That is, mask_func(x, k) returns a boolean array, shaped like x. k is an optional argument to the function.", "An optional argument which is passed through to mask_func. Functions like triu, tril take a second argument that is interpreted as an offset.", "The n arrays of indices corresponding to the locations where mask_func(np.ones((n, n)), k) is True.", "See also", "New in version 1.4.0.", "These are the indices that would allow you to access the upper triangular part of any 3x3 array:", "For example, if a is a 3x3 array:", "An offset can be passed also to the masking function. This gets us the indices starting on the first diagonal right of the main one:", "with which we now extract only three elements:"]}, {"name": "numpy.mat()", "path": "reference/generated/numpy.mat", "type": "numpy.mat", "text": ["Interpret the input as a matrix.", "Unlike matrix, asmatrix does not make a copy if the input is already a matrix or an ndarray. Equivalent to matrix(data, copy=False).", "Input data.", "Data-type of the output matrix.", "data interpreted as a matrix."]}, {"name": "numpy.matmul()", "path": "reference/generated/numpy.matmul", "type": "numpy.matmul", "text": ["Matrix product of two arrays.", "Input arrays, scalars not allowed.", "A location into which the result is stored. If provided, it must have a shape that matches the signature (n,k),(k,m)->(n,m). If not provided or None, a freshly-allocated array is returned.", "For other keyword-only arguments, see the ufunc docs.", "New in version 1.16: Now handles ufunc kwargs", "The matrix product of the inputs. This is a scalar only when both x1, x2 are 1-d vectors.", "If the last dimension of x1 is not the same size as the second-to-last dimension of x2.", "If a scalar value is passed in.", "See also", "Complex-conjugating dot product.", "Sum products over arbitrary axes.", "Einstein summation convention.", "alternative matrix product with different broadcasting rules.", "The behavior depends on the arguments in the following way.", "matmul differs from dot in two important ways:", "Stacks of matrices are broadcast together as if the matrices were elements, respecting the signature (n,k),(k,m)->(n,m):", "The matmul function implements the semantics of the @ operator introduced in Python 3.5 following PEP 465.", "For 2-D arrays it is the matrix product:", "For 2-D mixed with 1-D, the result is the usual.", "Broadcasting is conventional for stacks of arrays", "Vector, vector returns the scalar inner product, but neither argument is complex-conjugated:", "Scalar multiplication raises an error.", "The @ operator can be used as a shorthand for np.matmul on ndarrays.", "New in version 1.10.0."]}, {"name": "numpy.matrix()", "path": "reference/generated/numpy.matrix", "type": "numpy.matrix", "text": ["Note", "It is no longer recommended to use this class, even for linear algebra. Instead use regular arrays. The class may be removed in the future.", "Returns a matrix from an array-like object, or from a string of data. A matrix is a specialized 2-D array that retains its 2-D nature through operations. It has certain special operators, such as * (matrix multiplication) and ** (matrix power).", "If data is a string, it is interpreted as a matrix with commas or spaces separating columns, and semicolons separating rows.", "Data-type of the output matrix.", "If data is already an ndarray, then this flag determines whether the data is copied (the default), or whether a view is constructed.", "See also", "Return self as an ndarray object.", "Return self as a flattened ndarray.", "Returns the (complex) conjugate transpose of self.", "Returns the (multiplicative) inverse of invertible self.", "Returns the transpose of the matrix.", "Base object if memory is from some other object.", "An object to simplify the interaction of the array with the ctypes module.", "Python buffer object pointing to the start of the array\u2019s data.", "Data-type of the array\u2019s elements.", "Information about the memory layout of the array.", "A 1-D iterator over the array.", "The imaginary part of the array.", "Length of one array element in bytes.", "Total bytes consumed by the elements of the array.", "Number of array dimensions.", "The real part of the array.", "Tuple of array dimensions.", "Number of elements in the array.", "Tuple of bytes to step in each dimension when traversing an array.", "all([axis, out])", "Test whether all matrix elements along a given axis evaluate to True.", "any([axis, out])", "Test whether any array element along a given axis evaluates to True.", "argmax([axis, out])", "Indexes of the maximum values along an axis.", "argmin([axis, out])", "Indexes of the minimum values along an axis.", "argpartition(kth[, axis, kind, order])", "Returns the indices that would partition this array.", "argsort([axis, kind, order])", "Returns the indices that would sort this array.", "astype(dtype[, order, casting, subok, copy])", "Copy of the array, cast to a specified type.", "byteswap([inplace])", "Swap the bytes of the array elements", "choose(choices[, out, mode])", "Use an index array to construct a new array from a set of choices.", "clip([min, max, out])", "Return an array whose values are limited to [min, max].", "compress(condition[, axis, out])", "Return selected slices of this array along given axis.", "conj()", "Complex-conjugate all elements.", "conjugate()", "Return the complex conjugate, element-wise.", "copy([order])", "Return a copy of the array.", "cumprod([axis, dtype, out])", "Return the cumulative product of the elements along the given axis.", "cumsum([axis, dtype, out])", "Return the cumulative sum of the elements along the given axis.", "diagonal([offset, axis1, axis2])", "Return specified diagonals.", "dump(file)", "Dump a pickle of the array to the specified file.", "dumps()", "Returns the pickle of the array as a string.", "fill(value)", "Fill the array with a scalar value.", "flatten([order])", "Return a flattened copy of the matrix.", "getA()", "Return self as an ndarray object.", "getA1()", "Return self as a flattened ndarray.", "getH()", "Returns the (complex) conjugate transpose of self.", "getI()", "Returns the (multiplicative) inverse of invertible self.", "getT()", "Returns the transpose of the matrix.", "getfield(dtype[, offset])", "Returns a field of the given array as a certain type.", "item(*args)", "Copy an element of an array to a standard Python scalar and return it.", "itemset(*args)", "Insert scalar into an array (scalar is cast to array's dtype, if possible)", "max([axis, out])", "Return the maximum value along an axis.", "mean([axis, dtype, out])", "Returns the average of the matrix elements along the given axis.", "min([axis, out])", "Return the minimum value along an axis.", "newbyteorder([new_order])", "Return the array with the same data viewed with a different byte order.", "nonzero()", "Return the indices of the elements that are non-zero.", "partition(kth[, axis, kind, order])", "Rearranges the elements in the array in such a way that the value of the element in kth position is in the position it would be in a sorted array.", "prod([axis, dtype, out])", "Return the product of the array elements over the given axis.", "ptp([axis, out])", "Peak-to-peak (maximum - minimum) value along the given axis.", "put(indices, values[, mode])", "Set a.flat[n] = values[n] for all n in indices.", "ravel([order])", "Return a flattened matrix.", "repeat(repeats[, axis])", "Repeat elements of an array.", "reshape(shape[, order])", "Returns an array containing the same data with a new shape.", "resize(new_shape[, refcheck])", "Change shape and size of array in-place.", "round([decimals, out])", "Return a with each element rounded to the given number of decimals.", "searchsorted(v[, side, sorter])", "Find indices where elements of v should be inserted in a to maintain order.", "setfield(val, dtype[, offset])", "Put a value into a specified place in a field defined by a data-type.", "setflags([write, align, uic])", "Set array flags WRITEABLE, ALIGNED, (WRITEBACKIFCOPY and UPDATEIFCOPY), respectively.", "sort([axis, kind, order])", "Sort an array in-place.", "squeeze([axis])", "Return a possibly reshaped matrix.", "std([axis, dtype, out, ddof])", "Return the standard deviation of the array elements along the given axis.", "sum([axis, dtype, out])", "Returns the sum of the matrix elements, along the given axis.", "swapaxes(axis1, axis2)", "Return a view of the array with axis1 and axis2 interchanged.", "take(indices[, axis, out, mode])", "Return an array formed from the elements of a at the given indices.", "tobytes([order])", "Construct Python bytes containing the raw data bytes in the array.", "tofile(fid[, sep, format])", "Write array to a file as text or binary (default).", "tolist()", "Return the matrix as a (possibly nested) list.", "tostring([order])", "A compatibility alias for tobytes, with exactly the same behavior.", "trace([offset, axis1, axis2, dtype, out])", "Return the sum along diagonals of the array.", "transpose(*axes)", "Returns a view of the array with axes transposed.", "var([axis, dtype, out, ddof])", "Returns the variance of the matrix elements, along the given axis.", "view([dtype][, type])", "New view of array with the same data.", "dot"]}, {"name": "numpy.maximum()", "path": "reference/generated/numpy.maximum", "type": "numpy.maximum", "text": ["Element-wise maximum of array elements.", "Compare two arrays and returns a new array containing the element-wise maxima. If one of the elements being compared is a NaN, then that element is returned. If both elements are NaNs then the first is returned. The latter distinction is important for complex NaNs, which are defined as at least one of the real or imaginary parts being a NaN. The net effect is that NaNs are propagated.", "The arrays holding the elements to be compared. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The maximum of x1 and x2, element-wise. This is a scalar if both x1 and x2 are scalars.", "See also", "Element-wise minimum of two arrays, propagates NaNs.", "Element-wise maximum of two arrays, ignores NaNs.", "The maximum value of an array along a given axis, propagates NaNs.", "The maximum value of an array along a given axis, ignores NaNs.", "The maximum is equivalent to np.where(x1 >= x2, x1, x2) when neither x1 nor x2 are nans, but it is faster and does proper broadcasting."]}, {"name": "numpy.maximum_sctype()", "path": "reference/generated/numpy.maximum_sctype", "type": "numpy.maximum_sctype", "text": ["Return the scalar type of highest precision of the same kind as the input.", "The input data type. This can be a dtype object or an object that is convertible to a dtype.", "The highest precision data type of the same kind (dtype.kind) as t.", "See also"]}, {"name": "numpy.may_share_memory()", "path": "reference/generated/numpy.may_share_memory", "type": "numpy.may_share_memory", "text": ["Determine if two arrays might share memory", "A return of True does not necessarily mean that the two arrays share any element. It just means that they might.", "Only the memory bounds of a and b are checked by default.", "Input arrays", "Effort to spend on solving the overlap problem. See shares_memory for details. Default for may_share_memory is to do a bounds check.", "See also"]}, {"name": "numpy.mean()", "path": "reference/generated/numpy.mean", "type": "numpy.mean", "text": ["Compute the arithmetic mean along the specified axis.", "Returns the average of the array elements. The average is taken over the flattened array by default, otherwise over the specified axis. float64 intermediate and return values are used for integer inputs.", "Array containing numbers whose mean is desired. If a is not an array, a conversion is attempted.", "Axis or axes along which the means are computed. The default is to compute the mean of the flattened array.", "New in version 1.7.0.", "If this is a tuple of ints, a mean is performed over multiple axes, instead of a single axis or all the axes as before.", "Type to use in computing the mean. For integer inputs, the default is float64; for floating point inputs, it is the same as the input dtype.", "Alternate output array in which to place the result. The default is None; if provided, it must have the same shape as the expected output, but the type will be cast if necessary. See Output type determination for more details.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.", "If the default value is passed, then keepdims will not be passed through to the mean method of sub-classes of ndarray, however any non-default value will be. If the sub-class\u2019 method does not implement keepdims any exceptions will be raised.", "Elements to include in the mean. See reduce for details.", "New in version 1.20.0.", "If out=None, returns a new array containing the mean values, otherwise a reference to the output array is returned.", "See also", "Weighted average", "The arithmetic mean is the sum of the elements along the axis divided by the number of elements.", "Note that for floating-point input, the mean is computed using the same precision the input has. Depending on the input data, this can cause the results to be inaccurate, especially for float32 (see example below). Specifying a higher-precision accumulator using the dtype keyword can alleviate this issue.", "By default, float16 results are computed using float32 intermediates for extra precision.", "In single precision, mean can be inaccurate:", "Computing the mean in float64 is more accurate:", "Specifying a where argument: >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]]) >>> np.mean(a) 12.0 >>> np.mean(a, where=[[True], [False], [False]]) 9.0"]}, {"name": "numpy.median()", "path": "reference/generated/numpy.median", "type": "numpy.median", "text": ["Compute the median along the specified axis.", "Returns the median of the array elements.", "Input array or object that can be converted to an array.", "Axis or axes along which the medians are computed. The default is to compute the median along a flattened version of the array. A sequence of axes is supported since version 1.9.0.", "Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output, but the type (of the output) will be cast if necessary.", "If True, then allow use of memory of input array a for calculations. The input array will be modified by the call to median. This will save memory when you do not need to preserve the contents of the input array. Treat the input as undefined, but it will probably be fully or partially sorted. Default is False. If overwrite_input is True and a is not already an ndarray, an error will be raised.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original arr.", "New in version 1.9.0.", "A new array holding the result. If the input contains integers or floats smaller than float64, then the output data-type is np.float64. Otherwise, the data-type of the output is the same as that of the input. If out is specified, that array is returned instead.", "See also", "Given a vector V of length N, the median of V is the middle value of a sorted copy of V, V_sorted - i e., V_sorted[(N-1)/2], when N is odd, and the average of the two middle values of V_sorted when N is even."]}, {"name": "numpy.memmap()", "path": "reference/generated/numpy.memmap", "type": "numpy.memmap", "text": ["Create a memory-map to an array stored in a binary file on disk.", "Memory-mapped files are used for accessing small segments of large files on disk, without reading the entire file into memory. NumPy\u2019s memmap\u2019s are array-like objects. This differs from Python\u2019s mmap module, which uses file-like objects.", "This subclass of ndarray has some unpleasant interactions with some operations, because it doesn\u2019t quite fit properly as a subclass. An alternative to using this subclass is to create the mmap object yourself, then create an ndarray with ndarray.__new__ directly, passing the object created in its \u2018buffer=\u2019 parameter.", "This class may at some point be turned into a factory function which returns a view into an mmap buffer.", "Flush the memmap instance to write the changes to the file. Currently there is no API to close the underlying mmap. It is tricky to ensure the resource is actually closed, since it may be shared between different memmap instances.", "The file name or file object to be used as the array data buffer.", "The data-type used to interpret the file contents. Default is uint8.", "The file is opened in this mode:", "\u2018r\u2019", "Open existing file for reading only.", "\u2018r+\u2019", "Open existing file for reading and writing.", "\u2018w+\u2019", "Create or overwrite existing file for reading and writing.", "\u2018c\u2019", "Copy-on-write: assignments affect data in memory, but changes are not saved to disk. The file on disk is read-only.", "Default is \u2018r+\u2019.", "In the file, array data starts at this offset. Since offset is measured in bytes, it should normally be a multiple of the byte-size of dtype. When mode != 'r', even positive offsets beyond end of file are valid; The file will be extended to accommodate the additional data. By default, memmap will start at the beginning of the file, even if filename is a file pointer fp and fp.tell() != 0.", "The desired shape of the array. If mode == 'r' and the number of remaining bytes after offset is not a multiple of the byte-size of dtype, you must specify shape. By default, the returned array will be 1-D with the number of elements determined by file size and data-type.", "Specify the order of the ndarray memory layout: row-major, C-style or column-major, Fortran-style. This only has an effect if the shape is greater than 1-D. The default order is \u2018C\u2019.", "See also", "Create or load a memory-mapped .npy file.", "The memmap object can be used anywhere an ndarray is accepted. Given a memmap fp, isinstance(fp, numpy.ndarray) returns True.", "Memory-mapped files cannot be larger than 2GB on 32-bit systems.", "When a memmap causes a file to be created or extended beyond its current size in the filesystem, the contents of the new part are unspecified. On systems with POSIX filesystem semantics, the extended part will be filled with zero bytes.", "This example uses a temporary file so that doctest doesn\u2019t write files to your directory. You would use a \u2018normal\u2019 filename.", "Create a memmap with dtype and shape that matches our data:", "Write data to memmap array:", "Flushes memory changes to disk in order to read them back", "Load the memmap and verify data was stored:", "Read-only memmap:", "Copy-on-write memmap:", "It\u2019s possible to assign to copy-on-write array, but values are only written into the memory copy of the array, and not written to disk:", "File on disk is unchanged:", "Offset into a memmap:", "Path to the mapped file.", "Offset position in the file.", "File mode.", "flush()", "Write any changes in the array to the file on disk."]}, {"name": "numpy.meshgrid()", "path": "reference/generated/numpy.meshgrid", "type": "numpy.meshgrid", "text": ["Return coordinate matrices from coordinate vectors.", "Make N-D coordinate arrays for vectorized evaluations of N-D scalar/vector fields over N-D grids, given one-dimensional coordinate arrays x1, x2,\u2026, xn.", "Changed in version 1.9: 1-D and 0-D cases are allowed.", "1-D arrays representing the coordinates of a grid.", "Cartesian (\u2018xy\u2019, default) or matrix (\u2018ij\u2019) indexing of output. See Notes for more details.", "New in version 1.7.0.", "If True the shape of the returned coordinate array for dimension i is reduced from (N1, ..., Ni, ... Nn) to (1, ..., 1, Ni, 1, ..., 1). These sparse coordinate grids are intended to be use with Broadcasting. When all coordinates are used in an expression, broadcasting still leads to a fully-dimensonal result array.", "Default is False.", "New in version 1.7.0.", "If False, a view into the original arrays are returned in order to conserve memory. Default is True. Please note that sparse=False, copy=False will likely return non-contiguous arrays. Furthermore, more than one element of a broadcast array may refer to a single memory location. If you need to write to the arrays, make copies first.", "New in version 1.7.0.", "For vectors x1, x2,\u2026, \u2018xn\u2019 with lengths Ni=len(xi) , return (N1, N2, N3,...Nn) shaped arrays if indexing=\u2019ij\u2019 or (N2, N1, N3,...Nn) shaped arrays if indexing=\u2019xy\u2019 with the elements of xi repeated to fill the matrix along the first dimension for x1, the second for x2 and so on.", "See also", "Construct a multi-dimensional \u201cmeshgrid\u201d using indexing notation.", "Construct an open multi-dimensional \u201cmeshgrid\u201d using indexing notation.", "This function supports both indexing conventions through the indexing keyword argument. Giving the string \u2018ij\u2019 returns a meshgrid with matrix indexing, while \u2018xy\u2019 returns a meshgrid with Cartesian indexing. In the 2-D case with inputs of length M and N, the outputs are of shape (N, M) for \u2018xy\u2019 indexing and (M, N) for \u2018ij\u2019 indexing. In the 3-D case with inputs of length M, N and P, outputs are of shape (N, M, P) for \u2018xy\u2019 indexing and (M, N, P) for \u2018ij\u2019 indexing. The difference is illustrated by the following code snippet:", "In the 1-D and 0-D case, the indexing and sparse keywords have no effect.", "meshgrid is very useful to evaluate functions on a grid. If the function depends on all coordinates, you can use the parameter sparse=True to save memory and computation time."]}, {"name": "numpy.mgrid", "path": "reference/generated/numpy.mgrid", "type": "numpy.mgrid", "text": ["nd_grid instance which returns a dense multi-dimensional \u201cmeshgrid\u201d.", "An instance of numpy.lib.index_tricks.nd_grid which returns an dense (or fleshed out) mesh-grid when indexed, so that each returned argument has the same shape. The dimensions and number of the output arrays are equal to the number of indexing dimensions. If the step length is not a complex number, then the stop is not inclusive.", "However, if the step length is a complex number (e.g. 5j), then the integer part of its magnitude is interpreted as specifying the number of points to create between the start and stop values, where the stop value is inclusive.", "See also", "class of ogrid and mgrid objects", "like mgrid but returns open (not fleshed out) mesh grids", "array concatenator"]}, {"name": "numpy.min_scalar_type()", "path": "reference/generated/numpy.min_scalar_type", "type": "numpy.min_scalar_type", "text": ["For scalar a, returns the data type with the smallest size and smallest scalar kind which can hold its value. For non-scalar array a, returns the vector\u2019s dtype unmodified.", "Floating point values are not demoted to integers, and complex values are not demoted to floats.", "The value whose minimal data type is to be found.", "The minimal data type.", "See also", "New in version 1.6.0."]}, {"name": "numpy.minimum()", "path": "reference/generated/numpy.minimum", "type": "numpy.minimum", "text": ["Element-wise minimum of array elements.", "Compare two arrays and returns a new array containing the element-wise minima. If one of the elements being compared is a NaN, then that element is returned. If both elements are NaNs then the first is returned. The latter distinction is important for complex NaNs, which are defined as at least one of the real or imaginary parts being a NaN. The net effect is that NaNs are propagated.", "The arrays holding the elements to be compared. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The minimum of x1 and x2, element-wise. This is a scalar if both x1 and x2 are scalars.", "See also", "Element-wise maximum of two arrays, propagates NaNs.", "Element-wise minimum of two arrays, ignores NaNs.", "The minimum value of an array along a given axis, propagates NaNs.", "The minimum value of an array along a given axis, ignores NaNs.", "The minimum is equivalent to np.where(x1 <= x2, x1, x2) when neither x1 nor x2 are NaNs, but it is faster and does proper broadcasting."]}, {"name": "numpy.mintypecode()", "path": "reference/generated/numpy.mintypecode", "type": "numpy.mintypecode", "text": ["Return the character for the minimum-size type to which given types can be safely cast.", "The returned type character must represent the smallest size dtype such that an array of the returned type can handle the data from an array of all types in typechars (or if typechars is an array, then its dtype.char).", "If a list of strings, each string should represent a dtype. If array_like, the character representation of the array dtype is used.", "The set of characters that the returned character is chosen from. The default set is \u2018GDFgdf\u2019.", "The default character, this is returned if none of the characters in typechars matches a character in typeset.", "The character representing the minimum-size type that was found.", "See also"]}, {"name": "numpy.mod()", "path": "reference/generated/numpy.mod", "type": "numpy.mod", "text": ["Returns the element-wise remainder of division.", "Computes the remainder complementary to the floor_divide function. It is equivalent to the Python modulus operator``x1 % x2`` and has the same sign as the divisor x2. The MATLAB function equivalent to np.remainder is mod.", "Warning", "This should not be confused with:", "Dividend array.", "Divisor array. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The element-wise remainder of the quotient floor_divide(x1, x2). This is a scalar if both x1 and x2 are scalars.", "See also", "Equivalent of Python // operator.", "Simultaneous floor division and remainder.", "Equivalent of the MATLAB rem function.", "Returns 0 when x2 is 0 and both x1 and x2 are (arrays of) integers. mod is an alias of remainder.", "The % operator can be used as a shorthand for np.remainder on ndarrays."]}, {"name": "numpy.modf()", "path": "reference/generated/numpy.modf", "type": "numpy.modf", "text": ["Return the fractional and integral parts of an array, element-wise.", "The fractional and integral parts are negative if the given number is negative.", "Input array.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Fractional part of x. This is a scalar if x is a scalar.", "Integral part of x. This is a scalar if x is a scalar.", "See also", "divmod(x, 1) is equivalent to modf with the return values switched, except it always has a positive remainder.", "For integer input the return values are floats."]}, {"name": "numpy.moveaxis()", "path": "reference/generated/numpy.moveaxis", "type": "numpy.moveaxis", "text": ["Move axes of an array to new positions.", "Other axes remain in their original order.", "New in version 1.11.0.", "The array whose axes should be reordered.", "Original positions of the axes to move. These must be unique.", "Destination positions for each of the original axes. These must also be unique.", "Array with moved axes. This array is a view of the input array.", "See also", "Permute the dimensions of an array.", "Interchange two axes of an array.", "These all achieve the same result:"]}, {"name": "numpy.msort()", "path": "reference/generated/numpy.msort", "type": "numpy.msort", "text": ["Return a copy of an array sorted along the first axis.", "Array to be sorted.", "Array of the same type and shape as a.", "See also", "np.msort(a) is equivalent to np.sort(a, axis=0)."]}, {"name": "numpy.multiply()", "path": "reference/generated/numpy.multiply", "type": "numpy.multiply", "text": ["Multiply arguments element-wise.", "Input arrays to be multiplied. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The product of x1 and x2, element-wise. This is a scalar if both x1 and x2 are scalars.", "Equivalent to x1 * x2 in terms of array broadcasting.", "The * operator can be used as a shorthand for np.multiply on ndarrays."]}, {"name": "numpy.NAN", "path": "reference/constants#numpy.NAN", "type": "Constants", "text": ["IEEE 754 floating point representation of Not a Number (NaN).", "NaN and NAN are equivalent definitions of nan. Please use nan instead of NAN.", "nan"]}, {"name": "numpy.NaN", "path": "reference/constants#numpy.NaN", "type": "Constants", "text": ["IEEE 754 floating point representation of Not a Number (NaN).", "NaN and NAN are equivalent definitions of nan. Please use nan instead of NaN.", "nan"]}, {"name": "numpy.nan", "path": "reference/constants#numpy.nan", "type": "Constants", "text": ["IEEE 754 floating point representation of Not a Number (NaN).", "y : A floating point representation of Not a Number.", "isnan : Shows which elements are Not a Number.", "isfinite : Shows which elements are finite (not one of Not a Number, positive infinity and negative infinity)", "NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic (IEEE 754). This means that Not a Number is not equivalent to infinity.", "NaN and NAN are aliases of nan."]}, {"name": "numpy.nan_to_num()", "path": "reference/generated/numpy.nan_to_num", "type": "numpy.nan_to_num", "text": ["Replace NaN with zero and infinity with large finite numbers (default behaviour) or with the numbers defined by the user using the nan, posinf and/or neginf keywords.", "If x is inexact, NaN is replaced by zero or by the user defined value in nan keyword, infinity is replaced by the largest finite floating point values representable by x.dtype or by the user defined value in posinf keyword and -infinity is replaced by the most negative finite floating point values representable by x.dtype or by the user defined value in neginf keyword.", "For complex dtypes, the above is applied to each of the real and imaginary components of x separately.", "If x is not inexact, then no replacements are made.", "Input data.", "Whether to create a copy of x (True) or to replace values in-place (False). The in-place operation only occurs if casting to an array does not require a copy. Default is True.", "New in version 1.13.", "Value to be used to fill NaN values. If no value is passed then NaN values will be replaced with 0.0.", "New in version 1.17.", "Value to be used to fill positive infinity values. If no value is passed then positive infinity values will be replaced with a very large number.", "New in version 1.17.", "Value to be used to fill negative infinity values. If no value is passed then negative infinity values will be replaced with a very small (or negative) number.", "New in version 1.17.", "x, with the non-finite values replaced. If copy is False, this may be x itself.", "See also", "Shows which elements are positive or negative infinity.", "Shows which elements are negative infinity.", "Shows which elements are positive infinity.", "Shows which elements are Not a Number (NaN).", "Shows which elements are finite (not NaN, not infinity)", "NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic (IEEE 754). This means that Not a Number is not equivalent to infinity."]}, {"name": "numpy.nanargmax()", "path": "reference/generated/numpy.nanargmax", "type": "numpy.nanargmax", "text": ["Return the indices of the maximum values in the specified axis ignoring NaNs. For all-NaN slices ValueError is raised. Warning: the results cannot be trusted if a slice contains only NaNs and -Infs.", "Input data.", "Axis along which to operate. By default flattened input is used.", "If provided, the result will be inserted into this array. It should be of the appropriate shape and dtype.", "New in version 1.22.0.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the array.", "New in version 1.22.0.", "An array of indices or a single index value.", "See also"]}, {"name": "numpy.nanargmin()", "path": "reference/generated/numpy.nanargmin", "type": "numpy.nanargmin", "text": ["Return the indices of the minimum values in the specified axis ignoring NaNs. For all-NaN slices ValueError is raised. Warning: the results cannot be trusted if a slice contains only NaNs and Infs.", "Input data.", "Axis along which to operate. By default flattened input is used.", "If provided, the result will be inserted into this array. It should be of the appropriate shape and dtype.", "New in version 1.22.0.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the array.", "New in version 1.22.0.", "An array of indices or a single index value.", "See also"]}, {"name": "numpy.nancumprod()", "path": "reference/generated/numpy.nancumprod", "type": "numpy.nancumprod", "text": ["Return the cumulative product of array elements over a given axis treating Not a Numbers (NaNs) as one. The cumulative product does not change when NaNs are encountered and leading NaNs are replaced by ones.", "Ones are returned for slices that are all-NaN or empty.", "New in version 1.12.0.", "Input array.", "Axis along which the cumulative product is computed. By default the input is flattened.", "Type of the returned array, as well as of the accumulator in which the elements are multiplied. If dtype is not specified, it defaults to the dtype of a, unless a has an integer dtype with a precision less than that of the default platform integer. In that case, the default platform integer is used instead.", "Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output but the type of the resulting values will be cast if necessary.", "A new array holding the result is returned unless out is specified, in which case it is returned.", "See also", "Cumulative product across array propagating NaNs.", "Show which elements are NaN."]}, {"name": "numpy.nancumsum()", "path": "reference/generated/numpy.nancumsum", "type": "numpy.nancumsum", "text": ["Return the cumulative sum of array elements over a given axis treating Not a Numbers (NaNs) as zero. The cumulative sum does not change when NaNs are encountered and leading NaNs are replaced by zeros.", "Zeros are returned for slices that are all-NaN or empty.", "New in version 1.12.0.", "Input array.", "Axis along which the cumulative sum is computed. The default (None) is to compute the cumsum over the flattened array.", "Type of the returned array and of the accumulator in which the elements are summed. If dtype is not specified, it defaults to the dtype of a, unless a has an integer dtype with a precision less than that of the default platform integer. In that case, the default platform integer is used.", "Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output but the type will be cast if necessary. See Output type determination for more details.", "A new array holding the result is returned unless out is specified, in which it is returned. The result has the same size as a, and the same shape as a if axis is not None or a is a 1-d array.", "See also", "Cumulative sum across array propagating NaNs.", "Show which elements are NaN."]}, {"name": "numpy.nanmax()", "path": "reference/generated/numpy.nanmax", "type": "numpy.nanmax", "text": ["Return the maximum of an array or maximum along an axis, ignoring any NaNs. When all-NaN slices are encountered a RuntimeWarning is raised and NaN is returned for that slice.", "Array containing numbers whose maximum is desired. If a is not an array, a conversion is attempted.", "Axis or axes along which the maximum is computed. The default is to compute the maximum of the flattened array.", "Alternate output array in which to place the result. The default is None; if provided, it must have the same shape as the expected output, but the type will be cast if necessary. See Output type determination for more details.", "New in version 1.8.0.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original a.", "If the value is anything but the default, then keepdims will be passed through to the max method of sub-classes of ndarray. If the sub-classes methods does not implement keepdims any exceptions will be raised.", "New in version 1.8.0.", "The minimum value of an output element. Must be present to allow computation on empty slice. See reduce for details.", "New in version 1.22.0.", "Elements to compare for the maximum. See reduce for details.", "New in version 1.22.0.", "An array with the same shape as a, with the specified axis removed. If a is a 0-d array, or if axis is None, an ndarray scalar is returned. The same dtype as a is returned.", "See also", "The minimum value of an array along a given axis, ignoring any NaNs.", "The maximum value of an array along a given axis, propagating any NaNs.", "Element-wise maximum of two arrays, ignoring any NaNs.", "Element-wise maximum of two arrays, propagating any NaNs.", "Shows which elements are Not a Number (NaN).", "Shows which elements are neither NaN nor infinity.", "NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic (IEEE 754). This means that Not a Number is not equivalent to infinity. Positive infinity is treated as a very large number and negative infinity is treated as a very small (i.e. negative) number.", "If the input has a integer type the function is equivalent to np.max.", "When positive infinity and negative infinity are present:"]}, {"name": "numpy.nanmean()", "path": "reference/generated/numpy.nanmean", "type": "numpy.nanmean", "text": ["Compute the arithmetic mean along the specified axis, ignoring NaNs.", "Returns the average of the array elements. The average is taken over the flattened array by default, otherwise over the specified axis. float64 intermediate and return values are used for integer inputs.", "For all-NaN slices, NaN is returned and a RuntimeWarning is raised.", "New in version 1.8.0.", "Array containing numbers whose mean is desired. If a is not an array, a conversion is attempted.", "Axis or axes along which the means are computed. The default is to compute the mean of the flattened array.", "Type to use in computing the mean. For integer inputs, the default is float64; for inexact inputs, it is the same as the input dtype.", "Alternate output array in which to place the result. The default is None; if provided, it must have the same shape as the expected output, but the type will be cast if necessary. See Output type determination for more details.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original a.", "If the value is anything but the default, then keepdims will be passed through to the mean or sum methods of sub-classes of ndarray. If the sub-classes methods does not implement keepdims any exceptions will be raised.", "Elements to include in the mean. See reduce for details.", "New in version 1.22.0.", "If out=None, returns a new array containing the mean values, otherwise a reference to the output array is returned. Nan is returned for slices that contain only NaNs.", "See also", "Weighted average", "Arithmetic mean taken while not ignoring NaNs", "The arithmetic mean is the sum of the non-NaN elements along the axis divided by the number of non-NaN elements.", "Note that for floating-point input, the mean is computed using the same precision the input has. Depending on the input data, this can cause the results to be inaccurate, especially for float32. Specifying a higher-precision accumulator using the dtype keyword can alleviate this issue."]}, {"name": "numpy.nanmedian()", "path": "reference/generated/numpy.nanmedian", "type": "numpy.nanmedian", "text": ["Compute the median along the specified axis, while ignoring NaNs.", "Returns the median of the array elements.", "New in version 1.9.0.", "Input array or object that can be converted to an array.", "Axis or axes along which the medians are computed. The default is to compute the median along a flattened version of the array. A sequence of axes is supported since version 1.9.0.", "Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output, but the type (of the output) will be cast if necessary.", "If True, then allow use of memory of input array a for calculations. The input array will be modified by the call to median. This will save memory when you do not need to preserve the contents of the input array. Treat the input as undefined, but it will probably be fully or partially sorted. Default is False. If overwrite_input is True and a is not already an ndarray, an error will be raised.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original a.", "If this is anything but the default value it will be passed through (in the special case of an empty array) to the mean function of the underlying array. If the array is a sub-class and mean does not have the kwarg keepdims this will raise a RuntimeError.", "A new array holding the result. If the input contains integers or floats smaller than float64, then the output data-type is np.float64. Otherwise, the data-type of the output is the same as that of the input. If out is specified, that array is returned instead.", "See also", "Given a vector V of length N, the median of V is the middle value of a sorted copy of V, V_sorted - i.e., V_sorted[(N-1)/2], when N is odd and the average of the two middle values of V_sorted when N is even."]}, {"name": "numpy.nanmin()", "path": "reference/generated/numpy.nanmin", "type": "numpy.nanmin", "text": ["Return minimum of an array or minimum along an axis, ignoring any NaNs. When all-NaN slices are encountered a RuntimeWarning is raised and Nan is returned for that slice.", "Array containing numbers whose minimum is desired. If a is not an array, a conversion is attempted.", "Axis or axes along which the minimum is computed. The default is to compute the minimum of the flattened array.", "Alternate output array in which to place the result. The default is None; if provided, it must have the same shape as the expected output, but the type will be cast if necessary. See Output type determination for more details.", "New in version 1.8.0.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original a.", "If the value is anything but the default, then keepdims will be passed through to the min method of sub-classes of ndarray. If the sub-classes methods does not implement keepdims any exceptions will be raised.", "New in version 1.8.0.", "The maximum value of an output element. Must be present to allow computation on empty slice. See reduce for details.", "New in version 1.22.0.", "Elements to compare for the minimum. See reduce for details.", "New in version 1.22.0.", "An array with the same shape as a, with the specified axis removed. If a is a 0-d array, or if axis is None, an ndarray scalar is returned. The same dtype as a is returned.", "See also", "The maximum value of an array along a given axis, ignoring any NaNs.", "The minimum value of an array along a given axis, propagating any NaNs.", "Element-wise minimum of two arrays, ignoring any NaNs.", "Element-wise minimum of two arrays, propagating any NaNs.", "Shows which elements are Not a Number (NaN).", "Shows which elements are neither NaN nor infinity.", "NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic (IEEE 754). This means that Not a Number is not equivalent to infinity. Positive infinity is treated as a very large number and negative infinity is treated as a very small (i.e. negative) number.", "If the input has a integer type the function is equivalent to np.min.", "When positive infinity and negative infinity are present:"]}, {"name": "numpy.nanpercentile()", "path": "reference/generated/numpy.nanpercentile", "type": "numpy.nanpercentile", "text": ["Compute the qth percentile of the data along the specified axis, while ignoring nan values.", "Returns the qth percentile(s) of the array elements.", "New in version 1.9.0.", "Input array or object that can be converted to an array, containing nan values to be ignored.", "Percentile or sequence of percentiles to compute, which must be between 0 and 100 inclusive.", "Axis or axes along which the percentiles are computed. The default is to compute the percentile(s) along a flattened version of the array.", "Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output, but the type (of the output) will be cast if necessary.", "If True, then allow the input array a to be modified by intermediate calculations, to save memory. In this case, the contents of the input a after this function completes is undefined.", "This parameter specifies the method to use for estimating the percentile. There are many different methods, some unique to NumPy. See the notes for explanation. The options sorted by their R type as summarized in the H&F paper [1] are:", "The first three methods are discontiuous. NumPy further defines the following discontinuous variations of the default \u2018linear\u2019 (7.) option:", "Changed in version 1.22.0: This argument was previously called \u201cinterpolation\u201d and only offered the \u201clinear\u201d default and last four options.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original array a.", "If this is anything but the default value it will be passed through (in the special case of an empty array) to the mean function of the underlying array. If the array is a sub-class and mean does not have the kwarg keepdims this will raise a RuntimeError.", "Deprecated name for the method keyword argument.", "Deprecated since version 1.22.0.", "If q is a single percentile and axis=None, then the result is a scalar. If multiple percentiles are given, first axis of the result corresponds to the percentiles. The other axes are the axes that remain after the reduction of a. If the input contains integers or floats smaller than float64, the output data-type is float64. Otherwise, the output data-type is the same as that of the input. If out is specified, that array is returned instead.", "See also", "equivalent to nanpercentile(..., 50)", "equivalent to nanpercentile, except q in range [0, 1].", "For more information please see numpy.percentile", "R. J. Hyndman and Y. Fan, \u201cSample quantiles in statistical packages,\u201d The American Statistician, 50(4), pp. 361-365, 1996"]}, {"name": "numpy.nanprod()", "path": "reference/generated/numpy.nanprod", "type": "numpy.nanprod", "text": ["Return the product of array elements over a given axis treating Not a Numbers (NaNs) as ones.", "One is returned for slices that are all-NaN or empty.", "New in version 1.10.0.", "Array containing numbers whose product is desired. If a is not an array, a conversion is attempted.", "Axis or axes along which the product is computed. The default is to compute the product of the flattened array.", "The type of the returned array and of the accumulator in which the elements are summed. By default, the dtype of a is used. An exception is when a has an integer type with less precision than the platform (u)intp. In that case, the default will be either (u)int32 or (u)int64 depending on whether the platform is 32 or 64 bits. For inexact inputs, dtype must be inexact.", "Alternate output array in which to place the result. The default is None. If provided, it must have the same shape as the expected output, but the type will be cast if necessary. See Output type determination for more details. The casting of NaN to integer can yield unexpected results.", "If True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original arr.", "The starting value for this product. See reduce for details.", "New in version 1.22.0.", "Elements to include in the product. See reduce for details.", "New in version 1.22.0.", "A new array holding the result is returned unless out is specified, in which case it is returned.", "See also", "Product across array propagating NaNs.", "Show which elements are NaN."]}, {"name": "numpy.nanquantile()", "path": "reference/generated/numpy.nanquantile", "type": "numpy.nanquantile", "text": ["Compute the qth quantile of the data along the specified axis, while ignoring nan values. Returns the qth quantile(s) of the array elements.", "New in version 1.15.0.", "Input array or object that can be converted to an array, containing nan values to be ignored", "Quantile or sequence of quantiles to compute, which must be between 0 and 1 inclusive.", "Axis or axes along which the quantiles are computed. The default is to compute the quantile(s) along a flattened version of the array.", "Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output, but the type (of the output) will be cast if necessary.", "If True, then allow the input array a to be modified by intermediate calculations, to save memory. In this case, the contents of the input a after this function completes is undefined.", "This parameter specifies the method to use for estimating the quantile. There are many different methods, some unique to NumPy. See the notes for explanation. The options sorted by their R type as summarized in the H&F paper [1] are:", "The first three methods are discontiuous. NumPy further defines the following discontinuous variations of the default \u2018linear\u2019 (7.) option:", "Changed in version 1.22.0: This argument was previously called \u201cinterpolation\u201d and only offered the \u201clinear\u201d default and last four options.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original array a.", "If this is anything but the default value it will be passed through (in the special case of an empty array) to the mean function of the underlying array. If the array is a sub-class and mean does not have the kwarg keepdims this will raise a RuntimeError.", "Deprecated name for the method keyword argument.", "Deprecated since version 1.22.0.", "If q is a single percentile and axis=None, then the result is a scalar. If multiple quantiles are given, first axis of the result corresponds to the quantiles. The other axes are the axes that remain after the reduction of a. If the input contains integers or floats smaller than float64, the output data-type is float64. Otherwise, the output data-type is the same as that of the input. If out is specified, that array is returned instead.", "See also", "equivalent to nanquantile(..., 0.5)", "same as nanquantile, but with q in the range [0, 100].", "For more information please see numpy.quantile", "R. J. Hyndman and Y. Fan, \u201cSample quantiles in statistical packages,\u201d The American Statistician, 50(4), pp. 361-365, 1996"]}, {"name": "numpy.nanstd()", "path": "reference/generated/numpy.nanstd", "type": "numpy.nanstd", "text": ["Compute the standard deviation along the specified axis, while ignoring NaNs.", "Returns the standard deviation, a measure of the spread of a distribution, of the non-NaN array elements. The standard deviation is computed for the flattened array by default, otherwise over the specified axis.", "For all-NaN slices or slices with zero degrees of freedom, NaN is returned and a RuntimeWarning is raised.", "New in version 1.8.0.", "Calculate the standard deviation of the non-NaN values.", "Axis or axes along which the standard deviation is computed. The default is to compute the standard deviation of the flattened array.", "Type to use in computing the standard deviation. For arrays of integer type the default is float64, for arrays of float types it is the same as the array type.", "Alternative output array in which to place the result. It must have the same shape as the expected output but the type (of the calculated values) will be cast if necessary.", "Means Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of non-NaN elements. By default ddof is zero.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original a.", "If this value is anything but the default it is passed through as-is to the relevant functions of the sub-classes. If these functions do not have a keepdims kwarg, a RuntimeError will be raised.", "Elements to include in the standard deviation. See reduce for details.", "New in version 1.22.0.", "If out is None, return a new array containing the standard deviation, otherwise return a reference to the output array. If ddof is >= the number of non-NaN elements in a slice or the slice contains only NaNs, then the result for that slice is NaN.", "See also", "The standard deviation is the square root of the average of the squared deviations from the mean: std = sqrt(mean(abs(x - x.mean())**2)).", "The average squared deviation is normally calculated as x.sum() / N, where N = len(x). If, however, ddof is specified, the divisor N - ddof is used instead. In standard statistical practice, ddof=1 provides an unbiased estimator of the variance of the infinite population. ddof=0 provides a maximum likelihood estimate of the variance for normally distributed variables. The standard deviation computed in this function is the square root of the estimated variance, so even with ddof=1, it will not be an unbiased estimate of the standard deviation per se.", "Note that, for complex numbers, std takes the absolute value before squaring, so that the result is always real and nonnegative.", "For floating-point input, the std is computed using the same precision the input has. Depending on the input data, this can cause the results to be inaccurate, especially for float32 (see example below). Specifying a higher-accuracy accumulator using the dtype keyword can alleviate this issue."]}, {"name": "numpy.nansum()", "path": "reference/generated/numpy.nansum", "type": "numpy.nansum", "text": ["Return the sum of array elements over a given axis treating Not a Numbers (NaNs) as zero.", "In NumPy versions <= 1.9.0 Nan is returned for slices that are all-NaN or empty. In later versions zero is returned.", "Array containing numbers whose sum is desired. If a is not an array, a conversion is attempted.", "Axis or axes along which the sum is computed. The default is to compute the sum of the flattened array.", "The type of the returned array and of the accumulator in which the elements are summed. By default, the dtype of a is used. An exception is when a has an integer type with less precision than the platform (u)intp. In that case, the default will be either (u)int32 or (u)int64 depending on whether the platform is 32 or 64 bits. For inexact inputs, dtype must be inexact.", "New in version 1.8.0.", "Alternate output array in which to place the result. The default is None. If provided, it must have the same shape as the expected output, but the type will be cast if necessary. See Output type determination for more details. The casting of NaN to integer can yield unexpected results.", "New in version 1.8.0.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original a.", "If the value is anything but the default, then keepdims will be passed through to the mean or sum methods of sub-classes of ndarray. If the sub-classes methods does not implement keepdims any exceptions will be raised.", "New in version 1.8.0.", "Starting value for the sum. See reduce for details.", "New in version 1.22.0.", "Elements to include in the sum. See reduce for details.", "New in version 1.22.0.", "A new array holding the result is returned unless out is specified, in which it is returned. The result has the same size as a, and the same shape as a if axis is not None or a is a 1-d array.", "See also", "Sum across array propagating NaNs.", "Show which elements are NaN.", "Show which elements are not NaN or +/-inf.", "If both positive and negative infinity are present, the sum will be Not A Number (NaN)."]}, {"name": "numpy.nanvar()", "path": "reference/generated/numpy.nanvar", "type": "numpy.nanvar", "text": ["Compute the variance along the specified axis, while ignoring NaNs.", "Returns the variance of the array elements, a measure of the spread of a distribution. The variance is computed for the flattened array by default, otherwise over the specified axis.", "For all-NaN slices or slices with zero degrees of freedom, NaN is returned and a RuntimeWarning is raised.", "New in version 1.8.0.", "Array containing numbers whose variance is desired. If a is not an array, a conversion is attempted.", "Axis or axes along which the variance is computed. The default is to compute the variance of the flattened array.", "Type to use in computing the variance. For arrays of integer type the default is float64; for arrays of float types it is the same as the array type.", "Alternate output array in which to place the result. It must have the same shape as the expected output, but the type is cast if necessary.", "\u201cDelta Degrees of Freedom\u201d: the divisor used in the calculation is N - ddof, where N represents the number of non-NaN elements. By default ddof is zero.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original a.", "Elements to include in the variance. See reduce for details.", "New in version 1.22.0.", "If out is None, return a new array containing the variance, otherwise return a reference to the output array. If ddof is >= the number of non-NaN elements in a slice or the slice contains only NaNs, then the result for that slice is NaN.", "See also", "Standard deviation", "Average", "Variance while not ignoring NaNs", "The variance is the average of the squared deviations from the mean, i.e., var = mean(abs(x - x.mean())**2).", "The mean is normally calculated as x.sum() / N, where N = len(x). If, however, ddof is specified, the divisor N - ddof is used instead. In standard statistical practice, ddof=1 provides an unbiased estimator of the variance of a hypothetical infinite population. ddof=0 provides a maximum likelihood estimate of the variance for normally distributed variables.", "Note that for complex numbers, the absolute value is taken before squaring, so that the result is always real and nonnegative.", "For floating-point input, the variance is computed using the same precision the input has. Depending on the input data, this can cause the results to be inaccurate, especially for float32 (see example below). Specifying a higher-accuracy accumulator using the dtype keyword can alleviate this issue.", "For this function to work on sub-classes of ndarray, they must define sum with the kwarg keepdims"]}, {"name": "numpy.ndarray()", "path": "reference/generated/numpy.ndarray", "type": "numpy.ndarray", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "The transposed array.", "Python buffer object pointing to the start of the array\u2019s data.", "Data-type of the array\u2019s elements.", "Information about the memory layout of the array.", "A 1-D iterator over the array.", "The imaginary part of the array.", "The real part of the array.", "Number of elements in the array.", "Length of one array element in bytes.", "Total bytes consumed by the elements of the array.", "Number of array dimensions.", "Tuple of array dimensions.", "Tuple of bytes to step in each dimension when traversing an array.", "An object to simplify the interaction of the array with the ctypes module.", "Base object if memory is from some other object.", "all([axis, out, keepdims, where])", "Returns True if all elements evaluate to True.", "any([axis, out, keepdims, where])", "Returns True if any of the elements of a evaluate to True.", "argmax([axis, out])", "Return indices of the maximum values along the given axis.", "argmin([axis, out])", "Return indices of the minimum values along the given axis.", "argpartition(kth[, axis, kind, order])", "Returns the indices that would partition this array.", "argsort([axis, kind, order])", "Returns the indices that would sort this array.", "astype(dtype[, order, casting, subok, copy])", "Copy of the array, cast to a specified type.", "byteswap([inplace])", "Swap the bytes of the array elements", "choose(choices[, out, mode])", "Use an index array to construct a new array from a set of choices.", "clip([min, max, out])", "Return an array whose values are limited to [min, max].", "compress(condition[, axis, out])", "Return selected slices of this array along given axis.", "conj()", "Complex-conjugate all elements.", "conjugate()", "Return the complex conjugate, element-wise.", "copy([order])", "Return a copy of the array.", "cumprod([axis, dtype, out])", "Return the cumulative product of the elements along the given axis.", "cumsum([axis, dtype, out])", "Return the cumulative sum of the elements along the given axis.", "diagonal([offset, axis1, axis2])", "Return specified diagonals.", "dump(file)", "Dump a pickle of the array to the specified file.", "dumps()", "Returns the pickle of the array as a string.", "fill(value)", "Fill the array with a scalar value.", "flatten([order])", "Return a copy of the array collapsed into one dimension.", "getfield(dtype[, offset])", "Returns a field of the given array as a certain type.", "item(*args)", "Copy an element of an array to a standard Python scalar and return it.", "itemset(*args)", "Insert scalar into an array (scalar is cast to array's dtype, if possible)", "max([axis, out, keepdims, initial, where])", "Return the maximum along a given axis.", "mean([axis, dtype, out, keepdims, where])", "Returns the average of the array elements along given axis.", "min([axis, out, keepdims, initial, where])", "Return the minimum along a given axis.", "newbyteorder([new_order])", "Return the array with the same data viewed with a different byte order.", "nonzero()", "Return the indices of the elements that are non-zero.", "partition(kth[, axis, kind, order])", "Rearranges the elements in the array in such a way that the value of the element in kth position is in the position it would be in a sorted array.", "prod([axis, dtype, out, keepdims, initial, ...])", "Return the product of the array elements over the given axis", "ptp([axis, out, keepdims])", "Peak to peak (maximum - minimum) value along a given axis.", "put(indices, values[, mode])", "Set a.flat[n] = values[n] for all n in indices.", "ravel([order])", "Return a flattened array.", "repeat(repeats[, axis])", "Repeat elements of an array.", "reshape(shape[, order])", "Returns an array containing the same data with a new shape.", "resize(new_shape[, refcheck])", "Change shape and size of array in-place.", "round([decimals, out])", "Return a with each element rounded to the given number of decimals.", "searchsorted(v[, side, sorter])", "Find indices where elements of v should be inserted in a to maintain order.", "setfield(val, dtype[, offset])", "Put a value into a specified place in a field defined by a data-type.", "setflags([write, align, uic])", "Set array flags WRITEABLE, ALIGNED, (WRITEBACKIFCOPY and UPDATEIFCOPY), respectively.", "sort([axis, kind, order])", "Sort an array in-place.", "squeeze([axis])", "Remove axes of length one from a.", "std([axis, dtype, out, ddof, keepdims, where])", "Returns the standard deviation of the array elements along given axis.", "sum([axis, dtype, out, keepdims, initial, where])", "Return the sum of the array elements over the given axis.", "swapaxes(axis1, axis2)", "Return a view of the array with axis1 and axis2 interchanged.", "take(indices[, axis, out, mode])", "Return an array formed from the elements of a at the given indices.", "tobytes([order])", "Construct Python bytes containing the raw data bytes in the array.", "tofile(fid[, sep, format])", "Write array to a file as text or binary (default).", "tolist()", "Return the array as an a.ndim-levels deep nested list of Python scalars.", "tostring([order])", "A compatibility alias for tobytes, with exactly the same behavior.", "trace([offset, axis1, axis2, dtype, out])", "Return the sum along diagonals of the array.", "transpose(*axes)", "Returns a view of the array with axes transposed.", "var([axis, dtype, out, ddof, keepdims, where])", "Returns the variance of the array elements, along given axis.", "view([dtype][, type])", "New view of array with the same data.", "dot"]}, {"name": "numpy.ndenumerate()", "path": "reference/generated/numpy.ndenumerate", "type": "numpy.ndenumerate", "text": ["Multidimensional index iterator.", "Return an iterator yielding pairs of array coordinates and values.", "Input array.", "See also"]}, {"name": "numpy.ndindex()", "path": "reference/generated/numpy.ndindex", "type": "numpy.ndindex", "text": ["An N-dimensional iterator object to index arrays.", "Given the shape of an array, an ndindex instance iterates over the N-dimensional index of the array. At each iteration a tuple of indices is returned, the last dimension is iterated over first.", "The size of each dimension of the array can be passed as individual parameters or as the elements of a tuple.", "See also", "Dimensions as individual arguments", "Same dimensions - but in a tuple (3, 2, 1)", "ndincr()", "Increment the multi-dimensional index by one."]}, {"name": "numpy.nditer()", "path": "reference/generated/numpy.nditer", "type": "numpy.nditer", "text": ["Efficient multi-dimensional iterator object to iterate over arrays. To get started using this object, see the introductory guide to array iteration.", "The array(s) to iterate over.", "Flags to control the behavior of the iterator.", "This is a list of flags for each operand. At minimum, one of readonly, readwrite, or writeonly must be specified.", "The required data type(s) of the operands. If copying or buffering is enabled, the data will be converted to/from their original types.", "Controls the iteration order. \u2018C\u2019 means C order, \u2018F\u2019 means Fortran order, \u2018A\u2019 means \u2018F\u2019 order if all the arrays are Fortran contiguous, \u2018C\u2019 order otherwise, and \u2018K\u2019 means as close to the order the array elements appear in memory as possible. This also affects the element memory order of allocate operands, as they are allocated to be compatible with iteration order. Default is \u2018K\u2019.", "Controls what kind of data casting may occur when making a copy or buffering. Setting this to \u2018unsafe\u2019 is not recommended, as it can adversely affect accumulations.", "If provided, is a list of ints or None for each operands. The list of axes for an operand is a mapping from the dimensions of the iterator to the dimensions of the operand. A value of -1 can be placed for entries, causing that dimension to be treated as newaxis.", "The desired shape of the iterator. This allows allocate operands with a dimension mapped by op_axes not corresponding to a dimension of a different operand to get a value not equal to 1 for that dimension.", "When buffering is enabled, controls the size of the temporary buffers. Set to 0 for the default value.", "nditer supersedes flatiter. The iterator implementation behind nditer is also exposed by the NumPy C API.", "The Python exposure supplies two iteration interfaces, one which follows the Python iterator protocol, and another which mirrors the C-style do-while pattern. The native Python approach is better in most cases, but if you need the coordinates or index of an iterator, use the C-style pattern.", "Here is how we might write an iter_add function, using the Python iterator protocol:", "Here is the same function, but following the C-style pattern:", "Here is an example outer product function:", "Here is an example function which operates like a \u201clambda\u201d ufunc:", "If operand flags \u201cwriteonly\u201d or \u201creadwrite\u201d are used the operands may be views into the original data with the WRITEBACKIFCOPY flag. In this case nditer must be used as a context manager or the nditer.close method must be called before using the result. The temporary data will be written back to the original data when the __exit__ function is called but not before:", "It is important to note that once the iterator is exited, dangling references (like x in the example) may or may not share data with the original data a. If writeback semantics were active, i.e. if x.base.flags.writebackifcopy is True, then exiting the iterator will sever the connection between x and a, writing to x will no longer write to a. If writeback semantics are not active, then x.data will still point at some part of a.data, and writing to one will affect the other.", "Context management and the close method appeared in version 1.15.0.", "The data types of the values provided in value. This may be different from the operand data types if buffering is enabled. Valid only before the iterator is closed.", "Whether the iteration over the operands is finished or not.", "If True, the iterator was created with the delay_bufalloc flag, and no reset() function was called on it yet.", "If True, the iterator was created with either the c_index or the f_index flag, and the property index can be used to retrieve it.", "If True, the iterator was created with the multi_index flag, and the property multi_index can be used to retrieve it.", "When the c_index or f_index flag was used, this property provides access to the index. Raises a ValueError if accessed and has_index is False.", "Whether iteration requires access to the Python API, for example if one of the operands is an object array.", "An index which matches the order of iteration.", "Size of the iterator.", "Structured view(s) of operands in memory, matching the reordered and optimized iterator access pattern. Valid only before the iterator is closed.", "When the multi_index flag was used, this property provides access to the index. Raises a ValueError if accessed accessed and has_multi_index is False.", "The dimensions of the iterator.", "The number of iterator operands.", "operands[Slice]", "Shape tuple, the shape of the iterator.", "Value of operands at current iteration. Normally, this is a tuple of array scalars, but if the flag external_loop is used, it is a tuple of one dimensional arrays.", "close()", "Resolve all writeback semantics in writeable operands.", "copy()", "Get a copy of the iterator in its current state.", "debug_print()", "Print the current state of the nditer instance and debug info to stdout.", "enable_external_loop()", "When the \"external_loop\" was not used during construction, but is desired, this modifies the iterator to behave as if the flag was specified.", "iternext()", "Check whether iterations are left, and perform a single internal iteration without returning the result.", "remove_axis(i, /)", "Removes axis i from the iterator.", "remove_multi_index()", "When the \"multi_index\" flag was specified, this removes it, allowing the internal iteration structure to be optimized further.", "reset()", "Reset the iterator to its initial state."]}, {"name": "numpy.negative()", "path": "reference/generated/numpy.negative", "type": "numpy.negative", "text": ["Numerical negative, element-wise.", "Input array.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Returned array or scalar: y = -x. This is a scalar if x is a scalar.", "The unary - operator can be used as a shorthand for np.negative on ndarrays."]}, {"name": "numpy.nested_iters()", "path": "reference/generated/numpy.nested_iters", "type": "numpy.nested_iters", "text": ["Create nditers for use in nested loops", "Create a tuple of nditer objects which iterate in nested loops over different axes of the op argument. The first iterator is used in the outermost loop, the last in the innermost loop. Advancing one will change the subsequent iterators to point at its new element.", "The array(s) to iterate over.", "Each item is used as an \u201cop_axes\u201d argument to an nditer", "See nditer parameters of the same name", "An nditer for each item in axes, outermost first", "See also", "Basic usage. Note how y is the \u201cflattened\u201d version of [a[:, 0, :], a[:, 1, 0], a[:, 2, :]] since we specified the first iter\u2019s axes as [1]"]}, {"name": "numpy.newaxis", "path": "reference/constants#numpy.newaxis", "type": "Constants", "text": ["A convenient alias for None, useful for indexing arrays.", "Outer product, same as outer(x, y):", "x[newaxis, :] is equivalent to x[newaxis] and x[None]:"]}, {"name": "numpy.nextafter()", "path": "reference/generated/numpy.nextafter", "type": "numpy.nextafter", "text": ["Return the next floating-point value after x1 towards x2, element-wise.", "Values to find the next representable value of.", "The direction where to look for the next representable value of x1. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The next representable values of x1 in the direction of x2. This is a scalar if both x1 and x2 are scalars."]}, {"name": "numpy.NINF", "path": "reference/constants#numpy.NINF", "type": "Constants", "text": ["IEEE 754 floating point representation of negative infinity.", "A floating point representation of negative infinity.", "isinf : Shows which elements are positive or negative infinity", "isposinf : Shows which elements are positive infinity", "isneginf : Shows which elements are negative infinity", "isnan : Shows which elements are Not a Number", "isfinite : Shows which elements are finite (not one of Not a Number, positive infinity and negative infinity)", "NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic (IEEE 754). This means that Not a Number is not equivalent to infinity. Also that positive infinity is not equivalent to negative infinity. But infinity is equivalent to positive infinity."]}, {"name": "numpy.nonzero()", "path": "reference/generated/numpy.nonzero", "type": "numpy.nonzero", "text": ["Return the indices of the elements that are non-zero.", "Returns a tuple of arrays, one for each dimension of a, containing the indices of the non-zero elements in that dimension. The values in a are always tested and returned in row-major, C-style order.", "To group the indices by element, rather than dimension, use argwhere, which returns a row for each non-zero element.", "Note", "When called on a zero-d array or scalar, nonzero(a) is treated as nonzero(atleast_1d(a)).", "Deprecated since version 1.17.0: Use atleast_1d explicitly if this behavior is deliberate.", "Input array.", "Indices of elements that are non-zero.", "See also", "Return indices that are non-zero in the flattened version of the input array.", "Equivalent ndarray method.", "Counts the number of non-zero elements in the input array.", "While the nonzero values can be obtained with a[nonzero(a)], it is recommended to use x[x.astype(bool)] or x[x != 0] instead, which will correctly handle 0-d arrays.", "A common use for nonzero is to find the indices of an array, where a condition is True. Given an array a, the condition a > 3 is a boolean array and since False is interpreted as 0, np.nonzero(a > 3) yields the indices of the a where the condition is true.", "Using this result to index a is equivalent to using the mask directly:", "nonzero can also be called as a method of the array."]}, {"name": "numpy.not_equal()", "path": "reference/generated/numpy.not_equal", "type": "numpy.not_equal", "text": ["Return (x1 != x2) element-wise.", "Input arrays. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Output array, element-wise comparison of x1 and x2. Typically of type bool, unless dtype=object is passed. This is a scalar if both x1 and x2 are scalars.", "See also", "The != operator can be used as a shorthand for np.not_equal on ndarrays."]}, {"name": "numpy.number", "path": "reference/arrays.scalars#numpy.number", "type": "Scalars", "text": ["Abstract base class of all numeric scalar types."]}, {"name": "numpy.NZERO", "path": "reference/constants#numpy.NZERO", "type": "Constants", "text": ["IEEE 754 floating point representation of negative zero.", "A floating point representation of negative zero.", "PZERO : Defines positive zero.", "isinf : Shows which elements are positive or negative infinity.", "isposinf : Shows which elements are positive infinity.", "isneginf : Shows which elements are negative infinity.", "isnan : Shows which elements are Not a Number.", "Not a Number, positive infinity and negative infinity.", "NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic (IEEE 754). Negative zero is considered to be a finite number."]}, {"name": "numpy.obj2sctype()", "path": "reference/generated/numpy.obj2sctype", "type": "numpy.obj2sctype", "text": ["Return the scalar dtype or NumPy equivalent of Python type of an object.", "The object of which the type is returned.", "If given, this is returned for objects whose types can not be determined. If not given, None is returned for those objects.", "The data type of rep.", "See also"]}, {"name": "numpy.object_", "path": "reference/arrays.scalars#numpy.object_", "type": "Scalars", "text": ["Any Python object.", "'O'"]}, {"name": "numpy.ogrid", "path": "reference/generated/numpy.ogrid", "type": "numpy.ogrid", "text": ["nd_grid instance which returns an open multi-dimensional \u201cmeshgrid\u201d.", "An instance of numpy.lib.index_tricks.nd_grid which returns an open (i.e. not fleshed out) mesh-grid when indexed, so that only one dimension of each returned array is greater than 1. The dimension and number of the output arrays are equal to the number of indexing dimensions. If the step length is not a complex number, then the stop is not inclusive.", "However, if the step length is a complex number (e.g. 5j), then the integer part of its magnitude is interpreted as specifying the number of points to create between the start and stop values, where the stop value is inclusive.", "ndarrays with only one dimension not equal to 1", "See also", "class of ogrid and mgrid objects", "like ogrid but returns dense (or fleshed out) mesh grids", "array concatenator"]}, {"name": "numpy.ones()", "path": "reference/generated/numpy.ones", "type": "numpy.ones", "text": ["Return a new array of given shape and type, filled with ones.", "Shape of the new array, e.g., (2, 3) or 2.", "The desired data-type for the array, e.g., numpy.int8. Default is numpy.float64.", "Whether to store multi-dimensional data in row-major (C-style) or column-major (Fortran-style) order in memory.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "Array of ones with the given shape, dtype, and order.", "See also", "Return an array of ones with shape and type of input.", "Return a new uninitialized array.", "Return a new array setting values to zero.", "Return a new array of given shape filled with value."]}, {"name": "numpy.ones_like()", "path": "reference/generated/numpy.ones_like", "type": "numpy.ones_like", "text": ["Return an array of ones with the same shape and type as a given array.", "The shape and data-type of a define these same attributes of the returned array.", "Overrides the data type of the result.", "New in version 1.6.0.", "Overrides the memory layout of the result. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of a as closely as possible.", "New in version 1.6.0.", "If True, then the newly created array will use the sub-class type of a, otherwise it will be a base-class array. Defaults to True.", "Overrides the shape of the result. If order=\u2019K\u2019 and the number of dimensions is unchanged, will try to keep order, otherwise, order=\u2019C\u2019 is implied.", "New in version 1.17.0.", "Array of ones with the same shape and type as a.", "See also", "Return an empty array with shape and type of input.", "Return an array of zeros with shape and type of input.", "Return a new array with shape of input filled with value.", "Return a new array setting values to one."]}, {"name": "numpy.outer()", "path": "reference/generated/numpy.outer", "type": "numpy.outer", "text": ["Compute the outer product of two vectors.", "Given two vectors, a = [a0, a1, ..., aM] and b = [b0, b1, ..., bN], the outer product [1] is:", "First input vector. Input is flattened if not already 1-dimensional.", "Second input vector. Input is flattened if not already 1-dimensional.", "A location where the result is stored", "New in version 1.9.0.", "out[i, j] = a[i] * b[j]", "See also", "einsum('i,j->ij', a.ravel(), b.ravel()) is the equivalent.", "A generalization to dimensions other than 1D and other operations. np.multiply.outer(a.ravel(), b.ravel()) is the equivalent.", "np.tensordot(a.ravel(), b.ravel(), axes=((), ())) is the equivalent.", ": G. H. Golub and C. F. Van Loan, Matrix Computations, 3rd ed., Baltimore, MD, Johns Hopkins University Press, 1996, pg. 8.", "Make a (very coarse) grid for computing a Mandelbrot set:", "An example using a \u201cvector\u201d of letters:"]}, {"name": "numpy.packbits()", "path": "reference/generated/numpy.packbits", "type": "numpy.packbits", "text": ["Packs the elements of a binary-valued array into bits in a uint8 array.", "The result is padded to full bytes by inserting zero bits at the end.", "An array of integers or booleans whose elements should be packed to bits.", "The dimension over which bit-packing is done. None implies packing the flattened array.", "The order of the input bits. \u2018big\u2019 will mimic bin(val), [0, 0, 0, 0, 0, 0, 1, 1] => 3 = 0b00000011, \u2018little\u2019 will reverse the order so [1, 1, 0, 0, 0, 0, 0, 0] => 3. Defaults to \u2018big\u2019.", "New in version 1.17.0.", "Array of type uint8 whose elements represent bits corresponding to the logical (0 or nonzero) value of the input elements. The shape of packed has the same number of dimensions as the input (unless axis is None, in which case the output is 1-D).", "See also", "Unpacks elements of a uint8 array into a binary-valued output array.", "Note that in binary 160 = 1010 0000, 64 = 0100 0000, 192 = 1100 0000, and 32 = 0010 0000."]}, {"name": "numpy.pad()", "path": "reference/generated/numpy.pad", "type": "numpy.pad", "text": ["Pad an array.", "The array to pad.", "Number of values padded to the edges of each axis. ((before_1, after_1), \u2026 (before_N, after_N)) unique pad widths for each axis. ((before, after),) yields same before and after pad for each axis. (pad,) or int is a shortcut for before = after = pad width for all axes.", "One of the following string values or a user supplied function.", "Pads with a constant value.", "Pads with the edge values of array.", "Pads with the linear ramp between end_value and the array edge value.", "Pads with the maximum value of all or part of the vector along each axis.", "Pads with the mean value of all or part of the vector along each axis.", "Pads with the median value of all or part of the vector along each axis.", "Pads with the minimum value of all or part of the vector along each axis.", "Pads with the reflection of the vector mirrored on the first and last values of the vector along each axis.", "Pads with the reflection of the vector mirrored along the edge of the array.", "Pads with the wrap of the vector along the axis. The first values are used to pad the end and the end values are used to pad the beginning.", "Pads with undefined values.", "New in version 1.17.", "Padding function, see Notes.", "Used in \u2018maximum\u2019, \u2018mean\u2019, \u2018median\u2019, and \u2018minimum\u2019. Number of values at edge of each axis used to calculate the statistic value.", "((before_1, after_1), \u2026 (before_N, after_N)) unique statistic lengths for each axis.", "((before, after),) yields same before and after statistic lengths for each axis.", "(stat_length,) or int is a shortcut for before = after = statistic length for all axes.", "Default is None, to use the entire axis.", "Used in \u2018constant\u2019. The values to set the padded values for each axis.", "((before_1, after_1), ... (before_N, after_N)) unique pad constants for each axis.", "((before, after),) yields same before and after constants for each axis.", "(constant,) or constant is a shortcut for before = after = constant for all axes.", "Default is 0.", "Used in \u2018linear_ramp\u2019. The values used for the ending value of the linear_ramp and that will form the edge of the padded array.", "((before_1, after_1), ... (before_N, after_N)) unique end values for each axis.", "((before, after),) yields same before and after end values for each axis.", "(constant,) or constant is a shortcut for before = after = constant for all axes.", "Default is 0.", "Used in \u2018reflect\u2019, and \u2018symmetric\u2019. The \u2018even\u2019 style is the default with an unaltered reflection around the edge value. For the \u2018odd\u2019 style, the extended part of the array is created by subtracting the reflected values from two times the edge value.", "Padded array of rank equal to array with shape increased according to pad_width.", "New in version 1.7.0.", "For an array with rank greater than 1, some of the padding of later axes is calculated from padding of previous axes. This is easiest to think about with a rank 2 array where the corners of the padded array are calculated by using padded values from the first axis.", "The padding function, if used, should modify a rank 1 array in-place. It has the following signature:", "where", "A rank 1 array already padded with zeros. Padded values are vector[:iaxis_pad_width[0]] and vector[-iaxis_pad_width[1]:].", "A 2-tuple of ints, iaxis_pad_width[0] represents the number of values padded at the beginning of vector where iaxis_pad_width[1] represents the number of values padded at the end of vector.", "The axis currently being calculated.", "Any keyword arguments the function requires."]}, {"name": "numpy.partition()", "path": "reference/generated/numpy.partition", "type": "numpy.partition", "text": ["Return a partitioned copy of an array.", "Creates a copy of the array with its elements rearranged in such a way that the value of the element in k-th position is in the position it would be in a sorted array. All elements smaller than the k-th element are moved before this element and all equal or greater are moved behind it. The ordering of the elements in the two partitions is undefined.", "New in version 1.8.0.", "Array to be sorted.", "Element index to partition by. The k-th value of the element will be in its final sorted position and all smaller elements will be moved before it and all equal or greater elements behind it. The order of all elements in the partitions is undefined. If provided with a sequence of k-th it will partition all elements indexed by k-th of them into their sorted position at once.", "Deprecated since version 1.22.0: Passing booleans as index is deprecated.", "Axis along which to sort. If None, the array is flattened before sorting. The default is -1, which sorts along the last axis.", "Selection algorithm. Default is \u2018introselect\u2019.", "When a is an array with fields defined, this argument specifies which fields to compare first, second, etc. A single field can be specified as a string. Not all fields need be specified, but unspecified fields will still be used, in the order in which they come up in the dtype, to break ties.", "Array of the same type and shape as a.", "See also", "Method to sort an array in-place.", "Indirect partition.", "Full sorting", "The various selection algorithms are characterized by their average speed, worst case performance, work space size, and whether they are stable. A stable sort keeps items with the same key in the same relative order. The available algorithms have the following properties:", "kind", "speed", "worst case", "work space", "stable", "\u2018introselect\u2019", "1", "O(n)", "0", "no", "All the partition algorithms make temporary copies of the data when partitioning along any but the last axis. Consequently, partitioning along the last axis is faster and uses less space than partitioning along any other axis.", "The sort order for complex numbers is lexicographic. If both the real and imaginary parts are non-nan then the order is determined by the real parts except when they are equal, in which case the order is determined by the imaginary parts."]}, {"name": "numpy.percentile()", "path": "reference/generated/numpy.percentile", "type": "numpy.percentile", "text": ["Compute the q-th percentile of the data along the specified axis.", "Returns the q-th percentile(s) of the array elements.", "Input array or object that can be converted to an array.", "Percentile or sequence of percentiles to compute, which must be between 0 and 100 inclusive.", "Axis or axes along which the percentiles are computed. The default is to compute the percentile(s) along a flattened version of the array.", "Changed in version 1.9.0: A tuple of axes is supported", "Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output, but the type (of the output) will be cast if necessary.", "If True, then allow the input array a to be modified by intermediate calculations, to save memory. In this case, the contents of the input a after this function completes is undefined.", "This parameter specifies the method to use for estimating the percentile. There are many different methods, some unique to NumPy. See the notes for explanation. The options sorted by their R type as summarized in the H&F paper [1] are:", "The first three methods are discontiuous. NumPy further defines the following discontinuous variations of the default \u2018linear\u2019 (7.) option:", "Changed in version 1.22.0: This argument was previously called \u201cinterpolation\u201d and only offered the \u201clinear\u201d default and last four options.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original array a.", "New in version 1.9.0.", "Deprecated name for the method keyword argument.", "Deprecated since version 1.22.0.", "If q is a single percentile and axis=None, then the result is a scalar. If multiple percentiles are given, first axis of the result corresponds to the percentiles. The other axes are the axes that remain after the reduction of a. If the input contains integers or floats smaller than float64, the output data-type is float64. Otherwise, the output data-type is the same as that of the input. If out is specified, that array is returned instead.", "See also", "equivalent to percentile(..., 50)", "equivalent to percentile, except q in the range [0, 1].", "Given a vector V of length N, the q-th percentile of V is the value q/100 of the way from the minimum to the maximum in a sorted copy of V. The values and distances of the two nearest neighbors as well as the method parameter will determine the percentile if the normalized ranking does not match the location of q exactly. This function is the same as the median if q=50, the same as the minimum if q=0 and the same as the maximum if q=100.", "This optional method parameter specifies the method to use when the desired quantile lies between two data points i < j. If g is the fractional part of the index surrounded by i and alpha and beta are correction constants modifying i and j.", "Below, \u2018q\u2019 is the quantile value, \u2018n\u2019 is the sample size and alpha and beta are constants. The following formula gives an interpolation \u201ci + g\u201d of where the quantile would be in the sorted sample. With \u2018i\u2019 being the floor and \u2018g\u2019 the fractional part of the result.", "The different methods then work as follows", "method 1 of H&F [1]. This method gives discontinuous results: * if g > 0 ; then take j * if g = 0 ; then take i", "method 2 of H&F [1]. This method give discontinuous results: * if g > 0 ; then take j * if g = 0 ; then average between bounds", "method 3 of H&F [1]. This method give discontinuous results: * if g > 0 ; then take j * if g = 0 and index is odd ; then take j * if g = 0 and index is even ; then take i", "method 4 of H&F [1]. This method give continuous results using: * alpha = 0 * beta = 1", "method 5 of H&F [1]. This method give continuous results using: * alpha = 1/2 * beta = 1/2", "method 6 of H&F [1]. This method give continuous results using: * alpha = 0 * beta = 0", "method 7 of H&F [1]. This method give continuous results using: * alpha = 1 * beta = 1", "method 8 of H&F [1]. This method is probably the best method if the sample distribution function is unknown (see reference). This method give continuous results using: * alpha = 1/3 * beta = 1/3", "method 9 of H&F [1]. This method is probably the best method if the sample distribution function is known to be normal. This method give continuous results using: * alpha = 3/8 * beta = 3/8", "NumPy method kept for backwards compatibility. Takes i as the interpolation point.", "NumPy method kept for backwards compatibility. Takes j as the interpolation point.", "NumPy method kept for backwards compatibility. Takes i or j, whichever is nearest.", "NumPy method kept for backwards compatibility. Uses (i + j) / 2.", "R. J. Hyndman and Y. Fan, \u201cSample quantiles in statistical packages,\u201d The American Statistician, 50(4), pp. 361-365, 1996", "The different methods can be visualized graphically:"]}, {"name": "numpy.pi", "path": "reference/constants#numpy.pi", "type": "Constants", "text": ["pi = 3.1415926535897932384626433...", "https://en.wikipedia.org/wiki/Pi"]}, {"name": "numpy.piecewise()", "path": "reference/generated/numpy.piecewise", "type": "numpy.piecewise", "text": ["Evaluate a piecewise-defined function.", "Given a set of conditions and corresponding functions, evaluate each function on the input data wherever its condition is true.", "The input domain.", "Each boolean array corresponds to a function in funclist. Wherever condlist[i] is True, funclist[i](x) is used as the output value.", "Each boolean array in condlist selects a piece of x, and should therefore be of the same shape as x.", "The length of condlist must correspond to that of funclist. If one extra function is given, i.e. if len(funclist) == len(condlist) + 1, then that extra function is the default value, used wherever all conditions are false.", "Each function is evaluated over x wherever its corresponding condition is True. It should take a 1d array as input and give an 1d array or a scalar value as output. If, instead of a callable, a scalar is provided then a constant function (lambda x: scalar) is assumed.", "Any further arguments given to piecewise are passed to the functions upon execution, i.e., if called piecewise(..., ..., 1, 'a'), then each function is called as f(x, 1, 'a').", "Keyword arguments used in calling piecewise are passed to the functions upon execution, i.e., if called piecewise(..., ..., alpha=1), then each function is called as f(x, alpha=1).", "The output is the same shape and type as x and is found by calling the functions in funclist on the appropriate portions of x, as defined by the boolean arrays in condlist. Portions not covered by any condition have a default value of 0.", "See also", "This is similar to choose or select, except that functions are evaluated on elements of x that satisfy the corresponding condition from condlist.", "The result is:", "Define the sigma function, which is -1 for x < 0 and +1 for x >= 0.", "Define the absolute value, which is -x for x <0 and x for x >= 0.", "Apply the same function to a scalar value."]}, {"name": "numpy.PINF", "path": "reference/constants#numpy.PINF", "type": "Constants", "text": ["IEEE 754 floating point representation of (positive) infinity.", "Use inf because Inf, Infinity, PINF and infty are aliases for inf. For more details, see inf.", "inf"]}, {"name": "numpy.place()", "path": "reference/generated/numpy.place", "type": "numpy.place", "text": ["Change elements of an array based on conditional and input values.", "Similar to np.copyto(arr, vals, where=mask), the difference is that place uses the first N elements of vals, where N is the number of True values in mask, while copyto uses the elements where mask is True.", "Note that extract does the exact opposite of place.", "Array to put data into.", "Boolean mask array. Must have the same size as a.", "Values to put into a. Only the first N elements are used, where N is the number of True values in mask. If vals is smaller than N, it will be repeated, and if elements of a are to be masked, this sequence must be non-empty.", "See also"]}, {"name": "numpy.poly()", "path": "reference/generated/numpy.poly", "type": "numpy.poly", "text": ["Find the coefficients of a polynomial with the given sequence of roots.", "Note", "This forms part of the old polynomial API. Since version 1.4, the new polynomial API defined in numpy.polynomial is preferred. A summary of the differences can be found in the transition guide.", "Returns the coefficients of the polynomial whose leading coefficient is one for the given sequence of zeros (multiple roots must be included in the sequence as many times as their multiplicity; see Examples). A square matrix (or array, which will be treated as a matrix) can also be given, in which case the coefficients of the characteristic polynomial of the matrix are returned.", "A sequence of polynomial roots, or a square array or matrix object.", "1D array of polynomial coefficients from highest to lowest degree:", "c[0] * x**(N) + c[1] * x**(N-1) + ... + c[N-1] * x + c[N] where c[0] always equals 1.", "If input is the wrong shape (the input must be a 1-D or square 2-D array).", "See also", "Compute polynomial values.", "Return the roots of a polynomial.", "Least squares polynomial fit.", "A one-dimensional polynomial class.", "Specifying the roots of a polynomial still leaves one degree of freedom, typically represented by an undetermined leading coefficient. [1] In the case of this function, that coefficient - the first one in the returned array - is always taken as one. (If for some reason you have one other point, the only automatic way presently to leverage that information is to use polyfit.)", "The characteristic polynomial, \\(p_a(t)\\), of an n-by-n matrix A is given by", "\\(p_a(t) = \\mathrm{det}(t\\, \\mathbf{I} - \\mathbf{A})\\),", "where I is the n-by-n identity matrix. [2]", "M. Sullivan and M. Sullivan, III, \u201cAlgebra and Trignometry, Enhanced With Graphing Utilities,\u201d Prentice-Hall, pg. 318, 1996.", "G. Strang, \u201cLinear Algebra and Its Applications, 2nd Edition,\u201d Academic Press, pg. 182, 1980.", "Given a sequence of a polynomial\u2019s zeros:", "The line above represents z**3 + 0*z**2 + 0*z + 0.", "The line above represents z**3 - z/4", "Given a square array object:", "Note how in all cases the leading coefficient is always 1."]}, {"name": "numpy.poly1d()", "path": "reference/generated/numpy.poly1d", "type": "numpy.poly1d", "text": ["A one-dimensional polynomial class.", "Note", "This forms part of the old polynomial API. Since version 1.4, the new polynomial API defined in numpy.polynomial is preferred. A summary of the differences can be found in the transition guide.", "A convenience class, used to encapsulate \u201cnatural\u201d operations on polynomials so that said operations may take on their customary form in code (see Examples).", "The polynomial\u2019s coefficients, in decreasing powers, or if the value of the second parameter is True, the polynomial\u2019s roots (values where the polynomial evaluates to 0). For example, poly1d([1, 2, 3]) returns an object that represents \\(x^2 + 2x + 3\\), whereas poly1d([1, 2, 3], True) returns one that represents \\((x-1)(x-2)(x-3) = x^3 - 6x^2 + 11x -6\\).", "If True, c_or_r specifies the polynomial\u2019s roots; the default is False.", "Changes the variable used when printing p from x to variable (see Examples).", "Construct the polynomial \\(x^2 + 2x + 3\\):", "Evaluate the polynomial at \\(x = 0.5\\):", "Find the roots:", "These numbers in the previous line represent (0, 0) to machine precision", "Show the coefficients:", "Display the order (the leading zero-coefficients are removed):", "Show the coefficient of the k-th power in the polynomial (which is equivalent to p.c[-(i+1)]):", "Polynomials can be added, subtracted, multiplied, and divided (returns quotient and remainder):", "asarray(p) gives the coefficient array, so polynomials can be used in all functions that accept arrays:", "The variable used in the string representation of p can be modified, using the variable parameter:", "Construct a polynomial from its roots:", "This is the same polynomial as obtained by:", "The polynomial coefficients", "The polynomial coefficients", "The polynomial coefficients", "The polynomial coefficients", "The order or degree of the polynomial", "The order or degree of the polynomial", "The roots of the polynomial, where self(x) == 0", "The roots of the polynomial, where self(x) == 0", "The name of the polynomial variable", "__call__(val)", "Call self as a function.", "deriv([m])", "Return a derivative of this polynomial.", "integ([m, k])", "Return an antiderivative (indefinite integral) of this polynomial."]}, {"name": "numpy.polyadd()", "path": "reference/generated/numpy.polyadd", "type": "numpy.polyadd", "text": ["Find the sum of two polynomials.", "Note", "This forms part of the old polynomial API. Since version 1.4, the new polynomial API defined in numpy.polynomial is preferred. A summary of the differences can be found in the transition guide.", "Returns the polynomial resulting from the sum of two input polynomials. Each input must be either a poly1d object or a 1D sequence of polynomial coefficients, from highest to lowest degree.", "Input polynomials.", "The sum of the inputs. If either input is a poly1d object, then the output is also a poly1d object. Otherwise, it is a 1D array of polynomial coefficients from highest to lowest degree.", "See also", "A one-dimensional polynomial class.", "Using poly1d objects:"]}, {"name": "numpy.polyder()", "path": "reference/generated/numpy.polyder", "type": "numpy.polyder", "text": ["Return the derivative of the specified order of a polynomial.", "Note", "This forms part of the old polynomial API. Since version 1.4, the new polynomial API defined in numpy.polynomial is preferred. A summary of the differences can be found in the transition guide.", "Polynomial to differentiate. A sequence is interpreted as polynomial coefficients, see poly1d.", "Order of differentiation (default: 1)", "A new polynomial representing the derivative.", "See also", "Anti-derivative of a polynomial.", "Class for one-dimensional polynomials.", "The derivative of the polynomial \\(x^3 + x^2 + x^1 + 1\\) is:", "which evaluates to:", "We can verify this, approximating the derivative with (f(x + h) - f(x))/h:", "The fourth-order derivative of a 3rd-order polynomial is zero:"]}, {"name": "numpy.polydiv()", "path": "reference/generated/numpy.polydiv", "type": "numpy.polydiv", "text": ["Returns the quotient and remainder of polynomial division.", "Note", "This forms part of the old polynomial API. Since version 1.4, the new polynomial API defined in numpy.polynomial is preferred. A summary of the differences can be found in the transition guide.", "The input arrays are the coefficients (including any coefficients equal to zero) of the \u201cnumerator\u201d (dividend) and \u201cdenominator\u201d (divisor) polynomials, respectively.", "Dividend polynomial\u2019s coefficients.", "Divisor polynomial\u2019s coefficients.", "Coefficients, including those equal to zero, of the quotient.", "Coefficients, including those equal to zero, of the remainder.", "See also", "Both u and v must be 0-d or 1-d (ndim = 0 or 1), but u.ndim need not equal v.ndim. In other words, all four possible combinations - u.ndim = v.ndim = 0, u.ndim = v.ndim = 1, u.ndim = 1, v.ndim = 0, and u.ndim = 0, v.ndim = 1 - work."]}, {"name": "numpy.polyfit()", "path": "reference/generated/numpy.polyfit", "type": "numpy.polyfit", "text": ["Least squares polynomial fit.", "Note", "This forms part of the old polynomial API. Since version 1.4, the new polynomial API defined in numpy.polynomial is preferred. A summary of the differences can be found in the transition guide.", "Fit a polynomial p(x) = p[0] * x**deg + ... + p[deg] of degree deg to points (x, y). Returns a vector of coefficients p that minimises the squared error in the order deg, deg-1, \u2026 0.", "The Polynomial.fit class method is recommended for new code as it is more stable numerically. See the documentation of the method for more information.", "x-coordinates of the M sample points (x[i], y[i]).", "y-coordinates of the sample points. Several data sets of sample points sharing the same x-coordinates can be fitted at once by passing in a 2D-array that contains one dataset per column.", "Degree of the fitting polynomial", "Relative condition number of the fit. Singular values smaller than this relative to the largest singular value will be ignored. The default value is len(x)*eps, where eps is the relative precision of the float type, about 2e-16 in most cases.", "Switch determining nature of return value. When it is False (the default) just the coefficients are returned, when True diagnostic information from the singular value decomposition is also returned.", "Weights. If not None, the weight w[i] applies to the unsquared residual y[i] - y_hat[i] at x[i]. Ideally the weights are chosen so that the errors of the products w[i]*y[i] all have the same variance. When using inverse-variance weighting, use w[i] = 1/sigma(y[i]). The default value is None.", "If given and not False, return not just the estimate but also its covariance matrix. By default, the covariance are scaled by chi2/dof, where dof = M - (deg + 1), i.e., the weights are presumed to be unreliable except in a relative sense and everything is scaled such that the reduced chi2 is unity. This scaling is omitted if cov='unscaled', as is relevant for the case that the weights are w = 1/sigma, with sigma known to be a reliable estimate of the uncertainty.", "Polynomial coefficients, highest power first. If y was 2-D, the coefficients for k-th data set are in p[:,k].", "These values are only returned if full == True", "coefficient matrix", "coefficient matrix", "For more details, see numpy.linalg.lstsq.", "Present only if full == False and cov == True. The covariance matrix of the polynomial coefficient estimates. The diagonal of this matrix are the variance estimates for each coefficient. If y is a 2-D array, then the covariance matrix for the k-th data set are in V[:,:,k]", "The rank of the coefficient matrix in the least-squares fit is deficient. The warning is only raised if full == False.", "The warnings can be turned off by", "See also", "Compute polynomial values.", "Computes a least-squares fit.", "Computes spline fits.", "The solution minimizes the squared error", "in the equations:", "The coefficient matrix of the coefficients p is a Vandermonde matrix.", "polyfit issues a RankWarning when the least-squares fit is badly conditioned. This implies that the best fit is not well-defined due to numerical error. The results may be improved by lowering the polynomial degree or by replacing x by x - x.mean(). The rcond parameter can also be set to a value smaller than its default, but the resulting fit may be spurious: including contributions from the small singular values can add numerical noise to the result.", "Note that fitting polynomial coefficients is inherently badly conditioned when the degree of the polynomial is large or the interval of sample points is badly centered. The quality of the fit should always be checked in these cases. When polynomial fits are not satisfactory, splines may be a good alternative.", "Wikipedia, \u201cCurve fitting\u201d, https://en.wikipedia.org/wiki/Curve_fitting", "Wikipedia, \u201cPolynomial interpolation\u201d, https://en.wikipedia.org/wiki/Polynomial_interpolation", "It is convenient to use poly1d objects for dealing with polynomials:", "High-order polynomials may oscillate wildly:", "Illustration:"]}, {"name": "numpy.polyint()", "path": "reference/generated/numpy.polyint", "type": "numpy.polyint", "text": ["Return an antiderivative (indefinite integral) of a polynomial.", "Note", "This forms part of the old polynomial API. Since version 1.4, the new polynomial API defined in numpy.polynomial is preferred. A summary of the differences can be found in the transition guide.", "The returned order m antiderivative P of polynomial p satisfies \\(\\frac{d^m}{dx^m}P(x) = p(x)\\) and is defined up to m - 1 integration constants k. The constants determine the low-order polynomial part", "of P so that \\(P^{(j)}(0) = k_{m-j-1}\\).", "Polynomial to integrate. A sequence is interpreted as polynomial coefficients, see poly1d.", "Order of the antiderivative. (Default: 1)", "Integration constants. They are given in the order of integration: those corresponding to highest-order terms come first.", "If None (default), all constants are assumed to be zero. If m = 1, a single scalar can be given instead of a list.", "See also", "derivative of a polynomial", "equivalent method", "The defining property of the antiderivative:", "The integration constants default to zero, but can be specified:", "Note that 3 = 6 / 2!, and that the constants are given in the order of integrations. Constant of the highest-order polynomial term comes first:"]}, {"name": "numpy.polymul()", "path": "reference/generated/numpy.polymul", "type": "numpy.polymul", "text": ["Find the product of two polynomials.", "Note", "This forms part of the old polynomial API. Since version 1.4, the new polynomial API defined in numpy.polynomial is preferred. A summary of the differences can be found in the transition guide.", "Finds the polynomial resulting from the multiplication of the two input polynomials. Each input must be either a poly1d object or a 1D sequence of polynomial coefficients, from highest to lowest degree.", "Input polynomials.", "The polynomial resulting from the multiplication of the inputs. If either inputs is a poly1d object, then the output is also a poly1d object. Otherwise, it is a 1D array of polynomial coefficients from highest to lowest degree.", "See also", "A one-dimensional polynomial class.", "Array convolution. Same output as polymul, but has parameter for overlap mode.", "Using poly1d objects:"]}, {"name": "numpy.polynomial.chebyshev.Chebyshev()", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev", "type": "numpy.polynomial.chebyshev.Chebyshev", "text": ["A Chebyshev series class.", "The Chebyshev class provides the standard Python numerical methods \u2018+\u2019, \u2018-\u2019, \u2018*\u2019, \u2018//\u2019, \u2018%\u2019, \u2018divmod\u2019, \u2018**\u2019, and \u2018()\u2019 as well as the methods listed below.", "Chebyshev coefficients in order of increasing degree, i.e., (1, 2, 3) gives 1*T_0(x) + 2*T_1(x) + 3*T_2(x).", "Domain to use. The interval [domain[0], domain[1]] is mapped to the interval [window[0], window[1]] by shifting and scaling. The default value is [-1, 1].", "Window, see domain for its use. The default value is [-1, 1].", "New in version 1.6.0.", "__call__(arg)", "Call self as a function.", "basis(deg[, domain, window])", "Series basis polynomial of degree deg.", "cast(series[, domain, window])", "Convert series to series of this class.", "convert([domain, kind, window])", "Convert series to a different kind and/or domain and/or window.", "copy()", "Return a copy.", "cutdeg(deg)", "Truncate series to the given degree.", "degree()", "The degree of the series.", "deriv([m])", "Differentiate.", "fit(x, y, deg[, domain, rcond, full, w, window])", "Least squares fit to data.", "fromroots(roots[, domain, window])", "Return series instance that has the specified roots.", "has_samecoef(other)", "Check if coefficients match.", "has_samedomain(other)", "Check if domains match.", "has_sametype(other)", "Check if types match.", "has_samewindow(other)", "Check if windows match.", "identity([domain, window])", "Identity function.", "integ([m, k, lbnd])", "Integrate.", "interpolate(func, deg[, domain, args])", "Interpolate a function at the Chebyshev points of the first kind.", "linspace([n, domain])", "Return x, y values at equally spaced points in domain.", "mapparms()", "Return the mapping parameters.", "roots()", "Return the roots of the series polynomial.", "trim([tol])", "Remove trailing coefficients", "truncate(size)", "Truncate series to length size."]}, {"name": "numpy.polynomial.hermite.Hermite()", "path": "reference/generated/numpy.polynomial.hermite.hermite", "type": "numpy.polynomial.hermite.Hermite", "text": ["An Hermite series class.", "The Hermite class provides the standard Python numerical methods \u2018+\u2019, \u2018-\u2019, \u2018*\u2019, \u2018//\u2019, \u2018%\u2019, \u2018divmod\u2019, \u2018**\u2019, and \u2018()\u2019 as well as the attributes and methods listed in the ABCPolyBase documentation.", "Hermite coefficients in order of increasing degree, i.e, (1, 2, 3) gives 1*H_0(x) + 2*H_1(X) + 3*H_2(x).", "Domain to use. The interval [domain[0], domain[1]] is mapped to the interval [window[0], window[1]] by shifting and scaling. The default value is [-1, 1].", "Window, see domain for its use. The default value is [-1, 1].", "New in version 1.6.0.", "__call__(arg)", "Call self as a function.", "basis(deg[, domain, window])", "Series basis polynomial of degree deg.", "cast(series[, domain, window])", "Convert series to series of this class.", "convert([domain, kind, window])", "Convert series to a different kind and/or domain and/or window.", "copy()", "Return a copy.", "cutdeg(deg)", "Truncate series to the given degree.", "degree()", "The degree of the series.", "deriv([m])", "Differentiate.", "fit(x, y, deg[, domain, rcond, full, w, window])", "Least squares fit to data.", "fromroots(roots[, domain, window])", "Return series instance that has the specified roots.", "has_samecoef(other)", "Check if coefficients match.", "has_samedomain(other)", "Check if domains match.", "has_sametype(other)", "Check if types match.", "has_samewindow(other)", "Check if windows match.", "identity([domain, window])", "Identity function.", "integ([m, k, lbnd])", "Integrate.", "linspace([n, domain])", "Return x, y values at equally spaced points in domain.", "mapparms()", "Return the mapping parameters.", "roots()", "Return the roots of the series polynomial.", "trim([tol])", "Remove trailing coefficients", "truncate(size)", "Truncate series to length size."]}, {"name": "numpy.polynomial.hermite_e.HermiteE()", "path": "reference/generated/numpy.polynomial.hermite_e.hermitee", "type": "numpy.polynomial.hermite_e.HermiteE", "text": ["An HermiteE series class.", "The HermiteE class provides the standard Python numerical methods \u2018+\u2019, \u2018-\u2019, \u2018*\u2019, \u2018//\u2019, \u2018%\u2019, \u2018divmod\u2019, \u2018**\u2019, and \u2018()\u2019 as well as the attributes and methods listed in the ABCPolyBase documentation.", "HermiteE coefficients in order of increasing degree, i.e, (1, 2, 3) gives 1*He_0(x) + 2*He_1(X) + 3*He_2(x).", "Domain to use. The interval [domain[0], domain[1]] is mapped to the interval [window[0], window[1]] by shifting and scaling. The default value is [-1, 1].", "Window, see domain for its use. The default value is [-1, 1].", "New in version 1.6.0.", "__call__(arg)", "Call self as a function.", "basis(deg[, domain, window])", "Series basis polynomial of degree deg.", "cast(series[, domain, window])", "Convert series to series of this class.", "convert([domain, kind, window])", "Convert series to a different kind and/or domain and/or window.", "copy()", "Return a copy.", "cutdeg(deg)", "Truncate series to the given degree.", "degree()", "The degree of the series.", "deriv([m])", "Differentiate.", "fit(x, y, deg[, domain, rcond, full, w, window])", "Least squares fit to data.", "fromroots(roots[, domain, window])", "Return series instance that has the specified roots.", "has_samecoef(other)", "Check if coefficients match.", "has_samedomain(other)", "Check if domains match.", "has_sametype(other)", "Check if types match.", "has_samewindow(other)", "Check if windows match.", "identity([domain, window])", "Identity function.", "integ([m, k, lbnd])", "Integrate.", "linspace([n, domain])", "Return x, y values at equally spaced points in domain.", "mapparms()", "Return the mapping parameters.", "roots()", "Return the roots of the series polynomial.", "trim([tol])", "Remove trailing coefficients", "truncate(size)", "Truncate series to length size."]}, {"name": "numpy.polynomial.laguerre.Laguerre()", "path": "reference/generated/numpy.polynomial.laguerre.laguerre", "type": "numpy.polynomial.laguerre.Laguerre", "text": ["A Laguerre series class.", "The Laguerre class provides the standard Python numerical methods \u2018+\u2019, \u2018-\u2019, \u2018*\u2019, \u2018//\u2019, \u2018%\u2019, \u2018divmod\u2019, \u2018**\u2019, and \u2018()\u2019 as well as the attributes and methods listed in the ABCPolyBase documentation.", "Laguerre coefficients in order of increasing degree, i.e, (1, 2, 3) gives 1*L_0(x) + 2*L_1(X) + 3*L_2(x).", "Domain to use. The interval [domain[0], domain[1]] is mapped to the interval [window[0], window[1]] by shifting and scaling. The default value is [0, 1].", "Window, see domain for its use. The default value is [0, 1].", "New in version 1.6.0.", "__call__(arg)", "Call self as a function.", "basis(deg[, domain, window])", "Series basis polynomial of degree deg.", "cast(series[, domain, window])", "Convert series to series of this class.", "convert([domain, kind, window])", "Convert series to a different kind and/or domain and/or window.", "copy()", "Return a copy.", "cutdeg(deg)", "Truncate series to the given degree.", "degree()", "The degree of the series.", "deriv([m])", "Differentiate.", "fit(x, y, deg[, domain, rcond, full, w, window])", "Least squares fit to data.", "fromroots(roots[, domain, window])", "Return series instance that has the specified roots.", "has_samecoef(other)", "Check if coefficients match.", "has_samedomain(other)", "Check if domains match.", "has_sametype(other)", "Check if types match.", "has_samewindow(other)", "Check if windows match.", "identity([domain, window])", "Identity function.", "integ([m, k, lbnd])", "Integrate.", "linspace([n, domain])", "Return x, y values at equally spaced points in domain.", "mapparms()", "Return the mapping parameters.", "roots()", "Return the roots of the series polynomial.", "trim([tol])", "Remove trailing coefficients", "truncate(size)", "Truncate series to length size."]}, {"name": "numpy.polynomial.legendre.Legendre()", "path": "reference/generated/numpy.polynomial.legendre.legendre", "type": "numpy.polynomial.legendre.Legendre", "text": ["A Legendre series class.", "The Legendre class provides the standard Python numerical methods \u2018+\u2019, \u2018-\u2019, \u2018*\u2019, \u2018//\u2019, \u2018%\u2019, \u2018divmod\u2019, \u2018**\u2019, and \u2018()\u2019 as well as the attributes and methods listed in the ABCPolyBase documentation.", "Legendre coefficients in order of increasing degree, i.e., (1, 2, 3) gives 1*P_0(x) + 2*P_1(x) + 3*P_2(x).", "Domain to use. The interval [domain[0], domain[1]] is mapped to the interval [window[0], window[1]] by shifting and scaling. The default value is [-1, 1].", "Window, see domain for its use. The default value is [-1, 1].", "New in version 1.6.0.", "__call__(arg)", "Call self as a function.", "basis(deg[, domain, window])", "Series basis polynomial of degree deg.", "cast(series[, domain, window])", "Convert series to series of this class.", "convert([domain, kind, window])", "Convert series to a different kind and/or domain and/or window.", "copy()", "Return a copy.", "cutdeg(deg)", "Truncate series to the given degree.", "degree()", "The degree of the series.", "deriv([m])", "Differentiate.", "fit(x, y, deg[, domain, rcond, full, w, window])", "Least squares fit to data.", "fromroots(roots[, domain, window])", "Return series instance that has the specified roots.", "has_samecoef(other)", "Check if coefficients match.", "has_samedomain(other)", "Check if domains match.", "has_sametype(other)", "Check if types match.", "has_samewindow(other)", "Check if windows match.", "identity([domain, window])", "Identity function.", "integ([m, k, lbnd])", "Integrate.", "linspace([n, domain])", "Return x, y values at equally spaced points in domain.", "mapparms()", "Return the mapping parameters.", "roots()", "Return the roots of the series polynomial.", "trim([tol])", "Remove trailing coefficients", "truncate(size)", "Truncate series to length size."]}, {"name": "numpy.polynomial.polynomial.Polynomial()", "path": "reference/generated/numpy.polynomial.polynomial.polynomial", "type": "numpy.polynomial.polynomial.Polynomial", "text": ["A power series class.", "The Polynomial class provides the standard Python numerical methods \u2018+\u2019, \u2018-\u2019, \u2018*\u2019, \u2018//\u2019, \u2018%\u2019, \u2018divmod\u2019, \u2018**\u2019, and \u2018()\u2019 as well as the attributes and methods listed in the ABCPolyBase documentation.", "Polynomial coefficients in order of increasing degree, i.e., (1, 2, 3) give 1 + 2*x + 3*x**2.", "Domain to use. The interval [domain[0], domain[1]] is mapped to the interval [window[0], window[1]] by shifting and scaling. The default value is [-1, 1].", "Window, see domain for its use. The default value is [-1, 1].", "New in version 1.6.0.", "__call__(arg)", "Call self as a function.", "basis(deg[, domain, window])", "Series basis polynomial of degree deg.", "cast(series[, domain, window])", "Convert series to series of this class.", "convert([domain, kind, window])", "Convert series to a different kind and/or domain and/or window.", "copy()", "Return a copy.", "cutdeg(deg)", "Truncate series to the given degree.", "degree()", "The degree of the series.", "deriv([m])", "Differentiate.", "fit(x, y, deg[, domain, rcond, full, w, window])", "Least squares fit to data.", "fromroots(roots[, domain, window])", "Return series instance that has the specified roots.", "has_samecoef(other)", "Check if coefficients match.", "has_samedomain(other)", "Check if domains match.", "has_sametype(other)", "Check if types match.", "has_samewindow(other)", "Check if windows match.", "identity([domain, window])", "Identity function.", "integ([m, k, lbnd])", "Integrate.", "linspace([n, domain])", "Return x, y values at equally spaced points in domain.", "mapparms()", "Return the mapping parameters.", "roots()", "Return the roots of the series polynomial.", "trim([tol])", "Remove trailing coefficients", "truncate(size)", "Truncate series to length size."]}, {"name": "numpy.polysub()", "path": "reference/generated/numpy.polysub", "type": "numpy.polysub", "text": ["Difference (subtraction) of two polynomials.", "Note", "This forms part of the old polynomial API. Since version 1.4, the new polynomial API defined in numpy.polynomial is preferred. A summary of the differences can be found in the transition guide.", "Given two polynomials a1 and a2, returns a1 - a2. a1 and a2 can be either array_like sequences of the polynomials\u2019 coefficients (including coefficients equal to zero), or poly1d objects.", "Minuend and subtrahend polynomials, respectively.", "Array or poly1d object of the difference polynomial\u2019s coefficients.", "See also"]}, {"name": "numpy.polyval()", "path": "reference/generated/numpy.polyval", "type": "numpy.polyval", "text": ["Evaluate a polynomial at specific values.", "Note", "This forms part of the old polynomial API. Since version 1.4, the new polynomial API defined in numpy.polynomial is preferred. A summary of the differences can be found in the transition guide.", "If p is of length N, this function returns the value:", "p[0]*x**(N-1) + p[1]*x**(N-2) + ... + p[N-2]*x + p[N-1]", "If x is a sequence, then p(x) is returned for each element of x. If x is another polynomial then the composite polynomial p(x(t)) is returned.", "1D array of polynomial coefficients (including coefficients equal to zero) from highest degree to the constant term, or an instance of poly1d.", "A number, an array of numbers, or an instance of poly1d, at which to evaluate p.", "If x is a poly1d instance, the result is the composition of the two polynomials, i.e., x is \u201csubstituted\u201d in p and the simplified result is returned. In addition, the type of x - array_like or poly1d - governs the type of the output: x array_like => values array_like, x a poly1d object => values is also.", "See also", "A polynomial class.", "Horner\u2019s scheme [1] is used to evaluate the polynomial. Even so, for polynomials of high degree the values may be inaccurate due to rounding errors. Use carefully.", "If x is a subtype of ndarray the return value will be of the same type.", "I. N. Bronshtein, K. A. Semendyayev, and K. A. Hirsch (Eng. trans. Ed.), Handbook of Mathematics, New York, Van Nostrand Reinhold Co., 1985, pg. 720."]}, {"name": "numpy.positive()", "path": "reference/generated/numpy.positive", "type": "numpy.positive", "text": ["Numerical positive, element-wise.", "New in version 1.13.0.", "Input array.", "Returned array or scalar: y = +x. This is a scalar if x is a scalar.", "Equivalent to x.copy(), but only defined for types that support arithmetic.", "The unary + operator can be used as a shorthand for np.positive on ndarrays."]}, {"name": "numpy.power()", "path": "reference/generated/numpy.power", "type": "numpy.power", "text": ["First array elements raised to powers from second array, element-wise.", "Raise each base in x1 to the positionally-corresponding power in x2. x1 and x2 must be broadcastable to the same shape.", "An integer type raised to a negative integer power will raise a ValueError.", "Negative values raised to a non-integral value will return nan. To get complex results, cast the input to complex, or specify the dtype to be complex (see the example below).", "The bases.", "The exponents. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The bases in x1 raised to the exponents in x2. This is a scalar if both x1 and x2 are scalars.", "See also", "power function that promotes integers to float", "Cube each element in an array.", "Raise the bases to different exponents.", "The effect of broadcasting.", "The ** operator can be used as a shorthand for np.power on ndarrays.", "Negative values raised to a non-integral value will result in nan (and a warning will be generated).", "To get complex results, give the argument dtype=complex."]}, {"name": "numpy.printoptions()", "path": "reference/generated/numpy.printoptions", "type": "numpy.printoptions", "text": ["Context manager for setting print options.", "Set print options for the scope of the with block, and restore the old options at the end. See set_printoptions for the full description of available options.", "See also", "The as-clause of the with-statement gives the current print options:"]}, {"name": "numpy.prod()", "path": "reference/generated/numpy.prod", "type": "numpy.prod", "text": ["Return the product of array elements over a given axis.", "Input data.", "Axis or axes along which a product is performed. The default, axis=None, will calculate the product of all the elements in the input array. If axis is negative it counts from the last to the first axis.", "New in version 1.7.0.", "If axis is a tuple of ints, a product is performed on all of the axes specified in the tuple instead of a single axis or all the axes as before.", "The type of the returned array, as well as of the accumulator in which the elements are multiplied. The dtype of a is used by default unless a has an integer dtype of less precision than the default platform integer. In that case, if a is signed then the platform integer is used while if a is unsigned then an unsigned integer of the same precision as the platform integer is used.", "Alternative output array in which to place the result. It must have the same shape as the expected output, but the type of the output values will be cast if necessary.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.", "If the default value is passed, then keepdims will not be passed through to the prod method of sub-classes of ndarray, however any non-default value will be. If the sub-class\u2019 method does not implement keepdims any exceptions will be raised.", "The starting value for this product. See reduce for details.", "New in version 1.15.0.", "Elements to include in the product. See reduce for details.", "New in version 1.17.0.", "An array shaped as a but with the specified axis removed. Returns a reference to out if specified.", "See also", "equivalent method", "Arithmetic is modular when using integer types, and no error is raised on overflow. That means that, on a 32-bit platform:", "The product of an empty array is the neutral element 1:", "By default, calculate the product of all elements:", "Even when the input array is two-dimensional:", "But we can also specify the axis over which to multiply:", "Or select specific elements to include:", "If the type of x is unsigned, then the output type is the unsigned platform integer:", "If x is of a signed integer type, then the output type is the default platform integer:", "You can also start the product with a value other than one:"]}, {"name": "numpy.promote_types()", "path": "reference/generated/numpy.promote_types", "type": "numpy.promote_types", "text": ["Returns the data type with the smallest size and smallest scalar kind to which both type1 and type2 may be safely cast. The returned data type is always in native byte order.", "This function is symmetric, but rarely associative.", "First data type.", "Second data type.", "The promoted data type.", "See also", "New in version 1.6.0.", "Starting in NumPy 1.9, promote_types function now returns a valid string length when given an integer or float dtype as one argument and a string dtype as another argument. Previously it always returned the input string dtype, even if it wasn\u2019t long enough to store the max integer/float value converted to a string.", "An example of a non-associative case:"]}, {"name": "numpy.ptp()", "path": "reference/generated/numpy.ptp", "type": "numpy.ptp", "text": ["Range of values (maximum - minimum) along an axis.", "The name of the function comes from the acronym for \u2018peak to peak\u2019.", "Warning", "ptp preserves the data type of the array. This means the return value for an input of signed integers with n bits (e.g. np.int8, np.int16, etc) is also a signed integer with n bits. In that case, peak-to-peak values greater than 2**(n-1)-1 will be returned as negative values. An example with a work-around is shown below.", "Input values.", "Axis along which to find the peaks. By default, flatten the array. axis may be negative, in which case it counts from the last to the first axis.", "New in version 1.15.0.", "If this is a tuple of ints, a reduction is performed on multiple axes, instead of a single axis or all the axes as before.", "Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output, but the type of the output values will be cast if necessary.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.", "If the default value is passed, then keepdims will not be passed through to the ptp method of sub-classes of ndarray, however any non-default value will be. If the sub-class\u2019 method does not implement keepdims any exceptions will be raised.", "A new array holding the result, unless out was specified, in which case a reference to out is returned.", "This example shows that a negative value can be returned when the input is an array of signed integers.", "A work-around is to use the view() method to view the result as unsigned integers with the same bit width:"]}, {"name": "numpy.put()", "path": "reference/generated/numpy.put", "type": "numpy.put", "text": ["Replaces specified elements of an array with given values.", "The indexing works on the flattened target array. put is roughly equivalent to:", "Target array.", "Target indices, interpreted as integers.", "Values to place in a at target indices. If v is shorter than ind it will be repeated as necessary.", "Specifies how out-of-bounds indices will behave.", "\u2018clip\u2019 mode means that all indices that are too large are replaced by the index that addresses the last element along that axis. Note that this disables indexing with negative numbers. In \u2018raise\u2019 mode, if an exception occurs the target array may still be modified.", "See also", "Put elements by matching the array and the index arrays"]}, {"name": "numpy.put_along_axis()", "path": "reference/generated/numpy.put_along_axis", "type": "numpy.put_along_axis", "text": ["Put values into the destination array by matching 1d index and data slices.", "This iterates over matching 1d slices oriented along the specified axis in the index and data arrays, and uses the former to place values into the latter. These slices can be different lengths.", "Functions returning an index along an axis, like argsort and argpartition, produce suitable indices for this function.", "New in version 1.15.0.", "Destination array.", "Indices to change along each 1d slice of arr. This must match the dimension of arr, but dimensions in Ni and Nj may be 1 to broadcast against arr.", "values to insert at those indices. Its shape and dimension are broadcast to match that of indices.", "The axis to take 1d slices along. If axis is None, the destination array is treated as if a flattened 1d view had been created of it.", "See also", "Take values from the input array by matching 1d index and data slices", "This is equivalent to (but faster than) the following use of ndindex and s_, which sets each of ii and kk to a tuple of indices:", "Equivalently, eliminating the inner loop, the last two lines would be:", "For this sample array", "We can replace the maximum values with:"]}, {"name": "numpy.putmask()", "path": "reference/generated/numpy.putmask", "type": "numpy.putmask", "text": ["Changes elements of an array based on conditional and input values.", "Sets a.flat[n] = values[n] for each n where mask.flat[n]==True.", "If values is not the same size as a and mask then it will repeat. This gives behavior different from a[mask] = values.", "Target array.", "Boolean mask array. It has to be the same shape as a.", "Values to put into a where mask is True. If values is smaller than a it will be repeated.", "See also", "If values is smaller than a it is repeated:"]}, {"name": "numpy.PZERO", "path": "reference/constants#numpy.PZERO", "type": "Constants", "text": ["IEEE 754 floating point representation of positive zero.", "A floating point representation of positive zero.", "NZERO : Defines negative zero.", "isinf : Shows which elements are positive or negative infinity.", "isposinf : Shows which elements are positive infinity.", "isneginf : Shows which elements are negative infinity.", "isnan : Shows which elements are Not a Number.", "Not a Number, positive infinity and negative infinity.", "NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic (IEEE 754). Positive zero is considered to be a finite number."]}, {"name": "numpy.quantile()", "path": "reference/generated/numpy.quantile", "type": "numpy.quantile", "text": ["Compute the q-th quantile of the data along the specified axis.", "New in version 1.15.0.", "Input array or object that can be converted to an array.", "Quantile or sequence of quantiles to compute, which must be between 0 and 1 inclusive.", "Axis or axes along which the quantiles are computed. The default is to compute the quantile(s) along a flattened version of the array.", "Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output, but the type (of the output) will be cast if necessary.", "If True, then allow the input array a to be modified by intermediate calculations, to save memory. In this case, the contents of the input a after this function completes is undefined.", "This parameter specifies the method to use for estimating the quantile. There are many different methods, some unique to NumPy. See the notes for explanation. The options sorted by their R type as summarized in the H&F paper [1] are:", "The first three methods are discontiuous. NumPy further defines the following discontinuous variations of the default \u2018linear\u2019 (7.) option:", "Changed in version 1.22.0: This argument was previously called \u201cinterpolation\u201d and only offered the \u201clinear\u201d default and last four options.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original array a.", "Deprecated name for the method keyword argument.", "Deprecated since version 1.22.0.", "If q is a single quantile and axis=None, then the result is a scalar. If multiple quantiles are given, first axis of the result corresponds to the quantiles. The other axes are the axes that remain after the reduction of a. If the input contains integers or floats smaller than float64, the output data-type is float64. Otherwise, the output data-type is the same as that of the input. If out is specified, that array is returned instead.", "See also", "equivalent to quantile, but with q in the range [0, 100].", "equivalent to quantile(..., 0.5)", "Given a vector V of length N, the q-th quantile of V is the value q of the way from the minimum to the maximum in a sorted copy of V. The values and distances of the two nearest neighbors as well as the method parameter will determine the quantile if the normalized ranking does not match the location of q exactly. This function is the same as the median if q=0.5, the same as the minimum if q=0.0 and the same as the maximum if q=1.0.", "This optional method parameter specifies the method to use when the desired quantile lies between two data points i < j. If g is the fractional part of the index surrounded by i and alpha and beta are correction constants modifying i and j.", "The different methods then work as follows", "method 1 of H&F [1]. This method gives discontinuous results: * if g > 0 ; then take j * if g = 0 ; then take i", "method 2 of H&F [1]. This method give discontinuous results: * if g > 0 ; then take j * if g = 0 ; then average between bounds", "method 3 of H&F [1]. This method give discontinuous results: * if g > 0 ; then take j * if g = 0 and index is odd ; then take j * if g = 0 and index is even ; then take i", "method 4 of H&F [1]. This method give continuous results using: * alpha = 0 * beta = 1", "method 5 of H&F [1]. This method give continuous results using: * alpha = 1/2 * beta = 1/2", "method 6 of H&F [1]. This method give continuous results using: * alpha = 0 * beta = 0", "method 7 of H&F [1]. This method give continuous results using: * alpha = 1 * beta = 1", "method 8 of H&F [1]. This method is probably the best method if the sample distribution function is unknown (see reference). This method give continuous results using: * alpha = 1/3 * beta = 1/3", "method 9 of H&F [1]. This method is probably the best method if the sample distribution function is known to be normal. This method give continuous results using: * alpha = 3/8 * beta = 3/8", "NumPy method kept for backwards compatibility. Takes i as the interpolation point.", "NumPy method kept for backwards compatibility. Takes j as the interpolation point.", "NumPy method kept for backwards compatibility. Takes i or j, whichever is nearest.", "NumPy method kept for backwards compatibility. Uses (i + j) / 2.", "R. J. Hyndman and Y. Fan, \u201cSample quantiles in statistical packages,\u201d The American Statistician, 50(4), pp. 361-365, 1996", "See also numpy.percentile for a visualization of most methods."]}, {"name": "numpy.r_", "path": "reference/generated/numpy.r_", "type": "numpy.r_", "text": ["Translates slice objects to concatenation along the first axis.", "This is a simple way to build up arrays quickly. There are two use cases.", "If slice notation is used, the syntax start:stop:step is equivalent to np.arange(start, stop, step) inside of the brackets. However, if step is an imaginary number (i.e. 100j) then its integer portion is interpreted as a number-of-points desired and the start and stop are inclusive. In other words start:stop:stepj is interpreted as np.linspace(start, stop, step, endpoint=1) inside of the brackets. After expansion of slice notation, all comma separated sequences are concatenated together.", "Optional character strings placed as the first element of the index expression can be used to change the output. The strings \u2018r\u2019 or \u2018c\u2019 result in matrix output. If the result is 1-D and \u2018r\u2019 is specified a 1 x N (row) matrix is produced. If the result is 1-D and \u2018c\u2019 is specified, then a N x 1 (column) matrix is produced. If the result is 2-D then both provide the same matrix result.", "A string integer specifies which axis to stack multiple comma separated arrays along. A string of two comma-separated integers allows indication of the minimum number of dimensions to force each entry into as the second integer (the axis to concatenate along is still the first integer).", "A string with three comma-separated integers allows specification of the axis to concatenate along, the minimum number of dimensions to force the entries to, and which axis should contain the start of the arrays which are less than the specified number of dimensions. In other words the third integer allows you to specify where the 1\u2019s should be placed in the shape of the arrays that have their shapes upgraded. By default, they are placed in the front of the shape tuple. The third argument allows you to specify where the start of the array should be instead. Thus, a third argument of \u20180\u2019 would place the 1\u2019s at the end of the array shape. Negative integers specify where in the new shape tuple the last dimension of upgraded arrays should be placed, so the default is \u2018-1\u2019.", "See also", "Join a sequence of arrays along an existing axis.", "Translates slice objects to concatenation along the second axis.", "String integers specify the axis to concatenate along or the minimum number of dimensions to force entries into.", "Using \u2018r\u2019 or \u2018c\u2019 as a first string argument creates a matrix."]}, {"name": "numpy.rad2deg()", "path": "reference/generated/numpy.rad2deg", "type": "numpy.rad2deg", "text": ["Convert angles from radians to degrees.", "Angle in radians.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The corresponding angle in degrees. This is a scalar if x is a scalar.", "See also", "Convert angles from degrees to radians.", "Remove large jumps in angle by wrapping.", "New in version 1.3.0.", "rad2deg(x) is 180 * x / pi."]}, {"name": "numpy.radians()", "path": "reference/generated/numpy.radians", "type": "numpy.radians", "text": ["Convert angles from degrees to radians.", "Input array in degrees.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The corresponding radian values. This is a scalar if x is a scalar.", "See also", "equivalent function", "Convert a degree array to radians"]}, {"name": "numpy.random.BitGenerator()", "path": "reference/random/bit_generators/generated/numpy.random.bitgenerator", "type": "numpy.random.BitGenerator", "text": ["Base Class for generic BitGenerators, which provide a stream of random bits based on different algorithms. Must be overridden.", "A seed to initialize the BitGenerator. If None, then fresh, unpredictable entropy will be pulled from the OS. If an int or array_like[ints] is passed, then it will be passed to ~`numpy.random.SeedSequence` to derive the initial BitGenerator state. One may also pass in a SeedSequence instance.", "See also", "Lock instance that is shared so that the same BitGenerator can be used in multiple Generators without corrupting the state. Code that generates values from a bit generator should hold the bit generator\u2019s lock.", "random_raw(self[, size])", "Return randoms as generated by the underlying BitGenerator"]}, {"name": "numpy.random.default_rng()", "path": "reference/random/generator", "type": "Random Generator", "text": ["The Generator provides access to a wide range of distributions, and served as a replacement for RandomState. The main difference between the two is that Generator relies on an additional BitGenerator to manage state and generate the random bits, which are then transformed into random values from useful distributions. The default BitGenerator used by Generator is PCG64. The BitGenerator can be changed by passing an instantized BitGenerator to Generator.", "Construct a new Generator with the default BitGenerator (PCG64).", "A seed to initialize the BitGenerator. If None, then fresh, unpredictable entropy will be pulled from the OS. If an int or array_like[ints] is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state. One may also pass in a SeedSequence instance. Additionally, when passed a BitGenerator, it will be wrapped by Generator. If passed a Generator, it will be returned unaltered.", "The initialized generator object.", "If seed is not a BitGenerator or a Generator, a new BitGenerator is instantiated. This function does not manage a default global instance.", "default_rng is the recommended constructor for the random number class Generator. Here are several ways we can construct a random number generator using default_rng and the Generator class.", "Here we use default_rng to generate a random float:", "Here we use default_rng to generate 3 random integers between 0 (inclusive) and 10 (exclusive):", "Here we specify a seed so that we have reproducible results:", "If we exit and restart our Python interpreter, we\u2019ll see that we generate the same random numbers again:", "Container for the BitGenerators.", "Generator exposes a number of methods for generating random numbers drawn from a variety of probability distributions. In addition to the distribution-specific arguments, each method takes a keyword argument size that defaults to None. If size is None, then a single value is generated and returned. If size is an integer, then a 1-D array filled with generated values is returned. If size is a tuple, then an array with that shape is filled and returned.", "The function numpy.random.default_rng will instantiate a Generator with numpy\u2019s default BitGenerator.", "No Compatibility Guarantee", "Generator does not provide a version compatibility guarantee. In particular, as better algorithms evolve the bit stream may change.", "BitGenerator to use as the core generator.", "See also", "Recommended constructor for Generator.", "The Python stdlib module random contains pseudo-random number generator with a number of methods that are similar to the ones available in Generator. It uses Mersenne Twister, and this bit generator can be accessed using MT19937. Generator, besides being NumPy-aware, has the advantage that it provides a much larger number of probability distributions to choose from.", "bit_generator", "Gets the bit generator instance used by the generator", "integers(low[, high, size, dtype, endpoint])", "Return random integers from low (inclusive) to high (exclusive), or if endpoint=True, low (inclusive) to high (inclusive).", "random([size, dtype, out])", "Return random floats in the half-open interval [0.0, 1.0).", "choice(a[, size, replace, p, axis, shuffle])", "Generates a random sample from a given array", "bytes(length)", "Return random bytes.", "The methods for randomly permuting a sequence are", "shuffle(x[, axis])", "Modify an array or sequence in-place by shuffling its contents.", "permutation(x[, axis])", "Randomly permute a sequence, or return a permuted range.", "permuted(x[, axis, out])", "Randomly permute x along axis axis.", "The following table summarizes the behaviors of the methods.", "method", "copy/in-place", "axis handling", "shuffle", "in-place", "as if 1d", "permutation", "copy", "as if 1d", "permuted", "either (use \u2018out\u2019 for in-place)", "axis independent", "The following subsections provide more details about the differences.", "The main difference between Generator.shuffle and Generator.permutation is that Generator.shuffle operates in-place, while Generator.permutation returns a copy.", "By default, Generator.permuted returns a copy. To operate in-place with Generator.permuted, pass the same array as the first argument and as the value of the out parameter. For example,", "Note that when out is given, the return value is out:", "An important distinction for these methods is how they handle the axis parameter. Both Generator.shuffle and Generator.permutation treat the input as a one-dimensional sequence, and the axis parameter determines which dimension of the input array to use as the sequence. In the case of a two-dimensional array, axis=0 will, in effect, rearrange the rows of the array, and axis=1 will rearrange the columns. For example", "Note that the columns have been rearranged \u201cin bulk\u201d: the values within each column have not changed.", "The method Generator.permuted treats the axis parameter similar to how numpy.sort treats it. Each slice along the given axis is shuffled independently of the others. Compare the following example of the use of Generator.permuted to the above example of Generator.permutation:", "In this example, the values within each row (i.e. the values along axis=1) have been shuffled independently. This is not a \u201cbulk\u201d shuffle of the columns.", "Generator.shuffle works on non-NumPy sequences. That is, if it is given a sequence that is not a NumPy array, it shuffles that sequence in-place. For example,", "beta(a, b[, size])", "Draw samples from a Beta distribution.", "binomial(n, p[, size])", "Draw samples from a binomial distribution.", "chisquare(df[, size])", "Draw samples from a chi-square distribution.", "dirichlet(alpha[, size])", "Draw samples from the Dirichlet distribution.", "exponential([scale, size])", "Draw samples from an exponential distribution.", "f(dfnum, dfden[, size])", "Draw samples from an F distribution.", "gamma(shape[, scale, size])", "Draw samples from a Gamma distribution.", "geometric(p[, size])", "Draw samples from the geometric distribution.", "gumbel([loc, scale, size])", "Draw samples from a Gumbel distribution.", "hypergeometric(ngood, nbad, nsample[, size])", "Draw samples from a Hypergeometric distribution.", "laplace([loc, scale, size])", "Draw samples from the Laplace or double exponential distribution with specified location (or mean) and scale (decay).", "logistic([loc, scale, size])", "Draw samples from a logistic distribution.", "lognormal([mean, sigma, size])", "Draw samples from a log-normal distribution.", "logseries(p[, size])", "Draw samples from a logarithmic series distribution.", "multinomial(n, pvals[, size])", "Draw samples from a multinomial distribution.", "multivariate_hypergeometric(colors, nsample)", "Generate variates from a multivariate hypergeometric distribution.", "multivariate_normal(mean, cov[, size, ...])", "Draw random samples from a multivariate normal distribution.", "negative_binomial(n, p[, size])", "Draw samples from a negative binomial distribution.", "noncentral_chisquare(df, nonc[, size])", "Draw samples from a noncentral chi-square distribution.", "noncentral_f(dfnum, dfden, nonc[, size])", "Draw samples from the noncentral F distribution.", "normal([loc, scale, size])", "Draw random samples from a normal (Gaussian) distribution.", "pareto(a[, size])", "Draw samples from a Pareto II or Lomax distribution with specified shape.", "poisson([lam, size])", "Draw samples from a Poisson distribution.", "power(a[, size])", "Draws samples in [0, 1] from a power distribution with positive exponent a - 1.", "rayleigh([scale, size])", "Draw samples from a Rayleigh distribution.", "standard_cauchy([size])", "Draw samples from a standard Cauchy distribution with mode = 0.", "standard_exponential([size, dtype, method, out])", "Draw samples from the standard exponential distribution.", "standard_gamma(shape[, size, dtype, out])", "Draw samples from a standard Gamma distribution.", "standard_normal([size, dtype, out])", "Draw samples from a standard Normal distribution (mean=0, stdev=1).", "standard_t(df[, size])", "Draw samples from a standard Student's t distribution with df degrees of freedom.", "triangular(left, mode, right[, size])", "Draw samples from the triangular distribution over the interval [left, right].", "uniform([low, high, size])", "Draw samples from a uniform distribution.", "vonmises(mu, kappa[, size])", "Draw samples from a von Mises distribution.", "wald(mean, scale[, size])", "Draw samples from a Wald, or inverse Gaussian, distribution.", "weibull(a[, size])", "Draw samples from a Weibull distribution.", "zipf(a[, size])", "Draw samples from a Zipf distribution."]}, {"name": "numpy.random.Generator()", "path": "reference/random/generator#numpy.random.Generator", "type": "Random Generator", "text": ["Container for the BitGenerators.", "Generator exposes a number of methods for generating random numbers drawn from a variety of probability distributions. In addition to the distribution-specific arguments, each method takes a keyword argument size that defaults to None. If size is None, then a single value is generated and returned. If size is an integer, then a 1-D array filled with generated values is returned. If size is a tuple, then an array with that shape is filled and returned.", "The function numpy.random.default_rng will instantiate a Generator with numpy\u2019s default BitGenerator.", "No Compatibility Guarantee", "Generator does not provide a version compatibility guarantee. In particular, as better algorithms evolve the bit stream may change.", "BitGenerator to use as the core generator.", "See also", "Recommended constructor for Generator.", "The Python stdlib module random contains pseudo-random number generator with a number of methods that are similar to the ones available in Generator. It uses Mersenne Twister, and this bit generator can be accessed using MT19937. Generator, besides being NumPy-aware, has the advantage that it provides a much larger number of probability distributions to choose from."]}, {"name": "numpy.random.MT19937()", "path": "reference/random/bit_generators/mt19937", "type": "MT19937", "text": ["Container for the Mersenne Twister pseudo-random number generator.", "A seed to initialize the BitGenerator. If None, then fresh, unpredictable entropy will be pulled from the OS. If an int or array_like[ints] is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state. One may also pass in a SeedSequence instance.", "MT19937 provides a capsule containing function pointers that produce doubles, and unsigned 32 and 64- bit integers [1]. These are not directly consumable in Python and must be consumed by a Generator or similar object that supports low-level access.", "The Python stdlib module \u201crandom\u201d also contains a Mersenne Twister pseudo-random number generator.", "State and Seeding", "The MT19937 state vector consists of a 624-element array of 32-bit unsigned integers plus a single integer value between 0 and 624 that indexes the current position within the main array.", "The input seed is processed by SeedSequence to fill the whole state. The first element is reset such that only its most significant bit is set.", "Parallel Features", "The preferred way to use a BitGenerator in parallel applications is to use the SeedSequence.spawn method to obtain entropy values, and to use these to generate new BitGenerators:", "Another method is to use MT19937.jumped which advances the state as-if \\(2^{128}\\) random numbers have been generated ([1], [2]). This allows the original sequence to be split so that distinct segments can be used in each worker process. All generators should be chained to ensure that the segments come from the same sequence.", "Compatibility Guarantee", "MT19937 makes a guarantee that a fixed seed and will always produce the same random integer stream.", "Hiroshi Haramoto, Makoto Matsumoto, and Pierre L\u2019Ecuyer, \u201cA Fast Jump Ahead Algorithm for Linear Recurrences in a Polynomial Space\u201d, Sequences and Their Applications - SETA, 290\u2013298, 2008.", "Hiroshi Haramoto, Makoto Matsumoto, Takuji Nishimura, Fran\u00e7ois Panneton, Pierre L\u2019Ecuyer, \u201cEfficient Jump Ahead for F2-Linear Random Number Generators\u201d, INFORMS JOURNAL ON COMPUTING, Vol. 20, No. 3, Summer 2008, pp. 385-390.", "Lock instance that is shared so that the same bit git generator can be used in multiple Generators without corrupting the state. Code that generates values from a bit generator should hold the bit generator\u2019s lock.", "state", "Get or set the PRNG state", "jumped([jumps])", "Returns a new bit generator with the state jumped", "cffi", "CFFI interface", "ctypes", "ctypes interface"]}, {"name": "numpy.random.PCG64()", "path": "reference/random/bit_generators/pcg64", "type": "PCG64", "text": ["BitGenerator for the PCG-64 pseudo-random number generator.", "A seed to initialize the BitGenerator. If None, then fresh, unpredictable entropy will be pulled from the OS. If an int or array_like[ints] is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state. One may also pass in a SeedSequence instance.", "PCG-64 is a 128-bit implementation of O\u2019Neill\u2019s permutation congruential generator ([1], [2]). PCG-64 has a period of \\(2^{128}\\) and supports advancing an arbitrary number of steps as well as \\(2^{127}\\) streams. The specific member of the PCG family that we use is PCG XSL RR 128/64 as described in the paper ([2]).", "PCG64 provides a capsule containing function pointers that produce doubles, and unsigned 32 and 64- bit integers. These are not directly consumable in Python and must be consumed by a Generator or similar object that supports low-level access.", "Supports the method advance to advance the RNG an arbitrary number of steps. The state of the PCG-64 RNG is represented by 2 128-bit unsigned integers.", "State and Seeding", "The PCG64 state vector consists of 2 unsigned 128-bit values, which are represented externally as Python ints. One is the state of the PRNG, which is advanced by a linear congruential generator (LCG). The second is a fixed odd increment used in the LCG.", "The input seed is processed by SeedSequence to generate both values. The increment is not independently settable.", "Parallel Features", "The preferred way to use a BitGenerator in parallel applications is to use the SeedSequence.spawn method to obtain entropy values, and to use these to generate new BitGenerators:", "Compatibility Guarantee", "PCG64 makes a guarantee that a fixed seed will always produce the same random integer stream.", "\u201cPCG, A Family of Better Random Number Generators\u201d", "O\u2019Neill, Melissa E. \u201cPCG: A Family of Simple Fast Space-Efficient Statistically Good Algorithms for Random Number Generation\u201d", "state", "Get or set the PRNG state", "advance(delta)", "Advance the underlying RNG as-if delta draws have occurred.", "jumped([jumps])", "Returns a new bit generator with the state jumped.", "cffi", "CFFI interface", "ctypes", "ctypes interface"]}, {"name": "numpy.random.PCG64DXSM()", "path": "reference/random/bit_generators/pcg64dxsm", "type": "PCG64DXSM", "text": ["BitGenerator for the PCG-64 DXSM pseudo-random number generator.", "A seed to initialize the BitGenerator. If None, then fresh, unpredictable entropy will be pulled from the OS. If an int or array_like[ints] is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state. One may also pass in a SeedSequence instance.", "PCG-64 DXSM is a 128-bit implementation of O\u2019Neill\u2019s permutation congruential generator ([1], [2]). PCG-64 DXSM has a period of \\(2^{128}\\) and supports advancing an arbitrary number of steps as well as \\(2^{127}\\) streams. The specific member of the PCG family that we use is PCG CM DXSM 128/64. It differs from PCG64 in that it uses the stronger DXSM output function, a 64-bit \u201ccheap multiplier\u201d in the LCG, and outputs from the state before advancing it rather than advance-then-output.", "PCG64DXSM provides a capsule containing function pointers that produce doubles, and unsigned 32 and 64- bit integers. These are not directly consumable in Python and must be consumed by a Generator or similar object that supports low-level access.", "Supports the method advance to advance the RNG an arbitrary number of steps. The state of the PCG-64 DXSM RNG is represented by 2 128-bit unsigned integers.", "State and Seeding", "The PCG64DXSM state vector consists of 2 unsigned 128-bit values, which are represented externally as Python ints. One is the state of the PRNG, which is advanced by a linear congruential generator (LCG). The second is a fixed odd increment used in the LCG.", "The input seed is processed by SeedSequence to generate both values. The increment is not independently settable.", "Parallel Features", "The preferred way to use a BitGenerator in parallel applications is to use the SeedSequence.spawn method to obtain entropy values, and to use these to generate new BitGenerators:", "Compatibility Guarantee", "PCG64DXSM makes a guarantee that a fixed seed will always produce the same random integer stream.", "\u201cPCG, A Family of Better Random Number Generators\u201d", "O\u2019Neill, Melissa E. \u201cPCG: A Family of Simple Fast Space-Efficient Statistically Good Algorithms for Random Number Generation\u201d", "state", "Get or set the PRNG state", "advance(delta)", "Advance the underlying RNG as-if delta draws have occurred.", "jumped([jumps])", "Returns a new bit generator with the state jumped.", "cffi", "CFFI interface", "ctypes", "ctypes interface"]}, {"name": "numpy.random.Philox()", "path": "reference/random/bit_generators/philox", "type": "Philox", "text": ["Container for the Philox (4x64) pseudo-random number generator.", "A seed to initialize the BitGenerator. If None, then fresh, unpredictable entropy will be pulled from the OS. If an int or array_like[ints] is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state. One may also pass in a SeedSequence instance.", "Counter to use in the Philox state. Can be either a Python int (long in 2.x) in [0, 2**256) or a 4-element uint64 array. If not provided, the RNG is initialized at 0.", "Key to use in the Philox state. Unlike seed, the value in key is directly set. Can be either a Python int in [0, 2**128) or a 2-element uint64 array. key and seed cannot both be used.", "Philox is a 64-bit PRNG that uses a counter-based design based on weaker (and faster) versions of cryptographic functions [1]. Instances using different values of the key produce independent sequences. Philox has a period of \\(2^{256} - 1\\) and supports arbitrary advancing and jumping the sequence in increments of \\(2^{128}\\). These features allow multiple non-overlapping sequences to be generated.", "Philox provides a capsule containing function pointers that produce doubles, and unsigned 32 and 64- bit integers. These are not directly consumable in Python and must be consumed by a Generator or similar object that supports low-level access.", "State and Seeding", "The Philox state vector consists of a 256-bit value encoded as a 4-element uint64 array and a 128-bit value encoded as a 2-element uint64 array. The former is a counter which is incremented by 1 for every 4 64-bit randoms produced. The second is a key which determined the sequence produced. Using different keys produces independent sequences.", "The input seed is processed by SeedSequence to generate the key. The counter is set to 0.", "Alternately, one can omit the seed parameter and set the key and counter directly.", "Parallel Features", "The preferred way to use a BitGenerator in parallel applications is to use the SeedSequence.spawn method to obtain entropy values, and to use these to generate new BitGenerators:", "Philox can be used in parallel applications by calling the jumped method to advances the state as-if \\(2^{128}\\) random numbers have been generated. Alternatively, advance can be used to advance the counter for any positive step in [0, 2**256). When using jumped, all generators should be chained to ensure that the segments come from the same sequence.", "Alternatively, Philox can be used in parallel applications by using a sequence of distinct keys where each instance uses different key.", "Compatibility Guarantee", "Philox makes a guarantee that a fixed seed will always produce the same random integer stream.", "John K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw, \u201cParallel Random Numbers: As Easy as 1, 2, 3,\u201d Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC11), New York, NY: ACM, 2011.", "Lock instance that is shared so that the same bit git generator can be used in multiple Generators without corrupting the state. Code that generates values from a bit generator should hold the bit generator\u2019s lock.", "state", "Get or set the PRNG state", "advance(delta)", "Advance the underlying RNG as-if delta draws have occurred.", "jumped([jumps])", "Returns a new bit generator with the state jumped", "cffi", "CFFI interface", "ctypes", "ctypes interface"]}, {"name": "numpy.random.RandomState()", "path": "reference/random/legacy", "type": "Legacy Generator (RandomState)", "text": ["The RandomState provides access to legacy generators. This generator is considered frozen and will have no further improvements. It is guaranteed to produce the same values as the final point release of NumPy v1.16. These all depend on Box-Muller normals or inverse CDF exponentials or gammas. This class should only be used if it is essential to have randoms that are identical to what would have been produced by previous versions of NumPy.", "RandomState adds additional information to the state which is required when using Box-Muller normals since these are produced in pairs. It is important to use RandomState.get_state, and not the underlying bit generators state, when accessing the state so that these extra values are saved.", "Although we provide the MT19937 BitGenerator for use independent of RandomState, note that its default seeding uses SeedSequence rather than the legacy seeding algorithm. RandomState will use the legacy seeding algorithm. The methods to use the legacy seeding algorithm are currently private as the main reason to use them is just to implement RandomState. However, one can reset the state of MT19937 using the state of the RandomState:", "Container for the slow Mersenne Twister pseudo-random number generator. Consider using a different BitGenerator with the Generator container instead.", "RandomState and Generator expose a number of methods for generating random numbers drawn from a variety of probability distributions. In addition to the distribution-specific arguments, each method takes a keyword argument size that defaults to None. If size is None, then a single value is generated and returned. If size is an integer, then a 1-D array filled with generated values is returned. If size is a tuple, then an array with that shape is filled and returned.", "Compatibility Guarantee", "A fixed bit generator using a fixed seed and a fixed series of calls to \u2018RandomState\u2019 methods using the same parameters will always produce the same results up to roundoff error except when the values were incorrect. RandomState is effectively frozen and will only receive updates that are required by changes in the the internals of Numpy. More substantial changes, including algorithmic improvements, are reserved for Generator.", "Random seed used to initialize the pseudo-random number generator or an instantized BitGenerator. If an integer or array, used as a seed for the MT19937 BitGenerator. Values can be any integer between 0 and 2**32 - 1 inclusive, an array (or other sequence) of such integers, or None (the default). If seed is None, then the MT19937 BitGenerator is initialized by reading data from /dev/urandom (or the Windows analogue) if available or seed from the clock otherwise.", "See also", "The Python stdlib module \u201crandom\u201d also contains a Mersenne Twister pseudo-random number generator with a number of methods that are similar to the ones available in RandomState. RandomState, besides being NumPy-aware, has the advantage that it provides a much larger number of probability distributions to choose from.", "get_state()", "Return a tuple representing the internal state of the generator.", "set_state(state)", "Set the internal state of the generator from a tuple.", "seed(self[, seed])", "Reseed a legacy MT19937 BitGenerator", "rand(d0, d1, ..., dn)", "Random values in a given shape.", "randn(d0, d1, ..., dn)", "Return a sample (or samples) from the \"standard normal\" distribution.", "randint(low[, high, size, dtype])", "Return random integers from low (inclusive) to high (exclusive).", "random_integers(low[, high, size])", "Random integers of type np.int_ between low and high, inclusive.", "random_sample([size])", "Return random floats in the half-open interval [0.0, 1.0).", "choice(a[, size, replace, p])", "Generates a random sample from a given 1-D array", "bytes(length)", "Return random bytes.", "shuffle(x)", "Modify a sequence in-place by shuffling its contents.", "permutation(x)", "Randomly permute a sequence, or return a permuted range.", "beta(a, b[, size])", "Draw samples from a Beta distribution.", "binomial(n, p[, size])", "Draw samples from a binomial distribution.", "chisquare(df[, size])", "Draw samples from a chi-square distribution.", "dirichlet(alpha[, size])", "Draw samples from the Dirichlet distribution.", "exponential([scale, size])", "Draw samples from an exponential distribution.", "f(dfnum, dfden[, size])", "Draw samples from an F distribution.", "gamma(shape[, scale, size])", "Draw samples from a Gamma distribution.", "geometric(p[, size])", "Draw samples from the geometric distribution.", "gumbel([loc, scale, size])", "Draw samples from a Gumbel distribution.", "hypergeometric(ngood, nbad, nsample[, size])", "Draw samples from a Hypergeometric distribution.", "laplace([loc, scale, size])", "Draw samples from the Laplace or double exponential distribution with specified location (or mean) and scale (decay).", "logistic([loc, scale, size])", "Draw samples from a logistic distribution.", "lognormal([mean, sigma, size])", "Draw samples from a log-normal distribution.", "logseries(p[, size])", "Draw samples from a logarithmic series distribution.", "multinomial(n, pvals[, size])", "Draw samples from a multinomial distribution.", "multivariate_normal(mean, cov[, size, ...])", "Draw random samples from a multivariate normal distribution.", "negative_binomial(n, p[, size])", "Draw samples from a negative binomial distribution.", "noncentral_chisquare(df, nonc[, size])", "Draw samples from a noncentral chi-square distribution.", "noncentral_f(dfnum, dfden, nonc[, size])", "Draw samples from the noncentral F distribution.", "normal([loc, scale, size])", "Draw random samples from a normal (Gaussian) distribution.", "pareto(a[, size])", "Draw samples from a Pareto II or Lomax distribution with specified shape.", "poisson([lam, size])", "Draw samples from a Poisson distribution.", "power(a[, size])", "Draws samples in [0, 1] from a power distribution with positive exponent a - 1.", "rayleigh([scale, size])", "Draw samples from a Rayleigh distribution.", "standard_cauchy([size])", "Draw samples from a standard Cauchy distribution with mode = 0.", "standard_exponential([size])", "Draw samples from the standard exponential distribution.", "standard_gamma(shape[, size])", "Draw samples from a standard Gamma distribution.", "standard_normal([size])", "Draw samples from a standard Normal distribution (mean=0, stdev=1).", "standard_t(df[, size])", "Draw samples from a standard Student's t distribution with df degrees of freedom.", "triangular(left, mode, right[, size])", "Draw samples from the triangular distribution over the interval [left, right].", "uniform([low, high, size])", "Draw samples from a uniform distribution.", "vonmises(mu, kappa[, size])", "Draw samples from a von Mises distribution.", "wald(mean, scale[, size])", "Draw samples from a Wald, or inverse Gaussian, distribution.", "weibull(a[, size])", "Draw samples from a Weibull distribution.", "zipf(a[, size])", "Draw samples from a Zipf distribution.", "Many of the RandomState methods above are exported as functions in numpy.random This usage is discouraged, as it is implemented via a global RandomState instance which is not advised on two counts:", "For backward compatible legacy reasons, we cannot change this. See Quick Start.", "beta(a, b[, size])", "Draw samples from a Beta distribution.", "binomial(n, p[, size])", "Draw samples from a binomial distribution.", "bytes(length)", "Return random bytes.", "chisquare(df[, size])", "Draw samples from a chi-square distribution.", "choice(a[, size, replace, p])", "Generates a random sample from a given 1-D array", "dirichlet(alpha[, size])", "Draw samples from the Dirichlet distribution.", "exponential([scale, size])", "Draw samples from an exponential distribution.", "f(dfnum, dfden[, size])", "Draw samples from an F distribution.", "gamma(shape[, scale, size])", "Draw samples from a Gamma distribution.", "geometric(p[, size])", "Draw samples from the geometric distribution.", "get_state()", "Return a tuple representing the internal state of the generator.", "gumbel([loc, scale, size])", "Draw samples from a Gumbel distribution.", "hypergeometric(ngood, nbad, nsample[, size])", "Draw samples from a Hypergeometric distribution.", "laplace([loc, scale, size])", "Draw samples from the Laplace or double exponential distribution with specified location (or mean) and scale (decay).", "logistic([loc, scale, size])", "Draw samples from a logistic distribution.", "lognormal([mean, sigma, size])", "Draw samples from a log-normal distribution.", "logseries(p[, size])", "Draw samples from a logarithmic series distribution.", "multinomial(n, pvals[, size])", "Draw samples from a multinomial distribution.", "multivariate_normal(mean, cov[, size, ...])", "Draw random samples from a multivariate normal distribution.", "negative_binomial(n, p[, size])", "Draw samples from a negative binomial distribution.", "noncentral_chisquare(df, nonc[, size])", "Draw samples from a noncentral chi-square distribution.", "noncentral_f(dfnum, dfden, nonc[, size])", "Draw samples from the noncentral F distribution.", "normal([loc, scale, size])", "Draw random samples from a normal (Gaussian) distribution.", "pareto(a[, size])", "Draw samples from a Pareto II or Lomax distribution with specified shape.", "permutation(x)", "Randomly permute a sequence, or return a permuted range.", "poisson([lam, size])", "Draw samples from a Poisson distribution.", "power(a[, size])", "Draws samples in [0, 1] from a power distribution with positive exponent a - 1.", "rand(d0, d1, ..., dn)", "Random values in a given shape.", "randint(low[, high, size, dtype])", "Return random integers from low (inclusive) to high (exclusive).", "randn(d0, d1, ..., dn)", "Return a sample (or samples) from the \"standard normal\" distribution.", "random([size])", "Return random floats in the half-open interval [0.0, 1.0).", "random_integers(low[, high, size])", "Random integers of type np.int_ between low and high, inclusive.", "random_sample([size])", "Return random floats in the half-open interval [0.0, 1.0).", "ranf", "This is an alias of random_sample.", "rayleigh([scale, size])", "Draw samples from a Rayleigh distribution.", "sample", "This is an alias of random_sample.", "seed(self[, seed])", "Reseed a legacy MT19937 BitGenerator", "set_state(state)", "Set the internal state of the generator from a tuple.", "shuffle(x)", "Modify a sequence in-place by shuffling its contents.", "standard_cauchy([size])", "Draw samples from a standard Cauchy distribution with mode = 0.", "standard_exponential([size])", "Draw samples from the standard exponential distribution.", "standard_gamma(shape[, size])", "Draw samples from a standard Gamma distribution.", "standard_normal([size])", "Draw samples from a standard Normal distribution (mean=0, stdev=1).", "standard_t(df[, size])", "Draw samples from a standard Student's t distribution with df degrees of freedom.", "triangular(left, mode, right[, size])", "Draw samples from the triangular distribution over the interval [left, right].", "uniform([low, high, size])", "Draw samples from a uniform distribution.", "vonmises(mu, kappa[, size])", "Draw samples from a von Mises distribution.", "wald(mean, scale[, size])", "Draw samples from a Wald, or inverse Gaussian, distribution.", "weibull(a[, size])", "Draw samples from a Weibull distribution.", "zipf(a[, size])", "Draw samples from a Zipf distribution."]}, {"name": "numpy.random.SeedSequence()", "path": "reference/random/bit_generators/generated/numpy.random.seedsequence", "type": "Random sampling ( \n      \n       numpy.random\n      \n      )", "text": ["SeedSequence mixes sources of entropy in a reproducible way to set the initial state for independent and very probably non-overlapping BitGenerators.", "Once the SeedSequence is instantiated, you can call the generate_state method to get an appropriately sized seed. Calling spawn(n) will create n SeedSequences that can be used to seed independent BitGenerators, i.e. for different threads.", "The entropy for creating a SeedSequence.", "A third source of entropy, used internally when calling SeedSequence.spawn", "Size of the pooled entropy to store. Default is 4 to give a 128-bit entropy pool. 8 (for 256 bits) is another reasonable choice if working with larger PRNGs, but there is very little to be gained by selecting another value.", "The number of children already spawned. Only pass this if reconstructing a SeedSequence from a serialized form.", "Best practice for achieving reproducible bit streams is to use the default None for the initial entropy, and then use SeedSequence.entropy to log/pickle the entropy for reproducibility:", "generate_state(n_words[, dtype])", "Return the requested number of words for PRNG seeding.", "spawn(n_children)", "Spawn a number of child SeedSequence s by extending the spawn_key."]}, {"name": "numpy.random.SFC64()", "path": "reference/random/bit_generators/sfc64", "type": "SFC64", "text": ["BitGenerator for Chris Doty-Humphrey\u2019s Small Fast Chaotic PRNG.", "A seed to initialize the BitGenerator. If None, then fresh, unpredictable entropy will be pulled from the OS. If an int or array_like[ints] is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state. One may also pass in a SeedSequence instance.", "SFC64 is a 256-bit implementation of Chris Doty-Humphrey\u2019s Small Fast Chaotic PRNG ([1]). SFC64 has a few different cycles that one might be on, depending on the seed; the expected period will be about \\(2^{255}\\) ([2]). SFC64 incorporates a 64-bit counter which means that the absolute minimum cycle length is \\(2^{64}\\) and that distinct seeds will not run into each other for at least \\(2^{64}\\) iterations.", "SFC64 provides a capsule containing function pointers that produce doubles, and unsigned 32 and 64- bit integers. These are not directly consumable in Python and must be consumed by a Generator or similar object that supports low-level access.", "State and Seeding", "The SFC64 state vector consists of 4 unsigned 64-bit values. The last is a 64-bit counter that increments by 1 each iteration.", "The input seed is processed by SeedSequence to generate the first 3 values, then the SFC64 algorithm is iterated a small number of times to mix.", "Compatibility Guarantee", "SFC64 makes a guarantee that a fixed seed will always produce the same random integer stream.", "\u201cPractRand\u201d", "\u201cRandom Invertible Mapping Statistics\u201d", "state", "Get or set the PRNG state", "cffi", "CFFI interface", "ctypes", "ctypes interface"]}, {"name": "numpy.RankWarning", "path": "reference/generated/numpy.rankwarning", "type": "numpy.RankWarning", "text": ["Issued by polyfit when the Vandermonde matrix is rank deficient.", "For more information, a way to suppress the warning, and an example of RankWarning being issued, see polyfit."]}, {"name": "numpy.ravel()", "path": "reference/generated/numpy.ravel", "type": "numpy.ravel", "text": ["Return a contiguous flattened array.", "A 1-D array, containing the elements of the input, is returned. A copy is made only if needed.", "As of NumPy 1.10, the returned array will have the same type as the input array. (for example, a masked array will be returned for a masked array input)", "Input array. The elements in a are read in the order specified by order, and packed as a 1-D array.", "The elements of a are read using this index order. \u2018C\u2019 means to index the elements in row-major, C-style order, with the last axis index changing fastest, back to the first axis index changing slowest. \u2018F\u2019 means to index the elements in column-major, Fortran-style order, with the first index changing fastest, and the last index changing slowest. Note that the \u2018C\u2019 and \u2018F\u2019 options take no account of the memory layout of the underlying array, and only refer to the order of axis indexing. \u2018A\u2019 means to read the elements in Fortran-like index order if a is Fortran contiguous in memory, C-like order otherwise. \u2018K\u2019 means to read the elements in the order they occur in memory, except for reversing the data when strides are negative. By default, \u2018C\u2019 index order is used.", "y is an array of the same subtype as a, with shape (a.size,). Note that matrices are special cased for backward compatibility, if a is a matrix, then y is a 1-D ndarray.", "See also", "1-D iterator over an array.", "1-D array copy of the elements of an array in row-major order.", "Change the shape of an array without changing its data.", "In row-major, C-style order, in two dimensions, the row index varies the slowest, and the column index the quickest. This can be generalized to multiple dimensions, where row-major order implies that the index along the first axis varies slowest, and the index along the last quickest. The opposite holds for column-major, Fortran-style index ordering.", "When a view is desired in as many cases as possible, arr.reshape(-1) may be preferable.", "It is equivalent to reshape(-1, order=order).", "When order is \u2018A\u2019, it will preserve the array\u2019s \u2018C\u2019 or \u2018F\u2019 ordering:", "When order is \u2018K\u2019, it will preserve orderings that are neither \u2018C\u2019 nor \u2018F\u2019, but won\u2019t reverse axes:"]}, {"name": "numpy.ravel_multi_index()", "path": "reference/generated/numpy.ravel_multi_index", "type": "numpy.ravel_multi_index", "text": ["Converts a tuple of index arrays into an array of flat indices, applying boundary modes to the multi-index.", "A tuple of integer arrays, one array for each dimension.", "The shape of array into which the indices from multi_index apply.", "Specifies how out-of-bounds indices are handled. Can specify either one mode or a tuple of modes, one mode per index.", "In \u2018clip\u2019 mode, a negative index which would normally wrap will clip to 0 instead.", "Determines whether the multi-index should be viewed as indexing in row-major (C-style) or column-major (Fortran-style) order.", "An array of indices into the flattened version of an array of dimensions dims.", "See also", "New in version 1.6.0."]}, {"name": "numpy.real()", "path": "reference/generated/numpy.real", "type": "numpy.real", "text": ["Return the real part of the complex argument.", "Input array.", "The real component of the complex argument. If val is real, the type of val is used for the output. If val has complex elements, the returned type is float.", "See also"]}, {"name": "numpy.real_if_close()", "path": "reference/generated/numpy.real_if_close", "type": "numpy.real_if_close", "text": ["If input is complex with all imaginary parts close to zero, return real parts.", "\u201cClose to zero\u201d is defined as tol * (machine epsilon of the type for a).", "Input array.", "Tolerance in machine epsilons for the complex part of the elements in the array.", "If a is real, the type of a is used for the output. If a has complex elements, the returned type is float.", "See also", "Machine epsilon varies from machine to machine and between data types but Python floats on most platforms have a machine epsilon equal to 2.2204460492503131e-16. You can use \u2018np.finfo(float).eps\u2019 to print out the machine epsilon for floats."]}, {"name": "numpy.recarray()", "path": "reference/generated/numpy.recarray", "type": "numpy.recarray", "text": ["Construct an ndarray that allows field access using attributes.", "Arrays may have a data-types containing fields, analogous to columns in a spread sheet. An example is [(x, int), (y, float)], where each entry in the array is a pair of (int, float). Normally, these attributes are accessed using dictionary lookups such as arr['x'] and arr['y']. Record arrays allow the fields to be accessed as members of the array, using arr.x and arr.y.", "Shape of output array.", "The desired data-type. By default, the data-type is determined from formats, names, titles, aligned and byteorder.", "A list containing the data-types for the different columns, e.g. ['i4', 'f8', 'i4']. formats does not support the new convention of using types directly, i.e. (int, float, int). Note that formats must be a list, not a tuple. Given that formats is somewhat limited, we recommend specifying dtype instead.", "The name of each column, e.g. ('x', 'y', 'z').", "By default, a new array is created of the given shape and data-type. If buf is specified and is an object exposing the buffer interface, the array will use the memory from the existing buffer. In this case, the offset and strides keywords are available.", "Empty array of the given shape and type.", "Aliases for column names. For example, if names were ('x', 'y', 'z') and titles is ('x_coordinate', 'y_coordinate', 'z_coordinate'), then arr['x'] is equivalent to both arr.x and arr.x_coordinate.", "Byte-order for all fields.", "Align the fields in memory as the C-compiler would.", "Buffer (buf) is interpreted according to these strides (strides define how many bytes each array element, row, column, etc. occupy in memory).", "Start reading buffer (buf) from this offset onwards.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct a record array from data.", "fundamental data-type for recarray.", "determine a data-type from formats, names, titles.", "This constructor can be compared to empty: it creates a new record array but does not fill it with data. To create a record array from data, use one of the following methods:", "Create an array with two fields, x and y:", "View the array as a record array:", "Create a new, empty record array:", "The transposed array.", "Base object if memory is from some other object.", "An object to simplify the interaction of the array with the ctypes module.", "Python buffer object pointing to the start of the array\u2019s data.", "Data-type of the array\u2019s elements.", "Information about the memory layout of the array.", "A 1-D iterator over the array.", "The imaginary part of the array.", "Length of one array element in bytes.", "Total bytes consumed by the elements of the array.", "Number of array dimensions.", "The real part of the array.", "Tuple of array dimensions.", "Number of elements in the array.", "Tuple of bytes to step in each dimension when traversing an array.", "all([axis, out, keepdims, where])", "Returns True if all elements evaluate to True.", "any([axis, out, keepdims, where])", "Returns True if any of the elements of a evaluate to True.", "argmax([axis, out])", "Return indices of the maximum values along the given axis.", "argmin([axis, out])", "Return indices of the minimum values along the given axis.", "argpartition(kth[, axis, kind, order])", "Returns the indices that would partition this array.", "argsort([axis, kind, order])", "Returns the indices that would sort this array.", "astype(dtype[, order, casting, subok, copy])", "Copy of the array, cast to a specified type.", "byteswap([inplace])", "Swap the bytes of the array elements", "choose(choices[, out, mode])", "Use an index array to construct a new array from a set of choices.", "clip([min, max, out])", "Return an array whose values are limited to [min, max].", "compress(condition[, axis, out])", "Return selected slices of this array along given axis.", "conj()", "Complex-conjugate all elements.", "conjugate()", "Return the complex conjugate, element-wise.", "copy([order])", "Return a copy of the array.", "cumprod([axis, dtype, out])", "Return the cumulative product of the elements along the given axis.", "cumsum([axis, dtype, out])", "Return the cumulative sum of the elements along the given axis.", "diagonal([offset, axis1, axis2])", "Return specified diagonals.", "dump(file)", "Dump a pickle of the array to the specified file.", "dumps()", "Returns the pickle of the array as a string.", "fill(value)", "Fill the array with a scalar value.", "flatten([order])", "Return a copy of the array collapsed into one dimension.", "getfield(dtype[, offset])", "Returns a field of the given array as a certain type.", "item(*args)", "Copy an element of an array to a standard Python scalar and return it.", "itemset(*args)", "Insert scalar into an array (scalar is cast to array's dtype, if possible)", "max([axis, out, keepdims, initial, where])", "Return the maximum along a given axis.", "mean([axis, dtype, out, keepdims, where])", "Returns the average of the array elements along given axis.", "min([axis, out, keepdims, initial, where])", "Return the minimum along a given axis.", "newbyteorder([new_order])", "Return the array with the same data viewed with a different byte order.", "nonzero()", "Return the indices of the elements that are non-zero.", "partition(kth[, axis, kind, order])", "Rearranges the elements in the array in such a way that the value of the element in kth position is in the position it would be in a sorted array.", "prod([axis, dtype, out, keepdims, initial, ...])", "Return the product of the array elements over the given axis", "ptp([axis, out, keepdims])", "Peak to peak (maximum - minimum) value along a given axis.", "put(indices, values[, mode])", "Set a.flat[n] = values[n] for all n in indices.", "ravel([order])", "Return a flattened array.", "repeat(repeats[, axis])", "Repeat elements of an array.", "reshape(shape[, order])", "Returns an array containing the same data with a new shape.", "resize(new_shape[, refcheck])", "Change shape and size of array in-place.", "round([decimals, out])", "Return a with each element rounded to the given number of decimals.", "searchsorted(v[, side, sorter])", "Find indices where elements of v should be inserted in a to maintain order.", "setfield(val, dtype[, offset])", "Put a value into a specified place in a field defined by a data-type.", "setflags([write, align, uic])", "Set array flags WRITEABLE, ALIGNED, (WRITEBACKIFCOPY and UPDATEIFCOPY), respectively.", "sort([axis, kind, order])", "Sort an array in-place.", "squeeze([axis])", "Remove axes of length one from a.", "std([axis, dtype, out, ddof, keepdims, where])", "Returns the standard deviation of the array elements along given axis.", "sum([axis, dtype, out, keepdims, initial, where])", "Return the sum of the array elements over the given axis.", "swapaxes(axis1, axis2)", "Return a view of the array with axis1 and axis2 interchanged.", "take(indices[, axis, out, mode])", "Return an array formed from the elements of a at the given indices.", "tobytes([order])", "Construct Python bytes containing the raw data bytes in the array.", "tofile(fid[, sep, format])", "Write array to a file as text or binary (default).", "tolist()", "Return the array as an a.ndim-levels deep nested list of Python scalars.", "tostring([order])", "A compatibility alias for tobytes, with exactly the same behavior.", "trace([offset, axis1, axis2, dtype, out])", "Return the sum along diagonals of the array.", "transpose(*axes)", "Returns a view of the array with axes transposed.", "var([axis, dtype, out, ddof, keepdims, where])", "Returns the variance of the array elements, along given axis.", "view([dtype][, type])", "New view of array with the same data.", "dot", "field"]}, {"name": "numpy.reciprocal()", "path": "reference/generated/numpy.reciprocal", "type": "numpy.reciprocal", "text": ["Return the reciprocal of the argument, element-wise.", "Calculates 1/x.", "Input array.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Return array. This is a scalar if x is a scalar.", "Note", "This function is not designed to work with integers.", "For integer arguments with absolute value larger than 1 the result is always zero because of the way Python handles integer division. For integer zero the result is an overflow."]}, {"name": "numpy.record", "path": "reference/generated/numpy.record", "type": "numpy.record", "text": ["A data-type scalar that allows field access as attribute lookup.", "Scalar attribute identical to the corresponding array attribute.", "base object", "Pointer to start of data.", "dtype object", "integer value of flags", "A 1-D view of the scalar.", "The imaginary part of the scalar.", "The length of one element in bytes.", "The length of the scalar in bytes.", "The number of array dimensions.", "The real part of the scalar.", "Tuple of array dimensions.", "The number of elements in the gentype.", "Tuple of bytes steps in each dimension.", "all", "Scalar method identical to the corresponding array attribute.", "any", "Scalar method identical to the corresponding array attribute.", "argmax", "Scalar method identical to the corresponding array attribute.", "argmin", "Scalar method identical to the corresponding array attribute.", "argsort", "Scalar method identical to the corresponding array attribute.", "astype", "Scalar method identical to the corresponding array attribute.", "byteswap", "Scalar method identical to the corresponding array attribute.", "choose", "Scalar method identical to the corresponding array attribute.", "clip", "Scalar method identical to the corresponding array attribute.", "compress", "Scalar method identical to the corresponding array attribute.", "conjugate", "Scalar method identical to the corresponding array attribute.", "copy", "Scalar method identical to the corresponding array attribute.", "cumprod", "Scalar method identical to the corresponding array attribute.", "cumsum", "Scalar method identical to the corresponding array attribute.", "diagonal", "Scalar method identical to the corresponding array attribute.", "dump", "Scalar method identical to the corresponding array attribute.", "dumps", "Scalar method identical to the corresponding array attribute.", "fill", "Scalar method identical to the corresponding array attribute.", "flatten", "Scalar method identical to the corresponding array attribute.", "getfield", "Scalar method identical to the corresponding array attribute.", "item", "Scalar method identical to the corresponding array attribute.", "itemset", "Scalar method identical to the corresponding array attribute.", "max", "Scalar method identical to the corresponding array attribute.", "mean", "Scalar method identical to the corresponding array attribute.", "min", "Scalar method identical to the corresponding array attribute.", "newbyteorder([new_order])", "Return a new dtype with a different byte order.", "nonzero", "Scalar method identical to the corresponding array attribute.", "pprint()", "Pretty-print all fields.", "prod", "Scalar method identical to the corresponding array attribute.", "ptp", "Scalar method identical to the corresponding array attribute.", "put", "Scalar method identical to the corresponding array attribute.", "ravel", "Scalar method identical to the corresponding array attribute.", "repeat", "Scalar method identical to the corresponding array attribute.", "reshape", "Scalar method identical to the corresponding array attribute.", "resize", "Scalar method identical to the corresponding array attribute.", "round", "Scalar method identical to the corresponding array attribute.", "searchsorted", "Scalar method identical to the corresponding array attribute.", "setfield", "Scalar method identical to the corresponding array attribute.", "setflags", "Scalar method identical to the corresponding array attribute.", "sort", "Scalar method identical to the corresponding array attribute.", "squeeze", "Scalar method identical to the corresponding array attribute.", "std", "Scalar method identical to the corresponding array attribute.", "sum", "Scalar method identical to the corresponding array attribute.", "swapaxes", "Scalar method identical to the corresponding array attribute.", "take", "Scalar method identical to the corresponding array attribute.", "tofile", "Scalar method identical to the corresponding array attribute.", "tolist", "Scalar method identical to the corresponding array attribute.", "tostring", "Scalar method identical to the corresponding array attribute.", "trace", "Scalar method identical to the corresponding array attribute.", "transpose", "Scalar method identical to the corresponding array attribute.", "var", "Scalar method identical to the corresponding array attribute.", "view", "Scalar method identical to the corresponding array attribute.", "conj", "tobytes"]}, {"name": "numpy.remainder()", "path": "reference/generated/numpy.remainder", "type": "numpy.remainder", "text": ["Returns the element-wise remainder of division.", "Computes the remainder complementary to the floor_divide function. It is equivalent to the Python modulus operator``x1 % x2`` and has the same sign as the divisor x2. The MATLAB function equivalent to np.remainder is mod.", "Warning", "This should not be confused with:", "Dividend array.", "Divisor array. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The element-wise remainder of the quotient floor_divide(x1, x2). This is a scalar if both x1 and x2 are scalars.", "See also", "Equivalent of Python // operator.", "Simultaneous floor division and remainder.", "Equivalent of the MATLAB rem function.", "Returns 0 when x2 is 0 and both x1 and x2 are (arrays of) integers. mod is an alias of remainder.", "The % operator can be used as a shorthand for np.remainder on ndarrays."]}, {"name": "numpy.repeat()", "path": "reference/generated/numpy.repeat", "type": "numpy.repeat", "text": ["Repeat elements of an array.", "Input array.", "The number of repetitions for each element. repeats is broadcasted to fit the shape of the given axis.", "The axis along which to repeat values. By default, use the flattened input array, and return a flat output array.", "Output array which has the same shape as a, except along the given axis.", "See also", "Tile an array.", "Find the unique elements of an array."]}, {"name": "numpy.require()", "path": "reference/generated/numpy.require", "type": "numpy.require", "text": ["Return an ndarray of the provided type that satisfies requirements.", "This function is useful to be sure that an array with the correct flags is returned for passing to compiled code (perhaps through ctypes).", "The object to be converted to a type-and-requirement-satisfying array.", "The required data-type. If None preserve the current dtype. If your application requires the data to be in native byteorder, include a byteorder specification as a part of the dtype specification.", "The requirements list can be any of the following", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "Array with specified requirements and type if given.", "See also", "Convert input to an ndarray.", "Convert to an ndarray, but pass through ndarray subclasses.", "Convert input to a contiguous array.", "Convert input to an ndarray with column-major memory order.", "Information about the memory layout of the array.", "The returned array will be guaranteed to have the listed requirements by making a copy if needed."]}, {"name": "numpy.reshape()", "path": "reference/generated/numpy.reshape", "type": "numpy.reshape", "text": ["Gives a new shape to an array without changing its data.", "Array to be reshaped.", "The new shape should be compatible with the original shape. If an integer, then the result will be a 1-D array of that length. One shape dimension can be -1. In this case, the value is inferred from the length of the array and remaining dimensions.", "Read the elements of a using this index order, and place the elements into the reshaped array using this index order. \u2018C\u2019 means to read / write the elements using C-like index order, with the last axis index changing fastest, back to the first axis index changing slowest. \u2018F\u2019 means to read / write the elements using Fortran-like index order, with the first index changing fastest, and the last index changing slowest. Note that the \u2018C\u2019 and \u2018F\u2019 options take no account of the memory layout of the underlying array, and only refer to the order of indexing. \u2018A\u2019 means to read / write the elements in Fortran-like index order if a is Fortran contiguous in memory, C-like order otherwise.", "This will be a new view object if possible; otherwise, it will be a copy. Note there is no guarantee of the memory layout (C- or Fortran- contiguous) of the returned array.", "See also", "Equivalent method.", "It is not always possible to change the shape of an array without copying the data. If you want an error to be raised when the data is copied, you should assign the new shape to the shape attribute of the array:", "The order keyword gives the index ordering both for fetching the values from a, and then placing the values into the output array. For example, let\u2019s say you have an array:", "You can think of reshaping as first raveling the array (using the given index order), then inserting the elements from the raveled array into the new array using the same kind of index ordering as was used for the raveling."]}, {"name": "numpy.resize()", "path": "reference/generated/numpy.resize", "type": "numpy.resize", "text": ["Return a new array with the specified shape.", "If the new array is larger than the original array, then the new array is filled with repeated copies of a. Note that this behavior is different from a.resize(new_shape) which fills with zeros instead of repeated copies of a.", "Array to be resized.", "Shape of resized array.", "The new array is formed from the data in the old array, repeated if necessary to fill out the required number of elements. The data are repeated iterating over the array in C-order.", "See also", "Reshape an array without changing the total size.", "Enlarge and pad an array.", "Repeat elements of an array.", "resize an array in-place.", "When the total size of the array does not change reshape should be used. In most other cases either indexing (to reduce the size) or padding (to increase the size) may be a more appropriate solution.", "Warning: This functionality does not consider axes separately, i.e. it does not apply interpolation/extrapolation. It fills the return array with the required number of elements, iterating over a in C-order, disregarding axes (and cycling back from the start if the new shape is larger). This functionality is therefore not suitable to resize images, or data where each axis represents a separate and distinct entity."]}, {"name": "numpy.result_type()", "path": "reference/generated/numpy.result_type", "type": "numpy.result_type", "text": ["Returns the type that results from applying the NumPy type promotion rules to the arguments.", "Type promotion in NumPy works similarly to the rules in languages like C++, with some slight differences. When both scalars and arrays are used, the array\u2019s type takes precedence and the actual value of the scalar is taken into account.", "For example, calculating 3*a, where a is an array of 32-bit floats, intuitively should result in a 32-bit float output. If the 3 is a 32-bit integer, the NumPy rules indicate it can\u2019t convert losslessly into a 32-bit float, so a 64-bit float should be the result type. By examining the value of the constant, \u20183\u2019, we see that it fits in an 8-bit integer, which can be cast losslessly into the 32-bit float.", "The operands of some operation whose result type is needed.", "The result type.", "See also", "New in version 1.6.0.", "The specific algorithm used is as follows.", "Categories are determined by first checking which of boolean, integer (int/uint), or floating point (float/complex) the maximum kind of all the arrays and the scalars are.", "If there are only scalars or the maximum category of the scalars is higher than the maximum category of the arrays, the data types are combined with promote_types to produce the return value.", "Otherwise, min_scalar_type is called on each array, and the resulting data types are all combined with promote_types to produce the return value.", "The set of int values is not a subset of the uint values for types with the same number of bits, something not reflected in min_scalar_type, but handled as a special case in result_type."]}, {"name": "numpy.right_shift()", "path": "reference/generated/numpy.right_shift", "type": "numpy.right_shift", "text": ["Shift the bits of an integer to the right.", "Bits are shifted to the right x2. Because the internal representation of numbers is in binary format, this operation is equivalent to dividing x1 by 2**x2.", "Input values.", "Number of bits to remove at the right of x1. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Return x1 with bits shifted x2 times to the right. This is a scalar if both x1 and x2 are scalars.", "See also", "Shift the bits of an integer to the left.", "Return the binary representation of the input number as a string.", "The >> operator can be used as a shorthand for np.right_shift on ndarrays."]}, {"name": "numpy.rint()", "path": "reference/generated/numpy.rint", "type": "numpy.rint", "text": ["Round elements of the array to the nearest integer.", "Input array.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Output array is same shape and type as x. This is a scalar if x is a scalar.", "See also", "For values exactly halfway between rounded decimal values, NumPy rounds to the nearest even value. Thus 1.5 and 2.5 round to 2.0, -0.5 and 0.5 round to 0.0, etc."]}, {"name": "numpy.roll()", "path": "reference/generated/numpy.roll", "type": "numpy.roll", "text": ["Roll array elements along a given axis.", "Elements that roll beyond the last position are re-introduced at the first.", "Input array.", "The number of places by which elements are shifted. If a tuple, then axis must be a tuple of the same size, and each of the given axes is shifted by the corresponding number. If an int while axis is a tuple of ints, then the same value is used for all given axes.", "Axis or axes along which elements are shifted. By default, the array is flattened before shifting, after which the original shape is restored.", "Output array, with the same shape as a.", "See also", "Roll the specified axis backwards, until it lies in a given position.", "New in version 1.12.0.", "Supports rolling over multiple dimensions simultaneously."]}, {"name": "numpy.rollaxis()", "path": "reference/generated/numpy.rollaxis", "type": "numpy.rollaxis", "text": ["Roll the specified axis backwards, until it lies in a given position.", "This function continues to be supported for backward compatibility, but you should prefer moveaxis. The moveaxis function was added in NumPy 1.11.", "Input array.", "The axis to be rolled. The positions of the other axes do not change relative to one another.", "When start <= axis, the axis is rolled back until it lies in this position. When start > axis, the axis is rolled until it lies before this position. The default, 0, results in a \u201ccomplete\u201d roll. The following table describes how negative values of start are interpreted:", "start", "Normalized start", "-(arr.ndim+1)", "raise AxisError", "-arr.ndim", "0", "\u22ee", "\u22ee", "-1", "arr.ndim-1", "0", "0", "\u22ee", "\u22ee", "arr.ndim", "arr.ndim", "arr.ndim + 1", "raise AxisError", "For NumPy >= 1.10.0 a view of a is always returned. For earlier NumPy versions a view of a is returned only if the order of the axes is changed, otherwise the input array is returned.", "See also", "Move array axes to new positions.", "Roll the elements of an array by a number of positions along a given axis."]}, {"name": "numpy.roots()", "path": "reference/generated/numpy.roots", "type": "numpy.roots", "text": ["Return the roots of a polynomial with coefficients given in p.", "Note", "This forms part of the old polynomial API. Since version 1.4, the new polynomial API defined in numpy.polynomial is preferred. A summary of the differences can be found in the transition guide.", "The values in the rank-1 array p are coefficients of a polynomial. If the length of p is n+1 then the polynomial is described by:", "Rank-1 array of polynomial coefficients.", "An array containing the roots of the polynomial.", "When p cannot be converted to a rank-1 array.", "See also", "Find the coefficients of a polynomial with a given sequence of roots.", "Compute polynomial values.", "Least squares polynomial fit.", "A one-dimensional polynomial class.", "The algorithm relies on computing the eigenvalues of the companion matrix [1].", "R. A. Horn & C. R. Johnson, Matrix Analysis. Cambridge, UK: Cambridge University Press, 1999, pp. 146-7."]}, {"name": "numpy.rot90()", "path": "reference/generated/numpy.rot90", "type": "numpy.rot90", "text": ["Rotate an array by 90 degrees in the plane specified by axes.", "Rotation direction is from the first towards the second axis.", "Array of two or more dimensions.", "Number of times the array is rotated by 90 degrees.", "The array is rotated in the plane defined by the axes. Axes must be different.", "New in version 1.12.0.", "A rotated view of m.", "See also", "Reverse the order of elements in an array along the given axis.", "Flip an array horizontally.", "Flip an array vertically.", "rot90(m, k=1, axes=(1,0)) is the reverse of rot90(m, k=1, axes=(0,1))", "rot90(m, k=1, axes=(1,0)) is equivalent to rot90(m, k=-1, axes=(0,1))"]}, {"name": "numpy.round_()", "path": "reference/generated/numpy.round_", "type": "numpy.round_", "text": ["Round an array to the given number of decimals.", "See also", "equivalent function; see for details."]}, {"name": "numpy.row_stack()", "path": "reference/generated/numpy.row_stack", "type": "numpy.row_stack", "text": ["Stack arrays in sequence vertically (row wise).", "This is equivalent to concatenation along the first axis after 1-D arrays of shape (N,) have been reshaped to (1,N). Rebuilds arrays divided by vsplit.", "This function makes most sense for arrays with up to 3 dimensions. For instance, for pixel-data with a height (first axis), width (second axis), and r/g/b channels (third axis). The functions concatenate, stack and block provide more general stacking and concatenation operations.", "The arrays must have the same shape along all but the first axis. 1-D arrays must have the same length.", "The array formed by stacking the given arrays, will be at least 2-D.", "See also", "Join a sequence of arrays along an existing axis.", "Join a sequence of arrays along a new axis.", "Assemble an nd-array from nested lists of blocks.", "Stack arrays in sequence horizontally (column wise).", "Stack arrays in sequence depth wise (along third axis).", "Stack 1-D arrays as columns into a 2-D array.", "Split an array into multiple sub-arrays vertically (row-wise)."]}, {"name": "numpy.s_", "path": "reference/generated/numpy.s_", "type": "numpy.s_", "text": ["A nicer way to build up index tuples for arrays.", "Note", "Use one of the two predefined instances index_exp or s_ rather than directly using IndexExpression.", "For any index combination, including slicing and axis insertion, a[indices] is the same as a[np.index_exp[indices]] for any array a. However, np.index_exp[indices] can be used anywhere in Python code and returns a tuple of slice objects that can be used in the construction of complex index expressions.", "If True, always returns a tuple.", "See also", "Predefined instance that always returns a tuple: index_exp = IndexExpression(maketuple=True).", "Predefined instance without tuple conversion: s_ = IndexExpression(maketuple=False).", "You can do all this with slice() plus a few special objects, but there\u2019s a lot to remember and this version is simpler because it uses the standard array indexing syntax."]}, {"name": "numpy.save()", "path": "reference/generated/numpy.save", "type": "numpy.save", "text": ["Save an array to a binary file in NumPy .npy format.", "File or filename to which the data is saved. If file is a file-object, then the filename is unchanged. If file is a string or Path, a .npy extension will be appended to the filename if it does not already have one.", "Array data to be saved.", "Allow saving object arrays using Python pickles. Reasons for disallowing pickles include security (loading pickled data can execute arbitrary code) and portability (pickled objects may not be loadable on different Python installations, for example if the stored objects require libraries that are not available, and not all pickled data is compatible between Python 2 and Python 3). Default: True", "Only useful in forcing objects in object arrays on Python 3 to be pickled in a Python 2 compatible way. If fix_imports is True, pickle will try to map the new Python 3 names to the old module names used in Python 2, so that the pickle data stream is readable with Python 2.", "See also", "Save several arrays into a .npz archive", "For a description of the .npy format, see numpy.lib.format.", "Any data saved to the file is appended to the end of the file."]}, {"name": "numpy.savetxt()", "path": "reference/generated/numpy.savetxt", "type": "numpy.savetxt", "text": ["Save an array to a text file.", "If the filename ends in .gz, the file is automatically saved in compressed gzip format. loadtxt understands gzipped files transparently.", "Data to be saved to a text file.", "A single format (%10.5f), a sequence of formats, or a multi-format string, e.g. \u2018Iteration %d \u2013 %10.5f\u2019, in which case delimiter is ignored. For complex X, the legal options for fmt are:", "String or character separating columns.", "String or character separating lines.", "New in version 1.5.0.", "String that will be written at the beginning of the file.", "New in version 1.7.0.", "String that will be written at the end of the file.", "New in version 1.7.0.", "String that will be prepended to the header and footer strings, to mark them as comments. Default: \u2018# \u2018, as expected by e.g. numpy.loadtxt.", "New in version 1.7.0.", "Encoding used to encode the outputfile. Does not apply to output streams. If the encoding is something other than \u2018bytes\u2019 or \u2018latin1\u2019 you will not be able to load the file in NumPy versions < 1.14. Default is \u2018latin1\u2019.", "New in version 1.14.0.", "See also", "Save an array to a binary file in NumPy .npy format", "Save several arrays into an uncompressed .npz archive", "Save several arrays into a compressed .npz archive", "Further explanation of the fmt parameter (%[flag]width[.precision]specifier):", "- : left justify", "+ : Forces to precede result with + or -.", "0 : Left pad the number with zeros instead of space (see width).", "Minimum number of characters to be printed. The value is not truncated if it has more characters.", "c : character", "d or i : signed decimal integer", "e or E : scientific notation with e or E.", "f : decimal floating point", "g,G : use the shorter of e,E or f", "o : signed octal", "s : string of characters", "u : unsigned decimal integer", "x,X : unsigned hexadecimal integer", "This explanation of fmt is not complete, for an exhaustive specification see [1].", "Format Specification Mini-Language, Python Documentation."]}, {"name": "numpy.savez()", "path": "reference/generated/numpy.savez", "type": "numpy.savez", "text": ["Save several arrays into a single file in uncompressed .npz format.", "Provide arrays as keyword arguments to store them under the corresponding name in the output file: savez(fn, x=x, y=y).", "If arrays are specified as positional arguments, i.e., savez(fn,\nx, y), their names will be arr_0, arr_1, etc.", "Either the filename (string) or an open file (file-like object) where the data will be saved. If file is a string or a Path, the .npz extension will be appended to the filename if it is not already there.", "Arrays to save to the file. Please use keyword arguments (see kwds below) to assign names to arrays. Arrays specified as args will be named \u201carr_0\u201d, \u201carr_1\u201d, and so on.", "Arrays to save to the file. Each array will be saved to the output file with its corresponding keyword name.", "See also", "Save a single array to a binary file in NumPy format.", "Save an array to a file as plain text.", "Save several arrays into a compressed .npz archive", "The .npz file format is a zipped archive of files named after the variables they contain. The archive is not compressed and each file in the archive contains one variable in .npy format. For a description of the .npy format, see numpy.lib.format.", "When opening the saved .npz file with load a NpzFile object is returned. This is a dictionary-like object which can be queried for its list of arrays (with the .files attribute), and for the arrays themselves.", "Keys passed in kwds are used as filenames inside the ZIP archive. Therefore, keys should be valid filenames; e.g., avoid keys that begin with / or contain ..", "When naming variables with keyword arguments, it is not possible to name a variable file, as this would cause the file argument to be defined twice in the call to savez.", "Using savez with *args, the arrays are saved with default names.", "Using savez with **kwds, the arrays are saved with the keyword names."]}, {"name": "numpy.savez_compressed()", "path": "reference/generated/numpy.savez_compressed", "type": "numpy.savez_compressed", "text": ["Save several arrays into a single file in compressed .npz format.", "Provide arrays as keyword arguments to store them under the corresponding name in the output file: savez(fn, x=x, y=y).", "If arrays are specified as positional arguments, i.e., savez(fn,\nx, y), their names will be arr_0, arr_1, etc.", "Either the filename (string) or an open file (file-like object) where the data will be saved. If file is a string or a Path, the .npz extension will be appended to the filename if it is not already there.", "Arrays to save to the file. Please use keyword arguments (see kwds below) to assign names to arrays. Arrays specified as args will be named \u201carr_0\u201d, \u201carr_1\u201d, and so on.", "Arrays to save to the file. Each array will be saved to the output file with its corresponding keyword name.", "See also", "Save a single array to a binary file in NumPy format.", "Save an array to a file as plain text.", "Save several arrays into an uncompressed .npz file format", "Load the files created by savez_compressed.", "The .npz file format is a zipped archive of files named after the variables they contain. The archive is compressed with zipfile.ZIP_DEFLATED and each file in the archive contains one variable in .npy format. For a description of the .npy format, see numpy.lib.format.", "When opening the saved .npz file with load a NpzFile object is returned. This is a dictionary-like object which can be queried for its list of arrays (with the .files attribute), and for the arrays themselves."]}, {"name": "numpy.sctype2char()", "path": "reference/generated/numpy.sctype2char", "type": "numpy.sctype2char", "text": ["Return the string representation of a scalar dtype.", "If a scalar dtype, the corresponding string character is returned. If an object, sctype2char tries to infer its scalar type and then return the corresponding string character.", "The string character corresponding to the scalar type.", "If sctype is an object for which the type can not be inferred.", "See also"]}, {"name": "numpy.searchsorted()", "path": "reference/generated/numpy.searchsorted", "type": "numpy.searchsorted", "text": ["Find indices where elements should be inserted to maintain order.", "Find the indices into a sorted array a such that, if the corresponding elements in v were inserted before the indices, the order of a would be preserved.", "Assuming that a is sorted:", "side", "returned index i satisfies", "left", "a[i-1] < v <= a[i]", "right", "a[i-1] <= v < a[i]", "Input array. If sorter is None, then it must be sorted in ascending order, otherwise sorter must be an array of indices that sort it.", "Values to insert into a.", "If \u2018left\u2019, the index of the first suitable location found is given. If \u2018right\u2019, return the last such index. If there is no suitable index, return either 0 or N (where N is the length of a).", "Optional array of integer indices that sort array a into ascending order. They are typically the result of argsort.", "New in version 1.7.0.", "Array of insertion points with the same shape as v, or an integer if v is a scalar.", "See also", "Return a sorted copy of an array.", "Produce histogram from 1-D data.", "Binary search is used to find the required insertion points.", "As of NumPy 1.4.0 searchsorted works with real/complex arrays containing nan values. The enhanced sort order is documented in sort.", "This function uses the same algorithm as the builtin python bisect.bisect_left (side='left') and bisect.bisect_right (side='right') functions, which is also vectorized in the v argument."]}, {"name": "numpy.select()", "path": "reference/generated/numpy.select", "type": "numpy.select", "text": ["Return an array drawn from elements in choicelist, depending on conditions.", "The list of conditions which determine from which array in choicelist the output elements are taken. When multiple conditions are satisfied, the first one encountered in condlist is used.", "The list of arrays from which the output elements are taken. It has to be of the same length as condlist.", "The element inserted in output when all conditions evaluate to False.", "The output at position m is the m-th element of the array in choicelist where the m-th element of the corresponding array in condlist is True.", "See also", "Return elements from one of two arrays depending on condition."]}, {"name": "numpy.set_printoptions()", "path": "reference/generated/numpy.set_printoptions", "type": "numpy.set_printoptions", "text": ["Set printing options.", "These options determine the way floating point numbers, arrays and other NumPy objects are displayed.", "Number of digits of precision for floating point output (default 8). May be None if floatmode is not fixed, to print as many digits as necessary to uniquely specify the value.", "Total number of array elements which trigger summarization rather than full repr (default 1000). To always use the full repr without summarization, pass sys.maxsize.", "Number of array items in summary at beginning and end of each dimension (default 3).", "The number of characters per line for the purpose of inserting line breaks (default 75).", "If True, always print floating point numbers using fixed point notation, in which case numbers equal to zero in the current precision will print as zero. If False, then scientific notation is used when absolute value of the smallest number is < 1e-4 or the ratio of the maximum absolute value to the minimum is > 1e3. The default is False.", "String representation of floating point not-a-number (default nan).", "String representation of floating point infinity (default inf).", "Controls printing of the sign of floating-point types. If \u2018+\u2019, always print the sign of positive values. If \u2018 \u2018, always prints a space (whitespace character) in the sign position of positive values. If \u2018-\u2019, omit the sign character of positive values. (default \u2018-\u2018)", "If not None, the keys should indicate the type(s) that the respective formatting function applies to. Callables should return a string. Types that are not specified (by their corresponding keys) are handled by the default formatters. Individual types for which a formatter can be set are:", "Other keys that can be used to set a group of types at once are:", "Controls the interpretation of the precision option for floating-point types. Can take the following values (default maxprec_equal):", "even if this would print more or fewer digits than necessary to specify the value uniquely.", "to represent each value uniquely. Different elements may have a different number of digits. The value of the precision option is ignored.", "an element can be uniquely represented with fewer digits only print it with that many.", "but if every element in the array can be uniquely represented with an equal number of fewer digits, use that many digits for all elements.", "If set to the string \u20181.13\u2019 enables 1.13 legacy printing mode. This approximates numpy 1.13 print output by including a space in the sign position of floats and different behavior for 0d arrays. This also enables 1.21 legacy printing mode (described below).", "If set to the string \u20181.21\u2019 enables 1.21 legacy printing mode. This approximates numpy 1.21 print output of complex structured dtypes by not inserting spaces after commas that separate fields and after colons.", "If set to False, disables legacy mode.", "Unrecognized strings will be ignored with a warning for forward compatibility.", "New in version 1.14.0.", "Changed in version 1.22.0.", "See also", "formatter is always reset with a call to set_printoptions.", "Use printoptions as a context manager to set the values temporarily.", "Floating point precision can be set:", "Long arrays can be summarised:", "Small results can be suppressed:", "A custom formatter can be used to display array elements as desired:", "To put back the default options, you can use:", "Also to temporarily override options, use printoptions as a context manager:"]}, {"name": "numpy.set_string_function()", "path": "reference/generated/numpy.set_string_function", "type": "numpy.set_string_function", "text": ["Set a Python function to be used when pretty printing arrays.", "Function to be used to pretty print arrays. The function should expect a single array argument and return a string of the representation of the array. If None, the function is reset to the default NumPy function to print arrays.", "If True (default), the function for pretty printing (__repr__) is set, if False the function that returns the default string representation (__str__) is set.", "See also", "We can reset the function to the default:", "repr affects either pretty printing or normal string representation. Note that __repr__ is still affected by setting __str__ because the width of each array element in the returned string becomes equal to the length of the result of __str__()."]}, {"name": "numpy.setbufsize()", "path": "reference/generated/numpy.setbufsize", "type": "numpy.setbufsize", "text": ["Set the size of the buffer used in ufuncs.", "Size of buffer."]}, {"name": "numpy.setdiff1d()", "path": "reference/generated/numpy.setdiff1d", "type": "numpy.setdiff1d", "text": ["Find the set difference of two arrays.", "Return the unique values in ar1 that are not in ar2.", "Input array.", "Input comparison array.", "If True, the input arrays are both assumed to be unique, which can speed up the calculation. Default is False.", "1D array of values in ar1 that are not in ar2. The result is sorted when assume_unique=False, but otherwise only sorted if the input is sorted.", "See also", "Module with a number of other functions for performing set operations on arrays."]}, {"name": "numpy.seterr()", "path": "reference/generated/numpy.seterr", "type": "numpy.seterr", "text": ["Set how floating-point errors are handled.", "Note that operations on integer scalar types (such as int16) are handled like floating point, and are affected by these settings.", "Set treatment for all types of floating-point errors at once:", "The default is not to change the current behavior.", "Treatment for division by zero.", "Treatment for floating-point overflow.", "Treatment for floating-point underflow.", "Treatment for invalid floating-point operation.", "Dictionary containing the old settings.", "See also", "Set a callback function for the \u2018call\u2019 mode.", "The floating-point exceptions are defined in the IEEE 754 standard [1]:", "https://en.wikipedia.org/wiki/IEEE_754"]}, {"name": "numpy.seterrcall()", "path": "reference/generated/numpy.seterrcall", "type": "numpy.seterrcall", "text": ["Set the floating-point error callback function or log object.", "There are two ways to capture floating-point error messages. The first is to set the error-handler to \u2018call\u2019, using seterr. Then, set the function to call using this function.", "The second is to set the error-handler to \u2018log\u2019, using seterr. Floating-point errors then trigger a call to the \u2018write\u2019 method of the provided object.", "Function to call upon floating-point errors (\u2018call\u2019-mode) or object whose \u2018write\u2019 method is used to log such message (\u2018log\u2019-mode).", "The call function takes two arguments. The first is a string describing the type of error (such as \u201cdivide by zero\u201d, \u201coverflow\u201d, \u201cunderflow\u201d, or \u201cinvalid value\u201d), and the second is the status flag. The flag is a byte, whose four least-significant bits indicate the type of error, one of \u201cdivide\u201d, \u201cover\u201d, \u201cunder\u201d, \u201cinvalid\u201d:", "In other words, flags = divide + 2*over + 4*under + 8*invalid.", "If an object is provided, its write method should take one argument, a string.", "The old error handler.", "See also", "Callback upon error:", "Log error message:"]}, {"name": "numpy.seterrobj()", "path": "reference/generated/numpy.seterrobj", "type": "numpy.seterrobj", "text": ["Set the object that defines floating-point error handling.", "The error object contains all information that defines the error handling behavior in NumPy. seterrobj is used internally by the other functions that set error handling behavior (seterr, seterrcall).", "The error object, a list containing three elements: [internal numpy buffer size, error mask, error callback function].", "The error mask is a single integer that holds the treatment information on all four floating point errors. The information for each error type is contained in three bits of the integer. If we print it in base 8, we can see what treatment is set for \u201cinvalid\u201d, \u201cunder\u201d, \u201cover\u201d, and \u201cdivide\u201d (in that order). The printed string can be interpreted with", "See also", "For complete documentation of the types of floating-point exceptions and treatment options, see seterr."]}, {"name": "numpy.setxor1d()", "path": "reference/generated/numpy.setxor1d", "type": "numpy.setxor1d", "text": ["Find the set exclusive-or of two arrays.", "Return the sorted, unique values that are in only one (not both) of the input arrays.", "Input arrays.", "If True, the input arrays are both assumed to be unique, which can speed up the calculation. Default is False.", "Sorted 1D array of unique values that are in only one of the input arrays."]}, {"name": "numpy.shape()", "path": "reference/generated/numpy.shape", "type": "numpy.shape", "text": ["Return the shape of an array.", "Input array.", "The elements of the shape tuple give the lengths of the corresponding array dimensions.", "See also", "Equivalent array method."]}, {"name": "numpy.shares_memory()", "path": "reference/generated/numpy.shares_memory", "type": "numpy.shares_memory", "text": ["Determine if two arrays share memory.", "Warning", "This function can be exponentially slow for some inputs, unless max_work is set to a finite number or MAY_SHARE_BOUNDS. If in doubt, use numpy.may_share_memory instead.", "Input arrays", "Effort to spend on solving the overlap problem (maximum number of candidate solutions to consider). The following special values are recognized:", "The problem is solved exactly. In this case, the function returns True only if there is an element shared between the arrays. Finding the exact solution may take extremely long in some cases.", "Only the memory bounds of a and b are checked.", "Exceeded max_work.", "See also", "Checking whether two arrays share memory is NP-complete, and runtime may increase exponentially in the number of dimensions. Hence, max_work should generally be set to a finite number, as it is possible to construct examples that take extremely long to run:", "Running np.shares_memory(x1, x2) without max_work set takes around 1 minute for this case. It is possible to find problems that take still significantly longer."]}, {"name": "numpy.short", "path": "reference/arrays.scalars#numpy.short", "type": "Scalars", "text": ["Signed integer type, compatible with C short.", "'h'", "numpy.int16: 16-bit signed integer (-32_768 to 32_767)."]}, {"name": "numpy.show_config()", "path": "reference/generated/numpy.show_config", "type": "numpy.show_config", "text": ["Show libraries in the system on which NumPy was built.", "Print information about various resources (libraries, library directories, include directories, etc.) in the system on which NumPy was built.", "See also", "Returns the directory containing NumPy C header files.", "Classes specifying the information to be printed are defined in the numpy.distutils.system_info module.", "Information may include:", "NumPy BLAS/LAPACK Installation Notes", "Installing a numpy wheel (pip install numpy or force it via pip install numpy --only-binary :numpy: numpy) includes an OpenBLAS implementation of the BLAS and LAPACK linear algebra APIs. In this case, library_dirs reports the original build time configuration as compiled with gcc/gfortran; at run time the OpenBLAS library is in site-packages/numpy.libs/ (linux), or site-packages/numpy/.dylibs/ (macOS), or site-packages/numpy/.libs/ (windows).", "Installing numpy from source (pip install numpy --no-binary numpy) searches for BLAS and LAPACK dynamic link libraries at build time as influenced by environment variables NPY_BLAS_LIBS, NPY_CBLAS_LIBS, and NPY_LAPACK_LIBS; or NPY_BLAS_ORDER and NPY_LAPACK_ORDER; or the optional file ~/.numpy-site.cfg. NumPy remembers those locations and expects to load the same libraries at run-time. In NumPy 1.21+ on macOS, \u2018accelerate\u2019 (Apple\u2019s Accelerate BLAS library) is in the default build-time search order after \u2018openblas\u2019."]}, {"name": "numpy.sign()", "path": "reference/generated/numpy.sign", "type": "numpy.sign", "text": ["Returns an element-wise indication of the sign of a number.", "The sign function returns -1 if x < 0, 0 if x==0, 1 if x > 0. nan is returned for nan inputs.", "For complex inputs, the sign function returns sign(x.real) + 0j if x.real != 0 else sign(x.imag) + 0j.", "complex(nan, 0) is returned for complex nan inputs.", "Input values.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The sign of x. This is a scalar if x is a scalar.", "There is more than one definition of sign in common use for complex numbers. The definition used here is equivalent to \\(x/\\sqrt{x*x}\\) which is different from a common alternative, \\(x/|x|\\)."]}, {"name": "numpy.signbit()", "path": "reference/generated/numpy.signbit", "type": "numpy.signbit", "text": ["Returns element-wise True where signbit is set (less than zero).", "The input value(s).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Output array, or reference to out if that was supplied. This is a scalar if x is a scalar."]}, {"name": "numpy.sin()", "path": "reference/generated/numpy.sin", "type": "numpy.sin", "text": ["Trigonometric sine, element-wise.", "Angle, in radians (\\(2 \\pi\\) rad equals 360 degrees).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The sine of each element of x. This is a scalar if x is a scalar.", "See also", "The sine is one of the fundamental functions of trigonometry (the mathematical study of triangles). Consider a circle of radius 1 centered on the origin. A ray comes in from the \\(+x\\) axis, makes an angle at the origin (measured counter-clockwise from that axis), and departs from the origin. The \\(y\\) coordinate of the outgoing ray\u2019s intersection with the unit circle is the sine of that angle. It ranges from -1 for \\(x=3\\pi / 2\\) to +1 for \\(\\pi / 2.\\) The function has zeroes where the angle is a multiple of \\(\\pi\\). Sines of angles between \\(\\pi\\) and \\(2\\pi\\) are negative. The numerous properties of the sine and related functions are included in any standard trigonometry text.", "Print sine of one angle:", "Print sines of an array of angles given in degrees:", "Plot the sine function:"]}, {"name": "numpy.sinc()", "path": "reference/generated/numpy.sinc", "type": "numpy.sinc", "text": ["Return the normalized sinc function.", "The sinc function is \\(\\sin(\\pi x)/(\\pi x)\\).", "Note", "Note the normalization factor of pi used in the definition. This is the most commonly used definition in signal processing. Use sinc(x / np.pi) to obtain the unnormalized sinc function \\(\\sin(x)/(x)\\) that is more common in mathematics.", "Array (possibly multi-dimensional) of values for which to to calculate sinc(x).", "sinc(x), which has the same shape as the input.", "sinc(0) is the limit value 1.", "The name sinc is short for \u201csine cardinal\u201d or \u201csinus cardinalis\u201d.", "The sinc function is used in various signal processing applications, including in anti-aliasing, in the construction of a Lanczos resampling filter, and in interpolation.", "For bandlimited interpolation of discrete-time signals, the ideal interpolation kernel is proportional to the sinc function.", "Weisstein, Eric W. \u201cSinc Function.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/SincFunction.html", "Wikipedia, \u201cSinc function\u201d, https://en.wikipedia.org/wiki/Sinc_function"]}, {"name": "numpy.single", "path": "reference/arrays.scalars#numpy.single", "type": "Scalars", "text": ["Single-precision floating-point number type, compatible with C float.", "'f'", "numpy.float32: 32-bit-precision floating-point number type: sign bit, 8 bits exponent, 23 bits mantissa."]}, {"name": "numpy.singlecomplex", "path": "reference/arrays.scalars#numpy.singlecomplex", "type": "Scalars", "text": ["alias of numpy.csingle"]}, {"name": "numpy.sinh()", "path": "reference/generated/numpy.sinh", "type": "numpy.sinh", "text": ["Hyperbolic sine, element-wise.", "Equivalent to 1/2 * (np.exp(x) - np.exp(-x)) or -1j * np.sin(1j*x).", "Input array.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The corresponding hyperbolic sine values. This is a scalar if x is a scalar.", "If out is provided, the function writes the result into it, and returns a reference to out. (See Examples)", "M. Abramowitz and I. A. Stegun, Handbook of Mathematical Functions. New York, NY: Dover, 1972, pg. 83."]}, {"name": "numpy.sort()", "path": "reference/generated/numpy.sort", "type": "numpy.sort", "text": ["Return a sorted copy of an array.", "Array to be sorted.", "Axis along which to sort. If None, the array is flattened before sorting. The default is -1, which sorts along the last axis.", "Sorting algorithm. The default is \u2018quicksort\u2019. Note that both \u2018stable\u2019 and \u2018mergesort\u2019 use timsort or radix sort under the covers and, in general, the actual implementation will vary with data type. The \u2018mergesort\u2019 option is retained for backwards compatibility.", "Changed in version 1.15.0.: The \u2018stable\u2019 option was added.", "When a is an array with fields defined, this argument specifies which fields to compare first, second, etc. A single field can be specified as a string, and not all fields need be specified, but unspecified fields will still be used, in the order in which they come up in the dtype, to break ties.", "Array of the same type and shape as a.", "See also", "Method to sort an array in-place.", "Indirect sort.", "Indirect stable sort on multiple keys.", "Find elements in a sorted array.", "Partial sort.", "The various sorting algorithms are characterized by their average speed, worst case performance, work space size, and whether they are stable. A stable sort keeps items with the same key in the same relative order. The four algorithms implemented in NumPy have the following properties:", "kind", "speed", "worst case", "work space", "stable", "\u2018quicksort\u2019", "1", "O(n^2)", "0", "no", "\u2018heapsort\u2019", "3", "O(n*log(n))", "0", "no", "\u2018mergesort\u2019", "2", "O(n*log(n))", "~n/2", "yes", "\u2018timsort\u2019", "2", "O(n*log(n))", "~n/2", "yes", "Note", "The datatype determines which of \u2018mergesort\u2019 or \u2018timsort\u2019 is actually used, even if \u2018mergesort\u2019 is specified. User selection at a finer scale is not currently available.", "All the sort algorithms make temporary copies of the data when sorting along any but the last axis. Consequently, sorting along the last axis is faster and uses less space than sorting along any other axis.", "The sort order for complex numbers is lexicographic. If both the real and imaginary parts are non-nan then the order is determined by the real parts except when they are equal, in which case the order is determined by the imaginary parts.", "Previous to numpy 1.4.0 sorting real and complex arrays containing nan values led to undefined behaviour. In numpy versions >= 1.4.0 nan values are sorted to the end. The extended sort order is:", "where R is a non-nan real value. Complex values with the same nan placements are sorted according to the non-nan part if it exists. Non-nan values are sorted as before.", "New in version 1.12.0.", "quicksort has been changed to introsort. When sorting does not make enough progress it switches to heapsort. This implementation makes quicksort O(n*log(n)) in the worst case.", "\u2018stable\u2019 automatically chooses the best stable sorting algorithm for the data type being sorted. It, along with \u2018mergesort\u2019 is currently mapped to timsort or radix sort depending on the data type. API forward compatibility currently limits the ability to select the implementation and it is hardwired for the different data types.", "New in version 1.17.0.", "Timsort is added for better performance on already or nearly sorted data. On random data timsort is almost identical to mergesort. It is now used for stable sort while quicksort is still the default sort if none is chosen. For timsort details, refer to CPython listsort.txt. \u2018mergesort\u2019 and \u2018stable\u2019 are mapped to radix sort for integer data types. Radix sort is an O(n) sort instead of O(n log n).", "Changed in version 1.18.0.", "NaT now sorts to the end of arrays for consistency with NaN.", "Use the order keyword to specify a field to use when sorting a structured array:", "Sort by age, then height if ages are equal:"]}, {"name": "numpy.sort_complex()", "path": "reference/generated/numpy.sort_complex", "type": "numpy.sort_complex", "text": ["Sort a complex array using the real part first, then the imaginary part.", "Input array", "Always returns a sorted complex array."]}, {"name": "numpy.source()", "path": "reference/generated/numpy.source", "type": "numpy.source", "text": ["Print or write to a file the source code for a NumPy object.", "The source code is only returned for objects written in Python. Many functions and classes are defined in C and will therefore not return useful information.", "Input object. This can be any object (function, class, module, \u2026).", "If output not supplied then source code is printed to screen (sys.stdout). File object must be created with either write \u2018w\u2019 or append \u2018a\u2019 modes.", "See also", "The source code is only returned for objects written in Python."]}, {"name": "numpy.spacing()", "path": "reference/generated/numpy.spacing", "type": "numpy.spacing", "text": ["Return the distance between x and the nearest adjacent number.", "Values to find the spacing of.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The spacing of values of x. This is a scalar if x is a scalar.", "It can be considered as a generalization of EPS: spacing(np.float64(1)) == np.finfo(np.float64).eps, and there should not be any representable number between x + spacing(x) and x for any finite x.", "Spacing of +- inf and NaN is NaN."]}, {"name": "numpy.split()", "path": "reference/generated/numpy.split", "type": "numpy.split", "text": ["Split an array into multiple sub-arrays as views into ary.", "Array to be divided into sub-arrays.", "If indices_or_sections is an integer, N, the array will be divided into N equal arrays along axis. If such a split is not possible, an error is raised.", "If indices_or_sections is a 1-D array of sorted integers, the entries indicate where along axis the array is split. For example, [2, 3] would, for axis=0, result in", "If an index exceeds the dimension of the array along axis, an empty sub-array is returned correspondingly.", "The axis along which to split, default is 0.", "A list of sub-arrays as views into ary.", "If indices_or_sections is given as an integer, but a split does not result in equal division.", "See also", "Split an array into multiple sub-arrays of equal or near-equal size. Does not raise an exception if an equal division cannot be made.", "Split array into multiple sub-arrays horizontally (column-wise).", "Split array into multiple sub-arrays vertically (row wise).", "Split array into multiple sub-arrays along the 3rd axis (depth).", "Join a sequence of arrays along an existing axis.", "Join a sequence of arrays along a new axis.", "Stack arrays in sequence horizontally (column wise).", "Stack arrays in sequence vertically (row wise).", "Stack arrays in sequence depth wise (along third dimension)."]}, {"name": "numpy.sqrt()", "path": "reference/generated/numpy.sqrt", "type": "numpy.sqrt", "text": ["Return the non-negative square-root of an array, element-wise.", "The values whose square-roots are required.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "An array of the same shape as x, containing the positive square-root of each element in x. If any element in x is complex, a complex array is returned (and the square-roots of negative reals are calculated). If all of the elements in x are real, so is y, with negative elements returning nan. If out was provided, y is a reference to it. This is a scalar if x is a scalar.", "See also", "A version which returns complex numbers when given negative reals.", "sqrt has\u2013consistent with common convention\u2013as its branch cut the real \u201cinterval\u201d [-inf, 0), and is continuous from above on it. A branch cut is a curve in the complex plane across which a given complex function fails to be continuous."]}, {"name": "numpy.square()", "path": "reference/generated/numpy.square", "type": "numpy.square", "text": ["Return the element-wise square of the input.", "Input data.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "Element-wise x*x, of the same shape and dtype as x. This is a scalar if x is a scalar.", "See also"]}, {"name": "numpy.squeeze()", "path": "reference/generated/numpy.squeeze", "type": "numpy.squeeze", "text": ["Remove axes of length one from a.", "Input data.", "New in version 1.7.0.", "Selects a subset of the entries of length one in the shape. If an axis is selected with shape entry greater than one, an error is raised.", "The input array, but with all or a subset of the dimensions of length 1 removed. This is always a itself or a view into a. Note that if all axes are squeezed, the result is a 0d array and not a scalar.", "If axis is not None, and an axis being squeezed is not of length 1", "See also", "The inverse operation, adding entries of length one", "Insert, remove, and combine dimensions, and resize existing ones"]}, {"name": "numpy.stack()", "path": "reference/generated/numpy.stack", "type": "numpy.stack", "text": ["Join a sequence of arrays along a new axis.", "The axis parameter specifies the index of the new axis in the dimensions of the result. For example, if axis=0 it will be the first dimension and if axis=-1 it will be the last dimension.", "New in version 1.10.0.", "Each array must have the same shape.", "The axis in the result array along which the input arrays are stacked.", "If provided, the destination to place the result. The shape must be correct, matching that of what stack would have returned if no out argument were specified.", "The stacked array has one more dimension than the input arrays.", "See also", "Join a sequence of arrays along an existing axis.", "Assemble an nd-array from nested lists of blocks.", "Split array into a list of multiple sub-arrays of equal size."]}, {"name": "numpy.std()", "path": "reference/generated/numpy.std", "type": "numpy.std", "text": ["Compute the standard deviation along the specified axis.", "Returns the standard deviation, a measure of the spread of a distribution, of the array elements. The standard deviation is computed for the flattened array by default, otherwise over the specified axis.", "Calculate the standard deviation of these values.", "Axis or axes along which the standard deviation is computed. The default is to compute the standard deviation of the flattened array.", "New in version 1.7.0.", "If this is a tuple of ints, a standard deviation is performed over multiple axes, instead of a single axis or all the axes as before.", "Type to use in computing the standard deviation. For arrays of integer type the default is float64, for arrays of float types it is the same as the array type.", "Alternative output array in which to place the result. It must have the same shape as the expected output but the type (of the calculated values) will be cast if necessary.", "Means Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. By default ddof is zero.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.", "If the default value is passed, then keepdims will not be passed through to the std method of sub-classes of ndarray, however any non-default value will be. If the sub-class\u2019 method does not implement keepdims any exceptions will be raised.", "Elements to include in the standard deviation. See reduce for details.", "New in version 1.20.0.", "If out is None, return a new array containing the standard deviation, otherwise return a reference to the output array.", "See also", "The standard deviation is the square root of the average of the squared deviations from the mean, i.e., std = sqrt(mean(x)), where x = abs(a - a.mean())**2.", "The average squared deviation is typically calculated as x.sum() / N, where N = len(x). If, however, ddof is specified, the divisor N - ddof is used instead. In standard statistical practice, ddof=1 provides an unbiased estimator of the variance of the infinite population. ddof=0 provides a maximum likelihood estimate of the variance for normally distributed variables. The standard deviation computed in this function is the square root of the estimated variance, so even with ddof=1, it will not be an unbiased estimate of the standard deviation per se.", "Note that, for complex numbers, std takes the absolute value before squaring, so that the result is always real and nonnegative.", "For floating-point input, the std is computed using the same precision the input has. Depending on the input data, this can cause the results to be inaccurate, especially for float32 (see example below). Specifying a higher-accuracy accumulator using the dtype keyword can alleviate this issue.", "In single precision, std() can be inaccurate:", "Computing the standard deviation in float64 is more accurate:", "Specifying a where argument:"]}, {"name": "numpy.str_", "path": "reference/arrays.scalars#numpy.str_", "type": "Scalars", "text": ["A unicode string.", "When used in arrays, this type strips trailing null codepoints.", "Unlike the builtin str, this supports the Buffer Protocol, exposing its contents as UCS4:", "'U'", "numpy.unicode_"]}, {"name": "numpy.string_", "path": "reference/arrays.scalars#numpy.string_", "type": "Scalars", "text": ["alias of numpy.bytes_"]}, {"name": "numpy.subtract()", "path": "reference/generated/numpy.subtract", "type": "numpy.subtract", "text": ["Subtract arguments, element-wise.", "The arrays to be subtracted from each other. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The difference of x1 and x2, element-wise. This is a scalar if both x1 and x2 are scalars.", "Equivalent to x1 - x2 in terms of array broadcasting.", "The - operator can be used as a shorthand for np.subtract on ndarrays."]}, {"name": "numpy.sum()", "path": "reference/generated/numpy.sum", "type": "numpy.sum", "text": ["Sum of array elements over a given axis.", "Elements to sum.", "Axis or axes along which a sum is performed. The default, axis=None, will sum all of the elements of the input array. If axis is negative it counts from the last to the first axis.", "New in version 1.7.0.", "If axis is a tuple of ints, a sum is performed on all of the axes specified in the tuple instead of a single axis or all the axes as before.", "The type of the returned array and of the accumulator in which the elements are summed. The dtype of a is used by default unless a has an integer dtype of less precision than the default platform integer. In that case, if a is signed then the platform integer is used while if a is unsigned then an unsigned integer of the same precision as the platform integer is used.", "Alternative output array in which to place the result. It must have the same shape as the expected output, but the type of the output values will be cast if necessary.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.", "If the default value is passed, then keepdims will not be passed through to the sum method of sub-classes of ndarray, however any non-default value will be. If the sub-class\u2019 method does not implement keepdims any exceptions will be raised.", "Starting value for the sum. See reduce for details.", "New in version 1.15.0.", "Elements to include in the sum. See reduce for details.", "New in version 1.17.0.", "An array with the same shape as a, with the specified axis removed. If a is a 0-d array, or if axis is None, a scalar is returned. If an output array is specified, a reference to out is returned.", "See also", "Equivalent method.", "Equivalent functionality of add.", "Cumulative sum of array elements.", "Integration of array values using the composite trapezoidal rule.", "Arithmetic is modular when using integer types, and no error is raised on overflow.", "The sum of an empty array is the neutral element 0:", "For floating point numbers the numerical precision of sum (and np.add.reduce) is in general limited by directly adding each number individually to the result causing rounding errors in every step. However, often numpy will use a numerically better approach (partial pairwise summation) leading to improved precision in many use-cases. This improved precision is always provided when no axis is given. When axis is given, it will depend on which axis is summed. Technically, to provide the best speed possible, the improved precision is only used when the summation is along the fast axis in memory. Note that the exact precision may vary depending on other parameters. In contrast to NumPy, Python\u2019s math.fsum function uses a slower but more precise approach to summation. Especially when summing a large number of lower precision floating point numbers, such as float32, numerical errors can become significant. In such cases it can be advisable to use dtype=\u201dfloat64\u201d to use a higher precision for the output.", "If the accumulator is too small, overflow occurs:", "You can also start the sum with a value other than zero:"]}, {"name": "numpy.swapaxes()", "path": "reference/generated/numpy.swapaxes", "type": "numpy.swapaxes", "text": ["Interchange two axes of an array.", "Input array.", "First axis.", "Second axis.", "For NumPy >= 1.10.0, if a is an ndarray, then a view of a is returned; otherwise a new array is created. For earlier NumPy versions a view of a is returned only if the order of the axes is changed, otherwise the input array is returned."]}, {"name": "numpy.take()", "path": "reference/generated/numpy.take", "type": "numpy.take", "text": ["Take elements from an array along an axis.", "When axis is not None, this function does the same thing as \u201cfancy\u201d indexing (indexing arrays using arrays); however, it can be easier to use if you need elements along a given axis. A call such as np.take(arr, indices, axis=3) is equivalent to arr[:,:,:,indices,...].", "Explained without fancy indexing, this is equivalent to the following use of ndindex, which sets each of ii, jj, and kk to a tuple of indices:", "The source array.", "The indices of the values to extract.", "New in version 1.8.0.", "Also allow scalars for indices.", "The axis over which to select values. By default, the flattened input array is used.", "If provided, the result will be placed in this array. It should be of the appropriate shape and dtype. Note that out is always buffered if mode=\u2019raise\u2019; use other modes for better performance.", "Specifies how out-of-bounds indices will behave.", "\u2018clip\u2019 mode means that all indices that are too large are replaced by the index that addresses the last element along that axis. Note that this disables indexing with negative numbers.", "The returned array has the same type as a.", "See also", "Take elements using a boolean mask", "equivalent method", "Take elements by matching the array and the index arrays", "By eliminating the inner loop in the description above, and using s_ to build simple slice objects, take can be expressed in terms of applying fancy indexing to each 1-d slice:", "For this reason, it is equivalent to (but faster than) the following use of apply_along_axis:", "In this example if a is an ndarray, \u201cfancy\u201d indexing can be used.", "If indices is not one dimensional, the output also has these dimensions."]}, {"name": "numpy.take_along_axis()", "path": "reference/generated/numpy.take_along_axis", "type": "numpy.take_along_axis", "text": ["Take values from the input array by matching 1d index and data slices.", "This iterates over matching 1d slices oriented along the specified axis in the index and data arrays, and uses the former to look up values in the latter. These slices can be different lengths.", "Functions returning an index along an axis, like argsort and argpartition, produce suitable indices for this function.", "New in version 1.15.0.", "Source array", "Indices to take along each 1d slice of arr. This must match the dimension of arr, but dimensions Ni and Nj only need to broadcast against arr.", "The axis to take 1d slices along. If axis is None, the input array is treated as if it had first been flattened to 1d, for consistency with sort and argsort.", "The indexed result.", "See also", "Take along an axis, using the same indices for every 1d slice", "Put values into the destination array by matching 1d index and data slices", "This is equivalent to (but faster than) the following use of ndindex and s_, which sets each of ii and kk to a tuple of indices:", "Equivalently, eliminating the inner loop, the last two lines would be:", "For this sample array", "We can sort either by using sort directly, or argsort and this function", "The same works for max and min, if you expand the dimensions:", "If we want to get the max and min at the same time, we can stack the indices first"]}, {"name": "numpy.tan()", "path": "reference/generated/numpy.tan", "type": "numpy.tan", "text": ["Compute tangent element-wise.", "Equivalent to np.sin(x)/np.cos(x) element-wise.", "Input array.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The corresponding tangent values. This is a scalar if x is a scalar.", "If out is provided, the function writes the result into it, and returns a reference to out. (See Examples)", "M. Abramowitz and I. A. Stegun, Handbook of Mathematical Functions. New York, NY: Dover, 1972."]}, {"name": "numpy.tanh()", "path": "reference/generated/numpy.tanh", "type": "numpy.tanh", "text": ["Compute hyperbolic tangent element-wise.", "Equivalent to np.sinh(x)/np.cosh(x) or -1j * np.tan(1j*x).", "Input array.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The corresponding hyperbolic tangent values. This is a scalar if x is a scalar.", "If out is provided, the function writes the result into it, and returns a reference to out. (See Examples)", "M. Abramowitz and I. A. Stegun, Handbook of Mathematical Functions. New York, NY: Dover, 1972, pg. 83. https://personal.math.ubc.ca/~cbm/aands/page_83.htm", "Wikipedia, \u201cHyperbolic function\u201d, https://en.wikipedia.org/wiki/Hyperbolic_function"]}, {"name": "numpy.tensordot()", "path": "reference/generated/numpy.tensordot", "type": "numpy.tensordot", "text": ["Compute tensor dot product along specified axes.", "Given two tensors, a and b, and an array_like object containing two array_like objects, (a_axes, b_axes), sum the products of a\u2019s and b\u2019s elements (components) over the axes specified by a_axes and b_axes. The third argument can be a single non-negative integer_like scalar, N; if it is such, then the last N dimensions of a and the first N dimensions of b are summed over.", "Tensors to \u201cdot\u201d.", "The tensor dot product of the input.", "See also", "When axes is integer_like, the sequence for evaluation will be: first the -Nth axis in a and 0th axis in b, and the -1th axis in a and Nth axis in b last.", "When there is more than one axis to sum over - and they are not the last (first) axes of a (b) - the argument axes should consist of two sequences of the same length, with the first axis to sum over given first in both sequences, the second axis second, and so forth.", "The shape of the result consists of the non-contracted axes of the first tensor, followed by the non-contracted axes of the second.", "A \u201ctraditional\u201d example:", "An extended example taking advantage of the overloading of + and *:"]}, {"name": "numpy.testing.extbuild.build_and_import_extension()", "path": "reference/testing", "type": "Testing Guidelines", "text": ["Until the 1.15 release, NumPy used the nose testing framework, it now uses the pytest framework. The older framework is still maintained in order to support downstream projects that use the old numpy framework, but all tests for NumPy should use pytest.", "Our goal is that every module and package in NumPy should have a thorough set of unit tests. These tests should exercise the full functionality of a given routine as well as its robustness to erroneous or unexpected input arguments. Well-designed tests with good coverage make an enormous difference to the ease of refactoring. Whenever a new bug is found in a routine, you should write a new test for that specific case and add it to the test suite to prevent that bug from creeping back in unnoticed.", "Note", "SciPy uses the testing framework from numpy.testing, so all of the NumPy examples shown below are also applicable to SciPy", "NumPy can be tested in a number of ways, choose any way you feel comfortable.", "You can test an installed NumPy by numpy.test, for example, To run NumPy\u2019s full test suite, use the following:", "The test method may take two or more arguments; the first label is a string specifying what should be tested and the second verbose is an integer giving the level of output verbosity. See the docstring numpy.test for details. The default value for label is \u2018fast\u2019 - which will run the standard tests. The string \u2018full\u2019 will run the full battery of tests, including those identified as being slow to run. If verbose is 1 or less, the tests will just show information messages about the tests that are run; but if it is greater than 1, then the tests will also provide warnings on missing tests. So if you want to run every test and get messages about which modules don\u2019t have tests:", "Finally, if you are only interested in testing a subset of NumPy, for example, the core module, use the following:", "If you want to build NumPy in order to work on NumPy itself, use runtests.py.To run NumPy\u2019s full test suite:", "Testing a subset of NumPy:", "For detailed info on testing, see Testing builds", "Run tests using your favourite IDE such as vscode or pycharm", "If you are writing a package that you\u2019d like to become part of NumPy, please write the tests as you develop the package. Every Python module, extension module, or subpackage in the NumPy package directory should have a corresponding test_<name>.py file. Pytest examines these files for test methods (named test*) and test classes (named Test*).", "Suppose you have a NumPy module numpy/xxx/yyy.py containing a function zzz(). To test this function you would create a test module called test_yyy.py. If you only need to test one aspect of zzz, you can simply add a test function:", "More often, we need to group a number of tests together, so we create a test class:", "Within these test methods, assert and related functions are used to test whether a certain assumption is valid. If the assertion fails, the test fails. pytest internally rewrites the assert statement to give informative output when it fails, so should be preferred over the legacy variant numpy.testing.assert_. Whereas plain assert statements are ignored when running Python in optimized mode with -O, this is not an issue when running tests with pytest.", "Similarly, the pytest functions pytest.raises and pytest.warns should be preferred over their legacy counterparts numpy.testing.assert_raises and numpy.testing.assert_warns, since the pytest variants are more broadly used and allow more explicit targeting of warnings and errors when used with the match regex.", "Note that test_ functions or methods should not have a docstring, because that makes it hard to identify the test from the output of running the test suite with verbose=2 (or similar verbosity setting). Use plain comments (#) if necessary.", "Also since much of NumPy is legacy code that was originally written without unit tests, there are still several modules that don\u2019t have tests yet. Please feel free to choose one of these modules and develop tests for it.", "NumPy exposes a rich C-API . These are tested using c-extension modules written \u201cas-if\u201d they know nothing about the internals of NumPy, rather using the official C-API interfaces only. Examples of such modules are tests for a user-defined rational dtype in _rational_tests or the ufunc machinery tests in _umath_tests which are part of the binary distribution. Starting from version 1.21, you can also write snippets of C code in tests that will be compiled locally into c-extension modules and loaded into python.", "Build and imports a c-extension module modname from a list of function fragments functions.", "Each fragment is a sequence of func_name, calling convention, snippet.", "Code to preceed the rest, usually extra #include or #define macros.", "Where to build the module, usually a temporary directory", "Extra directories to find include files when compiling", "Code to appear in the module PyMODINIT_FUNC", "The module will have been loaded and is ready for use", "Unlabeled tests like the ones above are run in the default numpy.test() run. If you want to label your test as slow - and therefore reserved for a full numpy.test(label='full') run, you can label it with pytest.mark.slow:", "Similarly for methods:", "Testing looks for module-level or class-level setup and teardown functions by name; thus:", "Setup and teardown functions to functions and methods are known as \u201cfixtures\u201d, and their use is not encouraged.", "One very nice feature of testing is allowing easy testing across a range of parameters - a nasty problem for standard unit tests. Use the pytest.mark.parametrize decorator.", "Doctests are a convenient way of documenting the behavior of a function and allowing that behavior to be tested at the same time. The output of an interactive Python session can be included in the docstring of a function, and the test framework can run the example and compare the actual output to the expected output.", "The doctests can be run by adding the doctests argument to the test() call; for example, to run all tests (including doctests) for numpy.lib:", "The doctests are run as if they are in a fresh Python instance which has executed import numpy as np. Tests that are part of a NumPy subpackage will have that subpackage already imported. E.g. for a test in numpy/linalg/tests/, the namespace will be created such that from numpy import linalg has already executed.", "Rather than keeping the code and the tests in the same directory, we put all the tests for a given subpackage in a tests/ subdirectory. For our example, if it doesn\u2019t already exist you will need to create a tests/ directory in numpy/xxx/. So the path for test_yyy.py is numpy/xxx/tests/test_yyy.py.", "Once the numpy/xxx/tests/test_yyy.py is written, its possible to run the tests by going to the tests/ directory and typing:", "Or if you add numpy/xxx/tests/ to the Python path, you could run the tests interactively in the interpreter like this:", "Usually, however, adding the tests/ directory to the python path isn\u2019t desirable. Instead it would better to invoke the test straight from the module xxx. To this end, simply place the following lines at the end of your package\u2019s __init__.py file:", "You will also need to add the tests directory in the configuration section of your setup.py:", "Now you can do the following to test your module:", "Also, when invoking the entire NumPy test suite, your tests will be found and run:", "If you have a collection of tests that must be run multiple times with minor variations, it can be helpful to create a base class containing all the common tests, and then create a subclass for each variation. Several examples of this technique exist in NumPy; below are excerpts from one in numpy/linalg/tests/test_linalg.py:", "In this case, we wanted to test solving a linear algebra problem using matrices of several data types, using linalg.solve and linalg.inv. The common test cases (for single-precision, double-precision, etc. matrices) are collected in LinalgTestCase.", "Sometimes you might want to skip a test or mark it as a known failure, such as when the test suite is being written before the code it\u2019s meant to test, or if a test only fails on a particular architecture.", "To skip a test, simply use skipif:", "The test is marked as skipped if SkipMyTest evaluates to nonzero, and the message in verbose test output is the second argument given to skipif. Similarly, a test can be marked as a known failure by using xfail:", "Of course, a test can be unconditionally skipped or marked as a known failure by using skip or xfail without argument, respectively.", "A total of the number of skipped and known failing tests is displayed at the end of the test run. Skipped tests are marked as 'S' in the test results (or 'SKIPPED' for verbose > 1), and known failing tests are marked as 'x' (or 'XFAIL' if verbose >\n1).", "Tests on random data are good, but since test failures are meant to expose new bugs or regressions, a test that passes most of the time but fails occasionally with no code changes is not helpful. Make the random data deterministic by setting the random number seed before generating it. Use either Python\u2019s random.seed(some_number) or NumPy\u2019s numpy.random.seed(some_number), depending on the source of random numbers.", "Alternatively, you can use Hypothesis to generate arbitrary data. Hypothesis manages both Python\u2019s and Numpy\u2019s random seeds for you, and provides a very concise and powerful way to describe data (including hypothesis.extra.numpy, e.g. for a set of mutually-broadcastable shapes).", "The advantages over random generation include tools to replay and share failures without requiring a fixed seed, reporting minimal examples for each failure, and better-than-naive-random techniques for triggering bugs.", "Pytest test runner.", "A test function is typically added to a package\u2019s __init__.py like so:", "Calling this test function finds and runs all tests associated with the module and all its sub-modules.", "The name of the module to test.", "Unlike the previous nose-based implementation, this class is not publicly exposed as it performs some numpy-specific warning suppression.", "Full path to the package to test."]}, {"name": "numpy.testing.suppress_warnings()", "path": "reference/generated/numpy.testing.suppress_warnings", "type": "numpy.testing.suppress_warnings", "text": ["Context manager and decorator doing much the same as warnings.catch_warnings.", "However, it also provides a filter mechanism to work around https://bugs.python.org/issue4180.", "This bug causes Python before 3.4 to not reliably show warnings again after they have been ignored once (even within catch_warnings). It means that no \u201cignore\u201d filter can be used easily, since following tests might need to see the warning. Additionally it allows easier specificity for testing warnings and can be nested.", "One of \u201calways\u201d, \u201conce\u201d, \u201cmodule\u201d, or \u201clocation\u201d. Analogous to the usual warnings module filter mode, it is useful to reduce noise mostly on the outmost level. Unsuppressed and unrecorded warnings will be forwarded based on this rule. Defaults to \u201calways\u201d. \u201clocation\u201d is equivalent to the warnings \u201cdefault\u201d, match by exact location the warning warning originated from.", "Filters added inside the context manager will be discarded again when leaving it. Upon entering all filters defined outside a context will be applied automatically.", "When a recording filter is added, matching warnings are stored in the log attribute as well as in the list returned by record.", "If filters are added and the module keyword is given, the warning registry of this module will additionally be cleared when applying it, entering the context, or exiting it. This could cause warnings to appear a second time after leaving the context if they were configured to be printed once (default) and were already printed before the context was entered.", "Nesting this context manager will work as expected when the forwarding rule is \u201calways\u201d (default). Unfiltered and unrecorded warnings will be passed out and be matched by the outer level. On the outmost level they will be printed (or caught by another warnings context). The forwarding rule argument can modify this behaviour.", "Like catch_warnings this context manager is not threadsafe.", "With a context manager:", "Or as a decorator:", "__call__(func)", "Function decorator to apply certain suppressions to a whole function.", "filter([category, message, module])", "Add a new suppressing filter or apply it if the state is entered.", "record([category, message, module])", "Append a new recording filter or apply it if the state is entered."]}, {"name": "numpy.testing.Tester", "path": "reference/generated/numpy.testing.tester", "type": "numpy.testing.Tester", "text": ["alias of numpy.testing._private.nosetester.NoseTester"]}, {"name": "numpy.tile()", "path": "reference/generated/numpy.tile", "type": "numpy.tile", "text": ["Construct an array by repeating A the number of times given by reps.", "If reps has length d, the result will have dimension of max(d, A.ndim).", "If A.ndim < d, A is promoted to be d-dimensional by prepending new axes. So a shape (3,) array is promoted to (1, 3) for 2-D replication, or shape (1, 1, 3) for 3-D replication. If this is not the desired behavior, promote A to d-dimensions manually before calling this function.", "If A.ndim > d, reps is promoted to A.ndim by pre-pending 1\u2019s to it. Thus for an A of shape (2, 3, 4, 5), a reps of (2, 2) is treated as (1, 1, 2, 2).", "Note : Although tile may be used for broadcasting, it is strongly recommended to use numpy\u2019s broadcasting operations and functions.", "The input array.", "The number of repetitions of A along each axis.", "The tiled output array.", "See also", "Repeat elements of an array.", "Broadcast an array to a new shape"]}, {"name": "numpy.timedelta64", "path": "reference/arrays.scalars#numpy.timedelta64", "type": "Scalars", "text": ["A timedelta stored as a 64-bit integer.", "See Datetimes and Timedeltas for more information.", "'m'"]}, {"name": "numpy.trace()", "path": "reference/generated/numpy.trace", "type": "numpy.trace", "text": ["Return the sum along diagonals of the array.", "If a is 2-D, the sum along its diagonal with the given offset is returned, i.e., the sum of elements a[i,i+offset] for all i.", "If a has more than two dimensions, then the axes specified by axis1 and axis2 are used to determine the 2-D sub-arrays whose traces are returned. The shape of the resulting array is the same as that of a with axis1 and axis2 removed.", "Input array, from which the diagonals are taken.", "Offset of the diagonal from the main diagonal. Can be both positive and negative. Defaults to 0.", "Axes to be used as the first and second axis of the 2-D sub-arrays from which the diagonals should be taken. Defaults are the first two axes of a.", "Determines the data-type of the returned array and of the accumulator where the elements are summed. If dtype has the value None and a is of integer type of precision less than the default integer precision, then the default integer precision is used. Otherwise, the precision is the same as that of a.", "Array into which the output is placed. Its type is preserved and it must be of the right shape to hold the output.", "If a is 2-D, the sum along the diagonal is returned. If a has larger dimensions, then an array of sums along diagonals is returned.", "See also"]}, {"name": "numpy.transpose()", "path": "reference/generated/numpy.transpose", "type": "numpy.transpose", "text": ["Reverse or permute the axes of an array; returns the modified array.", "For an array a with two axes, transpose(a) gives the matrix transpose.", "Refer to numpy.ndarray.transpose for full documentation.", "Input array.", "If specified, it must be a tuple or list which contains a permutation of [0,1,..,N-1] where N is the number of axes of a. The i\u2019th axis of the returned array will correspond to the axis numbered axes[i] of the input. If not specified, defaults to range(a.ndim)[::-1], which reverses the order of the axes.", "a with its axes permuted. A view is returned whenever possible.", "See also", "Equivalent method", "Use transpose(a, argsort(axes)) to invert the transposition of tensors when using the axes keyword argument.", "Transposing a 1-D array returns an unchanged view of the original array."]}, {"name": "numpy.trapz()", "path": "reference/generated/numpy.trapz", "type": "numpy.trapz", "text": ["Integrate along the given axis using the composite trapezoidal rule.", "If x is provided, the integration happens in sequence along its elements - they are not sorted.", "Integrate y (x) along each 1d slice on the given axis, compute \\(\\int y(x) dx\\). When x is specified, this integrates along the parametric curve, computing \\(\\int_t y(t) dt = \\int_t y(t) \\left.\\frac{dx}{dt}\\right|_{x=x(t)} dt\\).", "Input array to integrate.", "The sample points corresponding to the y values. If x is None, the sample points are assumed to be evenly spaced dx apart. The default is None.", "The spacing between sample points when x is None. The default is 1.", "The axis along which to integrate.", "Definite integral of \u2018y\u2019 = n-dimensional array as approximated along a single axis by the trapezoidal rule. If \u2018y\u2019 is a 1-dimensional array, then the result is a float. If \u2018n\u2019 is greater than 1, then the result is an \u2018n-1\u2019 dimensional array.", "See also", "Image [2] illustrates trapezoidal rule \u2013 y-axis locations of points will be taken from y array, by default x-axis distances between points will be 1.0, alternatively they can be provided with x array or with dx scalar. Return value will be equal to combined area under the red lines.", "Wikipedia page: https://en.wikipedia.org/wiki/Trapezoidal_rule", "Illustration image: https://en.wikipedia.org/wiki/File:Composite_trapezoidal_rule_illustration.png", "Using a decreasing x corresponds to integrating in reverse:", "More generally x is used to integrate along a parametric curve. This finds the area of a circle, noting we repeat the sample which closes the curve:"]}, {"name": "numpy.tri()", "path": "reference/generated/numpy.tri", "type": "numpy.tri", "text": ["An array with ones at and below the given diagonal and zeros elsewhere.", "Number of rows in the array.", "Number of columns in the array. By default, M is taken equal to N.", "The sub-diagonal at and below which the array is filled. k = 0 is the main diagonal, while k < 0 is below it, and k > 0 is above. The default is 0.", "Data type of the returned array. The default is float.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "Array with its lower triangle filled with ones and zero elsewhere; in other words T[i,j] == 1 for j <= i + k, 0 otherwise."]}, {"name": "numpy.tril()", "path": "reference/generated/numpy.tril", "type": "numpy.tril", "text": ["Lower triangle of an array.", "Return a copy of an array with elements above the k-th diagonal zeroed. For arrays with ndim exceeding 2, tril will apply to the final two axes.", "Input array.", "Diagonal above which to zero elements. k = 0 (the default) is the main diagonal, k < 0 is below it and k > 0 is above.", "Lower triangle of m, of same shape and data-type as m.", "See also", "same thing, only for the upper triangle"]}, {"name": "numpy.tril_indices()", "path": "reference/generated/numpy.tril_indices", "type": "numpy.tril_indices", "text": ["Return the indices for the lower-triangle of an (n, m) array.", "The row dimension of the arrays for which the returned indices will be valid.", "Diagonal offset (see tril for details).", "New in version 1.9.0.", "The column dimension of the arrays for which the returned arrays will be valid. By default m is taken equal to n.", "The indices for the triangle. The returned tuple contains two arrays, each with the indices along one dimension of the array.", "See also", "similar function, for upper-triangular.", "generic function accepting an arbitrary mask function.", "New in version 1.4.0.", "Compute two different sets of indices to access 4x4 arrays, one for the lower triangular part starting at the main diagonal, and one starting two diagonals further right:", "Here is how they can be used with a sample array:", "Both for indexing:", "And for assigning values:", "These cover almost the whole array (two diagonals right of the main one):"]}, {"name": "numpy.tril_indices_from()", "path": "reference/generated/numpy.tril_indices_from", "type": "numpy.tril_indices_from", "text": ["Return the indices for the lower-triangle of arr.", "See tril_indices for full details.", "The indices will be valid for square arrays whose dimensions are the same as arr.", "Diagonal offset (see tril for details).", "See also", "New in version 1.4.0."]}, {"name": "numpy.trim_zeros()", "path": "reference/generated/numpy.trim_zeros", "type": "numpy.trim_zeros", "text": ["Trim the leading and/or trailing zeros from a 1-D array or sequence.", "Input array.", "A string with \u2018f\u2019 representing trim from front and \u2018b\u2019 to trim from back. Default is \u2018fb\u2019, trim zeros from both front and back of the array.", "The result of trimming the input. The input data type is preserved.", "The input data type is preserved, list/tuple in means list/tuple out."]}, {"name": "numpy.triu()", "path": "reference/generated/numpy.triu", "type": "numpy.triu", "text": ["Upper triangle of an array.", "Return a copy of an array with the elements below the k-th diagonal zeroed. For arrays with ndim exceeding 2, triu will apply to the final two axes.", "Please refer to the documentation for tril for further details.", "See also", "lower triangle of an array"]}, {"name": "numpy.triu_indices()", "path": "reference/generated/numpy.triu_indices", "type": "numpy.triu_indices", "text": ["Return the indices for the upper-triangle of an (n, m) array.", "The size of the arrays for which the returned indices will be valid.", "Diagonal offset (see triu for details).", "New in version 1.9.0.", "The column dimension of the arrays for which the returned arrays will be valid. By default m is taken equal to n.", "The indices for the triangle. The returned tuple contains two arrays, each with the indices along one dimension of the array. Can be used to slice a ndarray of shape(n, n).", "See also", "similar function, for lower-triangular.", "generic function accepting an arbitrary mask function.", "New in version 1.4.0.", "Compute two different sets of indices to access 4x4 arrays, one for the upper triangular part starting at the main diagonal, and one starting two diagonals further right:", "Here is how they can be used with a sample array:", "Both for indexing:", "And for assigning values:", "These cover only a small part of the whole array (two diagonals right of the main one):"]}, {"name": "numpy.triu_indices_from()", "path": "reference/generated/numpy.triu_indices_from", "type": "numpy.triu_indices_from", "text": ["Return the indices for the upper-triangle of arr.", "See triu_indices for full details.", "The indices will be valid for square arrays.", "Diagonal offset (see triu for details).", "Indices for the upper-triangle of arr.", "See also", "New in version 1.4.0."]}, {"name": "numpy.true_divide()", "path": "reference/generated/numpy.true_divide", "type": "numpy.true_divide", "text": ["Returns a true division of the inputs, element-wise.", "Unlike \u2018floor division\u2019, true division adjusts the output type to present the best answer, regardless of input types.", "Dividend array.", "Divisor array. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "This is a scalar if both x1 and x2 are scalars.", "In Python, // is the floor division operator and / the true division operator. The true_divide(x1, x2) function is equivalent to true division in Python.", "The / operator can be used as a shorthand for np.true_divide on ndarrays."]}, {"name": "numpy.trunc()", "path": "reference/generated/numpy.trunc", "type": "numpy.trunc", "text": ["Return the truncated value of the input, element-wise.", "The truncated value of the scalar x is the nearest integer i which is closer to zero than x is. In short, the fractional part of the signed number x is discarded.", "Input data.", "A location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "The truncated value of each element in x. This is a scalar if x is a scalar.", "See also", "New in version 1.3.0."]}, {"name": "numpy.typename()", "path": "reference/generated/numpy.typename", "type": "numpy.typename", "text": ["Return a description for the given data type code.", "Data type code.", "Description of the input data type code.", "See also"]}, {"name": "numpy.typing.ArrayLike", "path": "reference/typing", "type": "Typing ( \n    \n     numpy.typing\n    \n    )", "text": ["New in version 1.20.", "Large parts of the NumPy API have PEP-484-style type annotations. In addition a number of type aliases are available to users, most prominently the two below:", "New in version 1.21.", "A mypy plugin for managing a number of platform-specific annotations. Its functionality can be split into three distinct parts:", "Assigning the (platform-dependent) precision of c_intp. Without the plugin the type will default to ctypes.c_int64.", "New in version 1.22.", "To enable the plugin, one must add it to their mypy configuration file:", "NumPy is very flexible. Trying to describe the full range of possibilities statically would result in types that are not very helpful. For that reason, the typed NumPy API is often stricter than the runtime NumPy API. This section describes some notable differences.", "The ArrayLike type tries to avoid creating object arrays. For example,", "is valid NumPy code which will create a 0-dimensional object array. Type checkers will complain about the above example when using the NumPy types however. If you really intended to do the above, then you can either use a # type: ignore comment:", "or explicitly type the array like object as Any:", "It\u2019s possible to mutate the dtype of an array at runtime. For example, the following code is valid:", "This sort of mutation is not allowed by the types. Users who want to write statically typed code should instead use the numpy.ndarray.view method to create a view of the array with a different dtype.", "The DTypeLike type tries to avoid creation of dtype objects using dictionary of fields like below:", "Although this is valid NumPy code, the type checker will complain about it, since its usage is discouraged. Please see : Data type objects", "The precision of numpy.number subclasses is treated as a covariant generic parameter (see NBitBase), simplifying the annotating of processes involving precision-based casting.", "Consequently, the likes of float16, float32 and float64 are still sub-types of floating, but, contrary to runtime, they\u2019re not necessarily considered as sub-classes.", "The timedelta64 class is not considered a subclass of signedinteger, the former only inheriting from generic while static type checking.", "During runtime numpy aggressively casts any passed 0D arrays into their corresponding generic instance. Until the introduction of shape typing (see PEP 646) it is unfortunately not possible to make the necessary distinction between 0D and >0D arrays. While thus not strictly correct, all operations are that can potentially perform a 0D-array -> scalar cast are currently annotated as exclusively returning an ndarray.", "If it is known in advance that an operation _will_ perform a 0D-array -> scalar cast, then one can consider manually remedying the situation with either typing.cast or a # type: ignore comment.", "The dtype of numpy.recarray, and the numpy.rec functions in general, can be specified in one of two ways:", "These two approaches are currently typed as being mutually exclusive, i.e. if dtype is specified than one may not specify formats. While this mutual exclusivity is not (strictly) enforced during runtime, combining both dtype specifiers can lead to unexpected or even downright buggy behavior.", "A Union representing objects that can be coerced into an ndarray.", "Among others this includes the likes of:", "New in version 1.20.", "See Also", "Any scalar or sequence that can be interpreted as an ndarray.", "A Union representing objects that can be coerced into a dtype.", "Among others this includes the likes of:", "New in version 1.20.", "See Also", "A comprehensive overview of all objects that can be coerced into data types.", "A generic version of np.ndarray[Any, np.dtype[+ScalarType]].", "Can be used during runtime for typing arrays with a given dtype and unspecified shape.", "New in version 1.21.", "A type representing numpy.number precision during static type checking.", "Used exclusively for the purpose static type checking, NBitBase represents the base of a hierarchical set of subclasses. Each subsequent subclass is herein used for representing a lower level of precision, e.g. 64Bit > 32Bit > 16Bit.", "New in version 1.20.", "Below is a typical usage example: NBitBase is herein used for annotating a function that takes a float and integer of arbitrary precision as arguments and returns a new float of whichever precision is largest (e.g. np.float16 + np.int64 -> np.float64)."]}, {"name": "numpy.typing.DTypeLike", "path": "reference/typing#numpy.typing.DTypeLike", "type": "Typing ( \n    \n     numpy.typing\n    \n    )", "text": ["A Union representing objects that can be coerced into a dtype.", "Among others this includes the likes of:", "New in version 1.20.", "See Also", "A comprehensive overview of all objects that can be coerced into data types."]}, {"name": "numpy.typing.NDArray", "path": "reference/typing#numpy.typing.NDArray", "type": "Typing ( \n    \n     numpy.typing\n    \n    )", "text": ["A generic version of np.ndarray[Any, np.dtype[+ScalarType]].", "Can be used during runtime for typing arrays with a given dtype and unspecified shape.", "New in version 1.21."]}, {"name": "numpy.ubyte", "path": "reference/arrays.scalars#numpy.ubyte", "type": "Scalars", "text": ["Unsigned integer type, compatible with C unsigned char.", "'B'", "numpy.uint8: 8-bit unsigned integer (0 to 255)."]}, {"name": "numpy.ufunc", "path": "reference/generated/numpy.ufunc", "type": "numpy.ufunc", "text": ["Functions that operate element by element on whole arrays.", "To see the documentation for a specific ufunc, use info. For example, np.info(np.sin). Because ufuncs are written in C (for speed) and linked into Python with NumPy\u2019s ufunc facility, Python\u2019s help() function finds this page whenever help() is called on a ufunc.", "A detailed explanation of ufuncs can be found in the docs for Universal functions (ufunc).", "Calling ufuncs: op(*x[, out], where=True, **kwargs)", "Apply op to the arguments *x elementwise, broadcasting the arguments.", "The broadcasting rules are:", "Input arrays.", "Alternate array object(s) in which to put the result; if provided, it must have a shape that the inputs broadcast to. A tuple of arrays (possible only as a keyword argument) must have length equal to the number of outputs; use None for uninitialized outputs to be allocated by the ufunc.", "This condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.", "For other keyword-only arguments, see the ufunc docs.", "r will have the shape that the arrays in x broadcast to; if out is provided, it will be returned. If not, r will be allocated and may contain uninitialized values. If the function has more than one output, then the result will be a tuple of arrays.", "The identity value.", "The number of arguments.", "The number of inputs.", "The number of outputs.", "The number of types.", "Definition of the core elements a generalized ufunc operates on.", "Returns a list with types grouped input->output.", "__call__(*args, **kwargs)", "Call self as a function.", "accumulate(array[, axis, dtype, out])", "Accumulate the result of applying the operator to all elements.", "at(a, indices[, b])", "Performs unbuffered in place operation on operand 'a' for elements specified by 'indices'.", "outer(A, B, /, **kwargs)", "Apply the ufunc op to all pairs (a, b) with a in A and b in B.", "reduce(array[, axis, dtype, out, keepdims, ...])", "Reduces array's dimension by one, by applying ufunc along one axis.", "reduceat(array, indices[, axis, dtype, out])", "Performs a (local) reduce with specified slices over a single axis."]}, {"name": "numpy.uint", "path": "reference/arrays.scalars#numpy.uint", "type": "Scalars", "text": ["Unsigned integer type, compatible with C unsigned long.", "'L'", "numpy.uint64: 64-bit unsigned integer (0 to 18_446_744_073_709_551_615).", "numpy.uintp: Unsigned integer large enough to fit pointer, compatible with C uintptr_t."]}, {"name": "numpy.uint16", "path": "reference/arrays.scalars#numpy.uint16", "type": "Scalars", "text": ["Alias for the unsigned integer types (one of numpy.ubyte, numpy.ushort, numpy.uintc, numpy.uint and numpy.ulonglong) with the specified number of bits.", "Compatible with the C99 uint8_t, uint16_t, uint32_t, and uint64_t, respectively."]}, {"name": "numpy.uint32", "path": "reference/arrays.scalars#numpy.uint32", "type": "Scalars", "text": ["Alias for the unsigned integer types (one of numpy.ubyte, numpy.ushort, numpy.uintc, numpy.uint and numpy.ulonglong) with the specified number of bits.", "Compatible with the C99 uint8_t, uint16_t, uint32_t, and uint64_t, respectively."]}, {"name": "numpy.uint64", "path": "reference/arrays.scalars#numpy.uint64", "type": "Scalars", "text": ["Alias for the unsigned integer types (one of numpy.ubyte, numpy.ushort, numpy.uintc, numpy.uint and numpy.ulonglong) with the specified number of bits.", "Compatible with the C99 uint8_t, uint16_t, uint32_t, and uint64_t, respectively."]}, {"name": "numpy.uint8", "path": "reference/arrays.scalars#numpy.uint8", "type": "Scalars", "text": ["Alias for the unsigned integer types (one of numpy.ubyte, numpy.ushort, numpy.uintc, numpy.uint and numpy.ulonglong) with the specified number of bits.", "Compatible with the C99 uint8_t, uint16_t, uint32_t, and uint64_t, respectively."]}, {"name": "numpy.uintc", "path": "reference/arrays.scalars#numpy.uintc", "type": "Scalars", "text": ["Unsigned integer type, compatible with C unsigned int.", "'I'", "numpy.uint32: 32-bit unsigned integer (0 to 4_294_967_295)."]}, {"name": "numpy.uintp", "path": "reference/arrays.scalars#numpy.uintp", "type": "Scalars", "text": ["Alias for the unsigned integer type (one of numpy.ubyte, numpy.ushort, numpy.uintc, numpy.uint and np.ulonglong) that is the same size as a pointer.", "Compatible with the C uintptr_t.", "'P'"]}, {"name": "numpy.ulonglong", "path": "reference/arrays.scalars#numpy.ulonglong", "type": "Scalars", "text": ["Signed integer type, compatible with C unsigned long long.", "'Q'"]}, {"name": "numpy.unicode_", "path": "reference/arrays.scalars#numpy.unicode_", "type": "Scalars", "text": ["alias of numpy.str_"]}, {"name": "numpy.union1d()", "path": "reference/generated/numpy.union1d", "type": "numpy.union1d", "text": ["Find the union of two arrays.", "Return the unique, sorted array of values that are in either of the two input arrays.", "Input arrays. They are flattened if they are not already 1D.", "Unique, sorted union of the input arrays.", "See also", "Module with a number of other functions for performing set operations on arrays.", "To find the union of more than two arrays, use functools.reduce:"]}, {"name": "numpy.unique()", "path": "reference/generated/numpy.unique", "type": "numpy.unique", "text": ["Find the unique elements of an array.", "Returns the sorted unique elements of an array. There are three optional outputs in addition to the unique elements:", "Input array. Unless axis is specified, this will be flattened if it is not already 1-D.", "If True, also return the indices of ar (along the specified axis, if provided, or in the flattened array) that result in the unique array.", "If True, also return the indices of the unique array (for the specified axis, if provided) that can be used to reconstruct ar.", "If True, also return the number of times each unique item appears in ar.", "New in version 1.9.0.", "The axis to operate on. If None, ar will be flattened. If an integer, the subarrays indexed by the given axis will be flattened and treated as the elements of a 1-D array with the dimension of the given axis, see the notes for more details. Object arrays or structured arrays that contain objects are not supported if the axis kwarg is used. The default is None.", "New in version 1.13.0.", "The sorted unique values.", "The indices of the first occurrences of the unique values in the original array. Only provided if return_index is True.", "The indices to reconstruct the original array from the unique array. Only provided if return_inverse is True.", "The number of times each of the unique values comes up in the original array. Only provided if return_counts is True.", "New in version 1.9.0.", "See also", "Module with a number of other functions for performing set operations on arrays.", "Repeat elements of an array.", "When an axis is specified the subarrays indexed by the axis are sorted. This is done by making the specified axis the first dimension of the array (move the axis to the first dimension to keep the order of the other axes) and then flattening the subarrays in C order. The flattened subarrays are then viewed as a structured type with each element given a label, with the effect that we end up with a 1-D array of structured types that can be treated in the same way as any other 1-D array. The result is that the flattened subarrays are sorted in lexicographic order starting with the first element.", "Return the unique rows of a 2D array", "Return the indices of the original array that give the unique values:", "Reconstruct the input array from the unique values and inverse:", "Reconstruct the input values from the unique values and counts:"]}, {"name": "numpy.unpackbits()", "path": "reference/generated/numpy.unpackbits", "type": "numpy.unpackbits", "text": ["Unpacks elements of a uint8 array into a binary-valued output array.", "Each element of a represents a bit-field that should be unpacked into a binary-valued output array. The shape of the output array is either 1-D (if axis is None) or the same shape as the input array with unpacking done along the axis specified.", "Input array.", "The dimension over which bit-unpacking is done. None implies unpacking the flattened array.", "The number of elements to unpack along axis, provided as a way of undoing the effect of packing a size that is not a multiple of eight. A non-negative number means to only unpack count bits. A negative number means to trim off that many bits from the end. None means to unpack the entire array (the default). Counts larger than the available number of bits will add zero padding to the output. Negative counts must not exceed the available number of bits.", "New in version 1.17.0.", "The order of the returned bits. \u2018big\u2019 will mimic bin(val), 3 = 0b00000011 => [0, 0, 0, 0, 0, 0, 1, 1], \u2018little\u2019 will reverse the order to [1, 1, 0, 0, 0, 0, 0, 0]. Defaults to \u2018big\u2019.", "New in version 1.17.0.", "The elements are binary-valued (0 or 1).", "See also", "Packs the elements of a binary-valued array into bits in a uint8 array."]}, {"name": "numpy.unravel_index()", "path": "reference/generated/numpy.unravel_index", "type": "numpy.unravel_index", "text": ["Converts a flat index or array of flat indices into a tuple of coordinate arrays.", "An integer array whose elements are indices into the flattened version of an array of dimensions shape. Before version 1.6.0, this function accepted just one index value.", "The shape of the array to use for unraveling indices.", "Changed in version 1.16.0: Renamed from dims to shape.", "Determines whether the indices should be viewed as indexing in row-major (C-style) or column-major (Fortran-style) order.", "New in version 1.6.0.", "Each array in the tuple has the same shape as the indices array.", "See also"]}, {"name": "numpy.unwrap()", "path": "reference/generated/numpy.unwrap", "type": "numpy.unwrap", "text": ["Unwrap by taking the complement of large deltas with respect to the period.", "This unwraps a signal p by changing elements which have an absolute difference from their predecessor of more than max(discont, period/2) to their period-complementary values.", "For the default case where period is \\(2\\pi\\) and discont is \\(\\pi\\), this unwraps a radian phase p such that adjacent differences are never greater than \\(\\pi\\) by adding \\(2k\\pi\\) for some integer \\(k\\).", "Input array.", "Maximum discontinuity between values, default is period/2. Values below period/2 are treated as if they were period/2. To have an effect different from the default, discont should be larger than period/2.", "Axis along which unwrap will operate, default is the last axis.", "Size of the range over which the input wraps. By default, it is 2 pi.", "New in version 1.21.0.", "Output array.", "See also", "If the discontinuity in p is smaller than period/2, but larger than discont, no unwrapping is done because taking the complement would only make the discontinuity larger."]}, {"name": "numpy.ushort", "path": "reference/arrays.scalars#numpy.ushort", "type": "Scalars", "text": ["Unsigned integer type, compatible with C unsigned short.", "'H'", "numpy.uint16: 16-bit unsigned integer (0 to 65_535)."]}, {"name": "numpy.vander()", "path": "reference/generated/numpy.vander", "type": "numpy.vander", "text": ["Generate a Vandermonde matrix.", "The columns of the output matrix are powers of the input vector. The order of the powers is determined by the increasing boolean argument. Specifically, when increasing is False, the i-th output column is the input vector raised element-wise to the power of N - i - 1. Such a matrix with a geometric progression in each row is named for Alexandre- Theophile Vandermonde.", "1-D input array.", "Number of columns in the output. If N is not specified, a square array is returned (N = len(x)).", "Order of the powers of the columns. If True, the powers increase from left to right, if False (the default) they are reversed.", "New in version 1.9.0.", "Vandermonde matrix. If increasing is False, the first column is x^(N-1), the second x^(N-2) and so forth. If increasing is True, the columns are x^0, x^1, ..., x^(N-1).", "See also", "The determinant of a square Vandermonde matrix is the product of the differences between the values of the input vector:"]}, {"name": "numpy.var()", "path": "reference/generated/numpy.var", "type": "numpy.var", "text": ["Compute the variance along the specified axis.", "Returns the variance of the array elements, a measure of the spread of a distribution. The variance is computed for the flattened array by default, otherwise over the specified axis.", "Array containing numbers whose variance is desired. If a is not an array, a conversion is attempted.", "Axis or axes along which the variance is computed. The default is to compute the variance of the flattened array.", "New in version 1.7.0.", "If this is a tuple of ints, a variance is performed over multiple axes, instead of a single axis or all the axes as before.", "Type to use in computing the variance. For arrays of integer type the default is float64; for arrays of float types it is the same as the array type.", "Alternate output array in which to place the result. It must have the same shape as the expected output, but the type is cast if necessary.", "\u201cDelta Degrees of Freedom\u201d: the divisor used in the calculation is N - ddof, where N represents the number of elements. By default ddof is zero.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.", "If the default value is passed, then keepdims will not be passed through to the var method of sub-classes of ndarray, however any non-default value will be. If the sub-class\u2019 method does not implement keepdims any exceptions will be raised.", "Elements to include in the variance. See reduce for details.", "New in version 1.20.0.", "If out=None, returns a new array containing the variance; otherwise, a reference to the output array is returned.", "See also", "The variance is the average of the squared deviations from the mean, i.e., var = mean(x), where x = abs(a - a.mean())**2.", "The mean is typically calculated as x.sum() / N, where N = len(x). If, however, ddof is specified, the divisor N - ddof is used instead. In standard statistical practice, ddof=1 provides an unbiased estimator of the variance of a hypothetical infinite population. ddof=0 provides a maximum likelihood estimate of the variance for normally distributed variables.", "Note that for complex numbers, the absolute value is taken before squaring, so that the result is always real and nonnegative.", "For floating-point input, the variance is computed using the same precision the input has. Depending on the input data, this can cause the results to be inaccurate, especially for float32 (see example below). Specifying a higher-accuracy accumulator using the dtype keyword can alleviate this issue.", "In single precision, var() can be inaccurate:", "Computing the variance in float64 is more accurate:", "Specifying a where argument:"]}, {"name": "numpy.vdot()", "path": "reference/generated/numpy.vdot", "type": "numpy.vdot", "text": ["Return the dot product of two vectors.", "The vdot(a, b) function handles complex numbers differently than dot(a, b). If the first argument is complex the complex conjugate of the first argument is used for the calculation of the dot product.", "Note that vdot handles multidimensional arrays differently than dot: it does not perform a matrix product, but flattens input arguments to 1-D vectors first. Consequently, it should only be used for vectors.", "If a is complex the complex conjugate is taken before calculation of the dot product.", "Second argument to the dot product.", "Dot product of a and b. Can be an int, float, or complex depending on the types of a and b.", "See also", "Return the dot product without using the complex conjugate of the first argument.", "Note that higher-dimensional arrays are flattened!"]}, {"name": "numpy.vectorize()", "path": "reference/generated/numpy.vectorize", "type": "numpy.vectorize", "text": ["Generalized function class.", "Define a vectorized function which takes a nested sequence of objects or numpy arrays as inputs and returns a single numpy array or a tuple of numpy arrays. The vectorized function evaluates pyfunc over successive tuples of the input arrays like the python map function, except it uses the broadcasting rules of numpy.", "The data type of the output of vectorized is determined by calling the function with the first element of the input. This can be avoided by specifying the otypes argument.", "A python function or method.", "The output data type. It must be specified as either a string of typecode characters or a list of data type specifiers. There should be one data type specifier for each output.", "The docstring for the function. If None, the docstring will be the pyfunc.__doc__.", "Set of strings or integers representing the positional or keyword arguments for which the function will not be vectorized. These will be passed directly to pyfunc unmodified.", "New in version 1.7.0.", "If True, then cache the first function call that determines the number of outputs if otypes is not provided.", "New in version 1.7.0.", "Generalized universal function signature, e.g., (m,n),(n)->(m) for vectorized matrix-vector multiplication. If provided, pyfunc will be called with (and expected to return) arrays with shapes given by the size of corresponding core dimensions. By default, pyfunc is assumed to take scalars as input and output.", "New in version 1.12.0.", "Vectorized function.", "See also", "Takes an arbitrary Python function and returns a ufunc", "The vectorize function is provided primarily for convenience, not for performance. The implementation is essentially a for loop.", "If otypes is not specified, then a call to the function with the first argument will be used to determine the number of outputs. The results of this call will be cached if cache is True to prevent calling the function twice. However, to implement the cache, the original function must be wrapped which will slow down subsequent calls, so only do this if your function is expensive.", "The new keyword argument interface and excluded argument support further degrades performance.", "Generalized Universal Function API", "The docstring is taken from the input function to vectorize unless it is specified:", "The output type is determined by evaluating the first element of the input, unless it is specified:", "The excluded argument can be used to prevent vectorizing over certain arguments. This can be useful for array-like arguments of a fixed length such as the coefficients for a polynomial as in polyval:", "Positional arguments may also be excluded by specifying their position:", "The signature argument allows for vectorizing functions that act on non-scalar arrays of fixed length. For example, you can use it for a vectorized calculation of Pearson correlation coefficient and its p-value:", "Or for a vectorized convolution:", "__call__(*args, **kwargs)", "Return arrays with the results of pyfunc broadcast (vectorized) over args and kwargs not in excluded."]}, {"name": "numpy.void", "path": "reference/arrays.scalars#numpy.void", "type": "Scalars", "text": ["Either an opaque sequence of bytes, or a structure.", "Structured void scalars can only be constructed via extraction from Structured arrays:", "'V'"]}, {"name": "numpy.vsplit()", "path": "reference/generated/numpy.vsplit", "type": "numpy.vsplit", "text": ["Split an array into multiple sub-arrays vertically (row-wise).", "Please refer to the split documentation. vsplit is equivalent to split with axis=0 (default), the array is always split along the first axis regardless of the array dimension.", "See also", "Split an array into multiple sub-arrays of equal size.", "With a higher dimensional array the split is still along the first axis."]}, {"name": "numpy.vstack()", "path": "reference/generated/numpy.vstack", "type": "numpy.vstack", "text": ["Stack arrays in sequence vertically (row wise).", "This is equivalent to concatenation along the first axis after 1-D arrays of shape (N,) have been reshaped to (1,N). Rebuilds arrays divided by vsplit.", "This function makes most sense for arrays with up to 3 dimensions. For instance, for pixel-data with a height (first axis), width (second axis), and r/g/b channels (third axis). The functions concatenate, stack and block provide more general stacking and concatenation operations.", "The arrays must have the same shape along all but the first axis. 1-D arrays must have the same length.", "The array formed by stacking the given arrays, will be at least 2-D.", "See also", "Join a sequence of arrays along an existing axis.", "Join a sequence of arrays along a new axis.", "Assemble an nd-array from nested lists of blocks.", "Stack arrays in sequence horizontally (column wise).", "Stack arrays in sequence depth wise (along third axis).", "Stack 1-D arrays as columns into a 2-D array.", "Split an array into multiple sub-arrays vertically (row-wise)."]}, {"name": "numpy.where()", "path": "reference/generated/numpy.where", "type": "numpy.where", "text": ["Return elements chosen from x or y depending on condition.", "Note", "When only condition is provided, this function is a shorthand for np.asarray(condition).nonzero(). Using nonzero directly should be preferred, as it behaves correctly for subclasses. The rest of this documentation covers only the case where all three arguments are provided.", "Where True, yield x, otherwise yield y.", "Values from which to choose. x, y and condition need to be broadcastable to some shape.", "An array with elements from x where condition is True, and elements from y elsewhere.", "See also", "The function that is called when x and y are omitted", "If all the arrays are 1-D, where is equivalent to:", "This can be used on multidimensional arrays too:", "The shapes of x, y, and the condition are broadcast together:"]}, {"name": "numpy.who()", "path": "reference/generated/numpy.who", "type": "numpy.who", "text": ["Print the NumPy arrays in the given dictionary.", "If there is no dictionary passed in or vardict is None then returns NumPy arrays in the globals() dictionary (all NumPy arrays in the namespace).", "A dictionary possibly containing ndarrays. Default is globals().", "Returns \u2018None\u2019.", "Prints out the name, shape, bytes and type of all of the ndarrays present in vardict."]}, {"name": "numpy.zeros()", "path": "reference/generated/numpy.zeros", "type": "numpy.zeros", "text": ["Return a new array of given shape and type, filled with zeros.", "Shape of the new array, e.g., (2, 3) or 2.", "The desired data-type for the array, e.g., numpy.int8. Default is numpy.float64.", "Whether to store multi-dimensional data in row-major (C-style) or column-major (Fortran-style) order in memory.", "Reference object to allow the creation of arrays which are not NumPy arrays. If an array-like passed in as like supports the __array_function__ protocol, the result will be defined by it. In this case, it ensures the creation of an array object compatible with that passed in via this argument.", "New in version 1.20.0.", "Array of zeros with the given shape, dtype, and order.", "See also", "Return an array of zeros with shape and type of input.", "Return a new uninitialized array.", "Return a new array setting values to one.", "Return a new array of given shape filled with value."]}, {"name": "numpy.zeros_like()", "path": "reference/generated/numpy.zeros_like", "type": "numpy.zeros_like", "text": ["Return an array of zeros with the same shape and type as a given array.", "The shape and data-type of a define these same attributes of the returned array.", "Overrides the data type of the result.", "New in version 1.6.0.", "Overrides the memory layout of the result. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of a as closely as possible.", "New in version 1.6.0.", "If True, then the newly created array will use the sub-class type of a, otherwise it will be a base-class array. Defaults to True.", "Overrides the shape of the result. If order=\u2019K\u2019 and the number of dimensions is unchanged, will try to keep order, otherwise, order=\u2019C\u2019 is implied.", "New in version 1.17.0.", "Array of zeros with the same shape and type as a.", "See also", "Return an empty array with shape and type of input.", "Return an array of ones with shape and type of input.", "Return a new array with shape of input filled with value.", "Return a new array setting values to zero."]}, {"name": "NumPy: the absolute basics for beginners", "path": "user/absolute_beginners", "type": "User Guide", "text": ["Welcome to the absolute beginner\u2019s guide to NumPy! If you have comments or suggestions, please don\u2019t hesitate to reach out!", "NumPy (Numerical Python) is an open source Python library that\u2019s used in almost every field of science and engineering. It\u2019s the universal standard for working with numerical data in Python, and it\u2019s at the core of the scientific Python and PyData ecosystems. NumPy users include everyone from beginning coders to experienced researchers doing state-of-the-art scientific and industrial research and development. The NumPy API is used extensively in Pandas, SciPy, Matplotlib, scikit-learn, scikit-image and most other data science and scientific Python packages.", "The NumPy library contains multidimensional array and matrix data structures (you\u2019ll find more information about this in later sections). It provides ndarray, a homogeneous n-dimensional array object, with methods to efficiently operate on it. NumPy can be used to perform a wide variety of mathematical operations on arrays. It adds powerful data structures to Python that guarantee efficient calculations with arrays and matrices and it supplies an enormous library of high-level mathematical functions that operate on these arrays and matrices.", "Learn more about NumPy here!", "To install NumPy, we strongly recommend using a scientific Python distribution. If you\u2019re looking for the full instructions for installing NumPy on your operating system, see Installing NumPy.", "If you already have Python, you can install NumPy with:", "or", "If you don\u2019t have Python yet, you might want to consider using Anaconda. It\u2019s the easiest way to get started. The good thing about getting this distribution is the fact that you don\u2019t need to worry too much about separately installing NumPy or any of the major packages that you\u2019ll be using for your data analyses, like pandas, Scikit-Learn, etc.", "To access NumPy and its functions import it in your Python code like this:", "We shorten the imported name to np for better readability of code using NumPy. This is a widely adopted convention that you should follow so that anyone working with your code can easily understand it.", "If you aren\u2019t already comfortable with reading tutorials that contain a lot of code, you might not know how to interpret a code block that looks like this:", "If you aren\u2019t familiar with this style, it\u2019s very easy to understand. If you see >>>, you\u2019re looking at input, or the code that you would enter. Everything that doesn\u2019t have >>> in front of it is output, or the results of running your code. This is the style you see when you run python on the command line, but if you\u2019re using IPython, you might see a different style. Note that it is not part of the code and will cause an error if typed or pasted into the Python shell. It can be safely typed or pasted into the IPython shell; the >>> is ignored.", "NumPy gives you an enormous range of fast and efficient ways of creating arrays and manipulating numerical data inside them. While a Python list can contain different data types within a single list, all of the elements in a NumPy array should be homogeneous. The mathematical operations that are meant to be performed on arrays would be extremely inefficient if the arrays weren\u2019t homogeneous.", "Why use NumPy?", "NumPy arrays are faster and more compact than Python lists. An array consumes less memory and is convenient to use. NumPy uses much less memory to store data and it provides a mechanism of specifying the data types. This allows the code to be optimized even further.", "An array is a central data structure of the NumPy library. An array is a grid of values and it contains information about the raw data, how to locate an element, and how to interpret an element. It has a grid of elements that can be indexed in various ways. The elements are all of the same type, referred to as the array dtype.", "An array can be indexed by a tuple of nonnegative integers, by booleans, by another array, or by integers. The rank of the array is the number of dimensions. The shape of the array is a tuple of integers giving the size of the array along each dimension.", "One way we can initialize NumPy arrays is from Python lists, using nested lists for two- or higher-dimensional data.", "For example:", "or:", "We can access the elements in the array using square brackets. When you\u2019re accessing elements, remember that indexing in NumPy starts at 0. That means that if you want to access the first element in your array, you\u2019ll be accessing element \u201c0\u201d.", "This section covers 1D array, 2D array, ndarray, vector, matrix", "You might occasionally hear an array referred to as a \u201cndarray,\u201d which is shorthand for \u201cN-dimensional array.\u201d An N-dimensional array is simply an array with any number of dimensions. You might also hear 1-D, or one-dimensional array, 2-D, or two-dimensional array, and so on. The NumPy ndarray class is used to represent both matrices and vectors. A vector is an array with a single dimension (there\u2019s no difference between row and column vectors), while a matrix refers to an array with two dimensions. For 3-D or higher dimensional arrays, the term tensor is also commonly used.", "What are the attributes of an array?", "An array is usually a fixed-size container of items of the same type and size. The number of dimensions and items in an array is defined by its shape. The shape of an array is a tuple of non-negative integers that specify the sizes of each dimension.", "In NumPy, dimensions are called axes. This means that if you have a 2D array that looks like this:", "Your array has 2 axes. The first axis has a length of 2 and the second axis has a length of 3.", "Just like in other Python container objects, the contents of an array can be accessed and modified by indexing or slicing the array. Unlike the typical container objects, different arrays can share the same data, so changes made on one array might be visible in another.", "Array attributes reflect information intrinsic to the array itself. If you need to get, or even set, properties of an array without creating a new array, you can often access an array through its attributes.", "Read more about array attributes here and learn about array objects here.", "This section covers np.array(), np.zeros(), np.ones(), np.empty(), np.arange(), np.linspace(), dtype", "To create a NumPy array, you can use the function np.array().", "All you need to do to create a simple array is pass a list to it. If you choose to, you can also specify the type of data in your list. You can find more information about data types here.", "You can visualize your array this way:", "Be aware that these visualizations are meant to simplify ideas and give you a basic understanding of NumPy concepts and mechanics. Arrays and array operations are much more complicated than are captured here!", "Besides creating an array from a sequence of elements, you can easily create an array filled with 0\u2019s:", "Or an array filled with 1\u2019s:", "Or even an empty array! The function empty creates an array whose initial content is random and depends on the state of the memory. The reason to use empty over zeros (or something similar) is speed - just make sure to fill every element afterwards!", "You can create an array with a range of elements:", "And even an array that contains a range of evenly spaced intervals. To do this, you will specify the first number, last number, and the step size.", "You can also use np.linspace() to create an array with values that are spaced linearly in a specified interval:", "Specifying your data type", "While the default data type is floating point (np.float64), you can explicitly specify which data type you want using the dtype keyword.", "Learn more about creating arrays here", "This section covers np.sort(), np.concatenate()", "Sorting an element is simple with np.sort(). You can specify the axis, kind, and order when you call the function.", "If you start with this array:", "You can quickly sort the numbers in ascending order with:", "In addition to sort, which returns a sorted copy of an array, you can use:", "To read more about sorting an array, see: sort.", "If you start with these arrays:", "You can concatenate them with np.concatenate().", "Or, if you start with these arrays:", "You can concatenate them with:", "In order to remove elements from an array, it\u2019s simple to use indexing to select the elements that you want to keep.", "To read more about concatenate, see: concatenate.", "This section covers ndarray.ndim, ndarray.size, ndarray.shape", "ndarray.ndim will tell you the number of axes, or dimensions, of the array.", "ndarray.size will tell you the total number of elements of the array. This is the product of the elements of the array\u2019s shape.", "ndarray.shape will display a tuple of integers that indicate the number of elements stored along each dimension of the array. If, for example, you have a 2-D array with 2 rows and 3 columns, the shape of your array is (2, 3).", "For example, if you create this array:", "To find the number of dimensions of the array, run:", "To find the total number of elements in the array, run:", "And to find the shape of your array, run:", "This section covers arr.reshape()", "Yes!", "Using arr.reshape() will give a new shape to an array without changing the data. Just remember that when you use the reshape method, the array you want to produce needs to have the same number of elements as the original array. If you start with an array with 12 elements, you\u2019ll need to make sure that your new array also has a total of 12 elements.", "If you start with this array:", "You can use reshape() to reshape your array. For example, you can reshape this array to an array with three rows and two columns:", "With np.reshape, you can specify a few optional parameters:", "a is the array to be reshaped.", "newshape is the new shape you want. You can specify an integer or a tuple of integers. If you specify an integer, the result will be an array of that length. The shape should be compatible with the original shape.", "order: C means to read/write the elements using C-like index order, F means to read/write the elements using Fortran-like index order, A means to read/write the elements in Fortran-like index order if a is Fortran contiguous in memory, C-like order otherwise. (This is an optional parameter and doesn\u2019t need to be specified.)", "If you want to learn more about C and Fortran order, you can read more about the internal organization of NumPy arrays here. Essentially, C and Fortran orders have to do with how indices correspond to the order the array is stored in memory. In Fortran, when moving through the elements of a two-dimensional array as it is stored in memory, the first index is the most rapidly varying index. As the first index moves to the next row as it changes, the matrix is stored one column at a time. This is why Fortran is thought of as a Column-major language. In C on the other hand, the last index changes the most rapidly. The matrix is stored by rows, making it a Row-major language. What you do for C or Fortran depends on whether it\u2019s more important to preserve the indexing convention or not reorder the data.", "Learn more about shape manipulation here.", "This section covers np.newaxis, np.expand_dims", "You can use np.newaxis and np.expand_dims to increase the dimensions of your existing array.", "Using np.newaxis will increase the dimensions of your array by one dimension when used once. This means that a 1D array will become a 2D array, a 2D array will become a 3D array, and so on.", "For example, if you start with this array:", "You can use np.newaxis to add a new axis:", "You can explicitly convert a 1D array with either a row vector or a column vector using np.newaxis. For example, you can convert a 1D array to a row vector by inserting an axis along the first dimension:", "Or, for a column vector, you can insert an axis along the second dimension:", "You can also expand an array by inserting a new axis at a specified position with np.expand_dims.", "For example, if you start with this array:", "You can use np.expand_dims to add an axis at index position 1 with:", "You can add an axis at index position 0 with:", "Find more information about newaxis here and expand_dims at expand_dims.", "You can index and slice NumPy arrays in the same ways you can slice Python lists.", "You can visualize it this way:", "You may want to take a section of your array or specific array elements to use in further analysis or additional operations. To do that, you\u2019ll need to subset, slice, and/or index your arrays.", "If you want to select values from your array that fulfill certain conditions, it\u2019s straightforward with NumPy.", "For example, if you start with this array:", "You can easily print all of the values in the array that are less than 5.", "You can also select, for example, numbers that are equal to or greater than 5, and use that condition to index an array.", "You can select elements that are divisible by 2:", "Or you can select elements that satisfy two conditions using the & and | operators:", "You can also make use of the logical operators & and | in order to return boolean values that specify whether or not the values in an array fulfill a certain condition. This can be useful with arrays that contain names or other categorical values.", "You can also use np.nonzero() to select elements or indices from an array.", "Starting with this array:", "You can use np.nonzero() to print the indices of elements that are, for example, less than 5:", "In this example, a tuple of arrays was returned: one for each dimension. The first array represents the row indices where these values are found, and the second array represents the column indices where the values are found.", "If you want to generate a list of coordinates where the elements exist, you can zip the arrays, iterate over the list of coordinates, and print them. For example:", "You can also use np.nonzero() to print the elements in an array that are less than 5 with:", "If the element you\u2019re looking for doesn\u2019t exist in the array, then the returned array of indices will be empty. For example:", "Learn more about indexing and slicing here and here.", "Read more about using the nonzero function at: nonzero.", "This section covers slicing and indexing, np.vstack(), np.hstack(), np.hsplit(), .view(), copy()", "You can easily create a new array from a section of an existing array.", "Let\u2019s say you have this array:", "You can create a new array from a section of your array any time by specifying where you want to slice your array.", "Here, you grabbed a section of your array from index position 3 through index position 8.", "You can also stack two existing arrays, both vertically and horizontally. Let\u2019s say you have two arrays, a1 and a2:", "You can stack them vertically with vstack:", "Or stack them horizontally with hstack:", "You can split an array into several smaller arrays using hsplit. You can specify either the number of equally shaped arrays to return or the columns after which the division should occur.", "Let\u2019s say you have this array:", "If you wanted to split this array into three equally shaped arrays, you would run:", "If you wanted to split your array after the third and fourth column, you\u2019d run:", "Learn more about stacking and splitting arrays here.", "You can use the view method to create a new array object that looks at the same data as the original array (a shallow copy).", "Views are an important NumPy concept! NumPy functions, as well as operations like indexing and slicing, will return views whenever possible. This saves memory and is faster (no copy of the data has to be made). However it\u2019s important to be aware of this - modifying data in a view also modifies the original array!", "Let\u2019s say you create this array:", "Now we create an array b1 by slicing a and modify the first element of b1. This will modify the corresponding element in a as well!", "Using the copy method will make a complete copy of the array and its data (a deep copy). To use this on your array, you could run:", "Learn more about copies and views here.", "This section covers addition, subtraction, multiplication, division, and more", "Once you\u2019ve created your arrays, you can start to work with them. Let\u2019s say, for example, that you\u2019ve created two arrays, one called \u201cdata\u201d and one called \u201cones\u201d", "You can add the arrays together with the plus sign.", "You can, of course, do more than just addition!", "Basic operations are simple with NumPy. If you want to find the sum of the elements in an array, you\u2019d use sum(). This works for 1D arrays, 2D arrays, and arrays in higher dimensions.", "To add the rows or the columns in a 2D array, you would specify the axis.", "If you start with this array:", "You can sum over the axis of rows with:", "You can sum over the axis of columns with:", "Learn more about basic operations here.", "There are times when you might want to carry out an operation between an array and a single number (also called an operation between a vector and a scalar) or between arrays of two different sizes. For example, your array (we\u2019ll call it \u201cdata\u201d) might contain information about distance in miles but you want to convert the information to kilometers. You can perform this operation with:", "NumPy understands that the multiplication should happen with each cell. That concept is called broadcasting. Broadcasting is a mechanism that allows NumPy to perform operations on arrays of different shapes. The dimensions of your array must be compatible, for example, when the dimensions of both arrays are equal or when one of them is 1. If the dimensions are not compatible, you will get a ValueError.", "Learn more about broadcasting here.", "This section covers maximum, minimum, sum, mean, product, standard deviation, and more", "NumPy also performs aggregation functions. In addition to min, max, and sum, you can easily run mean to get the average, prod to get the result of multiplying the elements together, std to get the standard deviation, and more.", "Let\u2019s start with this array, called \u201ca\u201d", "It\u2019s very common to want to aggregate along a row or column. By default, every NumPy aggregation function will return the aggregate of the entire array. To find the sum or the minimum of the elements in your array, run:", "Or:", "You can specify on which axis you want the aggregation function to be computed. For example, you can find the minimum value within each column by specifying axis=0.", "The four values listed above correspond to the number of columns in your array. With a four-column array, you will get four values as your result.", "Read more about array methods here.", "You can pass Python lists of lists to create a 2-D array (or \u201cmatrix\u201d) to represent them in NumPy.", "Indexing and slicing operations are useful when you\u2019re manipulating matrices:", "You can aggregate matrices the same way you aggregated vectors:", "You can aggregate all the values in a matrix and you can aggregate them across columns or rows using the axis parameter. To illustrate this point, let\u2019s look at a slightly modified dataset:", "Once you\u2019ve created your matrices, you can add and multiply them using arithmetic operators if you have two matrices that are the same size.", "You can do these arithmetic operations on matrices of different sizes, but only if one matrix has only one column or one row. In this case, NumPy will use its broadcast rules for the operation.", "Be aware that when NumPy prints N-dimensional arrays, the last axis is looped over the fastest while the first axis is the slowest. For instance:", "There are often instances where we want NumPy to initialize the values of an array. NumPy offers functions like ones() and zeros(), and the random.Generator class for random number generation for that. All you need to do is pass in the number of elements you want it to generate:", "You can also use ones(), zeros(), and random() to create a 2D array if you give them a tuple describing the dimensions of the matrix:", "Read more about creating arrays, filled with 0\u2019s, 1\u2019s, other values or uninitialized, at array creation routines.", "The use of random number generation is an important part of the configuration and evaluation of many numerical and machine learning algorithms. Whether you need to randomly initialize weights in an artificial neural network, split data into random sets, or randomly shuffle your dataset, being able to generate random numbers (actually, repeatable pseudo-random numbers) is essential.", "With Generator.integers, you can generate random integers from low (remember that this is inclusive with NumPy) to high (exclusive). You can set endpoint=True to make the high number inclusive.", "You can generate a 2 x 4 array of random integers between 0 and 4 with:", "Read more about random number generation here.", "This section covers np.unique()", "You can find the unique elements in an array easily with np.unique.", "For example, if you start with this array:", "you can use np.unique to print the unique values in your array:", "To get the indices of unique values in a NumPy array (an array of first index positions of unique values in the array), just pass the return_index argument in np.unique() as well as your array.", "You can pass the return_counts argument in np.unique() along with your array to get the frequency count of unique values in a NumPy array.", "This also works with 2D arrays! If you start with this array:", "You can find unique values with:", "If the axis argument isn\u2019t passed, your 2D array will be flattened.", "If you want to get the unique rows or columns, make sure to pass the axis argument. To find the unique rows, specify axis=0 and for columns, specify axis=1.", "To get the unique rows, index position, and occurrence count, you can use:", "To learn more about finding the unique elements in an array, see unique.", "This section covers arr.reshape(), arr.transpose(), arr.T", "It\u2019s common to need to transpose your matrices. NumPy arrays have the property T that allows you to transpose a matrix.", "You may also need to switch the dimensions of a matrix. This can happen when, for example, you have a model that expects a certain input shape that is different from your dataset. This is where the reshape method can be useful. You simply need to pass in the new dimensions that you want for the matrix.", "You can also use .transpose() to reverse or change the axes of an array according to the values you specify.", "If you start with this array:", "You can transpose your array with arr.transpose().", "You can also use arr.T:", "To learn more about transposing and reshaping arrays, see transpose and reshape.", "This section covers np.flip()", "NumPy\u2019s np.flip() function allows you to flip, or reverse, the contents of an array along an axis. When using np.flip(), specify the array you would like to reverse and the axis. If you don\u2019t specify the axis, NumPy will reverse the contents along all of the axes of your input array.", "Reversing a 1D array", "If you begin with a 1D array like this one:", "You can reverse it with:", "If you want to print your reversed array, you can run:", "Reversing a 2D array", "A 2D array works much the same way.", "If you start with this array:", "You can reverse the content in all of the rows and all of the columns with:", "You can easily reverse only the rows with:", "Or reverse only the columns with:", "You can also reverse the contents of only one column or row. For example, you can reverse the contents of the row at index position 1 (the second row):", "You can also reverse the column at index position 1 (the second column):", "Read more about reversing arrays at flip.", "This section covers .flatten(), ravel()", "There are two popular ways to flatten an array: .flatten() and .ravel(). The primary difference between the two is that the new array created using ravel() is actually a reference to the parent array (i.e., a \u201cview\u201d). This means that any changes to the new array will affect the parent array as well. Since ravel does not create a copy, it\u2019s memory efficient.", "If you start with this array:", "You can use flatten to flatten your array into a 1D array.", "When you use flatten, changes to your new array won\u2019t change the parent array.", "For example:", "But when you use ravel, the changes you make to the new array will affect the parent array.", "For example:", "Read more about flatten at ndarray.flatten and ravel at ravel.", "This section covers help(), ?, ??", "When it comes to the data science ecosystem, Python and NumPy are built with the user in mind. One of the best examples of this is the built-in access to documentation. Every object contains the reference to a string, which is known as the docstring. In most cases, this docstring contains a quick and concise summary of the object and how to use it. Python has a built-in help() function that can help you access this information. This means that nearly any time you need more information, you can use help() to quickly find the information that you need.", "For example:", "Because access to additional information is so useful, IPython uses the ? character as a shorthand for accessing this documentation along with other relevant information. IPython is a command shell for interactive computing in multiple languages. You can find more information about IPython here.", "For example:", "You can even use this notation for object methods and objects themselves.", "Let\u2019s say you create this array:", "Then you can obtain a lot of useful information (first details about a itself, followed by the docstring of ndarray of which a is an instance):", "This also works for functions and other objects that you create. Just remember to include a docstring with your function using a string literal (\"\"\" \"\"\" or ''' ''' around your documentation).", "For example, if you create this function:", "You can obtain information about the function:", "You can reach another level of information by reading the source code of the object you\u2019re interested in. Using a double question mark (??) allows you to access the source code.", "For example:", "If the object in question is compiled in a language other than Python, using ?? will return the same information as ?. You\u2019ll find this with a lot of built-in objects and types, for example:", "and :", "have the same output because they were compiled in a programming language other than Python.", "The ease of implementing mathematical formulas that work on arrays is one of the things that make NumPy so widely used in the scientific Python community.", "For example, this is the mean square error formula (a central formula used in supervised machine learning models that deal with regression):", "Implementing this formula is simple and straightforward in NumPy:", "What makes this work so well is that predictions and labels can contain one or a thousand values. They only need to be the same size.", "You can visualize it this way:", "In this example, both the predictions and labels vectors contain three values, meaning n has a value of three. After we carry out subtractions the values in the vector are squared. Then NumPy sums the values, and your result is the error value for that prediction and a score for the quality of the model.", "This section covers np.save, np.savez, np.savetxt, np.load, np.loadtxt", "You will, at some point, want to save your arrays to disk and load them back without having to re-run the code. Fortunately, there are several ways to save and load objects with NumPy. The ndarray objects can be saved to and loaded from the disk files with loadtxt and savetxt functions that handle normal text files, load and save functions that handle NumPy binary files with a .npy file extension, and a savez function that handles NumPy files with a .npz file extension.", "The .npy and .npz files store data, shape, dtype, and other information required to reconstruct the ndarray in a way that allows the array to be correctly retrieved, even when the file is on another machine with different architecture.", "If you want to store a single ndarray object, store it as a .npy file using np.save. If you want to store more than one ndarray object in a single file, save it as a .npz file using np.savez. You can also save several arrays into a single file in compressed npz format with savez_compressed.", "It\u2019s easy to save and load and array with np.save(). Just make sure to specify the array you want to save and a file name. For example, if you create this array:", "You can save it as \u201cfilename.npy\u201d with:", "You can use np.load() to reconstruct your array.", "If you want to check your array, you can run::", "You can save a NumPy array as a plain text file like a .csv or .txt file with np.savetxt.", "For example, if you create this array:", "You can easily save it as a .csv file with the name \u201cnew_file.csv\u201d like this:", "You can quickly and easily load your saved text file using loadtxt():", "The savetxt() and loadtxt() functions accept additional optional parameters such as header, footer, and delimiter. While text files can be easier for sharing, .npy and .npz files are smaller and faster to read. If you need more sophisticated handling of your text file (for example, if you need to work with lines that contain missing values), you will want to use the genfromtxt function.", "With savetxt, you can specify headers, footers, comments, and more.", "Learn more about input and output routines here.", "It\u2019s simple to read in a CSV that contains existing information. The best and easiest way to do this is to use Pandas.", "It\u2019s simple to use Pandas in order to export your array as well. If you are new to NumPy, you may want to create a Pandas dataframe from the values in your array and then write the data frame to a CSV file with Pandas.", "If you created this array \u201ca\u201d", "You could create a Pandas dataframe", "You can easily save your dataframe with:", "And read your CSV with:", "You can also save your array with the NumPy savetxt method.", "If you\u2019re using the command line, you can read your saved CSV any time with a command such as:", "Or you can open the file any time with a text editor!", "If you\u2019re interested in learning more about Pandas, take a look at the official Pandas documentation. Learn how to install Pandas with the official Pandas installation information.", "If you need to generate a plot for your values, it\u2019s very simple with Matplotlib.", "For example, you may have an array like this one:", "If you already have Matplotlib installed, you can import it with:", "All you need to do to plot your values is run:", "For example, you can plot a 1D array like this:", "With Matplotlib, you have access to an enormous number of visualization options.", "To read more about Matplotlib and what it can do, take a look at the official documentation. For directions regarding installing Matplotlib, see the official installation section.", "Image credits: Jay Alammar http://jalammar.github.io/"]}, {"name": "Optionally SciPy-accelerated routines (numpy.dual)", "path": "reference/routines.dual", "type": "Optionally SciPy-accelerated routines ( \n      \n       numpy.dual\n      \n      )", "text": ["Deprecated since version 1.20.", "This module is deprecated. Instead of importing functions from numpy.dual, the functions should be imported directly from NumPy or SciPy.", "Aliases for functions which may be accelerated by SciPy.", "SciPy can be built to use accelerated or otherwise improved libraries for FFTs, linear algebra, and special functions. This module allows developers to transparently support these accelerated functions when SciPy is available but still support users who have only installed NumPy.", "cholesky(a)", "Cholesky decomposition.", "det(a)", "Compute the determinant of an array.", "eig(a)", "Compute the eigenvalues and right eigenvectors of a square array.", "eigh(a[, UPLO])", "Return the eigenvalues and eigenvectors of a complex Hermitian (conjugate symmetric) or a real symmetric matrix.", "eigvals(a)", "Compute the eigenvalues of a general matrix.", "eigvalsh(a[, UPLO])", "Compute the eigenvalues of a complex Hermitian or real symmetric matrix.", "inv(a)", "Compute the (multiplicative) inverse of a matrix.", "lstsq(a, b[, rcond])", "Return the least-squares solution to a linear matrix equation.", "norm(x[, ord, axis, keepdims])", "Matrix or vector norm.", "pinv(a[, rcond, hermitian])", "Compute the (Moore-Penrose) pseudo-inverse of a matrix.", "solve(a, b)", "Solve a linear matrix equation, or system of linear scalar equations.", "svd(a[, full_matrices, compute_uv, hermitian])", "Singular Value Decomposition.", "fft(a[, n, axis, norm])", "Compute the one-dimensional discrete Fourier Transform.", "fft2(a[, s, axes, norm])", "Compute the 2-dimensional discrete Fourier Transform.", "fftn(a[, s, axes, norm])", "Compute the N-dimensional discrete Fourier Transform.", "ifft(a[, n, axis, norm])", "Compute the one-dimensional inverse discrete Fourier Transform.", "ifft2(a[, s, axes, norm])", "Compute the 2-dimensional inverse discrete Fourier Transform.", "ifftn(a[, s, axes, norm])", "Compute the N-dimensional inverse discrete Fourier Transform.", "i0(x)", "Modified Bessel function of the first kind, order 0."]}, {"name": "Padding Arrays", "path": "reference/routines.padding", "type": "Padding Arrays", "text": ["pad(array, pad_width[, mode])", "Pad an array."]}, {"name": "Parameters", "path": "reference/distutils", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": ["NumPy provides enhanced distutils functionality to make it easier to build and install sub-packages, auto-generate code, and extension modules that use Fortran-compiled libraries. To use features of NumPy distutils, use the setup command from numpy.distutils.core. A useful Configuration class is also provided in numpy.distutils.misc_util that can make it easier to construct keyword arguments to pass to the setup function (by passing the dictionary obtained from the todict() method of the class). More information is available in the NumPy Distutils - Users Guide.", "The choice and location of linked libraries such as BLAS and LAPACK as well as include paths and other such build options can be specified in a site.cfg file located in the NumPy root repository or a .numpy-site.cfg file in your home directory. See the site.cfg.example example file included in the NumPy repository or sdist for documentation.", "ccompiler", "ccompiler_opt", "Provides the CCompilerOpt class, used for handling the CPU/hardware optimization, starting from parsing the command arguments, to managing the relation between the CPU baseline and dispatch-able features, also generating the required C headers and ending with compiling the sources with proper compiler's flags.", "cpuinfo.cpu", "core.Extension(name, sources[, ...])", "exec_command", "exec_command", "log.set_verbosity(v[, force])", "system_info.get_info(name[, notfound_action])", "notfound_action:", "system_info.get_standard_file(fname)", "Returns a list of files named 'fname' from 1) System-wide directory (directory-location of this module) 2) Users HOME directory (os.environ['HOME']) 3) Local directory", "Construct a configuration instance for the given package name. If parent_name is not None, then construct the package as a sub-package of the parent_name package. If top_path and package_path are None then they are assumed equal to the path of the file this instance was created in. The setup.py files in the numpy distribution are good examples of how to use the Configuration instance.", "Return a dictionary compatible with the keyword arguments of distutils setup function.", "Return the distutils distribution object for self.", "Return list of subpackage configurations.", "Name of the subpackage to get the configuration. \u2018*\u2019 in subpackage_name is handled as a wildcard.", "If None, then the path is assumed to be the local path plus the subpackage_name. If a setup.py file is not found in the subpackage_path, then a default configuration is used.", "Parent name.", "Add a sub-package to the current Configuration instance.", "This is useful in a setup.py script for adding sub-packages to a package.", "name of the subpackage", "if given, the subpackage path such as the subpackage is in subpackage_path / subpackage_name. If None,the subpackage is assumed to be located in the local path / subpackage_name.", "Add data files to configuration data_files.", "Argument(s) can be either", "The form of each element of the files sequence is very flexible allowing many combinations of where to get the files from the package and where they should ultimately be installed on the system. The most basic usage is for an element of the files argument sequence to be a simple filename. This will cause that file from the local path to be installed to the installation path of the self.name package (package path). The file argument can also be a relative path in which case the entire relative path will be installed into the package directory. Finally, the file can be an absolute path name in which case the file will be found at the absolute path name but installed to the package path.", "This basic behavior can be augmented by passing a 2-tuple in as the file argument. The first element of the tuple should specify the relative path (under the package install directory) where the remaining sequence of files should be installed to (it has nothing to do with the file-names in the source distribution). The second element of the tuple is the sequence of files that should be installed. The files in this sequence can be filenames, relative paths, or absolute paths. For absolute paths the file will be installed in the top-level package installation directory (regardless of the first argument). Filenames and relative path names will be installed in the package install directory under the path name given as the first element of the tuple.", "Rules for installation paths:", "An additional feature is that the path to a data-file can actually be a function that takes no arguments and returns the actual path(s) to the data-files. This is useful when the data files are generated while building the package.", "Add files to the list of data_files to be included with the package.", "will install these data files to:", "where <package install directory> is the package (or sub-package) directory such as \u2018/usr/lib/python2.4/site-packages/mypackage\u2019 (\u2018C: Python2.4 Lib site-packages mypackage\u2019) or \u2018/usr/lib/python2.4/site- packages/mypackage/mysubpackage\u2019 (\u2018C: Python2.4 Lib site-packages mypackage mysubpackage\u2019).", "Recursively add files under data_path to data_files list.", "Recursively add files under data_path to the list of data_files to be installed (and distributed). The data_path can be either a relative path-name, or an absolute path-name, or a 2-tuple where the first argument shows where in the install directory the data directory should be installed to.", "Argument can be either", "Rules for installation paths:", "For example suppose the source directory contains fun/foo.dat and fun/bar/car.dat:", "Will install data-files to the locations:", "Add paths to configuration include directories.", "Add the given sequence of paths to the beginning of the include_dirs list. This list will be visible to all extension modules of the current package.", "Add installable headers to configuration.", "Add the given sequence of files to the beginning of the headers list. By default, headers will be installed under <python- include>/<self.name.replace(\u2018.\u2019,\u2019/\u2019)>/ directory. If an item of files is a tuple, then its first argument specifies the actual installation location relative to the <python-include> path.", "Argument(s) can be either:", "Add extension to configuration.", "Create and add an Extension instance to the ext_modules list. This method also takes the following optional keyword arguments that are passed on to the Extension constructor.", "name of the extension", "list of the sources. The list of sources may contain functions (called source generators) which must take an extension instance and a build directory as inputs and return a source file or list of source files or None. If None is returned then no sources are generated. If the Extension instance has no sources after processing all source generators, then no extension module is built.", "The depends list contains paths to files or directories that the sources of the extension module depend on. If any path in the depends list is newer than the extension module, then the module will be rebuilt.", "dict or list of dict of keywords to be appended to keywords.", "The self.paths(\u2026) method is applied to all lists that may contain paths.", "Add library to configuration.", "Name of the extension.", "List of the sources. The list of sources may contain functions (called source generators) which must take an extension instance and a build directory as inputs and return a source file or list of source files or None. If None is returned then no sources are generated. If the Extension instance has no sources after processing all source generators, then no extension module is built.", "The following keys are allowed:", "Add scripts to configuration.", "Add the sequence of files to the beginning of the scripts list. Scripts will be installed under the <prefix>/bin/ directory.", "Similar to add_library, but the specified library is installed.", "Most C libraries used with distutils are only used to build python extensions, but libraries built through this method will be installed so that they can be reused by third-party packages.", "Name of the installed library.", "List of the library\u2019s source files. See add_library for details.", "Path to install the library, relative to the current sub-package.", "The following keys are allowed:", "See also", "The best way to encode the options required to link against the specified C libraries is to use a \u201clibname.ini\u201d file, and use get_info to retrieve the required options (see add_npy_pkg_config for more information).", "Generate and install a npy-pkg config file from a template.", "The config file generated from template is installed in the given install directory, using subst_dict for variable substitution.", "The path of the template, relatively to the current package path.", "Where to install the npy-pkg config file, relatively to the current package path.", "If given, any string of the form @key@ will be replaced by subst_dict[key] in the template file when installed. The install prefix is always available through the variable @prefix@, since the install prefix is not easy to get reliably from setup.py.", "See also", "This works for both standard installs and in-place builds, i.e. the @prefix@ refer to the source directory for in-place builds.", "Assuming the foo.ini.in file has the following content:", "The generated file will have the following content:", "and will be installed as foo.ini in the \u2018lib\u2019 subpath.", "When cross-compiling with numpy distutils, it might be necessary to use modified npy-pkg-config files. Using the default/generated files will link with the host libraries (i.e. libnpymath.a). For cross-compilation you of-course need to link with target libraries, while using the host Python installation.", "You can copy out the numpy/core/lib/npy-pkg-config directory, add a pkgdir value to the .ini files and set NPY_PKG_CONFIG_PATH environment variable to point to the directory with the modified npy-pkg-config files.", "Example npymath.ini modified for cross-compilation:", "Apply glob to paths and prepend local_path if needed.", "Applies glob.glob(\u2026) to each path in the sequence (if needed) and pre-pends the local_path if needed. Because this is called on all source lists, this allows wildcard characters to be specified in lists of sources for extension modules and libraries and scripts and allows path-names be relative to the source directory.", "Returns the numpy.distutils config command instance.", "Return a path to a temporary directory where temporary files should be placed.", "Check for availability of Fortran 77 compiler.", "Use it inside source generating function to ensure that setup distribution instance has been initialized.", "True if a Fortran 77 compiler is available (because a simple Fortran 77 code was able to be compiled successfully).", "Check for availability of Fortran 90 compiler.", "Use it inside source generating function to ensure that setup distribution instance has been initialized.", "True if a Fortran 90 compiler is available (because a simple Fortran 90 code was able to be compiled successfully)", "Try to get version string of a package.", "Return a version string of the current package or None if the version information could not be detected.", "This method scans files named __version__.py, <packagename>_version.py, version.py, and __svn_version__.py for string variables version, __version__, and <packagename>_version, until a version number is found.", "Appends a data function to the data_files list that will generate __svn_version__.py file to the current package directory.", "Generate package __svn_version__.py file from SVN revision number, it will be removed after python exits but will be available when sdist, etc commands are executed.", "If __svn_version__.py existed before, nothing is done.", "This is intended for working with source directories that are in an SVN repository.", "Generate package __config__.py file containing system_info information used during building the package.", "This file is installed to the package installation directory.", "Get resources information.", "Return information (from system_info.get_info) for all of the names in the argument list in a single dictionary.", "Conventional C libraries (installed through add_library) are not installed, and are just used during the build (they are statically linked). An installable C library is a pure C library, which does not depend on the python C runtime, and is installed such that it may be used by third-party packages. To build and install the C library, you just use the method add_installed_library instead of add_library, which takes the same arguments except for an additional install_dir argument:", "To make the necessary build options available to third parties, you could use the npy-pkg-config mechanism implemented in numpy.distutils. This mechanism is based on a .ini file which contains all the options. A .ini file is very similar to .pc files as used by the pkg-config unix utility:", "Generally, the file needs to be generated during the build, since it needs some information known at build time only (e.g. prefix). This is mostly automatic if one uses the Configuration method add_npy_pkg_config. Assuming we have a template file foo.ini.in as follows:", "and the following code in setup.py:", "This will install the file foo.ini into the directory package_dir/lib, and the foo.ini file will be generated from foo.ini.in, where each @version@ will be replaced by subst_dict['version']. The dictionary has an additional prefix substitution rule automatically added, which contains the install prefix (since this is not easy to get from setup.py). npy-pkg-config files can also be installed at the same location as used for numpy, using the path returned from get_npy_pkg_dir function.", "Info are easily retrieved from the get_info function in numpy.distutils.misc_util:", "An additional list of paths to look for .ini files can be given to get_info.", "NumPy distutils supports automatic conversion of source files named <somefile>.src. This facility can be used to maintain very similar code blocks requiring only simple changes between blocks. During the build phase of setup, if a template file named <somefile>.src is encountered, a new file named <somefile> is constructed from the template and placed in the build directory to be used instead. Two forms of template conversion are supported. The first form occurs for files named <file>.ext.src where ext is a recognized Fortran extension (f, f90, f95, f77, for, ftn, pyf). The second form is used for all other cases. See Conversion of .src files using Templates."]}, {"name": "paths()", "path": "reference/distutils#numpy.distutils.misc_util.Configuration.paths", "type": "Packaging ( \n    \n     numpy.distutils\n    \n    )", "text": ["Apply glob to paths and prepend local_path if needed.", "Applies glob.glob(\u2026) to each path in the sequence (if needed) and pre-pends the local_path if needed. Because this is called on all source lists, this allows wildcard characters to be specified in lists of sources for extension modules and libraries and scripts and allows path-names be relative to the source directory."]}, {"name": "Performance", "path": "reference/random/performance", "type": "Comparing Performance", "text": ["The recommended generator for general use is PCG64 or its upgraded variant PCG64DXSM for heavily-parallel use cases. They are statistically high quality, full-featured, and fast on most platforms, but somewhat slow when compiled for 32-bit processes. See Upgrading PCG64 with PCG64DXSM for details on when heavy parallelism would indicate using PCG64DXSM.", "Philox is fairly slow, but its statistical properties have very high quality, and it is easy to get an assuredly-independent stream by using unique keys. If that is the style you wish to use for parallel streams, or you are porting from another system that uses that style, then Philox is your choice.", "SFC64 is statistically high quality and very fast. However, it lacks jumpability. If you are not using that capability and want lots of speed, even on 32-bit processes, this is your choice.", "MT19937 fails some statistical tests and is not especially fast compared to modern PRNGs. For these reasons, we mostly do not recommend using it on its own, only through the legacy RandomState for reproducing old results. That said, it has a very long history as a default in many systems.", "The timings below are the time in ns to produce 1 random value from a specific distribution. The original MT19937 generator is much slower since it requires 2 32-bit values to equal the output of the faster generators.", "Integer performance has a similar ordering.", "The pattern is similar for other, more complex generators. The normal performance of the legacy RandomState generator is much lower than the other since it uses the Box-Muller transform rather than the Ziggurat method. The performance gap for Exponentials is also large due to the cost of computing the log function to invert the CDF. The column labeled MT19973 uses the same 32-bit generator as RandomState but produces random variates using Generator.", "MT19937", "PCG64", "PCG64DXSM", "Philox", "SFC64", "RandomState", "32-bit Unsigned Ints", "3.3", "1.9", "2.0", "3.3", "1.8", "3.1", "64-bit Unsigned Ints", "5.6", "3.2", "2.9", "4.9", "2.5", "5.5", "Uniforms", "5.9", "3.1", "2.9", "5.0", "2.6", "6.0", "Normals", "13.9", "10.8", "10.5", "12.0", "8.3", "56.8", "Exponentials", "9.1", "6.0", "5.8", "8.1", "5.4", "63.9", "Gammas", "37.2", "30.8", "28.9", "34.0", "27.5", "77.0", "Binomials", "21.3", "17.4", "17.6", "19.3", "15.6", "21.4", "Laplaces", "73.2", "72.3", "76.1", "73.0", "72.3", "82.5", "Poissons", "111.7", "103.4", "100.5", "109.4", "90.7", "115.2", "The next table presents the performance in percentage relative to values generated by the legacy generator, RandomState(MT19937()). The overall performance was computed using a geometric mean.", "MT19937", "PCG64", "PCG64DXSM", "Philox", "SFC64", "32-bit Unsigned Ints", "96", "162", "160", "96", "175", "64-bit Unsigned Ints", "97", "171", "188", "113", "218", "Uniforms", "102", "192", "206", "121", "233", "Normals", "409", "526", "541", "471", "684", "Exponentials", "701", "1071", "1101", "784", "1179", "Gammas", "207", "250", "266", "227", "281", "Binomials", "100", "123", "122", "111", "138", "Laplaces", "113", "114", "108", "113", "114", "Poissons", "103", "111", "115", "105", "127", "Overall", "159", "219", "225", "174", "251", "Note", "All timings were taken using Linux on an AMD Ryzen 9 3900X processor.", "Performance differs across platforms due to compiler and hardware availability (e.g., register width) differences. The default bit generator has been chosen to perform well on 64-bit platforms. Performance on 32-bit operating systems is very different.", "The values reported are normalized relative to the speed of MT19937 in each table. A value of 100 indicates that the performance matches the MT19937. Higher values indicate improved performance. These values cannot be compared across tables.", "Distribution", "MT19937", "PCG64", "PCG64DXSM", "Philox", "SFC64", "32-bit Unsigned Ints", "100", "168", "166", "100", "182", "64-bit Unsigned Ints", "100", "176", "193", "116", "224", "Uniforms", "100", "188", "202", "118", "228", "Normals", "100", "128", "132", "115", "167", "Exponentials", "100", "152", "157", "111", "168", "Overall", "100", "161", "168", "112", "192", "The relative performance on 64-bit Linux and 64-bit Windows is broadly similar with the notable exception of the Philox generator.", "Distribution", "MT19937", "PCG64", "PCG64DXSM", "Philox", "SFC64", "32-bit Unsigned Ints", "100", "155", "131", "29", "150", "64-bit Unsigned Ints", "100", "157", "143", "25", "154", "Uniforms", "100", "151", "144", "24", "155", "Normals", "100", "129", "128", "37", "150", "Exponentials", "100", "150", "145", "28", "159", "Overall", "100", "148", "138", "28", "154", "The performance of 64-bit generators on 32-bit Windows is much lower than on 64-bit operating systems due to register width. MT19937, the generator that has been in NumPy since 2005, operates on 32-bit integers.", "Distribution", "MT19937", "PCG64", "PCG64DXSM", "Philox", "SFC64", "32-bit Unsigned Ints", "100", "24", "34", "14", "57", "64-bit Unsigned Ints", "100", "21", "32", "14", "74", "Uniforms", "100", "21", "34", "16", "73", "Normals", "100", "36", "57", "28", "101", "Exponentials", "100", "28", "44", "20", "88", "Overall", "100", "25", "39", "18", "77", "Note", "Linux timings used Ubuntu 20.04 and GCC 9.3.0. Windows timings were made on Windows 10 using Microsoft C/C++ Optimizing Compiler Version 19 (Visual Studio 2019). All timings were produced on an AMD Ryzen 9 3900X processor."]}, {"name": "Poly1d", "path": "reference/routines.polynomials.poly1d", "type": "Poly1d", "text": ["poly1d(c_or_r[, r, variable])", "A one-dimensional polynomial class.", "polyval(p, x)", "Evaluate a polynomial at specific values.", "poly(seq_of_zeros)", "Find the coefficients of a polynomial with the given sequence of roots.", "roots(p)", "Return the roots of a polynomial with coefficients given in p.", "polyfit(x, y, deg[, rcond, full, w, cov])", "Least squares polynomial fit.", "polyder(p[, m])", "Return the derivative of the specified order of a polynomial.", "polyint(p[, m, k])", "Return an antiderivative (indefinite integral) of a polynomial.", "polyadd(a1, a2)", "Find the sum of two polynomials.", "polydiv(u, v)", "Returns the quotient and remainder of polynomial division.", "polymul(a1, a2)", "Find the product of two polynomials.", "polysub(a1, a2)", "Difference (subtraction) of two polynomials.", "RankWarning", "Issued by polyfit when the Vandermonde matrix is rank deficient."]}, {"name": "poly1d.__call__()", "path": "reference/generated/numpy.poly1d.__call__", "type": "numpy.poly1d", "text": ["method", "Call self as a function."]}, {"name": "poly1d.deriv()", "path": "reference/generated/numpy.poly1d.deriv", "type": "numpy.poly1d", "text": ["method", "Return a derivative of this polynomial.", "Refer to polyder for full documentation.", "See also", "equivalent function"]}, {"name": "poly1d.integ()", "path": "reference/generated/numpy.poly1d.integ", "type": "numpy.poly1d", "text": ["method", "Return an antiderivative (indefinite integral) of this polynomial.", "Refer to polyint for full documentation.", "See also", "equivalent function"]}, {"name": "polynomial.chebyshev.cheb2poly()", "path": "reference/generated/numpy.polynomial.chebyshev.cheb2poly", "type": "numpy.polynomial.chebyshev.cheb2poly", "text": ["Convert a Chebyshev series to a polynomial.", "Convert an array representing the coefficients of a Chebyshev series, ordered from lowest degree to highest, to an array of the coefficients of the equivalent polynomial (relative to the \u201cstandard\u201d basis) ordered from lowest to highest degree.", "1-D array containing the Chebyshev series coefficients, ordered from lowest order term to highest.", "1-D array containing the coefficients of the equivalent polynomial (relative to the \u201cstandard\u201d basis) ordered from lowest order term to highest.", "See also", "The easy way to do conversions between polynomial basis sets is to use the convert method of a class instance."]}, {"name": "polynomial.chebyshev.chebadd()", "path": "reference/generated/numpy.polynomial.chebyshev.chebadd", "type": "numpy.polynomial.chebyshev.chebadd", "text": ["Add one Chebyshev series to another.", "Returns the sum of two Chebyshev series c1 + c2. The arguments are sequences of coefficients ordered from lowest order term to highest, i.e., [1,2,3] represents the series T_0 + 2*T_1 + 3*T_2.", "1-D arrays of Chebyshev series coefficients ordered from low to high.", "Array representing the Chebyshev series of their sum.", "See also", "Unlike multiplication, division, etc., the sum of two Chebyshev series is a Chebyshev series (without having to \u201creproject\u201d the result onto the basis set) so addition, just like that of \u201cstandard\u201d polynomials, is simply \u201ccomponent-wise.\u201d"]}, {"name": "polynomial.chebyshev.chebcompanion()", "path": "reference/generated/numpy.polynomial.chebyshev.chebcompanion", "type": "numpy.polynomial.chebyshev.chebcompanion", "text": ["Return the scaled companion matrix of c.", "The basis polynomials are scaled so that the companion matrix is symmetric when c is a Chebyshev basis polynomial. This provides better eigenvalue estimates than the unscaled case and for basis polynomials the eigenvalues are guaranteed to be real if numpy.linalg.eigvalsh is used to obtain them.", "1-D array of Chebyshev series coefficients ordered from low to high degree.", "Scaled companion matrix of dimensions (deg, deg).", "New in version 1.7.0."]}, {"name": "polynomial.chebyshev.chebder()", "path": "reference/generated/numpy.polynomial.chebyshev.chebder", "type": "numpy.polynomial.chebyshev.chebder", "text": ["Differentiate a Chebyshev series.", "Returns the Chebyshev series coefficients c differentiated m times along axis. At each iteration the result is multiplied by scl (the scaling factor is for use in a linear change of variable). The argument c is an array of coefficients from low to high degree along each axis, e.g., [1,2,3] represents the series 1*T_0 + 2*T_1 + 3*T_2 while [[1,2],[1,2]] represents 1*T_0(x)*T_0(y) + 1*T_1(x)*T_0(y) +\n2*T_0(x)*T_1(y) + 2*T_1(x)*T_1(y) if axis=0 is x and axis=1 is y.", "Array of Chebyshev series coefficients. If c is multidimensional the different axis correspond to different variables with the degree in each axis given by the corresponding index.", "Number of derivatives taken, must be non-negative. (Default: 1)", "Each differentiation is multiplied by scl. The end result is multiplication by scl**m. This is for use in a linear change of variable. (Default: 1)", "Axis over which the derivative is taken. (Default: 0).", "New in version 1.7.0.", "Chebyshev series of the derivative.", "See also", "In general, the result of differentiating a C-series needs to be \u201creprojected\u201d onto the C-series basis set. Thus, typically, the result of this function is \u201cunintuitive,\u201d albeit correct; see Examples section below."]}, {"name": "polynomial.chebyshev.chebdiv()", "path": "reference/generated/numpy.polynomial.chebyshev.chebdiv", "type": "numpy.polynomial.chebyshev.chebdiv", "text": ["Divide one Chebyshev series by another.", "Returns the quotient-with-remainder of two Chebyshev series c1 / c2. The arguments are sequences of coefficients from lowest order \u201cterm\u201d to highest, e.g., [1,2,3] represents the series T_0 + 2*T_1 + 3*T_2.", "1-D arrays of Chebyshev series coefficients ordered from low to high.", "Of Chebyshev series coefficients representing the quotient and remainder.", "See also", "In general, the (polynomial) division of one C-series by another results in quotient and remainder terms that are not in the Chebyshev polynomial basis set. Thus, to express these results as C-series, it is typically necessary to \u201creproject\u201d the results onto said basis set, which typically produces \u201cunintuitive\u201d (but correct) results; see Examples section below."]}, {"name": "polynomial.chebyshev.chebdomain", "path": "reference/generated/numpy.polynomial.chebyshev.chebdomain", "type": "numpy.polynomial.chebyshev.chebdomain", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.chebyshev.chebfit()", "path": "reference/generated/numpy.polynomial.chebyshev.chebfit", "type": "numpy.polynomial.chebyshev.chebfit", "text": ["Least squares fit of Chebyshev series to data.", "Return the coefficients of a Chebyshev series of degree deg that is the least squares fit to the data values y given at points x. If y is 1-D the returned coefficients will also be 1-D. If y is 2-D multiple fits are done, one for each column of y, and the resulting coefficients are stored in the corresponding columns of a 2-D return. The fitted polynomial(s) are in the form", "where n is deg.", "x-coordinates of the M sample points (x[i], y[i]).", "y-coordinates of the sample points. Several data sets of sample points sharing the same x-coordinates can be fitted at once by passing in a 2D-array that contains one dataset per column.", "Degree(s) of the fitting polynomials. If deg is a single integer, all terms up to and including the deg\u2019th term are included in the fit. For NumPy versions >= 1.11.0 a list of integers specifying the degrees of the terms to include may be used instead.", "Relative condition number of the fit. Singular values smaller than this relative to the largest singular value will be ignored. The default value is len(x)*eps, where eps is the relative precision of the float type, about 2e-16 in most cases.", "Switch determining nature of return value. When it is False (the default) just the coefficients are returned, when True diagnostic information from the singular value decomposition is also returned.", "Weights. If not None, the weight w[i] applies to the unsquared residual y[i] - y_hat[i] at x[i]. Ideally the weights are chosen so that the errors of the products w[i]*y[i] all have the same variance. When using inverse-variance weighting, use w[i] = 1/sigma(y[i]). The default value is None.", "New in version 1.5.0.", "Chebyshev coefficients ordered from low to high. If y was 2-D, the coefficients for the data in column k of y are in column k.", "These values are only returned if full == True", "For more details, see numpy.linalg.lstsq.", "The rank of the coefficient matrix in the least-squares fit is deficient. The warning is only raised if full == False. The warnings can be turned off by", "See also", "Evaluates a Chebyshev series.", "Vandermonde matrix of Chebyshev series.", "Chebyshev weight function.", "Computes a least-squares fit from the matrix.", "Computes spline fits.", "The solution is the coefficients of the Chebyshev series p that minimizes the sum of the weighted squared errors", "where \\(w_j\\) are the weights. This problem is solved by setting up as the (typically) overdetermined matrix equation", "where V is the weighted pseudo Vandermonde matrix of x, c are the coefficients to be solved for, w are the weights, and y are the observed values. This equation is then solved using the singular value decomposition of V.", "If some of the singular values of V are so small that they are neglected, then a RankWarning will be issued. This means that the coefficient values may be poorly determined. Using a lower order fit will usually get rid of the warning. The rcond parameter can also be set to a value smaller than its default, but the resulting fit may be spurious and have large contributions from roundoff error.", "Fits using Chebyshev series are usually better conditioned than fits using power series, but much can depend on the distribution of the sample points and the smoothness of the data. If the quality of the fit is inadequate splines may be a good alternative.", "Wikipedia, \u201cCurve fitting\u201d, https://en.wikipedia.org/wiki/Curve_fitting"]}, {"name": "polynomial.chebyshev.chebfromroots()", "path": "reference/generated/numpy.polynomial.chebyshev.chebfromroots", "type": "numpy.polynomial.chebyshev.chebfromroots", "text": ["Generate a Chebyshev series with given roots.", "The function returns the coefficients of the polynomial", "in Chebyshev form, where the r_n are the roots specified in roots. If a zero has multiplicity n, then it must appear in roots n times. For instance, if 2 is a root of multiplicity three and 3 is a root of multiplicity 2, then roots looks something like [2, 2, 2, 3, 3]. The roots can appear in any order.", "If the returned coefficients are c, then", "The coefficient of the last term is not generally 1 for monic polynomials in Chebyshev form.", "Sequence containing the roots.", "1-D array of coefficients. If all roots are real then out is a real array, if some of the roots are complex, then out is complex even if all the coefficients in the result are real (see Examples below).", "See also"]}, {"name": "polynomial.chebyshev.chebgauss()", "path": "reference/generated/numpy.polynomial.chebyshev.chebgauss", "type": "numpy.polynomial.chebyshev.chebgauss", "text": ["Gauss-Chebyshev quadrature.", "Computes the sample points and weights for Gauss-Chebyshev quadrature. These sample points and weights will correctly integrate polynomials of degree \\(2*deg - 1\\) or less over the interval \\([-1, 1]\\) with the weight function \\(f(x) = 1/\\sqrt{1 - x^2}\\).", "Number of sample points and weights. It must be >= 1.", "1-D ndarray containing the sample points.", "1-D ndarray containing the weights.", "New in version 1.7.0.", "The results have only been tested up to degree 100, higher degrees may be problematic. For Gauss-Chebyshev there are closed form solutions for the sample points and weights. If n = deg, then"]}, {"name": "polynomial.chebyshev.chebgrid2d()", "path": "reference/generated/numpy.polynomial.chebyshev.chebgrid2d", "type": "numpy.polynomial.chebyshev.chebgrid2d", "text": ["Evaluate a 2-D Chebyshev series on the Cartesian product of x and y.", "This function returns the values:", "where the points (a, b) consist of all pairs formed by taking a from x and b from y. The resulting points form a grid with x in the first dimension and y in the second.", "The parameters x and y are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars. In either case, either x and y or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c has fewer than two dimensions, ones are implicitly appended to its shape to make it 2-D. The shape of the result will be c.shape[2:] + x.shape + y.shape.", "The two dimensional series is evaluated at the points in the Cartesian product of x and y. If x or y is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and, if it isn\u2019t an ndarray, it is treated as a scalar.", "Array of coefficients ordered so that the coefficient of the term of multi-degree i,j is contained in c[i,j]. If c has dimension greater than two the remaining indices enumerate multiple sets of coefficients.", "The values of the two dimensional Chebyshev series at points in the Cartesian product of x and y.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.chebyshev.chebgrid3d()", "path": "reference/generated/numpy.polynomial.chebyshev.chebgrid3d", "type": "numpy.polynomial.chebyshev.chebgrid3d", "text": ["Evaluate a 3-D Chebyshev series on the Cartesian product of x, y, and z.", "This function returns the values:", "where the points (a, b, c) consist of all triples formed by taking a from x, b from y, and c from z. The resulting points form a grid with x in the first dimension, y in the second, and z in the third.", "The parameters x, y, and z are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars. In either case, either x, y, and z or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c has fewer than three dimensions, ones are implicitly appended to its shape to make it 3-D. The shape of the result will be c.shape[3:] + x.shape + y.shape + z.shape.", "The three dimensional series is evaluated at the points in the Cartesian product of x, y, and z. If x,`y`, or z is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and, if it isn\u2019t an ndarray, it is treated as a scalar.", "Array of coefficients ordered so that the coefficients for terms of degree i,j are contained in c[i,j]. If c has dimension greater than two the remaining indices enumerate multiple sets of coefficients.", "The values of the two dimensional polynomial at points in the Cartesian product of x and y.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.chebyshev.chebint()", "path": "reference/generated/numpy.polynomial.chebyshev.chebint", "type": "numpy.polynomial.chebyshev.chebint", "text": ["Integrate a Chebyshev series.", "Returns the Chebyshev series coefficients c integrated m times from lbnd along axis. At each iteration the resulting series is multiplied by scl and an integration constant, k, is added. The scaling factor is for use in a linear change of variable. (\u201cBuyer beware\u201d: note that, depending on what one is doing, one may want scl to be the reciprocal of what one might expect; for more information, see the Notes section below.) The argument c is an array of coefficients from low to high degree along each axis, e.g., [1,2,3] represents the series T_0 + 2*T_1 + 3*T_2 while [[1,2],[1,2]] represents 1*T_0(x)*T_0(y) + 1*T_1(x)*T_0(y) + 2*T_0(x)*T_1(y) +\n2*T_1(x)*T_1(y) if axis=0 is x and axis=1 is y.", "Array of Chebyshev series coefficients. If c is multidimensional the different axis correspond to different variables with the degree in each axis given by the corresponding index.", "Order of integration, must be positive. (Default: 1)", "Integration constant(s). The value of the first integral at zero is the first value in the list, the value of the second integral at zero is the second value, etc. If k == [] (the default), all constants are set to zero. If m == 1, a single scalar can be given instead of a list.", "The lower bound of the integral. (Default: 0)", "Following each integration the result is multiplied by scl before the integration constant is added. (Default: 1)", "Axis over which the integral is taken. (Default: 0).", "New in version 1.7.0.", "C-series coefficients of the integral.", "If m < 1, len(k) > m, np.ndim(lbnd) != 0, or np.ndim(scl) != 0.", "See also", "Note that the result of each integration is multiplied by scl. Why is this important to note? Say one is making a linear change of variable \\(u = ax + b\\) in an integral relative to x. Then \\(dx = du/a\\), so one will need to set scl equal to \\(1/a\\)- perhaps not what one would have first thought.", "Also note that, in general, the result of integrating a C-series needs to be \u201creprojected\u201d onto the C-series basis set. Thus, typically, the result of this function is \u201cunintuitive,\u201d albeit correct; see Examples section below."]}, {"name": "polynomial.chebyshev.chebinterpolate()", "path": "reference/generated/numpy.polynomial.chebyshev.chebinterpolate", "type": "numpy.polynomial.chebyshev.chebinterpolate", "text": ["Interpolate a function at the Chebyshev points of the first kind.", "Returns the Chebyshev series that interpolates func at the Chebyshev points of the first kind in the interval [-1, 1]. The interpolating series tends to a minmax approximation to func with increasing deg if the function is continuous in the interval.", "New in version 1.14.0.", "The function to be approximated. It must be a function of a single variable of the form f(x, a, b, c...), where a, b, c... are extra arguments passed in the args parameter.", "Degree of the interpolating polynomial", "Extra arguments to be used in the function call. Default is no extra arguments.", "Chebyshev coefficients of the interpolating series ordered from low to high.", "The Chebyshev polynomials used in the interpolation are orthogonal when sampled at the Chebyshev points of the first kind. If it is desired to constrain some of the coefficients they can simply be set to the desired value after the interpolation, no new interpolation or fit is needed. This is especially useful if it is known apriori that some of coefficients are zero. For instance, if the function is even then the coefficients of the terms of odd degree in the result can be set to zero."]}, {"name": "polynomial.chebyshev.chebline()", "path": "reference/generated/numpy.polynomial.chebyshev.chebline", "type": "numpy.polynomial.chebyshev.chebline", "text": ["Chebyshev series whose graph is a straight line.", "The specified line is given by off + scl*x.", "This module\u2019s representation of the Chebyshev series for off + scl*x.", "See also"]}, {"name": "polynomial.chebyshev.chebmul()", "path": "reference/generated/numpy.polynomial.chebyshev.chebmul", "type": "numpy.polynomial.chebyshev.chebmul", "text": ["Multiply one Chebyshev series by another.", "Returns the product of two Chebyshev series c1 * c2. The arguments are sequences of coefficients, from lowest order \u201cterm\u201d to highest, e.g., [1,2,3] represents the series T_0 + 2*T_1 + 3*T_2.", "1-D arrays of Chebyshev series coefficients ordered from low to high.", "Of Chebyshev series coefficients representing their product.", "See also", "In general, the (polynomial) product of two C-series results in terms that are not in the Chebyshev polynomial basis set. Thus, to express the product as a C-series, it is typically necessary to \u201creproject\u201d the product onto said basis set, which typically produces \u201cunintuitive live\u201d (but correct) results; see Examples section below."]}, {"name": "polynomial.chebyshev.chebmulx()", "path": "reference/generated/numpy.polynomial.chebyshev.chebmulx", "type": "numpy.polynomial.chebyshev.chebmulx", "text": ["Multiply a Chebyshev series by x.", "Multiply the polynomial c by x, where x is the independent variable.", "1-D array of Chebyshev series coefficients ordered from low to high.", "Array representing the result of the multiplication.", "New in version 1.5.0."]}, {"name": "polynomial.chebyshev.chebone", "path": "reference/generated/numpy.polynomial.chebyshev.chebone", "type": "numpy.polynomial.chebyshev.chebone", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.chebyshev.chebpow()", "path": "reference/generated/numpy.polynomial.chebyshev.chebpow", "type": "numpy.polynomial.chebyshev.chebpow", "text": ["Raise a Chebyshev series to a power.", "Returns the Chebyshev series c raised to the power pow. The argument c is a sequence of coefficients ordered from low to high. i.e., [1,2,3] is the series T_0 + 2*T_1 + 3*T_2.", "1-D array of Chebyshev series coefficients ordered from low to high.", "Power to which the series will be raised", "Maximum power allowed. This is mainly to limit growth of the series to unmanageable size. Default is 16", "Chebyshev series of power.", "See also"]}, {"name": "polynomial.chebyshev.chebpts1()", "path": "reference/generated/numpy.polynomial.chebyshev.chebpts1", "type": "numpy.polynomial.chebyshev.chebpts1", "text": ["Chebyshev points of the first kind.", "The Chebyshev points of the first kind are the points cos(x), where x = [pi*(k + .5)/npts for k in range(npts)].", "Number of sample points desired.", "The Chebyshev points of the first kind.", "See also", "New in version 1.5.0."]}, {"name": "polynomial.chebyshev.chebpts2()", "path": "reference/generated/numpy.polynomial.chebyshev.chebpts2", "type": "numpy.polynomial.chebyshev.chebpts2", "text": ["Chebyshev points of the second kind.", "The Chebyshev points of the second kind are the points cos(x), where x = [pi*k/(npts - 1) for k in range(npts)].", "Number of sample points desired.", "The Chebyshev points of the second kind.", "New in version 1.5.0."]}, {"name": "polynomial.chebyshev.chebroots()", "path": "reference/generated/numpy.polynomial.chebyshev.chebroots", "type": "numpy.polynomial.chebyshev.chebroots", "text": ["Compute the roots of a Chebyshev series.", "Return the roots (a.k.a. \u201czeros\u201d) of the polynomial", "1-D array of coefficients.", "Array of the roots of the series. If all the roots are real, then out is also real, otherwise it is complex.", "See also", "The root estimates are obtained as the eigenvalues of the companion matrix, Roots far from the origin of the complex plane may have large errors due to the numerical instability of the series for such values. Roots with multiplicity greater than 1 will also show larger errors as the value of the series near such points is relatively insensitive to errors in the roots. Isolated roots near the origin can be improved by a few iterations of Newton\u2019s method.", "The Chebyshev series basis polynomials aren\u2019t powers of x so the results of this function may seem unintuitive."]}, {"name": "polynomial.chebyshev.chebsub()", "path": "reference/generated/numpy.polynomial.chebyshev.chebsub", "type": "numpy.polynomial.chebyshev.chebsub", "text": ["Subtract one Chebyshev series from another.", "Returns the difference of two Chebyshev series c1 - c2. The sequences of coefficients are from lowest order term to highest, i.e., [1,2,3] represents the series T_0 + 2*T_1 + 3*T_2.", "1-D arrays of Chebyshev series coefficients ordered from low to high.", "Of Chebyshev series coefficients representing their difference.", "See also", "Unlike multiplication, division, etc., the difference of two Chebyshev series is a Chebyshev series (without having to \u201creproject\u201d the result onto the basis set) so subtraction, just like that of \u201cstandard\u201d polynomials, is simply \u201ccomponent-wise.\u201d"]}, {"name": "polynomial.chebyshev.chebtrim()", "path": "reference/generated/numpy.polynomial.chebyshev.chebtrim", "type": "numpy.polynomial.chebyshev.chebtrim", "text": ["Remove \u201csmall\u201d \u201ctrailing\u201d coefficients from a polynomial.", "\u201cSmall\u201d means \u201csmall in absolute value\u201d and is controlled by the parameter tol; \u201ctrailing\u201d means highest order coefficient(s), e.g., in [0, 1, 1, 0, 0] (which represents 0 + x + x**2 + 0*x**3 + 0*x**4) both the 3-rd and 4-th order coefficients would be \u201ctrimmed.\u201d", "1-d array of coefficients, ordered from lowest order to highest.", "Trailing (i.e., highest order) elements with absolute value less than or equal to tol (default value is zero) are removed.", "1-d array with trailing zeros removed. If the resulting series would be empty, a series containing a single zero is returned.", "If tol < 0", "See also"]}, {"name": "polynomial.chebyshev.chebval()", "path": "reference/generated/numpy.polynomial.chebyshev.chebval", "type": "numpy.polynomial.chebyshev.chebval", "text": ["Evaluate a Chebyshev series at points x.", "If c is of length n + 1, this function returns the value:", "The parameter x is converted to an array only if it is a tuple or a list, otherwise it is treated as a scalar. In either case, either x or its elements must support multiplication and addition both with themselves and with the elements of c.", "If c is a 1-D array, then p(x) will have the same shape as x. If c is multidimensional, then the shape of the result depends on the value of tensor. If tensor is true the shape will be c.shape[1:] + x.shape. If tensor is false the shape will be c.shape[1:]. Note that scalars have shape (,).", "Trailing zeros in the coefficients will be used in the evaluation, so they should be avoided if efficiency is a concern.", "If x is a list or tuple, it is converted to an ndarray, otherwise it is left unchanged and treated as a scalar. In either case, x or its elements must support addition and multiplication with with themselves and with the elements of c.", "Array of coefficients ordered so that the coefficients for terms of degree n are contained in c[n]. If c is multidimensional the remaining indices enumerate multiple polynomials. In the two dimensional case the coefficients may be thought of as stored in the columns of c.", "If True, the shape of the coefficient array is extended with ones on the right, one for each dimension of x. Scalars have dimension 0 for this action. The result is that every column of coefficients in c is evaluated for every element of x. If False, x is broadcast over the columns of c for the evaluation. This keyword is useful when c is multidimensional. The default value is True.", "New in version 1.7.0.", "The shape of the return value is described above.", "See also", "The evaluation uses Clenshaw recursion, aka synthetic division."]}, {"name": "polynomial.chebyshev.chebval2d()", "path": "reference/generated/numpy.polynomial.chebyshev.chebval2d", "type": "numpy.polynomial.chebyshev.chebval2d", "text": ["Evaluate a 2-D Chebyshev series at points (x, y).", "This function returns the values:", "The parameters x and y are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars and they must have the same shape after conversion. In either case, either x and y or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c is a 1-D array a one is implicitly appended to its shape to make it 2-D. The shape of the result will be c.shape[2:] + x.shape.", "The two dimensional series is evaluated at the points (x, y), where x and y must have the same shape. If x or y is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and if it isn\u2019t an ndarray it is treated as a scalar.", "Array of coefficients ordered so that the coefficient of the term of multi-degree i,j is contained in c[i,j]. If c has dimension greater than 2 the remaining indices enumerate multiple sets of coefficients.", "The values of the two dimensional Chebyshev series at points formed from pairs of corresponding values from x and y.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.chebyshev.chebval3d()", "path": "reference/generated/numpy.polynomial.chebyshev.chebval3d", "type": "numpy.polynomial.chebyshev.chebval3d", "text": ["Evaluate a 3-D Chebyshev series at points (x, y, z).", "This function returns the values:", "The parameters x, y, and z are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars and they must have the same shape after conversion. In either case, either x, y, and z or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c has fewer than 3 dimensions, ones are implicitly appended to its shape to make it 3-D. The shape of the result will be c.shape[3:] + x.shape.", "The three dimensional series is evaluated at the points (x, y, z), where x, y, and z must have the same shape. If any of x, y, or z is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and if it isn\u2019t an ndarray it is treated as a scalar.", "Array of coefficients ordered so that the coefficient of the term of multi-degree i,j,k is contained in c[i,j,k]. If c has dimension greater than 3 the remaining indices enumerate multiple sets of coefficients.", "The values of the multidimensional polynomial on points formed with triples of corresponding values from x, y, and z.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.chebyshev.chebvander()", "path": "reference/generated/numpy.polynomial.chebyshev.chebvander", "type": "numpy.polynomial.chebyshev.chebvander", "text": ["Pseudo-Vandermonde matrix of given degree.", "Returns the pseudo-Vandermonde matrix of degree deg and sample points x. The pseudo-Vandermonde matrix is defined by", "where 0 <= i <= deg. The leading indices of V index the elements of x and the last index is the degree of the Chebyshev polynomial.", "If c is a 1-D array of coefficients of length n + 1 and V is the matrix V = chebvander(x, n), then np.dot(V, c) and chebval(x, c) are the same up to roundoff. This equivalence is useful both for least squares fitting and for the evaluation of a large number of Chebyshev series of the same degree and sample points.", "Array of points. The dtype is converted to float64 or complex128 depending on whether any of the elements are complex. If x is scalar it is converted to a 1-D array.", "Degree of the resulting matrix.", "The pseudo Vandermonde matrix. The shape of the returned matrix is x.shape + (deg + 1,), where The last index is the degree of the corresponding Chebyshev polynomial. The dtype will be the same as the converted x."]}, {"name": "polynomial.chebyshev.chebvander2d()", "path": "reference/generated/numpy.polynomial.chebyshev.chebvander2d", "type": "numpy.polynomial.chebyshev.chebvander2d", "text": ["Pseudo-Vandermonde matrix of given degrees.", "Returns the pseudo-Vandermonde matrix of degrees deg and sample points (x, y). The pseudo-Vandermonde matrix is defined by", "where 0 <= i <= deg[0] and 0 <= j <= deg[1]. The leading indices of V index the points (x, y) and the last index encodes the degrees of the Chebyshev polynomials.", "If V = chebvander2d(x, y, [xdeg, ydeg]), then the columns of V correspond to the elements of a 2-D coefficient array c of shape (xdeg + 1, ydeg + 1) in the order", "and np.dot(V, c.flat) and chebval2d(x, y, c) will be the same up to roundoff. This equivalence is useful both for least squares fitting and for the evaluation of a large number of 2-D Chebyshev series of the same degrees and sample points.", "Arrays of point coordinates, all of the same shape. The dtypes will be converted to either float64 or complex128 depending on whether any of the elements are complex. Scalars are converted to 1-D arrays.", "List of maximum degrees of the form [x_deg, y_deg].", "The shape of the returned matrix is x.shape + (order,), where \\(order = (deg[0]+1)*(deg[1]+1)\\). The dtype will be the same as the converted x and y.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.chebyshev.chebvander3d()", "path": "reference/generated/numpy.polynomial.chebyshev.chebvander3d", "type": "numpy.polynomial.chebyshev.chebvander3d", "text": ["Pseudo-Vandermonde matrix of given degrees.", "Returns the pseudo-Vandermonde matrix of degrees deg and sample points (x, y, z). If l, m, n are the given degrees in x, y, z, then The pseudo-Vandermonde matrix is defined by", "where 0 <= i <= l, 0 <= j <= m, and 0 <= j <= n. The leading indices of V index the points (x, y, z) and the last index encodes the degrees of the Chebyshev polynomials.", "If V = chebvander3d(x, y, z, [xdeg, ydeg, zdeg]), then the columns of V correspond to the elements of a 3-D coefficient array c of shape (xdeg + 1, ydeg + 1, zdeg + 1) in the order", "and np.dot(V, c.flat) and chebval3d(x, y, z, c) will be the same up to roundoff. This equivalence is useful both for least squares fitting and for the evaluation of a large number of 3-D Chebyshev series of the same degrees and sample points.", "Arrays of point coordinates, all of the same shape. The dtypes will be converted to either float64 or complex128 depending on whether any of the elements are complex. Scalars are converted to 1-D arrays.", "List of maximum degrees of the form [x_deg, y_deg, z_deg].", "The shape of the returned matrix is x.shape + (order,), where \\(order = (deg[0]+1)*(deg[1]+1)*(deg[2]+1)\\). The dtype will be the same as the converted x, y, and z.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.chebyshev.chebweight()", "path": "reference/generated/numpy.polynomial.chebyshev.chebweight", "type": "numpy.polynomial.chebyshev.chebweight", "text": ["The weight function of the Chebyshev polynomials.", "The weight function is \\(1/\\sqrt{1 - x^2}\\) and the interval of integration is \\([-1, 1]\\). The Chebyshev polynomials are orthogonal, but not normalized, with respect to this weight function.", "Values at which the weight function will be computed.", "The weight function at x.", "New in version 1.7.0."]}, {"name": "polynomial.chebyshev.chebx", "path": "reference/generated/numpy.polynomial.chebyshev.chebx", "type": "numpy.polynomial.chebyshev.chebx", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.chebyshev.Chebyshev.__call__()", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev.__call__", "type": "numpy.polynomial.chebyshev.Chebyshev", "text": ["method", "Call self as a function."]}, {"name": "polynomial.chebyshev.Chebyshev.basis()", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev.basis", "type": "numpy.polynomial.chebyshev.Chebyshev", "text": ["method", "Series basis polynomial of degree deg.", "Returns the series representing the basis polynomial of degree deg.", "New in version 1.7.0.", "Degree of the basis polynomial for the series. Must be >= 0.", "If given, the array must be of the form [beg, end], where beg and end are the endpoints of the domain. If None is given then the class domain is used. The default is None.", "If given, the resulting array must be if the form [beg, end], where beg and end are the endpoints of the window. If None is given then the class window is used. The default is None.", "A series with the coefficient of the deg term set to one and all others zero."]}, {"name": "polynomial.chebyshev.Chebyshev.cast()", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev.cast", "type": "numpy.polynomial.chebyshev.Chebyshev", "text": ["method", "Convert series to series of this class.", "The series is expected to be an instance of some polynomial series of one of the types supported by by the numpy.polynomial module, but could be some other class that supports the convert method.", "New in version 1.7.0.", "The series instance to be converted.", "If given, the array must be of the form [beg, end], where beg and end are the endpoints of the domain. If None is given then the class domain is used. The default is None.", "If given, the resulting array must be if the form [beg, end], where beg and end are the endpoints of the window. If None is given then the class window is used. The default is None.", "A series of the same kind as the calling class and equal to series when evaluated.", "See also", "similar instance method"]}, {"name": "polynomial.chebyshev.Chebyshev.convert()", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev.convert", "type": "numpy.polynomial.chebyshev.Chebyshev", "text": ["method", "Convert series to a different kind and/or domain and/or window.", "The domain of the converted series. If the value is None, the default domain of kind is used.", "The polynomial series type class to which the current instance should be converted. If kind is None, then the class of the current instance is used.", "The window of the converted series. If the value is None, the default window of kind is used.", "The returned class can be of different type than the current instance and/or have a different domain and/or different window.", "Conversion between domains and class types can result in numerically ill defined series."]}, {"name": "polynomial.chebyshev.Chebyshev.copy()", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev.copy", "type": "numpy.polynomial.chebyshev.Chebyshev", "text": ["method", "Return a copy.", "Copy of self."]}, {"name": "polynomial.chebyshev.Chebyshev.cutdeg()", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev.cutdeg", "type": "numpy.polynomial.chebyshev.Chebyshev", "text": ["method", "Truncate series to the given degree.", "Reduce the degree of the series to deg by discarding the high order terms. If deg is greater than the current degree a copy of the current series is returned. This can be useful in least squares where the coefficients of the high degree terms may be very small.", "New in version 1.5.0.", "The series is reduced to degree deg by discarding the high order terms. The value of deg must be a non-negative integer.", "New instance of series with reduced degree."]}, {"name": "polynomial.chebyshev.Chebyshev.degree()", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev.degree", "type": "numpy.polynomial.chebyshev.Chebyshev", "text": ["method", "The degree of the series.", "New in version 1.5.0.", "Degree of the series, one less than the number of coefficients."]}, {"name": "polynomial.chebyshev.Chebyshev.deriv()", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev.deriv", "type": "numpy.polynomial.chebyshev.Chebyshev", "text": ["method", "Differentiate.", "Return a series instance of that is the derivative of the current series.", "Find the derivative of order m.", "A new series representing the derivative. The domain is the same as the domain of the differentiated series."]}, {"name": "polynomial.chebyshev.Chebyshev.domain", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev.domain", "type": "Polynomials", "text": ["attribute"]}, {"name": "polynomial.chebyshev.Chebyshev.fit()", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev.fit", "type": "numpy.polynomial.chebyshev.Chebyshev", "text": ["method", "Least squares fit to data.", "Return a series instance that is the least squares fit to the data y sampled at x. The domain of the returned instance can be specified and this will often result in a superior fit with less chance of ill conditioning.", "x-coordinates of the M sample points (x[i], y[i]).", "y-coordinates of the M sample points (x[i], y[i]).", "Degree(s) of the fitting polynomials. If deg is a single integer all terms up to and including the deg\u2019th term are included in the fit. For NumPy versions >= 1.11.0 a list of integers specifying the degrees of the terms to include may be used instead.", "Domain to use for the returned series. If None, then a minimal domain that covers the points x is chosen. If [] the class domain is used. The default value was the class domain in NumPy 1.4 and None in later versions. The [] option was added in numpy 1.5.0.", "Relative condition number of the fit. Singular values smaller than this relative to the largest singular value will be ignored. The default value is len(x)*eps, where eps is the relative precision of the float type, about 2e-16 in most cases.", "Switch determining nature of return value. When it is False (the default) just the coefficients are returned, when True diagnostic information from the singular value decomposition is also returned.", "Weights. If not None, the weight w[i] applies to the unsquared residual y[i] - y_hat[i] at x[i]. Ideally the weights are chosen so that the errors of the products w[i]*y[i] all have the same variance. When using inverse-variance weighting, use w[i] = 1/sigma(y[i]). The default value is None.", "New in version 1.5.0.", "Window to use for the returned series. The default value is the default class domain", "New in version 1.6.0.", "A series that represents the least squares fit to the data and has the domain and window specified in the call. If the coefficients for the unscaled and unshifted basis polynomials are of interest, do new_series.convert().coef.", "These values are only returned if full == True", "For more details, see linalg.lstsq."]}, {"name": "polynomial.chebyshev.Chebyshev.fromroots()", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev.fromroots", "type": "numpy.polynomial.chebyshev.Chebyshev", "text": ["method", "Return series instance that has the specified roots.", "Returns a series representing the product (x - r[0])*(x - r[1])*...*(x - r[n-1]), where r is a list of roots.", "List of roots.", "Domain for the resulting series. If None the domain is the interval from the smallest root to the largest. If [] the domain is the class domain. The default is [].", "Window for the returned series. If None the class window is used. The default is None.", "Series with the specified roots."]}, {"name": "polynomial.chebyshev.Chebyshev.has_samecoef()", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev.has_samecoef", "type": "numpy.polynomial.chebyshev.Chebyshev", "text": ["method", "Check if coefficients match.", "New in version 1.6.0.", "The other class must have the coef attribute.", "True if the coefficients are the same, False otherwise."]}, {"name": "polynomial.chebyshev.Chebyshev.has_samedomain()", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev.has_samedomain", "type": "numpy.polynomial.chebyshev.Chebyshev", "text": ["method", "Check if domains match.", "New in version 1.6.0.", "The other class must have the domain attribute.", "True if the domains are the same, False otherwise."]}, {"name": "polynomial.chebyshev.Chebyshev.has_sametype()", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev.has_sametype", "type": "numpy.polynomial.chebyshev.Chebyshev", "text": ["method", "Check if types match.", "New in version 1.7.0.", "Class instance.", "True if other is same class as self"]}, {"name": "polynomial.chebyshev.Chebyshev.has_samewindow()", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev.has_samewindow", "type": "numpy.polynomial.chebyshev.Chebyshev", "text": ["method", "Check if windows match.", "New in version 1.6.0.", "The other class must have the window attribute.", "True if the windows are the same, False otherwise."]}, {"name": "polynomial.chebyshev.Chebyshev.identity()", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev.identity", "type": "numpy.polynomial.chebyshev.Chebyshev", "text": ["method", "Identity function.", "If p is the returned series, then p(x) == x for all values of x.", "If given, the array must be of the form [beg, end], where beg and end are the endpoints of the domain. If None is given then the class domain is used. The default is None.", "If given, the resulting array must be if the form [beg, end], where beg and end are the endpoints of the window. If None is given then the class window is used. The default is None.", "Series of representing the identity."]}, {"name": "polynomial.chebyshev.Chebyshev.integ()", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev.integ", "type": "numpy.polynomial.chebyshev.Chebyshev", "text": ["method", "Integrate.", "Return a series instance that is the definite integral of the current series.", "The number of integrations to perform.", "Integration constants. The first constant is applied to the first integration, the second to the second, and so on. The list of values must less than or equal to m in length and any missing values are set to zero.", "The lower bound of the definite integral.", "A new series representing the integral. The domain is the same as the domain of the integrated series."]}, {"name": "polynomial.chebyshev.Chebyshev.interpolate()", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev.interpolate", "type": "numpy.polynomial.chebyshev.Chebyshev", "text": ["method", "Interpolate a function at the Chebyshev points of the first kind.", "Returns the series that interpolates func at the Chebyshev points of the first kind scaled and shifted to the domain. The resulting series tends to a minmax approximation of func when the function is continuous in the domain.", "New in version 1.14.0.", "The function to be interpolated. It must be a function of a single variable of the form f(x, a, b, c...), where a, b, c... are extra arguments passed in the args parameter.", "Degree of the interpolating polynomial.", "Domain over which func is interpolated. The default is None, in which case the domain is [-1, 1].", "Extra arguments to be used in the function call. Default is no extra arguments.", "Interpolating Chebyshev instance.", "See numpy.polynomial.chebfromfunction for more details."]}, {"name": "polynomial.chebyshev.Chebyshev.linspace()", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev.linspace", "type": "numpy.polynomial.chebyshev.Chebyshev", "text": ["method", "Return x, y values at equally spaced points in domain.", "Returns the x, y values at n linearly spaced points across the domain. Here y is the value of the polynomial at the points x. By default the domain is the same as that of the series instance. This method is intended mostly as a plotting aid.", "New in version 1.5.0.", "Number of point pairs to return. The default value is 100.", "If not None, the specified domain is used instead of that of the calling instance. It should be of the form [beg,end]. The default is None which case the class domain is used.", "x is equal to linspace(self.domain[0], self.domain[1], n) and y is the series evaluated at element of x."]}, {"name": "polynomial.chebyshev.Chebyshev.mapparms()", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev.mapparms", "type": "numpy.polynomial.chebyshev.Chebyshev", "text": ["method", "Return the mapping parameters.", "The returned values define a linear map off + scl*x that is applied to the input arguments before the series is evaluated. The map depends on the domain and window; if the current domain is equal to the window the resulting map is the identity. If the coefficients of the series instance are to be used by themselves outside this class, then the linear function must be substituted for the x in the standard representation of the base polynomials.", "The mapping function is defined by off + scl*x.", "If the current domain is the interval [l1, r1] and the window is [l2, r2], then the linear mapping function L is defined by the equations:"]}, {"name": "polynomial.chebyshev.Chebyshev.roots()", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev.roots", "type": "numpy.polynomial.chebyshev.Chebyshev", "text": ["method", "Return the roots of the series polynomial.", "Compute the roots for the series. Note that the accuracy of the roots decrease the further outside the domain they lie.", "Array containing the roots of the series."]}, {"name": "polynomial.chebyshev.Chebyshev.trim()", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev.trim", "type": "numpy.polynomial.chebyshev.Chebyshev", "text": ["method", "Remove trailing coefficients", "Remove trailing coefficients until a coefficient is reached whose absolute value greater than tol or the beginning of the series is reached. If all the coefficients would be removed the series is set to [0]. A new series instance is returned with the new coefficients. The current instance remains unchanged.", "All trailing coefficients less than tol will be removed.", "New instance of series with trimmed coefficients."]}, {"name": "polynomial.chebyshev.Chebyshev.truncate()", "path": "reference/generated/numpy.polynomial.chebyshev.chebyshev.truncate", "type": "numpy.polynomial.chebyshev.Chebyshev", "text": ["method", "Truncate series to length size.", "Reduce the series to length size by discarding the high degree terms. The value of size must be a positive integer. This can be useful in least squares where the coefficients of the high degree terms may be very small.", "The series is reduced to length size by discarding the high degree terms. The value of size must be a positive integer.", "New instance of series with truncated coefficients."]}, {"name": "polynomial.chebyshev.chebzero", "path": "reference/generated/numpy.polynomial.chebyshev.chebzero", "type": "numpy.polynomial.chebyshev.chebzero", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.chebyshev.poly2cheb()", "path": "reference/generated/numpy.polynomial.chebyshev.poly2cheb", "type": "numpy.polynomial.chebyshev.poly2cheb", "text": ["Convert a polynomial to a Chebyshev series.", "Convert an array representing the coefficients of a polynomial (relative to the \u201cstandard\u201d basis) ordered from lowest degree to highest, to an array of the coefficients of the equivalent Chebyshev series, ordered from lowest to highest degree.", "1-D array containing the polynomial coefficients", "1-D array containing the coefficients of the equivalent Chebyshev series.", "See also", "The easy way to do conversions between polynomial basis sets is to use the convert method of a class instance."]}, {"name": "polynomial.hermite.herm2poly()", "path": "reference/generated/numpy.polynomial.hermite.herm2poly", "type": "numpy.polynomial.hermite.herm2poly", "text": ["Convert a Hermite series to a polynomial.", "Convert an array representing the coefficients of a Hermite series, ordered from lowest degree to highest, to an array of the coefficients of the equivalent polynomial (relative to the \u201cstandard\u201d basis) ordered from lowest to highest degree.", "1-D array containing the Hermite series coefficients, ordered from lowest order term to highest.", "1-D array containing the coefficients of the equivalent polynomial (relative to the \u201cstandard\u201d basis) ordered from lowest order term to highest.", "See also", "The easy way to do conversions between polynomial basis sets is to use the convert method of a class instance."]}, {"name": "polynomial.hermite.hermadd()", "path": "reference/generated/numpy.polynomial.hermite.hermadd", "type": "numpy.polynomial.hermite.hermadd", "text": ["Add one Hermite series to another.", "Returns the sum of two Hermite series c1 + c2. The arguments are sequences of coefficients ordered from lowest order term to highest, i.e., [1,2,3] represents the series P_0 + 2*P_1 + 3*P_2.", "1-D arrays of Hermite series coefficients ordered from low to high.", "Array representing the Hermite series of their sum.", "See also", "Unlike multiplication, division, etc., the sum of two Hermite series is a Hermite series (without having to \u201creproject\u201d the result onto the basis set) so addition, just like that of \u201cstandard\u201d polynomials, is simply \u201ccomponent-wise.\u201d"]}, {"name": "polynomial.hermite.hermcompanion()", "path": "reference/generated/numpy.polynomial.hermite.hermcompanion", "type": "numpy.polynomial.hermite.hermcompanion", "text": ["Return the scaled companion matrix of c.", "The basis polynomials are scaled so that the companion matrix is symmetric when c is an Hermite basis polynomial. This provides better eigenvalue estimates than the unscaled case and for basis polynomials the eigenvalues are guaranteed to be real if numpy.linalg.eigvalsh is used to obtain them.", "1-D array of Hermite series coefficients ordered from low to high degree.", "Scaled companion matrix of dimensions (deg, deg).", "New in version 1.7.0."]}, {"name": "polynomial.hermite.hermder()", "path": "reference/generated/numpy.polynomial.hermite.hermder", "type": "numpy.polynomial.hermite.hermder", "text": ["Differentiate a Hermite series.", "Returns the Hermite series coefficients c differentiated m times along axis. At each iteration the result is multiplied by scl (the scaling factor is for use in a linear change of variable). The argument c is an array of coefficients from low to high degree along each axis, e.g., [1,2,3] represents the series 1*H_0 + 2*H_1 + 3*H_2 while [[1,2],[1,2]] represents 1*H_0(x)*H_0(y) + 1*H_1(x)*H_0(y) +\n2*H_0(x)*H_1(y) + 2*H_1(x)*H_1(y) if axis=0 is x and axis=1 is y.", "Array of Hermite series coefficients. If c is multidimensional the different axis correspond to different variables with the degree in each axis given by the corresponding index.", "Number of derivatives taken, must be non-negative. (Default: 1)", "Each differentiation is multiplied by scl. The end result is multiplication by scl**m. This is for use in a linear change of variable. (Default: 1)", "Axis over which the derivative is taken. (Default: 0).", "New in version 1.7.0.", "Hermite series of the derivative.", "See also", "In general, the result of differentiating a Hermite series does not resemble the same operation on a power series. Thus the result of this function may be \u201cunintuitive,\u201d albeit correct; see Examples section below."]}, {"name": "polynomial.hermite.hermdiv()", "path": "reference/generated/numpy.polynomial.hermite.hermdiv", "type": "numpy.polynomial.hermite.hermdiv", "text": ["Divide one Hermite series by another.", "Returns the quotient-with-remainder of two Hermite series c1 / c2. The arguments are sequences of coefficients from lowest order \u201cterm\u201d to highest, e.g., [1,2,3] represents the series P_0 + 2*P_1 + 3*P_2.", "1-D arrays of Hermite series coefficients ordered from low to high.", "Of Hermite series coefficients representing the quotient and remainder.", "See also", "In general, the (polynomial) division of one Hermite series by another results in quotient and remainder terms that are not in the Hermite polynomial basis set. Thus, to express these results as a Hermite series, it is necessary to \u201creproject\u201d the results onto the Hermite basis set, which may produce \u201cunintuitive\u201d (but correct) results; see Examples section below."]}, {"name": "polynomial.hermite.hermdomain", "path": "reference/generated/numpy.polynomial.hermite.hermdomain", "type": "numpy.polynomial.hermite.hermdomain", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.hermite.hermfit()", "path": "reference/generated/numpy.polynomial.hermite.hermfit", "type": "numpy.polynomial.hermite.hermfit", "text": ["Least squares fit of Hermite series to data.", "Return the coefficients of a Hermite series of degree deg that is the least squares fit to the data values y given at points x. If y is 1-D the returned coefficients will also be 1-D. If y is 2-D multiple fits are done, one for each column of y, and the resulting coefficients are stored in the corresponding columns of a 2-D return. The fitted polynomial(s) are in the form", "where n is deg.", "x-coordinates of the M sample points (x[i], y[i]).", "y-coordinates of the sample points. Several data sets of sample points sharing the same x-coordinates can be fitted at once by passing in a 2D-array that contains one dataset per column.", "Degree(s) of the fitting polynomials. If deg is a single integer all terms up to and including the deg\u2019th term are included in the fit. For NumPy versions >= 1.11.0 a list of integers specifying the degrees of the terms to include may be used instead.", "Relative condition number of the fit. Singular values smaller than this relative to the largest singular value will be ignored. The default value is len(x)*eps, where eps is the relative precision of the float type, about 2e-16 in most cases.", "Switch determining nature of return value. When it is False (the default) just the coefficients are returned, when True diagnostic information from the singular value decomposition is also returned.", "Weights. If not None, the weight w[i] applies to the unsquared residual y[i] - y_hat[i] at x[i]. Ideally the weights are chosen so that the errors of the products w[i]*y[i] all have the same variance. When using inverse-variance weighting, use w[i] = 1/sigma(y[i]). The default value is None.", "Hermite coefficients ordered from low to high. If y was 2-D, the coefficients for the data in column k of y are in column k.", "These values are only returned if full == True", "For more details, see numpy.linalg.lstsq.", "The rank of the coefficient matrix in the least-squares fit is deficient. The warning is only raised if full == False. The warnings can be turned off by", "See also", "Evaluates a Hermite series.", "Vandermonde matrix of Hermite series.", "Hermite weight function", "Computes a least-squares fit from the matrix.", "Computes spline fits.", "The solution is the coefficients of the Hermite series p that minimizes the sum of the weighted squared errors", "where the \\(w_j\\) are the weights. This problem is solved by setting up the (typically) overdetermined matrix equation", "where V is the weighted pseudo Vandermonde matrix of x, c are the coefficients to be solved for, w are the weights, y are the observed values. This equation is then solved using the singular value decomposition of V.", "If some of the singular values of V are so small that they are neglected, then a RankWarning will be issued. This means that the coefficient values may be poorly determined. Using a lower order fit will usually get rid of the warning. The rcond parameter can also be set to a value smaller than its default, but the resulting fit may be spurious and have large contributions from roundoff error.", "Fits using Hermite series are probably most useful when the data can be approximated by sqrt(w(x)) * p(x), where w(x) is the Hermite weight. In that case the weight sqrt(w(x[i])) should be used together with data values y[i]/sqrt(w(x[i])). The weight function is available as hermweight.", "Wikipedia, \u201cCurve fitting\u201d, https://en.wikipedia.org/wiki/Curve_fitting"]}, {"name": "polynomial.hermite.hermfromroots()", "path": "reference/generated/numpy.polynomial.hermite.hermfromroots", "type": "numpy.polynomial.hermite.hermfromroots", "text": ["Generate a Hermite series with given roots.", "The function returns the coefficients of the polynomial", "in Hermite form, where the r_n are the roots specified in roots. If a zero has multiplicity n, then it must appear in roots n times. For instance, if 2 is a root of multiplicity three and 3 is a root of multiplicity 2, then roots looks something like [2, 2, 2, 3, 3]. The roots can appear in any order.", "If the returned coefficients are c, then", "The coefficient of the last term is not generally 1 for monic polynomials in Hermite form.", "Sequence containing the roots.", "1-D array of coefficients. If all roots are real then out is a real array, if some of the roots are complex, then out is complex even if all the coefficients in the result are real (see Examples below).", "See also"]}, {"name": "polynomial.hermite.hermgauss()", "path": "reference/generated/numpy.polynomial.hermite.hermgauss", "type": "numpy.polynomial.hermite.hermgauss", "text": ["Gauss-Hermite quadrature.", "Computes the sample points and weights for Gauss-Hermite quadrature. These sample points and weights will correctly integrate polynomials of degree \\(2*deg - 1\\) or less over the interval \\([-\\inf, \\inf]\\) with the weight function \\(f(x) = \\exp(-x^2)\\).", "Number of sample points and weights. It must be >= 1.", "1-D ndarray containing the sample points.", "1-D ndarray containing the weights.", "New in version 1.7.0.", "The results have only been tested up to degree 100, higher degrees may be problematic. The weights are determined by using the fact that", "where \\(c\\) is a constant independent of \\(k\\) and \\(x_k\\) is the k\u2019th root of \\(H_n\\), and then scaling the results to get the right value when integrating 1."]}, {"name": "polynomial.hermite.hermgrid2d()", "path": "reference/generated/numpy.polynomial.hermite.hermgrid2d", "type": "numpy.polynomial.hermite.hermgrid2d", "text": ["Evaluate a 2-D Hermite series on the Cartesian product of x and y.", "This function returns the values:", "where the points (a, b) consist of all pairs formed by taking a from x and b from y. The resulting points form a grid with x in the first dimension and y in the second.", "The parameters x and y are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars. In either case, either x and y or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c has fewer than two dimensions, ones are implicitly appended to its shape to make it 2-D. The shape of the result will be c.shape[2:] + x.shape.", "The two dimensional series is evaluated at the points in the Cartesian product of x and y. If x or y is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and, if it isn\u2019t an ndarray, it is treated as a scalar.", "Array of coefficients ordered so that the coefficients for terms of degree i,j are contained in c[i,j]. If c has dimension greater than two the remaining indices enumerate multiple sets of coefficients.", "The values of the two dimensional polynomial at points in the Cartesian product of x and y.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.hermite.hermgrid3d()", "path": "reference/generated/numpy.polynomial.hermite.hermgrid3d", "type": "numpy.polynomial.hermite.hermgrid3d", "text": ["Evaluate a 3-D Hermite series on the Cartesian product of x, y, and z.", "This function returns the values:", "where the points (a, b, c) consist of all triples formed by taking a from x, b from y, and c from z. The resulting points form a grid with x in the first dimension, y in the second, and z in the third.", "The parameters x, y, and z are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars. In either case, either x, y, and z or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c has fewer than three dimensions, ones are implicitly appended to its shape to make it 3-D. The shape of the result will be c.shape[3:] + x.shape + y.shape + z.shape.", "The three dimensional series is evaluated at the points in the Cartesian product of x, y, and z. If x,`y`, or z is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and, if it isn\u2019t an ndarray, it is treated as a scalar.", "Array of coefficients ordered so that the coefficients for terms of degree i,j are contained in c[i,j]. If c has dimension greater than two the remaining indices enumerate multiple sets of coefficients.", "The values of the two dimensional polynomial at points in the Cartesian product of x and y.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.hermite.hermint()", "path": "reference/generated/numpy.polynomial.hermite.hermint", "type": "numpy.polynomial.hermite.hermint", "text": ["Integrate a Hermite series.", "Returns the Hermite series coefficients c integrated m times from lbnd along axis. At each iteration the resulting series is multiplied by scl and an integration constant, k, is added. The scaling factor is for use in a linear change of variable. (\u201cBuyer beware\u201d: note that, depending on what one is doing, one may want scl to be the reciprocal of what one might expect; for more information, see the Notes section below.) The argument c is an array of coefficients from low to high degree along each axis, e.g., [1,2,3] represents the series H_0 + 2*H_1 + 3*H_2 while [[1,2],[1,2]] represents 1*H_0(x)*H_0(y) + 1*H_1(x)*H_0(y) + 2*H_0(x)*H_1(y) +\n2*H_1(x)*H_1(y) if axis=0 is x and axis=1 is y.", "Array of Hermite series coefficients. If c is multidimensional the different axis correspond to different variables with the degree in each axis given by the corresponding index.", "Order of integration, must be positive. (Default: 1)", "Integration constant(s). The value of the first integral at lbnd is the first value in the list, the value of the second integral at lbnd is the second value, etc. If k == [] (the default), all constants are set to zero. If m == 1, a single scalar can be given instead of a list.", "The lower bound of the integral. (Default: 0)", "Following each integration the result is multiplied by scl before the integration constant is added. (Default: 1)", "Axis over which the integral is taken. (Default: 0).", "New in version 1.7.0.", "Hermite series coefficients of the integral.", "If m < 0, len(k) > m, np.ndim(lbnd) != 0, or np.ndim(scl) != 0.", "See also", "Note that the result of each integration is multiplied by scl. Why is this important to note? Say one is making a linear change of variable \\(u = ax + b\\) in an integral relative to x. Then \\(dx = du/a\\), so one will need to set scl equal to \\(1/a\\) - perhaps not what one would have first thought.", "Also note that, in general, the result of integrating a C-series needs to be \u201creprojected\u201d onto the C-series basis set. Thus, typically, the result of this function is \u201cunintuitive,\u201d albeit correct; see Examples section below."]}, {"name": "polynomial.hermite.Hermite.__call__()", "path": "reference/generated/numpy.polynomial.hermite.hermite.__call__", "type": "numpy.polynomial.hermite.Hermite", "text": ["method", "Call self as a function."]}, {"name": "polynomial.hermite.Hermite.basis()", "path": "reference/generated/numpy.polynomial.hermite.hermite.basis", "type": "numpy.polynomial.hermite.Hermite", "text": ["method", "Series basis polynomial of degree deg.", "Returns the series representing the basis polynomial of degree deg.", "New in version 1.7.0.", "Degree of the basis polynomial for the series. Must be >= 0.", "If given, the array must be of the form [beg, end], where beg and end are the endpoints of the domain. If None is given then the class domain is used. The default is None.", "If given, the resulting array must be if the form [beg, end], where beg and end are the endpoints of the window. If None is given then the class window is used. The default is None.", "A series with the coefficient of the deg term set to one and all others zero."]}, {"name": "polynomial.hermite.Hermite.cast()", "path": "reference/generated/numpy.polynomial.hermite.hermite.cast", "type": "numpy.polynomial.hermite.Hermite", "text": ["method", "Convert series to series of this class.", "The series is expected to be an instance of some polynomial series of one of the types supported by by the numpy.polynomial module, but could be some other class that supports the convert method.", "New in version 1.7.0.", "The series instance to be converted.", "If given, the array must be of the form [beg, end], where beg and end are the endpoints of the domain. If None is given then the class domain is used. The default is None.", "If given, the resulting array must be if the form [beg, end], where beg and end are the endpoints of the window. If None is given then the class window is used. The default is None.", "A series of the same kind as the calling class and equal to series when evaluated.", "See also", "similar instance method"]}, {"name": "polynomial.hermite.Hermite.convert()", "path": "reference/generated/numpy.polynomial.hermite.hermite.convert", "type": "numpy.polynomial.hermite.Hermite", "text": ["method", "Convert series to a different kind and/or domain and/or window.", "The domain of the converted series. If the value is None, the default domain of kind is used.", "The polynomial series type class to which the current instance should be converted. If kind is None, then the class of the current instance is used.", "The window of the converted series. If the value is None, the default window of kind is used.", "The returned class can be of different type than the current instance and/or have a different domain and/or different window.", "Conversion between domains and class types can result in numerically ill defined series."]}, {"name": "polynomial.hermite.Hermite.copy()", "path": "reference/generated/numpy.polynomial.hermite.hermite.copy", "type": "numpy.polynomial.hermite.Hermite", "text": ["method", "Return a copy.", "Copy of self."]}, {"name": "polynomial.hermite.Hermite.cutdeg()", "path": "reference/generated/numpy.polynomial.hermite.hermite.cutdeg", "type": "numpy.polynomial.hermite.Hermite", "text": ["method", "Truncate series to the given degree.", "Reduce the degree of the series to deg by discarding the high order terms. If deg is greater than the current degree a copy of the current series is returned. This can be useful in least squares where the coefficients of the high degree terms may be very small.", "New in version 1.5.0.", "The series is reduced to degree deg by discarding the high order terms. The value of deg must be a non-negative integer.", "New instance of series with reduced degree."]}, {"name": "polynomial.hermite.Hermite.degree()", "path": "reference/generated/numpy.polynomial.hermite.hermite.degree", "type": "numpy.polynomial.hermite.Hermite", "text": ["method", "The degree of the series.", "New in version 1.5.0.", "Degree of the series, one less than the number of coefficients."]}, {"name": "polynomial.hermite.Hermite.deriv()", "path": "reference/generated/numpy.polynomial.hermite.hermite.deriv", "type": "numpy.polynomial.hermite.Hermite", "text": ["method", "Differentiate.", "Return a series instance of that is the derivative of the current series.", "Find the derivative of order m.", "A new series representing the derivative. The domain is the same as the domain of the differentiated series."]}, {"name": "polynomial.hermite.Hermite.domain", "path": "reference/generated/numpy.polynomial.hermite.hermite.domain", "type": "Polynomials", "text": ["attribute"]}, {"name": "polynomial.hermite.Hermite.fit()", "path": "reference/generated/numpy.polynomial.hermite.hermite.fit", "type": "numpy.polynomial.hermite.Hermite", "text": ["method", "Least squares fit to data.", "Return a series instance that is the least squares fit to the data y sampled at x. The domain of the returned instance can be specified and this will often result in a superior fit with less chance of ill conditioning.", "x-coordinates of the M sample points (x[i], y[i]).", "y-coordinates of the M sample points (x[i], y[i]).", "Degree(s) of the fitting polynomials. If deg is a single integer all terms up to and including the deg\u2019th term are included in the fit. For NumPy versions >= 1.11.0 a list of integers specifying the degrees of the terms to include may be used instead.", "Domain to use for the returned series. If None, then a minimal domain that covers the points x is chosen. If [] the class domain is used. The default value was the class domain in NumPy 1.4 and None in later versions. The [] option was added in numpy 1.5.0.", "Relative condition number of the fit. Singular values smaller than this relative to the largest singular value will be ignored. The default value is len(x)*eps, where eps is the relative precision of the float type, about 2e-16 in most cases.", "Switch determining nature of return value. When it is False (the default) just the coefficients are returned, when True diagnostic information from the singular value decomposition is also returned.", "Weights. If not None, the weight w[i] applies to the unsquared residual y[i] - y_hat[i] at x[i]. Ideally the weights are chosen so that the errors of the products w[i]*y[i] all have the same variance. When using inverse-variance weighting, use w[i] = 1/sigma(y[i]). The default value is None.", "New in version 1.5.0.", "Window to use for the returned series. The default value is the default class domain", "New in version 1.6.0.", "A series that represents the least squares fit to the data and has the domain and window specified in the call. If the coefficients for the unscaled and unshifted basis polynomials are of interest, do new_series.convert().coef.", "These values are only returned if full == True", "For more details, see linalg.lstsq."]}, {"name": "polynomial.hermite.Hermite.fromroots()", "path": "reference/generated/numpy.polynomial.hermite.hermite.fromroots", "type": "numpy.polynomial.hermite.Hermite", "text": ["method", "Return series instance that has the specified roots.", "Returns a series representing the product (x - r[0])*(x - r[1])*...*(x - r[n-1]), where r is a list of roots.", "List of roots.", "Domain for the resulting series. If None the domain is the interval from the smallest root to the largest. If [] the domain is the class domain. The default is [].", "Window for the returned series. If None the class window is used. The default is None.", "Series with the specified roots."]}, {"name": "polynomial.hermite.Hermite.has_samecoef()", "path": "reference/generated/numpy.polynomial.hermite.hermite.has_samecoef", "type": "numpy.polynomial.hermite.Hermite", "text": ["method", "Check if coefficients match.", "New in version 1.6.0.", "The other class must have the coef attribute.", "True if the coefficients are the same, False otherwise."]}, {"name": "polynomial.hermite.Hermite.has_samedomain()", "path": "reference/generated/numpy.polynomial.hermite.hermite.has_samedomain", "type": "numpy.polynomial.hermite.Hermite", "text": ["method", "Check if domains match.", "New in version 1.6.0.", "The other class must have the domain attribute.", "True if the domains are the same, False otherwise."]}, {"name": "polynomial.hermite.Hermite.has_sametype()", "path": "reference/generated/numpy.polynomial.hermite.hermite.has_sametype", "type": "numpy.polynomial.hermite.Hermite", "text": ["method", "Check if types match.", "New in version 1.7.0.", "Class instance.", "True if other is same class as self"]}, {"name": "polynomial.hermite.Hermite.has_samewindow()", "path": "reference/generated/numpy.polynomial.hermite.hermite.has_samewindow", "type": "numpy.polynomial.hermite.Hermite", "text": ["method", "Check if windows match.", "New in version 1.6.0.", "The other class must have the window attribute.", "True if the windows are the same, False otherwise."]}, {"name": "polynomial.hermite.Hermite.identity()", "path": "reference/generated/numpy.polynomial.hermite.hermite.identity", "type": "numpy.polynomial.hermite.Hermite", "text": ["method", "Identity function.", "If p is the returned series, then p(x) == x for all values of x.", "If given, the array must be of the form [beg, end], where beg and end are the endpoints of the domain. If None is given then the class domain is used. The default is None.", "If given, the resulting array must be if the form [beg, end], where beg and end are the endpoints of the window. If None is given then the class window is used. The default is None.", "Series of representing the identity."]}, {"name": "polynomial.hermite.Hermite.integ()", "path": "reference/generated/numpy.polynomial.hermite.hermite.integ", "type": "numpy.polynomial.hermite.Hermite", "text": ["method", "Integrate.", "Return a series instance that is the definite integral of the current series.", "The number of integrations to perform.", "Integration constants. The first constant is applied to the first integration, the second to the second, and so on. The list of values must less than or equal to m in length and any missing values are set to zero.", "The lower bound of the definite integral.", "A new series representing the integral. The domain is the same as the domain of the integrated series."]}, {"name": "polynomial.hermite.Hermite.linspace()", "path": "reference/generated/numpy.polynomial.hermite.hermite.linspace", "type": "numpy.polynomial.hermite.Hermite", "text": ["method", "Return x, y values at equally spaced points in domain.", "Returns the x, y values at n linearly spaced points across the domain. Here y is the value of the polynomial at the points x. By default the domain is the same as that of the series instance. This method is intended mostly as a plotting aid.", "New in version 1.5.0.", "Number of point pairs to return. The default value is 100.", "If not None, the specified domain is used instead of that of the calling instance. It should be of the form [beg,end]. The default is None which case the class domain is used.", "x is equal to linspace(self.domain[0], self.domain[1], n) and y is the series evaluated at element of x."]}, {"name": "polynomial.hermite.Hermite.mapparms()", "path": "reference/generated/numpy.polynomial.hermite.hermite.mapparms", "type": "numpy.polynomial.hermite.Hermite", "text": ["method", "Return the mapping parameters.", "The returned values define a linear map off + scl*x that is applied to the input arguments before the series is evaluated. The map depends on the domain and window; if the current domain is equal to the window the resulting map is the identity. If the coefficients of the series instance are to be used by themselves outside this class, then the linear function must be substituted for the x in the standard representation of the base polynomials.", "The mapping function is defined by off + scl*x.", "If the current domain is the interval [l1, r1] and the window is [l2, r2], then the linear mapping function L is defined by the equations:"]}, {"name": "polynomial.hermite.Hermite.roots()", "path": "reference/generated/numpy.polynomial.hermite.hermite.roots", "type": "numpy.polynomial.hermite.Hermite", "text": ["method", "Return the roots of the series polynomial.", "Compute the roots for the series. Note that the accuracy of the roots decrease the further outside the domain they lie.", "Array containing the roots of the series."]}, {"name": "polynomial.hermite.Hermite.trim()", "path": "reference/generated/numpy.polynomial.hermite.hermite.trim", "type": "numpy.polynomial.hermite.Hermite", "text": ["method", "Remove trailing coefficients", "Remove trailing coefficients until a coefficient is reached whose absolute value greater than tol or the beginning of the series is reached. If all the coefficients would be removed the series is set to [0]. A new series instance is returned with the new coefficients. The current instance remains unchanged.", "All trailing coefficients less than tol will be removed.", "New instance of series with trimmed coefficients."]}, {"name": "polynomial.hermite.Hermite.truncate()", "path": "reference/generated/numpy.polynomial.hermite.hermite.truncate", "type": "numpy.polynomial.hermite.Hermite", "text": ["method", "Truncate series to length size.", "Reduce the series to length size by discarding the high degree terms. The value of size must be a positive integer. This can be useful in least squares where the coefficients of the high degree terms may be very small.", "The series is reduced to length size by discarding the high degree terms. The value of size must be a positive integer.", "New instance of series with truncated coefficients."]}, {"name": "polynomial.hermite.hermline()", "path": "reference/generated/numpy.polynomial.hermite.hermline", "type": "numpy.polynomial.hermite.hermline", "text": ["Hermite series whose graph is a straight line.", "The specified line is given by off + scl*x.", "This module\u2019s representation of the Hermite series for off + scl*x.", "See also"]}, {"name": "polynomial.hermite.hermmul()", "path": "reference/generated/numpy.polynomial.hermite.hermmul", "type": "numpy.polynomial.hermite.hermmul", "text": ["Multiply one Hermite series by another.", "Returns the product of two Hermite series c1 * c2. The arguments are sequences of coefficients, from lowest order \u201cterm\u201d to highest, e.g., [1,2,3] represents the series P_0 + 2*P_1 + 3*P_2.", "1-D arrays of Hermite series coefficients ordered from low to high.", "Of Hermite series coefficients representing their product.", "See also", "In general, the (polynomial) product of two C-series results in terms that are not in the Hermite polynomial basis set. Thus, to express the product as a Hermite series, it is necessary to \u201creproject\u201d the product onto said basis set, which may produce \u201cunintuitive\u201d (but correct) results; see Examples section below."]}, {"name": "polynomial.hermite.hermmulx()", "path": "reference/generated/numpy.polynomial.hermite.hermmulx", "type": "numpy.polynomial.hermite.hermmulx", "text": ["Multiply a Hermite series by x.", "Multiply the Hermite series c by x, where x is the independent variable.", "1-D array of Hermite series coefficients ordered from low to high.", "Array representing the result of the multiplication.", "See also", "The multiplication uses the recursion relationship for Hermite polynomials in the form"]}, {"name": "polynomial.hermite.hermone", "path": "reference/generated/numpy.polynomial.hermite.hermone", "type": "numpy.polynomial.hermite.hermone", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.hermite.hermpow()", "path": "reference/generated/numpy.polynomial.hermite.hermpow", "type": "numpy.polynomial.hermite.hermpow", "text": ["Raise a Hermite series to a power.", "Returns the Hermite series c raised to the power pow. The argument c is a sequence of coefficients ordered from low to high. i.e., [1,2,3] is the series P_0 + 2*P_1 + 3*P_2.", "1-D array of Hermite series coefficients ordered from low to high.", "Power to which the series will be raised", "Maximum power allowed. This is mainly to limit growth of the series to unmanageable size. Default is 16", "Hermite series of power.", "See also"]}, {"name": "polynomial.hermite.hermroots()", "path": "reference/generated/numpy.polynomial.hermite.hermroots", "type": "numpy.polynomial.hermite.hermroots", "text": ["Compute the roots of a Hermite series.", "Return the roots (a.k.a. \u201czeros\u201d) of the polynomial", "1-D array of coefficients.", "Array of the roots of the series. If all the roots are real, then out is also real, otherwise it is complex.", "See also", "The root estimates are obtained as the eigenvalues of the companion matrix, Roots far from the origin of the complex plane may have large errors due to the numerical instability of the series for such values. Roots with multiplicity greater than 1 will also show larger errors as the value of the series near such points is relatively insensitive to errors in the roots. Isolated roots near the origin can be improved by a few iterations of Newton\u2019s method.", "The Hermite series basis polynomials aren\u2019t powers of x so the results of this function may seem unintuitive."]}, {"name": "polynomial.hermite.hermsub()", "path": "reference/generated/numpy.polynomial.hermite.hermsub", "type": "numpy.polynomial.hermite.hermsub", "text": ["Subtract one Hermite series from another.", "Returns the difference of two Hermite series c1 - c2. The sequences of coefficients are from lowest order term to highest, i.e., [1,2,3] represents the series P_0 + 2*P_1 + 3*P_2.", "1-D arrays of Hermite series coefficients ordered from low to high.", "Of Hermite series coefficients representing their difference.", "See also", "Unlike multiplication, division, etc., the difference of two Hermite series is a Hermite series (without having to \u201creproject\u201d the result onto the basis set) so subtraction, just like that of \u201cstandard\u201d polynomials, is simply \u201ccomponent-wise.\u201d"]}, {"name": "polynomial.hermite.hermtrim()", "path": "reference/generated/numpy.polynomial.hermite.hermtrim", "type": "numpy.polynomial.hermite.hermtrim", "text": ["Remove \u201csmall\u201d \u201ctrailing\u201d coefficients from a polynomial.", "\u201cSmall\u201d means \u201csmall in absolute value\u201d and is controlled by the parameter tol; \u201ctrailing\u201d means highest order coefficient(s), e.g., in [0, 1, 1, 0, 0] (which represents 0 + x + x**2 + 0*x**3 + 0*x**4) both the 3-rd and 4-th order coefficients would be \u201ctrimmed.\u201d", "1-d array of coefficients, ordered from lowest order to highest.", "Trailing (i.e., highest order) elements with absolute value less than or equal to tol (default value is zero) are removed.", "1-d array with trailing zeros removed. If the resulting series would be empty, a series containing a single zero is returned.", "If tol < 0", "See also"]}, {"name": "polynomial.hermite.hermval()", "path": "reference/generated/numpy.polynomial.hermite.hermval", "type": "numpy.polynomial.hermite.hermval", "text": ["Evaluate an Hermite series at points x.", "If c is of length n + 1, this function returns the value:", "The parameter x is converted to an array only if it is a tuple or a list, otherwise it is treated as a scalar. In either case, either x or its elements must support multiplication and addition both with themselves and with the elements of c.", "If c is a 1-D array, then p(x) will have the same shape as x. If c is multidimensional, then the shape of the result depends on the value of tensor. If tensor is true the shape will be c.shape[1:] + x.shape. If tensor is false the shape will be c.shape[1:]. Note that scalars have shape (,).", "Trailing zeros in the coefficients will be used in the evaluation, so they should be avoided if efficiency is a concern.", "If x is a list or tuple, it is converted to an ndarray, otherwise it is left unchanged and treated as a scalar. In either case, x or its elements must support addition and multiplication with with themselves and with the elements of c.", "Array of coefficients ordered so that the coefficients for terms of degree n are contained in c[n]. If c is multidimensional the remaining indices enumerate multiple polynomials. In the two dimensional case the coefficients may be thought of as stored in the columns of c.", "If True, the shape of the coefficient array is extended with ones on the right, one for each dimension of x. Scalars have dimension 0 for this action. The result is that every column of coefficients in c is evaluated for every element of x. If False, x is broadcast over the columns of c for the evaluation. This keyword is useful when c is multidimensional. The default value is True.", "New in version 1.7.0.", "The shape of the return value is described above.", "See also", "The evaluation uses Clenshaw recursion, aka synthetic division."]}, {"name": "polynomial.hermite.hermval2d()", "path": "reference/generated/numpy.polynomial.hermite.hermval2d", "type": "numpy.polynomial.hermite.hermval2d", "text": ["Evaluate a 2-D Hermite series at points (x, y).", "This function returns the values:", "The parameters x and y are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars and they must have the same shape after conversion. In either case, either x and y or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c is a 1-D array a one is implicitly appended to its shape to make it 2-D. The shape of the result will be c.shape[2:] + x.shape.", "The two dimensional series is evaluated at the points (x, y), where x and y must have the same shape. If x or y is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and if it isn\u2019t an ndarray it is treated as a scalar.", "Array of coefficients ordered so that the coefficient of the term of multi-degree i,j is contained in c[i,j]. If c has dimension greater than two the remaining indices enumerate multiple sets of coefficients.", "The values of the two dimensional polynomial at points formed with pairs of corresponding values from x and y.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.hermite.hermval3d()", "path": "reference/generated/numpy.polynomial.hermite.hermval3d", "type": "numpy.polynomial.hermite.hermval3d", "text": ["Evaluate a 3-D Hermite series at points (x, y, z).", "This function returns the values:", "The parameters x, y, and z are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars and they must have the same shape after conversion. In either case, either x, y, and z or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c has fewer than 3 dimensions, ones are implicitly appended to its shape to make it 3-D. The shape of the result will be c.shape[3:] + x.shape.", "The three dimensional series is evaluated at the points (x, y, z), where x, y, and z must have the same shape. If any of x, y, or z is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and if it isn\u2019t an ndarray it is treated as a scalar.", "Array of coefficients ordered so that the coefficient of the term of multi-degree i,j,k is contained in c[i,j,k]. If c has dimension greater than 3 the remaining indices enumerate multiple sets of coefficients.", "The values of the multidimensional polynomial on points formed with triples of corresponding values from x, y, and z.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.hermite.hermvander()", "path": "reference/generated/numpy.polynomial.hermite.hermvander", "type": "numpy.polynomial.hermite.hermvander", "text": ["Pseudo-Vandermonde matrix of given degree.", "Returns the pseudo-Vandermonde matrix of degree deg and sample points x. The pseudo-Vandermonde matrix is defined by", "where 0 <= i <= deg. The leading indices of V index the elements of x and the last index is the degree of the Hermite polynomial.", "If c is a 1-D array of coefficients of length n + 1 and V is the array V = hermvander(x, n), then np.dot(V, c) and hermval(x, c) are the same up to roundoff. This equivalence is useful both for least squares fitting and for the evaluation of a large number of Hermite series of the same degree and sample points.", "Array of points. The dtype is converted to float64 or complex128 depending on whether any of the elements are complex. If x is scalar it is converted to a 1-D array.", "Degree of the resulting matrix.", "The pseudo-Vandermonde matrix. The shape of the returned matrix is x.shape + (deg + 1,), where The last index is the degree of the corresponding Hermite polynomial. The dtype will be the same as the converted x."]}, {"name": "polynomial.hermite.hermvander2d()", "path": "reference/generated/numpy.polynomial.hermite.hermvander2d", "type": "numpy.polynomial.hermite.hermvander2d", "text": ["Pseudo-Vandermonde matrix of given degrees.", "Returns the pseudo-Vandermonde matrix of degrees deg and sample points (x, y). The pseudo-Vandermonde matrix is defined by", "where 0 <= i <= deg[0] and 0 <= j <= deg[1]. The leading indices of V index the points (x, y) and the last index encodes the degrees of the Hermite polynomials.", "If V = hermvander2d(x, y, [xdeg, ydeg]), then the columns of V correspond to the elements of a 2-D coefficient array c of shape (xdeg + 1, ydeg + 1) in the order", "and np.dot(V, c.flat) and hermval2d(x, y, c) will be the same up to roundoff. This equivalence is useful both for least squares fitting and for the evaluation of a large number of 2-D Hermite series of the same degrees and sample points.", "Arrays of point coordinates, all of the same shape. The dtypes will be converted to either float64 or complex128 depending on whether any of the elements are complex. Scalars are converted to 1-D arrays.", "List of maximum degrees of the form [x_deg, y_deg].", "The shape of the returned matrix is x.shape + (order,), where \\(order = (deg[0]+1)*(deg[1]+1)\\). The dtype will be the same as the converted x and y.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.hermite.hermvander3d()", "path": "reference/generated/numpy.polynomial.hermite.hermvander3d", "type": "numpy.polynomial.hermite.hermvander3d", "text": ["Pseudo-Vandermonde matrix of given degrees.", "Returns the pseudo-Vandermonde matrix of degrees deg and sample points (x, y, z). If l, m, n are the given degrees in x, y, z, then The pseudo-Vandermonde matrix is defined by", "where 0 <= i <= l, 0 <= j <= m, and 0 <= j <= n. The leading indices of V index the points (x, y, z) and the last index encodes the degrees of the Hermite polynomials.", "If V = hermvander3d(x, y, z, [xdeg, ydeg, zdeg]), then the columns of V correspond to the elements of a 3-D coefficient array c of shape (xdeg + 1, ydeg + 1, zdeg + 1) in the order", "and np.dot(V, c.flat) and hermval3d(x, y, z, c) will be the same up to roundoff. This equivalence is useful both for least squares fitting and for the evaluation of a large number of 3-D Hermite series of the same degrees and sample points.", "Arrays of point coordinates, all of the same shape. The dtypes will be converted to either float64 or complex128 depending on whether any of the elements are complex. Scalars are converted to 1-D arrays.", "List of maximum degrees of the form [x_deg, y_deg, z_deg].", "The shape of the returned matrix is x.shape + (order,), where \\(order = (deg[0]+1)*(deg[1]+1)*(deg[2]+1)\\). The dtype will be the same as the converted x, y, and z.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.hermite.hermweight()", "path": "reference/generated/numpy.polynomial.hermite.hermweight", "type": "numpy.polynomial.hermite.hermweight", "text": ["Weight function of the Hermite polynomials.", "The weight function is \\(\\exp(-x^2)\\) and the interval of integration is \\([-\\inf, \\inf]\\). the Hermite polynomials are orthogonal, but not normalized, with respect to this weight function.", "Values at which the weight function will be computed.", "The weight function at x.", "New in version 1.7.0."]}, {"name": "polynomial.hermite.hermx", "path": "reference/generated/numpy.polynomial.hermite.hermx", "type": "numpy.polynomial.hermite.hermx", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.hermite.hermzero", "path": "reference/generated/numpy.polynomial.hermite.hermzero", "type": "numpy.polynomial.hermite.hermzero", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.hermite.poly2herm()", "path": "reference/generated/numpy.polynomial.hermite.poly2herm", "type": "numpy.polynomial.hermite.poly2herm", "text": ["Convert a polynomial to a Hermite series.", "Convert an array representing the coefficients of a polynomial (relative to the \u201cstandard\u201d basis) ordered from lowest degree to highest, to an array of the coefficients of the equivalent Hermite series, ordered from lowest to highest degree.", "1-D array containing the polynomial coefficients", "1-D array containing the coefficients of the equivalent Hermite series.", "See also", "The easy way to do conversions between polynomial basis sets is to use the convert method of a class instance."]}, {"name": "polynomial.hermite_e.herme2poly()", "path": "reference/generated/numpy.polynomial.hermite_e.herme2poly", "type": "numpy.polynomial.hermite_e.herme2poly", "text": ["Convert a Hermite series to a polynomial.", "Convert an array representing the coefficients of a Hermite series, ordered from lowest degree to highest, to an array of the coefficients of the equivalent polynomial (relative to the \u201cstandard\u201d basis) ordered from lowest to highest degree.", "1-D array containing the Hermite series coefficients, ordered from lowest order term to highest.", "1-D array containing the coefficients of the equivalent polynomial (relative to the \u201cstandard\u201d basis) ordered from lowest order term to highest.", "See also", "The easy way to do conversions between polynomial basis sets is to use the convert method of a class instance."]}, {"name": "polynomial.hermite_e.hermeadd()", "path": "reference/generated/numpy.polynomial.hermite_e.hermeadd", "type": "numpy.polynomial.hermite_e.hermeadd", "text": ["Add one Hermite series to another.", "Returns the sum of two Hermite series c1 + c2. The arguments are sequences of coefficients ordered from lowest order term to highest, i.e., [1,2,3] represents the series P_0 + 2*P_1 + 3*P_2.", "1-D arrays of Hermite series coefficients ordered from low to high.", "Array representing the Hermite series of their sum.", "See also", "Unlike multiplication, division, etc., the sum of two Hermite series is a Hermite series (without having to \u201creproject\u201d the result onto the basis set) so addition, just like that of \u201cstandard\u201d polynomials, is simply \u201ccomponent-wise.\u201d"]}, {"name": "polynomial.hermite_e.hermecompanion()", "path": "reference/generated/numpy.polynomial.hermite_e.hermecompanion", "type": "numpy.polynomial.hermite_e.hermecompanion", "text": ["Return the scaled companion matrix of c.", "The basis polynomials are scaled so that the companion matrix is symmetric when c is an HermiteE basis polynomial. This provides better eigenvalue estimates than the unscaled case and for basis polynomials the eigenvalues are guaranteed to be real if numpy.linalg.eigvalsh is used to obtain them.", "1-D array of HermiteE series coefficients ordered from low to high degree.", "Scaled companion matrix of dimensions (deg, deg).", "New in version 1.7.0."]}, {"name": "polynomial.hermite_e.hermeder()", "path": "reference/generated/numpy.polynomial.hermite_e.hermeder", "type": "numpy.polynomial.hermite_e.hermeder", "text": ["Differentiate a Hermite_e series.", "Returns the series coefficients c differentiated m times along axis. At each iteration the result is multiplied by scl (the scaling factor is for use in a linear change of variable). The argument c is an array of coefficients from low to high degree along each axis, e.g., [1,2,3] represents the series 1*He_0 + 2*He_1 + 3*He_2 while [[1,2],[1,2]] represents 1*He_0(x)*He_0(y) + 1*He_1(x)*He_0(y)\n+ 2*He_0(x)*He_1(y) + 2*He_1(x)*He_1(y) if axis=0 is x and axis=1 is y.", "Array of Hermite_e series coefficients. If c is multidimensional the different axis correspond to different variables with the degree in each axis given by the corresponding index.", "Number of derivatives taken, must be non-negative. (Default: 1)", "Each differentiation is multiplied by scl. The end result is multiplication by scl**m. This is for use in a linear change of variable. (Default: 1)", "Axis over which the derivative is taken. (Default: 0).", "New in version 1.7.0.", "Hermite series of the derivative.", "See also", "In general, the result of differentiating a Hermite series does not resemble the same operation on a power series. Thus the result of this function may be \u201cunintuitive,\u201d albeit correct; see Examples section below."]}, {"name": "polynomial.hermite_e.hermediv()", "path": "reference/generated/numpy.polynomial.hermite_e.hermediv", "type": "numpy.polynomial.hermite_e.hermediv", "text": ["Divide one Hermite series by another.", "Returns the quotient-with-remainder of two Hermite series c1 / c2. The arguments are sequences of coefficients from lowest order \u201cterm\u201d to highest, e.g., [1,2,3] represents the series P_0 + 2*P_1 + 3*P_2.", "1-D arrays of Hermite series coefficients ordered from low to high.", "Of Hermite series coefficients representing the quotient and remainder.", "See also", "In general, the (polynomial) division of one Hermite series by another results in quotient and remainder terms that are not in the Hermite polynomial basis set. Thus, to express these results as a Hermite series, it is necessary to \u201creproject\u201d the results onto the Hermite basis set, which may produce \u201cunintuitive\u201d (but correct) results; see Examples section below."]}, {"name": "polynomial.hermite_e.hermedomain", "path": "reference/generated/numpy.polynomial.hermite_e.hermedomain", "type": "numpy.polynomial.hermite_e.hermedomain", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.hermite_e.hermefit()", "path": "reference/generated/numpy.polynomial.hermite_e.hermefit", "type": "numpy.polynomial.hermite_e.hermefit", "text": ["Least squares fit of Hermite series to data.", "Return the coefficients of a HermiteE series of degree deg that is the least squares fit to the data values y given at points x. If y is 1-D the returned coefficients will also be 1-D. If y is 2-D multiple fits are done, one for each column of y, and the resulting coefficients are stored in the corresponding columns of a 2-D return. The fitted polynomial(s) are in the form", "where n is deg.", "x-coordinates of the M sample points (x[i], y[i]).", "y-coordinates of the sample points. Several data sets of sample points sharing the same x-coordinates can be fitted at once by passing in a 2D-array that contains one dataset per column.", "Degree(s) of the fitting polynomials. If deg is a single integer all terms up to and including the deg\u2019th term are included in the fit. For NumPy versions >= 1.11.0 a list of integers specifying the degrees of the terms to include may be used instead.", "Relative condition number of the fit. Singular values smaller than this relative to the largest singular value will be ignored. The default value is len(x)*eps, where eps is the relative precision of the float type, about 2e-16 in most cases.", "Switch determining nature of return value. When it is False (the default) just the coefficients are returned, when True diagnostic information from the singular value decomposition is also returned.", "Weights. If not None, the weight w[i] applies to the unsquared residual y[i] - y_hat[i] at x[i]. Ideally the weights are chosen so that the errors of the products w[i]*y[i] all have the same variance. When using inverse-variance weighting, use w[i] = 1/sigma(y[i]). The default value is None.", "Hermite coefficients ordered from low to high. If y was 2-D, the coefficients for the data in column k of y are in column k.", "These values are only returned if full == True", "For more details, see numpy.linalg.lstsq.", "The rank of the coefficient matrix in the least-squares fit is deficient. The warning is only raised if full = False. The warnings can be turned off by", "See also", "Evaluates a Hermite series.", "pseudo Vandermonde matrix of Hermite series.", "HermiteE weight function.", "Computes a least-squares fit from the matrix.", "Computes spline fits.", "The solution is the coefficients of the HermiteE series p that minimizes the sum of the weighted squared errors", "where the \\(w_j\\) are the weights. This problem is solved by setting up the (typically) overdetermined matrix equation", "where V is the pseudo Vandermonde matrix of x, the elements of c are the coefficients to be solved for, and the elements of y are the observed values. This equation is then solved using the singular value decomposition of V.", "If some of the singular values of V are so small that they are neglected, then a RankWarning will be issued. This means that the coefficient values may be poorly determined. Using a lower order fit will usually get rid of the warning. The rcond parameter can also be set to a value smaller than its default, but the resulting fit may be spurious and have large contributions from roundoff error.", "Fits using HermiteE series are probably most useful when the data can be approximated by sqrt(w(x)) * p(x), where w(x) is the HermiteE weight. In that case the weight sqrt(w(x[i])) should be used together with data values y[i]/sqrt(w(x[i])). The weight function is available as hermeweight.", "Wikipedia, \u201cCurve fitting\u201d, https://en.wikipedia.org/wiki/Curve_fitting"]}, {"name": "polynomial.hermite_e.hermefromroots()", "path": "reference/generated/numpy.polynomial.hermite_e.hermefromroots", "type": "numpy.polynomial.hermite_e.hermefromroots", "text": ["Generate a HermiteE series with given roots.", "The function returns the coefficients of the polynomial", "in HermiteE form, where the r_n are the roots specified in roots. If a zero has multiplicity n, then it must appear in roots n times. For instance, if 2 is a root of multiplicity three and 3 is a root of multiplicity 2, then roots looks something like [2, 2, 2, 3, 3]. The roots can appear in any order.", "If the returned coefficients are c, then", "The coefficient of the last term is not generally 1 for monic polynomials in HermiteE form.", "Sequence containing the roots.", "1-D array of coefficients. If all roots are real then out is a real array, if some of the roots are complex, then out is complex even if all the coefficients in the result are real (see Examples below).", "See also"]}, {"name": "polynomial.hermite_e.hermegauss()", "path": "reference/generated/numpy.polynomial.hermite_e.hermegauss", "type": "numpy.polynomial.hermite_e.hermegauss", "text": ["Gauss-HermiteE quadrature.", "Computes the sample points and weights for Gauss-HermiteE quadrature. These sample points and weights will correctly integrate polynomials of degree \\(2*deg - 1\\) or less over the interval \\([-\\inf, \\inf]\\) with the weight function \\(f(x) = \\exp(-x^2/2)\\).", "Number of sample points and weights. It must be >= 1.", "1-D ndarray containing the sample points.", "1-D ndarray containing the weights.", "New in version 1.7.0.", "The results have only been tested up to degree 100, higher degrees may be problematic. The weights are determined by using the fact that", "where \\(c\\) is a constant independent of \\(k\\) and \\(x_k\\) is the k\u2019th root of \\(He_n\\), and then scaling the results to get the right value when integrating 1."]}, {"name": "polynomial.hermite_e.hermegrid2d()", "path": "reference/generated/numpy.polynomial.hermite_e.hermegrid2d", "type": "numpy.polynomial.hermite_e.hermegrid2d", "text": ["Evaluate a 2-D HermiteE series on the Cartesian product of x and y.", "This function returns the values:", "where the points (a, b) consist of all pairs formed by taking a from x and b from y. The resulting points form a grid with x in the first dimension and y in the second.", "The parameters x and y are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars. In either case, either x and y or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c has fewer than two dimensions, ones are implicitly appended to its shape to make it 2-D. The shape of the result will be c.shape[2:] + x.shape.", "The two dimensional series is evaluated at the points in the Cartesian product of x and y. If x or y is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and, if it isn\u2019t an ndarray, it is treated as a scalar.", "Array of coefficients ordered so that the coefficients for terms of degree i,j are contained in c[i,j]. If c has dimension greater than two the remaining indices enumerate multiple sets of coefficients.", "The values of the two dimensional polynomial at points in the Cartesian product of x and y.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.hermite_e.hermegrid3d()", "path": "reference/generated/numpy.polynomial.hermite_e.hermegrid3d", "type": "numpy.polynomial.hermite_e.hermegrid3d", "text": ["Evaluate a 3-D HermiteE series on the Cartesian product of x, y, and z.", "This function returns the values:", "where the points (a, b, c) consist of all triples formed by taking a from x, b from y, and c from z. The resulting points form a grid with x in the first dimension, y in the second, and z in the third.", "The parameters x, y, and z are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars. In either case, either x, y, and z or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c has fewer than three dimensions, ones are implicitly appended to its shape to make it 3-D. The shape of the result will be c.shape[3:] + x.shape + y.shape + z.shape.", "The three dimensional series is evaluated at the points in the Cartesian product of x, y, and z. If x,`y`, or z is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and, if it isn\u2019t an ndarray, it is treated as a scalar.", "Array of coefficients ordered so that the coefficients for terms of degree i,j are contained in c[i,j]. If c has dimension greater than two the remaining indices enumerate multiple sets of coefficients.", "The values of the two dimensional polynomial at points in the Cartesian product of x and y.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.hermite_e.hermeint()", "path": "reference/generated/numpy.polynomial.hermite_e.hermeint", "type": "numpy.polynomial.hermite_e.hermeint", "text": ["Integrate a Hermite_e series.", "Returns the Hermite_e series coefficients c integrated m times from lbnd along axis. At each iteration the resulting series is multiplied by scl and an integration constant, k, is added. The scaling factor is for use in a linear change of variable. (\u201cBuyer beware\u201d: note that, depending on what one is doing, one may want scl to be the reciprocal of what one might expect; for more information, see the Notes section below.) The argument c is an array of coefficients from low to high degree along each axis, e.g., [1,2,3] represents the series H_0 + 2*H_1 + 3*H_2 while [[1,2],[1,2]] represents 1*H_0(x)*H_0(y) + 1*H_1(x)*H_0(y) + 2*H_0(x)*H_1(y) +\n2*H_1(x)*H_1(y) if axis=0 is x and axis=1 is y.", "Array of Hermite_e series coefficients. If c is multidimensional the different axis correspond to different variables with the degree in each axis given by the corresponding index.", "Order of integration, must be positive. (Default: 1)", "Integration constant(s). The value of the first integral at lbnd is the first value in the list, the value of the second integral at lbnd is the second value, etc. If k == [] (the default), all constants are set to zero. If m == 1, a single scalar can be given instead of a list.", "The lower bound of the integral. (Default: 0)", "Following each integration the result is multiplied by scl before the integration constant is added. (Default: 1)", "Axis over which the integral is taken. (Default: 0).", "New in version 1.7.0.", "Hermite_e series coefficients of the integral.", "If m < 0, len(k) > m, np.ndim(lbnd) != 0, or np.ndim(scl) != 0.", "See also", "Note that the result of each integration is multiplied by scl. Why is this important to note? Say one is making a linear change of variable \\(u = ax + b\\) in an integral relative to x. Then \\(dx = du/a\\), so one will need to set scl equal to \\(1/a\\) - perhaps not what one would have first thought.", "Also note that, in general, the result of integrating a C-series needs to be \u201creprojected\u201d onto the C-series basis set. Thus, typically, the result of this function is \u201cunintuitive,\u201d albeit correct; see Examples section below."]}, {"name": "polynomial.hermite_e.hermeline()", "path": "reference/generated/numpy.polynomial.hermite_e.hermeline", "type": "numpy.polynomial.hermite_e.hermeline", "text": ["Hermite series whose graph is a straight line.", "The specified line is given by off + scl*x.", "This module\u2019s representation of the Hermite series for off + scl*x.", "See also"]}, {"name": "polynomial.hermite_e.hermemul()", "path": "reference/generated/numpy.polynomial.hermite_e.hermemul", "type": "numpy.polynomial.hermite_e.hermemul", "text": ["Multiply one Hermite series by another.", "Returns the product of two Hermite series c1 * c2. The arguments are sequences of coefficients, from lowest order \u201cterm\u201d to highest, e.g., [1,2,3] represents the series P_0 + 2*P_1 + 3*P_2.", "1-D arrays of Hermite series coefficients ordered from low to high.", "Of Hermite series coefficients representing their product.", "See also", "In general, the (polynomial) product of two C-series results in terms that are not in the Hermite polynomial basis set. Thus, to express the product as a Hermite series, it is necessary to \u201creproject\u201d the product onto said basis set, which may produce \u201cunintuitive\u201d (but correct) results; see Examples section below."]}, {"name": "polynomial.hermite_e.hermemulx()", "path": "reference/generated/numpy.polynomial.hermite_e.hermemulx", "type": "numpy.polynomial.hermite_e.hermemulx", "text": ["Multiply a Hermite series by x.", "Multiply the Hermite series c by x, where x is the independent variable.", "1-D array of Hermite series coefficients ordered from low to high.", "Array representing the result of the multiplication.", "The multiplication uses the recursion relationship for Hermite polynomials in the form"]}, {"name": "polynomial.hermite_e.hermeone", "path": "reference/generated/numpy.polynomial.hermite_e.hermeone", "type": "numpy.polynomial.hermite_e.hermeone", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.hermite_e.hermepow()", "path": "reference/generated/numpy.polynomial.hermite_e.hermepow", "type": "numpy.polynomial.hermite_e.hermepow", "text": ["Raise a Hermite series to a power.", "Returns the Hermite series c raised to the power pow. The argument c is a sequence of coefficients ordered from low to high. i.e., [1,2,3] is the series P_0 + 2*P_1 + 3*P_2.", "1-D array of Hermite series coefficients ordered from low to high.", "Power to which the series will be raised", "Maximum power allowed. This is mainly to limit growth of the series to unmanageable size. Default is 16", "Hermite series of power.", "See also"]}, {"name": "polynomial.hermite_e.hermeroots()", "path": "reference/generated/numpy.polynomial.hermite_e.hermeroots", "type": "numpy.polynomial.hermite_e.hermeroots", "text": ["Compute the roots of a HermiteE series.", "Return the roots (a.k.a. \u201czeros\u201d) of the polynomial", "1-D array of coefficients.", "Array of the roots of the series. If all the roots are real, then out is also real, otherwise it is complex.", "See also", "The root estimates are obtained as the eigenvalues of the companion matrix, Roots far from the origin of the complex plane may have large errors due to the numerical instability of the series for such values. Roots with multiplicity greater than 1 will also show larger errors as the value of the series near such points is relatively insensitive to errors in the roots. Isolated roots near the origin can be improved by a few iterations of Newton\u2019s method.", "The HermiteE series basis polynomials aren\u2019t powers of x so the results of this function may seem unintuitive."]}, {"name": "polynomial.hermite_e.hermesub()", "path": "reference/generated/numpy.polynomial.hermite_e.hermesub", "type": "numpy.polynomial.hermite_e.hermesub", "text": ["Subtract one Hermite series from another.", "Returns the difference of two Hermite series c1 - c2. The sequences of coefficients are from lowest order term to highest, i.e., [1,2,3] represents the series P_0 + 2*P_1 + 3*P_2.", "1-D arrays of Hermite series coefficients ordered from low to high.", "Of Hermite series coefficients representing their difference.", "See also", "Unlike multiplication, division, etc., the difference of two Hermite series is a Hermite series (without having to \u201creproject\u201d the result onto the basis set) so subtraction, just like that of \u201cstandard\u201d polynomials, is simply \u201ccomponent-wise.\u201d"]}, {"name": "polynomial.hermite_e.hermetrim()", "path": "reference/generated/numpy.polynomial.hermite_e.hermetrim", "type": "numpy.polynomial.hermite_e.hermetrim", "text": ["Remove \u201csmall\u201d \u201ctrailing\u201d coefficients from a polynomial.", "\u201cSmall\u201d means \u201csmall in absolute value\u201d and is controlled by the parameter tol; \u201ctrailing\u201d means highest order coefficient(s), e.g., in [0, 1, 1, 0, 0] (which represents 0 + x + x**2 + 0*x**3 + 0*x**4) both the 3-rd and 4-th order coefficients would be \u201ctrimmed.\u201d", "1-d array of coefficients, ordered from lowest order to highest.", "Trailing (i.e., highest order) elements with absolute value less than or equal to tol (default value is zero) are removed.", "1-d array with trailing zeros removed. If the resulting series would be empty, a series containing a single zero is returned.", "If tol < 0", "See also"]}, {"name": "polynomial.hermite_e.hermeval()", "path": "reference/generated/numpy.polynomial.hermite_e.hermeval", "type": "numpy.polynomial.hermite_e.hermeval", "text": ["Evaluate an HermiteE series at points x.", "If c is of length n + 1, this function returns the value:", "The parameter x is converted to an array only if it is a tuple or a list, otherwise it is treated as a scalar. In either case, either x or its elements must support multiplication and addition both with themselves and with the elements of c.", "If c is a 1-D array, then p(x) will have the same shape as x. If c is multidimensional, then the shape of the result depends on the value of tensor. If tensor is true the shape will be c.shape[1:] + x.shape. If tensor is false the shape will be c.shape[1:]. Note that scalars have shape (,).", "Trailing zeros in the coefficients will be used in the evaluation, so they should be avoided if efficiency is a concern.", "If x is a list or tuple, it is converted to an ndarray, otherwise it is left unchanged and treated as a scalar. In either case, x or its elements must support addition and multiplication with with themselves and with the elements of c.", "Array of coefficients ordered so that the coefficients for terms of degree n are contained in c[n]. If c is multidimensional the remaining indices enumerate multiple polynomials. In the two dimensional case the coefficients may be thought of as stored in the columns of c.", "If True, the shape of the coefficient array is extended with ones on the right, one for each dimension of x. Scalars have dimension 0 for this action. The result is that every column of coefficients in c is evaluated for every element of x. If False, x is broadcast over the columns of c for the evaluation. This keyword is useful when c is multidimensional. The default value is True.", "New in version 1.7.0.", "The shape of the return value is described above.", "See also", "The evaluation uses Clenshaw recursion, aka synthetic division."]}, {"name": "polynomial.hermite_e.hermeval2d()", "path": "reference/generated/numpy.polynomial.hermite_e.hermeval2d", "type": "numpy.polynomial.hermite_e.hermeval2d", "text": ["Evaluate a 2-D HermiteE series at points (x, y).", "This function returns the values:", "The parameters x and y are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars and they must have the same shape after conversion. In either case, either x and y or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c is a 1-D array a one is implicitly appended to its shape to make it 2-D. The shape of the result will be c.shape[2:] + x.shape.", "The two dimensional series is evaluated at the points (x, y), where x and y must have the same shape. If x or y is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and if it isn\u2019t an ndarray it is treated as a scalar.", "Array of coefficients ordered so that the coefficient of the term of multi-degree i,j is contained in c[i,j]. If c has dimension greater than two the remaining indices enumerate multiple sets of coefficients.", "The values of the two dimensional polynomial at points formed with pairs of corresponding values from x and y.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.hermite_e.hermeval3d()", "path": "reference/generated/numpy.polynomial.hermite_e.hermeval3d", "type": "numpy.polynomial.hermite_e.hermeval3d", "text": ["Evaluate a 3-D Hermite_e series at points (x, y, z).", "This function returns the values:", "The parameters x, y, and z are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars and they must have the same shape after conversion. In either case, either x, y, and z or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c has fewer than 3 dimensions, ones are implicitly appended to its shape to make it 3-D. The shape of the result will be c.shape[3:] + x.shape.", "The three dimensional series is evaluated at the points (x, y, z), where x, y, and z must have the same shape. If any of x, y, or z is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and if it isn\u2019t an ndarray it is treated as a scalar.", "Array of coefficients ordered so that the coefficient of the term of multi-degree i,j,k is contained in c[i,j,k]. If c has dimension greater than 3 the remaining indices enumerate multiple sets of coefficients.", "The values of the multidimensional polynomial on points formed with triples of corresponding values from x, y, and z.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.hermite_e.hermevander()", "path": "reference/generated/numpy.polynomial.hermite_e.hermevander", "type": "numpy.polynomial.hermite_e.hermevander", "text": ["Pseudo-Vandermonde matrix of given degree.", "Returns the pseudo-Vandermonde matrix of degree deg and sample points x. The pseudo-Vandermonde matrix is defined by", "where 0 <= i <= deg. The leading indices of V index the elements of x and the last index is the degree of the HermiteE polynomial.", "If c is a 1-D array of coefficients of length n + 1 and V is the array V = hermevander(x, n), then np.dot(V, c) and hermeval(x, c) are the same up to roundoff. This equivalence is useful both for least squares fitting and for the evaluation of a large number of HermiteE series of the same degree and sample points.", "Array of points. The dtype is converted to float64 or complex128 depending on whether any of the elements are complex. If x is scalar it is converted to a 1-D array.", "Degree of the resulting matrix.", "The pseudo-Vandermonde matrix. The shape of the returned matrix is x.shape + (deg + 1,), where The last index is the degree of the corresponding HermiteE polynomial. The dtype will be the same as the converted x."]}, {"name": "polynomial.hermite_e.hermevander2d()", "path": "reference/generated/numpy.polynomial.hermite_e.hermevander2d", "type": "numpy.polynomial.hermite_e.hermevander2d", "text": ["Pseudo-Vandermonde matrix of given degrees.", "Returns the pseudo-Vandermonde matrix of degrees deg and sample points (x, y). The pseudo-Vandermonde matrix is defined by", "where 0 <= i <= deg[0] and 0 <= j <= deg[1]. The leading indices of V index the points (x, y) and the last index encodes the degrees of the HermiteE polynomials.", "If V = hermevander2d(x, y, [xdeg, ydeg]), then the columns of V correspond to the elements of a 2-D coefficient array c of shape (xdeg + 1, ydeg + 1) in the order", "and np.dot(V, c.flat) and hermeval2d(x, y, c) will be the same up to roundoff. This equivalence is useful both for least squares fitting and for the evaluation of a large number of 2-D HermiteE series of the same degrees and sample points.", "Arrays of point coordinates, all of the same shape. The dtypes will be converted to either float64 or complex128 depending on whether any of the elements are complex. Scalars are converted to 1-D arrays.", "List of maximum degrees of the form [x_deg, y_deg].", "The shape of the returned matrix is x.shape + (order,), where \\(order = (deg[0]+1)*(deg[1]+1)\\). The dtype will be the same as the converted x and y.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.hermite_e.hermevander3d()", "path": "reference/generated/numpy.polynomial.hermite_e.hermevander3d", "type": "numpy.polynomial.hermite_e.hermevander3d", "text": ["Pseudo-Vandermonde matrix of given degrees.", "Returns the pseudo-Vandermonde matrix of degrees deg and sample points (x, y, z). If l, m, n are the given degrees in x, y, z, then Hehe pseudo-Vandermonde matrix is defined by", "where 0 <= i <= l, 0 <= j <= m, and 0 <= j <= n. The leading indices of V index the points (x, y, z) and the last index encodes the degrees of the HermiteE polynomials.", "If V = hermevander3d(x, y, z, [xdeg, ydeg, zdeg]), then the columns of V correspond to the elements of a 3-D coefficient array c of shape (xdeg + 1, ydeg + 1, zdeg + 1) in the order", "and np.dot(V, c.flat) and hermeval3d(x, y, z, c) will be the same up to roundoff. This equivalence is useful both for least squares fitting and for the evaluation of a large number of 3-D HermiteE series of the same degrees and sample points.", "Arrays of point coordinates, all of the same shape. The dtypes will be converted to either float64 or complex128 depending on whether any of the elements are complex. Scalars are converted to 1-D arrays.", "List of maximum degrees of the form [x_deg, y_deg, z_deg].", "The shape of the returned matrix is x.shape + (order,), where \\(order = (deg[0]+1)*(deg[1]+1)*(deg[2]+1)\\). The dtype will be the same as the converted x, y, and z.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.hermite_e.hermeweight()", "path": "reference/generated/numpy.polynomial.hermite_e.hermeweight", "type": "numpy.polynomial.hermite_e.hermeweight", "text": ["Weight function of the Hermite_e polynomials.", "The weight function is \\(\\exp(-x^2/2)\\) and the interval of integration is \\([-\\inf, \\inf]\\). the HermiteE polynomials are orthogonal, but not normalized, with respect to this weight function.", "Values at which the weight function will be computed.", "The weight function at x.", "New in version 1.7.0."]}, {"name": "polynomial.hermite_e.hermex", "path": "reference/generated/numpy.polynomial.hermite_e.hermex", "type": "numpy.polynomial.hermite_e.hermex", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.hermite_e.hermezero", "path": "reference/generated/numpy.polynomial.hermite_e.hermezero", "type": "numpy.polynomial.hermite_e.hermezero", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.hermite_e.HermiteE.__call__()", "path": "reference/generated/numpy.polynomial.hermite_e.hermitee.__call__", "type": "numpy.polynomial.hermite_e.HermiteE", "text": ["method", "Call self as a function."]}, {"name": "polynomial.hermite_e.HermiteE.basis()", "path": "reference/generated/numpy.polynomial.hermite_e.hermitee.basis", "type": "numpy.polynomial.hermite_e.HermiteE", "text": ["method", "Series basis polynomial of degree deg.", "Returns the series representing the basis polynomial of degree deg.", "New in version 1.7.0.", "Degree of the basis polynomial for the series. Must be >= 0.", "If given, the array must be of the form [beg, end], where beg and end are the endpoints of the domain. If None is given then the class domain is used. The default is None.", "If given, the resulting array must be if the form [beg, end], where beg and end are the endpoints of the window. If None is given then the class window is used. The default is None.", "A series with the coefficient of the deg term set to one and all others zero."]}, {"name": "polynomial.hermite_e.HermiteE.cast()", "path": "reference/generated/numpy.polynomial.hermite_e.hermitee.cast", "type": "numpy.polynomial.hermite_e.HermiteE", "text": ["method", "Convert series to series of this class.", "The series is expected to be an instance of some polynomial series of one of the types supported by by the numpy.polynomial module, but could be some other class that supports the convert method.", "New in version 1.7.0.", "The series instance to be converted.", "If given, the array must be of the form [beg, end], where beg and end are the endpoints of the domain. If None is given then the class domain is used. The default is None.", "If given, the resulting array must be if the form [beg, end], where beg and end are the endpoints of the window. If None is given then the class window is used. The default is None.", "A series of the same kind as the calling class and equal to series when evaluated.", "See also", "similar instance method"]}, {"name": "polynomial.hermite_e.HermiteE.convert()", "path": "reference/generated/numpy.polynomial.hermite_e.hermitee.convert", "type": "numpy.polynomial.hermite_e.HermiteE", "text": ["method", "Convert series to a different kind and/or domain and/or window.", "The domain of the converted series. If the value is None, the default domain of kind is used.", "The polynomial series type class to which the current instance should be converted. If kind is None, then the class of the current instance is used.", "The window of the converted series. If the value is None, the default window of kind is used.", "The returned class can be of different type than the current instance and/or have a different domain and/or different window.", "Conversion between domains and class types can result in numerically ill defined series."]}, {"name": "polynomial.hermite_e.HermiteE.copy()", "path": "reference/generated/numpy.polynomial.hermite_e.hermitee.copy", "type": "numpy.polynomial.hermite_e.HermiteE", "text": ["method", "Return a copy.", "Copy of self."]}, {"name": "polynomial.hermite_e.HermiteE.cutdeg()", "path": "reference/generated/numpy.polynomial.hermite_e.hermitee.cutdeg", "type": "numpy.polynomial.hermite_e.HermiteE", "text": ["method", "Truncate series to the given degree.", "Reduce the degree of the series to deg by discarding the high order terms. If deg is greater than the current degree a copy of the current series is returned. This can be useful in least squares where the coefficients of the high degree terms may be very small.", "New in version 1.5.0.", "The series is reduced to degree deg by discarding the high order terms. The value of deg must be a non-negative integer.", "New instance of series with reduced degree."]}, {"name": "polynomial.hermite_e.HermiteE.degree()", "path": "reference/generated/numpy.polynomial.hermite_e.hermitee.degree", "type": "numpy.polynomial.hermite_e.HermiteE", "text": ["method", "The degree of the series.", "New in version 1.5.0.", "Degree of the series, one less than the number of coefficients."]}, {"name": "polynomial.hermite_e.HermiteE.deriv()", "path": "reference/generated/numpy.polynomial.hermite_e.hermitee.deriv", "type": "numpy.polynomial.hermite_e.HermiteE", "text": ["method", "Differentiate.", "Return a series instance of that is the derivative of the current series.", "Find the derivative of order m.", "A new series representing the derivative. The domain is the same as the domain of the differentiated series."]}, {"name": "polynomial.hermite_e.HermiteE.domain", "path": "reference/generated/numpy.polynomial.hermite_e.hermitee.domain", "type": "Polynomials", "text": ["attribute"]}, {"name": "polynomial.hermite_e.HermiteE.fit()", "path": "reference/generated/numpy.polynomial.hermite_e.hermitee.fit", "type": "numpy.polynomial.hermite_e.HermiteE", "text": ["method", "Least squares fit to data.", "Return a series instance that is the least squares fit to the data y sampled at x. The domain of the returned instance can be specified and this will often result in a superior fit with less chance of ill conditioning.", "x-coordinates of the M sample points (x[i], y[i]).", "y-coordinates of the M sample points (x[i], y[i]).", "Degree(s) of the fitting polynomials. If deg is a single integer all terms up to and including the deg\u2019th term are included in the fit. For NumPy versions >= 1.11.0 a list of integers specifying the degrees of the terms to include may be used instead.", "Domain to use for the returned series. If None, then a minimal domain that covers the points x is chosen. If [] the class domain is used. The default value was the class domain in NumPy 1.4 and None in later versions. The [] option was added in numpy 1.5.0.", "Relative condition number of the fit. Singular values smaller than this relative to the largest singular value will be ignored. The default value is len(x)*eps, where eps is the relative precision of the float type, about 2e-16 in most cases.", "Switch determining nature of return value. When it is False (the default) just the coefficients are returned, when True diagnostic information from the singular value decomposition is also returned.", "Weights. If not None, the weight w[i] applies to the unsquared residual y[i] - y_hat[i] at x[i]. Ideally the weights are chosen so that the errors of the products w[i]*y[i] all have the same variance. When using inverse-variance weighting, use w[i] = 1/sigma(y[i]). The default value is None.", "New in version 1.5.0.", "Window to use for the returned series. The default value is the default class domain", "New in version 1.6.0.", "A series that represents the least squares fit to the data and has the domain and window specified in the call. If the coefficients for the unscaled and unshifted basis polynomials are of interest, do new_series.convert().coef.", "These values are only returned if full == True", "For more details, see linalg.lstsq."]}, {"name": "polynomial.hermite_e.HermiteE.fromroots()", "path": "reference/generated/numpy.polynomial.hermite_e.hermitee.fromroots", "type": "numpy.polynomial.hermite_e.HermiteE", "text": ["method", "Return series instance that has the specified roots.", "Returns a series representing the product (x - r[0])*(x - r[1])*...*(x - r[n-1]), where r is a list of roots.", "List of roots.", "Domain for the resulting series. If None the domain is the interval from the smallest root to the largest. If [] the domain is the class domain. The default is [].", "Window for the returned series. If None the class window is used. The default is None.", "Series with the specified roots."]}, {"name": "polynomial.hermite_e.HermiteE.has_samecoef()", "path": "reference/generated/numpy.polynomial.hermite_e.hermitee.has_samecoef", "type": "numpy.polynomial.hermite_e.HermiteE", "text": ["method", "Check if coefficients match.", "New in version 1.6.0.", "The other class must have the coef attribute.", "True if the coefficients are the same, False otherwise."]}, {"name": "polynomial.hermite_e.HermiteE.has_samedomain()", "path": "reference/generated/numpy.polynomial.hermite_e.hermitee.has_samedomain", "type": "numpy.polynomial.hermite_e.HermiteE", "text": ["method", "Check if domains match.", "New in version 1.6.0.", "The other class must have the domain attribute.", "True if the domains are the same, False otherwise."]}, {"name": "polynomial.hermite_e.HermiteE.has_sametype()", "path": "reference/generated/numpy.polynomial.hermite_e.hermitee.has_sametype", "type": "numpy.polynomial.hermite_e.HermiteE", "text": ["method", "Check if types match.", "New in version 1.7.0.", "Class instance.", "True if other is same class as self"]}, {"name": "polynomial.hermite_e.HermiteE.has_samewindow()", "path": "reference/generated/numpy.polynomial.hermite_e.hermitee.has_samewindow", "type": "numpy.polynomial.hermite_e.HermiteE", "text": ["method", "Check if windows match.", "New in version 1.6.0.", "The other class must have the window attribute.", "True if the windows are the same, False otherwise."]}, {"name": "polynomial.hermite_e.HermiteE.identity()", "path": "reference/generated/numpy.polynomial.hermite_e.hermitee.identity", "type": "numpy.polynomial.hermite_e.HermiteE", "text": ["method", "Identity function.", "If p is the returned series, then p(x) == x for all values of x.", "If given, the array must be of the form [beg, end], where beg and end are the endpoints of the domain. If None is given then the class domain is used. The default is None.", "If given, the resulting array must be if the form [beg, end], where beg and end are the endpoints of the window. If None is given then the class window is used. The default is None.", "Series of representing the identity."]}, {"name": "polynomial.hermite_e.HermiteE.integ()", "path": "reference/generated/numpy.polynomial.hermite_e.hermitee.integ", "type": "numpy.polynomial.hermite_e.HermiteE", "text": ["method", "Integrate.", "Return a series instance that is the definite integral of the current series.", "The number of integrations to perform.", "Integration constants. The first constant is applied to the first integration, the second to the second, and so on. The list of values must less than or equal to m in length and any missing values are set to zero.", "The lower bound of the definite integral.", "A new series representing the integral. The domain is the same as the domain of the integrated series."]}, {"name": "polynomial.hermite_e.HermiteE.linspace()", "path": "reference/generated/numpy.polynomial.hermite_e.hermitee.linspace", "type": "numpy.polynomial.hermite_e.HermiteE", "text": ["method", "Return x, y values at equally spaced points in domain.", "Returns the x, y values at n linearly spaced points across the domain. Here y is the value of the polynomial at the points x. By default the domain is the same as that of the series instance. This method is intended mostly as a plotting aid.", "New in version 1.5.0.", "Number of point pairs to return. The default value is 100.", "If not None, the specified domain is used instead of that of the calling instance. It should be of the form [beg,end]. The default is None which case the class domain is used.", "x is equal to linspace(self.domain[0], self.domain[1], n) and y is the series evaluated at element of x."]}, {"name": "polynomial.hermite_e.HermiteE.mapparms()", "path": "reference/generated/numpy.polynomial.hermite_e.hermitee.mapparms", "type": "numpy.polynomial.hermite_e.HermiteE", "text": ["method", "Return the mapping parameters.", "The returned values define a linear map off + scl*x that is applied to the input arguments before the series is evaluated. The map depends on the domain and window; if the current domain is equal to the window the resulting map is the identity. If the coefficients of the series instance are to be used by themselves outside this class, then the linear function must be substituted for the x in the standard representation of the base polynomials.", "The mapping function is defined by off + scl*x.", "If the current domain is the interval [l1, r1] and the window is [l2, r2], then the linear mapping function L is defined by the equations:"]}, {"name": "polynomial.hermite_e.HermiteE.roots()", "path": "reference/generated/numpy.polynomial.hermite_e.hermitee.roots", "type": "numpy.polynomial.hermite_e.HermiteE", "text": ["method", "Return the roots of the series polynomial.", "Compute the roots for the series. Note that the accuracy of the roots decrease the further outside the domain they lie.", "Array containing the roots of the series."]}, {"name": "polynomial.hermite_e.HermiteE.trim()", "path": "reference/generated/numpy.polynomial.hermite_e.hermitee.trim", "type": "numpy.polynomial.hermite_e.HermiteE", "text": ["method", "Remove trailing coefficients", "Remove trailing coefficients until a coefficient is reached whose absolute value greater than tol or the beginning of the series is reached. If all the coefficients would be removed the series is set to [0]. A new series instance is returned with the new coefficients. The current instance remains unchanged.", "All trailing coefficients less than tol will be removed.", "New instance of series with trimmed coefficients."]}, {"name": "polynomial.hermite_e.HermiteE.truncate()", "path": "reference/generated/numpy.polynomial.hermite_e.hermitee.truncate", "type": "numpy.polynomial.hermite_e.HermiteE", "text": ["method", "Truncate series to length size.", "Reduce the series to length size by discarding the high degree terms. The value of size must be a positive integer. This can be useful in least squares where the coefficients of the high degree terms may be very small.", "The series is reduced to length size by discarding the high degree terms. The value of size must be a positive integer.", "New instance of series with truncated coefficients."]}, {"name": "polynomial.hermite_e.poly2herme()", "path": "reference/generated/numpy.polynomial.hermite_e.poly2herme", "type": "numpy.polynomial.hermite_e.poly2herme", "text": ["Convert a polynomial to a Hermite series.", "Convert an array representing the coefficients of a polynomial (relative to the \u201cstandard\u201d basis) ordered from lowest degree to highest, to an array of the coefficients of the equivalent Hermite series, ordered from lowest to highest degree.", "1-D array containing the polynomial coefficients", "1-D array containing the coefficients of the equivalent Hermite series.", "See also", "The easy way to do conversions between polynomial basis sets is to use the convert method of a class instance."]}, {"name": "polynomial.laguerre.lag2poly()", "path": "reference/generated/numpy.polynomial.laguerre.lag2poly", "type": "numpy.polynomial.laguerre.lag2poly", "text": ["Convert a Laguerre series to a polynomial.", "Convert an array representing the coefficients of a Laguerre series, ordered from lowest degree to highest, to an array of the coefficients of the equivalent polynomial (relative to the \u201cstandard\u201d basis) ordered from lowest to highest degree.", "1-D array containing the Laguerre series coefficients, ordered from lowest order term to highest.", "1-D array containing the coefficients of the equivalent polynomial (relative to the \u201cstandard\u201d basis) ordered from lowest order term to highest.", "See also", "The easy way to do conversions between polynomial basis sets is to use the convert method of a class instance."]}, {"name": "polynomial.laguerre.lagadd()", "path": "reference/generated/numpy.polynomial.laguerre.lagadd", "type": "numpy.polynomial.laguerre.lagadd", "text": ["Add one Laguerre series to another.", "Returns the sum of two Laguerre series c1 + c2. The arguments are sequences of coefficients ordered from lowest order term to highest, i.e., [1,2,3] represents the series P_0 + 2*P_1 + 3*P_2.", "1-D arrays of Laguerre series coefficients ordered from low to high.", "Array representing the Laguerre series of their sum.", "See also", "Unlike multiplication, division, etc., the sum of two Laguerre series is a Laguerre series (without having to \u201creproject\u201d the result onto the basis set) so addition, just like that of \u201cstandard\u201d polynomials, is simply \u201ccomponent-wise.\u201d"]}, {"name": "polynomial.laguerre.lagcompanion()", "path": "reference/generated/numpy.polynomial.laguerre.lagcompanion", "type": "numpy.polynomial.laguerre.lagcompanion", "text": ["Return the companion matrix of c.", "The usual companion matrix of the Laguerre polynomials is already symmetric when c is a basis Laguerre polynomial, so no scaling is applied.", "1-D array of Laguerre series coefficients ordered from low to high degree.", "Companion matrix of dimensions (deg, deg).", "New in version 1.7.0."]}, {"name": "polynomial.laguerre.lagder()", "path": "reference/generated/numpy.polynomial.laguerre.lagder", "type": "numpy.polynomial.laguerre.lagder", "text": ["Differentiate a Laguerre series.", "Returns the Laguerre series coefficients c differentiated m times along axis. At each iteration the result is multiplied by scl (the scaling factor is for use in a linear change of variable). The argument c is an array of coefficients from low to high degree along each axis, e.g., [1,2,3] represents the series 1*L_0 + 2*L_1 + 3*L_2 while [[1,2],[1,2]] represents 1*L_0(x)*L_0(y) + 1*L_1(x)*L_0(y) +\n2*L_0(x)*L_1(y) + 2*L_1(x)*L_1(y) if axis=0 is x and axis=1 is y.", "Array of Laguerre series coefficients. If c is multidimensional the different axis correspond to different variables with the degree in each axis given by the corresponding index.", "Number of derivatives taken, must be non-negative. (Default: 1)", "Each differentiation is multiplied by scl. The end result is multiplication by scl**m. This is for use in a linear change of variable. (Default: 1)", "Axis over which the derivative is taken. (Default: 0).", "New in version 1.7.0.", "Laguerre series of the derivative.", "See also", "In general, the result of differentiating a Laguerre series does not resemble the same operation on a power series. Thus the result of this function may be \u201cunintuitive,\u201d albeit correct; see Examples section below."]}, {"name": "polynomial.laguerre.lagdiv()", "path": "reference/generated/numpy.polynomial.laguerre.lagdiv", "type": "numpy.polynomial.laguerre.lagdiv", "text": ["Divide one Laguerre series by another.", "Returns the quotient-with-remainder of two Laguerre series c1 / c2. The arguments are sequences of coefficients from lowest order \u201cterm\u201d to highest, e.g., [1,2,3] represents the series P_0 + 2*P_1 + 3*P_2.", "1-D arrays of Laguerre series coefficients ordered from low to high.", "Of Laguerre series coefficients representing the quotient and remainder.", "See also", "In general, the (polynomial) division of one Laguerre series by another results in quotient and remainder terms that are not in the Laguerre polynomial basis set. Thus, to express these results as a Laguerre series, it is necessary to \u201creproject\u201d the results onto the Laguerre basis set, which may produce \u201cunintuitive\u201d (but correct) results; see Examples section below."]}, {"name": "polynomial.laguerre.lagdomain", "path": "reference/generated/numpy.polynomial.laguerre.lagdomain", "type": "numpy.polynomial.laguerre.lagdomain", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.laguerre.lagfit()", "path": "reference/generated/numpy.polynomial.laguerre.lagfit", "type": "numpy.polynomial.laguerre.lagfit", "text": ["Least squares fit of Laguerre series to data.", "Return the coefficients of a Laguerre series of degree deg that is the least squares fit to the data values y given at points x. If y is 1-D the returned coefficients will also be 1-D. If y is 2-D multiple fits are done, one for each column of y, and the resulting coefficients are stored in the corresponding columns of a 2-D return. The fitted polynomial(s) are in the form", "where n is deg.", "x-coordinates of the M sample points (x[i], y[i]).", "y-coordinates of the sample points. Several data sets of sample points sharing the same x-coordinates can be fitted at once by passing in a 2D-array that contains one dataset per column.", "Degree(s) of the fitting polynomials. If deg is a single integer all terms up to and including the deg\u2019th term are included in the fit. For NumPy versions >= 1.11.0 a list of integers specifying the degrees of the terms to include may be used instead.", "Relative condition number of the fit. Singular values smaller than this relative to the largest singular value will be ignored. The default value is len(x)*eps, where eps is the relative precision of the float type, about 2e-16 in most cases.", "Switch determining nature of return value. When it is False (the default) just the coefficients are returned, when True diagnostic information from the singular value decomposition is also returned.", "Weights. If not None, the weight w[i] applies to the unsquared residual y[i] - y_hat[i] at x[i]. Ideally the weights are chosen so that the errors of the products w[i]*y[i] all have the same variance. When using inverse-variance weighting, use w[i] = 1/sigma(y[i]). The default value is None.", "Laguerre coefficients ordered from low to high. If y was 2-D, the coefficients for the data in column k of y are in column k.", "These values are only returned if full == True", "For more details, see numpy.linalg.lstsq.", "The rank of the coefficient matrix in the least-squares fit is deficient. The warning is only raised if full == False. The warnings can be turned off by", "See also", "Evaluates a Laguerre series.", "pseudo Vandermonde matrix of Laguerre series.", "Laguerre weight function.", "Computes a least-squares fit from the matrix.", "Computes spline fits.", "The solution is the coefficients of the Laguerre series p that minimizes the sum of the weighted squared errors", "where the \\(w_j\\) are the weights. This problem is solved by setting up as the (typically) overdetermined matrix equation", "where V is the weighted pseudo Vandermonde matrix of x, c are the coefficients to be solved for, w are the weights, and y are the observed values. This equation is then solved using the singular value decomposition of V.", "If some of the singular values of V are so small that they are neglected, then a RankWarning will be issued. This means that the coefficient values may be poorly determined. Using a lower order fit will usually get rid of the warning. The rcond parameter can also be set to a value smaller than its default, but the resulting fit may be spurious and have large contributions from roundoff error.", "Fits using Laguerre series are probably most useful when the data can be approximated by sqrt(w(x)) * p(x), where w(x) is the Laguerre weight. In that case the weight sqrt(w(x[i])) should be used together with data values y[i]/sqrt(w(x[i])). The weight function is available as lagweight.", "Wikipedia, \u201cCurve fitting\u201d, https://en.wikipedia.org/wiki/Curve_fitting"]}, {"name": "polynomial.laguerre.lagfromroots()", "path": "reference/generated/numpy.polynomial.laguerre.lagfromroots", "type": "numpy.polynomial.laguerre.lagfromroots", "text": ["Generate a Laguerre series with given roots.", "The function returns the coefficients of the polynomial", "in Laguerre form, where the r_n are the roots specified in roots. If a zero has multiplicity n, then it must appear in roots n times. For instance, if 2 is a root of multiplicity three and 3 is a root of multiplicity 2, then roots looks something like [2, 2, 2, 3, 3]. The roots can appear in any order.", "If the returned coefficients are c, then", "The coefficient of the last term is not generally 1 for monic polynomials in Laguerre form.", "Sequence containing the roots.", "1-D array of coefficients. If all roots are real then out is a real array, if some of the roots are complex, then out is complex even if all the coefficients in the result are real (see Examples below).", "See also"]}, {"name": "polynomial.laguerre.laggauss()", "path": "reference/generated/numpy.polynomial.laguerre.laggauss", "type": "numpy.polynomial.laguerre.laggauss", "text": ["Gauss-Laguerre quadrature.", "Computes the sample points and weights for Gauss-Laguerre quadrature. These sample points and weights will correctly integrate polynomials of degree \\(2*deg - 1\\) or less over the interval \\([0, \\inf]\\) with the weight function \\(f(x) = \\exp(-x)\\).", "Number of sample points and weights. It must be >= 1.", "1-D ndarray containing the sample points.", "1-D ndarray containing the weights.", "New in version 1.7.0.", "The results have only been tested up to degree 100 higher degrees may be problematic. The weights are determined by using the fact that", "where \\(c\\) is a constant independent of \\(k\\) and \\(x_k\\) is the k\u2019th root of \\(L_n\\), and then scaling the results to get the right value when integrating 1."]}, {"name": "polynomial.laguerre.laggrid2d()", "path": "reference/generated/numpy.polynomial.laguerre.laggrid2d", "type": "numpy.polynomial.laguerre.laggrid2d", "text": ["Evaluate a 2-D Laguerre series on the Cartesian product of x and y.", "This function returns the values:", "where the points (a, b) consist of all pairs formed by taking a from x and b from y. The resulting points form a grid with x in the first dimension and y in the second.", "The parameters x and y are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars. In either case, either x and y or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c has fewer than two dimensions, ones are implicitly appended to its shape to make it 2-D. The shape of the result will be c.shape[2:] + x.shape + y.shape.", "The two dimensional series is evaluated at the points in the Cartesian product of x and y. If x or y is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and, if it isn\u2019t an ndarray, it is treated as a scalar.", "Array of coefficients ordered so that the coefficient of the term of multi-degree i,j is contained in c[i,j]. If c has dimension greater than two the remaining indices enumerate multiple sets of coefficients.", "The values of the two dimensional Chebyshev series at points in the Cartesian product of x and y.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.laguerre.laggrid3d()", "path": "reference/generated/numpy.polynomial.laguerre.laggrid3d", "type": "numpy.polynomial.laguerre.laggrid3d", "text": ["Evaluate a 3-D Laguerre series on the Cartesian product of x, y, and z.", "This function returns the values:", "where the points (a, b, c) consist of all triples formed by taking a from x, b from y, and c from z. The resulting points form a grid with x in the first dimension, y in the second, and z in the third.", "The parameters x, y, and z are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars. In either case, either x, y, and z or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c has fewer than three dimensions, ones are implicitly appended to its shape to make it 3-D. The shape of the result will be c.shape[3:] + x.shape + y.shape + z.shape.", "The three dimensional series is evaluated at the points in the Cartesian product of x, y, and z. If x,`y`, or z is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and, if it isn\u2019t an ndarray, it is treated as a scalar.", "Array of coefficients ordered so that the coefficients for terms of degree i,j are contained in c[i,j]. If c has dimension greater than two the remaining indices enumerate multiple sets of coefficients.", "The values of the two dimensional polynomial at points in the Cartesian product of x and y.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.laguerre.lagint()", "path": "reference/generated/numpy.polynomial.laguerre.lagint", "type": "numpy.polynomial.laguerre.lagint", "text": ["Integrate a Laguerre series.", "Returns the Laguerre series coefficients c integrated m times from lbnd along axis. At each iteration the resulting series is multiplied by scl and an integration constant, k, is added. The scaling factor is for use in a linear change of variable. (\u201cBuyer beware\u201d: note that, depending on what one is doing, one may want scl to be the reciprocal of what one might expect; for more information, see the Notes section below.) The argument c is an array of coefficients from low to high degree along each axis, e.g., [1,2,3] represents the series L_0 + 2*L_1 + 3*L_2 while [[1,2],[1,2]] represents 1*L_0(x)*L_0(y) + 1*L_1(x)*L_0(y) + 2*L_0(x)*L_1(y) +\n2*L_1(x)*L_1(y) if axis=0 is x and axis=1 is y.", "Array of Laguerre series coefficients. If c is multidimensional the different axis correspond to different variables with the degree in each axis given by the corresponding index.", "Order of integration, must be positive. (Default: 1)", "Integration constant(s). The value of the first integral at lbnd is the first value in the list, the value of the second integral at lbnd is the second value, etc. If k == [] (the default), all constants are set to zero. If m == 1, a single scalar can be given instead of a list.", "The lower bound of the integral. (Default: 0)", "Following each integration the result is multiplied by scl before the integration constant is added. (Default: 1)", "Axis over which the integral is taken. (Default: 0).", "New in version 1.7.0.", "Laguerre series coefficients of the integral.", "If m < 0, len(k) > m, np.ndim(lbnd) != 0, or np.ndim(scl) != 0.", "See also", "Note that the result of each integration is multiplied by scl. Why is this important to note? Say one is making a linear change of variable \\(u = ax + b\\) in an integral relative to x. Then \\(dx = du/a\\), so one will need to set scl equal to \\(1/a\\) - perhaps not what one would have first thought.", "Also note that, in general, the result of integrating a C-series needs to be \u201creprojected\u201d onto the C-series basis set. Thus, typically, the result of this function is \u201cunintuitive,\u201d albeit correct; see Examples section below."]}, {"name": "polynomial.laguerre.lagline()", "path": "reference/generated/numpy.polynomial.laguerre.lagline", "type": "numpy.polynomial.laguerre.lagline", "text": ["Laguerre series whose graph is a straight line.", "The specified line is given by off + scl*x.", "This module\u2019s representation of the Laguerre series for off + scl*x.", "See also"]}, {"name": "polynomial.laguerre.lagmul()", "path": "reference/generated/numpy.polynomial.laguerre.lagmul", "type": "numpy.polynomial.laguerre.lagmul", "text": ["Multiply one Laguerre series by another.", "Returns the product of two Laguerre series c1 * c2. The arguments are sequences of coefficients, from lowest order \u201cterm\u201d to highest, e.g., [1,2,3] represents the series P_0 + 2*P_1 + 3*P_2.", "1-D arrays of Laguerre series coefficients ordered from low to high.", "Of Laguerre series coefficients representing their product.", "See also", "In general, the (polynomial) product of two C-series results in terms that are not in the Laguerre polynomial basis set. Thus, to express the product as a Laguerre series, it is necessary to \u201creproject\u201d the product onto said basis set, which may produce \u201cunintuitive\u201d (but correct) results; see Examples section below."]}, {"name": "polynomial.laguerre.lagmulx()", "path": "reference/generated/numpy.polynomial.laguerre.lagmulx", "type": "numpy.polynomial.laguerre.lagmulx", "text": ["Multiply a Laguerre series by x.", "Multiply the Laguerre series c by x, where x is the independent variable.", "1-D array of Laguerre series coefficients ordered from low to high.", "Array representing the result of the multiplication.", "See also", "The multiplication uses the recursion relationship for Laguerre polynomials in the form"]}, {"name": "polynomial.laguerre.lagone", "path": "reference/generated/numpy.polynomial.laguerre.lagone", "type": "numpy.polynomial.laguerre.lagone", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.laguerre.lagpow()", "path": "reference/generated/numpy.polynomial.laguerre.lagpow", "type": "numpy.polynomial.laguerre.lagpow", "text": ["Raise a Laguerre series to a power.", "Returns the Laguerre series c raised to the power pow. The argument c is a sequence of coefficients ordered from low to high. i.e., [1,2,3] is the series P_0 + 2*P_1 + 3*P_2.", "1-D array of Laguerre series coefficients ordered from low to high.", "Power to which the series will be raised", "Maximum power allowed. This is mainly to limit growth of the series to unmanageable size. Default is 16", "Laguerre series of power.", "See also"]}, {"name": "polynomial.laguerre.lagroots()", "path": "reference/generated/numpy.polynomial.laguerre.lagroots", "type": "numpy.polynomial.laguerre.lagroots", "text": ["Compute the roots of a Laguerre series.", "Return the roots (a.k.a. \u201czeros\u201d) of the polynomial", "1-D array of coefficients.", "Array of the roots of the series. If all the roots are real, then out is also real, otherwise it is complex.", "See also", "The root estimates are obtained as the eigenvalues of the companion matrix, Roots far from the origin of the complex plane may have large errors due to the numerical instability of the series for such values. Roots with multiplicity greater than 1 will also show larger errors as the value of the series near such points is relatively insensitive to errors in the roots. Isolated roots near the origin can be improved by a few iterations of Newton\u2019s method.", "The Laguerre series basis polynomials aren\u2019t powers of x so the results of this function may seem unintuitive."]}, {"name": "polynomial.laguerre.lagsub()", "path": "reference/generated/numpy.polynomial.laguerre.lagsub", "type": "numpy.polynomial.laguerre.lagsub", "text": ["Subtract one Laguerre series from another.", "Returns the difference of two Laguerre series c1 - c2. The sequences of coefficients are from lowest order term to highest, i.e., [1,2,3] represents the series P_0 + 2*P_1 + 3*P_2.", "1-D arrays of Laguerre series coefficients ordered from low to high.", "Of Laguerre series coefficients representing their difference.", "See also", "Unlike multiplication, division, etc., the difference of two Laguerre series is a Laguerre series (without having to \u201creproject\u201d the result onto the basis set) so subtraction, just like that of \u201cstandard\u201d polynomials, is simply \u201ccomponent-wise.\u201d"]}, {"name": "polynomial.laguerre.lagtrim()", "path": "reference/generated/numpy.polynomial.laguerre.lagtrim", "type": "numpy.polynomial.laguerre.lagtrim", "text": ["Remove \u201csmall\u201d \u201ctrailing\u201d coefficients from a polynomial.", "\u201cSmall\u201d means \u201csmall in absolute value\u201d and is controlled by the parameter tol; \u201ctrailing\u201d means highest order coefficient(s), e.g., in [0, 1, 1, 0, 0] (which represents 0 + x + x**2 + 0*x**3 + 0*x**4) both the 3-rd and 4-th order coefficients would be \u201ctrimmed.\u201d", "1-d array of coefficients, ordered from lowest order to highest.", "Trailing (i.e., highest order) elements with absolute value less than or equal to tol (default value is zero) are removed.", "1-d array with trailing zeros removed. If the resulting series would be empty, a series containing a single zero is returned.", "If tol < 0", "See also"]}, {"name": "polynomial.laguerre.Laguerre.__call__()", "path": "reference/generated/numpy.polynomial.laguerre.laguerre.__call__", "type": "numpy.polynomial.laguerre.Laguerre", "text": ["method", "Call self as a function."]}, {"name": "polynomial.laguerre.Laguerre.basis()", "path": "reference/generated/numpy.polynomial.laguerre.laguerre.basis", "type": "numpy.polynomial.laguerre.Laguerre", "text": ["method", "Series basis polynomial of degree deg.", "Returns the series representing the basis polynomial of degree deg.", "New in version 1.7.0.", "Degree of the basis polynomial for the series. Must be >= 0.", "If given, the array must be of the form [beg, end], where beg and end are the endpoints of the domain. If None is given then the class domain is used. The default is None.", "If given, the resulting array must be if the form [beg, end], where beg and end are the endpoints of the window. If None is given then the class window is used. The default is None.", "A series with the coefficient of the deg term set to one and all others zero."]}, {"name": "polynomial.laguerre.Laguerre.cast()", "path": "reference/generated/numpy.polynomial.laguerre.laguerre.cast", "type": "numpy.polynomial.laguerre.Laguerre", "text": ["method", "Convert series to series of this class.", "The series is expected to be an instance of some polynomial series of one of the types supported by by the numpy.polynomial module, but could be some other class that supports the convert method.", "New in version 1.7.0.", "The series instance to be converted.", "If given, the array must be of the form [beg, end], where beg and end are the endpoints of the domain. If None is given then the class domain is used. The default is None.", "If given, the resulting array must be if the form [beg, end], where beg and end are the endpoints of the window. If None is given then the class window is used. The default is None.", "A series of the same kind as the calling class and equal to series when evaluated.", "See also", "similar instance method"]}, {"name": "polynomial.laguerre.Laguerre.convert()", "path": "reference/generated/numpy.polynomial.laguerre.laguerre.convert", "type": "numpy.polynomial.laguerre.Laguerre", "text": ["method", "Convert series to a different kind and/or domain and/or window.", "The domain of the converted series. If the value is None, the default domain of kind is used.", "The polynomial series type class to which the current instance should be converted. If kind is None, then the class of the current instance is used.", "The window of the converted series. If the value is None, the default window of kind is used.", "The returned class can be of different type than the current instance and/or have a different domain and/or different window.", "Conversion between domains and class types can result in numerically ill defined series."]}, {"name": "polynomial.laguerre.Laguerre.copy()", "path": "reference/generated/numpy.polynomial.laguerre.laguerre.copy", "type": "numpy.polynomial.laguerre.Laguerre", "text": ["method", "Return a copy.", "Copy of self."]}, {"name": "polynomial.laguerre.Laguerre.cutdeg()", "path": "reference/generated/numpy.polynomial.laguerre.laguerre.cutdeg", "type": "numpy.polynomial.laguerre.Laguerre", "text": ["method", "Truncate series to the given degree.", "Reduce the degree of the series to deg by discarding the high order terms. If deg is greater than the current degree a copy of the current series is returned. This can be useful in least squares where the coefficients of the high degree terms may be very small.", "New in version 1.5.0.", "The series is reduced to degree deg by discarding the high order terms. The value of deg must be a non-negative integer.", "New instance of series with reduced degree."]}, {"name": "polynomial.laguerre.Laguerre.degree()", "path": "reference/generated/numpy.polynomial.laguerre.laguerre.degree", "type": "numpy.polynomial.laguerre.Laguerre", "text": ["method", "The degree of the series.", "New in version 1.5.0.", "Degree of the series, one less than the number of coefficients."]}, {"name": "polynomial.laguerre.Laguerre.deriv()", "path": "reference/generated/numpy.polynomial.laguerre.laguerre.deriv", "type": "numpy.polynomial.laguerre.Laguerre", "text": ["method", "Differentiate.", "Return a series instance of that is the derivative of the current series.", "Find the derivative of order m.", "A new series representing the derivative. The domain is the same as the domain of the differentiated series."]}, {"name": "polynomial.laguerre.Laguerre.domain", "path": "reference/generated/numpy.polynomial.laguerre.laguerre.domain", "type": "Polynomials", "text": ["attribute"]}, {"name": "polynomial.laguerre.Laguerre.fit()", "path": "reference/generated/numpy.polynomial.laguerre.laguerre.fit", "type": "numpy.polynomial.laguerre.Laguerre", "text": ["method", "Least squares fit to data.", "Return a series instance that is the least squares fit to the data y sampled at x. The domain of the returned instance can be specified and this will often result in a superior fit with less chance of ill conditioning.", "x-coordinates of the M sample points (x[i], y[i]).", "y-coordinates of the M sample points (x[i], y[i]).", "Degree(s) of the fitting polynomials. If deg is a single integer all terms up to and including the deg\u2019th term are included in the fit. For NumPy versions >= 1.11.0 a list of integers specifying the degrees of the terms to include may be used instead.", "Domain to use for the returned series. If None, then a minimal domain that covers the points x is chosen. If [] the class domain is used. The default value was the class domain in NumPy 1.4 and None in later versions. The [] option was added in numpy 1.5.0.", "Relative condition number of the fit. Singular values smaller than this relative to the largest singular value will be ignored. The default value is len(x)*eps, where eps is the relative precision of the float type, about 2e-16 in most cases.", "Switch determining nature of return value. When it is False (the default) just the coefficients are returned, when True diagnostic information from the singular value decomposition is also returned.", "Weights. If not None, the weight w[i] applies to the unsquared residual y[i] - y_hat[i] at x[i]. Ideally the weights are chosen so that the errors of the products w[i]*y[i] all have the same variance. When using inverse-variance weighting, use w[i] = 1/sigma(y[i]). The default value is None.", "New in version 1.5.0.", "Window to use for the returned series. The default value is the default class domain", "New in version 1.6.0.", "A series that represents the least squares fit to the data and has the domain and window specified in the call. If the coefficients for the unscaled and unshifted basis polynomials are of interest, do new_series.convert().coef.", "These values are only returned if full == True", "For more details, see linalg.lstsq."]}, {"name": "polynomial.laguerre.Laguerre.fromroots()", "path": "reference/generated/numpy.polynomial.laguerre.laguerre.fromroots", "type": "numpy.polynomial.laguerre.Laguerre", "text": ["method", "Return series instance that has the specified roots.", "Returns a series representing the product (x - r[0])*(x - r[1])*...*(x - r[n-1]), where r is a list of roots.", "List of roots.", "Domain for the resulting series. If None the domain is the interval from the smallest root to the largest. If [] the domain is the class domain. The default is [].", "Window for the returned series. If None the class window is used. The default is None.", "Series with the specified roots."]}, {"name": "polynomial.laguerre.Laguerre.has_samecoef()", "path": "reference/generated/numpy.polynomial.laguerre.laguerre.has_samecoef", "type": "numpy.polynomial.laguerre.Laguerre", "text": ["method", "Check if coefficients match.", "New in version 1.6.0.", "The other class must have the coef attribute.", "True if the coefficients are the same, False otherwise."]}, {"name": "polynomial.laguerre.Laguerre.has_samedomain()", "path": "reference/generated/numpy.polynomial.laguerre.laguerre.has_samedomain", "type": "numpy.polynomial.laguerre.Laguerre", "text": ["method", "Check if domains match.", "New in version 1.6.0.", "The other class must have the domain attribute.", "True if the domains are the same, False otherwise."]}, {"name": "polynomial.laguerre.Laguerre.has_sametype()", "path": "reference/generated/numpy.polynomial.laguerre.laguerre.has_sametype", "type": "numpy.polynomial.laguerre.Laguerre", "text": ["method", "Check if types match.", "New in version 1.7.0.", "Class instance.", "True if other is same class as self"]}, {"name": "polynomial.laguerre.Laguerre.has_samewindow()", "path": "reference/generated/numpy.polynomial.laguerre.laguerre.has_samewindow", "type": "numpy.polynomial.laguerre.Laguerre", "text": ["method", "Check if windows match.", "New in version 1.6.0.", "The other class must have the window attribute.", "True if the windows are the same, False otherwise."]}, {"name": "polynomial.laguerre.Laguerre.identity()", "path": "reference/generated/numpy.polynomial.laguerre.laguerre.identity", "type": "numpy.polynomial.laguerre.Laguerre", "text": ["method", "Identity function.", "If p is the returned series, then p(x) == x for all values of x.", "If given, the array must be of the form [beg, end], where beg and end are the endpoints of the domain. If None is given then the class domain is used. The default is None.", "If given, the resulting array must be if the form [beg, end], where beg and end are the endpoints of the window. If None is given then the class window is used. The default is None.", "Series of representing the identity."]}, {"name": "polynomial.laguerre.Laguerre.integ()", "path": "reference/generated/numpy.polynomial.laguerre.laguerre.integ", "type": "numpy.polynomial.laguerre.Laguerre", "text": ["method", "Integrate.", "Return a series instance that is the definite integral of the current series.", "The number of integrations to perform.", "Integration constants. The first constant is applied to the first integration, the second to the second, and so on. The list of values must less than or equal to m in length and any missing values are set to zero.", "The lower bound of the definite integral.", "A new series representing the integral. The domain is the same as the domain of the integrated series."]}, {"name": "polynomial.laguerre.Laguerre.linspace()", "path": "reference/generated/numpy.polynomial.laguerre.laguerre.linspace", "type": "numpy.polynomial.laguerre.Laguerre", "text": ["method", "Return x, y values at equally spaced points in domain.", "Returns the x, y values at n linearly spaced points across the domain. Here y is the value of the polynomial at the points x. By default the domain is the same as that of the series instance. This method is intended mostly as a plotting aid.", "New in version 1.5.0.", "Number of point pairs to return. The default value is 100.", "If not None, the specified domain is used instead of that of the calling instance. It should be of the form [beg,end]. The default is None which case the class domain is used.", "x is equal to linspace(self.domain[0], self.domain[1], n) and y is the series evaluated at element of x."]}, {"name": "polynomial.laguerre.Laguerre.mapparms()", "path": "reference/generated/numpy.polynomial.laguerre.laguerre.mapparms", "type": "numpy.polynomial.laguerre.Laguerre", "text": ["method", "Return the mapping parameters.", "The returned values define a linear map off + scl*x that is applied to the input arguments before the series is evaluated. The map depends on the domain and window; if the current domain is equal to the window the resulting map is the identity. If the coefficients of the series instance are to be used by themselves outside this class, then the linear function must be substituted for the x in the standard representation of the base polynomials.", "The mapping function is defined by off + scl*x.", "If the current domain is the interval [l1, r1] and the window is [l2, r2], then the linear mapping function L is defined by the equations:"]}, {"name": "polynomial.laguerre.Laguerre.roots()", "path": "reference/generated/numpy.polynomial.laguerre.laguerre.roots", "type": "numpy.polynomial.laguerre.Laguerre", "text": ["method", "Return the roots of the series polynomial.", "Compute the roots for the series. Note that the accuracy of the roots decrease the further outside the domain they lie.", "Array containing the roots of the series."]}, {"name": "polynomial.laguerre.Laguerre.trim()", "path": "reference/generated/numpy.polynomial.laguerre.laguerre.trim", "type": "numpy.polynomial.laguerre.Laguerre", "text": ["method", "Remove trailing coefficients", "Remove trailing coefficients until a coefficient is reached whose absolute value greater than tol or the beginning of the series is reached. If all the coefficients would be removed the series is set to [0]. A new series instance is returned with the new coefficients. The current instance remains unchanged.", "All trailing coefficients less than tol will be removed.", "New instance of series with trimmed coefficients."]}, {"name": "polynomial.laguerre.Laguerre.truncate()", "path": "reference/generated/numpy.polynomial.laguerre.laguerre.truncate", "type": "numpy.polynomial.laguerre.Laguerre", "text": ["method", "Truncate series to length size.", "Reduce the series to length size by discarding the high degree terms. The value of size must be a positive integer. This can be useful in least squares where the coefficients of the high degree terms may be very small.", "The series is reduced to length size by discarding the high degree terms. The value of size must be a positive integer.", "New instance of series with truncated coefficients."]}, {"name": "polynomial.laguerre.lagval()", "path": "reference/generated/numpy.polynomial.laguerre.lagval", "type": "numpy.polynomial.laguerre.lagval", "text": ["Evaluate a Laguerre series at points x.", "If c is of length n + 1, this function returns the value:", "The parameter x is converted to an array only if it is a tuple or a list, otherwise it is treated as a scalar. In either case, either x or its elements must support multiplication and addition both with themselves and with the elements of c.", "If c is a 1-D array, then p(x) will have the same shape as x. If c is multidimensional, then the shape of the result depends on the value of tensor. If tensor is true the shape will be c.shape[1:] + x.shape. If tensor is false the shape will be c.shape[1:]. Note that scalars have shape (,).", "Trailing zeros in the coefficients will be used in the evaluation, so they should be avoided if efficiency is a concern.", "If x is a list or tuple, it is converted to an ndarray, otherwise it is left unchanged and treated as a scalar. In either case, x or its elements must support addition and multiplication with with themselves and with the elements of c.", "Array of coefficients ordered so that the coefficients for terms of degree n are contained in c[n]. If c is multidimensional the remaining indices enumerate multiple polynomials. In the two dimensional case the coefficients may be thought of as stored in the columns of c.", "If True, the shape of the coefficient array is extended with ones on the right, one for each dimension of x. Scalars have dimension 0 for this action. The result is that every column of coefficients in c is evaluated for every element of x. If False, x is broadcast over the columns of c for the evaluation. This keyword is useful when c is multidimensional. The default value is True.", "New in version 1.7.0.", "The shape of the return value is described above.", "See also", "The evaluation uses Clenshaw recursion, aka synthetic division."]}, {"name": "polynomial.laguerre.lagval2d()", "path": "reference/generated/numpy.polynomial.laguerre.lagval2d", "type": "numpy.polynomial.laguerre.lagval2d", "text": ["Evaluate a 2-D Laguerre series at points (x, y).", "This function returns the values:", "The parameters x and y are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars and they must have the same shape after conversion. In either case, either x and y or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c is a 1-D array a one is implicitly appended to its shape to make it 2-D. The shape of the result will be c.shape[2:] + x.shape.", "The two dimensional series is evaluated at the points (x, y), where x and y must have the same shape. If x or y is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and if it isn\u2019t an ndarray it is treated as a scalar.", "Array of coefficients ordered so that the coefficient of the term of multi-degree i,j is contained in c[i,j]. If c has dimension greater than two the remaining indices enumerate multiple sets of coefficients.", "The values of the two dimensional polynomial at points formed with pairs of corresponding values from x and y.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.laguerre.lagval3d()", "path": "reference/generated/numpy.polynomial.laguerre.lagval3d", "type": "numpy.polynomial.laguerre.lagval3d", "text": ["Evaluate a 3-D Laguerre series at points (x, y, z).", "This function returns the values:", "The parameters x, y, and z are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars and they must have the same shape after conversion. In either case, either x, y, and z or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c has fewer than 3 dimensions, ones are implicitly appended to its shape to make it 3-D. The shape of the result will be c.shape[3:] + x.shape.", "The three dimensional series is evaluated at the points (x, y, z), where x, y, and z must have the same shape. If any of x, y, or z is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and if it isn\u2019t an ndarray it is treated as a scalar.", "Array of coefficients ordered so that the coefficient of the term of multi-degree i,j,k is contained in c[i,j,k]. If c has dimension greater than 3 the remaining indices enumerate multiple sets of coefficients.", "The values of the multidimensional polynomial on points formed with triples of corresponding values from x, y, and z.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.laguerre.lagvander()", "path": "reference/generated/numpy.polynomial.laguerre.lagvander", "type": "numpy.polynomial.laguerre.lagvander", "text": ["Pseudo-Vandermonde matrix of given degree.", "Returns the pseudo-Vandermonde matrix of degree deg and sample points x. The pseudo-Vandermonde matrix is defined by", "where 0 <= i <= deg. The leading indices of V index the elements of x and the last index is the degree of the Laguerre polynomial.", "If c is a 1-D array of coefficients of length n + 1 and V is the array V = lagvander(x, n), then np.dot(V, c) and lagval(x, c) are the same up to roundoff. This equivalence is useful both for least squares fitting and for the evaluation of a large number of Laguerre series of the same degree and sample points.", "Array of points. The dtype is converted to float64 or complex128 depending on whether any of the elements are complex. If x is scalar it is converted to a 1-D array.", "Degree of the resulting matrix.", "The pseudo-Vandermonde matrix. The shape of the returned matrix is x.shape + (deg + 1,), where The last index is the degree of the corresponding Laguerre polynomial. The dtype will be the same as the converted x."]}, {"name": "polynomial.laguerre.lagvander2d()", "path": "reference/generated/numpy.polynomial.laguerre.lagvander2d", "type": "numpy.polynomial.laguerre.lagvander2d", "text": ["Pseudo-Vandermonde matrix of given degrees.", "Returns the pseudo-Vandermonde matrix of degrees deg and sample points (x, y). The pseudo-Vandermonde matrix is defined by", "where 0 <= i <= deg[0] and 0 <= j <= deg[1]. The leading indices of V index the points (x, y) and the last index encodes the degrees of the Laguerre polynomials.", "If V = lagvander2d(x, y, [xdeg, ydeg]), then the columns of V correspond to the elements of a 2-D coefficient array c of shape (xdeg + 1, ydeg + 1) in the order", "and np.dot(V, c.flat) and lagval2d(x, y, c) will be the same up to roundoff. This equivalence is useful both for least squares fitting and for the evaluation of a large number of 2-D Laguerre series of the same degrees and sample points.", "Arrays of point coordinates, all of the same shape. The dtypes will be converted to either float64 or complex128 depending on whether any of the elements are complex. Scalars are converted to 1-D arrays.", "List of maximum degrees of the form [x_deg, y_deg].", "The shape of the returned matrix is x.shape + (order,), where \\(order = (deg[0]+1)*(deg[1]+1)\\). The dtype will be the same as the converted x and y.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.laguerre.lagvander3d()", "path": "reference/generated/numpy.polynomial.laguerre.lagvander3d", "type": "numpy.polynomial.laguerre.lagvander3d", "text": ["Pseudo-Vandermonde matrix of given degrees.", "Returns the pseudo-Vandermonde matrix of degrees deg and sample points (x, y, z). If l, m, n are the given degrees in x, y, z, then The pseudo-Vandermonde matrix is defined by", "where 0 <= i <= l, 0 <= j <= m, and 0 <= j <= n. The leading indices of V index the points (x, y, z) and the last index encodes the degrees of the Laguerre polynomials.", "If V = lagvander3d(x, y, z, [xdeg, ydeg, zdeg]), then the columns of V correspond to the elements of a 3-D coefficient array c of shape (xdeg + 1, ydeg + 1, zdeg + 1) in the order", "and np.dot(V, c.flat) and lagval3d(x, y, z, c) will be the same up to roundoff. This equivalence is useful both for least squares fitting and for the evaluation of a large number of 3-D Laguerre series of the same degrees and sample points.", "Arrays of point coordinates, all of the same shape. The dtypes will be converted to either float64 or complex128 depending on whether any of the elements are complex. Scalars are converted to 1-D arrays.", "List of maximum degrees of the form [x_deg, y_deg, z_deg].", "The shape of the returned matrix is x.shape + (order,), where \\(order = (deg[0]+1)*(deg[1]+1)*(deg[2]+1)\\). The dtype will be the same as the converted x, y, and z.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.laguerre.lagweight()", "path": "reference/generated/numpy.polynomial.laguerre.lagweight", "type": "numpy.polynomial.laguerre.lagweight", "text": ["Weight function of the Laguerre polynomials.", "The weight function is \\(exp(-x)\\) and the interval of integration is \\([0, \\inf]\\). The Laguerre polynomials are orthogonal, but not normalized, with respect to this weight function.", "Values at which the weight function will be computed.", "The weight function at x.", "New in version 1.7.0."]}, {"name": "polynomial.laguerre.lagx", "path": "reference/generated/numpy.polynomial.laguerre.lagx", "type": "numpy.polynomial.laguerre.lagx", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.laguerre.lagzero", "path": "reference/generated/numpy.polynomial.laguerre.lagzero", "type": "numpy.polynomial.laguerre.lagzero", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.laguerre.poly2lag()", "path": "reference/generated/numpy.polynomial.laguerre.poly2lag", "type": "numpy.polynomial.laguerre.poly2lag", "text": ["Convert a polynomial to a Laguerre series.", "Convert an array representing the coefficients of a polynomial (relative to the \u201cstandard\u201d basis) ordered from lowest degree to highest, to an array of the coefficients of the equivalent Laguerre series, ordered from lowest to highest degree.", "1-D array containing the polynomial coefficients", "1-D array containing the coefficients of the equivalent Laguerre series.", "See also", "The easy way to do conversions between polynomial basis sets is to use the convert method of a class instance."]}, {"name": "polynomial.legendre.leg2poly()", "path": "reference/generated/numpy.polynomial.legendre.leg2poly", "type": "numpy.polynomial.legendre.leg2poly", "text": ["Convert a Legendre series to a polynomial.", "Convert an array representing the coefficients of a Legendre series, ordered from lowest degree to highest, to an array of the coefficients of the equivalent polynomial (relative to the \u201cstandard\u201d basis) ordered from lowest to highest degree.", "1-D array containing the Legendre series coefficients, ordered from lowest order term to highest.", "1-D array containing the coefficients of the equivalent polynomial (relative to the \u201cstandard\u201d basis) ordered from lowest order term to highest.", "See also", "The easy way to do conversions between polynomial basis sets is to use the convert method of a class instance."]}, {"name": "polynomial.legendre.legadd()", "path": "reference/generated/numpy.polynomial.legendre.legadd", "type": "numpy.polynomial.legendre.legadd", "text": ["Add one Legendre series to another.", "Returns the sum of two Legendre series c1 + c2. The arguments are sequences of coefficients ordered from lowest order term to highest, i.e., [1,2,3] represents the series P_0 + 2*P_1 + 3*P_2.", "1-D arrays of Legendre series coefficients ordered from low to high.", "Array representing the Legendre series of their sum.", "See also", "Unlike multiplication, division, etc., the sum of two Legendre series is a Legendre series (without having to \u201creproject\u201d the result onto the basis set) so addition, just like that of \u201cstandard\u201d polynomials, is simply \u201ccomponent-wise.\u201d"]}, {"name": "polynomial.legendre.legcompanion()", "path": "reference/generated/numpy.polynomial.legendre.legcompanion", "type": "numpy.polynomial.legendre.legcompanion", "text": ["Return the scaled companion matrix of c.", "The basis polynomials are scaled so that the companion matrix is symmetric when c is an Legendre basis polynomial. This provides better eigenvalue estimates than the unscaled case and for basis polynomials the eigenvalues are guaranteed to be real if numpy.linalg.eigvalsh is used to obtain them.", "1-D array of Legendre series coefficients ordered from low to high degree.", "Scaled companion matrix of dimensions (deg, deg).", "New in version 1.7.0."]}, {"name": "polynomial.legendre.legder()", "path": "reference/generated/numpy.polynomial.legendre.legder", "type": "numpy.polynomial.legendre.legder", "text": ["Differentiate a Legendre series.", "Returns the Legendre series coefficients c differentiated m times along axis. At each iteration the result is multiplied by scl (the scaling factor is for use in a linear change of variable). The argument c is an array of coefficients from low to high degree along each axis, e.g., [1,2,3] represents the series 1*L_0 + 2*L_1 + 3*L_2 while [[1,2],[1,2]] represents 1*L_0(x)*L_0(y) + 1*L_1(x)*L_0(y) +\n2*L_0(x)*L_1(y) + 2*L_1(x)*L_1(y) if axis=0 is x and axis=1 is y.", "Array of Legendre series coefficients. If c is multidimensional the different axis correspond to different variables with the degree in each axis given by the corresponding index.", "Number of derivatives taken, must be non-negative. (Default: 1)", "Each differentiation is multiplied by scl. The end result is multiplication by scl**m. This is for use in a linear change of variable. (Default: 1)", "Axis over which the derivative is taken. (Default: 0).", "New in version 1.7.0.", "Legendre series of the derivative.", "See also", "In general, the result of differentiating a Legendre series does not resemble the same operation on a power series. Thus the result of this function may be \u201cunintuitive,\u201d albeit correct; see Examples section below."]}, {"name": "polynomial.legendre.legdiv()", "path": "reference/generated/numpy.polynomial.legendre.legdiv", "type": "numpy.polynomial.legendre.legdiv", "text": ["Divide one Legendre series by another.", "Returns the quotient-with-remainder of two Legendre series c1 / c2. The arguments are sequences of coefficients from lowest order \u201cterm\u201d to highest, e.g., [1,2,3] represents the series P_0 + 2*P_1 + 3*P_2.", "1-D arrays of Legendre series coefficients ordered from low to high.", "Of Legendre series coefficients representing the quotient and remainder.", "See also", "In general, the (polynomial) division of one Legendre series by another results in quotient and remainder terms that are not in the Legendre polynomial basis set. Thus, to express these results as a Legendre series, it is necessary to \u201creproject\u201d the results onto the Legendre basis set, which may produce \u201cunintuitive\u201d (but correct) results; see Examples section below."]}, {"name": "polynomial.legendre.legdomain", "path": "reference/generated/numpy.polynomial.legendre.legdomain", "type": "numpy.polynomial.legendre.legdomain", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.legendre.Legendre.__call__()", "path": "reference/generated/numpy.polynomial.legendre.legendre.__call__", "type": "numpy.polynomial.legendre.Legendre", "text": ["method", "Call self as a function."]}, {"name": "polynomial.legendre.Legendre.basis()", "path": "reference/generated/numpy.polynomial.legendre.legendre.basis", "type": "numpy.polynomial.legendre.Legendre", "text": ["method", "Series basis polynomial of degree deg.", "Returns the series representing the basis polynomial of degree deg.", "New in version 1.7.0.", "Degree of the basis polynomial for the series. Must be >= 0.", "If given, the array must be of the form [beg, end], where beg and end are the endpoints of the domain. If None is given then the class domain is used. The default is None.", "If given, the resulting array must be if the form [beg, end], where beg and end are the endpoints of the window. If None is given then the class window is used. The default is None.", "A series with the coefficient of the deg term set to one and all others zero."]}, {"name": "polynomial.legendre.Legendre.cast()", "path": "reference/generated/numpy.polynomial.legendre.legendre.cast", "type": "numpy.polynomial.legendre.Legendre", "text": ["method", "Convert series to series of this class.", "The series is expected to be an instance of some polynomial series of one of the types supported by by the numpy.polynomial module, but could be some other class that supports the convert method.", "New in version 1.7.0.", "The series instance to be converted.", "If given, the array must be of the form [beg, end], where beg and end are the endpoints of the domain. If None is given then the class domain is used. The default is None.", "If given, the resulting array must be if the form [beg, end], where beg and end are the endpoints of the window. If None is given then the class window is used. The default is None.", "A series of the same kind as the calling class and equal to series when evaluated.", "See also", "similar instance method"]}, {"name": "polynomial.legendre.Legendre.convert()", "path": "reference/generated/numpy.polynomial.legendre.legendre.convert", "type": "numpy.polynomial.legendre.Legendre", "text": ["method", "Convert series to a different kind and/or domain and/or window.", "The domain of the converted series. If the value is None, the default domain of kind is used.", "The polynomial series type class to which the current instance should be converted. If kind is None, then the class of the current instance is used.", "The window of the converted series. If the value is None, the default window of kind is used.", "The returned class can be of different type than the current instance and/or have a different domain and/or different window.", "Conversion between domains and class types can result in numerically ill defined series."]}, {"name": "polynomial.legendre.Legendre.copy()", "path": "reference/generated/numpy.polynomial.legendre.legendre.copy", "type": "numpy.polynomial.legendre.Legendre", "text": ["method", "Return a copy.", "Copy of self."]}, {"name": "polynomial.legendre.Legendre.cutdeg()", "path": "reference/generated/numpy.polynomial.legendre.legendre.cutdeg", "type": "numpy.polynomial.legendre.Legendre", "text": ["method", "Truncate series to the given degree.", "Reduce the degree of the series to deg by discarding the high order terms. If deg is greater than the current degree a copy of the current series is returned. This can be useful in least squares where the coefficients of the high degree terms may be very small.", "New in version 1.5.0.", "The series is reduced to degree deg by discarding the high order terms. The value of deg must be a non-negative integer.", "New instance of series with reduced degree."]}, {"name": "polynomial.legendre.Legendre.degree()", "path": "reference/generated/numpy.polynomial.legendre.legendre.degree", "type": "numpy.polynomial.legendre.Legendre", "text": ["method", "The degree of the series.", "New in version 1.5.0.", "Degree of the series, one less than the number of coefficients."]}, {"name": "polynomial.legendre.Legendre.deriv()", "path": "reference/generated/numpy.polynomial.legendre.legendre.deriv", "type": "numpy.polynomial.legendre.Legendre", "text": ["method", "Differentiate.", "Return a series instance of that is the derivative of the current series.", "Find the derivative of order m.", "A new series representing the derivative. The domain is the same as the domain of the differentiated series."]}, {"name": "polynomial.legendre.Legendre.domain", "path": "reference/generated/numpy.polynomial.legendre.legendre.domain", "type": "Polynomials", "text": ["attribute"]}, {"name": "polynomial.legendre.Legendre.fit()", "path": "reference/generated/numpy.polynomial.legendre.legendre.fit", "type": "numpy.polynomial.legendre.Legendre", "text": ["method", "Least squares fit to data.", "Return a series instance that is the least squares fit to the data y sampled at x. The domain of the returned instance can be specified and this will often result in a superior fit with less chance of ill conditioning.", "x-coordinates of the M sample points (x[i], y[i]).", "y-coordinates of the M sample points (x[i], y[i]).", "Degree(s) of the fitting polynomials. If deg is a single integer all terms up to and including the deg\u2019th term are included in the fit. For NumPy versions >= 1.11.0 a list of integers specifying the degrees of the terms to include may be used instead.", "Domain to use for the returned series. If None, then a minimal domain that covers the points x is chosen. If [] the class domain is used. The default value was the class domain in NumPy 1.4 and None in later versions. The [] option was added in numpy 1.5.0.", "Relative condition number of the fit. Singular values smaller than this relative to the largest singular value will be ignored. The default value is len(x)*eps, where eps is the relative precision of the float type, about 2e-16 in most cases.", "Switch determining nature of return value. When it is False (the default) just the coefficients are returned, when True diagnostic information from the singular value decomposition is also returned.", "Weights. If not None, the weight w[i] applies to the unsquared residual y[i] - y_hat[i] at x[i]. Ideally the weights are chosen so that the errors of the products w[i]*y[i] all have the same variance. When using inverse-variance weighting, use w[i] = 1/sigma(y[i]). The default value is None.", "New in version 1.5.0.", "Window to use for the returned series. The default value is the default class domain", "New in version 1.6.0.", "A series that represents the least squares fit to the data and has the domain and window specified in the call. If the coefficients for the unscaled and unshifted basis polynomials are of interest, do new_series.convert().coef.", "These values are only returned if full == True", "For more details, see linalg.lstsq."]}, {"name": "polynomial.legendre.Legendre.fromroots()", "path": "reference/generated/numpy.polynomial.legendre.legendre.fromroots", "type": "numpy.polynomial.legendre.Legendre", "text": ["method", "Return series instance that has the specified roots.", "Returns a series representing the product (x - r[0])*(x - r[1])*...*(x - r[n-1]), where r is a list of roots.", "List of roots.", "Domain for the resulting series. If None the domain is the interval from the smallest root to the largest. If [] the domain is the class domain. The default is [].", "Window for the returned series. If None the class window is used. The default is None.", "Series with the specified roots."]}, {"name": "polynomial.legendre.Legendre.has_samecoef()", "path": "reference/generated/numpy.polynomial.legendre.legendre.has_samecoef", "type": "numpy.polynomial.legendre.Legendre", "text": ["method", "Check if coefficients match.", "New in version 1.6.0.", "The other class must have the coef attribute.", "True if the coefficients are the same, False otherwise."]}, {"name": "polynomial.legendre.Legendre.has_samedomain()", "path": "reference/generated/numpy.polynomial.legendre.legendre.has_samedomain", "type": "numpy.polynomial.legendre.Legendre", "text": ["method", "Check if domains match.", "New in version 1.6.0.", "The other class must have the domain attribute.", "True if the domains are the same, False otherwise."]}, {"name": "polynomial.legendre.Legendre.has_sametype()", "path": "reference/generated/numpy.polynomial.legendre.legendre.has_sametype", "type": "numpy.polynomial.legendre.Legendre", "text": ["method", "Check if types match.", "New in version 1.7.0.", "Class instance.", "True if other is same class as self"]}, {"name": "polynomial.legendre.Legendre.has_samewindow()", "path": "reference/generated/numpy.polynomial.legendre.legendre.has_samewindow", "type": "numpy.polynomial.legendre.Legendre", "text": ["method", "Check if windows match.", "New in version 1.6.0.", "The other class must have the window attribute.", "True if the windows are the same, False otherwise."]}, {"name": "polynomial.legendre.Legendre.identity()", "path": "reference/generated/numpy.polynomial.legendre.legendre.identity", "type": "numpy.polynomial.legendre.Legendre", "text": ["method", "Identity function.", "If p is the returned series, then p(x) == x for all values of x.", "If given, the array must be of the form [beg, end], where beg and end are the endpoints of the domain. If None is given then the class domain is used. The default is None.", "If given, the resulting array must be if the form [beg, end], where beg and end are the endpoints of the window. If None is given then the class window is used. The default is None.", "Series of representing the identity."]}, {"name": "polynomial.legendre.Legendre.integ()", "path": "reference/generated/numpy.polynomial.legendre.legendre.integ", "type": "numpy.polynomial.legendre.Legendre", "text": ["method", "Integrate.", "Return a series instance that is the definite integral of the current series.", "The number of integrations to perform.", "Integration constants. The first constant is applied to the first integration, the second to the second, and so on. The list of values must less than or equal to m in length and any missing values are set to zero.", "The lower bound of the definite integral.", "A new series representing the integral. The domain is the same as the domain of the integrated series."]}, {"name": "polynomial.legendre.Legendre.linspace()", "path": "reference/generated/numpy.polynomial.legendre.legendre.linspace", "type": "numpy.polynomial.legendre.Legendre", "text": ["method", "Return x, y values at equally spaced points in domain.", "Returns the x, y values at n linearly spaced points across the domain. Here y is the value of the polynomial at the points x. By default the domain is the same as that of the series instance. This method is intended mostly as a plotting aid.", "New in version 1.5.0.", "Number of point pairs to return. The default value is 100.", "If not None, the specified domain is used instead of that of the calling instance. It should be of the form [beg,end]. The default is None which case the class domain is used.", "x is equal to linspace(self.domain[0], self.domain[1], n) and y is the series evaluated at element of x."]}, {"name": "polynomial.legendre.Legendre.mapparms()", "path": "reference/generated/numpy.polynomial.legendre.legendre.mapparms", "type": "numpy.polynomial.legendre.Legendre", "text": ["method", "Return the mapping parameters.", "The returned values define a linear map off + scl*x that is applied to the input arguments before the series is evaluated. The map depends on the domain and window; if the current domain is equal to the window the resulting map is the identity. If the coefficients of the series instance are to be used by themselves outside this class, then the linear function must be substituted for the x in the standard representation of the base polynomials.", "The mapping function is defined by off + scl*x.", "If the current domain is the interval [l1, r1] and the window is [l2, r2], then the linear mapping function L is defined by the equations:"]}, {"name": "polynomial.legendre.Legendre.roots()", "path": "reference/generated/numpy.polynomial.legendre.legendre.roots", "type": "numpy.polynomial.legendre.Legendre", "text": ["method", "Return the roots of the series polynomial.", "Compute the roots for the series. Note that the accuracy of the roots decrease the further outside the domain they lie.", "Array containing the roots of the series."]}, {"name": "polynomial.legendre.Legendre.trim()", "path": "reference/generated/numpy.polynomial.legendre.legendre.trim", "type": "numpy.polynomial.legendre.Legendre", "text": ["method", "Remove trailing coefficients", "Remove trailing coefficients until a coefficient is reached whose absolute value greater than tol or the beginning of the series is reached. If all the coefficients would be removed the series is set to [0]. A new series instance is returned with the new coefficients. The current instance remains unchanged.", "All trailing coefficients less than tol will be removed.", "New instance of series with trimmed coefficients."]}, {"name": "polynomial.legendre.Legendre.truncate()", "path": "reference/generated/numpy.polynomial.legendre.legendre.truncate", "type": "numpy.polynomial.legendre.Legendre", "text": ["method", "Truncate series to length size.", "Reduce the series to length size by discarding the high degree terms. The value of size must be a positive integer. This can be useful in least squares where the coefficients of the high degree terms may be very small.", "The series is reduced to length size by discarding the high degree terms. The value of size must be a positive integer.", "New instance of series with truncated coefficients."]}, {"name": "polynomial.legendre.legfit()", "path": "reference/generated/numpy.polynomial.legendre.legfit", "type": "numpy.polynomial.legendre.legfit", "text": ["Least squares fit of Legendre series to data.", "Return the coefficients of a Legendre series of degree deg that is the least squares fit to the data values y given at points x. If y is 1-D the returned coefficients will also be 1-D. If y is 2-D multiple fits are done, one for each column of y, and the resulting coefficients are stored in the corresponding columns of a 2-D return. The fitted polynomial(s) are in the form", "where n is deg.", "x-coordinates of the M sample points (x[i], y[i]).", "y-coordinates of the sample points. Several data sets of sample points sharing the same x-coordinates can be fitted at once by passing in a 2D-array that contains one dataset per column.", "Degree(s) of the fitting polynomials. If deg is a single integer all terms up to and including the deg\u2019th term are included in the fit. For NumPy versions >= 1.11.0 a list of integers specifying the degrees of the terms to include may be used instead.", "Relative condition number of the fit. Singular values smaller than this relative to the largest singular value will be ignored. The default value is len(x)*eps, where eps is the relative precision of the float type, about 2e-16 in most cases.", "Switch determining nature of return value. When it is False (the default) just the coefficients are returned, when True diagnostic information from the singular value decomposition is also returned.", "Weights. If not None, the weight w[i] applies to the unsquared residual y[i] - y_hat[i] at x[i]. Ideally the weights are chosen so that the errors of the products w[i]*y[i] all have the same variance. When using inverse-variance weighting, use w[i] = 1/sigma(y[i]). The default value is None.", "New in version 1.5.0.", "Legendre coefficients ordered from low to high. If y was 2-D, the coefficients for the data in column k of y are in column k. If deg is specified as a list, coefficients for terms not included in the fit are set equal to zero in the returned coef.", "These values are only returned if full == True", "For more details, see numpy.linalg.lstsq.", "The rank of the coefficient matrix in the least-squares fit is deficient. The warning is only raised if full == False. The warnings can be turned off by", "See also", "Evaluates a Legendre series.", "Vandermonde matrix of Legendre series.", "Legendre weight function (= 1).", "Computes a least-squares fit from the matrix.", "Computes spline fits.", "The solution is the coefficients of the Legendre series p that minimizes the sum of the weighted squared errors", "where \\(w_j\\) are the weights. This problem is solved by setting up as the (typically) overdetermined matrix equation", "where V is the weighted pseudo Vandermonde matrix of x, c are the coefficients to be solved for, w are the weights, and y are the observed values. This equation is then solved using the singular value decomposition of V.", "If some of the singular values of V are so small that they are neglected, then a RankWarning will be issued. This means that the coefficient values may be poorly determined. Using a lower order fit will usually get rid of the warning. The rcond parameter can also be set to a value smaller than its default, but the resulting fit may be spurious and have large contributions from roundoff error.", "Fits using Legendre series are usually better conditioned than fits using power series, but much can depend on the distribution of the sample points and the smoothness of the data. If the quality of the fit is inadequate splines may be a good alternative.", "Wikipedia, \u201cCurve fitting\u201d, https://en.wikipedia.org/wiki/Curve_fitting"]}, {"name": "polynomial.legendre.legfromroots()", "path": "reference/generated/numpy.polynomial.legendre.legfromroots", "type": "numpy.polynomial.legendre.legfromroots", "text": ["Generate a Legendre series with given roots.", "The function returns the coefficients of the polynomial", "in Legendre form, where the r_n are the roots specified in roots. If a zero has multiplicity n, then it must appear in roots n times. For instance, if 2 is a root of multiplicity three and 3 is a root of multiplicity 2, then roots looks something like [2, 2, 2, 3, 3]. The roots can appear in any order.", "If the returned coefficients are c, then", "The coefficient of the last term is not generally 1 for monic polynomials in Legendre form.", "Sequence containing the roots.", "1-D array of coefficients. If all roots are real then out is a real array, if some of the roots are complex, then out is complex even if all the coefficients in the result are real (see Examples below).", "See also"]}, {"name": "polynomial.legendre.leggauss()", "path": "reference/generated/numpy.polynomial.legendre.leggauss", "type": "numpy.polynomial.legendre.leggauss", "text": ["Gauss-Legendre quadrature.", "Computes the sample points and weights for Gauss-Legendre quadrature. These sample points and weights will correctly integrate polynomials of degree \\(2*deg - 1\\) or less over the interval \\([-1, 1]\\) with the weight function \\(f(x) = 1\\).", "Number of sample points and weights. It must be >= 1.", "1-D ndarray containing the sample points.", "1-D ndarray containing the weights.", "New in version 1.7.0.", "The results have only been tested up to degree 100, higher degrees may be problematic. The weights are determined by using the fact that", "where \\(c\\) is a constant independent of \\(k\\) and \\(x_k\\) is the k\u2019th root of \\(L_n\\), and then scaling the results to get the right value when integrating 1."]}, {"name": "polynomial.legendre.leggrid2d()", "path": "reference/generated/numpy.polynomial.legendre.leggrid2d", "type": "numpy.polynomial.legendre.leggrid2d", "text": ["Evaluate a 2-D Legendre series on the Cartesian product of x and y.", "This function returns the values:", "where the points (a, b) consist of all pairs formed by taking a from x and b from y. The resulting points form a grid with x in the first dimension and y in the second.", "The parameters x and y are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars. In either case, either x and y or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c has fewer than two dimensions, ones are implicitly appended to its shape to make it 2-D. The shape of the result will be c.shape[2:] + x.shape + y.shape.", "The two dimensional series is evaluated at the points in the Cartesian product of x and y. If x or y is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and, if it isn\u2019t an ndarray, it is treated as a scalar.", "Array of coefficients ordered so that the coefficient of the term of multi-degree i,j is contained in c[i,j]. If c has dimension greater than two the remaining indices enumerate multiple sets of coefficients.", "The values of the two dimensional Chebyshev series at points in the Cartesian product of x and y.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.legendre.leggrid3d()", "path": "reference/generated/numpy.polynomial.legendre.leggrid3d", "type": "numpy.polynomial.legendre.leggrid3d", "text": ["Evaluate a 3-D Legendre series on the Cartesian product of x, y, and z.", "This function returns the values:", "where the points (a, b, c) consist of all triples formed by taking a from x, b from y, and c from z. The resulting points form a grid with x in the first dimension, y in the second, and z in the third.", "The parameters x, y, and z are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars. In either case, either x, y, and z or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c has fewer than three dimensions, ones are implicitly appended to its shape to make it 3-D. The shape of the result will be c.shape[3:] + x.shape + y.shape + z.shape.", "The three dimensional series is evaluated at the points in the Cartesian product of x, y, and z. If x,`y`, or z is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and, if it isn\u2019t an ndarray, it is treated as a scalar.", "Array of coefficients ordered so that the coefficients for terms of degree i,j are contained in c[i,j]. If c has dimension greater than two the remaining indices enumerate multiple sets of coefficients.", "The values of the two dimensional polynomial at points in the Cartesian product of x and y.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.legendre.legint()", "path": "reference/generated/numpy.polynomial.legendre.legint", "type": "numpy.polynomial.legendre.legint", "text": ["Integrate a Legendre series.", "Returns the Legendre series coefficients c integrated m times from lbnd along axis. At each iteration the resulting series is multiplied by scl and an integration constant, k, is added. The scaling factor is for use in a linear change of variable. (\u201cBuyer beware\u201d: note that, depending on what one is doing, one may want scl to be the reciprocal of what one might expect; for more information, see the Notes section below.) The argument c is an array of coefficients from low to high degree along each axis, e.g., [1,2,3] represents the series L_0 + 2*L_1 + 3*L_2 while [[1,2],[1,2]] represents 1*L_0(x)*L_0(y) + 1*L_1(x)*L_0(y) + 2*L_0(x)*L_1(y) +\n2*L_1(x)*L_1(y) if axis=0 is x and axis=1 is y.", "Array of Legendre series coefficients. If c is multidimensional the different axis correspond to different variables with the degree in each axis given by the corresponding index.", "Order of integration, must be positive. (Default: 1)", "Integration constant(s). The value of the first integral at lbnd is the first value in the list, the value of the second integral at lbnd is the second value, etc. If k == [] (the default), all constants are set to zero. If m == 1, a single scalar can be given instead of a list.", "The lower bound of the integral. (Default: 0)", "Following each integration the result is multiplied by scl before the integration constant is added. (Default: 1)", "Axis over which the integral is taken. (Default: 0).", "New in version 1.7.0.", "Legendre series coefficient array of the integral.", "If m < 0, len(k) > m, np.ndim(lbnd) != 0, or np.ndim(scl) != 0.", "See also", "Note that the result of each integration is multiplied by scl. Why is this important to note? Say one is making a linear change of variable \\(u = ax + b\\) in an integral relative to x. Then \\(dx = du/a\\), so one will need to set scl equal to \\(1/a\\) - perhaps not what one would have first thought.", "Also note that, in general, the result of integrating a C-series needs to be \u201creprojected\u201d onto the C-series basis set. Thus, typically, the result of this function is \u201cunintuitive,\u201d albeit correct; see Examples section below."]}, {"name": "polynomial.legendre.legline()", "path": "reference/generated/numpy.polynomial.legendre.legline", "type": "numpy.polynomial.legendre.legline", "text": ["Legendre series whose graph is a straight line.", "The specified line is given by off + scl*x.", "This module\u2019s representation of the Legendre series for off + scl*x.", "See also"]}, {"name": "polynomial.legendre.legmul()", "path": "reference/generated/numpy.polynomial.legendre.legmul", "type": "numpy.polynomial.legendre.legmul", "text": ["Multiply one Legendre series by another.", "Returns the product of two Legendre series c1 * c2. The arguments are sequences of coefficients, from lowest order \u201cterm\u201d to highest, e.g., [1,2,3] represents the series P_0 + 2*P_1 + 3*P_2.", "1-D arrays of Legendre series coefficients ordered from low to high.", "Of Legendre series coefficients representing their product.", "See also", "In general, the (polynomial) product of two C-series results in terms that are not in the Legendre polynomial basis set. Thus, to express the product as a Legendre series, it is necessary to \u201creproject\u201d the product onto said basis set, which may produce \u201cunintuitive\u201d (but correct) results; see Examples section below."]}, {"name": "polynomial.legendre.legmulx()", "path": "reference/generated/numpy.polynomial.legendre.legmulx", "type": "numpy.polynomial.legendre.legmulx", "text": ["Multiply a Legendre series by x.", "Multiply the Legendre series c by x, where x is the independent variable.", "1-D array of Legendre series coefficients ordered from low to high.", "Array representing the result of the multiplication.", "See also", "The multiplication uses the recursion relationship for Legendre polynomials in the form"]}, {"name": "polynomial.legendre.legone", "path": "reference/generated/numpy.polynomial.legendre.legone", "type": "numpy.polynomial.legendre.legone", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.legendre.legpow()", "path": "reference/generated/numpy.polynomial.legendre.legpow", "type": "numpy.polynomial.legendre.legpow", "text": ["Raise a Legendre series to a power.", "Returns the Legendre series c raised to the power pow. The argument c is a sequence of coefficients ordered from low to high. i.e., [1,2,3] is the series P_0 + 2*P_1 + 3*P_2.", "1-D array of Legendre series coefficients ordered from low to high.", "Power to which the series will be raised", "Maximum power allowed. This is mainly to limit growth of the series to unmanageable size. Default is 16", "Legendre series of power.", "See also"]}, {"name": "polynomial.legendre.legroots()", "path": "reference/generated/numpy.polynomial.legendre.legroots", "type": "numpy.polynomial.legendre.legroots", "text": ["Compute the roots of a Legendre series.", "Return the roots (a.k.a. \u201czeros\u201d) of the polynomial", "1-D array of coefficients.", "Array of the roots of the series. If all the roots are real, then out is also real, otherwise it is complex.", "See also", "The root estimates are obtained as the eigenvalues of the companion matrix, Roots far from the origin of the complex plane may have large errors due to the numerical instability of the series for such values. Roots with multiplicity greater than 1 will also show larger errors as the value of the series near such points is relatively insensitive to errors in the roots. Isolated roots near the origin can be improved by a few iterations of Newton\u2019s method.", "The Legendre series basis polynomials aren\u2019t powers of x so the results of this function may seem unintuitive."]}, {"name": "polynomial.legendre.legsub()", "path": "reference/generated/numpy.polynomial.legendre.legsub", "type": "numpy.polynomial.legendre.legsub", "text": ["Subtract one Legendre series from another.", "Returns the difference of two Legendre series c1 - c2. The sequences of coefficients are from lowest order term to highest, i.e., [1,2,3] represents the series P_0 + 2*P_1 + 3*P_2.", "1-D arrays of Legendre series coefficients ordered from low to high.", "Of Legendre series coefficients representing their difference.", "See also", "Unlike multiplication, division, etc., the difference of two Legendre series is a Legendre series (without having to \u201creproject\u201d the result onto the basis set) so subtraction, just like that of \u201cstandard\u201d polynomials, is simply \u201ccomponent-wise.\u201d"]}, {"name": "polynomial.legendre.legtrim()", "path": "reference/generated/numpy.polynomial.legendre.legtrim", "type": "numpy.polynomial.legendre.legtrim", "text": ["Remove \u201csmall\u201d \u201ctrailing\u201d coefficients from a polynomial.", "\u201cSmall\u201d means \u201csmall in absolute value\u201d and is controlled by the parameter tol; \u201ctrailing\u201d means highest order coefficient(s), e.g., in [0, 1, 1, 0, 0] (which represents 0 + x + x**2 + 0*x**3 + 0*x**4) both the 3-rd and 4-th order coefficients would be \u201ctrimmed.\u201d", "1-d array of coefficients, ordered from lowest order to highest.", "Trailing (i.e., highest order) elements with absolute value less than or equal to tol (default value is zero) are removed.", "1-d array with trailing zeros removed. If the resulting series would be empty, a series containing a single zero is returned.", "If tol < 0", "See also"]}, {"name": "polynomial.legendre.legval()", "path": "reference/generated/numpy.polynomial.legendre.legval", "type": "numpy.polynomial.legendre.legval", "text": ["Evaluate a Legendre series at points x.", "If c is of length n + 1, this function returns the value:", "The parameter x is converted to an array only if it is a tuple or a list, otherwise it is treated as a scalar. In either case, either x or its elements must support multiplication and addition both with themselves and with the elements of c.", "If c is a 1-D array, then p(x) will have the same shape as x. If c is multidimensional, then the shape of the result depends on the value of tensor. If tensor is true the shape will be c.shape[1:] + x.shape. If tensor is false the shape will be c.shape[1:]. Note that scalars have shape (,).", "Trailing zeros in the coefficients will be used in the evaluation, so they should be avoided if efficiency is a concern.", "If x is a list or tuple, it is converted to an ndarray, otherwise it is left unchanged and treated as a scalar. In either case, x or its elements must support addition and multiplication with with themselves and with the elements of c.", "Array of coefficients ordered so that the coefficients for terms of degree n are contained in c[n]. If c is multidimensional the remaining indices enumerate multiple polynomials. In the two dimensional case the coefficients may be thought of as stored in the columns of c.", "If True, the shape of the coefficient array is extended with ones on the right, one for each dimension of x. Scalars have dimension 0 for this action. The result is that every column of coefficients in c is evaluated for every element of x. If False, x is broadcast over the columns of c for the evaluation. This keyword is useful when c is multidimensional. The default value is True.", "New in version 1.7.0.", "The shape of the return value is described above.", "See also", "The evaluation uses Clenshaw recursion, aka synthetic division."]}, {"name": "polynomial.legendre.legval2d()", "path": "reference/generated/numpy.polynomial.legendre.legval2d", "type": "numpy.polynomial.legendre.legval2d", "text": ["Evaluate a 2-D Legendre series at points (x, y).", "This function returns the values:", "The parameters x and y are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars and they must have the same shape after conversion. In either case, either x and y or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c is a 1-D array a one is implicitly appended to its shape to make it 2-D. The shape of the result will be c.shape[2:] + x.shape.", "The two dimensional series is evaluated at the points (x, y), where x and y must have the same shape. If x or y is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and if it isn\u2019t an ndarray it is treated as a scalar.", "Array of coefficients ordered so that the coefficient of the term of multi-degree i,j is contained in c[i,j]. If c has dimension greater than two the remaining indices enumerate multiple sets of coefficients.", "The values of the two dimensional Legendre series at points formed from pairs of corresponding values from x and y.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.legendre.legval3d()", "path": "reference/generated/numpy.polynomial.legendre.legval3d", "type": "numpy.polynomial.legendre.legval3d", "text": ["Evaluate a 3-D Legendre series at points (x, y, z).", "This function returns the values:", "The parameters x, y, and z are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars and they must have the same shape after conversion. In either case, either x, y, and z or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c has fewer than 3 dimensions, ones are implicitly appended to its shape to make it 3-D. The shape of the result will be c.shape[3:] + x.shape.", "The three dimensional series is evaluated at the points (x, y, z), where x, y, and z must have the same shape. If any of x, y, or z is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and if it isn\u2019t an ndarray it is treated as a scalar.", "Array of coefficients ordered so that the coefficient of the term of multi-degree i,j,k is contained in c[i,j,k]. If c has dimension greater than 3 the remaining indices enumerate multiple sets of coefficients.", "The values of the multidimensional polynomial on points formed with triples of corresponding values from x, y, and z.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.legendre.legvander()", "path": "reference/generated/numpy.polynomial.legendre.legvander", "type": "numpy.polynomial.legendre.legvander", "text": ["Pseudo-Vandermonde matrix of given degree.", "Returns the pseudo-Vandermonde matrix of degree deg and sample points x. The pseudo-Vandermonde matrix is defined by", "where 0 <= i <= deg. The leading indices of V index the elements of x and the last index is the degree of the Legendre polynomial.", "If c is a 1-D array of coefficients of length n + 1 and V is the array V = legvander(x, n), then np.dot(V, c) and legval(x, c) are the same up to roundoff. This equivalence is useful both for least squares fitting and for the evaluation of a large number of Legendre series of the same degree and sample points.", "Array of points. The dtype is converted to float64 or complex128 depending on whether any of the elements are complex. If x is scalar it is converted to a 1-D array.", "Degree of the resulting matrix.", "The pseudo-Vandermonde matrix. The shape of the returned matrix is x.shape + (deg + 1,), where The last index is the degree of the corresponding Legendre polynomial. The dtype will be the same as the converted x."]}, {"name": "polynomial.legendre.legvander2d()", "path": "reference/generated/numpy.polynomial.legendre.legvander2d", "type": "numpy.polynomial.legendre.legvander2d", "text": ["Pseudo-Vandermonde matrix of given degrees.", "Returns the pseudo-Vandermonde matrix of degrees deg and sample points (x, y). The pseudo-Vandermonde matrix is defined by", "where 0 <= i <= deg[0] and 0 <= j <= deg[1]. The leading indices of V index the points (x, y) and the last index encodes the degrees of the Legendre polynomials.", "If V = legvander2d(x, y, [xdeg, ydeg]), then the columns of V correspond to the elements of a 2-D coefficient array c of shape (xdeg + 1, ydeg + 1) in the order", "and np.dot(V, c.flat) and legval2d(x, y, c) will be the same up to roundoff. This equivalence is useful both for least squares fitting and for the evaluation of a large number of 2-D Legendre series of the same degrees and sample points.", "Arrays of point coordinates, all of the same shape. The dtypes will be converted to either float64 or complex128 depending on whether any of the elements are complex. Scalars are converted to 1-D arrays.", "List of maximum degrees of the form [x_deg, y_deg].", "The shape of the returned matrix is x.shape + (order,), where \\(order = (deg[0]+1)*(deg[1]+1)\\). The dtype will be the same as the converted x and y.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.legendre.legvander3d()", "path": "reference/generated/numpy.polynomial.legendre.legvander3d", "type": "numpy.polynomial.legendre.legvander3d", "text": ["Pseudo-Vandermonde matrix of given degrees.", "Returns the pseudo-Vandermonde matrix of degrees deg and sample points (x, y, z). If l, m, n are the given degrees in x, y, z, then The pseudo-Vandermonde matrix is defined by", "where 0 <= i <= l, 0 <= j <= m, and 0 <= j <= n. The leading indices of V index the points (x, y, z) and the last index encodes the degrees of the Legendre polynomials.", "If V = legvander3d(x, y, z, [xdeg, ydeg, zdeg]), then the columns of V correspond to the elements of a 3-D coefficient array c of shape (xdeg + 1, ydeg + 1, zdeg + 1) in the order", "and np.dot(V, c.flat) and legval3d(x, y, z, c) will be the same up to roundoff. This equivalence is useful both for least squares fitting and for the evaluation of a large number of 3-D Legendre series of the same degrees and sample points.", "Arrays of point coordinates, all of the same shape. The dtypes will be converted to either float64 or complex128 depending on whether any of the elements are complex. Scalars are converted to 1-D arrays.", "List of maximum degrees of the form [x_deg, y_deg, z_deg].", "The shape of the returned matrix is x.shape + (order,), where \\(order = (deg[0]+1)*(deg[1]+1)*(deg[2]+1)\\). The dtype will be the same as the converted x, y, and z.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.legendre.legweight()", "path": "reference/generated/numpy.polynomial.legendre.legweight", "type": "numpy.polynomial.legendre.legweight", "text": ["Weight function of the Legendre polynomials.", "The weight function is \\(1\\) and the interval of integration is \\([-1, 1]\\). The Legendre polynomials are orthogonal, but not normalized, with respect to this weight function.", "Values at which the weight function will be computed.", "The weight function at x.", "New in version 1.7.0."]}, {"name": "polynomial.legendre.legx", "path": "reference/generated/numpy.polynomial.legendre.legx", "type": "numpy.polynomial.legendre.legx", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.legendre.legzero", "path": "reference/generated/numpy.polynomial.legendre.legzero", "type": "numpy.polynomial.legendre.legzero", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.legendre.poly2leg()", "path": "reference/generated/numpy.polynomial.legendre.poly2leg", "type": "numpy.polynomial.legendre.poly2leg", "text": ["Convert a polynomial to a Legendre series.", "Convert an array representing the coefficients of a polynomial (relative to the \u201cstandard\u201d basis) ordered from lowest degree to highest, to an array of the coefficients of the equivalent Legendre series, ordered from lowest to highest degree.", "1-D array containing the polynomial coefficients", "1-D array containing the coefficients of the equivalent Legendre series.", "See also", "The easy way to do conversions between polynomial basis sets is to use the convert method of a class instance."]}, {"name": "polynomial.polynomial.polyadd()", "path": "reference/generated/numpy.polynomial.polynomial.polyadd", "type": "numpy.polynomial.polynomial.polyadd", "text": ["Add one polynomial to another.", "Returns the sum of two polynomials c1 + c2. The arguments are sequences of coefficients from lowest order term to highest, i.e., [1,2,3] represents the polynomial 1 + 2*x + 3*x**2.", "1-D arrays of polynomial coefficients ordered from low to high.", "The coefficient array representing their sum.", "See also"]}, {"name": "polynomial.polynomial.polycompanion()", "path": "reference/generated/numpy.polynomial.polynomial.polycompanion", "type": "numpy.polynomial.polynomial.polycompanion", "text": ["Return the companion matrix of c.", "The companion matrix for power series cannot be made symmetric by scaling the basis, so this function differs from those for the orthogonal polynomials.", "1-D array of polynomial coefficients ordered from low to high degree.", "Companion matrix of dimensions (deg, deg).", "New in version 1.7.0."]}, {"name": "polynomial.polynomial.polyder()", "path": "reference/generated/numpy.polynomial.polynomial.polyder", "type": "numpy.polynomial.polynomial.polyder", "text": ["Differentiate a polynomial.", "Returns the polynomial coefficients c differentiated m times along axis. At each iteration the result is multiplied by scl (the scaling factor is for use in a linear change of variable). The argument c is an array of coefficients from low to high degree along each axis, e.g., [1,2,3] represents the polynomial 1 + 2*x + 3*x**2 while [[1,2],[1,2]] represents 1 + 1*x + 2*y + 2*x*y if axis=0 is x and axis=1 is y.", "Array of polynomial coefficients. If c is multidimensional the different axis correspond to different variables with the degree in each axis given by the corresponding index.", "Number of derivatives taken, must be non-negative. (Default: 1)", "Each differentiation is multiplied by scl. The end result is multiplication by scl**m. This is for use in a linear change of variable. (Default: 1)", "Axis over which the derivative is taken. (Default: 0).", "New in version 1.7.0.", "Polynomial coefficients of the derivative.", "See also"]}, {"name": "polynomial.polynomial.polydiv()", "path": "reference/generated/numpy.polynomial.polynomial.polydiv", "type": "numpy.polynomial.polynomial.polydiv", "text": ["Divide one polynomial by another.", "Returns the quotient-with-remainder of two polynomials c1 / c2. The arguments are sequences of coefficients, from lowest order term to highest, e.g., [1,2,3] represents 1 + 2*x + 3*x**2.", "1-D arrays of polynomial coefficients ordered from low to high.", "Of coefficient series representing the quotient and remainder.", "See also"]}, {"name": "polynomial.polynomial.polydomain", "path": "reference/generated/numpy.polynomial.polynomial.polydomain", "type": "numpy.polynomial.polynomial.polydomain", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.polynomial.polyfit()", "path": "reference/generated/numpy.polynomial.polynomial.polyfit", "type": "numpy.polynomial.polynomial.polyfit", "text": ["Least-squares fit of a polynomial to data.", "Return the coefficients of a polynomial of degree deg that is the least squares fit to the data values y given at points x. If y is 1-D the returned coefficients will also be 1-D. If y is 2-D multiple fits are done, one for each column of y, and the resulting coefficients are stored in the corresponding columns of a 2-D return. The fitted polynomial(s) are in the form", "where n is deg.", "x-coordinates of the M sample (data) points (x[i], y[i]).", "y-coordinates of the sample points. Several sets of sample points sharing the same x-coordinates can be (independently) fit with one call to polyfit by passing in for y a 2-D array that contains one data set per column.", "Degree(s) of the fitting polynomials. If deg is a single integer all terms up to and including the deg\u2019th term are included in the fit. For NumPy versions >= 1.11.0 a list of integers specifying the degrees of the terms to include may be used instead.", "Relative condition number of the fit. Singular values smaller than rcond, relative to the largest singular value, will be ignored. The default value is len(x)*eps, where eps is the relative precision of the platform\u2019s float type, about 2e-16 in most cases.", "Switch determining the nature of the return value. When False (the default) just the coefficients are returned; when True, diagnostic information from the singular value decomposition (used to solve the fit\u2019s matrix equation) is also returned.", "Weights. If not None, the weight w[i] applies to the unsquared residual y[i] - y_hat[i] at x[i]. Ideally the weights are chosen so that the errors of the products w[i]*y[i] all have the same variance. When using inverse-variance weighting, use w[i] = 1/sigma(y[i]). The default value is None.", "New in version 1.5.0.", "Polynomial coefficients ordered from low to high. If y was 2-D, the coefficients in column k of coef represent the polynomial fit to the data in y\u2019s k-th column.", "These values are only returned if full == True", "For more details, see numpy.linalg.lstsq.", "Raised if the matrix in the least-squares fit is rank deficient. The warning is only raised if full == False. The warnings can be turned off by:", "See also", "Evaluates a polynomial.", "Vandermonde matrix for powers.", "Computes a least-squares fit from the matrix.", "Computes spline fits.", "The solution is the coefficients of the polynomial p that minimizes the sum of the weighted squared errors", "where the \\(w_j\\) are the weights. This problem is solved by setting up the (typically) over-determined matrix equation:", "where V is the weighted pseudo Vandermonde matrix of x, c are the coefficients to be solved for, w are the weights, and y are the observed values. This equation is then solved using the singular value decomposition of V.", "If some of the singular values of V are so small that they are neglected (and full == False), a RankWarning will be raised. This means that the coefficient values may be poorly determined. Fitting to a lower order polynomial will usually get rid of the warning (but may not be what you want, of course; if you have independent reason(s) for choosing the degree which isn\u2019t working, you may have to: a) reconsider those reasons, and/or b) reconsider the quality of your data). The rcond parameter can also be set to a value smaller than its default, but the resulting fit may be spurious and have large contributions from roundoff error.", "Polynomial fits using double precision tend to \u201cfail\u201d at about (polynomial) degree 20. Fits using Chebyshev or Legendre series are generally better conditioned, but much can still depend on the distribution of the sample points and the smoothness of the data. If the quality of the fit is inadequate, splines may be a good alternative.", "Same thing without the added noise"]}, {"name": "polynomial.polynomial.polyfromroots()", "path": "reference/generated/numpy.polynomial.polynomial.polyfromroots", "type": "numpy.polynomial.polynomial.polyfromroots", "text": ["Generate a monic polynomial with given roots.", "Return the coefficients of the polynomial", "where the r_n are the roots specified in roots. If a zero has multiplicity n, then it must appear in roots n times. For instance, if 2 is a root of multiplicity three and 3 is a root of multiplicity 2, then roots looks something like [2, 2, 2, 3, 3]. The roots can appear in any order.", "If the returned coefficients are c, then", "The coefficient of the last term is 1 for monic polynomials in this form.", "Sequence containing the roots.", "1-D array of the polynomial\u2019s coefficients If all the roots are real, then out is also real, otherwise it is complex. (see Examples below).", "See also", "The coefficients are determined by multiplying together linear factors of the form (x - r_i), i.e.", "where n == len(roots) - 1; note that this implies that 1 is always returned for \\(a_n\\)."]}, {"name": "polynomial.polynomial.polygrid2d()", "path": "reference/generated/numpy.polynomial.polynomial.polygrid2d", "type": "numpy.polynomial.polynomial.polygrid2d", "text": ["Evaluate a 2-D polynomial on the Cartesian product of x and y.", "This function returns the values:", "where the points (a, b) consist of all pairs formed by taking a from x and b from y. The resulting points form a grid with x in the first dimension and y in the second.", "The parameters x and y are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars. In either case, either x and y or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c has fewer than two dimensions, ones are implicitly appended to its shape to make it 2-D. The shape of the result will be c.shape[2:] + x.shape + y.shape.", "The two dimensional series is evaluated at the points in the Cartesian product of x and y. If x or y is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and, if it isn\u2019t an ndarray, it is treated as a scalar.", "Array of coefficients ordered so that the coefficients for terms of degree i,j are contained in c[i,j]. If c has dimension greater than two the remaining indices enumerate multiple sets of coefficients.", "The values of the two dimensional polynomial at points in the Cartesian product of x and y.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.polynomial.polygrid3d()", "path": "reference/generated/numpy.polynomial.polynomial.polygrid3d", "type": "numpy.polynomial.polynomial.polygrid3d", "text": ["Evaluate a 3-D polynomial on the Cartesian product of x, y and z.", "This function returns the values:", "where the points (a, b, c) consist of all triples formed by taking a from x, b from y, and c from z. The resulting points form a grid with x in the first dimension, y in the second, and z in the third.", "The parameters x, y, and z are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars. In either case, either x, y, and z or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c has fewer than three dimensions, ones are implicitly appended to its shape to make it 3-D. The shape of the result will be c.shape[3:] + x.shape + y.shape + z.shape.", "The three dimensional series is evaluated at the points in the Cartesian product of x, y, and z. If x,`y`, or z is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and, if it isn\u2019t an ndarray, it is treated as a scalar.", "Array of coefficients ordered so that the coefficients for terms of degree i,j are contained in c[i,j]. If c has dimension greater than two the remaining indices enumerate multiple sets of coefficients.", "The values of the two dimensional polynomial at points in the Cartesian product of x and y.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.polynomial.polyint()", "path": "reference/generated/numpy.polynomial.polynomial.polyint", "type": "numpy.polynomial.polynomial.polyint", "text": ["Integrate a polynomial.", "Returns the polynomial coefficients c integrated m times from lbnd along axis. At each iteration the resulting series is multiplied by scl and an integration constant, k, is added. The scaling factor is for use in a linear change of variable. (\u201cBuyer beware\u201d: note that, depending on what one is doing, one may want scl to be the reciprocal of what one might expect; for more information, see the Notes section below.) The argument c is an array of coefficients, from low to high degree along each axis, e.g., [1,2,3] represents the polynomial 1 + 2*x + 3*x**2 while [[1,2],[1,2]] represents 1 + 1*x + 2*y + 2*x*y if axis=0 is x and axis=1 is y.", "1-D array of polynomial coefficients, ordered from low to high.", "Order of integration, must be positive. (Default: 1)", "Integration constant(s). The value of the first integral at zero is the first value in the list, the value of the second integral at zero is the second value, etc. If k == [] (the default), all constants are set to zero. If m == 1, a single scalar can be given instead of a list.", "The lower bound of the integral. (Default: 0)", "Following each integration the result is multiplied by scl before the integration constant is added. (Default: 1)", "Axis over which the integral is taken. (Default: 0).", "New in version 1.7.0.", "Coefficient array of the integral.", "If m < 1, len(k) > m, np.ndim(lbnd) != 0, or np.ndim(scl) != 0.", "See also", "Note that the result of each integration is multiplied by scl. Why is this important to note? Say one is making a linear change of variable \\(u = ax + b\\) in an integral relative to x. Then \\(dx = du/a\\), so one will need to set scl equal to \\(1/a\\) - perhaps not what one would have first thought."]}, {"name": "polynomial.polynomial.polyline()", "path": "reference/generated/numpy.polynomial.polynomial.polyline", "type": "numpy.polynomial.polynomial.polyline", "text": ["Returns an array representing a linear polynomial.", "The \u201cy-intercept\u201d and \u201cslope\u201d of the line, respectively.", "This module\u2019s representation of the linear polynomial off +\nscl*x.", "See also"]}, {"name": "polynomial.polynomial.polymul()", "path": "reference/generated/numpy.polynomial.polynomial.polymul", "type": "numpy.polynomial.polynomial.polymul", "text": ["Multiply one polynomial by another.", "Returns the product of two polynomials c1 * c2. The arguments are sequences of coefficients, from lowest order term to highest, e.g., [1,2,3] represents the polynomial 1 + 2*x + 3*x**2.", "1-D arrays of coefficients representing a polynomial, relative to the \u201cstandard\u201d basis, and ordered from lowest order term to highest.", "Of the coefficients of their product.", "See also"]}, {"name": "polynomial.polynomial.polymulx()", "path": "reference/generated/numpy.polynomial.polynomial.polymulx", "type": "numpy.polynomial.polynomial.polymulx", "text": ["Multiply a polynomial by x.", "Multiply the polynomial c by x, where x is the independent variable.", "1-D array of polynomial coefficients ordered from low to high.", "Array representing the result of the multiplication.", "See also", "New in version 1.5.0."]}, {"name": "polynomial.polynomial.Polynomial.__call__()", "path": "reference/generated/numpy.polynomial.polynomial.polynomial.__call__", "type": "numpy.polynomial.polynomial.Polynomial", "text": ["method", "Call self as a function."]}, {"name": "polynomial.polynomial.Polynomial.basis()", "path": "reference/generated/numpy.polynomial.polynomial.polynomial.basis", "type": "numpy.polynomial.polynomial.Polynomial", "text": ["method", "Series basis polynomial of degree deg.", "Returns the series representing the basis polynomial of degree deg.", "New in version 1.7.0.", "Degree of the basis polynomial for the series. Must be >= 0.", "If given, the array must be of the form [beg, end], where beg and end are the endpoints of the domain. If None is given then the class domain is used. The default is None.", "If given, the resulting array must be if the form [beg, end], where beg and end are the endpoints of the window. If None is given then the class window is used. The default is None.", "A series with the coefficient of the deg term set to one and all others zero."]}, {"name": "polynomial.polynomial.Polynomial.cast()", "path": "reference/generated/numpy.polynomial.polynomial.polynomial.cast", "type": "numpy.polynomial.polynomial.Polynomial", "text": ["method", "Convert series to series of this class.", "The series is expected to be an instance of some polynomial series of one of the types supported by by the numpy.polynomial module, but could be some other class that supports the convert method.", "New in version 1.7.0.", "The series instance to be converted.", "If given, the array must be of the form [beg, end], where beg and end are the endpoints of the domain. If None is given then the class domain is used. The default is None.", "If given, the resulting array must be if the form [beg, end], where beg and end are the endpoints of the window. If None is given then the class window is used. The default is None.", "A series of the same kind as the calling class and equal to series when evaluated.", "See also", "similar instance method"]}, {"name": "polynomial.polynomial.Polynomial.convert()", "path": "reference/generated/numpy.polynomial.polynomial.polynomial.convert", "type": "numpy.polynomial.polynomial.Polynomial", "text": ["method", "Convert series to a different kind and/or domain and/or window.", "The domain of the converted series. If the value is None, the default domain of kind is used.", "The polynomial series type class to which the current instance should be converted. If kind is None, then the class of the current instance is used.", "The window of the converted series. If the value is None, the default window of kind is used.", "The returned class can be of different type than the current instance and/or have a different domain and/or different window.", "Conversion between domains and class types can result in numerically ill defined series."]}, {"name": "polynomial.polynomial.Polynomial.copy()", "path": "reference/generated/numpy.polynomial.polynomial.polynomial.copy", "type": "numpy.polynomial.polynomial.Polynomial", "text": ["method", "Return a copy.", "Copy of self."]}, {"name": "polynomial.polynomial.Polynomial.cutdeg()", "path": "reference/generated/numpy.polynomial.polynomial.polynomial.cutdeg", "type": "numpy.polynomial.polynomial.Polynomial", "text": ["method", "Truncate series to the given degree.", "Reduce the degree of the series to deg by discarding the high order terms. If deg is greater than the current degree a copy of the current series is returned. This can be useful in least squares where the coefficients of the high degree terms may be very small.", "New in version 1.5.0.", "The series is reduced to degree deg by discarding the high order terms. The value of deg must be a non-negative integer.", "New instance of series with reduced degree."]}, {"name": "polynomial.polynomial.Polynomial.degree()", "path": "reference/generated/numpy.polynomial.polynomial.polynomial.degree", "type": "numpy.polynomial.polynomial.Polynomial", "text": ["method", "The degree of the series.", "New in version 1.5.0.", "Degree of the series, one less than the number of coefficients."]}, {"name": "polynomial.polynomial.Polynomial.deriv()", "path": "reference/generated/numpy.polynomial.polynomial.polynomial.deriv", "type": "numpy.polynomial.polynomial.Polynomial", "text": ["method", "Differentiate.", "Return a series instance of that is the derivative of the current series.", "Find the derivative of order m.", "A new series representing the derivative. The domain is the same as the domain of the differentiated series."]}, {"name": "polynomial.polynomial.Polynomial.domain", "path": "reference/generated/numpy.polynomial.polynomial.polynomial.domain", "type": "Polynomials", "text": ["attribute"]}, {"name": "polynomial.polynomial.Polynomial.fit()", "path": "reference/generated/numpy.polynomial.polynomial.polynomial.fit", "type": "numpy.polynomial.polynomial.Polynomial", "text": ["method", "Least squares fit to data.", "Return a series instance that is the least squares fit to the data y sampled at x. The domain of the returned instance can be specified and this will often result in a superior fit with less chance of ill conditioning.", "x-coordinates of the M sample points (x[i], y[i]).", "y-coordinates of the M sample points (x[i], y[i]).", "Degree(s) of the fitting polynomials. If deg is a single integer all terms up to and including the deg\u2019th term are included in the fit. For NumPy versions >= 1.11.0 a list of integers specifying the degrees of the terms to include may be used instead.", "Domain to use for the returned series. If None, then a minimal domain that covers the points x is chosen. If [] the class domain is used. The default value was the class domain in NumPy 1.4 and None in later versions. The [] option was added in numpy 1.5.0.", "Relative condition number of the fit. Singular values smaller than this relative to the largest singular value will be ignored. The default value is len(x)*eps, where eps is the relative precision of the float type, about 2e-16 in most cases.", "Switch determining nature of return value. When it is False (the default) just the coefficients are returned, when True diagnostic information from the singular value decomposition is also returned.", "Weights. If not None, the weight w[i] applies to the unsquared residual y[i] - y_hat[i] at x[i]. Ideally the weights are chosen so that the errors of the products w[i]*y[i] all have the same variance. When using inverse-variance weighting, use w[i] = 1/sigma(y[i]). The default value is None.", "New in version 1.5.0.", "Window to use for the returned series. The default value is the default class domain", "New in version 1.6.0.", "A series that represents the least squares fit to the data and has the domain and window specified in the call. If the coefficients for the unscaled and unshifted basis polynomials are of interest, do new_series.convert().coef.", "These values are only returned if full == True", "For more details, see linalg.lstsq."]}, {"name": "polynomial.polynomial.Polynomial.fromroots()", "path": "reference/generated/numpy.polynomial.polynomial.polynomial.fromroots", "type": "numpy.polynomial.polynomial.Polynomial", "text": ["method", "Return series instance that has the specified roots.", "Returns a series representing the product (x - r[0])*(x - r[1])*...*(x - r[n-1]), where r is a list of roots.", "List of roots.", "Domain for the resulting series. If None the domain is the interval from the smallest root to the largest. If [] the domain is the class domain. The default is [].", "Window for the returned series. If None the class window is used. The default is None.", "Series with the specified roots."]}, {"name": "polynomial.polynomial.Polynomial.has_samecoef()", "path": "reference/generated/numpy.polynomial.polynomial.polynomial.has_samecoef", "type": "numpy.polynomial.polynomial.Polynomial", "text": ["method", "Check if coefficients match.", "New in version 1.6.0.", "The other class must have the coef attribute.", "True if the coefficients are the same, False otherwise."]}, {"name": "polynomial.polynomial.Polynomial.has_samedomain()", "path": "reference/generated/numpy.polynomial.polynomial.polynomial.has_samedomain", "type": "numpy.polynomial.polynomial.Polynomial", "text": ["method", "Check if domains match.", "New in version 1.6.0.", "The other class must have the domain attribute.", "True if the domains are the same, False otherwise."]}, {"name": "polynomial.polynomial.Polynomial.has_sametype()", "path": "reference/generated/numpy.polynomial.polynomial.polynomial.has_sametype", "type": "numpy.polynomial.polynomial.Polynomial", "text": ["method", "Check if types match.", "New in version 1.7.0.", "Class instance.", "True if other is same class as self"]}, {"name": "polynomial.polynomial.Polynomial.has_samewindow()", "path": "reference/generated/numpy.polynomial.polynomial.polynomial.has_samewindow", "type": "numpy.polynomial.polynomial.Polynomial", "text": ["method", "Check if windows match.", "New in version 1.6.0.", "The other class must have the window attribute.", "True if the windows are the same, False otherwise."]}, {"name": "polynomial.polynomial.Polynomial.identity()", "path": "reference/generated/numpy.polynomial.polynomial.polynomial.identity", "type": "numpy.polynomial.polynomial.Polynomial", "text": ["method", "Identity function.", "If p is the returned series, then p(x) == x for all values of x.", "If given, the array must be of the form [beg, end], where beg and end are the endpoints of the domain. If None is given then the class domain is used. The default is None.", "If given, the resulting array must be if the form [beg, end], where beg and end are the endpoints of the window. If None is given then the class window is used. The default is None.", "Series of representing the identity."]}, {"name": "polynomial.polynomial.Polynomial.integ()", "path": "reference/generated/numpy.polynomial.polynomial.polynomial.integ", "type": "numpy.polynomial.polynomial.Polynomial", "text": ["method", "Integrate.", "Return a series instance that is the definite integral of the current series.", "The number of integrations to perform.", "Integration constants. The first constant is applied to the first integration, the second to the second, and so on. The list of values must less than or equal to m in length and any missing values are set to zero.", "The lower bound of the definite integral.", "A new series representing the integral. The domain is the same as the domain of the integrated series."]}, {"name": "polynomial.polynomial.Polynomial.linspace()", "path": "reference/generated/numpy.polynomial.polynomial.polynomial.linspace", "type": "numpy.polynomial.polynomial.Polynomial", "text": ["method", "Return x, y values at equally spaced points in domain.", "Returns the x, y values at n linearly spaced points across the domain. Here y is the value of the polynomial at the points x. By default the domain is the same as that of the series instance. This method is intended mostly as a plotting aid.", "New in version 1.5.0.", "Number of point pairs to return. The default value is 100.", "If not None, the specified domain is used instead of that of the calling instance. It should be of the form [beg,end]. The default is None which case the class domain is used.", "x is equal to linspace(self.domain[0], self.domain[1], n) and y is the series evaluated at element of x."]}, {"name": "polynomial.polynomial.Polynomial.mapparms()", "path": "reference/generated/numpy.polynomial.polynomial.polynomial.mapparms", "type": "numpy.polynomial.polynomial.Polynomial", "text": ["method", "Return the mapping parameters.", "The returned values define a linear map off + scl*x that is applied to the input arguments before the series is evaluated. The map depends on the domain and window; if the current domain is equal to the window the resulting map is the identity. If the coefficients of the series instance are to be used by themselves outside this class, then the linear function must be substituted for the x in the standard representation of the base polynomials.", "The mapping function is defined by off + scl*x.", "If the current domain is the interval [l1, r1] and the window is [l2, r2], then the linear mapping function L is defined by the equations:"]}, {"name": "polynomial.polynomial.Polynomial.roots()", "path": "reference/generated/numpy.polynomial.polynomial.polynomial.roots", "type": "numpy.polynomial.polynomial.Polynomial", "text": ["method", "Return the roots of the series polynomial.", "Compute the roots for the series. Note that the accuracy of the roots decrease the further outside the domain they lie.", "Array containing the roots of the series."]}, {"name": "polynomial.polynomial.Polynomial.trim()", "path": "reference/generated/numpy.polynomial.polynomial.polynomial.trim", "type": "numpy.polynomial.polynomial.Polynomial", "text": ["method", "Remove trailing coefficients", "Remove trailing coefficients until a coefficient is reached whose absolute value greater than tol or the beginning of the series is reached. If all the coefficients would be removed the series is set to [0]. A new series instance is returned with the new coefficients. The current instance remains unchanged.", "All trailing coefficients less than tol will be removed.", "New instance of series with trimmed coefficients."]}, {"name": "polynomial.polynomial.Polynomial.truncate()", "path": "reference/generated/numpy.polynomial.polynomial.polynomial.truncate", "type": "numpy.polynomial.polynomial.Polynomial", "text": ["method", "Truncate series to length size.", "Reduce the series to length size by discarding the high degree terms. The value of size must be a positive integer. This can be useful in least squares where the coefficients of the high degree terms may be very small.", "The series is reduced to length size by discarding the high degree terms. The value of size must be a positive integer.", "New instance of series with truncated coefficients."]}, {"name": "polynomial.polynomial.polyone", "path": "reference/generated/numpy.polynomial.polynomial.polyone", "type": "numpy.polynomial.polynomial.polyone", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.polynomial.polypow()", "path": "reference/generated/numpy.polynomial.polynomial.polypow", "type": "numpy.polynomial.polynomial.polypow", "text": ["Raise a polynomial to a power.", "Returns the polynomial c raised to the power pow. The argument c is a sequence of coefficients ordered from low to high. i.e., [1,2,3] is the series 1 + 2*x + 3*x**2.", "1-D array of array of series coefficients ordered from low to high degree.", "Power to which the series will be raised", "Maximum power allowed. This is mainly to limit growth of the series to unmanageable size. Default is 16", "Power series of power.", "See also"]}, {"name": "polynomial.polynomial.polyroots()", "path": "reference/generated/numpy.polynomial.polynomial.polyroots", "type": "numpy.polynomial.polynomial.polyroots", "text": ["Compute the roots of a polynomial.", "Return the roots (a.k.a. \u201czeros\u201d) of the polynomial", "1-D array of polynomial coefficients.", "Array of the roots of the polynomial. If all the roots are real, then out is also real, otherwise it is complex.", "See also", "The root estimates are obtained as the eigenvalues of the companion matrix, Roots far from the origin of the complex plane may have large errors due to the numerical instability of the power series for such values. Roots with multiplicity greater than 1 will also show larger errors as the value of the series near such points is relatively insensitive to errors in the roots. Isolated roots near the origin can be improved by a few iterations of Newton\u2019s method."]}, {"name": "polynomial.polynomial.polysub()", "path": "reference/generated/numpy.polynomial.polynomial.polysub", "type": "numpy.polynomial.polynomial.polysub", "text": ["Subtract one polynomial from another.", "Returns the difference of two polynomials c1 - c2. The arguments are sequences of coefficients from lowest order term to highest, i.e., [1,2,3] represents the polynomial 1 + 2*x + 3*x**2.", "1-D arrays of polynomial coefficients ordered from low to high.", "Of coefficients representing their difference.", "See also"]}, {"name": "polynomial.polynomial.polytrim()", "path": "reference/generated/numpy.polynomial.polynomial.polytrim", "type": "numpy.polynomial.polynomial.polytrim", "text": ["Remove \u201csmall\u201d \u201ctrailing\u201d coefficients from a polynomial.", "\u201cSmall\u201d means \u201csmall in absolute value\u201d and is controlled by the parameter tol; \u201ctrailing\u201d means highest order coefficient(s), e.g., in [0, 1, 1, 0, 0] (which represents 0 + x + x**2 + 0*x**3 + 0*x**4) both the 3-rd and 4-th order coefficients would be \u201ctrimmed.\u201d", "1-d array of coefficients, ordered from lowest order to highest.", "Trailing (i.e., highest order) elements with absolute value less than or equal to tol (default value is zero) are removed.", "1-d array with trailing zeros removed. If the resulting series would be empty, a series containing a single zero is returned.", "If tol < 0", "See also"]}, {"name": "polynomial.polynomial.polyval()", "path": "reference/generated/numpy.polynomial.polynomial.polyval", "type": "numpy.polynomial.polynomial.polyval", "text": ["Evaluate a polynomial at points x.", "If c is of length n + 1, this function returns the value", "The parameter x is converted to an array only if it is a tuple or a list, otherwise it is treated as a scalar. In either case, either x or its elements must support multiplication and addition both with themselves and with the elements of c.", "If c is a 1-D array, then p(x) will have the same shape as x. If c is multidimensional, then the shape of the result depends on the value of tensor. If tensor is true the shape will be c.shape[1:] + x.shape. If tensor is false the shape will be c.shape[1:]. Note that scalars have shape (,).", "Trailing zeros in the coefficients will be used in the evaluation, so they should be avoided if efficiency is a concern.", "If x is a list or tuple, it is converted to an ndarray, otherwise it is left unchanged and treated as a scalar. In either case, x or its elements must support addition and multiplication with with themselves and with the elements of c.", "Array of coefficients ordered so that the coefficients for terms of degree n are contained in c[n]. If c is multidimensional the remaining indices enumerate multiple polynomials. In the two dimensional case the coefficients may be thought of as stored in the columns of c.", "If True, the shape of the coefficient array is extended with ones on the right, one for each dimension of x. Scalars have dimension 0 for this action. The result is that every column of coefficients in c is evaluated for every element of x. If False, x is broadcast over the columns of c for the evaluation. This keyword is useful when c is multidimensional. The default value is True.", "New in version 1.7.0.", "The shape of the returned array is described above.", "See also", "The evaluation uses Horner\u2019s method."]}, {"name": "polynomial.polynomial.polyval2d()", "path": "reference/generated/numpy.polynomial.polynomial.polyval2d", "type": "numpy.polynomial.polynomial.polyval2d", "text": ["Evaluate a 2-D polynomial at points (x, y).", "This function returns the value", "The parameters x and y are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars and they must have the same shape after conversion. In either case, either x and y or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c has fewer than two dimensions, ones are implicitly appended to its shape to make it 2-D. The shape of the result will be c.shape[2:] + x.shape.", "The two dimensional series is evaluated at the points (x, y), where x and y must have the same shape. If x or y is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and, if it isn\u2019t an ndarray, it is treated as a scalar.", "Array of coefficients ordered so that the coefficient of the term of multi-degree i,j is contained in c[i,j]. If c has dimension greater than two the remaining indices enumerate multiple sets of coefficients.", "The values of the two dimensional polynomial at points formed with pairs of corresponding values from x and y.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.polynomial.polyval3d()", "path": "reference/generated/numpy.polynomial.polynomial.polyval3d", "type": "numpy.polynomial.polynomial.polyval3d", "text": ["Evaluate a 3-D polynomial at points (x, y, z).", "This function returns the values:", "The parameters x, y, and z are converted to arrays only if they are tuples or a lists, otherwise they are treated as a scalars and they must have the same shape after conversion. In either case, either x, y, and z or their elements must support multiplication and addition both with themselves and with the elements of c.", "If c has fewer than 3 dimensions, ones are implicitly appended to its shape to make it 3-D. The shape of the result will be c.shape[3:] + x.shape.", "The three dimensional series is evaluated at the points (x, y, z), where x, y, and z must have the same shape. If any of x, y, or z is a list or tuple, it is first converted to an ndarray, otherwise it is left unchanged and if it isn\u2019t an ndarray it is treated as a scalar.", "Array of coefficients ordered so that the coefficient of the term of multi-degree i,j,k is contained in c[i,j,k]. If c has dimension greater than 3 the remaining indices enumerate multiple sets of coefficients.", "The values of the multidimensional polynomial on points formed with triples of corresponding values from x, y, and z.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.polynomial.polyvalfromroots()", "path": "reference/generated/numpy.polynomial.polynomial.polyvalfromroots", "type": "numpy.polynomial.polynomial.polyvalfromroots", "text": ["Evaluate a polynomial specified by its roots at points x.", "If r is of length N, this function returns the value", "The parameter x is converted to an array only if it is a tuple or a list, otherwise it is treated as a scalar. In either case, either x or its elements must support multiplication and addition both with themselves and with the elements of r.", "If r is a 1-D array, then p(x) will have the same shape as x. If r is multidimensional, then the shape of the result depends on the value of tensor. If tensor is ``True` the shape will be r.shape[1:] + x.shape; that is, each polynomial is evaluated at every value of x. If tensor is False, the shape will be r.shape[1:]; that is, each polynomial is evaluated only for the corresponding broadcast value of x. Note that scalars have shape (,).", "New in version 1.12.", "If x is a list or tuple, it is converted to an ndarray, otherwise it is left unchanged and treated as a scalar. In either case, x or its elements must support addition and multiplication with with themselves and with the elements of r.", "Array of roots. If r is multidimensional the first index is the root index, while the remaining indices enumerate multiple polynomials. For instance, in the two dimensional case the roots of each polynomial may be thought of as stored in the columns of r.", "If True, the shape of the roots array is extended with ones on the right, one for each dimension of x. Scalars have dimension 0 for this action. The result is that every column of coefficients in r is evaluated for every element of x. If False, x is broadcast over the columns of r for the evaluation. This keyword is useful when r is multidimensional. The default value is True.", "The shape of the returned array is described above.", "See also"]}, {"name": "polynomial.polynomial.polyvander()", "path": "reference/generated/numpy.polynomial.polynomial.polyvander", "type": "numpy.polynomial.polynomial.polyvander", "text": ["Vandermonde matrix of given degree.", "Returns the Vandermonde matrix of degree deg and sample points x. The Vandermonde matrix is defined by", "where 0 <= i <= deg. The leading indices of V index the elements of x and the last index is the power of x.", "If c is a 1-D array of coefficients of length n + 1 and V is the matrix V = polyvander(x, n), then np.dot(V, c) and polyval(x, c) are the same up to roundoff. This equivalence is useful both for least squares fitting and for the evaluation of a large number of polynomials of the same degree and sample points.", "Array of points. The dtype is converted to float64 or complex128 depending on whether any of the elements are complex. If x is scalar it is converted to a 1-D array.", "Degree of the resulting matrix.", "The Vandermonde matrix. The shape of the returned matrix is x.shape + (deg + 1,), where the last index is the power of x. The dtype will be the same as the converted x.", "See also"]}, {"name": "polynomial.polynomial.polyvander2d()", "path": "reference/generated/numpy.polynomial.polynomial.polyvander2d", "type": "numpy.polynomial.polynomial.polyvander2d", "text": ["Pseudo-Vandermonde matrix of given degrees.", "Returns the pseudo-Vandermonde matrix of degrees deg and sample points (x, y). The pseudo-Vandermonde matrix is defined by", "where 0 <= i <= deg[0] and 0 <= j <= deg[1]. The leading indices of V index the points (x, y) and the last index encodes the powers of x and y.", "If V = polyvander2d(x, y, [xdeg, ydeg]), then the columns of V correspond to the elements of a 2-D coefficient array c of shape (xdeg + 1, ydeg + 1) in the order", "and np.dot(V, c.flat) and polyval2d(x, y, c) will be the same up to roundoff. This equivalence is useful both for least squares fitting and for the evaluation of a large number of 2-D polynomials of the same degrees and sample points.", "Arrays of point coordinates, all of the same shape. The dtypes will be converted to either float64 or complex128 depending on whether any of the elements are complex. Scalars are converted to 1-D arrays.", "List of maximum degrees of the form [x_deg, y_deg].", "The shape of the returned matrix is x.shape + (order,), where \\(order = (deg[0]+1)*(deg([1]+1)\\). The dtype will be the same as the converted x and y.", "See also"]}, {"name": "polynomial.polynomial.polyvander3d()", "path": "reference/generated/numpy.polynomial.polynomial.polyvander3d", "type": "numpy.polynomial.polynomial.polyvander3d", "text": ["Pseudo-Vandermonde matrix of given degrees.", "Returns the pseudo-Vandermonde matrix of degrees deg and sample points (x, y, z). If l, m, n are the given degrees in x, y, z, then The pseudo-Vandermonde matrix is defined by", "where 0 <= i <= l, 0 <= j <= m, and 0 <= j <= n. The leading indices of V index the points (x, y, z) and the last index encodes the powers of x, y, and z.", "If V = polyvander3d(x, y, z, [xdeg, ydeg, zdeg]), then the columns of V correspond to the elements of a 3-D coefficient array c of shape (xdeg + 1, ydeg + 1, zdeg + 1) in the order", "and np.dot(V, c.flat) and polyval3d(x, y, z, c) will be the same up to roundoff. This equivalence is useful both for least squares fitting and for the evaluation of a large number of 3-D polynomials of the same degrees and sample points.", "Arrays of point coordinates, all of the same shape. The dtypes will be converted to either float64 or complex128 depending on whether any of the elements are complex. Scalars are converted to 1-D arrays.", "List of maximum degrees of the form [x_deg, y_deg, z_deg].", "The shape of the returned matrix is x.shape + (order,), where \\(order = (deg[0]+1)*(deg([1]+1)*(deg[2]+1)\\). The dtype will be the same as the converted x, y, and z.", "See also", "New in version 1.7.0."]}, {"name": "polynomial.polynomial.polyx", "path": "reference/generated/numpy.polynomial.polynomial.polyx", "type": "numpy.polynomial.polynomial.polyx", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.polynomial.polyzero", "path": "reference/generated/numpy.polynomial.polynomial.polyzero", "type": "numpy.polynomial.polynomial.polyzero", "text": ["An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.)", "Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(\u2026)) for instantiating an array.", "For more information, refer to the numpy module and examine the methods and attributes of an array.", "Shape of created array.", "Any object that can be interpreted as a numpy data type.", "Used to fill the array with data.", "Offset of array data in buffer.", "Strides of data in memory.", "Row-major (C-style) or column-major (Fortran-style) order.", "See also", "Construct an array.", "Create an array, each element of which is zero.", "Create an array, but leave its allocated memory unchanged (i.e., it contains \u201cgarbage\u201d).", "Create a data-type.", "An ndarray alias generic w.r.t. its dtype.type.", "There are two modes of creating an array using __new__:", "No __init__ method is needed because the array is fully initialized after the __new__ method.", "These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray.", "First mode, buffer is None:", "Second mode:", "Transpose of the array.", "The array\u2019s elements, in memory.", "Describes the format of the elements in the array.", "Dictionary containing information related to memory use, e.g., \u2018C_CONTIGUOUS\u2019, \u2018OWNDATA\u2019, \u2018WRITEABLE\u2019, etc.", "Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO).", "Imaginary part of the array.", "Real part of the array.", "Number of elements in the array.", "The memory use of each array element in bytes.", "The total number of bytes required to store the array data, i.e., itemsize * size.", "The array\u2019s number of dimensions.", "Shape of the array.", "The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2). This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time (2 * 4).", "Class containing properties of the array needed for interaction with ctypes.", "If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored."]}, {"name": "polynomial.polyutils.as_series()", "path": "reference/generated/numpy.polynomial.polyutils.as_series", "type": "numpy.polynomial.polyutils.as_series", "text": ["Return argument as a list of 1-d arrays.", "The returned list contains array(s) of dtype double, complex double, or object. A 1-d argument of shape (N,) is parsed into N arrays of size one; a 2-d argument of shape (M,N) is parsed into M arrays of size N (i.e., is \u201cparsed by row\u201d); and a higher dimensional array raises a Value Error if it is not first reshaped into either a 1-d or 2-d array.", "A 1- or 2-d array_like", "When True, trailing zeros are removed from the inputs. When False, the inputs are passed through intact.", "A copy of the input data as a list of 1-d arrays.", "Raised when as_series cannot convert its input to 1-d arrays, or at least one of the resulting arrays is empty."]}, {"name": "polynomial.polyutils.getdomain()", "path": "reference/generated/numpy.polynomial.polyutils.getdomain", "type": "numpy.polynomial.polyutils.getdomain", "text": ["Return a domain suitable for given abscissae.", "Find a domain suitable for a polynomial or Chebyshev series defined at the values supplied.", "1-d array of abscissae whose domain will be determined.", "1-d array containing two values. If the inputs are complex, then the two returned points are the lower left and upper right corners of the smallest rectangle (aligned with the axes) in the complex plane containing the points x. If the inputs are real, then the two points are the ends of the smallest interval containing the points x.", "See also"]}, {"name": "polynomial.polyutils.mapdomain()", "path": "reference/generated/numpy.polynomial.polyutils.mapdomain", "type": "numpy.polynomial.polyutils.mapdomain", "text": ["Apply linear map to input points.", "The linear map offset + scale*x that maps the domain old to the domain new is applied to the points x.", "Points to be mapped. If x is a subtype of ndarray the subtype will be preserved.", "The two domains that determine the map. Each must (successfully) convert to 1-d arrays containing precisely two values.", "Array of points of the same shape as x, after application of the linear map between the two domains.", "See also", "Effectively, this implements:", "where", "Also works for complex numbers (and thus can be used to map any line in the complex plane to any other line therein)."]}, {"name": "polynomial.polyutils.mapparms()", "path": "reference/generated/numpy.polynomial.polyutils.mapparms", "type": "numpy.polynomial.polyutils.mapparms", "text": ["Linear map parameters between domains.", "Return the parameters of the linear map offset + scale*x that maps old to new such that old[i] -> new[i], i = 0, 1.", "Domains. Each domain must (successfully) convert to a 1-d array containing precisely two values.", "The map L(x) = offset + scale*x maps the first domain to the second.", "See also", "Also works for complex numbers, and thus can be used to calculate the parameters required to map any line in the complex plane to any other line therein."]}, {"name": "polynomial.polyutils.RankWarning", "path": "reference/generated/numpy.polynomial.polyutils.rankwarning", "type": "numpy.polynomial.polyutils.RankWarning", "text": ["Issued by chebfit when the design matrix is rank deficient."]}, {"name": "polynomial.polyutils.trimcoef()", "path": "reference/generated/numpy.polynomial.polyutils.trimcoef", "type": "numpy.polynomial.polyutils.trimcoef", "text": ["Remove \u201csmall\u201d \u201ctrailing\u201d coefficients from a polynomial.", "\u201cSmall\u201d means \u201csmall in absolute value\u201d and is controlled by the parameter tol; \u201ctrailing\u201d means highest order coefficient(s), e.g., in [0, 1, 1, 0, 0] (which represents 0 + x + x**2 + 0*x**3 + 0*x**4) both the 3-rd and 4-th order coefficients would be \u201ctrimmed.\u201d", "1-d array of coefficients, ordered from lowest order to highest.", "Trailing (i.e., highest order) elements with absolute value less than or equal to tol (default value is zero) are removed.", "1-d array with trailing zeros removed. If the resulting series would be empty, a series containing a single zero is returned.", "If tol < 0", "See also"]}, {"name": "polynomial.polyutils.trimseq()", "path": "reference/generated/numpy.polynomial.polyutils.trimseq", "type": "numpy.polynomial.polyutils.trimseq", "text": ["Remove small Poly series coefficients.", "Sequence of Poly series coefficients. This routine fails for empty sequences.", "Subsequence with trailing zeros removed. If the resulting sequence would be empty, return the first element. The returned sequence may or may not be a view.", "Do not lose the type info if the sequence contains unknown objects."]}, {"name": "polynomial.set_default_printstyle()", "path": "reference/generated/numpy.polynomial.set_default_printstyle", "type": "Polynomials", "text": ["Set the default format for the string representation of polynomials.", "Values for style must be valid inputs to __format__, i.e. \u2018ascii\u2019 or \u2018unicode\u2019.", "Format string for default printing style. Must be either \u2018ascii\u2019 or \u2018unicode\u2019.", "The default format depends on the platform: \u2018unicode\u2019 is used on Unix-based systems and \u2018ascii\u2019 on Windows. This determination is based on default font support for the unicode superscript and subscript ranges."]}, {"name": "Polyutils", "path": "reference/routines.polynomials.polyutils", "type": "Polyutils", "text": ["Utility classes and functions for the polynomial modules.", "This module provides: error and warning objects; a polynomial base class; and some routines used in both the polynomial and chebyshev modules.", "RankWarning", "Issued by chebfit when the design matrix is rank deficient.", "as_series(alist[, trim])", "Return argument as a list of 1-d arrays.", "trimseq(seq)", "Remove small Poly series coefficients.", "trimcoef(c[, tol])", "Remove \"small\" \"trailing\" coefficients from a polynomial.", "getdomain(x)", "Return a domain suitable for given abscissae.", "mapdomain(x, old, new)", "Apply linear map to input points.", "mapparms(old, new)", "Linear map parameters between domains."]}, {"name": "Power Series (numpy.polynomial.polynomial)", "path": "reference/routines.polynomials.polynomial", "type": "Power Series ( \n        \n         numpy.polynomial.polynomial\n        \n        )", "text": ["This module provides a number of objects (mostly functions) useful for dealing with polynomials, including a Polynomial class that encapsulates the usual arithmetic operations. (General information on how this module represents and works with polynomial objects is in the docstring for its \u201cparent\u201d sub-package, numpy.polynomial).", "Polynomial(coef[, domain, window])", "A power series class.", "polydomain", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "polyzero", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "polyone", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "polyx", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "polyadd(c1, c2)", "Add one polynomial to another.", "polysub(c1, c2)", "Subtract one polynomial from another.", "polymulx(c)", "Multiply a polynomial by x.", "polymul(c1, c2)", "Multiply one polynomial by another.", "polydiv(c1, c2)", "Divide one polynomial by another.", "polypow(c, pow[, maxpower])", "Raise a polynomial to a power.", "polyval(x, c[, tensor])", "Evaluate a polynomial at points x.", "polyval2d(x, y, c)", "Evaluate a 2-D polynomial at points (x, y).", "polyval3d(x, y, z, c)", "Evaluate a 3-D polynomial at points (x, y, z).", "polygrid2d(x, y, c)", "Evaluate a 2-D polynomial on the Cartesian product of x and y.", "polygrid3d(x, y, z, c)", "Evaluate a 3-D polynomial on the Cartesian product of x, y and z.", "polyder(c[, m, scl, axis])", "Differentiate a polynomial.", "polyint(c[, m, k, lbnd, scl, axis])", "Integrate a polynomial.", "polyfromroots(roots)", "Generate a monic polynomial with given roots.", "polyroots(c)", "Compute the roots of a polynomial.", "polyvalfromroots(x, r[, tensor])", "Evaluate a polynomial specified by its roots at points x.", "polyvander(x, deg)", "Vandermonde matrix of given degree.", "polyvander2d(x, y, deg)", "Pseudo-Vandermonde matrix of given degrees.", "polyvander3d(x, y, z, deg)", "Pseudo-Vandermonde matrix of given degrees.", "polycompanion(c)", "Return the companion matrix of c.", "polyfit(x, y, deg[, rcond, full, w])", "Least-squares fit of a polynomial to data.", "polytrim(c[, tol])", "Remove \"small\" \"trailing\" coefficients from a polynomial.", "polyline(off, scl)", "Returns an array representing a linear polynomial.", "numpy.polynomial"]}, {"name": "property finfo.machar", "path": "reference/generated/numpy.finfo.machar", "type": "NumPy.finfo.machar", "text": ["property", "The object which calculated these parameters and holds more detailed information.", "Deprecated since version 1.22."]}, {"name": "property finfo.smallest_normal", "path": "reference/generated/numpy.finfo.smallest_normal", "type": "NumPy.finfo.smallest_normal", "text": ["property", "Return the value for the smallest normal.", "Value for the smallest normal.", "If the calculated value for the smallest normal is requested for double-double."]}, {"name": "property finfo.tiny", "path": "reference/generated/numpy.finfo.tiny", "type": "NumPy.finfo.tiny", "text": ["property", "Return the value for tiny, alias of smallest_normal.", "Value for the smallest normal, alias of smallest_normal.", "If the calculated value for the smallest normal is requested for double-double."]}, {"name": "property iinfo.max", "path": "reference/generated/numpy.iinfo.max", "type": "Data type routines", "text": ["property", "Maximum value of given dtype."]}, {"name": "property iinfo.min", "path": "reference/generated/numpy.iinfo.min", "type": "Data type routines", "text": ["property", "Minimum value of given dtype."]}, {"name": "property lib.Arrayterator.flat", "path": "reference/generated/numpy.lib.arrayterator.flat", "type": "Indexing routines", "text": ["property", "A 1-D flat iterator for Arrayterator objects.", "This iterator returns elements of the array to be iterated over in Arrayterator one by one. It is similar to flatiter.", "See also"]}, {"name": "property lib.Arrayterator.shape", "path": "reference/generated/numpy.lib.arrayterator.shape", "type": "Indexing routines", "text": ["property", "The shape of the array to be iterated over.", "For an example, see Arrayterator."]}, {"name": "property ma.masked_array.mask", "path": "reference/generated/numpy.ma.masked_array.mask", "type": "numpy.ma.masked_array.mask", "text": ["property", "Current mask."]}, {"name": "property ma.MaskedArray.dtype", "path": "reference/generated/numpy.ma.maskedarray.dtype", "type": "Masked arrays", "text": ["property", "Data-type of the array\u2019s elements.", "See also"]}, {"name": "property ma.MaskedArray.flat", "path": "reference/generated/numpy.ma.maskedarray.flat", "type": "Masked arrays", "text": ["property", "Return a flat iterator, or set a flattened version of self to value."]}, {"name": "property ma.MaskedArray.imag", "path": "reference/generated/numpy.ma.maskedarray.imag", "type": "Masked arrays", "text": ["property", "The imaginary part of the masked array.", "This property is a view on the imaginary part of this MaskedArray.", "See also"]}, {"name": "property ma.MaskedArray.real", "path": "reference/generated/numpy.ma.maskedarray.real", "type": "Masked arrays", "text": ["property", "The real part of the masked array.", "This property is a view on the real part of this MaskedArray.", "See also"]}, {"name": "property ma.MaskedArray.shape", "path": "reference/generated/numpy.ma.maskedarray.shape", "type": "Masked arrays", "text": ["property", "Tuple of array dimensions.", "The shape property is usually used to get the current shape of an array, but may also be used to reshape the array in-place by assigning a tuple of array dimensions to it. As with numpy.reshape, one of the new shape dimensions can be -1, in which case its value is inferred from the size of the array and the remaining dimensions. Reshaping an array in-place will fail if a copy is required.", "See also", "similar function", "similar method"]}, {"name": "property ma.MaskedArray.T", "path": "reference/generated/numpy.ma.maskedarray.t", "type": "Masked arrays", "text": ["property", "The transposed array.", "Same as self.transpose().", "See also"]}, {"name": "property matrix.A", "path": "reference/generated/numpy.matrix.a", "type": "numpy.matrix.A", "text": ["property", "Return self as an ndarray object.", "Equivalent to np.asarray(self).", "self as an ndarray"]}, {"name": "property matrix.A1", "path": "reference/generated/numpy.matrix.a1", "type": "Standard array subclasses", "text": ["property", "Return self as a flattened ndarray.", "Equivalent to np.asarray(x).ravel()", "self, 1-D, as an ndarray"]}, {"name": "property matrix.H", "path": "reference/generated/numpy.matrix.h", "type": "numpy.matrix.H", "text": ["property", "Returns the (complex) conjugate transpose of self.", "Equivalent to np.transpose(self) if self is real-valued.", "complex conjugate transpose of self"]}, {"name": "property matrix.I", "path": "reference/generated/numpy.matrix.i", "type": "numpy.matrix.I", "text": ["property", "Returns the (multiplicative) inverse of invertible self.", "If self is non-singular, ret is such that ret * self == self * ret == np.matrix(np.eye(self[0,:].size)) all return True.", "If self is singular.", "See also"]}, {"name": "property matrix.T", "path": "reference/generated/numpy.matrix.t", "type": "numpy.matrix.T", "text": ["property", "Returns the transpose of the matrix.", "Does not conjugate! For the complex conjugate transpose, use .H.", "The (non-conjugated) transpose of the matrix.", "See also"]}, {"name": "property poly1d.c", "path": "reference/generated/numpy.poly1d.c", "type": "Polynomials", "text": ["property", "The polynomial coefficients"]}, {"name": "property poly1d.coef", "path": "reference/generated/numpy.poly1d.coef", "type": "Polynomials", "text": ["property", "The polynomial coefficients"]}, {"name": "property poly1d.coefficients", "path": "reference/generated/numpy.poly1d.coefficients", "type": "Polynomials", "text": ["property", "The polynomial coefficients"]}, {"name": "property poly1d.coeffs", "path": "reference/generated/numpy.poly1d.coeffs", "type": "Polynomials", "text": ["property", "The polynomial coefficients"]}, {"name": "property poly1d.o", "path": "reference/generated/numpy.poly1d.o", "type": "Polynomials", "text": ["property", "The order or degree of the polynomial"]}, {"name": "property poly1d.order", "path": "reference/generated/numpy.poly1d.order", "type": "Polynomials", "text": ["property", "The order or degree of the polynomial"]}, {"name": "property poly1d.r", "path": "reference/generated/numpy.poly1d.r", "type": "Polynomials", "text": ["property", "The roots of the polynomial, where self(x) == 0"]}, {"name": "property poly1d.variable", "path": "reference/generated/numpy.poly1d.variable", "type": "Polynomials", "text": ["property", "The name of the polynomial variable"]}, {"name": "PY_ARRAY_UNIQUE_SYMBOL", "path": "reference/c-api/array#c.PY_ARRAY_UNIQUE_SYMBOL", "type": "Array API", "text": []}, {"name": "PyArray_ArrayDescr *subarray", "path": "reference/c-api/types-and-structures#c.NPY_USE_SETITEM.subarray", "type": "Python Types and C-Structures", "text": ["If this is non- NULL, then this data-type descriptor is a C-style contiguous array of another data-type descriptor. In other-words, each element that this descriptor describes is actually an array of some other base descriptor. This is most useful as the data-type descriptor for a field in another data-type descriptor. The fields member should be NULL if this is non- NULL (the fields member of the base descriptor can be non- NULL however).", "The data-type-descriptor object of the base-type.", "The shape (always C-style contiguous) of the sub-array as a Python tuple."]}, {"name": "PyArray_ArrFuncs *f", "path": "reference/c-api/types-and-structures#c.NPY_USE_SETITEM.f", "type": "Python Types and C-Structures", "text": ["A pointer to a structure containing functions that the type needs to implement internal features. These functions are not the same thing as the universal functions (ufuncs) described later. Their signatures can vary arbitrarily."]}, {"name": "PyArray_CEQ()", "path": "reference/c-api/array#c.PyArray_CEQ", "type": "Array API", "text": []}, {"name": "PyArray_CGE()", "path": "reference/c-api/array#c.PyArray_CGE", "type": "Array API", "text": []}, {"name": "PyArray_CGT()", "path": "reference/c-api/array#c.PyArray_CGT", "type": "Array API", "text": []}, {"name": "PyArray_CLE()", "path": "reference/c-api/array#c.PyArray_CLE", "type": "Array API", "text": []}, {"name": "PyArray_CLT()", "path": "reference/c-api/array#c.PyArray_CLT", "type": "Array API", "text": []}, {"name": "PyArray_CNE()", "path": "reference/c-api/array#c.PyArray_CNE", "type": "Array API", "text": ["Implements the complex comparisons between two complex numbers (structures with a real and imag member) using NumPy\u2019s definition of the ordering which is lexicographic: comparing the real parts first and then the complex parts if the real parts are equal."]}, {"name": "PyArray_Descr **NpyIter_GetDescrArray()", "path": "reference/c-api/iterator#c.NpyIter_GetDescrArray", "type": "Array Iterator API", "text": ["This gives back a pointer to the nop data type Descrs for the objects being iterated. The result points into iter, so the caller does not gain any references to the Descrs.", "This pointer may be cached before the iteration loop, calling iternext will not change it."]}, {"name": "PyArray_Descr *descr", "path": "reference/c-api/types-and-structures#c.NPY_AO.descr", "type": "Python Types and C-Structures", "text": ["A pointer to a data-type descriptor object (see below). The data-type descriptor object is an instance of a new built-in type which allows a generic description of memory. There is a descriptor structure for each data type supported. This descriptor structure contains useful information about the type as well as a pointer to a table of function pointers to implement specific functionality. As the name suggests, it is associated with the macro PyArray_DESCR."]}, {"name": "PyArray_Descr *PyArray_DESCR()", "path": "reference/c-api/array#c.PyArray_DESCR", "type": "Array API", "text": ["Returns a borrowed reference to the dtype property of the array."]}, {"name": "PyArray_Descr *PyArray_DescrFromObject()", "path": "reference/c-api/array#c.PyArray_DescrFromObject", "type": "Array API", "text": ["Determine an appropriate data-type object from the object op (which should be a \u201cnested\u201d sequence object) and the minimum data-type descriptor mintype (which can be NULL ). Similar in behavior to array(op).dtype. Don\u2019t confuse this function with PyArray_DescrConverter. This function essentially looks at all the objects in the (nested) sequence and determines the data-type from the elements it finds."]}, {"name": "PyArray_Descr *PyArray_DescrFromScalar()", "path": "reference/c-api/array#c.PyArray_DescrFromScalar", "type": "Array API", "text": ["Return a data-type object from an array-scalar object. No checking is done to be sure that scalar is an array scalar. If no suitable data-type can be determined, then a data-type of NPY_OBJECT is returned by default."]}, {"name": "PyArray_Descr *PyArray_DescrFromType()", "path": "reference/c-api/array#c.PyArray_DescrFromType", "type": "Array API", "text": ["Returns a data-type object corresponding to typenum. The typenum can be one of the enumerated types, a character code for one of the enumerated types, or a user-defined type. If you want to use a flexible size array, then you need to flexible typenum and set the results elsize parameter to the desired size. The typenum is one of the NPY_TYPES."]}, {"name": "PyArray_Descr *PyArray_DescrNew()", "path": "reference/c-api/array#c.PyArray_DescrNew", "type": "Array API", "text": ["Return a new data-type object copied from obj (the fields reference is just updated so that the new object points to the same fields dictionary if any)."]}, {"name": "PyArray_Descr *PyArray_DescrNewByteorder()", "path": "reference/c-api/array#c.PyArray_DescrNewByteorder", "type": "Array API", "text": ["Create a new data-type object with the byteorder set according to newendian. All referenced data-type objects (in subdescr and fields members of the data-type object) are also changed (recursively).", "The value of newendian is one of these macros:"]}, {"name": "PyArray_Descr *PyArray_DescrNewFromType()", "path": "reference/c-api/array#c.PyArray_DescrNewFromType", "type": "Array API", "text": ["Create a new data-type object from the built-in (or user-registered) data-type indicated by typenum. All builtin types should not have any of their fields changed. This creates a new copy of the PyArray_Descr structure so that you can fill it in as appropriate. This function is especially needed for flexible data-types which need to have a new elsize member in order to be meaningful in array construction."]}, {"name": "PyArray_Descr *PyArray_DTYPE()", "path": "reference/c-api/array#c.PyArray_DTYPE", "type": "Array API", "text": ["New in version 1.7.", "A synonym for PyArray_DESCR, named to be consistent with the \u2018dtype\u2019 usage within Python."]}, {"name": "PyArray_Descr *PyArray_MinScalarType()", "path": "reference/c-api/array#c.PyArray_MinScalarType", "type": "Array API", "text": ["New in version 1.6.", "If arr is an array, returns its data type descriptor, but if arr is an array scalar (has 0 dimensions), it finds the data type of smallest size to which the value may be converted without overflow or truncation to an integer.", "This function will not demote complex to float or anything to boolean, but will demote a signed integer to an unsigned integer when the scalar value is positive."]}, {"name": "PyArray_Descr *PyArray_PromoteTypes()", "path": "reference/c-api/array#c.PyArray_PromoteTypes", "type": "Array API", "text": ["New in version 1.6.", "Finds the data type of smallest size and kind to which type1 and type2 may be safely converted. This function is symmetric and associative. A string or unicode result will be the proper size for storing the max value of the input types converted to a string or unicode."]}, {"name": "PyArray_Descr *PyArray_ResultType()", "path": "reference/c-api/array#c.PyArray_ResultType", "type": "Array API", "text": ["New in version 1.6.", "This applies type promotion to all the inputs, using the NumPy rules for combining scalars and arrays, to determine the output type of a set of operands. This is the same result type that ufuncs produce. The specific algorithm used is as follows.", "Categories are determined by first checking which of boolean, integer (int/uint), or floating point (float/complex) the maximum kind of all the arrays and the scalars are.", "If there are only scalars or the maximum category of the scalars is higher than the maximum category of the arrays, the data types are combined with PyArray_PromoteTypes to produce the return value.", "Otherwise, PyArray_MinScalarType is called on each array, and the resulting data types are all combined with PyArray_PromoteTypes to produce the return value.", "The set of int values is not a subset of the uint values for types with the same number of bits, something not reflected in PyArray_MinScalarType, but handled as a special case in PyArray_ResultType."]}, {"name": "PyArray_IsScalar()", "path": "reference/c-api/array#c.PyArray_IsScalar", "type": "Array API", "text": ["Evaluates true if op is an instance of Py{cls}ArrType_Type."]}, {"name": "PyArray_MAX()", "path": "reference/c-api/array#c.PyArray_MAX", "type": "Array API", "text": ["Returns the maximum of a and b. If (a) or (b) are expressions they are evaluated twice."]}, {"name": "PyArray_MIN()", "path": "reference/c-api/array#c.PyArray_MIN", "type": "Array API", "text": ["Returns the minimum of a and b. If (a) or (b) are expressions they are evaluated twice."]}, {"name": "PyArray_VectorUnaryFunc *PyArray_GetCastFunc()", "path": "reference/c-api/array#c.PyArray_GetCastFunc", "type": "Array API", "text": ["Return the low-level casting function to cast from the given descriptor to the builtin type number. If no casting function exists return NULL and set an error. Using this function instead of direct access to from ->f->cast will allow support of any user-defined casting functions added to a descriptors casting dictionary."]}, {"name": "PyArrayIterObject **iters", "path": "reference/c-api/types-and-structures#c.PyArrayMultiIterObject.iters", "type": "Python Types and C-Structures", "text": ["An array of iterator objects that holds the iterators for the arrays to be broadcast together. On return, the iterators are adjusted for broadcasting."]}, {"name": "PyArrayObject **PyArray_ConvertToCommonType()", "path": "reference/c-api/array#c.PyArray_ConvertToCommonType", "type": "Array API", "text": ["The functionality this provides is largely superseded by iterator NpyIter introduced in 1.6, with flag NPY_ITER_COMMON_DTYPE or with the same dtype parameter for all operands.", "Convert a sequence of Python objects contained in op to an array of ndarrays each having the same data type. The type is selected in the same way as PyArray_ResultType. The length of the sequence is returned in n, and an n -length array of PyArrayObject pointers is the return value (or NULL if an error occurs). The returned array must be freed by the caller of this routine (using PyDataMem_FREE ) and all the array objects in it DECREF \u2018d or a memory-leak will occur. The example template-code below shows a typically usage:", "Changed in version 1.18.0: A mix of scalars and zero-dimensional arrays now produces a type capable of holding the scalar value. Previously priority was given to the dtype of the arrays."]}, {"name": "PyArrayObject *ao", "path": "reference/c-api/types-and-structures#c.PyArrayIterObject.ao", "type": "Python Types and C-Structures", "text": ["A pointer to the underlying ndarray this iterator was created to represent."]}, {"name": "PyArrayObject *PyArray_GETCONTIGUOUS()", "path": "reference/c-api/array#c.PyArray_GETCONTIGUOUS", "type": "Array API", "text": ["If op is already (C-style) contiguous and well-behaved then just return a reference, otherwise return a (contiguous and well-behaved) copy of the array. The parameter op must be a (sub-class of an) ndarray and no checking for that is done."]}, {"name": "PyDataMem_EventHookFunc *PyDataMem_SetEventHook()", "path": "reference/c-api/data_memory#c.PyDataMem_SetEventHook", "type": "Memory management in NumPy", "text": ["Sets the allocation event hook for numpy array data.", "Returns a pointer to the previous hook or NULL. If old_data is non-NULL, the previous user_data pointer will be copied to it.", "If not NULL, hook will be called at the end of each PyDataMem_NEW/FREE/RENEW:", "When the hook is called, the GIL will be held by the calling thread. The hook should be written to be reentrant, if it performs operations that might cause new allocation events (such as the creation/destruction numpy objects, or creating/destroying Python objects which might cause a gc)"]}, {"name": "PyObject **NpyIter_GetOperandArray()", "path": "reference/c-api/iterator#c.NpyIter_GetOperandArray", "type": "Array Iterator API", "text": ["This gives back a pointer to the nop operand PyObjects that are being iterated. The result points into iter, so the caller does not gain any references to the PyObjects."]}, {"name": "PyObject *base", "path": "reference/c-api/types-and-structures#c.NPY_AO.base", "type": "Python Types and C-Structures", "text": ["Pointed to by PyArray_BASE, this member is used to hold a pointer to another Python object that is related to this array. There are two use cases:", "When PyArray_ResolveWritebackIfCopy is called, the array pointed to by base will be updated with the contents of this array."]}, {"name": "PyObject *castdict", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.castdict", "type": "Python Types and C-Structures", "text": ["Either NULL or a dictionary containing low-level casting functions for user- defined data-types. Each function is wrapped in a PyCapsule* and keyed by the data-type number."]}, {"name": "PyObject *descr", "path": "reference/c-api/types-and-structures#c.PyArrayInterface.descr", "type": "Python Types and C-Structures", "text": ["A Python object describing the data-type in more detail (same as the descr key in __array_interface__). This can be NULL if typekind and itemsize provide enough information. This field is also ignored unless NPY_ARR_HAS_DESCR flag is on in flags."]}, {"name": "PyObject *fields", "path": "reference/c-api/types-and-structures#c.NPY_USE_SETITEM.fields", "type": "Python Types and C-Structures", "text": ["If this is non-NULL, then this data-type-descriptor has fields described by a Python dictionary whose keys are names (and also titles if given) and whose values are tuples that describe the fields. Recall that a data-type-descriptor always describes a fixed-length set of bytes. A field is a named sub-region of that total, fixed-length collection. A field is described by a tuple composed of another data- type-descriptor and a byte offset. Optionally, the tuple may contain a title which is normally a Python string. These tuples are placed in this dictionary keyed by name (and also title if given)."]}, {"name": "PyObject *getitem()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.getitem", "type": "Python Types and C-Structures", "text": ["A pointer to a function that returns a standard Python object from a single element of the array object arr pointed to by data. This function must be able to deal with \u201cmisbehaved \u201c(misaligned and/or swapped) arrays correctly."]}, {"name": "PyObject *metadata", "path": "reference/c-api/types-and-structures#c.NPY_USE_SETITEM.metadata", "type": "Python Types and C-Structures", "text": ["Metadata about this dtype."]}, {"name": "PyObject *names", "path": "reference/c-api/types-and-structures#c.NPY_USE_SETITEM.names", "type": "Python Types and C-Structures", "text": ["An ordered tuple of field names. It is NULL if no field is defined."]}, {"name": "PyObject *NpyIter_GetIterView()", "path": "reference/c-api/iterator#c.NpyIter_GetIterView", "type": "Array Iterator API", "text": ["This gives back a reference to a new ndarray view, which is a view into the i-th object in the array NpyIter_GetOperandArray, whose dimensions and strides match the internal optimized iteration pattern. A C-order iteration of this view is equivalent to the iterator\u2019s iteration order.", "For example, if an iterator was created with a single array as its input, and it was possible to rearrange all its axes and then collapse it into a single strided iteration, this would return a view that is a one-dimensional array."]}, {"name": "PyObject *obj", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.obj", "type": "Python Types and C-Structures", "text": ["For ufuncs dynamically created from python functions, this member holds a reference to the underlying Python function."]}, {"name": "PyObject *PyArray_All()", "path": "reference/c-api/array#c.PyArray_All", "type": "Array API", "text": ["Equivalent to ndarray.all (self, axis). Return an array with True elements for every 1-d sub-array of self defined by axis in which all the elements are True."]}, {"name": "PyObject *PyArray_Any()", "path": "reference/c-api/array#c.PyArray_Any", "type": "Array API", "text": ["Equivalent to ndarray.any (self, axis). Return an array with True elements for every 1-d sub-array of self defined by axis in which any of the elements are True."]}, {"name": "PyObject *PyArray_Arange()", "path": "reference/c-api/array#c.PyArray_Arange", "type": "Array API", "text": ["Construct a new 1-dimensional array of data-type, typenum, that ranges from start to stop (exclusive) in increments of step . Equivalent to arange (start, stop, step, dtype)."]}, {"name": "PyObject *PyArray_ArangeObj()", "path": "reference/c-api/array#c.PyArray_ArangeObj", "type": "Array API", "text": ["Construct a new 1-dimensional array of data-type determined by descr, that ranges from start to stop (exclusive) in increments of step. Equivalent to arange( start, stop, step, typenum )."]}, {"name": "PyObject *PyArray_ArgMin()", "path": "reference/c-api/array#c.PyArray_ArgMin", "type": "Array API", "text": ["Equivalent to ndarray.argmin (self, axis). Return the index of the smallest element of self along axis."]}, {"name": "PyObject *PyArray_ArgPartition()", "path": "reference/c-api/array#c.PyArray_ArgPartition", "type": "Array API", "text": ["Equivalent to ndarray.argpartition (self, ktharray, axis, kind). Return an array of indices such that selection of these indices along the given axis would return a partitioned version of self."]}, {"name": "PyObject *PyArray_ArgSort()", "path": "reference/c-api/array#c.PyArray_ArgSort", "type": "Array API", "text": ["Equivalent to ndarray.argsort (self, axis). Return an array of indices such that selection of these indices along the given axis would return a sorted version of self. If self ->descr is a data-type with fields defined, then self->descr->names is used to determine the sort order. A comparison where the first field is equal will use the second field and so on. To alter the sort order of a structured array, create a new data-type with a different order of names and construct a view of the array with that new data-type."]}, {"name": "PyObject *PyArray_BASE()", "path": "reference/c-api/array#c.PyArray_BASE", "type": "Array API", "text": ["This returns the base object of the array. In most cases, this means the object which owns the memory the array is pointing at.", "If you are constructing an array using the C API, and specifying your own memory, you should use the function PyArray_SetBaseObject to set the base to an object which owns the memory.", "If the (deprecated) NPY_ARRAY_UPDATEIFCOPY or the NPY_ARRAY_WRITEBACKIFCOPY flags are set, it has a different meaning, namely base is the array into which the current array will be copied upon copy resolution. This overloading of the base property for two functions is likely to change in a future version of NumPy."]}, {"name": "PyObject *PyArray_BroadcastToShape()", "path": "reference/c-api/array#c.PyArray_BroadcastToShape", "type": "Array API", "text": ["Return an array iterator that is broadcast to iterate as an array of the shape provided by dimensions and nd."]}, {"name": "PyObject *PyArray_Byteswap()", "path": "reference/c-api/array#c.PyArray_Byteswap", "type": "Array API", "text": ["Equivalent to ndarray.byteswap (self, inplace). Return an array whose data area is byteswapped. If inplace is non-zero, then do the byteswap inplace and return a reference to self. Otherwise, create a byteswapped copy and leave self unchanged."]}, {"name": "PyObject *PyArray_CastToType()", "path": "reference/c-api/array#c.PyArray_CastToType", "type": "Array API", "text": ["Return a new array of the type specified, casting the elements of arr as appropriate. The fortran argument specifies the ordering of the output array."]}, {"name": "PyObject *PyArray_CheckAxis()", "path": "reference/c-api/array#c.PyArray_CheckAxis", "type": "Array API", "text": ["Encapsulate the functionality of functions and methods that take the axis= keyword and work properly with None as the axis argument. The input array is obj, while *axis is a converted integer (so that >=MAXDIMS is the None value), and requirements gives the needed properties of obj. The output is a converted version of the input so that requirements are met and if needed a flattening has occurred. On output negative values of *axis are converted and the new value is checked to ensure consistency with the shape of obj."]}, {"name": "PyObject *PyArray_CheckFromAny()", "path": "reference/c-api/array#c.PyArray_CheckFromAny", "type": "Array API", "text": ["Nearly identical to PyArray_FromAny (\u2026) except requirements can contain NPY_ARRAY_NOTSWAPPED (over-riding the specification in dtype) and NPY_ARRAY_ELEMENTSTRIDES which indicates that the array should be aligned in the sense that the strides are multiples of the element size.", "In versions 1.6 and earlier of NumPy, the following flags did not have the _ARRAY_ macro namespace in them. That form of the constant names is deprecated in 1.7."]}, {"name": "PyObject *PyArray_Choose()", "path": "reference/c-api/array#c.PyArray_Choose", "type": "Array API", "text": ["Equivalent to ndarray.choose (self, op, ret, clipmode). Create a new array by selecting elements from the sequence of arrays in op based on the integer values in self. The arrays must all be broadcastable to the same shape and the entries in self should be between 0 and len(op). The output is placed in ret unless it is NULL in which case a new output is created. The clipmode argument determines behavior for when entries in self are not between 0 and len(op).", "raise a ValueError;", "wrap values < 0 by adding len(op) and values >=len(op) by subtracting len(op) until they are in range;", "all values are clipped to the region [0, len(op) )."]}, {"name": "PyObject *PyArray_Clip()", "path": "reference/c-api/array#c.PyArray_Clip", "type": "Array API", "text": ["Equivalent to ndarray.clip (self, min, max). Clip an array, self, so that values larger than max are fixed to max and values less than min are fixed to min."]}, {"name": "PyObject *PyArray_Compress()", "path": "reference/c-api/array#c.PyArray_Compress", "type": "Array API", "text": ["Equivalent to ndarray.compress (self, condition, axis ). Return the elements along axis corresponding to elements of condition that are true."]}, {"name": "PyObject *PyArray_Concatenate()", "path": "reference/c-api/array#c.PyArray_Concatenate", "type": "Array API", "text": ["Join the sequence of objects in obj together along axis into a single array. If the dimensions or types are not compatible an error is raised."]}, {"name": "PyObject *PyArray_Conjugate()", "path": "reference/c-api/array#c.PyArray_Conjugate", "type": "Array API", "text": ["Equivalent to ndarray.conjugate (self). Return the complex conjugate of self. If self is not of complex data type, then return self with a reference."]}, {"name": "PyObject *PyArray_ContiguousFromAny()", "path": "reference/c-api/array#c.PyArray_ContiguousFromAny", "type": "Array API", "text": ["This function returns a (C-style) contiguous and behaved function array from any nested sequence or array interface exporting object, op, of (non-flexible) type given by the enumerated typenum, of minimum depth min_depth, and of maximum depth max_depth. Equivalent to a call to PyArray_FromAny with requirements set to NPY_ARRAY_DEFAULT and the type_num member of the type argument set to typenum."]}, {"name": "PyObject *PyArray_ContiguousFromObject()", "path": "reference/c-api/array#c.PyArray_ContiguousFromObject", "type": "Array API", "text": ["This function returns a well-behaved C-style contiguous array from any nested sequence or array-interface exporting object. The minimum number of dimensions the array can have is given by min_depth while the maximum is max_depth. This is equivalent to call PyArray_FromAny with requirements NPY_ARRAY_DEFAULT and NPY_ARRAY_ENSUREARRAY."]}, {"name": "PyObject *PyArray_CopyAndTranspose()", "path": "reference/c-api/array#c.PyArray_CopyAndTranspose", "type": "Array API", "text": ["A specialized copy and transpose function that works only for 2-d arrays. The returned array is a transposed copy of op."]}, {"name": "PyObject *PyArray_Correlate()", "path": "reference/c-api/array#c.PyArray_Correlate", "type": "Array API", "text": ["Compute the 1-d correlation of the 1-d arrays op1 and op2 . The correlation is computed at each output point by multiplying op1 by a shifted version of op2 and summing the result. As a result of the shift, needed values outside of the defined range of op1 and op2 are interpreted as zero. The mode determines how many shifts to return: 0 - return only shifts that did not need to assume zero- values; 1 - return an object that is the same size as op1, 2 - return all possible shifts (any overlap at all is accepted).", "This does not compute the usual correlation: if op2 is larger than op1, the arguments are swapped, and the conjugate is never taken for complex arrays. See PyArray_Correlate2 for the usual signal processing correlation."]}, {"name": "PyObject *PyArray_Correlate2()", "path": "reference/c-api/array#c.PyArray_Correlate2", "type": "Array API", "text": ["Updated version of PyArray_Correlate, which uses the usual definition of correlation for 1d arrays. The correlation is computed at each output point by multiplying op1 by a shifted version of op2 and summing the result. As a result of the shift, needed values outside of the defined range of op1 and op2 are interpreted as zero. The mode determines how many shifts to return: 0 - return only shifts that did not need to assume zero- values; 1 - return an object that is the same size as op1, 2 - return all possible shifts (any overlap at all is accepted).", "Compute z as follows:"]}, {"name": "PyObject *PyArray_CumProd()", "path": "reference/c-api/array#c.PyArray_CumProd", "type": "Array API", "text": ["Equivalent to ndarray.cumprod (self, axis, rtype). Return 1-d cumulative products of elements in self along axis. Perform the product after converting data to data type rtype."]}, {"name": "PyObject *PyArray_CumSum()", "path": "reference/c-api/array#c.PyArray_CumSum", "type": "Array API", "text": ["Equivalent to ndarray.cumsum (self, axis, rtype). Return cumulative 1-d sums of elements in self along axis. Perform the sum after converting data to data type rtype."]}, {"name": "PyObject *PyArray_Diagonal()", "path": "reference/c-api/array#c.PyArray_Diagonal", "type": "Array API", "text": ["Equivalent to ndarray.diagonal (self, offset, axis1, axis2 ). Return the offset diagonals of the 2-d arrays defined by axis1 and axis2."]}, {"name": "PyObject *PyArray_Dumps()", "path": "reference/c-api/array#c.PyArray_Dumps", "type": "Array API", "text": ["Pickle the object in self to a Python string and return it. Use the Pickle protocol provided (or the highest available if protocol is negative)."]}, {"name": "PyObject *PyArray_EinsteinSum()", "path": "reference/c-api/array#c.PyArray_EinsteinSum", "type": "Array API", "text": ["New in version 1.6.", "Applies the Einstein summation convention to the array operands provided, returning a new array or placing the result in out. The string in subscripts is a comma separated list of index letters. The number of operands is in nop, and op_in is an array containing those operands. The data type of the output can be forced with dtype, the output order can be forced with order (NPY_KEEPORDER is recommended), and when dtype is specified, casting indicates how permissive the data conversion should be.", "See the einsum function for more details."]}, {"name": "PyObject *PyArray_EMPTY()", "path": "reference/c-api/array#c.PyArray_EMPTY", "type": "Array API", "text": ["Macro form of PyArray_Empty which takes a type-number, typenum, instead of a data-type object."]}, {"name": "PyObject *PyArray_Empty()", "path": "reference/c-api/array#c.PyArray_Empty", "type": "Array API", "text": ["Construct a new nd -dimensional array with shape given by dims and data type given by dtype. If fortran is non-zero, then a Fortran-order array is created, otherwise a C-order array is created. The array is uninitialized unless the data type corresponds to NPY_OBJECT in which case the array is filled with Py_None."]}, {"name": "PyObject *PyArray_EnsureArray()", "path": "reference/c-api/array#c.PyArray_EnsureArray", "type": "Array API", "text": ["This function steals a reference to op and makes sure that op is a base-class ndarray. It special cases array scalars, but otherwise calls PyArray_FromAny ( op, NULL, 0, 0, NPY_ARRAY_ENSUREARRAY, NULL)."]}, {"name": "PyObject *PyArray_FieldNames()", "path": "reference/c-api/array#c.PyArray_FieldNames", "type": "Array API", "text": ["Take the fields dictionary, dict, such as the one attached to a data-type object and construct an ordered-list of field names such as is stored in the names field of the PyArray_Descr object."]}, {"name": "PyObject *PyArray_Flatten()", "path": "reference/c-api/array#c.PyArray_Flatten", "type": "Array API", "text": ["Equivalent to ndarray.flatten (self, order). Return a 1-d copy of the array. If order is NPY_FORTRANORDER the elements are scanned out in Fortran order (first-dimension varies the fastest). If order is NPY_CORDER, the elements of self are scanned in C-order (last dimension varies the fastest). If order NPY_ANYORDER, then the result of PyArray_ISFORTRAN (self) is used to determine which order to flatten."]}, {"name": "PyObject *PyArray_FROM_O()", "path": "reference/c-api/array#c.PyArray_FROM_O", "type": "Array API", "text": ["Convert obj to an ndarray. The argument can be any nested sequence or object that exports the array interface. This is a macro form of PyArray_FromAny using NULL, 0, 0, 0 for the other arguments. Your code must be able to handle any data-type descriptor and any combination of data-flags to use this macro."]}, {"name": "PyObject *PyArray_FROM_OF()", "path": "reference/c-api/array#c.PyArray_FROM_OF", "type": "Array API", "text": ["Similar to PyArray_FROM_O except it can take an argument of requirements indicating properties the resulting array must have. Available requirements that can be enforced are NPY_ARRAY_C_CONTIGUOUS, NPY_ARRAY_F_CONTIGUOUS, NPY_ARRAY_ALIGNED, NPY_ARRAY_WRITEABLE, NPY_ARRAY_NOTSWAPPED, NPY_ARRAY_ENSURECOPY, NPY_ARRAY_WRITEBACKIFCOPY, NPY_ARRAY_UPDATEIFCOPY, NPY_ARRAY_FORCECAST, and NPY_ARRAY_ENSUREARRAY. Standard combinations of flags can also be used:"]}, {"name": "PyObject *PyArray_FROM_OT()", "path": "reference/c-api/array#c.PyArray_FROM_OT", "type": "Array API", "text": ["Similar to PyArray_FROM_O except it can take an argument of typenum specifying the type-number the returned array."]}, {"name": "PyObject *PyArray_FROM_OTF()", "path": "reference/c-api/array#c.PyArray_FROM_OTF", "type": "Array API", "text": ["Combination of PyArray_FROM_OF and PyArray_FROM_OT allowing both a typenum and a flags argument to be provided."]}, {"name": "PyObject *PyArray_FROMANY()", "path": "reference/c-api/array#c.PyArray_FROMANY", "type": "Array API", "text": ["Similar to PyArray_FromAny except the data-type is specified using a typenumber. PyArray_DescrFromType (typenum) is passed directly to PyArray_FromAny. This macro also adds NPY_ARRAY_DEFAULT to requirements if NPY_ARRAY_ENSURECOPY is passed in as requirements."]}, {"name": "PyObject *PyArray_FromArray()", "path": "reference/c-api/array#c.PyArray_FromArray", "type": "Array API", "text": ["Special case of PyArray_FromAny for when op is already an array but it needs to be of a specific newtype (including byte-order) or has certain requirements."]}, {"name": "PyObject *PyArray_FromArrayAttr()", "path": "reference/c-api/array#c.PyArray_FromArrayAttr", "type": "Array API", "text": ["Return an ndarray object from a Python object that exposes the __array__ method. The __array__ method can take 0, or 1 argument ([dtype]). context is unused."]}, {"name": "PyObject *PyArray_FromBuffer()", "path": "reference/c-api/array#c.PyArray_FromBuffer", "type": "Array API", "text": ["Construct a one-dimensional ndarray of a single type from an object, buf, that exports the (single-segment) buffer protocol (or has an attribute __buffer__ that returns an object that exports the buffer protocol). A writeable buffer will be tried first followed by a read- only buffer. The NPY_ARRAY_WRITEABLE flag of the returned array will reflect which one was successful. The data is assumed to start at offset bytes from the start of the memory location for the object. The type of the data in the buffer will be interpreted depending on the data- type descriptor, dtype. If count is negative then it will be determined from the size of the buffer and the requested itemsize, otherwise, count represents how many elements should be converted from the buffer."]}, {"name": "PyObject *PyArray_FromFile()", "path": "reference/c-api/array#c.PyArray_FromFile", "type": "Array API", "text": ["Construct a one-dimensional ndarray of a single type from a binary or text file. The open file pointer is fp, the data-type of the array to be created is given by dtype. This must match the data in the file. If num is -1, then read until the end of the file and return an appropriately sized array, otherwise, num is the number of items to read. If sep is NULL (or \u201c\u201d), then read from the file in binary mode, otherwise read from the file in text mode with sep providing the item separator. Some array types cannot be read in text mode in which case an error is raised."]}, {"name": "PyObject *PyArray_FromInterface()", "path": "reference/c-api/array#c.PyArray_FromInterface", "type": "Array API", "text": ["Returns an ndarray object from a Python object that exposes the __array_interface__ attribute following the array interface protocol. If the object does not contain this attribute then a borrowed reference to Py_NotImplemented is returned."]}, {"name": "PyObject *PyArray_FromObject()", "path": "reference/c-api/array#c.PyArray_FromObject", "type": "Array API", "text": ["Return an aligned and in native-byteorder array from any nested sequence or array-interface exporting object, op, of a type given by the enumerated typenum. The minimum number of dimensions the array can have is given by min_depth while the maximum is max_depth. This is equivalent to a call to PyArray_FromAny with requirements set to BEHAVED."]}, {"name": "PyObject *PyArray_FromScalar()", "path": "reference/c-api/array#c.PyArray_FromScalar", "type": "Array API", "text": ["Return a 0-dimensional array of type determined by outcode from scalar which should be an array-scalar object. If outcode is NULL, then the type is determined from scalar."]}, {"name": "PyObject *PyArray_FromString()", "path": "reference/c-api/array#c.PyArray_FromString", "type": "Array API", "text": ["Construct a one-dimensional ndarray of a single type from a binary or (ASCII) text string of length slen. The data-type of the array to-be-created is given by dtype. If num is -1, then copy the entire string and return an appropriately sized array, otherwise, num is the number of items to copy from the string. If sep is NULL (or \u201c\u201d), then interpret the string as bytes of binary data, otherwise convert the sub-strings separated by sep to items of data-type dtype. Some data-types may not be readable in text mode and an error will be raised if that occurs. All errors return NULL."]}, {"name": "PyObject *PyArray_FromStructInterface()", "path": "reference/c-api/array#c.PyArray_FromStructInterface", "type": "Array API", "text": ["Returns an ndarray object from a Python object that exposes the __array_struct__ attribute and follows the array interface protocol. If the object does not contain this attribute then a borrowed reference to Py_NotImplemented is returned."]}, {"name": "PyObject *PyArray_GETITEM()", "path": "reference/c-api/array#c.PyArray_GETITEM", "type": "Array API", "text": ["Get a Python object of a builtin type from the ndarray, arr, at the location pointed to by itemptr. Return NULL on failure.", "numpy.ndarray.item is identical to PyArray_GETITEM."]}, {"name": "PyObject *PyArray_GetNumericOps()", "path": "reference/c-api/array#c.PyArray_GetNumericOps", "type": "Array API", "text": ["Return a Python dictionary containing the callable Python objects stored in the internal arithmetic operation table. The keys of this dictionary are given in the explanation for PyArray_SetNumericOps.", "Deprecated since version 1.16."]}, {"name": "PyObject *PyArray_InnerProduct()", "path": "reference/c-api/array#c.PyArray_InnerProduct", "type": "Array API", "text": ["Compute a product-sum over the last dimensions of obj1 and obj2. Neither array is conjugated."]}, {"name": "PyObject *PyArray_IterAllButAxis()", "path": "reference/c-api/array#c.PyArray_IterAllButAxis", "type": "Array API", "text": ["Return an array iterator that will iterate over all axes but the one provided in *axis. The returned iterator cannot be used with PyArray_ITER_GOTO1D. This iterator could be used to write something similar to what ufuncs do wherein the loop over the largest axis is done by a separate sub-routine. If *axis is negative then *axis will be set to the axis having the smallest stride and that axis will be used."]}, {"name": "PyObject *PyArray_LexSort()", "path": "reference/c-api/array#c.PyArray_LexSort", "type": "Array API", "text": ["Given a sequence of arrays (sort_keys) of the same shape, return an array of indices (similar to PyArray_ArgSort (\u2026)) that would sort the arrays lexicographically. A lexicographic sort specifies that when two keys are found to be equal, the order is based on comparison of subsequent keys. A merge sort (which leaves equal entries unmoved) is required to be defined for the types. The sort is accomplished by sorting the indices first using the first sort_key and then using the second sort_key and so forth. This is equivalent to the lexsort(sort_keys, axis) Python command. Because of the way the merge-sort works, be sure to understand the order the sort_keys must be in (reversed from the order you would use when comparing two elements).", "If these arrays are all collected in a structured array, then PyArray_Sort (\u2026) can also be used to sort the array directly."]}, {"name": "PyObject *PyArray_MapIterArrayCopyIfOverlap()", "path": "reference/c-api/array#c.PyArray_MapIterArrayCopyIfOverlap", "type": "Array API", "text": ["Similar to PyArray_MapIterArray but with an additional copy_if_overlap argument. If copy_if_overlap != 0, checks if a has memory overlap with any of the arrays in index and with extra_op, and make copies as appropriate to avoid problems if the input is modified during the iteration. iter->array may contain a copied array (UPDATEIFCOPY/WRITEBACKIFCOPY set)."]}, {"name": "PyObject *PyArray_MatrixProduct()", "path": "reference/c-api/array#c.PyArray_MatrixProduct", "type": "Array API", "text": ["Compute a product-sum over the last dimension of obj1 and the second-to-last dimension of obj2. For 2-d arrays this is a matrix-product. Neither array is conjugated."]}, {"name": "PyObject *PyArray_MatrixProduct2()", "path": "reference/c-api/array#c.PyArray_MatrixProduct2", "type": "Array API", "text": ["New in version 1.6.", "Same as PyArray_MatrixProduct, but store the result in out. The output array must have the correct shape, type, and be C-contiguous, or an exception is raised."]}, {"name": "PyObject *PyArray_Max()", "path": "reference/c-api/array#c.PyArray_Max", "type": "Array API", "text": ["Equivalent to ndarray.max (self, axis). Returns the largest element of self along the given axis. When the result is a single element, returns a numpy scalar instead of an ndarray."]}, {"name": "PyObject *PyArray_Mean()", "path": "reference/c-api/array#c.PyArray_Mean", "type": "Array API", "text": ["Equivalent to ndarray.mean (self, axis, rtype). Returns the mean of the elements along the given axis, using the enumerated type rtype as the data type to sum in. Default sum behavior is obtained using NPY_NOTYPE for rtype."]}, {"name": "PyObject *PyArray_Min()", "path": "reference/c-api/array#c.PyArray_Min", "type": "Array API", "text": ["Equivalent to ndarray.min (self, axis). Return the smallest element of self along the given axis. When the result is a single element, returns a numpy scalar instead of an ndarray."]}, {"name": "PyObject *PyArray_New()", "path": "reference/c-api/array#c.PyArray_New", "type": "Array API", "text": ["This is similar to PyArray_NewFromDescr (\u2026) except you specify the data-type descriptor with type_num and itemsize, where type_num corresponds to a builtin (or user-defined) type. If the type always has the same number of bytes, then itemsize is ignored. Otherwise, itemsize specifies the particular size of this array."]}, {"name": "PyObject *PyArray_NewCopy()", "path": "reference/c-api/array#c.PyArray_NewCopy", "type": "Array API", "text": ["Equivalent to ndarray.copy (self, fortran). Make a copy of the old array. The returned array is always aligned and writeable with data interpreted the same as the old array. If order is NPY_CORDER, then a C-style contiguous array is returned. If order is NPY_FORTRANORDER, then a Fortran-style contiguous array is returned. If order is NPY_ANYORDER, then the array returned is Fortran-style contiguous only if the old one is; otherwise, it is C-style contiguous."]}, {"name": "PyObject *PyArray_NewLikeArray()", "path": "reference/c-api/array#c.PyArray_NewLikeArray", "type": "Array API", "text": ["New in version 1.6.", "This function steals a reference to descr if it is not NULL. This array creation routine allows for the convenient creation of a new array matching an existing array\u2019s shapes and memory layout, possibly changing the layout and/or data type.", "When order is NPY_ANYORDER, the result order is NPY_FORTRANORDER if prototype is a fortran array, NPY_CORDER otherwise. When order is NPY_KEEPORDER, the result order matches that of prototype, even when the axes of prototype aren\u2019t in C or Fortran order.", "If descr is NULL, the data type of prototype is used.", "If subok is 1, the newly created array will use the sub-type of prototype to create the new array, otherwise it will create a base-class array."]}, {"name": "PyObject *PyArray_Nonzero()", "path": "reference/c-api/array#c.PyArray_Nonzero", "type": "Array API", "text": ["Equivalent to ndarray.nonzero (self). Returns a tuple of index arrays that select elements of self that are nonzero. If (nd= PyArray_NDIM ( self ))==1, then a single index array is returned. The index arrays have data type NPY_INTP. If a tuple is returned (nd \\(\\neq\\) 1), then its length is nd."]}, {"name": "PyObject *PyArray_Prod()", "path": "reference/c-api/array#c.PyArray_Prod", "type": "Array API", "text": ["Equivalent to ndarray.prod (self, axis, rtype). Return 1-d products of elements in self along axis. Perform the product after converting data to data type rtype."]}, {"name": "PyObject *PyArray_Ptp()", "path": "reference/c-api/array#c.PyArray_Ptp", "type": "Array API", "text": ["Equivalent to ndarray.ptp (self, axis). Return the difference between the largest element of self along axis and the smallest element of self along axis. When the result is a single element, returns a numpy scalar instead of an ndarray."]}, {"name": "PyObject *PyArray_PutMask()", "path": "reference/c-api/array#c.PyArray_PutMask", "type": "Array API", "text": ["Place the values in self wherever corresponding positions (using a flattened context) in mask are true. The mask and self arrays must have the same total number of elements. If values is too small, it will be repeated as necessary."]}, {"name": "PyObject *PyArray_PutTo()", "path": "reference/c-api/array#c.PyArray_PutTo", "type": "Array API", "text": ["Equivalent to self.put(values, indices, clipmode ). Put values into self at the corresponding (flattened) indices. If values is too small it will be repeated as necessary."]}, {"name": "PyObject *PyArray_Ravel()", "path": "reference/c-api/array#c.PyArray_Ravel", "type": "Array API", "text": ["Equivalent to self.ravel(order). Same basic functionality as PyArray_Flatten (self, order) except if order is 0 and self is C-style contiguous, the shape is altered but no copy is performed."]}, {"name": "PyObject *PyArray_Repeat()", "path": "reference/c-api/array#c.PyArray_Repeat", "type": "Array API", "text": ["Equivalent to ndarray.repeat (self, op, axis). Copy the elements of self, op times along the given axis. Either op is a scalar integer or a sequence of length self ->dimensions[ axis ] indicating how many times to repeat each item along the axis."]}, {"name": "PyObject *PyArray_Reshape()", "path": "reference/c-api/array#c.PyArray_Reshape", "type": "Array API", "text": ["Equivalent to ndarray.reshape (self, shape) where shape is a sequence. Converts shape to a PyArray_Dims structure and calls PyArray_Newshape internally. For back-ward compatibility \u2013 Not recommended"]}, {"name": "PyObject *PyArray_Resize()", "path": "reference/c-api/array#c.PyArray_Resize", "type": "Array API", "text": ["Equivalent to ndarray.resize (self, newshape, refcheck = refcheck, order= fortran ). This function only works on single-segment arrays. It changes the shape of self inplace and will reallocate the memory for self if newshape has a different total number of elements then the old shape. If reallocation is necessary, then self must own its data, have self - >base==NULL, have self - >weakrefs==NULL, and (unless refcheck is 0) not be referenced by any other array. The fortran argument can be NPY_ANYORDER, NPY_CORDER, or NPY_FORTRANORDER. It currently has no effect. Eventually it could be used to determine how the resize operation should view the data when constructing a differently-dimensioned array. Returns None on success and NULL on error."]}, {"name": "PyObject *PyArray_Round()", "path": "reference/c-api/array#c.PyArray_Round", "type": "Array API", "text": ["Equivalent to ndarray.round (self, decimals, out). Returns the array with elements rounded to the nearest decimal place. The decimal place is defined as the \\(10^{-\\textrm{decimals}}\\) digit so that negative decimals cause rounding to the nearest 10\u2019s, 100\u2019s, etc. If out is NULL, then the output array is created, otherwise the output is placed in out which must be the correct size and type."]}, {"name": "PyObject *PyArray_Scalar()", "path": "reference/c-api/array#c.PyArray_Scalar", "type": "Array API", "text": ["Return an array scalar object of the given dtype by copying from memory pointed to by data. base is expected to be the array object that is the owner of the data. base is required if dtype is a void scalar, or if the NPY_USE_GETITEM flag is set and it is known that the getitem method uses the arr argument without checking if it is NULL. Otherwise base may be NULL.", "If the data is not in native byte order (as indicated by dtype->byteorder) then this function will byteswap the data, because array scalars are always in correct machine-byte order."]}, {"name": "PyObject *PyArray_SearchSorted()", "path": "reference/c-api/array#c.PyArray_SearchSorted", "type": "Array API", "text": ["Equivalent to ndarray.searchsorted (self, values, side, perm). Assuming self is a 1-d array in ascending order, then the output is an array of indices the same shape as values such that, if the elements in values were inserted before the indices, the order of self would be preserved. No checking is done on whether or not self is in ascending order.", "The side argument indicates whether the index returned should be that of the first suitable location (if NPY_SEARCHLEFT) or of the last (if NPY_SEARCHRIGHT).", "The sorter argument, if not NULL, must be a 1D array of integer indices the same length as self, that sorts it into ascending order. This is typically the result of a call to PyArray_ArgSort (\u2026) Binary search is used to find the required insertion points."]}, {"name": "PyObject *PyArray_SimpleNew()", "path": "reference/c-api/array#c.PyArray_SimpleNew", "type": "Array API", "text": ["Create a new uninitialized array of type, typenum, whose size in each of nd dimensions is given by the integer array, dims.The memory for the array is uninitialized (unless typenum is NPY_OBJECT in which case each element in the array is set to NULL). The typenum argument allows specification of any of the builtin data-types such as NPY_FLOAT or NPY_LONG. The memory for the array can be set to zero if desired using PyArray_FILLWBYTE (return_object, 0).This function cannot be used to create a flexible-type array (no itemsize given)."]}, {"name": "PyObject *PyArray_SimpleNewFromData()", "path": "reference/c-api/array#c.PyArray_SimpleNewFromData", "type": "Array API", "text": ["Create an array wrapper around data pointed to by the given pointer. The array flags will have a default that the data area is well-behaved and C-style contiguous. The shape of the array is given by the dims c-array of length nd. The data-type of the array is indicated by typenum. If data comes from another reference-counted Python object, the reference count on this object should be increased after the pointer is passed in, and the base member of the returned ndarray should point to the Python object that owns the data. This will ensure that the provided memory is not freed while the returned array is in existence."]}, {"name": "PyObject *PyArray_SimpleNewFromDescr()", "path": "reference/c-api/array#c.PyArray_SimpleNewFromDescr", "type": "Array API", "text": ["This function steals a reference to descr.", "Create a new array with the provided data-type descriptor, descr, of the shape determined by nd and dims."]}, {"name": "PyObject *PyArray_Sort()", "path": "reference/c-api/array#c.PyArray_Sort", "type": "Array API", "text": ["Equivalent to ndarray.sort (self, axis, kind). Return an array with the items of self sorted along axis. The array is sorted using the algorithm denoted by kind, which is an integer/enum pointing to the type of sorting algorithms used."]}, {"name": "PyObject *PyArray_Squeeze()", "path": "reference/c-api/array#c.PyArray_Squeeze", "type": "Array API", "text": ["Equivalent to ndarray.squeeze (self). Return a new view of self with all of the dimensions of length 1 removed from the shape."]}, {"name": "PyObject *PyArray_Std()", "path": "reference/c-api/array#c.PyArray_Std", "type": "Array API", "text": ["Equivalent to ndarray.std (self, axis, rtype). Return the standard deviation using data along axis converted to data type rtype."]}, {"name": "PyObject *PyArray_Sum()", "path": "reference/c-api/array#c.PyArray_Sum", "type": "Array API", "text": ["Equivalent to ndarray.sum (self, axis, rtype). Return 1-d vector sums of elements in self along axis. Perform the sum after converting data to data type rtype."]}, {"name": "PyObject *PyArray_SwapAxes()", "path": "reference/c-api/array#c.PyArray_SwapAxes", "type": "Array API", "text": ["Equivalent to ndarray.swapaxes (self, a1, a2). The returned array is a new view of the data in self with the given axes, a1 and a2, swapped."]}, {"name": "PyObject *PyArray_ToFile()", "path": "reference/c-api/array#c.PyArray_ToFile", "type": "Array API", "text": ["Write the contents of self to the file pointer fp in C-style contiguous fashion. Write the data as binary bytes if sep is the string \u201c\u201dor NULL. Otherwise, write the contents of self as text using the sep string as the item separator. Each item will be printed to the file. If the format string is not NULL or \u201c\u201d, then it is a Python print statement format string showing how the items are to be written."]}, {"name": "PyObject *PyArray_ToList()", "path": "reference/c-api/array#c.PyArray_ToList", "type": "Array API", "text": ["Equivalent to ndarray.tolist (self). Return a nested Python list from self."]}, {"name": "PyObject *PyArray_ToScalar()", "path": "reference/c-api/array#c.PyArray_ToScalar", "type": "Array API", "text": ["Return an array scalar object of the type and itemsize indicated by the array object arr copied from the memory pointed to by data and swapping if the data in arr is not in machine byte-order."]}, {"name": "PyObject *PyArray_ToString()", "path": "reference/c-api/array#c.PyArray_ToString", "type": "Array API", "text": ["Equivalent to ndarray.tobytes (self, order). Return the bytes of this array in a Python string."]}, {"name": "PyObject *PyArray_Trace()", "path": "reference/c-api/array#c.PyArray_Trace", "type": "Array API", "text": ["Equivalent to ndarray.trace (self, offset, axis1, axis2, rtype). Return the sum (using rtype as the data type of summation) over the offset diagonal elements of the 2-d arrays defined by axis1 and axis2 variables. A positive offset chooses diagonals above the main diagonal. A negative offset selects diagonals below the main diagonal."]}, {"name": "PyObject *PyArray_Transpose()", "path": "reference/c-api/array#c.PyArray_Transpose", "type": "Array API", "text": ["Equivalent to ndarray.transpose (self, permute). Permute the axes of the ndarray object self according to the data structure permute and return the result. If permute is NULL, then the resulting array has its axes reversed. For example if self has shape \\(10\\times20\\times30\\), and permute .ptr is (0,2,1) the shape of the result is \\(10\\times30\\times20.\\) If permute is NULL, the shape of the result is \\(30\\times20\\times10.\\)"]}, {"name": "PyObject *PyArray_TypeObjectFromType()", "path": "reference/c-api/array#c.PyArray_TypeObjectFromType", "type": "Array API", "text": ["Returns a scalar type-object from a type-number, type . Equivalent to PyArray_DescrFromType (type)->typeobj except for reference counting and error-checking. Returns a new reference to the typeobject on success or NULL on failure."]}, {"name": "PyObject *PyArray_View()", "path": "reference/c-api/array#c.PyArray_View", "type": "Array API", "text": ["Equivalent to ndarray.view (self, dtype). Return a new view of the array self as possibly a different data-type, dtype, and different array subclass ptype.", "If dtype is NULL, then the returned array will have the same data type as self. The new data-type must be consistent with the size of self. Either the itemsizes must be identical, or self must be single-segment and the total number of bytes must be the same. In the latter case the dimensions of the returned array will be altered in the last (or first for Fortran-style contiguous arrays) dimension. The data area of the returned array and self is exactly the same."]}, {"name": "PyObject *PyArray_Where()", "path": "reference/c-api/array#c.PyArray_Where", "type": "Array API", "text": ["If both x and y are NULL, then return PyArray_Nonzero (condition). Otherwise, both x and y must be given and the object returned is shaped like condition and has elements of x and y where condition is respectively True or False."]}, {"name": "PyObject *PyArray_Zeros()", "path": "reference/c-api/array#c.PyArray_Zeros", "type": "Array API", "text": ["Construct a new nd -dimensional array with shape given by dims and data type given by dtype. If fortran is non-zero, then a Fortran-order array is created, otherwise a C-order array is created. Fill the memory with zeros (or the 0 object if dtype corresponds to NPY_OBJECT )."]}, {"name": "PyObject *PyArray_ZEROS()", "path": "reference/c-api/array#c.PyArray_ZEROS", "type": "Array API", "text": ["Macro form of PyArray_Zeros which takes a type-number instead of a data-type object."]}, {"name": "PyObject *PyDataMem_GetHandler()", "path": "reference/c-api/data_memory#c.PyDataMem_GetHandler", "type": "Memory management in NumPy", "text": ["Return the current policy that will be used to allocate data for the next PyArrayObject. On failure, return NULL."]}, {"name": "PyObject *PyDataMem_SetHandler()", "path": "reference/c-api/data_memory#c.PyDataMem_SetHandler", "type": "Memory management in NumPy", "text": ["Set a new allocation policy. If the input value is NULL, will reset the policy to the default. Return the previous policy, or return NULL if an error has occurred. We wrap the user-provided functions so they will still call the python and numpy memory management callback hooks."]}, {"name": "PyObject *PyUFunc_FromFuncAndDataAndSignature()", "path": "reference/c-api/ufunc#c.PyUFunc_FromFuncAndDataAndSignature", "type": "UFunc API", "text": ["This function is very similar to PyUFunc_FromFuncAndData above, but has an extra signature argument, to define a generalized universal functions. Similarly to how ufuncs are built around an element-by-element operation, gufuncs are around subarray-by-subarray operations, the signature defining the subarrays to operate on."]}, {"name": "PyObject *PyUFunc_FromFuncAndDataAndSignatureAndIdentity()", "path": "reference/c-api/ufunc#c.PyUFunc_FromFuncAndDataAndSignatureAndIdentity", "type": "UFunc API", "text": ["This function is very similar to PyUFunc_FromFuncAndDataAndSignature above, but has an extra identity_value argument, to define an arbitrary identity for the ufunc when identity is passed as PyUFunc_IdentityValue."]}, {"name": "PyObject *shape", "path": "reference/c-api/types-and-structures#c.NPY_USE_SETITEM.subarray.PyArray_ArrayDescr.shape", "type": "Python Types and C-Structures", "text": ["The shape (always C-style contiguous) of the sub-array as a Python tuple."]}, {"name": "PyObject *userloops", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.userloops", "type": "Python Types and C-Structures", "text": ["A dictionary of user-defined 1-d vector loops (stored as CObject ptrs) for user-defined types. A loop may be registered by the user for any user-defined type. It is retrieved by type number. User defined type numbers are always larger than NPY_USERDEF."]}, {"name": "PyObject *weakreflist", "path": "reference/c-api/types-and-structures#c.NPY_AO.weakreflist", "type": "Python Types and C-Structures", "text": ["This member allows array objects to have weak references (using the weakref module)."]}, {"name": "PyTypeObject PyArray_Type", "path": "reference/c-api/types-and-structures", "type": "Python Types and C-Structures", "text": ["Several new types are defined in the C-code. Most of these are accessible from Python, but a few are not exposed due to their limited use. Every new Python type has an associated PyObject* with an internal structure that includes a pointer to a \u201cmethod table\u201d that defines how the new object behaves in Python. When you receive a Python object into C code, you always get a pointer to a PyObject structure. Because a PyObject structure is very generic and defines only PyObject_HEAD, by itself it is not very interesting. However, different objects contain more details after the PyObject_HEAD (but you have to cast to the correct type to access them \u2014 or use accessor functions or macros).", "Python types are the functional equivalent in C of classes in Python. By constructing a new Python type you make available a new object for Python. The ndarray object is an example of a new type defined in C. New types are defined in C by two basic steps:", "Instead of special method names which define behavior for Python classes, there are \u201cfunction tables\u201d which point to functions that implement the desired results. Since Python 2.2, the PyTypeObject itself has become dynamic which allows C types that can be \u201csub-typed \u201cfrom other C-types in C, and sub-classed in Python. The children types inherit the attributes and methods from their parent(s).", "There are two major new types: the ndarray ( PyArray_Type ) and the ufunc ( PyUFunc_Type ). Additional types play a supportive role: the PyArrayIter_Type, the PyArrayMultiIter_Type, and the PyArrayDescr_Type . The PyArrayIter_Type is the type for a flat iterator for an ndarray (the object that is returned when getting the flat attribute). The PyArrayMultiIter_Type is the type of the object returned when calling broadcast (). It handles iteration and broadcasting over a collection of nested sequences. Also, the PyArrayDescr_Type is the data-type-descriptor type whose instances describe the data. Finally, there are 21 new scalar-array types which are new Python scalars corresponding to each of the fundamental data types available for arrays. An additional 10 other types are place holders that allow the array scalars to fit into a hierarchy of actual Python types.", "The Python type of the ndarray is PyArray_Type. In C, every ndarray is a pointer to a PyArrayObject structure. The ob_type member of this structure contains a pointer to the PyArray_Type typeobject.", "The PyArrayObject C-structure contains all of the required information for an array. All instances of an ndarray (and its subclasses) will have this structure. For future compatibility, these structure members should normally be accessed using the provided macros. If you need a shorter name, then you can make use of NPY_AO (deprecated) which is defined to be equivalent to PyArrayObject. Direct access to the struct fields are deprecated. Use the PyArray_*(arr) form instead. As of NumPy 1.20, the size of this struct is not considered part of the NumPy ABI (see note at the end of the member list).", "This is needed by all Python objects. It consists of (at least) a reference count member ( ob_refcnt ) and a pointer to the typeobject ( ob_type ). (Other elements may also be present if Python was compiled with special options see Include/object.h in the Python source tree for more information). The ob_type member points to a Python type object.", "Accessible via PyArray_DATA, this data member is a pointer to the first element of the array. This pointer can (and normally should) be recast to the data type of the array.", "An integer providing the number of dimensions for this array. When nd is 0, the array is sometimes called a rank-0 array. Such arrays have undefined dimensions and strides and cannot be accessed. Macro PyArray_NDIM defined in ndarraytypes.h points to this data member. NPY_MAXDIMS is the largest number of dimensions for any array.", "An array of integers providing the shape in each dimension as long as nd \\(\\geq\\) 1. The integer is always large enough to hold a pointer on the platform, so the dimension size is only limited by memory. PyArray_DIMS is the macro associated with this data member.", "An array of integers providing for each dimension the number of bytes that must be skipped to get to the next element in that dimension. Associated with macro PyArray_STRIDES.", "Pointed to by PyArray_BASE, this member is used to hold a pointer to another Python object that is related to this array. There are two use cases:", "When PyArray_ResolveWritebackIfCopy is called, the array pointed to by base will be updated with the contents of this array.", "A pointer to a data-type descriptor object (see below). The data-type descriptor object is an instance of a new built-in type which allows a generic description of memory. There is a descriptor structure for each data type supported. This descriptor structure contains useful information about the type as well as a pointer to a table of function pointers to implement specific functionality. As the name suggests, it is associated with the macro PyArray_DESCR.", "Pointed to by the macro PyArray_FLAGS, this data member represents the flags indicating how the memory pointed to by data is to be interpreted. Possible flags are NPY_ARRAY_C_CONTIGUOUS, NPY_ARRAY_F_CONTIGUOUS, NPY_ARRAY_OWNDATA, NPY_ARRAY_ALIGNED, NPY_ARRAY_WRITEABLE, NPY_ARRAY_WRITEBACKIFCOPY, and NPY_ARRAY_UPDATEIFCOPY.", "This member allows array objects to have weak references (using the weakref module).", "Note", "Further members are considered private and version dependent. If the size of the struct is important for your code, special care must be taken. A possible use-case when this is relevant is subclassing in C. If your code relies on sizeof(PyArrayObject) to be constant, you must add the following check at import time:", "To ensure that your code does not have to be compiled for a specific NumPy version, you may add a constant, leaving room for changes in NumPy. A solution guaranteed to be compatible with any future NumPy version requires the use of a runtime calculate offset and allocation size.", "The PyArrayDescr_Type is the built-in type of the data-type-descriptor objects used to describe how the bytes comprising the array are to be interpreted. There are 21 statically-defined PyArray_Descr objects for the built-in data-types. While these participate in reference counting, their reference count should never reach zero. There is also a dynamic table of user-defined PyArray_Descr objects that is also maintained. Once a data-type-descriptor object is \u201cregistered\u201d it should never be deallocated either. The function PyArray_DescrFromType (\u2026) can be used to retrieve a PyArray_Descr object from an enumerated type-number (either built-in or user- defined).", "The PyArray_Descr structure lies at the heart of the PyArrayDescr_Type. While it is described here for completeness, it should be considered internal to NumPy and manipulated via PyArrayDescr_* or PyDataType* functions and macros. The size of this structure is subject to change across versions of NumPy. To ensure compatibility:", "It has the following structure:", "Pointer to a typeobject that is the corresponding Python type for the elements of this array. For the builtin types, this points to the corresponding array scalar. For user-defined types, this should point to a user-defined typeobject. This typeobject can either inherit from array scalars or not. If it does not inherit from array scalars, then the NPY_USE_GETITEM and NPY_USE_SETITEM flags should be set in the flags member.", "A character code indicating the kind of array (using the array interface typestring notation). A \u2018b\u2019 represents Boolean, a \u2018i\u2019 represents signed integer, a \u2018u\u2019 represents unsigned integer, \u2018f\u2019 represents floating point, \u2018c\u2019 represents complex floating point, \u2018S\u2019 represents 8-bit zero-terminated bytes, \u2018U\u2019 represents 32-bit/character unicode string, and \u2018V\u2019 represents arbitrary.", "A traditional character code indicating the data type.", "A character indicating the byte-order: \u2018>\u2019 (big-endian), \u2018<\u2019 (little- endian), \u2018=\u2019 (native), \u2018|\u2019 (irrelevant, ignore). All builtin data- types have byteorder \u2018=\u2019.", "A data-type bit-flag that determines if the data-type exhibits object- array like behavior. Each bit in this member is a flag which are named as:", "Indicates that items of this data-type must be reference counted (using Py_INCREF and Py_DECREF ).", "Same as NPY_ITEM_REFCOUNT.", "Indicates arrays of this data-type must be converted to a list before pickling.", "Indicates the item is a pointer to some other data-type", "Indicates memory for this data-type must be initialized (set to 0) on creation.", "Indicates this data-type requires the Python C-API during access (so don\u2019t give up the GIL if array access is going to be needed).", "On array access use the f->getitem function pointer instead of the standard conversion to an array scalar. Must use if you don\u2019t define an array scalar to go along with the data-type.", "When creating a 0-d array from an array scalar use f->setitem instead of the standard copy from an array scalar. Must use if you don\u2019t define an array scalar to go along with the data-type.", "The bits that are inherited for the parent data-type if these bits are set in any field of the data-type. Currently ( NPY_NEEDS_INIT | NPY_LIST_PICKLE | NPY_ITEM_REFCOUNT | NPY_NEEDS_PYAPI ).", "Bits set for the object data-type: ( NPY_LIST_PICKLE | NPY_USE_GETITEM | NPY_ITEM_IS_POINTER | NPY_ITEM_REFCOUNT | NPY_NEEDS_INIT | NPY_NEEDS_PYAPI).", "Return true if all the given flags are set for the data-type object.", "Equivalent to PyDataType_FLAGCHK (dtype, NPY_ITEM_REFCOUNT).", "A number that uniquely identifies the data type. For new data-types, this number is assigned when the data-type is registered.", "For data types that are always the same size (such as long), this holds the size of the data type. For flexible data types where different arrays can have a different elementsize, this should be 0.", "A number providing alignment information for this data type. Specifically, it shows how far from the start of a 2-element structure (whose first element is a char ), the compiler places an item of this type: offsetof(struct {char c; type v;},\nv)", "If this is non- NULL, then this data-type descriptor is a C-style contiguous array of another data-type descriptor. In other-words, each element that this descriptor describes is actually an array of some other base descriptor. This is most useful as the data-type descriptor for a field in another data-type descriptor. The fields member should be NULL if this is non- NULL (the fields member of the base descriptor can be non- NULL however).", "The data-type-descriptor object of the base-type.", "The shape (always C-style contiguous) of the sub-array as a Python tuple.", "If this is non-NULL, then this data-type-descriptor has fields described by a Python dictionary whose keys are names (and also titles if given) and whose values are tuples that describe the fields. Recall that a data-type-descriptor always describes a fixed-length set of bytes. A field is a named sub-region of that total, fixed-length collection. A field is described by a tuple composed of another data- type-descriptor and a byte offset. Optionally, the tuple may contain a title which is normally a Python string. These tuples are placed in this dictionary keyed by name (and also title if given).", "An ordered tuple of field names. It is NULL if no field is defined.", "A pointer to a structure containing functions that the type needs to implement internal features. These functions are not the same thing as the universal functions (ufuncs) described later. Their signatures can vary arbitrarily.", "Metadata about this dtype.", "Metadata specific to the C implementation of the particular dtype. Added for NumPy 1.7.0.", "Currently unused. Reserved for future use in caching hash values.", "Functions implementing internal features. Not all of these function pointers must be defined for a given type. The required members are nonzero, copyswap, copyswapn, setitem, getitem, and cast. These are assumed to be non- NULL and NULL entries will cause a program crash. The other functions may be NULL which will just mean reduced functionality for that data-type. (Also, the nonzero function will be filled in with a default function if it is NULL when you register a user-defined data-type).", "The concept of a behaved segment is used in the description of the function pointers. A behaved segment is one that is aligned and in native machine byte-order for the data-type. The nonzero, copyswap, copyswapn, getitem, and setitem functions can (and must) deal with mis-behaved arrays. The other functions require behaved memory segments.", "An array of function pointers to cast from the current type to all of the other builtin types. Each function casts a contiguous, aligned, and notswapped buffer pointed at by from to a contiguous, aligned, and notswapped buffer pointed at by to The number of items to cast is given by n, and the arguments fromarr and toarr are interpreted as PyArrayObjects for flexible arrays to get itemsize information.", "A pointer to a function that returns a standard Python object from a single element of the array object arr pointed to by data. This function must be able to deal with \u201cmisbehaved \u201c(misaligned and/or swapped) arrays correctly.", "A pointer to a function that sets the Python object item into the array, arr, at the position pointed to by data . This function deals with \u201cmisbehaved\u201d arrays. If successful, a zero is returned, otherwise, a negative one is returned (and a Python error set).", "These members are both pointers to functions to copy data from src to dest and swap if indicated. The value of arr is only used for flexible ( NPY_STRING, NPY_UNICODE, and NPY_VOID ) arrays (and is obtained from arr->descr->elsize ). The second function copies a single value, while the first loops over n values with the provided strides. These functions can deal with misbehaved src data. If src is NULL then no copy is performed. If swap is 0, then no byteswapping occurs. It is assumed that dest and src do not overlap. If they overlap, then use memmove (\u2026) first followed by copyswap(n) with NULL valued src.", "A pointer to a function that compares two elements of the array, arr, pointed to by d1 and d2. This function requires behaved (aligned and not swapped) arrays. The return value is 1 if * d1 > * d2, 0 if * d1 == * d2, and -1 if * d1 < * d2. The array object arr is used to retrieve itemsize and field information for flexible arrays.", "A pointer to a function that retrieves the index of the largest of n elements in arr beginning at the element pointed to by data. This function requires that the memory segment be contiguous and behaved. The return value is always 0. The index of the largest element is returned in max_ind.", "A pointer to a function that multiplies two n -length sequences together, adds them, and places the result in element pointed to by op of arr. The start of the two sequences are pointed to by ip1 and ip2. To get to the next element in each sequence requires a jump of is1 and is2 bytes, respectively. This function requires behaved (though not necessarily contiguous) memory.", "A pointer to a function that scans (scanf style) one element of the corresponding type from the file descriptor fd into the array memory pointed to by ip. The array is assumed to be behaved. The last argument arr is the array to be scanned into. Returns number of receiving arguments successfully assigned (which may be zero in case a matching failure occurred before the first receiving argument was assigned), or EOF if input failure occurs before the first receiving argument was assigned. This function should be called without holding the Python GIL, and has to grab it for error reporting.", "A pointer to a function that converts the string pointed to by str to one element of the corresponding type and places it in the memory location pointed to by ip. After the conversion is completed, *endptr points to the rest of the string. The last argument arr is the array into which ip points (needed for variable-size data- types). Returns 0 on success or -1 on failure. Requires a behaved array. This function should be called without holding the Python GIL, and has to grab it for error reporting.", "A pointer to a function that returns TRUE if the item of arr pointed to by data is nonzero. This function can deal with misbehaved arrays.", "A pointer to a function that fills a contiguous array of given length with data. The first two elements of the array must already be filled- in. From these two values, a delta will be computed and the values from item 3 to the end will be computed by repeatedly adding this computed delta. The data buffer must be well-behaved.", "A pointer to a function that fills a contiguous buffer of the given length with a single scalar value whose address is given. The final argument is the array which is needed to get the itemsize for variable-length arrays.", "An array of function pointers to a particular sorting algorithms. A particular sorting algorithm is obtained using a key (so far NPY_QUICKSORT, NPY_HEAPSORT, and NPY_MERGESORT are defined). These sorts are done in-place assuming contiguous and aligned data.", "An array of function pointers to sorting algorithms for this data type. The same sorting algorithms as for sort are available. The indices producing the sort are returned in result (which must be initialized with indices 0 to length-1 inclusive).", "Either NULL or a dictionary containing low-level casting functions for user- defined data-types. Each function is wrapped in a PyCapsule* and keyed by the data-type number.", "A function to determine how scalars of this type should be interpreted. The argument is NULL or a 0-dimensional array containing the data (if that is needed to determine the kind of scalar). The return value must be of type NPY_SCALARKIND.", "Either NULL or an array of NPY_NSCALARKINDS pointers. These pointers should each be either NULL or a pointer to an array of integers (terminated by NPY_NOTYPE) indicating data-types that a scalar of this data-type of the specified kind can be cast to safely (this usually means without losing precision).", "Either NULL or an array of integers (terminated by NPY_NOTYPE ) indicated data-types that this data-type can be cast to safely (this usually means without losing precision).", "Deprecated since version 1.17: The use of this function will give a deprecation warning when np.clip. Instead of this function, the datatype must instead use PyUFunc_RegisterLoopForDescr to attach a custom loop to np.core.umath.clip, np.minimum, and np.maximum.", "Deprecated since version 1.19: Setting this function is deprecated and should always be NULL, if set, it will be ignored.", "A function that reads n_in items from in, and writes to out the read value if it is within the limits pointed to by min and max, or the corresponding limit if outside. The memory segments must be contiguous and behaved, and either min or max may be NULL, but not both.", "Deprecated since version 1.19: Setting this function is deprecated and should always be NULL, if set, it will be ignored.", "A function that takes a pointer in to an array of n_in items, a pointer mask to an array of n_in boolean values, and a pointer vals to an array of nv items. Items from vals are copied into in wherever the value in mask is non-zero, tiling vals as needed if nv < n_in. All arrays must be contiguous and behaved.", "Deprecated since version 1.19: Setting this function is deprecated and should always be NULL, if set, it will be ignored.", "A function that takes a pointer src to a C contiguous, behaved segment, interpreted as a 3-dimensional array of shape (n_outer, nindarray, nelem), a pointer indarray to a contiguous, behaved segment of m_middle integer indices, and a pointer dest to a C contiguous, behaved segment, interpreted as a 3-dimensional array of shape (n_outer, m_middle, nelem). The indices in indarray are used to index src along the second dimension, and copy the corresponding chunks of nelem items into dest. clipmode (which can take on the values NPY_RAISE, NPY_WRAP or NPY_CLIP) determines how will indices smaller than 0 or larger than nindarray will be handled.", "A pointer to a function that retrieves the index of the smallest of n elements in arr beginning at the element pointed to by data. This function requires that the memory segment be contiguous and behaved. The return value is always 0. The index of the smallest element is returned in min_ind.", "The PyArray_Type typeobject implements many of the features of Python objects including the tp_as_number, tp_as_sequence, tp_as_mapping, and tp_as_buffer interfaces. The rich comparison) is also used along with new-style attribute lookup for member (tp_members) and properties (tp_getset). The PyArray_Type can also be sub-typed.", "Tip", "The tp_as_number methods use a generic approach to call whatever function has been registered for handling the operation. When the _multiarray_umath module is imported, it sets the numeric operations for all arrays to the corresponding ufuncs. This choice can be changed with PyUFunc_ReplaceLoopBySignature The tp_str and tp_repr methods can also be altered using PyArray_SetStringFunction.", "The ufunc object is implemented by creation of the PyUFunc_Type. It is a very simple type that implements only basic getattribute behavior, printing behavior, and has call behavior which allows these objects to act like functions. The basic idea behind the ufunc is to hold a reference to fast 1-dimensional (vector) loops for each data type that supports the operation. These one-dimensional loops all have the same signature and are the key to creating a new ufunc. They are called by the generic looping code as appropriate to implement the N-dimensional function. There are also some generic 1-d loops defined for floating and complexfloating arrays that allow you to define a ufunc using a single scalar function (e.g. atanh).", "The core of the ufunc is the PyUFuncObject which contains all the information needed to call the underlying C-code loops that perform the actual work. While it is described here for completeness, it should be considered internal to NumPy and manipulated via PyUFunc_* functions. The size of this structure is subject to change across versions of NumPy. To ensure compatibility:", "It has the following structure:", "The number of input arguments.", "The number of output arguments.", "The total number of arguments (nin + nout). This must be less than NPY_MAXARGS.", "Either PyUFunc_One, PyUFunc_Zero, PyUFunc_MinusOne, PyUFunc_None, PyUFunc_ReorderableNone, or PyUFunc_IdentityValue to indicate the identity for this operation. It is only used for a reduce-like call on an empty array.", "An array of function pointers \u2014 one for each data type supported by the ufunc. This is the vector loop that is called to implement the underlying function dims [0] times. The first argument, args, is an array of nargs pointers to behaved memory. Pointers to the data for the input arguments are first, followed by the pointers to the data for the output arguments. How many bytes must be skipped to get to the next element in the sequence is specified by the corresponding entry in the steps array. The last argument allows the loop to receive extra information. This is commonly used so that a single, generic vector loop can be used for multiple functions. In this case, the actual scalar function to call is passed in as extradata. The size of this function pointer array is ntypes.", "Extra data to be passed to the 1-d vector loops or NULL if no extra-data is needed. This C-array must be the same size ( i.e. ntypes) as the functions array. NULL is used if extra_data is not needed. Several C-API calls for UFuncs are just 1-d vector loops that make use of this extra data to receive a pointer to the actual function to call.", "The number of supported data types for the ufunc. This number specifies how many different 1-d loops (of the builtin data types) are available.", "Unused.", "A string name for the ufunc. This is used dynamically to build the __doc__ attribute of ufuncs.", "An array of \\(nargs \\times ntypes\\) 8-bit type_numbers which contains the type signature for the function for each of the supported (builtin) data types. For each of the ntypes functions, the corresponding set of type numbers in this array shows how the args argument should be interpreted in the 1-d vector loop. These type numbers do not have to be the same type and mixed-type ufuncs are supported.", "Documentation for the ufunc. Should not contain the function signature as this is generated dynamically when __doc__ is retrieved.", "Any dynamically allocated memory. Currently, this is used for dynamic ufuncs created from a python function to store room for the types, data, and name members.", "For ufuncs dynamically created from python functions, this member holds a reference to the underlying Python function.", "A dictionary of user-defined 1-d vector loops (stored as CObject ptrs) for user-defined types. A loop may be registered by the user for any user-defined type. It is retrieved by type number. User defined type numbers are always larger than NPY_USERDEF.", "0 for scalar ufuncs; 1 for generalized ufuncs", "Number of distinct core dimension names in the signature", "Number of core dimensions of each argument", "Dimension indices in a flattened form; indices of argument k are stored in core_dim_ixs[core_offsets[k] : core_offsets[k] +\ncore_numdims[k]]", "Position of 1st core dimension of each argument in core_dim_ixs, equivalent to cumsum(core_num_dims)", "Core signature string", "A function which resolves the types and fills an array with the dtypes for the inputs and outputs", "Deprecated since version 1.22: Some fallback support for this slot exists, but will be removed eventually. A universal function that relied on this will have to be ported eventually. See ref:NEP 41 and ref:NEP 43", "For a possible future loop selector with a different signature.", "Override the default operand flags for each ufunc operand.", "Override the default nditer flags for the ufunc.", "Added in API version 0x0000000D", "For each distinct core dimension, the possible frozen size if UFUNC_CORE_DIM_SIZE_INFERRED is 0", "For each distinct core dimension, a set of UFUNC_CORE_DIM* flags", "if the dim name ends in ?", "if the dim size will be determined from the operands and not from a frozen signature", "Identity for reduction, when PyUFuncObject.identity is equal to PyUFunc_IdentityValue.", "This is an iterator object that makes it easy to loop over an N-dimensional array. It is the object returned from the flat attribute of an ndarray. It is also used extensively throughout the implementation internals to loop over an N-dimensional array. The tp_as_mapping interface is implemented so that the iterator object can be indexed (using 1-d indexing), and a few methods are implemented through the tp_methods table. This object implements the next method and can be used anywhere an iterator can be used in Python.", "The C-structure corresponding to an object of PyArrayIter_Type is the PyArrayIterObject. The PyArrayIterObject is used to keep track of a pointer into an N-dimensional array. It contains associated information used to quickly march through the array. The pointer can be adjusted in three basic ways: 1) advance to the \u201cnext\u201d position in the array in a C-style contiguous fashion, 2) advance to an arbitrary N-dimensional coordinate in the array, and 3) advance to an arbitrary one-dimensional index into the array. The members of the PyArrayIterObject structure are used in these calculations. Iterator objects keep their own dimension and strides information about an array. This can be adjusted as needed for \u201cbroadcasting,\u201d or to loop over only specific dimensions.", "\\(N-1\\) where \\(N\\) is the number of dimensions in the underlying array.", "The current 1-d index into the array.", "The total size of the underlying array.", "An \\(N\\) -dimensional index into the array.", "The size of the array minus 1 in each dimension.", "The strides of the array. How many bytes needed to jump to the next element in each dimension.", "How many bytes needed to jump from the end of a dimension back to its beginning. Note that backstrides[k] == strides[k] *\ndims_m1[k], but it is stored here as an optimization.", "This array is used in computing an N-d index from a 1-d index. It contains needed products of the dimensions.", "A pointer to the underlying ndarray this iterator was created to represent.", "This member points to an element in the ndarray indicated by the index.", "This flag is true if the underlying array is NPY_ARRAY_C_CONTIGUOUS. It is used to simplify calculations when possible.", "How to use an array iterator on a C-level is explained more fully in later sections. Typically, you do not need to concern yourself with the internal structure of the iterator object, and merely interact with it through the use of the macros PyArray_ITER_NEXT (it), PyArray_ITER_GOTO (it, dest), or PyArray_ITER_GOTO1D (it, index). All of these macros require the argument it to be a PyArrayIterObject*.", "This type provides an iterator that encapsulates the concept of broadcasting. It allows \\(N\\) arrays to be broadcast together so that the loop progresses in C-style contiguous fashion over the broadcasted array. The corresponding C-structure is the PyArrayMultiIterObject whose memory layout must begin any object, obj, passed in to the PyArray_Broadcast (obj) function. Broadcasting is performed by adjusting array iterators so that each iterator represents the broadcasted shape and size, but has its strides adjusted so that the correct element from the array is used at each iteration.", "The number of arrays that need to be broadcast to the same shape.", "The total broadcasted size.", "The current (1-d) index into the broadcasted result.", "The number of dimensions in the broadcasted result.", "The shape of the broadcasted result (only nd slots are used).", "An array of iterator objects that holds the iterators for the arrays to be broadcast together. On return, the iterators are adjusted for broadcasting.", "This is an iterator object that makes it easy to loop over an N-dimensional neighborhood.", "The C-structure corresponding to an object of PyArrayNeighborhoodIter_Type is the PyArrayNeighborhoodIterObject.", "When the flags attribute is retrieved from Python, a special builtin object of this type is constructed. This special type makes it easier to work with the different flags by accessing them as attributes or by accessing them as if the object were a dictionary with the flag names as entries.", "There is a Python type for each of the different built-in data types that can be present in the array Most of these are simple wrappers around the corresponding data type in C. The C-names for these types are Py{TYPE}ArrType_Type where {TYPE} can be", "Bool, Byte, Short, Int, Long, LongLong, UByte, UShort, UInt, ULong, ULongLong, Half, Float, Double, LongDouble, CFloat, CDouble, CLongDouble, String, Unicode, Void, and Object.", "These type names are part of the C-API and can therefore be created in extension C-code. There is also a PyIntpArrType_Type and a PyUIntpArrType_Type that are simple substitutes for one of the integer types that can hold a pointer on the platform. The structure of these scalar objects is not exposed to C-code. The function PyArray_ScalarAsCtype (..) can be used to extract the C-type value from the array scalar and the function PyArray_Scalar (\u2026) can be used to construct an array scalar from a C-value.", "A few new C-structures were found to be useful in the development of NumPy. These C-structures are used in at least one C-API call and are therefore documented here. The main reason these structures were defined is to make it easy to use the Python ParseTuple C-API to convert from Python objects to a useful C-Object.", "This structure is very useful when shape and/or strides information is supposed to be interpreted. The structure is:", "The members of this structure are", "A pointer to a list of (npy_intp) integers which usually represent array shape or array strides.", "The length of the list of integers. It is assumed safe to access ptr [0] to ptr [len-1].", "This is equivalent to the buffer object structure in Python up to the ptr member. On 32-bit platforms (i.e. if NPY_SIZEOF_INT == NPY_SIZEOF_INTP), the len member also matches an equivalent member of the buffer object. It is useful to represent a generic single-segment chunk of memory.", "The members are", "The Python object this chunk of memory comes from. Needed so that memory can be accounted for properly.", "A pointer to the start of the single-segment chunk of memory.", "The length of the segment in bytes.", "Any data flags (e.g. NPY_ARRAY_WRITEABLE ) that should be used to interpret the memory.", "See also", "The Array Interface", "The PyArrayInterface structure is defined so that NumPy and other extension modules can use the rapid array interface protocol. The __array_struct__ method of an object that supports the rapid array interface protocol should return a PyCapsule that contains a pointer to a PyArrayInterface structure with the relevant details of the array. After the new array is created, the attribute should be DECREF\u2019d which will free the PyArrayInterface structure. Remember to INCREF the object (whose __array_struct__ attribute was retrieved) and point the base member of the new PyArrayObject to this same object. In this way the memory for the array will be managed correctly.", "the integer 2 as a sanity check.", "the number of dimensions in the array.", "A character indicating what kind of array is present according to the typestring convention with \u2018t\u2019 -> bitfield, \u2018b\u2019 -> Boolean, \u2018i\u2019 -> signed integer, \u2018u\u2019 -> unsigned integer, \u2018f\u2019 -> floating point, \u2018c\u2019 -> complex floating point, \u2018O\u2019 -> object, \u2018S\u2019 -> (byte-)string, \u2018U\u2019 -> unicode, \u2018V\u2019 -> void.", "The number of bytes each item in the array requires.", "Any of the bits NPY_ARRAY_C_CONTIGUOUS (1), NPY_ARRAY_F_CONTIGUOUS (2), NPY_ARRAY_ALIGNED (0x100), NPY_ARRAY_NOTSWAPPED (0x200), or NPY_ARRAY_WRITEABLE (0x400) to indicate something about the data. The NPY_ARRAY_ALIGNED, NPY_ARRAY_C_CONTIGUOUS, and NPY_ARRAY_F_CONTIGUOUS flags can actually be determined from the other parameters. The flag NPY_ARR_HAS_DESCR (0x800) can also be set to indicate to objects consuming the version 3 array interface that the descr member of the structure is present (it will be ignored by objects consuming version 2 of the array interface).", "An array containing the size of the array in each dimension.", "An array containing the number of bytes to jump to get to the next element in each dimension.", "A pointer to the first element of the array.", "A Python object describing the data-type in more detail (same as the descr key in __array_interface__). This can be NULL if typekind and itemsize provide enough information. This field is also ignored unless NPY_ARR_HAS_DESCR flag is on in flags.", "Internally, the code uses some additional Python objects primarily for memory management. These types are not accessible directly from Python, and are not exposed to the C-API. They are included here only for completeness and assistance in understanding the code.", "A loose wrapper for a C-structure that contains the information needed for looping. This is useful if you are trying to understand the ufunc looping code. The PyUFuncLoopObject is the associated C-structure. It is defined in the ufuncobject.h header.", "A loose wrapper for the C-structure that contains the information needed for reduce-like methods of ufuncs. This is useful if you are trying to understand the reduce, accumulate, and reduce-at code. The PyUFuncReduceObject is the associated C-structure. It is defined in the ufuncobject.h header.", "A simple linked-list of C-structures containing the information needed to define a 1-d loop for a ufunc for every defined signature of a user-defined data-type.", "Advanced indexing is handled with this Python type. It is simply a loose wrapper around the C-structure containing the variables needed for advanced array indexing. The associated C-structure, PyArrayMapIterObject, is useful if you are trying to understand the advanced-index mapping code. It is defined in the arrayobject.h header. This type is not exposed to Python and could be replaced with a C-structure. As a Python type it takes advantage of reference- counted memory management."]}, {"name": "PyTypeObject PyArrayMapIter_Type", "path": "reference/c-api/types-and-structures#c.PyArrayMapIter_Type", "type": "Python Types and C-Structures", "text": ["Advanced indexing is handled with this Python type. It is simply a loose wrapper around the C-structure containing the variables needed for advanced array indexing. The associated C-structure, PyArrayMapIterObject, is useful if you are trying to understand the advanced-index mapping code. It is defined in the arrayobject.h header. This type is not exposed to Python and could be replaced with a C-structure. As a Python type it takes advantage of reference- counted memory management."]}, {"name": "PyUFunc_IdentityValue", "path": "reference/c-api/ufunc#c.PyUFunc_IdentityValue", "type": "UFunc API", "text": []}, {"name": "PyUFunc_LegacyInnerLoopSelectionFunc *legacy_inner_loop_selector", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.legacy_inner_loop_selector", "type": "Python Types and C-Structures", "text": ["Deprecated since version 1.22: Some fallback support for this slot exists, but will be removed eventually. A universal function that relied on this will have to be ported eventually. See ref:NEP 41 and ref:NEP 43"]}, {"name": "PyUFunc_MinusOne", "path": "reference/c-api/ufunc#c.PyUFunc_MinusOne", "type": "UFunc API", "text": []}, {"name": "PyUFunc_None", "path": "reference/c-api/ufunc#c.PyUFunc_None", "type": "UFunc API", "text": []}, {"name": "PyUFunc_ReorderableNone", "path": "reference/c-api/ufunc#c.PyUFunc_ReorderableNone", "type": "UFunc API", "text": []}, {"name": "PyUFunc_TypeResolutionFunc *type_resolver", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.type_resolver", "type": "Python Types and C-Structures", "text": ["A function which resolves the types and fills an array with the dtypes for the inputs and outputs"]}, {"name": "PyUFunc_Zero", "path": "reference/c-api/ufunc#c.PyUFunc_Zero", "type": "UFunc API", "text": []}, {"name": "Random sampling (numpy.random)", "path": "reference/random/index", "type": "Random sampling ( \n      \n       numpy.random\n      \n      )", "text": ["Numpy\u2019s random number routines produce pseudo random numbers using combinations of a BitGenerator to create sequences and a Generator to use those sequences to sample from different statistical distributions:", "Since Numpy version 1.17.0 the Generator can be initialized with a number of different BitGenerators. It exposes many different probability distributions. See NEP 19 for context on the updated random Numpy number routines. The legacy RandomState random number routines are still available, but limited to a single BitGenerator. See What\u2019s New or Different for a complete list of improvements and differences from the legacy RandomState.", "For convenience and backward compatibility, a single RandomState instance\u2019s methods are imported into the numpy.random namespace, see Legacy Random Generation for the complete list.", "Call default_rng to get a new instance of a Generator, then call its methods to obtain samples from different distributions. By default, Generator uses bits provided by PCG64 which has better statistical properties than the legacy MT19937 used in RandomState.", "Generator can be used as a replacement for RandomState. Both class instances hold an internal BitGenerator instance to provide the bit stream, it is accessible as gen.bit_generator. Some long-overdue API cleanup means that legacy and compatibility methods have been removed from Generator", "RandomState", "Generator", "Notes", "random_sample,", "random", "Compatible with random.random", "rand", "randint,", "integers", "Add an endpoint kwarg", "random_integers", "tomaxint", "removed", "Use integers(0, np.iinfo(np.int_).max, endpoint=False)", "seed", "removed", "Use SeedSequence.spawn", "See What\u2019s New or Different for more information.", "Something like the following code can be used to support both RandomState and Generator, with the understanding that the interfaces are slightly different", "Seeds can be passed to any of the BitGenerators. The provided value is mixed via SeedSequence to spread a possible sequence of seeds across a wider range of initialization states for the BitGenerator. Here PCG64 is used and is wrapped with a Generator.", "Here we use default_rng to create an instance of Generator to generate a random float:", "Here we use default_rng to create an instance of Generator to generate 3 random integers between 0 (inclusive) and 10 (exclusive):", "The new infrastructure takes a different approach to producing random numbers from the RandomState object. Random number generation is separated into two components, a bit generator and a random generator.", "The BitGenerator has a limited set of responsibilities. It manages state and provides functions to produce random doubles and random unsigned 32- and 64-bit values.", "The random generator takes the bit generator-provided stream and transforms them into more useful distributions, e.g., simulated normal random values. This structure allows alternative bit generators to be used with little code duplication.", "The Generator is the user-facing object that is nearly identical to the legacy RandomState. It accepts a bit generator instance as an argument. The default is currently PCG64 but this may change in future versions. As a convenience NumPy provides the default_rng function to hide these details:", "One can also instantiate Generator directly with a BitGenerator instance.", "To use the default PCG64 bit generator, one can instantiate it directly and pass it to Generator:", "Similarly to use the older MT19937 bit generator (not recommended), one can instantiate it directly and pass it to Generator:", "Warning", "The Box-Muller method used to produce NumPy\u2019s normals is no longer available in Generator. It is not possible to reproduce the exact random values using Generator for the normal distribution or any other distribution that relies on the normal such as the RandomState.gamma or RandomState.standard_t. If you require bitwise backward compatible streams, use RandomState.", "See What\u2019s New or Different for a complete list of improvements and differences from the traditional Randomstate.", "The included generators can be used in parallel, distributed applications in one of three ways:", "Users with a very large amount of parallelism will want to consult Upgrading PCG64 with PCG64DXSM.", "This package was developed independently of NumPy and was integrated in version 1.17.0. The original repo is at https://github.com/bashtage/randomgen."]}, {"name": "random.beta()", "path": "reference/random/generated/numpy.random.beta", "type": "numpy.random.beta", "text": ["Draw samples from a Beta distribution.", "The Beta distribution is a special case of the Dirichlet distribution, and is related to the Gamma distribution. It has the probability distribution function", "where the normalization, B, is the beta function,", "It is often seen in Bayesian inference and order statistics.", "Note", "New code should use the beta method of a default_rng() instance instead; please see the Quick Start.", "Alpha, positive (>0).", "Beta, positive (>0).", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if a and b are both scalars. Otherwise, np.broadcast(a, b).size samples are drawn.", "Drawn samples from the parameterized beta distribution.", "See also", "which should be used for new code."]}, {"name": "random.binomial()", "path": "reference/random/generated/numpy.random.binomial", "type": "numpy.random.binomial", "text": ["Draw samples from a binomial distribution.", "Samples are drawn from a binomial distribution with specified parameters, n trials and p probability of success where n an integer >= 0 and p is in the interval [0,1]. (n may be input as a float, but it is truncated to an integer in use)", "Note", "New code should use the binomial method of a default_rng() instance instead; please see the Quick Start.", "Parameter of the distribution, >= 0. Floats are also accepted, but they will be truncated to integers.", "Parameter of the distribution, >= 0 and <=1.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if n and p are both scalars. Otherwise, np.broadcast(n, p).size samples are drawn.", "Drawn samples from the parameterized binomial distribution, where each sample is equal to the number of successes over the n trials.", "See also", "probability density function, distribution or cumulative density function, etc.", "which should be used for new code.", "The probability density for the binomial distribution is", "where \\(n\\) is the number of trials, \\(p\\) is the probability of success, and \\(N\\) is the number of successes.", "When estimating the standard error of a proportion in a population by using a random sample, the normal distribution works well unless the product p*n <=5, where p = population proportion estimate, and n = number of samples, in which case the binomial distribution is used instead. For example, a sample of 15 people shows 4 who are left handed, and 11 who are right handed. Then p = 4/15 = 27%. 0.27*15 = 4, so the binomial distribution should be used in this case.", "Dalgaard, Peter, \u201cIntroductory Statistics with R\u201d, Springer-Verlag, 2002.", "Glantz, Stanton A. \u201cPrimer of Biostatistics.\u201d, McGraw-Hill, Fifth Edition, 2002.", "Lentner, Marvin, \u201cElementary Applied Statistics\u201d, Bogden and Quigley, 1972.", "Weisstein, Eric W. \u201cBinomial Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/BinomialDistribution.html", "Wikipedia, \u201cBinomial distribution\u201d, https://en.wikipedia.org/wiki/Binomial_distribution", "Draw samples from the distribution:", "A real world example. A company drills 9 wild-cat oil exploration wells, each with an estimated probability of success of 0.1. All nine wells fail. What is the probability of that happening?", "Let\u2019s do 20,000 trials of the model, and count the number that generate zero positive results."]}, {"name": "random.BitGenerator.cffi", "path": "reference/random/bit_generators/generated/numpy.random.bitgenerator.cffi", "type": "Random sampling", "text": ["attribute", "CFFI interface", "Named tuple containing CFFI wrapper"]}, {"name": "random.BitGenerator.random_raw()", "path": "reference/random/bit_generators/generated/numpy.random.bitgenerator.random_raw", "type": "numpy.random.BitGenerator", "text": ["method", "Return randoms as generated by the underlying BitGenerator", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.", "Output values. Used for performance testing since the generated values are not returned.", "Drawn samples.", "This method directly exposes the the raw underlying pseudo-random number generator. All values are returned as unsigned 64-bit values irrespective of the number of bits produced by the PRNG.", "See the class docstring for the number of bits returned."]}, {"name": "random.bytes()", "path": "reference/random/generated/numpy.random.bytes", "type": "numpy.random.bytes", "text": ["Return random bytes.", "Note", "New code should use the bytes method of a default_rng() instance instead; please see the Quick Start.", "Number of random bytes.", "String of length length.", "See also", "which should be used for new code."]}, {"name": "random.chisquare()", "path": "reference/random/generated/numpy.random.chisquare", "type": "numpy.random.chisquare", "text": ["Draw samples from a chi-square distribution.", "When df independent random variables, each with standard normal distributions (mean 0, variance 1), are squared and summed, the resulting distribution is chi-square (see Notes). This distribution is often used in hypothesis testing.", "Note", "New code should use the chisquare method of a default_rng() instance instead; please see the Quick Start.", "Number of degrees of freedom, must be > 0.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if df is a scalar. Otherwise, np.array(df).size samples are drawn.", "Drawn samples from the parameterized chi-square distribution.", "When df <= 0 or when an inappropriate size (e.g. size=-1) is given.", "See also", "which should be used for new code.", "The variable obtained by summing the squares of df independent, standard normally distributed random variables:", "is chi-square distributed, denoted", "The probability density function of the chi-squared distribution is", "where \\(\\Gamma\\) is the gamma function,", "NIST \u201cEngineering Statistics Handbook\u201d https://www.itl.nist.gov/div898/handbook/eda/section3/eda3666.htm"]}, {"name": "random.choice()", "path": "reference/random/generated/numpy.random.choice", "type": "numpy.random.choice", "text": ["Generates a random sample from a given 1-D array", "New in version 1.7.0.", "Note", "New code should use the choice method of a default_rng() instance instead; please see the Quick Start.", "If an ndarray, a random sample is generated from its elements. If an int, the random sample is generated as if it were np.arange(a)", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.", "Whether the sample is with or without replacement. Default is True, meaning that a value of a can be selected multiple times.", "The probabilities associated with each entry in a. If not given, the sample assumes a uniform distribution over all entries in a.", "The generated random samples", "If a is an int and less than zero, if a or p are not 1-dimensional, if a is an array-like of size 0, if p is not a vector of probabilities, if a and p have different lengths, or if replace=False and the sample size is greater than the population size", "See also", "which should be used in new code", "Setting user-specified probabilities through p uses a more general but less efficient sampler than the default. The general sampler produces a different sample than the optimized sampler even if each element of p is 1 / len(a).", "Sampling random rows from a 2-D array is not possible with this function, but is possible with Generator.choice through its axis keyword.", "Generate a uniform random sample from np.arange(5) of size 3:", "Generate a non-uniform random sample from np.arange(5) of size 3:", "Generate a uniform random sample from np.arange(5) of size 3 without replacement:", "Generate a non-uniform random sample from np.arange(5) of size 3 without replacement:", "Any of the above can be repeated with an arbitrary array-like instead of just integers. For instance:"]}, {"name": "random.dirichlet()", "path": "reference/random/generated/numpy.random.dirichlet", "type": "numpy.random.dirichlet", "text": ["Draw samples from the Dirichlet distribution.", "Draw size samples of dimension k from a Dirichlet distribution. A Dirichlet-distributed random variable can be seen as a multivariate generalization of a Beta distribution. The Dirichlet distribution is a conjugate prior of a multinomial distribution in Bayesian inference.", "Note", "New code should use the dirichlet method of a default_rng() instance instead; please see the Quick Start.", "Parameter of the distribution (length k for sample of length k).", "Output shape. If the given shape is, e.g., (m, n), then m * n * k samples are drawn. Default is None, in which case a vector of length k is returned.", "The drawn samples, of shape (size, k).", "If any value in alpha is less than or equal to zero", "See also", "which should be used for new code.", "The Dirichlet distribution is a distribution over vectors \\(x\\) that fulfil the conditions \\(x_i>0\\) and \\(\\sum_{i=1}^k x_i = 1\\).", "The probability density function \\(p\\) of a Dirichlet-distributed random vector \\(X\\) is proportional to", "where \\(\\alpha\\) is a vector containing the positive concentration parameters.", "The method uses the following property for computation: let \\(Y\\) be a random vector which has components that follow a standard gamma distribution, then \\(X = \\frac{1}{\\sum_{i=1}^k{Y_i}} Y\\) is Dirichlet-distributed", "David McKay, \u201cInformation Theory, Inference and Learning Algorithms,\u201d chapter 23, http://www.inference.org.uk/mackay/itila/", "Wikipedia, \u201cDirichlet distribution\u201d, https://en.wikipedia.org/wiki/Dirichlet_distribution", "Taking an example cited in Wikipedia, this distribution can be used if one wanted to cut strings (each of initial length 1.0) into K pieces with different lengths, where each piece had, on average, a designated average length, but allowing some variation in the relative sizes of the pieces."]}, {"name": "random.exponential()", "path": "reference/random/generated/numpy.random.exponential", "type": "numpy.random.exponential", "text": ["Draw samples from an exponential distribution.", "Its probability density function is", "for x > 0 and 0 elsewhere. \\(\\beta\\) is the scale parameter, which is the inverse of the rate parameter \\(\\lambda = 1/\\beta\\). The rate parameter is an alternative, widely used parameterization of the exponential distribution [3].", "The exponential distribution is a continuous analogue of the geometric distribution. It describes many common situations, such as the size of raindrops measured over many rainstorms [1], or the time between page requests to Wikipedia [2].", "Note", "New code should use the exponential method of a default_rng() instance instead; please see the Quick Start.", "The scale parameter, \\(\\beta = 1/\\lambda\\). Must be non-negative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if scale is a scalar. Otherwise, np.array(scale).size samples are drawn.", "Drawn samples from the parameterized exponential distribution.", "See also", "which should be used for new code.", "Peyton Z. Peebles Jr., \u201cProbability, Random Variables and Random Signal Principles\u201d, 4th ed, 2001, p. 57.", "Wikipedia, \u201cPoisson process\u201d, https://en.wikipedia.org/wiki/Poisson_process", "Wikipedia, \u201cExponential distribution\u201d, https://en.wikipedia.org/wiki/Exponential_distribution"]}, {"name": "random.f()", "path": "reference/random/generated/numpy.random.f", "type": "numpy.random.f", "text": ["Draw samples from an F distribution.", "Samples are drawn from an F distribution with specified parameters, dfnum (degrees of freedom in numerator) and dfden (degrees of freedom in denominator), where both parameters must be greater than zero.", "The random variate of the F distribution (also known as the Fisher distribution) is a continuous probability distribution that arises in ANOVA tests, and is the ratio of two chi-square variates.", "Note", "New code should use the f method of a default_rng() instance instead; please see the Quick Start.", "Degrees of freedom in numerator, must be > 0.", "Degrees of freedom in denominator, must be > 0.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if dfnum and dfden are both scalars. Otherwise, np.broadcast(dfnum, dfden).size samples are drawn.", "Drawn samples from the parameterized Fisher distribution.", "See also", "probability density function, distribution or cumulative density function, etc.", "which should be used for new code.", "The F statistic is used to compare in-group variances to between-group variances. Calculating the distribution depends on the sampling, and so it is a function of the respective degrees of freedom in the problem. The variable dfnum is the number of samples minus one, the between-groups degrees of freedom, while dfden is the within-groups degrees of freedom, the sum of the number of samples in each group minus the number of groups.", "Glantz, Stanton A. \u201cPrimer of Biostatistics.\u201d, McGraw-Hill, Fifth Edition, 2002.", "Wikipedia, \u201cF-distribution\u201d, https://en.wikipedia.org/wiki/F-distribution", "An example from Glantz[1], pp 47-40:", "Two groups, children of diabetics (25 people) and children from people without diabetes (25 controls). Fasting blood glucose was measured, case group had a mean value of 86.1, controls had a mean value of 82.2. Standard deviations were 2.09 and 2.49 respectively. Are these data consistent with the null hypothesis that the parents diabetic status does not affect their children\u2019s blood glucose levels? Calculating the F statistic from the data gives a value of 36.01.", "Draw samples from the distribution:", "The lower bound for the top 1% of the samples is :", "So there is about a 1% chance that the F statistic will exceed 7.62, the measured value is 36, so the null hypothesis is rejected at the 1% level."]}, {"name": "random.gamma()", "path": "reference/random/generated/numpy.random.gamma", "type": "numpy.random.gamma", "text": ["Draw samples from a Gamma distribution.", "Samples are drawn from a Gamma distribution with specified parameters, shape (sometimes designated \u201ck\u201d) and scale (sometimes designated \u201ctheta\u201d), where both parameters are > 0.", "Note", "New code should use the gamma method of a default_rng() instance instead; please see the Quick Start.", "The shape of the gamma distribution. Must be non-negative.", "The scale of the gamma distribution. Must be non-negative. Default is equal to 1.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if shape and scale are both scalars. Otherwise, np.broadcast(shape, scale).size samples are drawn.", "Drawn samples from the parameterized gamma distribution.", "See also", "probability density function, distribution or cumulative density function, etc.", "which should be used for new code.", "The probability density for the Gamma distribution is", "where \\(k\\) is the shape and \\(\\theta\\) the scale, and \\(\\Gamma\\) is the Gamma function.", "The Gamma distribution is often used to model the times to failure of electronic components, and arises naturally in processes for which the waiting times between Poisson distributed events are relevant.", "Weisstein, Eric W. \u201cGamma Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/GammaDistribution.html", "Wikipedia, \u201cGamma distribution\u201d, https://en.wikipedia.org/wiki/Gamma_distribution", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:"]}, {"name": "random.Generator.beta()", "path": "reference/random/generated/numpy.random.generator.beta", "type": "numpy.random.Generator.beta", "text": ["method", "Draw samples from a Beta distribution.", "The Beta distribution is a special case of the Dirichlet distribution, and is related to the Gamma distribution. It has the probability distribution function", "where the normalization, B, is the beta function,", "It is often seen in Bayesian inference and order statistics.", "Alpha, positive (>0).", "Beta, positive (>0).", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if a and b are both scalars. Otherwise, np.broadcast(a, b).size samples are drawn.", "Drawn samples from the parameterized beta distribution."]}, {"name": "random.Generator.binomial()", "path": "reference/random/generated/numpy.random.generator.binomial", "type": "numpy.random.Generator.binomial", "text": ["method", "Draw samples from a binomial distribution.", "Samples are drawn from a binomial distribution with specified parameters, n trials and p probability of success where n an integer >= 0 and p is in the interval [0,1]. (n may be input as a float, but it is truncated to an integer in use)", "Parameter of the distribution, >= 0. Floats are also accepted, but they will be truncated to integers.", "Parameter of the distribution, >= 0 and <=1.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if n and p are both scalars. Otherwise, np.broadcast(n, p).size samples are drawn.", "Drawn samples from the parameterized binomial distribution, where each sample is equal to the number of successes over the n trials.", "See also", "probability density function, distribution or cumulative density function, etc.", "The probability density for the binomial distribution is", "where \\(n\\) is the number of trials, \\(p\\) is the probability of success, and \\(N\\) is the number of successes.", "When estimating the standard error of a proportion in a population by using a random sample, the normal distribution works well unless the product p*n <=5, where p = population proportion estimate, and n = number of samples, in which case the binomial distribution is used instead. For example, a sample of 15 people shows 4 who are left handed, and 11 who are right handed. Then p = 4/15 = 27%. 0.27*15 = 4, so the binomial distribution should be used in this case.", "Dalgaard, Peter, \u201cIntroductory Statistics with R\u201d, Springer-Verlag, 2002.", "Glantz, Stanton A. \u201cPrimer of Biostatistics.\u201d, McGraw-Hill, Fifth Edition, 2002.", "Lentner, Marvin, \u201cElementary Applied Statistics\u201d, Bogden and Quigley, 1972.", "Weisstein, Eric W. \u201cBinomial Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/BinomialDistribution.html", "Wikipedia, \u201cBinomial distribution\u201d, https://en.wikipedia.org/wiki/Binomial_distribution", "Draw samples from the distribution:", "A real world example. A company drills 9 wild-cat oil exploration wells, each with an estimated probability of success of 0.1. All nine wells fail. What is the probability of that happening?", "Let\u2019s do 20,000 trials of the model, and count the number that generate zero positive results."]}, {"name": "random.Generator.bit_generator", "path": "reference/random/generated/numpy.random.generator.bit_generator", "type": "numpy.random.Generator.bit_generator", "text": ["attribute", "Gets the bit generator instance used by the generator", "The bit generator instance used by the generator"]}, {"name": "random.Generator.bytes()", "path": "reference/random/generated/numpy.random.generator.bytes", "type": "numpy.random.Generator.bytes", "text": ["method", "Return random bytes.", "Number of random bytes.", "String of length length."]}, {"name": "random.Generator.chisquare()", "path": "reference/random/generated/numpy.random.generator.chisquare", "type": "numpy.random.Generator.chisquare", "text": ["method", "Draw samples from a chi-square distribution.", "When df independent random variables, each with standard normal distributions (mean 0, variance 1), are squared and summed, the resulting distribution is chi-square (see Notes). This distribution is often used in hypothesis testing.", "Number of degrees of freedom, must be > 0.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if df is a scalar. Otherwise, np.array(df).size samples are drawn.", "Drawn samples from the parameterized chi-square distribution.", "When df <= 0 or when an inappropriate size (e.g. size=-1) is given.", "The variable obtained by summing the squares of df independent, standard normally distributed random variables:", "is chi-square distributed, denoted", "The probability density function of the chi-squared distribution is", "where \\(\\Gamma\\) is the gamma function,", "NIST \u201cEngineering Statistics Handbook\u201d https://www.itl.nist.gov/div898/handbook/eda/section3/eda3666.htm"]}, {"name": "random.Generator.choice()", "path": "reference/random/generated/numpy.random.generator.choice", "type": "numpy.random.Generator.choice", "text": ["method", "Generates a random sample from a given array", "If an ndarray, a random sample is generated from its elements. If an int, the random sample is generated from np.arange(a).", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn from the 1-d a. If a has more than one dimension, the size shape will be inserted into the axis dimension, so the output ndim will be a.ndim - 1 +\nlen(size). Default is None, in which case a single value is returned.", "Whether the sample is with or without replacement. Default is True, meaning that a value of a can be selected multiple times.", "The probabilities associated with each entry in a. If not given, the sample assumes a uniform distribution over all entries in a.", "The axis along which the selection is performed. The default, 0, selects by row.", "Whether the sample is shuffled when sampling without replacement. Default is True, False provides a speedup.", "The generated random samples", "If a is an int and less than zero, if p is not 1-dimensional, if a is array-like with a size 0, if p is not a vector of probabilities, if a and p have different lengths, or if replace=False and the sample size is greater than the population size.", "See also", "Setting user-specified probabilities through p uses a more general but less efficient sampler than the default. The general sampler produces a different sample than the optimized sampler even if each element of p is 1 / len(a).", "Generate a uniform random sample from np.arange(5) of size 3:", "Generate a non-uniform random sample from np.arange(5) of size 3:", "Generate a uniform random sample from np.arange(5) of size 3 without replacement:", "Generate a uniform random sample from a 2-D array along the first axis (the default), without replacement:", "Generate a non-uniform random sample from np.arange(5) of size 3 without replacement:", "Any of the above can be repeated with an arbitrary array-like instead of just integers. For instance:"]}, {"name": "random.Generator.dirichlet()", "path": "reference/random/generated/numpy.random.generator.dirichlet", "type": "numpy.random.Generator.dirichlet", "text": ["method", "Draw samples from the Dirichlet distribution.", "Draw size samples of dimension k from a Dirichlet distribution. A Dirichlet-distributed random variable can be seen as a multivariate generalization of a Beta distribution. The Dirichlet distribution is a conjugate prior of a multinomial distribution in Bayesian inference.", "Parameter of the distribution (length k for sample of length k).", "Output shape. If the given shape is, e.g., (m, n), then m * n * k samples are drawn. Default is None, in which case a vector of length k is returned.", "The drawn samples, of shape (size, k).", "If any value in alpha is less than or equal to zero", "The Dirichlet distribution is a distribution over vectors \\(x\\) that fulfil the conditions \\(x_i>0\\) and \\(\\sum_{i=1}^k x_i = 1\\).", "The probability density function \\(p\\) of a Dirichlet-distributed random vector \\(X\\) is proportional to", "where \\(\\alpha\\) is a vector containing the positive concentration parameters.", "The method uses the following property for computation: let \\(Y\\) be a random vector which has components that follow a standard gamma distribution, then \\(X = \\frac{1}{\\sum_{i=1}^k{Y_i}} Y\\) is Dirichlet-distributed", "David McKay, \u201cInformation Theory, Inference and Learning Algorithms,\u201d chapter 23, http://www.inference.org.uk/mackay/itila/", "Wikipedia, \u201cDirichlet distribution\u201d, https://en.wikipedia.org/wiki/Dirichlet_distribution", "Taking an example cited in Wikipedia, this distribution can be used if one wanted to cut strings (each of initial length 1.0) into K pieces with different lengths, where each piece had, on average, a designated average length, but allowing some variation in the relative sizes of the pieces."]}, {"name": "random.Generator.exponential()", "path": "reference/random/generated/numpy.random.generator.exponential", "type": "numpy.random.Generator.exponential", "text": ["method", "Draw samples from an exponential distribution.", "Its probability density function is", "for x > 0 and 0 elsewhere. \\(\\beta\\) is the scale parameter, which is the inverse of the rate parameter \\(\\lambda = 1/\\beta\\). The rate parameter is an alternative, widely used parameterization of the exponential distribution [3].", "The exponential distribution is a continuous analogue of the geometric distribution. It describes many common situations, such as the size of raindrops measured over many rainstorms [1], or the time between page requests to Wikipedia [2].", "The scale parameter, \\(\\beta = 1/\\lambda\\). Must be non-negative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if scale is a scalar. Otherwise, np.array(scale).size samples are drawn.", "Drawn samples from the parameterized exponential distribution.", "Peyton Z. Peebles Jr., \u201cProbability, Random Variables and Random Signal Principles\u201d, 4th ed, 2001, p. 57.", "Wikipedia, \u201cPoisson process\u201d, https://en.wikipedia.org/wiki/Poisson_process", "Wikipedia, \u201cExponential distribution\u201d, https://en.wikipedia.org/wiki/Exponential_distribution"]}, {"name": "random.Generator.f()", "path": "reference/random/generated/numpy.random.generator.f", "type": "numpy.random.Generator.f", "text": ["method", "Draw samples from an F distribution.", "Samples are drawn from an F distribution with specified parameters, dfnum (degrees of freedom in numerator) and dfden (degrees of freedom in denominator), where both parameters must be greater than zero.", "The random variate of the F distribution (also known as the Fisher distribution) is a continuous probability distribution that arises in ANOVA tests, and is the ratio of two chi-square variates.", "Degrees of freedom in numerator, must be > 0.", "Degrees of freedom in denominator, must be > 0.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if dfnum and dfden are both scalars. Otherwise, np.broadcast(dfnum, dfden).size samples are drawn.", "Drawn samples from the parameterized Fisher distribution.", "See also", "probability density function, distribution or cumulative density function, etc.", "The F statistic is used to compare in-group variances to between-group variances. Calculating the distribution depends on the sampling, and so it is a function of the respective degrees of freedom in the problem. The variable dfnum is the number of samples minus one, the between-groups degrees of freedom, while dfden is the within-groups degrees of freedom, the sum of the number of samples in each group minus the number of groups.", "Glantz, Stanton A. \u201cPrimer of Biostatistics.\u201d, McGraw-Hill, Fifth Edition, 2002.", "Wikipedia, \u201cF-distribution\u201d, https://en.wikipedia.org/wiki/F-distribution", "An example from Glantz[1], pp 47-40:", "Two groups, children of diabetics (25 people) and children from people without diabetes (25 controls). Fasting blood glucose was measured, case group had a mean value of 86.1, controls had a mean value of 82.2. Standard deviations were 2.09 and 2.49 respectively. Are these data consistent with the null hypothesis that the parents diabetic status does not affect their children\u2019s blood glucose levels? Calculating the F statistic from the data gives a value of 36.01.", "Draw samples from the distribution:", "The lower bound for the top 1% of the samples is :", "So there is about a 1% chance that the F statistic will exceed 7.62, the measured value is 36, so the null hypothesis is rejected at the 1% level."]}, {"name": "random.Generator.gamma()", "path": "reference/random/generated/numpy.random.generator.gamma", "type": "numpy.random.Generator.gamma", "text": ["method", "Draw samples from a Gamma distribution.", "Samples are drawn from a Gamma distribution with specified parameters, shape (sometimes designated \u201ck\u201d) and scale (sometimes designated \u201ctheta\u201d), where both parameters are > 0.", "The shape of the gamma distribution. Must be non-negative.", "The scale of the gamma distribution. Must be non-negative. Default is equal to 1.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if shape and scale are both scalars. Otherwise, np.broadcast(shape, scale).size samples are drawn.", "Drawn samples from the parameterized gamma distribution.", "See also", "probability density function, distribution or cumulative density function, etc.", "The probability density for the Gamma distribution is", "where \\(k\\) is the shape and \\(\\theta\\) the scale, and \\(\\Gamma\\) is the Gamma function.", "The Gamma distribution is often used to model the times to failure of electronic components, and arises naturally in processes for which the waiting times between Poisson distributed events are relevant.", "Weisstein, Eric W. \u201cGamma Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/GammaDistribution.html", "Wikipedia, \u201cGamma distribution\u201d, https://en.wikipedia.org/wiki/Gamma_distribution", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:"]}, {"name": "random.Generator.geometric()", "path": "reference/random/generated/numpy.random.generator.geometric", "type": "numpy.random.Generator.geometric", "text": ["method", "Draw samples from the geometric distribution.", "Bernoulli trials are experiments with one of two outcomes: success or failure (an example of such an experiment is flipping a coin). The geometric distribution models the number of trials that must be run in order to achieve success. It is therefore supported on the positive integers, k = 1, 2, ....", "The probability mass function of the geometric distribution is", "where p is the probability of success of an individual trial.", "The probability of success of an individual trial.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if p is a scalar. Otherwise, np.array(p).size samples are drawn.", "Drawn samples from the parameterized geometric distribution.", "Draw ten thousand values from the geometric distribution, with the probability of an individual success equal to 0.35:", "How many trials succeeded after a single run?"]}, {"name": "random.Generator.gumbel()", "path": "reference/random/generated/numpy.random.generator.gumbel", "type": "numpy.random.Generator.gumbel", "text": ["method", "Draw samples from a Gumbel distribution.", "Draw samples from a Gumbel distribution with specified location and scale. For more information on the Gumbel distribution, see Notes and References below.", "The location of the mode of the distribution. Default is 0.", "The scale parameter of the distribution. Default is 1. Must be non- negative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if loc and scale are both scalars. Otherwise, np.broadcast(loc, scale).size samples are drawn.", "Drawn samples from the parameterized Gumbel distribution.", "See also", "The Gumbel (or Smallest Extreme Value (SEV) or the Smallest Extreme Value Type I) distribution is one of a class of Generalized Extreme Value (GEV) distributions used in modeling extreme value problems. The Gumbel is a special case of the Extreme Value Type I distribution for maximums from distributions with \u201cexponential-like\u201d tails.", "The probability density for the Gumbel distribution is", "where \\(\\mu\\) is the mode, a location parameter, and \\(\\beta\\) is the scale parameter.", "The Gumbel (named for German mathematician Emil Julius Gumbel) was used very early in the hydrology literature, for modeling the occurrence of flood events. It is also used for modeling maximum wind speed and rainfall rates. It is a \u201cfat-tailed\u201d distribution - the probability of an event in the tail of the distribution is larger than if one used a Gaussian, hence the surprisingly frequent occurrence of 100-year floods. Floods were initially modeled as a Gaussian process, which underestimated the frequency of extreme events.", "It is one of a class of extreme value distributions, the Generalized Extreme Value (GEV) distributions, which also includes the Weibull and Frechet.", "The function has a mean of \\(\\mu + 0.57721\\beta\\) and a variance of \\(\\frac{\\pi^2}{6}\\beta^2\\).", "Gumbel, E. J., \u201cStatistics of Extremes,\u201d New York: Columbia University Press, 1958.", "Reiss, R.-D. and Thomas, M., \u201cStatistical Analysis of Extreme Values from Insurance, Finance, Hydrology and Other Fields,\u201d Basel: Birkhauser Verlag, 2001.", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:", "Show how an extreme value distribution can arise from a Gaussian process and compare to a Gaussian:"]}, {"name": "random.Generator.hypergeometric()", "path": "reference/random/generated/numpy.random.generator.hypergeometric", "type": "numpy.random.Generator.hypergeometric", "text": ["method", "Draw samples from a Hypergeometric distribution.", "Samples are drawn from a hypergeometric distribution with specified parameters, ngood (ways to make a good selection), nbad (ways to make a bad selection), and nsample (number of items sampled, which is less than or equal to the sum ngood + nbad).", "Number of ways to make a good selection. Must be nonnegative and less than 10**9.", "Number of ways to make a bad selection. Must be nonnegative and less than 10**9.", "Number of items sampled. Must be nonnegative and less than ngood + nbad.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if ngood, nbad, and nsample are all scalars. Otherwise, np.broadcast(ngood, nbad, nsample).size samples are drawn.", "Drawn samples from the parameterized hypergeometric distribution. Each sample is the number of good items within a randomly selected subset of size nsample taken from a set of ngood good items and nbad bad items.", "See also", "Draw samples from the multivariate hypergeometric distribution.", "probability density function, distribution or cumulative density function, etc.", "The probability density for the Hypergeometric distribution is", "where \\(0 \\le x \\le n\\) and \\(n-b \\le x \\le g\\)", "for P(x) the probability of x good results in the drawn sample, g = ngood, b = nbad, and n = nsample.", "Consider an urn with black and white marbles in it, ngood of them are black and nbad are white. If you draw nsample balls without replacement, then the hypergeometric distribution describes the distribution of black balls in the drawn sample.", "Note that this distribution is very similar to the binomial distribution, except that in this case, samples are drawn without replacement, whereas in the Binomial case samples are drawn with replacement (or the sample space is infinite). As the sample space becomes large, this distribution approaches the binomial.", "The arguments ngood and nbad each must be less than 10**9. For extremely large arguments, the algorithm that is used to compute the samples [4] breaks down because of loss of precision in floating point calculations. For such large values, if nsample is not also large, the distribution can be approximated with the binomial distribution, binomial(n=nsample, p=ngood/(ngood + nbad)).", "Lentner, Marvin, \u201cElementary Applied Statistics\u201d, Bogden and Quigley, 1972.", "Weisstein, Eric W. \u201cHypergeometric Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/HypergeometricDistribution.html", "Wikipedia, \u201cHypergeometric distribution\u201d, https://en.wikipedia.org/wiki/Hypergeometric_distribution", "Stadlober, Ernst, \u201cThe ratio of uniforms approach for generating discrete random variates\u201d, Journal of Computational and Applied Mathematics, 31, pp. 181-189 (1990).", "Draw samples from the distribution:", "Suppose you have an urn with 15 white and 15 black marbles. If you pull 15 marbles at random, how likely is it that 12 or more of them are one color?"]}, {"name": "random.Generator.integers()", "path": "reference/random/generated/numpy.random.generator.integers", "type": "numpy.random.Generator.integers", "text": ["method", "Return random integers from low (inclusive) to high (exclusive), or if endpoint=True, low (inclusive) to high (inclusive). Replaces RandomState.randint (with endpoint=False) and RandomState.random_integers (with endpoint=True)", "Return random integers from the \u201cdiscrete uniform\u201d distribution of the specified dtype. If high is None (the default), then results are from 0 to low.", "Lowest (signed) integers to be drawn from the distribution (unless high=None, in which case this parameter is 0 and this value is used for high).", "If provided, one above the largest (signed) integer to be drawn from the distribution (see above for behavior if high=None). If array-like, must contain integer values", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.", "Desired dtype of the result. Byteorder must be native. The default value is np.int64.", "If true, sample from the interval [low, high] instead of the default [low, high) Defaults to False", "size-shaped array of random integers from the appropriate distribution, or a single such random int if size not provided.", "When using broadcasting with uint64 dtypes, the maximum value (2**64) cannot be represented as a standard integer type. The high array (or low if high is None) must have object dtype, e.g., array([2**64]).", "Daniel Lemire., \u201cFast Random Integer Generation in an Interval\u201d, ACM Transactions on Modeling and Computer Simulation 29 (1), 2019, http://arxiv.org/abs/1805.10941.", "Generate a 2 x 4 array of ints between 0 and 4, inclusive:", "Generate a 1 x 3 array with 3 different upper bounds", "Generate a 1 by 3 array with 3 different lower bounds", "Generate a 2 by 4 array using broadcasting with dtype of uint8"]}, {"name": "random.Generator.laplace()", "path": "reference/random/generated/numpy.random.generator.laplace", "type": "numpy.random.Generator.laplace", "text": ["method", "Draw samples from the Laplace or double exponential distribution with specified location (or mean) and scale (decay).", "The Laplace distribution is similar to the Gaussian/normal distribution, but is sharper at the peak and has fatter tails. It represents the difference between two independent, identically distributed exponential random variables.", "The position, \\(\\mu\\), of the distribution peak. Default is 0.", "\\(\\lambda\\), the exponential decay. Default is 1. Must be non- negative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if loc and scale are both scalars. Otherwise, np.broadcast(loc, scale).size samples are drawn.", "Drawn samples from the parameterized Laplace distribution.", "It has the probability density function", "The first law of Laplace, from 1774, states that the frequency of an error can be expressed as an exponential function of the absolute magnitude of the error, which leads to the Laplace distribution. For many problems in economics and health sciences, this distribution seems to model the data better than the standard Gaussian distribution.", "Abramowitz, M. and Stegun, I. A. (Eds.). \u201cHandbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing,\u201d New York: Dover, 1972.", "Kotz, Samuel, et. al. \u201cThe Laplace Distribution and Generalizations, \u201d Birkhauser, 2001.", "Weisstein, Eric W. \u201cLaplace Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/LaplaceDistribution.html", "Wikipedia, \u201cLaplace distribution\u201d, https://en.wikipedia.org/wiki/Laplace_distribution", "Draw samples from the distribution", "Display the histogram of the samples, along with the probability density function:", "Plot Gaussian for comparison:"]}, {"name": "random.Generator.logistic()", "path": "reference/random/generated/numpy.random.generator.logistic", "type": "numpy.random.Generator.logistic", "text": ["method", "Draw samples from a logistic distribution.", "Samples are drawn from a logistic distribution with specified parameters, loc (location or mean, also median), and scale (>0).", "Parameter of the distribution. Default is 0.", "Parameter of the distribution. Must be non-negative. Default is 1.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if loc and scale are both scalars. Otherwise, np.broadcast(loc, scale).size samples are drawn.", "Drawn samples from the parameterized logistic distribution.", "See also", "probability density function, distribution or cumulative density function, etc.", "The probability density for the Logistic distribution is", "where \\(\\mu\\) = location and \\(s\\) = scale.", "The Logistic distribution is used in Extreme Value problems where it can act as a mixture of Gumbel distributions, in Epidemiology, and by the World Chess Federation (FIDE) where it is used in the Elo ranking system, assuming the performance of each player is a logistically distributed random variable.", "Reiss, R.-D. and Thomas M. (2001), \u201cStatistical Analysis of Extreme Values, from Insurance, Finance, Hydrology and Other Fields,\u201d Birkhauser Verlag, Basel, pp 132-133.", "Weisstein, Eric W. \u201cLogistic Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/LogisticDistribution.html", "Wikipedia, \u201cLogistic-distribution\u201d, https://en.wikipedia.org/wiki/Logistic_distribution", "Draw samples from the distribution:", "# plot against distribution"]}, {"name": "random.Generator.lognormal()", "path": "reference/random/generated/numpy.random.generator.lognormal", "type": "numpy.random.Generator.lognormal", "text": ["method", "Draw samples from a log-normal distribution.", "Draw samples from a log-normal distribution with specified mean, standard deviation, and array shape. Note that the mean and standard deviation are not the values for the distribution itself, but of the underlying normal distribution it is derived from.", "Mean value of the underlying normal distribution. Default is 0.", "Standard deviation of the underlying normal distribution. Must be non-negative. Default is 1.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if mean and sigma are both scalars. Otherwise, np.broadcast(mean, sigma).size samples are drawn.", "Drawn samples from the parameterized log-normal distribution.", "See also", "probability density function, distribution, cumulative density function, etc.", "A variable x has a log-normal distribution if log(x) is normally distributed. The probability density function for the log-normal distribution is:", "where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation of the normally distributed logarithm of the variable. A log-normal distribution results if a random variable is the product of a large number of independent, identically-distributed variables in the same way that a normal distribution results if the variable is the sum of a large number of independent, identically-distributed variables.", "Limpert, E., Stahel, W. A., and Abbt, M., \u201cLog-normal Distributions across the Sciences: Keys and Clues,\u201d BioScience, Vol. 51, No. 5, May, 2001. https://stat.ethz.ch/~stahel/lognormal/bioscience.pdf", "Reiss, R.D. and Thomas, M., \u201cStatistical Analysis of Extreme Values,\u201d Basel: Birkhauser Verlag, 2001, pp. 31-32.", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:", "Demonstrate that taking the products of random samples from a uniform distribution can be fit well by a log-normal probability density function."]}, {"name": "random.Generator.logseries()", "path": "reference/random/generated/numpy.random.generator.logseries", "type": "numpy.random.Generator.logseries", "text": ["method", "Draw samples from a logarithmic series distribution.", "Samples are drawn from a log series distribution with specified shape parameter, 0 < p < 1.", "Shape parameter for the distribution. Must be in the range (0, 1).", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if p is a scalar. Otherwise, np.array(p).size samples are drawn.", "Drawn samples from the parameterized logarithmic series distribution.", "See also", "probability density function, distribution or cumulative density function, etc.", "The probability mass function for the Log Series distribution is", "where p = probability.", "The log series distribution is frequently used to represent species richness and occurrence, first proposed by Fisher, Corbet, and Williams in 1943 [2]. It may also be used to model the numbers of occupants seen in cars [3].", "Buzas, Martin A.; Culver, Stephen J., Understanding regional species diversity through the log series distribution of occurrences: BIODIVERSITY RESEARCH Diversity & Distributions, Volume 5, Number 5, September 1999 , pp. 187-195(9).", "Fisher, R.A,, A.S. Corbet, and C.B. Williams. 1943. The relation between the number of species and the number of individuals in a random sample of an animal population. Journal of Animal Ecology, 12:42-58.", "D. J. Hand, F. Daly, D. Lunn, E. Ostrowski, A Handbook of Small Data Sets, CRC Press, 1994.", "Wikipedia, \u201cLogarithmic distribution\u201d, https://en.wikipedia.org/wiki/Logarithmic_distribution", "Draw samples from the distribution:", "# plot against distribution"]}, {"name": "random.Generator.multinomial()", "path": "reference/random/generated/numpy.random.generator.multinomial", "type": "numpy.random.Generator.multinomial", "text": ["method", "Draw samples from a multinomial distribution.", "The multinomial distribution is a multivariate generalization of the binomial distribution. Take an experiment with one of p possible outcomes. An example of such an experiment is throwing a dice, where the outcome can be 1 through 6. Each sample drawn from the distribution represents n such experiments. Its values, X_i = [X_0, X_1, ..., X_p], represent the number of times the outcome was i.", "Number of experiments.", "Probabilities of each of the p different outcomes with shape (k0, k1, ..., kn, p). Each element pvals[i,j,...,:] must sum to 1 (however, the last element is always assumed to account for the remaining probability, as long as sum(pvals[..., :-1], axis=-1) <= 1.0. Must have at least 1 dimension where pvals.shape[-1] > 0.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn each with p elements. Default is None where the output size is determined by the broadcast shape of n and all by the final dimension of pvals, which is denoted as b=(b0, b1, ..., bq). If size is not None, then it must be compatible with the broadcast shape b. Specifically, size must have q or more elements and size[-(q-j):] must equal bj.", "The drawn samples, of shape size, if provided. When size is provided, the output shape is size + (p,) If not specified, the shape is determined by the broadcast shape of n and pvals, (b0, b1, ..., bq) augmented with the dimension of the multinomial, p, so that that output shape is (b0, b1, ..., bq, p).", "Each entry out[i,j,...,:] is a p-dimensional value drawn from the distribution.", "Throw a dice 20 times:", "It landed 4 times on 1, once on 2, etc.", "Now, throw the dice 20 times, and 20 times again:", "For the first run, we threw 3 times 1, 4 times 2, etc. For the second, we threw 2 times 1, 4 times 2, etc.", "Now, do one experiment throwing the dice 10 time, and 10 times again, and another throwing the dice 20 times, and 20 times again:", "The first array shows the outcomes of throwing the dice 10 times, and the second shows the outcomes from throwing the dice 20 times.", "A loaded die is more likely to land on number 6:", "Simulate 10 throws of a 4-sided die and 20 throws of a 6-sided die", "Generate categorical random variates from two categories where the first has 3 outcomes and the second has 2.", "argmax(axis=-1) is then used to return the categories.", "The same output dimension can be produced using broadcasting.", "The probability inputs should be normalized. As an implementation detail, the value of the last entry is ignored and assumed to take up any leftover probability mass, but this should not be relied on. A biased coin which has twice as much weight on one side as on the other should be sampled like so:", "not like:"]}, {"name": "random.Generator.multivariate_hypergeometric()", "path": "reference/random/generated/numpy.random.generator.multivariate_hypergeometric", "type": "numpy.random.Generator.multivariate_hypergeometric", "text": ["method", "Generate variates from a multivariate hypergeometric distribution.", "The multivariate hypergeometric distribution is a generalization of the hypergeometric distribution.", "Choose nsample items at random without replacement from a collection with N distinct types. N is the length of colors, and the values in colors are the number of occurrences of that type in the collection. The total number of items in the collection is sum(colors). Each random variate generated by this function is a vector of length N holding the counts of the different types that occurred in the nsample items.", "The name colors comes from a common description of the distribution: it is the probability distribution of the number of marbles of each color selected without replacement from an urn containing marbles of different colors; colors[i] is the number of marbles in the urn with color i.", "The number of each type of item in the collection from which a sample is drawn. The values in colors must be nonnegative. To avoid loss of precision in the algorithm, sum(colors) must be less than 10**9 when method is \u201cmarginals\u201d.", "The number of items selected. nsample must not be greater than sum(colors).", "The number of variates to generate, either an integer or a tuple holding the shape of the array of variates. If the given size is, e.g., (k, m), then k * m variates are drawn, where one variate is a vector of length len(colors), and the return value has shape (k, m, len(colors)). If size is an integer, the output has shape (size, len(colors)). Default is None, in which case a single variate is returned as an array with shape (len(colors),).", "Specify the algorithm that is used to generate the variates. Must be \u2018count\u2019 or \u2018marginals\u2019 (the default). See the Notes for a description of the methods.", "Array of variates drawn from the multivariate hypergeometric distribution.", "See also", "Draw samples from the (univariate) hypergeometric distribution.", "The two methods do not return the same sequence of variates.", "The \u201ccount\u201d algorithm is roughly equivalent to the following numpy code:", "The \u201ccount\u201d algorithm uses a temporary array of integers with length sum(colors).", "The \u201cmarginals\u201d algorithm generates a variate by using repeated calls to the univariate hypergeometric sampler. It is roughly equivalent to:", "The default method is \u201cmarginals\u201d. For some cases (e.g. when colors contains relatively small integers), the \u201ccount\u201d method can be significantly faster than the \u201cmarginals\u201d method. If performance of the algorithm is important, test the two methods with typical inputs to decide which works best.", "New in version 1.18.0."]}, {"name": "random.Generator.multivariate_normal()", "path": "reference/random/generated/numpy.random.generator.multivariate_normal", "type": "numpy.random.Generator.multivariate_normal", "text": ["method", "Draw random samples from a multivariate normal distribution.", "The multivariate normal, multinormal or Gaussian distribution is a generalization of the one-dimensional normal distribution to higher dimensions. Such a distribution is specified by its mean and covariance matrix. These parameters are analogous to the mean (average or \u201ccenter\u201d) and variance (standard deviation, or \u201cwidth,\u201d squared) of the one-dimensional normal distribution.", "Mean of the N-dimensional distribution.", "Covariance matrix of the distribution. It must be symmetric and positive-semidefinite for proper sampling.", "Given a shape of, for example, (m,n,k), m*n*k samples are generated, and packed in an m-by-n-by-k arrangement. Because each sample is N-dimensional, the output shape is (m,n,k,N). If no shape is specified, a single (N-D) sample is returned.", "Behavior when the covariance matrix is not positive semidefinite.", "Tolerance when checking the singular values in covariance matrix. cov is cast to double before the check.", "The cov input is used to compute a factor matrix A such that A @ A.T = cov. This argument is used to select the method used to compute the factor matrix A. The default method \u2018svd\u2019 is the slowest, while \u2018cholesky\u2019 is the fastest but less robust than the slowest method. The method eigh uses eigen decomposition to compute A and is faster than svd but slower than cholesky.", "New in version 1.18.0.", "The drawn samples, of shape size, if that was provided. If not, the shape is (N,).", "In other words, each entry out[i,j,...,:] is an N-dimensional value drawn from the distribution.", "The mean is a coordinate in N-dimensional space, which represents the location where samples are most likely to be generated. This is analogous to the peak of the bell curve for the one-dimensional or univariate normal distribution.", "Covariance indicates the level to which two variables vary together. From the multivariate normal distribution, we draw N-dimensional samples, \\(X = [x_1, x_2, ... x_N]\\). The covariance matrix element \\(C_{ij}\\) is the covariance of \\(x_i\\) and \\(x_j\\). The element \\(C_{ii}\\) is the variance of \\(x_i\\) (i.e. its \u201cspread\u201d).", "Instead of specifying the full covariance matrix, popular approximations include:", "This geometrical property can be seen in two dimensions by plotting generated data-points:", "Diagonal covariance means that points are oriented along x or y-axis:", "Note that the covariance matrix must be positive semidefinite (a.k.a. nonnegative-definite). Otherwise, the behavior of this method is undefined and backwards compatibility is not guaranteed.", "Papoulis, A., \u201cProbability, Random Variables, and Stochastic Processes,\u201d 3rd ed., New York: McGraw-Hill, 1991.", "Duda, R. O., Hart, P. E., and Stork, D. G., \u201cPattern Classification,\u201d 2nd ed., New York: Wiley, 2001.", "We can use a different method other than the default to factorize cov:", "The following is probably true, given that 0.6 is roughly twice the standard deviation:"]}, {"name": "random.Generator.negative_binomial()", "path": "reference/random/generated/numpy.random.generator.negative_binomial", "type": "numpy.random.Generator.negative_binomial", "text": ["method", "Draw samples from a negative binomial distribution.", "Samples are drawn from a negative binomial distribution with specified parameters, n successes and p probability of success where n is > 0 and p is in the interval (0, 1].", "Parameter of the distribution, > 0.", "Parameter of the distribution. Must satisfy 0 < p <= 1.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if n and p are both scalars. Otherwise, np.broadcast(n, p).size samples are drawn.", "Drawn samples from the parameterized negative binomial distribution, where each sample is equal to N, the number of failures that occurred before a total of n successes was reached.", "The probability mass function of the negative binomial distribution is", "where \\(n\\) is the number of successes, \\(p\\) is the probability of success, \\(N+n\\) is the number of trials, and \\(\\Gamma\\) is the gamma function. When \\(n\\) is an integer, \\(\\frac{\\Gamma(N+n)}{N!\\Gamma(n)} = \\binom{N+n-1}{N}\\), which is the more common form of this term in the the pmf. The negative binomial distribution gives the probability of N failures given n successes, with a success on the last trial.", "If one throws a die repeatedly until the third time a \u201c1\u201d appears, then the probability distribution of the number of non-\u201c1\u201ds that appear before the third \u201c1\u201d is a negative binomial distribution.", "Weisstein, Eric W. \u201cNegative Binomial Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/NegativeBinomialDistribution.html", "Wikipedia, \u201cNegative binomial distribution\u201d, https://en.wikipedia.org/wiki/Negative_binomial_distribution", "Draw samples from the distribution:", "A real world example. A company drills wild-cat oil exploration wells, each with an estimated probability of success of 0.1. What is the probability of having one success for each successive well, that is what is the probability of a single success after drilling 5 wells, after 6 wells, etc.?"]}, {"name": "random.Generator.noncentral_chisquare()", "path": "reference/random/generated/numpy.random.generator.noncentral_chisquare", "type": "numpy.random.Generator.noncentral_chisquare", "text": ["method", "Draw samples from a noncentral chi-square distribution.", "The noncentral \\(\\chi^2\\) distribution is a generalization of the \\(\\chi^2\\) distribution.", "Degrees of freedom, must be > 0.", "Changed in version 1.10.0: Earlier NumPy versions required dfnum > 1.", "Non-centrality, must be non-negative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if df and nonc are both scalars. Otherwise, np.broadcast(df, nonc).size samples are drawn.", "Drawn samples from the parameterized noncentral chi-square distribution.", "The probability density function for the noncentral Chi-square distribution is", "where \\(Y_{q}\\) is the Chi-square with q degrees of freedom.", "Wikipedia, \u201cNoncentral chi-squared distribution\u201d https://en.wikipedia.org/wiki/Noncentral_chi-squared_distribution", "Draw values from the distribution and plot the histogram", "Draw values from a noncentral chisquare with very small noncentrality, and compare to a chisquare.", "Demonstrate how large values of non-centrality lead to a more symmetric distribution."]}, {"name": "random.Generator.noncentral_f()", "path": "reference/random/generated/numpy.random.generator.noncentral_f", "type": "numpy.random.Generator.noncentral_f", "text": ["method", "Draw samples from the noncentral F distribution.", "Samples are drawn from an F distribution with specified parameters, dfnum (degrees of freedom in numerator) and dfden (degrees of freedom in denominator), where both parameters > 1. nonc is the non-centrality parameter.", "Numerator degrees of freedom, must be > 0.", "Changed in version 1.14.0: Earlier NumPy versions required dfnum > 1.", "Denominator degrees of freedom, must be > 0.", "Non-centrality parameter, the sum of the squares of the numerator means, must be >= 0.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if dfnum, dfden, and nonc are all scalars. Otherwise, np.broadcast(dfnum, dfden, nonc).size samples are drawn.", "Drawn samples from the parameterized noncentral Fisher distribution.", "When calculating the power of an experiment (power = probability of rejecting the null hypothesis when a specific alternative is true) the non-central F statistic becomes important. When the null hypothesis is true, the F statistic follows a central F distribution. When the null hypothesis is not true, then it follows a non-central F statistic.", "Weisstein, Eric W. \u201cNoncentral F-Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/NoncentralF-Distribution.html", "Wikipedia, \u201cNoncentral F-distribution\u201d, https://en.wikipedia.org/wiki/Noncentral_F-distribution", "In a study, testing for a specific alternative to the null hypothesis requires use of the Noncentral F distribution. We need to calculate the area in the tail of the distribution that exceeds the value of the F distribution for the null hypothesis. We\u2019ll plot the two probability distributions for comparison."]}, {"name": "random.Generator.normal()", "path": "reference/random/generated/numpy.random.generator.normal", "type": "numpy.random.Generator.normal", "text": ["method", "Draw random samples from a normal (Gaussian) distribution.", "The probability density function of the normal distribution, first derived by De Moivre and 200 years later by both Gauss and Laplace independently [2], is often called the bell curve because of its characteristic shape (see the example below).", "The normal distributions occurs often in nature. For example, it describes the commonly occurring distribution of samples influenced by a large number of tiny, random disturbances, each with its own unique distribution [2].", "Mean (\u201ccentre\u201d) of the distribution.", "Standard deviation (spread or \u201cwidth\u201d) of the distribution. Must be non-negative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if loc and scale are both scalars. Otherwise, np.broadcast(loc, scale).size samples are drawn.", "Drawn samples from the parameterized normal distribution.", "See also", "probability density function, distribution or cumulative density function, etc.", "The probability density for the Gaussian distribution is", "where \\(\\mu\\) is the mean and \\(\\sigma\\) the standard deviation. The square of the standard deviation, \\(\\sigma^2\\), is called the variance.", "The function has its peak at the mean, and its \u201cspread\u201d increases with the standard deviation (the function reaches 0.607 times its maximum at \\(x + \\sigma\\) and \\(x - \\sigma\\) [2]). This implies that normal is more likely to return samples lying close to the mean, rather than those far away.", "Wikipedia, \u201cNormal distribution\u201d, https://en.wikipedia.org/wiki/Normal_distribution", "P. R. Peebles Jr., \u201cCentral Limit Theorem\u201d in \u201cProbability, Random Variables and Random Signal Principles\u201d, 4th ed., 2001, pp. 51, 51, 125.", "Draw samples from the distribution:", "Verify the mean and the variance:", "Display the histogram of the samples, along with the probability density function:", "Two-by-four array of samples from N(3, 6.25):"]}, {"name": "random.Generator.pareto()", "path": "reference/random/generated/numpy.random.generator.pareto", "type": "numpy.random.Generator.pareto", "text": ["method", "Draw samples from a Pareto II or Lomax distribution with specified shape.", "The Lomax or Pareto II distribution is a shifted Pareto distribution. The classical Pareto distribution can be obtained from the Lomax distribution by adding 1 and multiplying by the scale parameter m (see Notes). The smallest value of the Lomax distribution is zero while for the classical Pareto distribution it is mu, where the standard Pareto distribution has location mu = 1. Lomax can also be considered as a simplified version of the Generalized Pareto distribution (available in SciPy), with the scale set to one and the location set to zero.", "The Pareto distribution must be greater than zero, and is unbounded above. It is also known as the \u201c80-20 rule\u201d. In this distribution, 80 percent of the weights are in the lowest 20 percent of the range, while the other 20 percent fill the remaining 80 percent of the range.", "Shape of the distribution. Must be positive.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if a is a scalar. Otherwise, np.array(a).size samples are drawn.", "Drawn samples from the parameterized Pareto distribution.", "See also", "probability density function, distribution or cumulative density function, etc.", "probability density function, distribution or cumulative density function, etc.", "The probability density for the Pareto distribution is", "where \\(a\\) is the shape and \\(m\\) the scale.", "The Pareto distribution, named after the Italian economist Vilfredo Pareto, is a power law probability distribution useful in many real world problems. Outside the field of economics it is generally referred to as the Bradford distribution. Pareto developed the distribution to describe the distribution of wealth in an economy. It has also found use in insurance, web page access statistics, oil field sizes, and many other problems, including the download frequency for projects in Sourceforge [1]. It is one of the so-called \u201cfat-tailed\u201d distributions.", "Francis Hunt and Paul Johnson, On the Pareto Distribution of Sourceforge projects.", "Pareto, V. (1896). Course of Political Economy. Lausanne.", "Reiss, R.D., Thomas, M.(2001), Statistical Analysis of Extreme Values, Birkhauser Verlag, Basel, pp 23-30.", "Wikipedia, \u201cPareto distribution\u201d, https://en.wikipedia.org/wiki/Pareto_distribution", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:"]}, {"name": "random.Generator.permutation()", "path": "reference/random/generated/numpy.random.generator.permutation", "type": "numpy.random.Generator.permutation", "text": ["method", "Randomly permute a sequence, or return a permuted range.", "If x is an integer, randomly permute np.arange(x). If x is an array, make a copy and shuffle the elements randomly.", "The axis which x is shuffled along. Default is 0.", "Permuted sequence or array range."]}, {"name": "random.Generator.permuted()", "path": "reference/random/generated/numpy.random.generator.permuted", "type": "numpy.random.Generator.permuted", "text": ["method", "Randomly permute x along axis axis.", "Unlike shuffle, each slice along the given axis is shuffled independently of the others.", "Array to be shuffled.", "Slices of x in this axis are shuffled. Each slice is shuffled independently of the others. If axis is None, the flattened array is shuffled.", "If given, this is the destinaton of the shuffled array. If out is None, a shuffled copy of the array is returned.", "If out is None, a shuffled copy of x is returned. Otherwise, the shuffled array is stored in out, and out is returned", "See also", "Create a numpy.random.Generator instance:", "Create a test array:", "Shuffle the rows of x:", "x has not been modified:", "To shuffle the rows of x in-place, pass x as the out parameter:", "Note that when the out parameter is given, the return value is out:"]}, {"name": "random.Generator.poisson()", "path": "reference/random/generated/numpy.random.generator.poisson", "type": "numpy.random.Generator.poisson", "text": ["method", "Draw samples from a Poisson distribution.", "The Poisson distribution is the limit of the binomial distribution for large N.", "Expected number of events occurring in a fixed-time interval, must be >= 0. A sequence must be broadcastable over the requested size.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if lam is a scalar. Otherwise, np.array(lam).size samples are drawn.", "Drawn samples from the parameterized Poisson distribution.", "The Poisson distribution", "For events with an expected separation \\(\\lambda\\) the Poisson distribution \\(f(k; \\lambda)\\) describes the probability of \\(k\\) events occurring within the observed interval \\(\\lambda\\).", "Because the output is limited to the range of the C int64 type, a ValueError is raised when lam is within 10 sigma of the maximum representable value.", "Weisstein, Eric W. \u201cPoisson Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/PoissonDistribution.html", "Wikipedia, \u201cPoisson distribution\u201d, https://en.wikipedia.org/wiki/Poisson_distribution", "Draw samples from the distribution:", "Display histogram of the sample:", "Draw each 100 values for lambda 100 and 500:"]}, {"name": "random.Generator.power()", "path": "reference/random/generated/numpy.random.generator.power", "type": "numpy.random.Generator.power", "text": ["method", "Draws samples in [0, 1] from a power distribution with positive exponent a - 1.", "Also known as the power function distribution.", "Parameter of the distribution. Must be non-negative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if a is a scalar. Otherwise, np.array(a).size samples are drawn.", "Drawn samples from the parameterized power distribution.", "If a <= 0.", "The probability density function is", "The power function distribution is just the inverse of the Pareto distribution. It may also be seen as a special case of the Beta distribution.", "It is used, for example, in modeling the over-reporting of insurance claims.", "Christian Kleiber, Samuel Kotz, \u201cStatistical size distributions in economics and actuarial sciences\u201d, Wiley, 2003.", "Heckert, N. A. and Filliben, James J. \u201cNIST Handbook 148: Dataplot Reference Manual, Volume 2: Let Subcommands and Library Functions\u201d, National Institute of Standards and Technology Handbook Series, June 2003. https://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/powpdf.pdf", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:", "Compare the power function distribution to the inverse of the Pareto."]}, {"name": "random.Generator.random()", "path": "reference/random/generated/numpy.random.generator.random", "type": "numpy.random.Generator.random", "text": ["method", "Return random floats in the half-open interval [0.0, 1.0).", "Results are from the \u201ccontinuous uniform\u201d distribution over the stated interval. To sample \\(Unif[a, b), b > a\\) multiply the output of random by (b-a) and add a:", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.", "Desired dtype of the result, only float64 and float32 are supported. Byteorder must be native. The default value is np.float64.", "Alternative output array in which to place the result. If size is not None, it must have the same shape as the provided size and must match the type of the output values.", "Array of random floats of shape size (unless size=None, in which case a single float is returned).", "Three-by-two array of random numbers from [-5, 0):"]}, {"name": "random.Generator.rayleigh()", "path": "reference/random/generated/numpy.random.generator.rayleigh", "type": "numpy.random.Generator.rayleigh", "text": ["method", "Draw samples from a Rayleigh distribution.", "The \\(\\chi\\) and Weibull distributions are generalizations of the Rayleigh.", "Scale, also equals the mode. Must be non-negative. Default is 1.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if scale is a scalar. Otherwise, np.array(scale).size samples are drawn.", "Drawn samples from the parameterized Rayleigh distribution.", "The probability density function for the Rayleigh distribution is", "The Rayleigh distribution would arise, for example, if the East and North components of the wind velocity had identical zero-mean Gaussian distributions. Then the wind speed would have a Rayleigh distribution.", "Brighton Webs Ltd., \u201cRayleigh Distribution,\u201d https://web.archive.org/web/20090514091424/http://brighton-webs.co.uk:80/distributions/rayleigh.asp", "Wikipedia, \u201cRayleigh distribution\u201d https://en.wikipedia.org/wiki/Rayleigh_distribution", "Draw values from the distribution and plot the histogram", "Wave heights tend to follow a Rayleigh distribution. If the mean wave height is 1 meter, what fraction of waves are likely to be larger than 3 meters?", "The percentage of waves larger than 3 meters is:"]}, {"name": "random.Generator.shuffle()", "path": "reference/random/generated/numpy.random.generator.shuffle", "type": "numpy.random.Generator.shuffle", "text": ["method", "Modify an array or sequence in-place by shuffling its contents.", "The order of sub-arrays is changed but their contents remains the same.", "The array, list or mutable sequence to be shuffled.", "The axis which x is shuffled along. Default is 0. It is only supported on ndarray objects."]}, {"name": "random.Generator.standard_cauchy()", "path": "reference/random/generated/numpy.random.generator.standard_cauchy", "type": "numpy.random.Generator.standard_cauchy", "text": ["method", "Draw samples from a standard Cauchy distribution with mode = 0.", "Also known as the Lorentz distribution.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.", "The drawn samples.", "The probability density function for the full Cauchy distribution is", "and the Standard Cauchy distribution just sets \\(x_0=0\\) and \\(\\gamma=1\\)", "The Cauchy distribution arises in the solution to the driven harmonic oscillator problem, and also describes spectral line broadening. It also describes the distribution of values at which a line tilted at a random angle will cut the x axis.", "When studying hypothesis tests that assume normality, seeing how the tests perform on data from a Cauchy distribution is a good indicator of their sensitivity to a heavy-tailed distribution, since the Cauchy looks very much like a Gaussian distribution, but with heavier tails.", "NIST/SEMATECH e-Handbook of Statistical Methods, \u201cCauchy Distribution\u201d, https://www.itl.nist.gov/div898/handbook/eda/section3/eda3663.htm", "Weisstein, Eric W. \u201cCauchy Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/CauchyDistribution.html", "Wikipedia, \u201cCauchy distribution\u201d https://en.wikipedia.org/wiki/Cauchy_distribution", "Draw samples and plot the distribution:"]}, {"name": "random.Generator.standard_exponential()", "path": "reference/random/generated/numpy.random.generator.standard_exponential", "type": "numpy.random.Generator.standard_exponential", "text": ["method", "Draw samples from the standard exponential distribution.", "standard_exponential is identical to the exponential distribution with a scale parameter of 1.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.", "Desired dtype of the result, only float64 and float32 are supported. Byteorder must be native. The default value is np.float64.", "Either \u2018inv\u2019 or \u2018zig\u2019. \u2018inv\u2019 uses the default inverse CDF method. \u2018zig\u2019 uses the much faster Ziggurat method of Marsaglia and Tsang.", "Alternative output array in which to place the result. If size is not None, it must have the same shape as the provided size and must match the type of the output values.", "Drawn samples.", "Output a 3x8000 array:"]}, {"name": "random.Generator.standard_gamma()", "path": "reference/random/generated/numpy.random.generator.standard_gamma", "type": "numpy.random.Generator.standard_gamma", "text": ["method", "Draw samples from a standard Gamma distribution.", "Samples are drawn from a Gamma distribution with specified parameters, shape (sometimes designated \u201ck\u201d) and scale=1.", "Parameter, must be non-negative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if shape is a scalar. Otherwise, np.array(shape).size samples are drawn.", "Desired dtype of the result, only float64 and float32 are supported. Byteorder must be native. The default value is np.float64.", "Alternative output array in which to place the result. If size is not None, it must have the same shape as the provided size and must match the type of the output values.", "Drawn samples from the parameterized standard gamma distribution.", "See also", "probability density function, distribution or cumulative density function, etc.", "The probability density for the Gamma distribution is", "where \\(k\\) is the shape and \\(\\theta\\) the scale, and \\(\\Gamma\\) is the Gamma function.", "The Gamma distribution is often used to model the times to failure of electronic components, and arises naturally in processes for which the waiting times between Poisson distributed events are relevant.", "Weisstein, Eric W. \u201cGamma Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/GammaDistribution.html", "Wikipedia, \u201cGamma distribution\u201d, https://en.wikipedia.org/wiki/Gamma_distribution", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:"]}, {"name": "random.Generator.standard_normal()", "path": "reference/random/generated/numpy.random.generator.standard_normal", "type": "numpy.random.Generator.standard_normal", "text": ["method", "Draw samples from a standard Normal distribution (mean=0, stdev=1).", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.", "Desired dtype of the result, only float64 and float32 are supported. Byteorder must be native. The default value is np.float64.", "Alternative output array in which to place the result. If size is not None, it must have the same shape as the provided size and must match the type of the output values.", "A floating-point array of shape size of drawn samples, or a single sample if size was not specified.", "See also", "Equivalent function with additional loc and scale arguments for setting the mean and standard deviation.", "For random samples from \\(N(\\mu, \\sigma^2)\\), use one of:", "Two-by-four array of samples from \\(N(3, 6.25)\\):"]}, {"name": "random.Generator.standard_t()", "path": "reference/random/generated/numpy.random.generator.standard_t", "type": "numpy.random.Generator.standard_t", "text": ["method", "Draw samples from a standard Student\u2019s t distribution with df degrees of freedom.", "A special case of the hyperbolic distribution. As df gets large, the result resembles that of the standard normal distribution (standard_normal).", "Degrees of freedom, must be > 0.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if df is a scalar. Otherwise, np.array(df).size samples are drawn.", "Drawn samples from the parameterized standard Student\u2019s t distribution.", "The probability density function for the t distribution is", "The t test is based on an assumption that the data come from a Normal distribution. The t test provides a way to test whether the sample mean (that is the mean calculated from the data) is a good estimate of the true mean.", "The derivation of the t-distribution was first published in 1908 by William Gosset while working for the Guinness Brewery in Dublin. Due to proprietary issues, he had to publish under a pseudonym, and so he used the name Student.", "Dalgaard, Peter, \u201cIntroductory Statistics With R\u201d, Springer, 2002.", "Wikipedia, \u201cStudent\u2019s t-distribution\u201d https://en.wikipedia.org/wiki/Student\u2019s_t-distribution", "From Dalgaard page 83 [1], suppose the daily energy intake for 11 women in kilojoules (kJ) is:", "Does their energy intake deviate systematically from the recommended value of 7725 kJ? Our null hypothesis will be the absence of deviation, and the alternate hypothesis will be the presence of an effect that could be either positive or negative, hence making our test 2-tailed.", "Because we are estimating the mean and we have N=11 values in our sample, we have N-1=10 degrees of freedom. We set our significance level to 95% and compute the t statistic using the empirical mean and empirical standard deviation of our intake. We use a ddof of 1 to base the computation of our empirical standard deviation on an unbiased estimate of the variance (note: the final estimate is not unbiased due to the concave nature of the square root).", "We draw 1000000 samples from Student\u2019s t distribution with the adequate degrees of freedom.", "Does our t statistic land in one of the two critical regions found at both tails of the distribution?", "The probability value for this 2-tailed test is about 1.83%, which is lower than the 5% pre-determined significance threshold.", "Therefore, the probability of observing values as extreme as our intake conditionally on the null hypothesis being true is too low, and we reject the null hypothesis of no deviation."]}, {"name": "random.Generator.triangular()", "path": "reference/random/generated/numpy.random.generator.triangular", "type": "numpy.random.Generator.triangular", "text": ["method", "Draw samples from the triangular distribution over the interval [left, right].", "The triangular distribution is a continuous probability distribution with lower limit left, peak at mode, and upper limit right. Unlike the other distributions, these parameters directly define the shape of the pdf.", "Lower limit.", "The value where the peak of the distribution occurs. The value must fulfill the condition left <= mode <= right.", "Upper limit, must be larger than left.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if left, mode, and right are all scalars. Otherwise, np.broadcast(left, mode, right).size samples are drawn.", "Drawn samples from the parameterized triangular distribution.", "The probability density function for the triangular distribution is", "The triangular distribution is often used in ill-defined problems where the underlying distribution is not known, but some knowledge of the limits and mode exists. Often it is used in simulations.", "Wikipedia, \u201cTriangular distribution\u201d https://en.wikipedia.org/wiki/Triangular_distribution", "Draw values from the distribution and plot the histogram:"]}, {"name": "random.Generator.uniform()", "path": "reference/random/generated/numpy.random.generator.uniform", "type": "numpy.random.Generator.uniform", "text": ["method", "Draw samples from a uniform distribution.", "Samples are uniformly distributed over the half-open interval [low, high) (includes low, but excludes high). In other words, any value within the given interval is equally likely to be drawn by uniform.", "Lower boundary of the output interval. All values generated will be greater than or equal to low. The default value is 0.", "Upper boundary of the output interval. All values generated will be less than high. The high limit may be included in the returned array of floats due to floating-point rounding in the equation low + (high-low) * random_sample(). high - low must be non-negative. The default value is 1.0.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if low and high are both scalars. Otherwise, np.broadcast(low, high).size samples are drawn.", "Drawn samples from the parameterized uniform distribution.", "See also", "Discrete uniform distribution, yielding integers.", "Floats uniformly distributed over [0, 1).", "The probability density function of the uniform distribution is", "anywhere within the interval [a, b), and zero elsewhere.", "When high == low, values of low will be returned.", "Draw samples from the distribution:", "All values are within the given interval:", "Display the histogram of the samples, along with the probability density function:"]}, {"name": "random.Generator.vonmises()", "path": "reference/random/generated/numpy.random.generator.vonmises", "type": "numpy.random.Generator.vonmises", "text": ["method", "Draw samples from a von Mises distribution.", "Samples are drawn from a von Mises distribution with specified mode (mu) and dispersion (kappa), on the interval [-pi, pi].", "The von Mises distribution (also known as the circular normal distribution) is a continuous probability distribution on the unit circle. It may be thought of as the circular analogue of the normal distribution.", "Mode (\u201ccenter\u201d) of the distribution.", "Dispersion of the distribution, has to be >=0.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if mu and kappa are both scalars. Otherwise, np.broadcast(mu, kappa).size samples are drawn.", "Drawn samples from the parameterized von Mises distribution.", "See also", "probability density function, distribution, or cumulative density function, etc.", "The probability density for the von Mises distribution is", "where \\(\\mu\\) is the mode and \\(\\kappa\\) the dispersion, and \\(I_0(\\kappa)\\) is the modified Bessel function of order 0.", "The von Mises is named for Richard Edler von Mises, who was born in Austria-Hungary, in what is now the Ukraine. He fled to the United States in 1939 and became a professor at Harvard. He worked in probability theory, aerodynamics, fluid mechanics, and philosophy of science.", "Abramowitz, M. and Stegun, I. A. (Eds.). \u201cHandbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing,\u201d New York: Dover, 1972.", "von Mises, R., \u201cMathematical Theory of Probability and Statistics\u201d, New York: Academic Press, 1964.", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:"]}, {"name": "random.Generator.wald()", "path": "reference/random/generated/numpy.random.generator.wald", "type": "numpy.random.Generator.wald", "text": ["method", "Draw samples from a Wald, or inverse Gaussian, distribution.", "As the scale approaches infinity, the distribution becomes more like a Gaussian. Some references claim that the Wald is an inverse Gaussian with mean equal to 1, but this is by no means universal.", "The inverse Gaussian distribution was first studied in relationship to Brownian motion. In 1956 M.C.K. Tweedie used the name inverse Gaussian because there is an inverse relationship between the time to cover a unit distance and distance covered in unit time.", "Distribution mean, must be > 0.", "Scale parameter, must be > 0.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if mean and scale are both scalars. Otherwise, np.broadcast(mean, scale).size samples are drawn.", "Drawn samples from the parameterized Wald distribution.", "The probability density function for the Wald distribution is", "As noted above the inverse Gaussian distribution first arise from attempts to model Brownian motion. It is also a competitor to the Weibull for use in reliability modeling and modeling stock returns and interest rate processes.", "Brighton Webs Ltd., Wald Distribution, https://web.archive.org/web/20090423014010/http://www.brighton-webs.co.uk:80/distributions/wald.asp", "Chhikara, Raj S., and Folks, J. Leroy, \u201cThe Inverse Gaussian Distribution: Theory : Methodology, and Applications\u201d, CRC Press, 1988.", "Wikipedia, \u201cInverse Gaussian distribution\u201d https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution", "Draw values from the distribution and plot the histogram:"]}, {"name": "random.Generator.weibull()", "path": "reference/random/generated/numpy.random.generator.weibull", "type": "numpy.random.Generator.weibull", "text": ["method", "Draw samples from a Weibull distribution.", "Draw samples from a 1-parameter Weibull distribution with the given shape parameter a.", "Here, U is drawn from the uniform distribution over (0,1].", "The more common 2-parameter Weibull, including a scale parameter \\(\\lambda\\) is just \\(X = \\lambda(-ln(U))^{1/a}\\).", "Shape parameter of the distribution. Must be nonnegative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if a is a scalar. Otherwise, np.array(a).size samples are drawn.", "Drawn samples from the parameterized Weibull distribution.", "See also", "The Weibull (or Type III asymptotic extreme value distribution for smallest values, SEV Type III, or Rosin-Rammler distribution) is one of a class of Generalized Extreme Value (GEV) distributions used in modeling extreme value problems. This class includes the Gumbel and Frechet distributions.", "The probability density for the Weibull distribution is", "where \\(a\\) is the shape and \\(\\lambda\\) the scale.", "The function has its peak (the mode) at \\(\\lambda(\\frac{a-1}{a})^{1/a}\\).", "When a = 1, the Weibull distribution reduces to the exponential distribution.", "Waloddi Weibull, Royal Technical University, Stockholm, 1939 \u201cA Statistical Theory Of The Strength Of Materials\u201d, Ingeniorsvetenskapsakademiens Handlingar Nr 151, 1939, Generalstabens Litografiska Anstalts Forlag, Stockholm.", "Waloddi Weibull, \u201cA Statistical Distribution Function of Wide Applicability\u201d, Journal Of Applied Mechanics ASME Paper 1951.", "Wikipedia, \u201cWeibull distribution\u201d, https://en.wikipedia.org/wiki/Weibull_distribution", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:"]}, {"name": "random.Generator.zipf()", "path": "reference/random/generated/numpy.random.generator.zipf", "type": "numpy.random.Generator.zipf", "text": ["method", "Draw samples from a Zipf distribution.", "Samples are drawn from a Zipf distribution with specified parameter a > 1.", "The Zipf distribution (also known as the zeta distribution) is a discrete probability distribution that satisfies Zipf\u2019s law: the frequency of an item is inversely proportional to its rank in a frequency table.", "Distribution parameter. Must be greater than 1.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if a is a scalar. Otherwise, np.array(a).size samples are drawn.", "Drawn samples from the parameterized Zipf distribution.", "See also", "probability density function, distribution, or cumulative density function, etc.", "The probability density for the Zipf distribution is", "for integers \\(k \\geq 1\\), where \\(\\zeta\\) is the Riemann Zeta function.", "It is named for the American linguist George Kingsley Zipf, who noted that the frequency of any word in a sample of a language is inversely proportional to its rank in the frequency table.", "Zipf, G. K., \u201cSelected Studies of the Principle of Relative Frequency in Language,\u201d Cambridge, MA: Harvard Univ. Press, 1932.", "Draw samples from the distribution:", "Display the histogram of the samples, along with the expected histogram based on the probability density function:", "bincount provides a fast histogram for small integers."]}, {"name": "random.geometric()", "path": "reference/random/generated/numpy.random.geometric", "type": "numpy.random.geometric", "text": ["Draw samples from the geometric distribution.", "Bernoulli trials are experiments with one of two outcomes: success or failure (an example of such an experiment is flipping a coin). The geometric distribution models the number of trials that must be run in order to achieve success. It is therefore supported on the positive integers, k = 1, 2, ....", "The probability mass function of the geometric distribution is", "where p is the probability of success of an individual trial.", "Note", "New code should use the geometric method of a default_rng() instance instead; please see the Quick Start.", "The probability of success of an individual trial.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if p is a scalar. Otherwise, np.array(p).size samples are drawn.", "Drawn samples from the parameterized geometric distribution.", "See also", "which should be used for new code.", "Draw ten thousand values from the geometric distribution, with the probability of an individual success equal to 0.35:", "How many trials succeeded after a single run?"]}, {"name": "random.get_state()", "path": "reference/random/generated/numpy.random.get_state", "type": "numpy.random.get_state", "text": ["Return a tuple representing the internal state of the generator.", "For more details, see set_state.", "Flag indicating to return a legacy tuple state when the BitGenerator is MT19937, instead of a dict.", "The returned tuple has the following items:", "If legacy is False, or the BitGenerator is not MT19937, then state is returned as a dictionary.", "See also", "set_state and get_state are not needed to work with any of the random distributions in NumPy. If the internal state is manually altered, the user should know exactly what he/she is doing."]}, {"name": "random.gumbel()", "path": "reference/random/generated/numpy.random.gumbel", "type": "numpy.random.gumbel", "text": ["Draw samples from a Gumbel distribution.", "Draw samples from a Gumbel distribution with specified location and scale. For more information on the Gumbel distribution, see Notes and References below.", "Note", "New code should use the gumbel method of a default_rng() instance instead; please see the Quick Start.", "The location of the mode of the distribution. Default is 0.", "The scale parameter of the distribution. Default is 1. Must be non- negative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if loc and scale are both scalars. Otherwise, np.broadcast(loc, scale).size samples are drawn.", "Drawn samples from the parameterized Gumbel distribution.", "See also", "which should be used for new code.", "The Gumbel (or Smallest Extreme Value (SEV) or the Smallest Extreme Value Type I) distribution is one of a class of Generalized Extreme Value (GEV) distributions used in modeling extreme value problems. The Gumbel is a special case of the Extreme Value Type I distribution for maximums from distributions with \u201cexponential-like\u201d tails.", "The probability density for the Gumbel distribution is", "where \\(\\mu\\) is the mode, a location parameter, and \\(\\beta\\) is the scale parameter.", "The Gumbel (named for German mathematician Emil Julius Gumbel) was used very early in the hydrology literature, for modeling the occurrence of flood events. It is also used for modeling maximum wind speed and rainfall rates. It is a \u201cfat-tailed\u201d distribution - the probability of an event in the tail of the distribution is larger than if one used a Gaussian, hence the surprisingly frequent occurrence of 100-year floods. Floods were initially modeled as a Gaussian process, which underestimated the frequency of extreme events.", "It is one of a class of extreme value distributions, the Generalized Extreme Value (GEV) distributions, which also includes the Weibull and Frechet.", "The function has a mean of \\(\\mu + 0.57721\\beta\\) and a variance of \\(\\frac{\\pi^2}{6}\\beta^2\\).", "Gumbel, E. J., \u201cStatistics of Extremes,\u201d New York: Columbia University Press, 1958.", "Reiss, R.-D. and Thomas, M., \u201cStatistical Analysis of Extreme Values from Insurance, Finance, Hydrology and Other Fields,\u201d Basel: Birkhauser Verlag, 2001.", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:", "Show how an extreme value distribution can arise from a Gaussian process and compare to a Gaussian:"]}, {"name": "random.hypergeometric()", "path": "reference/random/generated/numpy.random.hypergeometric", "type": "numpy.random.hypergeometric", "text": ["Draw samples from a Hypergeometric distribution.", "Samples are drawn from a hypergeometric distribution with specified parameters, ngood (ways to make a good selection), nbad (ways to make a bad selection), and nsample (number of items sampled, which is less than or equal to the sum ngood + nbad).", "Note", "New code should use the hypergeometric method of a default_rng() instance instead; please see the Quick Start.", "Number of ways to make a good selection. Must be nonnegative.", "Number of ways to make a bad selection. Must be nonnegative.", "Number of items sampled. Must be at least 1 and at most ngood + nbad.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if ngood, nbad, and nsample are all scalars. Otherwise, np.broadcast(ngood, nbad, nsample).size samples are drawn.", "Drawn samples from the parameterized hypergeometric distribution. Each sample is the number of good items within a randomly selected subset of size nsample taken from a set of ngood good items and nbad bad items.", "See also", "probability density function, distribution or cumulative density function, etc.", "which should be used for new code.", "The probability density for the Hypergeometric distribution is", "where \\(0 \\le x \\le n\\) and \\(n-b \\le x \\le g\\)", "for P(x) the probability of x good results in the drawn sample, g = ngood, b = nbad, and n = nsample.", "Consider an urn with black and white marbles in it, ngood of them are black and nbad are white. If you draw nsample balls without replacement, then the hypergeometric distribution describes the distribution of black balls in the drawn sample.", "Note that this distribution is very similar to the binomial distribution, except that in this case, samples are drawn without replacement, whereas in the Binomial case samples are drawn with replacement (or the sample space is infinite). As the sample space becomes large, this distribution approaches the binomial.", "Lentner, Marvin, \u201cElementary Applied Statistics\u201d, Bogden and Quigley, 1972.", "Weisstein, Eric W. \u201cHypergeometric Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/HypergeometricDistribution.html", "Wikipedia, \u201cHypergeometric distribution\u201d, https://en.wikipedia.org/wiki/Hypergeometric_distribution", "Draw samples from the distribution:", "Suppose you have an urn with 15 white and 15 black marbles. If you pull 15 marbles at random, how likely is it that 12 or more of them are one color?"]}, {"name": "random.laplace()", "path": "reference/random/generated/numpy.random.laplace", "type": "numpy.random.laplace", "text": ["Draw samples from the Laplace or double exponential distribution with specified location (or mean) and scale (decay).", "The Laplace distribution is similar to the Gaussian/normal distribution, but is sharper at the peak and has fatter tails. It represents the difference between two independent, identically distributed exponential random variables.", "Note", "New code should use the laplace method of a default_rng() instance instead; please see the Quick Start.", "The position, \\(\\mu\\), of the distribution peak. Default is 0.", "\\(\\lambda\\), the exponential decay. Default is 1. Must be non- negative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if loc and scale are both scalars. Otherwise, np.broadcast(loc, scale).size samples are drawn.", "Drawn samples from the parameterized Laplace distribution.", "See also", "which should be used for new code.", "It has the probability density function", "The first law of Laplace, from 1774, states that the frequency of an error can be expressed as an exponential function of the absolute magnitude of the error, which leads to the Laplace distribution. For many problems in economics and health sciences, this distribution seems to model the data better than the standard Gaussian distribution.", "Abramowitz, M. and Stegun, I. A. (Eds.). \u201cHandbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing,\u201d New York: Dover, 1972.", "Kotz, Samuel, et. al. \u201cThe Laplace Distribution and Generalizations, \u201d Birkhauser, 2001.", "Weisstein, Eric W. \u201cLaplace Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/LaplaceDistribution.html", "Wikipedia, \u201cLaplace distribution\u201d, https://en.wikipedia.org/wiki/Laplace_distribution", "Draw samples from the distribution", "Display the histogram of the samples, along with the probability density function:", "Plot Gaussian for comparison:"]}, {"name": "random.logistic()", "path": "reference/random/generated/numpy.random.logistic", "type": "numpy.random.logistic", "text": ["Draw samples from a logistic distribution.", "Samples are drawn from a logistic distribution with specified parameters, loc (location or mean, also median), and scale (>0).", "Note", "New code should use the logistic method of a default_rng() instance instead; please see the Quick Start.", "Parameter of the distribution. Default is 0.", "Parameter of the distribution. Must be non-negative. Default is 1.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if loc and scale are both scalars. Otherwise, np.broadcast(loc, scale).size samples are drawn.", "Drawn samples from the parameterized logistic distribution.", "See also", "probability density function, distribution or cumulative density function, etc.", "which should be used for new code.", "The probability density for the Logistic distribution is", "where \\(\\mu\\) = location and \\(s\\) = scale.", "The Logistic distribution is used in Extreme Value problems where it can act as a mixture of Gumbel distributions, in Epidemiology, and by the World Chess Federation (FIDE) where it is used in the Elo ranking system, assuming the performance of each player is a logistically distributed random variable.", "Reiss, R.-D. and Thomas M. (2001), \u201cStatistical Analysis of Extreme Values, from Insurance, Finance, Hydrology and Other Fields,\u201d Birkhauser Verlag, Basel, pp 132-133.", "Weisstein, Eric W. \u201cLogistic Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/LogisticDistribution.html", "Wikipedia, \u201cLogistic-distribution\u201d, https://en.wikipedia.org/wiki/Logistic_distribution", "Draw samples from the distribution:", "# plot against distribution"]}, {"name": "random.lognormal()", "path": "reference/random/generated/numpy.random.lognormal", "type": "numpy.random.lognormal", "text": ["Draw samples from a log-normal distribution.", "Draw samples from a log-normal distribution with specified mean, standard deviation, and array shape. Note that the mean and standard deviation are not the values for the distribution itself, but of the underlying normal distribution it is derived from.", "Note", "New code should use the lognormal method of a default_rng() instance instead; please see the Quick Start.", "Mean value of the underlying normal distribution. Default is 0.", "Standard deviation of the underlying normal distribution. Must be non-negative. Default is 1.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if mean and sigma are both scalars. Otherwise, np.broadcast(mean, sigma).size samples are drawn.", "Drawn samples from the parameterized log-normal distribution.", "See also", "probability density function, distribution, cumulative density function, etc.", "which should be used for new code.", "A variable x has a log-normal distribution if log(x) is normally distributed. The probability density function for the log-normal distribution is:", "where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation of the normally distributed logarithm of the variable. A log-normal distribution results if a random variable is the product of a large number of independent, identically-distributed variables in the same way that a normal distribution results if the variable is the sum of a large number of independent, identically-distributed variables.", "Limpert, E., Stahel, W. A., and Abbt, M., \u201cLog-normal Distributions across the Sciences: Keys and Clues,\u201d BioScience, Vol. 51, No. 5, May, 2001. https://stat.ethz.ch/~stahel/lognormal/bioscience.pdf", "Reiss, R.D. and Thomas, M., \u201cStatistical Analysis of Extreme Values,\u201d Basel: Birkhauser Verlag, 2001, pp. 31-32.", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:", "Demonstrate that taking the products of random samples from a uniform distribution can be fit well by a log-normal probability density function."]}, {"name": "random.logseries()", "path": "reference/random/generated/numpy.random.logseries", "type": "numpy.random.logseries", "text": ["Draw samples from a logarithmic series distribution.", "Samples are drawn from a log series distribution with specified shape parameter, 0 < p < 1.", "Note", "New code should use the logseries method of a default_rng() instance instead; please see the Quick Start.", "Shape parameter for the distribution. Must be in the range (0, 1).", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if p is a scalar. Otherwise, np.array(p).size samples are drawn.", "Drawn samples from the parameterized logarithmic series distribution.", "See also", "probability density function, distribution or cumulative density function, etc.", "which should be used for new code.", "The probability density for the Log Series distribution is", "where p = probability.", "The log series distribution is frequently used to represent species richness and occurrence, first proposed by Fisher, Corbet, and Williams in 1943 [2]. It may also be used to model the numbers of occupants seen in cars [3].", "Buzas, Martin A.; Culver, Stephen J., Understanding regional species diversity through the log series distribution of occurrences: BIODIVERSITY RESEARCH Diversity & Distributions, Volume 5, Number 5, September 1999 , pp. 187-195(9).", "Fisher, R.A,, A.S. Corbet, and C.B. Williams. 1943. The relation between the number of species and the number of individuals in a random sample of an animal population. Journal of Animal Ecology, 12:42-58.", "D. J. Hand, F. Daly, D. Lunn, E. Ostrowski, A Handbook of Small Data Sets, CRC Press, 1994.", "Wikipedia, \u201cLogarithmic distribution\u201d, https://en.wikipedia.org/wiki/Logarithmic_distribution", "Draw samples from the distribution:", "# plot against distribution"]}, {"name": "random.MT19937.cffi", "path": "reference/random/bit_generators/generated/numpy.random.mt19937.cffi", "type": "MT19937", "text": ["attribute", "CFFI interface", "Named tuple containing CFFI wrapper"]}, {"name": "random.MT19937.ctypes", "path": "reference/random/bit_generators/generated/numpy.random.mt19937.ctypes", "type": "MT19937", "text": ["attribute", "ctypes interface", "Named tuple containing ctypes wrapper"]}, {"name": "random.MT19937.jumped()", "path": "reference/random/bit_generators/generated/numpy.random.mt19937.jumped", "type": "MT19937", "text": ["method", "Returns a new bit generator with the state jumped", "The state of the returned big generator is jumped as-if 2**(128 * jumps) random numbers have been generated.", "Number of times to jump the state of the bit generator returned", "New instance of generator jumped iter times", "The jump step is computed using a modified version of Matsumoto\u2019s implementation of Horner\u2019s method. The step polynomial is precomputed to perform 2**128 steps. The jumped state has been verified to match the state produced using Matsumoto\u2019s original code.", "Matsumoto, M, Generating multiple disjoint streams of pseudorandom number sequences. Accessed on: May 6, 2020. http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/JUMP/", "Hiroshi Haramoto, Makoto Matsumoto, Takuji Nishimura, Fran\u00e7ois Panneton, Pierre L\u2019Ecuyer, \u201cEfficient Jump Ahead for F2-Linear Random Number Generators\u201d, INFORMS JOURNAL ON COMPUTING, Vol. 20, No. 3, Summer 2008, pp. 385-390."]}, {"name": "random.MT19937.state", "path": "reference/random/bit_generators/generated/numpy.random.mt19937.state", "type": "MT19937", "text": ["attribute", "Get or set the PRNG state", "Dictionary containing the information required to describe the state of the PRNG"]}, {"name": "random.multinomial()", "path": "reference/random/generated/numpy.random.multinomial", "type": "numpy.random.multinomial", "text": ["Draw samples from a multinomial distribution.", "The multinomial distribution is a multivariate generalization of the binomial distribution. Take an experiment with one of p possible outcomes. An example of such an experiment is throwing a dice, where the outcome can be 1 through 6. Each sample drawn from the distribution represents n such experiments. Its values, X_i = [X_0, X_1, ..., X_p], represent the number of times the outcome was i.", "Note", "New code should use the multinomial method of a default_rng() instance instead; please see the Quick Start.", "Number of experiments.", "Probabilities of each of the p different outcomes. These must sum to 1 (however, the last element is always assumed to account for the remaining probability, as long as sum(pvals[:-1]) <= 1).", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.", "The drawn samples, of shape size, if that was provided. If not, the shape is (N,).", "In other words, each entry out[i,j,...,:] is an N-dimensional value drawn from the distribution.", "See also", "which should be used for new code.", "Throw a dice 20 times:", "It landed 4 times on 1, once on 2, etc.", "Now, throw the dice 20 times, and 20 times again:", "For the first run, we threw 3 times 1, 4 times 2, etc. For the second, we threw 2 times 1, 4 times 2, etc.", "A loaded die is more likely to land on number 6:", "The probability inputs should be normalized. As an implementation detail, the value of the last entry is ignored and assumed to take up any leftover probability mass, but this should not be relied on. A biased coin which has twice as much weight on one side as on the other should be sampled like so:", "not like:"]}, {"name": "random.multivariate_normal()", "path": "reference/random/generated/numpy.random.multivariate_normal", "type": "numpy.random.multivariate_normal", "text": ["Draw random samples from a multivariate normal distribution.", "The multivariate normal, multinormal or Gaussian distribution is a generalization of the one-dimensional normal distribution to higher dimensions. Such a distribution is specified by its mean and covariance matrix. These parameters are analogous to the mean (average or \u201ccenter\u201d) and variance (standard deviation, or \u201cwidth,\u201d squared) of the one-dimensional normal distribution.", "Note", "New code should use the multivariate_normal method of a default_rng() instance instead; please see the Quick Start.", "Mean of the N-dimensional distribution.", "Covariance matrix of the distribution. It must be symmetric and positive-semidefinite for proper sampling.", "Given a shape of, for example, (m,n,k), m*n*k samples are generated, and packed in an m-by-n-by-k arrangement. Because each sample is N-dimensional, the output shape is (m,n,k,N). If no shape is specified, a single (N-D) sample is returned.", "Behavior when the covariance matrix is not positive semidefinite.", "Tolerance when checking the singular values in covariance matrix. cov is cast to double before the check.", "The drawn samples, of shape size, if that was provided. If not, the shape is (N,).", "In other words, each entry out[i,j,...,:] is an N-dimensional value drawn from the distribution.", "See also", "which should be used for new code.", "The mean is a coordinate in N-dimensional space, which represents the location where samples are most likely to be generated. This is analogous to the peak of the bell curve for the one-dimensional or univariate normal distribution.", "Covariance indicates the level to which two variables vary together. From the multivariate normal distribution, we draw N-dimensional samples, \\(X = [x_1, x_2, ... x_N]\\). The covariance matrix element \\(C_{ij}\\) is the covariance of \\(x_i\\) and \\(x_j\\). The element \\(C_{ii}\\) is the variance of \\(x_i\\) (i.e. its \u201cspread\u201d).", "Instead of specifying the full covariance matrix, popular approximations include:", "This geometrical property can be seen in two dimensions by plotting generated data-points:", "Diagonal covariance means that points are oriented along x or y-axis:", "Note that the covariance matrix must be positive semidefinite (a.k.a. nonnegative-definite). Otherwise, the behavior of this method is undefined and backwards compatibility is not guaranteed.", "Papoulis, A., \u201cProbability, Random Variables, and Stochastic Processes,\u201d 3rd ed., New York: McGraw-Hill, 1991.", "Duda, R. O., Hart, P. E., and Stork, D. G., \u201cPattern Classification,\u201d 2nd ed., New York: Wiley, 2001.", "The following is probably true, given that 0.6 is roughly twice the standard deviation:"]}, {"name": "random.negative_binomial()", "path": "reference/random/generated/numpy.random.negative_binomial", "type": "numpy.random.negative_binomial", "text": ["Draw samples from a negative binomial distribution.", "Samples are drawn from a negative binomial distribution with specified parameters, n successes and p probability of success where n is > 0 and p is in the interval [0, 1].", "Note", "New code should use the negative_binomial method of a default_rng() instance instead; please see the Quick Start.", "Parameter of the distribution, > 0.", "Parameter of the distribution, >= 0 and <=1.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if n and p are both scalars. Otherwise, np.broadcast(n, p).size samples are drawn.", "Drawn samples from the parameterized negative binomial distribution, where each sample is equal to N, the number of failures that occurred before a total of n successes was reached.", "See also", "which should be used for new code.", "The probability mass function of the negative binomial distribution is", "where \\(n\\) is the number of successes, \\(p\\) is the probability of success, \\(N+n\\) is the number of trials, and \\(\\Gamma\\) is the gamma function. When \\(n\\) is an integer, \\(\\frac{\\Gamma(N+n)}{N!\\Gamma(n)} = \\binom{N+n-1}{N}\\), which is the more common form of this term in the the pmf. The negative binomial distribution gives the probability of N failures given n successes, with a success on the last trial.", "If one throws a die repeatedly until the third time a \u201c1\u201d appears, then the probability distribution of the number of non-\u201c1\u201ds that appear before the third \u201c1\u201d is a negative binomial distribution.", "Weisstein, Eric W. \u201cNegative Binomial Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/NegativeBinomialDistribution.html", "Wikipedia, \u201cNegative binomial distribution\u201d, https://en.wikipedia.org/wiki/Negative_binomial_distribution", "Draw samples from the distribution:", "A real world example. A company drills wild-cat oil exploration wells, each with an estimated probability of success of 0.1. What is the probability of having one success for each successive well, that is what is the probability of a single success after drilling 5 wells, after 6 wells, etc.?"]}, {"name": "random.noncentral_chisquare()", "path": "reference/random/generated/numpy.random.noncentral_chisquare", "type": "numpy.random.noncentral_chisquare", "text": ["Draw samples from a noncentral chi-square distribution.", "The noncentral \\(\\chi^2\\) distribution is a generalization of the \\(\\chi^2\\) distribution.", "Note", "New code should use the noncentral_chisquare method of a default_rng() instance instead; please see the Quick Start.", "Degrees of freedom, must be > 0.", "Changed in version 1.10.0: Earlier NumPy versions required dfnum > 1.", "Non-centrality, must be non-negative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if df and nonc are both scalars. Otherwise, np.broadcast(df, nonc).size samples are drawn.", "Drawn samples from the parameterized noncentral chi-square distribution.", "See also", "which should be used for new code.", "The probability density function for the noncentral Chi-square distribution is", "where \\(Y_{q}\\) is the Chi-square with q degrees of freedom.", "Wikipedia, \u201cNoncentral chi-squared distribution\u201d https://en.wikipedia.org/wiki/Noncentral_chi-squared_distribution", "Draw values from the distribution and plot the histogram", "Draw values from a noncentral chisquare with very small noncentrality, and compare to a chisquare.", "Demonstrate how large values of non-centrality lead to a more symmetric distribution."]}, {"name": "random.noncentral_f()", "path": "reference/random/generated/numpy.random.noncentral_f", "type": "numpy.random.noncentral_f", "text": ["Draw samples from the noncentral F distribution.", "Samples are drawn from an F distribution with specified parameters, dfnum (degrees of freedom in numerator) and dfden (degrees of freedom in denominator), where both parameters > 1. nonc is the non-centrality parameter.", "Note", "New code should use the noncentral_f method of a default_rng() instance instead; please see the Quick Start.", "Numerator degrees of freedom, must be > 0.", "Changed in version 1.14.0: Earlier NumPy versions required dfnum > 1.", "Denominator degrees of freedom, must be > 0.", "Non-centrality parameter, the sum of the squares of the numerator means, must be >= 0.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if dfnum, dfden, and nonc are all scalars. Otherwise, np.broadcast(dfnum, dfden, nonc).size samples are drawn.", "Drawn samples from the parameterized noncentral Fisher distribution.", "See also", "which should be used for new code.", "When calculating the power of an experiment (power = probability of rejecting the null hypothesis when a specific alternative is true) the non-central F statistic becomes important. When the null hypothesis is true, the F statistic follows a central F distribution. When the null hypothesis is not true, then it follows a non-central F statistic.", "Weisstein, Eric W. \u201cNoncentral F-Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/NoncentralF-Distribution.html", "Wikipedia, \u201cNoncentral F-distribution\u201d, https://en.wikipedia.org/wiki/Noncentral_F-distribution", "In a study, testing for a specific alternative to the null hypothesis requires use of the Noncentral F distribution. We need to calculate the area in the tail of the distribution that exceeds the value of the F distribution for the null hypothesis. We\u2019ll plot the two probability distributions for comparison."]}, {"name": "random.normal()", "path": "reference/random/generated/numpy.random.normal", "type": "numpy.random.normal", "text": ["Draw random samples from a normal (Gaussian) distribution.", "The probability density function of the normal distribution, first derived by De Moivre and 200 years later by both Gauss and Laplace independently [2], is often called the bell curve because of its characteristic shape (see the example below).", "The normal distributions occurs often in nature. For example, it describes the commonly occurring distribution of samples influenced by a large number of tiny, random disturbances, each with its own unique distribution [2].", "Note", "New code should use the normal method of a default_rng() instance instead; please see the Quick Start.", "Mean (\u201ccentre\u201d) of the distribution.", "Standard deviation (spread or \u201cwidth\u201d) of the distribution. Must be non-negative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if loc and scale are both scalars. Otherwise, np.broadcast(loc, scale).size samples are drawn.", "Drawn samples from the parameterized normal distribution.", "See also", "probability density function, distribution or cumulative density function, etc.", "which should be used for new code.", "The probability density for the Gaussian distribution is", "where \\(\\mu\\) is the mean and \\(\\sigma\\) the standard deviation. The square of the standard deviation, \\(\\sigma^2\\), is called the variance.", "The function has its peak at the mean, and its \u201cspread\u201d increases with the standard deviation (the function reaches 0.607 times its maximum at \\(x + \\sigma\\) and \\(x - \\sigma\\) [2]). This implies that normal is more likely to return samples lying close to the mean, rather than those far away.", "Wikipedia, \u201cNormal distribution\u201d, https://en.wikipedia.org/wiki/Normal_distribution", "P. R. Peebles Jr., \u201cCentral Limit Theorem\u201d in \u201cProbability, Random Variables and Random Signal Principles\u201d, 4th ed., 2001, pp. 51, 51, 125.", "Draw samples from the distribution:", "Verify the mean and the variance:", "Display the histogram of the samples, along with the probability density function:", "Two-by-four array of samples from N(3, 6.25):"]}, {"name": "random.pareto()", "path": "reference/random/generated/numpy.random.pareto", "type": "numpy.random.pareto", "text": ["Draw samples from a Pareto II or Lomax distribution with specified shape.", "The Lomax or Pareto II distribution is a shifted Pareto distribution. The classical Pareto distribution can be obtained from the Lomax distribution by adding 1 and multiplying by the scale parameter m (see Notes). The smallest value of the Lomax distribution is zero while for the classical Pareto distribution it is mu, where the standard Pareto distribution has location mu = 1. Lomax can also be considered as a simplified version of the Generalized Pareto distribution (available in SciPy), with the scale set to one and the location set to zero.", "The Pareto distribution must be greater than zero, and is unbounded above. It is also known as the \u201c80-20 rule\u201d. In this distribution, 80 percent of the weights are in the lowest 20 percent of the range, while the other 20 percent fill the remaining 80 percent of the range.", "Note", "New code should use the pareto method of a default_rng() instance instead; please see the Quick Start.", "Shape of the distribution. Must be positive.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if a is a scalar. Otherwise, np.array(a).size samples are drawn.", "Drawn samples from the parameterized Pareto distribution.", "See also", "probability density function, distribution or cumulative density function, etc.", "probability density function, distribution or cumulative density function, etc.", "which should be used for new code.", "The probability density for the Pareto distribution is", "where \\(a\\) is the shape and \\(m\\) the scale.", "The Pareto distribution, named after the Italian economist Vilfredo Pareto, is a power law probability distribution useful in many real world problems. Outside the field of economics it is generally referred to as the Bradford distribution. Pareto developed the distribution to describe the distribution of wealth in an economy. It has also found use in insurance, web page access statistics, oil field sizes, and many other problems, including the download frequency for projects in Sourceforge [1]. It is one of the so-called \u201cfat-tailed\u201d distributions.", "Francis Hunt and Paul Johnson, On the Pareto Distribution of Sourceforge projects.", "Pareto, V. (1896). Course of Political Economy. Lausanne.", "Reiss, R.D., Thomas, M.(2001), Statistical Analysis of Extreme Values, Birkhauser Verlag, Basel, pp 23-30.", "Wikipedia, \u201cPareto distribution\u201d, https://en.wikipedia.org/wiki/Pareto_distribution", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:"]}, {"name": "random.PCG64.advance()", "path": "reference/random/bit_generators/generated/numpy.random.pcg64.advance", "type": "PCG64", "text": ["method", "Advance the underlying RNG as-if delta draws have occurred.", "Number of draws to advance the RNG. Must be less than the size state variable in the underlying RNG.", "RNG advanced delta steps", "Advancing a RNG updates the underlying RNG state as-if a given number of calls to the underlying RNG have been made. In general there is not a one-to-one relationship between the number output random values from a particular distribution and the number of draws from the core RNG. This occurs for two reasons:", "Advancing the RNG state resets any pre-computed random numbers. This is required to ensure exact reproducibility."]}, {"name": "random.PCG64.cffi", "path": "reference/random/bit_generators/generated/numpy.random.pcg64.cffi", "type": "PCG64", "text": ["attribute", "CFFI interface", "Named tuple containing CFFI wrapper"]}, {"name": "random.PCG64.ctypes", "path": "reference/random/bit_generators/generated/numpy.random.pcg64.ctypes", "type": "PCG64", "text": ["attribute", "ctypes interface", "Named tuple containing ctypes wrapper"]}, {"name": "random.PCG64.jumped()", "path": "reference/random/bit_generators/generated/numpy.random.pcg64.jumped", "type": "PCG64", "text": ["method", "Returns a new bit generator with the state jumped.", "Jumps the state as-if jumps * 210306068529402873165736369884012333109 random numbers have been generated.", "Number of times to jump the state of the bit generator returned", "New instance of generator jumped iter times", "The step size is phi-1 when multiplied by 2**128 where phi is the golden ratio."]}, {"name": "random.PCG64.state", "path": "reference/random/bit_generators/generated/numpy.random.pcg64.state", "type": "PCG64", "text": ["attribute", "Get or set the PRNG state", "Dictionary containing the information required to describe the state of the PRNG"]}, {"name": "random.PCG64DXSM.advance()", "path": "reference/random/bit_generators/generated/numpy.random.pcg64dxsm.advance", "type": "PCG64DXSM", "text": ["method", "Advance the underlying RNG as-if delta draws have occurred.", "Number of draws to advance the RNG. Must be less than the size state variable in the underlying RNG.", "RNG advanced delta steps", "Advancing a RNG updates the underlying RNG state as-if a given number of calls to the underlying RNG have been made. In general there is not a one-to-one relationship between the number output random values from a particular distribution and the number of draws from the core RNG. This occurs for two reasons:", "Advancing the RNG state resets any pre-computed random numbers. This is required to ensure exact reproducibility."]}, {"name": "random.PCG64DXSM.cffi", "path": "reference/random/bit_generators/generated/numpy.random.pcg64dxsm.cffi", "type": "PCG64DXSM", "text": ["attribute", "CFFI interface", "Named tuple containing CFFI wrapper"]}, {"name": "random.PCG64DXSM.ctypes", "path": "reference/random/bit_generators/generated/numpy.random.pcg64dxsm.ctypes", "type": "PCG64DXSM", "text": ["attribute", "ctypes interface", "Named tuple containing ctypes wrapper"]}, {"name": "random.PCG64DXSM.jumped()", "path": "reference/random/bit_generators/generated/numpy.random.pcg64dxsm.jumped", "type": "PCG64DXSM", "text": ["method", "Returns a new bit generator with the state jumped.", "Jumps the state as-if jumps * 210306068529402873165736369884012333109 random numbers have been generated.", "Number of times to jump the state of the bit generator returned", "New instance of generator jumped iter times", "The step size is phi-1 when multiplied by 2**128 where phi is the golden ratio."]}, {"name": "random.PCG64DXSM.state", "path": "reference/random/bit_generators/generated/numpy.random.pcg64dxsm.state", "type": "PCG64DXSM", "text": ["attribute", "Get or set the PRNG state", "Dictionary containing the information required to describe the state of the PRNG"]}, {"name": "random.permutation()", "path": "reference/random/generated/numpy.random.permutation", "type": "numpy.random.permutation", "text": ["Randomly permute a sequence, or return a permuted range.", "If x is a multi-dimensional array, it is only shuffled along its first index.", "Note", "New code should use the permutation method of a default_rng() instance instead; please see the Quick Start.", "If x is an integer, randomly permute np.arange(x). If x is an array, make a copy and shuffle the elements randomly.", "Permuted sequence or array range.", "See also", "which should be used for new code."]}, {"name": "random.Philox.advance()", "path": "reference/random/bit_generators/generated/numpy.random.philox.advance", "type": "Philox", "text": ["method", "Advance the underlying RNG as-if delta draws have occurred.", "Number of draws to advance the RNG. Must be less than the size state variable in the underlying RNG.", "RNG advanced delta steps", "Advancing a RNG updates the underlying RNG state as-if a given number of calls to the underlying RNG have been made. In general there is not a one-to-one relationship between the number output random values from a particular distribution and the number of draws from the core RNG. This occurs for two reasons:", "Advancing the RNG state resets any pre-computed random numbers. This is required to ensure exact reproducibility."]}, {"name": "random.Philox.cffi", "path": "reference/random/bit_generators/generated/numpy.random.philox.cffi", "type": "Philox", "text": ["attribute", "CFFI interface", "Named tuple containing CFFI wrapper"]}, {"name": "random.Philox.ctypes", "path": "reference/random/bit_generators/generated/numpy.random.philox.ctypes", "type": "Philox", "text": ["attribute", "ctypes interface", "Named tuple containing ctypes wrapper"]}, {"name": "random.Philox.jumped()", "path": "reference/random/bit_generators/generated/numpy.random.philox.jumped", "type": "Philox", "text": ["method", "Returns a new bit generator with the state jumped", "The state of the returned big generator is jumped as-if 2**(128 * jumps) random numbers have been generated.", "Number of times to jump the state of the bit generator returned", "New instance of generator jumped iter times"]}, {"name": "random.Philox.state", "path": "reference/random/bit_generators/generated/numpy.random.philox.state", "type": "Philox", "text": ["attribute", "Get or set the PRNG state", "Dictionary containing the information required to describe the state of the PRNG"]}, {"name": "random.poisson()", "path": "reference/random/generated/numpy.random.poisson", "type": "numpy.random.poisson", "text": ["Draw samples from a Poisson distribution.", "The Poisson distribution is the limit of the binomial distribution for large N.", "Note", "New code should use the poisson method of a default_rng() instance instead; please see the Quick Start.", "Expected number of events occurring in a fixed-time interval, must be >= 0. A sequence must be broadcastable over the requested size.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if lam is a scalar. Otherwise, np.array(lam).size samples are drawn.", "Drawn samples from the parameterized Poisson distribution.", "See also", "which should be used for new code.", "The Poisson distribution", "For events with an expected separation \\(\\lambda\\) the Poisson distribution \\(f(k; \\lambda)\\) describes the probability of \\(k\\) events occurring within the observed interval \\(\\lambda\\).", "Because the output is limited to the range of the C int64 type, a ValueError is raised when lam is within 10 sigma of the maximum representable value.", "Weisstein, Eric W. \u201cPoisson Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/PoissonDistribution.html", "Wikipedia, \u201cPoisson distribution\u201d, https://en.wikipedia.org/wiki/Poisson_distribution", "Draw samples from the distribution:", "Display histogram of the sample:", "Draw each 100 values for lambda 100 and 500:"]}, {"name": "random.power()", "path": "reference/random/generated/numpy.random.power", "type": "numpy.random.power", "text": ["Draws samples in [0, 1] from a power distribution with positive exponent a - 1.", "Also known as the power function distribution.", "Note", "New code should use the power method of a default_rng() instance instead; please see the Quick Start.", "Parameter of the distribution. Must be non-negative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if a is a scalar. Otherwise, np.array(a).size samples are drawn.", "Drawn samples from the parameterized power distribution.", "If a <= 0.", "See also", "which should be used for new code.", "The probability density function is", "The power function distribution is just the inverse of the Pareto distribution. It may also be seen as a special case of the Beta distribution.", "It is used, for example, in modeling the over-reporting of insurance claims.", "Christian Kleiber, Samuel Kotz, \u201cStatistical size distributions in economics and actuarial sciences\u201d, Wiley, 2003.", "Heckert, N. A. and Filliben, James J. \u201cNIST Handbook 148: Dataplot Reference Manual, Volume 2: Let Subcommands and Library Functions\u201d, National Institute of Standards and Technology Handbook Series, June 2003. https://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/powpdf.pdf", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:", "Compare the power function distribution to the inverse of the Pareto."]}, {"name": "random.rand()", "path": "reference/random/generated/numpy.random.rand", "type": "numpy.random.rand", "text": ["Random values in a given shape.", "Note", "This is a convenience function for users porting code from Matlab, and wraps random_sample. That function takes a tuple to specify the size of the output, which is consistent with other NumPy functions like numpy.zeros and numpy.ones.", "Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1).", "The dimensions of the returned array, must be non-negative. If no argument is given a single Python float is returned.", "Random values.", "See also"]}, {"name": "random.randint()", "path": "reference/random/generated/numpy.random.randint", "type": "numpy.random.randint", "text": ["Return random integers from low (inclusive) to high (exclusive).", "Return random integers from the \u201cdiscrete uniform\u201d distribution of the specified dtype in the \u201chalf-open\u201d interval [low, high). If high is None (the default), then results are from [0, low).", "Note", "New code should use the integers method of a default_rng() instance instead; please see the Quick Start.", "Lowest (signed) integers to be drawn from the distribution (unless high=None, in which case this parameter is one above the highest such integer).", "If provided, one above the largest (signed) integer to be drawn from the distribution (see above for behavior if high=None). If array-like, must contain integer values", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.", "Desired dtype of the result. Byteorder must be native. The default value is int.", "New in version 1.11.0.", "size-shaped array of random integers from the appropriate distribution, or a single such random int if size not provided.", "See also", "similar to randint, only for the closed interval [low, high], and 1 is the lowest value if high is omitted.", "which should be used for new code.", "Generate a 2 x 4 array of ints between 0 and 4, inclusive:", "Generate a 1 x 3 array with 3 different upper bounds", "Generate a 1 by 3 array with 3 different lower bounds", "Generate a 2 by 4 array using broadcasting with dtype of uint8"]}, {"name": "random.randn()", "path": "reference/random/generated/numpy.random.randn", "type": "numpy.random.randn", "text": ["Return a sample (or samples) from the \u201cstandard normal\u201d distribution.", "Note", "This is a convenience function for users porting code from Matlab, and wraps standard_normal. That function takes a tuple to specify the size of the output, which is consistent with other NumPy functions like numpy.zeros and numpy.ones.", "Note", "New code should use the standard_normal method of a default_rng() instance instead; please see the Quick Start.", "If positive int_like arguments are provided, randn generates an array of shape (d0, d1, ..., dn), filled with random floats sampled from a univariate \u201cnormal\u201d (Gaussian) distribution of mean 0 and variance 1. A single float randomly sampled from the distribution is returned if no argument is provided.", "The dimensions of the returned array, must be non-negative. If no argument is given a single Python float is returned.", "A (d0, d1, ..., dn)-shaped array of floating-point samples from the standard normal distribution, or a single such float if no parameters were supplied.", "See also", "Similar, but takes a tuple as its argument.", "Also accepts mu and sigma arguments.", "which should be used for new code.", "For random samples from \\(N(\\mu, \\sigma^2)\\), use:", "sigma * np.random.randn(...) + mu", "Two-by-four array of samples from N(3, 6.25):"]}, {"name": "random.random()", "path": "reference/random/generated/numpy.random.random", "type": "numpy.random.random", "text": ["Return random floats in the half-open interval [0.0, 1.0). Alias for random_sample to ease forward-porting to the new random API."]}, {"name": "random.random_integers()", "path": "reference/random/generated/numpy.random.random_integers", "type": "numpy.random.random_integers", "text": ["Random integers of type np.int_ between low and high, inclusive.", "Return random integers of type np.int_ from the \u201cdiscrete uniform\u201d distribution in the closed interval [low, high]. If high is None (the default), then results are from [1, low]. The np.int_ type translates to the C long integer type and its precision is platform dependent.", "This function has been deprecated. Use randint instead.", "Deprecated since version 1.11.0.", "Lowest (signed) integer to be drawn from the distribution (unless high=None, in which case this parameter is the highest such integer).", "If provided, the largest (signed) integer to be drawn from the distribution (see above for behavior if high=None).", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.", "size-shaped array of random integers from the appropriate distribution, or a single such random int if size not provided.", "See also", "Similar to random_integers, only for the half-open interval [low, high), and 0 is the lowest value if high is omitted.", "To sample from N evenly spaced floating-point numbers between a and b, use:", "Choose five random numbers from the set of five evenly-spaced numbers between 0 and 2.5, inclusive (i.e., from the set \\({0, 5/8, 10/8, 15/8, 20/8}\\)):", "Roll two six sided dice 1000 times and sum the results:", "Display results as a histogram:"]}, {"name": "random.random_sample()", "path": "reference/random/generated/numpy.random.random_sample", "type": "numpy.random.random_sample", "text": ["Return random floats in the half-open interval [0.0, 1.0).", "Results are from the \u201ccontinuous uniform\u201d distribution over the stated interval. To sample \\(Unif[a, b), b > a\\) multiply the output of random_sample by (b-a) and add a:", "Note", "New code should use the random method of a default_rng() instance instead; please see the Quick Start.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.", "Array of random floats of shape size (unless size=None, in which case a single float is returned).", "See also", "which should be used for new code.", "Three-by-two array of random numbers from [-5, 0):"]}, {"name": "random.RandomState.beta()", "path": "reference/random/generated/numpy.random.randomstate.beta", "type": "numpy.random.RandomState.beta", "text": ["method", "Draw samples from a Beta distribution.", "The Beta distribution is a special case of the Dirichlet distribution, and is related to the Gamma distribution. It has the probability distribution function", "where the normalization, B, is the beta function,", "It is often seen in Bayesian inference and order statistics.", "Note", "New code should use the beta method of a default_rng() instance instead; please see the Quick Start.", "Alpha, positive (>0).", "Beta, positive (>0).", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if a and b are both scalars. Otherwise, np.broadcast(a, b).size samples are drawn.", "Drawn samples from the parameterized beta distribution.", "See also", "which should be used for new code."]}, {"name": "random.RandomState.binomial()", "path": "reference/random/generated/numpy.random.randomstate.binomial", "type": "numpy.random.RandomState.binomial", "text": ["method", "Draw samples from a binomial distribution.", "Samples are drawn from a binomial distribution with specified parameters, n trials and p probability of success where n an integer >= 0 and p is in the interval [0,1]. (n may be input as a float, but it is truncated to an integer in use)", "Note", "New code should use the binomial method of a default_rng() instance instead; please see the Quick Start.", "Parameter of the distribution, >= 0. Floats are also accepted, but they will be truncated to integers.", "Parameter of the distribution, >= 0 and <=1.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if n and p are both scalars. Otherwise, np.broadcast(n, p).size samples are drawn.", "Drawn samples from the parameterized binomial distribution, where each sample is equal to the number of successes over the n trials.", "See also", "probability density function, distribution or cumulative density function, etc.", "which should be used for new code.", "The probability density for the binomial distribution is", "where \\(n\\) is the number of trials, \\(p\\) is the probability of success, and \\(N\\) is the number of successes.", "When estimating the standard error of a proportion in a population by using a random sample, the normal distribution works well unless the product p*n <=5, where p = population proportion estimate, and n = number of samples, in which case the binomial distribution is used instead. For example, a sample of 15 people shows 4 who are left handed, and 11 who are right handed. Then p = 4/15 = 27%. 0.27*15 = 4, so the binomial distribution should be used in this case.", "Dalgaard, Peter, \u201cIntroductory Statistics with R\u201d, Springer-Verlag, 2002.", "Glantz, Stanton A. \u201cPrimer of Biostatistics.\u201d, McGraw-Hill, Fifth Edition, 2002.", "Lentner, Marvin, \u201cElementary Applied Statistics\u201d, Bogden and Quigley, 1972.", "Weisstein, Eric W. \u201cBinomial Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/BinomialDistribution.html", "Wikipedia, \u201cBinomial distribution\u201d, https://en.wikipedia.org/wiki/Binomial_distribution", "Draw samples from the distribution:", "A real world example. A company drills 9 wild-cat oil exploration wells, each with an estimated probability of success of 0.1. All nine wells fail. What is the probability of that happening?", "Let\u2019s do 20,000 trials of the model, and count the number that generate zero positive results."]}, {"name": "random.RandomState.bytes()", "path": "reference/random/generated/numpy.random.randomstate.bytes", "type": "numpy.random.RandomState.bytes", "text": ["method", "Return random bytes.", "Note", "New code should use the bytes method of a default_rng() instance instead; please see the Quick Start.", "Number of random bytes.", "String of length length.", "See also", "which should be used for new code."]}, {"name": "random.RandomState.chisquare()", "path": "reference/random/generated/numpy.random.randomstate.chisquare", "type": "numpy.random.RandomState.chisquare", "text": ["method", "Draw samples from a chi-square distribution.", "When df independent random variables, each with standard normal distributions (mean 0, variance 1), are squared and summed, the resulting distribution is chi-square (see Notes). This distribution is often used in hypothesis testing.", "Note", "New code should use the chisquare method of a default_rng() instance instead; please see the Quick Start.", "Number of degrees of freedom, must be > 0.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if df is a scalar. Otherwise, np.array(df).size samples are drawn.", "Drawn samples from the parameterized chi-square distribution.", "When df <= 0 or when an inappropriate size (e.g. size=-1) is given.", "See also", "which should be used for new code.", "The variable obtained by summing the squares of df independent, standard normally distributed random variables:", "is chi-square distributed, denoted", "The probability density function of the chi-squared distribution is", "where \\(\\Gamma\\) is the gamma function,", "NIST \u201cEngineering Statistics Handbook\u201d https://www.itl.nist.gov/div898/handbook/eda/section3/eda3666.htm"]}, {"name": "random.RandomState.choice()", "path": "reference/random/generated/numpy.random.randomstate.choice", "type": "numpy.random.RandomState.choice", "text": ["method", "Generates a random sample from a given 1-D array", "New in version 1.7.0.", "Note", "New code should use the choice method of a default_rng() instance instead; please see the Quick Start.", "If an ndarray, a random sample is generated from its elements. If an int, the random sample is generated as if it were np.arange(a)", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.", "Whether the sample is with or without replacement. Default is True, meaning that a value of a can be selected multiple times.", "The probabilities associated with each entry in a. If not given, the sample assumes a uniform distribution over all entries in a.", "The generated random samples", "If a is an int and less than zero, if a or p are not 1-dimensional, if a is an array-like of size 0, if p is not a vector of probabilities, if a and p have different lengths, or if replace=False and the sample size is greater than the population size", "See also", "which should be used in new code", "Setting user-specified probabilities through p uses a more general but less efficient sampler than the default. The general sampler produces a different sample than the optimized sampler even if each element of p is 1 / len(a).", "Sampling random rows from a 2-D array is not possible with this function, but is possible with Generator.choice through its axis keyword.", "Generate a uniform random sample from np.arange(5) of size 3:", "Generate a non-uniform random sample from np.arange(5) of size 3:", "Generate a uniform random sample from np.arange(5) of size 3 without replacement:", "Generate a non-uniform random sample from np.arange(5) of size 3 without replacement:", "Any of the above can be repeated with an arbitrary array-like instead of just integers. For instance:"]}, {"name": "random.RandomState.dirichlet()", "path": "reference/random/generated/numpy.random.randomstate.dirichlet", "type": "numpy.random.RandomState.dirichlet", "text": ["method", "Draw samples from the Dirichlet distribution.", "Draw size samples of dimension k from a Dirichlet distribution. A Dirichlet-distributed random variable can be seen as a multivariate generalization of a Beta distribution. The Dirichlet distribution is a conjugate prior of a multinomial distribution in Bayesian inference.", "Note", "New code should use the dirichlet method of a default_rng() instance instead; please see the Quick Start.", "Parameter of the distribution (length k for sample of length k).", "Output shape. If the given shape is, e.g., (m, n), then m * n * k samples are drawn. Default is None, in which case a vector of length k is returned.", "The drawn samples, of shape (size, k).", "If any value in alpha is less than or equal to zero", "See also", "which should be used for new code.", "The Dirichlet distribution is a distribution over vectors \\(x\\) that fulfil the conditions \\(x_i>0\\) and \\(\\sum_{i=1}^k x_i = 1\\).", "The probability density function \\(p\\) of a Dirichlet-distributed random vector \\(X\\) is proportional to", "where \\(\\alpha\\) is a vector containing the positive concentration parameters.", "The method uses the following property for computation: let \\(Y\\) be a random vector which has components that follow a standard gamma distribution, then \\(X = \\frac{1}{\\sum_{i=1}^k{Y_i}} Y\\) is Dirichlet-distributed", "David McKay, \u201cInformation Theory, Inference and Learning Algorithms,\u201d chapter 23, http://www.inference.org.uk/mackay/itila/", "Wikipedia, \u201cDirichlet distribution\u201d, https://en.wikipedia.org/wiki/Dirichlet_distribution", "Taking an example cited in Wikipedia, this distribution can be used if one wanted to cut strings (each of initial length 1.0) into K pieces with different lengths, where each piece had, on average, a designated average length, but allowing some variation in the relative sizes of the pieces."]}, {"name": "random.RandomState.exponential()", "path": "reference/random/generated/numpy.random.randomstate.exponential", "type": "numpy.random.RandomState.exponential", "text": ["method", "Draw samples from an exponential distribution.", "Its probability density function is", "for x > 0 and 0 elsewhere. \\(\\beta\\) is the scale parameter, which is the inverse of the rate parameter \\(\\lambda = 1/\\beta\\). The rate parameter is an alternative, widely used parameterization of the exponential distribution [3].", "The exponential distribution is a continuous analogue of the geometric distribution. It describes many common situations, such as the size of raindrops measured over many rainstorms [1], or the time between page requests to Wikipedia [2].", "Note", "New code should use the exponential method of a default_rng() instance instead; please see the Quick Start.", "The scale parameter, \\(\\beta = 1/\\lambda\\). Must be non-negative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if scale is a scalar. Otherwise, np.array(scale).size samples are drawn.", "Drawn samples from the parameterized exponential distribution.", "See also", "which should be used for new code.", "Peyton Z. Peebles Jr., \u201cProbability, Random Variables and Random Signal Principles\u201d, 4th ed, 2001, p. 57.", "Wikipedia, \u201cPoisson process\u201d, https://en.wikipedia.org/wiki/Poisson_process", "Wikipedia, \u201cExponential distribution\u201d, https://en.wikipedia.org/wiki/Exponential_distribution"]}, {"name": "random.RandomState.f()", "path": "reference/random/generated/numpy.random.randomstate.f", "type": "numpy.random.RandomState.f", "text": ["method", "Draw samples from an F distribution.", "Samples are drawn from an F distribution with specified parameters, dfnum (degrees of freedom in numerator) and dfden (degrees of freedom in denominator), where both parameters must be greater than zero.", "The random variate of the F distribution (also known as the Fisher distribution) is a continuous probability distribution that arises in ANOVA tests, and is the ratio of two chi-square variates.", "Note", "New code should use the f method of a default_rng() instance instead; please see the Quick Start.", "Degrees of freedom in numerator, must be > 0.", "Degrees of freedom in denominator, must be > 0.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if dfnum and dfden are both scalars. Otherwise, np.broadcast(dfnum, dfden).size samples are drawn.", "Drawn samples from the parameterized Fisher distribution.", "See also", "probability density function, distribution or cumulative density function, etc.", "which should be used for new code.", "The F statistic is used to compare in-group variances to between-group variances. Calculating the distribution depends on the sampling, and so it is a function of the respective degrees of freedom in the problem. The variable dfnum is the number of samples minus one, the between-groups degrees of freedom, while dfden is the within-groups degrees of freedom, the sum of the number of samples in each group minus the number of groups.", "Glantz, Stanton A. \u201cPrimer of Biostatistics.\u201d, McGraw-Hill, Fifth Edition, 2002.", "Wikipedia, \u201cF-distribution\u201d, https://en.wikipedia.org/wiki/F-distribution", "An example from Glantz[1], pp 47-40:", "Two groups, children of diabetics (25 people) and children from people without diabetes (25 controls). Fasting blood glucose was measured, case group had a mean value of 86.1, controls had a mean value of 82.2. Standard deviations were 2.09 and 2.49 respectively. Are these data consistent with the null hypothesis that the parents diabetic status does not affect their children\u2019s blood glucose levels? Calculating the F statistic from the data gives a value of 36.01.", "Draw samples from the distribution:", "The lower bound for the top 1% of the samples is :", "So there is about a 1% chance that the F statistic will exceed 7.62, the measured value is 36, so the null hypothesis is rejected at the 1% level."]}, {"name": "random.RandomState.gamma()", "path": "reference/random/generated/numpy.random.randomstate.gamma", "type": "numpy.random.RandomState.gamma", "text": ["method", "Draw samples from a Gamma distribution.", "Samples are drawn from a Gamma distribution with specified parameters, shape (sometimes designated \u201ck\u201d) and scale (sometimes designated \u201ctheta\u201d), where both parameters are > 0.", "Note", "New code should use the gamma method of a default_rng() instance instead; please see the Quick Start.", "The shape of the gamma distribution. Must be non-negative.", "The scale of the gamma distribution. Must be non-negative. Default is equal to 1.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if shape and scale are both scalars. Otherwise, np.broadcast(shape, scale).size samples are drawn.", "Drawn samples from the parameterized gamma distribution.", "See also", "probability density function, distribution or cumulative density function, etc.", "which should be used for new code.", "The probability density for the Gamma distribution is", "where \\(k\\) is the shape and \\(\\theta\\) the scale, and \\(\\Gamma\\) is the Gamma function.", "The Gamma distribution is often used to model the times to failure of electronic components, and arises naturally in processes for which the waiting times between Poisson distributed events are relevant.", "Weisstein, Eric W. \u201cGamma Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/GammaDistribution.html", "Wikipedia, \u201cGamma distribution\u201d, https://en.wikipedia.org/wiki/Gamma_distribution", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:"]}, {"name": "random.RandomState.geometric()", "path": "reference/random/generated/numpy.random.randomstate.geometric", "type": "numpy.random.RandomState.geometric", "text": ["method", "Draw samples from the geometric distribution.", "Bernoulli trials are experiments with one of two outcomes: success or failure (an example of such an experiment is flipping a coin). The geometric distribution models the number of trials that must be run in order to achieve success. It is therefore supported on the positive integers, k = 1, 2, ....", "The probability mass function of the geometric distribution is", "where p is the probability of success of an individual trial.", "Note", "New code should use the geometric method of a default_rng() instance instead; please see the Quick Start.", "The probability of success of an individual trial.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if p is a scalar. Otherwise, np.array(p).size samples are drawn.", "Drawn samples from the parameterized geometric distribution.", "See also", "which should be used for new code.", "Draw ten thousand values from the geometric distribution, with the probability of an individual success equal to 0.35:", "How many trials succeeded after a single run?"]}, {"name": "random.RandomState.get_state()", "path": "reference/random/generated/numpy.random.randomstate.get_state", "type": "numpy.random.RandomState.get_state", "text": ["method", "Return a tuple representing the internal state of the generator.", "For more details, see set_state.", "Flag indicating to return a legacy tuple state when the BitGenerator is MT19937, instead of a dict.", "The returned tuple has the following items:", "If legacy is False, or the BitGenerator is not MT19937, then state is returned as a dictionary.", "See also", "set_state and get_state are not needed to work with any of the random distributions in NumPy. If the internal state is manually altered, the user should know exactly what he/she is doing."]}, {"name": "random.RandomState.gumbel()", "path": "reference/random/generated/numpy.random.randomstate.gumbel", "type": "numpy.random.RandomState.gumbel", "text": ["method", "Draw samples from a Gumbel distribution.", "Draw samples from a Gumbel distribution with specified location and scale. For more information on the Gumbel distribution, see Notes and References below.", "Note", "New code should use the gumbel method of a default_rng() instance instead; please see the Quick Start.", "The location of the mode of the distribution. Default is 0.", "The scale parameter of the distribution. Default is 1. Must be non- negative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if loc and scale are both scalars. Otherwise, np.broadcast(loc, scale).size samples are drawn.", "Drawn samples from the parameterized Gumbel distribution.", "See also", "which should be used for new code.", "The Gumbel (or Smallest Extreme Value (SEV) or the Smallest Extreme Value Type I) distribution is one of a class of Generalized Extreme Value (GEV) distributions used in modeling extreme value problems. The Gumbel is a special case of the Extreme Value Type I distribution for maximums from distributions with \u201cexponential-like\u201d tails.", "The probability density for the Gumbel distribution is", "where \\(\\mu\\) is the mode, a location parameter, and \\(\\beta\\) is the scale parameter.", "The Gumbel (named for German mathematician Emil Julius Gumbel) was used very early in the hydrology literature, for modeling the occurrence of flood events. It is also used for modeling maximum wind speed and rainfall rates. It is a \u201cfat-tailed\u201d distribution - the probability of an event in the tail of the distribution is larger than if one used a Gaussian, hence the surprisingly frequent occurrence of 100-year floods. Floods were initially modeled as a Gaussian process, which underestimated the frequency of extreme events.", "It is one of a class of extreme value distributions, the Generalized Extreme Value (GEV) distributions, which also includes the Weibull and Frechet.", "The function has a mean of \\(\\mu + 0.57721\\beta\\) and a variance of \\(\\frac{\\pi^2}{6}\\beta^2\\).", "Gumbel, E. J., \u201cStatistics of Extremes,\u201d New York: Columbia University Press, 1958.", "Reiss, R.-D. and Thomas, M., \u201cStatistical Analysis of Extreme Values from Insurance, Finance, Hydrology and Other Fields,\u201d Basel: Birkhauser Verlag, 2001.", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:", "Show how an extreme value distribution can arise from a Gaussian process and compare to a Gaussian:"]}, {"name": "random.RandomState.hypergeometric()", "path": "reference/random/generated/numpy.random.randomstate.hypergeometric", "type": "numpy.random.RandomState.hypergeometric", "text": ["method", "Draw samples from a Hypergeometric distribution.", "Samples are drawn from a hypergeometric distribution with specified parameters, ngood (ways to make a good selection), nbad (ways to make a bad selection), and nsample (number of items sampled, which is less than or equal to the sum ngood + nbad).", "Note", "New code should use the hypergeometric method of a default_rng() instance instead; please see the Quick Start.", "Number of ways to make a good selection. Must be nonnegative.", "Number of ways to make a bad selection. Must be nonnegative.", "Number of items sampled. Must be at least 1 and at most ngood + nbad.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if ngood, nbad, and nsample are all scalars. Otherwise, np.broadcast(ngood, nbad, nsample).size samples are drawn.", "Drawn samples from the parameterized hypergeometric distribution. Each sample is the number of good items within a randomly selected subset of size nsample taken from a set of ngood good items and nbad bad items.", "See also", "probability density function, distribution or cumulative density function, etc.", "which should be used for new code.", "The probability density for the Hypergeometric distribution is", "where \\(0 \\le x \\le n\\) and \\(n-b \\le x \\le g\\)", "for P(x) the probability of x good results in the drawn sample, g = ngood, b = nbad, and n = nsample.", "Consider an urn with black and white marbles in it, ngood of them are black and nbad are white. If you draw nsample balls without replacement, then the hypergeometric distribution describes the distribution of black balls in the drawn sample.", "Note that this distribution is very similar to the binomial distribution, except that in this case, samples are drawn without replacement, whereas in the Binomial case samples are drawn with replacement (or the sample space is infinite). As the sample space becomes large, this distribution approaches the binomial.", "Lentner, Marvin, \u201cElementary Applied Statistics\u201d, Bogden and Quigley, 1972.", "Weisstein, Eric W. \u201cHypergeometric Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/HypergeometricDistribution.html", "Wikipedia, \u201cHypergeometric distribution\u201d, https://en.wikipedia.org/wiki/Hypergeometric_distribution", "Draw samples from the distribution:", "Suppose you have an urn with 15 white and 15 black marbles. If you pull 15 marbles at random, how likely is it that 12 or more of them are one color?"]}, {"name": "random.RandomState.laplace()", "path": "reference/random/generated/numpy.random.randomstate.laplace", "type": "numpy.random.RandomState.laplace", "text": ["method", "Draw samples from the Laplace or double exponential distribution with specified location (or mean) and scale (decay).", "The Laplace distribution is similar to the Gaussian/normal distribution, but is sharper at the peak and has fatter tails. It represents the difference between two independent, identically distributed exponential random variables.", "Note", "New code should use the laplace method of a default_rng() instance instead; please see the Quick Start.", "The position, \\(\\mu\\), of the distribution peak. Default is 0.", "\\(\\lambda\\), the exponential decay. Default is 1. Must be non- negative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if loc and scale are both scalars. Otherwise, np.broadcast(loc, scale).size samples are drawn.", "Drawn samples from the parameterized Laplace distribution.", "See also", "which should be used for new code.", "It has the probability density function", "The first law of Laplace, from 1774, states that the frequency of an error can be expressed as an exponential function of the absolute magnitude of the error, which leads to the Laplace distribution. For many problems in economics and health sciences, this distribution seems to model the data better than the standard Gaussian distribution.", "Abramowitz, M. and Stegun, I. A. (Eds.). \u201cHandbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing,\u201d New York: Dover, 1972.", "Kotz, Samuel, et. al. \u201cThe Laplace Distribution and Generalizations, \u201d Birkhauser, 2001.", "Weisstein, Eric W. \u201cLaplace Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/LaplaceDistribution.html", "Wikipedia, \u201cLaplace distribution\u201d, https://en.wikipedia.org/wiki/Laplace_distribution", "Draw samples from the distribution", "Display the histogram of the samples, along with the probability density function:", "Plot Gaussian for comparison:"]}, {"name": "random.RandomState.logistic()", "path": "reference/random/generated/numpy.random.randomstate.logistic", "type": "numpy.random.RandomState.logistic", "text": ["method", "Draw samples from a logistic distribution.", "Samples are drawn from a logistic distribution with specified parameters, loc (location or mean, also median), and scale (>0).", "Note", "New code should use the logistic method of a default_rng() instance instead; please see the Quick Start.", "Parameter of the distribution. Default is 0.", "Parameter of the distribution. Must be non-negative. Default is 1.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if loc and scale are both scalars. Otherwise, np.broadcast(loc, scale).size samples are drawn.", "Drawn samples from the parameterized logistic distribution.", "See also", "probability density function, distribution or cumulative density function, etc.", "which should be used for new code.", "The probability density for the Logistic distribution is", "where \\(\\mu\\) = location and \\(s\\) = scale.", "The Logistic distribution is used in Extreme Value problems where it can act as a mixture of Gumbel distributions, in Epidemiology, and by the World Chess Federation (FIDE) where it is used in the Elo ranking system, assuming the performance of each player is a logistically distributed random variable.", "Reiss, R.-D. and Thomas M. (2001), \u201cStatistical Analysis of Extreme Values, from Insurance, Finance, Hydrology and Other Fields,\u201d Birkhauser Verlag, Basel, pp 132-133.", "Weisstein, Eric W. \u201cLogistic Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/LogisticDistribution.html", "Wikipedia, \u201cLogistic-distribution\u201d, https://en.wikipedia.org/wiki/Logistic_distribution", "Draw samples from the distribution:", "# plot against distribution"]}, {"name": "random.RandomState.lognormal()", "path": "reference/random/generated/numpy.random.randomstate.lognormal", "type": "numpy.random.RandomState.lognormal", "text": ["method", "Draw samples from a log-normal distribution.", "Draw samples from a log-normal distribution with specified mean, standard deviation, and array shape. Note that the mean and standard deviation are not the values for the distribution itself, but of the underlying normal distribution it is derived from.", "Note", "New code should use the lognormal method of a default_rng() instance instead; please see the Quick Start.", "Mean value of the underlying normal distribution. Default is 0.", "Standard deviation of the underlying normal distribution. Must be non-negative. Default is 1.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if mean and sigma are both scalars. Otherwise, np.broadcast(mean, sigma).size samples are drawn.", "Drawn samples from the parameterized log-normal distribution.", "See also", "probability density function, distribution, cumulative density function, etc.", "which should be used for new code.", "A variable x has a log-normal distribution if log(x) is normally distributed. The probability density function for the log-normal distribution is:", "where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation of the normally distributed logarithm of the variable. A log-normal distribution results if a random variable is the product of a large number of independent, identically-distributed variables in the same way that a normal distribution results if the variable is the sum of a large number of independent, identically-distributed variables.", "Limpert, E., Stahel, W. A., and Abbt, M., \u201cLog-normal Distributions across the Sciences: Keys and Clues,\u201d BioScience, Vol. 51, No. 5, May, 2001. https://stat.ethz.ch/~stahel/lognormal/bioscience.pdf", "Reiss, R.D. and Thomas, M., \u201cStatistical Analysis of Extreme Values,\u201d Basel: Birkhauser Verlag, 2001, pp. 31-32.", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:", "Demonstrate that taking the products of random samples from a uniform distribution can be fit well by a log-normal probability density function."]}, {"name": "random.RandomState.logseries()", "path": "reference/random/generated/numpy.random.randomstate.logseries", "type": "numpy.random.RandomState.logseries", "text": ["method", "Draw samples from a logarithmic series distribution.", "Samples are drawn from a log series distribution with specified shape parameter, 0 < p < 1.", "Note", "New code should use the logseries method of a default_rng() instance instead; please see the Quick Start.", "Shape parameter for the distribution. Must be in the range (0, 1).", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if p is a scalar. Otherwise, np.array(p).size samples are drawn.", "Drawn samples from the parameterized logarithmic series distribution.", "See also", "probability density function, distribution or cumulative density function, etc.", "which should be used for new code.", "The probability density for the Log Series distribution is", "where p = probability.", "The log series distribution is frequently used to represent species richness and occurrence, first proposed by Fisher, Corbet, and Williams in 1943 [2]. It may also be used to model the numbers of occupants seen in cars [3].", "Buzas, Martin A.; Culver, Stephen J., Understanding regional species diversity through the log series distribution of occurrences: BIODIVERSITY RESEARCH Diversity & Distributions, Volume 5, Number 5, September 1999 , pp. 187-195(9).", "Fisher, R.A,, A.S. Corbet, and C.B. Williams. 1943. The relation between the number of species and the number of individuals in a random sample of an animal population. Journal of Animal Ecology, 12:42-58.", "D. J. Hand, F. Daly, D. Lunn, E. Ostrowski, A Handbook of Small Data Sets, CRC Press, 1994.", "Wikipedia, \u201cLogarithmic distribution\u201d, https://en.wikipedia.org/wiki/Logarithmic_distribution", "Draw samples from the distribution:", "# plot against distribution"]}, {"name": "random.RandomState.multinomial()", "path": "reference/random/generated/numpy.random.randomstate.multinomial", "type": "numpy.random.RandomState.multinomial", "text": ["method", "Draw samples from a multinomial distribution.", "The multinomial distribution is a multivariate generalization of the binomial distribution. Take an experiment with one of p possible outcomes. An example of such an experiment is throwing a dice, where the outcome can be 1 through 6. Each sample drawn from the distribution represents n such experiments. Its values, X_i = [X_0, X_1, ..., X_p], represent the number of times the outcome was i.", "Note", "New code should use the multinomial method of a default_rng() instance instead; please see the Quick Start.", "Number of experiments.", "Probabilities of each of the p different outcomes. These must sum to 1 (however, the last element is always assumed to account for the remaining probability, as long as sum(pvals[:-1]) <= 1).", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.", "The drawn samples, of shape size, if that was provided. If not, the shape is (N,).", "In other words, each entry out[i,j,...,:] is an N-dimensional value drawn from the distribution.", "See also", "which should be used for new code.", "Throw a dice 20 times:", "It landed 4 times on 1, once on 2, etc.", "Now, throw the dice 20 times, and 20 times again:", "For the first run, we threw 3 times 1, 4 times 2, etc. For the second, we threw 2 times 1, 4 times 2, etc.", "A loaded die is more likely to land on number 6:", "The probability inputs should be normalized. As an implementation detail, the value of the last entry is ignored and assumed to take up any leftover probability mass, but this should not be relied on. A biased coin which has twice as much weight on one side as on the other should be sampled like so:", "not like:"]}, {"name": "random.RandomState.multivariate_normal()", "path": "reference/random/generated/numpy.random.randomstate.multivariate_normal", "type": "numpy.random.RandomState.multivariate_normal", "text": ["method", "Draw random samples from a multivariate normal distribution.", "The multivariate normal, multinormal or Gaussian distribution is a generalization of the one-dimensional normal distribution to higher dimensions. Such a distribution is specified by its mean and covariance matrix. These parameters are analogous to the mean (average or \u201ccenter\u201d) and variance (standard deviation, or \u201cwidth,\u201d squared) of the one-dimensional normal distribution.", "Note", "New code should use the multivariate_normal method of a default_rng() instance instead; please see the Quick Start.", "Mean of the N-dimensional distribution.", "Covariance matrix of the distribution. It must be symmetric and positive-semidefinite for proper sampling.", "Given a shape of, for example, (m,n,k), m*n*k samples are generated, and packed in an m-by-n-by-k arrangement. Because each sample is N-dimensional, the output shape is (m,n,k,N). If no shape is specified, a single (N-D) sample is returned.", "Behavior when the covariance matrix is not positive semidefinite.", "Tolerance when checking the singular values in covariance matrix. cov is cast to double before the check.", "The drawn samples, of shape size, if that was provided. If not, the shape is (N,).", "In other words, each entry out[i,j,...,:] is an N-dimensional value drawn from the distribution.", "See also", "which should be used for new code.", "The mean is a coordinate in N-dimensional space, which represents the location where samples are most likely to be generated. This is analogous to the peak of the bell curve for the one-dimensional or univariate normal distribution.", "Covariance indicates the level to which two variables vary together. From the multivariate normal distribution, we draw N-dimensional samples, \\(X = [x_1, x_2, ... x_N]\\). The covariance matrix element \\(C_{ij}\\) is the covariance of \\(x_i\\) and \\(x_j\\). The element \\(C_{ii}\\) is the variance of \\(x_i\\) (i.e. its \u201cspread\u201d).", "Instead of specifying the full covariance matrix, popular approximations include:", "This geometrical property can be seen in two dimensions by plotting generated data-points:", "Diagonal covariance means that points are oriented along x or y-axis:", "Note that the covariance matrix must be positive semidefinite (a.k.a. nonnegative-definite). Otherwise, the behavior of this method is undefined and backwards compatibility is not guaranteed.", "Papoulis, A., \u201cProbability, Random Variables, and Stochastic Processes,\u201d 3rd ed., New York: McGraw-Hill, 1991.", "Duda, R. O., Hart, P. E., and Stork, D. G., \u201cPattern Classification,\u201d 2nd ed., New York: Wiley, 2001.", "The following is probably true, given that 0.6 is roughly twice the standard deviation:"]}, {"name": "random.RandomState.negative_binomial()", "path": "reference/random/generated/numpy.random.randomstate.negative_binomial", "type": "numpy.random.RandomState.negative_binomial", "text": ["method", "Draw samples from a negative binomial distribution.", "Samples are drawn from a negative binomial distribution with specified parameters, n successes and p probability of success where n is > 0 and p is in the interval [0, 1].", "Note", "New code should use the negative_binomial method of a default_rng() instance instead; please see the Quick Start.", "Parameter of the distribution, > 0.", "Parameter of the distribution, >= 0 and <=1.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if n and p are both scalars. Otherwise, np.broadcast(n, p).size samples are drawn.", "Drawn samples from the parameterized negative binomial distribution, where each sample is equal to N, the number of failures that occurred before a total of n successes was reached.", "See also", "which should be used for new code.", "The probability mass function of the negative binomial distribution is", "where \\(n\\) is the number of successes, \\(p\\) is the probability of success, \\(N+n\\) is the number of trials, and \\(\\Gamma\\) is the gamma function. When \\(n\\) is an integer, \\(\\frac{\\Gamma(N+n)}{N!\\Gamma(n)} = \\binom{N+n-1}{N}\\), which is the more common form of this term in the the pmf. The negative binomial distribution gives the probability of N failures given n successes, with a success on the last trial.", "If one throws a die repeatedly until the third time a \u201c1\u201d appears, then the probability distribution of the number of non-\u201c1\u201ds that appear before the third \u201c1\u201d is a negative binomial distribution.", "Weisstein, Eric W. \u201cNegative Binomial Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/NegativeBinomialDistribution.html", "Wikipedia, \u201cNegative binomial distribution\u201d, https://en.wikipedia.org/wiki/Negative_binomial_distribution", "Draw samples from the distribution:", "A real world example. A company drills wild-cat oil exploration wells, each with an estimated probability of success of 0.1. What is the probability of having one success for each successive well, that is what is the probability of a single success after drilling 5 wells, after 6 wells, etc.?"]}, {"name": "random.RandomState.noncentral_chisquare()", "path": "reference/random/generated/numpy.random.randomstate.noncentral_chisquare", "type": "numpy.random.RandomState.noncentral_chisquare", "text": ["method", "Draw samples from a noncentral chi-square distribution.", "The noncentral \\(\\chi^2\\) distribution is a generalization of the \\(\\chi^2\\) distribution.", "Note", "New code should use the noncentral_chisquare method of a default_rng() instance instead; please see the Quick Start.", "Degrees of freedom, must be > 0.", "Changed in version 1.10.0: Earlier NumPy versions required dfnum > 1.", "Non-centrality, must be non-negative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if df and nonc are both scalars. Otherwise, np.broadcast(df, nonc).size samples are drawn.", "Drawn samples from the parameterized noncentral chi-square distribution.", "See also", "which should be used for new code.", "The probability density function for the noncentral Chi-square distribution is", "where \\(Y_{q}\\) is the Chi-square with q degrees of freedom.", "Wikipedia, \u201cNoncentral chi-squared distribution\u201d https://en.wikipedia.org/wiki/Noncentral_chi-squared_distribution", "Draw values from the distribution and plot the histogram", "Draw values from a noncentral chisquare with very small noncentrality, and compare to a chisquare.", "Demonstrate how large values of non-centrality lead to a more symmetric distribution."]}, {"name": "random.RandomState.noncentral_f()", "path": "reference/random/generated/numpy.random.randomstate.noncentral_f", "type": "numpy.random.RandomState.noncentral_f", "text": ["method", "Draw samples from the noncentral F distribution.", "Samples are drawn from an F distribution with specified parameters, dfnum (degrees of freedom in numerator) and dfden (degrees of freedom in denominator), where both parameters > 1. nonc is the non-centrality parameter.", "Note", "New code should use the noncentral_f method of a default_rng() instance instead; please see the Quick Start.", "Numerator degrees of freedom, must be > 0.", "Changed in version 1.14.0: Earlier NumPy versions required dfnum > 1.", "Denominator degrees of freedom, must be > 0.", "Non-centrality parameter, the sum of the squares of the numerator means, must be >= 0.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if dfnum, dfden, and nonc are all scalars. Otherwise, np.broadcast(dfnum, dfden, nonc).size samples are drawn.", "Drawn samples from the parameterized noncentral Fisher distribution.", "See also", "which should be used for new code.", "When calculating the power of an experiment (power = probability of rejecting the null hypothesis when a specific alternative is true) the non-central F statistic becomes important. When the null hypothesis is true, the F statistic follows a central F distribution. When the null hypothesis is not true, then it follows a non-central F statistic.", "Weisstein, Eric W. \u201cNoncentral F-Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/NoncentralF-Distribution.html", "Wikipedia, \u201cNoncentral F-distribution\u201d, https://en.wikipedia.org/wiki/Noncentral_F-distribution", "In a study, testing for a specific alternative to the null hypothesis requires use of the Noncentral F distribution. We need to calculate the area in the tail of the distribution that exceeds the value of the F distribution for the null hypothesis. We\u2019ll plot the two probability distributions for comparison."]}, {"name": "random.RandomState.normal()", "path": "reference/random/generated/numpy.random.randomstate.normal", "type": "numpy.random.RandomState.normal", "text": ["method", "Draw random samples from a normal (Gaussian) distribution.", "The probability density function of the normal distribution, first derived by De Moivre and 200 years later by both Gauss and Laplace independently [2], is often called the bell curve because of its characteristic shape (see the example below).", "The normal distributions occurs often in nature. For example, it describes the commonly occurring distribution of samples influenced by a large number of tiny, random disturbances, each with its own unique distribution [2].", "Note", "New code should use the normal method of a default_rng() instance instead; please see the Quick Start.", "Mean (\u201ccentre\u201d) of the distribution.", "Standard deviation (spread or \u201cwidth\u201d) of the distribution. Must be non-negative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if loc and scale are both scalars. Otherwise, np.broadcast(loc, scale).size samples are drawn.", "Drawn samples from the parameterized normal distribution.", "See also", "probability density function, distribution or cumulative density function, etc.", "which should be used for new code.", "The probability density for the Gaussian distribution is", "where \\(\\mu\\) is the mean and \\(\\sigma\\) the standard deviation. The square of the standard deviation, \\(\\sigma^2\\), is called the variance.", "The function has its peak at the mean, and its \u201cspread\u201d increases with the standard deviation (the function reaches 0.607 times its maximum at \\(x + \\sigma\\) and \\(x - \\sigma\\) [2]). This implies that normal is more likely to return samples lying close to the mean, rather than those far away.", "Wikipedia, \u201cNormal distribution\u201d, https://en.wikipedia.org/wiki/Normal_distribution", "P. R. Peebles Jr., \u201cCentral Limit Theorem\u201d in \u201cProbability, Random Variables and Random Signal Principles\u201d, 4th ed., 2001, pp. 51, 51, 125.", "Draw samples from the distribution:", "Verify the mean and the variance:", "Display the histogram of the samples, along with the probability density function:", "Two-by-four array of samples from N(3, 6.25):"]}, {"name": "random.RandomState.pareto()", "path": "reference/random/generated/numpy.random.randomstate.pareto", "type": "numpy.random.RandomState.pareto", "text": ["method", "Draw samples from a Pareto II or Lomax distribution with specified shape.", "The Lomax or Pareto II distribution is a shifted Pareto distribution. The classical Pareto distribution can be obtained from the Lomax distribution by adding 1 and multiplying by the scale parameter m (see Notes). The smallest value of the Lomax distribution is zero while for the classical Pareto distribution it is mu, where the standard Pareto distribution has location mu = 1. Lomax can also be considered as a simplified version of the Generalized Pareto distribution (available in SciPy), with the scale set to one and the location set to zero.", "The Pareto distribution must be greater than zero, and is unbounded above. It is also known as the \u201c80-20 rule\u201d. In this distribution, 80 percent of the weights are in the lowest 20 percent of the range, while the other 20 percent fill the remaining 80 percent of the range.", "Note", "New code should use the pareto method of a default_rng() instance instead; please see the Quick Start.", "Shape of the distribution. Must be positive.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if a is a scalar. Otherwise, np.array(a).size samples are drawn.", "Drawn samples from the parameterized Pareto distribution.", "See also", "probability density function, distribution or cumulative density function, etc.", "probability density function, distribution or cumulative density function, etc.", "which should be used for new code.", "The probability density for the Pareto distribution is", "where \\(a\\) is the shape and \\(m\\) the scale.", "The Pareto distribution, named after the Italian economist Vilfredo Pareto, is a power law probability distribution useful in many real world problems. Outside the field of economics it is generally referred to as the Bradford distribution. Pareto developed the distribution to describe the distribution of wealth in an economy. It has also found use in insurance, web page access statistics, oil field sizes, and many other problems, including the download frequency for projects in Sourceforge [1]. It is one of the so-called \u201cfat-tailed\u201d distributions.", "Francis Hunt and Paul Johnson, On the Pareto Distribution of Sourceforge projects.", "Pareto, V. (1896). Course of Political Economy. Lausanne.", "Reiss, R.D., Thomas, M.(2001), Statistical Analysis of Extreme Values, Birkhauser Verlag, Basel, pp 23-30.", "Wikipedia, \u201cPareto distribution\u201d, https://en.wikipedia.org/wiki/Pareto_distribution", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:"]}, {"name": "random.RandomState.permutation()", "path": "reference/random/generated/numpy.random.randomstate.permutation", "type": "numpy.random.RandomState.permutation", "text": ["method", "Randomly permute a sequence, or return a permuted range.", "If x is a multi-dimensional array, it is only shuffled along its first index.", "Note", "New code should use the permutation method of a default_rng() instance instead; please see the Quick Start.", "If x is an integer, randomly permute np.arange(x). If x is an array, make a copy and shuffle the elements randomly.", "Permuted sequence or array range.", "See also", "which should be used for new code."]}, {"name": "random.RandomState.poisson()", "path": "reference/random/generated/numpy.random.randomstate.poisson", "type": "numpy.random.RandomState.poisson", "text": ["method", "Draw samples from a Poisson distribution.", "The Poisson distribution is the limit of the binomial distribution for large N.", "Note", "New code should use the poisson method of a default_rng() instance instead; please see the Quick Start.", "Expected number of events occurring in a fixed-time interval, must be >= 0. A sequence must be broadcastable over the requested size.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if lam is a scalar. Otherwise, np.array(lam).size samples are drawn.", "Drawn samples from the parameterized Poisson distribution.", "See also", "which should be used for new code.", "The Poisson distribution", "For events with an expected separation \\(\\lambda\\) the Poisson distribution \\(f(k; \\lambda)\\) describes the probability of \\(k\\) events occurring within the observed interval \\(\\lambda\\).", "Because the output is limited to the range of the C int64 type, a ValueError is raised when lam is within 10 sigma of the maximum representable value.", "Weisstein, Eric W. \u201cPoisson Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/PoissonDistribution.html", "Wikipedia, \u201cPoisson distribution\u201d, https://en.wikipedia.org/wiki/Poisson_distribution", "Draw samples from the distribution:", "Display histogram of the sample:", "Draw each 100 values for lambda 100 and 500:"]}, {"name": "random.RandomState.power()", "path": "reference/random/generated/numpy.random.randomstate.power", "type": "numpy.random.RandomState.power", "text": ["method", "Draws samples in [0, 1] from a power distribution with positive exponent a - 1.", "Also known as the power function distribution.", "Note", "New code should use the power method of a default_rng() instance instead; please see the Quick Start.", "Parameter of the distribution. Must be non-negative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if a is a scalar. Otherwise, np.array(a).size samples are drawn.", "Drawn samples from the parameterized power distribution.", "If a <= 0.", "See also", "which should be used for new code.", "The probability density function is", "The power function distribution is just the inverse of the Pareto distribution. It may also be seen as a special case of the Beta distribution.", "It is used, for example, in modeling the over-reporting of insurance claims.", "Christian Kleiber, Samuel Kotz, \u201cStatistical size distributions in economics and actuarial sciences\u201d, Wiley, 2003.", "Heckert, N. A. and Filliben, James J. \u201cNIST Handbook 148: Dataplot Reference Manual, Volume 2: Let Subcommands and Library Functions\u201d, National Institute of Standards and Technology Handbook Series, June 2003. https://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/powpdf.pdf", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:", "Compare the power function distribution to the inverse of the Pareto."]}, {"name": "random.RandomState.rand()", "path": "reference/random/generated/numpy.random.randomstate.rand", "type": "numpy.random.RandomState.rand", "text": ["method", "Random values in a given shape.", "Note", "This is a convenience function for users porting code from Matlab, and wraps random_sample. That function takes a tuple to specify the size of the output, which is consistent with other NumPy functions like numpy.zeros and numpy.ones.", "Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1).", "The dimensions of the returned array, must be non-negative. If no argument is given a single Python float is returned.", "Random values.", "See also"]}, {"name": "random.RandomState.randint()", "path": "reference/random/generated/numpy.random.randomstate.randint", "type": "numpy.random.RandomState.randint", "text": ["method", "Return random integers from low (inclusive) to high (exclusive).", "Return random integers from the \u201cdiscrete uniform\u201d distribution of the specified dtype in the \u201chalf-open\u201d interval [low, high). If high is None (the default), then results are from [0, low).", "Note", "New code should use the integers method of a default_rng() instance instead; please see the Quick Start.", "Lowest (signed) integers to be drawn from the distribution (unless high=None, in which case this parameter is one above the highest such integer).", "If provided, one above the largest (signed) integer to be drawn from the distribution (see above for behavior if high=None). If array-like, must contain integer values", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.", "Desired dtype of the result. Byteorder must be native. The default value is int.", "New in version 1.11.0.", "size-shaped array of random integers from the appropriate distribution, or a single such random int if size not provided.", "See also", "similar to randint, only for the closed interval [low, high], and 1 is the lowest value if high is omitted.", "which should be used for new code.", "Generate a 2 x 4 array of ints between 0 and 4, inclusive:", "Generate a 1 x 3 array with 3 different upper bounds", "Generate a 1 by 3 array with 3 different lower bounds", "Generate a 2 by 4 array using broadcasting with dtype of uint8"]}, {"name": "random.RandomState.randn()", "path": "reference/random/generated/numpy.random.randomstate.randn", "type": "numpy.random.RandomState.randn", "text": ["method", "Return a sample (or samples) from the \u201cstandard normal\u201d distribution.", "Note", "This is a convenience function for users porting code from Matlab, and wraps standard_normal. That function takes a tuple to specify the size of the output, which is consistent with other NumPy functions like numpy.zeros and numpy.ones.", "Note", "New code should use the standard_normal method of a default_rng() instance instead; please see the Quick Start.", "If positive int_like arguments are provided, randn generates an array of shape (d0, d1, ..., dn), filled with random floats sampled from a univariate \u201cnormal\u201d (Gaussian) distribution of mean 0 and variance 1. A single float randomly sampled from the distribution is returned if no argument is provided.", "The dimensions of the returned array, must be non-negative. If no argument is given a single Python float is returned.", "A (d0, d1, ..., dn)-shaped array of floating-point samples from the standard normal distribution, or a single such float if no parameters were supplied.", "See also", "Similar, but takes a tuple as its argument.", "Also accepts mu and sigma arguments.", "which should be used for new code.", "For random samples from \\(N(\\mu, \\sigma^2)\\), use:", "sigma * np.random.randn(...) + mu", "Two-by-four array of samples from N(3, 6.25):"]}, {"name": "random.RandomState.random_integers()", "path": "reference/random/generated/numpy.random.randomstate.random_integers", "type": "numpy.random.RandomState.random_integers", "text": ["method", "Random integers of type np.int_ between low and high, inclusive.", "Return random integers of type np.int_ from the \u201cdiscrete uniform\u201d distribution in the closed interval [low, high]. If high is None (the default), then results are from [1, low]. The np.int_ type translates to the C long integer type and its precision is platform dependent.", "This function has been deprecated. Use randint instead.", "Deprecated since version 1.11.0.", "Lowest (signed) integer to be drawn from the distribution (unless high=None, in which case this parameter is the highest such integer).", "If provided, the largest (signed) integer to be drawn from the distribution (see above for behavior if high=None).", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.", "size-shaped array of random integers from the appropriate distribution, or a single such random int if size not provided.", "See also", "Similar to random_integers, only for the half-open interval [low, high), and 0 is the lowest value if high is omitted.", "To sample from N evenly spaced floating-point numbers between a and b, use:", "Choose five random numbers from the set of five evenly-spaced numbers between 0 and 2.5, inclusive (i.e., from the set \\({0, 5/8, 10/8, 15/8, 20/8}\\)):", "Roll two six sided dice 1000 times and sum the results:", "Display results as a histogram:"]}, {"name": "random.RandomState.random_sample()", "path": "reference/random/generated/numpy.random.randomstate.random_sample", "type": "numpy.random.RandomState.random_sample", "text": ["method", "Return random floats in the half-open interval [0.0, 1.0).", "Results are from the \u201ccontinuous uniform\u201d distribution over the stated interval. To sample \\(Unif[a, b), b > a\\) multiply the output of random_sample by (b-a) and add a:", "Note", "New code should use the random method of a default_rng() instance instead; please see the Quick Start.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.", "Array of random floats of shape size (unless size=None, in which case a single float is returned).", "See also", "which should be used for new code.", "Three-by-two array of random numbers from [-5, 0):"]}, {"name": "random.RandomState.rayleigh()", "path": "reference/random/generated/numpy.random.randomstate.rayleigh", "type": "numpy.random.RandomState.rayleigh", "text": ["method", "Draw samples from a Rayleigh distribution.", "The \\(\\chi\\) and Weibull distributions are generalizations of the Rayleigh.", "Note", "New code should use the rayleigh method of a default_rng() instance instead; please see the Quick Start.", "Scale, also equals the mode. Must be non-negative. Default is 1.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if scale is a scalar. Otherwise, np.array(scale).size samples are drawn.", "Drawn samples from the parameterized Rayleigh distribution.", "See also", "which should be used for new code.", "The probability density function for the Rayleigh distribution is", "The Rayleigh distribution would arise, for example, if the East and North components of the wind velocity had identical zero-mean Gaussian distributions. Then the wind speed would have a Rayleigh distribution.", "Brighton Webs Ltd., \u201cRayleigh Distribution,\u201d https://web.archive.org/web/20090514091424/http://brighton-webs.co.uk:80/distributions/rayleigh.asp", "Wikipedia, \u201cRayleigh distribution\u201d https://en.wikipedia.org/wiki/Rayleigh_distribution", "Draw values from the distribution and plot the histogram", "Wave heights tend to follow a Rayleigh distribution. If the mean wave height is 1 meter, what fraction of waves are likely to be larger than 3 meters?", "The percentage of waves larger than 3 meters is:"]}, {"name": "random.RandomState.seed()", "path": "reference/random/generated/numpy.random.randomstate.seed", "type": "numpy.random.RandomState.seed", "text": ["method", "Reseed a legacy MT19937 BitGenerator", "This is a convenience, legacy function.", "The best practice is to not reseed a BitGenerator, rather to recreate a new one. This method is here for legacy reasons. This example demonstrates best practice."]}, {"name": "random.RandomState.set_state()", "path": "reference/random/generated/numpy.random.randomstate.set_state", "type": "numpy.random.RandomState.set_state", "text": ["method", "Set the internal state of the generator from a tuple.", "For use if one has reason to manually (re-)set the internal state of the bit generator used by the RandomState instance. By default, RandomState uses the \u201cMersenne Twister\u201d[1] pseudo-random number generating algorithm.", "The state tuple has the following items:", "If state is a dictionary, it is directly set using the BitGenerators state property.", "Returns \u2018None\u2019 on success.", "See also", "set_state and get_state are not needed to work with any of the random distributions in NumPy. If the internal state is manually altered, the user should know exactly what he/she is doing.", "For backwards compatibility, the form (str, array of 624 uints, int) is also accepted although it is missing some information about the cached Gaussian value: state = ('MT19937', keys, pos).", "M. Matsumoto and T. Nishimura, \u201cMersenne Twister: A 623-dimensionally equidistributed uniform pseudorandom number generator,\u201d ACM Trans. on Modeling and Computer Simulation, Vol. 8, No. 1, pp. 3-30, Jan. 1998."]}, {"name": "random.RandomState.shuffle()", "path": "reference/random/generated/numpy.random.randomstate.shuffle", "type": "numpy.random.RandomState.shuffle", "text": ["method", "Modify a sequence in-place by shuffling its contents.", "This function only shuffles the array along the first axis of a multi-dimensional array. The order of sub-arrays is changed but their contents remains the same.", "Note", "New code should use the shuffle method of a default_rng() instance instead; please see the Quick Start.", "The array, list or mutable sequence to be shuffled.", "See also", "which should be used for new code.", "Multi-dimensional arrays are only shuffled along the first axis:"]}, {"name": "random.RandomState.standard_cauchy()", "path": "reference/random/generated/numpy.random.randomstate.standard_cauchy", "type": "numpy.random.RandomState.standard_cauchy", "text": ["method", "Draw samples from a standard Cauchy distribution with mode = 0.", "Also known as the Lorentz distribution.", "Note", "New code should use the standard_cauchy method of a default_rng() instance instead; please see the Quick Start.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.", "The drawn samples.", "See also", "which should be used for new code.", "The probability density function for the full Cauchy distribution is", "and the Standard Cauchy distribution just sets \\(x_0=0\\) and \\(\\gamma=1\\)", "The Cauchy distribution arises in the solution to the driven harmonic oscillator problem, and also describes spectral line broadening. It also describes the distribution of values at which a line tilted at a random angle will cut the x axis.", "When studying hypothesis tests that assume normality, seeing how the tests perform on data from a Cauchy distribution is a good indicator of their sensitivity to a heavy-tailed distribution, since the Cauchy looks very much like a Gaussian distribution, but with heavier tails.", "NIST/SEMATECH e-Handbook of Statistical Methods, \u201cCauchy Distribution\u201d, https://www.itl.nist.gov/div898/handbook/eda/section3/eda3663.htm", "Weisstein, Eric W. \u201cCauchy Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/CauchyDistribution.html", "Wikipedia, \u201cCauchy distribution\u201d https://en.wikipedia.org/wiki/Cauchy_distribution", "Draw samples and plot the distribution:"]}, {"name": "random.RandomState.standard_exponential()", "path": "reference/random/generated/numpy.random.randomstate.standard_exponential", "type": "numpy.random.RandomState.standard_exponential", "text": ["method", "Draw samples from the standard exponential distribution.", "standard_exponential is identical to the exponential distribution with a scale parameter of 1.", "Note", "New code should use the standard_exponential method of a default_rng() instance instead; please see the Quick Start.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.", "Drawn samples.", "See also", "which should be used for new code.", "Output a 3x8000 array:"]}, {"name": "random.RandomState.standard_gamma()", "path": "reference/random/generated/numpy.random.randomstate.standard_gamma", "type": "numpy.random.RandomState.standard_gamma", "text": ["method", "Draw samples from a standard Gamma distribution.", "Samples are drawn from a Gamma distribution with specified parameters, shape (sometimes designated \u201ck\u201d) and scale=1.", "Note", "New code should use the standard_gamma method of a default_rng() instance instead; please see the Quick Start.", "Parameter, must be non-negative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if shape is a scalar. Otherwise, np.array(shape).size samples are drawn.", "Drawn samples from the parameterized standard gamma distribution.", "See also", "probability density function, distribution or cumulative density function, etc.", "which should be used for new code.", "The probability density for the Gamma distribution is", "where \\(k\\) is the shape and \\(\\theta\\) the scale, and \\(\\Gamma\\) is the Gamma function.", "The Gamma distribution is often used to model the times to failure of electronic components, and arises naturally in processes for which the waiting times between Poisson distributed events are relevant.", "Weisstein, Eric W. \u201cGamma Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/GammaDistribution.html", "Wikipedia, \u201cGamma distribution\u201d, https://en.wikipedia.org/wiki/Gamma_distribution", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:"]}, {"name": "random.RandomState.standard_normal()", "path": "reference/random/generated/numpy.random.randomstate.standard_normal", "type": "numpy.random.RandomState.standard_normal", "text": ["method", "Draw samples from a standard Normal distribution (mean=0, stdev=1).", "Note", "New code should use the standard_normal method of a default_rng() instance instead; please see the Quick Start.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.", "A floating-point array of shape size of drawn samples, or a single sample if size was not specified.", "See also", "Equivalent function with additional loc and scale arguments for setting the mean and standard deviation.", "which should be used for new code.", "For random samples from \\(N(\\mu, \\sigma^2)\\), use one of:", "Two-by-four array of samples from \\(N(3, 6.25)\\):"]}, {"name": "random.RandomState.standard_t()", "path": "reference/random/generated/numpy.random.randomstate.standard_t", "type": "numpy.random.RandomState.standard_t", "text": ["method", "Draw samples from a standard Student\u2019s t distribution with df degrees of freedom.", "A special case of the hyperbolic distribution. As df gets large, the result resembles that of the standard normal distribution (standard_normal).", "Note", "New code should use the standard_t method of a default_rng() instance instead; please see the Quick Start.", "Degrees of freedom, must be > 0.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if df is a scalar. Otherwise, np.array(df).size samples are drawn.", "Drawn samples from the parameterized standard Student\u2019s t distribution.", "See also", "which should be used for new code.", "The probability density function for the t distribution is", "The t test is based on an assumption that the data come from a Normal distribution. The t test provides a way to test whether the sample mean (that is the mean calculated from the data) is a good estimate of the true mean.", "The derivation of the t-distribution was first published in 1908 by William Gosset while working for the Guinness Brewery in Dublin. Due to proprietary issues, he had to publish under a pseudonym, and so he used the name Student.", "Dalgaard, Peter, \u201cIntroductory Statistics With R\u201d, Springer, 2002.", "Wikipedia, \u201cStudent\u2019s t-distribution\u201d https://en.wikipedia.org/wiki/Student\u2019s_t-distribution", "From Dalgaard page 83 [1], suppose the daily energy intake for 11 women in kilojoules (kJ) is:", "Does their energy intake deviate systematically from the recommended value of 7725 kJ? Our null hypothesis will be the absence of deviation, and the alternate hypothesis will be the presence of an effect that could be either positive or negative, hence making our test 2-tailed.", "Because we are estimating the mean and we have N=11 values in our sample, we have N-1=10 degrees of freedom. We set our significance level to 95% and compute the t statistic using the empirical mean and empirical standard deviation of our intake. We use a ddof of 1 to base the computation of our empirical standard deviation on an unbiased estimate of the variance (note: the final estimate is not unbiased due to the concave nature of the square root).", "We draw 1000000 samples from Student\u2019s t distribution with the adequate degrees of freedom.", "Does our t statistic land in one of the two critical regions found at both tails of the distribution?", "The probability value for this 2-tailed test is about 1.83%, which is lower than the 5% pre-determined significance threshold.", "Therefore, the probability of observing values as extreme as our intake conditionally on the null hypothesis being true is too low, and we reject the null hypothesis of no deviation."]}, {"name": "random.RandomState.triangular()", "path": "reference/random/generated/numpy.random.randomstate.triangular", "type": "numpy.random.RandomState.triangular", "text": ["method", "Draw samples from the triangular distribution over the interval [left, right].", "The triangular distribution is a continuous probability distribution with lower limit left, peak at mode, and upper limit right. Unlike the other distributions, these parameters directly define the shape of the pdf.", "Note", "New code should use the triangular method of a default_rng() instance instead; please see the Quick Start.", "Lower limit.", "The value where the peak of the distribution occurs. The value must fulfill the condition left <= mode <= right.", "Upper limit, must be larger than left.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if left, mode, and right are all scalars. Otherwise, np.broadcast(left, mode, right).size samples are drawn.", "Drawn samples from the parameterized triangular distribution.", "See also", "which should be used for new code.", "The probability density function for the triangular distribution is", "The triangular distribution is often used in ill-defined problems where the underlying distribution is not known, but some knowledge of the limits and mode exists. Often it is used in simulations.", "Wikipedia, \u201cTriangular distribution\u201d https://en.wikipedia.org/wiki/Triangular_distribution", "Draw values from the distribution and plot the histogram:"]}, {"name": "random.RandomState.uniform()", "path": "reference/random/generated/numpy.random.randomstate.uniform", "type": "numpy.random.RandomState.uniform", "text": ["method", "Draw samples from a uniform distribution.", "Samples are uniformly distributed over the half-open interval [low, high) (includes low, but excludes high). In other words, any value within the given interval is equally likely to be drawn by uniform.", "Note", "New code should use the uniform method of a default_rng() instance instead; please see the Quick Start.", "Lower boundary of the output interval. All values generated will be greater than or equal to low. The default value is 0.", "Upper boundary of the output interval. All values generated will be less than or equal to high. The high limit may be included in the returned array of floats due to floating-point rounding in the equation low + (high-low) * random_sample(). The default value is 1.0.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if low and high are both scalars. Otherwise, np.broadcast(low, high).size samples are drawn.", "Drawn samples from the parameterized uniform distribution.", "See also", "Discrete uniform distribution, yielding integers.", "Discrete uniform distribution over the closed interval [low, high].", "Floats uniformly distributed over [0, 1).", "Alias for random_sample.", "Convenience function that accepts dimensions as input, e.g., rand(2,2) would generate a 2-by-2 array of floats, uniformly distributed over [0, 1).", "which should be used for new code.", "The probability density function of the uniform distribution is", "anywhere within the interval [a, b), and zero elsewhere.", "When high == low, values of low will be returned. If high < low, the results are officially undefined and may eventually raise an error, i.e. do not rely on this function to behave when passed arguments satisfying that inequality condition. The high limit may be included in the returned array of floats due to floating-point rounding in the equation low + (high-low) * random_sample(). For example:", "Draw samples from the distribution:", "All values are within the given interval:", "Display the histogram of the samples, along with the probability density function:"]}, {"name": "random.RandomState.vonmises()", "path": "reference/random/generated/numpy.random.randomstate.vonmises", "type": "numpy.random.RandomState.vonmises", "text": ["method", "Draw samples from a von Mises distribution.", "Samples are drawn from a von Mises distribution with specified mode (mu) and dispersion (kappa), on the interval [-pi, pi].", "The von Mises distribution (also known as the circular normal distribution) is a continuous probability distribution on the unit circle. It may be thought of as the circular analogue of the normal distribution.", "Note", "New code should use the vonmises method of a default_rng() instance instead; please see the Quick Start.", "Mode (\u201ccenter\u201d) of the distribution.", "Dispersion of the distribution, has to be >=0.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if mu and kappa are both scalars. Otherwise, np.broadcast(mu, kappa).size samples are drawn.", "Drawn samples from the parameterized von Mises distribution.", "See also", "probability density function, distribution, or cumulative density function, etc.", "which should be used for new code.", "The probability density for the von Mises distribution is", "where \\(\\mu\\) is the mode and \\(\\kappa\\) the dispersion, and \\(I_0(\\kappa)\\) is the modified Bessel function of order 0.", "The von Mises is named for Richard Edler von Mises, who was born in Austria-Hungary, in what is now the Ukraine. He fled to the United States in 1939 and became a professor at Harvard. He worked in probability theory, aerodynamics, fluid mechanics, and philosophy of science.", "Abramowitz, M. and Stegun, I. A. (Eds.). \u201cHandbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing,\u201d New York: Dover, 1972.", "von Mises, R., \u201cMathematical Theory of Probability and Statistics\u201d, New York: Academic Press, 1964.", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:"]}, {"name": "random.RandomState.wald()", "path": "reference/random/generated/numpy.random.randomstate.wald", "type": "numpy.random.RandomState.wald", "text": ["method", "Draw samples from a Wald, or inverse Gaussian, distribution.", "As the scale approaches infinity, the distribution becomes more like a Gaussian. Some references claim that the Wald is an inverse Gaussian with mean equal to 1, but this is by no means universal.", "The inverse Gaussian distribution was first studied in relationship to Brownian motion. In 1956 M.C.K. Tweedie used the name inverse Gaussian because there is an inverse relationship between the time to cover a unit distance and distance covered in unit time.", "Note", "New code should use the wald method of a default_rng() instance instead; please see the Quick Start.", "Distribution mean, must be > 0.", "Scale parameter, must be > 0.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if mean and scale are both scalars. Otherwise, np.broadcast(mean, scale).size samples are drawn.", "Drawn samples from the parameterized Wald distribution.", "See also", "which should be used for new code.", "The probability density function for the Wald distribution is", "As noted above the inverse Gaussian distribution first arise from attempts to model Brownian motion. It is also a competitor to the Weibull for use in reliability modeling and modeling stock returns and interest rate processes.", "Brighton Webs Ltd., Wald Distribution, https://web.archive.org/web/20090423014010/http://www.brighton-webs.co.uk:80/distributions/wald.asp", "Chhikara, Raj S., and Folks, J. Leroy, \u201cThe Inverse Gaussian Distribution: Theory : Methodology, and Applications\u201d, CRC Press, 1988.", "Wikipedia, \u201cInverse Gaussian distribution\u201d https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution", "Draw values from the distribution and plot the histogram:"]}, {"name": "random.RandomState.weibull()", "path": "reference/random/generated/numpy.random.randomstate.weibull", "type": "numpy.random.RandomState.weibull", "text": ["method", "Draw samples from a Weibull distribution.", "Draw samples from a 1-parameter Weibull distribution with the given shape parameter a.", "Here, U is drawn from the uniform distribution over (0,1].", "The more common 2-parameter Weibull, including a scale parameter \\(\\lambda\\) is just \\(X = \\lambda(-ln(U))^{1/a}\\).", "Note", "New code should use the weibull method of a default_rng() instance instead; please see the Quick Start.", "Shape parameter of the distribution. Must be nonnegative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if a is a scalar. Otherwise, np.array(a).size samples are drawn.", "Drawn samples from the parameterized Weibull distribution.", "See also", "which should be used for new code.", "The Weibull (or Type III asymptotic extreme value distribution for smallest values, SEV Type III, or Rosin-Rammler distribution) is one of a class of Generalized Extreme Value (GEV) distributions used in modeling extreme value problems. This class includes the Gumbel and Frechet distributions.", "The probability density for the Weibull distribution is", "where \\(a\\) is the shape and \\(\\lambda\\) the scale.", "The function has its peak (the mode) at \\(\\lambda(\\frac{a-1}{a})^{1/a}\\).", "When a = 1, the Weibull distribution reduces to the exponential distribution.", "Waloddi Weibull, Royal Technical University, Stockholm, 1939 \u201cA Statistical Theory Of The Strength Of Materials\u201d, Ingeniorsvetenskapsakademiens Handlingar Nr 151, 1939, Generalstabens Litografiska Anstalts Forlag, Stockholm.", "Waloddi Weibull, \u201cA Statistical Distribution Function of Wide Applicability\u201d, Journal Of Applied Mechanics ASME Paper 1951.", "Wikipedia, \u201cWeibull distribution\u201d, https://en.wikipedia.org/wiki/Weibull_distribution", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:"]}, {"name": "random.RandomState.zipf()", "path": "reference/random/generated/numpy.random.randomstate.zipf", "type": "numpy.random.RandomState.zipf", "text": ["method", "Draw samples from a Zipf distribution.", "Samples are drawn from a Zipf distribution with specified parameter a > 1.", "The Zipf distribution (also known as the zeta distribution) is a discrete probability distribution that satisfies Zipf\u2019s law: the frequency of an item is inversely proportional to its rank in a frequency table.", "Note", "New code should use the zipf method of a default_rng() instance instead; please see the Quick Start.", "Distribution parameter. Must be greater than 1.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if a is a scalar. Otherwise, np.array(a).size samples are drawn.", "Drawn samples from the parameterized Zipf distribution.", "See also", "probability density function, distribution, or cumulative density function, etc.", "which should be used for new code.", "The probability density for the Zipf distribution is", "for integers \\(k \\geq 1\\), where \\(\\zeta\\) is the Riemann Zeta function.", "It is named for the American linguist George Kingsley Zipf, who noted that the frequency of any word in a sample of a language is inversely proportional to its rank in the frequency table.", "Zipf, G. K., \u201cSelected Studies of the Principle of Relative Frequency in Language,\u201d Cambridge, MA: Harvard Univ. Press, 1932.", "Draw samples from the distribution:", "Display the histogram of the samples, along with the expected histogram based on the probability density function:", "bincount provides a fast histogram for small integers."]}, {"name": "random.ranf()", "path": "reference/random/generated/numpy.random.ranf", "type": "numpy.random.ranf", "text": ["This is an alias of random_sample. See random_sample for the complete documentation."]}, {"name": "random.rayleigh()", "path": "reference/random/generated/numpy.random.rayleigh", "type": "numpy.random.rayleigh", "text": ["Draw samples from a Rayleigh distribution.", "The \\(\\chi\\) and Weibull distributions are generalizations of the Rayleigh.", "Note", "New code should use the rayleigh method of a default_rng() instance instead; please see the Quick Start.", "Scale, also equals the mode. Must be non-negative. Default is 1.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if scale is a scalar. Otherwise, np.array(scale).size samples are drawn.", "Drawn samples from the parameterized Rayleigh distribution.", "See also", "which should be used for new code.", "The probability density function for the Rayleigh distribution is", "The Rayleigh distribution would arise, for example, if the East and North components of the wind velocity had identical zero-mean Gaussian distributions. Then the wind speed would have a Rayleigh distribution.", "Brighton Webs Ltd., \u201cRayleigh Distribution,\u201d https://web.archive.org/web/20090514091424/http://brighton-webs.co.uk:80/distributions/rayleigh.asp", "Wikipedia, \u201cRayleigh distribution\u201d https://en.wikipedia.org/wiki/Rayleigh_distribution", "Draw values from the distribution and plot the histogram", "Wave heights tend to follow a Rayleigh distribution. If the mean wave height is 1 meter, what fraction of waves are likely to be larger than 3 meters?", "The percentage of waves larger than 3 meters is:"]}, {"name": "random.sample()", "path": "reference/random/generated/numpy.random.sample", "type": "numpy.random.sample", "text": ["This is an alias of random_sample. See random_sample for the complete documentation."]}, {"name": "random.seed()", "path": "reference/random/generated/numpy.random.seed", "type": "numpy.random.seed", "text": ["Reseed a legacy MT19937 BitGenerator", "This is a convenience, legacy function.", "The best practice is to not reseed a BitGenerator, rather to recreate a new one. This method is here for legacy reasons. This example demonstrates best practice."]}, {"name": "random.SeedSequence.entropy", "path": "reference/random/bit_generators/generated/numpy.random.seedsequence.entropy", "type": "Random sampling", "text": ["attribute"]}, {"name": "random.SeedSequence.generate_state()", "path": "reference/random/bit_generators/generated/numpy.random.seedsequence.generate_state", "type": "Random sampling ( \n      \n       numpy.random\n      \n      )", "text": ["method", "Return the requested number of words for PRNG seeding.", "A BitGenerator should call this method in its constructor with an appropriate n_words parameter to properly seed itself.", "The size of each word. This should only be either uint32 or uint64. Strings (\u2018uint32\u2019, \u2018uint64\u2019) are fine. Note that requesting uint64 will draw twice as many bits as uint32 for the same n_words. This is a convenience for BitGenerator`s that express their states as `uint64 arrays."]}, {"name": "random.SeedSequence.spawn()", "path": "reference/random/bit_generators/generated/numpy.random.seedsequence.spawn", "type": "Random sampling ( \n      \n       numpy.random\n      \n      )", "text": ["method", "Spawn a number of child SeedSequence s by extending the spawn_key."]}, {"name": "random.SeedSequence.spawn_key", "path": "reference/random/bit_generators/generated/numpy.random.seedsequence.spawn_key", "type": "Random sampling", "text": ["attribute"]}, {"name": "random.set_state()", "path": "reference/random/generated/numpy.random.set_state", "type": "numpy.random.set_state", "text": ["Set the internal state of the generator from a tuple.", "For use if one has reason to manually (re-)set the internal state of the bit generator used by the RandomState instance. By default, RandomState uses the \u201cMersenne Twister\u201d[1] pseudo-random number generating algorithm.", "The state tuple has the following items:", "If state is a dictionary, it is directly set using the BitGenerators state property.", "Returns \u2018None\u2019 on success.", "See also", "set_state and get_state are not needed to work with any of the random distributions in NumPy. If the internal state is manually altered, the user should know exactly what he/she is doing.", "For backwards compatibility, the form (str, array of 624 uints, int) is also accepted although it is missing some information about the cached Gaussian value: state = ('MT19937', keys, pos).", "M. Matsumoto and T. Nishimura, \u201cMersenne Twister: A 623-dimensionally equidistributed uniform pseudorandom number generator,\u201d ACM Trans. on Modeling and Computer Simulation, Vol. 8, No. 1, pp. 3-30, Jan. 1998."]}, {"name": "random.SFC64.cffi", "path": "reference/random/bit_generators/generated/numpy.random.sfc64.cffi", "type": "SFC64", "text": ["attribute", "CFFI interface", "Named tuple containing CFFI wrapper"]}, {"name": "random.SFC64.ctypes", "path": "reference/random/bit_generators/generated/numpy.random.sfc64.ctypes", "type": "SFC64", "text": ["attribute", "ctypes interface", "Named tuple containing ctypes wrapper"]}, {"name": "random.SFC64.state", "path": "reference/random/bit_generators/generated/numpy.random.sfc64.state", "type": "SFC64", "text": ["attribute", "Get or set the PRNG state", "Dictionary containing the information required to describe the state of the PRNG"]}, {"name": "random.shuffle()", "path": "reference/random/generated/numpy.random.shuffle", "type": "numpy.random.shuffle", "text": ["Modify a sequence in-place by shuffling its contents.", "This function only shuffles the array along the first axis of a multi-dimensional array. The order of sub-arrays is changed but their contents remains the same.", "Note", "New code should use the shuffle method of a default_rng() instance instead; please see the Quick Start.", "The array, list or mutable sequence to be shuffled.", "See also", "which should be used for new code.", "Multi-dimensional arrays are only shuffled along the first axis:"]}, {"name": "random.standard_cauchy()", "path": "reference/random/generated/numpy.random.standard_cauchy", "type": "numpy.random.standard_cauchy", "text": ["Draw samples from a standard Cauchy distribution with mode = 0.", "Also known as the Lorentz distribution.", "Note", "New code should use the standard_cauchy method of a default_rng() instance instead; please see the Quick Start.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.", "The drawn samples.", "See also", "which should be used for new code.", "The probability density function for the full Cauchy distribution is", "and the Standard Cauchy distribution just sets \\(x_0=0\\) and \\(\\gamma=1\\)", "The Cauchy distribution arises in the solution to the driven harmonic oscillator problem, and also describes spectral line broadening. It also describes the distribution of values at which a line tilted at a random angle will cut the x axis.", "When studying hypothesis tests that assume normality, seeing how the tests perform on data from a Cauchy distribution is a good indicator of their sensitivity to a heavy-tailed distribution, since the Cauchy looks very much like a Gaussian distribution, but with heavier tails.", "NIST/SEMATECH e-Handbook of Statistical Methods, \u201cCauchy Distribution\u201d, https://www.itl.nist.gov/div898/handbook/eda/section3/eda3663.htm", "Weisstein, Eric W. \u201cCauchy Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/CauchyDistribution.html", "Wikipedia, \u201cCauchy distribution\u201d https://en.wikipedia.org/wiki/Cauchy_distribution", "Draw samples and plot the distribution:"]}, {"name": "random.standard_exponential()", "path": "reference/random/generated/numpy.random.standard_exponential", "type": "numpy.random.standard_exponential", "text": ["Draw samples from the standard exponential distribution.", "standard_exponential is identical to the exponential distribution with a scale parameter of 1.", "Note", "New code should use the standard_exponential method of a default_rng() instance instead; please see the Quick Start.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.", "Drawn samples.", "See also", "which should be used for new code.", "Output a 3x8000 array:"]}, {"name": "random.standard_gamma()", "path": "reference/random/generated/numpy.random.standard_gamma", "type": "numpy.random.standard_gamma", "text": ["Draw samples from a standard Gamma distribution.", "Samples are drawn from a Gamma distribution with specified parameters, shape (sometimes designated \u201ck\u201d) and scale=1.", "Note", "New code should use the standard_gamma method of a default_rng() instance instead; please see the Quick Start.", "Parameter, must be non-negative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if shape is a scalar. Otherwise, np.array(shape).size samples are drawn.", "Drawn samples from the parameterized standard gamma distribution.", "See also", "probability density function, distribution or cumulative density function, etc.", "which should be used for new code.", "The probability density for the Gamma distribution is", "where \\(k\\) is the shape and \\(\\theta\\) the scale, and \\(\\Gamma\\) is the Gamma function.", "The Gamma distribution is often used to model the times to failure of electronic components, and arises naturally in processes for which the waiting times between Poisson distributed events are relevant.", "Weisstein, Eric W. \u201cGamma Distribution.\u201d From MathWorld\u2013A Wolfram Web Resource. http://mathworld.wolfram.com/GammaDistribution.html", "Wikipedia, \u201cGamma distribution\u201d, https://en.wikipedia.org/wiki/Gamma_distribution", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:"]}, {"name": "random.standard_normal()", "path": "reference/random/generated/numpy.random.standard_normal", "type": "numpy.random.standard_normal", "text": ["Draw samples from a standard Normal distribution (mean=0, stdev=1).", "Note", "New code should use the standard_normal method of a default_rng() instance instead; please see the Quick Start.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.", "A floating-point array of shape size of drawn samples, or a single sample if size was not specified.", "See also", "Equivalent function with additional loc and scale arguments for setting the mean and standard deviation.", "which should be used for new code.", "For random samples from \\(N(\\mu, \\sigma^2)\\), use one of:", "Two-by-four array of samples from \\(N(3, 6.25)\\):"]}, {"name": "random.standard_t()", "path": "reference/random/generated/numpy.random.standard_t", "type": "numpy.random.standard_t", "text": ["Draw samples from a standard Student\u2019s t distribution with df degrees of freedom.", "A special case of the hyperbolic distribution. As df gets large, the result resembles that of the standard normal distribution (standard_normal).", "Note", "New code should use the standard_t method of a default_rng() instance instead; please see the Quick Start.", "Degrees of freedom, must be > 0.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if df is a scalar. Otherwise, np.array(df).size samples are drawn.", "Drawn samples from the parameterized standard Student\u2019s t distribution.", "See also", "which should be used for new code.", "The probability density function for the t distribution is", "The t test is based on an assumption that the data come from a Normal distribution. The t test provides a way to test whether the sample mean (that is the mean calculated from the data) is a good estimate of the true mean.", "The derivation of the t-distribution was first published in 1908 by William Gosset while working for the Guinness Brewery in Dublin. Due to proprietary issues, he had to publish under a pseudonym, and so he used the name Student.", "Dalgaard, Peter, \u201cIntroductory Statistics With R\u201d, Springer, 2002.", "Wikipedia, \u201cStudent\u2019s t-distribution\u201d https://en.wikipedia.org/wiki/Student\u2019s_t-distribution", "From Dalgaard page 83 [1], suppose the daily energy intake for 11 women in kilojoules (kJ) is:", "Does their energy intake deviate systematically from the recommended value of 7725 kJ? Our null hypothesis will be the absence of deviation, and the alternate hypothesis will be the presence of an effect that could be either positive or negative, hence making our test 2-tailed.", "Because we are estimating the mean and we have N=11 values in our sample, we have N-1=10 degrees of freedom. We set our significance level to 95% and compute the t statistic using the empirical mean and empirical standard deviation of our intake. We use a ddof of 1 to base the computation of our empirical standard deviation on an unbiased estimate of the variance (note: the final estimate is not unbiased due to the concave nature of the square root).", "We draw 1000000 samples from Student\u2019s t distribution with the adequate degrees of freedom.", "Does our t statistic land in one of the two critical regions found at both tails of the distribution?", "The probability value for this 2-tailed test is about 1.83%, which is lower than the 5% pre-determined significance threshold.", "Therefore, the probability of observing values as extreme as our intake conditionally on the null hypothesis being true is too low, and we reject the null hypothesis of no deviation."]}, {"name": "random.triangular()", "path": "reference/random/generated/numpy.random.triangular", "type": "numpy.random.triangular", "text": ["Draw samples from the triangular distribution over the interval [left, right].", "The triangular distribution is a continuous probability distribution with lower limit left, peak at mode, and upper limit right. Unlike the other distributions, these parameters directly define the shape of the pdf.", "Note", "New code should use the triangular method of a default_rng() instance instead; please see the Quick Start.", "Lower limit.", "The value where the peak of the distribution occurs. The value must fulfill the condition left <= mode <= right.", "Upper limit, must be larger than left.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if left, mode, and right are all scalars. Otherwise, np.broadcast(left, mode, right).size samples are drawn.", "Drawn samples from the parameterized triangular distribution.", "See also", "which should be used for new code.", "The probability density function for the triangular distribution is", "The triangular distribution is often used in ill-defined problems where the underlying distribution is not known, but some knowledge of the limits and mode exists. Often it is used in simulations.", "Wikipedia, \u201cTriangular distribution\u201d https://en.wikipedia.org/wiki/Triangular_distribution", "Draw values from the distribution and plot the histogram:"]}, {"name": "random.uniform()", "path": "reference/random/generated/numpy.random.uniform", "type": "numpy.random.uniform", "text": ["Draw samples from a uniform distribution.", "Samples are uniformly distributed over the half-open interval [low, high) (includes low, but excludes high). In other words, any value within the given interval is equally likely to be drawn by uniform.", "Note", "New code should use the uniform method of a default_rng() instance instead; please see the Quick Start.", "Lower boundary of the output interval. All values generated will be greater than or equal to low. The default value is 0.", "Upper boundary of the output interval. All values generated will be less than or equal to high. The high limit may be included in the returned array of floats due to floating-point rounding in the equation low + (high-low) * random_sample(). The default value is 1.0.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if low and high are both scalars. Otherwise, np.broadcast(low, high).size samples are drawn.", "Drawn samples from the parameterized uniform distribution.", "See also", "Discrete uniform distribution, yielding integers.", "Discrete uniform distribution over the closed interval [low, high].", "Floats uniformly distributed over [0, 1).", "Alias for random_sample.", "Convenience function that accepts dimensions as input, e.g., rand(2,2) would generate a 2-by-2 array of floats, uniformly distributed over [0, 1).", "which should be used for new code.", "The probability density function of the uniform distribution is", "anywhere within the interval [a, b), and zero elsewhere.", "When high == low, values of low will be returned. If high < low, the results are officially undefined and may eventually raise an error, i.e. do not rely on this function to behave when passed arguments satisfying that inequality condition. The high limit may be included in the returned array of floats due to floating-point rounding in the equation low + (high-low) * random_sample(). For example:", "Draw samples from the distribution:", "All values are within the given interval:", "Display the histogram of the samples, along with the probability density function:"]}, {"name": "random.vonmises()", "path": "reference/random/generated/numpy.random.vonmises", "type": "numpy.random.vonmises", "text": ["Draw samples from a von Mises distribution.", "Samples are drawn from a von Mises distribution with specified mode (mu) and dispersion (kappa), on the interval [-pi, pi].", "The von Mises distribution (also known as the circular normal distribution) is a continuous probability distribution on the unit circle. It may be thought of as the circular analogue of the normal distribution.", "Note", "New code should use the vonmises method of a default_rng() instance instead; please see the Quick Start.", "Mode (\u201ccenter\u201d) of the distribution.", "Dispersion of the distribution, has to be >=0.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if mu and kappa are both scalars. Otherwise, np.broadcast(mu, kappa).size samples are drawn.", "Drawn samples from the parameterized von Mises distribution.", "See also", "probability density function, distribution, or cumulative density function, etc.", "which should be used for new code.", "The probability density for the von Mises distribution is", "where \\(\\mu\\) is the mode and \\(\\kappa\\) the dispersion, and \\(I_0(\\kappa)\\) is the modified Bessel function of order 0.", "The von Mises is named for Richard Edler von Mises, who was born in Austria-Hungary, in what is now the Ukraine. He fled to the United States in 1939 and became a professor at Harvard. He worked in probability theory, aerodynamics, fluid mechanics, and philosophy of science.", "Abramowitz, M. and Stegun, I. A. (Eds.). \u201cHandbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing,\u201d New York: Dover, 1972.", "von Mises, R., \u201cMathematical Theory of Probability and Statistics\u201d, New York: Academic Press, 1964.", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:"]}, {"name": "random.wald()", "path": "reference/random/generated/numpy.random.wald", "type": "numpy.random.wald", "text": ["Draw samples from a Wald, or inverse Gaussian, distribution.", "As the scale approaches infinity, the distribution becomes more like a Gaussian. Some references claim that the Wald is an inverse Gaussian with mean equal to 1, but this is by no means universal.", "The inverse Gaussian distribution was first studied in relationship to Brownian motion. In 1956 M.C.K. Tweedie used the name inverse Gaussian because there is an inverse relationship between the time to cover a unit distance and distance covered in unit time.", "Note", "New code should use the wald method of a default_rng() instance instead; please see the Quick Start.", "Distribution mean, must be > 0.", "Scale parameter, must be > 0.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if mean and scale are both scalars. Otherwise, np.broadcast(mean, scale).size samples are drawn.", "Drawn samples from the parameterized Wald distribution.", "See also", "which should be used for new code.", "The probability density function for the Wald distribution is", "As noted above the inverse Gaussian distribution first arise from attempts to model Brownian motion. It is also a competitor to the Weibull for use in reliability modeling and modeling stock returns and interest rate processes.", "Brighton Webs Ltd., Wald Distribution, https://web.archive.org/web/20090423014010/http://www.brighton-webs.co.uk:80/distributions/wald.asp", "Chhikara, Raj S., and Folks, J. Leroy, \u201cThe Inverse Gaussian Distribution: Theory : Methodology, and Applications\u201d, CRC Press, 1988.", "Wikipedia, \u201cInverse Gaussian distribution\u201d https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution", "Draw values from the distribution and plot the histogram:"]}, {"name": "random.weibull()", "path": "reference/random/generated/numpy.random.weibull", "type": "numpy.random.weibull", "text": ["Draw samples from a Weibull distribution.", "Draw samples from a 1-parameter Weibull distribution with the given shape parameter a.", "Here, U is drawn from the uniform distribution over (0,1].", "The more common 2-parameter Weibull, including a scale parameter \\(\\lambda\\) is just \\(X = \\lambda(-ln(U))^{1/a}\\).", "Note", "New code should use the weibull method of a default_rng() instance instead; please see the Quick Start.", "Shape parameter of the distribution. Must be nonnegative.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if a is a scalar. Otherwise, np.array(a).size samples are drawn.", "Drawn samples from the parameterized Weibull distribution.", "See also", "which should be used for new code.", "The Weibull (or Type III asymptotic extreme value distribution for smallest values, SEV Type III, or Rosin-Rammler distribution) is one of a class of Generalized Extreme Value (GEV) distributions used in modeling extreme value problems. This class includes the Gumbel and Frechet distributions.", "The probability density for the Weibull distribution is", "where \\(a\\) is the shape and \\(\\lambda\\) the scale.", "The function has its peak (the mode) at \\(\\lambda(\\frac{a-1}{a})^{1/a}\\).", "When a = 1, the Weibull distribution reduces to the exponential distribution.", "Waloddi Weibull, Royal Technical University, Stockholm, 1939 \u201cA Statistical Theory Of The Strength Of Materials\u201d, Ingeniorsvetenskapsakademiens Handlingar Nr 151, 1939, Generalstabens Litografiska Anstalts Forlag, Stockholm.", "Waloddi Weibull, \u201cA Statistical Distribution Function of Wide Applicability\u201d, Journal Of Applied Mechanics ASME Paper 1951.", "Wikipedia, \u201cWeibull distribution\u201d, https://en.wikipedia.org/wiki/Weibull_distribution", "Draw samples from the distribution:", "Display the histogram of the samples, along with the probability density function:"]}, {"name": "random.zipf()", "path": "reference/random/generated/numpy.random.zipf", "type": "numpy.random.zipf", "text": ["Draw samples from a Zipf distribution.", "Samples are drawn from a Zipf distribution with specified parameter a > 1.", "The Zipf distribution (also known as the zeta distribution) is a discrete probability distribution that satisfies Zipf\u2019s law: the frequency of an item is inversely proportional to its rank in a frequency table.", "Note", "New code should use the zipf method of a default_rng() instance instead; please see the Quick Start.", "Distribution parameter. Must be greater than 1.", "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. If size is None (default), a single value is returned if a is a scalar. Otherwise, np.array(a).size samples are drawn.", "Drawn samples from the parameterized Zipf distribution.", "See also", "probability density function, distribution, or cumulative density function, etc.", "which should be used for new code.", "The probability density for the Zipf distribution is", "for integers \\(k \\geq 1\\), where \\(\\zeta\\) is the Riemann Zeta function.", "It is named for the American linguist George Kingsley Zipf, who noted that the frequency of any word in a sample of a language is inversely proportional to its rank in the frequency table.", "Zipf, G. K., \u201cSelected Studies of the Principle of Relative Frequency in Language,\u201d Cambridge, MA: Harvard Univ. Press, 1932.", "Draw samples from the distribution:", "Display the histogram of the samples, along with the expected histogram based on the probability density function:", "bincount provides a fast histogram for small integers."]}, {"name": "Reading and writing files", "path": "user/how-to-io", "type": "User Guide", "text": ["This page tackles common applications; for the full collection of I/O routines, see Input and output.", "Use numpy.loadtxt.", "Use numpy.genfromtxt.", "numpy.genfromtxt will either", "numpy.genfromtxt can also parse whitespace-delimited data files that have missing values if", "Each field has a fixed width: Use the width as the delimiter argument.", "A special value (e.g. \u201cx\u201d) indicates a missing field: Use it as the missing_values argument.", "You want to skip the rows with missing values: Set invalid_raise=False.", "The delimiter whitespace character is different from the whitespace that indicates missing data. For instance, if columns are delimited by \\t, then missing data will be recognized if it consists of one or more spaces.", "Choices:", "Use numpy.save, or to store multiple arrays numpy.savez or numpy.savez_compressed.", "For security and portability, set allow_pickle=False unless the dtype contains Python objects, which requires pickling.", "Masked arrays can't currently be saved, nor can other arbitrary array subclasses.", "numpy.save and numpy.savez create binary files. To write a human-readable file, use numpy.savetxt. The array can only be 1- or 2-dimensional, and there\u2019s no ` savetxtz` for multiple files.", "See Write or read large arrays.", "Use a structured array.", "Example:", "The .wav file header is a 44-byte block preceding data_size bytes of the actual sound data:", "The .wav file header as a NumPy structured dtype:", "This .wav example is for illustration; to read a .wav file in real life, use Python\u2019s built-in module wave.", "(Adapted from Pauli Virtanen, Advanced NumPy, licensed under CC BY 4.0.)", "Arrays too large to fit in memory can be treated like ordinary in-memory arrays using memory mapping.", "Raw array data written with numpy.ndarray.tofile or numpy.ndarray.tobytes can be read with numpy.memmap:", "Files output by numpy.save (that is, using the numpy format) can be read using numpy.load with the mmap_mode keyword argument:", "Memory mapping lacks features like data chunking and compression; more full-featured formats and libraries usable with NumPy include:", "For tradeoffs among memmap, Zarr, and HDF5, see pythonspeed.com.", "Formats for exchanging data with other tools include HDF5, Zarr, and NetCDF (see Write or read large arrays).", "NumPy arrays are not directly JSON serializable.", "Avoid when possible; pickles are not secure against erroneous or maliciously constructed data.", "Use numpy.save and numpy.load. Set allow_pickle=False, unless the array dtype includes Python objects, in which case pickling is required.", "See pandas.DataFrame.to_numpy.", "In general, prefer numpy.save and numpy.load.", "numpy.ndarray.tofile and numpy.fromfile lose information on endianness and precision and so are unsuitable for anything but scratch storage."]}, {"name": "recarray.all()", "path": "reference/generated/numpy.recarray.all", "type": "numpy.recarray.all", "text": ["method", "Returns True if all elements evaluate to True.", "Refer to numpy.all for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.any()", "path": "reference/generated/numpy.recarray.any", "type": "numpy.recarray.any", "text": ["method", "Returns True if any of the elements of a evaluate to True.", "Refer to numpy.any for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.argmax()", "path": "reference/generated/numpy.recarray.argmax", "type": "numpy.recarray.argmax", "text": ["method", "Return indices of the maximum values along the given axis.", "Refer to numpy.argmax for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.argmin()", "path": "reference/generated/numpy.recarray.argmin", "type": "numpy.recarray.argmin", "text": ["method", "Return indices of the minimum values along the given axis.", "Refer to numpy.argmin for detailed documentation.", "See also", "equivalent function"]}, {"name": "recarray.argpartition()", "path": "reference/generated/numpy.recarray.argpartition", "type": "numpy.recarray.argpartition", "text": ["method", "Returns the indices that would partition this array.", "Refer to numpy.argpartition for full documentation.", "New in version 1.8.0.", "See also", "equivalent function"]}, {"name": "recarray.argsort()", "path": "reference/generated/numpy.recarray.argsort", "type": "numpy.recarray.argsort", "text": ["method", "Returns the indices that would sort this array.", "Refer to numpy.argsort for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.astype()", "path": "reference/generated/numpy.recarray.astype", "type": "numpy.recarray.astype", "text": ["method", "Copy of the array, cast to a specified type.", "Typecode or data-type to which the array is cast.", "Controls the memory layout order of the result. \u2018C\u2019 means C order, \u2018F\u2019 means Fortran order, \u2018A\u2019 means \u2018F\u2019 order if all the arrays are Fortran contiguous, \u2018C\u2019 order otherwise, and \u2018K\u2019 means as close to the order the array elements appear in memory as possible. Default is \u2018K\u2019.", "Controls what kind of data casting may occur. Defaults to \u2018unsafe\u2019 for backwards compatibility.", "If True, then sub-classes will be passed-through (default), otherwise the returned array will be forced to be a base-class array.", "By default, astype always returns a newly allocated array. If this is set to false, and the dtype, order, and subok requirements are satisfied, the input array is returned instead of a copy.", "Unless copy is False and the other conditions for returning the input array are satisfied (see description for copy input parameter), arr_t is a new array of the same shape as the input array, with dtype, order given by dtype, order.", "When casting from complex to float or int. To avoid this, one should use a.real.astype(t).", "Changed in version 1.17.0: Casting between a simple data type and a structured one is possible only for \u201cunsafe\u201d casting. Casting to multiple fields is allowed, but casting from multiple fields is not.", "Changed in version 1.9.0: Casting from numeric to string types in \u2018safe\u2019 casting mode requires that the string dtype length is long enough to store the max integer/float value converted."]}, {"name": "recarray.base", "path": "reference/generated/numpy.recarray.base", "type": "Standard array subclasses", "text": ["attribute", "Base object if memory is from some other object.", "The base of an array that owns its memory is None:", "Slicing creates a view, whose memory is shared with x:"]}, {"name": "recarray.byteswap()", "path": "reference/generated/numpy.recarray.byteswap", "type": "numpy.recarray.byteswap", "text": ["method", "Swap the bytes of the array elements", "Toggle between low-endian and big-endian data representation by returning a byteswapped array, optionally swapped in-place. Arrays of byte-strings are not swapped. The real and imaginary parts of a complex number are swapped individually.", "If True, swap bytes in-place, default is False.", "The byteswapped array. If inplace is True, this is a view to self.", "Arrays of byte-strings are not swapped", "but different representation in memory"]}, {"name": "recarray.choose()", "path": "reference/generated/numpy.recarray.choose", "type": "numpy.recarray.choose", "text": ["method", "Use an index array to construct a new array from a set of choices.", "Refer to numpy.choose for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.clip()", "path": "reference/generated/numpy.recarray.clip", "type": "numpy.recarray.clip", "text": ["method", "Return an array whose values are limited to [min, max]. One of max or min must be given.", "Refer to numpy.clip for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.compress()", "path": "reference/generated/numpy.recarray.compress", "type": "numpy.recarray.compress", "text": ["method", "Return selected slices of this array along given axis.", "Refer to numpy.compress for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.conj()", "path": "reference/generated/numpy.recarray.conj", "type": "numpy.recarray.conj", "text": ["method", "Complex-conjugate all elements.", "Refer to numpy.conjugate for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.conjugate()", "path": "reference/generated/numpy.recarray.conjugate", "type": "numpy.recarray.conjugate", "text": ["method", "Return the complex conjugate, element-wise.", "Refer to numpy.conjugate for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.copy()", "path": "reference/generated/numpy.recarray.copy", "type": "numpy.recarray.copy", "text": ["method", "Return a copy of the array.", "Controls the memory layout of the copy. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. \u2018K\u2019 means match the layout of a as closely as possible. (Note that this function and numpy.copy are very similar but have different default values for their order= arguments, and this function always passes sub-classes through.)", "See also", "Similar function with different default behavior", "This function is the preferred method for creating an array copy. The function numpy.copy is similar, but it defaults to using order \u2018K\u2019, and will not pass sub-classes through by default."]}, {"name": "recarray.ctypes", "path": "reference/generated/numpy.recarray.ctypes", "type": "Standard array subclasses", "text": ["attribute", "An object to simplify the interaction of the array with the ctypes module.", "This attribute creates an object that makes it easier to use arrays when calling shared libraries with the ctypes module. The returned object has, among others, data, shape, and strides attributes (see Notes below) which themselves return ctypes objects that can be used as arguments to a shared library.", "Possessing attributes data, shape, strides, etc.", "See also", "Below are the public attributes of this object which were documented in \u201cGuide to NumPy\u201d (we have omitted undocumented public attributes, as well as documented private attributes):", "A pointer to the memory area of the array as a Python integer. This memory area may contain data that is not aligned, or not in correct byte-order. The memory area may not even be writeable. The array flags and data-type of this array should be respected when passing this attribute to arbitrary C-code to avoid trouble that can include Python crashing. User Beware! The value of this attribute is exactly the same as self._array_interface_['data'][0].", "Note that unlike data_as, a reference will not be kept to the array: code like ctypes.c_void_p((a + b).ctypes.data) will result in a pointer to a deallocated array, and should be spelt (a + b).ctypes.data_as(ctypes.c_void_p)", "(c_intp*self.ndim): A ctypes array of length self.ndim where the basetype is the C-integer corresponding to dtype('p') on this platform (see c_intp). This base-type could be ctypes.c_int, ctypes.c_long, or ctypes.c_longlong depending on the platform. The ctypes array contains the shape of the underlying array.", "(c_intp*self.ndim): A ctypes array of length self.ndim where the basetype is the same as for the shape attribute. This ctypes array contains the strides information from the underlying array. This strides information is important for showing how many bytes must be jumped to get to the next element in the array.", "Return the data pointer cast to a particular c-types object. For example, calling self._as_parameter_ is equivalent to self.data_as(ctypes.c_void_p). Perhaps you want to use the data as a pointer to a ctypes array of floating-point data: self.data_as(ctypes.POINTER(ctypes.c_double)).", "The returned pointer will keep a reference to the array.", "Return the shape tuple as an array of some other c-types type. For example: self.shape_as(ctypes.c_short).", "Return the strides tuple as an array of some other c-types type. For example: self.strides_as(ctypes.c_longlong).", "If the ctypes module is not available, then the ctypes attribute of array objects still returns something useful, but ctypes objects are not returned and errors may be raised instead. In particular, the object will still have the as_parameter attribute which will return an integer equal to the data attribute."]}, {"name": "recarray.cumprod()", "path": "reference/generated/numpy.recarray.cumprod", "type": "numpy.recarray.cumprod", "text": ["method", "Return the cumulative product of the elements along the given axis.", "Refer to numpy.cumprod for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.cumsum()", "path": "reference/generated/numpy.recarray.cumsum", "type": "numpy.recarray.cumsum", "text": ["method", "Return the cumulative sum of the elements along the given axis.", "Refer to numpy.cumsum for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.data", "path": "reference/generated/numpy.recarray.data", "type": "Standard array subclasses", "text": ["attribute", "Python buffer object pointing to the start of the array\u2019s data."]}, {"name": "recarray.diagonal()", "path": "reference/generated/numpy.recarray.diagonal", "type": "numpy.recarray.diagonal", "text": ["method", "Return specified diagonals. In NumPy 1.9 the returned array is a read-only view instead of a copy as in previous NumPy versions. In a future version the read-only restriction will be removed.", "Refer to numpy.diagonal for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.dump()", "path": "reference/generated/numpy.recarray.dump", "type": "numpy.recarray.dump", "text": ["method", "Dump a pickle of the array to the specified file. The array can be read back with pickle.load or numpy.load.", "A string naming the dump file.", "Changed in version 1.17.0: pathlib.Path objects are now accepted."]}, {"name": "recarray.dumps()", "path": "reference/generated/numpy.recarray.dumps", "type": "numpy.recarray.dumps", "text": ["method", "Returns the pickle of the array as a string. pickle.loads will convert the string back to an array."]}, {"name": "recarray.fill()", "path": "reference/generated/numpy.recarray.fill", "type": "numpy.recarray.fill", "text": ["method", "Fill the array with a scalar value.", "All elements of a will be assigned this value."]}, {"name": "recarray.flags", "path": "reference/generated/numpy.recarray.flags", "type": "Standard array subclasses", "text": ["attribute", "Information about the memory layout of the array.", "The flags object can be accessed dictionary-like (as in a.flags['WRITEABLE']), or by using lowercased attribute names (as in a.flags.writeable). Short flag names are only supported in dictionary access.", "Only the WRITEBACKIFCOPY, UPDATEIFCOPY, WRITEABLE, and ALIGNED flags can be changed by the user, via direct assignment to the attribute or dictionary entry, or by calling ndarray.setflags.", "The array flags cannot be set arbitrarily:", "Arrays can be both C-style and Fortran-style contiguous simultaneously. This is clear for 1-dimensional arrays, but can also be true for higher dimensional arrays.", "Even for contiguous arrays a stride for a given dimension arr.strides[dim] may be arbitrary if arr.shape[dim] == 1 or the array has no elements. It does not generally hold that self.strides[-1] == self.itemsize for C-style contiguous arrays or self.strides[0] == self.itemsize for Fortran-style contiguous arrays is true.", "The data is in a single, C-style contiguous segment.", "The data is in a single, Fortran-style contiguous segment.", "The array owns the memory it uses or borrows it from another object.", "The data area can be written to. Setting this to False locks the data, making it read-only. A view (slice, etc.) inherits WRITEABLE from its base array at creation time, but a view of a writeable array may be subsequently locked while the base array remains writeable. (The opposite is not true, in that a view of a locked array may not be made writeable. However, currently, locking a base object does not lock any views that already reference it, so under that circumstance it is possible to alter the contents of a locked array via a previously created writeable view onto it.) Attempting to change a non-writeable array raises a RuntimeError exception.", "The data and all elements are aligned appropriately for the hardware.", "This array is a copy of some other array. The C-API function PyArray_ResolveWritebackIfCopy must be called before deallocating to the base array will be updated with the contents of this array.", "(Deprecated, use WRITEBACKIFCOPY) This array is a copy of some other array. When this array is deallocated, the base array will be updated with the contents of this array.", "F_CONTIGUOUS and not C_CONTIGUOUS.", "F_CONTIGUOUS or C_CONTIGUOUS (one-segment test).", "ALIGNED and WRITEABLE.", "BEHAVED and C_CONTIGUOUS.", "BEHAVED and F_CONTIGUOUS and not C_CONTIGUOUS."]}, {"name": "recarray.flat", "path": "reference/generated/numpy.recarray.flat", "type": "Standard array subclasses", "text": ["attribute", "A 1-D iterator over the array.", "This is a numpy.flatiter instance, which acts similarly to, but is not a subclass of, Python\u2019s built-in iterator object.", "See also", "Return a copy of the array collapsed into one dimension.", "An assignment example:"]}, {"name": "recarray.flatten()", "path": "reference/generated/numpy.recarray.flatten", "type": "numpy.recarray.flatten", "text": ["method", "Return a copy of the array collapsed into one dimension.", "\u2018C\u2019 means to flatten in row-major (C-style) order. \u2018F\u2019 means to flatten in column-major (Fortran- style) order. \u2018A\u2019 means to flatten in column-major order if a is Fortran contiguous in memory, row-major order otherwise. \u2018K\u2019 means to flatten a in the order the elements occur in memory. The default is \u2018C\u2019.", "A copy of the input array, flattened to one dimension.", "See also", "Return a flattened array.", "A 1-D flat iterator over the array."]}, {"name": "recarray.getfield()", "path": "reference/generated/numpy.recarray.getfield", "type": "numpy.recarray.getfield", "text": ["method", "Returns a field of the given array as a certain type.", "A field is a view of the array data with a given data-type. The values in the view are determined by the given type and the offset into the current array in bytes. The offset needs to be such that the view dtype fits in the array dtype; for example an array of dtype complex128 has 16-byte elements. If taking a view with a 32-bit integer (4 bytes), the offset needs to be between 0 and 12 bytes.", "The data type of the view. The dtype size of the view can not be larger than that of the array itself.", "Number of bytes to skip before beginning the element view.", "By choosing an offset of 8 bytes we can select the complex part of the array for our view:"]}, {"name": "recarray.item()", "path": "reference/generated/numpy.recarray.item", "type": "numpy.recarray.item", "text": ["method", "Copy an element of an array to a standard Python scalar and return it.", "A copy of the specified element of the array as a suitable Python scalar", "When the data type of a is longdouble or clongdouble, item() returns a scalar array object because there is no available Python scalar that would not lose information. Void arrays return a buffer object for item(), unless fields are defined, in which case a tuple is returned.", "item is very similar to a[args], except, instead of an array scalar, a standard Python scalar is returned. This can be useful for speeding up access to elements of the array and doing arithmetic on elements of the array using Python\u2019s optimized math."]}, {"name": "recarray.itemset()", "path": "reference/generated/numpy.recarray.itemset", "type": "numpy.recarray.itemset", "text": ["method", "Insert scalar into an array (scalar is cast to array\u2019s dtype, if possible)", "There must be at least 1 argument, and define the last argument as item. Then, a.itemset(*args) is equivalent to but faster than a[args] = item. The item should be a scalar value and args must select a single item in the array a.", "If one argument: a scalar, only used in case a is of size 1. If two arguments: the last argument is the value to be set and must be a scalar, the first argument specifies a single array element location. It is either an int or a tuple.", "Compared to indexing syntax, itemset provides some speed increase for placing a scalar into a particular location in an ndarray, if you must do this. However, generally this is discouraged: among other problems, it complicates the appearance of the code. Also, when using itemset (and item) inside a loop, be sure to assign the methods to a local variable to avoid the attribute look-up at each loop iteration."]}, {"name": "recarray.itemsize", "path": "reference/generated/numpy.recarray.itemsize", "type": "Standard array subclasses", "text": ["attribute", "Length of one array element in bytes."]}, {"name": "recarray.max()", "path": "reference/generated/numpy.recarray.max", "type": "numpy.recarray.max", "text": ["method", "Return the maximum along a given axis.", "Refer to numpy.amax for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.mean()", "path": "reference/generated/numpy.recarray.mean", "type": "numpy.recarray.mean", "text": ["method", "Returns the average of the array elements along given axis.", "Refer to numpy.mean for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.min()", "path": "reference/generated/numpy.recarray.min", "type": "numpy.recarray.min", "text": ["method", "Return the minimum along a given axis.", "Refer to numpy.amin for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.nbytes", "path": "reference/generated/numpy.recarray.nbytes", "type": "Standard array subclasses", "text": ["attribute", "Total bytes consumed by the elements of the array.", "Does not include memory consumed by non-element attributes of the array object."]}, {"name": "recarray.ndim", "path": "reference/generated/numpy.recarray.ndim", "type": "Standard array subclasses", "text": ["attribute", "Number of array dimensions."]}, {"name": "recarray.newbyteorder()", "path": "reference/generated/numpy.recarray.newbyteorder", "type": "numpy.recarray.newbyteorder", "text": ["method", "Return the array with the same data viewed with a different byte order.", "Equivalent to:", "Changes are also made in all fields and sub-arrays of the array data type.", "Byte order to force; a value from the byte order specifications below. new_order codes can be any of:", "The default value (\u2018S\u2019) results in swapping the current byte order.", "New array object with the dtype reflecting given change to the byte order."]}, {"name": "recarray.nonzero()", "path": "reference/generated/numpy.recarray.nonzero", "type": "numpy.recarray.nonzero", "text": ["method", "Return the indices of the elements that are non-zero.", "Refer to numpy.nonzero for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.partition()", "path": "reference/generated/numpy.recarray.partition", "type": "numpy.recarray.partition", "text": ["method", "Rearranges the elements in the array in such a way that the value of the element in kth position is in the position it would be in a sorted array. All elements smaller than the kth element are moved before this element and all equal or greater are moved behind it. The ordering of the elements in the two partitions is undefined.", "New in version 1.8.0.", "Element index to partition by. The kth element value will be in its final sorted position and all smaller elements will be moved before it and all equal or greater elements behind it. The order of all elements in the partitions is undefined. If provided with a sequence of kth it will partition all elements indexed by kth of them into their sorted position at once.", "Deprecated since version 1.22.0: Passing booleans as index is deprecated.", "Axis along which to sort. Default is -1, which means sort along the last axis.", "Selection algorithm. Default is \u2018introselect\u2019.", "When a is an array with fields defined, this argument specifies which fields to compare first, second, etc. A single field can be specified as a string, and not all fields need to be specified, but unspecified fields will still be used, in the order in which they come up in the dtype, to break ties.", "See also", "Return a parititioned copy of an array.", "Indirect partition.", "Full sort.", "See np.partition for notes on the different algorithms."]}, {"name": "recarray.prod()", "path": "reference/generated/numpy.recarray.prod", "type": "numpy.recarray.prod", "text": ["method", "Return the product of the array elements over the given axis", "Refer to numpy.prod for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.ptp()", "path": "reference/generated/numpy.recarray.ptp", "type": "numpy.recarray.ptp", "text": ["method", "Peak to peak (maximum - minimum) value along a given axis.", "Refer to numpy.ptp for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.put()", "path": "reference/generated/numpy.recarray.put", "type": "numpy.recarray.put", "text": ["method", "Set a.flat[n] = values[n] for all n in indices.", "Refer to numpy.put for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.ravel()", "path": "reference/generated/numpy.recarray.ravel", "type": "numpy.recarray.ravel", "text": ["method", "Return a flattened array.", "Refer to numpy.ravel for full documentation.", "See also", "equivalent function", "a flat iterator on the array."]}, {"name": "recarray.repeat()", "path": "reference/generated/numpy.recarray.repeat", "type": "numpy.recarray.repeat", "text": ["method", "Repeat elements of an array.", "Refer to numpy.repeat for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.reshape()", "path": "reference/generated/numpy.recarray.reshape", "type": "numpy.recarray.reshape", "text": ["method", "Returns an array containing the same data with a new shape.", "Refer to numpy.reshape for full documentation.", "See also", "equivalent function", "Unlike the free function numpy.reshape, this method on ndarray allows the elements of the shape parameter to be passed in as separate arguments. For example, a.reshape(10, 11) is equivalent to a.reshape((10, 11))."]}, {"name": "recarray.resize()", "path": "reference/generated/numpy.recarray.resize", "type": "numpy.recarray.resize", "text": ["method", "Change shape and size of array in-place.", "Shape of resized array.", "If False, reference count will not be checked. Default is True.", "If a does not own its own data or references or views to it exist, and the data memory must be changed. PyPy only: will always raise if the data memory must be changed, since there is no reliable way to determine if references or views to it exist.", "If the order keyword argument is specified. This behaviour is a bug in NumPy.", "See also", "Return a new array with the specified shape.", "This reallocates space for the data area if necessary.", "Only contiguous arrays (data elements consecutive in memory) can be resized.", "The purpose of the reference count check is to make sure you do not use this array as a buffer for another Python object and then reallocate the memory. However, reference counts can increase in other ways so if you are sure that you have not shared the memory for this array with another Python object, then you may safely set refcheck to False.", "Shrinking an array: array is flattened (in the order that the data are stored in memory), resized, and reshaped:", "Enlarging an array: as above, but missing entries are filled with zeros:", "Referencing an array prevents resizing\u2026", "Unless refcheck is False:"]}, {"name": "recarray.round()", "path": "reference/generated/numpy.recarray.round", "type": "numpy.recarray.round", "text": ["method", "Return a with each element rounded to the given number of decimals.", "Refer to numpy.around for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.searchsorted()", "path": "reference/generated/numpy.recarray.searchsorted", "type": "numpy.recarray.searchsorted", "text": ["method", "Find indices where elements of v should be inserted in a to maintain order.", "For full documentation, see numpy.searchsorted", "See also", "equivalent function"]}, {"name": "recarray.setfield()", "path": "reference/generated/numpy.recarray.setfield", "type": "numpy.recarray.setfield", "text": ["method", "Put a value into a specified place in a field defined by a data-type.", "Place val into a\u2019s field defined by dtype and beginning offset bytes into the field.", "Value to be placed in field.", "Data-type of the field in which to place val.", "The number of bytes into the field at which to place val.", "See also"]}, {"name": "recarray.setflags()", "path": "reference/generated/numpy.recarray.setflags", "type": "numpy.recarray.setflags", "text": ["method", "Set array flags WRITEABLE, ALIGNED, (WRITEBACKIFCOPY and UPDATEIFCOPY), respectively.", "These Boolean-valued flags affect how numpy interprets the memory area used by a (see Notes below). The ALIGNED flag can only be set to True if the data is actually aligned according to the type. The WRITEBACKIFCOPY and (deprecated) UPDATEIFCOPY flags can never be set to True. The flag WRITEABLE can only be set to True if the array owns its own memory, or the ultimate owner of the memory exposes a writeable buffer interface, or is a string. (The exception for string is made so that unpickling can be done without copying memory.)", "Describes whether or not a can be written to.", "Describes whether or not a is aligned properly for its type.", "Describes whether or not a is a copy of another \u201cbase\u201d array.", "Array flags provide information about how the memory area used for the array is to be interpreted. There are 7 Boolean flags in use, only four of which can be changed by the user: WRITEBACKIFCOPY, UPDATEIFCOPY, WRITEABLE, and ALIGNED.", "WRITEABLE (W) the data area can be written to;", "ALIGNED (A) the data and strides are aligned appropriately for the hardware (as determined by the compiler);", "UPDATEIFCOPY (U) (deprecated), replaced by WRITEBACKIFCOPY;", "WRITEBACKIFCOPY (X) this array is a copy of some other array (referenced by .base). When the C-API function PyArray_ResolveWritebackIfCopy is called, the base array will be updated with the contents of this array.", "All flags can be accessed using the single (upper case) letter as well as the full name."]}, {"name": "recarray.size", "path": "reference/generated/numpy.recarray.size", "type": "Standard array subclasses", "text": ["attribute", "Number of elements in the array.", "Equal to np.prod(a.shape), i.e., the product of the array\u2019s dimensions.", "a.size returns a standard arbitrary precision Python integer. This may not be the case with other methods of obtaining the same value (like the suggested np.prod(a.shape), which returns an instance of np.int_), and may be relevant if the value is used further in calculations that may overflow a fixed size integer type."]}, {"name": "recarray.sort()", "path": "reference/generated/numpy.recarray.sort", "type": "numpy.recarray.sort", "text": ["method", "Sort an array in-place. Refer to numpy.sort for full documentation.", "Axis along which to sort. Default is -1, which means sort along the last axis.", "Sorting algorithm. The default is \u2018quicksort\u2019. Note that both \u2018stable\u2019 and \u2018mergesort\u2019 use timsort under the covers and, in general, the actual implementation will vary with datatype. The \u2018mergesort\u2019 option is retained for backwards compatibility.", "Changed in version 1.15.0: The \u2018stable\u2019 option was added.", "When a is an array with fields defined, this argument specifies which fields to compare first, second, etc. A single field can be specified as a string, and not all fields need be specified, but unspecified fields will still be used, in the order in which they come up in the dtype, to break ties.", "See also", "Return a sorted copy of an array.", "Indirect sort.", "Indirect stable sort on multiple keys.", "Find elements in sorted array.", "Partial sort.", "See numpy.sort for notes on the different sorting algorithms.", "Use the order keyword to specify a field to use when sorting a structured array:"]}, {"name": "recarray.squeeze()", "path": "reference/generated/numpy.recarray.squeeze", "type": "numpy.recarray.squeeze", "text": ["method", "Remove axes of length one from a.", "Refer to numpy.squeeze for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.std()", "path": "reference/generated/numpy.recarray.std", "type": "numpy.recarray.std", "text": ["method", "Returns the standard deviation of the array elements along given axis.", "Refer to numpy.std for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.strides", "path": "reference/generated/numpy.recarray.strides", "type": "Standard array subclasses", "text": ["attribute", "Tuple of bytes to step in each dimension when traversing an array.", "The byte offset of element (i[0], i[1], ..., i[n]) in an array a is:", "A more detailed explanation of strides can be found in the \u201cndarray.rst\u201d file in the NumPy reference guide.", "See also", "Imagine an array of 32-bit integers (each 4 bytes):", "This array is stored in memory as 40 bytes, one after the other (known as a contiguous block of memory). The strides of an array tell us how many bytes we have to skip in memory to move to the next position along a certain axis. For example, we have to skip 4 bytes (1 value) to move to the next column, but 20 bytes (5 values) to get to the same position in the next row. As such, the strides for the array x will be (20, 4)."]}, {"name": "recarray.sum()", "path": "reference/generated/numpy.recarray.sum", "type": "numpy.recarray.sum", "text": ["method", "Return the sum of the array elements over the given axis.", "Refer to numpy.sum for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.swapaxes()", "path": "reference/generated/numpy.recarray.swapaxes", "type": "numpy.recarray.swapaxes", "text": ["method", "Return a view of the array with axis1 and axis2 interchanged.", "Refer to numpy.swapaxes for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.T", "path": "reference/generated/numpy.recarray.t", "type": "Standard array subclasses", "text": ["attribute", "The transposed array.", "Same as self.transpose().", "See also"]}, {"name": "recarray.take()", "path": "reference/generated/numpy.recarray.take", "type": "numpy.recarray.take", "text": ["method", "Return an array formed from the elements of a at the given indices.", "Refer to numpy.take for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.tobytes()", "path": "reference/generated/numpy.recarray.tobytes", "type": "numpy.recarray.tobytes", "text": ["method", "Construct Python bytes containing the raw data bytes in the array.", "Constructs Python bytes showing a copy of the raw contents of data memory. The bytes object is produced in C-order by default. This behavior is controlled by the order parameter.", "New in version 1.9.0.", "Controls the memory layout of the bytes object. \u2018C\u2019 means C-order, \u2018F\u2019 means F-order, \u2018A\u2019 (short for Any) means \u2018F\u2019 if a is Fortran contiguous, \u2018C\u2019 otherwise. Default is \u2018C\u2019.", "Python bytes exhibiting a copy of a\u2019s raw data."]}, {"name": "recarray.tofile()", "path": "reference/generated/numpy.recarray.tofile", "type": "numpy.recarray.tofile", "text": ["method", "Write array to a file as text or binary (default).", "Data is always written in \u2018C\u2019 order, independent of the order of a. The data produced by this method can be recovered using the function fromfile().", "An open file object, or a string containing a filename.", "Changed in version 1.17.0: pathlib.Path objects are now accepted.", "Separator between array items for text output. If \u201c\u201d (empty), a binary file is written, equivalent to file.write(a.tobytes()).", "Format string for text file output. Each entry in the array is formatted to text by first converting it to the closest Python type, and then using \u201cformat\u201d % item.", "This is a convenience function for quick storage of array data. Information on endianness and precision is lost, so this method is not a good choice for files intended to archive data or transport data between machines with different endianness. Some of these problems can be overcome by outputting the data as text files, at the expense of speed and file size.", "When fid is a file object, array contents are directly written to the file, bypassing the file object\u2019s write method. As a result, tofile cannot be used with files objects supporting compression (e.g., GzipFile) or file-like objects that do not support fileno() (e.g., BytesIO)."]}, {"name": "recarray.tolist()", "path": "reference/generated/numpy.recarray.tolist", "type": "numpy.recarray.tolist", "text": ["method", "Return the array as an a.ndim-levels deep nested list of Python scalars.", "Return a copy of the array data as a (nested) Python list. Data items are converted to the nearest compatible builtin Python type, via the item function.", "If a.ndim is 0, then since the depth of the nested list is 0, it will not be a list at all, but a simple Python scalar.", "The possibly nested list of array elements.", "The array may be recreated via a = np.array(a.tolist()), although this may sometimes lose precision.", "For a 1D array, a.tolist() is almost the same as list(a), except that tolist changes numpy scalars to Python scalars:", "Additionally, for a 2D array, tolist applies recursively:", "The base case for this recursion is a 0D array:"]}, {"name": "recarray.tostring()", "path": "reference/generated/numpy.recarray.tostring", "type": "numpy.recarray.tostring", "text": ["method", "A compatibility alias for tobytes, with exactly the same behavior.", "Despite its name, it returns bytes not strs.", "Deprecated since version 1.19.0."]}, {"name": "recarray.trace()", "path": "reference/generated/numpy.recarray.trace", "type": "numpy.recarray.trace", "text": ["method", "Return the sum along diagonals of the array.", "Refer to numpy.trace for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.transpose()", "path": "reference/generated/numpy.recarray.transpose", "type": "numpy.recarray.transpose", "text": ["method", "Returns a view of the array with axes transposed.", "For a 1-D array this has no effect, as a transposed vector is simply the same vector. To convert a 1-D array into a 2D column vector, an additional dimension must be added. np.atleast2d(a).T achieves this, as does a[:, np.newaxis]. For a 2-D array, this is a standard matrix transpose. For an n-D array, if axes are given, their order indicates how the axes are permuted (see Examples). If axes are not provided and a.shape = (i[0], i[1], ... i[n-2], i[n-1]), then a.transpose().shape = (i[n-1], i[n-2], ... i[1], i[0]).", "View of a, with axes suitably permuted.", "See also", "Equivalent function", "Array property returning the array transposed.", "Give a new shape to an array without changing its data."]}, {"name": "recarray.var()", "path": "reference/generated/numpy.recarray.var", "type": "numpy.recarray.var", "text": ["method", "Returns the variance of the array elements, along given axis.", "Refer to numpy.var for full documentation.", "See also", "equivalent function"]}, {"name": "recarray.view()", "path": "reference/generated/numpy.recarray.view", "type": "numpy.recarray.view", "text": ["method", "New view of array with the same data.", "Note", "Passing None for dtype is different from omitting the parameter, since the former invokes dtype(None) which is an alias for dtype('float_').", "Data-type descriptor of the returned view, e.g., float32 or int16. Omitting it results in the view having the same data-type as a. This argument can also be specified as an ndarray sub-class, which then specifies the type of the returned object (this is equivalent to setting the type parameter).", "Type of the returned view, e.g., ndarray or matrix. Again, omission of the parameter results in type preservation.", "a.view() is used two different ways:", "a.view(some_dtype) or a.view(dtype=some_dtype) constructs a view of the array\u2019s memory with a different data-type. This can cause a reinterpretation of the bytes of memory.", "a.view(ndarray_subclass) or a.view(type=ndarray_subclass) just returns an instance of ndarray_subclass that looks at the same array (same shape, dtype, etc.) This does not cause a reinterpretation of the memory.", "For a.view(some_dtype), if some_dtype has a different number of bytes per entry than the previous dtype (for example, converting a regular array to a structured array), then the behavior of the view cannot be predicted just from the superficial appearance of a (shown by print(a)). It also depends on exactly how a is stored in memory. Therefore if a is C-ordered versus fortran-ordered, versus defined as a slice or transpose, etc., the view may give different results.", "Viewing array data using a different type and dtype:", "Creating a view on a structured array so it can be used in calculations", "Making changes to the view changes the underlying array", "Using a view to convert an array to a recarray:", "Views share data:", "Views that change the dtype size (bytes per entry) should normally be avoided on arrays defined by slices, transposes, fortran-ordering, etc.:"]}, {"name": "record.all()", "path": "reference/generated/numpy.record.all", "type": "numpy.record.all", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.all."]}, {"name": "record.any()", "path": "reference/generated/numpy.record.any", "type": "numpy.record.any", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.any."]}, {"name": "record.argmax()", "path": "reference/generated/numpy.record.argmax", "type": "numpy.record.argmax", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.argmax."]}, {"name": "record.argmin()", "path": "reference/generated/numpy.record.argmin", "type": "numpy.record.argmin", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.argmin."]}, {"name": "record.argsort()", "path": "reference/generated/numpy.record.argsort", "type": "numpy.record.argsort", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.argsort."]}, {"name": "record.astype()", "path": "reference/generated/numpy.record.astype", "type": "numpy.record.astype", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.astype."]}, {"name": "record.base", "path": "reference/generated/numpy.record.base", "type": "Standard array subclasses", "text": ["attribute", "base object"]}, {"name": "record.byteswap()", "path": "reference/generated/numpy.record.byteswap", "type": "numpy.record.byteswap", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.byteswap."]}, {"name": "record.choose()", "path": "reference/generated/numpy.record.choose", "type": "numpy.record.choose", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.choose."]}, {"name": "record.clip()", "path": "reference/generated/numpy.record.clip", "type": "numpy.record.clip", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.clip."]}, {"name": "record.compress()", "path": "reference/generated/numpy.record.compress", "type": "numpy.record.compress", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.compress."]}, {"name": "record.conjugate()", "path": "reference/generated/numpy.record.conjugate", "type": "numpy.record.conjugate", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.conjugate."]}, {"name": "record.copy()", "path": "reference/generated/numpy.record.copy", "type": "numpy.record.copy", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.copy."]}, {"name": "record.cumprod()", "path": "reference/generated/numpy.record.cumprod", "type": "numpy.record.cumprod", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.cumprod."]}, {"name": "record.cumsum()", "path": "reference/generated/numpy.record.cumsum", "type": "numpy.record.cumsum", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.cumsum."]}, {"name": "record.data", "path": "reference/generated/numpy.record.data", "type": "Standard array subclasses", "text": ["attribute", "Pointer to start of data."]}, {"name": "record.diagonal()", "path": "reference/generated/numpy.record.diagonal", "type": "numpy.record.diagonal", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.diagonal."]}, {"name": "record.dump()", "path": "reference/generated/numpy.record.dump", "type": "numpy.record.dump", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.dump."]}, {"name": "record.dumps()", "path": "reference/generated/numpy.record.dumps", "type": "numpy.record.dumps", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.dumps."]}, {"name": "record.fill()", "path": "reference/generated/numpy.record.fill", "type": "numpy.record.fill", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.fill."]}, {"name": "record.flags", "path": "reference/generated/numpy.record.flags", "type": "Standard array subclasses", "text": ["attribute", "integer value of flags"]}, {"name": "record.flat", "path": "reference/generated/numpy.record.flat", "type": "Standard array subclasses", "text": ["attribute", "A 1-D view of the scalar."]}, {"name": "record.flatten()", "path": "reference/generated/numpy.record.flatten", "type": "numpy.record.flatten", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.flatten."]}, {"name": "record.getfield()", "path": "reference/generated/numpy.record.getfield", "type": "numpy.record.getfield", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.getfield."]}, {"name": "record.item()", "path": "reference/generated/numpy.record.item", "type": "numpy.record.item", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.item."]}, {"name": "record.itemset()", "path": "reference/generated/numpy.record.itemset", "type": "numpy.record.itemset", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.itemset."]}, {"name": "record.itemsize", "path": "reference/generated/numpy.record.itemsize", "type": "Standard array subclasses", "text": ["attribute", "The length of one element in bytes."]}, {"name": "record.max()", "path": "reference/generated/numpy.record.max", "type": "numpy.record.max", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.max."]}, {"name": "record.mean()", "path": "reference/generated/numpy.record.mean", "type": "numpy.record.mean", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.mean."]}, {"name": "record.min()", "path": "reference/generated/numpy.record.min", "type": "numpy.record.min", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.min."]}, {"name": "record.nbytes", "path": "reference/generated/numpy.record.nbytes", "type": "Standard array subclasses", "text": ["attribute", "The length of the scalar in bytes."]}, {"name": "record.ndim", "path": "reference/generated/numpy.record.ndim", "type": "Standard array subclasses", "text": ["attribute", "The number of array dimensions."]}, {"name": "record.newbyteorder()", "path": "reference/generated/numpy.record.newbyteorder", "type": "numpy.record.newbyteorder", "text": ["method", "Return a new dtype with a different byte order.", "Changes are also made in all fields and sub-arrays of the data type.", "The new_order code can be any from the following:", "Byte order to force; a value from the byte order specifications above. The default value (\u2018S\u2019) results in swapping the current byte order.", "New dtype object with the given change to the byte order."]}, {"name": "record.nonzero()", "path": "reference/generated/numpy.record.nonzero", "type": "numpy.record.nonzero", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.nonzero."]}, {"name": "record.pprint()", "path": "reference/generated/numpy.record.pprint", "type": "numpy.record.pprint", "text": ["method", "Pretty-print all fields."]}, {"name": "record.prod()", "path": "reference/generated/numpy.record.prod", "type": "numpy.record.prod", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.prod."]}, {"name": "record.ptp()", "path": "reference/generated/numpy.record.ptp", "type": "numpy.record.ptp", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.ptp."]}, {"name": "record.put()", "path": "reference/generated/numpy.record.put", "type": "numpy.record.put", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.put."]}, {"name": "record.ravel()", "path": "reference/generated/numpy.record.ravel", "type": "numpy.record.ravel", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.ravel."]}, {"name": "record.repeat()", "path": "reference/generated/numpy.record.repeat", "type": "numpy.record.repeat", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.repeat."]}, {"name": "record.reshape()", "path": "reference/generated/numpy.record.reshape", "type": "numpy.record.reshape", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.reshape."]}, {"name": "record.resize()", "path": "reference/generated/numpy.record.resize", "type": "numpy.record.resize", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.resize."]}, {"name": "record.round()", "path": "reference/generated/numpy.record.round", "type": "numpy.record.round", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.round."]}, {"name": "record.searchsorted()", "path": "reference/generated/numpy.record.searchsorted", "type": "numpy.record.searchsorted", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.searchsorted."]}, {"name": "record.setfield()", "path": "reference/generated/numpy.record.setfield", "type": "numpy.record.setfield", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.setfield."]}, {"name": "record.setflags()", "path": "reference/generated/numpy.record.setflags", "type": "numpy.record.setflags", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.setflags."]}, {"name": "record.size", "path": "reference/generated/numpy.record.size", "type": "Standard array subclasses", "text": ["attribute", "The number of elements in the gentype."]}, {"name": "record.sort()", "path": "reference/generated/numpy.record.sort", "type": "numpy.record.sort", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.sort."]}, {"name": "record.squeeze()", "path": "reference/generated/numpy.record.squeeze", "type": "numpy.record.squeeze", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.squeeze."]}, {"name": "record.std()", "path": "reference/generated/numpy.record.std", "type": "numpy.record.std", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.std."]}, {"name": "record.strides", "path": "reference/generated/numpy.record.strides", "type": "Standard array subclasses", "text": ["attribute", "Tuple of bytes steps in each dimension."]}, {"name": "record.sum()", "path": "reference/generated/numpy.record.sum", "type": "numpy.record.sum", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.sum."]}, {"name": "record.swapaxes()", "path": "reference/generated/numpy.record.swapaxes", "type": "numpy.record.swapaxes", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.swapaxes."]}, {"name": "record.T", "path": "reference/generated/numpy.record.t", "type": "Standard array subclasses", "text": ["attribute", "Scalar attribute identical to the corresponding array attribute.", "Please see ndarray.T."]}, {"name": "record.take()", "path": "reference/generated/numpy.record.take", "type": "numpy.record.take", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.take."]}, {"name": "record.tofile()", "path": "reference/generated/numpy.record.tofile", "type": "numpy.record.tofile", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.tofile."]}, {"name": "record.tolist()", "path": "reference/generated/numpy.record.tolist", "type": "numpy.record.tolist", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.tolist."]}, {"name": "record.tostring()", "path": "reference/generated/numpy.record.tostring", "type": "numpy.record.tostring", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.tostring."]}, {"name": "record.trace()", "path": "reference/generated/numpy.record.trace", "type": "numpy.record.trace", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.trace."]}, {"name": "record.transpose()", "path": "reference/generated/numpy.record.transpose", "type": "numpy.record.transpose", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.transpose."]}, {"name": "record.var()", "path": "reference/generated/numpy.record.var", "type": "numpy.record.var", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.var."]}, {"name": "record.view()", "path": "reference/generated/numpy.record.view", "type": "numpy.record.view", "text": ["method", "Scalar method identical to the corresponding array attribute.", "Please see ndarray.view."]}, {"name": "Release", "path": "reference/index", "type": "API reference", "text": ["1.22", "December 31, 2021", "This reference manual details functions, modules, and objects included in NumPy, describing what they are and what they do. For learning how to use NumPy, see the complete documentation.", "Large parts of this manual originate from Travis E. Oliphant\u2019s book Guide to NumPy (which generously entered Public Domain in August 2008). The reference documentation for many of the functions are written by numerous contributors and developers of NumPy."]}, {"name": "Release notes", "path": "release", "type": "Release notes", "text": []}, {"name": "Releasing a version", "path": "dev/releasing", "type": "Development", "text": ["This file gives an overview of what is necessary to build binary releases for NumPy.", "The current info on building and releasing NumPy and SciPy is scattered in several places. It should be summarized in one place, updated, and where necessary described in more detail. The sections below list all places where useful info can be found.", "NEP 29 outlines which Python versions are supported; For the first half of 2020, this will be Python >= 3.6. We test NumPy against all these versions every time we merge code to main. Binary installers may be available for a subset of these versions (see below).", "OS X versions >= 10.9 are supported, for Python version support see NEP 29. We build binary wheels for OSX that are compatible with Python.org Python, system Python, homebrew and macports - see this OSX wheel building summary for details.", "We build 32- and 64-bit wheels on Windows. Windows 7, 8 and 10 are supported. We build NumPy using the mingw-w64 toolchain on Appveyor.", "We build and ship manylinux1 wheels for NumPy. Many Linux distributions include their own binary builds of NumPy.", "No binaries are provided, but successful builds on Solaris and BSD have been reported.", "We build all our wheels on cloud infrastructure - so this list of compilers is for information and debugging builds locally. See the .travis.yml script in the numpy wheels repo for the definitive source of the build recipes. Packages that are available using pip are noted.", "The same gcc version is used as the one with which Python itself is built on each platform. At the moment this means:", "You will need Cython for building the binaries. Cython compiles the .pyx files in the NumPy distribution to .c files.", "All the wheels link to a version of OpenBLAS supplied via the openblas-libs repo. The shared object (or DLL) is shipped with in the wheel, renamed to prevent name collisions with other OpenBLAS shared objects that may exist in the filesystem.", "You will need write permission for numpy-wheels in order to trigger wheel builds.", "Building the documents requires a number of latex .sty files. Install them all to avoid aggravation.", "You will need a personal access token https://help.github.com/articles/creating-a-personal-access-token-for-the-command-line/ so that scripts can access the github NumPy repository.", "Virtualenv is a very useful tool to keep several versions of packages around. It is also used in the Paver script to build the docs.", "We currently support Python 3.6-3.8 on Windows, OSX, and Linux", "See the numpy wheels building repository for more detail.", "We build source releases in both .zip and .tar.gz formats.", "A typical release schedule is one beta, two release candidates and a final release. It\u2019s best to discuss the timing on the mailing list first, in order for people to get their commits in on time, get doc wiki edits merged, etc. After a date is set, create a new maintenance/x.y.z branch, add new empty release notes for the next version in the main branch and update the Trac Milestones.", "For details of the build process itself, it is best to read the pavement.py script.", "Note", "The following steps are repeated for the beta(s), release candidates(s) and the final release.", "Before the release branch is made, it should be checked that all deprecated code that should be removed is actually removed, and all new deprecations say in the docstring or deprecation warning at what version the code will be removed.", "The C API version needs to be tracked in three places", "There are three steps to the process.", "If the C_API_VERSION in the first step has changed, or if the hash of the API has changed, the cversions.txt file needs to be updated. To check the hash, run the script numpy/core/cversions.py and note the API hash that is printed. If that hash does not match the last hash in numpy/core/code_generators/cversions.txt the hash has changed. Using both the appropriate C_API_VERSION and hash, add a new entry to cversions.txt. If the API version was not changed, but the hash differs, you will need to comment out the previous entry for that API version. For instance, in NumPy 1.9 annotations were added, which changed the hash, but the API was the same as in 1.8. The hash serves as a check for API changes, but it is not definitive.", "If steps 1 and 2 are done correctly, compiling the release should not give a warning \u201cAPI mismatch detect at the beginning of the build\u201d.", "The C ABI version number in numpy/core/setup_common.py should only be updated for a major release.", "Use towncrier to build the release note and commit the changes. This will remove all the fragments from doc/release/upcoming_changes and add doc/release/<version>-note.rst.", "towncrier build \u2013version \u201c<version>\u201d git commit -m\u201dCreate release note\u201d", "Check that the release notes are up-to-date.", "Update the release notes with a Highlights section. Mention some of the following:", "Identify the commit hash of the release, e.g. 1b2e1d63ff:", "First, change/check the following variables in pavement.py depending on the release version:", "Do any other changes. When you are ready to release, do the following changes:", "And make sure the VERSION variable is set properly.", "Now you can make the release commit and tag. We recommend you don\u2019t push the commit or tag immediately, just in case you need to do more cleanup. We prefer to defer the push of the tag until we\u2019re confident this is the exact form of the released code (see: Push the release tag and commit):", "git commit -s -m \u201cREL: Release.\u201d setup.py git tag -s <version>", "The -s flag makes a PGP (usually GPG) signed tag. Please do sign the release tags.", "The release tag should have the release number in the annotation (tag message). Unfortunately, the name of a tag can be changed without breaking the signature, the contents of the message cannot.", "See: https://github.com/scipy/scipy/issues/4919 for a discussion of signing release tags, and https://keyring.debian.org/creating-key.html for instructions on creating a GPG key if you do not have one.", "To make your key more readily identifiable as you, consider sending your key to public keyservers, with a command such as:", "Increment the release number in setup.py. Release candidates should have \u201crc1\u201d (or \u201crc2\u201d, \u201crcN\u201d) appended to the X.Y.Z format.", "Also create a new version hash in cversions.txt and a corresponding version define NPY_x_y_API_VERSION in numpyconfig.h", "See the numpy wheels repository.", "In that repository edit the files:", "In both cases, set the BUILD_COMMIT variable to the current release tag - e.g. v1.19.0:", "Make sure that the release tag has been pushed.", "Trigger a build by pushing a commit of your edits to the repository. Note that you can do this on a branch, but it must be pushed upstream to the MacPython/numpy-wheels repository to trigger uploads since only that repo has the appropriate tokens to allow uploads.", "The wheels, once built, appear at https://anaconda.org/multibuild-wheels-staging/numpy", "Build the changelog and notes for upload with:", "Do:", "to check that the documentation is in a buildable state. Then, after tagging, create an archive of the documentation in the numpy/doc repo:", "The wheels and source should be uploaded to PyPI.", "You should upload the wheels first, and the source formats last, to make sure that pip users don\u2019t accidentally get a source install when they were expecting a binary wheel.", "You can do this automatically using the wheel-uploader script from https://github.com/MacPython/terryfy. Here is the recommended incantation for downloading all the Windows, Manylinux, OSX wheels and uploading to PyPI.", "The -v flag gives verbose feedback, -s causes the script to sign the wheels with your GPG key before upload. Don\u2019t forget to upload the wheels before the source tarball, so there is no period for which people switch from an expected binary install to a source install from PyPI.", "There are two ways to update the source release on PyPI, the first one is:", "This will ask for your key PGP passphrase, in order to sign the built source packages.", "The second way is to upload the PKG_INFO file inside the sdist dir in the web interface of PyPI. The source tarball can also be uploaded through this interface.", "Finally, now you are confident this tag correctly defines the source code that you released you can push the tag and release commit up to github:", "where upstream points to the main https://github.com/numpy/numpy.git repository.", "A release announcement with a link to the download site should be placed in the sidebar of the front page of scipy.org.", "The scipy.org should be a PR at https://github.com/scipy/scipy.org. The file that needs modification is www/index.rst. Search for News.", "If this release is the first one to support a new Python version, or the first to provide wheels for a new platform or PyPy version, the version pinnings in https://github.com/scipy/oldest-supported-numpy should be updated. Either submit a PR with changes to setup.cfg there, or open an issue with info on needed changes.", "The release should be announced on the mailing lists of NumPy and SciPy, to python-announce, and possibly also those of Matplotlib, IPython and/or Pygame.", "During the beta/RC phase, an explicit request for testing the binaries with several other libraries (SciPy/Matplotlib/Pygame) should be posted on the mailing list.", "Email the editor of LWN to let them know of the release. Directions at: https://lwn.net/op/FAQ.lwn#contact", "After the final release is announced, a few administrative tasks are left to be done:", "This file contains a walkthrough of the NumPy 1.21.0 release on Linux, modified for building on azure and uploading to anaconda.org The commands can be copied into the command line, but be sure to replace 1.21.0 by the correct version.", "This should be read together with the general directions in releasing.", "Changes that have been marked for this release must be backported to the maintenance/1.21.x branch.", "Four documents usually need to be updated or created before making a release:", "These changes should be made as an ordinary PR against the maintenance branch. After release all files except doc/source/release.rst will need to be forward ported to the main branch.", "The changelog is generated using the changelog tool:", "where GITHUB contains your GitHub access token. The text will need to be checked for non-standard contributor names and dependabot entries removed. It is also a good idea to remove any links that may be present in the PR titles as they don\u2019t translate well to markdown, replace them with monospaced text. The non-standard contributor names should be fixed by updating the .mailmap file, which is a lot of work. It is best to make several trial runs before reaching this point and ping the malefactors using a GitHub issue to get the needed information.", "If this is the first release in a series the release note is generated, see the release note in doc/release/upcoming_changes/README.rst to see how to do this. Generating the release notes will also delete all the news fragment files in doc/release/upcoming_changes/.", "The generated release note will always need some fixups, the introduction will need to be written, and significant changes should be called out. For patch releases the changelog text may also be appended, but not for the initial release as it is too long. Check previous release notes to see how this is done. Note that the :orphan: markup at the top, if present, will need changing to .. currentmodule:: numpy and the doc/source/release.rst index file will need updating.", "Check that the pavement.py file points to the correct release notes. It should have been updated after the last release, but if not, fix it now:", "Note that in the code snippets below, upstream refers to the root repository on GitHub and origin to its fork in your personal GitHub repositories. You may need to make adjustments if you have not forked the repository but simply cloned it locally. You can also edit .git/config and add upstream if it isn\u2019t already present.", "Checkout the branch for the release, make sure it is up to date, and clean the repository:", "Sanity check:", "Tag the release and push the tag. This requires write permission for the numpy repository:", "Paver is used to build the source releases. It will create the release and release/installers directories and put the *.zip and *.tar.gz source releases in the latter.", "Trigger the wheels build by pointing the numpy-wheels repository at this commit. This can take up to an hour. The numpy-wheels repository is cloned from https://github.com/MacPython/numpy-wheels. If this is the first release in a series, start with a pull as the repo may have been accessed and changed by someone else, then create a new branch for the series. If the branch already exists skip this:", "Checkout the new branch and edit the azure-pipelines.yml and .travis.yml files to make sure they have the correct version, and put in the commit hash for the REL commit created above for BUILD_COMMIT variable. The azure/posix.yml and .travis.yml files may also need the Cython versions updated to keep up with Python releases, but generally just do:", "Now wait. If you get nervous at the amount of time taken \u2013 the builds can take a while \u2013 you can check the build progress by following the links provided at https://github.com/MacPython/numpy-wheels to check the build status. Check if all the needed wheels have been built and uploaded to the staging repository before proceeding.", "Note that sometimes builds, like tests, fail for unrelated reasons and you will need to rerun them. You will need to be logged in under \u2018numpy\u2019 to do this on azure.", "When the wheels have all been successfully built and staged, download them from the Anaconda staging directory using the tools/download-wheels.py script:", "This needs to be done after all installers are downloaded, but before the pavement file is updated for continued development:", "Create release notes for next release and edit them to set the version. These notes will be a skeleton and have little content:", "Add new release notes to the documentation release list and update the RELEASE_NOTES variable in pavement.py.", "$ gvim doc/source/release.rst pavement.py", "Commit the result:", "Upload to PyPI using twine. A recent version of twine of is needed after recent PyPI changes, version 3.4.1 was used here:", "If one of the commands breaks in the middle, you may need to selectively upload the remaining files because PyPI does not allow the same file to be uploaded twice. The source file should be uploaded last to avoid synchronization problems that might occur if pip users access the files while this is in process, causing pip to build from source rather than downloading a binary wheel. PyPI only allows a single source distribution, here we have chosen the zip archive.", "Go to https://github.com/numpy/numpy/releases, there should be a v1.21.0\ntag, click on it and hit the edit button for that tag. There are two ways to add files, using an editable text window and as binary uploads. Start by editing the release/README.md that is translated from the rst version using pandoc. Things that will need fixing: PR lines from the changelog, if included, are wrapped and need unwrapping, links should be changed to monospaced text. Then copy the contents to the clipboard and paste them into the text window. It may take several tries to get it look right. Then", "This step is only needed for final releases and can be skipped for pre-releases and most patch releases. make merge-doc clones the numpy/doc repo into doc/build/merge and updates it with the new documentation:: Note that if you have a local numpy install, you should either remove it or install the current version for the docs to pick up the correct NumPy version.", "$ pushd doc $ make dist $ make merge-doc $ popd", "If the release series is a new one, you will need to add a new section to the doc/build/merge/index.html front page just after the \u201cinsert here\u201d comment:", "Otherwise, only the zip and pdf links should be updated with the new tag name:", "You can \u201ctest run\u201d the new documentation in a browser to make sure the links work:", "Update the stable link:", "Once everything seems satisfactory, commit and upload the changes:", "This assumes that you have forked https://github.com/scipy/scipy.org:", "Now go to your fork and make a pull request for the branch.", "The release should be announced on the numpy-discussion, scipy-devel, scipy-user, and python-announce-list mailing lists. Look at previous announcements for the basic template. The contributor and PR lists are the same as generated for the release notes above. If you crosspost, make sure that python-announce-list is BCC so that replies will not be sent to that list.", "Checkout main and forward port the documentation changes:", "Go to GitHub and make a PR."]}, {"name": "Reporting bugs", "path": "bugs", "type": "Reporting bugs", "text": ["File bug reports or feature requests, and make contributions (e.g. code patches), by opening a \u201cnew issue\u201d on GitHub:", "Please give as much information as you can in the ticket. It is extremely useful if you can supply a small self-contained code snippet that reproduces the problem. Also specify the component, the version you are referring to and the milestone.", "Report bugs to the appropriate GitHub project (there is one for NumPy and a different one for SciPy).", "More information can be found on the https://www.scipy.org/scipylib/dev-zone.html website."]}, {"name": "Routines", "path": "reference/routines", "type": "Routines", "text": ["In this chapter routine docstrings are presented, grouped by functionality. Many docstrings contain example code, which demonstrates basic usage of the routine. The examples assume that NumPy is imported with:", "A convenient way to execute examples is the %doctest_mode mode of IPython, which allows for pasting of multi-line examples and preserves indentation."]}, {"name": "self.typeStr", "path": "reference/swig.testing", "type": "Testing the numpy.i Typemaps", "text": ["Writing tests for the numpy.i SWIG interface file is a combinatorial headache. At present, 12 different data types are supported, each with 74 different argument signatures, for a total of 888 typemaps supported \u201cout of the box\u201d. Each of these typemaps, in turn, might require several unit tests in order to verify expected behavior for both proper and improper inputs. Currently, this results in more than 1,000 individual unit tests executed when make test is run in the numpy/tools/swig subdirectory.", "To facilitate this many similar unit tests, some high-level programming techniques are employed, including C and SWIG macros, as well as Python inheritance. The purpose of this document is to describe the testing infrastructure employed to verify that the numpy.i typemaps are working as expected.", "There are three independent testing frameworks supported, for one-, two-, and three-dimensional arrays respectively. For one-dimensional arrays, there are two C++ files, a header and a source, named:", "that contain prototypes and code for a variety of functions that have one-dimensional arrays as function arguments. The file:", "is a SWIG interface file that defines a python module Vector that wraps the functions in Vector.h while utilizing the typemaps in numpy.i to correctly handle the C arrays.", "The Makefile calls swig to generate Vector.py and Vector_wrap.cxx, and also executes the setup.py script that compiles Vector_wrap.cxx and links together the extension module _Vector.so or _Vector.dylib, depending on the platform. This extension module and the proxy file Vector.py are both placed in a subdirectory under the build directory.", "The actual testing takes place with a Python script named:", "that uses the standard Python library module unittest, which performs several tests of each function defined in Vector.h for each data type supported.", "Two-dimensional arrays are tested in exactly the same manner. The above description applies, but with Matrix substituted for Vector. For three-dimensional tests, substitute Tensor for Vector. For four-dimensional tests, substitute SuperTensor for Vector. For flat in-place array tests, substitute Flat for Vector. For the descriptions that follow, we will reference the Vector tests, but the same information applies to Matrix, Tensor and SuperTensor tests.", "The command make test will ensure that all of the test software is built and then run all three test scripts.", "Vector.h is a C++ header file that defines a C macro called TEST_FUNC_PROTOS that takes two arguments: TYPE, which is a data type name such as unsigned int; and SNAME, which is a short name for the same data type with no spaces, e.g. uint. This macro defines several function prototypes that have the prefix SNAME and have at least one argument that is an array of type TYPE. Those functions that have return arguments return a TYPE value.", "TEST_FUNC_PROTOS is then implemented for all of the data types supported by numpy.i:", "Vector.cxx is a C++ source file that implements compilable code for each of the function prototypes specified in Vector.h. It defines a C macro TEST_FUNCS that has the same arguments and works in the same way as TEST_FUNC_PROTOS does in Vector.h. TEST_FUNCS is implemented for each of the 12 data types as above.", "Vector.i is a SWIG interface file that defines python module Vector. It follows the conventions for using numpy.i as described in this chapter. It defines a SWIG macro %apply_numpy_typemaps that has a single argument TYPE. It uses the SWIG directive %apply to apply the provided typemaps to the argument signatures found in Vector.h. This macro is then implemented for all of the data types supported by numpy.i. It then does a %include \"Vector.h\" to wrap all of the function prototypes in Vector.h using the typemaps in numpy.i.", "After make is used to build the testing extension modules, testVector.py can be run to execute the tests. As with other scripts that use unittest to facilitate unit testing, testVector.py defines a class that inherits from unittest.TestCase:", "However, this class is not run directly. Rather, it serves as a base class to several other python classes, each one specific to a particular data type. The VectorTestCase class stores two strings for typing information:", "A string that matches one of the SNAME prefixes used in Vector.h and Vector.cxx. For example, \"double\".", "A short (typically single-character) string that represents a data type in numpy and corresponds to self.typeStr. For example, if self.typeStr is \"double\", then self.typeCode should be \"d\".", "Each test defined by the VectorTestCase class extracts the python function it is trying to test by accessing the Vector module\u2019s dictionary:", "In the case of double precision tests, this will return the python function Vector.doubleLength.", "We then define a new test case class for each supported data type with a short definition such as:", "Each of these 12 classes is collected into a unittest.TestSuite, which is then executed. Errors and failures are summed together and returned as the exit argument. Any non-zero result indicates that at least one test did not pass."]}, {"name": "Set routines", "path": "reference/routines.set", "type": "Set routines", "text": ["lib.arraysetops", "Set operations for arrays based on sorting.", "unique(ar[, return_index, return_inverse, ...])", "Find the unique elements of an array.", "in1d(ar1, ar2[, assume_unique, invert])", "Test whether each element of a 1-D array is also present in a second array.", "intersect1d(ar1, ar2[, assume_unique, ...])", "Find the intersection of two arrays.", "isin(element, test_elements[, ...])", "Calculates element in test_elements, broadcasting over element only.", "setdiff1d(ar1, ar2[, assume_unique])", "Find the set difference of two arrays.", "setxor1d(ar1, ar2[, assume_unique])", "Find the set exclusive-or of two arrays.", "union1d(ar1, ar2)", "Find the union of two arrays."]}, {"name": "Setting up and using your development environment", "path": "dev/development_environment", "type": "Development", "text": ["Since NumPy contains parts written in C and Cython that need to be compiled before use, make sure you have the necessary compilers and Python development headers installed - see Building from source. Building NumPy as of version 1.17 requires a C99 compliant compiler.", "Having compiled code also means that importing NumPy from the development sources needs some additional steps, which are explained below. For the rest of this chapter we assume that you have set up your git repo as described in Git for development.", "To build the development version of NumPy and run tests, spawn interactive shells with the Python import paths properly set up etc., do one of:", "This builds NumPy first, so the first time it may take a few minutes. If you specify -n, the tests are run against the version of NumPy (if any) found on current PYTHONPATH.", "When specifying a target using -s, -t, or --python, additional arguments may be forwarded to the target embedded by runtests.py by passing the extra arguments after a bare --. For example, to run a test method with the --pdb flag forwarded to the target, run the following:", "When using pytest as a target (the default), you can match test names using python operators by passing the -k argument to pytest:", "Note", "Remember that all tests of NumPy should pass before committing your changes.", "Using runtests.py is the recommended approach to running tests. There are also a number of alternatives to it, for example in-place build or installing to a virtualenv or a conda environment. See the FAQ below for details.", "Note", "Some of the tests in the test suite require a large amount of memory, and are skipped if your system does not have enough.", "To override the automatic detection of available memory, set the environment variable NPY_AVAILABLE_MEM, for example NPY_AVAILABLE_MEM=32GB, or using pytest --available-memory=32GB target option.", "For development, you can set up an in-place build so that changes made to .py files have effect without rebuild. First, run:", "This allows you to import the in-place built NumPy from the repo base directory only. If you want the in-place build to be visible outside that base dir, you need to point your PYTHONPATH environment variable to this directory. Some IDEs (Spyder for example) have utilities to manage PYTHONPATH. On Linux and OSX, you can run the command:", "and on Windows:", "Now editing a Python source file in NumPy allows you to immediately test and use your changes (in .py files), by simply restarting the interpreter.", "Note that another way to do an inplace build visible outside the repo base dir is with python setup.py develop. Instead of adjusting PYTHONPATH, this installs a .egg-link file into your site-packages as well as adjusts the easy-install.pth there, so its a more permanent (and magical) operation.", "Build options can be discovered by running any of:", "It\u2019s possible to do a parallel build with numpy.distutils with the -j option; see Parallel builds for more details.", "A similar approach to in-place builds and use of PYTHONPATH but outside the source tree is to use:", "NumPy uses a series of tests to probe the compiler and libc libraries for functions. The results are stored in _numpyconfig.h and config.h files using HAVE_XXX definitions. These tests are run during the build_src phase of the _multiarray_umath module in the generate_config_h and generate_numpyconfig_h functions. Since the output of these calls includes many compiler warnings and errors, by default it is run quietly. If you wish to see this output, you can run the build_src stage verbosely:", "A frequently asked question is \u201cHow do I set up a development version of NumPy in parallel to a released version that I use to do my job/research?\u201d.", "One simple way to achieve this is to install the released version in site-packages, by using pip or conda for example, and set up the development version in a virtual environment.", "If you use conda, we recommend creating a separate virtual environment for numpy development using the environment.yml file in the root of the repo (this will create the environment and install all development dependencies at once):", "If you installed Python some other way than conda, first install virtualenv (optionally use virtualenvwrapper), then create your virtualenv (named numpy-dev here) with:", "Now, whenever you want to switch to the virtual environment, you can use the command source numpy-dev/bin/activate, and deactivate to exit from the virtual environment and back to your previous shell.", "Besides using runtests.py, there are various ways to run the tests. Inside the interpreter, tests can be run like this:", "Or a similar way from the command line:", "Tests can also be run with pytest numpy, however then the NumPy-specific plugin is not found which causes strange side effects", "Running individual test files can be useful; it\u2019s much faster than running the whole test suite or that of a whole module (example: np.random.test()). This can be done with:", "That also takes extra arguments, like --pdb which drops you into the Python debugger when a test fails or an exception is raised.", "Running tests with tox is also supported. For example, to build NumPy and run the test suite with Python 3.7, use:", "For more extensive information, see Testing Guidelines", "Note: do not run the tests from the root directory of your numpy git repo without ``runtests.py``, that will result in strange test errors.", "Lint checks can be performed on newly added lines of Python code.", "Install all dependent packages using pip:", "To run lint checks before committing new code, run:", "To check all changes in newly added Python code of current branch with target branch, run:", "If there are no errors, the script exits with no message. In case of errors:", "It is advisable to run lint checks before pushing commits to a remote branch since the linter runs as part of the CI pipeline.", "For more details on Style Guidelines:", "Rebuilding NumPy after making changes to compiled code can be done with the same build command as you used previously - only the changed files will be re-built. Doing a full build, which sometimes is necessary, requires cleaning the workspace first. The standard way of doing this is (note: deletes any uncommitted files!):", "When you want to discard all changes and go back to the last commit in the repo, use one of:", "Another frequently asked question is \u201cHow do I debug C code inside NumPy?\u201d. First, ensure that you have gdb installed on your system with the Python extensions (often the default on Linux). You can see which version of Python is running inside gdb to verify your setup:", "Next you need to write a Python script that invokes the C code whose execution you want to debug. For instance mytest.py:", "Now, you can run:", "And then in the debugger:", "The execution will now stop at the corresponding C function and you can step through it as usual. A number of useful Python-specific commands are available. For example to see where in the Python code you are, use py-list. For more details, see DebuggingWithGdb. Here are some commonly used commands:", "Instead of plain gdb you can of course use your favourite alternative debugger; run it on the python binary with arguments runtests.py -g --python mytest.py.", "Building NumPy with a Python built with debug support (on Linux distributions typically packaged as python-dbg) is highly recommended.", "The best strategy to better understand the code base is to pick something you want to change and start reading the code to figure out how it works. When in doubt, you can ask questions on the mailing list. It is perfectly okay if your pull requests aren\u2019t perfect, the community is always happy to help. As a volunteer project, things do sometimes get dropped and it\u2019s totally fine to ping us if something has sat without a response for about two to four weeks.", "So go ahead and pick something that annoys or confuses you about NumPy, experiment with the code, hang around for discussions or go through the reference documents to try to fix it. Things will fall in place and soon you\u2019ll have a pretty good understanding of the project as a whole. Good Luck!"]}, {"name": "Setting up git for NumPy development", "path": "dev/gitwash/development_setup", "type": "Development", "text": ["To contribute code or documentation, you first need", "You may already have git; check by typing git --version. If it\u2019s installed you\u2019ll see some variation of git version 2.11.0. If instead you see command is not recognized, command not\nfound, etc., install git.", "Then set your name and email:", "If you don\u2019t have a GitHub account, visit https://github.com/join to create one.", "Forking has two steps \u2013 visit GitHub to create a fork repo in your account, then make a copy of it on your own machine.", "At the upper right of the page, click Fork:", "You\u2019ll see", "and then you\u2019ll be taken to the home page of your forked copy:", "In the directory where you want the copy created, run", "You\u2019ll see something like:", "A directory numpy is created on your machine. (If you already have a numpy directory, GitHub will choose a different name like numpy-1.)", "Give the name upstream to the main NumPy repo:", "Set up your repository so git pull pulls from upstream by default:", "The branches shown by git branch -a will include", "If upstream isn\u2019t there, it will be added after you access the NumPy repo with a command like git fetch or git pull.", "The repos shown by git remote -v show will include your fork on GitHub and the main repo:", "git config --list will include", "Cloning your NumPy fork repo required no password, because it read the remote repo without changing it. Later, though, submitting your pull requests will write to it, and GitHub will ask for your username and password \u2013 even though it\u2019s your own repo. You can eliminate this authentication without compromising security by setting up SSH keys .", "If you set up the keys before cloning, the instructions above change slightly. Instead of", "run", "and instead of showing an https URL, git remote -v will show", "If you have cloned already and want to start using SSH, see Switching remote URLs from HTTPS to SSH ."]}, {"name": "setup.py", "path": "reference/random/examples/cython/setup.py", "type": "Cython", "text": []}, {"name": "Some cases where uint and true alignment are different ()", "path": "dev/alignment", "type": "Development", "text": ["There are three use-cases related to memory alignment in NumPy (as of 1.14):", "NumPy uses two different forms of alignment to achieve these goals: \u201cTrue alignment\u201d and \u201cUint alignment\u201d.", "\u201cTrue\u201d alignment refers to the architecture-dependent alignment of an equivalent C-type in C. For example, in x64 systems float64 is equivalent to double in C. On most systems, this has either an alignment of 4 or 8 bytes (and this can be controlled in GCC by the option malign-double). A variable is aligned in memory if its memory offset is a multiple of its alignment. On some systems (eg. sparc) memory alignment is required; on others, it gives a speedup.", "\u201cUint\u201d alignment depends on the size of a datatype. It is defined to be the \u201cTrue alignment\u201d of the uint used by NumPy\u2019s copy-code to copy the datatype, or undefined/unaligned if there is no equivalent uint. Currently, NumPy uses uint8, uint16, uint32, uint64, and uint64 to copy data of size 1, 2, 4, 8, 16 bytes respectively, and all other sized datatypes cannot be uint-aligned.", "For example, on a (typical Linux x64 GCC) system, the NumPy complex64 datatype is implemented as struct { float real, imag; }. This has \u201ctrue\u201d alignment of 4 and \u201cuint\u201d alignment of 8 (equal to the true alignment of uint64).", "arch", "type", "true-aln", "uint-aln", "x86_64", "complex64", "4", "8", "x86_64", "float128", "16", "8", "x86", "float96", "4", "-", "There are 4 relevant uses of the word align used in NumPy:", "Here is how the variables above are used:", "Note that the strided-copy and strided-cast code are deeply intertwined and so any arrays being processed by them must be both uint and true aligned, even though the copy-code only needs uint alignment and the cast code only true alignment. If there is ever a big rewrite of this code it would be good to allow them to use different alignments."]}, {"name": "Sorting, searching, and counting", "path": "reference/routines.sort", "type": "Sorting, searching, and counting", "text": ["sort(a[, axis, kind, order])", "Return a sorted copy of an array.", "lexsort(keys[, axis])", "Perform an indirect stable sort using a sequence of keys.", "argsort(a[, axis, kind, order])", "Returns the indices that would sort an array.", "ndarray.sort([axis, kind, order])", "Sort an array in-place.", "msort(a)", "Return a copy of an array sorted along the first axis.", "sort_complex(a)", "Sort a complex array using the real part first, then the imaginary part.", "partition(a, kth[, axis, kind, order])", "Return a partitioned copy of an array.", "argpartition(a, kth[, axis, kind, order])", "Perform an indirect partition along the given axis using the algorithm specified by the kind keyword.", "argmax(a[, axis, out, keepdims])", "Returns the indices of the maximum values along an axis.", "nanargmax(a[, axis, out, keepdims])", "Return the indices of the maximum values in the specified axis ignoring NaNs.", "argmin(a[, axis, out, keepdims])", "Returns the indices of the minimum values along an axis.", "nanargmin(a[, axis, out, keepdims])", "Return the indices of the minimum values in the specified axis ignoring NaNs.", "argwhere(a)", "Find the indices of array elements that are non-zero, grouped by element.", "nonzero(a)", "Return the indices of the elements that are non-zero.", "flatnonzero(a)", "Return indices that are non-zero in the flattened version of a.", "where(condition, [x, y], /)", "Return elements chosen from x or y depending on condition.", "searchsorted(a, v[, side, sorter])", "Find indices where elements should be inserted to maintain order.", "extract(condition, arr)", "Return the elements of an array that satisfy some condition.", "count_nonzero(a[, axis, keepdims])", "Counts the number of non-zero values in the array a."]}, {"name": "static distutils.ccompiler_opt.CCompilerOpt.dist_error()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.dist_error", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.dist_error", "text": ["method", "Raise a compiler error"]}, {"name": "static distutils.ccompiler_opt.CCompilerOpt.dist_fatal()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.dist_fatal", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.dist_fatal", "text": ["method", "Raise a distutils error"]}, {"name": "static distutils.ccompiler_opt.CCompilerOpt.dist_load_module()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.dist_load_module", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.dist_load_module", "text": ["method", "Load a module from file, required by the abstract class \u2018_Cache\u2019."]}, {"name": "static distutils.ccompiler_opt.CCompilerOpt.dist_log()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.dist_log", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.dist_log", "text": ["method", "Print a console message"]}, {"name": "static distutils.ccompiler_opt.CCompilerOpt.me()", "path": "reference/generated/numpy.distutils.ccompiler_opt.ccompileropt.me", "type": "numpy.distutils.ccompiler_opt.CCompilerOpt.me", "text": ["method", "A static method that can be treated as a decorator to dynamically cache certain methods."]}, {"name": "static ma.MaskedArray.__new__()", "path": "reference/generated/numpy.ma.maskedarray.__new__", "type": "Masked arrays", "text": ["method", "Create a new masked array from scratch.", "A masked array can also be created by taking a .view(MaskedArray)."]}, {"name": "Statistics", "path": "reference/routines.statistics", "type": "Statistics", "text": ["ptp(a[, axis, out, keepdims])", "Range of values (maximum - minimum) along an axis.", "percentile(a, q[, axis, out, ...])", "Compute the q-th percentile of the data along the specified axis.", "nanpercentile(a, q[, axis, out, ...])", "Compute the qth percentile of the data along the specified axis, while ignoring nan values.", "quantile(a, q[, axis, out, overwrite_input, ...])", "Compute the q-th quantile of the data along the specified axis.", "nanquantile(a, q[, axis, out, ...])", "Compute the qth quantile of the data along the specified axis, while ignoring nan values.", "median(a[, axis, out, overwrite_input, keepdims])", "Compute the median along the specified axis.", "average(a[, axis, weights, returned])", "Compute the weighted average along the specified axis.", "mean(a[, axis, dtype, out, keepdims, where])", "Compute the arithmetic mean along the specified axis.", "std(a[, axis, dtype, out, ddof, keepdims, where])", "Compute the standard deviation along the specified axis.", "var(a[, axis, dtype, out, ddof, keepdims, where])", "Compute the variance along the specified axis.", "nanmedian(a[, axis, out, overwrite_input, ...])", "Compute the median along the specified axis, while ignoring NaNs.", "nanmean(a[, axis, dtype, out, keepdims, where])", "Compute the arithmetic mean along the specified axis, ignoring NaNs.", "nanstd(a[, axis, dtype, out, ddof, ...])", "Compute the standard deviation along the specified axis, while ignoring NaNs.", "nanvar(a[, axis, dtype, out, ddof, ...])", "Compute the variance along the specified axis, while ignoring NaNs.", "corrcoef(x[, y, rowvar, bias, ddof, dtype])", "Return Pearson product-moment correlation coefficients.", "correlate(a, v[, mode])", "Cross-correlation of two 1-dimensional sequences.", "cov(m[, y, rowvar, bias, ddof, fweights, ...])", "Estimate a covariance matrix, given data and weights.", "histogram(a[, bins, range, normed, weights, ...])", "Compute the histogram of a dataset.", "histogram2d(x, y[, bins, range, normed, ...])", "Compute the bi-dimensional histogram of two data samples.", "histogramdd(sample[, bins, range, normed, ...])", "Compute the multidimensional histogram of some data.", "bincount(x, /[, weights, minlength])", "Count number of occurrences of each value in array of non-negative ints.", "histogram_bin_edges(a[, bins, range, weights])", "Function to calculate only the edges of the bins used by the histogram function.", "digitize(x, bins[, right])", "Return the indices of the bins to which each value in input array belongs."]}, {"name": "String operations", "path": "reference/routines.char", "type": "String operations", "text": ["The numpy.char module provides a set of vectorized string operations for arrays of type numpy.str_ or numpy.bytes_. All of them are based on the string methods in the Python standard library.", "add(x1, x2)", "Return element-wise string concatenation for two arrays of str or unicode.", "multiply(a, i)", "Return (a * i), that is string multiple concatenation, element-wise.", "mod(a, values)", "Return (a % i), that is pre-Python 2.6 string formatting (interpolation), element-wise for a pair of array_likes of str or unicode.", "capitalize(a)", "Return a copy of a with only the first character of each element capitalized.", "center(a, width[, fillchar])", "Return a copy of a with its elements centered in a string of length width.", "decode(a[, encoding, errors])", "Calls str.decode element-wise.", "encode(a[, encoding, errors])", "Calls str.encode element-wise.", "expandtabs(a[, tabsize])", "Return a copy of each string element where all tab characters are replaced by one or more spaces.", "join(sep, seq)", "Return a string which is the concatenation of the strings in the sequence seq.", "ljust(a, width[, fillchar])", "Return an array with the elements of a left-justified in a string of length width.", "lower(a)", "Return an array with the elements converted to lowercase.", "lstrip(a[, chars])", "For each element in a, return a copy with the leading characters removed.", "partition(a, sep)", "Partition each element in a around sep.", "replace(a, old, new[, count])", "For each element in a, return a copy of the string with all occurrences of substring old replaced by new.", "rjust(a, width[, fillchar])", "Return an array with the elements of a right-justified in a string of length width.", "rpartition(a, sep)", "Partition (split) each element around the right-most separator.", "rsplit(a[, sep, maxsplit])", "For each element in a, return a list of the words in the string, using sep as the delimiter string.", "rstrip(a[, chars])", "For each element in a, return a copy with the trailing characters removed.", "split(a[, sep, maxsplit])", "For each element in a, return a list of the words in the string, using sep as the delimiter string.", "splitlines(a[, keepends])", "For each element in a, return a list of the lines in the element, breaking at line boundaries.", "strip(a[, chars])", "For each element in a, return a copy with the leading and trailing characters removed.", "swapcase(a)", "Return element-wise a copy of the string with uppercase characters converted to lowercase and vice versa.", "title(a)", "Return element-wise title cased version of string or unicode.", "translate(a, table[, deletechars])", "For each element in a, return a copy of the string where all characters occurring in the optional argument deletechars are removed, and the remaining characters have been mapped through the given translation table.", "upper(a)", "Return an array with the elements converted to uppercase.", "zfill(a, width)", "Return the numeric string left-filled with zeros", "Unlike the standard numpy comparison operators, the ones in the char module strip trailing whitespace characters before performing the comparison.", "equal(x1, x2)", "Return (x1 == x2) element-wise.", "not_equal(x1, x2)", "Return (x1 != x2) element-wise.", "greater_equal(x1, x2)", "Return (x1 >= x2) element-wise.", "less_equal(x1, x2)", "Return (x1 <= x2) element-wise.", "greater(x1, x2)", "Return (x1 > x2) element-wise.", "less(x1, x2)", "Return (x1 < x2) element-wise.", "compare_chararrays(a1, a2, cmp, rstrip)", "Performs element-wise comparison of two string arrays using the comparison operator specified by cmp_op.", "count(a, sub[, start, end])", "Returns an array with the number of non-overlapping occurrences of substring sub in the range [start, end].", "endswith(a, suffix[, start, end])", "Returns a boolean array which is True where the string element in a ends with suffix, otherwise False.", "find(a, sub[, start, end])", "For each element, return the lowest index in the string where substring sub is found.", "index(a, sub[, start, end])", "Like find, but raises ValueError when the substring is not found.", "isalpha(a)", "Returns true for each element if all characters in the string are alphabetic and there is at least one character, false otherwise.", "isalnum(a)", "Returns true for each element if all characters in the string are alphanumeric and there is at least one character, false otherwise.", "isdecimal(a)", "For each element, return True if there are only decimal characters in the element.", "isdigit(a)", "Returns true for each element if all characters in the string are digits and there is at least one character, false otherwise.", "islower(a)", "Returns true for each element if all cased characters in the string are lowercase and there is at least one cased character, false otherwise.", "isnumeric(a)", "For each element, return True if there are only numeric characters in the element.", "isspace(a)", "Returns true for each element if there are only whitespace characters in the string and there is at least one character, false otherwise.", "istitle(a)", "Returns true for each element if the element is a titlecased string and there is at least one character, false otherwise.", "isupper(a)", "Returns true for each element if all cased characters in the string are uppercase and there is at least one character, false otherwise.", "rfind(a, sub[, start, end])", "For each element in a, return the highest index in the string where substring sub is found, such that sub is contained within [start, end].", "rindex(a, sub[, start, end])", "Like rfind, but raises ValueError when the substring sub is not found.", "startswith(a, prefix[, start, end])", "Returns a boolean array which is True where the string element in a starts with prefix, otherwise False.", "str_len(a)", "Return len(a) element-wise.", "array(obj[, itemsize, copy, unicode, order])", "Create a chararray.", "asarray(obj[, itemsize, unicode, order])", "Convert the input to a chararray, copying the data only if necessary.", "chararray(shape[, itemsize, unicode, ...])", "Provides a convenient view on arrays of string and unicode values."]}, {"name": "Subclassing ndarray", "path": "user/basics.subclassing", "type": "User Guide", "text": ["Subclassing ndarray is relatively simple, but it has some complications compared to other Python objects. On this page we explain the machinery that allows you to subclass ndarray, and the implications for implementing a subclass.", "Subclassing ndarray is complicated by the fact that new instances of ndarray classes can come about in three different ways. These are:", "The last two are characteristics of ndarrays - in order to support things like array slicing. The complications of subclassing ndarray are due to the mechanisms numpy has to support these latter two routes of instance creation.", "View casting is the standard ndarray mechanism by which you take an ndarray of any subclass, and return a view of the array as another (specified) subclass:", "New instances of an ndarray subclass can also come about by a very similar mechanism to View casting, when numpy finds it needs to create a new instance from a template instance. The most obvious place this has to happen is when you are taking slices of subclassed arrays. For example:", "The slice is a view onto the original c_arr data. So, when we take a view from the ndarray, we return a new ndarray, of the same class, that points to the data in the original.", "There are other points in the use of ndarrays where we need such views, such as copying arrays (c_arr.copy()), creating ufunc output arrays (see also __array_wrap__ for ufuncs and other functions), and reducing methods (like c_arr.mean()).", "These paths both use the same machinery. We make the distinction here, because they result in different input to your methods. Specifically, View casting means you have created a new instance of your array type from any potential subclass of ndarray. Creating new from template means you have created a new instance of your class from a pre-existing instance, allowing you - for example - to copy across attributes that are particular to your subclass.", "If we subclass ndarray, we need to deal not only with explicit construction of our array type, but also View casting or Creating new from template. NumPy has the machinery to do this, and it is this machinery that makes subclassing slightly non-standard.", "There are two aspects to the machinery that ndarray uses to support views and new-from-template in subclasses.", "The first is the use of the ndarray.__new__ method for the main work of object initialization, rather then the more usual __init__ method. The second is the use of the __array_finalize__ method to allow subclasses to clean up after the creation of views and new instances from templates.", "__new__ is a standard Python method, and, if present, is called before __init__ when we create a class instance. See the python __new__ documentation for more detail.", "For example, consider the following Python code:", "meaning that we get:", "When we call C('hello'), the __new__ method gets its own class as first argument, and the passed argument, which is the string 'hello'. After python calls __new__, it usually (see below) calls our __init__ method, with the output of __new__ as the first argument (now a class instance), and the passed arguments following.", "As you can see, the object can be initialized in the __new__ method or the __init__ method, or both, and in fact ndarray does not have an __init__ method, because all the initialization is done in the __new__ method.", "Why use __new__ rather than just the usual __init__? Because in some cases, as for ndarray, we want to be able to return an object of some other class. Consider the following:", "meaning that:", "The definition of C is the same as before, but for D, the __new__ method returns an instance of class C rather than D. Note that the __init__ method of D does not get called. In general, when the __new__ method returns an object of class other than the class in which it is defined, the __init__ method of that class is not called.", "This is how subclasses of the ndarray class are able to return views that preserve the class type. When taking a view, the standard ndarray machinery creates the new ndarray object with something like:", "where subdtype is the subclass. Thus the returned view is of the same class as the subclass, rather than being of class ndarray.", "That solves the problem of returning views of the same type, but now we have a new problem. The machinery of ndarray can set the class this way, in its standard methods for taking views, but the ndarray __new__ method knows nothing of what we have done in our own __new__ method in order to set attributes, and so on. (Aside - why not call obj = subdtype.__new__(... then? Because we may not have a __new__ method with the same call signature).", "__array_finalize__ is the mechanism that numpy provides to allow subclasses to handle the various ways that new instances get created.", "Remember that subclass instances can come about in these three ways:", "Our MySubClass.__new__ method only gets called in the case of the explicit constructor call, so we can\u2019t rely on MySubClass.__new__ or MySubClass.__init__ to deal with the view casting and new-from-template. It turns out that MySubClass.__array_finalize__ does get called for all three methods of object creation, so this is where our object creation housekeeping usually goes.", "The arguments that __array_finalize__ receives differ for the three methods of instance creation above.", "The following code allows us to look at the call sequences and arguments:", "Now:", "The signature of __array_finalize__ is:", "One sees that the super call, which goes to ndarray.__new__, passes __array_finalize__ the new object, of our own class (self) as well as the object from which the view has been taken (obj). As you can see from the output above, the self is always a newly created instance of our subclass, and the type of obj differs for the three instance creation methods:", "Because __array_finalize__ is the only method that always sees new instances being created, it is the sensible place to fill in instance defaults for new object attributes, among other tasks.", "This may be clearer with an example.", "Using the object looks like this:", "This class isn\u2019t very useful, because it has the same constructor as the bare ndarray object, including passing in buffers and shapes and so on. We would probably prefer the constructor to be able to take an already formed ndarray from the usual numpy calls to np.array and return an object.", "Here is a class that takes a standard ndarray that already exists, casts as our type, and adds an extra attribute.", "So:", "New in version 1.13.", "A subclass can override what happens when executing numpy ufuncs on it by overriding the default ndarray.__array_ufunc__ method. This method is executed instead of the ufunc and should return either the result of the operation, or NotImplemented if the operation requested is not implemented.", "The signature of __array_ufunc__ is:", "A typical implementation would convert any inputs or outputs that are instances of one\u2019s own class, pass everything on to a superclass using super(), and finally return the results after possible back-conversion. An example, taken from the test case test_ufunc_override_with_super in core/tests/test_umath.py, is the following.", "So, this class does not actually do anything interesting: it just converts any instances of its own to regular ndarray (otherwise, we\u2019d get infinite recursion!), and adds an info dictionary that tells which inputs and outputs it converted. Hence, e.g.,", "Note that another approach would be to to use getattr(ufunc,\nmethods)(*inputs, **kwargs) instead of the super call. For this example, the result would be identical, but there is a difference if another operand also defines __array_ufunc__. E.g., lets assume that we evalulate np.add(a, b), where b is an instance of another class B that has an override. If you use super as in the example, ndarray.__array_ufunc__ will notice that b has an override, which means it cannot evaluate the result itself. Thus, it will return NotImplemented and so will our class A. Then, control will be passed over to b, which either knows how to deal with us and produces a result, or does not and returns NotImplemented, raising a TypeError.", "If instead, we replace our super call with getattr(ufunc, method), we effectively do np.add(a.view(np.ndarray), b). Again, B.__array_ufunc__ will be called, but now it sees an ndarray as the other argument. Likely, it will know how to handle this, and return a new instance of the B class to us. Our example class is not set up to handle this, but it might well be the best approach if, e.g., one were to re-implement MaskedArray using __array_ufunc__.", "As a final note: if the super route is suited to a given class, an advantage of using it is that it helps in constructing class hierarchies. E.g., suppose that our other class B also used the super in its __array_ufunc__ implementation, and we created a class C that depended on both, i.e., class C(A, B) (with, for simplicity, not another __array_ufunc__ override). Then any ufunc on an instance of C would pass on to A.__array_ufunc__, the super call in A would go to B.__array_ufunc__, and the super call in B would go to ndarray.__array_ufunc__, thus allowing A and B to collaborate.", "Prior to numpy 1.13, the behaviour of ufuncs could only be tuned using __array_wrap__ and __array_prepare__. These two allowed one to change the output type of a ufunc, but, in contrast to __array_ufunc__, did not allow one to make any changes to the inputs. It is hoped to eventually deprecate these, but __array_wrap__ is also used by other numpy functions and methods, such as squeeze, so at the present time is still needed for full functionality.", "Conceptually, __array_wrap__ \u201cwraps up the action\u201d in the sense of allowing a subclass to set the type of the return value and update attributes and metadata. Let\u2019s show how this works with an example. First we return to the simpler example subclass, but with a different name and some print statements:", "We run a ufunc on an instance of our new array:", "Note that the ufunc (np.add) has called the __array_wrap__ method with arguments self as obj, and out_arr as the (ndarray) result of the addition. In turn, the default __array_wrap__ (ndarray.__array_wrap__) has cast the result to class MySubClass, and called __array_finalize__ - hence the copying of the info attribute. This has all happened at the C level.", "But, we could do anything we wanted:", "So, by defining a specific __array_wrap__ method for our subclass, we can tweak the output from ufuncs. The __array_wrap__ method requires self, then an argument - which is the result of the ufunc - and an optional parameter context. This parameter is returned by ufuncs as a 3-element tuple: (name of the ufunc, arguments of the ufunc, domain of the ufunc), but is not set by other numpy functions. Though, as seen above, it is possible to do otherwise, __array_wrap__ should return an instance of its containing class. See the masked array subclass for an implementation.", "In addition to __array_wrap__, which is called on the way out of the ufunc, there is also an __array_prepare__ method which is called on the way into the ufunc, after the output arrays are created but before any computation has been performed. The default implementation does nothing but pass through the array. __array_prepare__ should not attempt to access the array data or resize the array, it is intended for setting the output array type, updating attributes and metadata, and performing any checks based on the input that may be desired before computation begins. Like __array_wrap__, __array_prepare__ must return an ndarray or subclass thereof or raise an error.", "One of the problems that ndarray solves is keeping track of memory ownership of ndarrays and their views. Consider the case where we have created an ndarray, arr and have taken a slice with v = arr[1:]. The two objects are looking at the same memory. NumPy keeps track of where the data came from for a particular array or view, with the base attribute:", "In general, if the array owns its own memory, as for arr in this case, then arr.base will be None - there are some exceptions to this - see the numpy book for more details.", "The base attribute is useful in being able to tell whether we have a view or the original array. This in turn can be useful if we need to know whether or not to do some specific cleanup when the subclassed array is deleted. For example, we may only want to do the cleanup if the original array is deleted, but not the views. For an example of how this can work, have a look at the memmap class in numpy.core.", "When sub-classing ndarray or creating duck-types that mimic the ndarray interface, it is your responsibility to decide how aligned your APIs will be with those of numpy. For convenience, many numpy functions that have a corresponding ndarray method (e.g., sum, mean, take, reshape) work by checking if the first argument to a function has a method of the same name. If it exists, the method is called instead of coercing the arguments to a numpy array.", "For example, if you want your sub-class or duck-type to be compatible with numpy\u2019s sum function, the method signature for this object\u2019s sum method should be the following:", "This is the exact same method signature for np.sum, so now if a user calls np.sum on this object, numpy will call the object\u2019s own sum method and pass in these arguments enumerated above in the signature, and no errors will be raised because the signatures are completely compatible with each other.", "If, however, you decide to deviate from this signature and do something like this:", "This object is no longer compatible with np.sum because if you call np.sum, it will pass in unexpected arguments out and keepdims, causing a TypeError to be raised.", "If you wish to maintain compatibility with numpy and its subsequent versions (which might add new keyword arguments) but do not want to surface all of numpy\u2019s arguments, your function\u2019s signature should accept **kwargs. For example:", "This object is now compatible with np.sum again because any extraneous arguments (i.e. keywords that are not axis or dtype) will be hidden away in the **unused_kwargs parameter."]}, {"name": "template<typename Tp, std::size_t N>class DoxyLimbo", "path": "dev/howto-docs#_CPPv4I0_NSt6size_tEE9DoxyLimbo", "type": "Development", "text": ["Template to represent limbo numbers. ", "Specializations for integer types that are part of nowhere. It doesn\u2019t support with any real types.", "Type of the integer. Required to be an integer type. ", "Number of elements. ", "Default constructor. Initialize nothing. ", "Set Default behavior for copy the limbo. ", "Returns the raw data for the limbo. ", "Example for inline comment. "]}, {"name": "Test Support (numpy.testing)", "path": "reference/routines.testing", "type": "Test Support ( \n      \n       numpy.testing\n      \n      )", "text": ["Common test support for all numpy test scripts.", "This single module should provide all the common functionality for numpy tests in a single location, so that test scripts can just import it and work right away. For background, see the Testing Guidelines", "assert_allclose(actual, desired[, rtol, ...])", "Raises an AssertionError if two objects are not equal up to desired tolerance.", "assert_array_almost_equal_nulp(x, y[, nulp])", "Compare two arrays relatively to their spacing.", "assert_array_max_ulp(a, b[, maxulp, dtype])", "Check that all items of arrays differ in at most N Units in the Last Place.", "assert_array_equal(x, y[, err_msg, verbose])", "Raises an AssertionError if two array_like objects are not equal.", "assert_array_less(x, y[, err_msg, verbose])", "Raises an AssertionError if two array_like objects are not ordered by less than.", "assert_equal(actual, desired[, err_msg, verbose])", "Raises an AssertionError if two objects are not equal.", "assert_raises(assert_raises)", "Fail unless an exception of class exception_class is thrown by callable when invoked with arguments args and keyword arguments kwargs.", "assert_raises_regex(exception_class, ...)", "Fail unless an exception of class exception_class and with message that matches expected_regexp is thrown by callable when invoked with arguments args and keyword arguments kwargs.", "assert_warns(warning_class, *args, **kwargs)", "Fail unless the given callable throws the specified warning.", "assert_string_equal(actual, desired)", "Test if two strings are equal.", "It is recommended to use one of assert_allclose, assert_array_almost_equal_nulp or assert_array_max_ulp instead of these functions for more consistent floating point comparisons.", "assert_almost_equal(actual, desired[, ...])", "Raises an AssertionError if two items are not equal up to desired precision.", "assert_approx_equal(actual, desired[, ...])", "Raises an AssertionError if two items are not equal up to significant digits.", "assert_array_almost_equal(x, y[, decimal, ...])", "Raises an AssertionError if two objects are not equal up to desired precision.", "dec.deprecated([conditional])", "Deprecated since version 1.21.", "dec.knownfailureif(fail_condition[, msg])", "Deprecated since version 1.21.", "dec.setastest([tf])", "Deprecated since version 1.21.", "dec.skipif(skip_condition[, msg])", "Deprecated since version 1.21.", "dec.slow(t)", "Deprecated since version 1.21.", "decorate_methods(cls, decorator[, testmatch])", "Apply a decorator to all methods in a class matching a regular expression.", "Tester", "alias of numpy.testing._private.nosetester.NoseTester", "run_module_suite([file_to_run, argv])", "Run a test module.", "rundocs([filename, raise_on_error])", "Run doctests found in the given file.", "suppress_warnings([forwarding_rule])", "Context manager and decorator doing much the same as warnings.catch_warnings."]}, {"name": "testing.assert_allclose()", "path": "reference/generated/numpy.testing.assert_allclose", "type": "numpy.testing.assert_allclose", "text": ["Raises an AssertionError if two objects are not equal up to desired tolerance.", "The test is equivalent to allclose(actual, desired, rtol, atol) (note that allclose has different default values). It compares the difference between actual and desired to atol + rtol * abs(desired).", "New in version 1.5.0.", "Array obtained.", "Array desired.", "Relative tolerance.", "Absolute tolerance.", "If True, NaNs will compare equal.", "The error message to be printed in case of failure.", "If True, the conflicting values are appended to the error message.", "If actual and desired are not equal up to specified precision.", "See also"]}, {"name": "testing.assert_almost_equal()", "path": "reference/generated/numpy.testing.assert_almost_equal", "type": "numpy.testing.assert_almost_equal", "text": ["Raises an AssertionError if two items are not equal up to desired precision.", "Note", "It is recommended to use one of assert_allclose, assert_array_almost_equal_nulp or assert_array_max_ulp instead of this function for more consistent floating point comparisons.", "The test verifies that the elements of actual and desired satisfy.", "abs(desired-actual) < 1.5 * 10**(-decimal)", "That is a looser test than originally documented, but agrees with what the actual implementation in assert_array_almost_equal did up to rounding vagaries. An exception is raised at conflicting values. For ndarrays this delegates to assert_array_almost_equal", "The object to check.", "The expected object.", "Desired precision, default is 7.", "The error message to be printed in case of failure.", "If True, the conflicting values are appended to the error message.", "If actual and desired are not equal up to specified precision.", "See also", "Compare two array_like objects for equality with desired relative and/or absolute precision."]}, {"name": "testing.assert_approx_equal()", "path": "reference/generated/numpy.testing.assert_approx_equal", "type": "numpy.testing.assert_approx_equal", "text": ["Raises an AssertionError if two items are not equal up to significant digits.", "Note", "It is recommended to use one of assert_allclose, assert_array_almost_equal_nulp or assert_array_max_ulp instead of this function for more consistent floating point comparisons.", "Given two numbers, check that they are approximately equal. Approximately equal is defined as the number of significant digits that agree.", "The object to check.", "The expected object.", "Desired precision, default is 7.", "The error message to be printed in case of failure.", "If True, the conflicting values are appended to the error message.", "If actual and desired are not equal up to specified precision.", "See also", "Compare two array_like objects for equality with desired relative and/or absolute precision.", "the evaluated condition that raises the exception is"]}, {"name": "testing.assert_array_almost_equal()", "path": "reference/generated/numpy.testing.assert_array_almost_equal", "type": "numpy.testing.assert_array_almost_equal", "text": ["Raises an AssertionError if two objects are not equal up to desired precision.", "Note", "It is recommended to use one of assert_allclose, assert_array_almost_equal_nulp or assert_array_max_ulp instead of this function for more consistent floating point comparisons.", "The test verifies identical shapes and that the elements of actual and desired satisfy.", "abs(desired-actual) < 1.5 * 10**(-decimal)", "That is a looser test than originally documented, but agrees with what the actual implementation did up to rounding vagaries. An exception is raised at shape mismatch or conflicting values. In contrast to the standard usage in numpy, NaNs are compared like numbers, no assertion is raised if both objects have NaNs in the same positions.", "The actual object to check.", "The desired, expected object.", "Desired precision, default is 6.", "The error message to be printed in case of failure.", "If True, the conflicting values are appended to the error message.", "If actual and desired are not equal up to specified precision.", "See also", "Compare two array_like objects for equality with desired relative and/or absolute precision.", "the first assert does not raise an exception"]}, {"name": "testing.assert_array_almost_equal_nulp()", "path": "reference/generated/numpy.testing.assert_array_almost_equal_nulp", "type": "numpy.testing.assert_array_almost_equal_nulp", "text": ["Compare two arrays relatively to their spacing.", "This is a relatively robust method to compare two arrays whose amplitude is variable.", "Input arrays.", "The maximum number of unit in the last place for tolerance (see Notes). Default is 1.", "If the spacing between x and y for one or more elements is larger than nulp.", "See also", "Check that all items of arrays differ in at most N Units in the Last Place.", "Return the distance between x and the nearest adjacent number.", "An assertion is raised if the following condition is not met:"]}, {"name": "testing.assert_array_equal()", "path": "reference/generated/numpy.testing.assert_array_equal", "type": "numpy.testing.assert_array_equal", "text": ["Raises an AssertionError if two array_like objects are not equal.", "Given two array_like objects, check that the shape is equal and all elements of these objects are equal (but see the Notes for the special handling of a scalar). An exception is raised at shape mismatch or conflicting values. In contrast to the standard usage in numpy, NaNs are compared like numbers, no assertion is raised if both objects have NaNs in the same positions.", "The usual caution for verifying equality with floating point numbers is advised.", "The actual object to check.", "The desired, expected object.", "The error message to be printed in case of failure.", "If True, the conflicting values are appended to the error message.", "If actual and desired objects are not equal.", "See also", "Compare two array_like objects for equality with desired relative and/or absolute precision.", "When one of x and y is a scalar and the other is array_like, the function checks that each element of the array_like object is equal to the scalar.", "The first assert does not raise an exception:", "Assert fails with numerical imprecision with floats:", "Use assert_allclose or one of the nulp (number of floating point values) functions for these cases instead:", "As mentioned in the Notes section, assert_array_equal has special handling for scalars. Here the test checks that each value in x is 3:"]}, {"name": "testing.assert_array_less()", "path": "reference/generated/numpy.testing.assert_array_less", "type": "numpy.testing.assert_array_less", "text": ["Raises an AssertionError if two array_like objects are not ordered by less than.", "Given two array_like objects, check that the shape is equal and all elements of the first object are strictly smaller than those of the second object. An exception is raised at shape mismatch or incorrectly ordered values. Shape mismatch does not raise if an object has zero dimension. In contrast to the standard usage in numpy, NaNs are compared, no assertion is raised if both objects have NaNs in the same positions.", "The smaller object to check.", "The larger object to compare.", "The error message to be printed in case of failure.", "If True, the conflicting values are appended to the error message.", "If actual and desired objects are not equal.", "See also", "tests objects for equality", "test objects for equality up to precision"]}, {"name": "testing.assert_array_max_ulp()", "path": "reference/generated/numpy.testing.assert_array_max_ulp", "type": "numpy.testing.assert_array_max_ulp", "text": ["Check that all items of arrays differ in at most N Units in the Last Place.", "Input arrays to be compared.", "The maximum number of units in the last place that elements of a and b can differ. Default is 1.", "Data-type to convert a and b to if given. Default is None.", "Array containing number of representable floating point numbers between items in a and b.", "If one or more elements differ by more than maxulp.", "See also", "Compare two arrays relatively to their spacing.", "For computing the ULP difference, this API does not differentiate between various representations of NAN (ULP difference between 0x7fc00000 and 0xffc00000 is zero)."]}, {"name": "testing.assert_equal()", "path": "reference/generated/numpy.testing.assert_equal", "type": "numpy.testing.assert_equal", "text": ["Raises an AssertionError if two objects are not equal.", "Given two objects (scalars, lists, tuples, dictionaries or numpy arrays), check that all elements of these objects are equal. An exception is raised at the first conflicting values.", "When one of actual and desired is a scalar and the other is array_like, the function checks that each element of the array_like object is equal to the scalar.", "This function handles NaN comparisons as if NaN was a \u201cnormal\u201d number. That is, AssertionError is not raised if both objects have NaNs in the same positions. This is in contrast to the IEEE standard on NaNs, which says that NaN compared to anything must return False.", "The object to check.", "The expected object.", "The error message to be printed in case of failure.", "If True, the conflicting values are appended to the error message.", "If actual and desired are not equal.", "The following comparison does not raise an exception. There are NaNs in the inputs, but they are in the same positions."]}, {"name": "testing.assert_raises()", "path": "reference/generated/numpy.testing.assert_raises", "type": "numpy.testing.assert_raises", "text": ["Fail unless an exception of class exception_class is thrown by callable when invoked with arguments args and keyword arguments kwargs. If a different type of exception is thrown, it will not be caught, and the test case will be deemed to have suffered an error, exactly as for an unexpected exception.", "Alternatively, assert_raises can be used as a context manager:", "is equivalent to"]}, {"name": "testing.assert_raises_regex()", "path": "reference/generated/numpy.testing.assert_raises_regex", "type": "numpy.testing.assert_raises_regex", "text": ["Fail unless an exception of class exception_class and with message that matches expected_regexp is thrown by callable when invoked with arguments args and keyword arguments kwargs.", "Alternatively, can be used as a context manager like assert_raises.", "Name of this function adheres to Python 3.2+ reference, but should work in all versions down to 2.6.", "New in version 1.9.0."]}, {"name": "testing.assert_string_equal()", "path": "reference/generated/numpy.testing.assert_string_equal", "type": "numpy.testing.assert_string_equal", "text": ["Test if two strings are equal.", "If the given strings are equal, assert_string_equal does nothing. If they are not equal, an AssertionError is raised, and the diff between the strings is shown.", "The string to test for equality against the expected string.", "The expected string."]}, {"name": "testing.assert_warns()", "path": "reference/generated/numpy.testing.assert_warns", "type": "numpy.testing.assert_warns", "text": ["Fail unless the given callable throws the specified warning.", "A warning of class warning_class should be thrown by the callable when invoked with arguments args and keyword arguments kwargs. If a different type of warning is thrown, it will not be caught.", "If called with all arguments other than the warning class omitted, may be used as a context manager:", "do_something()", "The ability to be used as a context manager is new in NumPy v1.11.0.", "New in version 1.4.0.", "The class defining the warning that func is expected to throw.", "Callable to test", "Arguments for func.", "Keyword arguments for func."]}, {"name": "testing.dec.deprecated()", "path": "reference/generated/numpy.testing.dec.deprecated", "type": "numpy.testing.dec.deprecated", "text": ["Deprecated since version 1.21: This decorator is retained for compatibility with the nose testing framework, which is being phased out. Please use the nose2 or pytest frameworks instead.", "Filter deprecation warnings while running the test suite.", "This decorator can be used to filter DeprecationWarning\u2019s, to avoid printing them during the test suite run, while checking that the test actually raises a DeprecationWarning.", "Flag to determine whether to mark test as deprecated or not. If the condition is a callable, it is used at runtime to dynamically make the decision. Default is True.", "The deprecated decorator itself.", "New in version 1.4.0."]}, {"name": "testing.dec.knownfailureif()", "path": "reference/generated/numpy.testing.dec.knownfailureif", "type": "numpy.testing.dec.knownfailureif", "text": ["Deprecated since version 1.21: This decorator is retained for compatibility with the nose testing framework, which is being phased out. Please use the nose2 or pytest frameworks instead.", "Make function raise KnownFailureException exception if given condition is true.", "If the condition is a callable, it is used at runtime to dynamically make the decision. This is useful for tests that may require costly imports, to delay the cost until the test suite is actually executed.", "Flag to determine whether to mark the decorated test as a known failure (if True) or not (if False).", "Message to give on raising a KnownFailureException exception. Default is None.", "Decorator, which, when applied to a function, causes KnownFailureException to be raised when fail_condition is True, and the function to be called normally otherwise.", "The decorator itself is decorated with the nose.tools.make_decorator function in order to transmit function name, and various other metadata."]}, {"name": "testing.dec.setastest()", "path": "reference/generated/numpy.testing.dec.setastest", "type": "numpy.testing.dec.setastest", "text": ["Deprecated since version 1.21: This decorator is retained for compatibility with the nose testing framework, which is being phased out. Please use the nose2 or pytest frameworks instead.", "Signals to nose that this function is or is not a test.", "If True, specifies that the decorated callable is a test. If False, specifies that the decorated callable is not a test. Default is True.", "This decorator can\u2019t use the nose namespace, because it can be called from a non-test module. See also istest and nottest in nose.tools.", "setastest can be used in the following way:"]}, {"name": "testing.dec.skipif()", "path": "reference/generated/numpy.testing.dec.skipif", "type": "numpy.testing.dec.skipif", "text": ["Deprecated since version 1.21: This decorator is retained for compatibility with the nose testing framework, which is being phased out. Please use the nose2 or pytest frameworks instead.", "Make function raise SkipTest exception if a given condition is true.", "If the condition is a callable, it is used at runtime to dynamically make the decision. This is useful for tests that may require costly imports, to delay the cost until the test suite is actually executed.", "Flag to determine whether to skip the decorated test.", "Message to give on raising a SkipTest exception. Default is None.", "Decorator which, when applied to a function, causes SkipTest to be raised when skip_condition is True, and the function to be called normally otherwise.", "The decorator itself is decorated with the nose.tools.make_decorator function in order to transmit function name, and various other metadata."]}, {"name": "testing.dec.slow()", "path": "reference/generated/numpy.testing.dec.slow", "type": "numpy.testing.dec.slow", "text": ["Deprecated since version 1.21: This decorator is retained for compatibility with the nose testing framework, which is being phased out. Please use the nose2 or pytest frameworks instead.", "Label a test as \u2018slow\u2019.", "The exact definition of a slow test is obviously both subjective and hardware-dependent, but in general any individual test that requires more than a second or two should be labeled as slow (the whole suite consists of thousands of tests, so even a second is significant).", "The test to label as slow.", "The decorated test t.", "The numpy.testing module includes import decorators as dec. A test can be decorated as slow like this:"]}, {"name": "testing.decorate_methods()", "path": "reference/generated/numpy.testing.decorate_methods", "type": "numpy.testing.decorate_methods", "text": ["Apply a decorator to all methods in a class matching a regular expression.", "The given decorator is applied to all public methods of cls that are matched by the regular expression testmatch (testmatch.search(methodname)). Methods that are private, i.e. start with an underscore, are ignored.", "Class whose methods to decorate.", "Decorator to apply to methods", "The regular expression. Default value is None, in which case the nose default (re.compile(r'(?:^|[\\b_\\.%s-])[Tt]est' % os.sep)) is used. If testmatch is a string, it is compiled to a regular expression first."]}, {"name": "testing.run_module_suite()", "path": "reference/generated/numpy.testing.run_module_suite", "type": "numpy.testing.run_module_suite", "text": ["Run a test module.", "Equivalent to calling $ nosetests <argv> <file_to_run> from the command line", "Path to test module, or None. By default, run the module from which this function is called.", "Arguments to be passed to the nose test runner. argv[0] is ignored. All command line arguments accepted by nosetests will work. If it is the default value None, sys.argv is used.", "New in version 1.9.0.", "Adding the following:", "at the end of a test module will run the tests when that module is called in the python interpreter.", "Alternatively, calling:", "from an interpreter will run all the test routine in \u2018test_matlib.py\u2019."]}, {"name": "testing.rundocs()", "path": "reference/generated/numpy.testing.rundocs", "type": "numpy.testing.rundocs", "text": ["Run doctests found in the given file.", "By default rundocs raises an AssertionError on failure.", "The path to the file for which the doctests are run.", "Whether to raise an AssertionError when a doctest fails. Default is True.", "The doctests can be run by the user/developer by adding the doctests argument to the test() call. For example, to run all tests (including doctests) for numpy.lib:"]}, {"name": "testing.suppress_warnings.__call__()", "path": "reference/generated/numpy.testing.suppress_warnings.__call__", "type": "numpy.testing.suppress_warnings.__call__", "text": ["method", "Function decorator to apply certain suppressions to a whole function."]}, {"name": "testing.suppress_warnings.filter()", "path": "reference/generated/numpy.testing.suppress_warnings.filter", "type": "numpy.testing.suppress_warnings.filter", "text": ["method", "Add a new suppressing filter or apply it if the state is entered.", "Warning class to filter", "Regular expression matching the warning message.", "Module to filter for. Note that the module (and its file) must match exactly and cannot be a submodule. This may make it unreliable for external modules.", "When added within a context, filters are only added inside the context and will be forgotten when the context is exited."]}, {"name": "testing.suppress_warnings.record()", "path": "reference/generated/numpy.testing.suppress_warnings.record", "type": "numpy.testing.suppress_warnings.record", "text": ["method", "Append a new recording filter or apply it if the state is entered.", "All warnings matching will be appended to the log attribute.", "Warning class to filter", "Regular expression matching the warning message.", "Module to filter for. Note that the module (and its file) must match exactly and cannot be a submodule. This may make it unreliable for external modules.", "A list which will be filled with all matched warnings.", "When added within a context, filters are only added inside the context and will be forgotten when the context is exited."]}, {"name": "The N-dimensional array (ndarray)", "path": "reference/arrays.ndarray", "type": "The N-dimensional array ( \n      \n       ndarray\n      \n      )", "text": ["An ndarray is a (usually fixed-size) multidimensional container of items of the same type and size. The number of dimensions and items in an array is defined by its shape, which is a tuple of N non-negative integers that specify the sizes of each dimension. The type of items in the array is specified by a separate data-type object (dtype), one of which is associated with each ndarray.", "As with other container objects in Python, the contents of an ndarray can be accessed and modified by indexing or slicing the array (using, for example, N integers), and via the methods and attributes of the ndarray.", "Different ndarrays can share the same data, so that changes made in one ndarray may be visible in another. That is, an ndarray can be a \u201cview\u201d to another ndarray, and the data it is referring to is taken care of by the \u201cbase\u201d ndarray. ndarrays can also be views to memory owned by Python strings or objects implementing the buffer or array interfaces.", "A 2-dimensional array of size 2 x 3, composed of 4-byte integer elements:", "The array can be indexed using Python container-like syntax:", "For example slicing can produce views of the array:", "New arrays can be constructed using the routines detailed in Array creation routines, and also by using the low-level ndarray constructor:", "ndarray(shape[, dtype, buffer, offset, ...])", "An array object represents a multidimensional, homogeneous array of fixed-size items.", "Arrays can be indexed using an extended Python slicing syntax, array[selection]. Similar syntax is also used for accessing fields in a structured data type.", "See also", "Array Indexing.", "An instance of class ndarray consists of a contiguous one-dimensional segment of computer memory (owned by the array, or by some other object), combined with an indexing scheme that maps N integers into the location of an item in the block. The ranges in which the indices can vary is specified by the shape of the array. How many bytes each item takes and how the bytes are interpreted is defined by the data-type object associated with the array.", "A segment of memory is inherently 1-dimensional, and there are many different schemes for arranging the items of an N-dimensional array in a 1-dimensional block. NumPy is flexible, and ndarray objects can accommodate any strided indexing scheme. In a strided scheme, the N-dimensional index \\((n_0, n_1, ..., n_{N-1})\\) corresponds to the offset (in bytes):", "from the beginning of the memory block associated with the array. Here, \\(s_k\\) are integers which specify the strides of the array. The column-major order (used, for example, in the Fortran language and in Matlab) and row-major order (used in C) schemes are just specific kinds of strided scheme, and correspond to memory that can be addressed by the strides:", "where \\(d_j\\) = self.shape[j].", "Both the C and Fortran orders are contiguous, i.e., single-segment, memory layouts, in which every part of the memory block can be accessed by some combination of the indices.", "Note", "Contiguous arrays and single-segment arrays are synonymous and are used interchangeably throughout the documentation.", "While a C-style and Fortran-style contiguous array, which has the corresponding flags set, can be addressed with the above strides, the actual strides may be different. This can happen in two cases:", "Point 1. means that self and self.squeeze() always have the same contiguity and aligned flags value. This also means that even a high dimensional array could be C-style and Fortran-style contiguous at the same time.", "An array is considered aligned if the memory offsets for all elements and the base offset itself is a multiple of self.itemsize. Understanding memory-alignment leads to better performance on most hardware.", "Note", "Points (1) and (2) can currently be disabled by the compile time environmental variable NPY_RELAXED_STRIDES_CHECKING=0, which was the default before NumPy 1.10. No users should have to do this. NPY_RELAXED_STRIDES_DEBUG=1 can be used to help find errors when incorrectly relying on the strides in C-extension code (see below warning).", "You can check whether this option was enabled when your NumPy was built by looking at the value of np.ones((10,1),\norder='C').flags.f_contiguous. If this is True, then your NumPy has relaxed strides checking enabled.", "Warning", "It does not generally hold that self.strides[-1] == self.itemsize for C-style contiguous arrays or self.strides[0] == self.itemsize for Fortran-style contiguous arrays is true.", "Data in new ndarrays is in the row-major (C) order, unless otherwise specified, but, for example, basic array slicing often produces views in a different scheme.", "Note", "Several algorithms in NumPy work on arbitrarily strided arrays. However, some algorithms require single-segment arrays. When an irregularly strided array is passed in to such algorithms, a copy is automatically made.", "Array attributes reflect information that is intrinsic to the array itself. Generally, accessing an array through its attributes allows you to get and sometimes set intrinsic properties of the array without creating a new array. The exposed attributes are the core parts of an array and only some of them can be reset meaningfully without creating a new array. Information on each attribute is given below.", "The following attributes contain information about the memory layout of the array:", "ndarray.flags", "Information about the memory layout of the array.", "ndarray.shape", "Tuple of array dimensions.", "ndarray.strides", "Tuple of bytes to step in each dimension when traversing an array.", "ndarray.ndim", "Number of array dimensions.", "ndarray.data", "Python buffer object pointing to the start of the array's data.", "ndarray.size", "Number of elements in the array.", "ndarray.itemsize", "Length of one array element in bytes.", "ndarray.nbytes", "Total bytes consumed by the elements of the array.", "ndarray.base", "Base object if memory is from some other object.", "See also", "Data type objects", "The data type object associated with the array can be found in the dtype attribute:", "ndarray.dtype", "Data-type of the array's elements.", "ndarray.T", "The transposed array.", "ndarray.real", "The real part of the array.", "ndarray.imag", "The imaginary part of the array.", "ndarray.flat", "A 1-D iterator over the array.", "See also", "The Array Interface.", "__array_interface__", "Python-side of the array interface", "__array_struct__", "C-side of the array interface", "ndarray.ctypes", "An object to simplify the interaction of the array with the ctypes module.", "An ndarray object has many methods which operate on or with the array in some fashion, typically returning an array result. These methods are briefly explained below. (Each method\u2019s docstring has a more complete description.)", "For the following methods there are also corresponding functions in numpy: all, any, argmax, argmin, argpartition, argsort, choose, clip, compress, copy, cumprod, cumsum, diagonal, imag, max, mean, min, nonzero, partition, prod, ptp, put, ravel, real, repeat, reshape, round, searchsorted, sort, squeeze, std, sum, swapaxes, take, trace, transpose, var.", "ndarray.item(*args)", "Copy an element of an array to a standard Python scalar and return it.", "ndarray.tolist()", "Return the array as an a.ndim-levels deep nested list of Python scalars.", "ndarray.itemset(*args)", "Insert scalar into an array (scalar is cast to array's dtype, if possible)", "ndarray.tostring([order])", "A compatibility alias for tobytes, with exactly the same behavior.", "ndarray.tobytes([order])", "Construct Python bytes containing the raw data bytes in the array.", "ndarray.tofile(fid[, sep, format])", "Write array to a file as text or binary (default).", "ndarray.dump(file)", "Dump a pickle of the array to the specified file.", "ndarray.dumps()", "Returns the pickle of the array as a string.", "ndarray.astype(dtype[, order, casting, ...])", "Copy of the array, cast to a specified type.", "ndarray.byteswap([inplace])", "Swap the bytes of the array elements", "ndarray.copy([order])", "Return a copy of the array.", "ndarray.view([dtype][, type])", "New view of array with the same data.", "ndarray.getfield(dtype[, offset])", "Returns a field of the given array as a certain type.", "ndarray.setflags([write, align, uic])", "Set array flags WRITEABLE, ALIGNED, (WRITEBACKIFCOPY and UPDATEIFCOPY), respectively.", "ndarray.fill(value)", "Fill the array with a scalar value.", "For reshape, resize, and transpose, the single tuple argument may be replaced with n integers which will be interpreted as an n-tuple.", "ndarray.reshape(shape[, order])", "Returns an array containing the same data with a new shape.", "ndarray.resize(new_shape[, refcheck])", "Change shape and size of array in-place.", "ndarray.transpose(*axes)", "Returns a view of the array with axes transposed.", "ndarray.swapaxes(axis1, axis2)", "Return a view of the array with axis1 and axis2 interchanged.", "ndarray.flatten([order])", "Return a copy of the array collapsed into one dimension.", "ndarray.ravel([order])", "Return a flattened array.", "ndarray.squeeze([axis])", "Remove axes of length one from a.", "For array methods that take an axis keyword, it defaults to None. If axis is None, then the array is treated as a 1-D array. Any other value for axis represents the dimension along which the operation should proceed.", "ndarray.take(indices[, axis, out, mode])", "Return an array formed from the elements of a at the given indices.", "ndarray.put(indices, values[, mode])", "Set a.flat[n] = values[n] for all n in indices.", "ndarray.repeat(repeats[, axis])", "Repeat elements of an array.", "ndarray.choose(choices[, out, mode])", "Use an index array to construct a new array from a set of choices.", "ndarray.sort([axis, kind, order])", "Sort an array in-place.", "ndarray.argsort([axis, kind, order])", "Returns the indices that would sort this array.", "ndarray.partition(kth[, axis, kind, order])", "Rearranges the elements in the array in such a way that the value of the element in kth position is in the position it would be in a sorted array.", "ndarray.argpartition(kth[, axis, kind, order])", "Returns the indices that would partition this array.", "ndarray.searchsorted(v[, side, sorter])", "Find indices where elements of v should be inserted in a to maintain order.", "ndarray.nonzero()", "Return the indices of the elements that are non-zero.", "ndarray.compress(condition[, axis, out])", "Return selected slices of this array along given axis.", "ndarray.diagonal([offset, axis1, axis2])", "Return specified diagonals.", "Many of these methods take an argument named axis. In such cases,", "Example of the axis argument", "A 3-dimensional array of size 3 x 3 x 3, summed over each of its three axes", "The parameter dtype specifies the data type over which a reduction operation (like summing) should take place. The default reduce data type is the same as the data type of self. To avoid overflow, it can be useful to perform the reduction using a larger data type.", "For several methods, an optional out argument can also be provided and the result will be placed into the output array given. The out argument must be an ndarray and have the same number of elements. It can have a different data type in which case casting will be performed.", "ndarray.max([axis, out, keepdims, initial, ...])", "Return the maximum along a given axis.", "ndarray.argmax([axis, out])", "Return indices of the maximum values along the given axis.", "ndarray.min([axis, out, keepdims, initial, ...])", "Return the minimum along a given axis.", "ndarray.argmin([axis, out])", "Return indices of the minimum values along the given axis.", "ndarray.ptp([axis, out, keepdims])", "Peak to peak (maximum - minimum) value along a given axis.", "ndarray.clip([min, max, out])", "Return an array whose values are limited to [min, max].", "ndarray.conj()", "Complex-conjugate all elements.", "ndarray.round([decimals, out])", "Return a with each element rounded to the given number of decimals.", "ndarray.trace([offset, axis1, axis2, dtype, out])", "Return the sum along diagonals of the array.", "ndarray.sum([axis, dtype, out, keepdims, ...])", "Return the sum of the array elements over the given axis.", "ndarray.cumsum([axis, dtype, out])", "Return the cumulative sum of the elements along the given axis.", "ndarray.mean([axis, dtype, out, keepdims, where])", "Returns the average of the array elements along given axis.", "ndarray.var([axis, dtype, out, ddof, ...])", "Returns the variance of the array elements, along given axis.", "ndarray.std([axis, dtype, out, ddof, ...])", "Returns the standard deviation of the array elements along given axis.", "ndarray.prod([axis, dtype, out, keepdims, ...])", "Return the product of the array elements over the given axis", "ndarray.cumprod([axis, dtype, out])", "Return the cumulative product of the elements along the given axis.", "ndarray.all([axis, out, keepdims, where])", "Returns True if all elements evaluate to True.", "ndarray.any([axis, out, keepdims, where])", "Returns True if any of the elements of a evaluate to True.", "Arithmetic and comparison operations on ndarrays are defined as element-wise operations, and generally yield ndarray objects as results.", "Each of the arithmetic operations (+, -, *, /, //, %, divmod(), ** or pow(), <<, >>, &, ^, |, ~) and the comparisons (==, <, >, <=, >=, !=) is equivalent to the corresponding universal function (or ufunc for short) in NumPy. For more information, see the section on Universal Functions.", "Comparison operators:", "ndarray.__lt__(value, /)", "Return self<value.", "ndarray.__le__(value, /)", "Return self<=value.", "ndarray.__gt__(value, /)", "Return self>value.", "ndarray.__ge__(value, /)", "Return self>=value.", "ndarray.__eq__(value, /)", "Return self==value.", "ndarray.__ne__(value, /)", "Return self!=value.", "Truth value of an array (bool()):", "ndarray.__bool__(/)", "self != 0", "Note", "Truth-value testing of an array invokes ndarray.__bool__, which raises an error if the number of elements in the array is larger than 1, because the truth value of such arrays is ambiguous. Use .any() and .all() instead to be clear about what is meant in such cases. (If the number of elements is 0, the array evaluates to False.)", "Unary operations:", "ndarray.__neg__(/)", "-self", "ndarray.__pos__(/)", "+self", "ndarray.__abs__(self)", "ndarray.__invert__(/)", "~self", "Arithmetic:", "ndarray.__add__(value, /)", "Return self+value.", "ndarray.__sub__(value, /)", "Return self-value.", "ndarray.__mul__(value, /)", "Return self*value.", "ndarray.__truediv__(value, /)", "Return self/value.", "ndarray.__floordiv__(value, /)", "Return self//value.", "ndarray.__mod__(value, /)", "Return self%value.", "ndarray.__divmod__(value, /)", "Return divmod(self, value).", "ndarray.__pow__(value[, mod])", "Return pow(self, value, mod).", "ndarray.__lshift__(value, /)", "Return self<<value.", "ndarray.__rshift__(value, /)", "Return self>>value.", "ndarray.__and__(value, /)", "Return self&value.", "ndarray.__or__(value, /)", "Return self|value.", "ndarray.__xor__(value, /)", "Return self^value.", "Note", "Arithmetic, in-place:", "ndarray.__iadd__(value, /)", "Return self+=value.", "ndarray.__isub__(value, /)", "Return self-=value.", "ndarray.__imul__(value, /)", "Return self*=value.", "ndarray.__itruediv__(value, /)", "Return self/=value.", "ndarray.__ifloordiv__(value, /)", "Return self//=value.", "ndarray.__imod__(value, /)", "Return self%=value.", "ndarray.__ipow__(value, /)", "Return self**=value.", "ndarray.__ilshift__(value, /)", "Return self<<=value.", "ndarray.__irshift__(value, /)", "Return self>>=value.", "ndarray.__iand__(value, /)", "Return self&=value.", "ndarray.__ior__(value, /)", "Return self|=value.", "ndarray.__ixor__(value, /)", "Return self^=value.", "Warning", "In place operations will perform the calculation using the precision decided by the data type of the two operands, but will silently downcast the result (if necessary) so it can fit back into the array. Therefore, for mixed precision calculations, A {op}=\nB can be different than A = A {op} B. For example, suppose a = ones((3,3)). Then, a += 3j is different than a = a +\n3j: while they both perform the same computation, a += 3 casts the result to fit back in a, whereas a = a + 3j re-binds the name a to the result.", "Matrix Multiplication:", "ndarray.__matmul__(value, /)", "Return self@value.", "Note", "Matrix operators @ and @= were introduced in Python 3.5 following PEP 465, and the @ operator has been introduced in NumPy 1.10.0. Further information can be found in the matmul documentation.", "For standard library functions:", "ndarray.__copy__()", "Used if copy.copy is called on an array.", "ndarray.__deepcopy__(memo, /)", "Used if copy.deepcopy is called on an array.", "ndarray.__reduce__()", "For pickling.", "ndarray.__setstate__(state, /)", "For unpickling.", "Basic customization:", "ndarray.__new__(*args, **kwargs)", "ndarray.__array__([dtype], /)", "Returns either a new reference to self if dtype is not given or a new array of provided data type if dtype is different from the current dtype of the array.", "ndarray.__array_wrap__(array[, context], /)", "Returns a view of array with the same type as self.", "Container customization: (see Indexing)", "ndarray.__len__(/)", "Return len(self).", "ndarray.__getitem__(key, /)", "Return self[key].", "ndarray.__setitem__(key, value, /)", "Set self[key] to value.", "ndarray.__contains__(key, /)", "Return key in self.", "Conversion; the operations int(), float() and complex(). They work only on arrays that have one element in them and return the appropriate scalar.", "ndarray.__int__(self)", "ndarray.__float__(self)", "ndarray.__complex__", "String representations:", "ndarray.__str__(/)", "Return str(self).", "ndarray.__repr__(/)", "Return repr(self).", "Utility method for typing:", "ndarray.__class_getitem__(item, /)", "Return a parametrized wrapper around the ndarray type."]}, {"name": "The numpy.ma module", "path": "reference/maskedarray.generic", "type": "The \n        \n         numpy.ma\n        \n        module", "text": ["Masked arrays are arrays that may have missing or invalid entries. The numpy.ma module provides a nearly work-alike replacement for numpy that supports data arrays with masks.", "In many circumstances, datasets can be incomplete or tainted by the presence of invalid data. For example, a sensor may have failed to record a data, or recorded an invalid value. The numpy.ma module provides a convenient way to address this issue, by introducing masked arrays.", "A masked array is the combination of a standard numpy.ndarray and a mask. A mask is either nomask, indicating that no value of the associated array is invalid, or an array of booleans that determines for each element of the associated array whether the value is valid or not. When an element of the mask is False, the corresponding element of the associated array is valid and is said to be unmasked. When an element of the mask is True, the corresponding element of the associated array is said to be masked (invalid).", "The package ensures that masked entries are not used in computations.", "As an illustration, let\u2019s consider the following dataset:", "We wish to mark the fourth entry as invalid. The easiest is to create a masked array:", "We can now compute the mean of the dataset, without taking the invalid data into account:", "The main feature of the numpy.ma module is the MaskedArray class, which is a subclass of numpy.ndarray. The class, its attributes and methods are described in more details in the MaskedArray class section.", "The numpy.ma module can be used as an addition to numpy:", "To create an array with the second element invalid, we would do:", "To create a masked array where all values close to 1.e20 are invalid, we would do:", "For a complete discussion of creation methods for masked arrays please see section Constructing masked arrays."]}, {"name": "Three ways to wrap - getting started", "path": "f2py/f2py.getting-started", "type": "Three ways to wrap - getting started", "text": ["Wrapping Fortran or C functions to Python using F2PY consists of the following steps:", "Creating the so-called signature file that contains descriptions of wrappers to Fortran or C functions, also called the signatures of the functions. For Fortran routines, F2PY can create an initial signature file by scanning Fortran source codes and tracking all relevant information needed to create wrapper functions.", "F2PY compiles all sources and builds an extension module containing the wrappers.", "Depending on the situation, these steps can be carried out in a single composite command or step-by-step; in which case some steps can be omitted or combined with others.", "Below, we describe three typical approaches of using F2PY. These can be read in order of increasing effort, but also cater to different access levels depending on whether the Fortran code can be freely modified.", "The following example Fortran 77 code will be used for illustration, save it as fib1.f:", "The quickest way to wrap the Fortran subroutine FIB for use in Python is to run", "This command compiles and wraps fib1.f (-c) to create the extension module fib1.so (-m) in the current directory. A list of command line options can be seen by executing python -m numpy.f2py. Now, in Python the Fortran subroutine FIB is accessible via fib1.fib:", "Note", "One can use different values for optional n:", "but an exception is raised when it is incompatible with the input array a:", "F2PY implements basic compatibility checks between related arguments in order to avoid unexpected crashes.", "When a NumPy array, that is Fortran contiguous and has a dtype corresponding to a presumed Fortran type, is used as an input array argument, then its C pointer is directly passed to Fortran.", "Otherwise F2PY makes a contiguous copy (with the proper dtype) of the input array and passes a C pointer of the copy to the Fortran subroutine. As a result, any possible changes to the (copy of) input array have no effect to the original argument, as demonstrated below:", "Clearly, this is unexpected, as Fortran typically passes by reference. That the above example worked with dtype=float is considered accidental.", "F2PY provides an intent(inplace) attribute that modifies the attributes of an input array so that any changes made by Fortran routine will be reflected in the input argument. For example, if one specifies the intent(inplace) a directive (see subsequent sections on how), then the example above would read:", "However, the recommended way to have changes made by Fortran subroutine propagate to Python is to use the intent(out) attribute. That approach is more efficient and also cleaner.", "Though the approach to wrapping Fortran routines for Python discussed so far is very straightforward, it has several drawbacks (see the comments above). The drawbacks are due to the fact that there is no way for F2PY to determine the actual intention of the arguments; that is there is ambiguity in distinguishing between input and output arguments. Consequently, F2PY assumes that all arguments are input arguments by default.", "However, there are ways (see below) to remove this ambiguity by \u201cteaching\u201d F2PY about the true intentions of function arguments, and F2PY is then able to generate more explicit, easier to use, and less error prone wrappers for Fortran functions.", "Let us apply the steps for wrapping Fortran functions to Python one by one.", "First, we create a signature file from fib1.f by running:", "The signature file is saved to fib1.pyf (see the -h flag) and its contents are shown below.", "Next, we\u2019ll teach F2PY that the argument n is an input argument (using the intent(in) attribute) and that the result, i.e., the contents of a after calling the Fortran function FIB, should be returned to Python (using the intent(out) attribute). In addition, an array a should be created dynamically using the size determined by the input argument n (using the depend(n) attribute to indicate this dependence relation).", "The contents of a suitably modified version of fib1.pyf (saved as fib2.pyf) is as follows:", "Finally, we build the extension module with numpy.distutils by running:", "In Python:", "Note", "The \u201csmart way\u201d of wrapping Fortran functions, as explained above, is suitable for wrapping (e.g. third party) Fortran codes for which modifications to their source codes are not desirable nor even possible.", "However, if editing Fortran codes is acceptable, then the generation of an intermediate signature file can be skipped in most cases. F2PY specific attributes can be inserted directly into Fortran source codes using F2PY directives. A F2PY directive consists of special comment lines (starting with Cf2py or !f2py, for example) which are ignored by Fortran compilers but interpreted by F2PY as normal lines.", "Consider a modified version of the previous Fortran code with F2PY directives, saved as fib3.f:", "Building the extension module can be now carried out in one command:", "Notice that the resulting wrapper to FIB is as \u201csmart\u201d (unambiguous) as in the previous case:"]}, {"name": "Two and three dots in difference specs", "path": "dev/gitwash/dot2_dot3", "type": "Development", "text": ["Thanks to Yarik Halchenko for this explanation.", "Imagine a series of commits A, B, C, D\u2026 Imagine that there are two branches, topic and main. You branched topic off main when main was at commit \u2018E\u2019. The graph of the commits looks like this:", "Then:", "will output the difference from G to C (i.e. with effects of F and G), while:", "would output just differences in the topic branch (i.e. only A, B, and C)."]}, {"name": "type binomial_t", "path": "reference/random/c-api#c.binomial_t", "type": "C API for random", "text": []}, {"name": "type bitgen_t", "path": "reference/random/c-api", "type": "C API for random", "text": ["New in version 1.19.0.", "Access to various distributions below is available via Cython or C-wrapper libraries like CFFI. All the functions accept a bitgen_t as their first argument. To access these from Cython or C, you must link with the npyrandom library which is part of the NumPy distribution, located in numpy/random/lib.", "The bitgen_t holds the current state of the BitGenerator and pointers to functions that return standard C types while advancing the state.", "See Extending for examples of using these functions.", "The functions are named with the following conventions:", "Generate a single integer", "Generate random uint64 numbers in closed interval [off, off + rng]."]}, {"name": "type NPY_AO", "path": "reference/c-api/types-and-structures#c.NPY_AO", "type": "Python Types and C-Structures", "text": ["The PyArrayObject C-structure contains all of the required information for an array. All instances of an ndarray (and its subclasses) will have this structure. For future compatibility, these structure members should normally be accessed using the provided macros. If you need a shorter name, then you can make use of NPY_AO (deprecated) which is defined to be equivalent to PyArrayObject. Direct access to the struct fields are deprecated. Use the PyArray_*(arr) form instead. As of NumPy 1.20, the size of this struct is not considered part of the NumPy ABI (see note at the end of the member list).", "This is needed by all Python objects. It consists of (at least) a reference count member ( ob_refcnt ) and a pointer to the typeobject ( ob_type ). (Other elements may also be present if Python was compiled with special options see Include/object.h in the Python source tree for more information). The ob_type member points to a Python type object.", "Accessible via PyArray_DATA, this data member is a pointer to the first element of the array. This pointer can (and normally should) be recast to the data type of the array.", "An integer providing the number of dimensions for this array. When nd is 0, the array is sometimes called a rank-0 array. Such arrays have undefined dimensions and strides and cannot be accessed. Macro PyArray_NDIM defined in ndarraytypes.h points to this data member. NPY_MAXDIMS is the largest number of dimensions for any array.", "An array of integers providing the shape in each dimension as long as nd \\(\\geq\\) 1. The integer is always large enough to hold a pointer on the platform, so the dimension size is only limited by memory. PyArray_DIMS is the macro associated with this data member.", "An array of integers providing for each dimension the number of bytes that must be skipped to get to the next element in that dimension. Associated with macro PyArray_STRIDES.", "Pointed to by PyArray_BASE, this member is used to hold a pointer to another Python object that is related to this array. There are two use cases:", "When PyArray_ResolveWritebackIfCopy is called, the array pointed to by base will be updated with the contents of this array.", "A pointer to a data-type descriptor object (see below). The data-type descriptor object is an instance of a new built-in type which allows a generic description of memory. There is a descriptor structure for each data type supported. This descriptor structure contains useful information about the type as well as a pointer to a table of function pointers to implement specific functionality. As the name suggests, it is associated with the macro PyArray_DESCR.", "Pointed to by the macro PyArray_FLAGS, this data member represents the flags indicating how the memory pointed to by data is to be interpreted. Possible flags are NPY_ARRAY_C_CONTIGUOUS, NPY_ARRAY_F_CONTIGUOUS, NPY_ARRAY_OWNDATA, NPY_ARRAY_ALIGNED, NPY_ARRAY_WRITEABLE, NPY_ARRAY_WRITEBACKIFCOPY, and NPY_ARRAY_UPDATEIFCOPY.", "This member allows array objects to have weak references (using the weakref module).", "Note", "Further members are considered private and version dependent. If the size of the struct is important for your code, special care must be taken. A possible use-case when this is relevant is subclassing in C. If your code relies on sizeof(PyArrayObject) to be constant, you must add the following check at import time:", "To ensure that your code does not have to be compiled for a specific NumPy version, you may add a constant, leaving room for changes in NumPy. A solution guaranteed to be compatible with any future NumPy version requires the use of a runtime calculate offset and allocation size."]}, {"name": "type npy_cdouble", "path": "reference/c-api/dtype#c.npy_cdouble", "type": "Data Type API", "text": ["64-bit complex double"]}, {"name": "type npy_cfloat", "path": "reference/c-api/dtype#c.npy_cfloat", "type": "Data Type API", "text": ["32-bit complex float"]}, {"name": "type npy_clongdouble", "path": "reference/c-api/dtype#c.npy_clongdouble", "type": "Data Type API", "text": ["long complex double"]}, {"name": "type npy_double", "path": "reference/c-api/dtype#c.npy_double", "type": "Data Type API", "text": ["64-bit double"]}, {"name": "type npy_float", "path": "reference/c-api/dtype#c.npy_float", "type": "Data Type API", "text": ["32-bit float"]}, {"name": "type npy_hash_t", "path": "reference/c-api/types-and-structures#c.NPY_USE_SETITEM.npy_hash_t", "type": "Python Types and C-Structures", "text": []}, {"name": "type npy_int", "path": "reference/c-api/dtype#c.npy_int", "type": "Data Type API", "text": ["int"]}, {"name": "type npy_int16", "path": "reference/c-api/dtype#c.npy_int16", "type": "Data Type API", "text": ["16-bit integer"]}, {"name": "type npy_int32", "path": "reference/c-api/dtype#c.npy_int32", "type": "Data Type API", "text": ["32-bit integer"]}, {"name": "type npy_int64", "path": "reference/c-api/dtype#c.npy_int64", "type": "Data Type API", "text": ["64-bit integer"]}, {"name": "type npy_intp", "path": "reference/c-api/dtype#c.npy_intp", "type": "Data Type API", "text": ["Py_intptr_t (an integer that is the size of a pointer on the platform)."]}, {"name": "type npy_long", "path": "reference/c-api/dtype#c.npy_long", "type": "Data Type API", "text": ["long int"]}, {"name": "type npy_longdouble", "path": "reference/c-api/dtype#c.npy_longdouble", "type": "Data Type API", "text": ["long double"]}, {"name": "type npy_longlong", "path": "reference/c-api/dtype#c.npy_longlong", "type": "Data Type API", "text": ["long long int"]}, {"name": "type npy_short", "path": "reference/c-api/dtype#c.npy_short", "type": "Data Type API", "text": ["short"]}, {"name": "type npy_ubyte", "path": "reference/c-api/dtype#c.npy_ubyte", "type": "Data Type API", "text": ["unsigned char"]}, {"name": "type npy_uint", "path": "reference/c-api/dtype#c.npy_uint", "type": "Data Type API", "text": ["unsigned int"]}, {"name": "type npy_uint16", "path": "reference/c-api/dtype#c.npy_uint16", "type": "Data Type API", "text": ["16-bit unsigned integer"]}, {"name": "type npy_uint32", "path": "reference/c-api/dtype#c.npy_uint32", "type": "Data Type API", "text": ["32-bit unsigned integer"]}, {"name": "type npy_uint64", "path": "reference/c-api/dtype#c.npy_uint64", "type": "Data Type API", "text": ["64-bit unsigned integer"]}, {"name": "type npy_uintp", "path": "reference/c-api/dtype#c.npy_uintp", "type": "Data Type API", "text": ["unsigned Py_intptr_t (an integer that is the size of a pointer on the platform)."]}, {"name": "type npy_ulong", "path": "reference/c-api/dtype#c.npy_ulong", "type": "Data Type API", "text": ["unsigned long int"]}, {"name": "type npy_ulonglong", "path": "reference/c-api/dtype#c.npy_ulonglong", "type": "Data Type API", "text": ["unsigned long long int"]}, {"name": "type npy_ushort", "path": "reference/c-api/dtype#c.npy_ushort", "type": "Data Type API", "text": ["unsigned short"]}, {"name": "type NpyAuxData_CloneFunc", "path": "reference/c-api/array#c.NpyAuxData_CloneFunc", "type": "Array API", "text": ["The function pointer type for NpyAuxData clone functions. These functions should never set the Python exception on error, because they may be called from a multi-threaded context."]}, {"name": "type NpyAuxData_FreeFunc", "path": "reference/c-api/array#c.NpyAuxData_FreeFunc", "type": "Array API", "text": ["The function pointer type for NpyAuxData free functions."]}, {"name": "type NpyIter", "path": "reference/c-api/iterator", "type": "Array Iterator API", "text": ["New in version 1.6.", "The array iterator encapsulates many of the key features in ufuncs, allowing user code to support features like output parameters, preservation of memory layouts, and buffering of data with the wrong alignment or type, without requiring difficult coding.", "This page documents the API for the iterator. The iterator is named NpyIter and functions are named NpyIter_*.", "There is an introductory guide to array iteration which may be of interest for those using this C API. In many instances, testing out ideas by creating the iterator in Python is a good idea before writing the C iteration code.", "The best way to become familiar with the iterator is to look at its usage within the NumPy codebase itself. For example, here is a slightly tweaked version of the code for PyArray_CountNonzero, which counts the number of non-zero elements in an array.", "Here is a simple copy function using the iterator. The order parameter is used to control the memory layout of the allocated result, typically NPY_KEEPORDER is desired.", "The iterator layout is an internal detail, and user code only sees an incomplete struct.", "This is an opaque pointer type for the iterator. Access to its contents can only be done through the iterator API.", "This is the type which exposes the iterator to Python. Currently, no API is exposed which provides access to the values of a Python-created iterator. If an iterator is created in Python, it must be used in Python and vice versa. Such an API will likely be created in a future version.", "This is a function pointer for the iteration loop, returned by NpyIter_GetIterNext.", "This is a function pointer for getting the current iterator multi-index, returned by NpyIter_GetGetMultiIndex.", "Creates an iterator for the given numpy array object op.", "Flags that may be passed in flags are any combination of the global and per-operand flags documented in NpyIter_MultiNew, except for NPY_ITER_ALLOCATE.", "Any of the NPY_ORDER enum values may be passed to order. For efficient iteration, NPY_KEEPORDER is the best option, and the other orders enforce the particular iteration pattern.", "Any of the NPY_CASTING enum values may be passed to casting. The values include NPY_NO_CASTING, NPY_EQUIV_CASTING, NPY_SAFE_CASTING, NPY_SAME_KIND_CASTING, and NPY_UNSAFE_CASTING. To allow the casts to occur, copying or buffering must also be enabled.", "If dtype isn\u2019t NULL, then it requires that data type. If copying is allowed, it will make a temporary copy if the data is castable. If NPY_ITER_UPDATEIFCOPY is enabled, it will also copy the data back with another cast upon iterator destruction.", "Returns NULL if there is an error, otherwise returns the allocated iterator.", "To make an iterator similar to the old iterator, this should work.", "If you want to edit an array with aligned double code, but the order doesn\u2019t matter, you would use this.", "Creates an iterator for broadcasting the nop array objects provided in op, using regular NumPy broadcasting rules.", "Any of the NPY_ORDER enum values may be passed to order. For efficient iteration, NPY_KEEPORDER is the best option, and the other orders enforce the particular iteration pattern. When using NPY_KEEPORDER, if you also want to ensure that the iteration is not reversed along an axis, you should pass the flag NPY_ITER_DONT_NEGATE_STRIDES.", "Any of the NPY_CASTING enum values may be passed to casting. The values include NPY_NO_CASTING, NPY_EQUIV_CASTING, NPY_SAFE_CASTING, NPY_SAME_KIND_CASTING, and NPY_UNSAFE_CASTING. To allow the casts to occur, copying or buffering must also be enabled.", "If op_dtypes isn\u2019t NULL, it specifies a data type or NULL for each op[i].", "Returns NULL if there is an error, otherwise returns the allocated iterator.", "Flags that may be passed in flags, applying to the whole iterator, are:", "Causes the iterator to track a raveled flat index matching C order. This option cannot be used with NPY_ITER_F_INDEX.", "Causes the iterator to track a raveled flat index matching Fortran order. This option cannot be used with NPY_ITER_C_INDEX.", "Causes the iterator to track a multi-index. This prevents the iterator from coalescing axes to produce bigger inner loops. If the loop is also not buffered and no index is being tracked (NpyIter_RemoveAxis can be called), then the iterator size can be -1 to indicate that the iterator is too large. This can happen due to complex broadcasting and will result in errors being created when the setting the iterator range, removing the multi index, or getting the next function. However, it is possible to remove axes again and use the iterator normally if the size is small enough after removal.", "Causes the iterator to skip iteration of the innermost loop, requiring the user of the iterator to handle it.", "This flag is incompatible with NPY_ITER_C_INDEX, NPY_ITER_F_INDEX, and NPY_ITER_MULTI_INDEX.", "This only affects the iterator when NPY_KEEPORDER is specified for the order parameter. By default with NPY_KEEPORDER, the iterator reverses axes which have negative strides, so that memory is traversed in a forward direction. This disables this step. Use this flag if you want to use the underlying memory-ordering of the axes, but don\u2019t want an axis reversed. This is the behavior of numpy.ravel(a, order='K'), for instance.", "Causes the iterator to convert all the operands to a common data type, calculated based on the ufunc type promotion rules. Copying or buffering must be enabled.", "If the common data type is known ahead of time, don\u2019t use this flag. Instead, set the requested dtype for all the operands.", "Indicates that arrays with reference types (object arrays or structured arrays containing an object type) may be accepted and used in the iterator. If this flag is enabled, the caller must be sure to check whether NpyIter_IterationNeedsAPI(iter) is true, in which case it may not release the GIL during iteration.", "Indicates that arrays with a size of zero should be permitted. Since the typical iteration loop does not naturally work with zero-sized arrays, you must check that the IterSize is larger than zero before entering the iteration loop. Currently only the operands are checked, not a forced shape.", "Permits writeable operands with a dimension with zero stride and size greater than one. Note that such operands must be read/write.", "When buffering is enabled, this also switches to a special buffering mode which reduces the loop length as necessary to not trample on values being reduced.", "Note that if you want to do a reduction on an automatically allocated output, you must use NpyIter_GetOperandArray to get its reference, then set every value to the reduction unit before doing the iteration loop. In the case of a buffered reduction, this means you must also specify the flag NPY_ITER_DELAY_BUFALLOC, then reset the iterator after initializing the allocated operand to prepare the buffers.", "Enables support for iteration of sub-ranges of the full iterindex range [0, NpyIter_IterSize(iter)). Use the function NpyIter_ResetToIterIndexRange to specify a range for iteration.", "This flag can only be used with NPY_ITER_EXTERNAL_LOOP when NPY_ITER_BUFFERED is enabled. This is because without buffering, the inner loop is always the size of the innermost iteration dimension, and allowing it to get cut up would require special handling, effectively making it more like the buffered version.", "Causes the iterator to store buffering data, and use buffering to satisfy data type, alignment, and byte-order requirements. To buffer an operand, do not specify the NPY_ITER_COPY or NPY_ITER_UPDATEIFCOPY flags, because they will override buffering. Buffering is especially useful for Python code using the iterator, allowing for larger chunks of data at once to amortize the Python interpreter overhead.", "If used with NPY_ITER_EXTERNAL_LOOP, the inner loop for the caller may get larger chunks than would be possible without buffering, because of how the strides are laid out.", "Note that if an operand is given the flag NPY_ITER_COPY or NPY_ITER_UPDATEIFCOPY, a copy will be made in preference to buffering. Buffering will still occur when the array was broadcast so elements need to be duplicated to get a constant stride.", "In normal buffering, the size of each inner loop is equal to the buffer size, or possibly larger if NPY_ITER_GROWINNER is specified. If NPY_ITER_REDUCE_OK is enabled and a reduction occurs, the inner loops may become smaller depending on the structure of the reduction.", "When buffering is enabled, this allows the size of the inner loop to grow when buffering isn\u2019t necessary. This option is best used if you\u2019re doing a straight pass through all the data, rather than anything with small cache-friendly arrays of temporary values for each inner loop.", "When buffering is enabled, this delays allocation of the buffers until NpyIter_Reset or another reset function is called. This flag exists to avoid wasteful copying of buffer data when making multiple copies of a buffered iterator for multi-threaded iteration.", "Another use of this flag is for setting up reduction operations. After the iterator is created, and a reduction output is allocated automatically by the iterator (be sure to use READWRITE access), its value may be initialized to the reduction unit. Use NpyIter_GetOperandArray to get the object. Then, call NpyIter_Reset to allocate and fill the buffers with their initial values.", "If any write operand has overlap with any read operand, eliminate all overlap by making temporary copies (enabling UPDATEIFCOPY for write operands, if necessary). A pair of operands has overlap if there is a memory address that contains data common to both arrays.", "Because exact overlap detection has exponential runtime in the number of dimensions, the decision is made based on heuristics, which has false positives (needless copies in unusual cases) but has no false negatives.", "If any read/write overlap exists, this flag ensures the result of the operation is the same as if all operands were copied. In cases where copies would need to be made, the result of the computation may be undefined without this flag!", "Flags that may be passed in op_flags[i], where 0 <= i < nop:", "Indicate how the user of the iterator will read or write to op[i]. Exactly one of these flags must be specified per operand. Using NPY_ITER_READWRITE or NPY_ITER_WRITEONLY for a user-provided operand may trigger WRITEBACKIFCOPY` semantics. The data will be written back to the original array when NpyIter_Deallocate is called.", "Allow a copy of op[i] to be made if it does not meet the data type or alignment requirements as specified by the constructor flags and parameters.", "Triggers NPY_ITER_COPY, and when an array operand is flagged for writing and is copied, causes the data in a copy to be copied back to op[i] when NpyIter_Deallocate is called.", "If the operand is flagged as write-only and a copy is needed, an uninitialized temporary array will be created and then copied to back to op[i] on calling NpyIter_Deallocate, instead of doing the unnecessary copy operation.", "Causes the iterator to provide data for op[i] that is in native byte order, aligned according to the dtype requirements, contiguous, or any combination.", "By default, the iterator produces pointers into the arrays provided, which may be aligned or unaligned, and with any byte order. If copying or buffering is not enabled and the operand data doesn\u2019t satisfy the constraints, an error will be raised.", "The contiguous constraint applies only to the inner loop, successive inner loops may have arbitrary pointer changes.", "If the requested data type is in non-native byte order, the NBO flag overrides it and the requested data type is converted to be in native byte order.", "This is for output arrays, and requires that the flag NPY_ITER_WRITEONLY or NPY_ITER_READWRITE be set. If op[i] is NULL, creates a new array with the final broadcast dimensions, and a layout matching the iteration order of the iterator.", "When op[i] is NULL, the requested data type op_dtypes[i] may be NULL as well, in which case it is automatically generated from the dtypes of the arrays which are flagged as readable. The rules for generating the dtype are the same is for UFuncs. Of special note is handling of byte order in the selected dtype. If there is exactly one input, the input\u2019s dtype is used as is. Otherwise, if more than one input dtypes are combined together, the output will be in native byte order.", "After being allocated with this flag, the caller may retrieve the new array by calling NpyIter_GetOperandArray and getting the i-th object in the returned C array. The caller must call Py_INCREF on it to claim a reference to the array.", "For use with NPY_ITER_ALLOCATE, this flag disables allocating an array subtype for the output, forcing it to be a straight ndarray.", "TODO: Maybe it would be better to introduce a function NpyIter_GetWrappedOutput and remove this flag?", "Ensures that the input or output matches the iteration dimensions exactly.", "New in version 1.7.", "Indicates that this operand is the mask to use for selecting elements when writing to operands which have the NPY_ITER_WRITEMASKED flag applied to them. Only one operand may have NPY_ITER_ARRAYMASK flag applied to it.", "The data type of an operand with this flag should be either NPY_BOOL, NPY_MASK, or a struct dtype whose fields are all valid mask dtypes. In the latter case, it must match up with a struct operand being WRITEMASKED, as it is specifying a mask for each field of that array.", "This flag only affects writing from the buffer back to the array. This means that if the operand is also NPY_ITER_READWRITE or NPY_ITER_WRITEONLY, code doing iteration can write to this operand to control which elements will be untouched and which ones will be modified. This is useful when the mask should be a combination of input masks.", "New in version 1.7.", "This array is the mask for all writemasked operands. Code uses the writemasked flag which indicates that only elements where the chosen ARRAYMASK operand is True will be written to. In general, the iterator does not enforce this, it is up to the code doing the iteration to follow that promise.", "When writemasked flag is used, and this operand is buffered, this changes how data is copied from the buffer into the array. A masked copying routine is used, which only copies the elements in the buffer for which writemasked returns true from the corresponding element in the ARRAYMASK operand.", "In memory overlap checks, assume that operands with NPY_ITER_OVERLAP_ASSUME_ELEMENTWISE enabled are accessed only in the iterator order.", "This enables the iterator to reason about data dependency, possibly avoiding unnecessary copies.", "This flag has effect only if NPY_ITER_COPY_IF_OVERLAP is enabled on the iterator.", "Extends NpyIter_MultiNew with several advanced options providing more control over broadcasting and buffering.", "If -1/NULL values are passed to oa_ndim, op_axes, itershape, and buffersize, it is equivalent to NpyIter_MultiNew.", "The parameter oa_ndim, when not zero or -1, specifies the number of dimensions that will be iterated with customized broadcasting. If it is provided, op_axes must and itershape can also be provided. The op_axes parameter let you control in detail how the axes of the operand arrays get matched together and iterated. In op_axes, you must provide an array of nop pointers to oa_ndim-sized arrays of type npy_intp. If an entry in op_axes is NULL, normal broadcasting rules will apply. In op_axes[j][i] is stored either a valid axis of op[j], or -1 which means newaxis. Within each op_axes[j] array, axes may not be repeated. The following example is how normal broadcasting applies to a 3-D array, a 2-D array, a 1-D array and a scalar.", "Note: Before NumPy 1.8 oa_ndim == 0` was used for signalling that\nthat ``op_axes and itershape are unused. This is deprecated and should be replaced with -1. Better backward compatibility may be achieved by using NpyIter_MultiNew for this case.", "The itershape parameter allows you to force the iterator to have a specific iteration shape. It is an array of length oa_ndim. When an entry is negative, its value is determined from the operands. This parameter allows automatically allocated outputs to get additional dimensions which don\u2019t match up with any dimension of an input.", "If buffersize is zero, a default buffer size is used, otherwise it specifies how big of a buffer to use. Buffers which are powers of 2 such as 4096 or 8192 are recommended.", "Returns NULL if there is an error, otherwise returns the allocated iterator.", "Makes a copy of the given iterator. This function is provided primarily to enable multi-threaded iteration of the data.", "TODO: Move this to a section about multithreaded iteration.", "The recommended approach to multithreaded iteration is to first create an iterator with the flags NPY_ITER_EXTERNAL_LOOP, NPY_ITER_RANGED, NPY_ITER_BUFFERED, NPY_ITER_DELAY_BUFALLOC, and possibly NPY_ITER_GROWINNER. Create a copy of this iterator for each thread (minus one for the first iterator). Then, take the iteration index range [0, NpyIter_GetIterSize(iter)) and split it up into tasks, for example using a TBB parallel_for loop. When a thread gets a task to execute, it then uses its copy of the iterator by calling NpyIter_ResetToIterIndexRange and iterating over the full range.", "When using the iterator in multi-threaded code or in code not holding the Python GIL, care must be taken to only call functions which are safe in that context. NpyIter_Copy cannot be safely called without the Python GIL, because it increments Python references. The Reset* and some other functions may be safely called by passing in the errmsg parameter as non-NULL, so that the functions will pass back errors through it instead of setting a Python exception.", "NpyIter_Deallocate must be called for each copy.", "Removes an axis from iteration. This requires that NPY_ITER_MULTI_INDEX was set for iterator creation, and does not work if buffering is enabled or an index is being tracked. This function also resets the iterator to its initial state.", "This is useful for setting up an accumulation loop, for example. The iterator can first be created with all the dimensions, including the accumulation axis, so that the output gets created correctly. Then, the accumulation axis can be removed, and the calculation done in a nested fashion.", "WARNING: This function may change the internal memory layout of the iterator. Any cached functions or pointers from the iterator must be retrieved again! The iterator range will be reset as well.", "Returns NPY_SUCCEED or NPY_FAIL.", "If the iterator is tracking a multi-index, this strips support for them, and does further iterator optimizations that are possible if multi-indices are not needed. This function also resets the iterator to its initial state.", "WARNING: This function may change the internal memory layout of the iterator. Any cached functions or pointers from the iterator must be retrieved again!", "After calling this function, NpyIter_HasMultiIndex(iter) will return false.", "Returns NPY_SUCCEED or NPY_FAIL.", "If NpyIter_RemoveMultiIndex was called, you may want to enable the flag NPY_ITER_EXTERNAL_LOOP. This flag is not permitted together with NPY_ITER_MULTI_INDEX, so this function is provided to enable the feature after NpyIter_RemoveMultiIndex is called. This function also resets the iterator to its initial state.", "WARNING: This function changes the internal logic of the iterator. Any cached functions or pointers from the iterator must be retrieved again!", "Returns NPY_SUCCEED or NPY_FAIL.", "Deallocates the iterator object and resolves any needed writebacks.", "Returns NPY_SUCCEED or NPY_FAIL.", "Resets the iterator back to its initial state, at the beginning of the iteration range.", "Returns NPY_SUCCEED or NPY_FAIL. If errmsg is non-NULL, no Python exception is set when NPY_FAIL is returned. Instead, *errmsg is set to an error message. When errmsg is non-NULL, the function may be safely called without holding the Python GIL.", "Resets the iterator and restricts it to the iterindex range [istart, iend). See NpyIter_Copy for an explanation of how to use this for multi-threaded iteration. This requires that the flag NPY_ITER_RANGED was passed to the iterator constructor.", "If you want to reset both the iterindex range and the base pointers at the same time, you can do the following to avoid extra buffer copying (be sure to add the return code error checks when you copy this code).", "Returns NPY_SUCCEED or NPY_FAIL. If errmsg is non-NULL, no Python exception is set when NPY_FAIL is returned. Instead, *errmsg is set to an error message. When errmsg is non-NULL, the function may be safely called without holding the Python GIL.", "Resets the iterator back to its initial state, but using the values in baseptrs for the data instead of the pointers from the arrays being iterated. This functions is intended to be used, together with the op_axes parameter, by nested iteration code with two or more iterators.", "Returns NPY_SUCCEED or NPY_FAIL. If errmsg is non-NULL, no Python exception is set when NPY_FAIL is returned. Instead, *errmsg is set to an error message. When errmsg is non-NULL, the function may be safely called without holding the Python GIL.", "TODO: Move the following into a special section on nested iterators.", "Creating iterators for nested iteration requires some care. All the iterator operands must match exactly, or the calls to NpyIter_ResetBasePointers will be invalid. This means that automatic copies and output allocation should not be used haphazardly. It is possible to still use the automatic data conversion and casting features of the iterator by creating one of the iterators with all the conversion parameters enabled, then grabbing the allocated operands with the NpyIter_GetOperandArray function and passing them into the constructors for the rest of the iterators.", "WARNING: When creating iterators for nested iteration, the code must not use a dimension more than once in the different iterators. If this is done, nested iteration will produce out-of-bounds pointers during iteration.", "WARNING: When creating iterators for nested iteration, buffering can only be applied to the innermost iterator. If a buffered iterator is used as the source for baseptrs, it will point into a small buffer instead of the array and the inner iteration will be invalid.", "The pattern for using nested iterators is as follows.", "Adjusts the iterator to point to the ndim indices pointed to by multi_index. Returns an error if a multi-index is not being tracked, the indices are out of bounds, or inner loop iteration is disabled.", "Returns NPY_SUCCEED or NPY_FAIL.", "Adjusts the iterator to point to the index specified. If the iterator was constructed with the flag NPY_ITER_C_INDEX, index is the C-order index, and if the iterator was constructed with the flag NPY_ITER_F_INDEX, index is the Fortran-order index. Returns an error if there is no index being tracked, the index is out of bounds, or inner loop iteration is disabled.", "Returns NPY_SUCCEED or NPY_FAIL.", "Returns the number of elements being iterated. This is the product of all the dimensions in the shape. When a multi index is being tracked (and NpyIter_RemoveAxis may be called) the size may be -1 to indicate an iterator is too large. Such an iterator is invalid, but may become valid after NpyIter_RemoveAxis is called. It is not necessary to check for this case.", "Gets the iterindex of the iterator, which is an index matching the iteration order of the iterator.", "Gets the iterindex sub-range that is being iterated. If NPY_ITER_RANGED was not specified, this always returns the range [0, NpyIter_IterSize(iter)).", "Adjusts the iterator to point to the iterindex specified. The IterIndex is an index matching the iteration order of the iterator. Returns an error if the iterindex is out of bounds, buffering is enabled, or inner loop iteration is disabled.", "Returns NPY_SUCCEED or NPY_FAIL.", "Returns 1 if the flag NPY_ITER_DELAY_BUFALLOC was passed to the iterator constructor, and no call to one of the Reset functions has been done yet, 0 otherwise.", "Returns 1 if the caller needs to handle the inner-most 1-dimensional loop, or 0 if the iterator handles all looping. This is controlled by the constructor flag NPY_ITER_EXTERNAL_LOOP or NpyIter_EnableExternalLoop.", "Returns 1 if the iterator was created with the NPY_ITER_MULTI_INDEX flag, 0 otherwise.", "Returns 1 if the iterator was created with the NPY_ITER_C_INDEX or NPY_ITER_F_INDEX flag, 0 otherwise.", "Returns 1 if the iterator requires buffering, which occurs when an operand needs conversion or alignment and so cannot be used directly.", "Returns 1 if the iterator was created with the NPY_ITER_BUFFERED flag, 0 otherwise.", "Returns 1 if the iterator was created with the NPY_ITER_GROWINNER flag, 0 otherwise.", "If the iterator is buffered, returns the size of the buffer being used, otherwise returns 0.", "Returns the number of dimensions being iterated. If a multi-index was not requested in the iterator constructor, this value may be smaller than the number of dimensions in the original objects.", "Returns the number of operands in the iterator.", "Gets the array of strides for the specified axis. Requires that the iterator be tracking a multi-index, and that buffering not be enabled.", "This may be used when you want to match up operand axes in some fashion, then remove them with NpyIter_RemoveAxis to handle their processing manually. By calling this function before removing the axes, you can get the strides for the manual processing.", "Returns NULL on error.", "Returns the broadcast shape of the iterator in outshape. This can only be called on an iterator which is tracking a multi-index.", "Returns NPY_SUCCEED or NPY_FAIL.", "This gives back a pointer to the nop data type Descrs for the objects being iterated. The result points into iter, so the caller does not gain any references to the Descrs.", "This pointer may be cached before the iteration loop, calling iternext will not change it.", "This gives back a pointer to the nop operand PyObjects that are being iterated. The result points into iter, so the caller does not gain any references to the PyObjects.", "This gives back a reference to a new ndarray view, which is a view into the i-th object in the array NpyIter_GetOperandArray, whose dimensions and strides match the internal optimized iteration pattern. A C-order iteration of this view is equivalent to the iterator\u2019s iteration order.", "For example, if an iterator was created with a single array as its input, and it was possible to rearrange all its axes and then collapse it into a single strided iteration, this would return a view that is a one-dimensional array.", "Fills nop flags. Sets outreadflags[i] to 1 if op[i] can be read from, and to 0 if not.", "Fills nop flags. Sets outwriteflags[i] to 1 if op[i] can be written to, and to 0 if not.", "Builds a set of strides which are the same as the strides of an output array created using the NPY_ITER_ALLOCATE flag, where NULL was passed for op_axes. This is for data packed contiguously, but not necessarily in C or Fortran order. This should be used together with NpyIter_GetShape and NpyIter_GetNDim with the flag NPY_ITER_MULTI_INDEX passed into the constructor.", "A use case for this function is to match the shape and layout of the iterator and tack on one or more dimensions. For example, in order to generate a vector per input value for a numerical gradient, you pass in ndim*itemsize for itemsize, then add another dimension to the end with size ndim and stride itemsize. To do the Hessian matrix, you do the same thing but add two dimensions, or take advantage of the symmetry and pack it into 1 dimension with a particular encoding.", "This function may only be called if the iterator is tracking a multi-index and if NPY_ITER_DONT_NEGATE_STRIDES was used to prevent an axis from being iterated in reverse order.", "If an array is created with this method, simply adding \u2018itemsize\u2019 for each iteration will traverse the new array matching the iterator.", "Returns NPY_SUCCEED or NPY_FAIL.", "New in version 1.7.", "Checks to see whether this is the first time the elements of the specified reduction operand which the iterator points at are being seen for the first time. The function returns a reasonable answer for reduction operands and when buffering is disabled. The answer may be incorrect for buffered non-reduction operands.", "This function is intended to be used in EXTERNAL_LOOP mode only, and will produce some wrong answers when that mode is not enabled.", "If this function returns true, the caller should also check the inner loop stride of the operand, because if that stride is 0, then only the first element of the innermost external loop is being visited for the first time.", "WARNING: For performance reasons, \u2018iop\u2019 is not bounds-checked, it is not confirmed that \u2018iop\u2019 is actually a reduction operand, and it is not confirmed that EXTERNAL_LOOP mode is enabled. These checks are the responsibility of the caller, and should be done outside of any inner loops.", "Returns a function pointer for iteration. A specialized version of the function pointer may be calculated by this function instead of being stored in the iterator structure. Thus, to get good performance, it is required that the function pointer be saved in a variable rather than retrieved for each loop iteration.", "Returns NULL if there is an error. If errmsg is non-NULL, no Python exception is set when NPY_FAIL is returned. Instead, *errmsg is set to an error message. When errmsg is non-NULL, the function may be safely called without holding the Python GIL.", "The typical looping construct is as follows.", "When NPY_ITER_EXTERNAL_LOOP is specified, the typical inner loop construct is as follows.", "Observe that we are using the dataptr array inside the iterator, not copying the values to a local temporary. This is possible because when iternext() is called, these pointers will be overwritten with fresh values, not incrementally updated.", "If a compile-time fixed buffer is being used (both flags NPY_ITER_BUFFERED and NPY_ITER_EXTERNAL_LOOP), the inner size may be used as a signal as well. The size is guaranteed to become zero when iternext() returns false, enabling the following loop construct. Note that if you use this construct, you should not pass NPY_ITER_GROWINNER as a flag, because it will cause larger sizes under some circumstances.", "Returns a function pointer for getting the current multi-index of the iterator. Returns NULL if the iterator is not tracking a multi-index. It is recommended that this function pointer be cached in a local variable before the iteration loop.", "Returns NULL if there is an error. If errmsg is non-NULL, no Python exception is set when NPY_FAIL is returned. Instead, *errmsg is set to an error message. When errmsg is non-NULL, the function may be safely called without holding the Python GIL.", "This gives back a pointer to the nop data pointers. If NPY_ITER_EXTERNAL_LOOP was not specified, each data pointer points to the current data item of the iterator. If no inner iteration was specified, it points to the first data item of the inner loop.", "This pointer may be cached before the iteration loop, calling iternext will not change it. This function may be safely called without holding the Python GIL.", "Gets the array of data pointers directly into the arrays (never into the buffers), corresponding to iteration index 0.", "These pointers are different from the pointers accepted by NpyIter_ResetBasePointers, because the direction along some axes may have been reversed.", "This function may be safely called without holding the Python GIL.", "This gives back a pointer to the index being tracked, or NULL if no index is being tracked. It is only usable if one of the flags NPY_ITER_C_INDEX or NPY_ITER_F_INDEX were specified during construction.", "When the flag NPY_ITER_EXTERNAL_LOOP is used, the code needs to know the parameters for doing the inner loop. These functions provide that information.", "Returns a pointer to an array of the nop strides, one for each iterated object, to be used by the inner loop.", "This pointer may be cached before the iteration loop, calling iternext will not change it. This function may be safely called without holding the Python GIL.", "WARNING: While the pointer may be cached, its values may change if the iterator is buffered.", "Returns a pointer to the number of iterations the inner loop should execute.", "This address may be cached before the iteration loop, calling iternext will not change it. The value itself may change during iteration, in particular if buffering is enabled. This function may be safely called without holding the Python GIL.", "Gets an array of strides which are fixed, or will not change during the entire iteration. For strides that may change, the value NPY_MAX_INTP is placed in the stride.", "Once the iterator is prepared for iteration (after a reset if NPY_ITER_DELAY_BUFALLOC was used), call this to get the strides which may be used to select a fast inner loop function. For example, if the stride is 0, that means the inner loop can always load its value into a variable once, then use the variable throughout the loop, or if the stride equals the itemsize, a contiguous version for that operand may be used.", "This function may be safely called without holding the Python GIL.", "The old iterator API includes functions like PyArrayIter_Check, PyArray_Iter* and PyArray_ITER_*. The multi-iterator array includes PyArray_MultiIter*, PyArray_Broadcast, and PyArray_RemoveSmallest. The new iterator design replaces all of this functionality with a single object and associated API. One goal of the new API is that all uses of the existing iterator should be replaceable with the new iterator without significant effort. In 1.6, the major exception to this is the neighborhood iterator, which does not have corresponding features in this iterator.", "Here is a conversion table for which functions to use with the new iterator:", "Iterator Functions", "PyArray_IterNew", "NpyIter_New", "PyArray_IterAllButAxis", "NpyIter_New + axes parameter or Iterator flag NPY_ITER_EXTERNAL_LOOP", "PyArray_BroadcastToShape", "NOT SUPPORTED (Use the support for multiple operands instead.)", "PyArrayIter_Check", "Will need to add this in Python exposure", "PyArray_ITER_RESET", "NpyIter_Reset", "PyArray_ITER_NEXT", "Function pointer from NpyIter_GetIterNext", "PyArray_ITER_DATA", "NpyIter_GetDataPtrArray", "PyArray_ITER_GOTO", "NpyIter_GotoMultiIndex", "PyArray_ITER_GOTO1D", "NpyIter_GotoIndex or NpyIter_GotoIterIndex", "PyArray_ITER_NOTDONE", "Return value of iternext function pointer", "Multi-iterator Functions", "PyArray_MultiIterNew", "NpyIter_MultiNew", "PyArray_MultiIter_RESET", "NpyIter_Reset", "PyArray_MultiIter_NEXT", "Function pointer from NpyIter_GetIterNext", "PyArray_MultiIter_DATA", "NpyIter_GetDataPtrArray", "PyArray_MultiIter_NEXTi", "NOT SUPPORTED (always lock-step iteration)", "PyArray_MultiIter_GOTO", "NpyIter_GotoMultiIndex", "PyArray_MultiIter_GOTO1D", "NpyIter_GotoIndex or NpyIter_GotoIterIndex", "PyArray_MultiIter_NOTDONE", "Return value of iternext function pointer", "PyArray_Broadcast", "Handled by NpyIter_MultiNew", "PyArray_RemoveSmallest", "Iterator flag NPY_ITER_EXTERNAL_LOOP", "Other Functions", "PyArray_ConvertToCommonType", "Iterator flag NPY_ITER_COMMON_DTYPE"]}, {"name": "type NpyIter_GetMultiIndexFunc", "path": "reference/c-api/iterator#c.NpyIter_GetMultiIndexFunc", "type": "Array Iterator API", "text": ["This is a function pointer for getting the current iterator multi-index, returned by NpyIter_GetGetMultiIndex."]}, {"name": "type NpyIter_IterNextFunc", "path": "reference/c-api/iterator#c.NpyIter_IterNextFunc", "type": "Array Iterator API", "text": ["This is a function pointer for the iteration loop, returned by NpyIter_GetIterNext."]}, {"name": "type NpyIter_Type", "path": "reference/c-api/iterator#c.NpyIter_Type", "type": "Array Iterator API", "text": ["This is the type which exposes the iterator to Python. Currently, no API is exposed which provides access to the values of a Python-created iterator. If an iterator is created in Python, it must be used in Python and vice versa. Such an API will likely be created in a future version."]}, {"name": "type PyArray_ArrFuncs", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs", "type": "Python Types and C-Structures", "text": ["Functions implementing internal features. Not all of these function pointers must be defined for a given type. The required members are nonzero, copyswap, copyswapn, setitem, getitem, and cast. These are assumed to be non- NULL and NULL entries will cause a program crash. The other functions may be NULL which will just mean reduced functionality for that data-type. (Also, the nonzero function will be filled in with a default function if it is NULL when you register a user-defined data-type).", "The concept of a behaved segment is used in the description of the function pointers. A behaved segment is one that is aligned and in native machine byte-order for the data-type. The nonzero, copyswap, copyswapn, getitem, and setitem functions can (and must) deal with mis-behaved arrays. The other functions require behaved memory segments.", "An array of function pointers to cast from the current type to all of the other builtin types. Each function casts a contiguous, aligned, and notswapped buffer pointed at by from to a contiguous, aligned, and notswapped buffer pointed at by to The number of items to cast is given by n, and the arguments fromarr and toarr are interpreted as PyArrayObjects for flexible arrays to get itemsize information.", "A pointer to a function that returns a standard Python object from a single element of the array object arr pointed to by data. This function must be able to deal with \u201cmisbehaved \u201c(misaligned and/or swapped) arrays correctly.", "A pointer to a function that sets the Python object item into the array, arr, at the position pointed to by data . This function deals with \u201cmisbehaved\u201d arrays. If successful, a zero is returned, otherwise, a negative one is returned (and a Python error set).", "These members are both pointers to functions to copy data from src to dest and swap if indicated. The value of arr is only used for flexible ( NPY_STRING, NPY_UNICODE, and NPY_VOID ) arrays (and is obtained from arr->descr->elsize ). The second function copies a single value, while the first loops over n values with the provided strides. These functions can deal with misbehaved src data. If src is NULL then no copy is performed. If swap is 0, then no byteswapping occurs. It is assumed that dest and src do not overlap. If they overlap, then use memmove (\u2026) first followed by copyswap(n) with NULL valued src.", "A pointer to a function that compares two elements of the array, arr, pointed to by d1 and d2. This function requires behaved (aligned and not swapped) arrays. The return value is 1 if * d1 > * d2, 0 if * d1 == * d2, and -1 if * d1 < * d2. The array object arr is used to retrieve itemsize and field information for flexible arrays.", "A pointer to a function that retrieves the index of the largest of n elements in arr beginning at the element pointed to by data. This function requires that the memory segment be contiguous and behaved. The return value is always 0. The index of the largest element is returned in max_ind.", "A pointer to a function that multiplies two n -length sequences together, adds them, and places the result in element pointed to by op of arr. The start of the two sequences are pointed to by ip1 and ip2. To get to the next element in each sequence requires a jump of is1 and is2 bytes, respectively. This function requires behaved (though not necessarily contiguous) memory.", "A pointer to a function that scans (scanf style) one element of the corresponding type from the file descriptor fd into the array memory pointed to by ip. The array is assumed to be behaved. The last argument arr is the array to be scanned into. Returns number of receiving arguments successfully assigned (which may be zero in case a matching failure occurred before the first receiving argument was assigned), or EOF if input failure occurs before the first receiving argument was assigned. This function should be called without holding the Python GIL, and has to grab it for error reporting.", "A pointer to a function that converts the string pointed to by str to one element of the corresponding type and places it in the memory location pointed to by ip. After the conversion is completed, *endptr points to the rest of the string. The last argument arr is the array into which ip points (needed for variable-size data- types). Returns 0 on success or -1 on failure. Requires a behaved array. This function should be called without holding the Python GIL, and has to grab it for error reporting.", "A pointer to a function that returns TRUE if the item of arr pointed to by data is nonzero. This function can deal with misbehaved arrays.", "A pointer to a function that fills a contiguous array of given length with data. The first two elements of the array must already be filled- in. From these two values, a delta will be computed and the values from item 3 to the end will be computed by repeatedly adding this computed delta. The data buffer must be well-behaved.", "A pointer to a function that fills a contiguous buffer of the given length with a single scalar value whose address is given. The final argument is the array which is needed to get the itemsize for variable-length arrays.", "An array of function pointers to a particular sorting algorithms. A particular sorting algorithm is obtained using a key (so far NPY_QUICKSORT, NPY_HEAPSORT, and NPY_MERGESORT are defined). These sorts are done in-place assuming contiguous and aligned data.", "An array of function pointers to sorting algorithms for this data type. The same sorting algorithms as for sort are available. The indices producing the sort are returned in result (which must be initialized with indices 0 to length-1 inclusive).", "Either NULL or a dictionary containing low-level casting functions for user- defined data-types. Each function is wrapped in a PyCapsule* and keyed by the data-type number.", "A function to determine how scalars of this type should be interpreted. The argument is NULL or a 0-dimensional array containing the data (if that is needed to determine the kind of scalar). The return value must be of type NPY_SCALARKIND.", "Either NULL or an array of NPY_NSCALARKINDS pointers. These pointers should each be either NULL or a pointer to an array of integers (terminated by NPY_NOTYPE) indicating data-types that a scalar of this data-type of the specified kind can be cast to safely (this usually means without losing precision).", "Either NULL or an array of integers (terminated by NPY_NOTYPE ) indicated data-types that this data-type can be cast to safely (this usually means without losing precision).", "Deprecated since version 1.17: The use of this function will give a deprecation warning when np.clip. Instead of this function, the datatype must instead use PyUFunc_RegisterLoopForDescr to attach a custom loop to np.core.umath.clip, np.minimum, and np.maximum.", "Deprecated since version 1.19: Setting this function is deprecated and should always be NULL, if set, it will be ignored.", "A function that reads n_in items from in, and writes to out the read value if it is within the limits pointed to by min and max, or the corresponding limit if outside. The memory segments must be contiguous and behaved, and either min or max may be NULL, but not both.", "Deprecated since version 1.19: Setting this function is deprecated and should always be NULL, if set, it will be ignored.", "A function that takes a pointer in to an array of n_in items, a pointer mask to an array of n_in boolean values, and a pointer vals to an array of nv items. Items from vals are copied into in wherever the value in mask is non-zero, tiling vals as needed if nv < n_in. All arrays must be contiguous and behaved.", "Deprecated since version 1.19: Setting this function is deprecated and should always be NULL, if set, it will be ignored.", "A function that takes a pointer src to a C contiguous, behaved segment, interpreted as a 3-dimensional array of shape (n_outer, nindarray, nelem), a pointer indarray to a contiguous, behaved segment of m_middle integer indices, and a pointer dest to a C contiguous, behaved segment, interpreted as a 3-dimensional array of shape (n_outer, m_middle, nelem). The indices in indarray are used to index src along the second dimension, and copy the corresponding chunks of nelem items into dest. clipmode (which can take on the values NPY_RAISE, NPY_WRAP or NPY_CLIP) determines how will indices smaller than 0 or larger than nindarray will be handled.", "A pointer to a function that retrieves the index of the smallest of n elements in arr beginning at the element pointed to by data. This function requires that the memory segment be contiguous and behaved. The return value is always 0. The index of the smallest element is returned in min_ind."]}, {"name": "type PyArray_Descr", "path": "reference/c-api/types-and-structures#c.PyArray_Descr", "type": "Python Types and C-Structures", "text": ["The PyArray_Descr structure lies at the heart of the PyArrayDescr_Type. While it is described here for completeness, it should be considered internal to NumPy and manipulated via PyArrayDescr_* or PyDataType* functions and macros. The size of this structure is subject to change across versions of NumPy. To ensure compatibility:", "It has the following structure:", "Pointer to a typeobject that is the corresponding Python type for the elements of this array. For the builtin types, this points to the corresponding array scalar. For user-defined types, this should point to a user-defined typeobject. This typeobject can either inherit from array scalars or not. If it does not inherit from array scalars, then the NPY_USE_GETITEM and NPY_USE_SETITEM flags should be set in the flags member.", "A character code indicating the kind of array (using the array interface typestring notation). A \u2018b\u2019 represents Boolean, a \u2018i\u2019 represents signed integer, a \u2018u\u2019 represents unsigned integer, \u2018f\u2019 represents floating point, \u2018c\u2019 represents complex floating point, \u2018S\u2019 represents 8-bit zero-terminated bytes, \u2018U\u2019 represents 32-bit/character unicode string, and \u2018V\u2019 represents arbitrary.", "A traditional character code indicating the data type.", "A character indicating the byte-order: \u2018>\u2019 (big-endian), \u2018<\u2019 (little- endian), \u2018=\u2019 (native), \u2018|\u2019 (irrelevant, ignore). All builtin data- types have byteorder \u2018=\u2019.", "A data-type bit-flag that determines if the data-type exhibits object- array like behavior. Each bit in this member is a flag which are named as:"]}, {"name": "type PyArrayFlagsObject", "path": "reference/c-api/types-and-structures#c.PyArrayFlagsObject", "type": "Python Types and C-Structures", "text": []}, {"name": "type PyArrayIterObject", "path": "reference/c-api/types-and-structures#c.PyArrayIterObject", "type": "Python Types and C-Structures", "text": ["The C-structure corresponding to an object of PyArrayIter_Type is the PyArrayIterObject. The PyArrayIterObject is used to keep track of a pointer into an N-dimensional array. It contains associated information used to quickly march through the array. The pointer can be adjusted in three basic ways: 1) advance to the \u201cnext\u201d position in the array in a C-style contiguous fashion, 2) advance to an arbitrary N-dimensional coordinate in the array, and 3) advance to an arbitrary one-dimensional index into the array. The members of the PyArrayIterObject structure are used in these calculations. Iterator objects keep their own dimension and strides information about an array. This can be adjusted as needed for \u201cbroadcasting,\u201d or to loop over only specific dimensions.", "\\(N-1\\) where \\(N\\) is the number of dimensions in the underlying array.", "The current 1-d index into the array.", "The total size of the underlying array.", "An \\(N\\) -dimensional index into the array.", "The size of the array minus 1 in each dimension.", "The strides of the array. How many bytes needed to jump to the next element in each dimension.", "How many bytes needed to jump from the end of a dimension back to its beginning. Note that backstrides[k] == strides[k] *\ndims_m1[k], but it is stored here as an optimization.", "This array is used in computing an N-d index from a 1-d index. It contains needed products of the dimensions.", "A pointer to the underlying ndarray this iterator was created to represent.", "This member points to an element in the ndarray indicated by the index.", "This flag is true if the underlying array is NPY_ARRAY_C_CONTIGUOUS. It is used to simplify calculations when possible."]}, {"name": "type PyArrayMultiIterObject", "path": "reference/c-api/types-and-structures#c.PyArrayMultiIterObject", "type": "Python Types and C-Structures", "text": ["The number of arrays that need to be broadcast to the same shape.", "The total broadcasted size.", "The current (1-d) index into the broadcasted result.", "The number of dimensions in the broadcasted result.", "The shape of the broadcasted result (only nd slots are used).", "An array of iterator objects that holds the iterators for the arrays to be broadcast together. On return, the iterators are adjusted for broadcasting."]}, {"name": "type PyArrayNeighborhoodIterObject", "path": "reference/c-api/types-and-structures#c.PyArrayNeighborhoodIterObject", "type": "Python Types and C-Structures", "text": ["The C-structure corresponding to an object of PyArrayNeighborhoodIter_Type is the PyArrayNeighborhoodIterObject."]}, {"name": "type PyArrayObject", "path": "reference/c-api/types-and-structures#c.PyArrayObject", "type": "Python Types and C-Structures", "text": ["The PyArrayObject C-structure contains all of the required information for an array. All instances of an ndarray (and its subclasses) will have this structure. For future compatibility, these structure members should normally be accessed using the provided macros. If you need a shorter name, then you can make use of NPY_AO (deprecated) which is defined to be equivalent to PyArrayObject. Direct access to the struct fields are deprecated. Use the PyArray_*(arr) form instead. As of NumPy 1.20, the size of this struct is not considered part of the NumPy ABI (see note at the end of the member list).", "This is needed by all Python objects. It consists of (at least) a reference count member ( ob_refcnt ) and a pointer to the typeobject ( ob_type ). (Other elements may also be present if Python was compiled with special options see Include/object.h in the Python source tree for more information). The ob_type member points to a Python type object.", "Accessible via PyArray_DATA, this data member is a pointer to the first element of the array. This pointer can (and normally should) be recast to the data type of the array.", "An integer providing the number of dimensions for this array. When nd is 0, the array is sometimes called a rank-0 array. Such arrays have undefined dimensions and strides and cannot be accessed. Macro PyArray_NDIM defined in ndarraytypes.h points to this data member. NPY_MAXDIMS is the largest number of dimensions for any array.", "An array of integers providing the shape in each dimension as long as nd \\(\\geq\\) 1. The integer is always large enough to hold a pointer on the platform, so the dimension size is only limited by memory. PyArray_DIMS is the macro associated with this data member.", "An array of integers providing for each dimension the number of bytes that must be skipped to get to the next element in that dimension. Associated with macro PyArray_STRIDES.", "Pointed to by PyArray_BASE, this member is used to hold a pointer to another Python object that is related to this array. There are two use cases:", "When PyArray_ResolveWritebackIfCopy is called, the array pointed to by base will be updated with the contents of this array.", "A pointer to a data-type descriptor object (see below). The data-type descriptor object is an instance of a new built-in type which allows a generic description of memory. There is a descriptor structure for each data type supported. This descriptor structure contains useful information about the type as well as a pointer to a table of function pointers to implement specific functionality. As the name suggests, it is associated with the macro PyArray_DESCR.", "Pointed to by the macro PyArray_FLAGS, this data member represents the flags indicating how the memory pointed to by data is to be interpreted. Possible flags are NPY_ARRAY_C_CONTIGUOUS, NPY_ARRAY_F_CONTIGUOUS, NPY_ARRAY_OWNDATA, NPY_ARRAY_ALIGNED, NPY_ARRAY_WRITEABLE, NPY_ARRAY_WRITEBACKIFCOPY, and NPY_ARRAY_UPDATEIFCOPY.", "This member allows array objects to have weak references (using the weakref module).", "Note", "Further members are considered private and version dependent. If the size of the struct is important for your code, special care must be taken. A possible use-case when this is relevant is subclassing in C. If your code relies on sizeof(PyArrayObject) to be constant, you must add the following check at import time:", "To ensure that your code does not have to be compiled for a specific NumPy version, you may add a constant, leaving room for changes in NumPy. A solution guaranteed to be compatible with any future NumPy version requires the use of a runtime calculate offset and allocation size."]}, {"name": "type PyDataMem_Handler", "path": "reference/c-api/data_memory", "type": "Memory management in NumPy", "text": ["The numpy.ndarray is a python class. It requires additional memory allocations to hold numpy.ndarray.strides, numpy.ndarray.shape and numpy.ndarray.data attributes. These attributes are specially allocated after creating the python object in __new__. The strides and shape are stored in a piece of memory allocated internally.", "The data allocation used to store the actual array values (which could be pointers in the case of object arrays) can be very large, so NumPy has provided interfaces to manage its allocation and release. This document details how those interfaces work.", "Since version 1.7.0, NumPy has exposed a set of PyDataMem_* functions (PyDataMem_NEW, PyDataMem_FREE, PyDataMem_RENEW) which are backed by alloc, free, realloc respectively. In that version NumPy also exposed the PyDataMem_EventHook function described below, which wrap the OS-level calls.", "Since those early days, Python also improved its memory management capabilities, and began providing various management policies beginning in version 3.4. These routines are divided into a set of domains, each domain has a PyMemAllocatorEx structure of routines for memory management. Python also added a tracemalloc module to trace calls to the various routines. These tracking hooks were added to the NumPy PyDataMem_* routines.", "NumPy added a small cache of allocated memory in its internal npy_alloc_cache, npy_alloc_cache_zero, and npy_free_cache functions. These wrap alloc, alloc-and-memset(0) and free respectively, but when npy_free_cache is called, it adds the pointer to a short list of available blocks marked by size. These blocks can be re-used by subsequent calls to npy_alloc*, avoiding memory thrashing.", "Users may wish to override the internal data memory routines with ones of their own. Since NumPy does not use the Python domain strategy to manage data memory, it provides an alternative set of C-APIs to change memory routines. There are no Python domain-wide strategies for large chunks of object data, so those are less suited to NumPy\u2019s needs. User who wish to change the NumPy data memory management routines can use PyDataMem_SetHandler, which uses a PyDataMem_Handler structure to hold pointers to functions used to manage the data memory. The calls are still wrapped by internal routines to call PyTraceMalloc_Track, PyTraceMalloc_Untrack, and will use the PyDataMem_EventHookFunc mechanism. Since the functions may change during the lifetime of the process, each ndarray carries with it the functions used at the time of its instantiation, and these will be used to reallocate or free the data memory of the instance.", "A struct to hold function pointers used to manipulate memory", "where the allocator structure is", "Set a new allocation policy. If the input value is NULL, will reset the policy to the default. Return the previous policy, or return NULL if an error has occurred. We wrap the user-provided functions so they will still call the python and numpy memory management callback hooks.", "Return the current policy that will be used to allocate data for the next PyArrayObject. On failure, return NULL.", "For an example of setting up and using the PyDataMem_Handler, see the test in numpy/core/tests/test_mem_policy.py", "This function will be called during data memory manipulation", "Sets the allocation event hook for numpy array data.", "Returns a pointer to the previous hook or NULL. If old_data is non-NULL, the previous user_data pointer will be copied to it.", "If not NULL, hook will be called at the end of each PyDataMem_NEW/FREE/RENEW:", "When the hook is called, the GIL will be held by the calling thread. The hook should be written to be reentrant, if it performs operations that might cause new allocation events (such as the creation/destruction numpy objects, or creating/destroying Python objects which might cause a gc)", "A rare but useful technique is to allocate a buffer outside NumPy, use PyArray_NewFromDescr to wrap the buffer in a ndarray, then switch the OWNDATA flag to true. When the ndarray is released, the appropriate function from the ndarray\u2019s PyDataMem_Handler should be called to free the buffer. But the PyDataMem_Handler field was never set, it will be NULL. For backward compatibility, NumPy will call free() to release the buffer. If NUMPY_WARN_IF_NO_MEM_POLICY is set to 1, a warning will be emitted. The current default is not to emit a warning, this may change in a future version of NumPy.", "A better technique would be to use a PyCapsule as a base object:"]}, {"name": "type PyUFunc_Loop1d", "path": "reference/c-api/types-and-structures#c.PyUFunc_Loop1d", "type": "Python Types and C-Structures", "text": ["A simple linked-list of C-structures containing the information needed to define a 1-d loop for a ufunc for every defined signature of a user-defined data-type."]}, {"name": "type PyUFuncObject", "path": "reference/c-api/types-and-structures#c.PyUFuncObject", "type": "Python Types and C-Structures", "text": ["The core of the ufunc is the PyUFuncObject which contains all the information needed to call the underlying C-code loops that perform the actual work. While it is described here for completeness, it should be considered internal to NumPy and manipulated via PyUFunc_* functions. The size of this structure is subject to change across versions of NumPy. To ensure compatibility:", "It has the following structure:", "The number of input arguments.", "The number of output arguments.", "The total number of arguments (nin + nout). This must be less than NPY_MAXARGS.", "Either PyUFunc_One, PyUFunc_Zero, PyUFunc_MinusOne, PyUFunc_None, PyUFunc_ReorderableNone, or PyUFunc_IdentityValue to indicate the identity for this operation. It is only used for a reduce-like call on an empty array.", "An array of function pointers \u2014 one for each data type supported by the ufunc. This is the vector loop that is called to implement the underlying function dims [0] times. The first argument, args, is an array of nargs pointers to behaved memory. Pointers to the data for the input arguments are first, followed by the pointers to the data for the output arguments. How many bytes must be skipped to get to the next element in the sequence is specified by the corresponding entry in the steps array. The last argument allows the loop to receive extra information. This is commonly used so that a single, generic vector loop can be used for multiple functions. In this case, the actual scalar function to call is passed in as extradata. The size of this function pointer array is ntypes.", "Extra data to be passed to the 1-d vector loops or NULL if no extra-data is needed. This C-array must be the same size ( i.e. ntypes) as the functions array. NULL is used if extra_data is not needed. Several C-API calls for UFuncs are just 1-d vector loops that make use of this extra data to receive a pointer to the actual function to call.", "The number of supported data types for the ufunc. This number specifies how many different 1-d loops (of the builtin data types) are available.", "Unused.", "A string name for the ufunc. This is used dynamically to build the __doc__ attribute of ufuncs.", "An array of \\(nargs \\times ntypes\\) 8-bit type_numbers which contains the type signature for the function for each of the supported (builtin) data types. For each of the ntypes functions, the corresponding set of type numbers in this array shows how the args argument should be interpreted in the 1-d vector loop. These type numbers do not have to be the same type and mixed-type ufuncs are supported.", "Documentation for the ufunc. Should not contain the function signature as this is generated dynamically when __doc__ is retrieved.", "Any dynamically allocated memory. Currently, this is used for dynamic ufuncs created from a python function to store room for the types, data, and name members.", "For ufuncs dynamically created from python functions, this member holds a reference to the underlying Python function.", "A dictionary of user-defined 1-d vector loops (stored as CObject ptrs) for user-defined types. A loop may be registered by the user for any user-defined type. It is retrieved by type number. User defined type numbers are always larger than NPY_USERDEF.", "0 for scalar ufuncs; 1 for generalized ufuncs", "Number of distinct core dimension names in the signature", "Number of core dimensions of each argument", "Dimension indices in a flattened form; indices of argument k are stored in core_dim_ixs[core_offsets[k] : core_offsets[k] +\ncore_numdims[k]]", "Position of 1st core dimension of each argument in core_dim_ixs, equivalent to cumsum(core_num_dims)", "Core signature string", "A function which resolves the types and fills an array with the dtypes for the inputs and outputs", "Deprecated since version 1.22: Some fallback support for this slot exists, but will be removed eventually. A universal function that relied on this will have to be ported eventually. See ref:NEP 41 and ref:NEP 43", "For a possible future loop selector with a different signature.", "Override the default operand flags for each ufunc operand.", "Override the default nditer flags for the ufunc.", "Added in API version 0x0000000D", "For each distinct core dimension, the possible frozen size if UFUNC_CORE_DIM_SIZE_INFERRED is 0", "For each distinct core dimension, a set of UFUNC_CORE_DIM* flags"]}, {"name": "type PyUFuncReduceObject", "path": "reference/c-api/types-and-structures#c.PyUFuncReduceObject", "type": "Python Types and C-Structures", "text": ["A loose wrapper for the C-structure that contains the information needed for reduce-like methods of ufuncs. This is useful if you are trying to understand the reduce, accumulate, and reduce-at code. The PyUFuncReduceObject is the associated C-structure. It is defined in the ufuncobject.h header."]}, {"name": "ufunc.__call__()", "path": "reference/generated/numpy.ufunc.__call__", "type": "numpy.ufunc.__call__", "text": ["method", "Call self as a function."]}, {"name": "ufunc.accumulate()", "path": "reference/generated/numpy.ufunc.accumulate", "type": "numpy.ufunc.accumulate", "text": ["method", "Accumulate the result of applying the operator to all elements.", "For a one-dimensional array, accumulate produces results equivalent to:", "For example, add.accumulate() is equivalent to np.cumsum().", "For a multi-dimensional array, accumulate is applied along only one axis (axis zero by default; see Examples below) so repeated use is necessary if one wants to accumulate over multiple axes.", "The array to act on.", "The axis along which to apply the accumulation; default is zero.", "The data-type used to represent the intermediate results. Defaults to the data-type of the output array if such is provided, or the the data-type of the input array if no output array is provided.", "A location into which the result is stored. If not provided or None, a freshly-allocated array is returned. For consistency with ufunc.__call__, if given as a keyword, this may be wrapped in a 1-element tuple.", "Changed in version 1.13.0: Tuples are allowed for keyword argument.", "The accumulated values. If out was supplied, r is a reference to out.", "1-D array examples:", "2-D array examples:", "Accumulate along axis 0 (rows), down columns:", "Accumulate along axis 1 (columns), through rows:"]}, {"name": "ufunc.at()", "path": "reference/generated/numpy.ufunc.at", "type": "numpy.ufunc.at", "text": ["method", "Performs unbuffered in place operation on operand \u2018a\u2019 for elements specified by \u2018indices\u2019. For addition ufunc, this method is equivalent to a[indices] += b, except that results are accumulated for elements that are indexed more than once. For example, a[[0,0]] += 1 will only increment the first element once because of buffering, whereas add.at(a, [0,0], 1) will increment the first element twice.", "New in version 1.8.0.", "The array to perform in place operation on.", "Array like index object or slice object for indexing into first operand. If first operand has multiple dimensions, indices can be a tuple of array like index objects or slice objects.", "Second operand for ufuncs requiring two operands. Operand must be broadcastable over first operand after indexing or slicing.", "Set items 0 and 1 to their negative values:", "Increment items 0 and 1, and increment item 2 twice:", "Add items 0 and 1 in first array to second array, and store results in first array:"]}, {"name": "ufunc.identity", "path": "reference/generated/numpy.ufunc.identity", "type": "numpy.ufunc.identity", "text": ["attribute", "The identity value.", "Data attribute containing the identity element for the ufunc, if it has one. If it does not, the attribute value is None."]}, {"name": "ufunc.nargs", "path": "reference/generated/numpy.ufunc.nargs", "type": "numpy.ufunc.nargs", "text": ["attribute", "The number of arguments.", "Data attribute containing the number of arguments the ufunc takes, including optional ones.", "Typically this value will be one more than what you might expect because all ufuncs take the optional \u201cout\u201d argument."]}, {"name": "ufunc.nin", "path": "reference/generated/numpy.ufunc.nin", "type": "numpy.ufunc.nin", "text": ["attribute", "The number of inputs.", "Data attribute containing the number of arguments the ufunc treats as input."]}, {"name": "ufunc.nout", "path": "reference/generated/numpy.ufunc.nout", "type": "numpy.ufunc.nout", "text": ["attribute", "The number of outputs.", "Data attribute containing the number of arguments the ufunc treats as output.", "Since all ufuncs can take output arguments, this will always be (at least) 1."]}, {"name": "ufunc.ntypes", "path": "reference/generated/numpy.ufunc.ntypes", "type": "numpy.ufunc.ntypes", "text": ["attribute", "The number of types.", "The number of numerical NumPy types - of which there are 18 total - on which the ufunc can operate.", "See also"]}, {"name": "ufunc.outer()", "path": "reference/generated/numpy.ufunc.outer", "type": "numpy.ufunc.outer", "text": ["method", "Apply the ufunc op to all pairs (a, b) with a in A and b in B.", "Let M = A.ndim, N = B.ndim. Then the result, C, of op.outer(A, B) is an array of dimension M + N such that:", "For A and B one-dimensional, this is equivalent to:", "First array", "Second array", "Arguments to pass on to the ufunc. Typically dtype or out. See ufunc for a comprehensive overview of all available arguments.", "Output array", "See also", "A less powerful version of np.multiply.outer that ravels all inputs to 1D. This exists primarily for compatibility with old code.", "np.tensordot(a, b, axes=((), ())) and np.multiply.outer(a, b) behave same for all dimensions of a and b.", "A multi-dimensional example:"]}, {"name": "ufunc.reduce()", "path": "reference/generated/numpy.ufunc.reduce", "type": "numpy.ufunc.reduce", "text": ["method", "Reduces array\u2019s dimension by one, by applying ufunc along one axis.", "Let \\(array.shape = (N_0, ..., N_i, ..., N_{M-1})\\). Then \\(ufunc.reduce(array, axis=i)[k_0, ..,k_{i-1}, k_{i+1}, .., k_{M-1}]\\) = the result of iterating j over \\(range(N_i)\\), cumulatively applying ufunc to each \\(array[k_0, ..,k_{i-1}, j, k_{i+1}, .., k_{M-1}]\\). For a one-dimensional array, reduce produces results equivalent to:", "For example, add.reduce() is equivalent to sum().", "The array to act on.", "Axis or axes along which a reduction is performed. The default (axis = 0) is perform a reduction over the first dimension of the input array. axis may be negative, in which case it counts from the last to the first axis.", "New in version 1.7.0.", "If this is None, a reduction is performed over all the axes. If this is a tuple of ints, a reduction is performed on multiple axes, instead of a single axis or all the axes as before.", "For operations which are either not commutative or not associative, doing a reduction over multiple axes is not well-defined. The ufuncs do not currently raise an exception in this case, but will likely do so in the future.", "The type used to represent the intermediate results. Defaults to the data-type of the output array if this is provided, or the data-type of the input array if no output array is provided.", "A location into which the result is stored. If not provided or None, a freshly-allocated array is returned. For consistency with ufunc.__call__, if given as a keyword, this may be wrapped in a 1-element tuple.", "Changed in version 1.13.0: Tuples are allowed for keyword argument.", "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original array.", "New in version 1.7.0.", "The value with which to start the reduction. If the ufunc has no identity or the dtype is object, this defaults to None - otherwise it defaults to ufunc.identity. If None is given, the first element of the reduction is used, and an error is thrown if the reduction is empty.", "New in version 1.15.0.", "A boolean array which is broadcasted to match the dimensions of array, and selects elements to include in the reduction. Note that for ufuncs like minimum that do not have an identity defined, one has to pass in also initial.", "New in version 1.17.0.", "The reduced array. If out was supplied, r is a reference to it.", "A multi-dimensional array example:", "You can use the initial keyword argument to initialize the reduction with a different value, and where to select specific elements to include:", "Allows reductions of empty arrays where they would normally fail, i.e. for ufuncs without an identity."]}, {"name": "ufunc.reduceat()", "path": "reference/generated/numpy.ufunc.reduceat", "type": "numpy.ufunc.reduceat", "text": ["method", "Performs a (local) reduce with specified slices over a single axis.", "For i in range(len(indices)), reduceat computes ufunc.reduce(array[indices[i]:indices[i+1]]), which becomes the i-th generalized \u201crow\u201d parallel to axis in the final result (i.e., in a 2-D array, for example, if axis = 0, it becomes the i-th row, but if axis = 1, it becomes the i-th column). There are three exceptions to this:", "The shape of the output depends on the size of indices, and may be larger than array (this happens if len(indices) > array.shape[axis]).", "The array to act on.", "Paired indices, comma separated (not colon), specifying slices to reduce.", "The axis along which to apply the reduceat.", "The type used to represent the intermediate results. Defaults to the data type of the output array if this is provided, or the data type of the input array if no output array is provided.", "A location into which the result is stored. If not provided or None, a freshly-allocated array is returned. For consistency with ufunc.__call__, if given as a keyword, this may be wrapped in a 1-element tuple.", "Changed in version 1.13.0: Tuples are allowed for keyword argument.", "The reduced values. If out was supplied, r is a reference to out.", "A descriptive example:", "If array is 1-D, the function ufunc.accumulate(array) is the same as ufunc.reduceat(array, indices)[::2] where indices is range(len(array) - 1) with a zero placed in every other element: indices = zeros(2 * len(array) - 1), indices[1::2] = range(1, len(array)).", "Don\u2019t be fooled by this attribute\u2019s name: reduceat(array) is not necessarily smaller than array.", "To take the running sum of four successive values:", "A 2-D example:"]}, {"name": "ufunc.signature", "path": "reference/generated/numpy.ufunc.signature", "type": "numpy.ufunc.signature", "text": ["attribute", "Definition of the core elements a generalized ufunc operates on.", "The signature determines how the dimensions of each input/output array are split into core and loop dimensions:", "Generalized ufuncs are used internally in many linalg functions, and in the testing suite; the examples below are taken from these. For ufuncs that operate on scalars, the signature is None, which is equivalent to \u2018()\u2019 for every argument."]}, {"name": "ufunc.types", "path": "reference/generated/numpy.ufunc.types", "type": "numpy.ufunc.types", "text": ["attribute", "Returns a list with types grouped input->output.", "Data attribute listing the data-type \u201cDomain-Range\u201d groupings the ufunc can deliver. The data-types are given using the character codes.", "See also"]}, {"name": "UFUNC_CORE_DIM_CAN_IGNORE", "path": "reference/c-api/types-and-structures#c.UFUNC_CORE_DIM_CAN_IGNORE", "type": "Python Types and C-Structures", "text": ["if the dim name ends in ?"]}, {"name": "UFUNC_CORE_DIM_SIZE_INFERRED", "path": "reference/c-api/types-and-structures#c.UFUNC_CORE_DIM_SIZE_INFERRED", "type": "Python Types and C-Structures", "text": ["if the dim size will be determined from the operands and not from a frozen signature", "Identity for reduction, when PyUFuncObject.identity is equal to PyUFunc_IdentityValue."]}, {"name": "UFUNC_ERR_CALL", "path": "reference/c-api/ufunc#c.UFUNC_ERR_CALL", "type": "UFunc API", "text": []}, {"name": "UFUNC_ERR_RAISE", "path": "reference/c-api/ufunc#c.UFUNC_ERR_RAISE", "type": "UFunc API", "text": []}, {"name": "UFUNC_ERR_WARN", "path": "reference/c-api/ufunc#c.UFUNC_ERR_WARN", "type": "UFunc API", "text": []}, {"name": "UFUNC_ERR_{HANDLER}", "path": "reference/c-api/ufunc", "type": "UFunc API", "text": ["Used in universal function code to only release the Python GIL if loop->obj is not true (i.e. this is not an OBJECT array loop). Requires use of NPY_BEGIN_THREADS_DEF in variable declaration area.", "Used in universal function code to re-acquire the Python GIL if it was released (because loop->obj was not true).", "pointers to functions that actually implement the underlying (element-by-element) function \\(N\\) times with the following signature:", "args", "An array of pointers to the actual data for the input and output arrays. The input arguments are given first followed by the output arguments.", "dimensions", "A pointer to the size of the dimension over which this function is looping.", "steps", "A pointer to the number of bytes to jump to get to the next element in this dimension for each of the input and output arguments.", "data", "Arbitrary data (extra arguments, function names, etc. ) that can be stored with the ufunc and will be passed in when it is called.", "This is an example of a func specialized for addition of doubles returning doubles.", "Create a new broadcasting universal function from required variables. Each ufunc builds around the notion of an element-by-element operation. Each ufunc object contains pointers to 1-d loops implementing the basic functionality for each supported type.", "Note", "The func, data, types, name, and doc arguments are not copied by PyUFunc_FromFuncAndData. The caller must ensure that the memory used by these arrays is not freed as long as the ufunc object is alive.", "types \u2013 ", "Length (nin + nout) * ntypes array of char encoding the numpy.dtype.num (built-in only) that the corresponding function in the func array accepts. For instance, for a comparison ufunc with three ntypes, two nin and one nout, where the first function accepts numpy.int32 and the the second numpy.int64, with both returning numpy.bool_, types would be (char[]) {5, 5, 0, 7, 7, 0} since NPY_INT32 is 5, NPY_INT64 is 7, and NPY_BOOL is 0.", "The bit-width names can also be used (e.g. NPY_INT32, NPY_COMPLEX128 ) if desired.", "Type casting rules will be used at runtime to find the first func callable by the input/output provided.", "This function is very similar to PyUFunc_FromFuncAndData above, but has an extra signature argument, to define a generalized universal functions. Similarly to how ufuncs are built around an element-by-element operation, gufuncs are around subarray-by-subarray operations, the signature defining the subarrays to operate on.", "This function is very similar to PyUFunc_FromFuncAndDataAndSignature above, but has an extra identity_value argument, to define an arbitrary identity for the ufunc when identity is passed as PyUFunc_IdentityValue.", "This function allows the user to register a 1-d loop with an already- created ufunc to be used whenever the ufunc is called with any of its input arguments as the user-defined data-type. This is needed in order to make ufuncs work with built-in data-types. The data-type must have been previously registered with the numpy system. The loop is passed in as function. This loop can take arbitrary data which should be passed in as data. The data-types the loop requires are passed in as arg_types which must be a pointer to memory at least as large as ufunc->nargs.", "This function behaves like PyUFunc_RegisterLoopForType above, except that it allows the user to register a 1-d loop using PyArray_Descr objects instead of dtype type num values. This allows a 1-d loop to be registered for structured array data-dtypes and custom data-types instead of scalar data-types.", "Replace a 1-d loop matching the given signature in the already-created ufunc with the new 1-d loop newfunc. Return the old 1-d loop function in oldfunc. Return 0 on success and -1 on failure. This function works only with built-in types (use PyUFunc_RegisterLoopForType for user-defined types). A signature is an array of data-type numbers indicating the inputs followed by the outputs assumed by the 1-d loop.", "A simple interface to the IEEE error-flag checking support. The errmask argument is a mask of UFUNC_MASK_{ERR} bitmasks indicating which errors to check for (and how to check for them). The errobj must be a Python tuple with two elements: a string containing the name which will be used in any communication of error and either a callable Python object (call-back function) or Py_None. The callable object will only be used if UFUNC_ERR_CALL is set as the desired error checking method. This routine manages the GIL and is safe to call even after releasing the GIL. If an error in the IEEE-compatible hardware is determined a -1 is returned, otherwise a 0 is returned.", "Clear the IEEE error flags.", "Get the Python values used for ufunc processing from the thread-local storage area unless the defaults have been set in which case the name lookup is bypassed. The name is placed as a string in the first element of *errobj. The second element is the looked-up function to call on error callback. The value of the looked-up buffer-size to use is passed into bufsize, and the value of the error mask is placed into errmask.", "At the core of every ufunc is a collection of type-specific functions that defines the basic functionality for each of the supported types. These functions must evaluate the underlying function \\(N\\geq1\\) times. Extra-data may be passed in that may be used during the calculation. This feature allows some general functions to be used as these basic looping functions. The general function has all the code needed to point variables to the right place and set up a function call. The general function assumes that the actual function to call is passed in as the extra data and calls it with the correct values. All of these functions are suitable for placing directly in the array of functions stored in the functions member of the PyUFuncObject structure.", "Type specific, core 1-d functions for ufuncs where each calculation is obtained by calling a function taking one input argument and returning one output. This function is passed in func. The letters correspond to dtypechar\u2019s of the supported data types ( e - half, f - float, d - double, g - long double, F - cfloat, D - cdouble, G - clongdouble). The argument func must support the same signature. The _As_X_X variants assume ndarray\u2019s of one data type but cast the values to use an underlying function that takes a different data type. Thus, PyUFunc_f_f_As_d_d uses ndarrays of data type NPY_FLOAT but calls out to a C-function that takes double and returns double.", "Type specific, core 1-d functions for ufuncs where each calculation is obtained by calling a function taking two input arguments and returning one output. The underlying function to call is passed in as func. The letters correspond to dtypechar\u2019s of the specific data type supported by the general-purpose function. The argument func must support the corresponding signature. The _As_XX_X variants assume ndarrays of one data type but cast the values at each iteration of the loop to use the underlying function that takes a different data type.", "One-input, one-output, and two-input, one-output core 1-d functions for the NPY_OBJECT data type. These functions handle reference count issues and return early on error. The actual function to call is func and it must accept calls with the signature (PyObject*)\n(PyObject*) for PyUFunc_O_O or (PyObject*)(PyObject *,\nPyObject *) for PyUFunc_OO_O.", "This general purpose 1-d core function assumes that func is a string representing a method of the input object. For each iteration of the loop, the Python object is extracted from the array and its func method is called returning the result to the output array.", "This general purpose 1-d core function assumes that func is a string representing a method of the input object that takes one argument. The first argument in args is the method whose function is called, the second argument in args is the argument passed to the function. The output of the function is stored in the third entry of args.", "This is the 1-d core function used by the dynamic ufuncs created by umath.frompyfunc(function, nin, nout). In this case func is a pointer to a PyUFunc_PyFuncData structure which has definition", "At each iteration of the loop, the nin input objects are extracted from their object arrays and placed into an argument tuple, the Python callable is called with the input arguments, and the nout outputs are placed into their object arrays.", "These are the constants and functions for accessing the ufunc C-API from extension modules in precisely the same way as the array C-API can be accessed. The import_ufunc () function must always be called (in the initialization subroutine of the extension module). If your extension module is in one file then that is all that is required. The other two constants are useful if your extension module makes use of multiple files. In that case, define PY_UFUNC_UNIQUE_SYMBOL to something unique to your code and then in source files that do not contain the module initialization function but still need access to the UFUNC API, define PY_UFUNC_UNIQUE_SYMBOL to the same name used previously and also define NO_IMPORT_UFUNC.", "The C-API is actually an array of function pointers. This array is created (and pointed to by a global variable) by import_ufunc. The global variable is either statically defined or allowed to be seen by other files depending on the state of PY_UFUNC_UNIQUE_SYMBOL and NO_IMPORT_UFUNC."]}, {"name": "UFUNC_FPE_DIVIDEBYZERO", "path": "reference/c-api/ufunc#c.UFUNC_FPE_DIVIDEBYZERO", "type": "UFunc API", "text": []}, {"name": "UFUNC_FPE_INVALID", "path": "reference/c-api/ufunc#c.UFUNC_FPE_INVALID", "type": "UFunc API", "text": []}, {"name": "UFUNC_FPE_OVERFLOW", "path": "reference/c-api/ufunc#c.UFUNC_FPE_OVERFLOW", "type": "UFunc API", "text": []}, {"name": "UFUNC_FPE_UNDERFLOW", "path": "reference/c-api/ufunc#c.UFUNC_FPE_UNDERFLOW", "type": "UFunc API", "text": []}, {"name": "UFUNC_MASK_INVALID", "path": "reference/c-api/ufunc#c.UFUNC_MASK_INVALID", "type": "UFunc API", "text": []}, {"name": "UFUNC_MASK_OVERFLOW", "path": "reference/c-api/ufunc#c.UFUNC_MASK_OVERFLOW", "type": "UFunc API", "text": []}, {"name": "UFUNC_MASK_UNDERFLOW", "path": "reference/c-api/ufunc#c.UFUNC_MASK_UNDERFLOW", "type": "UFunc API", "text": []}, {"name": "UFUNC_SHIFT_DIVIDEBYZERO", "path": "reference/c-api/ufunc#c.UFUNC_SHIFT_DIVIDEBYZERO", "type": "UFunc API", "text": []}, {"name": "UFUNC_SHIFT_INVALID", "path": "reference/c-api/ufunc#c.UFUNC_SHIFT_INVALID", "type": "UFunc API", "text": []}, {"name": "UFUNC_SHIFT_OVERFLOW", "path": "reference/c-api/ufunc#c.UFUNC_SHIFT_OVERFLOW", "type": "UFunc API", "text": []}, {"name": "UFUNC_SHIFT_UNDERFLOW", "path": "reference/c-api/ufunc#c.UFUNC_SHIFT_UNDERFLOW", "type": "UFunc API", "text": []}, {"name": "Under-the-hood Documentation for developers", "path": "dev/underthehood", "type": "Development", "text": ["These documents are intended as a low-level look into NumPy; focused towards developers."]}, {"name": "Universal functions (ufunc)", "path": "reference/ufuncs", "type": "Universal functions ( \n    \n     ufunc\n    \n    )", "text": ["See also", "Universal functions (ufunc) basics", "A universal function (or ufunc for short) is a function that operates on ndarrays in an element-by-element fashion, supporting array broadcasting, type casting, and several other standard features. That is, a ufunc is a \u201cvectorized\u201d wrapper for a function that takes a fixed number of specific inputs and produces a fixed number of specific outputs. For detailed information on universal functions, see Universal functions (ufunc) basics.", "numpy.ufunc()", "Functions that operate element by element on whole arrays.", "All ufuncs take optional keyword arguments. Most of these represent advanced usage and will not typically be used.", "New in version 1.6.", "The first output can be provided as either a positional or a keyword parameter. Keyword \u2018out\u2019 arguments are incompatible with positional ones.", "New in version 1.10.", "The \u2018out\u2019 keyword argument is expected to be a tuple with one entry per output (which can be None for arrays to be allocated by the ufunc). For ufuncs with a single output, passing a single array (instead of a tuple holding a single array) is also valid.", "Passing a single array in the \u2018out\u2019 keyword argument to a ufunc with multiple outputs is deprecated, and will raise a warning in numpy 1.10, and an error in a future release.", "If \u2018out\u2019 is None (the default), a uninitialized return array is created. The output array is then filled with the results of the ufunc in the places that the broadcast \u2018where\u2019 is True. If \u2018where\u2019 is the scalar True (the default), then this corresponds to the entire output being filled. Note that outputs not explicitly filled are left with their uninitialized values.", "New in version 1.13.", "Operations where ufunc input and output operands have memory overlap are defined to be the same as for equivalent operations where there is no memory overlap. Operations affected make temporary copies as needed to eliminate data dependency. As detecting these cases is computationally expensive, a heuristic is used, which may in rare cases result in needless temporary copies. For operations where the data dependency is simple enough for the heuristic to analyze, temporary copies will not be made even if the arrays overlap, if it can be deduced copies are not necessary. As an example, np.add(a, b, out=a) will not involve copies.", "New in version 1.7.", "Accepts a boolean array which is broadcast together with the operands. Values of True indicate to calculate the ufunc at that position, values of False indicate to leave the value in the output alone. This argument cannot be used for generalized ufuncs as those take non-scalar input.", "Note that if an uninitialized return array is created, values of False will leave those values uninitialized.", "New in version 1.15.", "A list of tuples with indices of axes a generalized ufunc should operate on. For instance, for a signature of (i,j),(j,k)->(i,k) appropriate for matrix multiplication, the base elements are two-dimensional matrices and these are taken to be stored in the two last axes of each argument. The corresponding axes keyword would be [(-2, -1), (-2, -1), (-2, -1)]. For simplicity, for generalized ufuncs that operate on 1-dimensional arrays (vectors), a single integer is accepted instead of a single-element tuple, and for generalized ufuncs for which all outputs are scalars, the output tuples can be omitted.", "New in version 1.15.", "A single axis over which a generalized ufunc should operate. This is a short-cut for ufuncs that operate over a single, shared core dimension, equivalent to passing in axes with entries of (axis,) for each single-core-dimension argument and () for all others. For instance, for a signature (i),(i)->(), it is equivalent to passing in axes=[(axis,), (axis,), ()].", "New in version 1.15.", "If this is set to True, axes which are reduced over will be left in the result as a dimension with size one, so that the result will broadcast correctly against the inputs. This option can only be used for generalized ufuncs that operate on inputs that all have the same number of core dimensions and with outputs that have no core dimensions, i.e., with signatures like (i),(i)->() or (m,m)->(). If used, the location of the dimensions in the output can be controlled with axes and axis.", "New in version 1.6.", "May be \u2018no\u2019, \u2018equiv\u2019, \u2018safe\u2019, \u2018same_kind\u2019, or \u2018unsafe\u2019. See can_cast for explanations of the parameter values.", "Provides a policy for what kind of casting is permitted. For compatibility with previous versions of NumPy, this defaults to \u2018unsafe\u2019 for numpy < 1.7. In numpy 1.7 a transition to \u2018same_kind\u2019 was begun where ufuncs produce a DeprecationWarning for calls which are allowed under the \u2018unsafe\u2019 rules, but not under the \u2018same_kind\u2019 rules. From numpy 1.10 and onwards, the default is \u2018same_kind\u2019.", "New in version 1.6.", "Specifies the calculation iteration order/memory layout of the output array. Defaults to \u2018K\u2019. \u2018C\u2019 means the output should be C-contiguous, \u2018F\u2019 means F-contiguous, \u2018A\u2019 means F-contiguous if the inputs are F-contiguous and not also not C-contiguous, C-contiguous otherwise, and \u2018K\u2019 means to match the element ordering of the inputs as closely as possible.", "New in version 1.6.", "Overrides the DType of the output arrays the same way as the signature. This should ensure a matching precision of the calculation. The exact calculation DTypes chosen may depend on the ufunc and the inputs may be cast to this DType to perform the calculation.", "New in version 1.6.", "Defaults to true. If set to false, the output will always be a strict array, not a subtype.", "Either a Dtype, a tuple of DTypes, or a special signature string indicating the input and output types of a ufunc.", "This argument allows the user to specify exact DTypes to be used for the calculation. Casting will be used as necessary. The actual DType of the input arrays is not considered unless signature is None for that array.", "When all DTypes are fixed, a specific loop is chosen or an error raised if no matching loop exists. If some DTypes are not specified and left None, the behaviour may depend on the ufunc. At this time, a list of available signatures is provided by the types attribute of the ufunc. (This list may be missing DTypes not defined by NumPy.)", "The signature only specifies the DType class/type. For example, it can specify that the operation should be datetime64 or float64 operation. It does not specify the datetime64 time-unit or the float64 byte-order.", "For backwards compatibility this argument can also be provided as sig, although the long form is preferred. Note that this should not be confused with the generalized ufunc signature that is stored in the signature attribute of the of the ufunc object.", "A list of length 3 specifying the ufunc buffer-size, the error mode integer, and the error call-back function. Normally, these values are looked up in a thread-specific dictionary. Passing them here circumvents that look up and uses the low-level specification provided for the error mode. This may be useful, for example, as an optimization for calculations requiring many ufunc calls on small arrays in a loop.", "There are some informational attributes that universal functions possess. None of the attributes can be set.", "__doc__", "A docstring for each ufunc. The first part of the docstring is dynamically generated from the number of outputs, the name, and the number of inputs. The second part of the docstring is provided at creation time and stored with the ufunc.", "__name__", "The name of the ufunc.", "ufunc.nin", "The number of inputs.", "ufunc.nout", "The number of outputs.", "ufunc.nargs", "The number of arguments.", "ufunc.ntypes", "The number of types.", "ufunc.types", "Returns a list with types grouped input->output.", "ufunc.identity", "The identity value.", "ufunc.signature", "Definition of the core elements a generalized ufunc operates on.", "ufunc.reduce(array[, axis, dtype, out, ...])", "Reduces array's dimension by one, by applying ufunc along one axis.", "ufunc.accumulate(array[, axis, dtype, out])", "Accumulate the result of applying the operator to all elements.", "ufunc.reduceat(array, indices[, axis, ...])", "Performs a (local) reduce with specified slices over a single axis.", "ufunc.outer(A, B, /, **kwargs)", "Apply the ufunc op to all pairs (a, b) with a in A and b in B.", "ufunc.at(a, indices[, b])", "Performs unbuffered in place operation on operand 'a' for elements specified by 'indices'.", "Warning", "A reduce-like operation on an array with a data-type that has a range \u201ctoo small\u201d to handle the result will silently wrap. One should use dtype to increase the size of the data-type over which reduction takes place.", "There are currently more than 60 universal functions defined in numpy on one or more types, covering a wide variety of operations. Some of these ufuncs are called automatically on arrays when the relevant infix notation is used (e.g., add(a, b) is called internally when a + b is written and a or b is an ndarray). Nevertheless, you may still want to use the ufunc call in order to use the optional output argument(s) to place the output(s) in an object (or objects) of your choice.", "Recall that each ufunc operates element-by-element. Therefore, each scalar ufunc will be described as if acting on a set of scalar inputs to return a set of scalar outputs.", "Note", "The ufunc still returns its output(s) even if you use the optional output argument(s).", "add(x1, x2, /[, out, where, casting, order, ...])", "Add arguments element-wise.", "subtract(x1, x2, /[, out, where, casting, ...])", "Subtract arguments, element-wise.", "multiply(x1, x2, /[, out, where, casting, ...])", "Multiply arguments element-wise.", "matmul(x1, x2, /[, out, casting, order, ...])", "Matrix product of two arrays.", "divide(x1, x2, /[, out, where, casting, ...])", "Returns a true division of the inputs, element-wise.", "logaddexp(x1, x2, /[, out, where, casting, ...])", "Logarithm of the sum of exponentiations of the inputs.", "logaddexp2(x1, x2, /[, out, where, casting, ...])", "Logarithm of the sum of exponentiations of the inputs in base-2.", "true_divide(x1, x2, /[, out, where, ...])", "Returns a true division of the inputs, element-wise.", "floor_divide(x1, x2, /[, out, where, ...])", "Return the largest integer smaller or equal to the division of the inputs.", "negative(x, /[, out, where, casting, order, ...])", "Numerical negative, element-wise.", "positive(x, /[, out, where, casting, order, ...])", "Numerical positive, element-wise.", "power(x1, x2, /[, out, where, casting, ...])", "First array elements raised to powers from second array, element-wise.", "float_power(x1, x2, /[, out, where, ...])", "First array elements raised to powers from second array, element-wise.", "remainder(x1, x2, /[, out, where, casting, ...])", "Returns the element-wise remainder of division.", "mod(x1, x2, /[, out, where, casting, order, ...])", "Returns the element-wise remainder of division.", "fmod(x1, x2, /[, out, where, casting, ...])", "Returns the element-wise remainder of division.", "divmod(x1, x2[, out1, out2], / [[, out, ...])", "Return element-wise quotient and remainder simultaneously.", "absolute(x, /[, out, where, casting, order, ...])", "Calculate the absolute value element-wise.", "fabs(x, /[, out, where, casting, order, ...])", "Compute the absolute values element-wise.", "rint(x, /[, out, where, casting, order, ...])", "Round elements of the array to the nearest integer.", "sign(x, /[, out, where, casting, order, ...])", "Returns an element-wise indication of the sign of a number.", "heaviside(x1, x2, /[, out, where, casting, ...])", "Compute the Heaviside step function.", "conj(x, /[, out, where, casting, order, ...])", "Return the complex conjugate, element-wise.", "conjugate(x, /[, out, where, casting, ...])", "Return the complex conjugate, element-wise.", "exp(x, /[, out, where, casting, order, ...])", "Calculate the exponential of all elements in the input array.", "exp2(x, /[, out, where, casting, order, ...])", "Calculate 2**p for all p in the input array.", "log(x, /[, out, where, casting, order, ...])", "Natural logarithm, element-wise.", "log2(x, /[, out, where, casting, order, ...])", "Base-2 logarithm of x.", "log10(x, /[, out, where, casting, order, ...])", "Return the base 10 logarithm of the input array, element-wise.", "expm1(x, /[, out, where, casting, order, ...])", "Calculate exp(x) - 1 for all elements in the array.", "log1p(x, /[, out, where, casting, order, ...])", "Return the natural logarithm of one plus the input array, element-wise.", "sqrt(x, /[, out, where, casting, order, ...])", "Return the non-negative square-root of an array, element-wise.", "square(x, /[, out, where, casting, order, ...])", "Return the element-wise square of the input.", "cbrt(x, /[, out, where, casting, order, ...])", "Return the cube-root of an array, element-wise.", "reciprocal(x, /[, out, where, casting, ...])", "Return the reciprocal of the argument, element-wise.", "gcd(x1, x2, /[, out, where, casting, order, ...])", "Returns the greatest common divisor of |x1| and |x2|", "lcm(x1, x2, /[, out, where, casting, order, ...])", "Returns the lowest common multiple of |x1| and |x2|", "Tip", "The optional output arguments can be used to help you save memory for large calculations. If your arrays are large, complicated expressions can take longer than absolutely necessary due to the creation and (later) destruction of temporary calculation spaces. For example, the expression G = A * B + C is equivalent to T1 = A * B; G = T1 + C; del T1. It will be more quickly executed as G = A * B; add(G, C, G) which is the same as G = A * B; G += C.", "All trigonometric functions use radians when an angle is called for. The ratio of degrees to radians is \\(180^{\\circ}/\\pi.\\)", "sin(x, /[, out, where, casting, order, ...])", "Trigonometric sine, element-wise.", "cos(x, /[, out, where, casting, order, ...])", "Cosine element-wise.", "tan(x, /[, out, where, casting, order, ...])", "Compute tangent element-wise.", "arcsin(x, /[, out, where, casting, order, ...])", "Inverse sine, element-wise.", "arccos(x, /[, out, where, casting, order, ...])", "Trigonometric inverse cosine, element-wise.", "arctan(x, /[, out, where, casting, order, ...])", "Trigonometric inverse tangent, element-wise.", "arctan2(x1, x2, /[, out, where, casting, ...])", "Element-wise arc tangent of x1/x2 choosing the quadrant correctly.", "hypot(x1, x2, /[, out, where, casting, ...])", "Given the \"legs\" of a right triangle, return its hypotenuse.", "sinh(x, /[, out, where, casting, order, ...])", "Hyperbolic sine, element-wise.", "cosh(x, /[, out, where, casting, order, ...])", "Hyperbolic cosine, element-wise.", "tanh(x, /[, out, where, casting, order, ...])", "Compute hyperbolic tangent element-wise.", "arcsinh(x, /[, out, where, casting, order, ...])", "Inverse hyperbolic sine element-wise.", "arccosh(x, /[, out, where, casting, order, ...])", "Inverse hyperbolic cosine, element-wise.", "arctanh(x, /[, out, where, casting, order, ...])", "Inverse hyperbolic tangent element-wise.", "degrees(x, /[, out, where, casting, order, ...])", "Convert angles from radians to degrees.", "radians(x, /[, out, where, casting, order, ...])", "Convert angles from degrees to radians.", "deg2rad(x, /[, out, where, casting, order, ...])", "Convert angles from degrees to radians.", "rad2deg(x, /[, out, where, casting, order, ...])", "Convert angles from radians to degrees.", "These function all require integer arguments and they manipulate the bit-pattern of those arguments.", "bitwise_and(x1, x2, /[, out, where, ...])", "Compute the bit-wise AND of two arrays element-wise.", "bitwise_or(x1, x2, /[, out, where, casting, ...])", "Compute the bit-wise OR of two arrays element-wise.", "bitwise_xor(x1, x2, /[, out, where, ...])", "Compute the bit-wise XOR of two arrays element-wise.", "invert(x, /[, out, where, casting, order, ...])", "Compute bit-wise inversion, or bit-wise NOT, element-wise.", "left_shift(x1, x2, /[, out, where, casting, ...])", "Shift the bits of an integer to the left.", "right_shift(x1, x2, /[, out, where, ...])", "Shift the bits of an integer to the right.", "greater(x1, x2, /[, out, where, casting, ...])", "Return the truth value of (x1 > x2) element-wise.", "greater_equal(x1, x2, /[, out, where, ...])", "Return the truth value of (x1 >= x2) element-wise.", "less(x1, x2, /[, out, where, casting, ...])", "Return the truth value of (x1 < x2) element-wise.", "less_equal(x1, x2, /[, out, where, casting, ...])", "Return the truth value of (x1 <= x2) element-wise.", "not_equal(x1, x2, /[, out, where, casting, ...])", "Return (x1 != x2) element-wise.", "equal(x1, x2, /[, out, where, casting, ...])", "Return (x1 == x2) element-wise.", "Warning", "Do not use the Python keywords and and or to combine logical array expressions. These keywords will test the truth value of the entire array (not element-by-element as you might expect). Use the bitwise operators & and | instead.", "logical_and(x1, x2, /[, out, where, ...])", "Compute the truth value of x1 AND x2 element-wise.", "logical_or(x1, x2, /[, out, where, casting, ...])", "Compute the truth value of x1 OR x2 element-wise.", "logical_xor(x1, x2, /[, out, where, ...])", "Compute the truth value of x1 XOR x2, element-wise.", "logical_not(x, /[, out, where, casting, ...])", "Compute the truth value of NOT x element-wise.", "Warning", "The bit-wise operators & and | are the proper way to perform element-by-element array comparisons. Be sure you understand the operator precedence: (a > 2) & (a < 5) is the proper syntax because a > 2 & a < 5 will result in an error due to the fact that 2 & a is evaluated first.", "maximum(x1, x2, /[, out, where, casting, ...])", "Element-wise maximum of array elements.", "Tip", "The Python function max() will find the maximum over a one-dimensional array, but it will do so using a slower sequence interface. The reduce method of the maximum ufunc is much faster. Also, the max() method will not give answers you might expect for arrays with greater than one dimension. The reduce method of minimum also allows you to compute a total minimum over an array.", "minimum(x1, x2, /[, out, where, casting, ...])", "Element-wise minimum of array elements.", "Warning", "the behavior of maximum(a, b) is different than that of max(a, b). As a ufunc, maximum(a, b) performs an element-by-element comparison of a and b and chooses each element of the result according to which element in the two arrays is larger. In contrast, max(a, b) treats the objects a and b as a whole, looks at the (total) truth value of a > b and uses it to return either a or b (as a whole). A similar difference exists between minimum(a, b) and min(a, b).", "fmax(x1, x2, /[, out, where, casting, ...])", "Element-wise maximum of array elements.", "fmin(x1, x2, /[, out, where, casting, ...])", "Element-wise minimum of array elements.", "Recall that all of these functions work element-by-element over an array, returning an array output. The description details only a single operation.", "isfinite(x, /[, out, where, casting, order, ...])", "Test element-wise for finiteness (not infinity and not Not a Number).", "isinf(x, /[, out, where, casting, order, ...])", "Test element-wise for positive or negative infinity.", "isnan(x, /[, out, where, casting, order, ...])", "Test element-wise for NaN and return result as a boolean array.", "isnat(x, /[, out, where, casting, order, ...])", "Test element-wise for NaT (not a time) and return result as a boolean array.", "fabs(x, /[, out, where, casting, order, ...])", "Compute the absolute values element-wise.", "signbit(x, /[, out, where, casting, order, ...])", "Returns element-wise True where signbit is set (less than zero).", "copysign(x1, x2, /[, out, where, casting, ...])", "Change the sign of x1 to that of x2, element-wise.", "nextafter(x1, x2, /[, out, where, casting, ...])", "Return the next floating-point value after x1 towards x2, element-wise.", "spacing(x, /[, out, where, casting, order, ...])", "Return the distance between x and the nearest adjacent number.", "modf(x[, out1, out2], / [[, out, where, ...])", "Return the fractional and integral parts of an array, element-wise.", "ldexp(x1, x2, /[, out, where, casting, ...])", "Returns x1 * 2**x2, element-wise.", "frexp(x[, out1, out2], / [[, out, where, ...])", "Decompose the elements of x into mantissa and twos exponent.", "fmod(x1, x2, /[, out, where, casting, ...])", "Returns the element-wise remainder of division.", "floor(x, /[, out, where, casting, order, ...])", "Return the floor of the input, element-wise.", "ceil(x, /[, out, where, casting, order, ...])", "Return the ceiling of the input, element-wise.", "trunc(x, /[, out, where, casting, order, ...])", "Return the truncated value of the input, element-wise."]}, {"name": "Universal functions (ufunc) basics", "path": "user/basics.ufuncs", "type": "User Guide", "text": ["See also", "Universal functions (ufunc)", "A universal function (or ufunc for short) is a function that operates on ndarrays in an element-by-element fashion, supporting array broadcasting, type casting, and several other standard features. That is, a ufunc is a \u201cvectorized\u201d wrapper for a function that takes a fixed number of specific inputs and produces a fixed number of specific outputs.", "In NumPy, universal functions are instances of the numpy.ufunc class. Many of the built-in functions are implemented in compiled C code. The basic ufuncs operate on scalars, but there is also a generalized kind for which the basic elements are sub-arrays (vectors, matrices, etc.), and broadcasting is done over other dimensions. The simplest example is the addition operator:", "One can also produce custom numpy.ufunc instances using the numpy.frompyfunc factory function.", "All ufuncs have four methods. They can be found at Methods. However, these methods only make sense on scalar ufuncs that take two input arguments and return one output argument. Attempting to call these methods on other ufuncs will cause a ValueError.", "The reduce-like methods all take an axis keyword, a dtype keyword, and an out keyword, and the arrays must all have dimension >= 1. The axis keyword specifies the axis of the array over which the reduction will take place (with negative values counting backwards). Generally, it is an integer, though for numpy.ufunc.reduce, it can also be a tuple of int to reduce over several axes at once, or None, to reduce over all axes. For example:", "The dtype keyword allows you to manage a very common problem that arises when naively using ufunc.reduce. Sometimes you may have an array of a certain data type and wish to add up all of its elements, but the result does not fit into the data type of the array. This commonly happens if you have an array of single-byte integers. The dtype keyword allows you to alter the data type over which the reduction takes place (and therefore the type of the output). Thus, you can ensure that the output is a data type with precision large enough to handle your output. The responsibility of altering the reduce type is mostly up to you. There is one exception: if no dtype is given for a reduction on the \u201cadd\u201d or \u201cmultiply\u201d operations, then if the input type is an integer (or Boolean) data-type and smaller than the size of the numpy.int_ data type, it will be internally upcast to the int_ (or numpy.uint) data-type. In the previous example:", "Finally, the out keyword allows you to provide an output array (for single-output ufuncs, which are currently the only ones supported; for future extension, however, a tuple with a single argument can be passed in). If out is given, the dtype argument is ignored. Considering x from the previous example:", "Ufuncs also have a fifth method, numpy.ufunc.at, that allows in place operations to be performed using advanced indexing. No buffering is used on the dimensions where advanced indexing is used, so the advanced index can list an item more than once and the operation will be performed on the result of the previous operation for that item.", "The output of the ufunc (and its methods) is not necessarily an ndarray, if all input arguments are not ndarrays. Indeed, if any input defines an __array_ufunc__ method, control will be passed completely to that function, i.e., the ufunc is overridden.", "If none of the inputs overrides the ufunc, then all output arrays will be passed to the __array_prepare__ and __array_wrap__ methods of the input (besides ndarrays, and scalars) that defines it and has the highest __array_priority__ of any other input to the universal function. The default __array_priority__ of the ndarray is 0.0, and the default __array_priority__ of a subtype is 0.0. Matrices have __array_priority__ equal to 10.0.", "All ufuncs can also take output arguments. If necessary, output will be cast to the data-type(s) of the provided output array(s). If a class with an __array__ method is used for the output, results will be written to the object returned by __array__. Then, if the class also has an __array_prepare__ method, it is called so metadata may be determined based on the context of the ufunc (the context consisting of the ufunc itself, the arguments passed to the ufunc, and the ufunc domain.) The array object returned by __array_prepare__ is passed to the ufunc for computation. Finally, if the class also has an __array_wrap__ method, the returned ndarray result will be passed to that method just before passing control back to the caller.", "See also", "Broadcasting basics", "Each universal function takes array inputs and produces array outputs by performing the core function element-wise on the inputs (where an element is generally a scalar, but can be a vector or higher-order sub-array for generalized ufuncs). Standard broadcasting rules are applied so that inputs not sharing exactly the same shapes can still be usefully operated on.", "By these rules, if an input has a dimension size of 1 in its shape, the first data entry in that dimension will be used for all calculations along that dimension. In other words, the stepping machinery of the ufunc will simply not step along that dimension (the stride will be 0 for that dimension).", "Note", "In NumPy 1.6.0, a type promotion API was created to encapsulate the mechanism for determining output types. See the functions numpy.result_type, numpy.promote_types, and numpy.min_scalar_type for more details.", "At the core of every ufunc is a one-dimensional strided loop that implements the actual function for a specific type combination. When a ufunc is created, it is given a static list of inner loops and a corresponding list of type signatures over which the ufunc operates. The ufunc machinery uses this list to determine which inner loop to use for a particular case. You can inspect the .types attribute for a particular ufunc to see which type combinations have a defined inner loop and which output type they produce (character codes are used in said output for brevity).", "Casting must be done on one or more of the inputs whenever the ufunc does not have a core loop implementation for the input types provided. If an implementation for the input types cannot be found, then the algorithm searches for an implementation with a type signature to which all of the inputs can be cast \u201csafely.\u201d The first one it finds in its internal list of loops is selected and performed, after all necessary type casting. Recall that internal copies during ufuncs (even for casting) are limited to the size of an internal buffer (which is user settable).", "Note", "Universal functions in NumPy are flexible enough to have mixed type signatures. Thus, for example, a universal function could be defined that works with floating-point and integer values. See numpy.ldexp for an example.", "By the above description, the casting rules are essentially implemented by the question of when a data type can be cast \u201csafely\u201d to another data type. The answer to this question can be determined in Python with a function call: can_cast(fromtype, totype). The example below shows the results of this call for the 24 internally supported types on the author\u2019s 64-bit system. You can generate this table for your system with the code given in the example.", "Code segment showing the \u201ccan cast safely\u201d table for a 64-bit system. Generally the output depends on the system; your system might result in a different table.", "You should note that, while included in the table for completeness, the \u2018S\u2019, \u2018U\u2019, and \u2018V\u2019 types cannot be operated on by ufuncs. Also, note that on a 32-bit system the integer types may have different sizes, resulting in a slightly altered table.", "Mixed scalar-array operations use a different set of casting rules that ensure that a scalar cannot \u201cupcast\u201d an array unless the scalar is of a fundamentally different kind of data (i.e., under a different hierarchy in the data-type hierarchy) than the array. This rule enables you to use scalar constants in your code (which, as Python types, are interpreted accordingly in ufuncs) without worrying about whether the precision of the scalar constant will cause upcasting on your large (small precision) array.", "Internally, buffers are used for misaligned data, swapped data, and data that has to be converted from one data type to another. The size of internal buffers is settable on a per-thread basis. There can be up to \\(2 (n_{\\mathrm{inputs}} + n_{\\mathrm{outputs}})\\) buffers of the specified size created to handle the data from all the inputs and outputs of a ufunc. The default size of a buffer is 10,000 elements. Whenever buffer-based calculation would be needed, but all input arrays are smaller than the buffer size, those misbehaved or incorrectly-typed arrays will be copied before the calculation proceeds. Adjusting the size of the buffer may therefore alter the speed at which ufunc calculations of various sorts are completed. A simple interface for setting this variable is accessible using the function numpy.setbufsize.", "Universal functions can trip special floating-point status registers in your hardware (such as divide-by-zero). If available on your platform, these registers will be regularly checked during calculation. Error handling is controlled on a per-thread basis, and can be configured using the functions numpy.seterr and numpy.seterrcall.", "Classes (including ndarray subclasses) can override how ufuncs act on them by defining certain special methods. For details, see Standard array subclasses."]}, {"name": "unsigned int PyArray_GetNDArrayCFeatureVersion()", "path": "reference/c-api/array#c.PyArray_GetNDArrayCFeatureVersion", "type": "Array API", "text": ["New in version 1.4.0.", "This just returns the value NPY_FEATURE_VERSION. NPY_FEATURE_VERSION changes whenever the API changes (e.g. a function is added). A changed value does not always require a recompile."]}, {"name": "unsigned int PyArray_GetNDArrayCVersion()", "path": "reference/c-api/array#c.PyArray_GetNDArrayCVersion", "type": "Array API", "text": ["This just returns the value NPY_VERSION. NPY_VERSION changes whenever a backward incompatible change at the ABI level. Because it is in the C-API, however, comparing the output of this function from the value defined in the current header gives a way to test if the C-API has changed thus requiring a re-compilation of extension modules that use the C-API. This is automatically checked in the function import_array."]}, {"name": "Upgrading PCG64 with PCG64DXSM", "path": "reference/random/upgrading-pcg64", "type": "Upgrading PCG64 with PCG64DXSM", "text": ["Uses of the PCG64 BitGenerator in a massively-parallel context have been shown to have statistical weaknesses that were not apparent at the first release in numpy 1.17. Most users will never observe this weakness and are safe to continue to use PCG64. We have introduced a new PCG64DXSM BitGenerator that will eventually become the new default BitGenerator implementation used by default_rng in future releases. PCG64DXSM solves the statistical weakness while preserving the performance and the features of PCG64.", "If you", "then this weakness does not affect you at all. Carry on.", "If you use moderate numbers of parallel streams created with default_rng or SeedSequence.spawn, in the 1000s, then the chance of observing this weakness is negligibly small. You can continue to use PCG64 comfortably.", "If you use very large numbers of parallel streams, in the millions, and draw large amounts of numbers from each, then the chance of observing this weakness can become non-negligible, if still small. An example of such a use case would be a very large distributed reinforcement learning problem with millions of long Monte Carlo playouts each generating billions of random number draws. Such use cases should consider using PCG64DXSM explicitly or another modern BitGenerator like SFC64 or Philox, but it is unlikely that any old results you may have calculated are invalid. In any case, the weakness is a kind of Birthday Paradox collision. That is, a single pair of parallel streams out of the millions, considered together, might fail a stringent set of statistical tests of randomness. The remaining millions of streams would all be perfectly fine, and the effect of the bad pair in the whole calculation is very likely to be swamped by the remaining streams in most applications.", "Like many PRNG algorithms, PCG64 is constructed from a transition function, which advances a 128-bit state, and an output function, that mixes the 128-bit state into a 64-bit integer to be output. One of the guiding design principles of the PCG family of PRNGs is to balance the computational cost (and pseudorandomness strength) between the transition function and the output function. The transition function is a 128-bit linear congruential generator (LCG), which consists of multiplying the 128-bit state with a fixed multiplication constant and then adding a user-chosen increment, in 128-bit modular arithmetic. LCGs are well-analyzed PRNGs with known weaknesses, though 128-bit LCGs are large enough to pass stringent statistical tests on their own, with only the trivial output function. The output function of PCG64 is intended to patch up some of those known weaknesses by doing \u201cjust enough\u201d scrambling of the bits to assist in the statistical properties without adding too much computational cost.", "One of these known weaknesses is that advancing the state of the LCG by steps numbering a power of two (bg.advance(2**N)) will leave the lower N bits identical to the state that was just left. For a single stream drawn from sequentially, this is of little consequence. The remaining \\(128-N\\) bits provide plenty of pseudorandomness that will be mixed in for any practical N that can be observed in a single stream, which is why one does not need to worry about this if you only use a single stream in your application. Similarly, the PCG64.jumped method uses a carefully chosen number of steps to avoid creating these collisions. However, once you start creating \u201crandomly-initialized\u201d parallel streams, either using OS entropy by calling default_rng repeatedly or using SeedSequence.spawn, then we need to consider how many lower bits need to \u201ccollide\u201d in order to create a bad pair of streams, and then evaluate the probability of creating such a collision. Empirically, it has been determined that if one shares the lower 58 bits of state and shares an increment, then the pair of streams, when interleaved, will fail PractRand in a reasonable amount of time, after drawing a few gigabytes of data. Following the standard Birthday Paradox calculations for a collision of 58 bits, we can see that we can create \\(2^{29}\\), or about half a billion, streams which is when the probability of such a collision becomes high. Half a billion streams is quite high, and the amount of data each stream needs to draw before the statistical correlations become apparent to even the strict PractRand tests is in the gigabytes. But this is on the horizon for very large applications like distributed reinforcement learning. There are reasons to expect that even in these applications a collision probably will not have a practical effect in the total result, since the statistical problem is constrained to just the colliding pair.", "Now, let us consider the case when the increment is not constrained to be the same. Our implementation of PCG64 seeds both the state and the increment; that is, two calls to default_rng (almost certainly) have different states and increments. Upon our first release, we believed that having the seeded increment would provide a certain amount of extra protection, that one would have to be \u201cclose\u201d in both the state space and increment space in order to observe correlations (PractRand failures) in a pair of streams. If that were true, then the \u201cbottleneck\u201d for collisions would be the 128-bit entropy pool size inside of SeedSequence (and 128-bit collisions are in the \u201cpreposterously unlikely\u201d category). Unfortunately, this is not true.", "One of the known properties of an LCG is that different increments create distinct streams, but with a known relationship. Each LCG has an orbit that traverses all \\(2^{128}\\) different 128-bit states. Two LCGs with different increments are related in that one can \u201crotate\u201d the orbit of the first LCG (advance it by a number of steps that we can compute from the two increments) such that then both LCGs will always then have the same state, up to an additive constant and maybe an inversion of the bits. If you then iterate both streams in lockstep, then the states will always remain related by that same additive constant (and the inversion, if present). Recall that PCG64 is constructed from both a transition function (the LCG) and an output function. It was expected that the scrambling effect of the output function would have been strong enough to make the distinct streams practically independent (i.e. \u201cpassing the PractRand tests\u201d) unless the two increments were pathologically related to each other (e.g. 1 and 3). The output function XSL-RR of the then-standard PCG algorithm that we implemented in PCG64 turns out to be too weak to cover up for the 58-bit collision of the underlying LCG that we described above. For any given pair of increments, the size of the \u201ccolliding\u201d space of states is the same, so for this weakness, the extra distinctness provided by the increments does not translate into extra protection from statistical correlations that PractRand can detect.", "Fortunately, strengthening the output function is able to correct this weakness and does turn the extra distinctness provided by differing increments into additional protection from these low-bit collisions. To the PCG author\u2019s credit, she had developed a stronger output function in response to related discussions during the long birth of the new BitGenerator system. We NumPy developers chose to be \u201cconservative\u201d and use the XSL-RR variant that had undergone a longer period of testing at that time. The DXSM output function adopts a \u201cxorshift-multiply\u201d construction used in strong integer hashes that has much better avalanche properties than the XSL-RR output function. While there are \u201cpathological\u201d pairs of increments that induce \u201cbad\u201d additive constants that relate the two streams, the vast majority of pairs induce \u201cgood\u201d additive constants that make the merely-distinct streams of LCG states into practically-independent output streams. Indeed, now the claim we once made about PCG64 is actually true of PCG64DXSM: collisions are possible, but both streams have to simultaneously be both \u201cclose\u201d in the 128 bit state space and \u201cclose\u201d in the 127-bit increment space, so that would be less likely than the negligible chance of colliding in the 128-bit internal SeedSequence pool. The DXSM output function is more computationally intensive than XSL-RR, but some optimizations in the LCG more than make up for the performance hit on most machines, so PCG64DXSM is a good, safe upgrade. There are, of course, an infinite number of stronger output functions that one could consider, but most will have a greater computational cost, and the DXSM output function has now received many CPU cycles of testing via PractRand at this time."]}, {"name": "Using F2PY bindings in Python", "path": "f2py/python-usage", "type": "Using F2PY bindings in Python", "text": ["All wrappers for Fortran/C routines, common blocks, or for Fortran 90 module data generated by F2PY are exposed to Python as fortran type objects. Routine wrappers are callable fortran type objects while wrappers to Fortran data have attributes referring to data objects.", "All fortran type objects have an attribute _cpointer that contains a CObject referring to the C pointer of the corresponding Fortran/C function or variable at the C level. Such CObjects can be used as a callback argument for F2PY generated functions to bypass the Python C/API layer for calling Python functions from Fortran or C when the computational aspects of such functions are implemented in C or Fortran and wrapped with F2PY (or any other tool capable of providing the CObject of a function).", "Consider a Fortran 77 file `ftype.f:", "and a wrapper built using f2py -c ftype.f -m ftype.", "In Python:", "In general, a scalar argument for a F2PY generated wrapper function can be an ordinary Python scalar (integer, float, complex number) as well as an arbitrary sequence object (list, tuple, array, string) of scalars. In the latter case, the first element of the sequence object is passed to Fortran routine as a scalar argument.", "Note", "When type-casting is required and there is possible loss of information via narrowing e.g. when type-casting float to integer or complex to float, F2PY does not raise an exception.", "Consider the following Fortran 77 code:", "and wrap it using f2py -c -m scalar scalar.f.", "In Python:", "F2PY generated wrapper functions accept almost any Python object as a string argument, since str is applied for non-string objects. Exceptions are NumPy arrays that must have type code 'c' or '1' when used as string arguments.", "A string can have an arbitrary length when used as a string argument for an F2PY generated wrapper function. If the length is greater than expected, the string is truncated silently. If the length is smaller than expected, additional memory is allocated and filled with \\0.", "Because Python strings are immutable, an intent(inout) argument expects an array version of a string in order to have in situ changes be effective.", "Consider the following Fortran 77 code:", "and wrap it using f2py -c -m mystring string.f.", "Python session:", "In general, array arguments for F2PY generated wrapper functions accept arbitrary sequences that can be transformed to NumPy array objects. There are two notable exceptions:", "In general, if a NumPy array is proper-contiguous and has a proper type then it is directly passed to the wrapped Fortran/C function. Otherwise, an element-wise copy of the input array is made and the copy, being proper-contiguous and with proper type, is used as the array argument.", "There are two types of proper-contiguous NumPy arrays:", "For one-dimensional arrays these notions coincide.", "For example, a 2x2 array A is Fortran-contiguous if its elements are stored in memory in the following order:", "and C-contiguous if the order is as follows:", "To test whether an array is C-contiguous, use the .flags.c_contiguous attribute of NumPy arrays. To test for Fortran contiguity, use the .flags.f_contiguous attribute.", "Usually there is no need to worry about how the arrays are stored in memory and whether the wrapped functions, being either Fortran or C functions, assume one or another storage order. F2PY automatically ensures that wrapped functions get arguments with the proper storage order; the underlying algorithm is designed to make copies of arrays only when absolutely necessary. However, when dealing with very large multidimensional input arrays with sizes close to the size of the physical memory in your computer, then care must be taken to ensure the usage of proper-contiguous and proper type arguments.", "To transform input arrays to column major storage order before passing them to Fortran routines, use the function numpy.asfortranarray(<array>).", "Consider the following Fortran 77 code:", "and wrap it using f2py -c -m arr array.f -DF2PY_REPORT_ON_ARRAY_COPY=1.", "In Python:", "F2PY supports calling Python functions from Fortran or C codes.", "Consider the following Fortran 77 code:", "and wrap it using f2py -c -m callback callback.f.", "In Python:", "In the above example F2PY was able to guess accurately the signature of the call-back function. However, sometimes F2PY cannot establish the appropriate signature; in these cases the signature of the call-back function must be explicitly defined in the signature file.", "To facilitate this, signature files may contain special modules (the names of these modules contain the special __user__ sub-string) that defines the various signatures for call-back functions. Callback arguments in routine signatures have the external attribute (see also the intent(callback) attribute). To relate a callback argument with its signature in a __user__ module block, a use statement can be utilized as illustrated below. The same signature for a callback argument can be referred to in different routine signatures.", "We use the same Fortran 77 code as in the previous example but now we will pretend that F2PY was not able to guess the signatures of call-back arguments correctly. First, we create an initial signature file callback2.pyf using F2PY:", "Then modify it as follows", "Finally, we build the extension module using f2py -c callback2.pyf callback.f.", "An example Python session for this snippet would be identical to the previous example except that the argument names would differ.", "Sometimes a Fortran package may require that users provide routines that the package will use. F2PY can construct an interface to such routines so that Python functions can be called from Fortran.", "Consider the following Fortran 77 subroutine that takes an array as its input and applies a function func to its elements.", "The Fortran code expects that the function func has been defined externally. In order to use a Python function for func, it must have an attribute intent(callback) and, it must be specified before the external statement.", "Finally, build an extension module using f2py -c -m foo calculate.f", "In Python:", "The function is included as an argument to the python function call to the Fortran subroutine even though it was not in the Fortran subroutine argument list. The \u201cexternal\u201d keyword refers to the C function generated by f2py, not the python function itself. The python function is essentially being supplied to the C function.", "The callback function may also be explicitly set in the module. Then it is not necessary to pass the function in the argument list to the Fortran function. This may be desired if the Fortran function calling the python callback function is itself called by another Fortran function.", "Consider the following Fortran 77 subroutine:", "and wrap it using f2py -c -m pfromf extcallback.f.", "In Python:", "F2PY generated interfaces are very flexible with respect to call-back arguments. For each call-back argument an additional optional argument <name>_extra_args is introduced by F2PY. This argument can be used to pass extra arguments to user provided call-back functions.", "If a F2PY generated wrapper function expects the following call-back argument:", "but the following Python function", "is provided by a user, and in addition,", "is used, then the following rules are applied when a Fortran or C function evaluates the call-back argument gun:", "If the function gun may return any number of objects as a tuple; then the following rules are applied:", "F2PY generates wrappers to common blocks defined in a routine signature block. Common blocks are visible to all Fortran codes linked to the current extension module, but not to other extension modules (this restriction is due to the way Python imports shared libraries). In Python, the F2PY wrappers to common blocks are fortran type objects that have (dynamic) attributes related to the data members of the common blocks. When accessed, these attributes return as NumPy array objects (multidimensional arrays are Fortran-contiguous) which directly link to data members in common blocks. Data members can be changed by direct assignment or by in-place changes to the corresponding array objects.", "Consider the following Fortran 77 code:", "and wrap it using f2py -c -m common common.f.", "In Python:", "The F2PY interface to Fortran 90 module data is similar to the handling of Fortran 77 common blocks.", "Consider the following Fortran 90 code:", "and wrap it using f2py -c -m moddata moddata.f90.", "In Python:", "F2PY has basic support for Fortran 90 module allocatable arrays.", "Consider the following Fortran 90 code:", "and wrap it using f2py -c -m allocarr allocarr.f90.", "In Python:"]}, {"name": "Using Gitpod for NumPy development", "path": "dev/development_gitpod", "type": "Development", "text": ["This section of the documentation will guide you through:", "Gitpod is an open-source platform for automated and ready-to-code development environments. It enables developers to describe their dev environment as code and start instant and fresh development environments for each new task directly from your browser. This reduces the need to install local development environments and deal with incompatible dependencies.", "To be able to use Gitpod, you will need to have the Gitpod app installed on your GitHub account, so if you do not have an account yet, you will need to create one first.", "Head over to the Gitpod website and click on the Continue with GitHub button. You will be redirected to the GitHub authentication page. You will then be asked to install the Gitpod GitHub app.", "Make sure to select All repositories access option to avoid issues with permissions later on. Click on the green Install button", "This will install the necessary hooks for the integration.", "The best way to work on NumPy as a contributor is by making a fork of the repository first.", "Once you have authenticated to Gitpod through GitHub, you can install the Gitpod browser extension which will add a Gitpod button next to the Code button in the repository:", "When your workspace is ready, you can test the build by entering:", "runtests.py is another script in the NumPy root directory. It runs a suite of tests that make sure NumPy is working as it should, and -v activates the --verbose option to show all the test output.", "Gitpod uses VSCode as the editor. If you have not used this editor before, you can check the Getting started VSCode docs to familiarize yourself with it.", "Your workspace will look similar to the image below:", "Note", "By default, VSCode initializes with a light theme. You can change to a dark theme by with the keyboard shortcut Cmd-K Cmd-T in Mac or Ctrl-K Ctrl-T in Linux and Windows.", "We have marked some important sections in the editor:", "We have also pre-installed a few tools and VSCode extensions to help with the development experience:", "The Development workflow section of this documentation contains information regarding the NumPy development workflow. Make sure to check this before working on your contributions.", "When using Gitpod, git is pre configured for you:", "As you started your workspace from your own NumPy fork, you will by default have both upstream and origin added as remotes. You can verify this by typing git remote on your terminal or by clicking on the branch name on the status bar (see image below).", "You can find the detailed documentation on how rendering the documentation with Sphinx works in the Building the NumPy API and reference docs section.", "The documentation is pre-built during your workspace initialization. So once this task is completed, you have two main options to render the documentation in Gitpod.", "To see the rendered version of a page, you can right-click on the .html file and click on Open with Live Serve. Alternatively, you can open the file in the editor and click on the Go live button on the status bar.", "A quick and easy way to see live changes in a .rst file as you work on it uses the rst extension with docutils.", "Note", "This will generate a simple live preview of the document without the html theme, and some backlinks might not be added correctly. But it is an easy and lightweight way to get instant feedback on your work.", "Open VSCode Command Palette with Cmd-Shift-P in Mac or Ctrl-Shift-P in Linux and Windows. Start typing \u201crestructured\u201d and choose either \u201cOpen preview\u201d or \u201cOpen preview to the Side\u201d.", "As you work on the document, you will see a live rendering of it on the editor.", "If you want to see the final output with the html theme you will need to rebuild the docs with make html and use Live Serve as described in option 1.", "Your stopped workspace will be kept for 14 days and deleted afterwards if you do not use them.", "Yes, let\u2019s say you stepped away for a while and you want to carry on working on your NumPy contributions. You need to visit https://gitpod.io/workspaces and click on the workspace you want to spin up again. All your changes will be there as you last left them.", "Absolutely! Any extensions you installed will be installed in your own workspace and preserved.", "Head to https://gitpod.io/integrations and make sure you are logged in. Hover over GitHub and click on the three buttons that appear on the right. Click on edit permissions and make sure you have user:email, read:user, and public_repo checked. Click on Update Permissions and confirm the changes in the GitHub application page.", "If you keep your workspace open in a browser tab but don\u2019t interact with it, it will shut down after 30 minutes. If you close the browser tab, it will shut down after 3 minutes.", "Unfortunately this is a known-issue on Gitpod\u2019s side. You can sort this issue in two ways:", "Head to https://gitpod.io/integrations and make sure you are logged in. Hover over GitHub and click on the three buttons that appear on the right. Click on edit permissions and make sure you have public_repo checked. Click on Update Permissions and confirm the changes in the GitHub application page."]}, {"name": "Using NumPy C-API", "path": "user/c-info", "type": "User Guide", "text": []}, {"name": "Using the Convenience Classes", "path": "reference/routines.polynomials.classes", "type": "Using the Convenience Classes", "text": ["The convenience classes provided by the polynomial package are:", "Name", "Provides", "Polynomial", "Power series", "Chebyshev", "Chebyshev series", "Legendre", "Legendre series", "Laguerre", "Laguerre series", "Hermite", "Hermite series", "HermiteE", "HermiteE series", "The series in this context are finite sums of the corresponding polynomial basis functions multiplied by coefficients. For instance, a power series looks like", "and has coefficients \\([1, 2, 3]\\). The Chebyshev series with the same coefficients looks like", "and more generally", "where in this case the \\(T_n\\) are the Chebyshev functions of degree \\(n\\), but could just as easily be the basis functions of any of the other classes. The convention for all the classes is that the coefficient \\(c[i]\\) goes with the basis function of degree i.", "All of the classes are immutable and have the same methods, and especially they implement the Python numeric operators +, -, *, //, %, divmod, **, ==, and !=. The last two can be a bit problematic due to floating point roundoff errors. We now give a quick demonstration of the various operations using NumPy version 1.7.0.", "First we need a polynomial class and a polynomial instance to play with. The classes can be imported directly from the polynomial package or from the module of the relevant type. Here we import from the package and use the conventional Polynomial class because of its familiarity:", "Note that there are three parts to the long version of the printout. The first is the coefficients, the second is the domain, and the third is the window:", "Printing a polynomial yields the polynomial expression in a more familiar format:", "Note that the string representation of polynomials uses Unicode characters by default (except on Windows) to express powers and subscripts. An ASCII-based representation is also available (default on Windows). The polynomial string format can be toggled at the package-level with the set_default_printstyle function:", "or controlled for individual polynomial instances with string formatting:", "We will deal with the domain and window when we get to fitting, for the moment we ignore them and run through the basic algebraic and arithmetic operations.", "Addition and Subtraction:", "Multiplication:", "Powers:", "Division:", "Floor division, \u2018//\u2019, is the division operator for the polynomial classes, polynomials are treated like integers in this regard. For Python versions < 3.x the \u2018/\u2019 operator maps to \u2018//\u2019, as it does for Python, for later versions the \u2018/\u2019 will only work for division by scalars. At some point it will be deprecated:", "Remainder:", "Divmod:", "Evaluation:", "Substitution:", "Substitute a polynomial for x and expand the result. Here we substitute p in itself leading to a new polynomial of degree 4 after expansion. If the polynomials are regarded as functions this is composition of functions:", "Roots:", "It isn\u2019t always convenient to explicitly use Polynomial instances, so tuples, lists, arrays, and scalars are automatically cast in the arithmetic operations:", "Polynomials that differ in domain, window, or class can\u2019t be mixed in arithmetic:", "But different types can be used for substitution. In fact, this is how conversion of Polynomial classes among themselves is done for type, domain, and window casting:", "Which gives the polynomial p in Chebyshev form. This works because \\(T_1(x) = x\\) and substituting \\(x\\) for \\(x\\) doesn\u2019t change the original polynomial. However, all the multiplications and divisions will be done using Chebyshev series, hence the type of the result.", "It is intended that all polynomial instances are immutable, therefore augmented operations (+=, -=, etc.) and any other functionality that would violate the immutablity of a polynomial instance are intentionally unimplemented.", "Polynomial instances can be integrated and differentiated.:", "The first example integrates p once, the second example integrates it twice. By default, the lower bound of the integration and the integration constant are 0, but both can be specified.:", "In the first case the lower bound of the integration is set to -1 and the integration constant is 0. In the second the constant of integration is set to 1 as well. Differentiation is simpler since the only option is the number of times the polynomial is differentiated:", "Constructing polynomials by specifying coefficients is just one way of obtaining a polynomial instance, they may also be created by specifying their roots, by conversion from other polynomial types, and by least squares fits. Fitting is discussed in its own section, the other methods are demonstrated below:", "The convert method can also convert domain and window:", "In numpy versions >= 1.7.0 the basis and cast class methods are also available. The cast method works like the convert method while the basis method returns the basis polynomial of given degree:", "Conversions between types can be useful, but it is not recommended for routine use. The loss of numerical precision in passing from a Chebyshev series of degree 50 to a Polynomial series of the same degree can make the results of numerical evaluation essentially random.", "Fitting is the reason that the domain and window attributes are part of the convenience classes. To illustrate the problem, the values of the Chebyshev polynomials up to degree 5 are plotted below.", "In the range -1 <= x <= 1 they are nice, equiripple functions lying between +/- 1. The same plots over the range -2 <= x <= 2 look very different:", "As can be seen, the \u201cgood\u201d parts have shrunk to insignificance. In using Chebyshev polynomials for fitting we want to use the region where x is between -1 and 1 and that is what the window specifies. However, it is unlikely that the data to be fit has all its data points in that interval, so we use domain to specify the interval where the data points lie. When the fit is done, the domain is first mapped to the window by a linear transformation and the usual least squares fit is done using the mapped data points. The window and domain of the fit are part of the returned series and are automatically used when computing values, derivatives, and such. If they aren\u2019t specified in the call the fitting routine will use the default window and the smallest domain that holds all the data points. This is illustrated below for a fit to a noisy sine curve."]}, {"name": "Using via cmake", "path": "f2py/buildtools/cmake", "type": "Using via \n        \n         cmake", "text": ["In terms of complexity, cmake falls between make and meson. The learning curve is steeper since CMake syntax is not pythonic and is closer to make with environment variables.", "However, the trade-off is enhanced flexibility and support for most architectures and compilers. An introduction to the syntax is out of scope for this document, but this extensive CMake collection of resources is great.", "Note", "cmake is very popular for mixed-language systems, however support for f2py is not particularly native or pleasant; and a more natural approach is to consider Using via scikit-build", "Returning to the fib example from Three ways to wrap - getting started section.", "We do not need to explicitly generate the python -m numpy.f2py fib1.f output, which is fib1module.c, which is beneficial. With this; we can now initialize a CMakeLists.txt file as follows:", "A key element of the CMakeLists.txt file defined above is that the add_custom_command is used to generate the wrapper C files and then added as a dependency of the actual shared library target via a add_custom_target directive which prevents the command from running every time. Additionally, the method used for obtaining the fortranobject.c file can also be used to grab the numpy headers on older cmake versions.", "This then works in the same manner as the other modules, although the naming conventions are different and the output library is not automatically prefixed with the cython information.", "This is particularly useful where an existing toolchain already exists and scikit-build or other additional python dependencies are discouraged."]}, {"name": "Using via meson", "path": "f2py/buildtools/meson", "type": "Using via \n        \n         meson", "text": ["The key advantage gained by leveraging meson over the techniques described in Using via numpy.distutils is that this feeds into existing systems and larger projects with ease. meson has a rather pythonic syntax which makes it more comfortable and amenable to extension for python users.", "Note", "Meson needs to be at-least 0.46.0 in order to resolve the python include directories.", "We will need the generated C wrapper before we can use a general purpose build system like meson. We will acquire this by:", "Now, consider the following meson.build file for the fib and scalar examples from Three ways to wrap - getting started section:", "At this point the build will complete, but the import will fail:", "Recall that the original example, as reproduced below, was in SCREAMCASE:", "With the standard approach, the subroutine exposed to python is fib and not FIB. This means we have a few options. One approach (where possible) is to lowercase the original Fortran file with say:", "However this requires the ability to modify the source which is not always possible. The easiest way to solve this is to let f2py deal with it:", "A major pain point in the workflow defined above, is the manual tracking of inputs. Although it would require more effort to figure out the actual outputs for reasons discussed in F2PY and Build Systems.", "However, we can augment our workflow in a straightforward to take into account files for which the outputs are known when the build system is set up.", "This can be compiled and run as before.", "It is worth keeping in mind the following:"]}, {"name": "Using via scikit-build", "path": "f2py/buildtools/skbuild", "type": "Using via \n        \n         scikit-build", "text": ["scikit-build provides two separate concepts geared towards the users of Python extension modules.", "Note", "It is possible to use scikit-build\u2019s cmake modules to bypass the cmake setup mechanism completely, and to write targets which call f2py\n-c. This usage is not recommended since the point of these build system documents are to move away from the internal numpy.distutils methods.", "For situations where no setuptools replacements are required or wanted (i.e. if wheels are not needed), it is recommended to instead use the vanilla cmake setup described in Using via cmake.", "We will consider the fib example from Three ways to wrap - getting started section.", "Consider using the following CMakeLists.txt.", "Much of the logic is the same as in Using via cmake, however notably here the appropriate module suffix is generated via sysconfig.get_config_var(\"SO\"). The resulting extension can be built and loaded in the standard workflow.", "Note", "As of November 2021", "The behavior described here of driving the cmake build of a module is considered to be legacy behaviour and should not be depended on.", "The utility of scikit-build lies in being able to drive the generation of more than extension modules, in particular a common usage pattern is the generation of Python distributables (for example for PyPI).", "The workflow with scikit-build straightforwardly supports such packaging requirements. Consider augmenting the project with a setup.py as defined:", "Along with a commensurate pyproject.toml", "Together these can build the extension using cmake in tandem with other standard setuptools outputs. Running cmake through setup.py is mostly used when it is necessary to integrate with extension modules not built with cmake.", "Where we have modified the path to the module as --inplace places the extension module in a subfolder."]}, {"name": "vectorize.__call__()", "path": "reference/generated/numpy.vectorize.__call__", "type": "numpy.vectorize.__call__", "text": ["method", "Return arrays with the results of pyfunc broadcast (vectorized) over args and kwargs not in excluded."]}, {"name": "version", "path": "reference/arrays.interface", "type": "The Array Interface", "text": ["Note", "This page describes the numpy-specific API for accessing the contents of a numpy array from other C extensions. PEP 3118 \u2013 The Revised Buffer Protocol introduces similar, standardized API to Python 2.6 and 3.0 for any extension module to use. Cython\u2019s buffer array support uses the PEP 3118 API; see the Cython numpy tutorial. Cython provides a way to write code that supports the buffer protocol with Python versions older than 2.6 because it has a backward-compatible implementation utilizing the array interface described here.", "3", "The array interface (sometimes called array protocol) was created in 2005 as a means for array-like Python objects to re-use each other\u2019s data buffers intelligently whenever possible. The homogeneous N-dimensional array interface is a default mechanism for objects to share N-dimensional array memory and information. The interface consists of a Python-side and a C-side using two attributes. Objects wishing to be considered an N-dimensional array in application code should support at least one of these attributes. Objects wishing to support an N-dimensional array in application code should look for at least one of these attributes and use the information provided appropriately.", "This interface describes homogeneous arrays in the sense that each item of the array has the same \u201ctype\u201d. This type can be very simple or it can be a quite arbitrary and complicated C-like structure.", "There are two ways to use the interface: A Python side and a C-side. Both are separate attributes.", "This approach to the interface consists of the object having an __array_interface__ attribute.", "A dictionary of items (3 required and 5 optional). The optional keys in the dictionary have implied defaults if they are not provided.", "The keys are:", "Tuple whose elements are the array size in each dimension. Each entry is an integer (a Python int). Note that these integers could be larger than the platform int or long could hold (a Python int is a C long). It is up to the code using this attribute to handle this appropriately; either by raising an error when overflow is possible, or by using long long as the C type for the shapes.", "A string providing the basic type of the homogeneous array The basic string format consists of 3 parts: a character describing the byteorder of the data (<: little-endian, >: big-endian, |: not-relevant), a character code giving the basic type of the array, and an integer providing the number of bytes the type uses.", "The basic type character codes are:", "t", "Bit field (following integer gives the number of bits in the bit field).", "b", "Boolean (integer type where all values are only True or False)", "i", "Integer", "u", "Unsigned integer", "f", "Floating point", "c", "Complex floating point", "m", "Timedelta", "M", "Datetime", "O", "Object (i.e. the memory contains a pointer to PyObject)", "S", "String (fixed-length sequence of char)", "U", "Unicode (fixed-length sequence of Py_UNICODE)", "V", "Other (void * \u2013 each item is a fixed-size chunk of memory)", "A list of tuples providing a more detailed description of the memory layout for each item in the homogeneous array. Each tuple in the list has two or three elements. Normally, this attribute would be used when typestr is V[0-9]+, but this is not a requirement. The only requirement is that the number of bytes represented in the typestr key is the same as the total number of bytes represented here. The idea is to support descriptions of C-like structs that make up array elements. The elements of each tuple in the list are", "Default: [('', typestr)]", "A 2-tuple whose first argument is an integer (a long integer if necessary) that points to the data-area storing the array contents. This pointer must point to the first element of data (in other words any offset is always ignored in this case). The second entry in the tuple is a read-only flag (true means the data area is read-only).", "This attribute can also be an object exposing the buffer interface which will be used to share the data. If this key is not present (or returns None), then memory sharing will be done through the buffer interface of the object itself. In this case, the offset key can be used to indicate the start of the buffer. A reference to the object exposing the array interface must be stored by the new object if the memory area is to be secured.", "Default: None", "Either None to indicate a C-style contiguous array or a Tuple of strides which provides the number of bytes needed to jump to the next array element in the corresponding dimension. Each entry must be an integer (a Python int). As with shape, the values may be larger than can be represented by a C int or long; the calling code should handle this appropriately, either by raising an error, or by using long long in C. The default is None which implies a C-style contiguous memory buffer. In this model, the last dimension of the array varies the fastest. For example, the default strides tuple for an object whose array entries are 8 bytes long and whose shape is (10, 20, 30) would be (4800, 240, 8)", "Default: None (C-style contiguous)", "None or an object exposing the array interface. All elements of the mask array should be interpreted only as true or not true indicating which elements of this array are valid. The shape of this object should be \u201cbroadcastable\u201d to the shape of the original array.", "Default: None (All array values are valid)", "An integer offset into the array data region. This can only be used when data is None or returns a buffer object.", "Default: 0.", "An integer showing the version of the interface (i.e. 3 for this version). Be careful not to use this to invalidate objects exposing future versions of the interface.", "This approach to the array interface allows for faster access to an array using only one attribute lookup and a well-defined C-structure.", "A PyCapsule whose pointer member contains a pointer to a filled PyArrayInterface structure. Memory for the structure is dynamically created and the PyCapsule is also created with an appropriate destructor so the retriever of this attribute simply has to apply Py_DECREF to the object returned by this attribute when it is finished. Also, either the data needs to be copied out, or a reference to the object exposing this attribute must be held to ensure the data is not freed. Objects exposing the __array_struct__ interface must also not reallocate their memory if other objects are referencing them.", "The PyArrayInterface structure is defined in numpy/ndarrayobject.h as:", "The flags member may consist of 5 bits showing how the data should be interpreted and one bit showing how the Interface should be interpreted. The data-bits are NPY_ARRAY_C_CONTIGUOUS (0x1), NPY_ARRAY_F_CONTIGUOUS (0x2), NPY_ARRAY_ALIGNED (0x100), NPY_ARRAY_NOTSWAPPED (0x200), and NPY_ARRAY_WRITEABLE (0x400). A final flag NPY_ARR_HAS_DESCR (0x800) indicates whether or not this structure has the arrdescr field. The field should not be accessed unless this flag is present.", "New since June 16, 2006:", "In the past most implementations used the desc member of the PyCObject (now PyCapsule) itself (do not confuse this with the \u201cdescr\u201d member of the PyArrayInterface structure above \u2014 they are two separate things) to hold the pointer to the object exposing the interface. This is now an explicit part of the interface. Be sure to take a reference to the object and call PyCapsule_SetContext before returning the PyCapsule, and configure a destructor to decref this reference.", "For clarity it is useful to provide some examples of the type description and corresponding __array_interface__ \u2018descr\u2019 entries. Thanks to Scott Gilbert for these examples:", "In every case, the \u2018descr\u2019 key is optional, but of course provides more information which may be important for various applications:", "It should be clear that any structured type could be described using this interface.", "The version 2 interface was very similar. The differences were largely aesthetic. In particular:", "The context member of the PyCapsule (formally the desc member of the PyCObject) returned from __array_struct__ was not specified. Usually, it was the object exposing the array (so that a reference to it could be kept and destroyed when the C-object was destroyed). It is now an explicit requirement that this field be used in some way to hold a reference to the owning object.", "Note", "Until August 2020, this said:", "Now it must be a tuple whose first element is a string with \u201cPyArrayInterface Version #\u201d and whose second element is the object exposing the array.", "This design was retracted almost immediately after it was proposed, in <https://mail.python.org/pipermail/numpy-discussion/2006-June/020995.html>. Despite 14 years of documentation to the contrary, at no point was it valid to assume that __array_interface__ capsules held this tuple content.", "There was no __array_interface__ attribute instead all of the keys (except for version) in the __array_interface__ dictionary were their own attribute: Thus to obtain the Python-side information you had to access separately the attributes:"]}, {"name": "void **data", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.data", "type": "Python Types and C-Structures", "text": ["Extra data to be passed to the 1-d vector loops or NULL if no extra-data is needed. This C-array must be the same size ( i.e. ntypes) as the functions array. NULL is used if extra_data is not needed. Several C-API calls for UFuncs are just 1-d vector loops that make use of this extra data to receive a pointer to the actual function to call."]}, {"name": "void *data", "path": "reference/c-api/types-and-structures#c.PyArrayInterface.data", "type": "Python Types and C-Structures", "text": ["A pointer to the first element of the array."]}, {"name": "void *ptr", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.ptr", "type": "Python Types and C-Structures", "text": ["Any dynamically allocated memory. Currently, this is used for dynamic ufuncs created from a python function to store room for the types, data, and name members."]}, {"name": "void *ptr", "path": "reference/c-api/types-and-structures#c.PyArray_Chunk.ptr", "type": "Python Types and C-Structures", "text": ["A pointer to the start of the single-segment chunk of memory."]}, {"name": "void *PyArray_DATA()", "path": "reference/c-api/array#c.PyArray_DATA", "type": "Array API", "text": []}, {"name": "void *PyArray_GETPTR1()", "path": "reference/c-api/array#c.PyArray_GETPTR1", "type": "Array API", "text": []}, {"name": "void *PyArray_GETPTR2()", "path": "reference/c-api/array#c.PyArray_GETPTR2", "type": "Array API", "text": []}, {"name": "void *PyArray_GETPTR3()", "path": "reference/c-api/array#c.PyArray_GETPTR3", "type": "Array API", "text": []}, {"name": "void *PyArray_GETPTR4()", "path": "reference/c-api/array#c.PyArray_GETPTR4", "type": "Array API", "text": ["Quick, inline access to the element at the given coordinates in the ndarray, obj, which must have respectively 1, 2, 3, or 4 dimensions (this is not checked). The corresponding i, j, k, and l coordinates can be any integer but will be interpreted as npy_intp. You may want to typecast the returned pointer to the data type of the ndarray."]}, {"name": "void *PyArray_ITER_DATA()", "path": "reference/c-api/array#c.PyArray_ITER_DATA", "type": "Array API", "text": ["A pointer to the current element of the array."]}, {"name": "void *PyArray_malloc()", "path": "reference/c-api/array#c.PyArray_malloc", "type": "Array API", "text": []}, {"name": "void *PyArray_MultiIter_DATA()", "path": "reference/c-api/array#c.PyArray_MultiIter_DATA", "type": "Array API", "text": ["Return the data-pointer of the i \\(^{\\textrm{th}}\\) iterator in a multi-iterator object."]}, {"name": "void *PyArray_realloc()", "path": "reference/c-api/array#c.PyArray_realloc", "type": "Array API", "text": ["These macros use different memory allocators, depending on the constant NPY_USE_PYMEM. The system malloc is used when NPY_USE_PYMEM is 0, if NPY_USE_PYMEM is 1, then the Python memory allocator is used."]}, {"name": "void *reserved2", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.reserved2", "type": "Python Types and C-Structures", "text": ["For a possible future loop selector with a different signature."]}, {"name": "void castfunc()", "path": "user/c-info.beyond-basics", "type": "User Guide", "text": ["One common algorithmic requirement is to be able to walk over all elements in a multidimensional array. The array iterator object makes this easy to do in a generic way that works for arrays of any dimension. Naturally, if you know the number of dimensions you will be using, then you can always write nested for loops to accomplish the iteration. If, however, you want to write code that works with any number of dimensions, then you can make use of the array iterator. An array iterator object is returned when accessing the .flat attribute of an array.", "Basic usage is to call PyArray_IterNew ( array ) where array is an ndarray object (or one of its sub-classes). The returned object is an array-iterator object (the same object returned by the .flat attribute of the ndarray). This object is usually cast to PyArrayIterObject* so that its members can be accessed. The only members that are needed are iter->size which contains the total size of the array, iter->index, which contains the current 1-d index into the array, and iter->dataptr which is a pointer to the data for the current element of the array. Sometimes it is also useful to access iter->ao which is a pointer to the underlying ndarray object.", "After processing data at the current element of the array, the next element of the array can be obtained using the macro PyArray_ITER_NEXT ( iter ). The iteration always proceeds in a C-style contiguous fashion (last index varying the fastest). The PyArray_ITER_GOTO ( iter, destination ) can be used to jump to a particular point in the array, where destination is an array of npy_intp data-type with space to handle at least the number of dimensions in the underlying array. Occasionally it is useful to use PyArray_ITER_GOTO1D ( iter, index ) which will jump to the 1-d index given by the value of index. The most common usage, however, is given in the following example.", "You can also use PyArrayIter_Check ( obj ) to ensure you have an iterator object and PyArray_ITER_RESET ( iter ) to reset an iterator object back to the beginning of the array.", "It should be emphasized at this point that you may not need the array iterator if your array is already contiguous (using an array iterator will work but will be slower than the fastest code you could write). The major purpose of array iterators is to encapsulate iteration over N-dimensional arrays with arbitrary strides. They are used in many, many places in the NumPy source code itself. If you already know your array is contiguous (Fortran or C), then simply adding the element- size to a running pointer variable will step you through the array very efficiently. In other words, code like this will probably be faster for you in the contiguous case (assuming doubles).", "A common algorithm is to loop over all elements of an array and perform some function with each element by issuing a function call. As function calls can be time consuming, one way to speed up this kind of algorithm is to write the function so it takes a vector of data and then write the iteration so the function call is performed for an entire dimension of data at a time. This increases the amount of work done per function call, thereby reducing the function-call over-head to a small(er) fraction of the total time. Even if the interior of the loop is performed without a function call it can be advantageous to perform the inner loop over the dimension with the highest number of elements to take advantage of speed enhancements available on micro- processors that use pipelining to enhance fundamental operations.", "The PyArray_IterAllButAxis ( array, &dim ) constructs an iterator object that is modified so that it will not iterate over the dimension indicated by dim. The only restriction on this iterator object, is that the PyArray_ITER_GOTO1D ( it, ind ) macro cannot be used (thus flat indexing won\u2019t work either if you pass this object back to Python \u2014 so you shouldn\u2019t do this). Note that the returned object from this routine is still usually cast to PyArrayIterObject *. All that\u2019s been done is to modify the strides and dimensions of the returned iterator to simulate iterating over array[\u2026,0,\u2026] where 0 is placed on the \\(\\textrm{dim}^{\\textrm{th}}\\) dimension. If dim is negative, then the dimension with the largest axis is found and used.", "Very often, it is desirable to iterate over several arrays at the same time. The universal functions are an example of this kind of behavior. If all you want to do is iterate over arrays with the same shape, then simply creating several iterator objects is the standard procedure. For example, the following code iterates over two arrays assumed to be the same shape and size (actually obj1 just has to have at least as many total elements as does obj2):", "When multiple arrays are involved in an operation, you may want to use the same broadcasting rules that the math operations (i.e. the ufuncs) use. This can be done easily using the PyArrayMultiIterObject. This is the object returned from the Python command numpy.broadcast and it is almost as easy to use from C. The function PyArray_MultiIterNew ( n, ... ) is used (with n input objects in place of ... ). The input objects can be arrays or anything that can be converted into an array. A pointer to a PyArrayMultiIterObject is returned. Broadcasting has already been accomplished which adjusts the iterators so that all that needs to be done to advance to the next element in each array is for PyArray_ITER_NEXT to be called for each of the inputs. This incrementing is automatically performed by PyArray_MultiIter_NEXT ( obj ) macro (which can handle a multiterator obj as either a PyArrayMultiIterObject* or a PyObject*). The data from input number i is available using PyArray_MultiIter_DATA ( obj, i ). An example of using this feature follows.", "The function PyArray_RemoveSmallest ( multi ) can be used to take a multi-iterator object and adjust all the iterators so that iteration does not take place over the largest dimension (it makes that dimension of size 1). The code being looped over that makes use of the pointers will very-likely also need the strides data for each of the iterators. This information is stored in multi->iters[i]->strides.", "There are several examples of using the multi-iterator in the NumPy source code as it makes N-dimensional broadcasting-code very simple to write. Browse the source for more examples.", "NumPy comes with 24 builtin data-types. While this covers a large majority of possible use cases, it is conceivable that a user may have a need for an additional data-type. There is some support for adding an additional data-type into the NumPy system. This additional data- type will behave much like a regular data-type except ufuncs must have 1-d loops registered to handle it separately. Also checking for whether or not other data-types can be cast \u201csafely\u201d to and from this new type or not will always return \u201ccan cast\u201d unless you also register which types your new data-type can be cast to and from.", "The NumPy source code includes an example of a custom data-type as part of its test suite. The file _rational_tests.c.src in the source code directory numpy/numpy/core/src/umath/ contains an implementation of a data-type that represents a rational number as the ratio of two 32 bit integers.", "To begin to make use of the new data-type, you need to first define a new Python type to hold the scalars of your new data-type. It should be acceptable to inherit from one of the array scalars if your new type has a binary compatible layout. This will allow your new data type to have the methods and attributes of array scalars. New data- types must have a fixed memory size (if you want to define a data-type that needs a flexible representation, like a variable-precision number, then use a pointer to the object as the data-type). The memory layout of the object structure for the new Python type must be PyObject_HEAD followed by the fixed-size memory needed for the data- type. For example, a suitable structure for the new Python type is:", "After you have defined a new Python type object, you must then define a new PyArray_Descr structure whose typeobject member will contain a pointer to the data-type you\u2019ve just defined. In addition, the required functions in the \u201c.f\u201d member must be defined: nonzero, copyswap, copyswapn, setitem, getitem, and cast. The more functions in the \u201c.f\u201d member you define, however, the more useful the new data-type will be. It is very important to initialize unused functions to NULL. This can be achieved using PyArray_InitArrFuncs (f).", "Once a new PyArray_Descr structure is created and filled with the needed information and useful functions you call PyArray_RegisterDataType (new_descr). The return value from this call is an integer providing you with a unique type_number that specifies your data-type. This type number should be stored and made available by your module so that other modules can use it to recognize your data-type (the other mechanism for finding a user-defined data-type number is to search based on the name of the type-object associated with the data-type using PyArray_TypeNumFromName ).", "You may want to allow builtin (and other user-defined) data-types to be cast automatically to your data-type. In order to make this possible, you must register a casting function with the data-type you want to be able to cast from. This requires writing low-level casting functions for each conversion you want to support and then registering these functions with the data-type descriptor. A low-level casting function has the signature.", "Cast n elements from one type to another. The data to cast from is in a contiguous, correctly-swapped and aligned chunk of memory pointed to by from. The buffer to cast to is also contiguous, correctly-swapped and aligned. The fromarr and toarr arguments should only be used for flexible-element-sized arrays (string, unicode, void).", "An example castfunc is:", "This could then be registered to convert doubles to floats using the code:", "By default, all user-defined data-types are not presumed to be safely castable to any builtin data-types. In addition builtin data-types are not presumed to be safely castable to user-defined data-types. This situation limits the ability of user-defined data-types to participate in the coercion system used by ufuncs and other times when automatic coercion takes place in NumPy. This can be changed by registering data-types as safely castable from a particular data-type object. The function PyArray_RegisterCanCast (from_descr, totype_number, scalarkind) should be used to specify that the data-type object from_descr can be cast to the data-type with type number totype_number. If you are not trying to alter scalar coercion rules, then use NPY_NOSCALAR for the scalarkind argument.", "If you want to allow your new data-type to also be able to share in the scalar coercion rules, then you need to specify the scalarkind function in the data-type object\u2019s \u201c.f\u201d member to return the kind of scalar the new data-type should be seen as (the value of the scalar is available to that function). Then, you can register data-types that can be cast to separately for each scalar kind that may be returned from your user-defined data-type. If you don\u2019t register scalar coercion handling, then all of your user-defined data-types will be seen as NPY_NOSCALAR.", "You may also want to register low-level ufunc loops for your data-type so that an ndarray of your data-type can have math applied to it seamlessly. Registering a new loop with exactly the same arg_types signature, silently replaces any previously registered loops for that data-type.", "Before you can register a 1-d loop for a ufunc, the ufunc must be previously created. Then you call PyUFunc_RegisterLoopForType (\u2026) with the information needed for the loop. The return value of this function is 0 if the process was successful and -1 with an error condition set if it was not successful.", "One of the lesser-used features that has been lurking in Python since 2.2 is the ability to sub-class types in C. This facility is one of the important reasons for basing NumPy off of the Numeric code-base which was already in C. A sub-type in C allows much more flexibility with regards to memory management. Sub-typing in C is not difficult even if you have only a rudimentary understanding of how to create new types for Python. While it is easiest to sub-type from a single parent type, sub-typing from multiple parent types is also possible. Multiple inheritance in C is generally less useful than it is in Python because a restriction on Python sub-types is that they have a binary compatible memory layout. Perhaps for this reason, it is somewhat easier to sub-type from a single parent type.", "All C-structures corresponding to Python objects must begin with PyObject_HEAD (or PyObject_VAR_HEAD). In the same way, any sub-type must have a C-structure that begins with exactly the same memory layout as the parent type (or all of the parent types in the case of multiple-inheritance). The reason for this is that Python may attempt to access a member of the sub-type structure as if it had the parent structure ( i.e. it will cast a given pointer to a pointer to the parent structure and then dereference one of it\u2019s members). If the memory layouts are not compatible, then this attempt will cause unpredictable behavior (eventually leading to a memory violation and program crash).", "One of the elements in PyObject_HEAD is a pointer to a type-object structure. A new Python type is created by creating a new type-object structure and populating it with functions and pointers to describe the desired behavior of the type. Typically, a new C-structure is also created to contain the instance-specific information needed for each object of the type as well. For example, &PyArray_Type is a pointer to the type-object table for the ndarray while a PyArrayObject* variable is a pointer to a particular instance of an ndarray (one of the members of the ndarray structure is, in turn, a pointer to the type- object table &PyArray_Type). Finally PyType_Ready (<pointer_to_type_object>) must be called for every new Python type.", "To create a sub-type, a similar procedure must be followed except only behaviors that are different require new entries in the type- object structure. All other entries can be NULL and will be filled in by PyType_Ready with appropriate functions from the parent type(s). In particular, to create a sub-type in C follow these steps:", "If needed create a new C-structure to handle each instance of your type. A typical C-structure would be:", "Notice that the full PyArrayObject is used as the first entry in order to ensure that the binary layout of instances of the new type is identical to the PyArrayObject.", "More information on creating sub-types in C can be learned by reading PEP 253 (available at https://www.python.org/dev/peps/pep-0253).", "Some special methods and attributes are used by arrays in order to facilitate the interoperation of sub-types with the base ndarray type.", "Several array-creation functions of the ndarray allow specification of a particular sub-type to be created. This allows sub-types to be handled seamlessly in many routines. When a sub-type is created in such a fashion, however, neither the __new__ method nor the __init__ method gets called. Instead, the sub-type is allocated and the appropriate instance-structure members are filled in. Finally, the __array_finalize__ attribute is looked-up in the object dictionary. If it is present and not None, then it can be either a CObject containing a pointer to a PyArray_FinalizeFunc or it can be a method taking a single argument (which could be None)", "If the __array_finalize__ attribute is a CObject, then the pointer must be a pointer to a function with the signature:", "The first argument is the newly created sub-type. The second argument (if not NULL) is the \u201cparent\u201d array (if the array was created using slicing or some other operation where a clearly-distinguishable parent is present). This routine can do anything it wants to. It should return a -1 on error and 0 otherwise.", "If the __array_finalize__ attribute is not None nor a CObject, then it must be a Python method that takes the parent array as an argument (which could be None if there is no parent), and returns nothing. Errors in this method will be caught and handled.", "This attribute allows simple but flexible determination of which sub- type should be considered \u201cprimary\u201d when an operation involving two or more sub-types arises. In operations where different sub-types are being used, the sub-type with the largest __array_priority__ attribute will determine the sub-type of the output(s). If two sub- types have the same __array_priority__ then the sub-type of the first argument determines the output. The default __array_priority__ attribute returns a value of 0.0 for the base ndarray type and 1.0 for a sub-type. This attribute can also be defined by objects that are not sub-types of the ndarray and can be used to determine which __array_wrap__ method should be called for the return output.", "Any class or type can define this method which should take an ndarray argument and return an instance of the type. It can be seen as the opposite of the __array__ method. This method is used by the ufuncs (and other NumPy functions) to allow other objects to pass through. For Python >2.4, it can also be used to write a decorator that converts a function that works only with ndarrays to one that works with any type with __array__ and __array_wrap__ methods."]}, {"name": "void copyswap()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.copyswap", "type": "Python Types and C-Structures", "text": ["These members are both pointers to functions to copy data from src to dest and swap if indicated. The value of arr is only used for flexible ( NPY_STRING, NPY_UNICODE, and NPY_VOID ) arrays (and is obtained from arr->descr->elsize ). The second function copies a single value, while the first loops over n values with the provided strides. These functions can deal with misbehaved src data. If src is NULL then no copy is performed. If swap is 0, then no byteswapping occurs. It is assumed that dest and src do not overlap. If they overlap, then use memmove (\u2026) first followed by copyswap(n) with NULL valued src."]}, {"name": "void copyswapn()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.copyswapn", "type": "Python Types and C-Structures", "text": []}, {"name": "void dotfunc()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.dotfunc", "type": "Python Types and C-Structures", "text": ["A pointer to a function that multiplies two n -length sequences together, adds them, and places the result in element pointed to by op of arr. The start of the two sequences are pointed to by ip1 and ip2. To get to the next element in each sequence requires a jump of is1 and is2 bytes, respectively. This function requires behaved (though not necessarily contiguous) memory."]}, {"name": "void fastclip()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.fastclip", "type": "Python Types and C-Structures", "text": ["Deprecated since version 1.17: The use of this function will give a deprecation warning when np.clip. Instead of this function, the datatype must instead use PyUFunc_RegisterLoopForDescr to attach a custom loop to np.core.umath.clip, np.minimum, and np.maximum.", "Deprecated since version 1.19: Setting this function is deprecated and should always be NULL, if set, it will be ignored.", "A function that reads n_in items from in, and writes to out the read value if it is within the limits pointed to by min and max, or the corresponding limit if outside. The memory segments must be contiguous and behaved, and either min or max may be NULL, but not both."]}, {"name": "void fastputmask()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.fastputmask", "type": "Python Types and C-Structures", "text": ["Deprecated since version 1.19: Setting this function is deprecated and should always be NULL, if set, it will be ignored.", "A function that takes a pointer in to an array of n_in items, a pointer mask to an array of n_in boolean values, and a pointer vals to an array of nv items. Items from vals are copied into in wherever the value in mask is non-zero, tiling vals as needed if nv < n_in. All arrays must be contiguous and behaved."]}, {"name": "void fasttake()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.fasttake", "type": "Python Types and C-Structures", "text": ["Deprecated since version 1.19: Setting this function is deprecated and should always be NULL, if set, it will be ignored.", "A function that takes a pointer src to a C contiguous, behaved segment, interpreted as a 3-dimensional array of shape (n_outer, nindarray, nelem), a pointer indarray to a contiguous, behaved segment of m_middle integer indices, and a pointer dest to a C contiguous, behaved segment, interpreted as a 3-dimensional array of shape (n_outer, m_middle, nelem). The indices in indarray are used to index src along the second dimension, and copy the corresponding chunks of nelem items into dest. clipmode (which can take on the values NPY_RAISE, NPY_WRAP or NPY_CLIP) determines how will indices smaller than 0 or larger than nindarray will be handled."]}, {"name": "void fill()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.fill", "type": "Python Types and C-Structures", "text": ["A pointer to a function that fills a contiguous array of given length with data. The first two elements of the array must already be filled- in. From these two values, a delta will be computed and the values from item 3 to the end will be computed by repeatedly adding this computed delta. The data buffer must be well-behaved."]}, {"name": "void fillwithscalar()", "path": "reference/c-api/types-and-structures#c.PyArray_ArrFuncs.fillwithscalar", "type": "Python Types and C-Structures", "text": ["A pointer to a function that fills a contiguous buffer of the given length with a single scalar value whose address is given. The final argument is the array which is needed to get the itemsize for variable-length arrays."]}, {"name": "void functions()", "path": "reference/c-api/types-and-structures#c.PyUFuncObject.functions", "type": "Python Types and C-Structures", "text": ["An array of function pointers \u2014 one for each data type supported by the ufunc. This is the vector loop that is called to implement the underlying function dims [0] times. The first argument, args, is an array of nargs pointers to behaved memory. Pointers to the data for the input arguments are first, followed by the pointers to the data for the output arguments. How many bytes must be skipped to get to the next element in the sequence is specified by the corresponding entry in the steps array. The last argument allows the loop to receive extra information. This is commonly used so that a single, generic vector loop can be used for multiple functions. In this case, the actual scalar function to call is passed in as extradata. The size of this function pointer array is ntypes."]}, {"name": "void import_ufunc()", "path": "reference/c-api/ufunc#c.import_ufunc", "type": "UFunc API", "text": ["These are the constants and functions for accessing the ufunc C-API from extension modules in precisely the same way as the array C-API can be accessed. The import_ufunc () function must always be called (in the initialization subroutine of the extension module). If your extension module is in one file then that is all that is required. The other two constants are useful if your extension module makes use of multiple files. In that case, define PY_UFUNC_UNIQUE_SYMBOL to something unique to your code and then in source files that do not contain the module initialization function but still need access to the UFUNC API, define PY_UFUNC_UNIQUE_SYMBOL to the same name used previously and also define NO_IMPORT_UFUNC.", "The C-API is actually an array of function pointers. This array is created (and pointed to by a global variable) by import_ufunc. The global variable is either statically defined or allowed to be seen by other files depending on the state of PY_UFUNC_UNIQUE_SYMBOL and NO_IMPORT_UFUNC."]}, {"name": "void NPY_AUXDATA_FREE()", "path": "reference/c-api/array#c.NPY_AUXDATA_FREE", "type": "Array API", "text": ["A macro which calls the auxdata\u2019s free function appropriately, does nothing if auxdata is NULL."]}, {"name": "void NPY_BEGIN_THREADS_DESCR()", "path": "reference/c-api/array#c.NPY_BEGIN_THREADS_DESCR", "type": "Array API", "text": ["Useful to release the GIL only if dtype does not contain arbitrary Python objects which may need the Python interpreter during execution of the loop."]}, {"name": "void NPY_BEGIN_THREADS_THRESHOLDED()", "path": "reference/c-api/array#c.NPY_BEGIN_THREADS_THRESHOLDED", "type": "Array API", "text": ["Useful to release the GIL only if loop_size exceeds a minimum threshold, currently set to 500. Should be matched with a NPY_END_THREADS to regain the GIL."]}, {"name": "void NPY_END_THREADS_DESCR()", "path": "reference/c-api/array#c.NPY_END_THREADS_DESCR", "type": "Array API", "text": ["Useful to regain the GIL in situations where it was released using the BEGIN form of this macro."]}, {"name": "void npy_set_floatstatus_divbyzero()", "path": "reference/c-api/coremath#c.npy_set_floatstatus_divbyzero", "type": "NumPy core libraries", "text": ["Set the divide by zero floating point exception", "New in version 1.6.0."]}, {"name": "void npy_set_floatstatus_invalid()", "path": "reference/c-api/coremath#c.npy_set_floatstatus_invalid", "type": "NumPy core libraries", "text": ["Set the invalid floating point exception", "New in version 1.6.0."]}, {"name": "void npy_set_floatstatus_overflow()", "path": "reference/c-api/coremath#c.npy_set_floatstatus_overflow", "type": "NumPy core libraries", "text": ["Set the overflow floating point exception", "New in version 1.6.0."]}, {"name": "void npy_set_floatstatus_underflow()", "path": "reference/c-api/coremath#c.npy_set_floatstatus_underflow", "type": "NumPy core libraries", "text": ["Set the underflow floating point exception", "New in version 1.6.0."]}, {"name": "void NpyIter_GetInnerFixedStrideArray()", "path": "reference/c-api/iterator#c.NpyIter_GetInnerFixedStrideArray", "type": "Array Iterator API", "text": ["Gets an array of strides which are fixed, or will not change during the entire iteration. For strides that may change, the value NPY_MAX_INTP is placed in the stride.", "Once the iterator is prepared for iteration (after a reset if NPY_ITER_DELAY_BUFALLOC was used), call this to get the strides which may be used to select a fast inner loop function. For example, if the stride is 0, that means the inner loop can always load its value into a variable once, then use the variable throughout the loop, or if the stride equals the itemsize, a contiguous version for that operand may be used.", "This function may be safely called without holding the Python GIL."]}, {"name": "void NpyIter_GetIterIndexRange()", "path": "reference/c-api/iterator#c.NpyIter_GetIterIndexRange", "type": "Array Iterator API", "text": ["Gets the iterindex sub-range that is being iterated. If NPY_ITER_RANGED was not specified, this always returns the range [0, NpyIter_IterSize(iter))."]}, {"name": "void NpyIter_GetReadFlags()", "path": "reference/c-api/iterator#c.NpyIter_GetReadFlags", "type": "Array Iterator API", "text": ["Fills nop flags. Sets outreadflags[i] to 1 if op[i] can be read from, and to 0 if not."]}, {"name": "void NpyIter_GetWriteFlags()", "path": "reference/c-api/iterator#c.NpyIter_GetWriteFlags", "type": "Array Iterator API", "text": ["Fills nop flags. Sets outwriteflags[i] to 1 if op[i] can be written to, and to 0 if not."]}, {"name": "void PyArray_ArrayType()", "path": "reference/c-api/array#c.PyArray_ArrayType", "type": "Array API", "text": ["This function is superseded by PyArray_ResultType.", "This function works similarly to PyArray_ObjectType (\u2026) except it handles flexible arrays. The mintype argument can have an itemsize member and the outtype argument will have an itemsize member at least as big but perhaps bigger depending on the object op."]}, {"name": "void PyArray_CastScalarToCtype()", "path": "reference/c-api/array#c.PyArray_CastScalarToCtype", "type": "Array API", "text": ["Return the data (cast to the data type indicated by outcode) from the array-scalar, scalar, into the memory pointed to by ctypeptr (which must be large enough to handle the incoming memory)."]}, {"name": "void PyArray_CLEARFLAGS()", "path": "reference/c-api/array#c.PyArray_CLEARFLAGS", "type": "Array API", "text": ["New in version 1.7.", "Clears the specified array flags. This function does no validation, and assumes that you know what you\u2019re doing."]}, {"name": "void PyArray_DiscardWritebackIfCopy()", "path": "reference/c-api/array#c.PyArray_DiscardWritebackIfCopy", "type": "Array API", "text": ["If obj.flags has NPY_ARRAY_WRITEBACKIFCOPY or (deprecated) NPY_ARRAY_UPDATEIFCOPY, this function clears the flags, DECREF s obj->base and makes it writeable, and sets obj->base to NULL. In contrast to PyArray_DiscardWritebackIfCopy it makes no attempt to copy the data from obj->base This undoes PyArray_SetWritebackIfCopyBase. Usually this is called after an error when you are finished with obj, just before Py_DECREF(obj). It may be called multiple times, or with NULL input."]}, {"name": "void PyArray_ENABLEFLAGS()", "path": "reference/c-api/array#c.PyArray_ENABLEFLAGS", "type": "Array API", "text": ["New in version 1.7.", "Enables the specified array flags. This function does no validation, and assumes that you know what you\u2019re doing."]}, {"name": "void PyArray_FillObjectArray()", "path": "reference/c-api/array#c.PyArray_FillObjectArray", "type": "Array API", "text": ["Fill a newly created array with a single value obj at all locations in the structure with object data-types. No checking is performed but arr must be of data-type NPY_OBJECT and be single-segment and uninitialized (no previous objects in position). Use PyArray_XDECREF (arr) if you need to decrement all the items in the object array prior to calling this function."]}, {"name": "void PyArray_FILLWBYTE()", "path": "reference/c-api/array#c.PyArray_FILLWBYTE", "type": "Array API", "text": ["Fill the array pointed to by obj \u2014which must be a (subclass of) ndarray\u2014with the contents of val (evaluated as a byte). This macro calls memset, so obj must be contiguous."]}, {"name": "void PyArray_free()", "path": "reference/c-api/array#c.PyArray_free", "type": "Array API", "text": []}, {"name": "void PyArray_Item_INCREF()", "path": "reference/c-api/array#c.PyArray_Item_INCREF", "type": "Array API", "text": ["A function to INCREF all the objects at the location ptr according to the data-type dtype. If ptr is the start of a structured type with an object at any offset, then this will (recursively) increment the reference count of all object-like items in the structured type."]}, {"name": "void PyArray_Item_XDECREF()", "path": "reference/c-api/array#c.PyArray_Item_XDECREF", "type": "Array API", "text": ["A function to XDECREF all the object-like items at the location ptr as recorded in the data-type, dtype. This works recursively so that if dtype itself has fields with data-types that contain object-like items, all the object-like fields will be XDECREF 'd."]}, {"name": "void PyArray_ITER_GOTO()", "path": "reference/c-api/array#c.PyArray_ITER_GOTO", "type": "Array API", "text": ["Set the iterator index, dataptr, and coordinates members to the location in the array indicated by the N-dimensional c-array, destination, which must have size at least iterator ->nd_m1+1."]}, {"name": "void PyArray_ITER_GOTO1D()", "path": "reference/c-api/array#c.PyArray_ITER_GOTO1D", "type": "Array API", "text": ["Set the iterator index and dataptr to the location in the array indicated by the integer index which points to an element in the C-styled flattened array."]}, {"name": "void PyArray_ITER_NEXT()", "path": "reference/c-api/array#c.PyArray_ITER_NEXT", "type": "Array API", "text": ["Incremement the index and the dataptr members of the iterator to point to the next element of the array. If the array is not (C-style) contiguous, also increment the N-dimensional coordinates array."]}, {"name": "void PyArray_ITER_RESET()", "path": "reference/c-api/array#c.PyArray_ITER_RESET", "type": "Array API", "text": ["Reset an iterator to the beginning of the array."]}, {"name": "void PyArray_MapIterNext()", "path": "reference/c-api/array#c.PyArray_MapIterNext", "type": "Array API", "text": ["This function needs to update the state of the map iterator and point mit->dataptr to the memory-location of the next object.", "Note that this function never handles an extra operand but provides compatibility for an old (exposed) API."]}, {"name": "void PyArray_MapIterSwapAxes()", "path": "reference/c-api/array#c.PyArray_MapIterSwapAxes", "type": "Array API", "text": ["Swap the axes to or from their inserted form. MapIter always puts the advanced (array) indices first in the iteration. But if they are consecutive, it will insert/transpose them back before returning. This is stored as mit->consec != 0 (the place where they are inserted). For assignments, the opposite happens: the values to be assigned are transposed (getmap=1 instead of getmap=0). getmap=0 and getmap=1 undo the other operation."]}, {"name": "void PyArray_MultiIter_GOTO()", "path": "reference/c-api/array#c.PyArray_MultiIter_GOTO", "type": "Array API", "text": ["Advance each iterator in a multi-iterator object, multi, to the given \\(N\\) -dimensional destination where \\(N\\) is the number of dimensions in the broadcasted array."]}, {"name": "void PyArray_MultiIter_GOTO1D()", "path": "reference/c-api/array#c.PyArray_MultiIter_GOTO1D", "type": "Array API", "text": ["Advance each iterator in a multi-iterator object, multi, to the corresponding location of the index into the flattened broadcasted array."]}, {"name": "void PyArray_MultiIter_NEXT()", "path": "reference/c-api/array#c.PyArray_MultiIter_NEXT", "type": "Array API", "text": ["Advance each iterator in a multi-iterator object, multi, to its next (broadcasted) element."]}, {"name": "void PyArray_MultiIter_NEXTi()", "path": "reference/c-api/array#c.PyArray_MultiIter_NEXTi", "type": "Array API", "text": ["Advance the pointer of only the i \\(^{\\textrm{th}}\\) iterator."]}, {"name": "void PyArray_MultiIter_RESET()", "path": "reference/c-api/array#c.PyArray_MultiIter_RESET", "type": "Array API", "text": ["Reset all the iterators to the beginning in a multi-iterator object, multi."]}, {"name": "void PyArray_ScalarAsCtype()", "path": "reference/c-api/array#c.PyArray_ScalarAsCtype", "type": "Array API", "text": ["Return in ctypeptr a pointer to the actual value in an array scalar. There is no error checking so scalar must be an array-scalar object, and ctypeptr must have enough space to hold the correct type. For flexible-sized types, a pointer to the data is copied into the memory of ctypeptr, for all other types, the actual data is copied into the address pointed to by ctypeptr."]}, {"name": "void PyArray_SetStringFunction()", "path": "reference/c-api/array#c.PyArray_SetStringFunction", "type": "Array API", "text": ["This function allows you to alter the tp_str and tp_repr methods of the array object to any Python function. Thus you can alter what happens for all arrays when str(arr) or repr(arr) is called from Python. The function to be called is passed in as op. If repr is non-zero, then this function will be called in response to repr(arr), otherwise the function will be called in response to str(arr). No check on whether or not op is callable is performed. The callable passed in to op should expect an array argument and should return a string to be printed."]}, {"name": "void PyArray_UpdateFlags()", "path": "reference/c-api/array#c.PyArray_UpdateFlags", "type": "Array API", "text": ["The NPY_ARRAY_C_CONTIGUOUS, NPY_ARRAY_ALIGNED, and NPY_ARRAY_F_CONTIGUOUS array flags can be \u201ccalculated\u201d from the array object itself. This routine updates one or more of these flags of arr as specified in flagmask by performing the required calculation."]}, {"name": "void PyArray_XDECREF_ERR()", "path": "reference/c-api/array#c.PyArray_XDECREF_ERR", "type": "Array API", "text": ["Deprecated in 1.14, use PyArray_DiscardWritebackIfCopy followed by Py_XDECREF", "DECREF\u2019s an array object which may have the (deprecated) NPY_ARRAY_UPDATEIFCOPY or NPY_ARRAY_WRITEBACKIFCOPY flag set without causing the contents to be copied back into the original array. Resets the NPY_ARRAY_WRITEABLE flag on the base object. This is useful for recovering from an error condition when writeback semantics are used, but will lead to wrong results."]}, {"name": "void PyDataMem_EventHookFunc()", "path": "reference/c-api/data_memory#c.PyDataMem_EventHookFunc", "type": "Memory management in NumPy", "text": ["This function will be called during data memory manipulation"]}, {"name": "void PyDataMem_FREE()", "path": "reference/c-api/array#c.PyDataMem_FREE", "type": "Array API", "text": []}, {"name": "void PyDimMem_FREE()", "path": "reference/c-api/array#c.PyDimMem_FREE", "type": "Array API", "text": []}, {"name": "void PyUFunc_clearfperr()", "path": "reference/c-api/ufunc#c.PyUFunc_clearfperr", "type": "UFunc API", "text": ["Clear the IEEE error flags."]}, {"name": "void PyUFunc_d_d()", "path": "reference/c-api/ufunc#c.PyUFunc_d_d", "type": "UFunc API", "text": []}, {"name": "void PyUFunc_D_D()", "path": "reference/c-api/ufunc#c.PyUFunc_D_D", "type": "UFunc API", "text": []}, {"name": "void PyUFunc_DD_D()", "path": "reference/c-api/ufunc#c.PyUFunc_DD_D", "type": "UFunc API", "text": []}, {"name": "void PyUFunc_dd_d()", "path": "reference/c-api/ufunc#c.PyUFunc_dd_d", "type": "UFunc API", "text": []}, {"name": "void PyUFunc_e_e()", "path": "reference/c-api/ufunc#c.PyUFunc_e_e", "type": "UFunc API", "text": []}, {"name": "void PyUFunc_e_e_As_d_d()", "path": "reference/c-api/ufunc#c.PyUFunc_e_e_As_d_d", "type": "UFunc API", "text": ["Type specific, core 1-d functions for ufuncs where each calculation is obtained by calling a function taking one input argument and returning one output. This function is passed in func. The letters correspond to dtypechar\u2019s of the supported data types ( e - half, f - float, d - double, g - long double, F - cfloat, D - cdouble, G - clongdouble). The argument func must support the same signature. The _As_X_X variants assume ndarray\u2019s of one data type but cast the values to use an underlying function that takes a different data type. Thus, PyUFunc_f_f_As_d_d uses ndarrays of data type NPY_FLOAT but calls out to a C-function that takes double and returns double."]}, {"name": "void PyUFunc_e_e_As_f_f()", "path": "reference/c-api/ufunc#c.PyUFunc_e_e_As_f_f", "type": "UFunc API", "text": []}, {"name": "void PyUFunc_ee_e()", "path": "reference/c-api/ufunc#c.PyUFunc_ee_e", "type": "UFunc API", "text": []}, {"name": "void PyUFunc_ee_e_As_dd_d()", "path": "reference/c-api/ufunc#c.PyUFunc_ee_e_As_dd_d", "type": "UFunc API", "text": ["Type specific, core 1-d functions for ufuncs where each calculation is obtained by calling a function taking two input arguments and returning one output. The underlying function to call is passed in as func. The letters correspond to dtypechar\u2019s of the specific data type supported by the general-purpose function. The argument func must support the corresponding signature. The _As_XX_X variants assume ndarrays of one data type but cast the values at each iteration of the loop to use the underlying function that takes a different data type."]}, {"name": "void PyUFunc_ee_e_As_ff_f()", "path": "reference/c-api/ufunc#c.PyUFunc_ee_e_As_ff_f", "type": "UFunc API", "text": []}, {"name": "void PyUFunc_F_F()", "path": "reference/c-api/ufunc#c.PyUFunc_F_F", "type": "UFunc API", "text": []}, {"name": "void PyUFunc_f_f()", "path": "reference/c-api/ufunc#c.PyUFunc_f_f", "type": "UFunc API", "text": []}, {"name": "void PyUFunc_F_F_As_D_D()", "path": "reference/c-api/ufunc#c.PyUFunc_F_F_As_D_D", "type": "UFunc API", "text": []}, {"name": "void PyUFunc_ff_f()", "path": "reference/c-api/ufunc#c.PyUFunc_ff_f", "type": "UFunc API", "text": []}, {"name": "void PyUFunc_FF_F()", "path": "reference/c-api/ufunc#c.PyUFunc_FF_F", "type": "UFunc API", "text": []}, {"name": "void PyUFunc_ff_f_As_dd_d()", "path": "reference/c-api/ufunc#c.PyUFunc_ff_f_As_dd_d", "type": "UFunc API", "text": []}, {"name": "void PyUFunc_FF_F_As_DD_D()", "path": "reference/c-api/ufunc#c.PyUFunc_FF_F_As_DD_D", "type": "UFunc API", "text": []}, {"name": "void PyUFunc_g_g()", "path": "reference/c-api/ufunc#c.PyUFunc_g_g", "type": "UFunc API", "text": []}, {"name": "void PyUFunc_G_G()", "path": "reference/c-api/ufunc#c.PyUFunc_G_G", "type": "UFunc API", "text": []}, {"name": "void PyUFunc_GetPyValues()", "path": "reference/c-api/ufunc#c.PyUFunc_GetPyValues", "type": "UFunc API", "text": ["Get the Python values used for ufunc processing from the thread-local storage area unless the defaults have been set in which case the name lookup is bypassed. The name is placed as a string in the first element of *errobj. The second element is the looked-up function to call on error callback. The value of the looked-up buffer-size to use is passed into bufsize, and the value of the error mask is placed into errmask."]}, {"name": "void PyUFunc_gg_g()", "path": "reference/c-api/ufunc#c.PyUFunc_gg_g", "type": "UFunc API", "text": []}, {"name": "void PyUFunc_GG_G()", "path": "reference/c-api/ufunc#c.PyUFunc_GG_G", "type": "UFunc API", "text": []}, {"name": "void PyUFunc_O_O()", "path": "reference/c-api/ufunc#c.PyUFunc_O_O", "type": "UFunc API", "text": []}, {"name": "void PyUFunc_O_O_method()", "path": "reference/c-api/ufunc#c.PyUFunc_O_O_method", "type": "UFunc API", "text": ["This general purpose 1-d core function assumes that func is a string representing a method of the input object. For each iteration of the loop, the Python object is extracted from the array and its func method is called returning the result to the output array."]}, {"name": "void PyUFunc_On_Om()", "path": "reference/c-api/ufunc#c.PyUFunc_On_Om", "type": "UFunc API", "text": ["This is the 1-d core function used by the dynamic ufuncs created by umath.frompyfunc(function, nin, nout). In this case func is a pointer to a PyUFunc_PyFuncData structure which has definition", "At each iteration of the loop, the nin input objects are extracted from their object arrays and placed into an argument tuple, the Python callable is called with the input arguments, and the nout outputs are placed into their object arrays."]}, {"name": "void PyUFunc_OO_O()", "path": "reference/c-api/ufunc#c.PyUFunc_OO_O", "type": "UFunc API", "text": ["One-input, one-output, and two-input, one-output core 1-d functions for the NPY_OBJECT data type. These functions handle reference count issues and return early on error. The actual function to call is func and it must accept calls with the signature (PyObject*)\n(PyObject*) for PyUFunc_O_O or (PyObject*)(PyObject *,\nPyObject *) for PyUFunc_OO_O."]}, {"name": "void PyUFunc_OO_O_method()", "path": "reference/c-api/ufunc#c.PyUFunc_OO_O_method", "type": "UFunc API", "text": ["This general purpose 1-d core function assumes that func is a string representing a method of the input object that takes one argument. The first argument in args is the method whose function is called, the second argument in args is the argument passed to the function. The output of the function is stored in the third entry of args."]}, {"name": "void random_multinomial()", "path": "reference/random/c-api#c.random_multinomial", "type": "C API for random", "text": []}, {"name": "void random_multivariate_hypergeometric_marginals()", "path": "reference/random/c-api#c.random_multivariate_hypergeometric_marginals", "type": "C API for random", "text": []}, {"name": "void random_standard_exponential_fill()", "path": "reference/random/c-api#c.random_standard_exponential_fill", "type": "C API for random", "text": []}, {"name": "void random_standard_exponential_fill_f()", "path": "reference/random/c-api#c.random_standard_exponential_fill_f", "type": "C API for random", "text": []}, {"name": "void random_standard_exponential_inv_fill()", "path": "reference/random/c-api#c.random_standard_exponential_inv_fill", "type": "C API for random", "text": []}, {"name": "void random_standard_exponential_inv_fill_f()", "path": "reference/random/c-api#c.random_standard_exponential_inv_fill_f", "type": "C API for random", "text": []}, {"name": "void random_standard_normal_fill()", "path": "reference/random/c-api#c.random_standard_normal_fill", "type": "C API for random", "text": []}, {"name": "void random_standard_normal_fill_f()", "path": "reference/random/c-api#c.random_standard_normal_fill_f", "type": "C API for random", "text": []}, {"name": "void random_standard_uniform_fill()", "path": "reference/random/c-api#c.random_standard_uniform_fill", "type": "C API for random", "text": []}, {"name": "void random_standard_uniform_fill_f()", "path": "reference/random/c-api#c.random_standard_uniform_fill_f", "type": "C API for random", "text": []}, {"name": "What is NumPy?", "path": "user/whatisnumpy", "type": "User Guide", "text": ["NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more.", "At the core of the NumPy package, is the ndarray object. This encapsulates n-dimensional arrays of homogeneous data types, with many operations being performed in compiled code for performance. There are several important differences between NumPy arrays and the standard Python sequences:", "The points about sequence size and speed are particularly important in scientific computing. As a simple example, consider the case of multiplying each element in a 1-D sequence with the corresponding element in another sequence of the same length. If the data are stored in two Python lists, a and b, we could iterate over each element:", "This produces the correct answer, but if a and b each contain millions of numbers, we will pay the price for the inefficiencies of looping in Python. We could accomplish the same task much more quickly in C by writing (for clarity we neglect variable declarations and initializations, memory allocation, etc.)", "This saves all the overhead involved in interpreting the Python code and manipulating Python objects, but at the expense of the benefits gained from coding in Python. Furthermore, the coding work required increases with the dimensionality of our data. In the case of a 2-D array, for example, the C code (abridged as before) expands to", "NumPy gives us the best of both worlds: element-by-element operations are the \u201cdefault mode\u201d when an ndarray is involved, but the element-by-element operation is speedily executed by pre-compiled C code. In NumPy", "does what the earlier examples do, at near-C speeds, but with the code simplicity we expect from something based on Python. Indeed, the NumPy idiom is even simpler! This last example illustrates two of NumPy\u2019s features which are the basis of much of its power: vectorization and broadcasting.", "Vectorization describes the absence of any explicit looping, indexing, etc., in the code - these things are taking place, of course, just \u201cbehind the scenes\u201d in optimized, pre-compiled C code. Vectorized code has many advantages, among which are:", "Broadcasting is the term used to describe the implicit element-by-element behavior of operations; generally speaking, in NumPy all operations, not just arithmetic operations, but logical, bit-wise, functional, etc., behave in this implicit element-by-element fashion, i.e., they broadcast. Moreover, in the example above, a and b could be multidimensional arrays of the same shape, or a scalar and an array, or even two arrays of with different shapes, provided that the smaller array is \u201cexpandable\u201d to the shape of the larger in such a way that the resulting broadcast is unambiguous. For detailed \u201crules\u201d of broadcasting see Broadcasting.", "NumPy fully supports an object-oriented approach, starting, once again, with ndarray. For example, ndarray is a class, possessing numerous methods and attributes. Many of its methods are mirrored by functions in the outer-most NumPy namespace, allowing the programmer to code in whichever paradigm they prefer. This flexibility has allowed the NumPy array dialect and NumPy ndarray class to become the de-facto language of multi-dimensional data interchange used in Python."]}, {"name": "What\u2019s New or Different", "path": "reference/random/new-or-different", "type": "What\u2019s New or Different", "text": ["Warning", "The Box-Muller method used to produce NumPy\u2019s normals is no longer available in Generator. It is not possible to reproduce the exact random values using Generator for the normal distribution or any other distribution that relies on the normal such as the Generator.gamma or Generator.standard_t. If you require bitwise backward compatible streams, use RandomState, i.e., RandomState.gamma or RandomState.standard_t.", "Quick comparison of legacy mtrand to the new Generator", "Feature", "Older Equivalent", "Notes", "Generator", "RandomState", "Generator requires a stream source, called a BitGenerator A number of these are provided. RandomState uses the Mersenne Twister MT19937 by default, but can also be instantiated with any BitGenerator.", "random", "random_sample, rand", "Access the values in a BitGenerator, convert them to float64 in the interval [0.0., `` 1.0)``. In addition to the size kwarg, now supports dtype='d' or dtype='f', and an out kwarg to fill a user- supplied array.", "Many other distributions are also supported.", "integers", "randint, random_integers", "Use the endpoint kwarg to adjust the inclusion or exclution of the high interval endpoint", "And in more detail:", "Optional dtype argument that accepts np.float32 or np.float64 to produce either single or double prevision uniform random variables for select distributions", "Optional out argument that allows existing arrays to be filled for select distributions", "This allows multithreading to fill large arrays in chunks using suitable BitGenerators in parallel."]}, {"name": "Window functions", "path": "reference/routines.window", "type": "Window functions", "text": ["bartlett(M)", "Return the Bartlett window.", "blackman(M)", "Return the Blackman window.", "hamming(M)", "Return the Hamming window.", "hanning(M)", "Return the Hanning window.", "kaiser(M, beta)", "Return the Kaiser window."]}, {"name": "WITH_THREADS", "path": "reference/c-api/array#c.WITH_THREADS", "type": "Array API", "text": []}, {"name": "Writing custom array containers", "path": "user/basics.dispatch", "type": "User Guide", "text": ["Numpy\u2019s dispatch mechanism, introduced in numpy version v1.16 is the recommended approach for writing custom N-dimensional array containers that are compatible with the numpy API and provide custom implementations of numpy functionality. Applications include dask arrays, an N-dimensional array distributed across multiple nodes, and cupy arrays, an N-dimensional array on a GPU.", "To get a feel for writing custom array containers, we\u2019ll begin with a simple example that has rather narrow utility but illustrates the concepts involved.", "Our custom array can be instantiated like:", "We can convert to a numpy array using numpy.array or numpy.asarray, which will call its __array__ method to obtain a standard numpy.ndarray.", "If we operate on arr with a numpy function, numpy will again use the __array__ interface to convert it to an array and then apply the function in the usual way.", "Notice that the return type is a standard numpy.ndarray.", "How can we pass our custom array type through this function? Numpy allows a class to indicate that it would like to handle computations in a custom-defined way through the interfaces __array_ufunc__ and __array_function__. Let\u2019s take one at a time, starting with _array_ufunc__. This method covers Universal functions (ufunc), a class of functions that includes, for example, numpy.multiply and numpy.sin.", "The __array_ufunc__ receives:", "For this example we will only handle the method __call__", "Now our custom array type passes through numpy functions.", "At this point arr + 3 does not work.", "To support it, we need to define the Python interfaces __add__, __lt__, and so on to dispatch to the corresponding ufunc. We can achieve this conveniently by inheriting from the mixin NDArrayOperatorsMixin.", "Now let\u2019s tackle __array_function__. We\u2019ll create dict that maps numpy functions to our custom variants.", "A convenient pattern is to define a decorator implements that can be used to add functions to HANDLED_FUNCTIONS.", "Now we write implementations of numpy functions for DiagonalArray. For completeness, to support the usage arr.sum() add a method sum that calls numpy.sum(self), and the same for mean.", "If the user tries to use any numpy functions not included in HANDLED_FUNCTIONS, a TypeError will be raised by numpy, indicating that this operation is not supported. For example, concatenating two DiagonalArrays does not produce another diagonal array, so it is not supported.", "Additionally, our implementations of sum and mean do not accept the optional arguments that numpy\u2019s implementation does.", "The user always has the option of converting to a normal numpy.ndarray with numpy.asarray and using standard numpy from there.", "Refer to the dask source code and cupy source code for more fully-worked examples of custom array containers.", "See also NEP 18."]}, {"name": "Writing your own ufunc", "path": "user/c-info.ufunc-tutorial", "type": "User Guide", "text": ["Before reading this, it may help to familiarize yourself with the basics of C extensions for Python by reading/skimming the tutorials in Section 1 of Extending and Embedding the Python Interpreter and in How to extend NumPy", "The umath module is a computer-generated C-module that creates many ufuncs. It provides a great many examples of how to create a universal function. Creating your own ufunc that will make use of the ufunc machinery is not difficult either. Suppose you have a function that you want to operate element-by-element over its inputs. By creating a new ufunc you will obtain a function that handles", "It is not difficult to create your own ufunc. All that is required is a 1-d loop for each data-type you want to support. Each 1-d loop must have a specific signature, and only ufuncs for fixed-size data-types can be used. The function call used to create a new ufunc to work on built-in data-types is given below. A different mechanism is used to register ufuncs for user-defined data-types.", "In the next several sections we give example code that can be easily modified to create your own ufuncs. The examples are successively more complete or complicated versions of the logit function, a common function in statistical modeling. Logit is also interesting because, due to the magic of IEEE standards (specifically IEEE 754), all of the logit functions created below automatically have the following behavior.", "This is wonderful because the function writer doesn\u2019t have to manually propagate infs or nans.", "For comparison and general edification of the reader we provide a simple implementation of a C extension of logit that uses no numpy.", "To do this we need two files. The first is the C file which contains the actual code, and the second is the setup.py file used to create the module.", "To use the setup.py file, place setup.py and spammodule.c in the same folder. Then python setup.py build will build the module to import, or setup.py install will install the module to your site-packages directory.", "Once the spam module is imported into python, you can call logit via spam.logit. Note that the function used above cannot be applied as-is to numpy arrays. To do so we must call numpy.vectorize on it. For example, if a python interpreter is opened in the file containing the spam library or spam has been installed, one can perform the following commands:", "THE RESULTING LOGIT FUNCTION IS NOT FAST! numpy.vectorize simply loops over spam.logit. The loop is done at the C level, but the numpy array is constantly being parsed and build back up. This is expensive. When the author compared numpy.vectorize(spam.logit) against the logit ufuncs constructed below, the logit ufuncs were almost exactly 4 times faster. Larger or smaller speedups are, of course, possible depending on the nature of the function.", "For simplicity we give a ufunc for a single dtype, the \u2018f8\u2019 double. As in the previous section, we first give the .c file and then the setup.py file used to create the module containing the ufunc.", "The place in the code corresponding to the actual computations for the ufunc are marked with /*BEGIN main ufunc computation*/ and /*END main ufunc computation*/. The code in between those lines is the primary thing that must be changed to create your own ufunc.", "This is a setup.py file for the above code. As before, the module can be build via calling python setup.py build at the command prompt, or installed to site-packages via python setup.py install.", "After the above has been installed, it can be imported and used as follows.", "We finally give an example of a full ufunc, with inner loops for half-floats, floats, doubles, and long doubles. As in the previous sections we first give the .c file and then the corresponding setup.py file.", "The places in the code corresponding to the actual computations for the ufunc are marked with /*BEGIN main ufunc computation*/ and /*END main ufunc computation*/. The code in between those lines is the primary thing that must be changed to create your own ufunc.", "This is a setup.py file for the above code. As before, the module can be build via calling python setup.py build at the command prompt, or installed to site-packages via python setup.py install.", "After the above has been installed, it can be imported and used as follows.", "Our final example is a ufunc with multiple arguments. It is a modification of the code for a logit ufunc for data with a single dtype. We compute (A*B, logit(A*B)).", "We only give the C code as the setup.py file is exactly the same as the setup.py file in Example NumPy ufunc for one dtype, except that the line", "is replaced with", "The C file is given below. The ufunc generated takes two arguments A and B. It returns a tuple whose first element is A*B and whose second element is logit(A*B). Note that it automatically supports broadcasting, as well as all other properties of a ufunc.", "This example shows how to create a ufunc for a structured array dtype. For the example we show a trivial ufunc for adding two arrays with dtype \u2018u8,u8,u8\u2019. The process is a bit different from the other examples since a call to PyUFunc_FromFuncAndData doesn\u2019t fully register ufuncs for custom dtypes and structured array dtypes. We need to also call PyUFunc_RegisterLoopForDescr to finish setting up the ufunc.", "We only give the C code as the setup.py file is exactly the same as the setup.py file in Example NumPy ufunc for one dtype, except that the line", "is replaced with", "The C file is given below.", "The returned ufunc object is a callable Python object. It should be placed in a (module) dictionary under the same name as was used in the name argument to the ufunc-creation routine. The following example is adapted from the umath module"]}, {"name": "\u201cdescr\u201ddtype.descr", "path": "reference/generated/numpy.lib.format", "type": "numpy.lib.format", "text": ["Binary serialization", "A simple format for saving numpy arrays to disk with the full information about them.", "The .npy format is the standard binary file format in NumPy for persisting a single arbitrary NumPy array on disk. The format stores all of the shape and dtype information necessary to reconstruct the array correctly even on another machine with a different architecture. The format is designed to be as simple as possible while achieving its limited goals.", "The .npz format is the standard format for persisting multiple NumPy arrays on disk. A .npz file is a zip file containing multiple .npy files, one for each array.", "Warning", "Due to limitations in the interpretation of structured dtypes, dtypes with fields with empty names will have the names replaced by \u2018f0\u2019, \u2018f1\u2019, etc. Such arrays will not round-trip through the format entirely accurately. The data is intact; only the field names will differ. We are working on a fix for this. This fix will not require a change in the file format. The arrays with such structures can still be saved and restored, and the correct dtype may be restored by using the loadedarray.view(correct_dtype) method.", "We recommend using the .npy and .npz extensions for files saved in this format. This is by no means a requirement; applications may wish to use these file formats but use an extension specific to the application. In the absence of an obvious alternative, however, we suggest using .npy and .npz.", "The version numbering of these formats is independent of NumPy version numbering. If the format is upgraded, the code in numpy.io will still be able to read and write Version 1.0 files.", "The first 6 bytes are a magic string: exactly \\x93NUMPY.", "The next 1 byte is an unsigned byte: the major version number of the file format, e.g. \\x01.", "The next 1 byte is an unsigned byte: the minor version number of the file format, e.g. \\x00. Note: the version of the file format is not tied to the version of the numpy package.", "The next 2 bytes form a little-endian unsigned short int: the length of the header data HEADER_LEN.", "The next HEADER_LEN bytes form the header data describing the array\u2019s format. It is an ASCII string which contains a Python literal expression of a dictionary. It is terminated by a newline (\\n) and padded with spaces (\\x20) to make the total of len(magic string) + 2 + len(length) + HEADER_LEN be evenly divisible by 64 for alignment purposes.", "The dictionary contains three keys:", "An object that can be passed as an argument to the numpy.dtype constructor to create the array\u2019s dtype.", "Whether the array data is Fortran-contiguous or not. Since Fortran-contiguous arrays are a common form of non-C-contiguity, we allow them to be written directly to disk for efficiency.", "The shape of the array.", "For repeatability and readability, the dictionary keys are sorted in alphabetic order. This is for convenience only. A writer SHOULD implement this if possible. A reader MUST NOT depend on this.", "Following the header comes the array data. If the dtype contains Python objects (i.e. dtype.hasobject is True), then the data is a Python pickle of the array. Otherwise the data is the contiguous (either C- or Fortran-, depending on fortran_order) bytes of the array. Consumers can figure out the number of bytes by multiplying the number of elements given by the shape (noting that shape=() means there is 1 element) by dtype.itemsize.", "The version 1.0 format only allowed the array header to have a total size of 65535 bytes. This can be exceeded by structured arrays with a large number of columns. The version 2.0 format extends the header size to 4 GiB. numpy.save will automatically save in 2.0 format if the data requires it, else it will always use the more compatible 1.0 format.", "The description of the fourth element of the header therefore has become: \u201cThe next 4 bytes form a little-endian unsigned int: the length of the header data HEADER_LEN.\u201d", "This version replaces the ASCII string (which in practice was latin1) with a utf8-encoded string, so supports structured types with any unicode field names.", "The .npy format, including motivation for creating it and a comparison of alternatives, is described in the \u201cnpy-format\u201d NEP, however details have evolved with time and this document is more current.", "descr_to_dtype(descr)", "Returns a dtype based off the given description.", "dtype_to_descr(dtype)", "Get a serializable descriptor from the dtype.", "header_data_from_array_1_0(array)", "Get the dictionary of header metadata from a numpy.ndarray.", "magic(major, minor)", "Return the magic string for the given file format version.", "open_memmap(filename[, mode, dtype, shape, ...])", "Open a .npy file as a memory-mapped array.", "read_array(fp[, allow_pickle, pickle_kwargs])", "Read an array from an NPY file.", "read_array_header_1_0(fp)", "Read an array header from a filelike object using the 1.0 file format version.", "read_array_header_2_0(fp)", "Read an array header from a filelike object using the 2.0 file format version.", "read_magic(fp)", "Read the magic string to get the version of the file format.", "write_array(fp, array[, version, ...])", "Write an array to an NPY file, including a header.", "write_array_header_1_0(fp, d)", "Write the header for an array using the 1.0 format.", "write_array_header_2_0(fp, d)", "Write the header for an array using the 2.0 format."]}]