<h1 id="torch-use-deterministic-algorithms">torch.use_deterministic_algorithms</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.use_deterministic_algorithms">
<code>torch.use_deterministic_algorithms(mode, *, warn_only=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#use_deterministic_algorithms"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Sets whether PyTorch operations must use “deterministic” algorithms. That is, algorithms which, given the same input, and when run on the same software and hardware, always produce the same output. When enabled, operations will use deterministic algorithms when available, and if only nondeterministic algorithms are available they will throw a <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.12)"><code>RuntimeError</code></a> when called.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This setting alone is not always enough to make an application reproducible. Refer to <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/randomness.html#reproducibility"><span class="std std-ref">Reproducibility</span></a> for more information.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p><a class="reference internal" href="torch.set_deterministic_debug_mode#torch.set_deterministic_debug_mode" title="torch.set_deterministic_debug_mode"><code>torch.set_deterministic_debug_mode()</code></a> offers an alternative interface for this feature.</p> </div> <p>The following normally-nondeterministic operations will act deterministically when <code>mode=True</code>:</p>  <ul class="simple"> <li>
<a class="reference internal" href="torch.nn.conv1d#torch.nn.Conv1d" title="torch.nn.Conv1d"><code>torch.nn.Conv1d</code></a> when called on CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.conv2d#torch.nn.Conv2d" title="torch.nn.Conv2d"><code>torch.nn.Conv2d</code></a> when called on CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.conv3d#torch.nn.Conv3d" title="torch.nn.Conv3d"><code>torch.nn.Conv3d</code></a> when called on CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.convtranspose1d#torch.nn.ConvTranspose1d" title="torch.nn.ConvTranspose1d"><code>torch.nn.ConvTranspose1d</code></a> when called on CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.convtranspose2d#torch.nn.ConvTranspose2d" title="torch.nn.ConvTranspose2d"><code>torch.nn.ConvTranspose2d</code></a> when called on CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.convtranspose3d#torch.nn.ConvTranspose3d" title="torch.nn.ConvTranspose3d"><code>torch.nn.ConvTranspose3d</code></a> when called on CUDA tensor</li> <li>
<a class="reference internal" href="torch.bmm#torch.bmm" title="torch.bmm"><code>torch.bmm()</code></a> when called on sparse-dense CUDA tensors</li> <li>
<code>torch.Tensor.__getitem__()</code> when attempting to differentiate a CPU tensor and the index is a list of tensors</li> <li>
<a class="reference internal" href="torch.tensor.index_put#torch.Tensor.index_put" title="torch.Tensor.index_put"><code>torch.Tensor.index_put()</code></a> with <code>accumulate=False</code>
</li> <li>
<a class="reference internal" href="torch.tensor.index_put#torch.Tensor.index_put" title="torch.Tensor.index_put"><code>torch.Tensor.index_put()</code></a> with <code>accumulate=True</code> when called on a CPU tensor</li> <li>
<a class="reference internal" href="torch.tensor.put_#torch.Tensor.put_" title="torch.Tensor.put_"><code>torch.Tensor.put_()</code></a> with <code>accumulate=True</code> when called on a CPU tensor</li> <li>
<a class="reference internal" href="torch.tensor.scatter_add_#torch.Tensor.scatter_add_" title="torch.Tensor.scatter_add_"><code>torch.Tensor.scatter_add_()</code></a> when called on a CUDA tensor</li> <li>
<a class="reference internal" href="torch.gather#torch.gather" title="torch.gather"><code>torch.gather()</code></a> when called on a CUDA tensor that requires grad</li> <li>
<a class="reference internal" href="torch.index_add#torch.index_add" title="torch.index_add"><code>torch.index_add()</code></a> when called on CUDA tensor</li> <li>
<a class="reference internal" href="torch.index_select#torch.index_select" title="torch.index_select"><code>torch.index_select()</code></a> when attempting to differentiate a CUDA tensor</li> <li>
<a class="reference internal" href="torch.repeat_interleave#torch.repeat_interleave" title="torch.repeat_interleave"><code>torch.repeat_interleave()</code></a> when attempting to differentiate a CUDA tensor</li> <li>
<a class="reference internal" href="torch.tensor.index_copy#torch.Tensor.index_copy" title="torch.Tensor.index_copy"><code>torch.Tensor.index_copy()</code></a> when called on a CPU or CUDA tensor</li> <li>
<a class="reference internal" href="torch.tensor.scatter#torch.Tensor.scatter" title="torch.Tensor.scatter"><code>torch.Tensor.scatter()</code></a> when <code>src</code> type is Tensor and called on CUDA tensor</li> <li>
<a class="reference internal" href="torch.tensor.scatter_reduce#torch.Tensor.scatter_reduce" title="torch.Tensor.scatter_reduce"><code>torch.Tensor.scatter_reduce()</code></a> when <code>reduce='sum'</code> or <code>reduce='mean'</code> and called on CUDA tensor</li> <li>
<a class="reference internal" href="torch.tensor.resize_#torch.Tensor.resize_" title="torch.Tensor.resize_"><code>torch.Tensor.resize_()</code></a>, when called with a tensor that is not quantized, sets new elements to a known value. Floating point or complex values are set to NaN. Integer values are set to the maximum value.</li> <li>
<a class="reference internal" href="torch.empty#torch.empty" title="torch.empty"><code>torch.empty()</code></a>, <a class="reference internal" href="torch.empty_like#torch.empty_like" title="torch.empty_like"><code>torch.empty_like()</code></a>, <a class="reference internal" href="torch.empty_strided#torch.empty_strided" title="torch.empty_strided"><code>torch.empty_strided()</code></a>, and <code>torch.empty_permuted()</code> will fill the output tensor with a known value. Floating point or complex dtype tensors are filled with NaN. Integer dtype tensors are filled with the maximum value.</li> </ul>  <p>The following normally-nondeterministic operations will throw a <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.12)"><code>RuntimeError</code></a> when <code>mode=True</code>:</p>  <ul class="simple"> <li>
<a class="reference internal" href="torch.nn.avgpool3d#torch.nn.AvgPool3d" title="torch.nn.AvgPool3d"><code>torch.nn.AvgPool3d</code></a> when attempting to differentiate a CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.adaptiveavgpool2d#torch.nn.AdaptiveAvgPool2d" title="torch.nn.AdaptiveAvgPool2d"><code>torch.nn.AdaptiveAvgPool2d</code></a> when attempting to differentiate a CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.adaptiveavgpool3d#torch.nn.AdaptiveAvgPool3d" title="torch.nn.AdaptiveAvgPool3d"><code>torch.nn.AdaptiveAvgPool3d</code></a> when attempting to differentiate a CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.maxpool3d#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code>torch.nn.MaxPool3d</code></a> when attempting to differentiate a CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.adaptivemaxpool2d#torch.nn.AdaptiveMaxPool2d" title="torch.nn.AdaptiveMaxPool2d"><code>torch.nn.AdaptiveMaxPool2d</code></a> when attempting to differentiate a CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.fractionalmaxpool2d#torch.nn.FractionalMaxPool2d" title="torch.nn.FractionalMaxPool2d"><code>torch.nn.FractionalMaxPool2d</code></a> when attempting to differentiate a CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.fractionalmaxpool3d#torch.nn.FractionalMaxPool3d" title="torch.nn.FractionalMaxPool3d"><code>torch.nn.FractionalMaxPool3d</code></a> when attempting to differentiate a CUDA tensor</li> <li><a class="reference internal" href="torch.nn.maxunpool1d#torch.nn.MaxUnpool1d" title="torch.nn.MaxUnpool1d"><code>torch.nn.MaxUnpool1d</code></a></li> <li><a class="reference internal" href="torch.nn.maxunpool2d#torch.nn.MaxUnpool2d" title="torch.nn.MaxUnpool2d"><code>torch.nn.MaxUnpool2d</code></a></li> <li><a class="reference internal" href="torch.nn.maxunpool3d#torch.nn.MaxUnpool3d" title="torch.nn.MaxUnpool3d"><code>torch.nn.MaxUnpool3d</code></a></li> <li>
<p><a class="reference internal" href="torch.nn.functional.interpolate#torch.nn.functional.interpolate" title="torch.nn.functional.interpolate"><code>torch.nn.functional.interpolate()</code></a> when attempting to differentiate a CUDA tensor and one of the following modes is used:</p> <ul> <li><code>linear</code></li> <li><code>bilinear</code></li> <li><code>bicubic</code></li> <li><code>trilinear</code></li> </ul> </li> <li>
<a class="reference internal" href="torch.nn.reflectionpad1d#torch.nn.ReflectionPad1d" title="torch.nn.ReflectionPad1d"><code>torch.nn.ReflectionPad1d</code></a> when attempting to differentiate a CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.reflectionpad2d#torch.nn.ReflectionPad2d" title="torch.nn.ReflectionPad2d"><code>torch.nn.ReflectionPad2d</code></a> when attempting to differentiate a CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.reflectionpad3d#torch.nn.ReflectionPad3d" title="torch.nn.ReflectionPad3d"><code>torch.nn.ReflectionPad3d</code></a> when attempting to differentiate a CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.replicationpad1d#torch.nn.ReplicationPad1d" title="torch.nn.ReplicationPad1d"><code>torch.nn.ReplicationPad1d</code></a> when attempting to differentiate a CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.replicationpad2d#torch.nn.ReplicationPad2d" title="torch.nn.ReplicationPad2d"><code>torch.nn.ReplicationPad2d</code></a> when attempting to differentiate a CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.replicationpad3d#torch.nn.ReplicationPad3d" title="torch.nn.ReplicationPad3d"><code>torch.nn.ReplicationPad3d</code></a> when attempting to differentiate a CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.nllloss#torch.nn.NLLLoss" title="torch.nn.NLLLoss"><code>torch.nn.NLLLoss</code></a> when called on a CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.ctcloss#torch.nn.CTCLoss" title="torch.nn.CTCLoss"><code>torch.nn.CTCLoss</code></a> when attempting to differentiate a CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.embeddingbag#torch.nn.EmbeddingBag" title="torch.nn.EmbeddingBag"><code>torch.nn.EmbeddingBag</code></a> when attempting to differentiate a CUDA tensor when <code>mode='max'</code>
</li> <li>
<a class="reference internal" href="torch.tensor.put_#torch.Tensor.put_" title="torch.Tensor.put_"><code>torch.Tensor.put_()</code></a> when <code>accumulate=False</code>
</li> <li>
<a class="reference internal" href="torch.tensor.put_#torch.Tensor.put_" title="torch.Tensor.put_"><code>torch.Tensor.put_()</code></a> when <code>accumulate=True</code> and called on a CUDA tensor</li> <li>
<a class="reference internal" href="torch.histc#torch.histc" title="torch.histc"><code>torch.histc()</code></a> when called on a CUDA tensor</li> <li>
<a class="reference internal" href="torch.bincount#torch.bincount" title="torch.bincount"><code>torch.bincount()</code></a> when called on a CUDA tensor and <code>weights</code> tensor is given</li> <li>
<a class="reference internal" href="torch.kthvalue#torch.kthvalue" title="torch.kthvalue"><code>torch.kthvalue()</code></a> with called on a CUDA tensor</li> <li>
<a class="reference internal" href="torch.median#torch.median" title="torch.median"><code>torch.median()</code></a> with indices output when called on a CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.functional.grid_sample#torch.nn.functional.grid_sample" title="torch.nn.functional.grid_sample"><code>torch.nn.functional.grid_sample()</code></a> when attempting to differentiate a CUDA tensor</li> <li>
<a class="reference internal" href="torch.cumsum#torch.cumsum" title="torch.cumsum"><code>torch.cumsum()</code></a> when called on a CUDA tensor when dtype is floating point or complex</li> <li>
<a class="reference internal" href="torch.tensor.scatter_reduce#torch.Tensor.scatter_reduce" title="torch.Tensor.scatter_reduce"><code>torch.Tensor.scatter_reduce()</code></a> when <code>reduce='prod'</code> and called on CUDA tensor</li> <li>
<a class="reference internal" href="torch.tensor.resize_#torch.Tensor.resize_" title="torch.Tensor.resize_"><code>torch.Tensor.resize_()</code></a> when called with a quantized tensor</li> </ul>  <p>A handful of CUDA operations are nondeterministic if the CUDA version is 10.2 or greater, unless the environment variable <code>CUBLAS_WORKSPACE_CONFIG=:4096:8</code> or <code>CUBLAS_WORKSPACE_CONFIG=:16:8</code> is set. See the CUDA documentation for more details: <a class="reference external" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility">https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility</a> If one of these environment variable configurations is not set, a <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.12)"><code>RuntimeError</code></a> will be raised from these operations when called with CUDA tensors:</p>  <ul class="simple"> <li><a class="reference internal" href="torch.mm#torch.mm" title="torch.mm"><code>torch.mm()</code></a></li> <li><a class="reference internal" href="torch.mv#torch.mv" title="torch.mv"><code>torch.mv()</code></a></li> <li><a class="reference internal" href="torch.bmm#torch.bmm" title="torch.bmm"><code>torch.bmm()</code></a></li> </ul>  <p>Note that deterministic operations tend to have worse performance than nondeterministic operations.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This flag does not detect or prevent nondeterministic behavior caused by calling an inplace operation on a tensor with an internal memory overlap or by giving such a tensor as the <code>out</code> argument for an operation. In these cases, multiple writes of different data may target a single memory location, and the order of writes is not guaranteed.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a>) – If True, makes potentially nondeterministic operations switch to a deterministic algorithm or throw a runtime error. If False, allows nondeterministic operations.</p> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<p><strong>warn_only</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><code>bool</code></a>, optional) – If True, operations that do not have a deterministic implementation will throw a warning instead of an error. Default: <code>False</code></p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; torch.use_deterministic_algorithms(True)

# Forward mode nondeterministic error
&gt;&gt;&gt; torch.randn(10, device='cuda').kthvalue(0)
...
RuntimeError: kthvalue CUDA does not have a deterministic implementation...

# Backward mode nondeterministic error
&gt;&gt;&gt; torch.nn.AvgPool3d(1)(torch.randn(3, 4, 5, 6, requires_grad=True).cuda()).sum().backward()
...
RuntimeError: avg_pool3d_backward_cuda does not have a deterministic implementation...
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.use_deterministic_algorithms.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.use_deterministic_algorithms.html</a>
  </p>
</div>
