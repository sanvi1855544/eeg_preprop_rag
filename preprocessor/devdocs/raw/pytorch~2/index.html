<h1 id="pytorch-documentation">PyTorch documentation</h1> <p>PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.</p> <p>Features described in this documentation are classified by release status:</p>  <p><em>Stable:</em> These features will be maintained long-term and there should generally be no major performance limitations or gaps in documentation. We also expect to maintain backwards compatibility (although breaking changes can happen and notice will be given one release ahead of time).</p> <p><em>Beta:</em> These features are tagged as Beta because the API may change based on user feedback, because the performance needs to improve, or because coverage across operators is not yet complete. For Beta features, we are committing to seeing the feature through to the Stable classification. We are not, however, committing to backwards compatibility.</p> <p><em>Prototype:</em> These features are typically not available as part of binary distributions like PyPI or Conda, except sometimes behind run-time flags, and are at an early stage for feedback and testing.</p>   <p class="caption" role="heading"><span class="caption-text">Community</span></p> <ul> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/community/build_ci_governance.html">PyTorch Governance | Build + CI</a></li> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/community/contribution_guide.html">PyTorch Contribution Guide</a></li> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/community/design.html">PyTorch Design Philosophy</a></li> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/community/governance.html">PyTorch Governance | Mechanics</a></li> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/community/persons_of_interest.html">PyTorch Governance | Maintainers</a></li> </ul>   <p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p> <ul> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/notes/amp_examples.html">CUDA Automatic Mixed Precision examples</a></li> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/notes/autograd.html">Autograd mechanics</a></li> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/notes/broadcasting.html">Broadcasting semantics</a></li> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/notes/cuda.html">CUDA semantics</a></li> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/notes/ddp.html">Distributed Data Parallel</a></li> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/notes/extending.html">Extending PyTorch</a></li> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/notes/extending.func.html">Extending torch.func with autograd.Function</a></li> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/notes/faq.html">Frequently Asked Questions</a></li> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/notes/gradcheck.html">Gradcheck mechanics</a></li> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/notes/hip.html">HIP (ROCm) semantics</a></li> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/notes/large_scale_deployments.html">Features for large-scale deployments</a></li> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/notes/modules.html">Modules</a></li> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/notes/mps.html">MPS backend</a></li> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/notes/multiprocessing.html">Multiprocessing best practices</a></li> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/notes/numerical_accuracy.html">Numerical accuracy</a></li> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/notes/randomness.html">Reproducibility</a></li> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/notes/serialization.html">Serialization semantics</a></li> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/notes/windows.html">Windows FAQ</a></li> </ul>   <p class="caption" role="heading"><span class="caption-text">Language Bindings</span></p> <ul> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/cpp_index.html">C++</a></li> <li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li> <li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/2.1/deploy.html">torch::deploy</a></li> </ul>   <p class="caption" role="heading"><span class="caption-text">Python API</span></p> <ul> <li class="toctree-l1">
<a class="reference internal" href="torch">torch</a><ul> <li class="toctree-l2"><a class="reference internal" href="torch#tensors">Tensors</a></li> <li class="toctree-l2"><a class="reference internal" href="torch#generators">Generators</a></li> <li class="toctree-l2"><a class="reference internal" href="torch#random-sampling">Random sampling</a></li> <li class="toctree-l2"><a class="reference internal" href="torch#serialization">Serialization</a></li> <li class="toctree-l2"><a class="reference internal" href="torch#parallelism">Parallelism</a></li> <li class="toctree-l2"><a class="reference internal" href="torch#locally-disabling-gradient-computation">Locally disabling gradient computation</a></li> <li class="toctree-l2"><a class="reference internal" href="torch#math-operations">Math operations</a></li> <li class="toctree-l2"><a class="reference internal" href="torch#utilities">Utilities</a></li> <li class="toctree-l2"><a class="reference internal" href="torch#symbolic-numbers">Symbolic Numbers</a></li> <li class="toctree-l2"><a class="reference internal" href="torch#export-path">Export Path</a></li> <li class="toctree-l2"><a class="reference internal" href="torch#optimizations">Optimizations</a></li> <li class="toctree-l2"><a class="reference internal" href="torch#operator-tags">Operator Tags</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="nn">torch.nn</a><ul> <li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.parameter">Parameter</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.uninitializedparameter">UninitializedParameter</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.nn.parameter.uninitializedbuffer">UninitializedBuffer</a></li> <li class="toctree-l2"><a class="reference internal" href="nn#containers">Containers</a></li> <li class="toctree-l2"><a class="reference internal" href="nn#convolution-layers">Convolution Layers</a></li> <li class="toctree-l2"><a class="reference internal" href="nn#pooling-layers">Pooling layers</a></li> <li class="toctree-l2"><a class="reference internal" href="nn#padding-layers">Padding Layers</a></li> <li class="toctree-l2"><a class="reference internal" href="nn#non-linear-activations-weighted-sum-nonlinearity">Non-linear Activations (weighted sum, nonlinearity)</a></li> <li class="toctree-l2"><a class="reference internal" href="nn#non-linear-activations-other">Non-linear Activations (other)</a></li> <li class="toctree-l2"><a class="reference internal" href="nn#normalization-layers">Normalization Layers</a></li> <li class="toctree-l2"><a class="reference internal" href="nn#recurrent-layers">Recurrent Layers</a></li> <li class="toctree-l2"><a class="reference internal" href="nn#transformer-layers">Transformer Layers</a></li> <li class="toctree-l2"><a class="reference internal" href="nn#linear-layers">Linear Layers</a></li> <li class="toctree-l2"><a class="reference internal" href="nn#dropout-layers">Dropout Layers</a></li> <li class="toctree-l2"><a class="reference internal" href="nn#sparse-layers">Sparse Layers</a></li> <li class="toctree-l2"><a class="reference internal" href="nn#distance-functions">Distance Functions</a></li> <li class="toctree-l2"><a class="reference internal" href="nn#loss-functions">Loss Functions</a></li> <li class="toctree-l2"><a class="reference internal" href="nn#vision-layers">Vision Layers</a></li> <li class="toctree-l2"><a class="reference internal" href="nn#shuffle-layers">Shuffle Layers</a></li> <li class="toctree-l2"><a class="reference internal" href="nn#module-torch.nn.parallel">DataParallel Layers (multi-GPU, distributed)</a></li> <li class="toctree-l2"><a class="reference internal" href="nn#module-torch.nn.utils">Utilities</a></li> <li class="toctree-l2"><a class="reference internal" href="nn#quantized-functions">Quantized Functions</a></li> <li class="toctree-l2"><a class="reference internal" href="nn#lazy-modules-initialization">Lazy Modules Initialization</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="nn.functional">torch.nn.functional</a><ul> <li class="toctree-l2"><a class="reference internal" href="nn.functional#convolution-functions">Convolution functions</a></li> <li class="toctree-l2"><a class="reference internal" href="nn.functional#pooling-functions">Pooling functions</a></li> <li class="toctree-l2"><a class="reference internal" href="nn.functional#attention-mechanisms">Attention Mechanisms</a></li> <li class="toctree-l2"><a class="reference internal" href="nn.functional#non-linear-activation-functions">Non-linear activation functions</a></li> <li class="toctree-l2"><a class="reference internal" href="nn.functional#linear-functions">Linear functions</a></li> <li class="toctree-l2"><a class="reference internal" href="nn.functional#dropout-functions">Dropout functions</a></li> <li class="toctree-l2"><a class="reference internal" href="nn.functional#sparse-functions">Sparse functions</a></li> <li class="toctree-l2"><a class="reference internal" href="nn.functional#distance-functions">Distance functions</a></li> <li class="toctree-l2"><a class="reference internal" href="nn.functional#loss-functions">Loss functions</a></li> <li class="toctree-l2"><a class="reference internal" href="nn.functional#vision-functions">Vision functions</a></li> <li class="toctree-l2"><a class="reference internal" href="nn.functional#dataparallel-functions-multi-gpu-distributed">DataParallel functions (multi-GPU, distributed)</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="tensors">torch.Tensor</a><ul> <li class="toctree-l2"><a class="reference internal" href="tensors#data-types">Data types</a></li> <li class="toctree-l2"><a class="reference internal" href="tensors#initializing-and-basic-operations">Initializing and basic operations</a></li> <li class="toctree-l2"><a class="reference internal" href="tensors#tensor-class-reference">Tensor class reference</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="tensor_attributes">Tensor Attributes</a><ul> <li class="toctree-l2"><a class="reference internal" href="tensor_attributes#torch-dtype">torch.dtype</a></li> <li class="toctree-l2"><a class="reference internal" href="tensor_attributes#torch-device">torch.device</a></li> <li class="toctree-l2"><a class="reference internal" href="tensor_attributes#torch-layout">torch.layout</a></li> <li class="toctree-l2"><a class="reference internal" href="tensor_attributes#torch-memory-format">torch.memory_format</a></li> </ul> </li> <li class="toctree-l1"><a class="reference internal" href="tensor_view">Tensor Views</a></li> <li class="toctree-l1">
<a class="reference internal" href="amp">torch.amp</a><ul> <li class="toctree-l2"><a class="reference internal" href="amp#autocasting">Autocasting</a></li> <li class="toctree-l2"><a class="reference internal" href="amp#gradient-scaling">Gradient Scaling</a></li> <li class="toctree-l2"><a class="reference internal" href="amp#autocast-op-reference">Autocast Op Reference</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="autograd">torch.autograd</a><ul> <li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.backward">torch.autograd.backward</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.autograd.grad">torch.autograd.grad</a></li> <li class="toctree-l2"><a class="reference internal" href="autograd#forward-mode-automatic-differentiation">Forward-mode Automatic Differentiation</a></li> <li class="toctree-l2"><a class="reference internal" href="autograd#functional-higher-level-api">Functional higher level API</a></li> <li class="toctree-l2"><a class="reference internal" href="autograd#locally-disabling-gradient-computation">Locally disabling gradient computation</a></li> <li class="toctree-l2"><a class="reference internal" href="autograd#default-gradient-layouts">Default gradient layouts</a></li> <li class="toctree-l2"><a class="reference internal" href="autograd#in-place-operations-on-tensors">In-place operations on Tensors</a></li> <li class="toctree-l2"><a class="reference internal" href="autograd#variable-deprecated">Variable (deprecated)</a></li> <li class="toctree-l2"><a class="reference internal" href="autograd#tensor-autograd-functions">Tensor autograd functions</a></li> <li class="toctree-l2"><a class="reference internal" href="autograd#function"><span class="hidden-section">Function</span></a></li> <li class="toctree-l2"><a class="reference internal" href="autograd#context-method-mixins">Context method mixins</a></li> <li class="toctree-l2"><a class="reference internal" href="autograd#numerical-gradient-checking">Numerical gradient checking</a></li> <li class="toctree-l2"><a class="reference internal" href="autograd#profiler">Profiler</a></li> <li class="toctree-l2"><a class="reference internal" href="autograd#anomaly-detection">Anomaly detection</a></li> <li class="toctree-l2"><a class="reference internal" href="autograd#autograd-graph">Autograd graph</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="library">torch.library</a><ul> <li class="toctree-l2"><a class="reference internal" href="library#torch.library.Library"><code>Library</code></a></li> <li class="toctree-l2"><a class="reference internal" href="library#torch.library.fallthrough_kernel"><code>fallthrough_kernel()</code></a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="cpu">torch.cpu</a><ul> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.current_stream">torch.cpu.current_stream</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.is_available">torch.cpu.is_available</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.synchronize">torch.cpu.synchronize</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.stream">torch.cpu.stream</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.device_count">torch.cpu.device_count</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cpu.streamcontext">StreamContext</a></li> <li class="toctree-l2"><a class="reference internal" href="cpu#streams-and-events">Streams and events</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="cuda">torch.cuda</a><ul> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.streamcontext">StreamContext</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.can_device_access_peer">torch.cuda.can_device_access_peer</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.current_blas_handle">torch.cuda.current_blas_handle</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.current_device">torch.cuda.current_device</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.current_stream">torch.cuda.current_stream</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.default_stream">torch.cuda.default_stream</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.device">device</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.device_count">torch.cuda.device_count</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.device_of">device_of</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_arch_list">torch.cuda.get_arch_list</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_device_capability">torch.cuda.get_device_capability</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_device_name">torch.cuda.get_device_name</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_device_properties">torch.cuda.get_device_properties</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_gencode_flags">torch.cuda.get_gencode_flags</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.get_sync_debug_mode">torch.cuda.get_sync_debug_mode</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.init">torch.cuda.init</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.ipc_collect">torch.cuda.ipc_collect</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.is_available">torch.cuda.is_available</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.is_initialized">torch.cuda.is_initialized</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.memory_usage">torch.cuda.memory_usage</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_device">torch.cuda.set_device</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_stream">torch.cuda.set_stream</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.set_sync_debug_mode">torch.cuda.set_sync_debug_mode</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.stream">torch.cuda.stream</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.synchronize">torch.cuda.synchronize</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.utilization">torch.cuda.utilization</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.temperature">torch.cuda.temperature</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.power_draw">torch.cuda.power_draw</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.clock_rate">torch.cuda.clock_rate</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.cuda.outofmemoryerror">torch.cuda.OutOfMemoryError</a></li> <li class="toctree-l2"><a class="reference internal" href="cuda#random-number-generator">Random Number Generator</a></li> <li class="toctree-l2"><a class="reference internal" href="cuda#communication-collectives">Communication collectives</a></li> <li class="toctree-l2"><a class="reference internal" href="cuda#streams-and-events">Streams and events</a></li> <li class="toctree-l2"><a class="reference internal" href="cuda#graphs-beta">Graphs (beta)</a></li> <li class="toctree-l2"><a class="reference internal" href="cuda#memory-management">Memory management</a></li> <li class="toctree-l2"><a class="reference internal" href="cuda#nvidia-tools-extension-nvtx">NVIDIA Tools Extension (NVTX)</a></li> <li class="toctree-l2"><a class="reference internal" href="cuda#jiterator-beta">Jiterator (beta)</a></li> <li class="toctree-l2"><a class="reference internal" href="cuda#stream-sanitizer-prototype">Stream Sanitizer (prototype)</a></li> </ul> </li> <li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory">Understanding CUDA Memory Usage</a></li> <li class="toctree-l1"><a class="reference internal" href="torch_cuda_memory#generating-a-snapshot">Generating a Snapshot</a></li> <li class="toctree-l1">
<a class="reference internal" href="torch_cuda_memory#using-the-visualizer">Using the visualizer</a><ul> <li class="toctree-l2"><a class="reference internal" href="torch_cuda_memory#active-memory-timeline">Active Memory Timeline</a></li> <li class="toctree-l2"><a class="reference internal" href="torch_cuda_memory#allocator-state-history">Allocator State History</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="torch_cuda_memory#snapshot-api-reference">Snapshot API Reference</a><ul> <li class="toctree-l2"><a class="reference internal" href="torch_cuda_memory#torch.cuda.memory._record_memory_history"><code>_record_memory_history()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="torch_cuda_memory#torch.cuda.memory._snapshot"><code>_snapshot()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="torch_cuda_memory#torch.cuda.memory._dump_snapshot"><code>_dump_snapshot()</code></a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="mps">torch.mps</a><ul> <li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.synchronize">torch.mps.synchronize</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.get_rng_state">torch.mps.get_rng_state</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.set_rng_state">torch.mps.set_rng_state</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.manual_seed">torch.mps.manual_seed</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.seed">torch.mps.seed</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.empty_cache">torch.mps.empty_cache</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.set_per_process_memory_fraction">torch.mps.set_per_process_memory_fraction</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.current_allocated_memory">torch.mps.current_allocated_memory</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.mps.driver_allocated_memory">torch.mps.driver_allocated_memory</a></li> <li class="toctree-l2"><a class="reference internal" href="mps#mps-profiler">MPS Profiler</a></li> <li class="toctree-l2"><a class="reference internal" href="mps#mps-event">MPS Event</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="backends">torch.backends</a><ul> <li class="toctree-l2"><a class="reference internal" href="backends#module-torch.backends.cpu">torch.backends.cpu</a></li> <li class="toctree-l2"><a class="reference internal" href="backends#module-torch.backends.cuda">torch.backends.cuda</a></li> <li class="toctree-l2"><a class="reference internal" href="backends#module-torch.backends.cudnn">torch.backends.cudnn</a></li> <li class="toctree-l2"><a class="reference internal" href="backends#module-torch.backends.mps">torch.backends.mps</a></li> <li class="toctree-l2"><a class="reference internal" href="backends#module-torch.backends.mkl">torch.backends.mkl</a></li> <li class="toctree-l2"><a class="reference internal" href="backends#module-torch.backends.mkldnn">torch.backends.mkldnn</a></li> <li class="toctree-l2"><a class="reference internal" href="backends#module-torch.backends.openmp">torch.backends.openmp</a></li> <li class="toctree-l2"><a class="reference internal" href="backends#module-torch.backends.opt_einsum">torch.backends.opt_einsum</a></li> <li class="toctree-l2"><a class="reference internal" href="backends#module-torch.backends.xeon">torch.backends.xeon</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="export">torch.export</a><ul> <li class="toctree-l2"><a class="reference internal" href="export#overview">Overview</a></li> <li class="toctree-l2"><a class="reference internal" href="export#exporting-a-pytorch-model">Exporting a PyTorch Model</a></li> <li class="toctree-l2"><a class="reference internal" href="export#limitations-of-torch-export">Limitations of torch.export</a></li> <li class="toctree-l2"><a class="reference internal" href="export#read-more">Read More</a></li> <li class="toctree-l2"><a class="reference internal" href="export#module-torch.export">API Reference</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="distributed">torch.distributed</a><ul> <li class="toctree-l2"><a class="reference internal" href="distributed#backends">Backends</a></li> <li class="toctree-l2"><a class="reference internal" href="distributed#basics">Basics</a></li> <li class="toctree-l2"><a class="reference internal" href="distributed#initialization">Initialization</a></li> <li class="toctree-l2"><a class="reference internal" href="distributed#post-initialization">Post-Initialization</a></li> <li class="toctree-l2"><a class="reference internal" href="distributed#distributed-key-value-store">Distributed Key-Value Store</a></li> <li class="toctree-l2"><a class="reference internal" href="distributed#groups">Groups</a></li> <li class="toctree-l2"><a class="reference internal" href="distributed#point-to-point-communication">Point-to-point communication</a></li> <li class="toctree-l2"><a class="reference internal" href="distributed#synchronous-and-asynchronous-collective-operations">Synchronous and asynchronous collective operations</a></li> <li class="toctree-l2"><a class="reference internal" href="distributed#collective-functions">Collective functions</a></li> <li class="toctree-l2"><a class="reference internal" href="distributed#profiling-collective-communication">Profiling Collective Communication</a></li> <li class="toctree-l2"><a class="reference internal" href="distributed#multi-gpu-collective-functions">Multi-GPU collective functions</a></li> <li class="toctree-l2"><a class="reference internal" href="distributed#third-party-backends">Third-party backends</a></li> <li class="toctree-l2"><a class="reference internal" href="distributed#launch-utility">Launch utility</a></li> <li class="toctree-l2"><a class="reference internal" href="distributed#spawn-utility">Spawn utility</a></li> <li class="toctree-l2"><a class="reference internal" href="distributed#debugging-torch-distributed-applications">Debugging <code>torch.distributed</code> applications</a></li> <li class="toctree-l2"><a class="reference internal" href="distributed#logging">Logging</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="distributed.algorithms.join">torch.distributed.algorithms.join</a><ul> <li class="toctree-l2"><a class="reference internal" href="distributed.algorithms.join#torch.distributed.algorithms.Join"><code>Join</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.algorithms.join#torch.distributed.algorithms.Joinable"><code>Joinable</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.algorithms.join#torch.distributed.algorithms.JoinHook"><code>JoinHook</code></a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="distributed.elastic">torch.distributed.elastic</a><ul> <li class="toctree-l2"><a class="reference internal" href="distributed.elastic#get-started">Get Started</a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.elastic#documentation">Documentation</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="fsdp">torch.distributed.fsdp</a><ul> <li class="toctree-l2"><a class="reference internal" href="fsdp#torch.distributed.fsdp.FullyShardedDataParallel"><code>FullyShardedDataParallel</code></a></li> <li class="toctree-l2"><a class="reference internal" href="fsdp#torch.distributed.fsdp.BackwardPrefetch"><code>BackwardPrefetch</code></a></li> <li class="toctree-l2"><a class="reference internal" href="fsdp#torch.distributed.fsdp.ShardingStrategy"><code>ShardingStrategy</code></a></li> <li class="toctree-l2"><a class="reference internal" href="fsdp#torch.distributed.fsdp.MixedPrecision"><code>MixedPrecision</code></a></li> <li class="toctree-l2"><a class="reference internal" href="fsdp#torch.distributed.fsdp.CPUOffload"><code>CPUOffload</code></a></li> <li class="toctree-l2"><a class="reference internal" href="fsdp#torch.distributed.fsdp.StateDictConfig"><code>StateDictConfig</code></a></li> <li class="toctree-l2"><a class="reference internal" href="fsdp#torch.distributed.fsdp.FullStateDictConfig"><code>FullStateDictConfig</code></a></li> <li class="toctree-l2"><a class="reference internal" href="fsdp#torch.distributed.fsdp.ShardedStateDictConfig"><code>ShardedStateDictConfig</code></a></li> <li class="toctree-l2"><a class="reference internal" href="fsdp#torch.distributed.fsdp.LocalStateDictConfig"><code>LocalStateDictConfig</code></a></li> <li class="toctree-l2"><a class="reference internal" href="fsdp#torch.distributed.fsdp.OptimStateDictConfig"><code>OptimStateDictConfig</code></a></li> <li class="toctree-l2"><a class="reference internal" href="fsdp#torch.distributed.fsdp.FullOptimStateDictConfig"><code>FullOptimStateDictConfig</code></a></li> <li class="toctree-l2"><a class="reference internal" href="fsdp#torch.distributed.fsdp.ShardedOptimStateDictConfig"><code>ShardedOptimStateDictConfig</code></a></li> <li class="toctree-l2"><a class="reference internal" href="fsdp#torch.distributed.fsdp.LocalOptimStateDictConfig"><code>LocalOptimStateDictConfig</code></a></li> <li class="toctree-l2"><a class="reference internal" href="fsdp#torch.distributed.fsdp.StateDictSettings"><code>StateDictSettings</code></a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="distributed.optim">torch.distributed.optim</a><ul> <li class="toctree-l2"><a class="reference internal" href="distributed.optim#torch.distributed.optim.DistributedOptimizer"><code>DistributedOptimizer</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.optim#torch.distributed.optim.PostLocalSGDOptimizer"><code>PostLocalSGDOptimizer</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.optim#torch.distributed.optim.ZeroRedundancyOptimizer"><code>ZeroRedundancyOptimizer</code></a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="distributed.tensor.parallel">torch.distributed.tensor.parallel</a><ul> <li class="toctree-l2"><a class="reference internal" href="distributed.tensor.parallel#torch.distributed.tensor.parallel.parallelize_module"><code>parallelize_module()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.tensor.parallel#torch.distributed.tensor.parallel.style.RowwiseParallel"><code>RowwiseParallel</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.tensor.parallel#torch.distributed.tensor.parallel.style.ColwiseParallel"><code>ColwiseParallel</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.tensor.parallel#torch.distributed.tensor.parallel.style.PairwiseParallel"><code>PairwiseParallel</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.tensor.parallel#torch.distributed.tensor.parallel.style.SequenceParallel"><code>SequenceParallel</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_input_replicate_1d"><code>make_input_replicate_1d()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_input_reshard_replicate"><code>make_input_reshard_replicate()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_input_shard_1d"><code>make_input_shard_1d()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_input_shard_1d_last_dim"><code>make_input_shard_1d_last_dim()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_output_replicate_1d"><code>make_output_replicate_1d()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_output_reshard_tensor"><code>make_output_reshard_tensor()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_output_shard_1d"><code>make_output_shard_1d()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_output_tensor"><code>make_output_tensor()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.tensor.parallel#torch.distributed.tensor.parallel.fsdp.enable_2d_with_fsdp"><code>enable_2d_with_fsdp()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.tensor.parallel#torch.distributed.tensor.parallel.ddp.pre_dp_module_transform"><code>pre_dp_module_transform()</code></a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="distributed.checkpoint">torch.distributed.checkpoint</a><ul> <li class="toctree-l2"><a class="reference internal" href="distributed.checkpoint#torch.distributed.checkpoint.load_state_dict"><code>load_state_dict()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.checkpoint#torch.distributed.checkpoint.save_state_dict"><code>save_state_dict()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.checkpoint#torch.distributed.checkpoint.StorageReader"><code>StorageReader</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.checkpoint#torch.distributed.checkpoint.StorageWriter"><code>StorageWriter</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner"><code>LoadPlanner</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.checkpoint#torch.distributed.checkpoint.LoadPlan"><code>LoadPlan</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.checkpoint#torch.distributed.checkpoint.ReadItem"><code>ReadItem</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.checkpoint#torch.distributed.checkpoint.SavePlanner"><code>SavePlanner</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.checkpoint#torch.distributed.checkpoint.SavePlan"><code>SavePlan</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.checkpoint#torch.distributed.checkpoint.WriteItem"><code>WriteItem</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.checkpoint#torch.distributed.checkpoint.FileSystemReader"><code>FileSystemReader</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.checkpoint#torch.distributed.checkpoint.FileSystemWriter"><code>FileSystemWriter</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.checkpoint#torch.distributed.checkpoint.DefaultSavePlanner"><code>DefaultSavePlanner</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributed.checkpoint#torch.distributed.checkpoint.DefaultLoadPlanner"><code>DefaultLoadPlanner</code></a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="distributions">torch.distributions</a><ul> <li class="toctree-l2"><a class="reference internal" href="distributions#score-function">Score function</a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#pathwise-derivative">Pathwise derivative</a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#distribution"><span class="hidden-section">Distribution</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#exponentialfamily"><span class="hidden-section">ExponentialFamily</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#bernoulli"><span class="hidden-section">Bernoulli</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#beta"><span class="hidden-section">Beta</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#binomial"><span class="hidden-section">Binomial</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#categorical"><span class="hidden-section">Categorical</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#cauchy"><span class="hidden-section">Cauchy</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#chi2"><span class="hidden-section">Chi2</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#continuousbernoulli"><span class="hidden-section">ContinuousBernoulli</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#dirichlet"><span class="hidden-section">Dirichlet</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#exponential"><span class="hidden-section">Exponential</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#fishersnedecor"><span class="hidden-section">FisherSnedecor</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#gamma"><span class="hidden-section">Gamma</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#geometric"><span class="hidden-section">Geometric</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#gumbel"><span class="hidden-section">Gumbel</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#halfcauchy"><span class="hidden-section">HalfCauchy</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#halfnormal"><span class="hidden-section">HalfNormal</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#independent"><span class="hidden-section">Independent</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#kumaraswamy"><span class="hidden-section">Kumaraswamy</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#lkjcholesky"><span class="hidden-section">LKJCholesky</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#laplace"><span class="hidden-section">Laplace</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#lognormal"><span class="hidden-section">LogNormal</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#lowrankmultivariatenormal"><span class="hidden-section">LowRankMultivariateNormal</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#mixturesamefamily"><span class="hidden-section">MixtureSameFamily</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#multinomial"><span class="hidden-section">Multinomial</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#multivariatenormal"><span class="hidden-section">MultivariateNormal</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#negativebinomial"><span class="hidden-section">NegativeBinomial</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#normal"><span class="hidden-section">Normal</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#onehotcategorical"><span class="hidden-section">OneHotCategorical</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#pareto"><span class="hidden-section">Pareto</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#poisson"><span class="hidden-section">Poisson</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#relaxedbernoulli"><span class="hidden-section">RelaxedBernoulli</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#logitrelaxedbernoulli"><span class="hidden-section">LogitRelaxedBernoulli</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#relaxedonehotcategorical"><span class="hidden-section">RelaxedOneHotCategorical</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#studentt"><span class="hidden-section">StudentT</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#transformeddistribution"><span class="hidden-section">TransformedDistribution</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#uniform"><span class="hidden-section">Uniform</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#vonmises"><span class="hidden-section">VonMises</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#weibull"><span class="hidden-section">Weibull</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#wishart"><span class="hidden-section">Wishart</span></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#module-torch.distributions.kl"><code>KL Divergence</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#module-torch.distributions.transforms"><code>Transforms</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#module-torch.distributions.constraints"><code>Constraints</code></a></li> <li class="toctree-l2"><a class="reference internal" href="distributions#module-torch.distributions.constraint_registry"><code>Constraint Registry</code></a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="torch.compiler">torch.compiler</a><ul> <li class="toctree-l2"><a class="reference internal" href="torch.compiler#read-more">Read More</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="fft">torch.fft</a><ul> <li class="toctree-l2"><a class="reference internal" href="fft#fast-fourier-transforms">Fast Fourier Transforms</a></li> <li class="toctree-l2"><a class="reference internal" href="fft#helper-functions">Helper Functions</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="func">torch.func</a><ul> <li class="toctree-l2"><a class="reference internal" href="func#what-are-composable-function-transforms">What are composable function transforms?</a></li> <li class="toctree-l2"><a class="reference internal" href="func#why-composable-function-transforms">Why composable function transforms?</a></li> <li class="toctree-l2"><a class="reference internal" href="func#read-more">Read More</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="futures">torch.futures</a><ul> <li class="toctree-l2"><a class="reference internal" href="futures#torch.futures.Future"><code>Future</code></a></li> <li class="toctree-l2"><a class="reference internal" href="futures#torch.futures.collect_all"><code>collect_all()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="futures#torch.futures.wait_all"><code>wait_all()</code></a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="fx">torch.fx</a><ul> <li class="toctree-l2"><a class="reference internal" href="fx#module-torch.fx">Overview</a></li> <li class="toctree-l2"><a class="reference internal" href="fx#writing-transformations">Writing Transformations</a></li> <li class="toctree-l2"><a class="reference internal" href="fx#debugging">Debugging</a></li> <li class="toctree-l2"><a class="reference internal" href="fx#limitations-of-symbolic-tracing">Limitations of Symbolic Tracing</a></li> <li class="toctree-l2"><a class="reference internal" href="fx#api-reference">API Reference</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="hub">torch.hub</a><ul> <li class="toctree-l2"><a class="reference internal" href="hub#publishing-models">Publishing models</a></li> <li class="toctree-l2"><a class="reference internal" href="hub#loading-models-from-hub">Loading models from Hub</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="jit">torch.jit</a><ul> <li class="toctree-l2"><a class="reference internal" href="jit_language_reference_v2">TorchScript Language Reference</a></li> <li class="toctree-l2"><a class="reference internal" href="jit#creating-torchscript-code">Creating TorchScript Code</a></li> <li class="toctree-l2"><a class="reference internal" href="jit#mixing-tracing-and-scripting">Mixing Tracing and Scripting</a></li> <li class="toctree-l2"><a class="reference internal" href="jit#torchscript-language">TorchScript Language</a></li> <li class="toctree-l2"><a class="reference internal" href="jit#built-in-functions-and-modules">Built-in Functions and Modules</a></li> <li class="toctree-l2"><a class="reference internal" href="jit#debugging">Debugging</a></li> <li class="toctree-l2"><a class="reference internal" href="jit#frequently-asked-questions">Frequently Asked Questions</a></li> <li class="toctree-l2"><a class="reference internal" href="jit#known-issues">Known Issues</a></li> <li class="toctree-l2"><a class="reference internal" href="jit#appendix">Appendix</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="linalg">torch.linalg</a><ul> <li class="toctree-l2"><a class="reference internal" href="linalg#matrix-properties">Matrix Properties</a></li> <li class="toctree-l2"><a class="reference internal" href="linalg#decompositions">Decompositions</a></li> <li class="toctree-l2"><a class="reference internal" href="linalg#solvers">Solvers</a></li> <li class="toctree-l2"><a class="reference internal" href="linalg#inverses">Inverses</a></li> <li class="toctree-l2"><a class="reference internal" href="linalg#matrix-functions">Matrix Functions</a></li> <li class="toctree-l2"><a class="reference internal" href="linalg#matrix-products">Matrix Products</a></li> <li class="toctree-l2"><a class="reference internal" href="linalg#tensor-operations">Tensor Operations</a></li> <li class="toctree-l2"><a class="reference internal" href="linalg#misc">Misc</a></li> <li class="toctree-l2"><a class="reference internal" href="linalg#experimental-functions">Experimental Functions</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="monitor">torch.monitor</a><ul> <li class="toctree-l2"><a class="reference internal" href="monitor#module-torch.monitor">API Reference</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="signal">torch.signal</a><ul> <li class="toctree-l2"><a class="reference internal" href="signal#module-torch.signal.windows">torch.signal.windows</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="special">torch.special</a><ul> <li class="toctree-l2"><a class="reference internal" href="special#functions">Functions</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="torch.overrides">torch.overrides</a><ul> <li class="toctree-l2"><a class="reference internal" href="torch.overrides#functions">Functions</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="package">torch.package</a><ul> <li class="toctree-l2"><a class="reference internal" href="package#tutorials">Tutorials</a></li> <li class="toctree-l2"><a class="reference internal" href="package#how-do-i">How do I…</a></li> <li class="toctree-l2"><a class="reference internal" href="package#explanation">Explanation</a></li> <li class="toctree-l2"><a class="reference internal" href="package#api-reference">API Reference</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="profiler">torch.profiler</a><ul> <li class="toctree-l2"><a class="reference internal" href="profiler#module-torch.profiler">Overview</a></li> <li class="toctree-l2"><a class="reference internal" href="profiler#api-reference">API Reference</a></li> <li class="toctree-l2"><a class="reference internal" href="profiler#intel-instrumentation-and-tracing-technology-apis">Intel Instrumentation and Tracing Technology APIs</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="nn.init">torch.nn.init</a><ul> <li class="toctree-l2"><a class="reference internal" href="nn.init#torch.nn.init.calculate_gain"><code>calculate_gain()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="nn.init#torch.nn.init.uniform_"><code>uniform_()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="nn.init#torch.nn.init.normal_"><code>normal_()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="nn.init#torch.nn.init.constant_"><code>constant_()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="nn.init#torch.nn.init.ones_"><code>ones_()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="nn.init#torch.nn.init.zeros_"><code>zeros_()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="nn.init#torch.nn.init.eye_"><code>eye_()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="nn.init#torch.nn.init.dirac_"><code>dirac_()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="nn.init#torch.nn.init.xavier_uniform_"><code>xavier_uniform_()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="nn.init#torch.nn.init.xavier_normal_"><code>xavier_normal_()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="nn.init#torch.nn.init.kaiming_uniform_"><code>kaiming_uniform_()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="nn.init#torch.nn.init.kaiming_normal_"><code>kaiming_normal_()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="nn.init#torch.nn.init.trunc_normal_"><code>trunc_normal_()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="nn.init#torch.nn.init.orthogonal_"><code>orthogonal_()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="nn.init#torch.nn.init.sparse_"><code>sparse_()</code></a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="onnx">torch.onnx</a><ul> <li class="toctree-l2"><a class="reference internal" href="onnx#overview">Overview</a></li> <li class="toctree-l2"><a class="reference internal" href="onnx#torchdynamo-based-onnx-exporter">TorchDynamo-based ONNX Exporter</a></li> <li class="toctree-l2"><a class="reference internal" href="onnx#torchscript-based-onnx-exporter">TorchScript-based ONNX Exporter</a></li> <li class="toctree-l2"><a class="reference internal" href="onnx#contributing-developing">Contributing / Developing</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="optim">torch.optim</a><ul> <li class="toctree-l2"><a class="reference internal" href="optim#how-to-use-an-optimizer">How to use an optimizer</a></li> <li class="toctree-l2"><a class="reference internal" href="optim#base-class">Base class</a></li> <li class="toctree-l2"><a class="reference internal" href="optim#algorithms">Algorithms</a></li> <li class="toctree-l2"><a class="reference internal" href="optim#how-to-adjust-learning-rate">How to adjust learning rate</a></li> <li class="toctree-l2"><a class="reference internal" href="optim#weight-averaging-swa-and-ema">Weight Averaging (SWA and EMA)</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="complex_numbers">Complex Numbers</a><ul> <li class="toctree-l2"><a class="reference internal" href="complex_numbers#creating-complex-tensors">Creating Complex Tensors</a></li> <li class="toctree-l2"><a class="reference internal" href="complex_numbers#transition-from-the-old-representation">Transition from the old representation</a></li> <li class="toctree-l2"><a class="reference internal" href="complex_numbers#accessing-real-and-imag">Accessing real and imag</a></li> <li class="toctree-l2"><a class="reference internal" href="complex_numbers#angle-and-abs">Angle and abs</a></li> <li class="toctree-l2"><a class="reference internal" href="complex_numbers#linear-algebra">Linear Algebra</a></li> <li class="toctree-l2"><a class="reference internal" href="complex_numbers#serialization">Serialization</a></li> <li class="toctree-l2"><a class="reference internal" href="complex_numbers#autograd">Autograd</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="ddp_comm_hooks">DDP Communication Hooks</a><ul> <li class="toctree-l2"><a class="reference internal" href="ddp_comm_hooks#how-to-use-a-communication-hook">How to Use a Communication Hook?</a></li> <li class="toctree-l2"><a class="reference internal" href="ddp_comm_hooks#what-does-a-communication-hook-operate-on">What Does a Communication Hook Operate On?</a></li> <li class="toctree-l2"><a class="reference internal" href="ddp_comm_hooks#default-communication-hooks">Default Communication Hooks</a></li> <li class="toctree-l2"><a class="reference internal" href="ddp_comm_hooks#powersgd-communication-hook">PowerSGD Communication Hook</a></li> <li class="toctree-l2"><a class="reference internal" href="ddp_comm_hooks#debugging-communication-hooks">Debugging Communication Hooks</a></li> <li class="toctree-l2"><a class="reference internal" href="ddp_comm_hooks#checkpointing-of-communication-hooks">Checkpointing of Communication Hooks</a></li> <li class="toctree-l2"><a class="reference internal" href="ddp_comm_hooks#acknowledgements">Acknowledgements</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="pipeline">Pipeline Parallelism</a><ul> <li class="toctree-l2"><a class="reference internal" href="pipeline#model-parallelism-using-multiple-gpus">Model Parallelism using multiple GPUs</a></li> <li class="toctree-l2"><a class="reference internal" href="pipeline#pipelined-execution">Pipelined Execution</a></li> <li class="toctree-l2"><a class="reference internal" href="pipeline#pipe-apis-in-pytorch">Pipe APIs in PyTorch</a></li> <li class="toctree-l2"><a class="reference internal" href="pipeline#tutorials">Tutorials</a></li> <li class="toctree-l2"><a class="reference internal" href="pipeline#acknowledgements">Acknowledgements</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="quantization">Quantization</a><ul> <li class="toctree-l2"><a class="reference internal" href="quantization#introduction-to-quantization">Introduction to Quantization</a></li> <li class="toctree-l2"><a class="reference internal" href="quantization#quantization-api-summary">Quantization API Summary</a></li> <li class="toctree-l2"><a class="reference internal" href="quantization#quantization-stack">Quantization Stack</a></li> <li class="toctree-l2"><a class="reference internal" href="quantization#quantization-support-matrix">Quantization Support Matrix</a></li> <li class="toctree-l2"><a class="reference internal" href="quantization#quantization-api-reference">Quantization API Reference</a></li> <li class="toctree-l2"><a class="reference internal" href="quantization#quantization-backend-configuration">Quantization Backend Configuration</a></li> <li class="toctree-l2"><a class="reference internal" href="quantization#quantization-accuracy-debugging">Quantization Accuracy Debugging</a></li> <li class="toctree-l2"><a class="reference internal" href="quantization#quantization-customizations">Quantization Customizations</a></li> <li class="toctree-l2"><a class="reference internal" href="quantization#best-practices">Best Practices</a></li> <li class="toctree-l2"><a class="reference internal" href="quantization#frequently-asked-questions">Frequently Asked Questions</a></li> <li class="toctree-l2"><a class="reference internal" href="quantization#common-errors">Common Errors</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="rpc">Distributed RPC Framework</a><ul> <li class="toctree-l2"><a class="reference internal" href="rpc#basics">Basics</a></li> <li class="toctree-l2"><a class="reference internal" href="rpc#rpc">RPC</a></li> <li class="toctree-l2"><a class="reference internal" href="rpc#rref">RRef</a></li> <li class="toctree-l2"><a class="reference internal" href="rpc#remotemodule">RemoteModule</a></li> <li class="toctree-l2"><a class="reference internal" href="rpc#distributed-autograd-framework">Distributed Autograd Framework</a></li> <li class="toctree-l2"><a class="reference internal" href="rpc#distributed-optimizer">Distributed Optimizer</a></li> <li class="toctree-l2"><a class="reference internal" href="rpc#design-notes">Design Notes</a></li> <li class="toctree-l2"><a class="reference internal" href="rpc#tutorials">Tutorials</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="random">torch.random</a><ul> <li class="toctree-l2"><a class="reference internal" href="random#torch.random.fork_rng"><code>fork_rng()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="random#torch.random.get_rng_state"><code>get_rng_state()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="random#torch.random.initial_seed"><code>initial_seed()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="random#torch.random.manual_seed"><code>manual_seed()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="random#torch.random.seed"><code>seed()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="random#torch.random.set_rng_state"><code>set_rng_state()</code></a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="masked">torch.masked</a><ul> <li class="toctree-l2"><a class="reference internal" href="masked#introduction">Introduction</a></li> <li class="toctree-l2"><a class="reference internal" href="masked#supported-operators">Supported Operators</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="nested">torch.nested</a><ul> <li class="toctree-l2"><a class="reference internal" href="nested#introduction">Introduction</a></li> <li class="toctree-l2"><a class="reference internal" href="nested#construction">Construction</a></li> <li class="toctree-l2"><a class="reference internal" href="nested#size">size</a></li> <li class="toctree-l2"><a class="reference internal" href="nested#unbind">unbind</a></li> <li class="toctree-l2"><a class="reference internal" href="nested#nested-tensor-constructor-and-conversion-functions">Nested tensor constructor and conversion functions</a></li> <li class="toctree-l2"><a class="reference internal" href="nested#supported-operations">Supported operations</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="sparse">torch.sparse</a><ul> <li class="toctree-l2"><a class="reference internal" href="sparse#why-and-when-to-use-sparsity">Why and when to use sparsity</a></li> <li class="toctree-l2"><a class="reference internal" href="sparse#functionality-overview">Functionality overview</a></li> <li class="toctree-l2"><a class="reference internal" href="sparse#operator-overview">Operator overview</a></li> <li class="toctree-l2"><a class="reference internal" href="sparse#sparse-semi-structured-tensors">Sparse Semi-Structured Tensors</a></li> <li class="toctree-l2"><a class="reference internal" href="sparse#sparse-coo-tensors">Sparse COO tensors</a></li> <li class="toctree-l2"><a class="reference internal" href="sparse#sparse-compressed-tensors">Sparse Compressed Tensors</a></li> <li class="toctree-l2"><a class="reference internal" href="sparse#supported-operations">Supported operations</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="storage">torch.Storage</a><ul> <li class="toctree-l2"><a class="reference internal" href="storage#torch.TypedStorage"><code>TypedStorage</code></a></li> <li class="toctree-l2"><a class="reference internal" href="storage#torch.UntypedStorage"><code>UntypedStorage</code></a></li> <li class="toctree-l2"><a class="reference internal" href="storage#torch.DoubleStorage"><code>DoubleStorage</code></a></li> <li class="toctree-l2"><a class="reference internal" href="storage#torch.FloatStorage"><code>FloatStorage</code></a></li> <li class="toctree-l2"><a class="reference internal" href="storage#torch.HalfStorage"><code>HalfStorage</code></a></li> <li class="toctree-l2"><a class="reference internal" href="storage#torch.LongStorage"><code>LongStorage</code></a></li> <li class="toctree-l2"><a class="reference internal" href="storage#torch.IntStorage"><code>IntStorage</code></a></li> <li class="toctree-l2"><a class="reference internal" href="storage#torch.ShortStorage"><code>ShortStorage</code></a></li> <li class="toctree-l2"><a class="reference internal" href="storage#torch.CharStorage"><code>CharStorage</code></a></li> <li class="toctree-l2"><a class="reference internal" href="storage#torch.ByteStorage"><code>ByteStorage</code></a></li> <li class="toctree-l2"><a class="reference internal" href="storage#torch.BoolStorage"><code>BoolStorage</code></a></li> <li class="toctree-l2"><a class="reference internal" href="storage#torch.BFloat16Storage"><code>BFloat16Storage</code></a></li> <li class="toctree-l2"><a class="reference internal" href="storage#torch.ComplexDoubleStorage"><code>ComplexDoubleStorage</code></a></li> <li class="toctree-l2"><a class="reference internal" href="storage#torch.ComplexFloatStorage"><code>ComplexFloatStorage</code></a></li> <li class="toctree-l2"><a class="reference internal" href="storage#torch.QUInt8Storage"><code>QUInt8Storage</code></a></li> <li class="toctree-l2"><a class="reference internal" href="storage#torch.QInt8Storage"><code>QInt8Storage</code></a></li> <li class="toctree-l2"><a class="reference internal" href="storage#torch.QInt32Storage"><code>QInt32Storage</code></a></li> <li class="toctree-l2"><a class="reference internal" href="storage#torch.QUInt4x2Storage"><code>QUInt4x2Storage</code></a></li> <li class="toctree-l2"><a class="reference internal" href="storage#torch.QUInt2x4Storage"><code>QUInt2x4Storage</code></a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="testing">torch.testing</a><ul> <li class="toctree-l2"><a class="reference internal" href="testing#torch.testing.assert_close"><code>assert_close()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="testing#torch.testing.make_tensor"><code>make_tensor()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="testing#torch.testing.assert_allclose"><code>assert_allclose()</code></a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="utils">torch.utils</a><ul> <li class="toctree-l2"><a class="reference internal" href="generated/torch.utils.rename_privateuse1_backend">torch.utils.rename_privateuse1_backend</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.utils.generate_methods_for_privateuse1_backend">torch.utils.generate_methods_for_privateuse1_backend</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.utils.get_cpp_backtrace">torch.utils.get_cpp_backtrace</a></li> <li class="toctree-l2"><a class="reference internal" href="generated/torch.utils.set_module">torch.utils.set_module</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="benchmark_utils">torch.utils.benchmark</a><ul> <li class="toctree-l2"><a class="reference internal" href="benchmark_utils#torch.utils.benchmark.Timer"><code>Timer</code></a></li> <li class="toctree-l2"><a class="reference internal" href="benchmark_utils#torch.utils.benchmark.Measurement"><code>Measurement</code></a></li> <li class="toctree-l2"><a class="reference internal" href="benchmark_utils#torch.utils.benchmark.CallgrindStats"><code>CallgrindStats</code></a></li> <li class="toctree-l2"><a class="reference internal" href="benchmark_utils#torch.utils.benchmark.FunctionCounts"><code>FunctionCounts</code></a></li> </ul> </li> <li class="toctree-l1"><a class="reference internal" href="bottleneck">torch.utils.bottleneck</a></li> <li class="toctree-l1">
<a class="reference internal" href="checkpoint">torch.utils.checkpoint</a><ul> <li class="toctree-l2"><a class="reference internal" href="checkpoint#torch.utils.checkpoint.checkpoint"><code>checkpoint()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="checkpoint#torch.utils.checkpoint.checkpoint_sequential"><code>checkpoint_sequential()</code></a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="cpp_extension">torch.utils.cpp_extension</a><ul> <li class="toctree-l2"><a class="reference internal" href="cpp_extension#torch.utils.cpp_extension.CppExtension"><code>CppExtension()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="cpp_extension#torch.utils.cpp_extension.CUDAExtension"><code>CUDAExtension()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="cpp_extension#torch.utils.cpp_extension.BuildExtension"><code>BuildExtension()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="cpp_extension#torch.utils.cpp_extension.load"><code>load()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="cpp_extension#torch.utils.cpp_extension.load_inline"><code>load_inline()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="cpp_extension#torch.utils.cpp_extension.include_paths"><code>include_paths()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="cpp_extension#torch.utils.cpp_extension.get_compiler_abi_compatibility_and_version"><code>get_compiler_abi_compatibility_and_version()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="cpp_extension#torch.utils.cpp_extension.verify_ninja_availability"><code>verify_ninja_availability()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="cpp_extension#torch.utils.cpp_extension.is_ninja_available"><code>is_ninja_available()</code></a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="data">torch.utils.data</a><ul> <li class="toctree-l2"><a class="reference internal" href="data#dataset-types">Dataset Types</a></li> <li class="toctree-l2"><a class="reference internal" href="data#data-loading-order-and-sampler">Data Loading Order and <code>Sampler</code></a></li> <li class="toctree-l2"><a class="reference internal" href="data#loading-batched-and-non-batched-data">Loading Batched and Non-Batched Data</a></li> <li class="toctree-l2"><a class="reference internal" href="data#single-and-multi-process-data-loading">Single- and Multi-process Data Loading</a></li> <li class="toctree-l2"><a class="reference internal" href="data#memory-pinning">Memory Pinning</a></li> </ul> </li> <li class="toctree-l1"><a class="reference internal" href="jit_utils">torch.utils.jit</a></li> <li class="toctree-l1">
<a class="reference internal" href="dlpack">torch.utils.dlpack</a><ul> <li class="toctree-l2"><a class="reference internal" href="dlpack#torch.utils.dlpack.from_dlpack"><code>from_dlpack()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="dlpack#torch.utils.dlpack.to_dlpack"><code>to_dlpack()</code></a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="mobile_optimizer">torch.utils.mobile_optimizer</a><ul> <li class="toctree-l2"><a class="reference internal" href="mobile_optimizer#torch.utils.mobile_optimizer.optimize_for_mobile"><code>optimize_for_mobile()</code></a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="model_zoo">torch.utils.model_zoo</a><ul> <li class="toctree-l2"><a class="reference internal" href="model_zoo#torch.utils.model_zoo.load_url"><code>load_url()</code></a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="tensorboard">torch.utils.tensorboard</a><ul> <li class="toctree-l2"><a class="reference internal" href="tensorboard#torch.utils.tensorboard.writer.SummaryWriter"><code>SummaryWriter</code></a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="type_info">Type Info</a><ul> <li class="toctree-l2"><a class="reference internal" href="type_info#torch-finfo">torch.finfo</a></li> <li class="toctree-l2"><a class="reference internal" href="type_info#torch-iinfo">torch.iinfo</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="named_tensor">Named Tensors</a><ul> <li class="toctree-l2"><a class="reference internal" href="named_tensor#creating-named-tensors">Creating named tensors</a></li> <li class="toctree-l2"><a class="reference internal" href="named_tensor#named-dimensions">Named dimensions</a></li> <li class="toctree-l2"><a class="reference internal" href="named_tensor#name-propagation-semantics">Name propagation semantics</a></li> <li class="toctree-l2"><a class="reference internal" href="named_tensor#explicit-alignment-by-names">Explicit alignment by names</a></li> <li class="toctree-l2"><a class="reference internal" href="named_tensor#manipulating-dimensions">Manipulating dimensions</a></li> <li class="toctree-l2"><a class="reference internal" href="named_tensor#autograd-support">Autograd support</a></li> <li class="toctree-l2"><a class="reference internal" href="named_tensor#currently-supported-operations-and-subsystems">Currently supported operations and subsystems</a></li> <li class="toctree-l2"><a class="reference internal" href="named_tensor#named-tensor-api-reference">Named tensor API reference</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="name_inference">Named Tensors operator coverage</a><ul> <li class="toctree-l2"><a class="reference internal" href="name_inference#keeps-input-names">Keeps input names</a></li> <li class="toctree-l2"><a class="reference internal" href="name_inference#removes-dimensions">Removes dimensions</a></li> <li class="toctree-l2"><a class="reference internal" href="name_inference#unifies-names-from-inputs">Unifies names from inputs</a></li> <li class="toctree-l2"><a class="reference internal" href="name_inference#permutes-dimensions">Permutes dimensions</a></li> <li class="toctree-l2"><a class="reference internal" href="name_inference#contracts-away-dims">Contracts away dims</a></li> <li class="toctree-l2"><a class="reference internal" href="name_inference#factory-functions">Factory functions</a></li> <li class="toctree-l2"><a class="reference internal" href="name_inference#out-function-and-in-place-variants">out function and in-place variants</a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="config_mod">torch.__config__</a><ul> <li class="toctree-l2"><a class="reference internal" href="config_mod#torch.__config__.show"><code>show()</code></a></li> <li class="toctree-l2"><a class="reference internal" href="config_mod#torch.__config__.parallel_info"><code>parallel_info()</code></a></li> </ul> </li> <li class="toctree-l1">
<a class="reference internal" href="logging">torch._logging</a><ul> <li class="toctree-l2"><a class="reference internal" href="generated/torch._logging.set_logs">torch._logging.set_logs</a></li> </ul> </li> </ul>   <p class="caption" role="heading"><span class="caption-text">Libraries</span></p> <ul> <li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable">torchaudio</a></li> <li class="toctree-l1"><a class="reference external" href="https://pytorch.org/data">TorchData</a></li> <li class="toctree-l1"><a class="reference external" href="https://pytorch.org/torchrec">TorchRec</a></li> <li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li> <li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text/stable">torchtext</a></li> <li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision/stable">torchvision</a></li> <li class="toctree-l1"><a class="reference external" href="https://pytorch.org/xla/">PyTorch on XLA Devices</a></li> </ul>    <h1 id="indices-and-tables">Indices and tables</h1> <ul class="simple"> <li><a class="reference internal" href="https://pytorch.org/docs/2.1/genindex.html"><span class="std std-ref">Index</span></a></li> <li><a class="reference internal" href="https://pytorch.org/docs/2.1/py-modindex.html"><span class="std std-ref">Module Index</span></a></li> </ul><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/" class="_attribution-link">https://pytorch.org/docs/2.1/</a>
  </p>
</div>
