[{"name": "torch.__config__", "path": "config_mod", "type": "Miscellaneous", "text": ["Return a human-readable string with descriptions of the configuration of PyTorch.", "Returns detailed string with parallelization settings"]}, {"name": "torch.__config__.parallel_info()", "path": "config_mod#torch.__config__.parallel_info", "type": "Miscellaneous", "text": ["Returns detailed string with parallelization settings"]}, {"name": "torch.__config__.show()", "path": "config_mod#torch.__config__.show", "type": "Miscellaneous", "text": ["Return a human-readable string with descriptions of the configuration of PyTorch."]}, {"name": "torch._assert", "path": "generated/torch._assert", "type": "Torch", "text": ["A wrapper around Python\u2019s assert which is symbolically traceable."]}, {"name": "torch._foreach_abs", "path": "generated/torch._foreach_abs", "type": "Torch", "text": ["Apply torch.abs() to each Tensor of the input list."]}, {"name": "torch._foreach_abs_", "path": "generated/torch._foreach_abs_", "type": "Torch", "text": ["Apply torch.abs() to each Tensor of the input list."]}, {"name": "torch._foreach_acos", "path": "generated/torch._foreach_acos", "type": "Torch", "text": ["Apply torch.acos() to each Tensor of the input list."]}, {"name": "torch._foreach_acos_", "path": "generated/torch._foreach_acos_", "type": "Torch", "text": ["Apply torch.acos() to each Tensor of the input list."]}, {"name": "torch._foreach_asin", "path": "generated/torch._foreach_asin", "type": "Torch", "text": ["Apply torch.asin() to each Tensor of the input list."]}, {"name": "torch._foreach_asin_", "path": "generated/torch._foreach_asin_", "type": "Torch", "text": ["Apply torch.asin() to each Tensor of the input list."]}, {"name": "torch._foreach_atan", "path": "generated/torch._foreach_atan", "type": "Torch", "text": ["Apply torch.atan() to each Tensor of the input list."]}, {"name": "torch._foreach_atan_", "path": "generated/torch._foreach_atan_", "type": "Torch", "text": ["Apply torch.atan() to each Tensor of the input list."]}, {"name": "torch._foreach_ceil", "path": "generated/torch._foreach_ceil", "type": "Torch", "text": ["Apply torch.ceil() to each Tensor of the input list."]}, {"name": "torch._foreach_ceil_", "path": "generated/torch._foreach_ceil_", "type": "Torch", "text": ["Apply torch.ceil() to each Tensor of the input list."]}, {"name": "torch._foreach_cos", "path": "generated/torch._foreach_cos", "type": "Torch", "text": ["Apply torch.cos() to each Tensor of the input list."]}, {"name": "torch._foreach_cos_", "path": "generated/torch._foreach_cos_", "type": "Torch", "text": ["Apply torch.cos() to each Tensor of the input list."]}, {"name": "torch._foreach_cosh", "path": "generated/torch._foreach_cosh", "type": "Torch", "text": ["Apply torch.cosh() to each Tensor of the input list."]}, {"name": "torch._foreach_cosh_", "path": "generated/torch._foreach_cosh_", "type": "Torch", "text": ["Apply torch.cosh() to each Tensor of the input list."]}, {"name": "torch._foreach_erf", "path": "generated/torch._foreach_erf", "type": "Torch", "text": ["Apply torch.erf() to each Tensor of the input list."]}, {"name": "torch._foreach_erf_", "path": "generated/torch._foreach_erf_", "type": "Torch", "text": ["Apply torch.erf() to each Tensor of the input list."]}, {"name": "torch._foreach_erfc", "path": "generated/torch._foreach_erfc", "type": "Torch", "text": ["Apply torch.erfc() to each Tensor of the input list."]}, {"name": "torch._foreach_erfc_", "path": "generated/torch._foreach_erfc_", "type": "Torch", "text": ["Apply torch.erfc() to each Tensor of the input list."]}, {"name": "torch._foreach_exp", "path": "generated/torch._foreach_exp", "type": "Torch", "text": ["Apply torch.exp() to each Tensor of the input list."]}, {"name": "torch._foreach_exp_", "path": "generated/torch._foreach_exp_", "type": "Torch", "text": ["Apply torch.exp() to each Tensor of the input list."]}, {"name": "torch._foreach_expm1", "path": "generated/torch._foreach_expm1", "type": "Torch", "text": ["Apply torch.expm1() to each Tensor of the input list."]}, {"name": "torch._foreach_expm1_", "path": "generated/torch._foreach_expm1_", "type": "Torch", "text": ["Apply torch.expm1() to each Tensor of the input list."]}, {"name": "torch._foreach_floor", "path": "generated/torch._foreach_floor", "type": "Torch", "text": ["Apply torch.floor() to each Tensor of the input list."]}, {"name": "torch._foreach_floor_", "path": "generated/torch._foreach_floor_", "type": "Torch", "text": ["Apply torch.floor() to each Tensor of the input list."]}, {"name": "torch._foreach_frac", "path": "generated/torch._foreach_frac", "type": "Torch", "text": ["Apply torch.frac() to each Tensor of the input list."]}, {"name": "torch._foreach_frac_", "path": "generated/torch._foreach_frac_", "type": "Torch", "text": ["Apply torch.frac() to each Tensor of the input list."]}, {"name": "torch._foreach_lgamma", "path": "generated/torch._foreach_lgamma", "type": "Torch", "text": ["Apply torch.lgamma() to each Tensor of the input list."]}, {"name": "torch._foreach_lgamma_", "path": "generated/torch._foreach_lgamma_", "type": "Torch", "text": ["Apply torch.lgamma() to each Tensor of the input list."]}, {"name": "torch._foreach_log", "path": "generated/torch._foreach_log", "type": "Torch", "text": ["Apply torch.log() to each Tensor of the input list."]}, {"name": "torch._foreach_log10", "path": "generated/torch._foreach_log10", "type": "Torch", "text": ["Apply torch.log10() to each Tensor of the input list."]}, {"name": "torch._foreach_log10_", "path": "generated/torch._foreach_log10_", "type": "Torch", "text": ["Apply torch.log10() to each Tensor of the input list."]}, {"name": "torch._foreach_log1p", "path": "generated/torch._foreach_log1p", "type": "Torch", "text": ["Apply torch.log1p() to each Tensor of the input list."]}, {"name": "torch._foreach_log1p_", "path": "generated/torch._foreach_log1p_", "type": "Torch", "text": ["Apply torch.log1p() to each Tensor of the input list."]}, {"name": "torch._foreach_log2", "path": "generated/torch._foreach_log2", "type": "Torch", "text": ["Apply torch.log2() to each Tensor of the input list."]}, {"name": "torch._foreach_log2_", "path": "generated/torch._foreach_log2_", "type": "Torch", "text": ["Apply torch.log2() to each Tensor of the input list."]}, {"name": "torch._foreach_log_", "path": "generated/torch._foreach_log_", "type": "Torch", "text": ["Apply torch.log() to each Tensor of the input list."]}, {"name": "torch._foreach_neg", "path": "generated/torch._foreach_neg", "type": "Torch", "text": ["Apply torch.neg() to each Tensor of the input list."]}, {"name": "torch._foreach_neg_", "path": "generated/torch._foreach_neg_", "type": "Torch", "text": ["Apply torch.neg() to each Tensor of the input list."]}, {"name": "torch._foreach_reciprocal", "path": "generated/torch._foreach_reciprocal", "type": "Torch", "text": ["Apply torch.reciprocal() to each Tensor of the input list."]}, {"name": "torch._foreach_reciprocal_", "path": "generated/torch._foreach_reciprocal_", "type": "Torch", "text": ["Apply torch.reciprocal() to each Tensor of the input list."]}, {"name": "torch._foreach_round", "path": "generated/torch._foreach_round", "type": "Torch", "text": ["Apply torch.round() to each Tensor of the input list."]}, {"name": "torch._foreach_round_", "path": "generated/torch._foreach_round_", "type": "Torch", "text": ["Apply torch.round() to each Tensor of the input list."]}, {"name": "torch._foreach_sigmoid", "path": "generated/torch._foreach_sigmoid", "type": "Torch", "text": ["Apply torch.sigmoid() to each Tensor of the input list."]}, {"name": "torch._foreach_sigmoid_", "path": "generated/torch._foreach_sigmoid_", "type": "Torch", "text": ["Apply torch.sigmoid() to each Tensor of the input list."]}, {"name": "torch._foreach_sin", "path": "generated/torch._foreach_sin", "type": "Torch", "text": ["Apply torch.sin() to each Tensor of the input list."]}, {"name": "torch._foreach_sin_", "path": "generated/torch._foreach_sin_", "type": "Torch", "text": ["Apply torch.sin() to each Tensor of the input list."]}, {"name": "torch._foreach_sinh", "path": "generated/torch._foreach_sinh", "type": "Torch", "text": ["Apply torch.sinh() to each Tensor of the input list."]}, {"name": "torch._foreach_sinh_", "path": "generated/torch._foreach_sinh_", "type": "Torch", "text": ["Apply torch.sinh() to each Tensor of the input list."]}, {"name": "torch._foreach_sqrt", "path": "generated/torch._foreach_sqrt", "type": "Torch", "text": ["Apply torch.sqrt() to each Tensor of the input list."]}, {"name": "torch._foreach_sqrt_", "path": "generated/torch._foreach_sqrt_", "type": "Torch", "text": ["Apply torch.sqrt() to each Tensor of the input list."]}, {"name": "torch._foreach_tan", "path": "generated/torch._foreach_tan", "type": "Torch", "text": ["Apply torch.tan() to each Tensor of the input list."]}, {"name": "torch._foreach_tan_", "path": "generated/torch._foreach_tan_", "type": "Torch", "text": ["Apply torch.tan() to each Tensor of the input list."]}, {"name": "torch._foreach_trunc", "path": "generated/torch._foreach_trunc", "type": "Torch", "text": ["Apply torch.trunc() to each Tensor of the input list."]}, {"name": "torch._foreach_trunc_", "path": "generated/torch._foreach_trunc_", "type": "Torch", "text": ["Apply torch.trunc() to each Tensor of the input list."]}, {"name": "torch._foreach_zero_", "path": "generated/torch._foreach_zero_", "type": "Torch", "text": ["Apply torch.zero() to each Tensor of the input list."]}, {"name": "torch._logging", "path": "logging", "type": "Miscellaneous", "text": ["PyTorch has a configurable logging system, where different components can be given different log level settings. For instance, one component\u2019s log messages can be completely disabled, while another component\u2019s log messages can be set to maximum verbosity.", "Warning", "This feature is a prototype and may have compatibility breaking changes in the future.", "Warning", "This feature has not been expanded to control the log messages of all components in PyTorch yet.", "There are two ways to configure the logging system: through the environment variable TORCH_LOGS or the python API torch._logging.set_logs.", "Sets the log level for individual components and toggles individual log artifact types.", "The environment variable TORCH_LOGS is a comma-separated list of [+-]<component> pairs, where <component> is a component specified below. The + prefix will decrease the log level of the component, displaying more log messages while the - prefix will increase the log level of the component and display fewer log messages. The default setting is the behavior when a component is not specified in TORCH_LOGS. In addition to components, there are also artifacts. Artifacts are specific pieces of debug information associated with a component that are either displayed or not displayed, so prefixing an artifact with + or - will be a no-op. Since they are associated with a component, enabling that component will typically also enable that artifact, unless that artifact was specified to be off_by_default. This option is specified in _registrations.py for artifacts that are so spammy they should only be displayed when explicitly enabled. The following components and artifacts are configurable through the TORCH_LOGS environment variable (see torch._logging.set_logs for the python API):", "Special component which configures the default log level of all components. Default: logging.WARN", "The log level for the TorchDynamo component. Default: logging.WARN", "The log level for the AOTAutograd component. Default: logging.WARN", "The log level for the TorchInductor component. Default: logging.WARN", "The log level for an arbitrary unregistered module. Provide the fully qualified name and the module will be enabled. Default: logging.WARN", "Whether to emit the original and generated bytecode from TorchDynamo. Default: False", "Whether to emit the graphs generated by AOTAutograd. Default: False", "Whether to emit the joint forward-backward graph generated by AOTAutograd. Default: False", "Whether to emit graphs generated by DDPOptimizer. Default: False", "Whether to emit the graph captured by TorchDynamo in tabular format. Default: False", "Whether to emit the python source of the graph captured by TorchDynamo. Default: False", "Whether to emit a message when a unique graph break is encountered during TorchDynamo tracing. Default: False", "Whether to emit the guards generated by TorchDynamo for each compiled function. Default: False", "Whether to emit a guard failure reason and message every time TorchDynamo recompiles a function. Default: False", "Whether to emit the TorchInductor output code. Default: False", "Whether to emit the TorchInductor schedule. Default: False", "TORCH_LOGS=\"+dynamo,aot\" will set the log level of TorchDynamo to logging.DEBUG and AOT to logging.INFO", "TORCH_LOGS=\"-dynamo,+inductor\" will set the log level of TorchDynamo to logging.ERROR and TorchInductor to logging.DEBUG", "TORCH_LOGS=\"aot_graphs\" will enable the aot_graphs artifact", "TORCH_LOGS=\"+dynamo,schedule\" will enable set the log level of TorchDynamo to logging.DEBUG and enable the schedule artifact", "TORCH_LOGS=\"+some.random.module,schedule\" will set the log level of some.random.module to logging.DEBUG and enable the schedule artifact"]}, {"name": "torch._logging.set_logs()", "path": "generated/torch._logging.set_logs#torch._logging.set_logs", "type": "Miscellaneous", "text": ["Sets the log level for individual components and toggles individual log artifact types.", "Warning", "This feature is a prototype and may have compatibility breaking changes in the future.", "Note", "The TORCH_LOGS environment variable has complete precedence over this function, so if it was set, this function does nothing.", "A component is a set of related features in PyTorch. All of the log messages emitted from a given component have their own log levels. If the log level of a particular message has priority greater than or equal to its component\u2019s log level setting, it is emitted. Otherwise, it is supressed. This allows you to, for instance, silence large groups of log messages that are not relevant to you and increase verbosity of logs for components that are relevant. The expected log level values, ordered from highest to lowest priority, are:", "See documentation for the Python logging module for more information on log levels: https://docs.python.org/3/library/logging.html#logging-levels", "An artifact is a particular type of log message. Each artifact is assigned to a parent component. A component can emit many different kinds of artifacts. In general, an artifact is emitted if either its corresponding setting in the argument list below is turned on or if its parent component is set to a log level less than or equal to the log level of the artifact.", "Example:"]}, {"name": "torch._logging.torch._logging.set_logs", "path": "generated/torch._logging.set_logs", "type": "Miscellaneous", "text": ["Sets the log level for individual components and toggles individual log artifact types.", "Warning", "This feature is a prototype and may have compatibility breaking changes in the future.", "Note", "The TORCH_LOGS environment variable has complete precedence over this function, so if it was set, this function does nothing.", "A component is a set of related features in PyTorch. All of the log messages emitted from a given component have their own log levels. If the log level of a particular message has priority greater than or equal to its component\u2019s log level setting, it is emitted. Otherwise, it is supressed. This allows you to, for instance, silence large groups of log messages that are not relevant to you and increase verbosity of logs for components that are relevant. The expected log level values, ordered from highest to lowest priority, are:", "See documentation for the Python logging module for more information on log levels: https://docs.python.org/3/library/logging.html#logging-levels", "An artifact is a particular type of log message. Each artifact is assigned to a parent component. A component can emit many different kinds of artifacts. In general, an artifact is emitted if either its corresponding setting in the argument list below is turned on or if its parent component is set to a log level less than or equal to the log level of the artifact.", "Example:"]}, {"name": "torch.abs", "path": "generated/torch.abs", "type": "Torch", "text": ["Computes the absolute value of each element in input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.absolute", "path": "generated/torch.absolute", "type": "Torch", "text": ["Alias for torch.abs()"]}, {"name": "torch.acos", "path": "generated/torch.acos", "type": "Torch", "text": ["Computes the inverse cosine of each element in input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.acosh", "path": "generated/torch.acosh", "type": "Torch", "text": ["Returns a new tensor with the inverse hyperbolic cosine of the elements of input.", "Note", "The domain of the inverse hyperbolic cosine is [1, inf) and values outside this range will be mapped to NaN, except for + INF for which the output is mapped to + INF.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.add", "path": "generated/torch.add", "type": "Torch", "text": ["Adds other, scaled by alpha, to input.", "Supports broadcasting to a common shape, type promotion, and integer, float, and complex inputs.", "Examples:"]}, {"name": "torch.addbmm", "path": "generated/torch.addbmm", "type": "Torch", "text": ["Performs a batch matrix-matrix product of matrices stored in batch1 and batch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension). input is added to the final result.", "batch1 and batch2 must be 3-D tensors each containing the same number of matrices.", "If batch1 is a (b\u00d7n\u00d7m)(b \\times n \\times m) tensor, batch2 is a (b\u00d7m\u00d7p)(b \\times m \\times p) tensor, input must be broadcastable with a (n\u00d7p)(n \\times p) tensor and out will be a (n\u00d7p)(n \\times p) tensor.", "If beta is 0, then input will be ignored, and nan and inf in it will not be propagated.", "For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers.", "This operator supports TensorFloat32.", "On certain ROCm devices, when using float16 inputs this module will use different precision for backward.", "Example:"]}, {"name": "torch.addcdiv", "path": "generated/torch.addcdiv", "type": "Torch", "text": ["Performs the element-wise division of tensor1 by tensor2, multiplies the result by the scalar value and adds it to input.", "Warning", "Integer division with addcdiv is no longer supported, and in a future release addcdiv will perform a true division of tensor1 and tensor2. The historic addcdiv behavior can be implemented as (input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype) for integer inputs and as (input + value * tensor1 / tensor2) for float inputs. The future addcdiv behavior is just the latter implementation: (input + value * tensor1 / tensor2), for all dtypes.", "The shapes of input, tensor1, and tensor2 must be broadcastable.", "For inputs of type FloatTensor or DoubleTensor, value must be a real number, otherwise an integer.", "Example:"]}, {"name": "torch.addcmul", "path": "generated/torch.addcmul", "type": "Torch", "text": ["Performs the element-wise multiplication of tensor1 by tensor2, multiplies the result by the scalar value and adds it to input.", "The shapes of tensor, tensor1, and tensor2 must be broadcastable.", "For inputs of type FloatTensor or DoubleTensor, value must be a real number, otherwise an integer.", "Example:"]}, {"name": "torch.addmm", "path": "generated/torch.addmm", "type": "Torch", "text": ["Performs a matrix multiplication of the matrices mat1 and mat2. The matrix input is added to the final result.", "If mat1 is a (n\u00d7m)(n \\times m) tensor, mat2 is a (m\u00d7p)(m \\times p) tensor, then input must be broadcastable with a (n\u00d7p)(n \\times p) tensor and out will be a (n\u00d7p)(n \\times p) tensor.", "alpha and beta are scaling factors on matrix-vector product between mat1 and mat2 and the added matrix input respectively.", "If beta is 0, then input will be ignored, and nan and inf in it will not be propagated.", "For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers.", "This operation has support for arguments with sparse layouts. If input is sparse the result will have the same layout and if out is provided it must have the same layout as input.", "Warning", "Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported, or may not have autograd support. If you notice missing functionality please open a feature request.", "This operator supports TensorFloat32.", "On certain ROCm devices, when using float16 inputs this module will use different precision for backward.", "Example:"]}, {"name": "torch.addmv", "path": "generated/torch.addmv", "type": "Torch", "text": ["Performs a matrix-vector product of the matrix mat and the vector vec. The vector input is added to the final result.", "If mat is a (n\u00d7m)(n \\times m) tensor, vec is a 1-D tensor of size m, then input must be broadcastable with a 1-D tensor of size n and out will be 1-D tensor of size n.", "alpha and beta are scaling factors on matrix-vector product between mat and vec and the added tensor input respectively.", "If beta is 0, then input will be ignored, and nan and inf in it will not be propagated.", "For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers.", "Example:"]}, {"name": "torch.addr", "path": "generated/torch.addr", "type": "Torch", "text": ["Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.", "Optional values beta and alpha are scaling factors on the outer product between vec1 and vec2 and the added matrix input respectively.", "If beta is 0, then input will be ignored, and nan and inf in it will not be propagated.", "If vec1 is a vector of size n and vec2 is a vector of size m, then input must be broadcastable with a matrix of size (n\u00d7m)(n \\times m) and out will be a matrix of size (n\u00d7m)(n \\times m).", "Example:"]}, {"name": "torch.adjoint", "path": "generated/torch.adjoint", "type": "Torch", "text": ["Returns a view of the tensor conjugated and with the last two dimensions transposed.", "x.adjoint() is equivalent to x.transpose(-2, -1).conj() for complex tensors and to x.transpose(-2, -1) for real tensors."]}, {"name": "torch.all", "path": "generated/torch.all", "type": "Torch", "text": ["Tests if all elements in input evaluate to True.", "Note", "This function matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself.", "Example:", "For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.", "If keepdim is True, the output tensor is of the same size as input except in the dimension dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension than input.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.allclose", "path": "generated/torch.allclose", "type": "Torch", "text": ["This function checks if input and other satisfy the condition:", "elementwise, for all elements of input and other. The behaviour of this function is analogous to numpy.allclose", "Example:"]}, {"name": "torch.amax", "path": "generated/torch.amax", "type": "Torch", "text": ["Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.", "Note", "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.amin", "path": "generated/torch.amin", "type": "Torch", "text": ["Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.", "Note", "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.aminmax", "path": "generated/torch.aminmax", "type": "Torch", "text": ["Computes the minimum and maximum values of the input tensor.", "input (Tensor) \u2013 The input tensor", "A named tuple (min, max) containing the minimum and maximum values.", "RuntimeError \u2013 If any of the dimensions to compute the values over has size 0.", "Note", "NaN values are propagated to the output if at least one value is NaN.", "See also", "torch.amin() computes just the minimum value torch.amax() computes just the maximum value", "Example:"]}, {"name": "torch.angle", "path": "generated/torch.angle", "type": "Torch", "text": ["Computes the element-wise angle (in radians) of the given input tensor.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Note", "Starting in PyTorch 1.8, angle returns pi for negative real numbers, zero for non-negative real numbers, and propagates NaNs. Previously the function would return zero for all real numbers and not propagate floating-point NaNs.", "Example:"]}, {"name": "torch.any", "path": "generated/torch.any", "type": "Torch", "text": ["Tests if any element in input evaluates to True.", "Note", "This function matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself.", "Example:", "For each row of input in the given dimension dim, returns True if any element in the row evaluate to True and False otherwise.", "If keepdim is True, the output tensor is of the same size as input except in the dimension dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension than input.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.ao.nn.intrinsic.BNReLU2d", "path": "generated/torch.ao.nn.intrinsic.bnrelu2d#torch.ao.nn.intrinsic.BNReLU2d", "type": "Quantization", "text": ["This is a sequential container which calls the BatchNorm 2d and ReLU modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.BNReLU3d", "path": "generated/torch.ao.nn.intrinsic.bnrelu3d#torch.ao.nn.intrinsic.BNReLU3d", "type": "Quantization", "text": ["This is a sequential container which calls the BatchNorm 3d and ReLU modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.ConvBn1d", "path": "generated/torch.ao.nn.intrinsic.convbn1d#torch.ao.nn.intrinsic.ConvBn1d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv 1d and Batch Norm 1d modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.ConvBn2d", "path": "generated/torch.ao.nn.intrinsic.convbn2d#torch.ao.nn.intrinsic.ConvBn2d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv 2d and Batch Norm 2d modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.ConvBn3d", "path": "generated/torch.ao.nn.intrinsic.convbn3d#torch.ao.nn.intrinsic.ConvBn3d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv 3d and Batch Norm 3d modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.ConvBnReLU1d", "path": "generated/torch.ao.nn.intrinsic.convbnrelu1d#torch.ao.nn.intrinsic.ConvBnReLU1d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv 1d, Batch Norm 1d, and ReLU modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.ConvBnReLU2d", "path": "generated/torch.ao.nn.intrinsic.convbnrelu2d#torch.ao.nn.intrinsic.ConvBnReLU2d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv 2d, Batch Norm 2d, and ReLU modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.ConvBnReLU3d", "path": "generated/torch.ao.nn.intrinsic.convbnrelu3d#torch.ao.nn.intrinsic.ConvBnReLU3d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv 3d, Batch Norm 3d, and ReLU modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.ConvReLU1d", "path": "generated/torch.ao.nn.intrinsic.convrelu1d#torch.ao.nn.intrinsic.ConvReLU1d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv1d and ReLU modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.ConvReLU2d", "path": "generated/torch.ao.nn.intrinsic.convrelu2d#torch.ao.nn.intrinsic.ConvReLU2d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv2d and ReLU modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.ConvReLU3d", "path": "generated/torch.ao.nn.intrinsic.convrelu3d#torch.ao.nn.intrinsic.ConvReLU3d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv3d and ReLU modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.LinearReLU", "path": "generated/torch.ao.nn.intrinsic.linearrelu#torch.ao.nn.intrinsic.LinearReLU", "type": "Quantization", "text": ["This is a sequential container which calls the Linear and ReLU modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.qat.ConvBn1d", "path": "generated/torch.ao.nn.intrinsic.qat.convbn1d#torch.ao.nn.intrinsic.qat.ConvBn1d", "type": "Quantization", "text": ["A ConvBn1d module is a module fused from Conv1d and BatchNorm1d, attached with FakeQuantize modules for weight, used in quantization aware training.", "We combined the interface of torch.nn.Conv1d and torch.nn.BatchNorm1d.", "Similar to torch.nn.Conv1d, with FakeQuantize modules initialized to default."]}, {"name": "torch.ao.nn.intrinsic.qat.ConvBn2d", "path": "generated/torch.ao.nn.intrinsic.qat.convbn2d#torch.ao.nn.intrinsic.qat.ConvBn2d", "type": "Quantization", "text": ["A ConvBn2d module is a module fused from Conv2d and BatchNorm2d, attached with FakeQuantize modules for weight, used in quantization aware training.", "We combined the interface of torch.nn.Conv2d and torch.nn.BatchNorm2d.", "Similar to torch.nn.Conv2d, with FakeQuantize modules initialized to default."]}, {"name": "torch.ao.nn.intrinsic.qat.ConvBn3d", "path": "generated/torch.ao.nn.intrinsic.qat.convbn3d#torch.ao.nn.intrinsic.qat.ConvBn3d", "type": "Quantization", "text": ["A ConvBn3d module is a module fused from Conv3d and BatchNorm3d, attached with FakeQuantize modules for weight, used in quantization aware training.", "We combined the interface of torch.nn.Conv3d and torch.nn.BatchNorm3d.", "Similar to torch.nn.Conv3d, with FakeQuantize modules initialized to default."]}, {"name": "torch.ao.nn.intrinsic.qat.ConvBnReLU1d", "path": "generated/torch.ao.nn.intrinsic.qat.convbnrelu1d#torch.ao.nn.intrinsic.qat.ConvBnReLU1d", "type": "Quantization", "text": ["A ConvBnReLU1d module is a module fused from Conv1d, BatchNorm1d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.", "We combined the interface of torch.nn.Conv1d and torch.nn.BatchNorm1d and torch.nn.ReLU.", "Similar to torch.nn.Conv1d, with FakeQuantize modules initialized to default.", "weight_fake_quant \u2013 fake quant module for weight"]}, {"name": "torch.ao.nn.intrinsic.qat.ConvBnReLU2d", "path": "generated/torch.ao.nn.intrinsic.qat.convbnrelu2d#torch.ao.nn.intrinsic.qat.ConvBnReLU2d", "type": "Quantization", "text": ["A ConvBnReLU2d module is a module fused from Conv2d, BatchNorm2d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.", "We combined the interface of torch.nn.Conv2d and torch.nn.BatchNorm2d and torch.nn.ReLU.", "Similar to torch.nn.Conv2d, with FakeQuantize modules initialized to default.", "weight_fake_quant \u2013 fake quant module for weight"]}, {"name": "torch.ao.nn.intrinsic.qat.ConvBnReLU3d", "path": "generated/torch.ao.nn.intrinsic.qat.convbnrelu3d#torch.ao.nn.intrinsic.qat.ConvBnReLU3d", "type": "Quantization", "text": ["A ConvBnReLU3d module is a module fused from Conv3d, BatchNorm3d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.", "We combined the interface of torch.nn.Conv3d and torch.nn.BatchNorm3d and torch.nn.ReLU.", "Similar to torch.nn.Conv3d, with FakeQuantize modules initialized to default.", "weight_fake_quant \u2013 fake quant module for weight"]}, {"name": "torch.ao.nn.intrinsic.qat.ConvReLU2d", "path": "generated/torch.ao.nn.intrinsic.qat.convrelu2d#torch.ao.nn.intrinsic.qat.ConvReLU2d", "type": "Quantization", "text": ["A ConvReLU2d module is a fused module of Conv2d and ReLU, attached with FakeQuantize modules for weight for quantization aware training.", "We combined the interface of Conv2d and BatchNorm2d.", "weight_fake_quant \u2013 fake quant module for weight"]}, {"name": "torch.ao.nn.intrinsic.qat.ConvReLU3d", "path": "generated/torch.ao.nn.intrinsic.qat.convrelu3d#torch.ao.nn.intrinsic.qat.ConvReLU3d", "type": "Quantization", "text": ["A ConvReLU3d module is a fused module of Conv3d and ReLU, attached with FakeQuantize modules for weight for quantization aware training.", "We combined the interface of Conv3d and BatchNorm3d.", "weight_fake_quant \u2013 fake quant module for weight"]}, {"name": "torch.ao.nn.intrinsic.qat.freeze_bn_stats", "path": "generated/torch.ao.nn.intrinsic.qat.freeze_bn_stats#torch.ao.nn.intrinsic.qat.freeze_bn_stats", "type": "Quantization", "text": []}, {"name": "torch.ao.nn.intrinsic.qat.LinearReLU", "path": "generated/torch.ao.nn.intrinsic.qat.linearrelu#torch.ao.nn.intrinsic.qat.LinearReLU", "type": "Quantization", "text": ["A LinearReLU module fused from Linear and ReLU modules, attached with FakeQuantize modules for weight, used in quantization aware training.", "We adopt the same interface as torch.nn.Linear.", "Similar to torch.ao.nn.intrinsic.LinearReLU, with FakeQuantize modules initialized to default.", "weight (torch.Tensor) \u2013 fake quant module for weight", "Examples:"]}, {"name": "torch.ao.nn.intrinsic.qat.update_bn_stats", "path": "generated/torch.ao.nn.intrinsic.qat.update_bn_stats#torch.ao.nn.intrinsic.qat.update_bn_stats", "type": "Quantization", "text": []}, {"name": "torch.ao.nn.intrinsic.quantized.BNReLU2d", "path": "generated/torch.ao.nn.intrinsic.quantized.bnrelu2d#torch.ao.nn.intrinsic.quantized.BNReLU2d", "type": "Quantization", "text": ["A BNReLU2d module is a fused module of BatchNorm2d and ReLU", "We adopt the same interface as torch.ao.nn.quantized.BatchNorm2d.", "torch.ao.nn.quantized.BatchNorm2d (Same as) \u2013 "]}, {"name": "torch.ao.nn.intrinsic.quantized.BNReLU3d", "path": "generated/torch.ao.nn.intrinsic.quantized.bnrelu3d#torch.ao.nn.intrinsic.quantized.BNReLU3d", "type": "Quantization", "text": ["A BNReLU3d module is a fused module of BatchNorm3d and ReLU", "We adopt the same interface as torch.ao.nn.quantized.BatchNorm3d.", "torch.ao.nn.quantized.BatchNorm3d (Same as) \u2013 "]}, {"name": "torch.ao.nn.intrinsic.quantized.ConvReLU1d", "path": "generated/torch.ao.nn.intrinsic.quantized.convrelu1d#torch.ao.nn.intrinsic.quantized.ConvReLU1d", "type": "Quantization", "text": ["A ConvReLU1d module is a fused module of Conv1d and ReLU", "We adopt the same interface as torch.ao.nn.quantized.Conv1d.", "torch.ao.nn.quantized.Conv1d (Same as) \u2013 "]}, {"name": "torch.ao.nn.intrinsic.quantized.ConvReLU2d", "path": "generated/torch.ao.nn.intrinsic.quantized.convrelu2d#torch.ao.nn.intrinsic.quantized.ConvReLU2d", "type": "Quantization", "text": ["A ConvReLU2d module is a fused module of Conv2d and ReLU", "We adopt the same interface as torch.ao.nn.quantized.Conv2d.", "torch.ao.nn.quantized.Conv2d (Same as) \u2013 "]}, {"name": "torch.ao.nn.intrinsic.quantized.ConvReLU3d", "path": "generated/torch.ao.nn.intrinsic.quantized.convrelu3d#torch.ao.nn.intrinsic.quantized.ConvReLU3d", "type": "Quantization", "text": ["A ConvReLU3d module is a fused module of Conv3d and ReLU", "We adopt the same interface as torch.ao.nn.quantized.Conv3d.", "Attributes: Same as torch.ao.nn.quantized.Conv3d"]}, {"name": "torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU", "path": "generated/torch.ao.nn.intrinsic.quantized.dynamic.linearrelu#torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU", "type": "Quantization", "text": ["A LinearReLU module fused from Linear and ReLU modules that can be used for dynamic quantization. Supports both, FP16 and INT8 quantization.", "We adopt the same interface as torch.ao.nn.quantized.dynamic.Linear.", "torch.ao.nn.quantized.dynamic.Linear (Same as) \u2013 ", "Examples:"]}, {"name": "torch.ao.nn.intrinsic.quantized.LinearReLU", "path": "generated/torch.ao.nn.intrinsic.quantized.linearrelu#torch.ao.nn.intrinsic.quantized.LinearReLU", "type": "Quantization", "text": ["A LinearReLU module fused from Linear and ReLU modules", "We adopt the same interface as torch.ao.nn.quantized.Linear.", "torch.ao.nn.quantized.Linear (Same as) \u2013 ", "Examples:"]}, {"name": "torch.ao.nn.qat.Conv2d", "path": "generated/torch.ao.nn.qat.conv2d#torch.ao.nn.qat.Conv2d", "type": "Quantization", "text": ["A Conv2d module attached with FakeQuantize modules for weight, used for quantization aware training.", "We adopt the same interface as torch.nn.Conv2d, please see https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d for documentation.", "Similar to torch.nn.Conv2d, with FakeQuantize modules initialized to default.", "weight_fake_quant \u2013 fake quant module for weight"]}, {"name": "torch.ao.nn.qat.Conv3d", "path": "generated/torch.ao.nn.qat.conv3d#torch.ao.nn.qat.Conv3d", "type": "Quantization", "text": ["A Conv3d module attached with FakeQuantize modules for weight, used for quantization aware training.", "We adopt the same interface as torch.nn.Conv3d, please see https://pytorch.org/docs/stable/nn.html?highlight=conv3d#torch.nn.Conv3d for documentation.", "Similar to torch.nn.Conv3d, with FakeQuantize modules initialized to default.", "weight_fake_quant \u2013 fake quant module for weight"]}, {"name": "torch.ao.nn.qat.dynamic.Linear", "path": "generated/torch.ao.nn.qat.dynamic.linear#torch.ao.nn.qat.dynamic.Linear", "type": "Quantization", "text": ["A linear module attached with FakeQuantize modules for weight, used for dynamic quantization aware training.", "We adopt the same interface as torch.nn.Linear, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.", "Similar to torch.nn.Linear, with FakeQuantize modules initialized to default."]}, {"name": "torch.ao.nn.qat.Linear", "path": "generated/torch.ao.nn.qat.linear#torch.ao.nn.qat.Linear", "type": "Quantization", "text": ["A linear module attached with FakeQuantize modules for weight, used for quantization aware training.", "We adopt the same interface as torch.nn.Linear, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.", "Similar to torch.nn.Linear, with FakeQuantize modules initialized to default.", "weight (torch.Tensor) \u2013 fake quant module for weight", "Create a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.ao.quantization utilities or directly from user"]}, {"name": "torch.ao.nn.qat.Linear.from_float()", "path": "generated/torch.ao.nn.qat.linear#torch.ao.nn.qat.Linear.from_float", "type": "Quantization", "text": ["Create a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.ao.quantization utilities or directly from user"]}, {"name": "torch.ao.nn.quantizable.LSTM", "path": "generated/torch.ao.nn.quantizable.lstm#torch.ao.nn.quantizable.LSTM", "type": "Quantization", "text": ["A quantizable long short-term memory (LSTM).", "For the description and the argument types, please, refer to LSTM", "layers \u2013 instances of the _LSTMLayer", "Note", "To access the weights and biases, you need to access them per layer. See examples below.", "Examples:"]}, {"name": "torch.ao.nn.quantizable.MultiheadAttention", "path": "generated/torch.ao.nn.quantizable.multiheadattention#torch.ao.nn.quantizable.MultiheadAttention", "type": "Quantization", "text": ["Utility to convert the quantized MHA back to float.", "The motivation for this is that it is not trivial to conver the weights from the format that is used in the quantized version back to the float.", "Please, refer to forward() for more information", "Tuple[Tensor, Optional[Tensor]]"]}, {"name": "torch.ao.nn.quantizable.MultiheadAttention.dequantize()", "path": "generated/torch.ao.nn.quantizable.multiheadattention#torch.ao.nn.quantizable.MultiheadAttention.dequantize", "type": "Quantization", "text": ["Utility to convert the quantized MHA back to float.", "The motivation for this is that it is not trivial to conver the weights from the format that is used in the quantized version back to the float."]}, {"name": "torch.ao.nn.quantizable.MultiheadAttention.forward()", "path": "generated/torch.ao.nn.quantizable.multiheadattention#torch.ao.nn.quantizable.MultiheadAttention.forward", "type": "Quantization", "text": ["Please, refer to forward() for more information", "Tuple[Tensor, Optional[Tensor]]"]}, {"name": "torch.ao.nn.quantized.BatchNorm2d", "path": "generated/torch.ao.nn.quantized.batchnorm2d#torch.ao.nn.quantized.BatchNorm2d", "type": "Quantization", "text": ["This is the quantized version of BatchNorm2d."]}, {"name": "torch.ao.nn.quantized.BatchNorm3d", "path": "generated/torch.ao.nn.quantized.batchnorm3d#torch.ao.nn.quantized.BatchNorm3d", "type": "Quantization", "text": ["This is the quantized version of BatchNorm3d."]}, {"name": "torch.ao.nn.quantized.Conv1d", "path": "generated/torch.ao.nn.quantized.conv1d#torch.ao.nn.quantized.Conv1d", "type": "Quantization", "text": ["Applies a 1D convolution over a quantized input signal composed of several quantized input planes.", "For details on input arguments, parameters, and implementation see Conv1d.", "Note", "Only zeros is supported for the padding_mode argument.", "Note", "Only torch.quint8 is supported for the input data type.", "See Conv1d for other attributes.", "Examples:", "Creates a quantized module from a float module or qparams_dict.", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user"]}, {"name": "torch.ao.nn.quantized.Conv1d.from_float()", "path": "generated/torch.ao.nn.quantized.conv1d#torch.ao.nn.quantized.Conv1d.from_float", "type": "Quantization", "text": ["Creates a quantized module from a float module or qparams_dict.", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user"]}, {"name": "torch.ao.nn.quantized.Conv2d", "path": "generated/torch.ao.nn.quantized.conv2d#torch.ao.nn.quantized.Conv2d", "type": "Quantization", "text": ["Applies a 2D convolution over a quantized input signal composed of several quantized input planes.", "For details on input arguments, parameters, and implementation see Conv2d.", "Note", "Only zeros is supported for the padding_mode argument.", "Note", "Only torch.quint8 is supported for the input data type.", "See Conv2d for other attributes.", "Examples:", "Creates a quantized module from a float module or qparams_dict.", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user"]}, {"name": "torch.ao.nn.quantized.Conv2d.from_float()", "path": "generated/torch.ao.nn.quantized.conv2d#torch.ao.nn.quantized.Conv2d.from_float", "type": "Quantization", "text": ["Creates a quantized module from a float module or qparams_dict.", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user"]}, {"name": "torch.ao.nn.quantized.Conv3d", "path": "generated/torch.ao.nn.quantized.conv3d#torch.ao.nn.quantized.Conv3d", "type": "Quantization", "text": ["Applies a 3D convolution over a quantized input signal composed of several quantized input planes.", "For details on input arguments, parameters, and implementation see Conv3d.", "Note", "Only zeros is supported for the padding_mode argument.", "Note", "Only torch.quint8 is supported for the input data type.", "See Conv3d for other attributes.", "Examples:", "Creates a quantized module from a float module or qparams_dict.", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user"]}, {"name": "torch.ao.nn.quantized.Conv3d.from_float()", "path": "generated/torch.ao.nn.quantized.conv3d#torch.ao.nn.quantized.Conv3d.from_float", "type": "Quantization", "text": ["Creates a quantized module from a float module or qparams_dict.", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user"]}, {"name": "torch.ao.nn.quantized.ConvTranspose1d", "path": "generated/torch.ao.nn.quantized.convtranspose1d#torch.ao.nn.quantized.ConvTranspose1d", "type": "Quantization", "text": ["Applies a 1D transposed convolution operator over an input image composed of several input planes. For details on input arguments, parameters, and implementation see ConvTranspose1d.", "Note", "Currently only the QNNPACK engine is implemented. Please, set the torch.backends.quantized.engine = \u2018qnnpack\u2019", "For special notes, please, see Conv1d", "See ConvTranspose2d for other attributes.", "Examples:"]}, {"name": "torch.ao.nn.quantized.ConvTranspose2d", "path": "generated/torch.ao.nn.quantized.convtranspose2d#torch.ao.nn.quantized.ConvTranspose2d", "type": "Quantization", "text": ["Applies a 2D transposed convolution operator over an input image composed of several input planes. For details on input arguments, parameters, and implementation see ConvTranspose2d.", "For special notes, please, see Conv2d", "See ConvTranspose2d for other attributes.", "Examples:"]}, {"name": "torch.ao.nn.quantized.ConvTranspose3d", "path": "generated/torch.ao.nn.quantized.convtranspose3d#torch.ao.nn.quantized.ConvTranspose3d", "type": "Quantization", "text": ["Applies a 3D transposed convolution operator over an input image composed of several input planes. For details on input arguments, parameters, and implementation see ConvTranspose3d.", "Note", "Currently only the FBGEMM engine is implemented. Please, set the torch.backends.quantized.engine = \u2018fbgemm\u2019", "For special notes, please, see Conv3d", "See ConvTranspose3d for other attributes.", "Examples:"]}, {"name": "torch.ao.nn.quantized.dynamic.GRU", "path": "generated/torch.ao.nn.quantized.dynamic.gru#torch.ao.nn.quantized.dynamic.GRU", "type": "Quantization", "text": ["Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.", "For each element in the input sequence, each layer computes the following function:", "where hth_t is the hidden state at time t, xtx_t is the input at time t, h(t\u22121)h_{(t-1)} is the hidden state of the layer at time t-1 or the initial hidden state at time 0, and rtr_t, ztz_t, ntn_t are the reset, update, and new gates, respectively. \u03c3\\sigma is the sigmoid function, and \u2217* is the Hadamard product.", "In a multilayer GRU, the input xt(l)x^{(l)}_t of the ll -th layer (l>=2l >= 2) is the hidden state ht(l\u22121)h^{(l-1)}_t of the previous layer multiplied by dropout \u03b4t(l\u22121)\\delta^{(l-1)}_t where each \u03b4t(l\u22121)\\delta^{(l-1)}_t is a Bernoulli random variable which is 00 with probability dropout.", "output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features h_t from the last layer of the GRU, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively.", "Similarly, the directions can be separated in the packed case.", "h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len", "Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size).", "Note", "All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}) where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}", "Note", "The calculation of new gate ntn_t subtly differs from the original paper and other frameworks. In the original implementation, the Hadamard product (\u2217)(*) between rtr_t and the previous hidden state h(t\u22121)h_{(t-1)} is done before the multiplication with the weight matrix W and addition of bias:", "This is in contrast to PyTorch implementation, which is done after Whnh(t\u22121)W_{hn} h_{(t-1)}", "This implementation differs on purpose for efficiency.", "Note", "If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance.", "Examples:"]}, {"name": "torch.ao.nn.quantized.dynamic.GRUCell", "path": "generated/torch.ao.nn.quantized.dynamic.grucell#torch.ao.nn.quantized.dynamic.GRUCell", "type": "Quantization", "text": ["A gated recurrent unit (GRU) cell", "A dynamic quantized GRUCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as torch.nn.GRUCell, please see https://pytorch.org/docs/stable/nn.html#torch.nn.GRUCell for documentation.", "Examples:"]}, {"name": "torch.ao.nn.quantized.dynamic.Linear", "path": "generated/torch.ao.nn.quantized.dynamic.linear#torch.ao.nn.quantized.dynamic.Linear", "type": "Quantization", "text": ["A dynamic quantized linear module with floating point tensor as inputs and outputs. We adopt the same interface as torch.nn.Linear, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.", "Similar to torch.nn.Linear, attributes will be randomly initialized at module creation time and will be overwritten later", "Examples:", "Create a dynamic quantized module from a float module or qparams_dict", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user", "Create a (fbgemm/qnnpack) dynamic quantized module from a reference quantized module :param ref_qlinear: a reference quantized module, either produced by :type ref_qlinear: Module :param torch.ao.quantization functions or provided by the user:"]}, {"name": "torch.ao.nn.quantized.dynamic.Linear.from_float()", "path": "generated/torch.ao.nn.quantized.dynamic.linear#torch.ao.nn.quantized.dynamic.Linear.from_float", "type": "Quantization", "text": ["Create a dynamic quantized module from a float module or qparams_dict", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user"]}, {"name": "torch.ao.nn.quantized.dynamic.Linear.from_reference()", "path": "generated/torch.ao.nn.quantized.dynamic.linear#torch.ao.nn.quantized.dynamic.Linear.from_reference", "type": "Quantization", "text": ["Create a (fbgemm/qnnpack) dynamic quantized module from a reference quantized module :param ref_qlinear: a reference quantized module, either produced by :type ref_qlinear: Module :param torch.ao.quantization functions or provided by the user:"]}, {"name": "torch.ao.nn.quantized.dynamic.LSTM", "path": "generated/torch.ao.nn.quantized.dynamic.lstm#torch.ao.nn.quantized.dynamic.LSTM", "type": "Quantization", "text": ["A dynamic quantized LSTM module with floating point tensor as inputs and outputs. We adopt the same interface as torch.nn.LSTM, please see https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM for documentation.", "Examples:"]}, {"name": "torch.ao.nn.quantized.dynamic.LSTMCell", "path": "generated/torch.ao.nn.quantized.dynamic.lstmcell#torch.ao.nn.quantized.dynamic.LSTMCell", "type": "Quantization", "text": ["A long short-term memory (LSTM) cell.", "A dynamic quantized LSTMCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as torch.nn.LSTMCell, please see https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell for documentation.", "Examples:"]}, {"name": "torch.ao.nn.quantized.dynamic.RNNCell", "path": "generated/torch.ao.nn.quantized.dynamic.rnncell#torch.ao.nn.quantized.dynamic.RNNCell", "type": "Quantization", "text": ["An Elman RNN cell with tanh or ReLU non-linearity. A dynamic quantized RNNCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as torch.nn.RNNCell, please see https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell for documentation.", "Examples:"]}, {"name": "torch.ao.nn.quantized.ELU", "path": "generated/torch.ao.nn.quantized.elu#torch.ao.nn.quantized.ELU", "type": "Quantization", "text": ["This is the quantized equivalent of ELU."]}, {"name": "torch.ao.nn.quantized.Embedding", "path": "generated/torch.ao.nn.quantized.embedding#torch.ao.nn.quantized.Embedding", "type": "Quantization", "text": ["A quantized Embedding module with quantized packed weights as inputs. We adopt the same interface as torch.nn.Embedding, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding for documentation.", "Similar to Embedding, attributes will be randomly initialized at module creation time and will be overwritten later", "weight (Tensor) \u2013 the non-learnable quantized weights of the module of shape (num_embeddings,embedding_dim)(\\text{num\\_embeddings}, \\text{embedding\\_dim}).", "Create a quantized embedding module from a float module", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by user"]}, {"name": "torch.ao.nn.quantized.Embedding.from_float()", "path": "generated/torch.ao.nn.quantized.embedding#torch.ao.nn.quantized.Embedding.from_float", "type": "Quantization", "text": ["Create a quantized embedding module from a float module", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by user"]}, {"name": "torch.ao.nn.quantized.EmbeddingBag", "path": "generated/torch.ao.nn.quantized.embeddingbag#torch.ao.nn.quantized.EmbeddingBag", "type": "Quantization", "text": ["A quantized EmbeddingBag module with quantized packed weights as inputs. We adopt the same interface as torch.nn.EmbeddingBag, please see https://pytorch.org/docs/stable/nn.html#torch.nn.EmbeddingBag for documentation.", "Similar to EmbeddingBag, attributes will be randomly initialized at module creation time and will be overwritten later", "weight (Tensor) \u2013 the non-learnable quantized weights of the module of shape (num_embeddings,embedding_dim)(\\text{num\\_embeddings}, \\text{embedding\\_dim}).", "Create a quantized embedding_bag module from a float module", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by user"]}, {"name": "torch.ao.nn.quantized.EmbeddingBag.from_float()", "path": "generated/torch.ao.nn.quantized.embeddingbag#torch.ao.nn.quantized.EmbeddingBag.from_float", "type": "Quantization", "text": ["Create a quantized embedding_bag module from a float module", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by user"]}, {"name": "torch.ao.nn.quantized.FloatFunctional", "path": "generated/torch.ao.nn.quantized.floatfunctional#torch.ao.nn.quantized.FloatFunctional", "type": "Quantization", "text": ["State collector class for float operations.", "The instance of this class can be used instead of the torch. prefix for some operations. See example usage below.", "Note", "This class does not provide a forward hook. Instead, you must use one of the underlying functions (e.g. add).", "Examples:"]}, {"name": "torch.ao.nn.quantized.functional.adaptive_avg_pool2d", "path": "generated/torch.ao.nn.quantized.functional.adaptive_avg_pool2d#torch.ao.nn.quantized.functional.adaptive_avg_pool2d", "type": "Quantization", "text": ["Applies a 2D adaptive average pooling over a quantized input signal composed of several quantized input planes.", "Note", "The input quantization parameters propagate to the output.", "See AdaptiveAvgPool2d for details and output shape.", "output_size (None) \u2013 the target output size (single integer or double-integer tuple)", "Tensor"]}, {"name": "torch.ao.nn.quantized.functional.adaptive_avg_pool3d", "path": "generated/torch.ao.nn.quantized.functional.adaptive_avg_pool3d#torch.ao.nn.quantized.functional.adaptive_avg_pool3d", "type": "Quantization", "text": ["Applies a 3D adaptive average pooling over a quantized input signal composed of several quantized input planes.", "Note", "The input quantization parameters propagate to the output.", "See AdaptiveAvgPool3d for details and output shape.", "output_size (None) \u2013 the target output size (single integer or double-integer tuple)", "Tensor"]}, {"name": "torch.ao.nn.quantized.functional.avg_pool2d", "path": "generated/torch.ao.nn.quantized.functional.avg_pool2d#torch.ao.nn.quantized.functional.avg_pool2d", "type": "Quantization", "text": ["Applies 2D average-pooling operation in kH\u00d7kWkH \\times kW regions by step size sH\u00d7sWsH \\times sW steps. The number of output features is equal to the number of input planes.", "Note", "The input quantization parameters propagate to the output.", "See AvgPool2d for details and output shape."]}, {"name": "torch.ao.nn.quantized.functional.avg_pool3d", "path": "generated/torch.ao.nn.quantized.functional.avg_pool3d#torch.ao.nn.quantized.functional.avg_pool3d", "type": "Quantization", "text": ["Applies 3D average-pooling operation in kDtimeskH\u00d7kWkD \\ times kH \\times kW regions by step size sD\u00d7sH\u00d7sWsD \\times sH \\times sW steps. The number of output features is equal to the number of input planes.", "Note", "The input quantization parameters propagate to the output."]}, {"name": "torch.ao.nn.quantized.functional.celu", "path": "generated/torch.ao.nn.quantized.functional.celu#torch.ao.nn.quantized.functional.celu", "type": "Quantization", "text": ["Applies the quantized CELU function element-wise.", "Tensor"]}, {"name": "torch.ao.nn.quantized.functional.clamp", "path": "generated/torch.ao.nn.quantized.functional.clamp#torch.ao.nn.quantized.functional.clamp", "type": "Quantization", "text": ["float(input, min_, max_) -> Tensor", "Applies the clamp function element-wise. See clamp for more details.", "Tensor"]}, {"name": "torch.ao.nn.quantized.functional.conv1d", "path": "generated/torch.ao.nn.quantized.functional.conv1d#torch.ao.nn.quantized.functional.conv1d", "type": "Quantization", "text": ["Applies a 1D convolution over a quantized 1D input composed of several input planes.", "See Conv1d for details and output shape.", "Examples:"]}, {"name": "torch.ao.nn.quantized.functional.conv2d", "path": "generated/torch.ao.nn.quantized.functional.conv2d#torch.ao.nn.quantized.functional.conv2d", "type": "Quantization", "text": ["Applies a 2D convolution over a quantized 2D input composed of several input planes.", "See Conv2d for details and output shape.", "Examples:"]}, {"name": "torch.ao.nn.quantized.functional.conv3d", "path": "generated/torch.ao.nn.quantized.functional.conv3d#torch.ao.nn.quantized.functional.conv3d", "type": "Quantization", "text": ["Applies a 3D convolution over a quantized 3D input composed of several input planes.", "See Conv3d for details and output shape.", "Examples:"]}, {"name": "torch.ao.nn.quantized.functional.elu", "path": "generated/torch.ao.nn.quantized.functional.elu#torch.ao.nn.quantized.functional.elu", "type": "Quantization", "text": ["This is the quantized version of elu().", "Tensor"]}, {"name": "torch.ao.nn.quantized.functional.hardsigmoid", "path": "generated/torch.ao.nn.quantized.functional.hardsigmoid#torch.ao.nn.quantized.functional.hardsigmoid", "type": "Quantization", "text": ["This is the quantized version of hardsigmoid().", "Tensor"]}, {"name": "torch.ao.nn.quantized.functional.hardswish", "path": "generated/torch.ao.nn.quantized.functional.hardswish#torch.ao.nn.quantized.functional.hardswish", "type": "Quantization", "text": ["This is the quantized version of hardswish().", "Tensor"]}, {"name": "torch.ao.nn.quantized.functional.hardtanh", "path": "generated/torch.ao.nn.quantized.functional.hardtanh#torch.ao.nn.quantized.functional.hardtanh", "type": "Quantization", "text": ["This is the quantized version of hardtanh().", "Tensor"]}, {"name": "torch.ao.nn.quantized.functional.interpolate", "path": "generated/torch.ao.nn.quantized.functional.interpolate#torch.ao.nn.quantized.functional.interpolate", "type": "Quantization", "text": ["Down/up samples the input to either the given size or the given scale_factor", "See torch.nn.functional.interpolate() for implementation details.", "The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width.", "Note", "The input quantization parameters propagate to the output.", "Note", "Only 2D/3D input is supported for quantized inputs", "Note", "Only the following modes are supported for the quantized inputs:"]}, {"name": "torch.ao.nn.quantized.functional.leaky_relu", "path": "generated/torch.ao.nn.quantized.functional.leaky_relu#torch.ao.nn.quantized.functional.leaky_relu", "type": "Quantization", "text": ["Quantized version of the. leaky_relu(input, negative_slope=0.01, inplace=False, scale, zero_point) -> Tensor", "Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)", "See LeakyReLU for more details."]}, {"name": "torch.ao.nn.quantized.functional.linear", "path": "generated/torch.ao.nn.quantized.functional.linear#torch.ao.nn.quantized.functional.linear", "type": "Quantization", "text": ["Applies a linear transformation to the incoming quantized data: y=xAT+by = xA^T + b. See Linear", "Note", "Current implementation packs weights on every call, which has penalty on performance. If you want to avoid the overhead, use Linear.", "Tensor"]}, {"name": "torch.ao.nn.quantized.functional.max_pool1d", "path": "generated/torch.ao.nn.quantized.functional.max_pool1d#torch.ao.nn.quantized.functional.max_pool1d", "type": "Quantization", "text": ["Applies a 1D max pooling over a quantized input signal composed of several quantized input planes.", "Note", "The input quantization parameters are propagated to the output.", "See MaxPool1d for details."]}, {"name": "torch.ao.nn.quantized.functional.max_pool2d", "path": "generated/torch.ao.nn.quantized.functional.max_pool2d#torch.ao.nn.quantized.functional.max_pool2d", "type": "Quantization", "text": ["Applies a 2D max pooling over a quantized input signal composed of several quantized input planes.", "Note", "The input quantization parameters are propagated to the output.", "See MaxPool2d for details."]}, {"name": "torch.ao.nn.quantized.functional.threshold", "path": "generated/torch.ao.nn.quantized.functional.threshold#torch.ao.nn.quantized.functional.threshold", "type": "Quantization", "text": ["Applies the quantized version of the threshold function element-wise:", "See Threshold for more details.", "Tensor"]}, {"name": "torch.ao.nn.quantized.functional.upsample", "path": "generated/torch.ao.nn.quantized.functional.upsample#torch.ao.nn.quantized.functional.upsample", "type": "Quantization", "text": ["Upsamples the input to either the given size or the given scale_factor", "Warning", "This function is deprecated in favor of torch.ao.nn.quantized.functional.interpolate(). This is equivalent with nn.quantized.functional.interpolate(...).", "See torch.nn.functional.interpolate() for implementation details.", "The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width.", "Note", "The input quantization parameters propagate to the output.", "Note", "Only 2D input is supported for quantized inputs", "Note", "Only the following modes are supported for the quantized inputs:", "Warning", "With align_corners = True, the linearly interpolating modes (bilinear) don\u2019t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See Upsample for concrete examples on how this affects the outputs."]}, {"name": "torch.ao.nn.quantized.functional.upsample_bilinear", "path": "generated/torch.ao.nn.quantized.functional.upsample_bilinear#torch.ao.nn.quantized.functional.upsample_bilinear", "type": "Quantization", "text": ["Upsamples the input, using bilinear upsampling.", "Warning", "This function is deprecated in favor of torch.ao.nn.quantized.functional.interpolate(). This is equivalent with nn.quantized.functional.interpolate(..., mode='bilinear', align_corners=True).", "Note", "The input quantization parameters propagate to the output.", "Note", "Only 2D inputs are supported"]}, {"name": "torch.ao.nn.quantized.functional.upsample_nearest", "path": "generated/torch.ao.nn.quantized.functional.upsample_nearest#torch.ao.nn.quantized.functional.upsample_nearest", "type": "Quantization", "text": ["Upsamples the input, using nearest neighbours\u2019 pixel values.", "Warning", "This function is deprecated in favor of torch.ao.nn.quantized.functional.interpolate(). This is equivalent with nn.quantized.functional.interpolate(..., mode='nearest').", "Note", "The input quantization parameters propagate to the output.", "Note", "Only 2D inputs are supported"]}, {"name": "torch.ao.nn.quantized.FXFloatFunctional", "path": "generated/torch.ao.nn.quantized.fxfloatfunctional#torch.ao.nn.quantized.FXFloatFunctional", "type": "Quantization", "text": ["module to replace FloatFunctional module before FX graph mode quantization, since activation_post_process will be inserted in top level module directly"]}, {"name": "torch.ao.nn.quantized.GroupNorm", "path": "generated/torch.ao.nn.quantized.groupnorm#torch.ao.nn.quantized.GroupNorm", "type": "Quantization", "text": ["This is the quantized version of GroupNorm."]}, {"name": "torch.ao.nn.quantized.Hardswish", "path": "generated/torch.ao.nn.quantized.hardswish#torch.ao.nn.quantized.Hardswish", "type": "Quantization", "text": ["This is the quantized version of Hardswish."]}, {"name": "torch.ao.nn.quantized.InstanceNorm1d", "path": "generated/torch.ao.nn.quantized.instancenorm1d#torch.ao.nn.quantized.InstanceNorm1d", "type": "Quantization", "text": ["This is the quantized version of InstanceNorm1d."]}, {"name": "torch.ao.nn.quantized.InstanceNorm2d", "path": "generated/torch.ao.nn.quantized.instancenorm2d#torch.ao.nn.quantized.InstanceNorm2d", "type": "Quantization", "text": ["This is the quantized version of InstanceNorm2d."]}, {"name": "torch.ao.nn.quantized.InstanceNorm3d", "path": "generated/torch.ao.nn.quantized.instancenorm3d#torch.ao.nn.quantized.InstanceNorm3d", "type": "Quantization", "text": ["This is the quantized version of InstanceNorm3d."]}, {"name": "torch.ao.nn.quantized.LayerNorm", "path": "generated/torch.ao.nn.quantized.layernorm#torch.ao.nn.quantized.LayerNorm", "type": "Quantization", "text": ["This is the quantized version of LayerNorm."]}, {"name": "torch.ao.nn.quantized.LeakyReLU", "path": "generated/torch.ao.nn.quantized.leakyrelu#torch.ao.nn.quantized.LeakyReLU", "type": "Quantization", "text": ["This is the quantized equivalent of LeakyReLU."]}, {"name": "torch.ao.nn.quantized.Linear", "path": "generated/torch.ao.nn.quantized.linear#torch.ao.nn.quantized.Linear", "type": "Quantization", "text": ["A quantized linear module with quantized tensor as inputs and outputs. We adopt the same interface as torch.nn.Linear, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.", "Similar to Linear, attributes will be randomly initialized at module creation time and will be overwritten later", "Examples:", "Create a quantized module from an observed float module", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user", "Create a (fbgemm/qnnpack) quantized module from a reference quantized module"]}, {"name": "torch.ao.nn.quantized.Linear.from_float()", "path": "generated/torch.ao.nn.quantized.linear#torch.ao.nn.quantized.Linear.from_float", "type": "Quantization", "text": ["Create a quantized module from an observed float module", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user"]}, {"name": "torch.ao.nn.quantized.Linear.from_reference()", "path": "generated/torch.ao.nn.quantized.linear#torch.ao.nn.quantized.Linear.from_reference", "type": "Quantization", "text": ["Create a (fbgemm/qnnpack) quantized module from a reference quantized module"]}, {"name": "torch.ao.nn.quantized.QFunctional", "path": "generated/torch.ao.nn.quantized.qfunctional#torch.ao.nn.quantized.QFunctional", "type": "Quantization", "text": ["Wrapper class for quantized operations.", "The instance of this class can be used instead of the torch.ops.quantized prefix. See example usage below.", "Note", "This class does not provide a forward hook. Instead, you must use one of the underlying functions (e.g. add).", "Examples:"]}, {"name": "torch.ao.nn.quantized.ReLU6", "path": "generated/torch.ao.nn.quantized.relu6#torch.ao.nn.quantized.ReLU6", "type": "Quantization", "text": ["Applies the element-wise function:", "ReLU6(x)=min\u2061(max\u2061(x0,x),q(6))\\text{ReLU6}(x) = \\min(\\max(x_0, x), q(6)), where x0x_0 is the zero_point, and q(6)q(6) is the quantized representation of number 6.", "inplace (bool) \u2013 can optionally do the operation in-place. Default: False", "Examples:"]}, {"name": "torch.ao.nn.quantized.Sigmoid", "path": "generated/torch.ao.nn.quantized.sigmoid#torch.ao.nn.quantized.Sigmoid", "type": "Quantization", "text": ["This is the quantized equivalent of Sigmoid."]}, {"name": "torch.ao.ns._numeric_suite.compare_model_outputs()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.compare_model_outputs", "type": "Quantization", "text": ["Compare output activations between float and quantized models at corresponding locations for the same input. Return a dict with key corresponding to quantized module names and each entry being a dictionary with two keys \u2018float\u2019 and \u2018quantized\u2019, containing the activations of quantized model and float model at matching locations. This dict can be used to compare and compute the propagation quantization error.", "Example usage:", "dict with key corresponding to quantized module names and each entry being a dictionary with two keys \u2018float\u2019 and \u2018quantized\u2019, containing the matching float and quantized activations", "act_compare_dict"]}, {"name": "torch.ao.ns._numeric_suite.compare_model_stub()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.compare_model_stub", "type": "Quantization", "text": ["Compare quantized module in a model with its floating point counterpart, feeding both of them the same input. Return a dict with key corresponding to module names and each entry being a dictionary with two keys \u2018float\u2019 and \u2018quantized\u2019, containing the output tensors of quantized and its matching float shadow module. This dict can be used to compare and compute the module level quantization error.", "This function first call prepare_model_with_stubs() to swap the quantized module that we want to compare with the Shadow module, which takes quantized module, corresponding float module and logger as input, and creates a forward path inside to make the float module to shadow quantized module sharing the same input. The logger can be customizable, default logger is ShadowLogger and it will save the outputs of the quantized module and float module that can be used to compute the module level quantization error.", "Example usage:", "Dict[str, Dict]"]}, {"name": "torch.ao.ns._numeric_suite.compare_weights()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.compare_weights", "type": "Quantization", "text": ["Compare the weights of the float module with its corresponding quantized module. Return a dict with key corresponding to module names and each entry being a dictionary with two keys \u2018float\u2019 and \u2018quantized\u2019, containing the float and quantized weights. This dict can be used to compare and compute the quantization error of the weights of float and quantized models.", "Example usage:", "dict with key corresponding to module names and each entry being a dictionary with two keys \u2018float\u2019 and \u2018quantized\u2019, containing the float and quantized weights", "weight_dict"]}, {"name": "torch.ao.ns._numeric_suite.get_logger_dict()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.get_logger_dict", "type": "Quantization", "text": ["Traverse the modules and save all logger stats into target dict. This is mainly used for quantization accuracy debug.", "ShadowLogger: used to log the outputs of the quantized module and its matching float shadow module, OutputLogger: used to log the outputs of the modules", "the dictionary used to save all logger stats", "target_dict"]}, {"name": "torch.ao.ns._numeric_suite.get_matching_activations()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.get_matching_activations", "type": "Quantization", "text": ["Find the matching activation between float and quantized modules.", "dict with key corresponding to quantized module names and each entry being a dictionary with two keys \u2018float\u2019 and \u2018quantized\u2019, containing the matching float and quantized activations", "act_dict"]}, {"name": "torch.ao.ns._numeric_suite.Logger", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Logger", "type": "Quantization", "text": ["Base class for stats logging"]}, {"name": "torch.ao.ns._numeric_suite.Logger.forward()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Logger.forward", "type": "Quantization", "text": []}, {"name": "torch.ao.ns._numeric_suite.OutputLogger", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.OutputLogger", "type": "Quantization", "text": ["Class used to log the outputs of the module"]}, {"name": "torch.ao.ns._numeric_suite.OutputLogger.forward()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.OutputLogger.forward", "type": "Quantization", "text": []}, {"name": "torch.ao.ns._numeric_suite.prepare_model_outputs()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.prepare_model_outputs", "type": "Quantization", "text": ["Prepare the model by attaching the logger to both float module and quantized module if they are in the allow_list."]}, {"name": "torch.ao.ns._numeric_suite.prepare_model_with_stubs()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.prepare_model_with_stubs", "type": "Quantization", "text": ["Prepare the model by attaching the float module to its matching quantized module as the shadow if the float module type is in module_swap_list.", "Example usage:"]}, {"name": "torch.ao.ns._numeric_suite.Shadow", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow", "type": "Quantization", "text": ["Shadow module attaches the float module to its matching quantized module as the shadow. Then it uses Logger module to process the outputs of both modules.", "Tensor", "Tensor", "Tensor", "Tensor", "Tensor", "Tensor", "Tensor"]}, {"name": "torch.ao.ns._numeric_suite.Shadow.add()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow.add", "type": "Quantization", "text": ["Tensor"]}, {"name": "torch.ao.ns._numeric_suite.Shadow.add_relu()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow.add_relu", "type": "Quantization", "text": ["Tensor"]}, {"name": "torch.ao.ns._numeric_suite.Shadow.add_scalar()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow.add_scalar", "type": "Quantization", "text": ["Tensor"]}, {"name": "torch.ao.ns._numeric_suite.Shadow.cat()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow.cat", "type": "Quantization", "text": ["Tensor"]}, {"name": "torch.ao.ns._numeric_suite.Shadow.forward()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow.forward", "type": "Quantization", "text": ["Tensor"]}, {"name": "torch.ao.ns._numeric_suite.Shadow.mul()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow.mul", "type": "Quantization", "text": ["Tensor"]}, {"name": "torch.ao.ns._numeric_suite.Shadow.mul_scalar()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow.mul_scalar", "type": "Quantization", "text": ["Tensor"]}, {"name": "torch.ao.ns._numeric_suite.ShadowLogger", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.ShadowLogger", "type": "Quantization", "text": ["Class used in Shadow module to record the outputs of the original and shadow modules."]}, {"name": "torch.ao.ns._numeric_suite.ShadowLogger.forward()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.ShadowLogger.forward", "type": "Quantization", "text": []}, {"name": "torch.ao.ns._numeric_suite_fx.add_loggers()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.add_loggers", "type": "Quantization", "text": ["Instrument model A and model B with loggers.", "Returns a tuple of (model_a_with_loggers, model_b_with_loggers). Modifies both models inplace.", "Tuple[Module, Module]"]}, {"name": "torch.ao.ns._numeric_suite_fx.add_shadow_loggers()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.add_shadow_loggers", "type": "Quantization", "text": ["Instrument model A and model B with shadow loggers."]}, {"name": "torch.ao.ns._numeric_suite_fx.convert_n_shadows_model()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.convert_n_shadows_model", "type": "Quantization", "text": ["Given a model from prepare_n_shadows_model, runs convert_fx on each shadow submodule.", "GraphModule"]}, {"name": "torch.ao.ns._numeric_suite_fx.extend_logger_results_with_comparison()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.extend_logger_results_with_comparison", "type": "Quantization", "text": ["Compares the logged values from model_name_2 against the corresponding values in model_name_1, using comparison_fn. Records the result in model_name_2\u2019s results under comparison_name. Modifies results inplace."]}, {"name": "torch.ao.ns._numeric_suite_fx.extract_logger_info()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.extract_logger_info", "type": "Quantization", "text": ["Traverse all loggers in model_a and model_b, and extract the logged information.", "NSResultsType, containing the logged comparisons", "Dict[str, Dict[str, Dict[str, List[Dict[str, Any]]]]]"]}, {"name": "torch.ao.ns._numeric_suite_fx.extract_results_n_shadows_model()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.extract_results_n_shadows_model", "type": "Quantization", "text": ["Extracts logger results from model.", "Dict[str, Dict[str, Dict[str, List[Dict[str, Any]]]]]"]}, {"name": "torch.ao.ns._numeric_suite_fx.extract_shadow_logger_info()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.extract_shadow_logger_info", "type": "Quantization", "text": ["Traverse all loggers in a shadow model, and extract the logged information.", "NSResultsType, containing the logged comparisons", "Dict[str, Dict[str, Dict[str, List[Dict[str, Any]]]]]"]}, {"name": "torch.ao.ns._numeric_suite_fx.extract_weights()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.extract_weights", "type": "Quantization", "text": ["Extract weights from model A and model B, and return a comparison.", "NSResultsType, containing the weight comparisons", "Dict[str, Dict[str, Dict[str, List[Dict[str, Any]]]]]"]}, {"name": "torch.ao.ns._numeric_suite_fx.loggers_set_enabled()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.loggers_set_enabled", "type": "Quantization", "text": ["Sets the enabled setting on a model\u2019s loggers"]}, {"name": "torch.ao.ns._numeric_suite_fx.loggers_set_save_activations()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.loggers_set_save_activations", "type": "Quantization", "text": ["Sets the save_activations setting on a model\u2019s loggers"]}, {"name": "torch.ao.ns._numeric_suite_fx.NSTracer", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.NSTracer", "type": "Quantization", "text": ["Just like a regular FX quantization tracer, but treats observers and fake_quantize modules as leaf modules.", "bool"]}, {"name": "torch.ao.ns._numeric_suite_fx.NSTracer.is_leaf_module()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.NSTracer.is_leaf_module", "type": "Quantization", "text": ["bool"]}, {"name": "torch.ao.ns._numeric_suite_fx.OutputComparisonLogger", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.OutputComparisonLogger", "type": "Quantization", "text": ["Same as OutputLogger, but also requires the original activation in order to calculate the comparison at calibration time"]}, {"name": "torch.ao.ns._numeric_suite_fx.OutputComparisonLogger.forward()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.OutputComparisonLogger.forward", "type": "Quantization", "text": []}, {"name": "torch.ao.ns._numeric_suite_fx.OutputLogger", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.OutputLogger", "type": "Quantization", "text": ["Base class for capturing intermediate values."]}, {"name": "torch.ao.ns._numeric_suite_fx.OutputLogger.forward()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.OutputLogger.forward", "type": "Quantization", "text": []}, {"name": "torch.ao.ns._numeric_suite_fx.prepare_n_shadows_model()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.prepare_n_shadows_model", "type": "Quantization", "text": ["Given a model with a graph with M ops such as", "args_kwargs_m -> op_m -> output_m", "And a set of N qconfigs for each op, creates a new model, with each of the subgraph of op_m transformed into", "Where op_m_n is op_m wrapped in a submodule and transformed with qconfig_n, and its inner graph looks like", "This is useful for testing different quantization of multiple layers in a single pass through the model.", "High level TODOs for future PRs: * figure out a better way to name the output structure * return a results data structure instead of printing it out * add examples to docblocks", "GraphModule"]}, {"name": "torch.ao.ns._numeric_suite_fx.print_comparisons_n_shadows_model()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.print_comparisons_n_shadows_model", "type": "Quantization", "text": ["Prints a summary of extracted results."]}, {"name": "torch.ao.ns.fx.utils.compute_cosine_similarity()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns.fx.utils.compute_cosine_similarity", "type": "Quantization", "text": []}, {"name": "torch.ao.ns.fx.utils.compute_normalized_l2_error()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns.fx.utils.compute_normalized_l2_error", "type": "Quantization", "text": []}, {"name": "torch.ao.ns.fx.utils.compute_sqnr()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns.fx.utils.compute_sqnr", "type": "Quantization", "text": []}, {"name": "torch.ao.quantization.add_quant_dequant", "path": "generated/torch.ao.quantization.add_quant_dequant#torch.ao.quantization.add_quant_dequant", "type": "Quantization", "text": ["Wrap the leaf child module in QuantWrapper if it has a valid qconfig Note that this function will modify the children of module inplace and it can return a new module which wraps the input module as well.", "Either the inplace modified module with submodules wrapped in QuantWrapper based on qconfig or a new QuantWrapper module which wraps the input module, the latter case only happens when the input module is a leaf module and we want to quantize it."]}, {"name": "torch.ao.quantization.backend_config.BackendConfig", "path": "generated/torch.ao.quantization.backend_config.backendconfig#torch.ao.quantization.backend_config.BackendConfig", "type": "Quantization", "text": ["Config that defines the set of patterns that can be quantized on a given backend, and how reference quantized models can be produced from these patterns.", "A pattern in this context refers to a module, a functional, an operator, or a directed acyclic graph of the above. Each pattern supported on the target backend can be individually configured through BackendPatternConfig in terms of:", "The format of the patterns is described in: https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/backend_config/README.md", "Example usage:", "Return a copy of the list of configs set in this BackendConfig.", "Create a BackendConfig from a dictionary with the following items:", "\u201cname\u201d: the name of the target backend", "\u201cconfigs\u201d: a list of dictionaries that each represents a BackendPatternConfig", "BackendConfig", "Set the config for an pattern that can be run on the target backend. This overrides any existing config for the given pattern.", "BackendConfig", "Set the configs for patterns that can be run on the target backend. This overrides any existing config for a given pattern if it was previously registered already.", "BackendConfig", "Set the name of the target backend.", "BackendConfig", "Convert this BackendConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.backend_config.BackendConfig.configs", "path": "generated/torch.ao.quantization.backend_config.backendconfig#torch.ao.quantization.backend_config.BackendConfig.configs", "type": "Quantization", "text": ["Return a copy of the list of configs set in this BackendConfig."]}, {"name": "torch.ao.quantization.backend_config.BackendConfig.from_dict()", "path": "generated/torch.ao.quantization.backend_config.backendconfig#torch.ao.quantization.backend_config.BackendConfig.from_dict", "type": "Quantization", "text": ["Create a BackendConfig from a dictionary with the following items:", "\u201cname\u201d: the name of the target backend", "\u201cconfigs\u201d: a list of dictionaries that each represents a BackendPatternConfig", "BackendConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendConfig.set_backend_pattern_config()", "path": "generated/torch.ao.quantization.backend_config.backendconfig#torch.ao.quantization.backend_config.BackendConfig.set_backend_pattern_config", "type": "Quantization", "text": ["Set the config for an pattern that can be run on the target backend. This overrides any existing config for the given pattern.", "BackendConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendConfig.set_backend_pattern_configs()", "path": "generated/torch.ao.quantization.backend_config.backendconfig#torch.ao.quantization.backend_config.BackendConfig.set_backend_pattern_configs", "type": "Quantization", "text": ["Set the configs for patterns that can be run on the target backend. This overrides any existing config for a given pattern if it was previously registered already.", "BackendConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendConfig.set_name()", "path": "generated/torch.ao.quantization.backend_config.backendconfig#torch.ao.quantization.backend_config.BackendConfig.set_name", "type": "Quantization", "text": ["Set the name of the target backend.", "BackendConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendConfig.to_dict()", "path": "generated/torch.ao.quantization.backend_config.backendconfig#torch.ao.quantization.backend_config.BackendConfig.to_dict", "type": "Quantization", "text": ["Convert this BackendConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig", "type": "Quantization", "text": ["Config object that specifies quantization behavior for a given operator pattern. For a detailed example usage, see BackendConfig.", "Add a set of supported data types passed as arguments to quantize ops in the reference model spec.", "BackendPatternConfig", "Create a BackendPatternConfig from a dictionary with the following items:", "\u201cpattern\u201d: the pattern being configured \u201cobservation_type\u201d: the ObservationType that specifies how observers should be inserted for this pattern \u201cdtype_configs\u201d: a list of dictionaries that represents DTypeConfig s \u201croot_module\u201d: a torch.nn.Module that represents the root for this pattern \u201cqat_module\u201d: a torch.nn.Module that represents the QAT implementation for this pattern \u201creference_quantized_module\u201d: a torch.nn.Module that represents the reference quantized implementation for this pattern\u2019s root module. \u201cfused_module\u201d: a torch.nn.Module that represents the fused implementation for this pattern \u201cfuser_method\u201d: a function that specifies how to fuse the pattern for this pattern \u201cpattern_complex_format\u201d: the pattern specified in the reversed nested tuple format (deprecated)", "BackendPatternConfig", "Set the supported data types passed as arguments to quantize ops in the reference model spec, overriding all previously registered data types.", "BackendPatternConfig", "Set the module that represents the fused implementation for this pattern.", "BackendPatternConfig", "Set the function that specifies how to fuse this BackendPatternConfig\u2019s pattern.", "The first argument of this function should be is_qat, and the rest of the arguments should be the items in the tuple pattern. The return value of this function should be the resulting fused module.", "For example, the fuser method for the pattern (torch.nn.Linear, torch.nn.ReLU) can be:", "return torch.ao.nn.intrinsic.LinearReLU(linear, relu)", "For a more complicated example, see https://gist.github.com/jerryzh168/8bea7180a8ba3c279f2c9b050f2a69a6.", "BackendPatternConfig", "Set how observers should be inserted in the graph for this pattern.", "Observation type here refers to how observers (or quant-dequant ops) will be placed in the graph. This is used to produce the desired reference patterns understood by the backend. Weighted ops such as linear and conv require different observers (or quantization parameters passed to quantize ops in the reference model) for the input and the output.", "There are two observation types:", "OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT (default): the output observer instance will be different from the input. This is the most common observation type.", "OUTPUT_SHARE_OBSERVER_WITH_INPUT: the output observer instance will be the same as the input. This is useful for operators like cat.", "Note: This will be renamed in the near future, since we will soon insert QuantDeQuantStubs with observers (and fake quantizes) attached instead of observers themselves.", "BackendPatternConfig", "Set the pattern to configure.", "The pattern can be a float module, functional operator, pytorch operator, or a tuple combination of the above. Tuple patterns are treated as sequential patterns, and currently only tuples of 2 or 3 elements are supported.", "BackendPatternConfig", "Set the module that represents the QAT implementation for this pattern.", "BackendPatternConfig", "Set the module that represents the reference quantized implementation for this pattern\u2019s root module.", "For more detail, see set_root_module().", "BackendPatternConfig", "Set the module that represents the root for this pattern.", "When we construct the reference quantized model during the convert phase, the root modules (e.g. torch.nn.Linear for torch.ao.nn.intrinsic.LinearReLU) will be swapped to the corresponding reference quantized modules (e.g. torch.ao.nn.reference.quantized.Linear). This allows custom backends to specify custom reference quantized module implementations to match the numerics of their lowered operators. Since this is a one-to-one mapping, both the root module and the reference quantized module must be specified in the same BackendPatternConfig in order for the conversion to take place.", "BackendPatternConfig", "Convert this BackendPatternConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.add_dtype_config()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.add_dtype_config", "type": "Quantization", "text": ["Add a set of supported data types passed as arguments to quantize ops in the reference model spec.", "BackendPatternConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.from_dict()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.from_dict", "type": "Quantization", "text": ["Create a BackendPatternConfig from a dictionary with the following items:", "\u201cpattern\u201d: the pattern being configured \u201cobservation_type\u201d: the ObservationType that specifies how observers should be inserted for this pattern \u201cdtype_configs\u201d: a list of dictionaries that represents DTypeConfig s \u201croot_module\u201d: a torch.nn.Module that represents the root for this pattern \u201cqat_module\u201d: a torch.nn.Module that represents the QAT implementation for this pattern \u201creference_quantized_module\u201d: a torch.nn.Module that represents the reference quantized implementation for this pattern\u2019s root module. \u201cfused_module\u201d: a torch.nn.Module that represents the fused implementation for this pattern \u201cfuser_method\u201d: a function that specifies how to fuse the pattern for this pattern \u201cpattern_complex_format\u201d: the pattern specified in the reversed nested tuple format (deprecated)", "BackendPatternConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_dtype_configs()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_dtype_configs", "type": "Quantization", "text": ["Set the supported data types passed as arguments to quantize ops in the reference model spec, overriding all previously registered data types.", "BackendPatternConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_fused_module()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_fused_module", "type": "Quantization", "text": ["Set the module that represents the fused implementation for this pattern.", "BackendPatternConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_fuser_method()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_fuser_method", "type": "Quantization", "text": ["Set the function that specifies how to fuse this BackendPatternConfig\u2019s pattern.", "The first argument of this function should be is_qat, and the rest of the arguments should be the items in the tuple pattern. The return value of this function should be the resulting fused module.", "For example, the fuser method for the pattern (torch.nn.Linear, torch.nn.ReLU) can be:", "return torch.ao.nn.intrinsic.LinearReLU(linear, relu)", "For a more complicated example, see https://gist.github.com/jerryzh168/8bea7180a8ba3c279f2c9b050f2a69a6.", "BackendPatternConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_observation_type()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_observation_type", "type": "Quantization", "text": ["Set how observers should be inserted in the graph for this pattern.", "Observation type here refers to how observers (or quant-dequant ops) will be placed in the graph. This is used to produce the desired reference patterns understood by the backend. Weighted ops such as linear and conv require different observers (or quantization parameters passed to quantize ops in the reference model) for the input and the output.", "There are two observation types:", "OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT (default): the output observer instance will be different from the input. This is the most common observation type.", "OUTPUT_SHARE_OBSERVER_WITH_INPUT: the output observer instance will be the same as the input. This is useful for operators like cat.", "Note: This will be renamed in the near future, since we will soon insert QuantDeQuantStubs with observers (and fake quantizes) attached instead of observers themselves.", "BackendPatternConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_pattern()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_pattern", "type": "Quantization", "text": ["Set the pattern to configure.", "The pattern can be a float module, functional operator, pytorch operator, or a tuple combination of the above. Tuple patterns are treated as sequential patterns, and currently only tuples of 2 or 3 elements are supported.", "BackendPatternConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_qat_module()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_qat_module", "type": "Quantization", "text": ["Set the module that represents the QAT implementation for this pattern.", "BackendPatternConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_reference_quantized_module()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_reference_quantized_module", "type": "Quantization", "text": ["Set the module that represents the reference quantized implementation for this pattern\u2019s root module.", "For more detail, see set_root_module().", "BackendPatternConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_root_module()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_root_module", "type": "Quantization", "text": ["Set the module that represents the root for this pattern.", "When we construct the reference quantized model during the convert phase, the root modules (e.g. torch.nn.Linear for torch.ao.nn.intrinsic.LinearReLU) will be swapped to the corresponding reference quantized modules (e.g. torch.ao.nn.reference.quantized.Linear). This allows custom backends to specify custom reference quantized module implementations to match the numerics of their lowered operators. Since this is a one-to-one mapping, both the root module and the reference quantized module must be specified in the same BackendPatternConfig in order for the conversion to take place.", "BackendPatternConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.to_dict()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.to_dict", "type": "Quantization", "text": ["Convert this BackendPatternConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.backend_config.DTypeConfig", "path": "generated/torch.ao.quantization.backend_config.dtypeconfig#torch.ao.quantization.backend_config.DTypeConfig", "type": "Quantization", "text": ["Config object that specifies the supported data types passed as arguments to quantize ops in the reference model spec, for input and output activations, weights, and biases.", "For example, consider the following reference model:", "quant1 - [dequant1 - fp32_linear - quant2] - dequant2", "The pattern in the square brackets refers to the reference pattern of statically quantized linear. Setting the input dtype as torch.quint8 in the DTypeConfig means we pass in torch.quint8 as the dtype argument to the first quantize op (quant1). Similarly, setting the output dtype as torch.quint8 means we pass in torch.quint8 as the dtype argument to the second quantize op (quant2).", "Note that the dtype here does not refer to the interface dtypes of the op. For example, the \u201cinput dtype\u201d here is not the dtype of the input tensor passed to the quantized linear op. Though it can still be the same as the interface dtype, this is not always the case, e.g. the interface dtype is fp32 in dynamic quantization but the \u201cinput dtype\u201d specified in the DTypeConfig would still be quint8. The semantics of dtypes here are the same as the semantics of the dtypes specified in the observers.", "These dtypes are matched against the ones specified in the user\u2019s QConfig. If there is a match, and the QConfig satisfies the constraints specified in the DTypeConfig (if any), then we will quantize the given pattern using this DTypeConfig. Otherwise, the QConfig is ignored and the pattern will not be quantized.", "Example usage:", "\u201cinput_dtype\u201d: torch.dtype or DTypeWithConstraints \u201coutput_dtype\u201d: torch.dtype or DTypeWithConstraints \u201cweight_dtype\u201d: torch.dtype or DTypeWithConstraints \u201cbias_type\u201d: torch.dtype \u201cis_dynamic\u201d: bool", "DTypeConfig", "Convert this DTypeConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.backend_config.DTypeConfig.from_dict()", "path": "generated/torch.ao.quantization.backend_config.dtypeconfig#torch.ao.quantization.backend_config.DTypeConfig.from_dict", "type": "Quantization", "text": ["\u201cinput_dtype\u201d: torch.dtype or DTypeWithConstraints \u201coutput_dtype\u201d: torch.dtype or DTypeWithConstraints \u201cweight_dtype\u201d: torch.dtype or DTypeWithConstraints \u201cbias_type\u201d: torch.dtype \u201cis_dynamic\u201d: bool", "DTypeConfig"]}, {"name": "torch.ao.quantization.backend_config.DTypeConfig.to_dict()", "path": "generated/torch.ao.quantization.backend_config.dtypeconfig#torch.ao.quantization.backend_config.DTypeConfig.to_dict", "type": "Quantization", "text": ["Convert this DTypeConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.backend_config.DTypeWithConstraints", "path": "generated/torch.ao.quantization.backend_config.dtypewithconstraints#torch.ao.quantization.backend_config.DTypeWithConstraints", "type": "Quantization", "text": ["Config for specifying additional constraints for a given dtype, such as quantization value ranges, scale value ranges, and fixed quantization params, to be used in DTypeConfig.", "The constraints currently supported are:"]}, {"name": "torch.ao.quantization.backend_config.ObservationType", "path": "generated/torch.ao.quantization.backend_config.observationtype#torch.ao.quantization.backend_config.ObservationType", "type": "Quantization", "text": ["An enum that represents different ways of how an operator/operator pattern should be observed", "this means the input and output are never observed example: x.shape, x.size", "this means the output will use the same observer instance as input, based on qconfig.activation example: torch.cat, maxpool", "this means input and output are observed with different observers, based on qconfig.activation example: conv, linear, softmax"]}, {"name": "torch.ao.quantization.backend_config.ObservationType.INPUT_OUTPUT_NOT_OBSERVED", "path": "generated/torch.ao.quantization.backend_config.observationtype#torch.ao.quantization.backend_config.ObservationType.INPUT_OUTPUT_NOT_OBSERVED", "type": "Quantization", "text": ["this means the input and output are never observed example: x.shape, x.size"]}, {"name": "torch.ao.quantization.backend_config.ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT", "path": "generated/torch.ao.quantization.backend_config.observationtype#torch.ao.quantization.backend_config.ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT", "type": "Quantization", "text": ["this means the output will use the same observer instance as input, based on qconfig.activation example: torch.cat, maxpool"]}, {"name": "torch.ao.quantization.backend_config.ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT", "path": "generated/torch.ao.quantization.backend_config.observationtype#torch.ao.quantization.backend_config.ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT", "type": "Quantization", "text": ["this means input and output are observed with different observers, based on qconfig.activation example: conv, linear, softmax"]}, {"name": "torch.ao.quantization.convert", "path": "generated/torch.ao.quantization.convert#torch.ao.quantization.convert", "type": "Quantization", "text": ["Converts submodules in input module to a different module according to mapping by calling from_float method on the target module class. And remove qconfig at the end if remove_qconfig is set to True."]}, {"name": "torch.ao.quantization.default_eval_fn", "path": "generated/torch.ao.quantization.default_eval_fn#torch.ao.quantization.default_eval_fn", "type": "Quantization", "text": ["Default evaluation function takes a torch.utils.data.Dataset or a list of input Tensors and run the model on the dataset"]}, {"name": "torch.ao.quantization.DeQuantStub", "path": "generated/torch.ao.quantization.dequantstub#torch.ao.quantization.DeQuantStub", "type": "Quantization", "text": ["Dequantize stub module, before calibration, this is same as identity, this will be swapped as nnq.DeQuantize in convert.", "qconfig \u2013 quantization configuration for the tensor, if qconfig is not provided, we will get qconfig from parent modules"]}, {"name": "torch.ao.quantization.fake_quantize.default_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.default_fake_quant#torch.ao.quantization.fake_quantize.default_fake_quant", "type": "Quantization", "text": ["Default fake_quant for activations."]}, {"name": "torch.ao.quantization.fake_quantize.default_fused_act_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.default_fused_act_fake_quant#torch.ao.quantization.fake_quantize.default_fused_act_fake_quant", "type": "Quantization", "text": ["Fused version of default_fake_quant, with improved performance."]}, {"name": "torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant#torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant", "type": "Quantization", "text": ["Fused version of default_per_channel_weight_fake_quant, with improved performance."]}, {"name": "torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant#torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant", "type": "Quantization", "text": ["Fused version of default_weight_fake_quant, with improved performance."]}, {"name": "torch.ao.quantization.fake_quantize.default_histogram_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.default_histogram_fake_quant#torch.ao.quantization.fake_quantize.default_histogram_fake_quant", "type": "Quantization", "text": ["Fake_quant for activations using a histogram.."]}, {"name": "torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant#torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant", "type": "Quantization", "text": ["Default fake_quant for per-channel weights. Observer is memoryless since averaging_constant is 1."]}, {"name": "torch.ao.quantization.fake_quantize.default_weight_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.default_weight_fake_quant#torch.ao.quantization.fake_quantize.default_weight_fake_quant", "type": "Quantization", "text": ["Default fake_quant for weights. Observer is memoryless since averaging_constant is 1."]}, {"name": "torch.ao.quantization.fake_quantize.disable_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.disable_fake_quant#torch.ao.quantization.fake_quantize.disable_fake_quant", "type": "Quantization", "text": ["Disable fake quantization for this module, if applicable. Example usage:"]}, {"name": "torch.ao.quantization.fake_quantize.disable_observer", "path": "generated/torch.ao.quantization.fake_quantize.disable_observer#torch.ao.quantization.fake_quantize.disable_observer", "type": "Quantization", "text": ["Disable observation for this module, if applicable. Example usage:"]}, {"name": "torch.ao.quantization.fake_quantize.enable_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.enable_fake_quant#torch.ao.quantization.fake_quantize.enable_fake_quant", "type": "Quantization", "text": ["Enable fake quantization for this module, if applicable. Example usage:"]}, {"name": "torch.ao.quantization.fake_quantize.enable_observer", "path": "generated/torch.ao.quantization.fake_quantize.enable_observer#torch.ao.quantization.fake_quantize.enable_observer", "type": "Quantization", "text": ["Enable observation for this module, if applicable. Example usage:"]}, {"name": "torch.ao.quantization.fake_quantize.FakeQuantize", "path": "generated/torch.ao.quantization.fake_quantize.fakequantize#torch.ao.quantization.fake_quantize.FakeQuantize", "type": "Quantization", "text": ["Simulate the quantize and dequantize operations in training time. The output of this module is given by:", "allowable values are torch.qint8 and torch.quint8.", "activation_post_process (Module) \u2013 User provided module that collects statistics on the input tensor and provides a method to calculate scale and zero-point."]}, {"name": "torch.ao.quantization.fake_quantize.FakeQuantizeBase", "path": "generated/torch.ao.quantization.fake_quantize.fakequantizebase#torch.ao.quantization.fake_quantize.FakeQuantizeBase", "type": "Quantization", "text": ["Base fake quantize module Any fake quantize implementation should derive from this class.", "Concrete fake quantize module should follow the same API. In forward, they will update the statistics of the observed Tensor and fake quantize the input. They should also provide a calculate_qparams function that computes the quantization parameters given the collected statistics."]}, {"name": "torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize", "path": "generated/torch.ao.quantization.fake_quantize.fixedqparamsfakequantize#torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize", "type": "Quantization", "text": ["Simulate quantize and dequantize with fixed quantization parameters in training time. Only per tensor quantization is supported."]}, {"name": "torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize", "path": "generated/torch.ao.quantization.fake_quantize.fusedmovingavgobsfakequantize#torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize", "type": "Quantization", "text": ["Fused module that is used to observe the input tensor (compute min/max), compute scale/zero_point and fake_quantize the tensor. This module uses calculation similar MovingAverageMinMaxObserver for the inputs, to compute the min/max values in order to compute the scale/zero_point. The qscheme input in the observer is used to differentiate between symmetric/affine quantization scheme.", "The output of this module is given by x_out = (clamp(round(x/scale + zero_point), quant_min, quant_max)-zero_point)*scale", "Similar to FakeQuantize, and accepts the same attributes as the base class."]}, {"name": "torch.ao.quantization.fuse_modules", "path": "generated/torch.ao.quantization.fuse_modules#torch.ao.quantization.fuse_modules", "type": "Quantization", "text": ["Fuses a list of modules into a single module", "Fuses only the following sequence of modules: conv, bn conv, bn, relu conv, relu linear, relu bn, relu All other sequences are left unchanged. For these sequences, replaces the first item in the list with the fused module, replacing the rest of the modules with identity.", "model with fused modules. A new copy is created if inplace=True.", "Examples:"]}, {"name": "torch.ao.quantization.fx.custom_config.ConvertCustomConfig", "path": "generated/torch.ao.quantization.fx.custom_config.convertcustomconfig#torch.ao.quantization.fx.custom_config.ConvertCustomConfig", "type": "Quantization", "text": ["Custom configuration for convert_fx().", "Example usage:", "Create a ConvertCustomConfig from a dictionary with the following items:", "\u201cobserved_to_quantized_custom_module_class\u201d: a nested dictionary mapping from quantization mode to an inner mapping from observed module classes to quantized module classes, e.g.:: { \u201cstatic\u201d: {FloatCustomModule: ObservedCustomModule}, \u201cdynamic\u201d: {FloatCustomModule: ObservedCustomModule}, \u201cweight_only\u201d: {FloatCustomModule: ObservedCustomModule} } \u201cpreserved_attributes\u201d: a list of attributes that persist even if they are not used in forward", "This function is primarily for backward compatibility and may be removed in the future.", "ConvertCustomConfig", "Set the mapping from a custom observed module class to a custom quantized module class.", "The quantized module class must have a from_observed class method that converts the observed module class to the quantized module class.", "ConvertCustomConfig", "Set the names of the attributes that will persist in the graph module even if they are not used in the model\u2019s forward method.", "ConvertCustomConfig", "Convert this ConvertCustomConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.fx.custom_config.ConvertCustomConfig.from_dict()", "path": "generated/torch.ao.quantization.fx.custom_config.convertcustomconfig#torch.ao.quantization.fx.custom_config.ConvertCustomConfig.from_dict", "type": "Quantization", "text": ["Create a ConvertCustomConfig from a dictionary with the following items:", "\u201cobserved_to_quantized_custom_module_class\u201d: a nested dictionary mapping from quantization mode to an inner mapping from observed module classes to quantized module classes, e.g.:: { \u201cstatic\u201d: {FloatCustomModule: ObservedCustomModule}, \u201cdynamic\u201d: {FloatCustomModule: ObservedCustomModule}, \u201cweight_only\u201d: {FloatCustomModule: ObservedCustomModule} } \u201cpreserved_attributes\u201d: a list of attributes that persist even if they are not used in forward", "This function is primarily for backward compatibility and may be removed in the future.", "ConvertCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.ConvertCustomConfig.set_observed_to_quantized_mapping()", "path": "generated/torch.ao.quantization.fx.custom_config.convertcustomconfig#torch.ao.quantization.fx.custom_config.ConvertCustomConfig.set_observed_to_quantized_mapping", "type": "Quantization", "text": ["Set the mapping from a custom observed module class to a custom quantized module class.", "The quantized module class must have a from_observed class method that converts the observed module class to the quantized module class.", "ConvertCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.ConvertCustomConfig.set_preserved_attributes()", "path": "generated/torch.ao.quantization.fx.custom_config.convertcustomconfig#torch.ao.quantization.fx.custom_config.ConvertCustomConfig.set_preserved_attributes", "type": "Quantization", "text": ["Set the names of the attributes that will persist in the graph module even if they are not used in the model\u2019s forward method.", "ConvertCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.ConvertCustomConfig.to_dict()", "path": "generated/torch.ao.quantization.fx.custom_config.convertcustomconfig#torch.ao.quantization.fx.custom_config.ConvertCustomConfig.to_dict", "type": "Quantization", "text": ["Convert this ConvertCustomConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.fx.custom_config.FuseCustomConfig", "path": "generated/torch.ao.quantization.fx.custom_config.fusecustomconfig#torch.ao.quantization.fx.custom_config.FuseCustomConfig", "type": "Quantization", "text": ["Custom configuration for fuse_fx().", "Example usage:", "Create a ConvertCustomConfig from a dictionary with the following items:", "\u201cpreserved_attributes\u201d: a list of attributes that persist even if they are not used in forward", "This function is primarily for backward compatibility and may be removed in the future.", "FuseCustomConfig", "Set the names of the attributes that will persist in the graph module even if they are not used in the model\u2019s forward method.", "FuseCustomConfig", "Convert this FuseCustomConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.fx.custom_config.FuseCustomConfig.from_dict()", "path": "generated/torch.ao.quantization.fx.custom_config.fusecustomconfig#torch.ao.quantization.fx.custom_config.FuseCustomConfig.from_dict", "type": "Quantization", "text": ["Create a ConvertCustomConfig from a dictionary with the following items:", "\u201cpreserved_attributes\u201d: a list of attributes that persist even if they are not used in forward", "This function is primarily for backward compatibility and may be removed in the future.", "FuseCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.FuseCustomConfig.set_preserved_attributes()", "path": "generated/torch.ao.quantization.fx.custom_config.fusecustomconfig#torch.ao.quantization.fx.custom_config.FuseCustomConfig.set_preserved_attributes", "type": "Quantization", "text": ["Set the names of the attributes that will persist in the graph module even if they are not used in the model\u2019s forward method.", "FuseCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.FuseCustomConfig.to_dict()", "path": "generated/torch.ao.quantization.fx.custom_config.fusecustomconfig#torch.ao.quantization.fx.custom_config.FuseCustomConfig.to_dict", "type": "Quantization", "text": ["Convert this FuseCustomConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig", "type": "Quantization", "text": ["Custom configuration for prepare_fx() and prepare_qat_fx().", "Example usage:", "Create a PrepareCustomConfig from a dictionary with the following items:", "\u201cstandalone_module_name\u201d: a list of (module_name, qconfig_mapping, example_inputs, child_prepare_custom_config, backend_config) tuples", "\u201cstandalone_module_class\u201d a list of (module_class, qconfig_mapping, example_inputs, child_prepare_custom_config, backend_config) tuples", "\u201cfloat_to_observed_custom_module_class\u201d: a nested dictionary mapping from quantization mode to an inner mapping from float module classes to observed module classes, e.g. {\u201cstatic\u201d: {FloatCustomModule: ObservedCustomModule}}", "\u201cnon_traceable_module_name\u201d: a list of modules names that are not symbolically traceable \u201cnon_traceable_module_class\u201d: a list of module classes that are not symbolically traceable \u201cinput_quantized_idxs\u201d: a list of indexes of graph inputs that should be quantized \u201coutput_quantized_idxs\u201d: a list of indexes of graph outputs that should be quantized \u201cpreserved_attributes\u201d: a list of attributes that persist even if they are not used in forward", "This function is primarily for backward compatibility and may be removed in the future.", "PrepareCustomConfig", "Set the mapping from a custom float module class to a custom observed module class.", "The observed module class must have a from_float class method that converts the float module class to the observed module class. This is currently only supported for static quantization.", "PrepareCustomConfig", "Set the indexes of the inputs of the graph that should be quantized. Inputs are otherwise assumed to be in fp32 by default instead.", "PrepareCustomConfig", "Set the modules that are not symbolically traceable, identified by class.", "PrepareCustomConfig", "Set the modules that are not symbolically traceable, identified by name.", "PrepareCustomConfig", "Set the indexes of the outputs of the graph that should be quantized. Outputs are otherwise assumed to be in fp32 by default instead.", "PrepareCustomConfig", "Set the names of the attributes that will persist in the graph module even if they are not used in the model\u2019s forward method.", "PrepareCustomConfig", "Set the configuration for running a standalone module identified by module_class.", "If qconfig_mapping is None, the parent qconfig_mapping will be used instead. If prepare_custom_config is None, an empty PrepareCustomConfig will be used. If backend_config is None, the parent backend_config will be used instead.", "PrepareCustomConfig", "Set the configuration for running a standalone module identified by module_name.", "If qconfig_mapping is None, the parent qconfig_mapping will be used instead. If prepare_custom_config is None, an empty PrepareCustomConfig will be used. If backend_config is None, the parent backend_config will be used instead.", "PrepareCustomConfig", "Convert this PrepareCustomConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.from_dict()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.from_dict", "type": "Quantization", "text": ["Create a PrepareCustomConfig from a dictionary with the following items:", "\u201cstandalone_module_name\u201d: a list of (module_name, qconfig_mapping, example_inputs, child_prepare_custom_config, backend_config) tuples", "\u201cstandalone_module_class\u201d a list of (module_class, qconfig_mapping, example_inputs, child_prepare_custom_config, backend_config) tuples", "\u201cfloat_to_observed_custom_module_class\u201d: a nested dictionary mapping from quantization mode to an inner mapping from float module classes to observed module classes, e.g. {\u201cstatic\u201d: {FloatCustomModule: ObservedCustomModule}}", "\u201cnon_traceable_module_name\u201d: a list of modules names that are not symbolically traceable \u201cnon_traceable_module_class\u201d: a list of module classes that are not symbolically traceable \u201cinput_quantized_idxs\u201d: a list of indexes of graph inputs that should be quantized \u201coutput_quantized_idxs\u201d: a list of indexes of graph outputs that should be quantized \u201cpreserved_attributes\u201d: a list of attributes that persist even if they are not used in forward", "This function is primarily for backward compatibility and may be removed in the future.", "PrepareCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_float_to_observed_mapping()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_float_to_observed_mapping", "type": "Quantization", "text": ["Set the mapping from a custom float module class to a custom observed module class.", "The observed module class must have a from_float class method that converts the float module class to the observed module class. This is currently only supported for static quantization.", "PrepareCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_input_quantized_indexes()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_input_quantized_indexes", "type": "Quantization", "text": ["Set the indexes of the inputs of the graph that should be quantized. Inputs are otherwise assumed to be in fp32 by default instead.", "PrepareCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_non_traceable_module_classes()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_non_traceable_module_classes", "type": "Quantization", "text": ["Set the modules that are not symbolically traceable, identified by class.", "PrepareCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_non_traceable_module_names()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_non_traceable_module_names", "type": "Quantization", "text": ["Set the modules that are not symbolically traceable, identified by name.", "PrepareCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_output_quantized_indexes()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_output_quantized_indexes", "type": "Quantization", "text": ["Set the indexes of the outputs of the graph that should be quantized. Outputs are otherwise assumed to be in fp32 by default instead.", "PrepareCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_preserved_attributes()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_preserved_attributes", "type": "Quantization", "text": ["Set the names of the attributes that will persist in the graph module even if they are not used in the model\u2019s forward method.", "PrepareCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_standalone_module_class()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_standalone_module_class", "type": "Quantization", "text": ["Set the configuration for running a standalone module identified by module_class.", "If qconfig_mapping is None, the parent qconfig_mapping will be used instead. If prepare_custom_config is None, an empty PrepareCustomConfig will be used. If backend_config is None, the parent backend_config will be used instead.", "PrepareCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_standalone_module_name()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_standalone_module_name", "type": "Quantization", "text": ["Set the configuration for running a standalone module identified by module_name.", "If qconfig_mapping is None, the parent qconfig_mapping will be used instead. If prepare_custom_config is None, an empty PrepareCustomConfig will be used. If backend_config is None, the parent backend_config will be used instead.", "PrepareCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.to_dict()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.to_dict", "type": "Quantization", "text": ["Convert this PrepareCustomConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry", "path": "generated/torch.ao.quantization.fx.custom_config.standalonemoduleconfigentry#torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry", "type": "Quantization", "text": []}, {"name": "torch.ao.quantization.observer.default_debug_observer", "path": "generated/torch.ao.quantization.observer.default_debug_observer#torch.ao.quantization.observer.default_debug_observer", "type": "Quantization", "text": ["Default debug-only observer."]}, {"name": "torch.ao.quantization.observer.default_dynamic_quant_observer", "path": "generated/torch.ao.quantization.observer.default_dynamic_quant_observer#torch.ao.quantization.observer.default_dynamic_quant_observer", "type": "Quantization", "text": ["Default observer for dynamic quantization."]}, {"name": "torch.ao.quantization.observer.default_float_qparams_observer", "path": "generated/torch.ao.quantization.observer.default_float_qparams_observer#torch.ao.quantization.observer.default_float_qparams_observer", "type": "Quantization", "text": ["Default observer for a floating point zero-point."]}, {"name": "torch.ao.quantization.observer.default_histogram_observer", "path": "generated/torch.ao.quantization.observer.default_histogram_observer#torch.ao.quantization.observer.default_histogram_observer", "type": "Quantization", "text": ["Default histogram observer, usually used for PTQ."]}, {"name": "torch.ao.quantization.observer.default_observer", "path": "generated/torch.ao.quantization.observer.default_observer#torch.ao.quantization.observer.default_observer", "type": "Quantization", "text": ["Default observer for static quantization, usually used for debugging."]}, {"name": "torch.ao.quantization.observer.default_per_channel_weight_observer", "path": "generated/torch.ao.quantization.observer.default_per_channel_weight_observer#torch.ao.quantization.observer.default_per_channel_weight_observer", "type": "Quantization", "text": ["Default per-channel weight observer, usually used on backends where per-channel weight quantization is supported, such as fbgemm."]}, {"name": "torch.ao.quantization.observer.default_placeholder_observer", "path": "generated/torch.ao.quantization.observer.default_placeholder_observer#torch.ao.quantization.observer.default_placeholder_observer", "type": "Quantization", "text": ["Default placeholder observer, usually used for quantization to torch.float16."]}, {"name": "torch.ao.quantization.observer.default_weight_observer", "path": "generated/torch.ao.quantization.observer.default_weight_observer#torch.ao.quantization.observer.default_weight_observer", "type": "Quantization", "text": ["Default weight observer."]}, {"name": "torch.ao.quantization.observer.get_observer_state_dict", "path": "generated/torch.ao.quantization.observer.get_observer_state_dict#torch.ao.quantization.observer.get_observer_state_dict", "type": "Quantization", "text": ["Returns the state dict corresponding to the observer stats. Traverse the model state_dict and extract out the stats."]}, {"name": "torch.ao.quantization.observer.HistogramObserver", "path": "generated/torch.ao.quantization.observer.histogramobserver#torch.ao.quantization.observer.HistogramObserver", "type": "Quantization", "text": ["The module records the running histogram of tensor values along with min/max values. calculate_qparams will calculate scale and zero_point.", "The scale and zero point are computed as follows:", "The histogram is computed continuously, and the ranges per bin change with every new tensor observed.", "The search for the min/max values ensures the minimization of the quantization error with respect to the floating point model.", "MinMaxObserver"]}, {"name": "torch.ao.quantization.observer.load_observer_state_dict", "path": "generated/torch.ao.quantization.observer.load_observer_state_dict#torch.ao.quantization.observer.load_observer_state_dict", "type": "Quantization", "text": ["Given input model and a state_dict containing model observer stats, load the stats back into the model. The observer state_dict can be saved using torch.ao.quantization.get_observer_state_dict"]}, {"name": "torch.ao.quantization.observer.MinMaxObserver", "path": "generated/torch.ao.quantization.observer.minmaxobserver#torch.ao.quantization.observer.MinMaxObserver", "type": "Quantization", "text": ["Observer module for computing the quantization parameters based on the running min and max values.", "This observer uses the tensor min/max statistics to compute the quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.", "Given running min/max as xminx_\\text{min} and xmaxx_\\text{max}, scale ss and zero point zz are computed as:", "The running minimum/maximum xmin/maxx_\\text{min/max} is computed as:", "where XX is the observed tensor.", "The scale ss and zero point zz are then computed as:", "where QminQ_\\text{min} and QmaxQ_\\text{max} are the minimum and maximum of the quantized data type.", "Warning", "dtype can only take torch.qint8 or torch.quint8.", "Note", "If the running minimum equals to the running maximum, the scale and zero_point are set to 1.0 and 0.", "Calculates the quantization parameters.", "Records the running minimum and maximum of x.", "Resets the min/max values."]}, {"name": "torch.ao.quantization.observer.MinMaxObserver.calculate_qparams()", "path": "generated/torch.ao.quantization.observer.minmaxobserver#torch.ao.quantization.observer.MinMaxObserver.calculate_qparams", "type": "Quantization", "text": ["Calculates the quantization parameters."]}, {"name": "torch.ao.quantization.observer.MinMaxObserver.forward()", "path": "generated/torch.ao.quantization.observer.minmaxobserver#torch.ao.quantization.observer.MinMaxObserver.forward", "type": "Quantization", "text": ["Records the running minimum and maximum of x."]}, {"name": "torch.ao.quantization.observer.MinMaxObserver.reset_min_max_vals()", "path": "generated/torch.ao.quantization.observer.minmaxobserver#torch.ao.quantization.observer.MinMaxObserver.reset_min_max_vals", "type": "Quantization", "text": ["Resets the min/max values."]}, {"name": "torch.ao.quantization.observer.MovingAverageMinMaxObserver", "path": "generated/torch.ao.quantization.observer.movingaverageminmaxobserver#torch.ao.quantization.observer.MovingAverageMinMaxObserver", "type": "Quantization", "text": ["Observer module for computing the quantization parameters based on the moving average of the min and max values.", "This observer computes the quantization parameters based on the moving averages of minimums and maximums of the incoming tensors. The module records the average minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.", "The moving average min/max is computed as follows", "where xmin/maxx_\\text{min/max} is the running average min/max, XX is is the incoming tensor, and cc is the averaging_constant.", "The scale and zero point are then computed as in MinMaxObserver.", "Note", "Only works with torch.per_tensor_affine quantization scheme.", "Note", "If the running minimum equals to the running maximum, the scale and zero_point are set to 1.0 and 0."]}, {"name": "torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver", "path": "generated/torch.ao.quantization.observer.movingaverageperchannelminmaxobserver#torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver", "type": "Quantization", "text": ["Observer module for computing the quantization parameters based on the running per channel min and max values.", "This observer uses the tensor min/max statistics to compute the per channel quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.", "The quantization parameters are computed the same way as in MovingAverageMinMaxObserver, with the difference that the running min/max values are stored per channel. Scales and zero points are thus computed per channel as well.", "Note", "If the running minimum equals to the running maximum, the scales and zero_points are set to 1.0 and 0."]}, {"name": "torch.ao.quantization.observer.NoopObserver", "path": "generated/torch.ao.quantization.observer.noopobserver#torch.ao.quantization.observer.NoopObserver", "type": "Quantization", "text": ["Observer that doesn\u2019t do anything and just passes its configuration to the quantized module\u2019s .from_float().", "Primarily used for quantization to float16 which doesn\u2019t require determining ranges."]}, {"name": "torch.ao.quantization.observer.ObserverBase", "path": "generated/torch.ao.quantization.observer.observerbase#torch.ao.quantization.observer.ObserverBase", "type": "Quantization", "text": ["Base observer Module. Any observer implementation should derive from this class.", "Concrete observers should follow the same API. In forward, they will update the statistics of the observed Tensor. And they should provide a calculate_qparams function that computes the quantization parameters given the collected statistics.", "dtype \u2013 dtype argument to the quantize node needed to implement the reference model spec.", "Wrapper that allows creation of class factories.", "This can be useful when there is a need to create classes with the same constructor arguments, but different instances. Can be used in conjunction with _callable_args", "Example:", "Wrapper that allows creation of class factories args that need to be called at construction time.", "This can be useful when there is a need to create classes with the same constructor arguments, but different instances and those arguments should only be calculated at construction time. Can be used in conjunction with _with_args", "Example:"]}, {"name": "torch.ao.quantization.observer.ObserverBase.with_args()", "path": "generated/torch.ao.quantization.observer.observerbase#torch.ao.quantization.observer.ObserverBase.with_args", "type": "Quantization", "text": ["Wrapper that allows creation of class factories.", "This can be useful when there is a need to create classes with the same constructor arguments, but different instances. Can be used in conjunction with _callable_args", "Example:"]}, {"name": "torch.ao.quantization.observer.ObserverBase.with_callable_args()", "path": "generated/torch.ao.quantization.observer.observerbase#torch.ao.quantization.observer.ObserverBase.with_callable_args", "type": "Quantization", "text": ["Wrapper that allows creation of class factories args that need to be called at construction time.", "This can be useful when there is a need to create classes with the same constructor arguments, but different instances and those arguments should only be calculated at construction time. Can be used in conjunction with _with_args", "Example:"]}, {"name": "torch.ao.quantization.observer.PerChannelMinMaxObserver", "path": "generated/torch.ao.quantization.observer.perchannelminmaxobserver#torch.ao.quantization.observer.PerChannelMinMaxObserver", "type": "Quantization", "text": ["Observer module for computing the quantization parameters based on the running per channel min and max values.", "This observer uses the tensor min/max statistics to compute the per channel quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.", "The quantization parameters are computed the same way as in MinMaxObserver, with the difference that the running min/max values are stored per channel. Scales and zero points are thus computed per channel as well.", "Note", "If the running minimum equals to the running maximum, the scales and zero_points are set to 1.0 and 0.", "Resets the min/max values."]}, {"name": "torch.ao.quantization.observer.PerChannelMinMaxObserver.reset_min_max_vals()", "path": "generated/torch.ao.quantization.observer.perchannelminmaxobserver#torch.ao.quantization.observer.PerChannelMinMaxObserver.reset_min_max_vals", "type": "Quantization", "text": ["Resets the min/max values."]}, {"name": "torch.ao.quantization.observer.PlaceholderObserver", "path": "generated/torch.ao.quantization.observer.placeholderobserver#torch.ao.quantization.observer.PlaceholderObserver", "type": "Quantization", "text": ["Observer that doesn\u2019t do anything and just passes its configuration to the quantized module\u2019s .from_float().", "Can be used for quantization to float16 which doesn\u2019t require determining ranges."]}, {"name": "torch.ao.quantization.observer.RecordingObserver", "path": "generated/torch.ao.quantization.observer.recordingobserver#torch.ao.quantization.observer.RecordingObserver", "type": "Quantization", "text": ["The module is mainly for debug and records the tensor values during runtime."]}, {"name": "torch.ao.quantization.prepare", "path": "generated/torch.ao.quantization.prepare#torch.ao.quantization.prepare", "type": "Quantization", "text": ["Prepares a copy of the model for quantization calibration or quantization-aware training.", "Quantization configuration should be assigned preemptively to individual submodules in .qconfig attribute.", "The model will be attached with observer or fake quant modules, and qconfig will be propagated."]}, {"name": "torch.ao.quantization.prepare_qat", "path": "generated/torch.ao.quantization.prepare_qat#torch.ao.quantization.prepare_qat", "type": "Quantization", "text": ["Prepares a copy of the model for quantization calibration or quantization-aware training and converts it to quantized version.", "Quantization configuration should be assigned preemptively to individual submodules in .qconfig attribute."]}, {"name": "torch.ao.quantization.propagate_qconfig_", "path": "generated/torch.ao.quantization.propagate_qconfig_#torch.ao.quantization.propagate_qconfig_", "type": "Quantization", "text": ["Propagate qconfig through the module hierarchy and assign qconfig attribute on each leaf module", "None, module is modified inplace with qconfig attached"]}, {"name": "torch.ao.quantization.qconfig.default_activation_only_qconfig", "path": "generated/torch.ao.quantization.qconfig.default_activation_only_qconfig#torch.ao.quantization.qconfig.default_activation_only_qconfig", "type": "Quantization", "text": ["Default qconfig for quantizing activations only."]}, {"name": "torch.ao.quantization.qconfig.default_debug_qconfig", "path": "generated/torch.ao.quantization.qconfig.default_debug_qconfig#torch.ao.quantization.qconfig.default_debug_qconfig", "type": "Quantization", "text": ["Default qconfig configuration for debugging."]}, {"name": "torch.ao.quantization.qconfig.default_dynamic_qconfig", "path": "generated/torch.ao.quantization.qconfig.default_dynamic_qconfig#torch.ao.quantization.qconfig.default_dynamic_qconfig", "type": "Quantization", "text": ["Default dynamic qconfig."]}, {"name": "torch.ao.quantization.qconfig.default_per_channel_qconfig", "path": "generated/torch.ao.quantization.qconfig.default_per_channel_qconfig#torch.ao.quantization.qconfig.default_per_channel_qconfig", "type": "Quantization", "text": ["Default qconfig configuration for per channel weight quantization."]}, {"name": "torch.ao.quantization.qconfig.default_qat_qconfig", "path": "generated/torch.ao.quantization.qconfig.default_qat_qconfig#torch.ao.quantization.qconfig.default_qat_qconfig", "type": "Quantization", "text": ["Default qconfig for QAT."]}, {"name": "torch.ao.quantization.qconfig.default_qat_qconfig_v2", "path": "generated/torch.ao.quantization.qconfig.default_qat_qconfig_v2#torch.ao.quantization.qconfig.default_qat_qconfig_v2", "type": "Quantization", "text": ["Fused version of default_qat_config, has performance benefits."]}, {"name": "torch.ao.quantization.qconfig.default_qconfig", "path": "generated/torch.ao.quantization.qconfig.default_qconfig#torch.ao.quantization.qconfig.default_qconfig", "type": "Quantization", "text": ["Default qconfig configuration."]}, {"name": "torch.ao.quantization.qconfig.default_weight_only_qconfig", "path": "generated/torch.ao.quantization.qconfig.default_weight_only_qconfig#torch.ao.quantization.qconfig.default_weight_only_qconfig", "type": "Quantization", "text": ["Default qconfig for quantizing weights only."]}, {"name": "torch.ao.quantization.qconfig.float16_dynamic_qconfig", "path": "generated/torch.ao.quantization.qconfig.float16_dynamic_qconfig#torch.ao.quantization.qconfig.float16_dynamic_qconfig", "type": "Quantization", "text": ["Dynamic qconfig with weights quantized to torch.float16."]}, {"name": "torch.ao.quantization.qconfig.float16_static_qconfig", "path": "generated/torch.ao.quantization.qconfig.float16_static_qconfig#torch.ao.quantization.qconfig.float16_static_qconfig", "type": "Quantization", "text": ["Dynamic qconfig with both activations and weights quantized to torch.float16."]}, {"name": "torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig", "path": "generated/torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig#torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig", "type": "Quantization", "text": ["Dynamic qconfig with weights quantized with a floating point zero_point."]}, {"name": "torch.ao.quantization.qconfig.per_channel_dynamic_qconfig", "path": "generated/torch.ao.quantization.qconfig.per_channel_dynamic_qconfig#torch.ao.quantization.qconfig.per_channel_dynamic_qconfig", "type": "Quantization", "text": ["Dynamic qconfig with weights quantized per channel."]}, {"name": "torch.ao.quantization.qconfig.QConfig", "path": "generated/torch.ao.quantization.qconfig.qconfig#torch.ao.quantization.qconfig.QConfig", "type": "Quantization", "text": ["Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.", "Note that QConfig needs to contain observer classes (like MinMaxObserver) or a callable that returns instances on invocation, not the concrete observer instances themselves. Quantization preparation function will instantiate observers multiple times for each of the layers.", "Observer classes have usually reasonable default arguments, but they can be overwritten with with_args method (that behaves like functools.partial):"]}, {"name": "torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping", "path": "generated/torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping#torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping", "type": "Quantization", "text": ["Return the default QConfigMapping for quantization aware training.", "QConfigMapping"]}, {"name": "torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping", "path": "generated/torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping#torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping", "type": "Quantization", "text": ["Return the default QConfigMapping for post training quantization.", "QConfigMapping"]}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping", "type": "Quantization", "text": ["Mapping from model ops to torch.ao.quantization.QConfig s.", "The user can specify QConfigs using the following methods (in increasing match priority):", "set_global : sets the global (default) QConfig", "set_object_type : sets the QConfig for a given module type, function, or method name", "set_module_name_regex : sets the QConfig for modules matching the given regex string", "set_module_name : sets the QConfig for modules matching the given module name", "set_module_name_object_type_order : sets the QConfig for modules matching a combination of the given module name, object type, and the index at which the module appears", "Example usage:", "Create a QConfigMapping from a dictionary with the following keys (all optional):", "\u201c\u201d (for global QConfig)", "\u201cobject_type\u201d", "\u201cmodule_name_regex\u201d", "\u201cmodule_name\u201d", "\u201cmodule_name_object_type_order\u201d", "The values of this dictionary are expected to be lists of tuples.", "QConfigMapping", "Set the global (default) QConfig.", "QConfigMapping", "Set the QConfig for modules matching the given module name. If the QConfig for an existing module name was already set, the new QConfig will override the old one.", "QConfigMapping", "Set the QConfig for modules matching a combination of the given module name, object type, and the index at which the module appears.", "If the QConfig for an existing (module name, object type, index) was already set, the new QConfig will override the old one.", "QConfigMapping", "Set the QConfig for modules matching the given regex string.", "Regexes will be matched in the order in which they are registered through this method. Thus, the caller should register more specific patterns first, e.g.:", "In this example, \u201cfoo.bar.conv0\u201d would match qconfig1, \u201cfoo.bar.linear\u201d would match qconfig2, and \u201cfoo.baz.relu\u201d would match qconfig3.", "If the QConfig for an existing module name regex was already set, the new QConfig will override the old one while preserving the order in which the regexes were originally registered.", "QConfigMapping", "Set the QConfig for a given module type, function, or method name. If the QConfig for an existing object type was already set, the new QConfig will override the old one.", "QConfigMapping", "Convert this QConfigMapping to a dictionary with the following keys:", "\u201c\u201d (for global QConfig)", "\u201cobject_type\u201d", "\u201cmodule_name_regex\u201d", "\u201cmodule_name\u201d", "\u201cmodule_name_object_type_order\u201d", "The values of this dictionary are lists of tuples.", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.from_dict()", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping.from_dict", "type": "Quantization", "text": ["Create a QConfigMapping from a dictionary with the following keys (all optional):", "\u201c\u201d (for global QConfig)", "\u201cobject_type\u201d", "\u201cmodule_name_regex\u201d", "\u201cmodule_name\u201d", "\u201cmodule_name_object_type_order\u201d", "The values of this dictionary are expected to be lists of tuples.", "QConfigMapping"]}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.set_global()", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_global", "type": "Quantization", "text": ["Set the global (default) QConfig.", "QConfigMapping"]}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name()", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name", "type": "Quantization", "text": ["Set the QConfig for modules matching the given module name. If the QConfig for an existing module name was already set, the new QConfig will override the old one.", "QConfigMapping"]}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name_object_type_order()", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name_object_type_order", "type": "Quantization", "text": ["Set the QConfig for modules matching a combination of the given module name, object type, and the index at which the module appears.", "If the QConfig for an existing (module name, object type, index) was already set, the new QConfig will override the old one.", "QConfigMapping"]}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name_regex()", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name_regex", "type": "Quantization", "text": ["Set the QConfig for modules matching the given regex string.", "Regexes will be matched in the order in which they are registered through this method. Thus, the caller should register more specific patterns first, e.g.:", "In this example, \u201cfoo.bar.conv0\u201d would match qconfig1, \u201cfoo.bar.linear\u201d would match qconfig2, and \u201cfoo.baz.relu\u201d would match qconfig3.", "If the QConfig for an existing module name regex was already set, the new QConfig will override the old one while preserving the order in which the regexes were originally registered.", "QConfigMapping"]}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.set_object_type()", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_object_type", "type": "Quantization", "text": ["Set the QConfig for a given module type, function, or method name. If the QConfig for an existing object type was already set, the new QConfig will override the old one.", "QConfigMapping"]}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.to_dict()", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping.to_dict", "type": "Quantization", "text": ["Convert this QConfigMapping to a dictionary with the following keys:", "\u201c\u201d (for global QConfig)", "\u201cobject_type\u201d", "\u201cmodule_name_regex\u201d", "\u201cmodule_name\u201d", "\u201cmodule_name_object_type_order\u201d", "The values of this dictionary are lists of tuples.", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.quantize", "path": "generated/torch.ao.quantization.quantize#torch.ao.quantization.quantize", "type": "Quantization", "text": ["Quantize the input float model with post training static quantization.", "First it will prepare the model for calibration, then it calls run_fn which will run the calibration step, after that we will convert the model to a quantized model.", "Quantized model."]}, {"name": "torch.ao.quantization.quantize_dynamic", "path": "generated/torch.ao.quantization.quantize_dynamic#torch.ao.quantization.quantize_dynamic", "type": "Quantization", "text": ["Converts a float model to dynamic (i.e. weights-only) quantized model.", "Replaces specified modules with dynamic weight-only quantized versions and output the quantized model.", "For simplest usage provide dtype argument that can be float16 or qint8. Weight-only quantization by default is performed for layers with large weights size - i.e. Linear and RNN variants.", "Fine grained control is possible with qconfig and mapping that act similarly to quantize(). If qconfig is provided, the dtype argument is ignored.", "qconfig_spec \u2013 ", "Either:"]}, {"name": "torch.ao.quantization.quantize_fx.convert_fx", "path": "generated/torch.ao.quantization.quantize_fx.convert_fx#torch.ao.quantization.quantize_fx.convert_fx", "type": "Quantization", "text": ["Convert a calibrated or trained model to a quantized model", "qconfig_mapping (*) \u2013 ", "config for specifying how to convert a model for quantization.", "The keys must include the ones in the qconfig_mapping passed to prepare_fx or prepare_qat_fx, with the same values or None. Additional keys can be specified with values set to None.", "For each entry whose value is set to None, we skip quantizing that entry in the model:", "operators should be quantized in the backend, this includes quantization mode support (static/dynamic/weight_only), dtype support (quint8/qint8 etc.), observer placement for each operators and fused operators. See BackendConfig for more details", "A quantized model (torch.nn.Module)", "GraphModule", "Example:"]}, {"name": "torch.ao.quantization.quantize_fx.fuse_fx", "path": "generated/torch.ao.quantization.quantize_fx.fuse_fx#torch.ao.quantization.quantize_fx.fuse_fx", "type": "Quantization", "text": ["Fuse modules like conv+bn, conv+bn+relu etc, model must be in eval mode. Fusion rules are defined in torch.ao.quantization.fx.fusion_pattern.py", "GraphModule", "Example:"]}, {"name": "torch.ao.quantization.quantize_fx.prepare_fx", "path": "generated/torch.ao.quantization.quantize_fx.prepare_fx#torch.ao.quantization.quantize_fx.prepare_fx", "type": "Quantization", "text": ["Prepare a model for post training static quantization", "A GraphModule with observer (configured by qconfig_mapping), ready for calibration", "GraphModule", "Example:"]}, {"name": "torch.ao.quantization.quantize_fx.prepare_qat_fx", "path": "generated/torch.ao.quantization.quantize_fx.prepare_qat_fx#torch.ao.quantization.quantize_fx.prepare_qat_fx", "type": "Quantization", "text": ["Prepare a model for quantization aware training", "A GraphModule with fake quant modules (configured by qconfig_mapping and backend_config), ready for quantization aware training", "GraphModule", "Example:"]}, {"name": "torch.ao.quantization.quantize_qat", "path": "generated/torch.ao.quantization.quantize_qat#torch.ao.quantization.quantize_qat", "type": "Quantization", "text": ["Do quantization aware training and output a quantized model", "Quantized model."]}, {"name": "torch.ao.quantization.QuantStub", "path": "generated/torch.ao.quantization.quantstub#torch.ao.quantization.QuantStub", "type": "Quantization", "text": ["Quantize stub module, before calibration, this is same as an observer, it will be swapped as nnq.Quantize in convert.", "qconfig \u2013 quantization configuration for the tensor, if qconfig is not provided, we will get qconfig from parent modules"]}, {"name": "torch.ao.quantization.QuantWrapper", "path": "generated/torch.ao.quantization.quantwrapper#torch.ao.quantization.QuantWrapper", "type": "Quantization", "text": ["A wrapper class that wraps the input module, adds QuantStub and DeQuantStub and surround the call to module with call to quant and dequant modules.", "This is used by the quantization utility functions to add the quant and dequant modules, before convert function QuantStub will just be observer, it observes the input tensor, after convert, QuantStub will be swapped to nnq.Quantize which does actual quantization. Similarly for DeQuantStub."]}, {"name": "torch.ao.quantization.swap_module", "path": "generated/torch.ao.quantization.swap_module#torch.ao.quantization.swap_module", "type": "Quantization", "text": ["Swaps the module if it has a quantized counterpart and it has an observer attached.", "The corresponding quantized module of mod"]}, {"name": "torch.arange", "path": "generated/torch.arange", "type": "Torch", "text": ["Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil with values from the interval [start, end) taken with common difference step beginning from start.", "Note that non-integer step is subject to floating point rounding errors when comparing against end; to avoid inconsistency, we advise subtracting a small epsilon from end in such cases.", "Example:"]}, {"name": "torch.arccos", "path": "generated/torch.arccos", "type": "Torch", "text": ["Alias for torch.acos()."]}, {"name": "torch.arccosh", "path": "generated/torch.arccosh", "type": "Torch", "text": ["Alias for torch.acosh()."]}, {"name": "torch.arcsin", "path": "generated/torch.arcsin", "type": "Torch", "text": ["Alias for torch.asin()."]}, {"name": "torch.arcsinh", "path": "generated/torch.arcsinh", "type": "Torch", "text": ["Alias for torch.asinh()."]}, {"name": "torch.arctan", "path": "generated/torch.arctan", "type": "Torch", "text": ["Alias for torch.atan()."]}, {"name": "torch.arctan2", "path": "generated/torch.arctan2", "type": "Torch", "text": ["Alias for torch.atan2()."]}, {"name": "torch.arctanh", "path": "generated/torch.arctanh", "type": "Torch", "text": ["Alias for torch.atanh()."]}, {"name": "torch.are_deterministic_algorithms_enabled", "path": "generated/torch.are_deterministic_algorithms_enabled", "type": "Torch", "text": ["Returns True if the global deterministic flag is turned on. Refer to torch.use_deterministic_algorithms() documentation for more details.", "bool"]}, {"name": "torch.argmax", "path": "generated/torch.argmax", "type": "Torch", "text": ["Returns the indices of the maximum value of all elements in the input tensor.", "This is the second value returned by torch.max(). See its documentation for the exact semantics of this method.", "Note", "If there are multiple maximal values then the indices of the first maximal value are returned.", "input (Tensor) \u2013 the input tensor.", "Example:", "Returns the indices of the maximum values of a tensor across a dimension.", "This is the second value returned by torch.max(). See its documentation for the exact semantics of this method.", "Example:"]}, {"name": "torch.argmin", "path": "generated/torch.argmin", "type": "Torch", "text": ["Returns the indices of the minimum value(s) of the flattened tensor or along a dimension", "This is the second value returned by torch.min(). See its documentation for the exact semantics of this method.", "Note", "If there are multiple minimal values then the indices of the first minimal value are returned.", "Example:"]}, {"name": "torch.argsort", "path": "generated/torch.argsort", "type": "Torch", "text": ["Returns the indices that sort a tensor along a given dimension in ascending order by value.", "This is the second value returned by torch.sort(). See its documentation for the exact semantics of this method.", "If stable is True then the sorting routine becomes stable, preserving the order of equivalent elements. If False, the relative order of values which compare equal is not guaranteed. True is slower.", "Example:"]}, {"name": "torch.argwhere", "path": "generated/torch.argwhere", "type": "Torch", "text": ["Returns a tensor containing the indices of all non-zero elements of input. Each row in the result contains the indices of a non-zero element in input. The result is sorted lexicographically, with the last index changing the fastest (C-style).", "If input has nn dimensions, then the resulting indices tensor out is of size (z\u00d7n)(z \\times n), where zz is the total number of non-zero elements in the input tensor.", "Note", "This function is similar to NumPy\u2019s argwhere.", "When input is on CUDA, this function causes host-device synchronization.", "{input} \u2013 ", "Example:"]}, {"name": "torch.as_strided", "path": "generated/torch.as_strided", "type": "Torch", "text": ["Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.", "Warning", "Prefer using other view functions, like torch.Tensor.expand(), to setting a view\u2019s strides manually with as_strided, as this function\u2019s behavior depends on the implementation of a tensor\u2019s storage. The constructed view of the storage must only refer to elements within the storage or a runtime error will be thrown, and if the view is \u201coverlapped\u201d (with multiple indices referring to the same element in memory) its behavior is undefined.", "Example:"]}, {"name": "torch.as_tensor", "path": "generated/torch.as_tensor", "type": "Torch", "text": ["Converts data into a tensor, sharing data and preserving autograd history if possible.", "If data is already a tensor with the requested dtype and device then data itself is returned, but if data is a tensor with a different dtype or device then it\u2019s copied as if using data.to(dtype=dtype, device=device).", "If data is a NumPy array (an ndarray) with the same dtype and device then a tensor is constructed using torch.from_numpy().", "See also", "torch.tensor() never shares its data and creates a new \u201cleaf tensor\u201d (see Autograd mechanics).", "Example:"]}, {"name": "torch.asarray", "path": "generated/torch.asarray", "type": "Torch", "text": ["Converts obj to a tensor.", "obj can be one of:", "When obj is a tensor, NumPy array, or DLPack capsule the returned tensor will, by default, not require a gradient, have the same datatype as obj, be on the same device, and share memory with it. These properties can be controlled with the dtype, device, copy, and requires_grad keyword arguments. If the returned tensor is of a different datatype, on a different device, or a copy is requested then it will not share its memory with obj. If requires_grad is True then the returned tensor will require a gradient, and if obj is also a tensor with an autograd history then the returned tensor will have the same history.", "When obj is not a tensor, NumPy array, or DLPack capsule but implements Python\u2019s buffer protocol then the buffer is interpreted as an array of bytes grouped according to the size of the datatype passed to the dtype keyword argument. (If no datatype is passed then the default floating point datatype is used, instead.) The returned tensor will have the specified datatype (or default floating point datatype if none is specified) and, by default, be on the CPU device and share memory with the buffer.", "When obj is a NumPy scalar, the returned tensor will be a 0-dimensional tensor on the CPU and that doesn\u2019t share its memory (i.e. copy=True). By default datatype will be the PyTorch datatype corresponding to the NumPy\u2019s scalar\u2019s datatype.", "When obj is none of the above but a scalar, or a sequence of scalars then the returned tensor will, by default, infer its datatype from the scalar values, be on the current default device, and not share its memory.", "See also", "torch.tensor() creates a tensor that always copies the data from the input object. torch.from_numpy() creates a tensor that always shares memory from NumPy arrays. torch.frombuffer() creates a tensor that always shares memory from objects that implement the buffer protocol. torch.from_dlpack() creates a tensor that always shares memory from DLPack capsules.", "obj (object) \u2013 a tensor, NumPy array, DLPack Capsule, object that implements Python\u2019s buffer protocol, scalar, or sequence of scalars.", "Example:"]}, {"name": "torch.asin", "path": "generated/torch.asin", "type": "Torch", "text": ["Returns a new tensor with the arcsine of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.asinh", "path": "generated/torch.asinh", "type": "Torch", "text": ["Returns a new tensor with the inverse hyperbolic sine of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.atan", "path": "generated/torch.atan", "type": "Torch", "text": ["Returns a new tensor with the arctangent of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.atan2", "path": "generated/torch.atan2", "type": "Torch", "text": ["Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i} with consideration of the quadrant. Returns a new tensor with the signed angles in radians between vector (otheri,inputi)(\\text{other}_{i}, \\text{input}_{i}) and vector (1,0)(1, 0). (Note that otheri\\text{other}_{i}, the second parameter, is the x-coordinate, while inputi\\text{input}_{i}, the first parameter, is the y-coordinate.)", "The shapes of input and other must be broadcastable.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.atanh", "path": "generated/torch.atanh", "type": "Torch", "text": ["Returns a new tensor with the inverse hyperbolic tangent of the elements of input.", "Note", "The domain of the inverse hyperbolic tangent is (-1, 1) and values outside this range will be mapped to NaN, except for the values 1 and -1 for which the output is mapped to +/-INF respectively.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.atleast_1d", "path": "generated/torch.atleast_1d", "type": "Torch", "text": ["Returns a 1-dimensional view of each input tensor with zero dimensions. Input tensors with one or more dimensions are returned as-is.", "input (Tensor or list of Tensors) \u2013 ", "output (Tensor or tuple of Tensors)", "Example:"]}, {"name": "torch.atleast_2d", "path": "generated/torch.atleast_2d", "type": "Torch", "text": ["Returns a 2-dimensional view of each input tensor with zero dimensions. Input tensors with two or more dimensions are returned as-is.", "input (Tensor or list of Tensors) \u2013 ", "output (Tensor or tuple of Tensors)", "Example:"]}, {"name": "torch.atleast_3d", "path": "generated/torch.atleast_3d", "type": "Torch", "text": ["Returns a 3-dimensional view of each input tensor with zero dimensions. Input tensors with three or more dimensions are returned as-is.", "input (Tensor or list of Tensors) \u2013 ", "output (Tensor or tuple of Tensors)"]}, {"name": "torch.autocast", "path": "amp#torch.autocast", "type": "Automatic Mixed Precision", "text": ["Instances of autocast serve as context managers or decorators that allow regions of your script to run in mixed precision.", "In these regions, ops run in an op-specific dtype chosen by autocast to improve performance while maintaining accuracy. See the Autocast Op Reference for details.", "When entering an autocast-enabled region, Tensors may be any type. You should not call half() or bfloat16() on your model(s) or inputs when using autocasting.", "autocast should wrap only the forward pass(es) of your network, including the loss computation(s). Backward passes under autocast are not recommended. Backward ops run in the same type that autocast used for corresponding forward ops.", "Example for CUDA Devices:", "See the CUDA Automatic Mixed Precision examples for usage (along with gradient scaling) in more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).", "autocast can also be used as a decorator, e.g., on the forward method of your model:", "Floating-point Tensors produced in an autocast-enabled region may be float16. After returning to an autocast-disabled region, using them with floating-point Tensors of different dtypes may cause type mismatch errors. If so, cast the Tensor(s) produced in the autocast region back to float32 (or other dtype if desired). If a Tensor from the autocast region is already float32, the cast is a no-op, and incurs no additional overhead. CUDA Example:", "CPU Training Example:", "CPU Inference Example:", "CPU Inference Example with Jit Trace:", "Type mismatch errors in an autocast-enabled region are a bug; if this is what you observe, please file an issue.", "autocast(enabled=False) subregions can be nested in autocast-enabled regions. Locally disabling autocast can be useful, for example, if you want to force a subregion to run in a particular dtype. Disabling autocast gives you explicit control over the execution type. In the subregion, inputs from the surrounding region should be cast to dtype before use:", "The autocast state is thread-local. If you want it enabled in a new thread, the context manager or decorator must be invoked in that thread. This affects torch.nn.DataParallel and torch.nn.parallel.DistributedDataParallel when used with more than one GPU per process (see Working with Multiple GPUs)."]}, {"name": "torch.autograd.backward()", "path": "generated/torch.autograd.backward#torch.autograd.backward", "type": "Automatic Differentiation", "text": ["Computes the sum of gradients of given tensors with respect to graph leaves.", "The graph is differentiated using the chain rule. If any of tensors are non-scalar (i.e. their data has more than one element) and require gradient, then the Jacobian-vector product would be computed, in this case the function additionally requires specifying grad_tensors. It should be a sequence of matching length, that contains the \u201cvector\u201d in the Jacobian-vector product, usually the gradient of the differentiated function w.r.t. corresponding tensors (None is an acceptable value for all tensors that don\u2019t need gradient tensors).", "This function accumulates gradients in the leaves - you might need to zero .grad attributes or set them to None before calling it. See Default gradient layouts for details on the memory layout of accumulated gradients.", "Note", "Using this method with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak.", "Note", "If you run any forward ops, create grad_tensors, and/or call backward in a user-specified CUDA stream context, see Stream semantics of backward passes.", "Note", "When inputs are provided and a given input is not a leaf, the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients). It is an implementation detail on which the user should not rely. See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details."]}, {"name": "torch.autograd.detect_anomaly", "path": "autograd#torch.autograd.detect_anomaly", "type": "Automatic Differentiation", "text": ["Context-manager that enable anomaly detection for the autograd engine.", "This does two things:", "Warning", "This mode should be enabled only for debugging as the different tests will slow down your program execution."]}, {"name": "torch.autograd.forward_ad.dual_level", "path": "generated/torch.autograd.forward_ad.dual_level#torch.autograd.forward_ad.dual_level", "type": "Automatic Differentiation", "text": ["Context-manager that enables forward AD. All forward AD computation must be performed in a dual_level context.", "Note", "The dual_level context appropriately enters and exit the dual level to controls the current forward AD level, which is used by default by the other functions in this API.", "We currently don\u2019t plan to support nested dual_level contexts, however, so only a single forward AD level is supported. To compute higher-order forward grads, one can use torch.func.jvp().", "Example:", "Please see the forward-mode AD tutorial for detailed steps on how to use this API."]}, {"name": "torch.autograd.forward_ad.make_dual()", "path": "generated/torch.autograd.forward_ad.make_dual#torch.autograd.forward_ad.make_dual", "type": "Automatic Differentiation", "text": ["Associates a tensor value with a forward gradient, the tangent, to create a \u201cdual tensor\u201d, which is used to compute forward AD gradients. The result is a new tensor aliased to tensor with tangent embedded as an attribute as-is if it has the same storage layout or copied otherwise. The tangent attribute can be recovered with unpack_dual().", "This function is backward differentiable.", "Given a function f whose jacobian is J, it allows one to compute the Jacobian-vector product (jvp) between J and a given vector v as follows.", "Example:", "Please see the forward-mode AD tutorial for detailed steps on how to use this API."]}, {"name": "torch.autograd.forward_ad.unpack_dual()", "path": "generated/torch.autograd.forward_ad.unpack_dual#torch.autograd.forward_ad.unpack_dual", "type": "Automatic Differentiation", "text": ["Unpacks a \u201cdual tensor\u201d to get both its Tensor value and its forward AD gradient. The result is a namedtuple (primal, tangent) where primal is a view of tensor\u2019s primal and tangent is tensor\u2019s tangent as-is. Neither of these tensors can be dual tensor of level level.", "This function is backward differentiable.", "Example:", "Please see the forward-mode AD tutorial for detailed steps on how to use this API."]}, {"name": "torch.autograd.Function", "path": "autograd#torch.autograd.Function", "type": "Automatic Differentiation", "text": ["Base class to create custom autograd.Function", "To create a custom autograd.Function, subclass this class and implement the forward() and backward() static methods. Then, to use your custom op in the forward pass, call the class method apply. Do not call forward() directly.", "To ensure correctness and best performance, make sure you are calling the correct methods on ctx and validating your backward function using torch.autograd.gradcheck().", "See Extending torch.autograd for more details on how to use this class.", "Examples:"]}, {"name": "torch.autograd.Function.backward()", "path": "generated/torch.autograd.function.backward#torch.autograd.Function.backward", "type": "Automatic Differentiation", "text": ["Defines a formula for differentiating the operation with backward mode automatic differentiation (alias to the vjp function).", "This function is to be overridden by all subclasses.", "It must accept a context ctx as the first argument, followed by as many outputs as the forward() returned (None will be passed in for non tensor outputs of the forward function), and it should return as many tensors, as there were inputs to forward(). Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not requiring grads, you can just pass None as a gradient for that input.", "The context can be used to retrieve tensors saved during the forward pass. It also has an attribute ctx.needs_input_grad as a tuple of booleans representing whether each input needs gradient. E.g., backward() will have ctx.needs_input_grad[0] = True if the first input to forward() needs gradient computed w.r.t. the output.", "Any"]}, {"name": "torch.autograd.Function.forward()", "path": "generated/torch.autograd.function.forward#torch.autograd.Function.forward", "type": "Automatic Differentiation", "text": ["This function is to be overridden by all subclasses. There are two ways to define forward:", "Usage 1 (Combined forward and ctx):", "Usage 2 (Separate forward and ctx):", "The context can be used to store arbitrary data that can be then retrieved during the backward pass. Tensors should not be stored directly on ctx (though this is not currently enforced for backward compatibility). Instead, tensors should be saved either with ctx.save_for_backward() if they are intended to be used in backward (equivalently, vjp) or ctx.save_for_forward() if they are intended to be used for in jvp.", "Any"]}, {"name": "torch.autograd.function.FunctionCtx.mark_dirty()", "path": "generated/torch.autograd.function.functionctx.mark_dirty#torch.autograd.function.FunctionCtx.mark_dirty", "type": "Automatic Differentiation", "text": ["Marks given tensors as modified in an in-place operation.", "This should be called at most once, only from inside the forward() method, and all arguments should be inputs.", "Every tensor that\u2019s been modified in-place in a call to forward() should be given to this function, to ensure correctness of our checks. It doesn\u2019t matter whether the function is called before or after modification."]}, {"name": "torch.autograd.function.FunctionCtx.mark_non_differentiable()", "path": "generated/torch.autograd.function.functionctx.mark_non_differentiable#torch.autograd.function.FunctionCtx.mark_non_differentiable", "type": "Automatic Differentiation", "text": ["Marks outputs as non-differentiable.", "This should be called at most once, only from inside the forward() method, and all arguments should be tensor outputs.", "This will mark outputs as not requiring gradients, increasing the efficiency of backward computation. You still need to accept a gradient for each output in backward(), but it\u2019s always going to be a zero tensor with the same shape as the shape of a corresponding output."]}, {"name": "torch.autograd.function.FunctionCtx.save_for_backward()", "path": "generated/torch.autograd.function.functionctx.save_for_backward#torch.autograd.function.FunctionCtx.save_for_backward", "type": "Automatic Differentiation", "text": ["Saves given tensors for a future call to backward().", "save_for_backward should be called at most once, only from inside the forward() method, and only with tensors.", "All tensors intended to be used in the backward pass should be saved with save_for_backward (as opposed to directly on ctx) to prevent incorrect gradients and memory leaks, and enable the application of saved tensor hooks. See torch.autograd.graph.saved_tensors_hooks.", "Note that if intermediary tensors, tensors that are neither inputs nor outputs of forward(), are saved for backward, your custom Function may not support double backward. Custom Functions that do not support double backward should decorate their backward() method with @once_differentiable so that performing double backward raises an error. If you\u2019d like to support double backward, you can either recompute intermediaries based on the inputs during backward or return the intermediaries as the outputs of the custom Function. See the double backward tutorial for more details.", "In backward(), saved tensors can be accessed through the saved_tensors attribute. Before returning them to the user, a check is made to ensure they weren\u2019t used in any in-place operation that modified their content.", "Arguments can also be None. This is a no-op.", "See Extending torch.autograd for more details on how to use this method."]}, {"name": "torch.autograd.function.FunctionCtx.set_materialize_grads()", "path": "generated/torch.autograd.function.functionctx.set_materialize_grads#torch.autograd.function.FunctionCtx.set_materialize_grads", "type": "Automatic Differentiation", "text": ["Sets whether to materialize grad tensors. Default is True.", "This should be called only from inside the forward() method", "If True, undefined grad tensors will be expanded to tensors full of zeros prior to calling the backward() and jvp() methods."]}, {"name": "torch.autograd.Function.jvp()", "path": "generated/torch.autograd.function.jvp#torch.autograd.Function.jvp", "type": "Automatic Differentiation", "text": ["Defines a formula for differentiating the operation with forward mode automatic differentiation. This function is to be overridden by all subclasses. It must accept a context ctx as the first argument, followed by as many inputs as the forward() got (None will be passed in for non tensor inputs of the forward function), and it should return as many tensors as there were outputs to forward(). Each argument is the gradient w.r.t the given input, and each returned value should be the gradient w.r.t. the corresponding output. If an output is not a Tensor or the function is not differentiable with respect to that output, you can just pass None as a gradient for that input.", "You can use the ctx object to pass any value from the forward to this functions.", "Any"]}, {"name": "torch.autograd.Function.vmap()", "path": "generated/torch.autograd.function.vmap#torch.autograd.Function.vmap", "type": "Automatic Differentiation", "text": ["Defines a rule for the behavior of this autograd.Function underneath torch.vmap(). For a torch.autograd.Function() to support torch.vmap(), you must either override this staticmethod, or set generate_vmap_rule to True (you may not do both).", "If you choose to override this staticmethod: it must accept", "The return of the vmap staticmethod is a tuple of (output, out_dims). Similar to in_dims, out_dims should be of the same structure as output and contain one out_dim per output that specifies if the output has the vmapped dimension and what index it is in.", "Please see Extending torch.func with autograd.Function for more details."]}, {"name": "torch.autograd.functional.hessian()", "path": "generated/torch.autograd.functional.hessian#torch.autograd.functional.hessian", "type": "Automatic Differentiation", "text": ["Function that computes the Hessian of a given scalar function.", "if there is a single input, this will be a single Tensor containing the Hessian for the input. If it is a tuple, then the Hessian will be a tuple of tuples where Hessian[i][j] will contain the Hessian of the ith input and jth input with size the sum of the size of the ith input plus the size of the jth input. Hessian[i][j] will have the same dtype and device as the corresponding ith input.", "Hessian (Tensor or a tuple of tuple of Tensors)"]}, {"name": "torch.autograd.functional.hvp()", "path": "generated/torch.autograd.functional.hvp#torch.autograd.functional.hvp", "type": "Automatic Differentiation", "text": ["Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.", "func_output (tuple of Tensors or Tensor): output of func(inputs)", "hvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.", "output (tuple)", "Note", "This function is significantly slower than vhp due to backward mode AD constraints. If your functions is twice continuously differentiable, then hvp = vhp.t(). So if you know that your function satisfies this condition, you should use vhp instead that is much faster with the current implementation."]}, {"name": "torch.autograd.functional.jacobian()", "path": "generated/torch.autograd.functional.jacobian#torch.autograd.functional.jacobian", "type": "Automatic Differentiation", "text": ["Function that computes the Jacobian of a given function.", "if there is a single input and output, this will be a single Tensor containing the Jacobian for the linearized inputs and output. If one of the two is a tuple, then the Jacobian will be a tuple of Tensors. If both of them are tuples, then the Jacobian will be a tuple of tuple of Tensors where Jacobian[i][j] will contain the Jacobian of the ith output and jth input and will have as size the concatenation of the sizes of the corresponding output and the corresponding input and will have same dtype and device as the corresponding input. If strategy is forward-mode, the dtype will be that of the output; otherwise, the input.", "Jacobian (Tensor or nested tuple of Tensors)"]}, {"name": "torch.autograd.functional.jvp()", "path": "generated/torch.autograd.functional.jvp#torch.autograd.functional.jvp", "type": "Automatic Differentiation", "text": ["Function that computes the dot product between the Jacobian of the given function at the point given by the inputs and a vector v.", "func_output (tuple of Tensors or Tensor): output of func(inputs)", "jvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the output.", "output (tuple)", "Note", "autograd.functional.jvp computes the jvp by using the backward of the backward (sometimes called the double backwards trick). This is not the most performant way of computing the jvp. Please consider using torch.func.jvp() or the low-level forward-mode AD API instead."]}, {"name": "torch.autograd.functional.vhp()", "path": "generated/torch.autograd.functional.vhp#torch.autograd.functional.vhp", "type": "Automatic Differentiation", "text": ["Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs.", "func_output (tuple of Tensors or Tensor): output of func(inputs)", "vhp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.", "output (tuple)"]}, {"name": "torch.autograd.functional.vjp()", "path": "generated/torch.autograd.functional.vjp#torch.autograd.functional.vjp", "type": "Automatic Differentiation", "text": ["Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs.", "func_output (tuple of Tensors or Tensor): output of func(inputs)", "vjp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.", "output (tuple)"]}, {"name": "torch.autograd.grad()", "path": "generated/torch.autograd.grad#torch.autograd.grad", "type": "Automatic Differentiation", "text": ["Computes and returns the sum of gradients of outputs with respect to the inputs.", "grad_outputs should be a sequence of length matching output containing the \u201cvector\u201d in vector-Jacobian product, usually the pre-computed gradients w.r.t. each of the outputs. If an output doesn\u2019t require_grad, then the gradient can be None).", "Note", "If you run any forward ops, create grad_outputs, and/or call grad in a user-specified CUDA stream context, see Stream semantics of backward passes.", "Note", "only_inputs argument is deprecated and is ignored now (defaults to True). To accumulate gradient for other parts of the graph, please use torch.autograd.backward.", "Tuple[Tensor, \u2026]"]}, {"name": "torch.autograd.gradcheck()", "path": "generated/torch.autograd.gradcheck#torch.autograd.gradcheck", "type": "Automatic Differentiation", "text": ["Check gradients computed via small finite differences against analytical gradients wrt tensors in inputs that are of floating point or complex type and with requires_grad=True.", "The check between numerical and analytical gradients uses allclose().", "For most of the complex functions we consider for optimization purposes, no notion of Jacobian exists. Instead, gradcheck verifies if the numerical and analytical values of the Wirtinger and Conjugate Wirtinger derivatives are consistent. Because the gradient computation is done under the assumption that the overall function has a real-valued output, we treat functions with complex output in a special way. For these functions, gradcheck is applied to two real-valued functions corresponding to taking the real components of the complex outputs for the first, and taking the imaginary components of the complex outputs for the second. For more details, check out Autograd for Complex Numbers.", "Note", "The default values are designed for input of double precision. This check will likely fail if input is of less precision, e.g., FloatTensor.", "Note", "Gradcheck may fail when evaluated on non-differentiable points because the numerically computed gradients via finite differencing may differ those computed analytically (not necessarily because either is incorrect). For more context, see Gradients for non-differentiable functions.", "Warning", "If any checked tensor in input has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from torch.expand()), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.", "True if all differences satisfy allclose condition", "bool"]}, {"name": "torch.autograd.gradgradcheck()", "path": "generated/torch.autograd.gradgradcheck#torch.autograd.gradgradcheck", "type": "Automatic Differentiation", "text": ["Check gradients of gradients computed via small finite differences against analytical gradients wrt tensors in inputs and grad_outputs that are of floating point or complex type and with requires_grad=True.", "This function checks that backpropagating through the gradients computed to the given grad_outputs are correct.", "The check between numerical and analytical gradients uses allclose().", "Note", "The default values are designed for input and grad_outputs of double precision. This check will likely fail if they are of less precision, e.g., FloatTensor.", "Warning", "If any checked tensor in input and grad_outputs has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from torch.expand()), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.", "True if all differences satisfy allclose condition", "bool"]}, {"name": "torch.autograd.graph.allow_mutation_on_saved_tensors", "path": "autograd#torch.autograd.graph.allow_mutation_on_saved_tensors", "type": "Automatic Differentiation", "text": ["Context manager under which mutating tensors saved for backward is allowed", "Under this context manager, tensors saved for backward are cloned on mutation, so the original version can still be used during backward. Normally, mutating a tensor saved for backward will result in an error raised when it\u2019s used during backward.", "To ensure the correct behavior, both the forward and backward should be run under the same context manager.", "An _AllowMutationOnSavedContext object storing the state managed by this context manager. This object can be useful for debugging purposes. The state managed by the context manager is automatically cleared upon exiting.", "Example:"]}, {"name": "torch.autograd.graph.disable_saved_tensors_hooks", "path": "autograd#torch.autograd.graph.disable_saved_tensors_hooks", "type": "Automatic Differentiation", "text": ["Context-manager that disables the saved tensors default hooks feature.", "Useful for if you are creating a feature that does not work with saved tensors default hooks.", "error_message (str) \u2013 When saved tensors default hooks are used when they have been are disabled, a RuntimeError with this error message gets raised.", "Example:"]}, {"name": "torch.autograd.graph.Node.metadata()", "path": "generated/torch.autograd.graph.node.metadata#torch.autograd.graph.Node.metadata", "type": "Automatic Differentiation", "text": ["Returns the metadata.", "dict"]}, {"name": "torch.autograd.graph.Node.name()", "path": "generated/torch.autograd.graph.node.name#torch.autograd.graph.Node.name", "type": "Automatic Differentiation", "text": ["Returns the name.", "Example:", "str"]}, {"name": "torch.autograd.graph.Node.next_functions", "path": "generated/torch.autograd.graph.node.next_functions#torch.autograd.graph.Node.next_functions", "type": "Automatic Differentiation", "text": []}, {"name": "torch.autograd.graph.Node.register_hook()", "path": "generated/torch.autograd.graph.node.register_hook#torch.autograd.graph.Node.register_hook", "type": "Automatic Differentiation", "text": ["Registers a backward hook.", "The hook will be called every time a gradient with respect to the Node is computed. The hook should have the following signature:", "The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of grad_inputs.", "This function returns a handle with a method handle.remove() that removes the hook from the module.", "Note", "See Backward Hooks execution for more information on how when this hook is executed, and how its execution is ordered relative to other hooks.", "Example:", "RemovableHandle"]}, {"name": "torch.autograd.graph.Node.register_prehook()", "path": "generated/torch.autograd.graph.node.register_prehook#torch.autograd.graph.Node.register_prehook", "type": "Automatic Differentiation", "text": ["Registers a backward pre-hook.", "The hook will be called every time a gradient with respect to the Node is computed. The hook should have the following signature:", "The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of grad_outputs.", "This function returns a handle with a method handle.remove() that removes the hook from the module.", "Note", "See Backward Hooks execution for more information on how when this hook is executed, and how its execution is ordered relative to other hooks.", "Example:", "RemovableHandle"]}, {"name": "torch.autograd.graph.register_multi_grad_hook", "path": "autograd#torch.autograd.graph.register_multi_grad_hook", "type": "Automatic Differentiation", "text": ["Registers a multi-grad backward hook.", "The hook will be called after gradients with respect to every tensor in tensors have been computed. If a tensor is in tensors but is not part of the graph, or if a tensor is not needed to compute the gradients for any inputs specified for the current .backward() or .grad() call, this tensor will be ignored and the hook will not wait for its gradient to be computed.", "After every non-ignored tensor\u2019s gradient has been computed, fn will be called with those gradients. None will be passed for tensors that did not have their gradients computed.", "The hook should not modify its arguments.", "This function returns a handle with a method handle.remove() that removes the hook.", "Note", "See Backward Hooks execution for more information on how when this hook is executed, and how its execution is ordered relative to other hooks.", "Example:"]}, {"name": "torch.autograd.graph.save_on_cpu", "path": "autograd#torch.autograd.graph.save_on_cpu", "type": "Automatic Differentiation", "text": ["Context-manager under which tensors saved by the forward pass will be stored on cpu, then retrieved for backward.", "When performing operations within this context manager, intermediary results saved in the graph during the forward pass will be moved to CPU, then copied back to the original device when needed for the backward pass. If the graph was already on CPU, no tensor copy is performed.", "Use this context-manager to trade compute for GPU memory usage (e.g. when your model doesn\u2019t fit in GPU memory during training).", "pin_memory (bool) \u2013 If True tensors will be saved to CPU pinned memory during packing and copied to GPU asynchronously during unpacking. Defaults to False. Also see Use pinned memory buffers.", "Example:"]}, {"name": "torch.autograd.graph.saved_tensors_hooks", "path": "autograd#torch.autograd.graph.saved_tensors_hooks", "type": "Automatic Differentiation", "text": ["Context-manager that sets a pair of pack / unpack hooks for saved tensors.", "Use this context-manager to define how intermediary results of an operation should be packed before saving, and unpacked on retrieval.", "In that context, the pack_hook function will be called everytime an operation saves a tensor for backward (this includes intermediary results saved using save_for_backward() but also those recorded by a PyTorch-defined operation). The output of pack_hook is then stored in the computation graph instead of the original tensor.", "The unpack_hook is called when the saved tensor needs to be accessed, namely when executing torch.Tensor.backward() or torch.autograd.grad(). It takes as argument the packed object returned by pack_hook and should return a tensor which has the same content as the original tensor (passed as input to the corresponding pack_hook).", "The hooks should have the following signatures:", "pack_hook(tensor: Tensor) -> Any", "unpack_hook(Any) -> Tensor", "where the return value of pack_hook is a valid input to unpack_hook.", "In general, you want unpack_hook(pack_hook(t)) to be equal to t in terms of value, size, dtype and device.", "Example:", "Warning", "Performing an inplace operation on the input to either hooks may lead to undefined behavior.", "Warning", "Only one pair of hooks is allowed at a time. When recursively nesting this context-manager, only the inner-most pair of hooks will be applied."]}, {"name": "torch.autograd.profiler.emit_itt", "path": "autograd#torch.autograd.profiler.emit_itt", "type": "Automatic Differentiation", "text": ["Context manager that makes every autograd operation emit an ITT range.", "It is useful when running the program under Intel(R) VTune Profiler:", "The Instrumentation and Tracing Technology (ITT) API enables your application to generate and control the collection of trace data during its execution across different Intel tools. This context manager is to annotate Intel(R) VTune Profiling trace. With help of this context manager, you will be able to see labled ranges in Intel(R) VTune Profiler GUI."]}, {"name": "torch.autograd.profiler.emit_nvtx", "path": "autograd#torch.autograd.profiler.emit_nvtx", "type": "Automatic Differentiation", "text": ["Context manager that makes every autograd operation emit an NVTX range.", "It is useful when running the program under nvprof:", "Unfortunately, there\u2019s no way to force nvprof to flush the data it collected to disk, so for CUDA profiling one has to use this context manager to annotate nvprof traces and wait for the process to exit before inspecting them. Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or torch.autograd.profiler.load_nvprof() can load the results for inspection e.g. in Python REPL.", "Forward-backward correlation", "When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler, correlating each backward-pass op with the corresponding forward-pass op can be difficult. To ease this task, emit_nvtx appends sequence number information to the ranges it generates.", "During the forward pass, each function range is decorated with seq=<N>. seq is a running counter, incremented each time a new backward Function object is created and stashed for backward. Thus, the seq=<N> annotation associated with each forward function range tells you that if a backward Function object is created by this forward function, the backward object will receive sequence number N. During the backward pass, the top-level range wrapping each C++ backward Function\u2019s apply() call is decorated with stashed seq=<M>. M is the sequence number that the backward object was created with. By comparing stashed seq numbers in backward with seq numbers in forward, you can track down which forward op created each backward Function.", "Any functions executed during the backward pass are also decorated with seq=<N>. During default backward (with create_graph=False) this information is irrelevant, and in fact, N may simply be 0 for all such functions. Only the top-level ranges associated with backward Function objects\u2019 apply() methods are useful, as a way to correlate these Function objects with the earlier forward pass.", "Double-backward", "If, on the other hand, a backward pass with create_graph=True is underway (in other words, if you are setting up for a double-backward), each function\u2019s execution during backward is given a nonzero, useful seq=<N>. Those functions may themselves create Function objects to be executed later during double-backward, just as the original functions in the forward pass did. The relationship between backward and double-backward is conceptually the same as the relationship between forward and backward: The functions still emit current-sequence-number-tagged ranges, the Function objects they create still stash those sequence numbers, and during the eventual double-backward, the Function objects\u2019 apply() ranges are still tagged with stashed seq numbers, which can be compared to seq numbers from the backward pass."]}, {"name": "torch.autograd.profiler.load_nvprof()", "path": "generated/torch.autograd.profiler.load_nvprof#torch.autograd.profiler.load_nvprof", "type": "Automatic Differentiation", "text": ["Opens an nvprof trace file and parses autograd annotations.", "path (str) \u2013 path to nvprof trace"]}, {"name": "torch.autograd.profiler.profile", "path": "autograd#torch.autograd.profiler.profile", "type": "Automatic Differentiation", "text": ["Context manager that manages autograd profiler state and holds a summary of results. Under the hood it just records events of functions being executed in C++ and exposes those events to Python. You can wrap any code into it and it will only report runtime of PyTorch functions. Note: profiler is thread local and is automatically propagated into the async tasks"]}, {"name": "torch.autograd.profiler.profile.export_chrome_trace()", "path": "generated/torch.autograd.profiler.profile.export_chrome_trace#torch.autograd.profiler.profile.export_chrome_trace", "type": "Automatic Differentiation", "text": ["Exports an EventList as a Chrome tracing tools file.", "The checkpoint can be later loaded and inspected under chrome://tracing URL.", "path (str) \u2013 Path where the trace will be written."]}, {"name": "torch.autograd.profiler.profile.key_averages()", "path": "generated/torch.autograd.profiler.profile.key_averages#torch.autograd.profiler.profile.key_averages", "type": "Automatic Differentiation", "text": ["Averages all function events over their keys.", "An EventList containing FunctionEventAvg objects."]}, {"name": "torch.autograd.profiler.profile.self_cpu_time_total", "path": "generated/torch.autograd.profiler.profile.self_cpu_time_total#torch.autograd.profiler.profile.self_cpu_time_total", "type": "Automatic Differentiation", "text": ["Returns total time spent on CPU obtained as a sum of all self times across all the events."]}, {"name": "torch.autograd.profiler.profile.total_average()", "path": "generated/torch.autograd.profiler.profile.total_average#torch.autograd.profiler.profile.total_average", "type": "Automatic Differentiation", "text": ["Averages all events.", "A FunctionEventAvg object."]}, {"name": "torch.autograd.set_detect_anomaly", "path": "autograd#torch.autograd.set_detect_anomaly", "type": "Automatic Differentiation", "text": ["Context-manager that sets the anomaly detection for the autograd engine on or off.", "set_detect_anomaly will enable or disable the autograd anomaly detection based on its argument mode. It can be used as a context-manager or as a function.", "See detect_anomaly above for details of the anomaly detection behaviour."]}, {"name": "torch.backends", "path": "backends", "type": "Backends", "text": ["torch.backends controls the behavior of various backends that PyTorch supports.", "These backends include:", "Returns cpu capability as a string value.", "Possible values: - \u201cDEFAULT\u201d - \u201cVSX\u201d - \u201cZ VECTOR\u201d - \u201cNO AVX\u201d - \u201cAVX2\u201d - \u201cAVX512\u201d", "str", "Returns whether PyTorch is built with CUDA support. Note that this doesn\u2019t necessarily mean CUDA is available; just that if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it.", "A bool that controls whether TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices.", "A bool that controls whether reduced precision reductions (e.g., with fp16 accumulation type) are allowed with fp16 GEMMs.", "A bool that controls whether reduced precision reductions are allowed with bf16 GEMMs.", "cufft_plan_cache contains the cuFFT plan caches for each CUDA device. Query a specific device i\u2019s cache via torch.backends.cuda.cufft_plan_cache[i].", "A readonly int that shows the number of plans currently in a cuFFT plan cache.", "A int that controls the capacity of a cuFFT plan cache.", "Clears a cuFFT plan cache.", "Warning", "This flag is experimental and subject to change.", "When PyTorch runs a CUDA linear algebra operation it often uses the cuSOLVER or MAGMA libraries, and if both are available it decides which to use with a heuristic. This flag (a str) allows overriding those heuristics.", "Note: When a library is preferred other libraries may still be used if the preferred library doesn\u2019t implement the operation(s) called. This flag may achieve better performance if PyTorch\u2019s heuristic library selection is incorrect for your application\u2019s inputs.", "Currently supported linalg operators:", "_LinalgBackend", "Enum class for the scaled dot product attention backends.", "Warning", "This class is in beta and subject to change.", "This class needs to stay aligned with the enum defined in: pytorch/aten/src/ATen/native/transformers/sdp_utils_cpp.h", "Warning", "This flag is beta and subject to change.", "Returns whether flash scaled dot product attention is enabled or not.", "Warning", "This flag is beta and subject to change.", "Enables or disables memory efficient scaled dot product attention.", "Warning", "This flag is beta and subject to change.", "Returns whether memory efficient scaled dot product attention is enabled or not.", "Warning", "This flag is beta and subject to change.", "Enables or disables flash scaled dot product attention.", "Warning", "This flag is beta and subject to change.", "Returns whether math scaled dot product attention is enabled or not.", "Warning", "This flag is beta and subject to change.", "Enables or disables math scaled dot product attention.", "Warning", "This flag is beta and subject to change.", "This context manager can be used to temporarily enable or disable any of the three backends for scaled dot product attention. Upon exiting the context manager, the previous state of the flags will be restored.", "Returns the version of cuDNN", "Returns a bool indicating if CUDNN is currently available.", "A bool that controls whether cuDNN is enabled.", "A bool that controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices.", "A bool that, if True, causes cuDNN to only use deterministic convolution algorithms. See also torch.are_deterministic_algorithms_enabled() and torch.use_deterministic_algorithms().", "A bool that, if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest.", "A int that specifies the maximum number of cuDNN convolution algorithms to try when torch.backends.cudnn.benchmark is True. Set benchmark_limit to zero to try every available algorithm. Note that this setting only affects convolutions dispatched via the cuDNN v8 API.", "Returns a bool indicating if MPS is currently available.", "bool", "Returns whether PyTorch is built with MPS support. Note that this doesn\u2019t necessarily mean MPS is available; just that if this PyTorch binary were run a machine with working MPS drivers and devices, we would be able to use it.", "bool", "Returns whether PyTorch is built with MKL support.", "On-demand oneMKL verbosing functionality To make it easier to debug performance issues, oneMKL can dump verbose messages containing execution information like duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named MKL_VERBOSE. However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only.", "level \u2013 Verbose level - VERBOSE_OFF: Disable verbosing - VERBOSE_ON: Enable verbosing", "Returns whether PyTorch is built with MKL-DNN support.", "On-demand oneDNN (former MKL-DNN) verbosing functionality To make it easier to debug performance issues, oneDNN can dump verbose messages containing information like kernel size, input data size and execution duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named DNNL_VERBOSE. However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only.", "level \u2013 Verbose level - VERBOSE_OFF: Disable verbosing - VERBOSE_ON: Enable verbosing - VERBOSE_ON_CREATION: Enable verbosing, including oneDNN kernel creation", "Returns whether PyTorch is built with OpenMP support.", "Returns a bool indicating if opt_einsum is currently available.", "bool", "Returns the opt_einsum package if opt_einsum is currently available, else None.", "Any", "A :class:bool that controls whether opt_einsum is enabled (True by default). If so, torch.einsum will use opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html) if available to calculate an optimal path of contraction for faster performance.", "If opt_einsum is not available, torch.einsum will fall back to the default contraction path of left to right.", "A :class:str that specifies which strategies to try when torch.backends.opt_einsum.enabled is True. By default, torch.einsum will try the \u201cauto\u201d strategy, but the \u201cgreedy\u201d and \u201coptimal\u201d strategies are also supported. Note that the \u201coptimal\u201d strategy is factorial on the number of inputs as it tries all possible paths. See more details in opt_einsum\u2019s docs (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html)."]}, {"name": "torch.backends.cpu.get_cpu_capability()", "path": "backends#torch.backends.cpu.get_cpu_capability", "type": "Backends", "text": ["Returns cpu capability as a string value.", "Possible values: - \u201cDEFAULT\u201d - \u201cVSX\u201d - \u201cZ VECTOR\u201d - \u201cNO AVX\u201d - \u201cAVX2\u201d - \u201cAVX512\u201d", "str"]}, {"name": "torch.backends.cuda.cufft_plan_cache", "path": "backends#torch.backends.cuda.cufft_plan_cache", "type": "Backends", "text": ["cufft_plan_cache contains the cuFFT plan caches for each CUDA device. Query a specific device i\u2019s cache via torch.backends.cuda.cufft_plan_cache[i].", "A readonly int that shows the number of plans currently in a cuFFT plan cache.", "A int that controls the capacity of a cuFFT plan cache.", "Clears a cuFFT plan cache."]}, {"name": "torch.backends.cuda.cufft_plan_cache.clear()", "path": "backends#torch.backends.cuda.cufft_plan_cache.clear", "type": "Backends", "text": ["Clears a cuFFT plan cache."]}, {"name": "torch.backends.cuda.cufft_plan_cache.max_size", "path": "backends#torch.backends.cuda.cufft_plan_cache.max_size", "type": "Backends", "text": ["A int that controls the capacity of a cuFFT plan cache."]}, {"name": "torch.backends.cuda.cufft_plan_cache.size", "path": "backends#torch.backends.cuda.cufft_plan_cache.size", "type": "Backends", "text": ["A readonly int that shows the number of plans currently in a cuFFT plan cache."]}, {"name": "torch.backends.cuda.enable_flash_sdp()", "path": "backends#torch.backends.cuda.enable_flash_sdp", "type": "Backends", "text": ["Warning", "This flag is beta and subject to change.", "Enables or disables flash scaled dot product attention."]}, {"name": "torch.backends.cuda.enable_math_sdp()", "path": "backends#torch.backends.cuda.enable_math_sdp", "type": "Backends", "text": ["Warning", "This flag is beta and subject to change.", "Enables or disables math scaled dot product attention."]}, {"name": "torch.backends.cuda.enable_mem_efficient_sdp()", "path": "backends#torch.backends.cuda.enable_mem_efficient_sdp", "type": "Backends", "text": ["Warning", "This flag is beta and subject to change.", "Enables or disables memory efficient scaled dot product attention."]}, {"name": "torch.backends.cuda.flash_sdp_enabled()", "path": "backends#torch.backends.cuda.flash_sdp_enabled", "type": "Backends", "text": ["Warning", "This flag is beta and subject to change.", "Returns whether flash scaled dot product attention is enabled or not."]}, {"name": "torch.backends.cuda.is_built()", "path": "backends#torch.backends.cuda.is_built", "type": "Backends", "text": ["Returns whether PyTorch is built with CUDA support. Note that this doesn\u2019t necessarily mean CUDA is available; just that if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it."]}, {"name": "torch.backends.cuda.math_sdp_enabled()", "path": "backends#torch.backends.cuda.math_sdp_enabled", "type": "Backends", "text": ["Warning", "This flag is beta and subject to change.", "Returns whether math scaled dot product attention is enabled or not."]}, {"name": "torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction", "path": "backends#torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction", "type": "Backends", "text": ["A bool that controls whether reduced precision reductions are allowed with bf16 GEMMs."]}, {"name": "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction", "path": "backends#torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction", "type": "Backends", "text": ["A bool that controls whether reduced precision reductions (e.g., with fp16 accumulation type) are allowed with fp16 GEMMs."]}, {"name": "torch.backends.cuda.matmul.allow_tf32", "path": "backends#torch.backends.cuda.matmul.allow_tf32", "type": "Backends", "text": ["A bool that controls whether TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices."]}, {"name": "torch.backends.cuda.mem_efficient_sdp_enabled()", "path": "backends#torch.backends.cuda.mem_efficient_sdp_enabled", "type": "Backends", "text": ["Warning", "This flag is beta and subject to change.", "Returns whether memory efficient scaled dot product attention is enabled or not."]}, {"name": "torch.backends.cuda.preferred_linalg_library()", "path": "backends#torch.backends.cuda.preferred_linalg_library", "type": "Backends", "text": ["Warning", "This flag is experimental and subject to change.", "When PyTorch runs a CUDA linear algebra operation it often uses the cuSOLVER or MAGMA libraries, and if both are available it decides which to use with a heuristic. This flag (a str) allows overriding those heuristics.", "Note: When a library is preferred other libraries may still be used if the preferred library doesn\u2019t implement the operation(s) called. This flag may achieve better performance if PyTorch\u2019s heuristic library selection is incorrect for your application\u2019s inputs.", "Currently supported linalg operators:", "_LinalgBackend"]}, {"name": "torch.backends.cuda.sdp_kernel()", "path": "backends#torch.backends.cuda.sdp_kernel", "type": "Backends", "text": ["Warning", "This flag is beta and subject to change.", "This context manager can be used to temporarily enable or disable any of the three backends for scaled dot product attention. Upon exiting the context manager, the previous state of the flags will be restored."]}, {"name": "torch.backends.cuda.SDPBackend", "path": "backends#torch.backends.cuda.SDPBackend", "type": "Backends", "text": ["Enum class for the scaled dot product attention backends.", "Warning", "This class is in beta and subject to change.", "This class needs to stay aligned with the enum defined in: pytorch/aten/src/ATen/native/transformers/sdp_utils_cpp.h"]}, {"name": "torch.backends.cudnn.allow_tf32", "path": "backends#torch.backends.cudnn.allow_tf32", "type": "Backends", "text": ["A bool that controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices."]}, {"name": "torch.backends.cudnn.benchmark", "path": "backends#torch.backends.cudnn.benchmark", "type": "Backends", "text": ["A bool that, if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest."]}, {"name": "torch.backends.cudnn.benchmark_limit", "path": "backends#torch.backends.cudnn.benchmark_limit", "type": "Backends", "text": ["A int that specifies the maximum number of cuDNN convolution algorithms to try when torch.backends.cudnn.benchmark is True. Set benchmark_limit to zero to try every available algorithm. Note that this setting only affects convolutions dispatched via the cuDNN v8 API."]}, {"name": "torch.backends.cudnn.deterministic", "path": "backends#torch.backends.cudnn.deterministic", "type": "Backends", "text": ["A bool that, if True, causes cuDNN to only use deterministic convolution algorithms. See also torch.are_deterministic_algorithms_enabled() and torch.use_deterministic_algorithms()."]}, {"name": "torch.backends.cudnn.enabled", "path": "backends#torch.backends.cudnn.enabled", "type": "Backends", "text": ["A bool that controls whether cuDNN is enabled."]}, {"name": "torch.backends.cudnn.is_available()", "path": "backends#torch.backends.cudnn.is_available", "type": "Backends", "text": ["Returns a bool indicating if CUDNN is currently available."]}, {"name": "torch.backends.cudnn.version()", "path": "backends#torch.backends.cudnn.version", "type": "Backends", "text": ["Returns the version of cuDNN"]}, {"name": "torch.backends.mkl.is_available()", "path": "backends#torch.backends.mkl.is_available", "type": "Backends", "text": ["Returns whether PyTorch is built with MKL support."]}, {"name": "torch.backends.mkl.verbose", "path": "backends#torch.backends.mkl.verbose", "type": "Backends", "text": ["On-demand oneMKL verbosing functionality To make it easier to debug performance issues, oneMKL can dump verbose messages containing execution information like duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named MKL_VERBOSE. However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only.", "level \u2013 Verbose level - VERBOSE_OFF: Disable verbosing - VERBOSE_ON: Enable verbosing"]}, {"name": "torch.backends.mkldnn.is_available()", "path": "backends#torch.backends.mkldnn.is_available", "type": "Backends", "text": ["Returns whether PyTorch is built with MKL-DNN support."]}, {"name": "torch.backends.mkldnn.verbose", "path": "backends#torch.backends.mkldnn.verbose", "type": "Backends", "text": ["On-demand oneDNN (former MKL-DNN) verbosing functionality To make it easier to debug performance issues, oneDNN can dump verbose messages containing information like kernel size, input data size and execution duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named DNNL_VERBOSE. However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only.", "level \u2013 Verbose level - VERBOSE_OFF: Disable verbosing - VERBOSE_ON: Enable verbosing - VERBOSE_ON_CREATION: Enable verbosing, including oneDNN kernel creation"]}, {"name": "torch.backends.mps.is_available()", "path": "backends#torch.backends.mps.is_available", "type": "Backends", "text": ["Returns a bool indicating if MPS is currently available.", "bool"]}, {"name": "torch.backends.mps.is_built()", "path": "backends#torch.backends.mps.is_built", "type": "Backends", "text": ["Returns whether PyTorch is built with MPS support. Note that this doesn\u2019t necessarily mean MPS is available; just that if this PyTorch binary were run a machine with working MPS drivers and devices, we would be able to use it.", "bool"]}, {"name": "torch.backends.openmp.is_available()", "path": "backends#torch.backends.openmp.is_available", "type": "Backends", "text": ["Returns whether PyTorch is built with OpenMP support."]}, {"name": "torch.backends.opt_einsum.enabled", "path": "backends#torch.backends.opt_einsum.enabled", "type": "Backends", "text": ["A :class:bool that controls whether opt_einsum is enabled (True by default). If so, torch.einsum will use opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html) if available to calculate an optimal path of contraction for faster performance.", "If opt_einsum is not available, torch.einsum will fall back to the default contraction path of left to right."]}, {"name": "torch.backends.opt_einsum.get_opt_einsum()", "path": "backends#torch.backends.opt_einsum.get_opt_einsum", "type": "Backends", "text": ["Returns the opt_einsum package if opt_einsum is currently available, else None.", "Any"]}, {"name": "torch.backends.opt_einsum.is_available()", "path": "backends#torch.backends.opt_einsum.is_available", "type": "Backends", "text": ["Returns a bool indicating if opt_einsum is currently available.", "bool"]}, {"name": "torch.backends.opt_einsum.strategy", "path": "backends#torch.backends.opt_einsum.strategy", "type": "Backends", "text": ["A :class:str that specifies which strategies to try when torch.backends.opt_einsum.enabled is True. By default, torch.einsum will try the \u201cauto\u201d strategy, but the \u201cgreedy\u201d and \u201coptimal\u201d strategies are also supported. Note that the \u201coptimal\u201d strategy is factorial on the number of inputs as it tries all possible paths. See more details in opt_einsum\u2019s docs (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html)."]}, {"name": "torch.baddbmm", "path": "generated/torch.baddbmm", "type": "Torch", "text": ["Performs a batch matrix-matrix product of matrices in batch1 and batch2. input is added to the final result.", "batch1 and batch2 must be 3-D tensors each containing the same number of matrices.", "If batch1 is a (b\u00d7n\u00d7m)(b \\times n \\times m) tensor, batch2 is a (b\u00d7m\u00d7p)(b \\times m \\times p) tensor, then input must be broadcastable with a (b\u00d7n\u00d7p)(b \\times n \\times p) tensor and out will be a (b\u00d7n\u00d7p)(b \\times n \\times p) tensor. Both alpha and beta mean the same as the scaling factors used in torch.addbmm().", "If beta is 0, then input will be ignored, and nan and inf in it will not be propagated.", "For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers.", "This operator supports TensorFloat32.", "On certain ROCm devices, when using float16 inputs this module will use different precision for backward.", "Example:"]}, {"name": "torch.bartlett_window", "path": "generated/torch.bartlett_window", "type": "Torch", "text": ["Bartlett window function.", "where NN is the full window size.", "The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NN in above formula is in fact window_length+1\\text{window\\_length} + 1. Also, we always have torch.bartlett_window(L, periodic=True) equal to torch.bartlett_window(L + 1, periodic=False)[:-1]).", "Note", "If window_length =1=1, the returned window contains a single value 1.", "A 1-D tensor of size (window_length,)(\\text{window\\_length},) containing the window", "Tensor"]}, {"name": "torch.bernoulli", "path": "generated/torch.bernoulli", "type": "Torch", "text": ["Draws binary random numbers (0 or 1) from a Bernoulli distribution.", "The input tensor should be a tensor containing probabilities to be used for drawing the binary random number. Hence, all values in input have to be in the range: 0\u2264inputi\u226410 \\leq \\text{input}_i \\leq 1.", "The ith\\text{i}^{th} element of the output tensor will draw a value 11 according to the ith\\text{i}^{th} probability value given in input.", "The returned out tensor only has values 0 or 1 and is of the same shape as input.", "out can have integral dtype, but input must have floating point dtype.", "input (Tensor) \u2013 the input tensor of probability values for the Bernoulli distribution", "Example:"]}, {"name": "torch.BFloat16Storage", "path": "storage#torch.BFloat16Storage", "type": "Storage", "text": []}, {"name": "torch.BFloat16Storage.dtype", "path": "storage#torch.BFloat16Storage.dtype", "type": "Storage", "text": []}, {"name": "torch.bincount", "path": "generated/torch.bincount", "type": "Torch", "text": ["Count the frequency of each value in an array of non-negative ints.", "The number of bins (size 1) is one larger than the largest value in input unless input is empty, in which case the result is a tensor of size 0. If minlength is specified, the number of bins is at least minlength and if input is empty, then the result is tensor of size minlength filled with zeros. If n is the value at position i, out[n] += weights[i] if weights is specified else out[n] += 1.", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.", "a tensor of shape Size([max(input) + 1]) if input is non-empty, else Size(0)", "output (Tensor)", "Example:"]}, {"name": "torch.bitwise_and", "path": "generated/torch.bitwise_and", "type": "Torch", "text": ["Computes the bitwise AND of input and other. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical AND.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.bitwise_left_shift", "path": "generated/torch.bitwise_left_shift", "type": "Torch", "text": ["Computes the left arithmetic shift of input by other bits. The input tensor must be of integral type. This operator supports broadcasting to a common shape and type promotion.", "The operation applied is:", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.bitwise_not", "path": "generated/torch.bitwise_not", "type": "Torch", "text": ["Computes the bitwise NOT of the given input tensor. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical NOT.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.bitwise_or", "path": "generated/torch.bitwise_or", "type": "Torch", "text": ["Computes the bitwise OR of input and other. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical OR.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.bitwise_right_shift", "path": "generated/torch.bitwise_right_shift", "type": "Torch", "text": ["Computes the right arithmetic shift of input by other bits. The input tensor must be of integral type. This operator supports broadcasting to a common shape and type promotion. In any case, if the value of the right operand is negative or is greater or equal to the number of bits in the promoted left operand, the behavior is undefined.", "The operation applied is:", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.bitwise_xor", "path": "generated/torch.bitwise_xor", "type": "Torch", "text": ["Computes the bitwise XOR of input and other. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical XOR.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.blackman_window", "path": "generated/torch.blackman_window", "type": "Torch", "text": ["Blackman window function.", "where NN is the full window size.", "The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NN in above formula is in fact window_length+1\\text{window\\_length} + 1. Also, we always have torch.blackman_window(L, periodic=True) equal to torch.blackman_window(L + 1, periodic=False)[:-1]).", "Note", "If window_length =1=1, the returned window contains a single value 1.", "A 1-D tensor of size (window_length,)(\\text{window\\_length},) containing the window", "Tensor"]}, {"name": "torch.block_diag", "path": "generated/torch.block_diag", "type": "Torch", "text": ["Create a block diagonal matrix from provided tensors.", "*tensors \u2013 One or more tensors with 0, 1, or 2 dimensions.", "A 2 dimensional tensor with all the input tensors arranged in order such that their upper left and lower right corners are diagonally adjacent. All other elements are set to 0.", "Tensor", "Example:"]}, {"name": "torch.bmm", "path": "generated/torch.bmm", "type": "Torch", "text": ["Performs a batch matrix-matrix product of matrices stored in input and mat2.", "input and mat2 must be 3-D tensors each containing the same number of matrices.", "If input is a (b\u00d7n\u00d7m)(b \\times n \\times m) tensor, mat2 is a (b\u00d7m\u00d7p)(b \\times m \\times p) tensor, out will be a (b\u00d7n\u00d7p)(b \\times n \\times p) tensor.", "This operator supports TensorFloat32.", "On certain ROCm devices, when using float16 inputs this module will use different precision for backward.", "Note", "This function does not broadcast. For broadcasting matrix products, see torch.matmul().", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.BoolStorage", "path": "storage#torch.BoolStorage", "type": "Storage", "text": []}, {"name": "torch.BoolStorage.dtype", "path": "storage#torch.BoolStorage.dtype", "type": "Storage", "text": []}, {"name": "torch.broadcast_shapes", "path": "generated/torch.broadcast_shapes", "type": "Torch", "text": ["Similar to broadcast_tensors() but for shapes.", "This is equivalent to torch.broadcast_tensors(*map(torch.empty, shapes))[0].shape but avoids the need create to intermediate tensors. This is useful for broadcasting tensors of common batch shape but different rightmost shape, e.g. to broadcast mean vectors with covariance matrices.", "Example:", "*shapes (torch.Size) \u2013 Shapes of tensors.", "A shape compatible with all input shapes.", "shape (torch.Size)", "RuntimeError \u2013 If shapes are incompatible."]}, {"name": "torch.broadcast_tensors", "path": "generated/torch.broadcast_tensors", "type": "Torch", "text": ["Broadcasts the given tensors according to Broadcasting semantics.", "*tensors \u2013 any number of tensors of the same type", "Warning", "More than one element of a broadcasted tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.", "Example:"]}, {"name": "torch.broadcast_to", "path": "generated/torch.broadcast_to", "type": "Torch", "text": ["Broadcasts input to the shape shape. Equivalent to calling input.expand(shape). See expand() for details.", "Example:"]}, {"name": "torch.bucketize", "path": "generated/torch.bucketize", "type": "Torch", "text": ["Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries. Return a new tensor with the same size as input. If right is False (default), then the left boundary is open. Note that this behavior is opposite the behavior of numpy.digitize. More formally, the returned index satisfies the following rules:", "right", "returned index satisfies", "False", "boundaries[i-1] < input[m][n]...[l][x] <= boundaries[i]", "True", "boundaries[i-1] <= input[m][n]...[l][x] < boundaries[i]", "Example:"]}, {"name": "torch.ByteStorage", "path": "storage#torch.ByteStorage", "type": "Storage", "text": []}, {"name": "torch.ByteStorage.dtype", "path": "storage#torch.ByteStorage.dtype", "type": "Storage", "text": []}, {"name": "torch.can_cast", "path": "generated/torch.can_cast", "type": "Torch", "text": ["Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation.", "Example:"]}, {"name": "torch.cartesian_prod", "path": "generated/torch.cartesian_prod", "type": "Torch", "text": ["Do cartesian product of the given sequence of tensors. The behavior is similar to python\u2019s itertools.product.", "*tensors (Tensor) \u2013 any number of 1 dimensional tensors.", "A tensor equivalent to converting all the input tensors into lists, do itertools.product on these lists, and finally convert the resulting list into tensor.", "Tensor", "Example:"]}, {"name": "torch.cat", "path": "generated/torch.cat", "type": "Torch", "text": ["Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.", "torch.cat() can be seen as an inverse operation for torch.split() and torch.chunk().", "torch.cat() can be best understood via examples.", "See also", "torch.stack() concatenates the given sequence along a new dimension.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.cdist", "path": "generated/torch.cdist", "type": "Torch", "text": ["Computes batched the p-norm distance between each pair of the two collections of row vectors.", "Tensor", "If x1 has shape B\u00d7P\u00d7MB \\times P \\times M and x2 has shape B\u00d7R\u00d7MB \\times R \\times M then the output will have shape B\u00d7P\u00d7RB \\times P \\times R.", "This function is equivalent to scipy.spatial.distance.cdist(input,\u2019minkowski\u2019, p=p) if p\u2208(0,\u221e)p \\in (0, \\infty). When p=0p = 0 it is equivalent to scipy.spatial.distance.cdist(input, \u2018hamming\u2019) * M. When p=\u221ep = \\infty, the closest scipy function is scipy.spatial.distance.cdist(xn, lambda x, y: np.abs(x - y).max())."]}, {"name": "torch.ceil", "path": "generated/torch.ceil", "type": "Torch", "text": ["Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.", "For integer inputs, follows the array-api convention of returning a copy of the input tensor.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.chain_matmul", "path": "generated/torch.chain_matmul", "type": "Torch", "text": ["Returns the matrix product of the NN 2-D tensors. This product is efficiently computed using the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms of arithmetic operations ([CLRS]). Note that since this is a function to compute the product, NN needs to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned. If NN is 1, then this is a no-op - the original matrix is returned as is.", "Warning", "torch.chain_matmul() is deprecated and will be removed in a future PyTorch release. Use torch.linalg.multi_dot() instead, which accepts a list of two or more tensors rather than multiple arguments.", "if the ithi^{th} tensor was of dimensions pi\u00d7pi+1p_{i} \\times p_{i + 1}, then the product would be of dimensions p1\u00d7pN+1p_{1} \\times p_{N + 1}.", "Tensor", "Example:"]}, {"name": "torch.CharStorage", "path": "storage#torch.CharStorage", "type": "Storage", "text": []}, {"name": "torch.CharStorage.dtype", "path": "storage#torch.CharStorage.dtype", "type": "Storage", "text": []}, {"name": "torch.cholesky", "path": "generated/torch.cholesky", "type": "Torch", "text": ["Computes the Cholesky decomposition of a symmetric positive-definite matrix AA or for batches of symmetric positive-definite matrices.", "If upper is True, the returned matrix U is upper-triangular, and the decomposition has the form:", "If upper is False, the returned matrix L is lower-triangular, and the decomposition has the form:", "If upper is True, and AA is a batch of symmetric positive-definite matrices, then the returned tensor will be composed of upper-triangular Cholesky factors of each of the individual matrices. Similarly, when upper is False, the returned tensor will be composed of lower-triangular Cholesky factors of each of the individual matrices.", "Warning", "torch.cholesky() is deprecated in favor of torch.linalg.cholesky() and will be removed in a future PyTorch release.", "L = torch.cholesky(A) should be replaced with", "U = torch.cholesky(A, upper=True) should be replaced with", "This transform will produce equivalent results for all valid (symmetric positive definite) inputs.", "out (Tensor, optional) \u2013 the output matrix", "Example:"]}, {"name": "torch.cholesky_inverse", "path": "generated/torch.cholesky_inverse", "type": "Torch", "text": ["Computes the inverse of a symmetric positive-definite matrix AA using its Cholesky factor uu: returns matrix inv. The inverse is computed using LAPACK routines dpotri and spotri (and the corresponding MAGMA routines).", "If upper is False, uu is lower triangular such that the returned tensor is", "If upper is True or not provided, uu is upper triangular such that the returned tensor is", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if AA is a batch of matrices then the output has the same batch dimensions.", "out (Tensor, optional) \u2013 the output tensor for inv", "Example:"]}, {"name": "torch.cholesky_solve", "path": "generated/torch.cholesky_solve", "type": "Torch", "text": ["Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uu.", "If upper is False, uu is and lower triangular and c is returned such that:", "If upper is True or not provided, uu is upper triangular and c is returned such that:", "torch.cholesky_solve(b, u) can take in 2D inputs b, u or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs c", "Supports real-valued and complex-valued inputs. For the complex-valued inputs the transpose operator above is the conjugate transpose.", "out (Tensor, optional) \u2013 the output tensor for c", "Example:"]}, {"name": "torch.chunk", "path": "generated/torch.chunk", "type": "Torch", "text": ["Attempts to split a tensor into the specified number of chunks. Each chunk is a view of the input tensor.", "Note", "This function may return fewer than the specified number of chunks!", "See also", "torch.tensor_split() a function that always returns exactly the specified number of chunks", "If the tensor size along the given dimension dim is divisible by chunks, all returned chunks will be the same size. If the tensor size along the given dimension dim is not divisible by chunks, all returned chunks will be the same size, except the last one. If such division is not possible, this function may return fewer than the specified number of chunks."]}, {"name": "torch.clamp", "path": "generated/torch.clamp", "type": "Torch", "text": ["Clamps all elements in input into the range [ min, max ]. Letting min_value and max_value be min and max, respectively, this returns:", "If min is None, there is no lower bound. Or, if max is None there is no upper bound.", "Note", "If min is greater than max torch.clamp(..., min, max) sets all elements in input to the value of max.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.clip", "path": "generated/torch.clip", "type": "Torch", "text": ["Alias for torch.clamp()."]}, {"name": "torch.clone", "path": "generated/torch.clone", "type": "Torch", "text": ["Returns a copy of input.", "Note", "This function is differentiable, so gradients will flow back from the result of this operation to input. To create a tensor without an autograd relationship to input see detach().", "input (Tensor) \u2013 the input tensor.", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned tensor. Default: torch.preserve_format."]}, {"name": "torch.column_stack", "path": "generated/torch.column_stack", "type": "Torch", "text": ["Creates a new tensor by horizontally stacking the tensors in tensors.", "Equivalent to torch.hstack(tensors), except each zero or one dimensional tensor t in tensors is first reshaped into a (t.numel(), 1) column before being stacked horizontally.", "tensors (sequence of Tensors) \u2013 sequence of tensors to concatenate", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.combinations", "path": "generated/torch.combinations", "type": "Torch", "text": ["Compute combinations of length rr of the given tensor. The behavior is similar to python\u2019s itertools.combinations when with_replacement is set to False, and itertools.combinations_with_replacement when with_replacement is set to True.", "A tensor equivalent to converting all the input tensors into lists, do itertools.combinations or itertools.combinations_with_replacement on these lists, and finally convert the resulting list into tensor.", "Tensor", "Example:"]}, {"name": "torch.compile", "path": "generated/torch.compile", "type": "Torch", "text": ["Optimizes given model/function using TorchDynamo and specified backend.", "Concretely, for every frame executed within the compiled region, we will attempt to compile it and cache the compiled result on the code object for future use. A single frame may be compiled multiple times if previous compiled results are not applicable for subsequent calls (this is called a \u201cguard failure), you can use TORCH_LOGS=guards to debug these situations. Multiple compiled results can be associated with a frame up to torch._dynamo.config.cache_size_limit, which defaults to 64; at which point we will fall back to eager. Note that compile caches are per code object, not frame; if you dynamically create multiple copies of a function, they will all share the same code cache.", "backend (str or Callable) \u2013 ", "backend to be used", "mode (str) \u2013 ", "Can be either \u201cdefault\u201d, \u201creduce-overhead\u201d, \u201cmax-autotune\u201d or \u201cmax-autotune-no-cudagraphs\u201d", "options (dict) \u2013 ", "A dictionary of options to pass to the backend. Some notable ones to try out are", "Callable", "Example:"]}, {"name": "torch.compiled_with_cxx11_abi", "path": "generated/torch.compiled_with_cxx11_abi", "type": "Torch", "text": ["Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1", "bool"]}, {"name": "torch.compiler", "path": "torch.compiler", "type": "Miscellaneous", "text": ["torch.compiler is a namespace through which some of the internal compiler methods are surfaced for user consumption. The main function and the feature in this namespace is torch.compile.", "torch.compile is a PyTorch function introduced in PyTorch 2.x that aims to solve the problem of accurate graph capturing in PyTorch and ultimately enable software engineers to run their PyTorch programs faster. torch.compile is written in Python and it marks the transition of PyTorch from C++ to Python.", "torch.compile leverages the following underlying technologies:", "Note", "In some cases, the terms torch.compile, TorchDynamo, torch.compiler might be used interchangeably in this documentation.", "As mentioned above, to run your workflows faster, torch.compile through TorchDynamo requires a backend that converts the captured graphs into a fast machine code. Different backends can result in various optimization gains. The default backend is called TorchInductor, also known as inductor, TorchDynamo has a list of supported backends developed by our partners, which can be see by running torch.compiler.list_backends() each of which with its optional dependencies.", "Some of the most commonly used backends include:", "Training & inference backends", "Backend", "Description", "torch.compile(m, backend=\"inductor\")", "Uses the TorchInductor backend. Read more", "torch.compile(m, backend=\"cudagraphs\")", "CUDA graphs with AOT Autograd. Read more", "torch.compile(m, backend=\"ipex\")", "Uses IPEX on CPU. Read more", "torch.compile(m, backend=\"onnxrt\")", "Uses ONNX Runtime for training on CPU/GPU. Read more", "Inference-only backends", "Backend", "Description", "torch.compile(m, backend=\"tensorrt\")", "Uses ONNX Runtime to run TensorRT for inference optimizations. Read more", "torch.compile(m, backend=\"ipex\")", "Uses IPEX for inference on CPU. Read more", "torch.compile(m, backend=\"tvm\")", "Uses Apache TVM for inference optimizations. Read more", "Getting Started for PyTorch Users", "Deep Dive for PyTorch Developers", "HowTo for PyTorch Backend Vendors"]}, {"name": "torch.compiler.allow_in_graph()", "path": "generated/torch.compiler.allow_in_graph#torch.compiler.allow_in_graph", "type": "Miscellaneous", "text": ["Customize which functions compilation will include in the generated graph. It bypasses all introspection of the symbolic python code in favor of directly writing it to the graph. If fn is a list or tuple of callables it recursively applies allow_in_graph() to each function and returns a new list or tuple containing the modified functions", "fn \u2013 A callable representing the function to be included in the graph.", "Warning", "allow_in_graph() skips TorchDynamo completely on the decorated function skipping all TorchDynamo safety checks (graph breaks, handling closures, etc). Therefore, one has to be very careful with allow_in_graph() since subsystems like AOT Autograd rely on torchdynamo If not careful, this could lead to soundness and really hard-to-debug issues."]}, {"name": "torch.compiler.assume_constant_result()", "path": "generated/torch.compiler.assume_constant_result#torch.compiler.assume_constant_result", "type": "Miscellaneous", "text": ["This function is used to mark a function fn as having a constant result. This allows the compiler to optimize away your function Returns The same function fn", "fn \u2013 The function to be marked as having a constant result.", "Warning", "assume_constant_result can if invalid cause safety and soundness issues, torch.compile() will not attempt to validate whether the constant assumption is true or not"]}, {"name": "torch.compiler.Best Practices for Backends", "path": "torch.compiler_best_practices_for_backends", "type": "Miscellaneous", "text": ["Compiled workloads on modern x86 CPUs are usually optimized by Single Instruction Multiple Data (SIMD) instruction sets. SIMD is a typical parallel processing technique for high performance computing, such as deep learning model training and inference. With SIMD applied, each compute unit performs the same instruction with different allocated data at any given time slot. The most commonly deployed x86 instruction set architectures (ISAs) enabling SIMD include AVX, AVX2, AVX-512 and AMX.", "You can check supported ISAs for your machine by using the collect_env script. As the script provides complete environment information for PyTorch, we can use grep to extract the line containing ISA information:", "Normally, if AVX-512 is supported, instructions start with \u201cavx512\u201d (like avx512f, avx512bw, avx512_vnni) should be observed. If AMX is supported, instructions start with \u201camx\u201d (like amx_tile, amx_bf16, amx_int8) should be observed.", "Specifically, with a server having AMX instructions enabled, workloads performance can be further boosted by leveraging AMX."]}, {"name": "torch.compiler.compile()", "path": "generated/torch.compiler.compile#torch.compiler.compile", "type": "Miscellaneous", "text": ["See torch.compile() for details on the arguments for this function."]}, {"name": "torch.compiler.CUDAGraph Trees", "path": "torch.compiler_cudagraph_trees", "type": "Miscellaneous", "text": ["For a longer background on CUDAGraphs, read accelerating pytorch with CUDAGraphs.", "CUDA Graphs, which made its debut in CUDA 10, let a series of CUDA kernels to be defined and encapsulated as a single unit, i.e., a graph of operations, rather than a sequence of individually-launched operations. It provides a mechanism to launch multiple GPU operations through a single CPU operation, and hence reduces the launching overheads.", "CUDA Graphs can give large speedups, especially for models with high CPU overhead or small compute. There are a number of limitations from requiring the same kernels to be run with the same arguments and dependencies, and memory addresses.", "PyTorch provides a convenience wrapper around CUDAGraphs that handles a couple of tricky interactions with PyTorch\u2019s caching allocator.", "The CachingAllocator uses a separate memory pool for all the new allocations. During CUDAGraph recording, memory is accounted for, allocated, and freed exactly as during eager run. On replay, just the kernels are invoked, and there are no changes to the allocator. Subsequent to initial recording, the allocator does not know which memory is actively being used in user programs.", "Using a separate memory pool between eager allocations and cudagraph allocations may increase the memory of your program if there is substantial memory allocated to both.", "Make Graphed Callables is a PyTorch Abstraction to share a single memory pool over a series of callables. Graphed Callables takes advantage of the fact that on CUDA Graph recording, memory is exactly accounted for by the caching allocator to safely share memory between separate CUDA Graph recordings. In each invocation, outputs are preserved as live memory, preventing one callable from overwriting the live memory of another. Graphed Callables can only be invoked in a single order; memory addresses from the first run are burned into the second, and so forth.", "Running with cudagraph_trees=False does not reuse memory across separate graph captures, which can lead to large memory regressions. Even for a model that has no graph breaks, this has issues. The forward and backward are separate graph captures, so the memory pools for forward and backward are not shared. In particular, memory for activations that are saved in the forward cannot be reclaimed in the backward.", "Like Graph Callables, CUDA Graph Trees use a single memory pool across all graph captures. However, instead of requiring a single sequence of invocations, CUDA Graph Trees create separate trees of CUDA Graph captures. Let\u2019s take a look at an illustrative example:", "In this example, there are two separate paths that we make through the function: 1 -> 2 -> 4, or 1 -> 3 -> 4.", "We share all of the memory in a single memory pool between separate recordings by building up a tape of CUDA Graph recordings, in this instance, 1 -> 2 -> 4. We add invariants to ensure that memory is always in the same location as it were recorded, and no live tensors exist in user programs that might be overwritten.", "All of the memory is shared in a single memory pool, so there is no additional memory overhead compared to eager. Now, what happens if we were to hit a new path and run Graph 3?", "Graph 1 gets replayed, and then we hit Graph 3, which we have not yet recorded. On graph replays, the private memory pool is not updated, so y is not reflected in the allocator. Without care, we would overwrite it. To support reusing the same memory pool after replaying other graphs, we checkpoint the memory pool back to its state at the end of graph 1. Now that our live tensors are reflected in the caching allocator, we are safe to run a new graph.", "First, we would hit the optimized, CUDAGraph.replay() path that we have already recorded in graph 1. Then we would hit Graph 3. Just as before, we will need to warm up the graph once before recording. On the warmup run, the memory addresses are not fixed, so graph 4 will also fallback to the inductor, non-cudagraph invocation.", "The second time we hit graph 3 we are warmed up and ready to record. We record graph 2 and then record graph 4 again since the input memory addresses have changed. This creates a tree of CUDA Graph recordings. A CUDA Graph Tree!", "Because CUDA Graph fixes memory addresses, CUDA Graphs do not have a great way of handling live tensors from a previous invocation.", "Let\u2019s say we are benchmarking running inference with the following code:", "In the Separate CUDA Graph implementation, the output from the first invocation will be overwritten by the second invocation. Similarly, in CUDA Graph Trees, naively, the live output of the first run would force a dependency between the first run and the second run, and we would never hit the optimized cudagraph replay invocation. CUDA Graph Trees will ignore outputs from a previous run of torch.compile and not force a memory dependency. In training, we will not ignore outputs from a previous run of torch.compile if we have pending backwards that have not been invoked. TODO - add API to increment generation manually, error on access of prior storage", "Footguns", "Separate CudaGraph", "CUDAGraph Trees", "Memory Can Increase", "On each graph compilation (new sizes, etc.)", "If you are also running non-cudagraph memory", "Recordings", "On any new invocation of a graph", "Will re-record on any new, unique path you take through your program", "Footguns", "Invocation of one graph will overwrite prior invocation", "Cannot persist memory between separate runs through your model - one training loop training, or one run of inference"]}, {"name": "torch.compiler.Custom Backends", "path": "torch.compiler_custom_backends", "type": "Miscellaneous", "text": ["torch.compile provides a straightforward method to enable users to define custom backends.", "A backend function has the contract (gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]) -> Callable.", "Backend functions can be called by TorchDynamo, the graph tracing component of torch.compile, after tracing an FX graph and are expected to return a compiled function that is equivalent to the traced FX graph. The returned callable should have the same contract as the forward function of the original torch.fx.GraphModule passed into the backend: (*args: torch.Tensor) -> List[torch.Tensor].", "In order for TorchDynamo to call your backend, pass your backend function as the backend kwarg in torch.compile. For example,", "See below for more examples.", "You can register your backend using the register_backend decorator, for example,", "Besides the register_backend decorator, if your backend is in another python package, you could also register your backend through entry points of python package, which provides a way for a package to register a plugin for another one.", "Hint", "You can learn more about entry_points in the python packaging documentation.", "To register your backend through entry_points, you could add your backend function to the torch_dynamo_backends entry point group in the setup.py file of your package like:", "Please replace the my_compiler before = to the name of your backend\u2019s name and replace the part after = to the module and function name of your backend function. The entry point will be added to your python environment after the installation of the package. When you call torch.compile(model, backend=\"my_compiler\"), PyTorch would first search the backend named my_compiler that has been registered with register_backend. If not found, it will continue to search in all backends registered via entry_points.", "Registration serves two purposes:", "It is possible to define custom backends that are called by AOTAutograd rather than TorchDynamo. This is useful for 2 main reasons:", "Wrap your backend with torch._dynamo.optimizations.training.aot_autograd and use torch.compile with the backend kwarg as before. Backend functions wrapped by aot_autograd should have the same contract as before.", "Backend functions are passed to aot_autograd through the fw_compiler (forward compiler) or bw_compiler (backward compiler) kwargs. If bw_compiler is not specified, the backward compile function defaults to the forward compile function.", "One caveat is that AOTAutograd requires compiled functions returned by backends to be \u201cboxed\u201d. This can be done by wrapping the compiled function with functorch.compile.make_boxed_func.", "For example,", "If you want to better understand what is going on during a compilation, you can create a custom compiler, which is referred to as backend in this section, that will print pretty print the fx GraphModule extracted from Dynamo\u2019s bytecode analysis and return a forward() callable.", "For example:", "Running the above example produces the following output:", "This works for torch.nn.Module as well as shown below:", "Let\u2019s take a look at one more example with control flow:", "Running this example produces the following output:", "The order of the last two graphs is nondeterministic depending on which one is encountered first by the just-in-time compiler.", "Integrating a custom backend that offers superior performance is also easy and we\u2019ll integrate a real one with optimize_for_inference:", "And then you should be able to optimize any existing code with:", "TorchDynamo includes many backends, which can be found in backends.py or torch._dynamo.list_backends(). You can combine these backends together with the following code:"]}, {"name": "torch.compiler.disable()", "path": "generated/torch.compiler.disable#torch.compiler.disable", "type": "Miscellaneous", "text": ["This function provides both a decorator and a context manager to disable compilation on a function It also provides the option of recursively disabling called functions"]}, {"name": "torch.compiler.Frequently Asked Questions", "path": "torch.compiler_faq", "type": "Miscellaneous", "text": ["Author: Mark Saroufim", "torch.compile supports training, using AOTAutograd to capture backwards:", "torch.compile supports DistributedDataParallel (DDP). Support for other distributed training libraries is being considered.", "The main reason why Distributed code is challenging with dynamo is because AOTAutograd unrolls both the forward and backward pass and provides 2 graphs for backends to optimize. This is a problem for distributed code because we\u2019d like to ideally overlap communication operations with computations. Eager pytorch accomplishes this in different ways for DDP/FSDP- using autograd hooks, module hooks, and modifications/mutations of module states. In a naive application of dynamo, hooks that should run directly after an operation during backwards may be delayed until after the entire compiled region of backwards ops, due to how AOTAutograd compiled functions interact with dispatcher hooks.", "The basic strategy for optimizing DDP with Dynamo is outlined in distributed.py where the main idea will be to graph break on DDP bucket boundaries.", "When each node in DDP needs to synchronize its weights with the other nodes it organizes its gradients and parameters into buckets which reduces communication times and allows a node to broadcast a fraction of its gradients to other waiting nodes.", "Graph breaks in distributed code mean you can expect dynamo and its backends to optimize the compute overhead of a distributed program but not its communication overhead. Graph-breaks may interfere with compilation speedups, if the reduced graph-size robs the compiler of fusion opportunities. However, there are diminishing returns with increasing graph size since most of the current compute optimizations are local fusions. So in practice this approach may be sufficient.", "For the vast majority of models you probably don\u2019t and you can use torch.compile() as is but there are a few situations where full graphs are necessary and you can can ensure a full graph by simply running torch.compile(..., nopython=True). These situations include:", "Future work will include tracing communication operations into graphs, coordinating these operations with compute optimizations, and optimizing the communication operations.", "If your code ran just fine without torch.compile and started to crash with it is enabled, then the most important first step is figuring out which part of the stack your failure occurred. To troubleshoot that, follow the steps below and only try the next step if the previous one succeeded.", "In some cases, you may not want unexpected compiles after a program has warmed up. For example, if you are serving production traffic in a latency critical application. For this, TorchDynamo provides an alternate mode where prior compiled graphs are used, but no new ones are generated:", "There are 3 major ways to accelerate PyTorch code:", "The above are general principles for accelerating PyTorch code but different backends will each make different tradeoffs on what to optimize. For example Inductor first takes care of fusing whatever it can and only then generates Triton kernels. It can also", "Triton in addition offers speedups because of automatic memory coalescing, memory management and scheduling within each Streaming Multiprocessor and has been designed to handle tiled computations.", "However, regardless of the backend you use it\u2019s best to use a benchmark and see approach so try out the PyTorch profiler, visually inspect the generated kernels and try to see what\u2019s going on for yourself.", "The main reason you won\u2019t see the speedups you\u2019d like to by using dynamo is excessive graph breaks. So what\u2019s a graph break?", "Given a program like:", "Torchdynamo will attempt to compile all of the torch/tensor operations within some_fun() into a single FX graph, but it may fail to capture everything into one graph.", "Some graph break reasons are insurmountable to TorchDynamo like calling into a C extension other than PyTorch is invisible to TorchDynamo, and could do arbitrary things without TorchDynamo being able to introduce necessary guards to ensure that the compiled program would be safe to reuse.", "To maximize performance, it\u2019s important to have as few graph breaks as possible.", "To identify all graph breaks in a program and the associated reasons for the breaks, torch._dynamo.explain can be used. This tool runs TorchDynamo on the supplied function and aggregates the graph breaks that are encountered. Here is an example usage:", "To throw an error on the first graph break encountered you can use disable python fallback by using nopython=True, this should be familiar if you\u2019ve worked with export based compilers.", "If you enabled dynamic shapes by setting env TORCHDYNAMO_DYNAMIC_SHAPES=1 python model.py then your code won\u2019t recompile on shape changes. We\u2019ve added support for dynamic shapes which avoids recompilations in the case when shapes vary by less than a factor of 2. This is especially useful in scenarios like varying image sizes in CV or variable sequence length in NLP. In inference scenarios it\u2019s often not possible to know what a batch size will be beforehand because you take what you can get from different client apps.", "In general, TorchDynamo tries very hard not to recompile things unnecessarily so if for example TorchDynamo finds 3 graphs and your change only modified one graph then only that graph will recompile. So another tip to avoid potentially slow compilation times is to warmup a model by compiling it once after which subsequent compilations will be much faster. Cold start compile times is still a metric we track visibly.", "Accuracy issues can also be minified if you set the environment variable TORCHDYNAMO_REPRO_LEVEL=4, it operates with a similar git bisect model and a full repro might be something like TORCHDYNAMO_REPRO_AFTER=\"aot\" TORCHDYNAMO_REPRO_LEVEL=4 the reason we need this is downstream compilers will codegen code whether it\u2019s Triton code or the C++ backend, the numerics from those downstream compilers can be different in subtle ways yet have dramatic impact on your training stability. So the accuracy debugger is very useful for us to detect bugs in our codegen or with a backend compiler.", "If you\u2019d like to ensure that random number generation is the same across both torch and triton then you can enable torch._inductor.config.fallback_random = True", "Dynamo is still an alpha product so there\u2019s a few sources of OOMs and if you\u2019re seeing an OOM try disabling the following configurations in this order and then open an issue on GitHub so we can solve the root problem 1. If you\u2019re using dynamic shapes try disabling them, we\u2019ve disabled them by default: env TORCHDYNAMO_DYNAMIC_SHAPES=0 python model.py 2. CUDA graphs with Triton are enabled by default in inductor but removing them may alleviate some OOM issues: torch._inductor.config.triton.cudagraphs = False.", "Applying a torch.func transform to a function that uses torch.compile does not work:", "This code will not work. There is an issue that you can track for this.", "As a workaround, use torch.compile outside of the torch.func function:", "Note", "This is an experimental feature and can be used by setting torch._dynamo.config.capture_func_transforms=True", "There are currently a few cases which are not supported and lead to graph breaks (that is, torch.compile falls back to eager-mode PyTorch on these). We are working on improving the situation for the next release (PyTorch 2.2)", "1. The inputs and outputs of the function being transformed over must be tensors. We do not yet support things like tuple of Tensors.", "3. Functions with observable side effects. For example, it is OK to mutate a list created in the function, but not OK to mutate a list created outside of the function.", "Note", "\u2018stride\u2019, \u2018requires_grad\u2019, \u2018storage_offset\u2019, \u2018layout\u2019, \u2018data\u2019, \u2018is_coalesced\u2019, \u2018is_complex\u2019, \u2018is_conj\u2019, \u2018is_contiguous\u2019, \u2018is_cpu\u2019, \u2018is_cuda\u2019, \u2018is_distributed\u2019, \u2018is_floating_point\u2019, \u2018is_inference\u2019, \u2018is_ipu\u2019, \u2018is_leaf\u2019, \u2018is_meta\u2019, \u2018is_mkldnn\u2019, \u2018is_mps\u2019, \u2018is_neg\u2019, \u2018is_nested\u2019, \u2018is_nonzero\u2019, \u2018is_ort\u2019, \u2018is_pinned\u2019, \u2018is_quantized\u2019, \u2018is_same_size\u2019, \u2018is_set_to\u2019, \u2018is_shared\u2019, \u2018is_signed\u2019, \u2018is_sparse\u2019, \u2018is_sparse_csr\u2019, \u2018is_vulkan\u2019, \u2018is_xla\u2019, \u2018is_xpu\u2019", "For other transforms, as a workaround, use torch._dynamo.allow_in_graph", "allow_in_graph is an escape hatch. If your code does not work with torch.compile, which introspects Python bytecode, but you believe it will work via a symbolic tracing approach (like jax.jit), then use allow_in_graph.", "By using allow_in_graph to annotate a function, you must make sure your code meets the following requirements:", "A common pitfall is using allow_in_graph to annotate a function that invokes an nn.Module. This is because the outputs now depend on the parameters of the nn.Module. To get this to work, use torch.func.functional_call to extract the module state.", "Starting in 2.1, torch.compile understands native NumPy programs that work on NumPy arrays, and mixed PyTorch-NumPy programs that convert from PyTorch to NumPy and back via x.numpy(), torch.from_numpy, and related functions.", "NumPy within torch.compile follows NumPy 2.0 pre-release.", "Generally, torch.compile is able to trace through most NumPy constructions, and when it cannot, it falls back to eager and lets NumPy execute that piece of code. Even then, there are a few features where torch.compile semantics slightly deviate from those of NumPy:", "There are other features for which we do not support tracing and we gracefully fallback to NumPy for their execution:", "Yes you can! To do so, you may simply execute your code within a torch.device(\"cuda\") context. Consider the example", "In this example, numpy_fn will be executed in CUDA. For this to be possible, torch.compile automatically moves X and Y from CPU to CUDA, and then it moves the result Z from CUDA to CPU. If we are executing this function several times in the same program run, we may want to avoid all these rather expensive memory copies. To do so, we just need to tweak our numpy_fn so that it accepts cuda Tensors and returns tensors:", "By doing this, we explicitly create the tensors in CUDA memory, and we keep them there. In this case X.numpy() and from_numpy() are hints to the compiler but no real data movement happens. Note that the original program would not run on eager mode now. If you want to run it in eager mode, you would need to call .numpy(force=True) doing Z = Z.cuda() before returning Z. Of course, doing this would execute the program on eager mode NumPy, and on CPU.", "Debugging JIT compiled code is challenging, given the complexity of modern compilers and the daunting errors that they raise. The tutorial on how to diagnose runtime errors within torch.compile contains a few tips and tricks on how to tackle this task.", "If the above is not enough to pinpoint the origin of the issue, there are still a few other NumPy-specific tools we can use. We can discern whether the bug is entirely in the PyTorch code by disabling tracing through NumPy functions:", "If the bug lies in the traced NumPy code, we can execute the NumPy code eagerly (without torch.compile) using PyTorch as a backend by importing import torch._numpy as np. This should just be used for debugging purposes and is in no way a replacement for the PyTorch API, as it is much less performant and, as a private API, may change without notice. At any rate, torch._numpy is a Python implementation of NumPy in terms of PyTorch and it is used internally by torch.compile to transform NumPy code into Pytorch code. It is rather easy to read and modify, so if you find any bug in it feel free to submit a PR fixing it or simply open an issue.", "If the program does work when importing torch._numpy as np, chances are that the bug is in TorchDynamo. If this is the case, please feel open an issue with a minimal reproducer.", "The best place to start is the tutorial with general advice for how to debug these sort of torch.compile issues.", "Some graph breaks may happen because of the use of unsupported features. See Which NumPy features does torch.compile support?. More generally, it is useful to keep in mind that some widely used NumPy features do not play well with compilers. For example, in-place modifications make reasoning difficult within the compiler and often yield worse performance than their out-of-place counterparts.As such, it is best to avoid them. Same goes for the use of the out= parameter. Instead, prefer out-of-place ops and let torch.compile optimize the memory use. Same goes for data-dependent ops like masked indexing through boolean masks, or data-dependent control flow like if or while constructions.", "In some cases, you might need to exclude small parts of your code from the torch.compile compilations. This section provides some of the answers and you can find more information in TorchDynamo APIs for fine-grained tracing.", "Graph break on a function is not enough to sufficiently express what you want PyTorch to do. You need to be more specific about your use case. Some of the most common use cases you might want to consider:", "Some of the uncommon use cases include:", "Disallow-in-graph works at the level of operators, or more specifically, the operators that you see in the TorchDynamo extracted graphs.", "Disable works at the function frame level and decides if TorchDynamo should look into the function frame or not.", "Note", "torch._dynamo_skip is deprecated.", "You most likely need torch._dynamo.disable. But in an unlikely scenario, you might need even finer control. Suppose you want to disable the tracing on just the a_fn function, but want to continue the tracing back in aa_fn and ab_fn. The image below demonstrates this use case:", "In this case, you can use torch._dynamo.disable(recursive=False). In previous versions, this functionality was provided by torch._dynamo.skip. This is now supported by the recursive flag inside torch._dynamo.disable."]}, {"name": "torch.compiler.Getting Started", "path": "torch.compiler_get_started", "type": "Miscellaneous", "text": ["Before you read this section, make sure to read the torch.compiler.", "Let\u2019s start by looking at a simple torch.compile example that demonstrates how to use torch.compile for inference. This example demonstrates the torch.cos() and torch.sin() features which are examples of pointwise operators as they operate element by element on a vector. This example might not show significant performance gains but should help you form an intuitive understanding of how you can use torch.compile in your own programs.", "Note", "To run this script, you need to have at least one GPU on your machine. If you do not have a GPU, you can remove the cuda() code in the snippet below and it will run on CPU.", "A more famous pointwise operator you might want to use would be something like torch.relu(). Pointwise ops in eager mode are suboptimal because each one would need to read a tensor from the memory, make some changes, and then write back those changes. The single most important optimization that inductor performs is fusion. In the example above we can turn 2 reads and 2 writes into 1 read and 1 write which is crucial especially for newer GPUs where the bottleneck is memory bandwidth (how quickly you can send data to a GPU) rather than compute (how quickly your GPU can crunch floating point operations).", "Another major optimization that inductor provides is automatic support for CUDA graphs. CUDA graphs help eliminate the overhead from launching individual kernels from a Python program which is especially relevant for newer GPUs.", "TorchDynamo supports many different backends, but TorchInductor specifically works by generating Triton kernels. Let\u2019s save our example above into a file called example.py. We can inspect the code generated Triton kernels by running TORCH_COMPILE_DEBUG=1 python example.py. As the script executes, you should see DEBUG messages printed to the terminal. Closer to the end of the log, you should see a path to a folder that contains torchinductor_<your_username>. In that folder, you can find the output_code.py file that contains the generated kernel code similar to the following:", "Note", "The above code snippet is an example. Depending on your hardware, you might see different code generated.", "And you can verify that fusing the cos and sin did actually occur because the cos and sin operations occur within a single Triton kernel and the temporary variables are held in registers with very fast access.", "Read more on Triton\u2019s performance here. Because the code is written in Python, it\u2019s fairly easy to understand even if you have not written all that many CUDA kernels.", "Next, let\u2019s try a real model like resnet50 from the PyTorch hub.", "And that is not the only available backend, you can run in a REPL torch.compiler.list_backends() to see all the available backends. Try out the cudagraphs next as inspiration.", "PyTorch users frequently leverage pretrained models from transformers or TIMM and one of the design goals is TorchDynamo and TorchInductor is to work out of the box with any model that people would like to author.", "Let\u2019s download a pretrained model directly from the HuggingFace hub and optimize it:", "If you remove the to(device=\"cuda:0\") from the model and encoded_input, then Triton will generate C++ kernels that will be optimized for running on your CPU. You can inspect both Triton or C++ kernels for BERT. They are more complex than the trigonometry example we tried above but you can similarly skim through it and see if you understand how PyTorch works.", "Similarly, let\u2019s try out a TIMM example:", "In this section, we have reviewed a few inference examples and developed a basic understanding of how torch.compile works. Here is what you check out next:"]}, {"name": "torch.compiler.Guards Overview", "path": "torch.compiler_guards_overview", "type": "Miscellaneous", "text": ["From a UX perspective, TorchDynamo is very easy to use. The user invokes torchdynamo.optimize as an annotation:", "Where a complete example looks like this:", "This allows TorchDynamo to capture the interpreted Python frames, grab any and all relevant information, and speed things up wherever it can. The speedup comes from a few places, and can be rather dependent on the backend (my_compiler in the example above) provided, but the one speedup that is important in this section is caching. Caching itself is not a direct speedup but a critical enablement that prevents recompilation. We dig a hole with dynamo, and caching allows us to get out. It enables us to hold perf neutrality while then enabling backends - the true source of our speedups.", "With even a pass-through no-op backend provided:", "We can see TorchDynamo speeding up Python execution even on regular Python, not just PyTorch.", "TorchDynamo operates through caching transformed (by TorchDynamo) user bytecode. When TorchDynamo receives a frame for evaluation, it checks if the objects referenced in the frame have changed in certain ways, and if not, TorchDynamo reads the previously transformed user bytecode to evaluate it. In this section, we will focus on how we can identify whether or not the objects referenced in the frame have changed. This is a critical piece of functionality in TorchDynamo, because it drives the entire invalidation lifecycle. This functionality is called guards.", "At a very high level, the flow can be summarized like this:", "For the objects captured in (2), TorchDynamo creates tracking objects that are:", "The functionality of TorchDynamo is based on PEP 523.", "TorchDynamo installs a frame evaluation function on Python by using _PyInterpreterState_SetEvalFrameFunc. TorchDynamo has a hook where Python can hand control back to us during evaluation.", "The function we have installed is convert_frame or convert_frame_assert in the nopython=True case, but glossing over that nuance for now, let\u2019s take a look at convert_frame_assert, as convert_frame proxies to it.", "We can find it on line 222 of convert_frame.py,", "with a signature as follows:", "This function wraps the entry point of where Python invokes TorchDynamo with a frame:", "Here is what this function does:", "Passes the frame, alongside a function that creates an InstructionTranslator through bytecode transformation, via transform_code_object. A few crucial things happen under the hood here:", "Now that we have learned about frame evaluation, let\u2019s review InstructionTranslator, and see how it turns the frame we handed it over into TorchDynamo internal types.", "InstructionTranslator does a lot! We won\u2019t cover the details of everything it does, but most importantly for this document, it produces a mapping of symbolic_locals which maintains a mapping from the frame\u2019s f_locals to TorchDynamo internal Variable objects (more on these in a moment. symbolic_locals is filled via traversing the frame\u2019s locals:", "The important component here is the invocation of a call into VariableBuilder. VariableBuilder\u2019s call implementation proxies into a function called _wrap, which in turn both constructs instances of VariableTracker and calls make_guards on them. More on that later.", "This mapping, in turn, is critical as each Variable has associated guards, which are then passed to self.output, the instance of OutputGraph, an fx tracer, mentioned in 4.2 of the section above. If you recall, this OutputGraph, stored in a variable called output is where our guards are stored before being passed on to become GuardedCode", "How does InstructionTranslator do this? At the heart of it, there is a loop that is pumped, which drives a function step.", "step is just that - a single processing step, taking exactly one instruction and doing something with it.", "Note", "These are real instructions processed by TorchDynamo\u2019s transform_code_object, and it is pretty cool.", "Note", "This section purposely skips the details of dis.get_instructions.", "For the example above, here is a snippet of a what a few Instruction's may look like:", "This is the core functionality of this function. Take a look at the opname, and then take a look at this little snippet from inside step;", "As we can see, the function checks if the current class, the InstructionTranslator has an attribute set matching the operator name (for example, LOAD_CONST). If it does, the function invokes it, passing the whole instruction object in. If it does not, the function drops the frame as unimplemented.", "For the LOAD_CONST example, we can see that we do indeed support it, with a relatively straightforward definition:", "We can see that this function creates a new instance of the class ConstantVariable , with a value, in our example case, -1, and then pushes it onto the stack.", "There are dozens of such methods - see symbolic_convert.py for all of them. Generally, we implement as many matching methods to Python bytecode instructions as possible.", "Across both the logic downstream of step and the logic from invoking VariableBuilder - we now have a lot of VariableTrackers and of course, we\u2019ve spoken about creating guards quiet a bit. Let\u2019s dig into what Variables are, and get a little closer to understanding guards.", "A ConstantVariable is an instance of VariableTracker. VariableTracker represents a tracked Python local or stack value.", "When it comes to representing an object inside TorchDynamo, a VariableTracker does exactly what it says - it tracks a given variable. It is an extremely flexible class, but there are a few points to keep in mind:", "It manages the guard relationship around the underlying object through:", "It acts as a proxy on behalf of the underlying object, implementing methods for the rest of TorchDynamo to get information about the tracked object:", "And this class (VariableTracker) is built around subclassing, somewhere between a full Abstract Base Class and fully fleshed out class - it leaves many methods raising NotImplementedError - with reliance on subclasses. See torchdynamo/variables/ for all subclasses to fulfill contracts and custom behaviors.", "Knowing what we know now, we can see an example of how an instruction from dis, BUILD_TUPLE:", "BUILD_TUPLE(count) Creates a tuple consuming count items from the stack, and pushes the resulting tuple onto the stack.", "In our case, our signature will be a little different due to the way we create Instruction objects, but the gist of it will be the same. Instead of passing in count, we pass in an object with a little extra bookkeeping, and of course, we deal with turning regular old python objects into TorchDynamo notions:", "Here is what this code does:", "Note", "Where did the first guards come from? Propagation is a good technique, but we need something created before it can be propagated. VariableBuilder calls make_guards as it creates VariableTracker instances, from f_locals. This in turn calls into the source, to have it create guards.", "After all this, bytecode translation is done and we are one step closer to producing GuardedCode. We now understand how locals become VariableTrackers, how instructions are handled, and where guards are called on for creation. Before we can go into seeing how code and guards are combined into a GuardedCode object, we need to dig a little bit into those make_guard and source.make_guard calls above. We can then understand, what was going on when we made guards alongside, and on, VariableTracker instances.", "Guards are just Python objects, of the class Guard. Let\u2019s look at them in more detail.", "Looking at the definition of the dataclass (and therefore, ctor signature), we see that it has a name, a source, and a create function.", "The name should be the name of the variable.", "The source here is an enum indicating what kind of source the guard belongs to.", "Note", "Not to be confused with Source and the other types in source.py, as stored on VariableTracker.", "create_fn provides the main functionality to transition from a simple dataclass to actually producing valid Python code to be invoked for knowing whether or not things have changed in between invocations, and whether we can safely read from the code cache or not.", "The most common code paths for getting an instance of a guard are through make_guards on VariableTracker. make_guards -> source.make_guard -> return Guard(self.name(), self.guard_source(), fn)", "Or, in a concrete example:", "Since source was set at the construction time of this VariableTracker, all that was needed here was to provide the fn, GuardBuilder.EQUALS_MATCH to the create_fn field.", "This create_fn must be a method on GuardBuilder. The reason for this becomes apparent in our next step. Once we have all the guards created for a frame, we move on to CheckFunctionManager and compile_check_fn.", "Before the convert_frame function can produce a GuardedCode, it needs to run the CheckFunctionManager, with all the guards, to produce a check_fn which will then, in turn get passed in alongside the code into GuardedCode. This is the same check_fn that we store in our cache entry, and the same one we run to know whether or not to retrieve the code stored alongside. For reference, here is that code:", "We now know how a check_fn function is used, and who makes it, and what it is composed of, but what we do not yet know is how. How does a list of Guard objects become a function we can run later on?", "First, we iterate these guards:", "Calling guard.create runs that create_fn we set on the Guard class above (don\u2019t confuse it with the check_fn we are working on producing, the names are similar, so it can get a little confusing). In our example above, our create_fn is GuardBuilder.EQUALS_MATCH. So we are now invoking it, passing in the self, the guard itself, in.", "The signature is: def EQUALS_MATCH(self, guard: Guard):", "And internally to that function, we can use the name on the guard to get back our original object, querying it for data and type information, which in turn gets us to the most important bit: appending code.", "At its simplest, EQUALS_MATCH appends just one line of code: self.code.append(f\"{ref} == {val!r}\"). Where ref is the name of the variable, and val is the value. It might produce code like this:", "This is a basic example. But if we append a few other kinds of GuardBuilder functions and then combine them all with and in between each statement (as we do), we might get something like this:", "Here is what this code performs:", "This becomes the heart of the code our check_fn, which in turn is evaluated the next time we encounter this code. It will then check:", "If all of these are still true, then we can use the code cached alongside this check_fn.", "Note", "For a deeper dive for how and where this happens you can read static PyCodeObject *lookup(CacheEntry *e, PyObject *f_locals) { of _eval_frame.c.", "If not, then, we can move on to recompiling the code anew, and storing that in the cache alongside this code, and a whole new check_fn, again to be checked on yet another subsequent frame.", "There are lots of other such functions on GuardBuilder which get coalesced into, at times massive, strings which then get evaluated as Python code and stored into check_fn. The example above illustrates of a simple case. To understand this functionality better, read the other functions on GuardBuilder, or better yet, dump the code variable in compile_check_fn to see what is getting produced, especially on larger, real models.", "In this section, we have reviewed:", "We covered how user provided code wrapped in a TorchDynamo context goes on to get traced and tracked internally, organized into VariableTrackers Sources and subsequently Guards, and how those Guards in turn guide cache entry selection and invalidation when handing Python code."]}, {"name": "torch.compiler.list_backends()", "path": "generated/torch.compiler.list_backends#torch.compiler.list_backends", "type": "Miscellaneous", "text": ["Return valid strings that can be passed to torch.compile(\u2026, backend=\u201dname\u201d).", "exclude_tags (optional) \u2013 A tuple of strings representing tags to exclude.", "List[str]"]}, {"name": "torch.compiler.Profiling to understand torch.compile performance", "path": "torch.compiler_profiling_torch_compile", "type": "Miscellaneous", "text": ["torch.profiler is helpful for understanding the performance of your program at a kernel-level granularity - for example, it can show graph breaks and GPU utilization at the level of the program. The data provided by the profiler can often help users understand where to investigate further to understand model performance.", "To understand kernel-level performance, other toosl exist. NVIDIA\u2019s ncu tool can be used, or inductor\u2019s profiling tools.", "See also the general pytorch profiler guide.", "Example program: We\u2019ll use this example of profiling resnet18. Notice the following parts of this example program:", "Viewing chrome traces: In the Chrome browser, open chrome://tracing and load the json file. Use the \u201cw\u201d and \u201cs\u201d keys to zoom in and out, and use \u201ca\u201d and \u201cd\u201d to scroll left and right. \u201c?\u201d will show a \u201chelp\u201d screen with a list of shortcuts.", "Here, we observe: * CompiledFunction and CompiledFunctionBackward events, which correspond to the dynamo-compiled regions. * CPU events at the top, and GPU events at the bottom.", "Flows between CPU and GPU events", "Every kernel on the GPU occurs after being launched by code running on the CPU. The profiler can draw connections (i.e. \u201cflows\u201d) between the GPU and CPU events to show which CPU event launched a GPU kernel. This is particularly helpful because, with a few exceptions, GPU kernels are launched asynchronously.", "To view a flow connection, click on a GPU kernel and click \u201cac2g\u201d:", "Alternatively, turn on all flows with the \u201cFlow events\u201d dropdown at the top.", "When CUDA graphs are enabled, some cuda configurations (driver version under 525.85.12 or CUDA < 12) can encounter issues between the profiling tools and CUDA graphs. To fix these issues, add an empty profiling context at the top of your program:", "To understand why compilation is taking a long time, you can profile the first invocation of a torch.compile-ed program. Keep in mind that profile traces of compilations can be distorted more than typical profiling, because compilation workloads can be quite different from typical PyTorch workloads. In some cases, trace files may also be quite large. Traces > 1GB can be difficult to open with the chrome tracing tool.", "Note: roughly the same information can also be obtained in non-graphical format with torch._dynamo.utils.compile_times(). This utility won\u2019t show when the compilation steps occur, but it will show the amount of time spent on each step - and times will not be affected by any profiling overhead.", "See an example below:", "Note a few things:", "Although there are logging tools for identifying graph breaks, the profiler provides a quick visual method of identifying graph breaks.", "When gradients are required for any inputs, graph breaks are easy to identify: each graph break will interrupt a CompiledFunction block, splitting it in two.", "See the synthetic example below for a demonstration:", "One common issue is bad GPU utilization. A quick way to identify this is if there are large gaps between kernels on the GPU:", "This is often the result of CPU overhead, e.g. if the amount of time spent on the CPU between kernel launches is larger than the amount of time spent by the GPU to process the kernels. The issue is more common for small batch sizes.", "When using inductor, enabling CUDA graphs can often help improve performance when launch overhead is a concern."]}, {"name": "torch.compiler.PyTorch 2.0 NNModule Support", "path": "torch.compiler_nn_module", "type": "Miscellaneous", "text": ["Author: Will Constable", "torch.compile has special handling for torch.nn.Module objects, tracing them differently than it traces arbitrary python classes, with the intent of producing faster code by making assumptions about the structure.", "This doc describes some of the tradeoffs or edge cases that come up due to this specialization.", "Previously, torch.compile had no support for hooks on nn.Modules, and if hooks were registered they would simply be ignored in the compiled program. Indeed many users do not use nn.Module hooks at all, or only use them for debug workflows, but there are valid use cases for composing nn.Module hooks with torch.compile.", "Hooks that are orchestrated via nn.Module.__call__ implementation include _forward_pre_hooks, forward_hooks, _backward_pre_hooks, and _backward_hooks, and will be referred to as \u2018call hooks\u2019. These hooks are partially supported by torch.compile with limitations described below.", "Another category of hooks includes _state_dict_hooks and its pre and load_ variants, and are still unsupported by torch.compile.", "By default, torch.compile will trace the contents of nn.Module.__call__ which means it will encounter and run forward/pre-forward hooks. If you install hooks before calling torch.compile and then do not remove or alter the hooks later, your use case should be supported by default.", "Backward/Pre-backward hooks are generally also supported, with similar caveats: currently graph-breaks in dynamo occur when accessing backward_hooks dicts, which is probably avoiable with some work. Graph-breaks also impact the timing of firing backward hooks, since graph-segments are run as autograd-functions which produce all their grads at the same time. Assuming it were possible for dynamo to not graph-break on the presence of backward-hooks, we would still expect the backward hooks for a series of modules to all fire together after the whole compiled graph\u2019s backward ran.", "hooks on \u2018allowed modules\u2019 torch.compile treats common modules such as torch.conv, as well as modules that are difficult to trace, specially by allowing them to be called opaquely in the dynamo graph instead of traced into by dynamo. For such modules, hooks currently trigger a graph-break so that the affected modules run outside of dynamo. Depending on the model, this could introduce a significant performance regression, and additional work is required to improve this support.", "skip_nnmodule_hook_guards By default, torch._dynamo.config.skip_nnmodule_hook_guards is set to True, meaning no guards will be installed on each nn.Module hook dictionary, improving runtime by reducing guard execution time, at the cost of not noticing if any hook dict is changed after compilation.", "If you want to be able to remove or modify hooks after compilation and have torch.compile react appropriately (by recompiling), then you need to set skip_nnmodule_hook_guards=False and expect a runtime penalty for the added guards.", "TODO: confirm if backward/pre_backward hooks are working or not and document accordingly", "State dict hooks have not yet been supported in torch.compile.", "TODO: warn_once if graph-breaking on hooks. warn_once to point to this doc if hooks are present."]}, {"name": "torch.compiler.PyTorch 2.0 Performance Dashboard", "path": "torch.compiler_performance_dashboard", "type": "Miscellaneous", "text": ["Author: Bin Bao and Huy Do", "PyTorch 2.0\u2019s performance is tracked nightly on this dashboard. The performance collection runs on 12 GCP A100 nodes every night. Each node contains a 40GB A100 Nvidia GPU and a 6-core 2.2GHz Intel Xeon CPU. The corresponding CI workflow file can be found here.", "The landing page shows tables for all three benchmark suites we measure, TorchBench, Huggingface, and TIMM, and graphs for one benchmark suite with the default setting. For example, the default graphs currently show the AMP training performance trend in the past 7 days for TorchBench. Droplists on the top of that page can be selected to view tables and graphs with different options. In addition to the pass rate, there are 3 key performance metrics reported there: Geometric mean speedup, Mean compilation time, and Peak memory footprint compression ratio. Both Geometric mean speedup and Peak memory footprint compression ratio are compared against the PyTorch eager performance, and the larger the better. Each individual performance number on those tables can be clicked, which will bring you to a view with detailed numbers for all the tests in that specific benchmark suite.", "All the dashboard tests are defined in this function. The exact test configurations are subject to change, but at the moment, we measure both inference and training performance with AMP precision on the three benchmark suites. We also measure different settings of TorchInductor, including default, with_cudagraphs (default + cudagraphs), and dynamic (default + dynamic_shapes).", "Individual dashboard runs can be triggered manually by clicking the Run workflow button here and submitting with your PR\u2019s branch selected. This will kick off a whole dashboard run with your PR\u2019s changes. Once it is done, you can check the results by selecting the corresponding branch name and commit ID on the performance dashboard UI. Be aware that this is an expensive CI run. With the limited resources, please use this functionality wisely.", "The exact command lines used during a complete dashboard run can be found in any recent CI run logs. The workflow page is a good place to look for logs from some of the recent runs. In those logs, you can search for lines like python benchmarks/dynamo/huggingface.py --performance --cold-start-latency --inference --amp --backend inductor --disable-cudagraphs --device cuda and run them locally if you have a GPU working with PyTorch 2.0. python benchmarks/dynamo/huggingface.py -h will give you a detailed explanation on options of the benchmarking script."]}, {"name": "torch.compiler.PyTorch 2.0 Troubleshooting", "path": "torch.compiler_troubleshooting", "type": "Miscellaneous", "text": ["Author: Michael Lazos", "We are actively developing debug tools, profilers, and improving our error and warning messages. Below is a table of the available tools and their typical usage. For additional help see Diagnosing Runtime Errors.", "Tool", "Purpose", "Usage", "Info logging", "View summarized steps of compilation", "torch._logging.set_logs(dynamo = logging.INFO) or TORCH_LOGS=\"dynamo\"", "Debug logging", "View detailed steps of compilation (print every instruction traced)", "torch._logging.set_logs(dynamo = logging.DEBUG) and torch._dynamo.config.verbose = True, or TORCH_LOGS=\"+dynamo\" TORCHDYNAMO_VERBOSE=1", "Minifier for any backend", "Find smallest subgraph which reproduces errors for any backend", "set environment variable TORCHDYNAMO_REPRO_AFTER=\"dynamo\"", "Minifier for TorchInductor", "If the error is known to occur after AOTAutograd find smallest subgraph which reproduces errors during TorchInductor lowering", "set environment variable TORCHDYNAMO_REPRO_AFTER=\"aot\"", "Dynamo accuracy minifier", "Finds the smallest subgraph which reproduces an accuracy issue between an eager mode model and optimized model, when you suspect the problem is in AOTAutograd", "TORCHDYNAMO_REPRO_AFTER=\"dynamo\" TORCHDYNAMO_REPRO_LEVEL=4", "Inductor accuracy minifier", "Finds the smallest subgraph which reproduces an accuracy issue between an eager mode model and optimized model, when you suspect the problem is in the backend (e.g., inductor). If this doesn\u2019t work, try the Dynamo accuracy minifier instead.", "TORCHDYNAMO_REPRO_AFTER=\"aot\" TORCHDYNAMO_REPRO_LEVEL=4", "torch._dynamo.explain", "Find graph breaks and display reasoning for them", "torch._dynamo.explain(fn)(*inputs)", "Record/Replay", "Record and replay frames which to reproduce errors during graph capture", "torch._dynamo.config.replay_record_enabled = True", "TorchDynamo function name filtering", "Only compile functions with the given name to reduce noise when debugging an issue", "set environment variable TORCHDYNAMO_DEBUG_FUNCTION=<name>", "TorchInductor Debug logging", "Print general TorchInductor debug info and generated Triton/C++ code", "torch._inductor.config.debug = True", "TorchInductor Tracing", "Show time taken in each TorchInductor stage + output code and graph visualization", "set the environment variable TORCH_COMPILE_DEBUG=1 or torch._inductor.config.trace.enabled = True", "In addition to info and debug logging, you can use torch._logging for more fine-grained logging.", "At a high level, the TorchDynamo stack consists of a graph capture from Python code (TorchDynamo) and a backend compiler. For example, a backend compiler may consist of backward graph tracing (AOTAutograd) and graph lowering (TorchInductor)*. Errors can occur in any component of the stack and will provide full stack traces.", "To determine in which component an error occurred, you may use info-level logging torch._logging.set_logs(dynamo = logging.INFO) or TORCH_LOGS=\"dynamo\" and look for Step #: ... outputs. Logs are made at the beginning and end of each step, so the step that an error should correspond to is the most recently logged step whose end has not yet been logged. The steps correspond to the following parts of the stack:", "Step", "Component", "1", "TorchDynamo", "2", "Compiler Backend", "3", "TorchInductor", "If info logging is insufficient, you can use available backend options. These options include:", "The general procedure to narrow down an issue is the following:", "Each of these cases are analyzed in the following sections.", "Note", "The TorchInductor backend consists of both AOTAutograd tracing and the TorchInductor compiler itself. We will disambiguate by referring to TorchInductor as the backend, and TorchInductor lowering as the phase which lowers the graph traced by AOTAutograd.", "If the error that is generated occurs with the \"eager\" backend, then TorchDynamo is most likely the source of the error. Here is a sample code which will generate an error.", "The code above generates the following error:", "As the message suggests you can set torch._dynamo.config.verbose=True to get a full stack trace to both the error in TorchDynamo and the user code. In addition to this flag, you can also set the log_level of TorchDynamo through torch._dynamo.config.log_level. These levels include:", "If a model is very large, the logs can become overwhelming. If an error occurs deep within a model\u2019s Python code, it can be useful to execute only the frame in which the error occurs to enable easier debugging. There are two tools available to enable this:", "If the error does not occur with the \"eager\" backend, then the backend compiler is the source of the error (example error). There are different choices for backend compilers for TorchDynamo, with TorchInductor fitting the needs of most users. This section focuses on TorchInductor as the motivating example, but some tools can also be used with other backend compilers.", "Below is the portion of the stack which we are focusing on:", "With TorchInductor as the chosen backend, AOTAutograd is used to generate the backward graph from the forward graph captured by torchdynamo. It is important to note that errors can occur during this tracing and also while TorchInductor lowers the forward and backward graphs to GPU code or C++. A model can often consist of hundreds or thousands of FX nodes, so narrowing the exact nodes where this problem occurred can be very difficult. Fortunately, there are tools available to automatically minify these input graphs to the nodes which are causing the issue. The first step is to determine whether the error occurs during tracing of the backward graph with AOTAutograd or during TorchInductor lowering. As mentioned above in step 2, the \"aot_eager\" backend can be used to run only AOTAutograd in isolation without lowering. If the error still occurs with this backend, this indicates that the error is occurring during AOTAutograd tracing.", "Here is an example:", "Running this should give you this error with a longer stack trace below it:", "error with full stack trace", "If you then change torch.compile(backend=\"inductor\") to torch.compile(backend=\"aot_eager\"), it will run without error, because the issue is in the TorchInductor lowering process, not in AOTAutograd.", "From here, let\u2019s run the minifier to get a minimal repro. Setting the environment variable TORCHDYNAMO_REPRO_AFTER=\u201caot\u201d (or setting torch._dynamo.config.repro_after=\"aot\" directly) will generate a Python program which reduces the graph produced by AOTAutograd to the smallest subgraph which reproduces the error. (See below for an example where we minify the graph produced by TorchDynamo) Running the program with this environment variable should show nearly identical output, with an additional line indicating where minifier_launcher.py has been written to. The output directory is configurable by setting torch._dynamo.config.base_dir to a valid directory name. The final step is to run the minifier and check that it runs successfully. A successful run looks like this. If the minifier runs successfully, it generates runnable python code which reproduces the exact error. For our example this is the following code:", "The forward method of the Repro module contains the exact op which causes the issue. When filing an issue, please include any minified repros to aid in debugging.", "With backend compilers other than TorchInductor the process for finding the subgraph causing the error is nearly identical to the procedure in errors in TorchInductor with one important caveat. Namely, that the minifier will now be run on the graph that is traced by TorchDynamo, not the output graph of AOTAutograd. Let\u2019s walk through an example.", "In order to run the code after TorchDynamo has traced the forward graph, you can use the TORCHDYNAMO_REPRO_AFTER environment variable. Running this program with TORCHDYNAMO_REPRO_AFTER=\u201cdynamo\u201d (or torch._dynamo.config.repro_after=\"dynamo\") should produce this outputand the following code in {torch._dynamo.config.base_dir}/repro.py.", "Note", "The other option for TORCHDYNAMO_REPRO_AFTER is \"aot\", which will run the minifier after the backward graph has been generated.", "The minifier successfully reduced the graph to the op that raises the error in toy_compiler. The other difference from the procedure in TorchInductor Errors is that the minifier is automatically run after encountering a backend compiler error. After a successful run, the minifier writes repro.py to torch._dynamo.config.base_dir.", "TorchDynamo has a built-in stats function for collecting and displaying the time spent in each compilation phase. These stats can be accessed by calling torch._dynamo.utils.compile_times() after executing Torch._Dynamo. By default, this returns a string representation of the compile times spent in each TorchDynamo function by name.", "TorchInductor has a builtin stats and trace function for displaying time spent in each compilation phase, output code, output graph visualization and IR dump. This is a debugging tool designed to make it easier to understand and troubleshoot the internals of TorchInductor.", "Let\u2019s run an example with the following test program (repro.py):", "Setting the environment variable TORCH_COMPILE_DEBUG=1 will cause a debug trace directory to be created, by default this directory will be in the current directory and named torch_compile_debug (this can be overridden in the torchdynamo configuration field debug_dir_root and also the env var TORCH_COMPILE_DEBUG_DIR). Inside this directory, each run will have a separate folder named with the timestamp and process id of the run:", "In the run folder there will be a torchdynamo directory which contains debug logs, and an torchinductor folder which contains a subfolder for each compiled kernel with inductor debug artifacts.", "Moving further into the torchinductor directory, the \\*.log files are logs from the AOT Autograd phase of compilation, model__0_forward_1.0 contains the inductor debug artifacts.", "Here is a summary of the contents:", "Here are example debug directory contents for the test program:", "Each file in that debug trace can be enabled and disabled through torch._inductor.config.trace.*. The profile and the diagram are both disabled by default since they are expensive to generate.", "A single node in this new debug format looks like:", "See the example debug directory output for more examples.", "Given a program like this:", "TorchDynamo will attempt to compile all of the torch/tensor operations within some_fun into a single FX graph, but it may fail to capture everything into one graph.", "Some graph break reasons are insurmountable to TorchDynamo, and can\u2019t be easily fixed. - calling into a C extension other than torch is invisible to torchdynamo, and could do arbitrary things without TorchDynamo being able to introduce necessary guards to ensure that the compiled program would be safe to reuse. Graph breaks can hinder performance if the resulting fragments are small. To maximize performance, it\u2019s important to have as few graph breaks as possible.", "To identify all graph breaks in a program and the associated reasons for the breaks, torch._dynamo.explain can be used. This tool runs TorchDynamo on the supplied function and aggregates the graph breaks that are encountered. Here is an example usage:", "Outputs include:", "To throw an error on the first graph break encountered, use the nopython mode. This mode disables TorchDynamo\u2019s Python fallback, and only succeeds if the entire program is convertible into a single graph. Example usage:", "When TorchDynamo compiles a function (or part of one), it makes certain assumptions about locals and globals in order to allow compiler optimizations, and expresses these assumptions as guards that check particular values at runtime. If any of these guards fail, Dynamo will recompile that function (or part) up to torch._dynamo.config.cache_size_limit times. If your program is hitting the cache limit, you will first need to determine which guard is failing and what part of your program is triggering it.", "The compile profiler automates the process of setting TorchDynamo\u2019s cache limit to 1 and running your program under an observation-only \u2018compiler\u2019 that records the causes of any guard failures. You should be sure to run your program for at least as long (as many iterations) as you were running when you ran into trouble, and the profiler will accumulate statistics over this duration.", "If your program exhibits a bounded amount of dynamism, you may be able to tune the TorchDynamo cache limit to allow for each variation to be compiled and cached, but if the cache limit is too high you may find the cost of recompilation outweighs any optimization benefits.", "TorchDynamo plans to support many common cases of dynamic tensor shapes, such as varying batch size or sequence length. It does not plan to support rank-dynamism. In the meantime, setting a specific cache limit can be used in coordination with bucketing techniques to achieve an acceptable number of recompilations for some dynamic models.", "Accuracy issues can also be minified if you set the environment variable TORCHDYNAMO_REPRO_LEVEL=4, it operates with a similar git bisect model and a full repro might be something like TORCHDYNAMO_REPRO_AFTER=\"aot\" TORCHDYNAMO_REPRO_LEVEL=4 the reason we need this is downstream compilers will codegen code whether it\u2019s Triton code or the C++ backend, the numerics from those downstream compilers can be different in subtle ways yet have dramatic impact on your training stability. So the accuracy debugger is very useful for us to detect bugs in our codegen or with a backend compiler.", "If you\u2019d like to ensure that random number generation is the same across both torch and triton then you can enable torch._inductor.config.fallback_random = True"]}, {"name": "torch.compiler.reset()", "path": "generated/torch.compiler.reset#torch.compiler.reset", "type": "Miscellaneous", "text": ["This function clears all compilation caches and restores the system to its initial state. It is recommended to call this function, especially after using operations like torch.compile(\u2026) to ensure a clean state before another unrelated compilation"]}, {"name": "torch.compiler.torch.compiler API reference", "path": "torch.compiler_api", "type": "Miscellaneous", "text": ["For a quick overview of torch.compiler, see torch.compiler.", "See torch.compile() for details on the arguments for this function.", "This function clears all compilation caches and restores the system to its initial state.", "Customize which functions compilation will include in the generated graph.", "This function is used to mark a function fn as having a constant result.", "Return valid strings that can be passed to torch.compile(..., backend=\"name\").", "This function provides both a decorator and a context manager to disable compilation on a function It also provides the option of recursively disabling called functions"]}, {"name": "torch.compiler.torch.compiler API reference.torch.compiler.allow_in_graph", "path": "generated/torch.compiler.allow_in_graph", "type": "Miscellaneous", "text": ["Customize which functions compilation will include in the generated graph. It bypasses all introspection of the symbolic python code in favor of directly writing it to the graph. If fn is a list or tuple of callables it recursively applies allow_in_graph() to each function and returns a new list or tuple containing the modified functions", "fn \u2013 A callable representing the function to be included in the graph.", "Warning", "allow_in_graph() skips TorchDynamo completely on the decorated function skipping all TorchDynamo safety checks (graph breaks, handling closures, etc). Therefore, one has to be very careful with allow_in_graph() since subsystems like AOT Autograd rely on torchdynamo If not careful, this could lead to soundness and really hard-to-debug issues."]}, {"name": "torch.compiler.torch.compiler API reference.torch.compiler.assume_constant_result", "path": "generated/torch.compiler.assume_constant_result", "type": "Miscellaneous", "text": ["This function is used to mark a function fn as having a constant result. This allows the compiler to optimize away your function Returns The same function fn", "fn \u2013 The function to be marked as having a constant result.", "Warning", "assume_constant_result can if invalid cause safety and soundness issues, torch.compile() will not attempt to validate whether the constant assumption is true or not"]}, {"name": "torch.compiler.torch.compiler API reference.torch.compiler.compile", "path": "generated/torch.compiler.compile", "type": "Miscellaneous", "text": ["See torch.compile() for details on the arguments for this function."]}, {"name": "torch.compiler.torch.compiler API reference.torch.compiler.disable", "path": "generated/torch.compiler.disable", "type": "Miscellaneous", "text": ["This function provides both a decorator and a context manager to disable compilation on a function It also provides the option of recursively disabling called functions"]}, {"name": "torch.compiler.torch.compiler API reference.torch.compiler.list_backends", "path": "generated/torch.compiler.list_backends", "type": "Miscellaneous", "text": ["Return valid strings that can be passed to torch.compile(\u2026, backend=\u201dname\u201d).", "exclude_tags (optional) \u2013 A tuple of strings representing tags to exclude.", "List[str]"]}, {"name": "torch.compiler.torch.compiler API reference.torch.compiler.reset", "path": "generated/torch.compiler.reset", "type": "Miscellaneous", "text": ["This function clears all compilation caches and restores the system to its initial state. It is recommended to call this function, especially after using operations like torch.compile(\u2026) to ensure a clean state before another unrelated compilation"]}, {"name": "torch.compiler.TorchDynamo APIs for fine-grained tracing", "path": "torch.compiler_fine_grain_apis", "type": "Miscellaneous", "text": ["Note", "In this document torch.compiler.compile and torch.compile are used interchangeably. Both versions will work in your code.", "torch.compile performs TorchDynamo tracing on the whole user model. However, it is possible that a small part of the model code cannot be handeled by torch.compiler. In this case, you might want to disable the compiler on that particular portion, while running compilation on the rest of the model. This section describe the existing APIs that use to define parts of your code in which you want to skip compilation and the relevant use cases.", "The API that you can use to define portions of the code on which you can disable compilation are listed in the following table:", "API", "Description", "When to use?", "torch.compiler.disable", "Disables Dynamo on the decorated function as well as recursively invoked functions.", "Excellent for unblocking a user, if a small portion of the model cannot be handeled with torch.compile.", "torch._dynamo.disallow_in_graph", "Disallows the marked op in the TorchDynamo graph. TorchDynamo causes graph break, and runs the op in the eager (no compile) mode.nnThis is suitable for the ops, while torch.compiler.disable is suitable for decorating functions.", "This API is excellent for both debugging and unblocking if a custom op like torch.ops.fbgemm.* is causing issues with the torch.compile function.", "torch.compile.allow_in_graph", "The annotated callable goes as is in the TorchDynamo graph. For example, a black-box for TorchDynamo Dynamo.nnNote that AOT Autograd will trace through it, so the allow_in_graph is only a Dynamo-level concept.", "This API is useful for portions of the model which have known TorchDynamo hard-to-support features, like hooks or autograd.Function. However, each usage of allow_in_graph must be carefully screened (no graph breaks, no closures).", "torch._dynamo.graph_break", "Adds a graph break. The code before and after the graph break goes through TorchDynamo.", "Rarely useful for deployment - If you think you need this, most probably you need either disable or disallow_in_graph.", "torch.compiler.disable disables compilation on the decorated function frame and all the function frames recursively invoked from the decorated function frame.", "TorchDynamo intercepts the execution of each Python function frame. So, suppose you have a code structure (image below) where the function fn calls functions a_fn and b_fn. And a_fn calls aa_fn and ab_fn. When you use the PyTorch eager mode rather than torch.compile, these function frames run as is. With torch.compile, TorchDynamo intercepts each of these function frames (indicated by the green color):", "Let\u2019s imagine, that function a_fn is causing troubles with torch.compile. And this is a non-critical portion of the model. You can use compiler.disable on function a_fn. As shown above, TorchDynamo will stop looking at frames originating from the a_fn call (white color indicates original Python behavior).", "To skip compilation, you can decorate the offending function with @torch.compiler.disable.", "You can also use the non-decorator syntax if you don\u2019t want to change the source code However, we recommend that you avoid this style if possible. Here, you have to take care that all users of the original function are now using the patched version.", "torch._dynamo.disallow_in_graph disallows an operator but not the function to be present in the TorchDynamo extracted graph. Note that this is suitable for operators and not general functions as in the case of _dynamo.disable.", "Let\u2019s imagine you compile your model with PyTorch. TorchDynamo is able to extract a graph, but then you see the downstream compiler failing. For example, the meta kernel is missing, or some Autograd dispatch key is set incorrectly for a particular operator. Then you can mark that operator as disallow_in_graph, and TorchDynamo will cause a graph break and run that operator by using the PyTorch eager mode.", "The catch is that you will have to find the corresponding Dynamo level operator, and not the ATen level operator. See more in the Limitations section of the doc.", "Warning", "torch._dynamo.disallow_in_graph is a global flag. If you are comparing different backend compilers, you might have to call allow_in_graph for the disallowed operator when switching to the other compiler.", "torch.compiler.allow_in_graph is useful when the relevant function frame has some known hard-to-support TorchDynamo feature, such as hooks and autograd.Function, and you are confident that downstream PyTorch components such as AOTAutograd can safely trace through the decorated function. When a function is decorated with allow_in_graph, TorchDynamo treats it as a black-box and puts it as is in the generated graph.", "Warning", "allow_in_graph skips TorchDynamo completely on the decorated function omitting all TorchDynamo safety checks, including graph breaks, handling closures, and others. Use allow_in_graph with caution. PyTorch downstream components, such as AOTAutograd rely on TorchDynamo to handle complex Python features, but allow_in_graph bypasses TorchDynamo. Using allow_in_graph could lead to soundness and hard-to-debug issues.", "All the existing APIs are applied at the TorchDynamo level. Therefore, these APIs have visibility to only what TorchDynamo sees. This can lead to confusing scenarios.", "For example, torch._dynamo.disallow_in_graph will not work for ATen operators because they are visible to AOT Autograd. For example, torch._dynamo.disallow_in_graph(torch.ops.aten.add) will not work in the above example."]}, {"name": "torch.compiler.TorchInductor GPU Profiling", "path": "torch.compiler_inductor_profiling", "type": "Miscellaneous", "text": ["This section lists useful commands and workflows that can help you dive into a model\u2019s performance in TorchInductor. When a model is not running as fast as expected, you may want to check individual kernels of the model. Usually, those kernels taking the majority of the GPU time are the most interesting ones. After that, you may also want to run individual kernels directly and inspect its perf. PyTorch provides tools to cover everything mentioned above.", "You can use the following environment variables in your analysis:", "TORCHINDUCTOR_UNIQUE_KERNEL_NAMES", "TORCHINDUCTOR_BENCHMARK_KERNEL", "TORCHINDUCTOR_MAX_AUTOTUNE", "Below are the steps to breakdown execution time of a model into individual kernels. We take mixnet_l as an example.", "Run the benchmark script for the model:", "Note", "The tool relies on kernel name to decide its category. Enabling TORCHINDUCTOR_UNIQUE_KERNEL_NAMES is crucial for that.", "In the output log, look for lines:", "We have one line for each compiled module. If there are no extra graph breaks, we would see 2 such lines in the log, one for the forward graph and one for the backward graph.", "For our example command, we get the following compiled module for the forward and backward graphs respectively:", "Now we can dive into the perf for each individual compiled module. Let\u2019s pick the one for the forward graph for illustration purposes. I\u2019ll name it fwd.py for convenience. Run it directly with the -p argument:", "See the full output log in this example gist.", "In the output, you can notice the following:", "Chrome trace for the profile is written to /tmp/compiled_module_profile.json", "Loading the trace into Chrome (visit chrome://tracing in the chrome browser and load the file as the UI suggested) will show UI as follows:", "You can zoom in and out to check the profile.", "We report the percent of GPU time regarding to the wall time by log line like:", "Percent of time when GPU is busy: 102.88%", "Sometimes you may see a value larger than 100%. The reason is because PyTorch uses the kernel execution time with profiling enabled while using wall time with profiling disabled. Profiling may distort the kernel execution time a bit. But overall it should not be a big deal.", "If we run the model like densenet121 with a small batch size, we would see low percent of time when GPU is busy:", "This means the model has a lot of CPU overhead. This is consistent with the fact that enabling cudagraphs improve densenet121\u2019s perf a lot.", "We can break down the GPU time to different categories of kernels. In the mixnet_l example, we see", "This information can be found in the summary line (last line) of the report for each kernel category.", "We also call zoom into a certain category of kernels. For example, let\u2019s check reduction kernels:", "We can see an ordered table of execution time for each individual reduction kernel. We also see how many times a kernel is executed. This is helpful for a few reasons:", "Let\u2019s say we want to take a closer look at triton_red_fused\\__native_batch_norm_legit_functional_16 which is the most expensive reduction kernel and takes 2.19% of overall wall time for the forward graph.", "We can lookup the kernel name in the fwd.py, and find comment like:", "# kernel path: /tmp/torchinductor_shunting/jk/cjk2vm3446xrk7rth7hr6pun7xxo3dnzubwcn6ydrpifal4eykrz.py", "I\u2019ll rename it k.py for convenience. Here is a paste for this file.", "k.py is a standalone Python module containing the kernel code and its benchmark.", "Run k.py directly will report its execution time and bandwidth:", "We can check if max-autotune helps this kernel, by running:", "We may also temporarily add more reduction heuristics and run the script again to check how that helps with the kernel."]}, {"name": "torch.complex", "path": "generated/torch.complex", "type": "Torch", "text": ["Constructs a complex tensor with its real part equal to real and its imaginary part equal to imag.", "out (Tensor) \u2013 If the inputs are torch.float32, must be torch.complex64. If the inputs are torch.float64, must be torch.complex128.", "Example:"]}, {"name": "torch.ComplexDoubleStorage", "path": "storage#torch.ComplexDoubleStorage", "type": "Storage", "text": []}, {"name": "torch.ComplexDoubleStorage.dtype", "path": "storage#torch.ComplexDoubleStorage.dtype", "type": "Storage", "text": []}, {"name": "torch.ComplexFloatStorage", "path": "storage#torch.ComplexFloatStorage", "type": "Storage", "text": []}, {"name": "torch.ComplexFloatStorage.dtype", "path": "storage#torch.ComplexFloatStorage.dtype", "type": "Storage", "text": []}, {"name": "torch.concat", "path": "generated/torch.concat", "type": "Torch", "text": ["Alias of torch.cat()."]}, {"name": "torch.concatenate", "path": "generated/torch.concatenate", "type": "Torch", "text": ["Alias of torch.cat()."]}, {"name": "torch.conj", "path": "generated/torch.conj", "type": "Torch", "text": ["Returns a view of input with a flipped conjugate bit. If input has a non-complex dtype, this function just returns input.", "Note", "torch.conj() performs a lazy conjugation, but the actual conjugated tensor can be materialized at any time using torch.resolve_conj().", "Warning", "In the future, torch.conj() may return a non-writeable view for an input of non-complex dtype. It\u2019s recommended that programs not modify the tensor returned by torch.conj_physical() when input is of non-complex dtype to be compatible with this change.", "input (Tensor) \u2013 the input tensor.", "Example:"]}, {"name": "torch.conj_physical", "path": "generated/torch.conj_physical", "type": "Torch", "text": ["Computes the element-wise conjugate of the given input tensor. If input has a non-complex dtype, this function just returns input.", "Note", "This performs the conjugate operation regardless of the fact conjugate bit is set or not.", "Warning", "In the future, torch.conj_physical() may return a non-writeable view for an input of non-complex dtype. It\u2019s recommended that programs not modify the tensor returned by torch.conj_physical() when input is of non-complex dtype to be compatible with this change.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.copysign", "path": "generated/torch.copysign", "type": "Torch", "text": ["Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.", "Supports broadcasting to a common shape, and integer and float inputs.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Note", "copysign handles signed zeros. If the other argument has a negative zero (-0), the corresponding output value will be negative."]}, {"name": "torch.corrcoef", "path": "generated/torch.corrcoef", "type": "Torch", "text": ["Estimates the Pearson product-moment correlation coefficient matrix of the variables given by the input matrix, where rows are the variables and columns are the observations.", "Note", "The correlation coefficient matrix R is computed using the covariance matrix C as given by Rij=CijCii\u2217CjjR_{ij} = \\frac{ C_{ij} } { \\sqrt{ C_{ii} * C_{jj} } }", "Note", "Due to floating point rounding, the resulting array may not be Hermitian and its diagonal elements may not be 1. The real and imaginary values are clipped to the interval [-1, 1] in an attempt to improve this situation.", "input (Tensor) \u2013 A 2D matrix containing multiple variables and observations, or a Scalar or 1D vector representing a single variable.", "(Tensor) The correlation coefficient matrix of the variables.", "See also", "torch.cov() covariance matrix.", "Example:"]}, {"name": "torch.cos", "path": "generated/torch.cos", "type": "Torch", "text": ["Returns a new tensor with the cosine of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.cosh", "path": "generated/torch.cosh", "type": "Torch", "text": ["Returns a new tensor with the hyperbolic cosine of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Note", "When input is on the CPU, the implementation of torch.cosh may use the Sleef library, which rounds very large results to infinity or negative infinity. See here for details."]}, {"name": "torch.count_nonzero", "path": "generated/torch.count_nonzero", "type": "Torch", "text": ["Counts the number of non-zero values in the tensor input along the given dim. If no dim is specified then all non-zeros in the tensor are counted.", "Example:"]}, {"name": "torch.cov", "path": "generated/torch.cov", "type": "Torch", "text": ["Estimates the covariance matrix of the variables given by the input matrix, where rows are the variables and columns are the observations.", "A covariance matrix is a square matrix giving the covariance of each pair of variables. The diagonal contains the variance of each variable (covariance of a variable with itself). By definition, if input represents a single variable (Scalar or 1D) then its variance is returned.", "The sample covariance of the variables xx and yy is given by:", "where x\u02c9\\bar{x} and y\u02c9\\bar{y} are the simple means of the xx and yy respectively, and \u03b4N\\delta N is the correction.", "If fweights and/or aweights are provided, the weighted covariance is calculated, which is given by:", "where ww denotes fweights or aweights (f and a for brevity) based on whichever is provided, or w=f\u00d7aw = f \\times a if both are provided, and \u03bcx\u2217=\u2211i=1Nwixi\u2211i=1Nwi\\mu_x^* = \\frac{\\sum^{N}_{i = 1}w_ix_{i} }{\\sum^{N}_{i = 1}w_i} is the weighted mean of the variable. If not provided, f and/or a can be seen as a 1\\mathbb{1} vector of appropriate size.", "input (Tensor) \u2013 A 2D matrix containing multiple variables and observations, or a Scalar or 1D vector representing a single variable.", "(Tensor) The covariance matrix of the variables.", "See also", "torch.corrcoef() normalized covariance matrix."]}, {"name": "torch.cpu", "path": "cpu", "type": "Miscellaneous", "text": ["This package implements abstractions found in torch.cuda to facilitate writing device-agnostic code.", "Returns the currently selected Stream for a given device.", "Returns a bool indicating if CPU is currently available.", "Waits for all kernels in all streams on the CPU device to complete.", "Wrapper around the Context-manager StreamContext that selects a given stream.", "Returns number of CPU devices (not cores).", "Context-manager that selects a given stream.", "N.B."]}, {"name": "torch.cpu.amp.autocast", "path": "amp#torch.cpu.amp.autocast", "type": "Automatic Mixed Precision", "text": ["See torch.autocast. torch.cpu.amp.autocast(args...) is equivalent to torch.autocast(\"cpu\", args...)"]}, {"name": "torch.cpu.current_stream()", "path": "generated/torch.cpu.current_stream#torch.cpu.current_stream", "type": "Miscellaneous", "text": ["Returns the currently selected Stream for a given device.", "device (torch.device or int, optional) \u2013 Ignored.", "Stream", "N.B. This function only exists to facilitate device-agnostic code"]}, {"name": "torch.cpu.device_count()", "path": "generated/torch.cpu.device_count#torch.cpu.device_count", "type": "Miscellaneous", "text": ["Returns number of CPU devices (not cores). Always 1.", "N.B. This function only exists to facilitate device-agnostic code", "int"]}, {"name": "torch.cpu.is_available()", "path": "generated/torch.cpu.is_available#torch.cpu.is_available", "type": "Miscellaneous", "text": ["Returns a bool indicating if CPU is currently available.", "N.B. This function only exists to facilitate device-agnostic code", "bool"]}, {"name": "torch.cpu.stream()", "path": "generated/torch.cpu.stream#torch.cpu.stream", "type": "Miscellaneous", "text": ["Wrapper around the Context-manager StreamContext that selects a given stream.", "N.B. This function only exists to facilitate device-agnostic code", "AbstractContextManager"]}, {"name": "torch.cpu.StreamContext", "path": "generated/torch.cpu.streamcontext", "type": "Miscellaneous", "text": ["Context-manager that selects a given stream.", "N.B. This class only exists to facilitate device-agnostic code"]}, {"name": "torch.cpu.synchronize()", "path": "generated/torch.cpu.synchronize#torch.cpu.synchronize", "type": "Miscellaneous", "text": ["Waits for all kernels in all streams on the CPU device to complete.", "device (torch.device or int, optional) \u2013 ignored, there\u2019s only one CPU device.", "N.B. This function only exists to facilitate device-agnostic code."]}, {"name": "torch.cpu.torch.cpu.current_stream", "path": "generated/torch.cpu.current_stream", "type": "Miscellaneous", "text": ["Returns the currently selected Stream for a given device.", "device (torch.device or int, optional) \u2013 Ignored.", "Stream", "N.B. This function only exists to facilitate device-agnostic code"]}, {"name": "torch.cpu.torch.cpu.device_count", "path": "generated/torch.cpu.device_count", "type": "Miscellaneous", "text": ["Returns number of CPU devices (not cores). Always 1.", "N.B. This function only exists to facilitate device-agnostic code", "int"]}, {"name": "torch.cpu.torch.cpu.is_available", "path": "generated/torch.cpu.is_available", "type": "Miscellaneous", "text": ["Returns a bool indicating if CPU is currently available.", "N.B. This function only exists to facilitate device-agnostic code", "bool"]}, {"name": "torch.cpu.torch.cpu.stream", "path": "generated/torch.cpu.stream", "type": "Miscellaneous", "text": ["Wrapper around the Context-manager StreamContext that selects a given stream.", "N.B. This function only exists to facilitate device-agnostic code", "AbstractContextManager"]}, {"name": "torch.cpu.torch.cpu.synchronize", "path": "generated/torch.cpu.synchronize", "type": "Miscellaneous", "text": ["Waits for all kernels in all streams on the CPU device to complete.", "device (torch.device or int, optional) \u2013 ignored, there\u2019s only one CPU device.", "N.B. This function only exists to facilitate device-agnostic code."]}, {"name": "torch.cross", "path": "generated/torch.cross", "type": "Torch", "text": ["Returns the cross product of vectors in dimension dim of input and other.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of vectors, for which it computes the product along the dimension dim. In this case, the output has the same batch dimensions as the inputs.", "If dim is not given, it defaults to the first dimension found with the size 3. Note that this might be unexpected.", "See also", "torch.linalg.cross() which requires specifying dim (defaulting to -1).", "Warning", "This function may change in a future PyTorch release to match the default behaviour in torch.linalg.cross(). We recommend using torch.linalg.cross().", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.cuda", "path": "cuda", "type": "CUDA", "text": ["This package adds support for CUDA tensor types, that implement the same function as CPU tensors, but they utilize GPUs for computation.", "It is lazily initialized, so you can always import it, and use is_available() to determine if your system supports CUDA.", "CUDA semantics has more details about working with CUDA.", "Context-manager that selects a given stream.", "Checks if peer access between two devices is possible.", "Returns cublasHandle_t pointer to current cuBLAS handle", "Returns the index of a currently selected device.", "Returns the currently selected Stream for a given device.", "Returns the default Stream for a given device.", "Context-manager that changes the selected device.", "Returns the number of GPUs available.", "Context-manager that changes the current device to that of given object.", "Returns list CUDA architectures this library was compiled for.", "Gets the cuda capability of a device.", "Gets the name of a device.", "Gets the properties of a device.", "Returns NVCC gencode flags this library was compiled with.", "Returns current value of debug mode for cuda synchronizing operations.", "Initialize PyTorch's CUDA state.", "Force collects GPU memory after it has been released by CUDA IPC.", "Returns a bool indicating if CUDA is currently available.", "Returns whether PyTorch's CUDA state has been initialized.", "Returns the percent of time over the past sample period during which global (device) memory was being read or written.", "Sets the current device.", "Sets the current stream.This is a wrapper API to set the stream.", "Sets the debug mode for cuda synchronizing operations.", "Wrapper around the Context-manager StreamContext that selects a given stream.", "Waits for all kernels in all streams on a CUDA device to complete.", "Returns the percent of time over the past sample period during which one or more kernels was executing on the GPU as given by nvidia-smi.", "Returns the average temperature of the GPU sensor in Degrees C (Centigrades)", "Returns the average power draw of the GPU sensor in mW (MilliWatts)", "Returns the clock speed of the GPU SM in Hz Hertz over the past sample period as given by nvidia-smi.", "Exception raised when CUDA is out of memory", "Returns the random number generator state of the specified GPU as a ByteTensor.", "Returns a list of ByteTensor representing the random number states of all devices.", "Sets the random number generator state of the specified GPU.", "Sets the random number generator state of all devices.", "Sets the seed for generating random numbers for the current GPU.", "Sets the seed for generating random numbers on all GPUs.", "Sets the seed for generating random numbers to a random number for the current GPU.", "Sets the seed for generating random numbers to a random number on all GPUs.", "Returns the current random seed of the current GPU.", "comm.broadcast", "Broadcasts a tensor to specified GPU devices.", "comm.broadcast_coalesced", "Broadcasts a sequence tensors to the specified GPUs.", "comm.reduce_add", "Sums tensors from multiple GPUs.", "comm.scatter", "Scatters tensor across multiple GPUs.", "comm.gather", "Gathers tensors from multiple GPU devices.", "Wrapper around a CUDA stream.", "Wrapper around an externally allocated CUDA stream.", "Wrapper around a CUDA event.", "Returns True if CUDA graph capture is underway on the current CUDA stream, False otherwise.", "Returns an opaque token representing the id of a graph memory pool.", "Wrapper around a CUDA graph.", "Context-manager that captures CUDA work into a torch.cuda.CUDAGraph object for later replay.", "Accepts callables (functions or nn.Modules) and returns graphed versions.", "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.", "Returns a human-readable printout of the running processes and their GPU memory use for a given device.", "Returns the global free and total GPU memory for a given device using cudaMemGetInfo.", "Returns a dictionary of CUDA memory allocator statistics for a given device.", "Returns a human-readable printout of the current memory allocator statistics for a given device.", "Returns a snapshot of the CUDA memory allocator state across all devices.", "Returns the current GPU memory occupied by tensors in bytes for a given device.", "Returns the maximum GPU memory occupied by tensors in bytes for a given device.", "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.", "Returns the current GPU memory managed by the caching allocator in bytes for a given device.", "Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.", "Set memory fraction for a process.", "Deprecated; see memory_reserved().", "Deprecated; see max_memory_reserved().", "Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.", "Resets the \"peak\" stats tracked by the CUDA memory allocator.", "Performs a memory allocation using the CUDA memory allocator.", "Deletes memory allocated using the CUDA memory allocator.", "Returns a string describing the active allocator backend as set by PYTORCH_CUDA_ALLOC_CONF.", "CUDA memory allocator loaded from a so file.", "Changes the currently used memory allocator to be the one provided.", "nvtx.mark", "Describe an instantaneous event that occurred at some point.", "nvtx.range_push", "Pushes a range onto a stack of nested range span.", "nvtx.range_pop", "Pops a range off of a stack of nested range spans.", "jiterator._create_jit_fn", "Create a jiterator-generated cuda kernel for an elementwise op.", "jiterator._create_multi_output_jit_fn", "Create a jiterator-generated cuda kernel for an elementwise op that supports returning one or more outputs.", "CUDA Sanitizer is a prototype tool for detecting synchronization errors between streams in PyTorch. See the documentation for information on how to use it."]}, {"name": "torch.cuda._sanitizer.enable_cuda_sanitizer()", "path": "cuda._sanitizer#torch.cuda._sanitizer.enable_cuda_sanitizer", "type": "CUDA", "text": ["Enables CUDA Sanitizer.", "The sanitizer will begin to analyze low-level CUDA calls invoked by torch functions for synchronization errors. All data races found will be printed to the standard error output along with stack traces of suspected causes. For best results, the sanitizer should be enabled at the very beginning of the program."]}, {"name": "torch.cuda.amp.autocast", "path": "amp#torch.cuda.amp.autocast", "type": "Automatic Mixed Precision", "text": ["See torch.autocast. torch.cuda.amp.autocast(args...) is equivalent to torch.autocast(\"cuda\", args...)"]}, {"name": "torch.cuda.amp.custom_bwd()", "path": "amp#torch.cuda.amp.custom_bwd", "type": "Automatic Mixed Precision", "text": ["Helper decorator for backward methods of custom autograd functions (subclasses of torch.autograd.Function). Ensures that backward executes with the same autocast state as forward. See the example page for more detail."]}, {"name": "torch.cuda.amp.custom_fwd()", "path": "amp#torch.cuda.amp.custom_fwd", "type": "Automatic Mixed Precision", "text": ["Helper decorator for forward methods of custom autograd functions (subclasses of torch.autograd.Function). See the example page for more detail.", "cast_inputs (torch.dtype or None, optional, default=None) \u2013 If not None, when forward runs in an autocast-enabled region, casts incoming floating-point CUDA Tensors to the target dtype (non-floating-point Tensors are not affected), then executes forward with autocast disabled. If None, forward\u2019s internal ops execute with the current autocast state.", "Note", "If the decorated forward is called outside an autocast-enabled region, custom_fwd is a no-op and cast_inputs has no effect."]}, {"name": "torch.cuda.amp.GradScaler", "path": "amp#torch.cuda.amp.GradScaler", "type": "Automatic Mixed Precision", "text": ["Returns a Python float containing the scale backoff factor.", "Returns a Python float containing the scale growth factor.", "Returns a Python int containing the growth interval.", "Returns a Python float containing the current scale, or 1.0 if scaling is disabled.", "Warning", "get_scale() incurs a CPU-GPU sync.", "Returns a bool indicating whether this instance is enabled.", "Loads the scaler state. If this instance is disabled, load_state_dict() is a no-op.", "state_dict (dict) \u2013 scaler state. Should be an object returned from a call to state_dict().", "Multiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor.", "Returns scaled outputs. If this instance of GradScaler is not enabled, outputs are returned unmodified.", "outputs (Tensor or iterable of Tensors) \u2013 Outputs to scale.", "new_scale (float) \u2013 Value to use as the new scale backoff factor.", "new_scale (float) \u2013 Value to use as the new scale growth factor.", "new_interval (int) \u2013 Value to use as the new growth interval.", "Returns the state of the scaler as a dict. It contains five entries:", "If this instance is not enabled, returns an empty dict.", "Note", "If you wish to checkpoint the scaler\u2019s state after a particular iteration, state_dict() should be called after update().", "step() carries out the following two operations:", "*args and **kwargs are forwarded to optimizer.step().", "Returns the return value of optimizer.step(*args, **kwargs).", "Warning", "Closure use is not currently supported.", "Divides (\u201cunscales\u201d) the optimizer\u2019s gradient tensors by the scale factor.", "unscale_() is optional, serving cases where you need to modify or inspect gradients between the backward pass(es) and step(). If unscale_() is not called explicitly, gradients will be unscaled automatically during step().", "Simple example, using unscale_() to enable clipping of unscaled gradients:", "optimizer (torch.optim.Optimizer) \u2013 Optimizer that owns the gradients to be unscaled.", "Note", "unscale_() does not incur a CPU-GPU sync.", "Warning", "unscale_() should only be called once per optimizer per step() call, and only after all gradients for that optimizer\u2019s assigned parameters have been accumulated. Calling unscale_() twice for a given optimizer between each step() triggers a RuntimeError.", "Warning", "unscale_() may unscale sparse gradients out of place, replacing the .grad attribute.", "Updates the scale factor.", "If any optimizer steps were skipped the scale is multiplied by backoff_factor to reduce it. If growth_interval unskipped iterations occurred consecutively, the scale is multiplied by growth_factor to increase it.", "Passing new_scale sets the new scale value manually. (new_scale is not used directly, it\u2019s used to fill GradScaler\u2019s internal scale tensor. So if new_scale was a tensor, later in-place changes to that tensor will not further affect the scale GradScaler uses internally.)", "new_scale (float or torch.cuda.FloatTensor, optional, default=None) \u2013 New scale factor.", "Warning", "update() should only be called at the end of the iteration, after scaler.step(optimizer) has been invoked for all optimizers used this iteration.", "Warning", "For performance reasons, we do not check the scale factor value to avoid synchronizations, so the scale factor is not guaranteed to be above 1. If the scale falls below 1 and/or you are seeing NaNs in your gradients or loss, something is likely wrong. For example, bf16-pretrained models are often incompatible with AMP/fp16 due to differing dynamic ranges."]}, {"name": "torch.cuda.amp.GradScaler.get_backoff_factor()", "path": "amp#torch.cuda.amp.GradScaler.get_backoff_factor", "type": "Automatic Mixed Precision", "text": ["Returns a Python float containing the scale backoff factor."]}, {"name": "torch.cuda.amp.GradScaler.get_growth_factor()", "path": "amp#torch.cuda.amp.GradScaler.get_growth_factor", "type": "Automatic Mixed Precision", "text": ["Returns a Python float containing the scale growth factor."]}, {"name": "torch.cuda.amp.GradScaler.get_growth_interval()", "path": "amp#torch.cuda.amp.GradScaler.get_growth_interval", "type": "Automatic Mixed Precision", "text": ["Returns a Python int containing the growth interval."]}, {"name": "torch.cuda.amp.GradScaler.get_scale()", "path": "amp#torch.cuda.amp.GradScaler.get_scale", "type": "Automatic Mixed Precision", "text": ["Returns a Python float containing the current scale, or 1.0 if scaling is disabled.", "Warning", "get_scale() incurs a CPU-GPU sync."]}, {"name": "torch.cuda.amp.GradScaler.is_enabled()", "path": "amp#torch.cuda.amp.GradScaler.is_enabled", "type": "Automatic Mixed Precision", "text": ["Returns a bool indicating whether this instance is enabled."]}, {"name": "torch.cuda.amp.GradScaler.load_state_dict()", "path": "amp#torch.cuda.amp.GradScaler.load_state_dict", "type": "Automatic Mixed Precision", "text": ["Loads the scaler state. If this instance is disabled, load_state_dict() is a no-op.", "state_dict (dict) \u2013 scaler state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.cuda.amp.GradScaler.scale()", "path": "amp#torch.cuda.amp.GradScaler.scale", "type": "Automatic Mixed Precision", "text": ["Multiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor.", "Returns scaled outputs. If this instance of GradScaler is not enabled, outputs are returned unmodified.", "outputs (Tensor or iterable of Tensors) \u2013 Outputs to scale."]}, {"name": "torch.cuda.amp.GradScaler.set_backoff_factor()", "path": "amp#torch.cuda.amp.GradScaler.set_backoff_factor", "type": "Automatic Mixed Precision", "text": ["new_scale (float) \u2013 Value to use as the new scale backoff factor."]}, {"name": "torch.cuda.amp.GradScaler.set_growth_factor()", "path": "amp#torch.cuda.amp.GradScaler.set_growth_factor", "type": "Automatic Mixed Precision", "text": ["new_scale (float) \u2013 Value to use as the new scale growth factor."]}, {"name": "torch.cuda.amp.GradScaler.set_growth_interval()", "path": "amp#torch.cuda.amp.GradScaler.set_growth_interval", "type": "Automatic Mixed Precision", "text": ["new_interval (int) \u2013 Value to use as the new growth interval."]}, {"name": "torch.cuda.amp.GradScaler.state_dict()", "path": "amp#torch.cuda.amp.GradScaler.state_dict", "type": "Automatic Mixed Precision", "text": ["Returns the state of the scaler as a dict. It contains five entries:", "If this instance is not enabled, returns an empty dict.", "Note", "If you wish to checkpoint the scaler\u2019s state after a particular iteration, state_dict() should be called after update()."]}, {"name": "torch.cuda.amp.GradScaler.step()", "path": "amp#torch.cuda.amp.GradScaler.step", "type": "Automatic Mixed Precision", "text": ["step() carries out the following two operations:", "*args and **kwargs are forwarded to optimizer.step().", "Returns the return value of optimizer.step(*args, **kwargs).", "Warning", "Closure use is not currently supported."]}, {"name": "torch.cuda.amp.GradScaler.unscale_()", "path": "amp#torch.cuda.amp.GradScaler.unscale_", "type": "Automatic Mixed Precision", "text": ["Divides (\u201cunscales\u201d) the optimizer\u2019s gradient tensors by the scale factor.", "unscale_() is optional, serving cases where you need to modify or inspect gradients between the backward pass(es) and step(). If unscale_() is not called explicitly, gradients will be unscaled automatically during step().", "Simple example, using unscale_() to enable clipping of unscaled gradients:", "optimizer (torch.optim.Optimizer) \u2013 Optimizer that owns the gradients to be unscaled.", "Note", "unscale_() does not incur a CPU-GPU sync.", "Warning", "unscale_() should only be called once per optimizer per step() call, and only after all gradients for that optimizer\u2019s assigned parameters have been accumulated. Calling unscale_() twice for a given optimizer between each step() triggers a RuntimeError.", "Warning", "unscale_() may unscale sparse gradients out of place, replacing the .grad attribute."]}, {"name": "torch.cuda.amp.GradScaler.update()", "path": "amp#torch.cuda.amp.GradScaler.update", "type": "Automatic Mixed Precision", "text": ["Updates the scale factor.", "If any optimizer steps were skipped the scale is multiplied by backoff_factor to reduce it. If growth_interval unskipped iterations occurred consecutively, the scale is multiplied by growth_factor to increase it.", "Passing new_scale sets the new scale value manually. (new_scale is not used directly, it\u2019s used to fill GradScaler\u2019s internal scale tensor. So if new_scale was a tensor, later in-place changes to that tensor will not further affect the scale GradScaler uses internally.)", "new_scale (float or torch.cuda.FloatTensor, optional, default=None) \u2013 New scale factor.", "Warning", "update() should only be called at the end of the iteration, after scaler.step(optimizer) has been invoked for all optimizers used this iteration.", "Warning", "For performance reasons, we do not check the scale factor value to avoid synchronizations, so the scale factor is not guaranteed to be above 1. If the scale falls below 1 and/or you are seeing NaNs in your gradients or loss, something is likely wrong. For example, bf16-pretrained models are often incompatible with AMP/fp16 due to differing dynamic ranges."]}, {"name": "torch.cuda.caching_allocator_alloc()", "path": "generated/torch.cuda.caching_allocator_alloc#torch.cuda.caching_allocator_alloc", "type": "CUDA", "text": ["Performs a memory allocation using the CUDA memory allocator.", "Memory is allocated for a given device and a stream, this function is intended to be used for interoperability with other frameworks. Allocated memory is released through caching_allocator_delete().", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.caching_allocator_delete()", "path": "generated/torch.cuda.caching_allocator_delete#torch.cuda.caching_allocator_delete", "type": "CUDA", "text": ["Deletes memory allocated using the CUDA memory allocator.", "Memory allocated with caching_allocator_alloc(). is freed here. The associated device and stream are tracked inside the allocator.", "mem_ptr (int) \u2013 memory address to be freed by the allocator.", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.can_device_access_peer()", "path": "generated/torch.cuda.can_device_access_peer#torch.cuda.can_device_access_peer", "type": "CUDA", "text": ["Checks if peer access between two devices is possible.", "bool"]}, {"name": "torch.cuda.change_current_allocator()", "path": "generated/torch.cuda.change_current_allocator#torch.cuda.change_current_allocator", "type": "CUDA", "text": ["Changes the currently used memory allocator to be the one provided. If the current allocator has already been used/initialized, this function will error.", "allocator (torch.cuda.memory._CUDAAllocator) \u2013 allocator to be set as the active one.", "Note", "See Memory management for details on creating and using a custom allocator"]}, {"name": "torch.cuda.clock_rate()", "path": "generated/torch.cuda.clock_rate#torch.cuda.clock_rate", "type": "CUDA", "text": ["Returns the clock speed of the GPU SM in Hz Hertz over the past sample period as given by nvidia-smi.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried."]}, {"name": "torch.cuda.comm.broadcast()", "path": "generated/torch.cuda.comm.broadcast#torch.cuda.comm.broadcast", "type": "CUDA", "text": ["Broadcasts a tensor to specified GPU devices.", "Note", "Exactly one of devices and out must be specified.", "a tuple containing copies of tensor, placed on devices.", "a tuple containing out tensors, each containing a copy of tensor."]}, {"name": "torch.cuda.comm.broadcast_coalesced()", "path": "generated/torch.cuda.comm.broadcast_coalesced#torch.cuda.comm.broadcast_coalesced", "type": "CUDA", "text": ["Broadcasts a sequence tensors to the specified GPUs. Small tensors are first coalesced into a buffer to reduce the number of synchronizations.", "A tuple containing copies of tensor, placed on devices."]}, {"name": "torch.cuda.comm.gather()", "path": "generated/torch.cuda.comm.gather#torch.cuda.comm.gather", "type": "CUDA", "text": ["Gathers tensors from multiple GPU devices.", "Note", "destination must not be specified when out is specified.", "a tensor located on destination device, that is a result of concatenating tensors along dim.", "the out tensor, now containing results of concatenating tensors along dim."]}, {"name": "torch.cuda.comm.reduce_add()", "path": "generated/torch.cuda.comm.reduce_add#torch.cuda.comm.reduce_add", "type": "CUDA", "text": ["Sums tensors from multiple GPUs.", "All inputs should have matching shapes, dtype, and layout. The output tensor will be of the same shape, dtype, and layout.", "A tensor containing an elementwise sum of all inputs, placed on the destination device."]}, {"name": "torch.cuda.comm.scatter()", "path": "generated/torch.cuda.comm.scatter#torch.cuda.comm.scatter", "type": "CUDA", "text": ["Scatters tensor across multiple GPUs.", "Note", "Exactly one of devices and out must be specified. When out is specified, chunk_sizes must not be specified and will be inferred from sizes of out.", "a tuple containing chunks of tensor, placed on devices.", "a tuple containing out tensors, each containing a chunk of tensor."]}, {"name": "torch.cuda.CUDA Stream Sanitizer", "path": "cuda._sanitizer", "type": "CUDA", "text": ["Note", "This is a prototype feature, which means it is at an early stage for feedback and testing, and its components are subject to change.", "This module introduces CUDA Sanitizer, a tool for detecting synchronization errors between kernels ran on different streams. It stores information on accesses to tensors to determine if they are synchronized or not. When enabled in a python program and a possible data race is detected, a detailed warning will be printed and the program will exit.", "It can be enabled either by importing this module and calling enable_cuda_sanitizer() or by exporting the TORCH_CUDA_SANITIZER environment variable.", "Here is an example of a simple synchronization error in PyTorch:", "The a tensor is initialized on the default stream and, without any synchronization methods, modified on a new stream. The two kernels will run concurrently on the same tensor, which might cause the second kernel to read uninitialized data before the first one was able to write it, or the first kernel might overwrite part of the result of the second. When this script is run on the commandline with:", "the following output is printed by CSAN:", "This gives extensive insight into the origin of the error:", "The error message also displays the schemas of the invoked operators, along with a note showing which arguments of the operators correspond to the affected tensor.", "See also", "The list of supported torch operators and their schemas can be viewed here.", "The bug can be fixed by forcing the new stream to wait for the default stream:", "When the script is run again, there are no errors reported.", "Enables CUDA Sanitizer.", "The sanitizer will begin to analyze low-level CUDA calls invoked by torch functions for synchronization errors. All data races found will be printed to the standard error output along with stack traces of suspected causes. For best results, the sanitizer should be enabled at the very beginning of the program."]}, {"name": "torch.cuda.CUDAGraph", "path": "generated/torch.cuda.cudagraph", "type": "CUDA", "text": ["Wrapper around a CUDA graph.", "Warning", "This API is in beta and may change in future releases.", "Begins capturing CUDA work on the current stream.", "Typically, you shouldn\u2019t call capture_begin yourself. Use graph or make_graphed_callables(), which call capture_begin internally.", "Ends CUDA graph capture on the current stream. After capture_end, replay may be called on this instance.", "Typically, you shouldn\u2019t call capture_end yourself. Use graph or make_graphed_callables(), which call capture_end internally.", "debug_path (required) \u2013 Path to dump the graph to.", "Calls a debugging function to dump the graph if the debugging is enabled via CUDAGraph.enable_debug_mode()", "Enables debugging mode for CUDAGraph.debug_dump.", "Returns an opaque token representing the id of this graph\u2019s memory pool. This id can optionally be passed to another graph\u2019s capture_begin, which hints the other graph may share the same memory pool.", "Replays the CUDA work captured by this graph.", "Deletes the graph currently held by this instance."]}, {"name": "torch.cuda.CUDAGraph.capture_begin()", "path": "generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph.capture_begin", "type": "CUDA", "text": ["Begins capturing CUDA work on the current stream.", "Typically, you shouldn\u2019t call capture_begin yourself. Use graph or make_graphed_callables(), which call capture_begin internally."]}, {"name": "torch.cuda.CUDAGraph.capture_end()", "path": "generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph.capture_end", "type": "CUDA", "text": ["Ends CUDA graph capture on the current stream. After capture_end, replay may be called on this instance.", "Typically, you shouldn\u2019t call capture_end yourself. Use graph or make_graphed_callables(), which call capture_end internally."]}, {"name": "torch.cuda.CUDAGraph.debug_dump()", "path": "generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph.debug_dump", "type": "CUDA", "text": ["debug_path (required) \u2013 Path to dump the graph to.", "Calls a debugging function to dump the graph if the debugging is enabled via CUDAGraph.enable_debug_mode()"]}, {"name": "torch.cuda.CUDAGraph.enable_debug_mode()", "path": "generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph.enable_debug_mode", "type": "CUDA", "text": ["Enables debugging mode for CUDAGraph.debug_dump."]}, {"name": "torch.cuda.CUDAGraph.pool()", "path": "generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph.pool", "type": "CUDA", "text": ["Returns an opaque token representing the id of this graph\u2019s memory pool. This id can optionally be passed to another graph\u2019s capture_begin, which hints the other graph may share the same memory pool."]}, {"name": "torch.cuda.CUDAGraph.replay()", "path": "generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph.replay", "type": "CUDA", "text": ["Replays the CUDA work captured by this graph."]}, {"name": "torch.cuda.CUDAGraph.reset()", "path": "generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph.reset", "type": "CUDA", "text": ["Deletes the graph currently held by this instance."]}, {"name": "torch.cuda.CUDAPluggableAllocator", "path": "generated/torch.cuda.cudapluggableallocator", "type": "CUDA", "text": ["CUDA memory allocator loaded from a so file.", "Memory allocators are compiled in .so files and loaded dynamically using ctypes. To change the active allocator use the torch.memory.cuda.change_current_allocator() function.", "Warning", "This is currently supported only in unix OSs", "Note", "See Memory management for details on creating and using a custom allocator"]}, {"name": "torch.cuda.current_blas_handle()", "path": "generated/torch.cuda.current_blas_handle#torch.cuda.current_blas_handle", "type": "CUDA", "text": ["Returns cublasHandle_t pointer to current cuBLAS handle"]}, {"name": "torch.cuda.current_device()", "path": "generated/torch.cuda.current_device#torch.cuda.current_device", "type": "CUDA", "text": ["Returns the index of a currently selected device.", "int"]}, {"name": "torch.cuda.current_stream()", "path": "generated/torch.cuda.current_stream#torch.cuda.current_stream", "type": "CUDA", "text": ["Returns the currently selected Stream for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns the currently selected Stream for the current device, given by current_device(), if device is None (default).", "Stream"]}, {"name": "torch.cuda.default_stream()", "path": "generated/torch.cuda.default_stream#torch.cuda.default_stream", "type": "CUDA", "text": ["Returns the default Stream for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns the default Stream for the current device, given by current_device(), if device is None (default).", "Stream"]}, {"name": "torch.cuda.device", "path": "generated/torch.cuda.device", "type": "CUDA", "text": ["Context-manager that changes the selected device.", "device (torch.device or int) \u2013 device index to select. It\u2019s a no-op if this argument is a negative integer or None."]}, {"name": "torch.cuda.device_count()", "path": "generated/torch.cuda.device_count#torch.cuda.device_count", "type": "CUDA", "text": ["Returns the number of GPUs available.", "int"]}, {"name": "torch.cuda.device_of", "path": "generated/torch.cuda.device_of", "type": "CUDA", "text": ["Context-manager that changes the current device to that of given object.", "You can use both tensors and storages as arguments. If a given object is not allocated on a GPU, this is a no-op.", "obj (Tensor or Storage) \u2013 object allocated on the selected device."]}, {"name": "torch.cuda.empty_cache()", "path": "generated/torch.cuda.empty_cache#torch.cuda.empty_cache", "type": "CUDA", "text": ["Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.", "Note", "empty_cache() doesn\u2019t increase the amount of GPU memory available for PyTorch. However, it may help reduce fragmentation of GPU memory in certain cases. See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.Event", "path": "generated/torch.cuda.event", "type": "CUDA", "text": ["Wrapper around a CUDA event.", "CUDA events are synchronization markers that can be used to monitor the device\u2019s progress, to accurately measure timing, and to synchronize CUDA streams.", "The underlying CUDA events are lazily initialized when the event is first recorded or exported to another process. After creation, only streams on the same device may record the event. However, streams on any device can wait on the event.", "Returns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded.", "Reconstruct an event from an IPC handle on the given device.", "Returns an IPC handle of this event. If not recorded yet, the event will use the current device.", "Checks if all work currently captured by event has completed.", "A boolean indicating if all work currently captured by event has completed.", "Records the event in a given stream.", "Uses torch.cuda.current_stream() if no stream is specified. The stream\u2019s device must match the event\u2019s device.", "Waits for the event to complete.", "Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes.", "Note", "This is a wrapper around cudaEventSynchronize(): see CUDA Event documentation for more info.", "Makes all future work submitted to the given stream wait for this event.", "Use torch.cuda.current_stream() if no stream is specified.", "Note", "This is a wrapper around cudaStreamWaitEvent(): see CUDA Event documentation for more info."]}, {"name": "torch.cuda.Event.elapsed_time()", "path": "generated/torch.cuda.event#torch.cuda.Event.elapsed_time", "type": "CUDA", "text": ["Returns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded."]}, {"name": "torch.cuda.Event.from_ipc_handle()", "path": "generated/torch.cuda.event#torch.cuda.Event.from_ipc_handle", "type": "CUDA", "text": ["Reconstruct an event from an IPC handle on the given device."]}, {"name": "torch.cuda.Event.ipc_handle()", "path": "generated/torch.cuda.event#torch.cuda.Event.ipc_handle", "type": "CUDA", "text": ["Returns an IPC handle of this event. If not recorded yet, the event will use the current device."]}, {"name": "torch.cuda.Event.query()", "path": "generated/torch.cuda.event#torch.cuda.Event.query", "type": "CUDA", "text": ["Checks if all work currently captured by event has completed.", "A boolean indicating if all work currently captured by event has completed."]}, {"name": "torch.cuda.Event.record()", "path": "generated/torch.cuda.event#torch.cuda.Event.record", "type": "CUDA", "text": ["Records the event in a given stream.", "Uses torch.cuda.current_stream() if no stream is specified. The stream\u2019s device must match the event\u2019s device."]}, {"name": "torch.cuda.Event.synchronize()", "path": "generated/torch.cuda.event#torch.cuda.Event.synchronize", "type": "CUDA", "text": ["Waits for the event to complete.", "Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes.", "Note", "This is a wrapper around cudaEventSynchronize(): see CUDA Event documentation for more info."]}, {"name": "torch.cuda.Event.wait()", "path": "generated/torch.cuda.event#torch.cuda.Event.wait", "type": "CUDA", "text": ["Makes all future work submitted to the given stream wait for this event.", "Use torch.cuda.current_stream() if no stream is specified.", "Note", "This is a wrapper around cudaStreamWaitEvent(): see CUDA Event documentation for more info."]}, {"name": "torch.cuda.ExternalStream", "path": "generated/torch.cuda.externalstream", "type": "CUDA", "text": ["Wrapper around an externally allocated CUDA stream.", "This class is used to wrap streams allocated in other libraries in order to facilitate data exchange and multi-library interactions.", "Note", "This class doesn\u2019t manage the stream life-cycle, it is the user responsibility to keep the referenced stream alive while this class is being used.", "Checks if all the work submitted has been completed.", "A boolean indicating if all kernels in this stream are completed.", "Records an event.", "event (torch.cuda.Event, optional) \u2013 event to record. If not given, a new one will be allocated.", "Recorded event.", "Wait for all the kernels in this stream to complete.", "Note", "This is a wrapper around cudaStreamSynchronize(): see CUDA Stream documentation for more info.", "Makes all future work submitted to the stream wait for an event.", "event (torch.cuda.Event) \u2013 an event to wait for.", "Note", "This is a wrapper around cudaStreamWaitEvent(): see CUDA Stream documentation for more info.", "This function returns without waiting for event: only future operations are affected.", "Synchronizes with another stream.", "All future work submitted to this stream will wait until all kernels submitted to a given stream at the time of call complete.", "stream (Stream) \u2013 a stream to synchronize.", "Note", "This function returns without waiting for currently enqueued kernels in stream: only future operations are affected."]}, {"name": "torch.cuda.ExternalStream.query()", "path": "generated/torch.cuda.externalstream#torch.cuda.ExternalStream.query", "type": "CUDA", "text": ["Checks if all the work submitted has been completed.", "A boolean indicating if all kernels in this stream are completed."]}, {"name": "torch.cuda.ExternalStream.record_event()", "path": "generated/torch.cuda.externalstream#torch.cuda.ExternalStream.record_event", "type": "CUDA", "text": ["Records an event.", "event (torch.cuda.Event, optional) \u2013 event to record. If not given, a new one will be allocated.", "Recorded event."]}, {"name": "torch.cuda.ExternalStream.synchronize()", "path": "generated/torch.cuda.externalstream#torch.cuda.ExternalStream.synchronize", "type": "CUDA", "text": ["Wait for all the kernels in this stream to complete.", "Note", "This is a wrapper around cudaStreamSynchronize(): see CUDA Stream documentation for more info."]}, {"name": "torch.cuda.ExternalStream.wait_event()", "path": "generated/torch.cuda.externalstream#torch.cuda.ExternalStream.wait_event", "type": "CUDA", "text": ["Makes all future work submitted to the stream wait for an event.", "event (torch.cuda.Event) \u2013 an event to wait for.", "Note", "This is a wrapper around cudaStreamWaitEvent(): see CUDA Stream documentation for more info.", "This function returns without waiting for event: only future operations are affected."]}, {"name": "torch.cuda.ExternalStream.wait_stream()", "path": "generated/torch.cuda.externalstream#torch.cuda.ExternalStream.wait_stream", "type": "CUDA", "text": ["Synchronizes with another stream.", "All future work submitted to this stream will wait until all kernels submitted to a given stream at the time of call complete.", "stream (Stream) \u2013 a stream to synchronize.", "Note", "This function returns without waiting for currently enqueued kernels in stream: only future operations are affected."]}, {"name": "torch.cuda.get_allocator_backend()", "path": "generated/torch.cuda.get_allocator_backend#torch.cuda.get_allocator_backend", "type": "CUDA", "text": ["Returns a string describing the active allocator backend as set by PYTORCH_CUDA_ALLOC_CONF. Currently available backends are native (PyTorch\u2019s native caching allocator) and cudaMallocAsync` (CUDA\u2019s built-in asynchronous allocator).", "Note", "See Memory management for details on choosing the allocator backend.", "str"]}, {"name": "torch.cuda.get_arch_list()", "path": "generated/torch.cuda.get_arch_list#torch.cuda.get_arch_list", "type": "CUDA", "text": ["Returns list CUDA architectures this library was compiled for.", "List[str]"]}, {"name": "torch.cuda.get_device_capability()", "path": "generated/torch.cuda.get_device_capability#torch.cuda.get_device_capability", "type": "CUDA", "text": ["Gets the cuda capability of a device.", "device (torch.device or int, optional) \u2013 device for which to return the device capability. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).", "the major and minor cuda capability of the device", "tuple(int, int)"]}, {"name": "torch.cuda.get_device_name()", "path": "generated/torch.cuda.get_device_name#torch.cuda.get_device_name", "type": "CUDA", "text": ["Gets the name of a device.", "device (torch.device or int, optional) \u2013 device for which to return the name. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).", "the name of the device", "str"]}, {"name": "torch.cuda.get_device_properties()", "path": "generated/torch.cuda.get_device_properties#torch.cuda.get_device_properties", "type": "CUDA", "text": ["Gets the properties of a device.", "device (torch.device or int or str) \u2013 device for which to return the properties of the device.", "the properties of the device", "_CudaDeviceProperties"]}, {"name": "torch.cuda.get_gencode_flags()", "path": "generated/torch.cuda.get_gencode_flags#torch.cuda.get_gencode_flags", "type": "CUDA", "text": ["Returns NVCC gencode flags this library was compiled with.", "str"]}, {"name": "torch.cuda.get_rng_state()", "path": "generated/torch.cuda.get_rng_state#torch.cuda.get_rng_state", "type": "CUDA", "text": ["Returns the random number generator state of the specified GPU as a ByteTensor.", "device (torch.device or int, optional) \u2013 The device to return the RNG state of. Default: 'cuda' (i.e., torch.device('cuda'), the current CUDA device).", "Tensor", "Warning", "This function eagerly initializes CUDA."]}, {"name": "torch.cuda.get_rng_state_all()", "path": "generated/torch.cuda.get_rng_state_all#torch.cuda.get_rng_state_all", "type": "CUDA", "text": ["Returns a list of ByteTensor representing the random number states of all devices.", "List[Tensor]"]}, {"name": "torch.cuda.get_sync_debug_mode()", "path": "generated/torch.cuda.get_sync_debug_mode#torch.cuda.get_sync_debug_mode", "type": "CUDA", "text": ["Returns current value of debug mode for cuda synchronizing operations.", "int"]}, {"name": "torch.cuda.graph", "path": "generated/torch.cuda.graph", "type": "CUDA", "text": ["Context-manager that captures CUDA work into a torch.cuda.CUDAGraph object for later replay.", "See CUDA Graphs for a general introduction, detailed use, and constraints.", "Note", "For effective memory sharing, if you pass a pool used by a previous capture and the previous capture used an explicit stream argument, you should pass the same stream argument to this capture.", "Warning", "This API is in beta and may change in future releases."]}, {"name": "torch.cuda.graph_pool_handle()", "path": "generated/torch.cuda.graph_pool_handle#torch.cuda.graph_pool_handle", "type": "CUDA", "text": ["Returns an opaque token representing the id of a graph memory pool. See Graph memory management.", "Warning", "This API is in beta and may change in future releases."]}, {"name": "torch.cuda.init()", "path": "generated/torch.cuda.init#torch.cuda.init", "type": "CUDA", "text": ["Initialize PyTorch\u2019s CUDA state. You may need to call this explicitly if you are interacting with PyTorch via its C API, as Python bindings for CUDA functionality will not be available until this initialization takes place. Ordinary users should not need this, as all of PyTorch\u2019s CUDA methods automatically initialize CUDA state on-demand.", "Does nothing if the CUDA state is already initialized."]}, {"name": "torch.cuda.initial_seed()", "path": "generated/torch.cuda.initial_seed#torch.cuda.initial_seed", "type": "CUDA", "text": ["Returns the current random seed of the current GPU.", "Warning", "This function eagerly initializes CUDA.", "int"]}, {"name": "torch.cuda.ipc_collect()", "path": "generated/torch.cuda.ipc_collect#torch.cuda.ipc_collect", "type": "CUDA", "text": ["Force collects GPU memory after it has been released by CUDA IPC.", "Note", "Checks if any sent CUDA tensors could be cleaned from the memory. Force closes shared memory file used for reference counting if there is no active counters. Useful when the producer process stopped actively sending tensors and want to release unused memory."]}, {"name": "torch.cuda.is_available()", "path": "generated/torch.cuda.is_available#torch.cuda.is_available", "type": "CUDA", "text": ["Returns a bool indicating if CUDA is currently available.", "bool"]}, {"name": "torch.cuda.is_current_stream_capturing()", "path": "generated/torch.cuda.is_current_stream_capturing#torch.cuda.is_current_stream_capturing", "type": "CUDA", "text": ["Returns True if CUDA graph capture is underway on the current CUDA stream, False otherwise.", "If a CUDA context does not exist on the current device, returns False without initializing the context."]}, {"name": "torch.cuda.is_initialized()", "path": "generated/torch.cuda.is_initialized#torch.cuda.is_initialized", "type": "CUDA", "text": ["Returns whether PyTorch\u2019s CUDA state has been initialized."]}, {"name": "torch.cuda.jiterator._create_jit_fn()", "path": "generated/torch.cuda.jiterator._create_jit_fn#torch.cuda.jiterator._create_jit_fn", "type": "CUDA", "text": ["Create a jiterator-generated cuda kernel for an elementwise op.", "The code string has to be a valid CUDA function that describes the computation for a single element. The code string has to follow the c++ template pattern, as shown in the example below. This function will be inlined into elementwise kernel template, and compiled on the fly. Compiled kernel will be cached in memory, as well as local temp dir.", "Jiterator-generated kernels accepts noncontiguous tensors, and supports broadcasting and type promotion.", "Callable", "Example:", "code_string also allows multiple function definitions, and the last function will be treated as the entry function.", "Example:", "Jiterator can be used together with python registration to override an operator\u2019s cuda kernel. Following example is overriding gelu\u2019s cuda kernel with relu.", "Example:", "Warning", "This API is in beta and may change in future releases.", "Warning", "This API only supports up to 8 inputs and 1 output", "Warning", "All input tensors must live in CUDA device"]}, {"name": "torch.cuda.jiterator._create_multi_output_jit_fn()", "path": "generated/torch.cuda.jiterator._create_multi_output_jit_fn#torch.cuda.jiterator._create_multi_output_jit_fn", "type": "CUDA", "text": ["Create a jiterator-generated cuda kernel for an elementwise op that supports returning one or more outputs.", "Callable", "Example:", "Warning", "This API is in beta and may change in future releases.", "Warning", "This API only supports up to 8 inputs and 8 outputs"]}, {"name": "torch.cuda.list_gpu_processes()", "path": "generated/torch.cuda.list_gpu_processes#torch.cuda.list_gpu_processes", "type": "CUDA", "text": ["Returns a human-readable printout of the running processes and their GPU memory use for a given device.", "This can be useful to display periodically during training, or when handling out-of-memory exceptions.", "device (torch.device or int, optional) \u2013 selected device. Returns printout for the current device, given by current_device(), if device is None (default).", "str"]}, {"name": "torch.cuda.make_graphed_callables()", "path": "generated/torch.cuda.make_graphed_callables#torch.cuda.make_graphed_callables", "type": "CUDA", "text": ["Accepts callables (functions or nn.Modules) and returns graphed versions.", "Each graphed callable\u2019s forward pass runs its source callable\u2019s forward CUDA work as a CUDA graph inside a single autograd node.", "The graphed callable\u2019s forward pass also appends a backward node to the autograd graph. During backward, this node runs the callable\u2019s backward work as a CUDA graph.", "Therefore, each graphed callable should be a drop-in replacement for its source callable in an autograd-enabled training loop.", "See Partial-network capture for detailed use and constraints.", "If you pass a tuple of several callables, their captures will use the same memory pool. See Graph memory management for when this is appropriate.", "Note", "The requires_grad state of each Tensor in sample_args must match the state that\u2019s expected for the corresponding real input in the training loop.", "Warning", "This API is in beta and may change in future releases.", "Warning", "sample_args for each callable must contain only Tensors. Other types are not allowed.", "Warning", "Returned callables do not support higher order differentiation (e.g., double backward).", "Warning", "In any Module passed to make_graphed_callables(), only parameters may be trainable. Buffers must have requires_grad=False.", "Warning", "After you pass a torch.nn.Module through make_graphed_callables(), you may not add or remove any of that Module\u2019s parameters or buffers.", "Warning", "torch.nn.Modules passed to make_graphed_callables() must not have module hooks registered on them at the time they are passed. However, registering hooks on modules after passing them through make_graphed_callables() is allowed.", "Warning", "When running a graphed callable, you must pass its arguments in the same order and format they appeared in that callable\u2019s sample_args.", "Warning", "The automatic mixed precision is supported in make_graphed_callables() only with disabled caching. The context manager torch.cuda.amp.autocast() must have cache_enabled=False."]}, {"name": "torch.cuda.manual_seed()", "path": "generated/torch.cuda.manual_seed#torch.cuda.manual_seed", "type": "CUDA", "text": ["Sets the seed for generating random numbers for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.", "seed (int) \u2013 The desired seed.", "Warning", "If you are working with a multi-GPU model, this function is insufficient to get determinism. To seed all GPUs, use manual_seed_all()."]}, {"name": "torch.cuda.manual_seed_all()", "path": "generated/torch.cuda.manual_seed_all#torch.cuda.manual_seed_all", "type": "CUDA", "text": ["Sets the seed for generating random numbers on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.", "seed (int) \u2013 The desired seed."]}, {"name": "torch.cuda.max_memory_allocated()", "path": "generated/torch.cuda.max_memory_allocated#torch.cuda.max_memory_allocated", "type": "CUDA", "text": ["Returns the maximum GPU memory occupied by tensors in bytes for a given device.", "By default, this returns the peak allocated memory since the beginning of this program. reset_peak_memory_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak allocated memory usage of each iteration in a training loop.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.max_memory_cached()", "path": "generated/torch.cuda.max_memory_cached#torch.cuda.max_memory_cached", "type": "CUDA", "text": ["Deprecated; see max_memory_reserved().", "int"]}, {"name": "torch.cuda.max_memory_reserved()", "path": "generated/torch.cuda.max_memory_reserved#torch.cuda.max_memory_reserved", "type": "CUDA", "text": ["Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.", "By default, this returns the peak cached memory since the beginning of this program. reset_peak_memory_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak cached memory amount of each iteration in a training loop.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.mem_get_info()", "path": "generated/torch.cuda.mem_get_info#torch.cuda.mem_get_info", "type": "CUDA", "text": ["Returns the global free and total GPU memory for a given device using cudaMemGetInfo.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Tuple[int, int]", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.memory._dump_snapshot()", "path": "torch_cuda_memory#torch.cuda.memory._dump_snapshot", "type": "Miscellaneous", "text": ["Saves a pickled version of the torch.memory._snapshot() dictionary to a file. This file can be opened by the interactive snapshot viewer at pytorch.org/memory_viz", "filename (str, optional) \u2013 Name of the file to create. Defaults to \u201cdump_snapshot.pickle\u201d."]}, {"name": "torch.cuda.memory._record_memory_history()", "path": "torch_cuda_memory#torch.cuda.memory._record_memory_history", "type": "Miscellaneous", "text": ["Enables recording of stack traces associated with memory allocations, so you can tell what allocated any piece of memory in torch.cuda.memory._snapshot().", "In addition too keeping stack traces with each current allocation and free, this will also enable recording of a history of all alloc/free events.", "Use torch.cuda.memory._snapshot() to retrieve this information, and the tools in _memory_viz.py to visualize snapshots.", "The Python trace collection is fast (2us per trace), so you may consider enabling this on production jobs if you anticipate ever having to debug memory issues.", "C++ trace collection is also fast (~50ns/frame), which for many typical programs works out to ~2us per trace, but can vary depending on stack depth."]}, {"name": "torch.cuda.memory._snapshot()", "path": "torch_cuda_memory#torch.cuda.memory._snapshot", "type": "Miscellaneous", "text": ["Saves a snapshot of CUDA memory state at the time it was called. The state is represented as a dictionary with the following structure.", "The Snapshot dictionary object"]}, {"name": "torch.cuda.memory_allocated()", "path": "generated/torch.cuda.memory_allocated#torch.cuda.memory_allocated", "type": "CUDA", "text": ["Returns the current GPU memory occupied by tensors in bytes for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Note", "This is likely less than the amount shown in nvidia-smi since some unused memory can be held by the caching allocator and some context needs to be created on GPU. See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.memory_cached()", "path": "generated/torch.cuda.memory_cached#torch.cuda.memory_cached", "type": "CUDA", "text": ["Deprecated; see memory_reserved().", "int"]}, {"name": "torch.cuda.memory_reserved()", "path": "generated/torch.cuda.memory_reserved#torch.cuda.memory_reserved", "type": "CUDA", "text": ["Returns the current GPU memory managed by the caching allocator in bytes for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.memory_snapshot()", "path": "generated/torch.cuda.memory_snapshot#torch.cuda.memory_snapshot", "type": "CUDA", "text": ["Returns a snapshot of the CUDA memory allocator state across all devices.", "Interpreting the output of this function requires familiarity with the memory allocator internals.", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.memory_stats()", "path": "generated/torch.cuda.memory_stats#torch.cuda.memory_stats", "type": "CUDA", "text": ["Returns a dictionary of CUDA memory allocator statistics for a given device.", "The return value of this function is a dictionary of statistics, each of which is a non-negative integer.", "Core statistics:", "For these core statistics, values are broken down as follows.", "Pool type:", "Metric type:", "In addition to the core statistics, we also provide some simple event counters:", "The caching allocator can be configured via ENV to not split blocks larger than a defined size (see Memory Management section of the Cuda Semantics documentation). This helps avoid memory fragmentation but may have a performance penalty. Additional outputs to assist with tuning and evaluating impact:", "The caching allocator can be configured via ENV to round memory allocations in order to reduce fragmentation. Sometimes the overhead from rounding can be higher than the fragmentation it helps reduce. The following stat can be used to check if rounding adds too much overhead:", "device (torch.device or int, optional) \u2013 selected device. Returns statistics for the current device, given by current_device(), if device is None (default).", "Dict[str, Any]", "Note", "See Memory management for more details about GPU memory management.", "Note", "With backend:cudaMallocAsync, some stats are not meaningful, and are always reported as zero."]}, {"name": "torch.cuda.memory_summary()", "path": "generated/torch.cuda.memory_summary#torch.cuda.memory_summary", "type": "CUDA", "text": ["Returns a human-readable printout of the current memory allocator statistics for a given device.", "This can be useful to display periodically during training, or when handling out-of-memory exceptions.", "str", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.memory_usage()", "path": "generated/torch.cuda.memory_usage#torch.cuda.memory_usage", "type": "CUDA", "text": ["Returns the percent of time over the past sample period during which global (device) memory was being read or written. as given by nvidia-smi.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried."]}, {"name": "torch.cuda.nvtx.mark()", "path": "generated/torch.cuda.nvtx.mark#torch.cuda.nvtx.mark", "type": "CUDA", "text": ["Describe an instantaneous event that occurred at some point.", "msg (str) \u2013 ASCII message to associate with the event."]}, {"name": "torch.cuda.nvtx.range_pop()", "path": "generated/torch.cuda.nvtx.range_pop#torch.cuda.nvtx.range_pop", "type": "CUDA", "text": ["Pops a range off of a stack of nested range spans. Returns the zero-based depth of the range that is ended."]}, {"name": "torch.cuda.nvtx.range_push()", "path": "generated/torch.cuda.nvtx.range_push#torch.cuda.nvtx.range_push", "type": "CUDA", "text": ["Pushes a range onto a stack of nested range span. Returns zero-based depth of the range that is started.", "msg (str) \u2013 ASCII message to associate with range"]}, {"name": "torch.cuda.power_draw()", "path": "generated/torch.cuda.power_draw#torch.cuda.power_draw", "type": "CUDA", "text": ["over the past sample period as given by nvidia-smi for Fermi or newer fully supported devices.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried."]}, {"name": "torch.cuda.reset_max_memory_allocated()", "path": "generated/torch.cuda.reset_max_memory_allocated#torch.cuda.reset_max_memory_allocated", "type": "CUDA", "text": ["Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.", "See max_memory_allocated() for details.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Warning", "This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.reset_max_memory_cached()", "path": "generated/torch.cuda.reset_max_memory_cached#torch.cuda.reset_max_memory_cached", "type": "CUDA", "text": ["Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.", "See max_memory_cached() for details.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Warning", "This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.reset_peak_memory_stats()", "path": "generated/torch.cuda.reset_peak_memory_stats#torch.cuda.reset_peak_memory_stats", "type": "CUDA", "text": ["Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator.", "See memory_stats() for details. Peak stats correspond to the \u201cpeak\u201d key in each individual stat dict.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.seed()", "path": "generated/torch.cuda.seed#torch.cuda.seed", "type": "CUDA", "text": ["Sets the seed for generating random numbers to a random number for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.", "Warning", "If you are working with a multi-GPU model, this function will only initialize the seed on one GPU. To initialize all GPUs, use seed_all()."]}, {"name": "torch.cuda.seed_all()", "path": "generated/torch.cuda.seed_all#torch.cuda.seed_all", "type": "CUDA", "text": ["Sets the seed for generating random numbers to a random number on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored."]}, {"name": "torch.cuda.set_device()", "path": "generated/torch.cuda.set_device#torch.cuda.set_device", "type": "CUDA", "text": ["Sets the current device.", "Usage of this function is discouraged in favor of device. In most cases it\u2019s better to use CUDA_VISIBLE_DEVICES environmental variable.", "device (torch.device or int) \u2013 selected device. This function is a no-op if this argument is negative."]}, {"name": "torch.cuda.set_per_process_memory_fraction()", "path": "generated/torch.cuda.set_per_process_memory_fraction#torch.cuda.set_per_process_memory_fraction", "type": "CUDA", "text": ["Set memory fraction for a process. The fraction is used to limit an caching allocator to allocated memory on a CUDA device. The allowed value equals the total visible memory multiplied fraction. If trying to allocate more than the allowed value in a process, will raise an out of memory error in allocator.", "Note", "In general, the total available free memory is less than the total capacity."]}, {"name": "torch.cuda.set_rng_state()", "path": "generated/torch.cuda.set_rng_state#torch.cuda.set_rng_state", "type": "CUDA", "text": ["Sets the random number generator state of the specified GPU."]}, {"name": "torch.cuda.set_rng_state_all()", "path": "generated/torch.cuda.set_rng_state_all#torch.cuda.set_rng_state_all", "type": "CUDA", "text": ["Sets the random number generator state of all devices.", "new_states (Iterable of torch.ByteTensor) \u2013 The desired state for each device"]}, {"name": "torch.cuda.set_stream()", "path": "generated/torch.cuda.set_stream#torch.cuda.set_stream", "type": "CUDA", "text": ["Usage of this function is discouraged in favor of the stream context manager.", "stream (Stream) \u2013 selected stream. This function is a no-op if this argument is None."]}, {"name": "torch.cuda.set_sync_debug_mode()", "path": "generated/torch.cuda.set_sync_debug_mode#torch.cuda.set_sync_debug_mode", "type": "CUDA", "text": ["Sets the debug mode for cuda synchronizing operations.", "debug_mode (str or int) \u2013 if \u201cdefault\u201d or 0, don\u2019t error or warn on synchronizing operations, if \u201cwarn\u201d or 1, warn on synchronizing operations, if \u201cerror\u201d or 2, error out synchronizing operations.", "Warning", "This is an experimental feature, and not all synchronizing operations will trigger warning or error. In particular, operations in torch.distributed and torch.sparse namespaces are not covered yet."]}, {"name": "torch.cuda.stream()", "path": "generated/torch.cuda.stream#torch.cuda.stream", "type": "CUDA", "text": ["Wrapper around the Context-manager StreamContext that selects a given stream.", "stream (Stream) \u2013 selected stream. This manager is a no-op if it\u2019s None.", "StreamContext", "..Note:: In eager mode stream is of type Stream class while in JIT it is an object of the custom class torch.classes.cuda.Stream."]}, {"name": "torch.cuda.StreamContext", "path": "generated/torch.cuda.streamcontext", "type": "CUDA", "text": ["Context-manager that selects a given stream.", "All CUDA kernels queued within its context will be enqueued on a selected stream.", "Stream (Stream) \u2013 selected stream. This manager is a no-op if it\u2019s None.", "Note", "Streams are per-device."]}, {"name": "torch.cuda.synchronize()", "path": "generated/torch.cuda.synchronize#torch.cuda.synchronize", "type": "CUDA", "text": ["Waits for all kernels in all streams on a CUDA device to complete.", "device (torch.device or int, optional) \u2013 device for which to synchronize. It uses the current device, given by current_device(), if device is None (default)."]}, {"name": "torch.cuda.temperature()", "path": "generated/torch.cuda.temperature#torch.cuda.temperature", "type": "CUDA", "text": ["over the past sample period as given by nvidia-smi.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried."]}, {"name": "torch.cuda.torch.cuda.caching_allocator_alloc", "path": "generated/torch.cuda.caching_allocator_alloc", "type": "CUDA", "text": ["Performs a memory allocation using the CUDA memory allocator.", "Memory is allocated for a given device and a stream, this function is intended to be used for interoperability with other frameworks. Allocated memory is released through caching_allocator_delete().", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.caching_allocator_delete", "path": "generated/torch.cuda.caching_allocator_delete", "type": "CUDA", "text": ["Deletes memory allocated using the CUDA memory allocator.", "Memory allocated with caching_allocator_alloc(). is freed here. The associated device and stream are tracked inside the allocator.", "mem_ptr (int) \u2013 memory address to be freed by the allocator.", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.can_device_access_peer", "path": "generated/torch.cuda.can_device_access_peer", "type": "CUDA", "text": ["Checks if peer access between two devices is possible.", "bool"]}, {"name": "torch.cuda.torch.cuda.change_current_allocator", "path": "generated/torch.cuda.change_current_allocator", "type": "CUDA", "text": ["Changes the currently used memory allocator to be the one provided. If the current allocator has already been used/initialized, this function will error.", "allocator (torch.cuda.memory._CUDAAllocator) \u2013 allocator to be set as the active one.", "Note", "See Memory management for details on creating and using a custom allocator"]}, {"name": "torch.cuda.torch.cuda.clock_rate", "path": "generated/torch.cuda.clock_rate", "type": "CUDA", "text": ["Returns the clock speed of the GPU SM in Hz Hertz over the past sample period as given by nvidia-smi.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried."]}, {"name": "torch.cuda.torch.cuda.comm.broadcast", "path": "generated/torch.cuda.comm.broadcast", "type": "CUDA", "text": ["Broadcasts a tensor to specified GPU devices.", "Note", "Exactly one of devices and out must be specified.", "a tuple containing copies of tensor, placed on devices.", "a tuple containing out tensors, each containing a copy of tensor."]}, {"name": "torch.cuda.torch.cuda.comm.broadcast_coalesced", "path": "generated/torch.cuda.comm.broadcast_coalesced", "type": "CUDA", "text": ["Broadcasts a sequence tensors to the specified GPUs. Small tensors are first coalesced into a buffer to reduce the number of synchronizations.", "A tuple containing copies of tensor, placed on devices."]}, {"name": "torch.cuda.torch.cuda.comm.gather", "path": "generated/torch.cuda.comm.gather", "type": "CUDA", "text": ["Gathers tensors from multiple GPU devices.", "Note", "destination must not be specified when out is specified.", "a tensor located on destination device, that is a result of concatenating tensors along dim.", "the out tensor, now containing results of concatenating tensors along dim."]}, {"name": "torch.cuda.torch.cuda.comm.reduce_add", "path": "generated/torch.cuda.comm.reduce_add", "type": "CUDA", "text": ["Sums tensors from multiple GPUs.", "All inputs should have matching shapes, dtype, and layout. The output tensor will be of the same shape, dtype, and layout.", "A tensor containing an elementwise sum of all inputs, placed on the destination device."]}, {"name": "torch.cuda.torch.cuda.comm.scatter", "path": "generated/torch.cuda.comm.scatter", "type": "CUDA", "text": ["Scatters tensor across multiple GPUs.", "Note", "Exactly one of devices and out must be specified. When out is specified, chunk_sizes must not be specified and will be inferred from sizes of out.", "a tuple containing chunks of tensor, placed on devices.", "a tuple containing out tensors, each containing a chunk of tensor."]}, {"name": "torch.cuda.torch.cuda.current_blas_handle", "path": "generated/torch.cuda.current_blas_handle", "type": "CUDA", "text": ["Returns cublasHandle_t pointer to current cuBLAS handle"]}, {"name": "torch.cuda.torch.cuda.current_device", "path": "generated/torch.cuda.current_device", "type": "CUDA", "text": ["Returns the index of a currently selected device.", "int"]}, {"name": "torch.cuda.torch.cuda.current_stream", "path": "generated/torch.cuda.current_stream", "type": "CUDA", "text": ["Returns the currently selected Stream for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns the currently selected Stream for the current device, given by current_device(), if device is None (default).", "Stream"]}, {"name": "torch.cuda.torch.cuda.default_stream", "path": "generated/torch.cuda.default_stream", "type": "CUDA", "text": ["Returns the default Stream for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns the default Stream for the current device, given by current_device(), if device is None (default).", "Stream"]}, {"name": "torch.cuda.torch.cuda.device_count", "path": "generated/torch.cuda.device_count", "type": "CUDA", "text": ["Returns the number of GPUs available.", "int"]}, {"name": "torch.cuda.torch.cuda.empty_cache", "path": "generated/torch.cuda.empty_cache", "type": "CUDA", "text": ["Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.", "Note", "empty_cache() doesn\u2019t increase the amount of GPU memory available for PyTorch. However, it may help reduce fragmentation of GPU memory in certain cases. See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.get_allocator_backend", "path": "generated/torch.cuda.get_allocator_backend", "type": "CUDA", "text": ["Returns a string describing the active allocator backend as set by PYTORCH_CUDA_ALLOC_CONF. Currently available backends are native (PyTorch\u2019s native caching allocator) and cudaMallocAsync` (CUDA\u2019s built-in asynchronous allocator).", "Note", "See Memory management for details on choosing the allocator backend.", "str"]}, {"name": "torch.cuda.torch.cuda.get_arch_list", "path": "generated/torch.cuda.get_arch_list", "type": "CUDA", "text": ["Returns list CUDA architectures this library was compiled for.", "List[str]"]}, {"name": "torch.cuda.torch.cuda.get_device_capability", "path": "generated/torch.cuda.get_device_capability", "type": "CUDA", "text": ["Gets the cuda capability of a device.", "device (torch.device or int, optional) \u2013 device for which to return the device capability. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).", "the major and minor cuda capability of the device", "tuple(int, int)"]}, {"name": "torch.cuda.torch.cuda.get_device_name", "path": "generated/torch.cuda.get_device_name", "type": "CUDA", "text": ["Gets the name of a device.", "device (torch.device or int, optional) \u2013 device for which to return the name. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).", "the name of the device", "str"]}, {"name": "torch.cuda.torch.cuda.get_device_properties", "path": "generated/torch.cuda.get_device_properties", "type": "CUDA", "text": ["Gets the properties of a device.", "device (torch.device or int or str) \u2013 device for which to return the properties of the device.", "the properties of the device", "_CudaDeviceProperties"]}, {"name": "torch.cuda.torch.cuda.get_gencode_flags", "path": "generated/torch.cuda.get_gencode_flags", "type": "CUDA", "text": ["Returns NVCC gencode flags this library was compiled with.", "str"]}, {"name": "torch.cuda.torch.cuda.get_rng_state", "path": "generated/torch.cuda.get_rng_state", "type": "CUDA", "text": ["Returns the random number generator state of the specified GPU as a ByteTensor.", "device (torch.device or int, optional) \u2013 The device to return the RNG state of. Default: 'cuda' (i.e., torch.device('cuda'), the current CUDA device).", "Tensor", "Warning", "This function eagerly initializes CUDA."]}, {"name": "torch.cuda.torch.cuda.get_rng_state_all", "path": "generated/torch.cuda.get_rng_state_all", "type": "CUDA", "text": ["Returns a list of ByteTensor representing the random number states of all devices.", "List[Tensor]"]}, {"name": "torch.cuda.torch.cuda.get_sync_debug_mode", "path": "generated/torch.cuda.get_sync_debug_mode", "type": "CUDA", "text": ["Returns current value of debug mode for cuda synchronizing operations.", "int"]}, {"name": "torch.cuda.torch.cuda.graph_pool_handle", "path": "generated/torch.cuda.graph_pool_handle", "type": "CUDA", "text": ["Returns an opaque token representing the id of a graph memory pool. See Graph memory management.", "Warning", "This API is in beta and may change in future releases."]}, {"name": "torch.cuda.torch.cuda.init", "path": "generated/torch.cuda.init", "type": "CUDA", "text": ["Initialize PyTorch\u2019s CUDA state. You may need to call this explicitly if you are interacting with PyTorch via its C API, as Python bindings for CUDA functionality will not be available until this initialization takes place. Ordinary users should not need this, as all of PyTorch\u2019s CUDA methods automatically initialize CUDA state on-demand.", "Does nothing if the CUDA state is already initialized."]}, {"name": "torch.cuda.torch.cuda.initial_seed", "path": "generated/torch.cuda.initial_seed", "type": "CUDA", "text": ["Returns the current random seed of the current GPU.", "Warning", "This function eagerly initializes CUDA.", "int"]}, {"name": "torch.cuda.torch.cuda.ipc_collect", "path": "generated/torch.cuda.ipc_collect", "type": "CUDA", "text": ["Force collects GPU memory after it has been released by CUDA IPC.", "Note", "Checks if any sent CUDA tensors could be cleaned from the memory. Force closes shared memory file used for reference counting if there is no active counters. Useful when the producer process stopped actively sending tensors and want to release unused memory."]}, {"name": "torch.cuda.torch.cuda.is_available", "path": "generated/torch.cuda.is_available", "type": "CUDA", "text": ["Returns a bool indicating if CUDA is currently available.", "bool"]}, {"name": "torch.cuda.torch.cuda.is_current_stream_capturing", "path": "generated/torch.cuda.is_current_stream_capturing", "type": "CUDA", "text": ["Returns True if CUDA graph capture is underway on the current CUDA stream, False otherwise.", "If a CUDA context does not exist on the current device, returns False without initializing the context."]}, {"name": "torch.cuda.torch.cuda.is_initialized", "path": "generated/torch.cuda.is_initialized", "type": "CUDA", "text": ["Returns whether PyTorch\u2019s CUDA state has been initialized."]}, {"name": "torch.cuda.torch.cuda.jiterator._create_jit_fn", "path": "generated/torch.cuda.jiterator._create_jit_fn", "type": "CUDA", "text": ["Create a jiterator-generated cuda kernel for an elementwise op.", "The code string has to be a valid CUDA function that describes the computation for a single element. The code string has to follow the c++ template pattern, as shown in the example below. This function will be inlined into elementwise kernel template, and compiled on the fly. Compiled kernel will be cached in memory, as well as local temp dir.", "Jiterator-generated kernels accepts noncontiguous tensors, and supports broadcasting and type promotion.", "Callable", "Example:", "code_string also allows multiple function definitions, and the last function will be treated as the entry function.", "Example:", "Jiterator can be used together with python registration to override an operator\u2019s cuda kernel. Following example is overriding gelu\u2019s cuda kernel with relu.", "Example:", "Warning", "This API is in beta and may change in future releases.", "Warning", "This API only supports up to 8 inputs and 1 output", "Warning", "All input tensors must live in CUDA device"]}, {"name": "torch.cuda.torch.cuda.jiterator._create_multi_output_jit_fn", "path": "generated/torch.cuda.jiterator._create_multi_output_jit_fn", "type": "CUDA", "text": ["Create a jiterator-generated cuda kernel for an elementwise op that supports returning one or more outputs.", "Callable", "Example:", "Warning", "This API is in beta and may change in future releases.", "Warning", "This API only supports up to 8 inputs and 8 outputs"]}, {"name": "torch.cuda.torch.cuda.list_gpu_processes", "path": "generated/torch.cuda.list_gpu_processes", "type": "CUDA", "text": ["Returns a human-readable printout of the running processes and their GPU memory use for a given device.", "This can be useful to display periodically during training, or when handling out-of-memory exceptions.", "device (torch.device or int, optional) \u2013 selected device. Returns printout for the current device, given by current_device(), if device is None (default).", "str"]}, {"name": "torch.cuda.torch.cuda.make_graphed_callables", "path": "generated/torch.cuda.make_graphed_callables", "type": "CUDA", "text": ["Accepts callables (functions or nn.Modules) and returns graphed versions.", "Each graphed callable\u2019s forward pass runs its source callable\u2019s forward CUDA work as a CUDA graph inside a single autograd node.", "The graphed callable\u2019s forward pass also appends a backward node to the autograd graph. During backward, this node runs the callable\u2019s backward work as a CUDA graph.", "Therefore, each graphed callable should be a drop-in replacement for its source callable in an autograd-enabled training loop.", "See Partial-network capture for detailed use and constraints.", "If you pass a tuple of several callables, their captures will use the same memory pool. See Graph memory management for when this is appropriate.", "Note", "The requires_grad state of each Tensor in sample_args must match the state that\u2019s expected for the corresponding real input in the training loop.", "Warning", "This API is in beta and may change in future releases.", "Warning", "sample_args for each callable must contain only Tensors. Other types are not allowed.", "Warning", "Returned callables do not support higher order differentiation (e.g., double backward).", "Warning", "In any Module passed to make_graphed_callables(), only parameters may be trainable. Buffers must have requires_grad=False.", "Warning", "After you pass a torch.nn.Module through make_graphed_callables(), you may not add or remove any of that Module\u2019s parameters or buffers.", "Warning", "torch.nn.Modules passed to make_graphed_callables() must not have module hooks registered on them at the time they are passed. However, registering hooks on modules after passing them through make_graphed_callables() is allowed.", "Warning", "When running a graphed callable, you must pass its arguments in the same order and format they appeared in that callable\u2019s sample_args.", "Warning", "The automatic mixed precision is supported in make_graphed_callables() only with disabled caching. The context manager torch.cuda.amp.autocast() must have cache_enabled=False."]}, {"name": "torch.cuda.torch.cuda.manual_seed", "path": "generated/torch.cuda.manual_seed", "type": "CUDA", "text": ["Sets the seed for generating random numbers for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.", "seed (int) \u2013 The desired seed.", "Warning", "If you are working with a multi-GPU model, this function is insufficient to get determinism. To seed all GPUs, use manual_seed_all()."]}, {"name": "torch.cuda.torch.cuda.manual_seed_all", "path": "generated/torch.cuda.manual_seed_all", "type": "CUDA", "text": ["Sets the seed for generating random numbers on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.", "seed (int) \u2013 The desired seed."]}, {"name": "torch.cuda.torch.cuda.max_memory_allocated", "path": "generated/torch.cuda.max_memory_allocated", "type": "CUDA", "text": ["Returns the maximum GPU memory occupied by tensors in bytes for a given device.", "By default, this returns the peak allocated memory since the beginning of this program. reset_peak_memory_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak allocated memory usage of each iteration in a training loop.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.max_memory_cached", "path": "generated/torch.cuda.max_memory_cached", "type": "CUDA", "text": ["Deprecated; see max_memory_reserved().", "int"]}, {"name": "torch.cuda.torch.cuda.max_memory_reserved", "path": "generated/torch.cuda.max_memory_reserved", "type": "CUDA", "text": ["Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.", "By default, this returns the peak cached memory since the beginning of this program. reset_peak_memory_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak cached memory amount of each iteration in a training loop.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.mem_get_info", "path": "generated/torch.cuda.mem_get_info", "type": "CUDA", "text": ["Returns the global free and total GPU memory for a given device using cudaMemGetInfo.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Tuple[int, int]", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.memory_allocated", "path": "generated/torch.cuda.memory_allocated", "type": "CUDA", "text": ["Returns the current GPU memory occupied by tensors in bytes for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Note", "This is likely less than the amount shown in nvidia-smi since some unused memory can be held by the caching allocator and some context needs to be created on GPU. See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.memory_cached", "path": "generated/torch.cuda.memory_cached", "type": "CUDA", "text": ["Deprecated; see memory_reserved().", "int"]}, {"name": "torch.cuda.torch.cuda.memory_reserved", "path": "generated/torch.cuda.memory_reserved", "type": "CUDA", "text": ["Returns the current GPU memory managed by the caching allocator in bytes for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.memory_snapshot", "path": "generated/torch.cuda.memory_snapshot", "type": "CUDA", "text": ["Returns a snapshot of the CUDA memory allocator state across all devices.", "Interpreting the output of this function requires familiarity with the memory allocator internals.", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.memory_stats", "path": "generated/torch.cuda.memory_stats", "type": "CUDA", "text": ["Returns a dictionary of CUDA memory allocator statistics for a given device.", "The return value of this function is a dictionary of statistics, each of which is a non-negative integer.", "Core statistics:", "For these core statistics, values are broken down as follows.", "Pool type:", "Metric type:", "In addition to the core statistics, we also provide some simple event counters:", "The caching allocator can be configured via ENV to not split blocks larger than a defined size (see Memory Management section of the Cuda Semantics documentation). This helps avoid memory fragmentation but may have a performance penalty. Additional outputs to assist with tuning and evaluating impact:", "The caching allocator can be configured via ENV to round memory allocations in order to reduce fragmentation. Sometimes the overhead from rounding can be higher than the fragmentation it helps reduce. The following stat can be used to check if rounding adds too much overhead:", "device (torch.device or int, optional) \u2013 selected device. Returns statistics for the current device, given by current_device(), if device is None (default).", "Dict[str, Any]", "Note", "See Memory management for more details about GPU memory management.", "Note", "With backend:cudaMallocAsync, some stats are not meaningful, and are always reported as zero."]}, {"name": "torch.cuda.torch.cuda.memory_summary", "path": "generated/torch.cuda.memory_summary", "type": "CUDA", "text": ["Returns a human-readable printout of the current memory allocator statistics for a given device.", "This can be useful to display periodically during training, or when handling out-of-memory exceptions.", "str", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.memory_usage", "path": "generated/torch.cuda.memory_usage", "type": "CUDA", "text": ["Returns the percent of time over the past sample period during which global (device) memory was being read or written. as given by nvidia-smi.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried."]}, {"name": "torch.cuda.torch.cuda.nvtx.mark", "path": "generated/torch.cuda.nvtx.mark", "type": "CUDA", "text": ["Describe an instantaneous event that occurred at some point.", "msg (str) \u2013 ASCII message to associate with the event."]}, {"name": "torch.cuda.torch.cuda.nvtx.range_pop", "path": "generated/torch.cuda.nvtx.range_pop", "type": "CUDA", "text": ["Pops a range off of a stack of nested range spans. Returns the zero-based depth of the range that is ended."]}, {"name": "torch.cuda.torch.cuda.nvtx.range_push", "path": "generated/torch.cuda.nvtx.range_push", "type": "CUDA", "text": ["Pushes a range onto a stack of nested range span. Returns zero-based depth of the range that is started.", "msg (str) \u2013 ASCII message to associate with range"]}, {"name": "torch.cuda.torch.cuda.OutOfMemoryError", "path": "generated/torch.cuda.outofmemoryerror", "type": "CUDA", "text": ["Exception raised when CUDA is out of memory"]}, {"name": "torch.cuda.torch.cuda.power_draw", "path": "generated/torch.cuda.power_draw", "type": "CUDA", "text": ["over the past sample period as given by nvidia-smi for Fermi or newer fully supported devices.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried."]}, {"name": "torch.cuda.torch.cuda.reset_max_memory_allocated", "path": "generated/torch.cuda.reset_max_memory_allocated", "type": "CUDA", "text": ["Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.", "See max_memory_allocated() for details.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Warning", "This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.reset_max_memory_cached", "path": "generated/torch.cuda.reset_max_memory_cached", "type": "CUDA", "text": ["Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.", "See max_memory_cached() for details.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Warning", "This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.reset_peak_memory_stats", "path": "generated/torch.cuda.reset_peak_memory_stats", "type": "CUDA", "text": ["Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator.", "See memory_stats() for details. Peak stats correspond to the \u201cpeak\u201d key in each individual stat dict.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.seed", "path": "generated/torch.cuda.seed", "type": "CUDA", "text": ["Sets the seed for generating random numbers to a random number for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.", "Warning", "If you are working with a multi-GPU model, this function will only initialize the seed on one GPU. To initialize all GPUs, use seed_all()."]}, {"name": "torch.cuda.torch.cuda.seed_all", "path": "generated/torch.cuda.seed_all", "type": "CUDA", "text": ["Sets the seed for generating random numbers to a random number on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored."]}, {"name": "torch.cuda.torch.cuda.set_device", "path": "generated/torch.cuda.set_device", "type": "CUDA", "text": ["Sets the current device.", "Usage of this function is discouraged in favor of device. In most cases it\u2019s better to use CUDA_VISIBLE_DEVICES environmental variable.", "device (torch.device or int) \u2013 selected device. This function is a no-op if this argument is negative."]}, {"name": "torch.cuda.torch.cuda.set_per_process_memory_fraction", "path": "generated/torch.cuda.set_per_process_memory_fraction", "type": "CUDA", "text": ["Set memory fraction for a process. The fraction is used to limit an caching allocator to allocated memory on a CUDA device. The allowed value equals the total visible memory multiplied fraction. If trying to allocate more than the allowed value in a process, will raise an out of memory error in allocator.", "Note", "In general, the total available free memory is less than the total capacity."]}, {"name": "torch.cuda.torch.cuda.set_rng_state", "path": "generated/torch.cuda.set_rng_state", "type": "CUDA", "text": ["Sets the random number generator state of the specified GPU."]}, {"name": "torch.cuda.torch.cuda.set_rng_state_all", "path": "generated/torch.cuda.set_rng_state_all", "type": "CUDA", "text": ["Sets the random number generator state of all devices.", "new_states (Iterable of torch.ByteTensor) \u2013 The desired state for each device"]}, {"name": "torch.cuda.torch.cuda.set_stream", "path": "generated/torch.cuda.set_stream", "type": "CUDA", "text": ["Usage of this function is discouraged in favor of the stream context manager.", "stream (Stream) \u2013 selected stream. This function is a no-op if this argument is None."]}, {"name": "torch.cuda.torch.cuda.set_sync_debug_mode", "path": "generated/torch.cuda.set_sync_debug_mode", "type": "CUDA", "text": ["Sets the debug mode for cuda synchronizing operations.", "debug_mode (str or int) \u2013 if \u201cdefault\u201d or 0, don\u2019t error or warn on synchronizing operations, if \u201cwarn\u201d or 1, warn on synchronizing operations, if \u201cerror\u201d or 2, error out synchronizing operations.", "Warning", "This is an experimental feature, and not all synchronizing operations will trigger warning or error. In particular, operations in torch.distributed and torch.sparse namespaces are not covered yet."]}, {"name": "torch.cuda.torch.cuda.stream", "path": "generated/torch.cuda.stream", "type": "CUDA", "text": ["Wrapper around the Context-manager StreamContext that selects a given stream.", "stream (Stream) \u2013 selected stream. This manager is a no-op if it\u2019s None.", "StreamContext", "..Note:: In eager mode stream is of type Stream class while in JIT it is an object of the custom class torch.classes.cuda.Stream."]}, {"name": "torch.cuda.torch.cuda.synchronize", "path": "generated/torch.cuda.synchronize", "type": "CUDA", "text": ["Waits for all kernels in all streams on a CUDA device to complete.", "device (torch.device or int, optional) \u2013 device for which to synchronize. It uses the current device, given by current_device(), if device is None (default)."]}, {"name": "torch.cuda.torch.cuda.temperature", "path": "generated/torch.cuda.temperature", "type": "CUDA", "text": ["over the past sample period as given by nvidia-smi.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried."]}, {"name": "torch.cuda.torch.cuda.utilization", "path": "generated/torch.cuda.utilization", "type": "CUDA", "text": ["Returns the percent of time over the past sample period during which one or more kernels was executing on the GPU as given by nvidia-smi.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried."]}, {"name": "torch.cuda.utilization()", "path": "generated/torch.cuda.utilization#torch.cuda.utilization", "type": "CUDA", "text": ["Returns the percent of time over the past sample period during which one or more kernels was executing on the GPU as given by nvidia-smi.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried."]}, {"name": "torch.cummax", "path": "generated/torch.cummax", "type": "Torch", "text": ["Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim. And indices is the index location of each maximum value found in the dimension dim.", "out (tuple, optional) \u2013 the result tuple of two output tensors (values, indices)", "Example:"]}, {"name": "torch.cummin", "path": "generated/torch.cummin", "type": "Torch", "text": ["Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim. And indices is the index location of each maximum value found in the dimension dim.", "out (tuple, optional) \u2013 the result tuple of two output tensors (values, indices)", "Example:"]}, {"name": "torch.cumprod", "path": "generated/torch.cumprod", "type": "Torch", "text": ["Returns the cumulative product of elements of input in the dimension dim.", "For example, if input is a vector of size N, the result will also be a vector of size N, with elements.", "Example:"]}, {"name": "torch.cumsum", "path": "generated/torch.cumsum", "type": "Torch", "text": ["Returns the cumulative sum of elements of input in the dimension dim.", "For example, if input is a vector of size N, the result will also be a vector of size N, with elements.", "Example:"]}, {"name": "torch.cumulative_trapezoid", "path": "generated/torch.cumulative_trapezoid", "type": "Torch", "text": ["Cumulatively computes the trapezoidal rule along dim. By default the spacing between elements is assumed to be 1, but dx can be used to specify a different constant spacing, and x can be used to specify arbitrary spacing along dim.", "For more details, please read torch.trapezoid(). The difference between torch.trapezoid() and this function is that, torch.trapezoid() returns a value for each integration, where as this function returns a cumulative value for every spacing within the integration. This is analogous to how .sum returns a value and .cumsum returns a cumulative sum.", "Examples:"]}, {"name": "torch.deg2rad", "path": "generated/torch.deg2rad", "type": "Torch", "text": ["Returns a new tensor with each of the elements of input converted from angles in degrees to radians.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.dequantize", "path": "generated/torch.dequantize", "type": "Torch", "text": ["Returns an fp32 Tensor by dequantizing a quantized Tensor", "tensor (Tensor) \u2013 A quantized Tensor", "Given a list of quantized Tensors, dequantize them and return a list of fp32 Tensors", "tensors (sequence of Tensors) \u2013 A list of quantized Tensors"]}, {"name": "torch.det", "path": "generated/torch.det", "type": "Torch", "text": ["Alias for torch.linalg.det()"]}, {"name": "torch.device", "path": "tensor_attributes#torch.device", "type": "Miscellaneous", "text": []}, {"name": "torch.diag", "path": "generated/torch.diag", "type": "Torch", "text": ["The argument diagonal controls which diagonal to consider:", "out (Tensor, optional) \u2013 the output tensor.", "See also", "torch.diagonal() always returns the diagonal of its input.", "torch.diagflat() always constructs a tensor with diagonal elements specified by the input.", "Examples:", "Get the square matrix where the input vector is the diagonal:", "Get the k-th diagonal of a given matrix:"]}, {"name": "torch.diag_embed", "path": "generated/torch.diag_embed", "type": "Torch", "text": ["Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input. To facilitate creating batched diagonal matrices, the 2D planes formed by the last two dimensions of the returned tensor are chosen by default.", "The argument offset controls which diagonal to consider:", "The size of the new matrix will be calculated to make the specified diagonal of the size of the last input dimension. Note that for offset other than 00, the order of dim1 and dim2 matters. Exchanging them is equivalent to changing the sign of offset.", "Applying torch.diagonal() to the output of this function with the same arguments yields a matrix identical to input. However, torch.diagonal() has different default dimensions, so those need to be explicitly specified.", "Example:"]}, {"name": "torch.diagflat", "path": "generated/torch.diagflat", "type": "Torch", "text": ["The argument offset controls which diagonal to consider:", "Examples:"]}, {"name": "torch.diagonal", "path": "generated/torch.diagonal", "type": "Torch", "text": ["Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.", "The argument offset controls which diagonal to consider:", "Applying torch.diag_embed() to the output of this function with the same arguments yields a diagonal matrix with the diagonal entries of the input. However, torch.diag_embed() has different default dimensions, so those need to be explicitly specified.", "Note", "To take a batch diagonal, pass in dim1=-2, dim2=-1.", "Examples:"]}, {"name": "torch.diagonal_scatter", "path": "generated/torch.diagonal_scatter", "type": "Torch", "text": ["Embeds the values of the src tensor into input along the diagonal elements of input, with respect to dim1 and dim2.", "This function returns a tensor with fresh storage; it does not return a view.", "The argument offset controls which diagonal to consider:", "Note", "src must be of the proper size in order to be embedded into input. Specifically, it should have the same shape as torch.diagonal(input, offset, dim1, dim2)", "Examples:"]}, {"name": "torch.diff", "path": "generated/torch.diff", "type": "Torch", "text": ["Computes the n-th forward difference along the given dimension.", "The first-order differences are given by out[i] = input[i + 1] - input[i]. Higher-order differences are calculated by using torch.diff() recursively.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.digamma", "path": "generated/torch.digamma", "type": "Torch", "text": ["Alias for torch.special.digamma()."]}, {"name": "torch.dist", "path": "generated/torch.dist", "type": "Torch", "text": ["Returns the p-norm of (input - other)", "The shapes of input and other must be broadcastable.", "Example:"]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks.noop_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks.noop_hook", "type": "DDP Communication Hooks", "text": ["This DDP communication hook returns a future that wraps the input, so it is a noop that does not incur any communication overheads.", "This hook should only be used for headroom analysis of allreduce optimization, instead of the normal gradient synchronization. For example, if only less than 10% speedup of training time can be observed after this hook is registered, it usually implies that allreduce is not a performance bottleneck for this case. Such instrumentation can be particularly useful if GPU traces cannot be easily retrieved or the trace analysis is complicated some factors such as the overlap between allreduce and computation or the desynchronization across ranks.", "Future[Tensor]"]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook", "type": "DDP Communication Hooks", "text": ["This DDP communication hook just calls allreduce using GradBucket tensors. Once gradient tensors are aggregated across all workers, its then callback takes the mean and returns the result. If user registers this hook, DDP results is expected to be same as the case where no hook was registered. Hence, this won\u2019t change behavior of DDP and user can use this as a reference or modify this hook to log useful information or any other purposes while unaffecting DDP behavior.", "Future[Tensor]"]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_hook", "type": "DDP Communication Hooks", "text": ["Warning: This API is experimental, and it requires NCCL version later than 2.9.6.", "This DDP communication hook implements a simple gradient compression approach that casts GradBucket tensor to half-precision Brain floating point format (torch.bfloat16) and then divides it by the process group size. It allreduces those bfloat16 gradient tensors. Once compressed gradient tensors are allreduced, the chained callback decompress casts it back to the input data type (such as float32).", "Future[Tensor]"]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_wrapper()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_wrapper", "type": "DDP Communication Hooks", "text": ["Warning: This API is experimental, and it requires NCCL version later than 2.9.6.", "This wrapper casts the input gradient tensor of a given DDP communication hook to half-precision Brain floating point format <https://en.wikipedia.org/wiki/Bfloat16_floating-point_format> `_ (``torch.bfloat16`), and casts the resulting tensor of the given hook back to the input data type, such as float32.", "Therefore, bf16_compress_hook is equivalent to bf16_compress_wrapper(allreduce_hook).", "Callable[[Any, GradBucket], Future[Tensor]]"]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook", "type": "DDP Communication Hooks", "text": ["This DDP communication hook implements a simple gradient compression approach that casts GradBucket tensor to half-precision floating-point format (torch.float16) and then divides it by the process group size. It allreduces those float16 gradient tensors. Once compressed gradient tensors are allreduced, the chained callback decompress casts it back to the input data type (such as float32).", "Future[Tensor]"]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_wrapper()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_wrapper", "type": "DDP Communication Hooks", "text": ["This wrapper casts the input gradient tensor of a given DDP communication hook to half-precision floating point format (torch.float16), and casts the resulting tensor of the given hook back to the input data type, such as float32.", "Therefore, fp16_compress_hook is equivalent to fp16_compress_wrapper(allreduce_hook).", "Callable[[Any, GradBucket], Future[Tensor]]"]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook", "type": "DDP Communication Hooks", "text": ["This DDP communication hook implements a simplified PowerSGD gradient compression algorithm described in the paper. This variant does not compress the gradients layer by layer, but instead compresses the flattened input tensor that batches all the gradients. Therefore, it is faster than powerSGD_hook(), but usually results in a much lower accuracy, unless matrix_approximation_rank is 1.", "Warning", "Increasing matrix_approximation_rank here may not necessarily increase the accuracy, because batching per-parameter tensors without column/row alignment can destroy low-rank structure. Therefore, the user should always consider powerSGD_hook() first, and only consider this variant when a satisfactory accuracy can be achieved when matrix_approximation_rank is 1.", "Once gradient tensors are aggregated across all workers, this hook applies compression as follows:", "Note that this communication hook enforces vanilla allreduce for the first state.start_powerSGD_iter iterations. This not only gives the user more control over the tradeoff between speedup and accuracy, but also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers.", "Future handler of the communication, which updates the gradients in place.", "Future[Tensor]"]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook", "type": "DDP Communication Hooks", "text": ["This DDP communication hook implements PowerSGD gradient compression algorithm described in the paper. Once gradient tensors are aggregated across all workers, this hook applies compression as follows:", "Views the input flattened 1D gradient tensor as a list of per-parameter tensors, and divides all the tensors into two groups:", "1.1 The tensors that should be compressed before allreduce, because the compression can give enough saving in bandwidth.", "1.2 Rest of the tensors will be directly allreduced without compression, including all the vector tensors (for biases).", "Handles uncompressed tensors:", "2.1. Allocate contiguous memory for those uncompressed tensors, and allreduces all the uncompressed tensors as a batch, without compression;", "2.2. Copies the individual uncompressed tensors from the contiguous memory back to the input tensor.", "Handles the tensors that should be compressed by PowerSGD compression:", "3.1. For each tensor M, creates two low-rank tensors P and Q for decomposing M, such that M = PQ^T, where Q is initialized from a standard normal distribution and orthogonalized;", "3.2. Computes each P in Ps, which is equal to MQ;", "3.3. Allreduces Ps as a batch;", "3.4. Orthogonalizes each P in Ps;", "3.5. Computes each Q in Qs, which is approximately equal to M^TP;", "3.6. Allreduces Qs as a batch;", "3.7. Computes each M among all the compressed tensors, which is approximately equal to PQ^T.", "Note that this communication hook enforces vanilla allreduce for the first state.start_powerSGD_iter iterations. This not only gives the user more control over the tradeoff between speedup and accuracy, but also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers.", "Future handler of the communication, which updates the gradients in place.", "Future[Tensor]"]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState", "type": "DDP Communication Hooks", "text": ["Stores both the algorithm\u2019s hyperparameters and the internal state for all the gradients during the training. Particularly, matrix_approximation_rank and start_powerSGD_iter are the main hyperparameters that should be tuned by the user. For performance, we suggest to keep binary hyperparameters use_error_feedback and warm_start on.", "matrix_approximation_rank controls the size of compressed low-rank tensors, which determines the compression rate. The lower the rank, the stronger the compression.", "1.1. If matrix_approximation_rank is too low, the full model quality will need more training steps to reach or will never reach and yield loss in accuracy.", "1.2. The increase of matrix_approximation_rank can substantially increase the computation costs of the compression, and the accuracy may not be further improved beyond a certain matrix_approximation_rank threshold.", "To tune matrix_approximation_rank, we suggest to start from 1 and increase by factors of 2 (like an exponential grid search, 1, 2, 4, \u2026), until a satisfactory accuracy is reached. Typically only a small value 1-4 is used. For some NLP tasks (as shown in Appendix D of the original paper), this value has been increased to 32.", "To tune start_powerSGD_iter, we suggest to start with 10% of total training steps, and increase it until a satisfactory accuracy is reached. If there is a warm-up stage in the training, start_powerSGD_iter typically should be no less than the number of warm-up steps.", "Compression statistics are logged every compression_stats_logging_frequency iterations once PowerSGD compression starts.", "Warning", "If error feedback or warm-up is enabled, the minimum value of start_powerSGD_iter allowed in DDP is 2. This is because there is another internal optimization that rebuilds buckets at iteration 1 in DDP, and this can conflict with any tensor memorized before the rebuild process."]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState.__getstate__()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState.__getstate__", "type": "DDP Communication Hooks", "text": ["Returns a Dict[str, Any] which will be pickled and saved. process_group is not serializable and excluded from a returned state."]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState.__setstate__()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState.__setstate__", "type": "DDP Communication Hooks", "text": ["Takes a provided state and retrieves PowerSGDState. process_group is set to default."]}, {"name": "torch.distributed.algorithms.Join", "path": "distributed.algorithms.join#torch.distributed.algorithms.Join", "type": "Miscellaneous", "text": ["This class defines the generic join context manager, which allows custom hooks to be called after a process joins. These hooks should shadow the collective communications of non-joined processes to prevent hanging and erroring and to ensure algorithmic correctness. Refer to JoinHook for details about the hook definition.", "Warning", "The context manager requires each participating Joinable to call the method notify_join_context() before its own per- iteration collective communications to ensure correctness.", "Warning", "The context manager requires that all process_group attributes in the JoinHook objects are the same. If there are multiple JoinHook objects, then the device of the first is used. The process group and device information is used for checking for non- joined processes and for notifying processes to throw an exception if throw_on_early_termination is enabled, both of which using an all- reduce.", "Example:", "Notifies the join context manager that the calling process has not yet joined; then, if throw_on_early_termination=True, checks if uneven inputs have been detected (i.e. if one process has already joined) and throws an exception if so.", "This method should be called from a Joinable object before its per-iteration collective communications. For example, this should be called at the beginning of the forward pass in DistributedDataParallel.", "Only the first Joinable object passed into the context manager performs the collective communications in this method, and for the others, this method is vacuous.", "joinable (Joinable) \u2013 the Joinable object calling this method.", "An async work handle for the all-reduce meant to notify the context manager that the process has not yet joined if joinable is the first one passed into the context manager; None otherwise."]}, {"name": "torch.distributed.algorithms.Join.notify_join_context()", "path": "distributed.algorithms.join#torch.distributed.algorithms.Join.notify_join_context", "type": "Miscellaneous", "text": ["Notifies the join context manager that the calling process has not yet joined; then, if throw_on_early_termination=True, checks if uneven inputs have been detected (i.e. if one process has already joined) and throws an exception if so.", "This method should be called from a Joinable object before its per-iteration collective communications. For example, this should be called at the beginning of the forward pass in DistributedDataParallel.", "Only the first Joinable object passed into the context manager performs the collective communications in this method, and for the others, this method is vacuous.", "joinable (Joinable) \u2013 the Joinable object calling this method.", "An async work handle for the all-reduce meant to notify the context manager that the process has not yet joined if joinable is the first one passed into the context manager; None otherwise."]}, {"name": "torch.distributed.algorithms.Joinable", "path": "distributed.algorithms.join#torch.distributed.algorithms.Joinable", "type": "Miscellaneous", "text": ["This defines an abstract base class for joinable classes. A joinable class (inheriting from Joinable) should implement join_hook(), which returns a JoinHook instance, in addition to join_device() and join_process_group() that return device and process group information, respectively.", "Returns the device from which to perform collective communications needed by the join context manager implementation itself.", "Returns a JoinHook instance for the given Joinable.", "kwargs (dict) \u2013 a dict containing any keyword arguments to modify the behavior of the join hook at run time; all Joinable instances sharing the same join context manager are forwarded the same value for kwargs.", "JoinHook", "Returns the process group for the collective communications needed by the join context manager itself."]}, {"name": "torch.distributed.algorithms.Joinable.join_device", "path": "distributed.algorithms.join#torch.distributed.algorithms.Joinable.join_device", "type": "Miscellaneous", "text": ["Returns the device from which to perform collective communications needed by the join context manager implementation itself."]}, {"name": "torch.distributed.algorithms.Joinable.join_hook()", "path": "distributed.algorithms.join#torch.distributed.algorithms.Joinable.join_hook", "type": "Miscellaneous", "text": ["Returns a JoinHook instance for the given Joinable.", "kwargs (dict) \u2013 a dict containing any keyword arguments to modify the behavior of the join hook at run time; all Joinable instances sharing the same join context manager are forwarded the same value for kwargs.", "JoinHook"]}, {"name": "torch.distributed.algorithms.Joinable.join_process_group", "path": "distributed.algorithms.join#torch.distributed.algorithms.Joinable.join_process_group", "type": "Miscellaneous", "text": ["Returns the process group for the collective communications needed by the join context manager itself."]}, {"name": "torch.distributed.algorithms.JoinHook", "path": "distributed.algorithms.join#torch.distributed.algorithms.JoinHook", "type": "Miscellaneous", "text": ["This defines a join hook, which provides two entry points in the join context manager: a main hook, which is called repeatedly while there exists a non-joined process, and a post-hook, which is called once all processes have joined.", "To implement a join hook for the generic join context manager, define a class that inherits from JoinHook and override main_hook() and post_hook() as appropriate.", "This hook is called repeatedly while there exists a non-joined process to shadow collective communications in one training iteration (i.e. in one forward pass, backward pass, and optimizer step).", "This hook is called after all processes have joined. It is passed an additional bool argument is_last_joiner, which indicates if the rank is one of the last to join.", "is_last_joiner (bool) \u2013 True if the rank is one of the last to join; False otherwise."]}, {"name": "torch.distributed.algorithms.JoinHook.main_hook()", "path": "distributed.algorithms.join#torch.distributed.algorithms.JoinHook.main_hook", "type": "Miscellaneous", "text": ["This hook is called repeatedly while there exists a non-joined process to shadow collective communications in one training iteration (i.e. in one forward pass, backward pass, and optimizer step)."]}, {"name": "torch.distributed.algorithms.JoinHook.post_hook()", "path": "distributed.algorithms.join#torch.distributed.algorithms.JoinHook.post_hook", "type": "Miscellaneous", "text": ["This hook is called after all processes have joined. It is passed an additional bool argument is_last_joiner, which indicates if the rank is one of the last to join.", "is_last_joiner (bool) \u2013 True if the rank is one of the last to join; False otherwise."]}, {"name": "torch.distributed.all_gather()", "path": "distributed#torch.distributed.all_gather", "type": "Distributed Communication", "text": ["Gathers tensors from the whole group in a list.", "Complex tensors are supported.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.all_gather_into_tensor()", "path": "distributed#torch.distributed.all_gather_into_tensor", "type": "Distributed Communication", "text": ["Gather tensors from all ranks and put them in a single output tensor.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group", "Warning", "The Gloo backend does not support this API."]}, {"name": "torch.distributed.all_gather_multigpu()", "path": "distributed#torch.distributed.all_gather_multigpu", "type": "Distributed Communication", "text": ["Gathers tensors from the whole group in a list. Each tensor in tensor_list should reside on a separate GPU", "Only nccl backend is currently supported tensors should only be GPU tensors", "Complex tensors are supported.", "output_tensor_lists (List[List[Tensor]]) \u2013 ", "Output lists. It should contain correctly-sized tensors on each GPU to be used for output of the collective, e.g. output_tensor_lists[i] contains the all_gather result that resides on the GPU of input_tensor_list[i].", "Note that each element of output_tensor_lists has the size of world_size * len(input_tensor_list), since the function all gathers the result from every single GPU in the group. To interpret each element of output_tensor_lists[i], note that input_tensor_list[j] of rank k will be appear in output_tensor_lists[i][k * world_size + j]", "Also note that len(output_tensor_lists), and the size of each element in output_tensor_lists (each element is a list, therefore len(output_tensor_lists[i])) need to be the same for all the distributed processes calling this function.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.all_gather_object()", "path": "distributed#torch.distributed.all_gather_object", "type": "Distributed Communication", "text": ["Gathers picklable objects from the whole group into a list. Similar to all_gather(), but Python objects can be passed in. Note that the object must be picklable in order to be gathered.", "None. If the calling rank is part of this group, the output of the collective will be populated into the input object_list. If the calling rank is not part of the group, the passed in object_list will be unmodified.", "Note", "Note that this API differs slightly from the all_gather() collective since it does not provide an async_op handle and thus will be a blocking call.", "Note", "For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsiblity to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().", "Warning", "all_gather_object() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.", "Warning", "Calling all_gather_object() with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using all_gather() instead."]}, {"name": "torch.distributed.all_reduce()", "path": "distributed#torch.distributed.all_reduce", "type": "Distributed Communication", "text": ["Reduces the tensor data across all machines in such a way that all get the final result.", "After the call tensor is going to be bitwise identical in all processes.", "Complex tensors are supported.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.all_reduce_multigpu()", "path": "distributed#torch.distributed.all_reduce_multigpu", "type": "Distributed Communication", "text": ["Reduces the tensor data across all machines in such a way that all get the final result. This function reduces a number of tensors on every node, while each tensor resides on different GPUs. Therefore, the input tensor in the tensor list needs to be GPU tensors. Also, each tensor in the tensor list needs to reside on a different GPU.", "After the call, all tensor in tensor_list is going to be bitwise identical in all processes.", "Complex tensors are supported.", "Only nccl and gloo backend is currently supported tensors should only be GPU tensors", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.all_to_all()", "path": "distributed#torch.distributed.all_to_all", "type": "Distributed Communication", "text": ["Each process scatters list of input tensors to all processes in a group and return gathered list of tensors in output list.", "Complex tensors are supported.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.", "Warning", "all_to_all is experimental and subject to change."]}, {"name": "torch.distributed.all_to_all_single()", "path": "distributed#torch.distributed.all_to_all_single", "type": "Distributed Communication", "text": ["Each process splits input tensor and then scatters the split list to all processes in a group. Then concatenate the received tensors from all the processes in the group and return single output tensor.", "Complex tensors are supported.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.", "Warning", "all_to_all_single is experimental and subject to change."]}, {"name": "torch.distributed.autograd.backward()", "path": "rpc#torch.distributed.autograd.backward", "type": "Distributed RPC", "text": ["Kicks off the distributed backward pass using the provided roots. This currently implements the FAST mode algorithm which assumes all RPC messages sent in the same distributed autograd context across workers would be part of the autograd graph during the backward pass.", "We use the provided roots to discover the autograd graph and compute appropriate dependencies. This method blocks until the entire autograd computation is done.", "We accumulate the gradients in the appropriate torch.distributed.autograd.context on each of the nodes. The autograd context to be used is looked up given the context_id that is passed in when torch.distributed.autograd.backward() is called. If there is no valid autograd context corresponding to the given ID, we throw an error. You can retrieve the accumulated gradients using the get_gradients() API."]}, {"name": "torch.distributed.autograd.context", "path": "rpc#torch.distributed.autograd.context", "type": "Distributed RPC", "text": ["Context object to wrap forward and backward passes when using distributed autograd. The context_id generated in the with statement is required to uniquely identify a distributed backward pass on all workers. Each worker stores metadata associated with this context_id, which is required to correctly execute a distributed autograd pass."]}, {"name": "torch.distributed.autograd.get_gradients()", "path": "rpc#torch.distributed.autograd.get_gradients", "type": "Distributed RPC", "text": ["Retrieves a map from Tensor to the appropriate gradient for that Tensor accumulated in the provided context corresponding to the given context_id as part of the distributed autograd backward pass.", "context_id (int) \u2013 The autograd context id for which we should retrieve the gradients.", "A map where the key is the Tensor and the value is the associated gradient for that Tensor."]}, {"name": "torch.distributed.Backend", "path": "distributed#torch.distributed.Backend", "type": "Distributed Communication", "text": ["An enum-like class of available backends: GLOO, NCCL, UCC, MPI, and other registered backends.", "The values of this class are lowercase strings, e.g., \"gloo\". They can be accessed as attributes, e.g., Backend.NCCL.", "This class can be directly called to parse the string, e.g., Backend(backend_str) will check if backend_str is valid, and return the parsed lowercase string if so. It also accepts uppercase strings, e.g., Backend(\"GLOO\") returns \"gloo\".", "Note", "The entry Backend.UNDEFINED is present but only used as initial value of some fields. Users should neither use it directly nor assume its existence.", "Registers a new backend with the given name and instantiating function.", "This class method is used by 3rd party ProcessGroup extension to register new backends.", "Note", "This support of 3rd party backend is experimental and subject to change."]}, {"name": "torch.distributed.Backend.register_backend()", "path": "distributed#torch.distributed.Backend.register_backend", "type": "Distributed Communication", "text": ["Registers a new backend with the given name and instantiating function.", "This class method is used by 3rd party ProcessGroup extension to register new backends.", "Note", "This support of 3rd party backend is experimental and subject to change."]}, {"name": "torch.distributed.barrier()", "path": "distributed#torch.distributed.barrier", "type": "Distributed Communication", "text": ["Synchronizes all processes.", "This collective blocks processes until the whole group enters this function, if async_op is False, or if async work handle is called on wait().", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.batch_isend_irecv()", "path": "distributed#torch.distributed.batch_isend_irecv", "type": "Distributed Communication", "text": ["Send or Receive a batch of tensors asynchronously and return a list of requests.", "Process each of the operations in p2p_op_list and return the corresponding requests. NCCL, Gloo, and UCC backend are currently supported.", "p2p_op_list \u2013 A list of point-to-point operations(type of each operator is torch.distributed.P2POp). The order of the isend/irecv in the list matters and it needs to match with corresponding isend/irecv on the remote end.", "A list of distributed request objects returned by calling the corresponding op in the op_list.", "Note", "Note that when this API is used with the NCCL PG backend, users must set the current GPU device with torch.cuda.set_device, otherwise it will lead to unexpected hang issues.", "In addition, if this API is the first collective call in the group passed to dist.P2POp, all ranks of the group must participate in this API call; otherwise, the behavior is undefined. If this API call is not the first collective call in the group, batched P2P operations involving only a subset of ranks of the group are allowed."]}, {"name": "torch.distributed.broadcast()", "path": "distributed#torch.distributed.broadcast", "type": "Distributed Communication", "text": ["Broadcasts the tensor to the whole group.", "tensor must have the same number of elements in all processes participating in the collective.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.broadcast_multigpu()", "path": "distributed#torch.distributed.broadcast_multigpu", "type": "Distributed Communication", "text": ["Broadcasts the tensor to the whole group with multiple GPU tensors per node.", "tensor must have the same number of elements in all the GPUs from all processes participating in the collective. each tensor in the list must be on a different GPU", "Only nccl and gloo backend are currently supported tensors should only be GPU tensors", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.broadcast_object_list()", "path": "distributed#torch.distributed.broadcast_object_list", "type": "Distributed Communication", "text": ["Broadcasts picklable objects in object_list to the whole group. Similar to broadcast(), but Python objects can be passed in. Note that all objects in object_list must be picklable in order to be broadcasted.", "None. If rank is part of the group, object_list will contain the broadcasted objects from src rank.", "Note", "For NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsibility to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().", "Note", "Note that this API differs slightly from the all_gather() collective since it does not provide an async_op handle and thus will be a blocking call.", "Warning", "broadcast_object_list() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.", "Warning", "Calling broadcast_object_list() with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using broadcast() instead."]}, {"name": "torch.distributed.checkpoint.DefaultLoadPlanner", "path": "distributed.checkpoint#torch.distributed.checkpoint.DefaultLoadPlanner", "type": "Distributed Checkpoint", "text": ["DefaultLoadPlanner that adds multiple features on top of LoadPlanner.", "In particular it adds the following:", "flatten_state_dict: Handle state_dict with nested dicts flatten_sharded_tensors: For FSDP in 2D parallel mode", "This is an extension from the planner interface to make it easy to extend the default planner", "Tensor", "This is an extension from the planner interface to make it easy to extend the default planner"]}, {"name": "torch.distributed.checkpoint.DefaultLoadPlanner.lookup_tensor()", "path": "distributed.checkpoint#torch.distributed.checkpoint.DefaultLoadPlanner.lookup_tensor", "type": "Distributed Checkpoint", "text": ["This is an extension from the planner interface to make it easy to extend the default planner", "Tensor"]}, {"name": "torch.distributed.checkpoint.DefaultLoadPlanner.transform_tensor()", "path": "distributed.checkpoint#torch.distributed.checkpoint.DefaultLoadPlanner.transform_tensor", "type": "Distributed Checkpoint", "text": ["This is an extension from the planner interface to make it easy to extend the default planner"]}, {"name": "torch.distributed.checkpoint.DefaultSavePlanner", "path": "distributed.checkpoint#torch.distributed.checkpoint.DefaultSavePlanner", "type": "Distributed Checkpoint", "text": ["This is an extension from the planner interface to make it easy to extend the default planner", "Any", "This is an extension from the planner interface to make it easy to extend the default planner"]}, {"name": "torch.distributed.checkpoint.DefaultSavePlanner.lookup_object()", "path": "distributed.checkpoint#torch.distributed.checkpoint.DefaultSavePlanner.lookup_object", "type": "Distributed Checkpoint", "text": ["This is an extension from the planner interface to make it easy to extend the default planner", "Any"]}, {"name": "torch.distributed.checkpoint.DefaultSavePlanner.transform_object()", "path": "distributed.checkpoint#torch.distributed.checkpoint.DefaultSavePlanner.transform_object", "type": "Distributed Checkpoint", "text": ["This is an extension from the planner interface to make it easy to extend the default planner"]}, {"name": "torch.distributed.checkpoint.FileSystemReader", "path": "distributed.checkpoint#torch.distributed.checkpoint.FileSystemReader", "type": "Distributed Checkpoint", "text": []}, {"name": "torch.distributed.checkpoint.FileSystemWriter", "path": "distributed.checkpoint#torch.distributed.checkpoint.FileSystemWriter", "type": "Distributed Checkpoint", "text": ["Basic implementation of StorageWriter using file IO.", "This implementation makes the following assumptions and simplifications:", "The checkpoint consist of one file per write request plus a .metadata file with the serialized metadata."]}, {"name": "torch.distributed.checkpoint.load_state_dict()", "path": "distributed.checkpoint#torch.distributed.checkpoint.load_state_dict", "type": "Distributed Checkpoint", "text": ["Loads a distributed state_dict in SPMD style.", "Each rank will try to read the least amount of data necessary to fullfill the requested state_dict. When loading ShardedTensor instances, each rank only reads data for their local shards.", "Warning", "All tensors in state_dict must be allocated on their destination device prior to calling this function.", "All non-tensor data is loaded using torch.load() and modified in place on state_dict.", "Warning", "Users must call load_state_dict on the root module to ensure load pos-processing and non-tensor data properly propagates.", "None.", "None", "Note", "load_state_dict uses collectives to coordinate reads across ranks. For NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsibility to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device()."]}, {"name": "torch.distributed.checkpoint.LoadPlan", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlan", "type": "Distributed Checkpoint", "text": []}, {"name": "torch.distributed.checkpoint.LoadPlanner", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner", "type": "Distributed Checkpoint", "text": ["Abstract class defining the protocol used by load_state_dict to plan the load process.", "LoadPlanner are stateful objects that can be used to customize the whole load process.", "LoadPlanner acts as an access proxy to the state_dict, so any transformation done to it will be visible to the whole process.", "A planner subclass can expect the following sequence of calls during load_state_dict:", "Signals the start of loading a checkpoint.", "Process the state_dict and produces a LoadPlan that will be sent for global planning.", "Takes the LoadPlan from all ranks and make any global decision.", "This is called once per non-tensor value in state_dict.", "They are called in pair for each Tensor value in state_dict.", "Users are recommended to extend DefaultLoadPlanner instead of this interface directly as most changes can be expressed by changes in a single method.", "There are two usual patterns of extension:", "Rewriting state_dict. This is the simplest way to extend the load process as it doesn\u2019t requite understanding the intrincacies of how LoadPlan works. We need to keep a reference to the original state_dict as load happens in place so we need to be able to perform it in place", "Modifying resolve_tensor and commit_tensor to handle load time transformation.", "This method is called once the StorageReader finished loading data into tensor.", "The provided tensor is the same one returned by the call to resolve_tensor. This method is only needed if this LoadPlanner needs to post process tensor prior to copying it back to the one in the state_dict.", "The contents of tensor will follow its device synchronization model.", "Compute the global load plan and return plans for each rank.", ". N.B. This is called on the coordinator rank only", "List[LoadPlan]", "Create a LoadPlan based on state_dict and metadata provided by set_up_planner.", ". N.B. This is called on every rank.", "LoadPlan", "Accept the plan from coordinator and return final LoadPlan.", "LoadPlan", "Load the item described by read_item``and ``value.", "This method is expected to modify in-place the underlying state_dict.", "The contents of value are defined by the SavePlanner used to produce the checkpoint being loaded.", "Return the tensor described by read_item to be used by the StorageReader to load read_item.", "The tensor should alias with one on the underlying state_dict as StorageReader will replace its contents. If, for any reason, that\u2019s not possible, the planner can use the commit_tensor method to copy the data back to the one in state_dict.", "Tensor", "Initialize this instance to load data into state_dict", ". N.B. This is called on every rank."]}, {"name": "torch.distributed.checkpoint.LoadPlanner.commit_tensor()", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner.commit_tensor", "type": "Distributed Checkpoint", "text": ["This method is called once the StorageReader finished loading data into tensor.", "The provided tensor is the same one returned by the call to resolve_tensor. This method is only needed if this LoadPlanner needs to post process tensor prior to copying it back to the one in the state_dict.", "The contents of tensor will follow its device synchronization model."]}, {"name": "torch.distributed.checkpoint.LoadPlanner.create_global_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner.create_global_plan", "type": "Distributed Checkpoint", "text": ["Compute the global load plan and return plans for each rank.", ". N.B. This is called on the coordinator rank only", "List[LoadPlan]"]}, {"name": "torch.distributed.checkpoint.LoadPlanner.create_local_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner.create_local_plan", "type": "Distributed Checkpoint", "text": ["Create a LoadPlan based on state_dict and metadata provided by set_up_planner.", ". N.B. This is called on every rank.", "LoadPlan"]}, {"name": "torch.distributed.checkpoint.LoadPlanner.finish_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner.finish_plan", "type": "Distributed Checkpoint", "text": ["Accept the plan from coordinator and return final LoadPlan.", "LoadPlan"]}, {"name": "torch.distributed.checkpoint.LoadPlanner.load_bytes()", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner.load_bytes", "type": "Distributed Checkpoint", "text": ["Load the item described by read_item``and ``value.", "This method is expected to modify in-place the underlying state_dict.", "The contents of value are defined by the SavePlanner used to produce the checkpoint being loaded."]}, {"name": "torch.distributed.checkpoint.LoadPlanner.resolve_tensor()", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner.resolve_tensor", "type": "Distributed Checkpoint", "text": ["Return the tensor described by read_item to be used by the StorageReader to load read_item.", "The tensor should alias with one on the underlying state_dict as StorageReader will replace its contents. If, for any reason, that\u2019s not possible, the planner can use the commit_tensor method to copy the data back to the one in state_dict.", "Tensor"]}, {"name": "torch.distributed.checkpoint.LoadPlanner.set_up_planner()", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner.set_up_planner", "type": "Distributed Checkpoint", "text": ["Initialize this instance to load data into state_dict", ". N.B. This is called on every rank."]}, {"name": "torch.distributed.checkpoint.ReadItem", "path": "distributed.checkpoint#torch.distributed.checkpoint.ReadItem", "type": "Distributed Checkpoint", "text": []}, {"name": "torch.distributed.checkpoint.save_state_dict()", "path": "distributed.checkpoint#torch.distributed.checkpoint.save_state_dict", "type": "Distributed Checkpoint", "text": ["Saves a distributed model in SPMD style.", "This function is different from torch.save() as it handles ShardedTensor by having each rank only save their local shards.", "Warning", "There is no guarantees of Backwards Compatibility across PyTorch versions for saved state_dicts.", "Warning", "If using the process_group argument, make sure that only its ranks call save_state_dict and that all data in state_dict belong to it.", "Note", "When saving checkpoint for FSDP\u2019s ShardingStrategy.HYBRID_SHARD, only one of the shard_group should be calling save_state_dict and the corresponding process group needs to be passed in.", "Note", "This function can be used to save a state_dict without having a process group initialized by passing no_dist=True.", "Metadata object for the saved checkpoint.", "Metadata", "Note", "save_state_dict uses collectives to coordinate writes across ranks. For NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsibility to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device()."]}, {"name": "torch.distributed.checkpoint.SavePlan", "path": "distributed.checkpoint#torch.distributed.checkpoint.SavePlan", "type": "Distributed Checkpoint", "text": []}, {"name": "torch.distributed.checkpoint.SavePlanner", "path": "distributed.checkpoint#torch.distributed.checkpoint.SavePlanner", "type": "Distributed Checkpoint", "text": ["Abstract class defining the protocol used by save_state_dict to plan the save process.", "SavePlanners are stateful objects that can be used to customize the whole save process.", "SavePlanner acts as an access proxy to the state_dict, so any transformation done to it will be visible to the whole process.", "A planner subclass can expect the following sequence of calls during save_state_dict:", "Signals the start of a checkpoint save.", "Process the state_dict and produces a SavePlan that will be sent for global planning.", "Takes the SavePlan from all ranks and make any global decision.", "This gives each rank a chance to adjust to global planning decisions.", "Lookups a value on the state_dict for the storage layer to write.", "Users are recommended to extend DefaultSavePlanner instead of this interface directly as most changes can be expressed by changes in a single method.", "There are 3 usual patterns of extension:", "Rewriting state_dict. This is the simplest way to extend the save process as it doesn\u2019t requite understanding the intrincacies of how SavePlan works:", "Modifying local plan and lookup in tandem. This is useful when fine control of how data is persisted", "Using the global planning step to make central decisions that can\u2019t be made individually by each rank", "Finally, some planners need to save additional metadata in the checkpoint, this is accomplished by having each rank contribute their data items in the local plan and the global planner aggregate them:", "Compute the global checkpoint plan and return the local plan of each rank.", "This is called on the coordinator rank only.", "Tuple[List[SavePlan], Metadata]", "Compute the save plan for the current rank. This will be aggregated and passed to create_global_plan. Planner specific data can be passed through SavePlan::planner_data.", "This is called on all ranks.", "SavePlan", "Merge the plan created by create_local_plan and the result of create_global_plan.", "This is called on all ranks.", "SavePlan", "Lookup the object associated with write_item in state_dict and apply any transformation (such as serialization) prior to the storage layer consuming it.", "Called on each rank multiple times, at least once per WriteItem in the final SavePlan.", "This method should be idempotent and thread-save. StorageWriter implementations are free to call it as frequently as they need.", "Any transformation that allocates memory should be lazily done when his method is called in order to reduce peak memory required by checkpointing.", "When returning tensors, they can be on any device or format, they can be views too. It\u2019s the storage layer responsibility to figure out how to save them.", "Union[Tensor, BytesIO]", "Initialize this planner to save state_dict.", "Implementations should save those values as they won\u2019t be provided lated in the save process.", "This is called on all ranks."]}, {"name": "torch.distributed.checkpoint.SavePlanner.create_global_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.SavePlanner.create_global_plan", "type": "Distributed Checkpoint", "text": ["Compute the global checkpoint plan and return the local plan of each rank.", "This is called on the coordinator rank only.", "Tuple[List[SavePlan], Metadata]"]}, {"name": "torch.distributed.checkpoint.SavePlanner.create_local_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.SavePlanner.create_local_plan", "type": "Distributed Checkpoint", "text": ["Compute the save plan for the current rank. This will be aggregated and passed to create_global_plan. Planner specific data can be passed through SavePlan::planner_data.", "This is called on all ranks.", "SavePlan"]}, {"name": "torch.distributed.checkpoint.SavePlanner.finish_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.SavePlanner.finish_plan", "type": "Distributed Checkpoint", "text": ["Merge the plan created by create_local_plan and the result of create_global_plan.", "This is called on all ranks.", "SavePlan"]}, {"name": "torch.distributed.checkpoint.SavePlanner.resolve_data()", "path": "distributed.checkpoint#torch.distributed.checkpoint.SavePlanner.resolve_data", "type": "Distributed Checkpoint", "text": ["Lookup the object associated with write_item in state_dict and apply any transformation (such as serialization) prior to the storage layer consuming it.", "Called on each rank multiple times, at least once per WriteItem in the final SavePlan.", "This method should be idempotent and thread-save. StorageWriter implementations are free to call it as frequently as they need.", "Any transformation that allocates memory should be lazily done when his method is called in order to reduce peak memory required by checkpointing.", "When returning tensors, they can be on any device or format, they can be views too. It\u2019s the storage layer responsibility to figure out how to save them.", "Union[Tensor, BytesIO]"]}, {"name": "torch.distributed.checkpoint.SavePlanner.set_up_planner()", "path": "distributed.checkpoint#torch.distributed.checkpoint.SavePlanner.set_up_planner", "type": "Distributed Checkpoint", "text": ["Initialize this planner to save state_dict.", "Implementations should save those values as they won\u2019t be provided lated in the save process.", "This is called on all ranks."]}, {"name": "torch.distributed.checkpoint.StorageReader", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageReader", "type": "Distributed Checkpoint", "text": ["Interface used by load_state_dict to read from storage.", "One StorageReader instance acts as both the coordinator and the follower in a distributed checkpoint. As part of initialization, each instance is told its role.", "A subclass should expected the following sequence of calls by load_state_dict:", "Perform centralized planning of storage loading.", "This method is only called on the coordinator instance.", "While this method can produce a completely different plan, the preferred way is to store storage specific data in LoadPlan::storage_data.", "plans (List[LoadPlan]) \u2013 A list of LoadPlan instances, one for each rank.", "A list of transformed LoadPlan after storage global planning", "List[LoadPlan]", "Perform storage-specific local planning.", "While this method can produce a completely different plan, the recommended way is to store storage specific data in LoadPlan::storage_data.", "plan (LoadPlan) \u2013 The local plan from the LoadPlan in use.", "A transformed LoadPlan after storage local planning", "LoadPlan", "Reads all items from plan using planner to resolve the data.", "A subclass should call LoadPlanner::load_bytes to deserialize a BytesIO object into the right place.", "A subclass should call LoadPlanner::resolve_tensor to get access to the tensors that in should load data into.", "It\u2019s the StorageLayer responsibility to properly schedule any cross device copies required.", "A future that completes once all reads are finished.", "Future[None]", "Reads the checkpoint metadata.", "The metadata object associated with the checkpoint being loaded.", "Metadata", "Initialize this instance."]}, {"name": "torch.distributed.checkpoint.StorageReader.prepare_global_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageReader.prepare_global_plan", "type": "Distributed Checkpoint", "text": ["Perform centralized planning of storage loading.", "This method is only called on the coordinator instance.", "While this method can produce a completely different plan, the preferred way is to store storage specific data in LoadPlan::storage_data.", "plans (List[LoadPlan]) \u2013 A list of LoadPlan instances, one for each rank.", "A list of transformed LoadPlan after storage global planning", "List[LoadPlan]"]}, {"name": "torch.distributed.checkpoint.StorageReader.prepare_local_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageReader.prepare_local_plan", "type": "Distributed Checkpoint", "text": ["Perform storage-specific local planning.", "While this method can produce a completely different plan, the recommended way is to store storage specific data in LoadPlan::storage_data.", "plan (LoadPlan) \u2013 The local plan from the LoadPlan in use.", "A transformed LoadPlan after storage local planning", "LoadPlan"]}, {"name": "torch.distributed.checkpoint.StorageReader.read_data()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageReader.read_data", "type": "Distributed Checkpoint", "text": ["Reads all items from plan using planner to resolve the data.", "A subclass should call LoadPlanner::load_bytes to deserialize a BytesIO object into the right place.", "A subclass should call LoadPlanner::resolve_tensor to get access to the tensors that in should load data into.", "It\u2019s the StorageLayer responsibility to properly schedule any cross device copies required.", "A future that completes once all reads are finished.", "Future[None]"]}, {"name": "torch.distributed.checkpoint.StorageReader.read_metadata()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageReader.read_metadata", "type": "Distributed Checkpoint", "text": ["Reads the checkpoint metadata.", "The metadata object associated with the checkpoint being loaded.", "Metadata"]}, {"name": "torch.distributed.checkpoint.StorageReader.set_up_storage_reader()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageReader.set_up_storage_reader", "type": "Distributed Checkpoint", "text": ["Initialize this instance."]}, {"name": "torch.distributed.checkpoint.StorageWriter", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageWriter", "type": "Distributed Checkpoint", "text": ["Interface used by save_state_dict to write to storage.", "One StorageWriter instance acts as both the coordinator and the follower in a distributed checkpoint. As part of initialization, each instance is told its role.", "A subclass should expect the following sequence of calls.", "Writes the metadata and marks the current checkpoint as successful.", "The actual format/schema used for serializing metadata is an implementation detail. The only requirement is that it\u2019s recoverable in to the same object graph.", "None", "None", "Perform centralized planning of storage.", "This method is only called on the coordinator instance.", "While this method can produce a completely different plan, the preferred way is to store storage specific data in SavePlan::storage_data.", "plans (List[SavePlan]) \u2013 A list of SavePlan instances, one for each rank.", "A list of transformed SavePlan after storage global planning", "List[SavePlan]", "Perform storage-specific local planning.", "While this method can produce a completely different plan, the recommended way is to store storage specific data in SavePlan::storage_data.", "plan (SavePlan) \u2013 The local plan from the SavePlanner in use.", "A transformed SavePlan after storage local planning", "SavePlan", "Initialize this instance.", "is_coordinator (bool) \u2013 Whether this instance is responsible for coordinating the checkpoint.", "Write all items from plan using planner to resolve the data.", "A subclass should call SavePlanner::resolve_data on each item from the plan to get access to the underlying object to write.", "Subclasses should lazily call resolve_data as it can allocate memory. In case of tensors, make following assumptions:", "A future that completes to a list of WriteResult", "Future[List[WriteResult]]"]}, {"name": "torch.distributed.checkpoint.StorageWriter.finish()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageWriter.finish", "type": "Distributed Checkpoint", "text": ["Writes the metadata and marks the current checkpoint as successful.", "The actual format/schema used for serializing metadata is an implementation detail. The only requirement is that it\u2019s recoverable in to the same object graph.", "None", "None"]}, {"name": "torch.distributed.checkpoint.StorageWriter.prepare_global_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageWriter.prepare_global_plan", "type": "Distributed Checkpoint", "text": ["Perform centralized planning of storage.", "This method is only called on the coordinator instance.", "While this method can produce a completely different plan, the preferred way is to store storage specific data in SavePlan::storage_data.", "plans (List[SavePlan]) \u2013 A list of SavePlan instances, one for each rank.", "A list of transformed SavePlan after storage global planning", "List[SavePlan]"]}, {"name": "torch.distributed.checkpoint.StorageWriter.prepare_local_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageWriter.prepare_local_plan", "type": "Distributed Checkpoint", "text": ["Perform storage-specific local planning.", "While this method can produce a completely different plan, the recommended way is to store storage specific data in SavePlan::storage_data.", "plan (SavePlan) \u2013 The local plan from the SavePlanner in use.", "A transformed SavePlan after storage local planning", "SavePlan"]}, {"name": "torch.distributed.checkpoint.StorageWriter.set_up_storage_writer()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageWriter.set_up_storage_writer", "type": "Distributed Checkpoint", "text": ["Initialize this instance.", "is_coordinator (bool) \u2013 Whether this instance is responsible for coordinating the checkpoint."]}, {"name": "torch.distributed.checkpoint.StorageWriter.write_data()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageWriter.write_data", "type": "Distributed Checkpoint", "text": ["Write all items from plan using planner to resolve the data.", "A subclass should call SavePlanner::resolve_data on each item from the plan to get access to the underlying object to write.", "Subclasses should lazily call resolve_data as it can allocate memory. In case of tensors, make following assumptions:", "A future that completes to a list of WriteResult", "Future[List[WriteResult]]"]}, {"name": "torch.distributed.checkpoint.WriteItem", "path": "distributed.checkpoint#torch.distributed.checkpoint.WriteItem", "type": "Distributed Checkpoint", "text": []}, {"name": "torch.distributed.DistBackendError", "path": "distributed#torch.distributed.DistBackendError", "type": "Distributed Communication", "text": ["Exception raised when a backend error occurs in distributed"]}, {"name": "torch.distributed.elastic.agent.server.api.RunResult", "path": "elastic/agent#torch.distributed.elastic.agent.server.api.RunResult", "type": "Distributed Elastic", "text": ["Results returned by the worker executions. Run results follow an \u201call-or-nothing\u201d policy where the run is successful if and only if ALL local workers managed by this agent complete successfully.", "If the result is successful (e.g. is_failed() = False) then the return_values field contains the outputs (return values) of the workers managed by THIS agent mapped by their GLOBAL ranks. That is result.return_values[0] is the return value of global rank 0.", "Note", "return_values are only meaningful for when the worker entrypoint is a function. Workers specified as a binary entrypoint do not canonically have a return value and the return_values field is meaningless and may be empty.", "If is_failed() returns True then the failures field contains the failure information, again, mapped by the GLOBAL rank of the worker that failed.", "The keys in return_values and failures are mutually exclusive, that is, a worker\u2019s final state can only be one of: succeeded, failed. Workers intentionally terminated by the agent according to the agent\u2019s restart policy, are not represented in either return_values nor failures."]}, {"name": "torch.distributed.elastic.agent.server.ElasticAgent", "path": "elastic/agent#torch.distributed.elastic.agent.server.ElasticAgent", "type": "Distributed Elastic", "text": ["Agent process responsible for managing one or more worker processes. The worker processes are assumed to be regular distributed PyTorch scripts. When the worker process is created by the agent, the agent provides the necessary information for the worker processes to properly initialize a torch process group.", "The exact deployment topology and ratio of agent-to-worker is dependent on the specific implementation of the agent and the user\u2019s job placement preferences. For instance, to run a distributed training job on GPU with 8 trainers (one per GPU) one can:", "Usage", "The WorkerGroup for the given role. Note that the worker group is a mutable object and hence in a multi-threaded/process environment it may change state. Implementors are encouraged (but not required) to return a defensive read-only copy.", "WorkerGroup", "Runs the agent, retrying the worker group on failures up to max_restarts.", "The result of the execution, containing the return values or failure details for each worker mapped by the worker\u2019s global rank.", "Exception - any other failures NOT related to worker process \u2013 ", "RunResult"]}, {"name": "torch.distributed.elastic.agent.server.ElasticAgent.get_worker_group()", "path": "elastic/agent#torch.distributed.elastic.agent.server.ElasticAgent.get_worker_group", "type": "Distributed Elastic", "text": ["The WorkerGroup for the given role. Note that the worker group is a mutable object and hence in a multi-threaded/process environment it may change state. Implementors are encouraged (but not required) to return a defensive read-only copy.", "WorkerGroup"]}, {"name": "torch.distributed.elastic.agent.server.ElasticAgent.run()", "path": "elastic/agent#torch.distributed.elastic.agent.server.ElasticAgent.run", "type": "Distributed Elastic", "text": ["Runs the agent, retrying the worker group on failures up to max_restarts.", "The result of the execution, containing the return values or failure details for each worker mapped by the worker\u2019s global rank.", "Exception - any other failures NOT related to worker process \u2013 ", "RunResult"]}, {"name": "torch.distributed.elastic.agent.server.local_elastic_agent.LocalElasticAgent", "path": "elastic/agent#torch.distributed.elastic.agent.server.local_elastic_agent.LocalElasticAgent", "type": "Distributed Elastic", "text": ["An implementation of torchelastic.agent.server.ElasticAgent that handles host-local workers. This agent is deployed per host and is configured to spawn n workers. When using GPUs, n maps to the number of GPUs available on the host.", "The local agent does not communicate to other local agents deployed on other hosts, even if the workers may communicate inter-host. The worker id is interpreted to be a local process. The agent starts and stops all worker processes as a single unit.", "The worker function and argument passed to the worker function must be python multiprocessing compatible. To pass multiprocessing data structures to the workers you may create the data structure in the same multiprocessing context as the specified start_method and pass it as a function argument.", "The exit_barrier_timeout specifies the amount of time (in seconds) to wait for other agents to finish. This acts as a safety net to handle cases where workers finish at different times, to prevent agents from viewing workers that finished early as a scale-down event. It is strongly advised that the user code deal with ensuring that workers are terminated in a synchronous manner rather than relying on the exit_barrier_timeout.", "A named pipe based watchdog can be enabled in `LocalElasticAgent` if an environment variable TORCHELASTIC_ENABLE_FILE_TIMER with value 1 has been defined in the `LocalElasticAgent` process. Optionally, another environment variable `TORCHELASTIC_TIMER_FILE` can be set with a unique file name for the named pipe. If the environment variable `TORCHELASTIC_TIMER_FILE` is not set, `LocalElasticAgent` will internally create a unique file name and set it to the environment variable `TORCHELASTIC_TIMER_FILE`, and this environment variable will be propagated to the worker processes to allow them to connect to the same named pipe that `LocalElasticAgent` uses.", "Example launching function", "Example launching binary"]}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent", "type": "Distributed Elastic", "text": ["An ElasticAgent that manages workers (WorkerGroup) for a single WorkerSpec (e.g. one particular type of worker role).", "Determines proper ranks for worker processes. The rank assignment is done according to the following algorithm:", "List[Worker]", "Wait for exit_barrier_timeout seconds for all agents to finish executing their local workers (either successfully or not). This acts as a safety guard against user scripts that terminate at different times. This barrier keeps the agent process alive until all workers finish.", "Starts a fresh set of workers for the worker_group. Essentially a rendezvous followed by a start_workers.", "The caller should first call _stop_workers() to stop running workers prior to calling this method.", "Optimistically sets the state of the worker group that just started as HEALTHY and delegates the actual monitoring of state to _monitor_workers() method", "Checks on the workers for the worker_group and returns the new state of the worker group.", "RunResult", "Runs rendezvous for the workers specified by worker spec. Assigns workers a new global rank and world size. Updates the rendezvous store for the worker group.", "Restarts (stops, rendezvous, starts) all local workers in the group.", "Cleans up any resources that were allocated during the agent\u2019s work.", "death_sig (Signals) \u2013 Signal to send to the child process, SIGTERM is default", "Starts worker_group.spec.local_world_size number of workers according to worker spec for the worker group .", "Returns a map of local_rank to worker id.", "Dict[int, Any]", "Stops all workers in the given worker group. Implementors must deal with workers in all states defined by WorkerState. That is, it must gracefully handle stopping non-existent workers, unhealthy (stuck) workers, etc."]}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._assign_worker_ranks()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._assign_worker_ranks", "type": "Distributed Elastic", "text": ["Determines proper ranks for worker processes. The rank assignment is done according to the following algorithm:", "List[Worker]"]}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._exit_barrier()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._exit_barrier", "type": "Distributed Elastic", "text": ["Wait for exit_barrier_timeout seconds for all agents to finish executing their local workers (either successfully or not). This acts as a safety guard against user scripts that terminate at different times. This barrier keeps the agent process alive until all workers finish."]}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._initialize_workers()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._initialize_workers", "type": "Distributed Elastic", "text": ["Starts a fresh set of workers for the worker_group. Essentially a rendezvous followed by a start_workers.", "The caller should first call _stop_workers() to stop running workers prior to calling this method.", "Optimistically sets the state of the worker group that just started as HEALTHY and delegates the actual monitoring of state to _monitor_workers() method"]}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._monitor_workers()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._monitor_workers", "type": "Distributed Elastic", "text": ["Checks on the workers for the worker_group and returns the new state of the worker group.", "RunResult"]}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._rendezvous()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._rendezvous", "type": "Distributed Elastic", "text": ["Runs rendezvous for the workers specified by worker spec. Assigns workers a new global rank and world size. Updates the rendezvous store for the worker group."]}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._restart_workers()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._restart_workers", "type": "Distributed Elastic", "text": ["Restarts (stops, rendezvous, starts) all local workers in the group."]}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._shutdown()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._shutdown", "type": "Distributed Elastic", "text": ["Cleans up any resources that were allocated during the agent\u2019s work.", "death_sig (Signals) \u2013 Signal to send to the child process, SIGTERM is default"]}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._start_workers()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._start_workers", "type": "Distributed Elastic", "text": ["Starts worker_group.spec.local_world_size number of workers according to worker spec for the worker group .", "Returns a map of local_rank to worker id.", "Dict[int, Any]"]}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._stop_workers()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._stop_workers", "type": "Distributed Elastic", "text": ["Stops all workers in the given worker group. Implementors must deal with workers in all states defined by WorkerState. That is, it must gracefully handle stopping non-existent workers, unhealthy (stuck) workers, etc."]}, {"name": "torch.distributed.elastic.agent.server.Worker", "path": "elastic/agent#torch.distributed.elastic.agent.server.Worker", "type": "Distributed Elastic", "text": ["Represents a worker instance. Contrast this with WorkerSpec that represents the specifications of a worker. A Worker is created from a WorkerSpec. A Worker is to a WorkerSpec as an object is to a class.", "The id of the worker is interpreted by the specific implementation of ElasticAgent. For a local agent, it could be the pid (int) of the worker, for a remote agent it could be encoded as host:port (string)."]}, {"name": "torch.distributed.elastic.agent.server.WorkerGroup", "path": "elastic/agent#torch.distributed.elastic.agent.server.WorkerGroup", "type": "Distributed Elastic", "text": ["Represents the set of Worker instances for the given WorkerSpec managed by ElasticAgent. Whether the worker group contains cross instance workers or not depends on the implementation of the agent."]}, {"name": "torch.distributed.elastic.agent.server.WorkerSpec", "path": "elastic/agent#torch.distributed.elastic.agent.server.WorkerSpec", "type": "Distributed Elastic", "text": ["Contains blueprint information about a particular type of worker. For a given role, there must only exist a single worker spec. Worker spec is expected to be homogeneous across all nodes (machine), that is each node runs the same number of workers for a particular spec.", "If the entrypoint is a function (e.g. Callable) returns its __qualname__, else if the entrypoint is a binary (e.g. str), returns the binary name."]}, {"name": "torch.distributed.elastic.agent.server.WorkerSpec.get_entrypoint_name()", "path": "elastic/agent#torch.distributed.elastic.agent.server.WorkerSpec.get_entrypoint_name", "type": "Distributed Elastic", "text": ["If the entrypoint is a function (e.g. Callable) returns its __qualname__, else if the entrypoint is a binary (e.g. str), returns the binary name."]}, {"name": "torch.distributed.elastic.agent.server.WorkerState", "path": "elastic/agent#torch.distributed.elastic.agent.server.WorkerState", "type": "Distributed Elastic", "text": ["State of the WorkerGroup. Workers in a worker group change state as a unit. If a single worker in a worker group fails the entire set is considered failed:", "A worker group starts from an initial INIT state, then progresses to HEALTHY or UNHEALTHY states, and finally reaches a terminal SUCCEEDED or FAILED state.", "Worker groups can be interrupted and temporarily put into STOPPED state by the agent. Workers in STOPPED state are scheduled to be restarted in the near future by the agent. Some examples of workers being put into STOPPED state are:", "When actions (start, stop, rdzv, retry, etc) on worker group fails and results in the action being partially applied to the worker group the state will be UNKNOWN. Typically this happens on uncaught/unhandled exceptions during state change events on the agent. The agent is not expected to recover worker groups in UNKNOWN state and is better off self terminating and allowing the job manager to retry the node.", "True if the worker state represents workers still running (e.g. that the process exists but not necessarily healthy).", "bool"]}, {"name": "torch.distributed.elastic.agent.server.WorkerState.is_running()", "path": "elastic/agent#torch.distributed.elastic.agent.server.WorkerState.is_running", "type": "Distributed Elastic", "text": ["True if the worker state represents workers still running (e.g. that the process exists but not necessarily healthy).", "bool"]}, {"name": "torch.distributed.elastic.events.api.Event", "path": "elastic/events#torch.distributed.elastic.events.api.Event", "type": "Distributed Elastic", "text": ["The class represents the generic event that occurs during the torchelastic job execution. The event can be any kind of meaningful action."]}, {"name": "torch.distributed.elastic.events.api.EventMetadataValue", "path": "elastic/events#torch.distributed.elastic.events.api.EventMetadataValue", "type": "Distributed Elastic", "text": ["alias of Optional[Union[str, int, float, bool]]"]}, {"name": "torch.distributed.elastic.events.api.EventSource", "path": "elastic/events#torch.distributed.elastic.events.api.EventSource", "type": "Distributed Elastic", "text": ["Known identifiers of the event producers."]}, {"name": "torch.distributed.elastic.events.get_logging_handler()", "path": "elastic/events#torch.distributed.elastic.events.get_logging_handler", "type": "Distributed Elastic", "text": ["Handler"]}, {"name": "torch.distributed.elastic.events.record()", "path": "elastic/events#torch.distributed.elastic.events.record", "type": "Distributed Elastic", "text": []}, {"name": "torch.distributed.elastic.metrics.api.ConsoleMetricHandler", "path": "elastic/metrics#torch.distributed.elastic.metrics.api.ConsoleMetricHandler", "type": "Distributed Elastic", "text": []}, {"name": "torch.distributed.elastic.metrics.api.MetricHandler", "path": "elastic/metrics#torch.distributed.elastic.metrics.api.MetricHandler", "type": "Distributed Elastic", "text": []}, {"name": "torch.distributed.elastic.metrics.api.NullMetricHandler", "path": "elastic/metrics#torch.distributed.elastic.metrics.api.NullMetricHandler", "type": "Distributed Elastic", "text": []}, {"name": "torch.distributed.elastic.metrics.configure()", "path": "elastic/metrics#torch.distributed.elastic.metrics.configure", "type": "Distributed Elastic", "text": []}, {"name": "torch.distributed.elastic.metrics.prof()", "path": "elastic/metrics#torch.distributed.elastic.metrics.prof", "type": "Distributed Elastic", "text": ["@profile decorator publishes duration.ms, count, success, failure metrics for the function that it decorates. The metric name defaults to the qualified name (class_name.def_name) of the function. If the function does not belong to a class, it uses the leaf module name instead.", "Usage"]}, {"name": "torch.distributed.elastic.metrics.put_metric()", "path": "elastic/metrics#torch.distributed.elastic.metrics.put_metric", "type": "Distributed Elastic", "text": ["Publishes a metric data point.", "Usage"]}, {"name": "torch.distributed.elastic.multiprocessing.api.MultiprocessContext", "path": "elastic/multiprocessing#torch.distributed.elastic.multiprocessing.api.MultiprocessContext", "type": "Distributed Elastic", "text": ["PContext holding worker processes invoked as a function."]}, {"name": "torch.distributed.elastic.multiprocessing.api.PContext", "path": "elastic/multiprocessing#torch.distributed.elastic.multiprocessing.api.PContext", "type": "Distributed Elastic", "text": ["The base class that standardizes operations over a set of processes that are launched via different mechanisms. The name PContext is intentional to disambiguate with torch.multiprocessing.ProcessContext.", "Warning", "stdouts and stderrs should ALWAYS be a superset of tee_stdouts and tee_stderrs (respectively) this is b/c tee is implemented as a redirect + tail -f <stdout/stderr.log>"]}, {"name": "torch.distributed.elastic.multiprocessing.api.RunProcsResult", "path": "elastic/multiprocessing#torch.distributed.elastic.multiprocessing.api.RunProcsResult", "type": "Distributed Elastic", "text": ["Results of a completed run of processes started with start_processes(). Returned by PContext.", "Note the following:"]}, {"name": "torch.distributed.elastic.multiprocessing.api.SubprocessContext", "path": "elastic/multiprocessing#torch.distributed.elastic.multiprocessing.api.SubprocessContext", "type": "Distributed Elastic", "text": ["PContext holding worker processes invoked as a binary."]}, {"name": "torch.distributed.elastic.multiprocessing.errors.ChildFailedError", "path": "elastic/errors#torch.distributed.elastic.multiprocessing.errors.ChildFailedError", "type": "Distributed Elastic", "text": ["Special exception type that can be raised from a function annotated with the @record decorator to have the child process\u2019 (root exception) propagate up the stack as-is (e.g. without being wrapped in the parent\u2019s traceback).", "Useful in cases where the parent is a simple nanny process and the child (worker) processes are actually doing meaningful compute. In this case, errors typically occur on the child process as the parent is not doing anything non-trivial, and child errors should be propagated to the scheduler for accurate root cause diagnostics.", "Note", "The propagation relies on error files rather than exception handling to support both function and binary launches.", "Example:", "In the example above, trainer 1\u2019s failure (written into error.json) is the root cause and should be reported to the scheduler\u2019s init process. The torchelastic agent raises a ChildFailedError(\"trainer\", {1: \"trainer_1/error.json\"}) upon detecting trainer 1\u2019s failure which would propagate the contents of trainer 1\u2019s error file to the scheduler\u2019s init process."]}, {"name": "torch.distributed.elastic.multiprocessing.errors.ErrorHandler", "path": "elastic/errors#torch.distributed.elastic.multiprocessing.errors.ErrorHandler", "type": "Distributed Elastic", "text": ["Writes the provided exception object along with some other metadata about the error in a structured way in JSON format to an error file specified by the environment variable: TORCHELASTIC_ERROR_FILE. If this environment variable is not set, then simply logs the contents of what would have been written to the error file.", "This handler may be subclassed to customize the handling of the error. Subclasses should override initialize() and record_exception()."]}, {"name": "torch.distributed.elastic.multiprocessing.errors.ProcessFailure", "path": "elastic/errors#torch.distributed.elastic.multiprocessing.errors.ProcessFailure", "type": "Distributed Elastic", "text": ["Represents the failed process result. When the worker process fails, it may record failure root cause into the file. Tries to read the failure timestamp from the provided error_file, if the error_file does not exist, the timestamp is the current timestamp (seconds since epoch).", "The message field is a concise explanation of the failure. If the error file exists then the message is obtained from the error file. Otherwise one is generated based on the failure signature.", "Note", "It is assumed that the error_file is written by torch.distributed.elastic.multiprocessing.errors.error_handler.ErrorHandler. Otherwise the behavior is undefined."]}, {"name": "torch.distributed.elastic.multiprocessing.errors.record()", "path": "elastic/errors#torch.distributed.elastic.multiprocessing.errors.record", "type": "Distributed Elastic", "text": ["Syntactic sugar to record errors/exceptions that happened in the decorated function using the provided error_handler.", "Using this decorator is equivalent to:", "Important", "use this decorator once per process at the top level method, typically this is the main method.", "Example", "Callable[[\u2026], T]"]}, {"name": "torch.distributed.elastic.multiprocessing.start_processes()", "path": "elastic/multiprocessing#torch.distributed.elastic.multiprocessing.start_processes", "type": "Distributed Elastic", "text": ["Starts n copies of entrypoint processes with the provided options. entrypoint is either a Callable (function) or a str (binary). The number of copies is determined by the number of entries for args and envs arguments, which need to have the same key set.", "args and env parameters are the arguments and environment variables to pass down to the entrypoint mapped by the replica index (local rank). All local ranks must be accounted for. That is, the keyset should be {0,1,...,(nprocs-1)}.", "Note", "When the entrypoint is a binary (str), args can only be strings. If any other type is given, then it is casted to a string representation (e.g. str(arg1)). Furthermore, a binary failure will only write an error.json error file if the main function is annotated with torch.distributed.elastic.multiprocessing.errors.record. For function launches, this is done by default and there is no need to manually annotate with the @record annotation.", "redirects and tee are bitmasks specifying which std stream(s) to redirect to a log file in the log_dir. Valid mask values are defined in Std. To redirect/tee only certain local ranks, pass redirects as a map with the key as the local rank to specify the redirect behavior for. Any missing local ranks will default to Std.NONE.", "tee acts like the unix \u201ctee\u201d command in that it redirects + prints to console. To avoid worker stdout/stderr from printing to console, use the redirects parameter.", "For each process, the log_dir will contain:", "Note", "It is expected that the log_dir exists, is empty, and is a directory.", "Example:", "PContext"]}, {"name": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend", "type": "Distributed Elastic", "text": ["Represents a C10d-backed rendezvous backend.", "See base class.", "Optional[Tuple[bytes, Any]]", "See base class.", "See base class.", "Optional[Tuple[bytes, Any, bool]]"]}, {"name": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.get_state()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.get_state", "type": "Distributed Elastic", "text": ["See base class.", "Optional[Tuple[bytes, Any]]"]}, {"name": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.name", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.name", "type": "Distributed Elastic", "text": ["See base class."]}, {"name": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.set_state()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.set_state", "type": "Distributed Elastic", "text": ["See base class.", "Optional[Tuple[bytes, Any, bool]]"]}, {"name": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.create_backend()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.create_backend", "type": "Distributed Elastic", "text": ["Creates a new C10dRendezvousBackend from the specified parameters.", "Parameter", "Description", "store_type", "The type of the C10d store. The currently supported types are \u201ctcp\u201d and \u201cfile\u201d which correspond to torch.distributed.TCPStore and torch.distributed.FileStore, respectively. Defaults to \u201ctcp\u201d.", "read_timeout", "The read timeout, in seconds, for store operations. Defaults to 60 seconds.", "Note this only applies to torch.distributed.TCPStore. It is not relevant to torch.distributed.FileStore which does not take in timeout as a parameter.", "is_host", "A boolean value indicating whether this backend instance will host the C10d store. If not specified it will be inferred heuristically by matching the hostname or the IP address of this machine against the specified rendezvous endpoint. Defaults to None.", "Note that this configuration option only applies to torch.distributed.TCPStore. In normal circumstances you can safely skip it; the only time when it is needed is if its value cannot be correctly determined (e.g. the rendezvous endpoint has a CNAME as the hostname or does not match the FQDN of the machine).", "Tuple[C10dRendezvousBackend, Store]"]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.create_handler()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.create_handler", "type": "Distributed Elastic", "text": ["Creates a new DynamicRendezvousHandler from the specified parameters.", "DynamicRendezvousHandler", "Parameter", "Description", "join_timeout", "The total time, in seconds, within which the rendezvous is expected to complete. Defaults to 600 seconds.", "last_call_timeout", "An additional wait amount, in seconds, before completing the rendezvous once the minimum number of nodes has been reached. Defaults to 30 seconds.", "close_timeout", "The time, in seconds, within which the rendezvous is expected to close after a call to RendezvousHandler.set_closed() or RendezvousHandler.shutdown(). Defaults to 30 seconds."]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler", "type": "Distributed Elastic", "text": ["Represents a handler that sets up a rendezvous among a set of nodes.", "Creates a new DynamicRendezvousHandler."]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler.from_backend()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler.from_backend", "type": "Distributed Elastic", "text": ["Creates a new DynamicRendezvousHandler."]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend", "type": "Distributed Elastic", "text": ["Represents a backend that holds the rendezvous state.", "Gets the rendezvous state.", "A tuple of the encoded rendezvous state and its fencing token or None if no state is found in the backend.", "Optional[Tuple[bytes, Any]]", "Gets the name of the backend.", "Sets the rendezvous state.", "The new rendezvous state is set conditionally:", "A tuple of the serialized rendezvous state, its fencing token, and a boolean value indicating whether our set attempt succeeded.", "Optional[Tuple[bytes, Any, bool]]"]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.get_state()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.get_state", "type": "Distributed Elastic", "text": ["Gets the rendezvous state.", "A tuple of the encoded rendezvous state and its fencing token or None if no state is found in the backend.", "Optional[Tuple[bytes, Any]]"]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.name", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.name", "type": "Distributed Elastic", "text": ["Gets the name of the backend."]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.set_state()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.set_state", "type": "Distributed Elastic", "text": ["Sets the rendezvous state.", "The new rendezvous state is set conditionally:", "A tuple of the serialized rendezvous state, its fencing token, and a boolean value indicating whether our set attempt succeeded.", "Optional[Tuple[bytes, Any, bool]]"]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout", "type": "Distributed Elastic", "text": ["Holds the timeout configuration of a rendezvous.", "Gets the close timeout.", "Gets the keep-alive heartbeat timeout.", "Gets the join timeout.", "Gets the last call timeout."]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.close", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.close", "type": "Distributed Elastic", "text": ["Gets the close timeout."]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.heartbeat", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.heartbeat", "type": "Distributed Elastic", "text": ["Gets the keep-alive heartbeat timeout."]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.join", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.join", "type": "Distributed Elastic", "text": ["Gets the join timeout."]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.last_call", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.last_call", "type": "Distributed Elastic", "text": ["Gets the last call timeout."]}, {"name": "torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvousHandler", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvousHandler", "type": "Distributed Elastic", "text": ["Implements a torch.distributed.elastic.rendezvous.RendezvousHandler interface backed by torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous. EtcdRendezvousHandler uses a URL to configure the type of rendezvous to use and to pass implementation specific configurations to the rendezvous module. The basic etcd rendezvous configuration URL looks like the following", "The URL above is interpreted as follows:", "Below are a full list of the parameters that can be passed to etcd rendezvous:", "Parameter", "Description", "min_workers", "minimum number of workers for the rendezvous to be valid", "max_workers", "maximum number of workers to admit", "timeout", "total timeout within which next_rendezvous is expected to succeed (default 600s)", "last_call_timeout", "additional wait amount (\u201clast call\u201d) after min number of workers has been reached (defaults to 30s)", "etcd_prefix", "path prefix (from etcd root), inside which all etcd nodes will be created (defaults to /torchelastic/p2p)"]}, {"name": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.create_backend()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.create_backend", "type": "Distributed Elastic", "text": ["Creates a new EtcdRendezvousBackend from the specified parameters.", "Parameter", "Description", "read_timeout", "The read timeout, in seconds, for etcd operations. Defaults to 60 seconds.", "protocol", "The protocol to use to communicate with etcd. Valid values are \u201chttp\u201d and \u201chttps\u201d. Defaults to \u201chttp\u201d.", "ssl_cert", "The path to the SSL client certificate to use along with HTTPS. Defaults to None.", "ssl_cert_key", "The path to the private key of the SSL client certificate to use along with HTTPS. Defaults to None.", "ca_cert", "The path to the rool SSL authority certificate. Defaults to None.", "Tuple[EtcdRendezvousBackend, Store]"]}, {"name": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend", "type": "Distributed Elastic", "text": ["Represents an etcd-based rendezvous backend.", "See base class.", "Optional[Tuple[bytes, Any]]", "See base class.", "See base class.", "Optional[Tuple[bytes, Any, bool]]"]}, {"name": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.get_state()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.get_state", "type": "Distributed Elastic", "text": ["See base class.", "Optional[Tuple[bytes, Any]]"]}, {"name": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.name", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.name", "type": "Distributed Elastic", "text": ["See base class."]}, {"name": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.set_state()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.set_state", "type": "Distributed Elastic", "text": ["See base class.", "Optional[Tuple[bytes, Any, bool]]"]}, {"name": "torch.distributed.elastic.rendezvous.etcd_server.EtcdServer", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_server.EtcdServer", "type": "Distributed Elastic", "text": ["Note", "tested on etcd server v3.4.3", "Starts and stops a local standalone etcd server on a random free port. Useful for single node, multi-worker launches or testing, where a sidecar etcd server is more convenient than having to separately setup an etcd server.", "This class registers a termination handler to shutdown the etcd subprocess on exit. This termination handler is NOT a substitute for calling the stop() method.", "The following fallback mechanism is used to find the etcd binary:", "Usage", "etcd_binary_path \u2013 path of etcd server binary (see above for fallback path)"]}, {"name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore", "type": "Distributed Elastic", "text": ["Implements a c10 Store interface by piggybacking on the rendezvous etcd instance. This is the store object returned by EtcdRendezvous", "Atomically increment a value by an integer amount. The integer is represented as a string using base 10. If key is not present, a default value of 0 will be assumed.", "the new (incremented) value", "int", "Check if all of the keys are immediately present (without waiting).", "bool", "Get a value by key, possibly doing a blocking wait.", "If key is not immediately present, will do a blocking wait for at most timeout duration or until the key is published.", "value (bytes)", "LookupError - If key still not published after timeout \u2013 ", "bytes", "Write a key/value pair into EtcdStore. Both key and value may be either Python str or bytes.", "Waits until all of the keys are published, or until timeout.", "LookupError - if timeout occurs \u2013 "]}, {"name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.add()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.add", "type": "Distributed Elastic", "text": ["Atomically increment a value by an integer amount. The integer is represented as a string using base 10. If key is not present, a default value of 0 will be assumed.", "the new (incremented) value", "int"]}, {"name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.check()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.check", "type": "Distributed Elastic", "text": ["Check if all of the keys are immediately present (without waiting).", "bool"]}, {"name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.get()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.get", "type": "Distributed Elastic", "text": ["Get a value by key, possibly doing a blocking wait.", "If key is not immediately present, will do a blocking wait for at most timeout duration or until the key is published.", "value (bytes)", "LookupError - If key still not published after timeout \u2013 ", "bytes"]}, {"name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.set()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.set", "type": "Distributed Elastic", "text": ["Write a key/value pair into EtcdStore. Both key and value may be either Python str or bytes."]}, {"name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.wait()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.wait", "type": "Distributed Elastic", "text": ["Waits until all of the keys are published, or until timeout.", "LookupError - if timeout occurs \u2013 "]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousClosedError", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousClosedError", "type": "Distributed Elastic", "text": ["Raised when a rendezvous is closed."]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousConnectionError", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousConnectionError", "type": "Distributed Elastic", "text": ["Raised when the connection to a rendezvous backend has failed."]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousError", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousError", "type": "Distributed Elastic", "text": ["Represents the base type for rendezvous errors."]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler", "type": "Distributed Elastic", "text": ["Main rendezvous interface.", "Note", "Distributed Torch users normally do not need to implement their own RendezvousHandler. An implementation based on C10d Store is already provided, and is recommended for most users.", "Returns the name of the rendezvous backend.", "str", "Returns the run id of the rendezvous.", "The run id is a user-defined id that uniquely identifies an instance of a distributed application. It typically maps to a job id and is used to allow nodes to join the correct distributed application.", "str", "Checks whether the rendezvous has been closed.", "A closed rendezvous means all future attempts to re-rendezvous within same job will fail.", "is_closed() and set_closed() have semantics of eventual propagation and should not be used for synchronization. The intention is that if at least one node decides the job is finished, it will close the rendezvous, and other nodes will soon observe this and stop running as well.", "bool", "Main entry-point into the rendezvous barrier.", "Blocks until the rendezvous is complete and the current process is included in the formed worker group, or a timeout occurs, or the rendezvous was marked closed.", "A tuple of torch.distributed.Store, rank, and world size.", "Tuple[Store, int, int]", "Returns the number of nodes who arrived late at the rendezvous barrier, hence were not included in the current worker group.", "Callers should periodically call this method to check whether new nodes are waiting to join the job and if so admit them by calling next_rendezvous() (re-rendezvous).", "int", "Marks the rendezvous as closed.", "Closes all resources that were open for the rendezvous.", "Example:", "bool"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler.get_backend()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler.get_backend", "type": "Distributed Elastic", "text": ["Returns the name of the rendezvous backend.", "str"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler.get_run_id()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler.get_run_id", "type": "Distributed Elastic", "text": ["Returns the run id of the rendezvous.", "The run id is a user-defined id that uniquely identifies an instance of a distributed application. It typically maps to a job id and is used to allow nodes to join the correct distributed application.", "str"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler.is_closed()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler.is_closed", "type": "Distributed Elastic", "text": ["Checks whether the rendezvous has been closed.", "A closed rendezvous means all future attempts to re-rendezvous within same job will fail.", "is_closed() and set_closed() have semantics of eventual propagation and should not be used for synchronization. The intention is that if at least one node decides the job is finished, it will close the rendezvous, and other nodes will soon observe this and stop running as well.", "bool"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler.next_rendezvous()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler.next_rendezvous", "type": "Distributed Elastic", "text": ["Main entry-point into the rendezvous barrier.", "Blocks until the rendezvous is complete and the current process is included in the formed worker group, or a timeout occurs, or the rendezvous was marked closed.", "A tuple of torch.distributed.Store, rank, and world size.", "Tuple[Store, int, int]"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler.num_nodes_waiting()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler.num_nodes_waiting", "type": "Distributed Elastic", "text": ["Returns the number of nodes who arrived late at the rendezvous barrier, hence were not included in the current worker group.", "Callers should periodically call this method to check whether new nodes are waiting to join the job and if so admit them by calling next_rendezvous() (re-rendezvous).", "int"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler.set_closed()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler.set_closed", "type": "Distributed Elastic", "text": ["Marks the rendezvous as closed."]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler.shutdown()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler.shutdown", "type": "Distributed Elastic", "text": ["Closes all resources that were open for the rendezvous.", "Example:", "bool"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry", "type": "Distributed Elastic", "text": ["Represents a registry of RendezvousHandler backends.", "Creates a new RendezvousHandler.", "RendezvousHandler", "Registers a new rendezvous backend."]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry.create_handler()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry.create_handler", "type": "Distributed Elastic", "text": ["Creates a new RendezvousHandler.", "RendezvousHandler"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry.register()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry.register", "type": "Distributed Elastic", "text": ["Registers a new rendezvous backend."]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousParameters", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousParameters", "type": "Distributed Elastic", "text": ["Holds the parameters to construct a RendezvousHandler.", "Returns the value for key if key exists, else default.", "Any", "Returns the value for key as a bool.", "Optional[bool]", "Returns the value for key as an int.", "Optional[int]"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousParameters.get()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousParameters.get", "type": "Distributed Elastic", "text": ["Returns the value for key if key exists, else default.", "Any"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousParameters.get_as_bool()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousParameters.get_as_bool", "type": "Distributed Elastic", "text": ["Returns the value for key as a bool.", "Optional[bool]"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousParameters.get_as_int()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousParameters.get_as_int", "type": "Distributed Elastic", "text": ["Returns the value for key as an int.", "Optional[int]"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousStateError", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousStateError", "type": "Distributed Elastic", "text": ["Raised when the state of a rendezvous is corrupt."]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousTimeoutError", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousTimeoutError", "type": "Distributed Elastic", "text": ["Raised when a rendezvous did not complete on time."]}, {"name": "torch.distributed.elastic.timer.configure()", "path": "elastic/timer#torch.distributed.elastic.timer.configure", "type": "Distributed Elastic", "text": ["Configures a timer client. Must be called before using expires."]}, {"name": "torch.distributed.elastic.timer.expires()", "path": "elastic/timer#torch.distributed.elastic.timer.expires", "type": "Distributed Elastic", "text": ["Acquires a countdown timer that expires in after seconds from now, unless the code-block that it wraps is finished within the timeframe. When the timer expires, this worker is eligible to be reaped. The exact meaning of \u201creaped\u201d depends on the client implementation. In most cases, reaping means to terminate the worker process. Note that the worker is NOT guaranteed to be reaped at exactly time.now() + after, but rather the worker is \u201celigible\u201d for being reaped and the TimerServer that the client talks to will ultimately make the decision when and how to reap the workers with expired timers.", "Usage:"]}, {"name": "torch.distributed.elastic.timer.FileTimerClient", "path": "elastic/timer#torch.distributed.elastic.timer.FileTimerClient", "type": "Distributed Elastic", "text": ["Client side of FileTimerServer. This client is meant to be used on the same host that the FileTimerServer is running on and uses pid to uniquely identify a worker. This client uses a named_pipe to send timer requests to the FileTimerServer. This client is a producer while the FileTimerServer is a consumer. Multiple clients can work with the same FileTimerServer."]}, {"name": "torch.distributed.elastic.timer.FileTimerServer", "path": "elastic/timer#torch.distributed.elastic.timer.FileTimerServer", "type": "Distributed Elastic", "text": ["Server that works with FileTimerClient. Clients are expected to be running on the same host as the process that is running this server. Each host in the job is expected to start its own timer server locally and each server instance manages timers for local workers (running on processes on the same host)."]}, {"name": "torch.distributed.elastic.timer.LocalTimerClient", "path": "elastic/timer#torch.distributed.elastic.timer.LocalTimerClient", "type": "Distributed Elastic", "text": ["Client side of LocalTimerServer. This client is meant to be used on the same host that the LocalTimerServer is running on and uses pid to uniquely identify a worker. This is particularly useful in situations where one spawns a subprocess (trainer) per GPU on a host with multiple GPU devices."]}, {"name": "torch.distributed.elastic.timer.LocalTimerServer", "path": "elastic/timer#torch.distributed.elastic.timer.LocalTimerServer", "type": "Distributed Elastic", "text": ["Server that works with LocalTimerClient. Clients are expected to be subprocesses to the parent process that is running this server. Each host in the job is expected to start its own timer server locally and each server instance manages timers for local workers (running on processes on the same host)."]}, {"name": "torch.distributed.elastic.timer.TimerClient", "path": "elastic/timer#torch.distributed.elastic.timer.TimerClient", "type": "Distributed Elastic", "text": ["Client library to acquire and release countdown timers by communicating with the TimerServer.", "Acquires a timer for the worker that holds this client object given the scope_id and expiration_time. Typically registers the timer with the TimerServer.", "Releases the timer for the scope_id on the worker this client represents. After this method is called, the countdown timer on the scope is no longer in effect."]}, {"name": "torch.distributed.elastic.timer.TimerClient.acquire()", "path": "elastic/timer#torch.distributed.elastic.timer.TimerClient.acquire", "type": "Distributed Elastic", "text": ["Acquires a timer for the worker that holds this client object given the scope_id and expiration_time. Typically registers the timer with the TimerServer."]}, {"name": "torch.distributed.elastic.timer.TimerClient.release()", "path": "elastic/timer#torch.distributed.elastic.timer.TimerClient.release", "type": "Distributed Elastic", "text": ["Releases the timer for the scope_id on the worker this client represents. After this method is called, the countdown timer on the scope is no longer in effect."]}, {"name": "torch.distributed.elastic.timer.TimerRequest", "path": "elastic/timer#torch.distributed.elastic.timer.TimerRequest", "type": "Distributed Elastic", "text": ["Data object representing a countdown timer acquisition and release that is used between the TimerClient and TimerServer. A negative expiration_time should be interpreted as a \u201crelease\u201d request.", "Note", "the type of worker_id is implementation specific. It is whatever the TimerServer and TimerClient implementations have on to uniquely identify a worker."]}, {"name": "torch.distributed.elastic.timer.TimerServer", "path": "elastic/timer#torch.distributed.elastic.timer.TimerServer", "type": "Distributed Elastic", "text": ["Entity that monitors active timers and expires them in a timely fashion. This server is responsible for reaping workers that have expired timers.", "Clears all timers for the given worker_ids.", "Returns all expired timers for each worker_id. An expired timer is a timer for which the expiration_time is less than or equal to the provided deadline.", "Dict[str, List[TimerRequest]]", "Processes the incoming timer requests and registers them with the server. The timer request can either be a acquire-timer or release-timer request. Timer requests with a negative expiration_time should be interpreted as a release-timer request."]}, {"name": "torch.distributed.elastic.timer.TimerServer.clear_timers()", "path": "elastic/timer#torch.distributed.elastic.timer.TimerServer.clear_timers", "type": "Distributed Elastic", "text": ["Clears all timers for the given worker_ids."]}, {"name": "torch.distributed.elastic.timer.TimerServer.get_expired_timers()", "path": "elastic/timer#torch.distributed.elastic.timer.TimerServer.get_expired_timers", "type": "Distributed Elastic", "text": ["Returns all expired timers for each worker_id. An expired timer is a timer for which the expiration_time is less than or equal to the provided deadline.", "Dict[str, List[TimerRequest]]"]}, {"name": "torch.distributed.elastic.timer.TimerServer.register_timers()", "path": "elastic/timer#torch.distributed.elastic.timer.TimerServer.register_timers", "type": "Distributed Elastic", "text": ["Processes the incoming timer requests and registers them with the server. The timer request can either be a acquire-timer or release-timer request. Timer requests with a negative expiration_time should be interpreted as a release-timer request."]}, {"name": "torch.distributed.FileStore", "path": "distributed#torch.distributed.FileStore", "type": "Distributed Communication", "text": ["A store implementation that uses a file to store the underlying key-value pairs."]}, {"name": "torch.distributed.fsdp.BackwardPrefetch", "path": "fsdp#torch.distributed.fsdp.BackwardPrefetch", "type": "Fully Sharded Data Parallel", "text": ["This configures explicit backward prefetching, which improves throughput by enabling communication and computation overlap in the backward pass at the cost of slightly increased memory usage.", "For more technical context: For a single process group using NCCL backend, any collectives, even if issued from different streams, contend for the same per-device NCCL stream, which implies that the relative order in which the collectives are issued matters for overlapping. The two backward prefetching values correspond to different issue orders."]}, {"name": "torch.distributed.fsdp.CPUOffload", "path": "fsdp#torch.distributed.fsdp.CPUOffload", "type": "Fully Sharded Data Parallel", "text": ["This configures CPU offloading.", "offload_params (bool) \u2013 This specifies whether to offload parameters to CPU when not involved in computation. If True, then this offloads gradients to CPU as well, meaning that the optimizer step runs on CPU."]}, {"name": "torch.distributed.fsdp.FullOptimStateDictConfig", "path": "fsdp#torch.distributed.fsdp.FullOptimStateDictConfig", "type": "Fully Sharded Data Parallel", "text": ["rank0_only (bool) \u2013 If True, then only rank 0 saves the full state dict, and nonzero ranks save an empty dict. If False, then all ranks save the full state dict. (Default: False)"]}, {"name": "torch.distributed.fsdp.FullStateDictConfig", "path": "fsdp#torch.distributed.fsdp.FullStateDictConfig", "type": "Fully Sharded Data Parallel", "text": ["FullStateDictConfig is a config class meant to be used with StateDictType.FULL_STATE_DICT. We recommend enabling both offload_to_cpu=True and rank0_only=True when saving full state dicts to save GPU memory and CPU memory, respectively. This config class is meant to be used via the state_dict_type() context manager as follows:", "rank0_only (bool) \u2013 If True, then only rank 0 saves the full state dict, and nonzero ranks save an empty dict. If False, then all ranks save the full state dict. (Default: False)"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel", "type": "Fully Sharded Data Parallel", "text": ["A wrapper for sharding module parameters across data parallel workers. This is inspired by Xu et al. as well as the ZeRO Stage 3 from DeepSpeed. FullyShardedDataParallel is commonly shortened to FSDP.", "Example:", "Warning", "The optimizer must be initialized after the module has been wrapped with FSDP since FSDP will shard and transform the module\u2019s parameters in a way that may not preserve the original parameter variables. Thus, the previously initialized optimizer may have stale references to the parameters.", "Warning", "If the destination CUDA device has ID dev_id, either (1) module should already be placed on that device, (2) the device should be set using torch.cuda.set_device(dev_id), or (3) dev_id should be passed into the device_id constructor argument. This FSDP instance\u2019s compute device will be that destination device. For (1) and (3), the FSDP initialization always occurs on GPU. For (2), the FSDP initialization happens on module \u2018s current device, which may be CPU.", "Warning", "FSDP currently does not support gradient accumulation outside no_sync() when using CPU offloading. Trying to do so yields incorrect results since FSDP will use the newly-reduced gradient instead of accumulating with any existing gradient.", "Warning", "Changing the original parameter variable names after construction will lead to undefined behavior.", "Warning", "Passing in the sync_module_states=True flag requires module to be on GPU or to use the device_id argument to specify a CUDA device that FSDP will move module to in the FSDP constructor. This is because sync_module_states=True requires GPU communication.", "Warning", "As of PyTorch 1.12, FSDP only offers limited support for shared parameters (for example, setting one Linear layer\u2019s weight to another\u2019s). In particular, modules that share parameters must be wrapped as part of the same FSDP unit. If enhanced shared parameter support is needed for your use case, please ping https://github.com/pytorch/pytorch/issues/77724", "Warning", "FSDP has some constraints on freezing parameters (i.e. setting param.requires_grad=False). For use_orig_params=False, each FSDP instance must manage parameters that are all frozen or all non-frozen. For use_orig_params=True, FSDP supports mixing frozen and non-frozen, but we recommend not doing so since then the gradient memory usage will be higher than expected (namely, equivalent to not freezing those parameters). This means that ideally, frozen parameters should be isolated into their own nn.Module s and wrapped separately with FSDP.", "Note", "Attempting to run the forward pass of a submodule that is contained in an FSDP instance is not supported and will result in errors. This is because the submodule\u2019s parameters will be sharded, but it itself is not an FSDP instance, so its forward pass will not all-gather the full parameters appropriately. This could potentially happen when attempting to run only the encoder of a encoder-decoder model, and the encoder is not wrapped in its own FSDP instance. To resolve this, please wrap the submodule in its own FSDP unit.", "Note", "FSDP moves input tensors to the forward method to the GPU compute device, so the user does not need to manually move them from CPU.", "Warning", "The user should not modify the parameters between forward and backward without using the summon_full_params() context since the modifications may not persist. Moreover, for use_orig_params=False, accessing the original parameters between forward and backward may raise an illegal memory access.", "Warning", "For use_orig_params=True, ShardingStrategy.SHARD_GRAD_OP exposes the unsharded parameters, not the sharded parameters, after forward since it does not free the unsharded ones, unlike ShardingStrategy.FULL_SHARD. One caveat is that, since gradients are always sharded or None, ShardingStrategy.SHARD_GRAD_OP will not expose the sharded gradients with the unsharded parameters after forward. If you want to inspect the gradients, try summon_full_params() with with_grads=True.", "Warning", "FSDP replaces managed modules\u2019 parameters with torch.Tensor views during forward and backward computation for autograd-related reasons. If your module\u2019s forward relies on saved references to the parameters instead of reacquiring the references each iteration, then it will not see FSDP\u2019s newly created views, and autograd will not work correctly.", "Note", "With limit_all_gathers=True, you may see a gap in the FSDP pre-forward where the CPU thread is not issuing any kernels. This is intentional and shows the rate limiter in effect. Synchronizing the CPU thread in that way prevents over-allocating memory for subsequent all-gathers, and it should not actually delay GPU kernel execution.", "Note", "When using sharding_strategy=ShardingStrategy.HYBRID_SHARD with the sharding process group being intra-node and the replication process group being inter-node, setting NCCL_CROSS_NIC=1 can help improve the all-reduce times over the replication process group for some cluster setups.", "auto_wrap_policy (Optional[Union[Callable[[nn.Module, bool, int], bool], ModuleWrapPolicy]]) \u2013 ", "This specifies a policy to apply FSDP to submodules of module, which is needed for communication and computation overlap and thus affects performance. If None, then FSDP only applies to module, and users should manually apply FSDP to parent modules themselves (proceeding bottom-up). For convenience, this accepts ModuleWrapPolicy directly, which allows users to specify the module classes to wrap (e.g. the transformer block). Otherwise, this should be a callable that takes in three arguments module: nn.Module, recurse: bool, and nonwrapped_numel: int and should return a bool specifying whether the passed-in module should have FSDP applied if recurse=False or if the traversal should continue into the module\u2019s subtree if recurse=True. Users may add additional arguments to the callable. The size_based_auto_wrap_policy in torch.distributed.fsdp.wrap.py gives an example callable that applies FSDP to a module if the parameters in its subtree exceed 100M numel. We recommend printing the model after applying FSDP and adjusting as needed.", "Example:", "param_init_fn (Optional[Callable[[nn.Module], None]]) \u2013 ", "A Callable[torch.nn.Module] -> None that specifies how modules that are currently on the meta device should be initialized onto an actual device. As of v1.12, FSDP detects modules with parameters or buffers on meta device via is_meta and either applies param_init_fn if specified or calls nn.Module.reset_parameters() otherwise. For both cases, the implementation should only initialize the parameters/buffers of the module, not those of its submodules. This is to avoid re-initialization. In addition, FSDP also supports deferred initialization via torchdistX\u2019s (https://github.com/pytorch/torchdistX) deferred_init() API, where the deferred modules are initialized by calling param_init_fn if specified or torchdistX\u2019s default materialize_module() otherwise. If param_init_fn is specified, then it is applied to all meta-device modules, meaning that it should probably case on the module type. FSDP calls the initialization function before parameter flattening and sharding.", "Example:", "Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).", "Compared to torch.nn.Module.apply, this version additionally gathers the full parameters before applying fn. It should not be called from within another summon_full_params context.", "fn (Module -> None) \u2013 function to be applied to each submodule", "self", "Clips the gradient norm of all parameters. The norm is computed over all parameters\u2019 gradients as viewed as a single vector, and the gradients are modified in-place.", "Total norm of the parameters (viewed as a single vector).", "Tensor", "Note", "If every FSDP instance uses NO_SHARD, meaning that no gradients are sharded across ranks, then you may directly use torch.nn.utils.clip_grad_norm_().", "Note", "If at least some FSDP instance uses a sharded strategy (i.e. one other than NO_SHARD), then you should use this method instead of torch.nn.utils.clip_grad_norm_() since this method handles the fact that gradients are sharded across ranks.", "Note", "The total norm returned will have the \u201clargest\u201d dtype across all parameters/gradients as defined by PyTorch\u2019s type promotion semantics. For example, if all parameters/gradients use a low precision dtype, then the returned norm\u2019s dtype will be that low precision dtype, but if there exists at least one parameter/ gradient using FP32, then the returned norm\u2019s dtype will be FP32.", "Warning", "This needs to be called on all ranks since it uses collective communications.", "The API is similar to shard_full_optim_state_dict(). The only difference is that the input sharded_optim_state_dict should be returned from sharded_optim_state_dict(). Therefore, there will be all-gather calls on each rank to gather ShardedTensor s.", "Refer to shard_full_optim_state_dict().", "Dict[str, Any]", "Runs the forward pass for the wrapped module, inserting FSDP-specific pre- and post-forward sharding logic.", "Any", "Returns all nested FSDP instances, possibly including module itself and only including FSDP root modules if root_only=True.", "FSDP modules that are nested in the input module.", "List[FullyShardedDataParallel]", "Consolidates the full optimizer state on rank 0 and returns it as a dict following the convention of torch.optim.Optimizer.state_dict(), i.e. with keys \"state\" and \"param_groups\". The flattened parameters in FSDP modules contained in model are mapped back to their unflattened parameters.", "Warning", "This needs to be called on all ranks since it uses collective communications. However, if rank0_only=True, then the state dict is only populated on rank 0, and all other ranks return an empty dict.", "Warning", "Unlike torch.optim.Optimizer.state_dict(), this method uses full parameter names as keys instead of parameter IDs.", "Note", "Like in torch.optim.Optimizer.state_dict(), the tensors contained in the optimizer state dict are not cloned, so there may be aliasing surprises. For best practices, consider saving the returned optimizer state dict immediately, e.g. using torch.save().", "A dict containing the optimizer state for model \u2018s original unflattened parameters and including keys \u201cstate\u201d and \u201cparam_groups\u201d following the convention of torch.optim.Optimizer.state_dict(). If rank0_only=True, then nonzero ranks return an empty dict.", "Dict[str, Any]", "Get the state_dict_type and the corresponding configurations for the FSDP modules rooted at module. The target module does not have to be an FSDP module.", "A StateDictSettings containing the state_dict_type and state_dict / optim_state_dict configs that are currently set.", "StateDictSettings", "Returns the wrapped module (like DistributedDataParallel).", "Overrides named_buffers() to intercept buffer names and remove all occurrences of the FSDP-specific flattened buffer prefix when inside the summon_full_params() context manager.", "Iterator[Tuple[str, Tensor]]", "Overrides named_parameters() to intercept parameter names and remove all occurrences of the FSDP-specific flattened parameter prefix when inside the summon_full_params() context manager.", "Iterator[Tuple[str, Parameter]]", "A context manager to disable gradient synchronizations across FSDP instances. Within this context, gradients will be accumulated in module variables, which will later be synchronized in the first forward-backward pass after exiting the context. This should only be used on the root FSDP instance and will recursively apply to all children FSDP instances.", "Note", "This likely results in higher memory usage because FSDP will accumulate the full model gradients (instead of gradient shards) until the eventual sync.", "Note", "When used with CPU offloading, the gradients will not be offloaded to CPU when inside the context manager. Instead, they will only be offloaded right after the eventual sync.", "Generator", "Transforms the state_dict of optim for the model that is sharded by FSDP to one of the three types: 1) full optimizer state_dict, 2) sharded optimizer state_dict, 3) local optimizer state_dict.", "For full optimizer state_dict, all states are unflattened and not sharded. Rank0 only and CPU only can be specified via state_dict_type() to avoid OOM.", "For sharded optimizer state_dict, all states are unflattened but sharded. CPU only can be specified via state_dict_type() to further save memory.", "For local state_dict, no transformation will be performed. But a state will be converted from nn.Tensor to ShardedTensor to represent its sharding nature (this is not supported yet).", "Example:", "A dict containing the optimizer state for model. The sharding of the optimizer state is based on state_dict_type.", "Dict[str, Any]", "Given a optim_state_dict that is transformed through optim_state_dict(), converts it to the flattened optimizer state_dict that can be loaded to optim which is the optimizer for model. model must be sharded by FullyShardedDataParallel.", "Dict[str, Any]", "Registers a communication hook which is an enhancement that provides a flexible hook to users where they can specify how FSDP aggregates gradients across multiple workers. This hook can be used to implement several algorithms like GossipGrad and gradient compression which involve different communication strategies for parameter syncs while training with FullyShardedDataParallel.", "Warning", "FSDP communication hook should be registered before running an initial forward pass and only once.", "state (object) \u2013 ", "Passed to the hook to maintain any state information during the training process. Examples include error feedback in gradient compression, peers to communicate with next in GossipGrad, etc. It is locally stored by each worker and shared by all the gradient tensors on the worker.", "Re-keys the optimizer state dict optim_state_dict to use the key type optim_state_key_type. This can be used to achieve compatibility between optimizer state dicts from models with FSDP instances and ones without.", "To re-key an FSDP full optimizer state dict (i.e. from full_optim_state_dict()) to use parameter IDs and be loadable to a non-wrapped model:", "To re-key a normal optimizer state dict from a non-wrapped model to be loadable to a wrapped model:", "The optimizer state dict re-keyed using the parameter keys specified by optim_state_key_type.", "Dict[str, Any]", "Scatters the full optimizer state dict from rank 0 to all other ranks, returning the sharded optimizer state dict on each rank. The return value is the same as shard_full_optim_state_dict(), and on rank 0, the first argument should be the return value of full_optim_state_dict().", "Example:", "Note", "Both shard_full_optim_state_dict() and scatter_full_optim_state_dict() may be used to get the sharded optimizer state dict to load. Assuming that the full optimizer state dict resides in CPU memory, the former requires each rank to have the full dict in CPU memory, where each rank individually shards the dict without any communication, while the latter requires only rank 0 to have the full dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and communicates it to ranks appropriately. Hence, the former has higher aggregate CPU memory cost, while the latter has higher communication cost.", "The full optimizer state dict now remapped to flattened parameters instead of unflattened parameters and restricted to only include this rank\u2019s part of the optimizer state.", "Dict[str, Any]", "Set the state_dict_type and the corresponding (optional) configurations of all the descendant FSDP modules of the target module. The target module does not have to be a FSDP module. If the target module is a FSDP module, its state_dict_type will also be changed.", "Note", "This API should be called for only the top-level (root) module.", "Note", "This API enables users to transparently use the conventional state_dict API to take model checkpoints in cases where the root FSDP module is wrapped by another nn.Module. For example, the following will ensure state_dict is called on all non-FSDP instances, while dispatching into sharded_state_dict implementation for FSDP:", "Example:", "A StateDictSettings that include the previous state_dict type and configuration for the module.", "StateDictSettings", "Shards the full optimizer state dict full_optim_state_dict by remapping the state to flattened parameters instead of unflattened parameters and restricting to only this rank\u2019s part of the optimizer state. The first argument should be the return value of full_optim_state_dict().", "Example:", "Note", "Both shard_full_optim_state_dict() and scatter_full_optim_state_dict() may be used to get the sharded optimizer state dict to load. Assuming that the full optimizer state dict resides in CPU memory, the former requires each rank to have the full dict in CPU memory, where each rank individually shards the dict without any communication, while the latter requires only rank 0 to have the full dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and communicates it to ranks appropriately. Hence, the former has higher aggregate CPU memory cost, while the latter has higher communication cost.", "The full optimizer state dict now remapped to flattened parameters instead of unflattened parameters and restricted to only include this rank\u2019s part of the optimizer state.", "Dict[str, Any]", "The API is similar to full_optim_state_dict() but this API chunks all non-zero-dimension states to ShardedTensor to save memory. This API should only be used when the model state_dict is derived with the context manager with state_dict_type(SHARDED_STATE_DICT):.", "For the detailed usage, refer to full_optim_state_dict().", "Warning", "The returned state dict contains ShardedTensor and cannot be directly used by the regular optim.load_state_dict.", "Dict[str, Any]", "A context manager to set the state_dict_type of all the descendant FSDP modules of the target module. This context manager has the same functions as set_state_dict_type(). Read the document of set_state_dict_type() for the detail.", "Example:", "Generator", "A context manager to expose full params for FSDP instances. Can be useful after forward/backward for a model to get the params for additional processing or checking. It can take a non-FSDP module and will summon full params for all contained FSDP modules as well as their children, depending on the recurse argument.", "Note", "This can be used on inner FSDPs.", "Note", "This can not be used within a forward or backward pass. Nor can forward and backward be started from within this context.", "Note", "Parameters will revert to their local shards after the context manager exits, storage behavior is the same as forward.", "Note", "The full parameters can be modified, but only the portion corresponding to the local param shard will persist after the context manager exits (unless writeback=False, in which case changes will be discarded). In the case where FSDP does not shard the parameters, currently only when world_size == 1, or NO_SHARD config, the modification is persisted regardless of writeback.", "Note", "This method works on modules which are not FSDP themselves but may contain multiple independent FSDP units. In that case, the given arguments will apply to all contained FSDP units.", "Warning", "Note that rank0_only=True in conjunction with writeback=True is not currently supported and will raise an error. This is because model parameter shapes would be different across ranks within the context, and writing to them can lead to inconsistency across ranks when the context is exited.", "Warning", "Note that offload_to_cpu and rank0_only=False will result in full parameters being redundantly copied to CPU memory for GPUs that reside on the same machine, which may incur the risk of CPU OOM. It is recommended to use offload_to_cpu with rank0_only=True.", "Generator"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.apply()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.apply", "type": "Fully Sharded Data Parallel", "text": ["Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).", "Compared to torch.nn.Module.apply, this version additionally gathers the full parameters before applying fn. It should not be called from within another summon_full_params context.", "fn (Module -> None) \u2013 function to be applied to each submodule", "self"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.clip_grad_norm_()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.clip_grad_norm_", "type": "Fully Sharded Data Parallel", "text": ["Clips the gradient norm of all parameters. The norm is computed over all parameters\u2019 gradients as viewed as a single vector, and the gradients are modified in-place.", "Total norm of the parameters (viewed as a single vector).", "Tensor", "Note", "If every FSDP instance uses NO_SHARD, meaning that no gradients are sharded across ranks, then you may directly use torch.nn.utils.clip_grad_norm_().", "Note", "If at least some FSDP instance uses a sharded strategy (i.e. one other than NO_SHARD), then you should use this method instead of torch.nn.utils.clip_grad_norm_() since this method handles the fact that gradients are sharded across ranks.", "Note", "The total norm returned will have the \u201clargest\u201d dtype across all parameters/gradients as defined by PyTorch\u2019s type promotion semantics. For example, if all parameters/gradients use a low precision dtype, then the returned norm\u2019s dtype will be that low precision dtype, but if there exists at least one parameter/ gradient using FP32, then the returned norm\u2019s dtype will be FP32.", "Warning", "This needs to be called on all ranks since it uses collective communications."]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.flatten_sharded_optim_state_dict()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.flatten_sharded_optim_state_dict", "type": "Fully Sharded Data Parallel", "text": ["The API is similar to shard_full_optim_state_dict(). The only difference is that the input sharded_optim_state_dict should be returned from sharded_optim_state_dict(). Therefore, there will be all-gather calls on each rank to gather ShardedTensor s.", "Refer to shard_full_optim_state_dict().", "Dict[str, Any]"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.forward()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.forward", "type": "Fully Sharded Data Parallel", "text": ["Runs the forward pass for the wrapped module, inserting FSDP-specific pre- and post-forward sharding logic.", "Any"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.fsdp_modules()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.fsdp_modules", "type": "Fully Sharded Data Parallel", "text": ["Returns all nested FSDP instances, possibly including module itself and only including FSDP root modules if root_only=True.", "FSDP modules that are nested in the input module.", "List[FullyShardedDataParallel]"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict", "type": "Fully Sharded Data Parallel", "text": ["Consolidates the full optimizer state on rank 0 and returns it as a dict following the convention of torch.optim.Optimizer.state_dict(), i.e. with keys \"state\" and \"param_groups\". The flattened parameters in FSDP modules contained in model are mapped back to their unflattened parameters.", "Warning", "This needs to be called on all ranks since it uses collective communications. However, if rank0_only=True, then the state dict is only populated on rank 0, and all other ranks return an empty dict.", "Warning", "Unlike torch.optim.Optimizer.state_dict(), this method uses full parameter names as keys instead of parameter IDs.", "Note", "Like in torch.optim.Optimizer.state_dict(), the tensors contained in the optimizer state dict are not cloned, so there may be aliasing surprises. For best practices, consider saving the returned optimizer state dict immediately, e.g. using torch.save().", "A dict containing the optimizer state for model \u2018s original unflattened parameters and including keys \u201cstate\u201d and \u201cparam_groups\u201d following the convention of torch.optim.Optimizer.state_dict(). If rank0_only=True, then nonzero ranks return an empty dict.", "Dict[str, Any]"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.get_state_dict_type()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.get_state_dict_type", "type": "Fully Sharded Data Parallel", "text": ["Get the state_dict_type and the corresponding configurations for the FSDP modules rooted at module. The target module does not have to be an FSDP module.", "A StateDictSettings containing the state_dict_type and state_dict / optim_state_dict configs that are currently set.", "StateDictSettings"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.module", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.module", "type": "Fully Sharded Data Parallel", "text": ["Returns the wrapped module (like DistributedDataParallel)."]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.named_buffers()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.named_buffers", "type": "Fully Sharded Data Parallel", "text": ["Overrides named_buffers() to intercept buffer names and remove all occurrences of the FSDP-specific flattened buffer prefix when inside the summon_full_params() context manager.", "Iterator[Tuple[str, Tensor]]"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.named_parameters()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.named_parameters", "type": "Fully Sharded Data Parallel", "text": ["Overrides named_parameters() to intercept parameter names and remove all occurrences of the FSDP-specific flattened parameter prefix when inside the summon_full_params() context manager.", "Iterator[Tuple[str, Parameter]]"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.no_sync()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.no_sync", "type": "Fully Sharded Data Parallel", "text": ["A context manager to disable gradient synchronizations across FSDP instances. Within this context, gradients will be accumulated in module variables, which will later be synchronized in the first forward-backward pass after exiting the context. This should only be used on the root FSDP instance and will recursively apply to all children FSDP instances.", "Note", "This likely results in higher memory usage because FSDP will accumulate the full model gradients (instead of gradient shards) until the eventual sync.", "Note", "When used with CPU offloading, the gradients will not be offloaded to CPU when inside the context manager. Instead, they will only be offloaded right after the eventual sync.", "Generator"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict", "type": "Fully Sharded Data Parallel", "text": ["Transforms the state_dict of optim for the model that is sharded by FSDP to one of the three types: 1) full optimizer state_dict, 2) sharded optimizer state_dict, 3) local optimizer state_dict.", "For full optimizer state_dict, all states are unflattened and not sharded. Rank0 only and CPU only can be specified via state_dict_type() to avoid OOM.", "For sharded optimizer state_dict, all states are unflattened but sharded. CPU only can be specified via state_dict_type() to further save memory.", "For local state_dict, no transformation will be performed. But a state will be converted from nn.Tensor to ShardedTensor to represent its sharding nature (this is not supported yet).", "Example:", "A dict containing the optimizer state for model. The sharding of the optimizer state is based on state_dict_type.", "Dict[str, Any]"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict_to_load()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict_to_load", "type": "Fully Sharded Data Parallel", "text": ["Given a optim_state_dict that is transformed through optim_state_dict(), converts it to the flattened optimizer state_dict that can be loaded to optim which is the optimizer for model. model must be sharded by FullyShardedDataParallel.", "Dict[str, Any]"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.register_comm_hook()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.register_comm_hook", "type": "Fully Sharded Data Parallel", "text": ["Registers a communication hook which is an enhancement that provides a flexible hook to users where they can specify how FSDP aggregates gradients across multiple workers. This hook can be used to implement several algorithms like GossipGrad and gradient compression which involve different communication strategies for parameter syncs while training with FullyShardedDataParallel.", "Warning", "FSDP communication hook should be registered before running an initial forward pass and only once.", "state (object) \u2013 ", "Passed to the hook to maintain any state information during the training process. Examples include error feedback in gradient compression, peers to communicate with next in GossipGrad, etc. It is locally stored by each worker and shared by all the gradient tensors on the worker."]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.rekey_optim_state_dict()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.rekey_optim_state_dict", "type": "Fully Sharded Data Parallel", "text": ["Re-keys the optimizer state dict optim_state_dict to use the key type optim_state_key_type. This can be used to achieve compatibility between optimizer state dicts from models with FSDP instances and ones without.", "To re-key an FSDP full optimizer state dict (i.e. from full_optim_state_dict()) to use parameter IDs and be loadable to a non-wrapped model:", "To re-key a normal optimizer state dict from a non-wrapped model to be loadable to a wrapped model:", "The optimizer state dict re-keyed using the parameter keys specified by optim_state_key_type.", "Dict[str, Any]"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict", "type": "Fully Sharded Data Parallel", "text": ["Scatters the full optimizer state dict from rank 0 to all other ranks, returning the sharded optimizer state dict on each rank. The return value is the same as shard_full_optim_state_dict(), and on rank 0, the first argument should be the return value of full_optim_state_dict().", "Example:", "Note", "Both shard_full_optim_state_dict() and scatter_full_optim_state_dict() may be used to get the sharded optimizer state dict to load. Assuming that the full optimizer state dict resides in CPU memory, the former requires each rank to have the full dict in CPU memory, where each rank individually shards the dict without any communication, while the latter requires only rank 0 to have the full dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and communicates it to ranks appropriately. Hence, the former has higher aggregate CPU memory cost, while the latter has higher communication cost.", "The full optimizer state dict now remapped to flattened parameters instead of unflattened parameters and restricted to only include this rank\u2019s part of the optimizer state.", "Dict[str, Any]"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type", "type": "Fully Sharded Data Parallel", "text": ["Set the state_dict_type and the corresponding (optional) configurations of all the descendant FSDP modules of the target module. The target module does not have to be a FSDP module. If the target module is a FSDP module, its state_dict_type will also be changed.", "Note", "This API should be called for only the top-level (root) module.", "Note", "This API enables users to transparently use the conventional state_dict API to take model checkpoints in cases where the root FSDP module is wrapped by another nn.Module. For example, the following will ensure state_dict is called on all non-FSDP instances, while dispatching into sharded_state_dict implementation for FSDP:", "Example:", "A StateDictSettings that include the previous state_dict type and configuration for the module.", "StateDictSettings"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict", "type": "Fully Sharded Data Parallel", "text": ["Shards the full optimizer state dict full_optim_state_dict by remapping the state to flattened parameters instead of unflattened parameters and restricting to only this rank\u2019s part of the optimizer state. The first argument should be the return value of full_optim_state_dict().", "Example:", "Note", "Both shard_full_optim_state_dict() and scatter_full_optim_state_dict() may be used to get the sharded optimizer state dict to load. Assuming that the full optimizer state dict resides in CPU memory, the former requires each rank to have the full dict in CPU memory, where each rank individually shards the dict without any communication, while the latter requires only rank 0 to have the full dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and communicates it to ranks appropriately. Hence, the former has higher aggregate CPU memory cost, while the latter has higher communication cost.", "The full optimizer state dict now remapped to flattened parameters instead of unflattened parameters and restricted to only include this rank\u2019s part of the optimizer state.", "Dict[str, Any]"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.sharded_optim_state_dict()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.sharded_optim_state_dict", "type": "Fully Sharded Data Parallel", "text": ["The API is similar to full_optim_state_dict() but this API chunks all non-zero-dimension states to ShardedTensor to save memory. This API should only be used when the model state_dict is derived with the context manager with state_dict_type(SHARDED_STATE_DICT):.", "For the detailed usage, refer to full_optim_state_dict().", "Warning", "The returned state dict contains ShardedTensor and cannot be directly used by the regular optim.load_state_dict.", "Dict[str, Any]"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type", "type": "Fully Sharded Data Parallel", "text": ["A context manager to set the state_dict_type of all the descendant FSDP modules of the target module. This context manager has the same functions as set_state_dict_type(). Read the document of set_state_dict_type() for the detail.", "Example:", "Generator"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params", "type": "Fully Sharded Data Parallel", "text": ["A context manager to expose full params for FSDP instances. Can be useful after forward/backward for a model to get the params for additional processing or checking. It can take a non-FSDP module and will summon full params for all contained FSDP modules as well as their children, depending on the recurse argument.", "Note", "This can be used on inner FSDPs.", "Note", "This can not be used within a forward or backward pass. Nor can forward and backward be started from within this context.", "Note", "Parameters will revert to their local shards after the context manager exits, storage behavior is the same as forward.", "Note", "The full parameters can be modified, but only the portion corresponding to the local param shard will persist after the context manager exits (unless writeback=False, in which case changes will be discarded). In the case where FSDP does not shard the parameters, currently only when world_size == 1, or NO_SHARD config, the modification is persisted regardless of writeback.", "Note", "This method works on modules which are not FSDP themselves but may contain multiple independent FSDP units. In that case, the given arguments will apply to all contained FSDP units.", "Warning", "Note that rank0_only=True in conjunction with writeback=True is not currently supported and will raise an error. This is because model parameter shapes would be different across ranks within the context, and writing to them can lead to inconsistency across ranks when the context is exited.", "Warning", "Note that offload_to_cpu and rank0_only=False will result in full parameters being redundantly copied to CPU memory for GPUs that reside on the same machine, which may incur the risk of CPU OOM. It is recommended to use offload_to_cpu with rank0_only=True.", "Generator"]}, {"name": "torch.distributed.fsdp.LocalOptimStateDictConfig", "path": "fsdp#torch.distributed.fsdp.LocalOptimStateDictConfig", "type": "Fully Sharded Data Parallel", "text": []}, {"name": "torch.distributed.fsdp.LocalStateDictConfig", "path": "fsdp#torch.distributed.fsdp.LocalStateDictConfig", "type": "Fully Sharded Data Parallel", "text": []}, {"name": "torch.distributed.fsdp.MixedPrecision", "path": "fsdp#torch.distributed.fsdp.MixedPrecision", "type": "Fully Sharded Data Parallel", "text": ["This configures FSDP-native mixed precision training.", "Note", "This API is experimental and subject to change.", "Note", "Only floating point tensors are cast to their specified dtypes.", "Note", "In summon_full_params, parameters are forced to full precision, but buffers are not.", "Note", "Layer norm and batch norm accumulate in float32 even when their inputs are in a low precision like float16 or bfloat16. Disabling FSDP\u2019s mixed precision for those norm modules only means that the affine parameters are kept in float32. However, this incurs separate all-gathers and reduce-scatters for those norm modules, which may be inefficient, so if the workload permits, the user should prefer to still apply mixed precision to those modules.", "Note", "By default, if the user passes a model with any _BatchNorm modules and specifies an auto_wrap_policy, then the batch norm modules will have FSDP applied to them separately with mixed precision disabled. See the _module_classes_to_ignore argument.", "Note", "MixedPrecision has cast_root_forward_inputs=True and cast_forward_inputs=False by default. For the root FSDP instance, its cast_root_forward_inputs takes precedence over its cast_forward_inputs. For non-root FSDP instances, their cast_root_forward_inputs values are ignored. The default setting is sufficient for the typical case where each FSDP instance has the same MixedPrecision configuration and only needs to cast inputs to the param_dtype at the beginning of the model\u2019s forward pass.", "Note", "For nested FSDP instances with different MixedPrecision configurations, we recommend setting individual cast_forward_inputs values to configure casting inputs or not before each instance\u2019s forward. In such a case, since the casts happen before each FSDP instance\u2019s forward, a parent FSDP instance should have its non-FSDP submodules run before its FSDP submodules to avoid the activation dtype being changed due to a different MixedPrecision configuration.", "Example:", "The above shows a working example. On the other hand, if model[1] were replaced with model[0], meaning that the submodule using different MixedPrecision ran its forward first, then model[1] would incorrectly see float16 activations instead of bfloat16 ones."]}, {"name": "torch.distributed.fsdp.OptimStateDictConfig", "path": "fsdp#torch.distributed.fsdp.OptimStateDictConfig", "type": "Fully Sharded Data Parallel", "text": ["OptimStateDictConfig is the base class for all optim_state_dict configuration classes. Users should instantiate a child class (e.g. FullOptimStateDictConfig) in order to configure settings for the corresponding optim_state_dict type supported by FSDP."]}, {"name": "torch.distributed.fsdp.ShardedOptimStateDictConfig", "path": "fsdp#torch.distributed.fsdp.ShardedOptimStateDictConfig", "type": "Fully Sharded Data Parallel", "text": []}, {"name": "torch.distributed.fsdp.ShardedStateDictConfig", "path": "fsdp#torch.distributed.fsdp.ShardedStateDictConfig", "type": "Fully Sharded Data Parallel", "text": []}, {"name": "torch.distributed.fsdp.ShardingStrategy", "path": "fsdp#torch.distributed.fsdp.ShardingStrategy", "type": "Fully Sharded Data Parallel", "text": ["This specifies the sharding strategy to be used for distributed training by FullyShardedDataParallel."]}, {"name": "torch.distributed.fsdp.StateDictConfig", "path": "fsdp#torch.distributed.fsdp.StateDictConfig", "type": "Fully Sharded Data Parallel", "text": ["StateDictConfig is the base class for all state_dict configuration classes. Users should instantiate a child class (e.g. FullStateDictConfig) in order to configure settings for the corresponding state_dict type supported by FSDP."]}, {"name": "torch.distributed.fsdp.StateDictSettings", "path": "fsdp#torch.distributed.fsdp.StateDictSettings", "type": "Fully Sharded Data Parallel", "text": []}, {"name": "torch.distributed.gather()", "path": "distributed#torch.distributed.gather", "type": "Distributed Communication", "text": ["Gathers a list of tensors in a single process.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.gather_object()", "path": "distributed#torch.distributed.gather_object", "type": "Distributed Communication", "text": ["Gathers picklable objects from the whole group in a single process. Similar to gather(), but Python objects can be passed in. Note that the object must be picklable in order to be gathered.", "None. On the dst rank, object_gather_list will contain the output of the collective.", "Note", "Note that this API differs slightly from the gather collective since it does not provide an async_op handle and thus will be a blocking call.", "Note", "For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsiblity to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().", "Warning", "gather_object() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.", "Warning", "Calling gather_object() with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using gather() instead."]}, {"name": "torch.distributed.get_backend()", "path": "distributed#torch.distributed.get_backend", "type": "Distributed Communication", "text": ["Returns the backend of the given process group.", "group (ProcessGroup, optional) \u2013 The process group to work on. The default is the general main process group. If another specific group is specified, the calling process must be part of group.", "The backend of the given process group as a lower case string.", "str"]}, {"name": "torch.distributed.get_global_rank()", "path": "distributed#torch.distributed.get_global_rank", "type": "Distributed Communication", "text": ["Translate a group rank into a global rank.", "group_rank must be part of group otherwise this raises RuntimeError.", "Global rank of group_rank relative to group", "int", "N.B. calling this function on the default process group returns identity"]}, {"name": "torch.distributed.get_group_rank()", "path": "distributed#torch.distributed.get_group_rank", "type": "Distributed Communication", "text": ["Translate a global rank into a group rank.", "global_rank must be part of group otherwise this raises RuntimeError.", "Group rank of global_rank relative to group", "int", "N.B. calling this function on the default process group returns identity"]}, {"name": "torch.distributed.get_process_group_ranks()", "path": "distributed#torch.distributed.get_process_group_ranks", "type": "Distributed Communication", "text": ["Get all ranks associated with group.", "group (ProcessGroup) \u2013 ProcessGroup to get all ranks from.", "List of global ranks ordered by group rank."]}, {"name": "torch.distributed.get_rank()", "path": "distributed#torch.distributed.get_rank", "type": "Distributed Communication", "text": ["Returns the rank of the current process in the provided group or the default group if none was provided.", "Rank is a unique identifier assigned to each process within a distributed process group. They are always consecutive integers ranging from 0 to world_size.", "group (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used.", "The rank of the process group -1, if not part of the group", "int"]}, {"name": "torch.distributed.get_world_size()", "path": "distributed#torch.distributed.get_world_size", "type": "Distributed Communication", "text": ["Returns the number of processes in the current process group", "group (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used.", "The world size of the process group -1, if not part of the group", "int"]}, {"name": "torch.distributed.GradBucket", "path": "ddp_comm_hooks#torch.distributed.GradBucket", "type": "DDP Communication Hooks", "text": ["This class mainly passes a flattened gradient tensor (returned by buffer()) to DDP communication hook. This tensor can be further decomposed into a list of per-parameter tensors within this bucket (returned by get_per_parameter_tensors()) to apply layer-wise operations."]}, {"name": "torch.distributed.GradBucket.buffer()", "path": "ddp_comm_hooks#torch.distributed.GradBucket.buffer", "type": "DDP Communication Hooks", "text": ["A flattened 1D torch.Tensor buffer, which can be further decomposed into a list of per-parameter tensors within this bucket."]}, {"name": "torch.distributed.GradBucket.gradients()", "path": "ddp_comm_hooks#torch.distributed.GradBucket.gradients", "type": "DDP Communication Hooks", "text": ["A list of torch.Tensor. Each tensor in the list corresponds to a gradient."]}, {"name": "torch.distributed.GradBucket.index()", "path": "ddp_comm_hooks#torch.distributed.GradBucket.index", "type": "DDP Communication Hooks", "text": ["Warning", "Since the buckets are rebuilt after the first iteration, should not rely on the indices at the beginning of training.", "The index of a bucket that stores gradients of a few contiguous layers. All the gradients are bucketized."]}, {"name": "torch.distributed.GradBucket.is_last()", "path": "ddp_comm_hooks#torch.distributed.GradBucket.is_last", "type": "DDP Communication Hooks", "text": ["Whether this bucket is the last bucket to allreduce in an iteration. This also means that this bucket corresponds to the first few layers in the forward pass."]}, {"name": "torch.distributed.GradBucket.parameters()", "path": "ddp_comm_hooks#torch.distributed.GradBucket.parameters", "type": "DDP Communication Hooks", "text": ["A list of torch.Tensor. Each tensor in the list corresponds to a model parameter."]}, {"name": "torch.distributed.GradBucket.set_buffer()", "path": "ddp_comm_hooks#torch.distributed.GradBucket.set_buffer", "type": "DDP Communication Hooks", "text": ["Replaces the tensor in the bucket with the input tensor buffer."]}, {"name": "torch.distributed.HashStore", "path": "distributed#torch.distributed.HashStore", "type": "Distributed Communication", "text": ["A thread-safe store implementation based on an underlying hashmap. This store can be used within the same process (for example, by other threads), but cannot be used across processes."]}, {"name": "torch.distributed.init_process_group()", "path": "distributed#torch.distributed.init_process_group", "type": "Distributed Communication", "text": ["Initializes the default distributed process group, and this will also initialize the distributed package.", "If neither is specified, init_method is assumed to be \u201cenv://\u201d.", "Note", "To enable backend == Backend.MPI, PyTorch needs to be built from source on a system that supports MPI.", "Note", "Support for multiple backends is experimental. Currently when no backend is specified, both gloo and nccl backends will be created. The gloo backend will be used for collectives with CPU tensors and the nccl backend will be used for collectives with CUDA tensors. A custom backend can be specified by passing in a string with format \u201c<device_type>:<backend_name>,<device_type>:<backend_name>\u201d, e.g. \u201ccpu:gloo,cuda:custom_backend\u201d."]}, {"name": "torch.distributed.irecv()", "path": "distributed#torch.distributed.irecv", "type": "Distributed Communication", "text": ["Receives a tensor asynchronously.", "Warning", "tag is not supported with the NCCL backend.", "A distributed request object. None, if not part of the group", "Work"]}, {"name": "torch.distributed.is_available()", "path": "distributed#torch.distributed.is_available", "type": "Distributed Communication", "text": ["Returns True if the distributed package is available. Otherwise, torch.distributed does not expose any other APIs. Currently, torch.distributed is available on Linux, MacOS and Windows. Set USE_DISTRIBUTED=1 to enable it when building PyTorch from source. Currently, the default value is USE_DISTRIBUTED=1 for Linux and Windows, USE_DISTRIBUTED=0 for MacOS.", "bool"]}, {"name": "torch.distributed.is_gloo_available()", "path": "distributed#torch.distributed.is_gloo_available", "type": "Distributed Communication", "text": ["Checks if the Gloo backend is available.", "bool"]}, {"name": "torch.distributed.is_initialized()", "path": "distributed#torch.distributed.is_initialized", "type": "Distributed Communication", "text": ["Checking if the default process group has been initialized", "bool"]}, {"name": "torch.distributed.is_mpi_available()", "path": "distributed#torch.distributed.is_mpi_available", "type": "Distributed Communication", "text": ["Checks if the MPI backend is available.", "bool"]}, {"name": "torch.distributed.is_nccl_available()", "path": "distributed#torch.distributed.is_nccl_available", "type": "Distributed Communication", "text": ["Checks if the NCCL backend is available.", "bool"]}, {"name": "torch.distributed.is_torchelastic_launched()", "path": "distributed#torch.distributed.is_torchelastic_launched", "type": "Distributed Communication", "text": ["Checks whether this process was launched with torch.distributed.elastic (aka torchelastic). The existence of TORCHELASTIC_RUN_ID environment variable is used as a proxy to determine whether the current process was launched with torchelastic. This is a reasonable proxy since TORCHELASTIC_RUN_ID maps to the rendezvous id which is always a non-null value indicating the job id for peer discovery purposes..", "bool"]}, {"name": "torch.distributed.isend()", "path": "distributed#torch.distributed.isend", "type": "Distributed Communication", "text": ["Sends a tensor asynchronously.", "Warning", "Modifying tensor before the request completes causes undefined behavior.", "Warning", "tag is not supported with the NCCL backend.", "A distributed request object. None, if not part of the group", "Work"]}, {"name": "torch.distributed.monitored_barrier()", "path": "distributed#torch.distributed.monitored_barrier", "type": "Distributed Communication", "text": ["Synchronizes all processes similar to torch.distributed.barrier, but takes a configurable timeout and is able to report ranks that did not pass this barrier within that timeout. Specifically, for non-zero ranks, will block until a send/recv is processed from rank 0. Rank 0 will block until all send /recv from other ranks are processed, and will report failures for ranks that failed to respond in time. Note that if one rank does not reach the monitored_barrier (for example due to a hang), all other ranks would fail in monitored_barrier.", "This collective will block all processes/ranks in the group, until the whole group exits the function successfully, making it useful for debugging and synchronizing. However, it can have a performance impact and should only be used for debugging or scenarios that require full synchronization points on the host-side. For debugging purposes, this barrier can be inserted before the application\u2019s collective calls to check if any ranks are desynchronized.", "Note", "Note that this collective is only supported with the GLOO backend.", "None."]}, {"name": "torch.distributed.new_group()", "path": "distributed#torch.distributed.new_group", "type": "Distributed Communication", "text": ["Creates a new distributed group.", "This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.", "Warning", "Using multiple process groups with the NCCL backend concurrently is not safe and the user should perform explicit synchronization in their application to ensure only one process group is used at a time. This means collectives from one process group should have completed execution on the device (not just enqueued since CUDA execution is async) before collectives from another process group are enqueued. See Using multiple NCCL communicators concurrently for more details.", "A handle of distributed group that can be given to collective calls or None if the rank is not part of ranks.", "N.B. use_local_synchronization doesn\u2019t work with MPI.", "N.B. While use_local_synchronization=True can be significantly faster with larger clusters and small process groups, care must be taken since it changes cluster behavior as non-member ranks don\u2019t join the group barrier().", "N.B. use_local_synchronization=True can lead to deadlocks when each rank creates multiple overlaping process groups. To avoid that, make sure all ranks follow the same global creation order."]}, {"name": "torch.distributed.nn.api.remote_module.RemoteModule", "path": "rpc#torch.distributed.nn.api.remote_module.RemoteModule", "type": "Distributed RPC", "text": ["A RemoteModule instance can only be created after RPC initialization. It creates a user-specified module on a specified remote node. It behaves like a regular nn.Module except that the forward method is executed on the remote node. It takes care of autograd recording to ensure the backward pass propagates gradients back to the corresponding remote module.", "It generates two methods forward_async and forward based on the signature of the forward method of module_cls. forward_async runs asynchronously and returns a Future. The arguments of forward_async and forward are the same as the forward method of the module returned by the module_cls.", "For example, if module_cls returns an instance of nn.Linear, that has forward method signature: def forward(input: Tensor) -> Tensor:, the generated RemoteModule will have 2 methods with the signatures:", "module_cls (nn.Module) \u2013 ", "Class for the module to be created remotely. For example,", "A remote module instance which wraps the Module created by the user-provided module_cls, it has a blocking forward method and an asynchronous forward_async method that returns a future of the forward call on the user-provided module on the remote side.", "Run the following code in two different processes:", "Furthermore, a more practical example that is combined with DistributedDataParallel (DDP) can be found in this tutorial.", "Returns an RRef (RRef[nn.Module]) pointing to the remote module.", "RRef[Module]", "Returns a list of RRef pointing to the remote module\u2019s parameters. This can typically be used in conjunction with DistributedOptimizer.", "recurse (bool) \u2013 if True, then returns parameters of the remote module and all submodules of the remote module. Otherwise, returns only parameters that are direct members of the remote module.", "A list of RRef (List[RRef[nn.Parameter]]) to remote module\u2019s parameters.", "List[RRef[Parameter]]"]}, {"name": "torch.distributed.nn.api.remote_module.RemoteModule.get_module_rref()", "path": "rpc#torch.distributed.nn.api.remote_module.RemoteModule.get_module_rref", "type": "Distributed RPC", "text": ["Returns an RRef (RRef[nn.Module]) pointing to the remote module.", "RRef[Module]"]}, {"name": "torch.distributed.nn.api.remote_module.RemoteModule.remote_parameters()", "path": "rpc#torch.distributed.nn.api.remote_module.RemoteModule.remote_parameters", "type": "Distributed RPC", "text": ["Returns a list of RRef pointing to the remote module\u2019s parameters. This can typically be used in conjunction with DistributedOptimizer.", "recurse (bool) \u2013 if True, then returns parameters of the remote module and all submodules of the remote module. Otherwise, returns only parameters that are direct members of the remote module.", "A list of RRef (List[RRef[nn.Parameter]]) to remote module\u2019s parameters.", "List[RRef[Parameter]]"]}, {"name": "torch.distributed.optim.DistributedOptimizer", "path": "distributed.optim#torch.distributed.optim.DistributedOptimizer", "type": "Distributed Optimizers", "text": ["DistributedOptimizer takes remote references to parameters scattered across workers and applies the given optimizer locally for each parameter.", "This class uses get_gradients() in order to retrieve the gradients for specific parameters.", "Concurrent calls to step(), either from the same or different clients, will be serialized on each worker \u2013 as each worker\u2019s optimizer can only work on one set of gradients at a time. However, there is no guarantee that the full forward-backward-optimizer sequence will execute for one client at a time. This means that the gradients being applied may not correspond to the latest forward pass executed on a given worker. Also, there is no guaranteed ordering across workers.", "DistributedOptimizer creates the local optimizer with TorchScript enabled by default, so that optimizer updates are not blocked by the Python Global Interpreter Lock (GIL) in the case of multithreaded training (e.g. Distributed Model Parallel). This feature is currently enabled for most optimizers. You can also follow the recipe in PyTorch tutorials to enable TorchScript support for your own custom optimizers.", "Performs a single optimization step.", "This will call torch.optim.Optimizer.step() on each worker containing parameters to be optimized, and will block until all workers return. The provided context_id will be used to retrieve the corresponding context that contains the gradients that should be applied to the parameters.", "context_id \u2013 the autograd context id for which we should run the optimizer step."]}, {"name": "torch.distributed.optim.DistributedOptimizer.step()", "path": "distributed.optim#torch.distributed.optim.DistributedOptimizer.step", "type": "Distributed Optimizers", "text": ["Performs a single optimization step.", "This will call torch.optim.Optimizer.step() on each worker containing parameters to be optimized, and will block until all workers return. The provided context_id will be used to retrieve the corresponding context that contains the gradients that should be applied to the parameters.", "context_id \u2013 the autograd context id for which we should run the optimizer step."]}, {"name": "torch.distributed.optim.PostLocalSGDOptimizer", "path": "distributed.optim#torch.distributed.optim.PostLocalSGDOptimizer", "type": "Distributed Optimizers", "text": ["Wraps an arbitrary torch.optim.Optimizer and runs post-local SGD, This optimizer runs local optimizer at every step. After the warm-up stage, it averages parameters periodically afer the local optimizer is applied.", "Example:", "This is the same as torch.optim.Optimizer load_state_dict(), but also restores model averager\u2019s step value to the one saved in the provided state_dict.", "If there is no \"step\" entry in state_dict, it will raise a warning and initialize the model averager\u2019s step to 0.", "This is the same as torch.optim.Optimizer state_dict(), but adds an extra entry to record model averager\u2019s step to the checkpoint to ensure reload does not cause unnecessary warm up again.", "Performs a single optimization step (parameter update)."]}, {"name": "torch.distributed.optim.PostLocalSGDOptimizer.load_state_dict()", "path": "distributed.optim#torch.distributed.optim.PostLocalSGDOptimizer.load_state_dict", "type": "Distributed Optimizers", "text": ["This is the same as torch.optim.Optimizer load_state_dict(), but also restores model averager\u2019s step value to the one saved in the provided state_dict.", "If there is no \"step\" entry in state_dict, it will raise a warning and initialize the model averager\u2019s step to 0."]}, {"name": "torch.distributed.optim.PostLocalSGDOptimizer.state_dict()", "path": "distributed.optim#torch.distributed.optim.PostLocalSGDOptimizer.state_dict", "type": "Distributed Optimizers", "text": ["This is the same as torch.optim.Optimizer state_dict(), but adds an extra entry to record model averager\u2019s step to the checkpoint to ensure reload does not cause unnecessary warm up again."]}, {"name": "torch.distributed.optim.PostLocalSGDOptimizer.step()", "path": "distributed.optim#torch.distributed.optim.PostLocalSGDOptimizer.step", "type": "Distributed Optimizers", "text": ["Performs a single optimization step (parameter update)."]}, {"name": "torch.distributed.optim.ZeroRedundancyOptimizer", "path": "distributed.optim#torch.distributed.optim.ZeroRedundancyOptimizer", "type": "Distributed Optimizers", "text": ["This class wraps an arbitrary optim.Optimizer and shards its states across ranks in the group as described by ZeRO. The local optimizer instance in each rank is only responsible for updating approximately 1 / world_size parameters and hence only needs to keep 1 / world_size optimizer states. After parameters are updated locally, each rank will broadcast its parameters to all other peers to keep all model replicas in the same state. ZeroRedundancyOptimizer can be used in conjunction with torch.nn.parallel.DistributedDataParallel to reduce per-rank peak memory consumption.", "ZeroRedundancyOptimizer uses a sorted-greedy algorithm to pack a number of parameters at each rank. Each parameter belongs to a single rank and is not divided among ranks. The partition is arbitrary and might not match the the parameter registration or usage order.", "params (Iterable) \u2013 an Iterable of torch.Tensor s or dict s giving all parameters, which will be sharded across ranks.", "Example:", "Warning", "Currently, ZeroRedundancyOptimizer requires that all of the passed-in parameters are the same dense type.", "Warning", "If you pass overlap_with_ddp=True, be wary of the following: Given the way that overlapping DistributedDataParallel with ZeroRedundancyOptimizer is currently implemented, the first two or three training iterations do not perform parameter updates in the optimizer step, depending on if static_graph=False or static_graph=True, respectively. This is because it needs information about the gradient bucketing strategy used by DistributedDataParallel, which is not finalized until the second forward pass if static_graph=False or until the third forward pass if static_graph=True. To adjust for this, one option is to prepend dummy inputs.", "Warning", "ZeroRedundancyOptimizer is experimental and subject to change.", "Add a parameter group to the Optimizer \u2018s param_groups.", "This can be useful when fine tuning a pre-trained network, as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 specifies the parameters to be optimized and group-specific optimization options.", "Warning", "This method handles updating the shards on all partitions but needs to be called on all ranks. Calling this on a subset of the ranks will cause the training to hang because communication primitives are called depending on the managed parameters and expect all the ranks to participate on the same set of parameters.", "Consolidate a list of state_dict s (one per rank) on the target rank.", "to (int) \u2013 the rank that receives the optimizer states (default: 0).", "RuntimeError \u2013 if overlap_with_ddp=True and this method is called before this ZeroRedundancyOptimizer instance has been fully initialized, which happens once DistributedDataParallel gradient buckets have been rebuilt.", "Warning", "This needs to be called on all ranks.", "Returns the ZeRO join hook, which enables training on uneven inputs by shadowing the collective communications in the optimizer step.", "Gradients must be properly set before this hook is called.", "kwargs (dict) \u2013 a dict containing any keyword arguments to modify the behavior of the join hook at run time; all Joinable instances sharing the same join context manager are forwarded the same value for kwargs.", "This hook does not support any keyword arguments; i.e. kwargs is unused.", "Load the state pertaining to the given rank from the input state_dict, updating the local optimizer as needed.", "state_dict (dict) \u2013 optimizer state; should be an object returned from a call to state_dict().", "RuntimeError \u2013 if overlap_with_ddp=True and this method is called before this ZeroRedundancyOptimizer instance has been fully initialized, which happens once DistributedDataParallel gradient buckets have been rebuilt.", "Returns the last global optimizer state known to this rank.", "RuntimeError \u2013 if overlap_with_ddp=True and this method is called before this ZeroRedundancyOptimizer instance has been fully initialized, which happens once DistributedDataParallel gradient buckets have been rebuilt; or if this method is called without a preceding call to consolidate_state_dict().", "Dict[str, Any]", "Performs a single optimizer step and syncs parameters across all ranks.", "closure (Callable) \u2013 a closure that re-evaluates the model and returns the loss; optional for most optimizers.", "Optional loss depending on the underlying local optimizer.", "Optional[float]"]}, {"name": "torch.distributed.optim.ZeroRedundancyOptimizer.add_param_group()", "path": "distributed.optim#torch.distributed.optim.ZeroRedundancyOptimizer.add_param_group", "type": "Distributed Optimizers", "text": ["Add a parameter group to the Optimizer \u2018s param_groups.", "This can be useful when fine tuning a pre-trained network, as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 specifies the parameters to be optimized and group-specific optimization options.", "Warning", "This method handles updating the shards on all partitions but needs to be called on all ranks. Calling this on a subset of the ranks will cause the training to hang because communication primitives are called depending on the managed parameters and expect all the ranks to participate on the same set of parameters."]}, {"name": "torch.distributed.optim.ZeroRedundancyOptimizer.consolidate_state_dict()", "path": "distributed.optim#torch.distributed.optim.ZeroRedundancyOptimizer.consolidate_state_dict", "type": "Distributed Optimizers", "text": ["Consolidate a list of state_dict s (one per rank) on the target rank.", "to (int) \u2013 the rank that receives the optimizer states (default: 0).", "RuntimeError \u2013 if overlap_with_ddp=True and this method is called before this ZeroRedundancyOptimizer instance has been fully initialized, which happens once DistributedDataParallel gradient buckets have been rebuilt.", "Warning", "This needs to be called on all ranks."]}, {"name": "torch.distributed.optim.ZeroRedundancyOptimizer.join_hook()", "path": "distributed.optim#torch.distributed.optim.ZeroRedundancyOptimizer.join_hook", "type": "Distributed Optimizers", "text": ["Returns the ZeRO join hook, which enables training on uneven inputs by shadowing the collective communications in the optimizer step.", "Gradients must be properly set before this hook is called.", "kwargs (dict) \u2013 a dict containing any keyword arguments to modify the behavior of the join hook at run time; all Joinable instances sharing the same join context manager are forwarded the same value for kwargs.", "This hook does not support any keyword arguments; i.e. kwargs is unused."]}, {"name": "torch.distributed.optim.ZeroRedundancyOptimizer.load_state_dict()", "path": "distributed.optim#torch.distributed.optim.ZeroRedundancyOptimizer.load_state_dict", "type": "Distributed Optimizers", "text": ["Load the state pertaining to the given rank from the input state_dict, updating the local optimizer as needed.", "state_dict (dict) \u2013 optimizer state; should be an object returned from a call to state_dict().", "RuntimeError \u2013 if overlap_with_ddp=True and this method is called before this ZeroRedundancyOptimizer instance has been fully initialized, which happens once DistributedDataParallel gradient buckets have been rebuilt."]}, {"name": "torch.distributed.optim.ZeroRedundancyOptimizer.state_dict()", "path": "distributed.optim#torch.distributed.optim.ZeroRedundancyOptimizer.state_dict", "type": "Distributed Optimizers", "text": ["Returns the last global optimizer state known to this rank.", "RuntimeError \u2013 if overlap_with_ddp=True and this method is called before this ZeroRedundancyOptimizer instance has been fully initialized, which happens once DistributedDataParallel gradient buckets have been rebuilt; or if this method is called without a preceding call to consolidate_state_dict().", "Dict[str, Any]"]}, {"name": "torch.distributed.optim.ZeroRedundancyOptimizer.step()", "path": "distributed.optim#torch.distributed.optim.ZeroRedundancyOptimizer.step", "type": "Distributed Optimizers", "text": ["Performs a single optimizer step and syncs parameters across all ranks.", "closure (Callable) \u2013 a closure that re-evaluates the model and returns the loss; optional for most optimizers.", "Optional loss depending on the underlying local optimizer.", "Optional[float]"]}, {"name": "torch.distributed.P2POp", "path": "distributed#torch.distributed.P2POp", "type": "Distributed Communication", "text": ["A class to build point-to-point operations for batch_isend_irecv.", "This class builds the type of P2P operation, communication buffer, peer rank, Process Group, and tag. Instances of this class will be passed to batch_isend_irecv for point-to-point communications."]}, {"name": "torch.distributed.pipeline.sync.Pipe", "path": "pipeline#torch.distributed.pipeline.sync.Pipe", "type": "Miscellaneous", "text": ["Wraps an arbitrary nn.Sequential module to train on using synchronous pipeline parallelism. If the module requires lots of memory and doesn\u2019t fit on a single GPU, pipeline parallelism is a useful technique to employ for training.", "The implementation is based on the torchgpipe paper.", "Pipe combines pipeline parallelism with checkpointing to reduce peak memory required to train while minimizing device under-utilization.", "You should place all the modules on the appropriate devices and wrap them into an nn.Sequential module defining the desired order of execution. If a module does not contain any parameters/buffers, it is assumed this module should be executed on CPU and appropriate input tensors to the module are moved to CPU before execution. This behavior can be overridden by the WithDevice wrapper which can be used to explicitly specify which device a module should run on.", "Pipeline of two FC layers across GPUs 0 and 1.", "Note", "You can wrap a Pipe model with torch.nn.parallel.DistributedDataParallel only when the checkpoint parameter of Pipe is 'never'.", "Note", "Pipe only supports intra-node pipelining currently, but will be expanded to support inter-node pipelining in the future. The forward function returns an RRef to allow for inter-node pipelining in the future, where the output might be on a remote host. For intra-node pipelining you can use local_value() to retrieve the output locally.", "Warning", "Pipe is experimental and subject to change.", "Processes a single input mini-batch through the pipe and returns an RRef pointing to the output. Pipe is a fairly transparent module wrapper. It doesn\u2019t modify the input and output signature of the underlying module. But there\u2019s type restriction. Input and output have to contain at least one tensor. This restriction is applied at partition boundaries too.", "The sequence of inputs are fed into the first stage of the pipeline as *inputs. As a result the positional args for this function should match the positional args for the first stage of the pipeline. The same condition applies for output of one stage of the pipeline which is the input for the next stage.", "The input tensor is split into multiple micro-batches based on the chunks parameter used to initialize Pipe. The batch size is assumed to be the first dimension of the tensor and if the batch size is less than chunks, the number of micro-batches is equal to the batch size.", "Only tensors are split into multiple micro-batches, non-Tensor inputs are just replicated as-is in each micro-batch. For non-Tensor outputs in the last stage of the pipeline, they are aggregated as a List and returned the user. For example, if you have 2 micro-batches returning the integer 5, the user would receive the consolidated output of [5, 5]", "All the input tensors need to be on the same device as the first partition of the pipeline.", "If a tensor is wrapped with the NoChunk wrapper, the tensor is not split across micro-batches and is replicated as-is similar to non-tensors.", "inputs \u2013 input mini-batch", "RRef to the output of the mini-batch", "TypeError \u2013 input doesn\u2019t contain at least one tensor", "RRef"]}, {"name": "torch.distributed.pipeline.sync.Pipe.forward()", "path": "pipeline#torch.distributed.pipeline.sync.Pipe.forward", "type": "Miscellaneous", "text": ["Processes a single input mini-batch through the pipe and returns an RRef pointing to the output. Pipe is a fairly transparent module wrapper. It doesn\u2019t modify the input and output signature of the underlying module. But there\u2019s type restriction. Input and output have to contain at least one tensor. This restriction is applied at partition boundaries too.", "The sequence of inputs are fed into the first stage of the pipeline as *inputs. As a result the positional args for this function should match the positional args for the first stage of the pipeline. The same condition applies for output of one stage of the pipeline which is the input for the next stage.", "The input tensor is split into multiple micro-batches based on the chunks parameter used to initialize Pipe. The batch size is assumed to be the first dimension of the tensor and if the batch size is less than chunks, the number of micro-batches is equal to the batch size.", "Only tensors are split into multiple micro-batches, non-Tensor inputs are just replicated as-is in each micro-batch. For non-Tensor outputs in the last stage of the pipeline, they are aggregated as a List and returned the user. For example, if you have 2 micro-batches returning the integer 5, the user would receive the consolidated output of [5, 5]", "All the input tensors need to be on the same device as the first partition of the pipeline.", "If a tensor is wrapped with the NoChunk wrapper, the tensor is not split across micro-batches and is replicated as-is similar to non-tensors.", "inputs \u2013 input mini-batch", "RRef to the output of the mini-batch", "TypeError \u2013 input doesn\u2019t contain at least one tensor", "RRef"]}, {"name": "torch.distributed.pipeline.sync.skip.skippable.pop", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.pop", "type": "Miscellaneous", "text": ["The command to pop a skip tensor.", "name (str) \u2013 name of skip tensor", "the skip tensor previously stashed by another layer under the same name", "None"]}, {"name": "torch.distributed.pipeline.sync.skip.skippable.skippable()", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.skippable", "type": "Miscellaneous", "text": ["The decorator to define a nn.Module with skip connections. Decorated modules are called \u201cskippable\u201d. This functionality works perfectly fine even when the module is not wrapped by Pipe.", "Each skip tensor is managed by its name. Before manipulating skip tensors, a skippable module must statically declare the names for skip tensors by stash and/or pop parameters. Skip tensors with pre-declared name can be stashed by yield stash(name, tensor) or popped by tensor = yield\npop(name).", "Here is an example with three layers. A skip tensor named \u201c1to3\u201d is stashed and popped at the first and last layer, respectively:", "One skippable module can stash or pop multiple skip tensors:", "Every skip tensor must be associated with exactly one pair of stash and pop. Pipe checks this restriction automatically when wrapping a module. You can also check the restriction by verify_skippables() without Pipe.", "Callable[[Type[Module]], Type[Skippable]]"]}, {"name": "torch.distributed.pipeline.sync.skip.skippable.stash", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.stash", "type": "Miscellaneous", "text": ["The command to stash a skip tensor."]}, {"name": "torch.distributed.pipeline.sync.skip.skippable.verify_skippables()", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.verify_skippables", "type": "Miscellaneous", "text": ["Verifies if the underlying skippable modules satisfy integrity.", "Every skip tensor must have only one pair of stash and pop. If there are one or more unmatched pairs, it will raise TypeError with the detailed messages.", "Here are a few failure cases. verify_skippables() will report failure for these cases:", "To use the same name for multiple skip tensors, they must be isolated by different namespaces. See isolate().", "TypeError \u2013 one or more pairs of stash and pop are not matched."]}, {"name": "torch.distributed.PrefixStore", "path": "distributed#torch.distributed.PrefixStore", "type": "Distributed Communication", "text": ["A wrapper around any of the 3 key-value stores (TCPStore, FileStore, and HashStore) that adds a prefix to each key inserted to the store."]}, {"name": "torch.distributed.recv()", "path": "distributed#torch.distributed.recv", "type": "Distributed Communication", "text": ["Receives a tensor synchronously.", "Sender rank -1, if not part of the group", "int"]}, {"name": "torch.distributed.reduce()", "path": "distributed#torch.distributed.reduce", "type": "Distributed Communication", "text": ["Reduces the tensor data across all machines.", "Only the process with rank dst is going to receive the final result.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.reduce_multigpu()", "path": "distributed#torch.distributed.reduce_multigpu", "type": "Distributed Communication", "text": ["Reduces the tensor data on multiple GPUs across all machines. Each tensor in tensor_list should reside on a separate GPU", "Only the GPU of tensor_list[dst_tensor] on the process with rank dst is going to receive the final result.", "Only nccl backend is currently supported tensors should only be GPU tensors", "Async work handle, if async_op is set to True. None, otherwise"]}, {"name": "torch.distributed.reduce_op", "path": "distributed#torch.distributed.reduce_op", "type": "Distributed Communication", "text": ["Deprecated enum-like class for reduction operations: SUM, PRODUCT, MIN, and MAX.", "ReduceOp is recommended to use instead."]}, {"name": "torch.distributed.reduce_scatter()", "path": "distributed#torch.distributed.reduce_scatter", "type": "Distributed Communication", "text": ["Reduces, then scatters a list of tensors to all processes in a group.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group."]}, {"name": "torch.distributed.reduce_scatter_multigpu()", "path": "distributed#torch.distributed.reduce_scatter_multigpu", "type": "Distributed Communication", "text": ["Reduce and scatter a list of tensors to the whole group. Only nccl backend is currently supported.", "Each tensor in output_tensor_list should reside on a separate GPU, as should each list of tensors in input_tensor_lists.", "output_tensor_list (List[Tensor]) \u2013 ", "Output tensors (on different GPUs) to receive the result of the operation.", "Note that len(output_tensor_list) needs to be the same for all the distributed processes calling this function.", "input_tensor_lists (List[List[Tensor]]) \u2013 ", "Input lists. It should contain correctly-sized tensors on each GPU to be used for input of the collective, e.g. input_tensor_lists[i] contains the reduce_scatter input that resides on the GPU of output_tensor_list[i].", "Note that each element of input_tensor_lists has the size of world_size * len(output_tensor_list), since the function scatters the result from every single GPU in the group. To interpret each element of input_tensor_lists[i], note that output_tensor_list[j] of rank k receives the reduce-scattered result from input_tensor_lists[i][k * world_size + j]", "Also note that len(input_tensor_lists), and the size of each element in input_tensor_lists (each element is a list, therefore len(input_tensor_lists[i])) need to be the same for all the distributed processes calling this function.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group."]}, {"name": "torch.distributed.reduce_scatter_tensor()", "path": "distributed#torch.distributed.reduce_scatter_tensor", "type": "Distributed Communication", "text": ["Reduces, then scatters a tensor to all ranks in a group.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.", "Warning", "The Gloo backend does not support this API."]}, {"name": "torch.distributed.ReduceOp", "path": "distributed#torch.distributed.ReduceOp", "type": "Distributed Communication", "text": ["An enum-like class for available reduction operations: SUM, PRODUCT, MIN, MAX, BAND, BOR, BXOR, and PREMUL_SUM.", "BAND, BOR, and BXOR reductions are not available when using the NCCL backend.", "AVG divides values by the world size before summing across ranks. AVG is only available with the NCCL backend, and only for NCCL versions 2.10 or later.", "PREMUL_SUM multiplies inputs by a given scalar locally before reduction. PREMUL_SUM is only available with the NCCL backend, and only available for NCCL versions 2.11 or later. Users are supposed to use torch.distributed._make_nccl_premul_sum.", "Additionally, MAX, MIN and PRODUCT are not supported for complex tensors.", "The values of this class can be accessed as attributes, e.g., ReduceOp.SUM. They are used in specifying strategies for reduction collectives, e.g., reduce(), all_reduce_multigpu(), etc.", "This class does not support __members__ property."]}, {"name": "torch.distributed.rpc.BackendType", "path": "rpc#torch.distributed.rpc.BackendType", "type": "Distributed RPC", "text": ["An enum class of available backends.", "PyTorch ships with a builtin BackendType.TENSORPIPE backend. Additional ones can be registered using the register_backend() function."]}, {"name": "torch.distributed.rpc.functions.async_execution()", "path": "rpc#torch.distributed.rpc.functions.async_execution", "type": "Distributed RPC", "text": ["A decorator for a function indicating that the return value of the function is guaranteed to be a Future object and this function can run asynchronously on the RPC callee. More specifically, the callee extracts the Future returned by the wrapped function and installs subsequent processing steps as a callback to that Future. The installed callback will read the value from the Future when completed and send the value back as the RPC response. That also means the returned Future only exists on the callee side and is never sent through RPC. This decorator is useful when the wrapped function\u2019s (fn) execution needs to pause and resume due to, e.g., containing rpc_async() or waiting for other signals.", "Note", "To enable asynchronous execution, applications must pass the function object returned by this decorator to RPC APIs. If RPC detected attributes installed by this decorator, it knows that this function returns a Future object and will handle that accordingly. However, this does not mean this decorator has to be outmost one when defining a function. For example, when combined with @staticmethod or @classmethod, @rpc.functions.async_execution needs to be the inner decorator to allow the target function be recognized as a static or class function. This target function can still execute asynchronously because, when accessed, the static or class method preserves attributes installed by @rpc.functions.async_execution.", "The returned Future object can come from rpc_async(), then(), or Future constructor. The example below shows directly using the Future returned by then().", "When combined with TorchScript decorators, this decorator must be the outmost one.", "When combined with static or class method, this decorator must be the inner one.", "This decorator also works with RRef helpers, i.e., . torch.distributed.rpc.RRef.rpc_sync(), torch.distributed.rpc.RRef.rpc_async(), and torch.distributed.rpc.RRef.remote()."]}, {"name": "torch.distributed.rpc.get_worker_info()", "path": "rpc#torch.distributed.rpc.get_worker_info", "type": "Distributed RPC", "text": ["Get WorkerInfo of a given worker name. Use this WorkerInfo to avoid passing an expensive string on every invocation.", "worker_name (str) \u2013 the string name of a worker. If None, return the the id of the current worker. (default None)", "WorkerInfo instance for the given worker_name or WorkerInfo of the current worker if worker_name is None."]}, {"name": "torch.distributed.rpc.init_rpc()", "path": "rpc#torch.distributed.rpc.init_rpc", "type": "Distributed RPC", "text": ["Initializes RPC primitives such as the local RPC agent and distributed autograd, which immediately makes the current process ready to send and receive RPCs."]}, {"name": "torch.distributed.rpc.PyRRef", "path": "rpc#torch.distributed.rpc.PyRRef", "type": "Distributed RPC", "text": ["A class encapsulating a reference to a value of some type on a remote worker. This handle will keep the referenced remote value alive on the worker. A UserRRef will be deleted when 1) no references to it in both the application code and in the local RRef context, or 2) the application has called a graceful shutdown. Invoking methods on a deleted RRef leads to undefined behaviors. RRef implementation only offers best-effort error detection, and applications should not use UserRRefs after rpc.shutdown().", "Warning", "RRefs can only be serialized and deserialized by the RPC module. Serializing and deserializing RRefs without RPC (e.g., Python pickle, torch save() / load(), JIT save() / load(), etc.) will lead to errors.", "Following examples skip RPC initialization and shutdown code for simplicity. Refer to RPC docs for those details.", "Runs the backward pass using the RRef as the root of the backward pass. If dist_autograd_ctx_id is provided, we perform a distributed backward pass using the provided ctx_id starting from the owner of the RRef. In this case, get_gradients() should be used to retrieve the gradients. If dist_autograd_ctx_id is None, it is assumed that this is a local autograd graph and we only perform a local backward pass. In the local case, the node calling this API has to be the owner of the RRef. The value of the RRef is expected to be a scalar Tensor.", "Returns whether this RRef has been confirmed by the owner. OwnerRRef always returns true, while UserRRef only returns true when the owner knowns about this UserRRef.", "Returns whether or not the current node is the owner of this RRef.", "If the current node is the owner, returns a reference to the local value. Otherwise, throws an exception.", "Returns worker information of the node that owns this RRef.", "Returns worker name of the node that owns this RRef.", "Create a helper proxy to easily launch a remote using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.remote().func_name(*args, **kwargs) is the same as the following:", "timeout (float, optional) \u2013 Timeout for rref.remote(). If the creation of this RRef is not successfully completed within the timeout, then the next time there is an attempt to use the RRef (such as to_here), a timeout will be raised. If not provided, the default RPC timeout will be used. Please see rpc.remote() for specific timeout semantics for RRef.", "Create a helper proxy to easily launch an rpc_async using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_async().func_name(*args, **kwargs) is the same as the following:", "timeout (float, optional) \u2013 Timeout for rref.rpc_async(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.", "Create a helper proxy to easily launch an rpc_sync using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_sync().func_name(*args, **kwargs) is the same as the following:", "timeout (float, optional) \u2013 Timeout for rref.rpc_sync(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.", "Blocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value.", "timeout (float, optional) \u2013 Timeout for to_here. If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout (60s) will be used."]}, {"name": "torch.distributed.rpc.PyRRef.backward()", "path": "rpc#torch.distributed.rpc.PyRRef.backward", "type": "Distributed RPC", "text": ["Runs the backward pass using the RRef as the root of the backward pass. If dist_autograd_ctx_id is provided, we perform a distributed backward pass using the provided ctx_id starting from the owner of the RRef. In this case, get_gradients() should be used to retrieve the gradients. If dist_autograd_ctx_id is None, it is assumed that this is a local autograd graph and we only perform a local backward pass. In the local case, the node calling this API has to be the owner of the RRef. The value of the RRef is expected to be a scalar Tensor."]}, {"name": "torch.distributed.rpc.PyRRef.confirmed_by_owner()", "path": "rpc#torch.distributed.rpc.PyRRef.confirmed_by_owner", "type": "Distributed RPC", "text": ["Returns whether this RRef has been confirmed by the owner. OwnerRRef always returns true, while UserRRef only returns true when the owner knowns about this UserRRef."]}, {"name": "torch.distributed.rpc.PyRRef.is_owner()", "path": "rpc#torch.distributed.rpc.PyRRef.is_owner", "type": "Distributed RPC", "text": ["Returns whether or not the current node is the owner of this RRef."]}, {"name": "torch.distributed.rpc.PyRRef.local_value()", "path": "rpc#torch.distributed.rpc.PyRRef.local_value", "type": "Distributed RPC", "text": ["If the current node is the owner, returns a reference to the local value. Otherwise, throws an exception."]}, {"name": "torch.distributed.rpc.PyRRef.owner()", "path": "rpc#torch.distributed.rpc.PyRRef.owner", "type": "Distributed RPC", "text": ["Returns worker information of the node that owns this RRef."]}, {"name": "torch.distributed.rpc.PyRRef.owner_name()", "path": "rpc#torch.distributed.rpc.PyRRef.owner_name", "type": "Distributed RPC", "text": ["Returns worker name of the node that owns this RRef."]}, {"name": "torch.distributed.rpc.PyRRef.remote()", "path": "rpc#torch.distributed.rpc.PyRRef.remote", "type": "Distributed RPC", "text": ["Create a helper proxy to easily launch a remote using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.remote().func_name(*args, **kwargs) is the same as the following:", "timeout (float, optional) \u2013 Timeout for rref.remote(). If the creation of this RRef is not successfully completed within the timeout, then the next time there is an attempt to use the RRef (such as to_here), a timeout will be raised. If not provided, the default RPC timeout will be used. Please see rpc.remote() for specific timeout semantics for RRef."]}, {"name": "torch.distributed.rpc.PyRRef.rpc_async()", "path": "rpc#torch.distributed.rpc.PyRRef.rpc_async", "type": "Distributed RPC", "text": ["Create a helper proxy to easily launch an rpc_async using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_async().func_name(*args, **kwargs) is the same as the following:", "timeout (float, optional) \u2013 Timeout for rref.rpc_async(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used."]}, {"name": "torch.distributed.rpc.PyRRef.rpc_sync()", "path": "rpc#torch.distributed.rpc.PyRRef.rpc_sync", "type": "Distributed RPC", "text": ["Create a helper proxy to easily launch an rpc_sync using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_sync().func_name(*args, **kwargs) is the same as the following:", "timeout (float, optional) \u2013 Timeout for rref.rpc_sync(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used."]}, {"name": "torch.distributed.rpc.PyRRef.to_here()", "path": "rpc#torch.distributed.rpc.PyRRef.to_here", "type": "Distributed RPC", "text": ["Blocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value.", "timeout (float, optional) \u2013 Timeout for to_here. If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout (60s) will be used."]}, {"name": "torch.distributed.rpc.remote()", "path": "rpc#torch.distributed.rpc.remote", "type": "Distributed RPC", "text": ["Make a remote call to run func on worker to and return an RRef to the result value immediately. Worker to will be the owner of the returned RRef, and the worker calling remote is a user. The owner manages the global reference count of its RRef, and the owner RRef is only destructed when globally there are no living references to it.", "A user RRef instance to the result value. Use the blocking API torch.distributed.rpc.RRef.to_here() to retrieve the result value locally.", "Warning", "The remote API does not copy storages of argument tensors until sending them over the wire, which could be done by a different thread depending on the RPC backend type. The caller should make sure that the contents of those tensors stay intact until the returned RRef is confirmed by the owner, which can be checked using the torch.distributed.rpc.RRef.confirmed_by_owner() API.", "Warning", "Errors such as timeouts for the remote API are handled on a best-effort basis. This means that when remote calls initiated by remote fail, such as with a timeout error, we take a best-effort approach to error handling. This means that errors are handled and set on the resulting RRef on an asynchronous basis. If the RRef has not been used by the application before this handling (such as to_here or fork call), then future uses of the RRef will appropriately raise errors. However, it is possible that the user application will use the RRef before the errors are handled. In this case, errors may not be raised as they have not yet been handled.", "Example:"]}, {"name": "torch.distributed.rpc.rpc_async()", "path": "rpc#torch.distributed.rpc.rpc_async", "type": "Distributed RPC", "text": ["Make a non-blocking RPC call to run function func on worker to. RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe. This method will immediately return a Future that can be awaited on.", "Returns a Future object that can be waited on. When completed, the return value of func on args and kwargs can be retrieved from the Future object.", "Warning", "Using GPU tensors as arguments or return values of func is not supported since we don\u2019t support sending GPU tensors over the wire. You need to explicitly copy GPU tensors to CPU before using them as arguments or return values of func.", "Warning", "The rpc_async API does not copy storages of argument tensors until sending them over the wire, which could be done by a different thread depending on the RPC backend type. The caller should make sure that the contents of those tensors stay intact until the returned Future completes.", "Make sure that MASTER_ADDR and MASTER_PORT are set properly on both workers. Refer to init_process_group() API for more details. For example,", "export MASTER_ADDR=localhost export MASTER_PORT=5678", "Then run the following code in two different processes:", "Below is an example of running a TorchScript function using RPC."]}, {"name": "torch.distributed.rpc.rpc_sync()", "path": "rpc#torch.distributed.rpc.rpc_sync", "type": "Distributed RPC", "text": ["Make a blocking RPC call to run function func on worker to. RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe.", "Returns the result of running func with args and kwargs.", "Make sure that MASTER_ADDR and MASTER_PORT are set properly on both workers. Refer to init_process_group() API for more details. For example,", "export MASTER_ADDR=localhost export MASTER_PORT=5678", "Then run the following code in two different processes:", "Below is an example of running a TorchScript function using RPC."]}, {"name": "torch.distributed.rpc.RpcBackendOptions", "path": "rpc#torch.distributed.rpc.RpcBackendOptions", "type": "Distributed RPC", "text": ["An abstract structure encapsulating the options passed into the RPC backend. An instance of this class can be passed in to init_rpc() in order to initialize RPC with specific configurations, such as the RPC timeout and init_method to be used.", "URL specifying how to initialize the process group. Default is env://", "A float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out."]}, {"name": "torch.distributed.rpc.RpcBackendOptions.init_method", "path": "rpc#torch.distributed.rpc.RpcBackendOptions.init_method", "type": "Distributed RPC", "text": ["URL specifying how to initialize the process group. Default is env://"]}, {"name": "torch.distributed.rpc.RpcBackendOptions.rpc_timeout", "path": "rpc#torch.distributed.rpc.RpcBackendOptions.rpc_timeout", "type": "Distributed RPC", "text": ["A float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out."]}, {"name": "torch.distributed.rpc.shutdown()", "path": "rpc#torch.distributed.rpc.shutdown", "type": "Distributed RPC", "text": ["Perform a shutdown of the RPC agent, and then destroy the RPC agent. This stops the local agent from accepting outstanding requests, and shuts down the RPC framework by terminating all RPC threads. If graceful=True, this will block until all local and remote RPC processes reach this method and wait for all outstanding work to complete. Otherwise, if graceful=False, this is a local shutdown, and it does not wait for other RPC processes to reach this method.", "Warning", "For Future objects returned by rpc_async(), future.wait() should not be called after shutdown().", "graceful (bool) \u2013 Whether to do a graceful shutdown or not. If True, this will 1) wait until there is no pending system messages for UserRRefs and delete them; 2) block until all local and remote RPC processes have reached this method and wait for all outstanding work to complete.", "Make sure that MASTER_ADDR and MASTER_PORT are set properly on both workers. Refer to init_process_group() API for more details. For example,", "export MASTER_ADDR=localhost export MASTER_PORT=5678", "Then run the following code in two different processes:"]}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions", "type": "Distributed RPC", "text": ["The backend options for TensorPipeAgent, derived from RpcBackendOptions.", "The device map locations.", "All devices used by the local agent.", "URL specifying how to initialize the process group. Default is env://", "The number of threads in the thread-pool used by TensorPipeAgent to execute requests.", "A float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out.", "Set device mapping between each RPC caller and callee pair. This function can be called multiple times to incrementally add device placement configurations.", "Set local devices used by the TensorPipe RPC agent. When processing CUDA RPC requests, the TensorPipe RPC agent will properly synchronize CUDA streams for all devices in this List.", "devices (List of int, str, or torch.device) \u2013 local devices used by the TensorPipe RPC agent."]}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.device_maps", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.device_maps", "type": "Distributed RPC", "text": ["The device map locations."]}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.devices", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.devices", "type": "Distributed RPC", "text": ["All devices used by the local agent."]}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.init_method", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.init_method", "type": "Distributed RPC", "text": ["URL specifying how to initialize the process group. Default is env://"]}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.num_worker_threads", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.num_worker_threads", "type": "Distributed RPC", "text": ["The number of threads in the thread-pool used by TensorPipeAgent to execute requests."]}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.rpc_timeout", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.rpc_timeout", "type": "Distributed RPC", "text": ["A float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out."]}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.set_device_map()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.set_device_map", "type": "Distributed RPC", "text": ["Set device mapping between each RPC caller and callee pair. This function can be called multiple times to incrementally add device placement configurations."]}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.set_devices()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.set_devices", "type": "Distributed RPC", "text": ["Set local devices used by the TensorPipe RPC agent. When processing CUDA RPC requests, the TensorPipe RPC agent will properly synchronize CUDA streams for all devices in this List.", "devices (List of int, str, or torch.device) \u2013 local devices used by the TensorPipe RPC agent."]}, {"name": "torch.distributed.rpc.WorkerInfo", "path": "rpc#torch.distributed.rpc.WorkerInfo", "type": "Distributed RPC", "text": ["A structure that encapsulates information of a worker in the system. Contains the name and ID of the worker. This class is not meant to be constructed directly, rather, an instance can be retrieved through get_worker_info() and the result can be passed in to functions such as rpc_sync(), rpc_async(), remote() to avoid copying a string on every invocation.", "Globally unique id to identify the worker.", "The name of the worker."]}, {"name": "torch.distributed.rpc.WorkerInfo.id", "path": "rpc#torch.distributed.rpc.WorkerInfo.id", "type": "Distributed RPC", "text": ["Globally unique id to identify the worker."]}, {"name": "torch.distributed.rpc.WorkerInfo.name", "path": "rpc#torch.distributed.rpc.WorkerInfo.name", "type": "Distributed RPC", "text": ["The name of the worker."]}, {"name": "torch.distributed.scatter()", "path": "distributed#torch.distributed.scatter", "type": "Distributed Communication", "text": ["Scatters a list of tensors to all processes in a group.", "Each process will receive exactly one tensor and store its data in the tensor argument.", "Complex tensors are supported.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group", "Note", "Note that all Tensors in scatter_list must have the same size."]}, {"name": "torch.distributed.scatter_object_list()", "path": "distributed#torch.distributed.scatter_object_list", "type": "Distributed Communication", "text": ["Scatters picklable objects in scatter_object_input_list to the whole group. Similar to scatter(), but Python objects can be passed in. On each rank, the scattered object will be stored as the first element of scatter_object_output_list. Note that all objects in scatter_object_input_list must be picklable in order to be scattered.", "None. If rank is part of the group, scatter_object_output_list will have its first element set to the scattered object for this rank.", "Note", "Note that this API differs slightly from the scatter collective since it does not provide an async_op handle and thus will be a blocking call.", "Warning", "scatter_object_list() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.", "Warning", "Calling scatter_object_list() with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using scatter() instead."]}, {"name": "torch.distributed.send()", "path": "distributed#torch.distributed.send", "type": "Distributed Communication", "text": ["Sends a tensor synchronously."]}, {"name": "torch.distributed.Store", "path": "distributed#torch.distributed.Store", "type": "Distributed Communication", "text": ["Base class for all store implementations, such as the 3 provided by PyTorch distributed: (TCPStore, FileStore, and HashStore)."]}, {"name": "torch.distributed.Store.add()", "path": "distributed#torch.distributed.Store.add", "type": "Distributed Communication", "text": ["The first call to add for a given key creates a counter associated with key in the store, initialized to amount. Subsequent calls to add with the same key increment the counter by the specified amount. Calling add() with a key that has already been set in the store by set() will result in an exception."]}, {"name": "torch.distributed.Store.compare_set()", "path": "distributed#torch.distributed.Store.compare_set", "type": "Distributed Communication", "text": ["Inserts the key-value pair into the store based on the supplied key and performs comparison between expected_value and desired_value before inserting. desired_value will only be set if expected_value for the key already exists in the store or if expected_value is an empty string."]}, {"name": "torch.distributed.Store.delete_key()", "path": "distributed#torch.distributed.Store.delete_key", "type": "Distributed Communication", "text": ["Deletes the key-value pair associated with key from the store. Returns true if the key was successfully deleted, and false if it was not.", "Warning", "The delete_key API is only supported by the TCPStore and HashStore. Using this API with the FileStore will result in an exception.", "key (str) \u2013 The key to be deleted from the store", "True if key was deleted, otherwise False."]}, {"name": "torch.distributed.Store.get()", "path": "distributed#torch.distributed.Store.get", "type": "Distributed Communication", "text": ["Retrieves the value associated with the given key in the store. If key is not present in the store, the function will wait for timeout, which is defined when initializing the store, before throwing an exception.", "key (str) \u2013 The function will return the value associated with this key.", "Value associated with key if key is in the store."]}, {"name": "torch.distributed.Store.num_keys()", "path": "distributed#torch.distributed.Store.num_keys", "type": "Distributed Communication", "text": ["Returns the number of keys set in the store. Note that this number will typically be one greater than the number of keys added by set() and add() since one key is used to coordinate all the workers using the store.", "Warning", "When used with the TCPStore, num_keys returns the number of keys written to the underlying file. If the store is destructed and another store is created with the same file, the original keys will be retained.", "The number of keys present in the store."]}, {"name": "torch.distributed.Store.set()", "path": "distributed#torch.distributed.Store.set", "type": "Distributed Communication", "text": ["Inserts the key-value pair into the store based on the supplied key and value. If key already exists in the store, it will overwrite the old value with the new supplied value."]}, {"name": "torch.distributed.Store.set_timeout()", "path": "distributed#torch.distributed.Store.set_timeout", "type": "Distributed Communication", "text": ["Sets the store\u2019s default timeout. This timeout is used during initialization and in wait() and get().", "timeout (timedelta) \u2013 timeout to be set in the store."]}, {"name": "torch.distributed.Store.wait()", "path": "distributed#torch.distributed.Store.wait", "type": "Distributed Communication", "text": ["Overloaded function.", "Waits for each key in keys to be added to the store. If not all keys are set before the timeout (set during store initialization), then wait will throw an exception.", "keys (list) \u2013 List of keys on which to wait until they are set in the store.", "Waits for each key in keys to be added to the store, and throws an exception if the keys have not been set by the supplied timeout."]}, {"name": "torch.distributed.TCPStore", "path": "distributed#torch.distributed.TCPStore", "type": "Distributed Communication", "text": ["A TCP-based distributed key-value store implementation. The server store holds the data, while the client stores can connect to the server store over TCP and perform actions such as set() to insert a key-value pair, get() to retrieve a key-value pair, etc. There should always be one server store initialized because the client store(s) will wait for the server to establish a connection."]}, {"name": "torch.distributed.tensor.parallel.ddp.pre_dp_module_transform()", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.ddp.pre_dp_module_transform", "type": "Tensor Parallelism", "text": ["Enable the composability between Tensor Parallelism (TP) and Data Parallelism(DP) in PyTorch when using DDP. We need to convert Parameters which are DTensors to local tensors before wrapping with data parallelism API. We then register two hooks, one for converting local tensors back to DTensor preforward and one to convert DTensors back to tensors after Forward. By integrating this way, we avoid any special handling of DTensor parameters by DDP and get DTensor\u2019s gradients propagated back to DP, e.g. gradient buckets of DDP.", "For now, this API only works with DistributedDataParallel. It will later support other DP methods such as FSDP.", "module (nn.Module) \u2013 Module which has been applied TP on."]}, {"name": "torch.distributed.tensor.parallel.fsdp.enable_2d_with_fsdp()", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.fsdp.enable_2d_with_fsdp", "type": "Tensor Parallelism", "text": ["The API registers the extension which is needed for Tensor Parallelism (TP) to work with FullyShardedDataParallel (FSDP). We first parallelize parameters within one module or sub_modules based on a parallelize_plan and will let FSDP reshard the local tensor of distributed parameter which is essentially a DTensor.", "A bool indicated whether extension registration succeeds or not.", "bool"]}, {"name": "torch.distributed.tensor.parallel.parallelize_module()", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.parallelize_module", "type": "Tensor Parallelism", "text": ["The API to apply Tensor Parallelism (TP) in PyTorch. We parallelize module or sub_modules based on a parallelize_plan. The parallelize_plan contains ParallelStyle, which indicates how user wants the module or sub_module to be parallelized.", "User can also specify different parallel style per module fully qualified name (FQN). The API supports 2D parallelism natively by accepting an n-dimension device_mesh and users just need to specify the dimension where we perform tensor parallelism on.", "A nn.Module object parallelized.", "Warning", "PairwiseParallel comes with constraints for now. If you need finer granularity, you need to pass in a dict of module FQN and parallel style instead."]}, {"name": "torch.distributed.tensor.parallel.style.ColwiseParallel", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.ColwiseParallel", "type": "Tensor Parallelism", "text": ["Partitioning the column of a tensor or module. We assume the input to be a replicated DTensor and output to be a sharded torch.Tensor."]}, {"name": "torch.distributed.tensor.parallel.style.make_input_replicate_1d()", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_input_replicate_1d", "type": "Tensor Parallelism", "text": ["Replicate input tensor over an 1-D device mesh. This function will be used in ParallelStyle.", "A DTensor replicated over device_mesh.", "DTensor"]}, {"name": "torch.distributed.tensor.parallel.style.make_input_reshard_replicate()", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_input_reshard_replicate", "type": "Tensor Parallelism", "text": ["To construct a Sharded DTensor from a tensor on different ranks and then convert to a replicate DTensor.", "and then converted to replicate.", "DTensor"]}, {"name": "torch.distributed.tensor.parallel.style.make_input_shard_1d()", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_input_shard_1d", "type": "Tensor Parallelism", "text": ["Shard input tensor on dim over an 1-D device mesh. This function will be used in ParallelStyle.", "A DTensor sharded on dimension dim over device_mesh.", "DTensor"]}, {"name": "torch.distributed.tensor.parallel.style.make_input_shard_1d_last_dim()", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_input_shard_1d_last_dim", "type": "Tensor Parallelism", "text": ["Wrapper func of make_input_shard_1d with dim = -1.", "A DTensor sharded on the last dimension over device_mesh.", "DTensor"]}, {"name": "torch.distributed.tensor.parallel.style.make_output_replicate_1d()", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_output_replicate_1d", "type": "Tensor Parallelism", "text": ["Convert Output DTensor to a replicated DTensor. This will be used in ParallelStyle.", "A DTensor object made replicate.", "DTensor"]}, {"name": "torch.distributed.tensor.parallel.style.make_output_reshard_tensor()", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_output_reshard_tensor", "type": "Tensor Parallelism", "text": ["Convert Output DTensor to a sharded DTensor and return the local tensor.", "A torch.Tensor object converted from output DTensor.", "Tensor"]}, {"name": "torch.distributed.tensor.parallel.style.make_output_shard_1d()", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_output_shard_1d", "type": "Tensor Parallelism", "text": ["Convert Output DTensor to a sharded DTensor. This will be used in ParallelStyle.", "A DTensor object sharded on the given dim.", "DTensor"]}, {"name": "torch.distributed.tensor.parallel.style.make_output_tensor()", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_output_tensor", "type": "Tensor Parallelism", "text": ["Convert Output DTensor to a replicated DTensor first and then convert it to Tensor.", "A torch.Tensor object converted from output DTensor.", "Tensor"]}, {"name": "torch.distributed.tensor.parallel.style.PairwiseParallel", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.PairwiseParallel", "type": "Tensor Parallelism", "text": ["PairwiseParallel concatenate colwise and rowwise styles as a fixed pair like what Megatron-LM(https://arxiv.org/abs/1909.08053) is doing. We assume both input and output need to be replicate DTensors.", "Warning", "PairwiseParallel does not support nn.MultiheadAttention, nn.Transformer well at this moment. One workaround is to apply ColwiseParallel and RowwiseParallel to the components of transformer. We recommend to use PairwiseParallel only for even-number-layer MLP for now."]}, {"name": "torch.distributed.tensor.parallel.style.RowwiseParallel", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.RowwiseParallel", "type": "Tensor Parallelism", "text": ["Partitioning the row of a module. We assume the input to be a sharded DTensor and output to be a torch.Tensor."]}, {"name": "torch.distributed.tensor.parallel.style.SequenceParallel", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.SequenceParallel", "type": "Tensor Parallelism", "text": ["SequenceParallel concatenate colwise and rowwise styles as a fixed pair together with sequence parallel like what Megatron-LM Sequence parallel (https://arxiv.org/pdf/2205.05198.pdf) is doing. We assume both input and output need to be sharded DTensors.", "Warning", "SequenceParallel does not support nn.MultiheadAttention, nn.Transformer well at this moment. One workaround is to apply ColwiseParallel and RowwiseParallel to the components of transformer. We recommend to use SequenceParallel only for even-number-layer MLP for now."]}, {"name": "torch.distributions.bernoulli.Bernoulli", "path": "distributions#torch.distributions.bernoulli.Bernoulli", "type": "Probability Distributions", "text": ["Bases: ExponentialFamily", "Creates a Bernoulli distribution parameterized by probs or logits (but not both).", "Samples are binary (0 or 1). They take the value 1 with probability p and 0 with probability 1 - p.", "Example:"]}, {"name": "torch.distributions.bernoulli.Bernoulli.arg_constraints", "path": "distributions#torch.distributions.bernoulli.Bernoulli.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.entropy()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.enumerate_support()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.enumerate_support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.expand()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.has_enumerate_support", "path": "distributions#torch.distributions.bernoulli.Bernoulli.has_enumerate_support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.log_prob()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.logits", "path": "distributions#torch.distributions.bernoulli.Bernoulli.logits", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.mean", "path": "distributions#torch.distributions.bernoulli.Bernoulli.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.mode", "path": "distributions#torch.distributions.bernoulli.Bernoulli.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.param_shape", "path": "distributions#torch.distributions.bernoulli.Bernoulli.param_shape", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.probs", "path": "distributions#torch.distributions.bernoulli.Bernoulli.probs", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.sample()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.support", "path": "distributions#torch.distributions.bernoulli.Bernoulli.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.variance", "path": "distributions#torch.distributions.bernoulli.Bernoulli.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta", "path": "distributions#torch.distributions.beta.Beta", "type": "Probability Distributions", "text": ["Bases: ExponentialFamily", "Beta distribution parameterized by concentration1 and concentration0.", "Example:"]}, {"name": "torch.distributions.beta.Beta.arg_constraints", "path": "distributions#torch.distributions.beta.Beta.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta.concentration0", "path": "distributions#torch.distributions.beta.Beta.concentration0", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta.concentration1", "path": "distributions#torch.distributions.beta.Beta.concentration1", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta.entropy()", "path": "distributions#torch.distributions.beta.Beta.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta.expand()", "path": "distributions#torch.distributions.beta.Beta.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta.has_rsample", "path": "distributions#torch.distributions.beta.Beta.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta.log_prob()", "path": "distributions#torch.distributions.beta.Beta.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta.mean", "path": "distributions#torch.distributions.beta.Beta.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta.mode", "path": "distributions#torch.distributions.beta.Beta.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta.rsample()", "path": "distributions#torch.distributions.beta.Beta.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta.support", "path": "distributions#torch.distributions.beta.Beta.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta.variance", "path": "distributions#torch.distributions.beta.Beta.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial", "path": "distributions#torch.distributions.binomial.Binomial", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a Binomial distribution parameterized by total_count and either probs or logits (but not both). total_count must be broadcastable with probs/logits.", "Example:"]}, {"name": "torch.distributions.binomial.Binomial.arg_constraints", "path": "distributions#torch.distributions.binomial.Binomial.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.entropy()", "path": "distributions#torch.distributions.binomial.Binomial.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.enumerate_support()", "path": "distributions#torch.distributions.binomial.Binomial.enumerate_support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.expand()", "path": "distributions#torch.distributions.binomial.Binomial.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.has_enumerate_support", "path": "distributions#torch.distributions.binomial.Binomial.has_enumerate_support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.log_prob()", "path": "distributions#torch.distributions.binomial.Binomial.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.logits", "path": "distributions#torch.distributions.binomial.Binomial.logits", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.mean", "path": "distributions#torch.distributions.binomial.Binomial.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.mode", "path": "distributions#torch.distributions.binomial.Binomial.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.param_shape", "path": "distributions#torch.distributions.binomial.Binomial.param_shape", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.probs", "path": "distributions#torch.distributions.binomial.Binomial.probs", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.sample()", "path": "distributions#torch.distributions.binomial.Binomial.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.support", "path": "distributions#torch.distributions.binomial.Binomial.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.variance", "path": "distributions#torch.distributions.binomial.Binomial.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical", "path": "distributions#torch.distributions.categorical.Categorical", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a categorical distribution parameterized by either probs or logits (but not both).", "Note", "It is equivalent to the distribution that torch.multinomial() samples from.", "Samples are integers from {0,\u2026,K\u22121}\\{0, \\ldots, K-1\\} where K is probs.size(-1).", "If probs is 1-dimensional with length-K, each element is the relative probability of sampling the class at that index.", "If probs is N-dimensional, the first N-1 dimensions are treated as a batch of relative probability vectors.", "Note", "The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. logits will return this normalized value.", "See also: torch.multinomial()", "Example:"]}, {"name": "torch.distributions.categorical.Categorical.arg_constraints", "path": "distributions#torch.distributions.categorical.Categorical.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.entropy()", "path": "distributions#torch.distributions.categorical.Categorical.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.enumerate_support()", "path": "distributions#torch.distributions.categorical.Categorical.enumerate_support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.expand()", "path": "distributions#torch.distributions.categorical.Categorical.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.has_enumerate_support", "path": "distributions#torch.distributions.categorical.Categorical.has_enumerate_support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.log_prob()", "path": "distributions#torch.distributions.categorical.Categorical.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.logits", "path": "distributions#torch.distributions.categorical.Categorical.logits", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.mean", "path": "distributions#torch.distributions.categorical.Categorical.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.mode", "path": "distributions#torch.distributions.categorical.Categorical.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.param_shape", "path": "distributions#torch.distributions.categorical.Categorical.param_shape", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.probs", "path": "distributions#torch.distributions.categorical.Categorical.probs", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.sample()", "path": "distributions#torch.distributions.categorical.Categorical.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.support", "path": "distributions#torch.distributions.categorical.Categorical.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.variance", "path": "distributions#torch.distributions.categorical.Categorical.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy", "path": "distributions#torch.distributions.cauchy.Cauchy", "type": "Probability Distributions", "text": ["Bases: Distribution", "Samples from a Cauchy (Lorentz) distribution. The distribution of the ratio of independent normally distributed random variables with means 0 follows a Cauchy distribution.", "Example:"]}, {"name": "torch.distributions.cauchy.Cauchy.arg_constraints", "path": "distributions#torch.distributions.cauchy.Cauchy.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.cdf()", "path": "distributions#torch.distributions.cauchy.Cauchy.cdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.entropy()", "path": "distributions#torch.distributions.cauchy.Cauchy.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.expand()", "path": "distributions#torch.distributions.cauchy.Cauchy.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.has_rsample", "path": "distributions#torch.distributions.cauchy.Cauchy.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.icdf()", "path": "distributions#torch.distributions.cauchy.Cauchy.icdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.log_prob()", "path": "distributions#torch.distributions.cauchy.Cauchy.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.mean", "path": "distributions#torch.distributions.cauchy.Cauchy.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.mode", "path": "distributions#torch.distributions.cauchy.Cauchy.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.rsample()", "path": "distributions#torch.distributions.cauchy.Cauchy.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.support", "path": "distributions#torch.distributions.cauchy.Cauchy.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.variance", "path": "distributions#torch.distributions.cauchy.Cauchy.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.chi2.Chi2", "path": "distributions#torch.distributions.chi2.Chi2", "type": "Probability Distributions", "text": ["Bases: Gamma", "Creates a Chi-squared distribution parameterized by shape parameter df. This is exactly equivalent to Gamma(alpha=0.5*df, beta=0.5)", "Example:", "df (float or Tensor) \u2013 shape parameter of the distribution"]}, {"name": "torch.distributions.chi2.Chi2.arg_constraints", "path": "distributions#torch.distributions.chi2.Chi2.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.chi2.Chi2.df", "path": "distributions#torch.distributions.chi2.Chi2.df", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.chi2.Chi2.expand()", "path": "distributions#torch.distributions.chi2.Chi2.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.constraint_registry.ConstraintRegistry", "path": "distributions#torch.distributions.constraint_registry.ConstraintRegistry", "type": "Probability Distributions", "text": ["Registry to link constraints to transforms.", "Registers a Constraint subclass in this registry. Usage:"]}, {"name": "torch.distributions.constraint_registry.ConstraintRegistry.register()", "path": "distributions#torch.distributions.constraint_registry.ConstraintRegistry.register", "type": "Probability Distributions", "text": ["Registers a Constraint subclass in this registry. Usage:"]}, {"name": "torch.distributions.constraints.cat", "path": "distributions#torch.distributions.constraints.cat", "type": "Probability Distributions", "text": ["alias of _Cat"]}, {"name": "torch.distributions.constraints.Constraint", "path": "distributions#torch.distributions.constraints.Constraint", "type": "Probability Distributions", "text": ["Abstract base class for constraints.", "A constraint object represents a region over which a variable is valid, e.g. within which a variable can be optimized.", "Returns a byte tensor of sample_shape + batch_shape indicating whether each event in value satisfies this constraint."]}, {"name": "torch.distributions.constraints.Constraint.check()", "path": "distributions#torch.distributions.constraints.Constraint.check", "type": "Probability Distributions", "text": ["Returns a byte tensor of sample_shape + batch_shape indicating whether each event in value satisfies this constraint."]}, {"name": "torch.distributions.constraints.dependent_property", "path": "distributions#torch.distributions.constraints.dependent_property", "type": "Probability Distributions", "text": ["alias of _DependentProperty"]}, {"name": "torch.distributions.constraints.greater_than", "path": "distributions#torch.distributions.constraints.greater_than", "type": "Probability Distributions", "text": ["alias of _GreaterThan"]}, {"name": "torch.distributions.constraints.greater_than_eq", "path": "distributions#torch.distributions.constraints.greater_than_eq", "type": "Probability Distributions", "text": ["alias of _GreaterThanEq"]}, {"name": "torch.distributions.constraints.half_open_interval", "path": "distributions#torch.distributions.constraints.half_open_interval", "type": "Probability Distributions", "text": ["alias of _HalfOpenInterval"]}, {"name": "torch.distributions.constraints.independent", "path": "distributions#torch.distributions.constraints.independent", "type": "Probability Distributions", "text": ["alias of _IndependentConstraint"]}, {"name": "torch.distributions.constraints.integer_interval", "path": "distributions#torch.distributions.constraints.integer_interval", "type": "Probability Distributions", "text": ["alias of _IntegerInterval"]}, {"name": "torch.distributions.constraints.interval", "path": "distributions#torch.distributions.constraints.interval", "type": "Probability Distributions", "text": ["alias of _Interval"]}, {"name": "torch.distributions.constraints.less_than", "path": "distributions#torch.distributions.constraints.less_than", "type": "Probability Distributions", "text": ["alias of _LessThan"]}, {"name": "torch.distributions.constraints.multinomial", "path": "distributions#torch.distributions.constraints.multinomial", "type": "Probability Distributions", "text": ["alias of _Multinomial"]}, {"name": "torch.distributions.constraints.stack", "path": "distributions#torch.distributions.constraints.stack", "type": "Probability Distributions", "text": ["alias of _Stack"]}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli", "type": "Probability Distributions", "text": ["Bases: ExponentialFamily", "Creates a continuous Bernoulli distribution parameterized by probs or logits (but not both).", "The distribution is supported in [0, 1] and parameterized by \u2018probs\u2019 (in (0,1)) or \u2018logits\u2019 (real-valued). Note that, unlike the Bernoulli, \u2018probs\u2019 does not correspond to a probability and \u2018logits\u2019 does not correspond to log-odds, but the same names are used due to the similarity with the Bernoulli. See [1] for more details.", "Example:", "[1] The continuous Bernoulli: fixing a pervasive error in variational autoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019. https://arxiv.org/abs/1907.06845"]}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.arg_constraints", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.has_rsample", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.support", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet", "path": "distributions#torch.distributions.dirichlet.Dirichlet", "type": "Probability Distributions", "text": ["Bases: ExponentialFamily", "Creates a Dirichlet distribution parameterized by concentration concentration.", "Example:", "concentration (Tensor) \u2013 concentration parameter of the distribution (often referred to as alpha)"]}, {"name": "torch.distributions.dirichlet.Dirichlet.arg_constraints", "path": "distributions#torch.distributions.dirichlet.Dirichlet.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.entropy()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.expand()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.has_rsample", "path": "distributions#torch.distributions.dirichlet.Dirichlet.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.log_prob()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.mean", "path": "distributions#torch.distributions.dirichlet.Dirichlet.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.mode", "path": "distributions#torch.distributions.dirichlet.Dirichlet.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.rsample()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.support", "path": "distributions#torch.distributions.dirichlet.Dirichlet.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.variance", "path": "distributions#torch.distributions.dirichlet.Dirichlet.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.distribution.Distribution", "path": "distributions#torch.distributions.distribution.Distribution", "type": "Probability Distributions", "text": ["Bases: object", "Distribution is the abstract base class for probability distributions.", "Returns a dictionary from argument names to Constraint objects that should be satisfied by each argument of this distribution. Args that are not tensors need not appear in this dict.", "Returns the shape over which parameters are batched.", "Returns the cumulative density/mass function evaluated at value.", "value (Tensor) \u2013 ", "Tensor", "Returns entropy of distribution, batched over batch_shape.", "Tensor of shape batch_shape.", "Tensor", "Returns tensor containing all values supported by a discrete distribution. The result will enumerate over dimension 0, so the shape of the result will be (cardinality,) + batch_shape + event_shape (where event_shape = () for univariate distributions).", "Note that this enumerates over all batched tensors in lock-step [[0, 0], [1, 1], \u2026]. With expand=False, enumeration happens along dim 0, but with the remaining batch dimensions being singleton dimensions, [[0], [1], ...", "To iterate over the full Cartesian product use itertools.product(m.enumerate_support()).", "expand (bool) \u2013 whether to expand the support over the batch dims to match the distribution\u2019s batch_shape.", "Tensor iterating over dimension 0.", "Tensor", "Returns the shape of a single sample (without batching).", "Returns a new distribution instance (or populates an existing instance provided by a derived class) with batch dimensions expanded to batch_shape. This method calls expand on the distribution\u2019s parameters. As such, this does not allocate new memory for the expanded distribution instance. Additionally, this does not repeat any args checking or parameter broadcasting in __init__.py, when an instance is first created.", "New distribution instance with batch dimensions expanded to batch_size.", "Returns the inverse cumulative density/mass function evaluated at value.", "value (Tensor) \u2013 ", "Tensor", "Returns the log of the probability density/mass function evaluated at value.", "value (Tensor) \u2013 ", "Tensor", "Returns the mean of the distribution.", "Returns the mode of the distribution.", "Returns perplexity of distribution, batched over batch_shape.", "Tensor of shape batch_shape.", "Tensor", "Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched.", "Tensor", "Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.", "Tensor", "Generates n samples or n batches of samples if the distribution parameters are batched.", "Tensor", "Sets whether validation is enabled or disabled.", "The default behavior mimics Python\u2019s assert statement: validation is on by default, but is disabled if Python is run in optimized mode (via python -O). Validation may be expensive, so you may want to disable it once a model is working.", "value (bool) \u2013 Whether to enable validation.", "Returns the standard deviation of the distribution.", "Returns a Constraint object representing this distribution\u2019s support.", "Returns the variance of the distribution."]}, {"name": "torch.distributions.distribution.Distribution.arg_constraints", "path": "distributions#torch.distributions.distribution.Distribution.arg_constraints", "type": "Probability Distributions", "text": ["Returns a dictionary from argument names to Constraint objects that should be satisfied by each argument of this distribution. Args that are not tensors need not appear in this dict."]}, {"name": "torch.distributions.distribution.Distribution.batch_shape", "path": "distributions#torch.distributions.distribution.Distribution.batch_shape", "type": "Probability Distributions", "text": ["Returns the shape over which parameters are batched."]}, {"name": "torch.distributions.distribution.Distribution.cdf()", "path": "distributions#torch.distributions.distribution.Distribution.cdf", "type": "Probability Distributions", "text": ["Returns the cumulative density/mass function evaluated at value.", "value (Tensor) \u2013 ", "Tensor"]}, {"name": "torch.distributions.distribution.Distribution.entropy()", "path": "distributions#torch.distributions.distribution.Distribution.entropy", "type": "Probability Distributions", "text": ["Returns entropy of distribution, batched over batch_shape.", "Tensor of shape batch_shape.", "Tensor"]}, {"name": "torch.distributions.distribution.Distribution.enumerate_support()", "path": "distributions#torch.distributions.distribution.Distribution.enumerate_support", "type": "Probability Distributions", "text": ["Returns tensor containing all values supported by a discrete distribution. The result will enumerate over dimension 0, so the shape of the result will be (cardinality,) + batch_shape + event_shape (where event_shape = () for univariate distributions).", "Note that this enumerates over all batched tensors in lock-step [[0, 0], [1, 1], \u2026]. With expand=False, enumeration happens along dim 0, but with the remaining batch dimensions being singleton dimensions, [[0], [1], ...", "To iterate over the full Cartesian product use itertools.product(m.enumerate_support()).", "expand (bool) \u2013 whether to expand the support over the batch dims to match the distribution\u2019s batch_shape.", "Tensor iterating over dimension 0.", "Tensor"]}, {"name": "torch.distributions.distribution.Distribution.event_shape", "path": "distributions#torch.distributions.distribution.Distribution.event_shape", "type": "Probability Distributions", "text": ["Returns the shape of a single sample (without batching)."]}, {"name": "torch.distributions.distribution.Distribution.expand()", "path": "distributions#torch.distributions.distribution.Distribution.expand", "type": "Probability Distributions", "text": ["Returns a new distribution instance (or populates an existing instance provided by a derived class) with batch dimensions expanded to batch_shape. This method calls expand on the distribution\u2019s parameters. As such, this does not allocate new memory for the expanded distribution instance. Additionally, this does not repeat any args checking or parameter broadcasting in __init__.py, when an instance is first created.", "New distribution instance with batch dimensions expanded to batch_size."]}, {"name": "torch.distributions.distribution.Distribution.icdf()", "path": "distributions#torch.distributions.distribution.Distribution.icdf", "type": "Probability Distributions", "text": ["Returns the inverse cumulative density/mass function evaluated at value.", "value (Tensor) \u2013 ", "Tensor"]}, {"name": "torch.distributions.distribution.Distribution.log_prob()", "path": "distributions#torch.distributions.distribution.Distribution.log_prob", "type": "Probability Distributions", "text": ["Returns the log of the probability density/mass function evaluated at value.", "value (Tensor) \u2013 ", "Tensor"]}, {"name": "torch.distributions.distribution.Distribution.mean", "path": "distributions#torch.distributions.distribution.Distribution.mean", "type": "Probability Distributions", "text": ["Returns the mean of the distribution."]}, {"name": "torch.distributions.distribution.Distribution.mode", "path": "distributions#torch.distributions.distribution.Distribution.mode", "type": "Probability Distributions", "text": ["Returns the mode of the distribution."]}, {"name": "torch.distributions.distribution.Distribution.perplexity()", "path": "distributions#torch.distributions.distribution.Distribution.perplexity", "type": "Probability Distributions", "text": ["Returns perplexity of distribution, batched over batch_shape.", "Tensor of shape batch_shape.", "Tensor"]}, {"name": "torch.distributions.distribution.Distribution.rsample()", "path": "distributions#torch.distributions.distribution.Distribution.rsample", "type": "Probability Distributions", "text": ["Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched.", "Tensor"]}, {"name": "torch.distributions.distribution.Distribution.sample()", "path": "distributions#torch.distributions.distribution.Distribution.sample", "type": "Probability Distributions", "text": ["Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.", "Tensor"]}, {"name": "torch.distributions.distribution.Distribution.sample_n()", "path": "distributions#torch.distributions.distribution.Distribution.sample_n", "type": "Probability Distributions", "text": ["Generates n samples or n batches of samples if the distribution parameters are batched.", "Tensor"]}, {"name": "torch.distributions.distribution.Distribution.set_default_validate_args()", "path": "distributions#torch.distributions.distribution.Distribution.set_default_validate_args", "type": "Probability Distributions", "text": ["Sets whether validation is enabled or disabled.", "The default behavior mimics Python\u2019s assert statement: validation is on by default, but is disabled if Python is run in optimized mode (via python -O). Validation may be expensive, so you may want to disable it once a model is working.", "value (bool) \u2013 Whether to enable validation."]}, {"name": "torch.distributions.distribution.Distribution.stddev", "path": "distributions#torch.distributions.distribution.Distribution.stddev", "type": "Probability Distributions", "text": ["Returns the standard deviation of the distribution."]}, {"name": "torch.distributions.distribution.Distribution.support", "path": "distributions#torch.distributions.distribution.Distribution.support", "type": "Probability Distributions", "text": ["Returns a Constraint object representing this distribution\u2019s support."]}, {"name": "torch.distributions.distribution.Distribution.variance", "path": "distributions#torch.distributions.distribution.Distribution.variance", "type": "Probability Distributions", "text": ["Returns the variance of the distribution."]}, {"name": "torch.distributions.exp_family.ExponentialFamily", "path": "distributions#torch.distributions.exp_family.ExponentialFamily", "type": "Probability Distributions", "text": ["Bases: Distribution", "ExponentialFamily is the abstract base class for probability distributions belonging to an exponential family, whose probability mass/density function has the form is defined below", "where \u03b8\\theta denotes the natural parameters, t(x)t(x) denotes the sufficient statistic, F(\u03b8)F(\\theta) is the log normalizer function for a given family and k(x)k(x) is the carrier measure.", "Note", "This class is an intermediary between the Distribution class and distributions which belong to an exponential family mainly to check the correctness of the .entropy() and analytic KL divergence methods. We use this class to compute the entropy and KL divergence using the AD framework and Bregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies and Cross-entropies of Exponential Families).", "Method to compute the entropy using Bregman divergence of the log normalizer."]}, {"name": "torch.distributions.exp_family.ExponentialFamily.entropy()", "path": "distributions#torch.distributions.exp_family.ExponentialFamily.entropy", "type": "Probability Distributions", "text": ["Method to compute the entropy using Bregman divergence of the log normalizer."]}, {"name": "torch.distributions.exponential.Exponential", "path": "distributions#torch.distributions.exponential.Exponential", "type": "Probability Distributions", "text": ["Bases: ExponentialFamily", "Creates a Exponential distribution parameterized by rate.", "Example:", "rate (float or Tensor) \u2013 rate = 1 / scale of the distribution"]}, {"name": "torch.distributions.exponential.Exponential.arg_constraints", "path": "distributions#torch.distributions.exponential.Exponential.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.cdf()", "path": "distributions#torch.distributions.exponential.Exponential.cdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.entropy()", "path": "distributions#torch.distributions.exponential.Exponential.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.expand()", "path": "distributions#torch.distributions.exponential.Exponential.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.has_rsample", "path": "distributions#torch.distributions.exponential.Exponential.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.icdf()", "path": "distributions#torch.distributions.exponential.Exponential.icdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.log_prob()", "path": "distributions#torch.distributions.exponential.Exponential.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.mean", "path": "distributions#torch.distributions.exponential.Exponential.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.mode", "path": "distributions#torch.distributions.exponential.Exponential.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.rsample()", "path": "distributions#torch.distributions.exponential.Exponential.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.stddev", "path": "distributions#torch.distributions.exponential.Exponential.stddev", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.support", "path": "distributions#torch.distributions.exponential.Exponential.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.variance", "path": "distributions#torch.distributions.exponential.Exponential.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a Fisher-Snedecor distribution parameterized by df1 and df2.", "Example:"]}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.expand()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.has_rsample", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.log_prob()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.mean", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.mode", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.rsample()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.support", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.variance", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma", "path": "distributions#torch.distributions.gamma.Gamma", "type": "Probability Distributions", "text": ["Bases: ExponentialFamily", "Creates a Gamma distribution parameterized by shape concentration and rate.", "Example:"]}, {"name": "torch.distributions.gamma.Gamma.arg_constraints", "path": "distributions#torch.distributions.gamma.Gamma.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.cdf()", "path": "distributions#torch.distributions.gamma.Gamma.cdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.entropy()", "path": "distributions#torch.distributions.gamma.Gamma.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.expand()", "path": "distributions#torch.distributions.gamma.Gamma.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.has_rsample", "path": "distributions#torch.distributions.gamma.Gamma.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.log_prob()", "path": "distributions#torch.distributions.gamma.Gamma.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.mean", "path": "distributions#torch.distributions.gamma.Gamma.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.mode", "path": "distributions#torch.distributions.gamma.Gamma.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.rsample()", "path": "distributions#torch.distributions.gamma.Gamma.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.support", "path": "distributions#torch.distributions.gamma.Gamma.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.variance", "path": "distributions#torch.distributions.gamma.Gamma.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric", "path": "distributions#torch.distributions.geometric.Geometric", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a Geometric distribution parameterized by probs, where probs is the probability of success of Bernoulli trials. It represents the probability that in k+1k + 1 Bernoulli trials, the first kk trials failed, before seeing a success.", "Samples are non-negative integers [0, inf\u2061\\inf).", "Example:"]}, {"name": "torch.distributions.geometric.Geometric.arg_constraints", "path": "distributions#torch.distributions.geometric.Geometric.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.entropy()", "path": "distributions#torch.distributions.geometric.Geometric.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.expand()", "path": "distributions#torch.distributions.geometric.Geometric.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.log_prob()", "path": "distributions#torch.distributions.geometric.Geometric.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.logits", "path": "distributions#torch.distributions.geometric.Geometric.logits", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.mean", "path": "distributions#torch.distributions.geometric.Geometric.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.mode", "path": "distributions#torch.distributions.geometric.Geometric.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.probs", "path": "distributions#torch.distributions.geometric.Geometric.probs", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.sample()", "path": "distributions#torch.distributions.geometric.Geometric.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.support", "path": "distributions#torch.distributions.geometric.Geometric.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.variance", "path": "distributions#torch.distributions.geometric.Geometric.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel", "path": "distributions#torch.distributions.gumbel.Gumbel", "type": "Probability Distributions", "text": ["Bases: TransformedDistribution", "Samples from a Gumbel Distribution.", "Examples:"]}, {"name": "torch.distributions.gumbel.Gumbel.arg_constraints", "path": "distributions#torch.distributions.gumbel.Gumbel.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel.entropy()", "path": "distributions#torch.distributions.gumbel.Gumbel.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel.expand()", "path": "distributions#torch.distributions.gumbel.Gumbel.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel.log_prob()", "path": "distributions#torch.distributions.gumbel.Gumbel.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel.mean", "path": "distributions#torch.distributions.gumbel.Gumbel.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel.mode", "path": "distributions#torch.distributions.gumbel.Gumbel.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel.stddev", "path": "distributions#torch.distributions.gumbel.Gumbel.stddev", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel.support", "path": "distributions#torch.distributions.gumbel.Gumbel.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel.variance", "path": "distributions#torch.distributions.gumbel.Gumbel.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy", "type": "Probability Distributions", "text": ["Bases: TransformedDistribution", "Creates a half-Cauchy distribution parameterized by scale where:", "Example:", "scale (float or Tensor) \u2013 scale of the full Cauchy distribution"]}, {"name": "torch.distributions.half_cauchy.HalfCauchy.arg_constraints", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.cdf()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.cdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.entropy()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.expand()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.has_rsample", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.icdf()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.icdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.log_prob()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.mean", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.mode", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.scale", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.scale", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.support", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.variance", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal", "path": "distributions#torch.distributions.half_normal.HalfNormal", "type": "Probability Distributions", "text": ["Bases: TransformedDistribution", "Creates a half-normal distribution parameterized by scale where:", "Example:", "scale (float or Tensor) \u2013 scale of the full Normal distribution"]}, {"name": "torch.distributions.half_normal.HalfNormal.arg_constraints", "path": "distributions#torch.distributions.half_normal.HalfNormal.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.cdf()", "path": "distributions#torch.distributions.half_normal.HalfNormal.cdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.entropy()", "path": "distributions#torch.distributions.half_normal.HalfNormal.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.expand()", "path": "distributions#torch.distributions.half_normal.HalfNormal.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.has_rsample", "path": "distributions#torch.distributions.half_normal.HalfNormal.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.icdf()", "path": "distributions#torch.distributions.half_normal.HalfNormal.icdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.log_prob()", "path": "distributions#torch.distributions.half_normal.HalfNormal.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.mean", "path": "distributions#torch.distributions.half_normal.HalfNormal.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.mode", "path": "distributions#torch.distributions.half_normal.HalfNormal.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.scale", "path": "distributions#torch.distributions.half_normal.HalfNormal.scale", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.support", "path": "distributions#torch.distributions.half_normal.HalfNormal.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.variance", "path": "distributions#torch.distributions.half_normal.HalfNormal.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent", "path": "distributions#torch.distributions.independent.Independent", "type": "Probability Distributions", "text": ["Bases: Distribution", "Reinterprets some of the batch dims of a distribution as event dims.", "This is mainly useful for changing the shape of the result of log_prob(). For example to create a diagonal Normal distribution with the same shape as a Multivariate Normal distribution (so they are interchangeable), you can:"]}, {"name": "torch.distributions.independent.Independent.arg_constraints", "path": "distributions#torch.distributions.independent.Independent.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.entropy()", "path": "distributions#torch.distributions.independent.Independent.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.enumerate_support()", "path": "distributions#torch.distributions.independent.Independent.enumerate_support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.expand()", "path": "distributions#torch.distributions.independent.Independent.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.has_enumerate_support", "path": "distributions#torch.distributions.independent.Independent.has_enumerate_support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.has_rsample", "path": "distributions#torch.distributions.independent.Independent.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.log_prob()", "path": "distributions#torch.distributions.independent.Independent.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.mean", "path": "distributions#torch.distributions.independent.Independent.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.mode", "path": "distributions#torch.distributions.independent.Independent.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.rsample()", "path": "distributions#torch.distributions.independent.Independent.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.sample()", "path": "distributions#torch.distributions.independent.Independent.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.support", "path": "distributions#torch.distributions.independent.Independent.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.variance", "path": "distributions#torch.distributions.independent.Independent.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.kl.kl_divergence()", "path": "distributions#torch.distributions.kl.kl_divergence", "type": "Probability Distributions", "text": ["Compute Kullback-Leibler divergence KL(p\u2225q)KL(p \\| q) between two distributions.", "A batch of KL divergences of shape batch_shape.", "Tensor", "NotImplementedError \u2013 If the distribution types have not been registered via register_kl()."]}, {"name": "torch.distributions.kl.register_kl()", "path": "distributions#torch.distributions.kl.register_kl", "type": "Probability Distributions", "text": ["Decorator to register a pairwise function with kl_divergence(). Usage:", "Lookup returns the most specific (type,type) match ordered by subclass. If the match is ambiguous, a RuntimeWarning is raised. For example to resolve the ambiguous situation:", "you should register a third most-specific implementation, e.g.:"]}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy", "type": "Probability Distributions", "text": ["Bases: TransformedDistribution", "Samples from a Kumaraswamy distribution.", "Example:"]}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.arg_constraints", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.entropy()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.expand()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.has_rsample", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.mean", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.mode", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.support", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.variance", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace", "path": "distributions#torch.distributions.laplace.Laplace", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a Laplace distribution parameterized by loc and scale.", "Example:"]}, {"name": "torch.distributions.laplace.Laplace.arg_constraints", "path": "distributions#torch.distributions.laplace.Laplace.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.cdf()", "path": "distributions#torch.distributions.laplace.Laplace.cdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.entropy()", "path": "distributions#torch.distributions.laplace.Laplace.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.expand()", "path": "distributions#torch.distributions.laplace.Laplace.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.has_rsample", "path": "distributions#torch.distributions.laplace.Laplace.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.icdf()", "path": "distributions#torch.distributions.laplace.Laplace.icdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.log_prob()", "path": "distributions#torch.distributions.laplace.Laplace.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.mean", "path": "distributions#torch.distributions.laplace.Laplace.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.mode", "path": "distributions#torch.distributions.laplace.Laplace.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.rsample()", "path": "distributions#torch.distributions.laplace.Laplace.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.stddev", "path": "distributions#torch.distributions.laplace.Laplace.stddev", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.support", "path": "distributions#torch.distributions.laplace.Laplace.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.variance", "path": "distributions#torch.distributions.laplace.Laplace.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky", "type": "Probability Distributions", "text": ["Bases: Distribution", "LKJ distribution for lower Cholesky factor of correlation matrices. The distribution is controlled by concentration parameter \u03b7\\eta to make the probability of the correlation matrix MM generated from a Cholesky factor proportional to det\u2061(M)\u03b7\u22121\\det(M)^{\\eta - 1}. Because of that, when concentration == 1, we have a uniform distribution over Cholesky factors of correlation matrices:", "Note that this distribution samples the Cholesky factor of correlation matrices and not the correlation matrices themselves and thereby differs slightly from the derivations in [1] for the LKJCorr distribution. For sampling, this uses the Onion method from [1] Section 3.", "Example:", "References", "[1] Generating random correlation matrices based on vines and extended onion method (2009), Daniel Lewandowski, Dorota Kurowicka, Harry Joe. Journal of Multivariate Analysis. 100. 10.1016/j.jmva.2009.04.008"]}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.arg_constraints", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.expand()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.log_prob()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.sample()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.support", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal", "path": "distributions#torch.distributions.log_normal.LogNormal", "type": "Probability Distributions", "text": ["Bases: TransformedDistribution", "Creates a log-normal distribution parameterized by loc and scale where:", "Example:"]}, {"name": "torch.distributions.log_normal.LogNormal.arg_constraints", "path": "distributions#torch.distributions.log_normal.LogNormal.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.entropy()", "path": "distributions#torch.distributions.log_normal.LogNormal.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.expand()", "path": "distributions#torch.distributions.log_normal.LogNormal.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.has_rsample", "path": "distributions#torch.distributions.log_normal.LogNormal.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.loc", "path": "distributions#torch.distributions.log_normal.LogNormal.loc", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.mean", "path": "distributions#torch.distributions.log_normal.LogNormal.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.mode", "path": "distributions#torch.distributions.log_normal.LogNormal.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.scale", "path": "distributions#torch.distributions.log_normal.LogNormal.scale", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.support", "path": "distributions#torch.distributions.log_normal.LogNormal.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.variance", "path": "distributions#torch.distributions.log_normal.LogNormal.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a multivariate normal distribution with covariance matrix having a low-rank form parameterized by cov_factor and cov_diag:", "Note", "The computation for determinant and inverse of covariance matrix is avoided when cov_factor.shape[1] << cov_factor.shape[0] thanks to Woodbury matrix identity and matrix determinant lemma. Thanks to these formulas, we just need to compute the determinant and inverse of the small size \u201ccapacitance\u201d matrix:"]}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mode", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily", "type": "Probability Distributions", "text": ["Bases: Distribution", "The MixtureSameFamily distribution implements a (batch of) mixture distribution where all component are from different parameterizations of the same distribution type. It is parameterized by a Categorical \u201cselecting distribution\u201d (over k component) and a component distribution, i.e., a Distribution with a rightmost batch shape (equal to [k]) which indexes each (batch of) component.", "Examples:"]}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.arg_constraints", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.cdf()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.cdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.expand()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.has_rsample", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.log_prob()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.mean", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.sample()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.support", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.variance", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial", "path": "distributions#torch.distributions.multinomial.Multinomial", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a Multinomial distribution parameterized by total_count and either probs or logits (but not both). The innermost dimension of probs indexes over categories. All other dimensions index over batches.", "Note that total_count need not be specified if only log_prob() is called (see example below)", "Note", "The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. logits will return this normalized value.", "Example:"]}, {"name": "torch.distributions.multinomial.Multinomial.arg_constraints", "path": "distributions#torch.distributions.multinomial.Multinomial.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.entropy()", "path": "distributions#torch.distributions.multinomial.Multinomial.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.expand()", "path": "distributions#torch.distributions.multinomial.Multinomial.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.log_prob()", "path": "distributions#torch.distributions.multinomial.Multinomial.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.logits", "path": "distributions#torch.distributions.multinomial.Multinomial.logits", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.mean", "path": "distributions#torch.distributions.multinomial.Multinomial.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.param_shape", "path": "distributions#torch.distributions.multinomial.Multinomial.param_shape", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.probs", "path": "distributions#torch.distributions.multinomial.Multinomial.probs", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.sample()", "path": "distributions#torch.distributions.multinomial.Multinomial.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.support", "path": "distributions#torch.distributions.multinomial.Multinomial.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.total_count", "path": "distributions#torch.distributions.multinomial.Multinomial.total_count", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.variance", "path": "distributions#torch.distributions.multinomial.Multinomial.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a multivariate normal (also called Gaussian) distribution parameterized by a mean vector and a covariance matrix.", "The multivariate normal distribution can be parameterized either in terms of a positive definite covariance matrix \u03a3\\mathbf{\\Sigma} or a positive definite precision matrix \u03a3\u22121\\mathbf{\\Sigma}^{-1} or a lower-triangular matrix L\\mathbf{L} with positive-valued diagonal entries, such that \u03a3=LL\u22a4\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top. This triangular matrix can be obtained via e.g. Cholesky decomposition of the covariance.", "Note", "Only one of covariance_matrix or precision_matrix or scale_tril can be specified.", "Using scale_tril will be more efficient: all computations internally are based on scale_tril. If covariance_matrix or precision_matrix is passed instead, it is only used to compute the corresponding lower triangular matrices using a Cholesky decomposition."]}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.entropy()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.expand()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.has_rsample", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.log_prob()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.mean", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.mode", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.rsample()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.scale_tril", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.support", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.variance", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a Negative Binomial distribution, i.e. distribution of the number of successful independent and identical Bernoulli trials before total_count failures are achieved. The probability of success of each Bernoulli trial is probs."]}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.arg_constraints", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.expand()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.log_prob()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.logits", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.logits", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.mean", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.mode", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.param_shape", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.param_shape", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.probs", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.probs", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.sample()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.support", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.variance", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal", "path": "distributions#torch.distributions.normal.Normal", "type": "Probability Distributions", "text": ["Bases: ExponentialFamily", "Creates a normal (also called Gaussian) distribution parameterized by loc and scale.", "Example:"]}, {"name": "torch.distributions.normal.Normal.arg_constraints", "path": "distributions#torch.distributions.normal.Normal.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.cdf()", "path": "distributions#torch.distributions.normal.Normal.cdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.entropy()", "path": "distributions#torch.distributions.normal.Normal.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.expand()", "path": "distributions#torch.distributions.normal.Normal.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.has_rsample", "path": "distributions#torch.distributions.normal.Normal.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.icdf()", "path": "distributions#torch.distributions.normal.Normal.icdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.log_prob()", "path": "distributions#torch.distributions.normal.Normal.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.mean", "path": "distributions#torch.distributions.normal.Normal.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.mode", "path": "distributions#torch.distributions.normal.Normal.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.rsample()", "path": "distributions#torch.distributions.normal.Normal.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.sample()", "path": "distributions#torch.distributions.normal.Normal.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.stddev", "path": "distributions#torch.distributions.normal.Normal.stddev", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.support", "path": "distributions#torch.distributions.normal.Normal.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.variance", "path": "distributions#torch.distributions.normal.Normal.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a one-hot categorical distribution parameterized by probs or logits.", "Samples are one-hot coded vectors of size probs.size(-1).", "Note", "The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. logits will return this normalized value.", "See also: torch.distributions.Categorical() for specifications of probs and logits.", "Example:"]}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.entropy()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.expand()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.log_prob()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.logits", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.logits", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.mean", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.mode", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.param_shape", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.param_shape", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.probs", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.probs", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.sample()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.support", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.variance", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.pareto.Pareto", "path": "distributions#torch.distributions.pareto.Pareto", "type": "Probability Distributions", "text": ["Bases: TransformedDistribution", "Samples from a Pareto Type 1 distribution.", "Example:"]}, {"name": "torch.distributions.pareto.Pareto.arg_constraints", "path": "distributions#torch.distributions.pareto.Pareto.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.pareto.Pareto.entropy()", "path": "distributions#torch.distributions.pareto.Pareto.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.pareto.Pareto.expand()", "path": "distributions#torch.distributions.pareto.Pareto.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.pareto.Pareto.mean", "path": "distributions#torch.distributions.pareto.Pareto.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.pareto.Pareto.mode", "path": "distributions#torch.distributions.pareto.Pareto.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.pareto.Pareto.support", "path": "distributions#torch.distributions.pareto.Pareto.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.pareto.Pareto.variance", "path": "distributions#torch.distributions.pareto.Pareto.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.poisson.Poisson", "path": "distributions#torch.distributions.poisson.Poisson", "type": "Probability Distributions", "text": ["Bases: ExponentialFamily", "Creates a Poisson distribution parameterized by rate, the rate parameter.", "Samples are nonnegative integers, with a pmf given by", "Example:", "rate (Number, Tensor) \u2013 the rate parameter"]}, {"name": "torch.distributions.poisson.Poisson.arg_constraints", "path": "distributions#torch.distributions.poisson.Poisson.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.poisson.Poisson.expand()", "path": "distributions#torch.distributions.poisson.Poisson.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.poisson.Poisson.log_prob()", "path": "distributions#torch.distributions.poisson.Poisson.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.poisson.Poisson.mean", "path": "distributions#torch.distributions.poisson.Poisson.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.poisson.Poisson.mode", "path": "distributions#torch.distributions.poisson.Poisson.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.poisson.Poisson.sample()", "path": "distributions#torch.distributions.poisson.Poisson.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.poisson.Poisson.support", "path": "distributions#torch.distributions.poisson.Poisson.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.poisson.Poisson.variance", "path": "distributions#torch.distributions.poisson.Poisson.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a LogitRelaxedBernoulli distribution parameterized by probs or logits (but not both), which is the logit of a RelaxedBernoulli distribution.", "Samples are logits of values in (0, 1). See [1] for more details.", "[1] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables (Maddison et al, 2017)", "[2] Categorical Reparametrization with Gumbel-Softmax (Jang et al, 2017)"]}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli", "type": "Probability Distributions", "text": ["Bases: TransformedDistribution", "Creates a RelaxedBernoulli distribution, parametrized by temperature, and either probs or logits (but not both). This is a relaxed version of the Bernoulli distribution, so the values are in (0, 1), and has reparametrizable samples.", "Example:"]}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical", "type": "Probability Distributions", "text": ["Bases: TransformedDistribution", "Creates a RelaxedOneHotCategorical distribution parametrized by temperature, and either probs or logits. This is a relaxed version of the OneHotCategorical distribution, so its samples are on simplex, and are reparametrizable.", "Example:"]}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT", "path": "distributions#torch.distributions.studentT.StudentT", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a Student\u2019s t-distribution parameterized by degree of freedom df, mean loc and scale scale.", "Example:"]}, {"name": "torch.distributions.studentT.StudentT.arg_constraints", "path": "distributions#torch.distributions.studentT.StudentT.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.entropy()", "path": "distributions#torch.distributions.studentT.StudentT.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.expand()", "path": "distributions#torch.distributions.studentT.StudentT.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.has_rsample", "path": "distributions#torch.distributions.studentT.StudentT.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.log_prob()", "path": "distributions#torch.distributions.studentT.StudentT.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.mean", "path": "distributions#torch.distributions.studentT.StudentT.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.mode", "path": "distributions#torch.distributions.studentT.StudentT.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.rsample()", "path": "distributions#torch.distributions.studentT.StudentT.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.support", "path": "distributions#torch.distributions.studentT.StudentT.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.variance", "path": "distributions#torch.distributions.studentT.StudentT.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution", "type": "Probability Distributions", "text": ["Bases: Distribution", "Extension of the Distribution class, which applies a sequence of Transforms to a base distribution. Let f be the composition of transforms applied:", "Note that the .event_shape of a TransformedDistribution is the maximum shape of its base distribution and its transforms, since transforms can introduce correlations among events.", "An example for the usage of TransformedDistribution would be:", "For more examples, please look at the implementations of Gumbel, HalfCauchy, HalfNormal, LogNormal, Pareto, Weibull, RelaxedBernoulli and RelaxedOneHotCategorical", "Computes the cumulative distribution function by inverting the transform(s) and computing the score of the base distribution.", "Computes the inverse cumulative distribution function using transform(s) and computing the score of the base distribution.", "Scores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian.", "Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list.", "Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list."]}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.cdf()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.cdf", "type": "Probability Distributions", "text": ["Computes the cumulative distribution function by inverting the transform(s) and computing the score of the base distribution."]}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.expand()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.has_rsample", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.icdf()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.icdf", "type": "Probability Distributions", "text": ["Computes the inverse cumulative distribution function using transform(s) and computing the score of the base distribution."]}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.log_prob()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.log_prob", "type": "Probability Distributions", "text": ["Scores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian."]}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.rsample()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.rsample", "type": "Probability Distributions", "text": ["Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list."]}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.sample()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.sample", "type": "Probability Distributions", "text": ["Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list."]}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.support", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.transforms.AbsTransform", "path": "distributions#torch.distributions.transforms.AbsTransform", "type": "Probability Distributions", "text": ["Transform via the mapping y=\u2223x\u2223y = |x|."]}, {"name": "torch.distributions.transforms.AffineTransform", "path": "distributions#torch.distributions.transforms.AffineTransform", "type": "Probability Distributions", "text": ["Transform via the pointwise affine mapping y=loc+scale\u00d7xy = \\text{loc} + \\text{scale} \\times x."]}, {"name": "torch.distributions.transforms.CatTransform", "path": "distributions#torch.distributions.transforms.CatTransform", "type": "Probability Distributions", "text": ["Transform functor that applies a sequence of transforms tseq component-wise to each submatrix at dim, of length lengths[dim], in a way compatible with torch.cat().", "Example:"]}, {"name": "torch.distributions.transforms.ComposeTransform", "path": "distributions#torch.distributions.transforms.ComposeTransform", "type": "Probability Distributions", "text": ["Composes multiple transforms in a chain. The transforms being composed are responsible for caching."]}, {"name": "torch.distributions.transforms.CorrCholeskyTransform", "path": "distributions#torch.distributions.transforms.CorrCholeskyTransform", "type": "Probability Distributions", "text": ["Transforms an uncontrained real vector xx with length D\u2217(D\u22121)/2D*(D-1)/2 into the Cholesky factor of a D-dimension correlation matrix. This Cholesky factor is a lower triangular matrix with positive diagonals and unit Euclidean norm for each row. The transform is processed as follows:"]}, {"name": "torch.distributions.transforms.CumulativeDistributionTransform", "path": "distributions#torch.distributions.transforms.CumulativeDistributionTransform", "type": "Probability Distributions", "text": ["Transform via the cumulative distribution function of a probability distribution.", "distribution (Distribution) \u2013 Distribution whose cumulative distribution function to use for the transformation.", "Example:"]}, {"name": "torch.distributions.transforms.ExpTransform", "path": "distributions#torch.distributions.transforms.ExpTransform", "type": "Probability Distributions", "text": ["Transform via the mapping y=exp\u2061(x)y = \\exp(x)."]}, {"name": "torch.distributions.transforms.IndependentTransform", "path": "distributions#torch.distributions.transforms.IndependentTransform", "type": "Probability Distributions", "text": ["Wrapper around another transform to treat reinterpreted_batch_ndims-many extra of the right most dimensions as dependent. This has no effect on the forward or backward transforms, but does sum out reinterpreted_batch_ndims-many of the rightmost dimensions in log_abs_det_jacobian()."]}, {"name": "torch.distributions.transforms.LowerCholeskyTransform", "path": "distributions#torch.distributions.transforms.LowerCholeskyTransform", "type": "Probability Distributions", "text": ["Transform from unconstrained matrices to lower-triangular matrices with nonnegative diagonal entries.", "This is useful for parameterizing positive definite matrices in terms of their Cholesky factorization."]}, {"name": "torch.distributions.transforms.PositiveDefiniteTransform", "path": "distributions#torch.distributions.transforms.PositiveDefiniteTransform", "type": "Probability Distributions", "text": ["Transform from unconstrained matrices to positive-definite matrices."]}, {"name": "torch.distributions.transforms.PowerTransform", "path": "distributions#torch.distributions.transforms.PowerTransform", "type": "Probability Distributions", "text": ["Transform via the mapping y=xexponenty = x^{\\text{exponent}}."]}, {"name": "torch.distributions.transforms.ReshapeTransform", "path": "distributions#torch.distributions.transforms.ReshapeTransform", "type": "Probability Distributions", "text": ["Unit Jacobian transform to reshape the rightmost part of a tensor.", "Note that in_shape and out_shape must have the same number of elements, just as for torch.Tensor.reshape()."]}, {"name": "torch.distributions.transforms.SigmoidTransform", "path": "distributions#torch.distributions.transforms.SigmoidTransform", "type": "Probability Distributions", "text": ["Transform via the mapping y=11+exp\u2061(\u2212x)y = \\frac{1}{1 + \\exp(-x)} and x=logit(y)x = \\text{logit}(y)."]}, {"name": "torch.distributions.transforms.SoftmaxTransform", "path": "distributions#torch.distributions.transforms.SoftmaxTransform", "type": "Probability Distributions", "text": ["Transform from unconstrained space to the simplex via y=exp\u2061(x)y = \\exp(x) then normalizing.", "This is not bijective and cannot be used for HMC. However this acts mostly coordinate-wise (except for the final normalization), and thus is appropriate for coordinate-wise optimization algorithms."]}, {"name": "torch.distributions.transforms.SoftplusTransform", "path": "distributions#torch.distributions.transforms.SoftplusTransform", "type": "Probability Distributions", "text": ["Transform via the mapping Softplus(x)=log\u2061(1+exp\u2061(x))\\text{Softplus}(x) = \\log(1 + \\exp(x)). The implementation reverts to the linear function when x>20x > 20."]}, {"name": "torch.distributions.transforms.StackTransform", "path": "distributions#torch.distributions.transforms.StackTransform", "type": "Probability Distributions", "text": ["Transform functor that applies a sequence of transforms tseq component-wise to each submatrix at dim in a way compatible with torch.stack().", "Example:"]}, {"name": "torch.distributions.transforms.StickBreakingTransform", "path": "distributions#torch.distributions.transforms.StickBreakingTransform", "type": "Probability Distributions", "text": ["Transform from unconstrained space to the simplex of one additional dimension via a stick-breaking process.", "This transform arises as an iterated sigmoid transform in a stick-breaking construction of the Dirichlet distribution: the first logit is transformed via sigmoid to the first probability and the probability of everything else, and then the process recurses.", "This is bijective and appropriate for use in HMC; however it mixes coordinates together and is less appropriate for optimization."]}, {"name": "torch.distributions.transforms.TanhTransform", "path": "distributions#torch.distributions.transforms.TanhTransform", "type": "Probability Distributions", "text": ["Transform via the mapping y=tanh\u2061(x)y = \\tanh(x).", "It is equivalent to `\nComposeTransform([AffineTransform(0., 2.), SigmoidTransform(), AffineTransform(-1., 2.)])\n` However this might not be numerically stable, thus it is recommended to use TanhTransform instead.", "Note that one should use cache_size=1 when it comes to NaN/Inf values."]}, {"name": "torch.distributions.transforms.Transform", "path": "distributions#torch.distributions.transforms.Transform", "type": "Probability Distributions", "text": ["Abstract class for invertable transformations with computable log det jacobians. They are primarily used in torch.distributions.TransformedDistribution.", "Caching is useful for transforms whose inverses are either expensive or numerically unstable. Note that care must be taken with memoized values since the autograd graph may be reversed. For example while the following works with or without caching:", "However the following will error when caching due to dependency reversal:", "Derived classes should implement one or both of _call() or _inverse(). Derived classes that set bijective=True should also implement log_abs_det_jacobian().", "cache_size (int) \u2013 Size of cache. If zero, no caching is done. If one, the latest single value is cached. Only 0 and 1 are supported.", "Returns the inverse Transform of this transform. This should satisfy t.inv.inv is t.", "Returns the sign of the determinant of the Jacobian, if applicable. In general this only makes sense for bijective transforms.", "Computes the log det jacobian log |dy/dx| given input and output.", "Infers the shape of the forward computation, given the input shape. Defaults to preserving shape.", "Infers the shapes of the inverse computation, given the output shape. Defaults to preserving shape."]}, {"name": "torch.distributions.transforms.Transform.forward_shape()", "path": "distributions#torch.distributions.transforms.Transform.forward_shape", "type": "Probability Distributions", "text": ["Infers the shape of the forward computation, given the input shape. Defaults to preserving shape."]}, {"name": "torch.distributions.transforms.Transform.inv", "path": "distributions#torch.distributions.transforms.Transform.inv", "type": "Probability Distributions", "text": ["Returns the inverse Transform of this transform. This should satisfy t.inv.inv is t."]}, {"name": "torch.distributions.transforms.Transform.inverse_shape()", "path": "distributions#torch.distributions.transforms.Transform.inverse_shape", "type": "Probability Distributions", "text": ["Infers the shapes of the inverse computation, given the output shape. Defaults to preserving shape."]}, {"name": "torch.distributions.transforms.Transform.log_abs_det_jacobian()", "path": "distributions#torch.distributions.transforms.Transform.log_abs_det_jacobian", "type": "Probability Distributions", "text": ["Computes the log det jacobian log |dy/dx| given input and output."]}, {"name": "torch.distributions.transforms.Transform.sign", "path": "distributions#torch.distributions.transforms.Transform.sign", "type": "Probability Distributions", "text": ["Returns the sign of the determinant of the Jacobian, if applicable. In general this only makes sense for bijective transforms."]}, {"name": "torch.distributions.uniform.Uniform", "path": "distributions#torch.distributions.uniform.Uniform", "type": "Probability Distributions", "text": ["Bases: Distribution", "Generates uniformly distributed random samples from the half-open interval [low, high).", "Example:"]}, {"name": "torch.distributions.uniform.Uniform.arg_constraints", "path": "distributions#torch.distributions.uniform.Uniform.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.cdf()", "path": "distributions#torch.distributions.uniform.Uniform.cdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.entropy()", "path": "distributions#torch.distributions.uniform.Uniform.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.expand()", "path": "distributions#torch.distributions.uniform.Uniform.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.has_rsample", "path": "distributions#torch.distributions.uniform.Uniform.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.icdf()", "path": "distributions#torch.distributions.uniform.Uniform.icdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.log_prob()", "path": "distributions#torch.distributions.uniform.Uniform.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.mean", "path": "distributions#torch.distributions.uniform.Uniform.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.mode", "path": "distributions#torch.distributions.uniform.Uniform.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.rsample()", "path": "distributions#torch.distributions.uniform.Uniform.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.stddev", "path": "distributions#torch.distributions.uniform.Uniform.stddev", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.support", "path": "distributions#torch.distributions.uniform.Uniform.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.variance", "path": "distributions#torch.distributions.uniform.Uniform.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.von_mises.VonMises", "path": "distributions#torch.distributions.von_mises.VonMises", "type": "Probability Distributions", "text": ["Bases: Distribution", "A circular von Mises distribution.", "This implementation uses polar coordinates. The loc and value args can be any real number (to facilitate unconstrained optimization), but are interpreted as angles modulo 2 pi.", "The provided mean is the circular one.", "The sampling algorithm for the von Mises distribution is based on the following paper: Best, D. J., and Nicholas I. Fisher. \u201cEfficient simulation of the von Mises distribution.\u201d Applied Statistics (1979): 152-157.", "The provided variance is the circular one."]}, {"name": "torch.distributions.von_mises.VonMises.arg_constraints", "path": "distributions#torch.distributions.von_mises.VonMises.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.von_mises.VonMises.expand()", "path": "distributions#torch.distributions.von_mises.VonMises.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.von_mises.VonMises.has_rsample", "path": "distributions#torch.distributions.von_mises.VonMises.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.von_mises.VonMises.log_prob()", "path": "distributions#torch.distributions.von_mises.VonMises.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.von_mises.VonMises.mean", "path": "distributions#torch.distributions.von_mises.VonMises.mean", "type": "Probability Distributions", "text": ["The provided mean is the circular one."]}, {"name": "torch.distributions.von_mises.VonMises.mode", "path": "distributions#torch.distributions.von_mises.VonMises.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.von_mises.VonMises.sample()", "path": "distributions#torch.distributions.von_mises.VonMises.sample", "type": "Probability Distributions", "text": ["The sampling algorithm for the von Mises distribution is based on the following paper: Best, D. J., and Nicholas I. Fisher. \u201cEfficient simulation of the von Mises distribution.\u201d Applied Statistics (1979): 152-157."]}, {"name": "torch.distributions.von_mises.VonMises.support", "path": "distributions#torch.distributions.von_mises.VonMises.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.von_mises.VonMises.variance", "path": "distributions#torch.distributions.von_mises.VonMises.variance", "type": "Probability Distributions", "text": ["The provided variance is the circular one."]}, {"name": "torch.distributions.weibull.Weibull", "path": "distributions#torch.distributions.weibull.Weibull", "type": "Probability Distributions", "text": ["Bases: TransformedDistribution", "Samples from a two-parameter Weibull distribution."]}, {"name": "torch.distributions.weibull.Weibull.arg_constraints", "path": "distributions#torch.distributions.weibull.Weibull.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.weibull.Weibull.entropy()", "path": "distributions#torch.distributions.weibull.Weibull.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.weibull.Weibull.expand()", "path": "distributions#torch.distributions.weibull.Weibull.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.weibull.Weibull.mean", "path": "distributions#torch.distributions.weibull.Weibull.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.weibull.Weibull.mode", "path": "distributions#torch.distributions.weibull.Weibull.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.weibull.Weibull.support", "path": "distributions#torch.distributions.weibull.Weibull.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.weibull.Weibull.variance", "path": "distributions#torch.distributions.weibull.Weibull.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart", "path": "distributions#torch.distributions.wishart.Wishart", "type": "Probability Distributions", "text": ["Bases: ExponentialFamily", "Creates a Wishart distribution parameterized by a symmetric positive definite matrix \u03a3\\Sigma, or its Cholesky decomposition \u03a3=LL\u22a4\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top", "Note", "Only one of covariance_matrix or precision_matrix or scale_tril can be specified. Using scale_tril will be more efficient: all computations internally are based on scale_tril. If covariance_matrix or precision_matrix is passed instead, it is only used to compute the corresponding lower triangular matrices using a Cholesky decomposition. \u2018torch.distributions.LKJCholesky\u2019 is a restricted Wishart distribution.[1]", "References", "[1] Wang, Z., Wu, Y. and Chu, H., 2018. On equivalence of the LKJ distribution and the restricted Wishart distribution. [2] Sawyer, S., 2007. Wishart Distributions and Inverse-Wishart Sampling. [3] Anderson, T. W., 2003. An Introduction to Multivariate Statistical Analysis (3rd ed.). [4] Odell, P. L. & Feiveson, A. H., 1966. A Numerical Procedure to Generate a SampleCovariance Matrix. JASA, 61(313):199-203. [5] Ku, Y.-C. & Bloomfield, P., 2010. Generating Random Wishart Matrices with Fractional Degrees of Freedom in OX.", "Warning", "In some cases, sampling algorithm based on Bartlett decomposition may return singular matrix samples. Several tries to correct singular samples are performed by default, but it may end up returning singular matrix samples. Singular samples may return -inf values in .log_prob(). In those cases, the user should validate the samples and either fix the value of df or adjust max_try_correction value for argument in .rsample accordingly."]}, {"name": "torch.distributions.wishart.Wishart.arg_constraints", "path": "distributions#torch.distributions.wishart.Wishart.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart.covariance_matrix", "path": "distributions#torch.distributions.wishart.Wishart.covariance_matrix", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart.entropy()", "path": "distributions#torch.distributions.wishart.Wishart.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart.expand()", "path": "distributions#torch.distributions.wishart.Wishart.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart.has_rsample", "path": "distributions#torch.distributions.wishart.Wishart.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart.log_prob()", "path": "distributions#torch.distributions.wishart.Wishart.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart.mean", "path": "distributions#torch.distributions.wishart.Wishart.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart.mode", "path": "distributions#torch.distributions.wishart.Wishart.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart.precision_matrix", "path": "distributions#torch.distributions.wishart.Wishart.precision_matrix", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart.rsample()", "path": "distributions#torch.distributions.wishart.Wishart.rsample", "type": "Probability Distributions", "text": ["Warning", "In some cases, sampling algorithm based on Bartlett decomposition may return singular matrix samples. Several tries to correct singular samples are performed by default, but it may end up returning singular matrix samples. Singular samples may return -inf values in .log_prob(). In those cases, the user should validate the samples and either fix the value of df or adjust max_try_correction value for argument in .rsample accordingly."]}, {"name": "torch.distributions.wishart.Wishart.scale_tril", "path": "distributions#torch.distributions.wishart.Wishart.scale_tril", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart.support", "path": "distributions#torch.distributions.wishart.Wishart.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart.variance", "path": "distributions#torch.distributions.wishart.Wishart.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.div", "path": "generated/torch.div", "type": "Torch", "text": ["Divides each element of the input input by the corresponding element of other.", "Note", "By default, this performs a \u201ctrue\u201d division like Python 3. See the rounding_mode argument for floor division.", "Supports broadcasting to a common shape, type promotion, and integer, float, and complex inputs. Always promotes integer types to the default scalar type.", "rounding_mode (str, optional) \u2013 ", "Type of rounding applied to the result:", "Examples:"]}, {"name": "torch.divide", "path": "generated/torch.divide", "type": "Torch", "text": ["Alias for torch.div()."]}, {"name": "torch.dot", "path": "generated/torch.dot", "type": "Torch", "text": ["Computes the dot product of two 1D tensors.", "Note", "Unlike NumPy\u2019s dot, torch.dot intentionally only supports computing the dot product of two 1D tensors with the same number of elements.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.DoubleStorage", "path": "storage#torch.DoubleStorage", "type": "Storage", "text": []}, {"name": "torch.DoubleStorage.dtype", "path": "storage#torch.DoubleStorage.dtype", "type": "Storage", "text": []}, {"name": "torch.dsplit", "path": "generated/torch.dsplit", "type": "Torch", "text": ["Splits input, a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections. Each split is a view of input.", "This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=2) (the split dimension is 2), except that if indices_or_sections is an integer it must evenly divide the split dimension or a runtime error will be thrown.", "This function is based on NumPy\u2019s numpy.dsplit()."]}, {"name": "torch.dstack", "path": "generated/torch.dstack", "type": "Torch", "text": ["Stack tensors in sequence depthwise (along third axis).", "This is equivalent to concatenation along the third axis after 1-D and 2-D tensors have been reshaped by torch.atleast_3d().", "tensors (sequence of Tensors) \u2013 sequence of tensors to concatenate", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.dtype", "path": "tensor_attributes#torch.dtype", "type": "Miscellaneous", "text": []}, {"name": "torch.einsum", "path": "generated/torch.einsum", "type": "Torch", "text": ["Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.", "Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them in a short-hand format based on the Einstein summation convention, given by equation. The details of this format are described below, but the general idea is to label every dimension of the input operands with some subscript and define which subscripts are part of the output. The output is then computed by summing the product of the elements of the operands along the dimensions whose subscripts are not part of the output. For example, matrix multiplication can be computed using einsum as torch.einsum(\u201cij,jk->ik\u201d, A, B). Here, j is the summation subscript and i and k the output subscripts (see section below for more details on why).", "Equation:", "The equation string specifies the subscripts (letters in [a-zA-Z]) for each dimension of the input operands in the same order as the dimensions, separating subscripts for each operand by a comma (\u2018,\u2019), e.g. \u2018ij,jk\u2019 specify subscripts for two 2D operands. The dimensions labeled with the same subscript must be broadcastable, that is, their size must either match or be 1. The exception is if a subscript is repeated for the same input operand, in which case the dimensions labeled with this subscript for this operand must match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that appear exactly once in the equation will be part of the output, sorted in increasing alphabetical order. The output is computed by multiplying the input operands element-wise, with their dimensions aligned based on the subscripts, and then summing out the dimensions whose subscripts are not part of the output.", "Optionally, the output subscripts can be explicitly defined by adding an arrow (\u2018->\u2019) at the end of the equation followed by the subscripts for the output. For instance, the following equation computes the transpose of a matrix multiplication: \u2018ij,jk->ki\u2019. The output subscripts must appear at least once for some input operand and at most once for the output.", "Ellipsis (\u2018\u2026\u2019) can be used in place of subscripts to broadcast the dimensions covered by the ellipsis. Each input operand may contain at most one ellipsis which will cover the dimensions not covered by subscripts, e.g. for an input operand with 5 dimensions, the ellipsis in the equation \u2018ab\u2026c\u2019 cover the third and fourth dimensions. The ellipsis does not need to cover the same number of dimensions across the operands but the \u2018shape\u2019 of the ellipsis (the size of the dimensions covered by them) must broadcast together. If the output is not explicitly defined with the arrow (\u2018->\u2019) notation, the ellipsis will come first in the output (left-most dimensions), before the subscript labels that appear exactly once for the input operands. e.g. the following equation implements batch matrix multiplication \u2018\u2026ij,\u2026jk\u2019.", "A few final notes: the equation may contain whitespaces between the different elements (subscripts, ellipsis, arrow and comma) but something like \u2018\u2026\u2019 is not valid. An empty string \u2018\u2019 is valid for scalar operands.", "Note", "torch.einsum handles ellipsis (\u2018\u2026\u2019) differently from NumPy in that it allows dimensions covered by the ellipsis to be summed over, that is, ellipsis are not required to be part of the output.", "Note", "This function uses opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/) to speed up computation or to consume less memory by optimizing contraction order. This optimization occurs when there are at least three inputs, since the order does not matter otherwise. Note that finding _the_ optimal path is an NP-hard problem, thus, opt_einsum relies on different heuristics to achieve near-optimal results. If opt_einsum is not available, the default order is to contract from left to right.", "To bypass this default behavior, add the following line to disable the usage of opt_einsum and skip path calculation: torch.backends.opt_einsum.enabled = False", "To specify which strategy you\u2019d like for opt_einsum to compute the contraction path, add the following line: torch.backends.opt_einsum.strategy = \u2018auto\u2019. The default strategy is \u2018auto\u2019, and we also support \u2018greedy\u2019 and \u2018optimal\u2019. Disclaimer that the runtime of \u2018optimal\u2019 is factorial in the number of inputs! See more details in the opt_einsum documentation (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html).", "Note", "As of PyTorch 1.10 torch.einsum() also supports the sublist format (see examples below). In this format, subscripts for each operand are specified by sublists, list of integers in the range [0, 52). These sublists follow their operands, and an extra sublist can appear at the end of the input to specify the output\u2019s subscripts., e.g. torch.einsum(op1, sublist1, op2, sublist2, \u2026, [subslist_out]). Python\u2019s Ellipsis object may be provided in a sublist to enable broadcasting as described in the Equation section above.", "Tensor", "Examples:"]}, {"name": "torch.empty", "path": "generated/torch.empty", "type": "Torch", "text": ["Returns a tensor filled with uninitialized data. The shape of the tensor is defined by the variable argument size.", "Note", "If torch.use_deterministic_algorithms() is set to True, the output tensor is initialized to prevent any possible nondeterministic behavior from using the data as an input to an operation. Floating point and complex tensors are filled with NaN, and integer tensors are filled with the maximum value.", "size (int...) \u2013 a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.", "Example:"]}, {"name": "torch.empty_like", "path": "generated/torch.empty_like", "type": "Torch", "text": ["Returns an uninitialized tensor with the same size as input. torch.empty_like(input) is equivalent to torch.empty(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).", "Note", "If torch.use_deterministic_algorithms() is set to True, the output tensor is initialized to prevent any possible nondeterministic behavior from using the data as an input to an operation. Floating point and complex tensors are filled with NaN, and integer tensors are filled with the maximum value.", "input (Tensor) \u2013 the size of input will determine size of the output tensor.", "Example:"]}, {"name": "torch.empty_strided", "path": "generated/torch.empty_strided", "type": "Torch", "text": ["Creates a tensor with the specified size and stride and filled with undefined data.", "Warning", "If the constructed tensor is \u201coverlapped\u201d (with multiple indices referring to the same element in memory) its behavior is undefined.", "Note", "If torch.use_deterministic_algorithms() is set to True, the output tensor is initialized to prevent any possible nondeterministic behavior from using the data as an input to an operation. Floating point and complex tensors are filled with NaN, and integer tensors are filled with the maximum value.", "Example:"]}, {"name": "torch.enable_grad", "path": "generated/torch.enable_grad#torch.enable_grad", "type": "Torch", "text": ["Context-manager that enables gradient calculation.", "Enables gradient calculation, if it has been disabled via no_grad or set_grad_enabled.", "This context manager is thread local; it will not affect computation in other threads.", "Also functions as a decorator.", "Note", "enable_grad is one of several mechanisms that can enable or disable gradients locally see Locally disabling gradient computation for more information on how they compare.", "Note", "This API does not apply to forward-mode AD."]}, {"name": "torch.eq", "path": "generated/torch.eq", "type": "Torch", "text": ["Computes element-wise equality", "The second argument can be a number or a tensor whose shape is broadcastable with the first argument.", "out (Tensor, optional) \u2013 the output tensor.", "A boolean tensor that is True where input is equal to other and False elsewhere", "Example:"]}, {"name": "torch.equal", "path": "generated/torch.equal", "type": "Torch", "text": ["True if two tensors have the same size and elements, False otherwise.", "Example:"]}, {"name": "torch.erf", "path": "generated/torch.erf", "type": "Torch", "text": ["Alias for torch.special.erf()."]}, {"name": "torch.erfc", "path": "generated/torch.erfc", "type": "Torch", "text": ["Alias for torch.special.erfc()."]}, {"name": "torch.erfinv", "path": "generated/torch.erfinv", "type": "Torch", "text": ["Alias for torch.special.erfinv()."]}, {"name": "torch.exp", "path": "generated/torch.exp", "type": "Torch", "text": ["Returns a new tensor with the exponential of the elements of the input tensor input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.exp2", "path": "generated/torch.exp2", "type": "Torch", "text": ["Alias for torch.special.exp2()."]}, {"name": "torch.expm1", "path": "generated/torch.expm1", "type": "Torch", "text": ["Alias for torch.special.expm1()."]}, {"name": "torch.export", "path": "export", "type": "Traced Graph Export", "text": ["Warning", "This feature is a prototype under active development and there WILL BE BREAKING CHANGES in the future.", "torch.export.export() takes an arbitrary Python callable (a torch.nn.Module, a function or a method) and produces a traced graph representing only the Tensor computation of the function in an Ahead-of-Time (AOT) fashion, which can subsequently be executed with different outputs or serialized.", "torch.export produces a clean intermediate representation (IR) with the following invariants. More specifications about the IR can be found here (coming soon!).", "Under the hood, torch.export leverages the following latest technologies:", "torch.compile() also utilizes the same PT2 stack as torch.export, but is slightly different:", "Compared to torch.fx.symbolic_trace(), torch.export traces using TorchDynamo which operates at the Python bytecode level, giving it the ability to trace arbitrary Python constructs not limited by what Python operator overloading supports. Additionally, torch.export keeps fine-grained track of tensor metadata, so that conditionals on things like tensor shapes do not fail tracing. In general, torch.export is expected to work on more user programs, and produce lower-level graphs (at the torch.ops.aten operator level). Note that users can still use torch.fx.symbolic_trace() as a preprocessing step before torch.export.", "Compared to torch.jit.script(), torch.export does not capture Python control flow or data structures, but it supports more Python language features than TorchScript (as it is easier to have comprehensive coverage over Python bytecodes). The resulting graphs are simpler and only have straight line control flow (except for explicit control flow operators).", "Compared to torch.jit.trace(), torch.export is sound: it is able to trace code that performs integer computation on sizes and records all of the side-conditions necessary to show that a particular trace is valid for other inputs.", "The main entrypoint is through torch.export.export(), which takes a callable (torch.nn.Module, function, or method) and sample inputs, and captures the computation graph into an torch.export.ExportedProgram. An example:", "Inspecting the ExportedProgram, we can note the following:", "By default torch.export will trace the program assuming all input shapes are static, and specializing the exported program to those dimensions. However, some dimensions, such as a batch dimension, can be dynamic and vary from run to run. Such dimensions must be marked dynamic using the torch.export.dynamic_dim() API, and passed into torch.export.export() through the constraints argument. An example:", "Some additional things to note:", "To save the ExportedProgram, users can use the torch.export.save() and torch.export.load() APIs. A convention is to save the ExportedProgram using a .pt2 file extension.", "An example:", "As mentioned before, by default, torch.export will trace the program specializing on the input tensors\u2019 shapes, unless a dimension is specified as dynamic via the torch.export.dynamic_dim() API. This means that if there exists shape-dependent control flow, torch.export will specialize on the branch that is being taken with the given sample inputs. For example:", "The conditional of (x.shape[0] > 5) does not appear in the ExportedProgram because the example inputs have the static shape of (10, 2). Since torch.export specializes on the inputs\u2019 static shapes, the else branch (x - 1) will never be reached. To preserve the dynamic branching behavior based on the shape of a tensor in the traced graph, torch.export.dynamic_dim() will need to be used to specify the dimension of the input tensor (x.shape[0]) to be dynamic, and the source code will need to be rewritten.", "torch.export also specializes the traced graph based on the values of inputs that are not torch.Tensor, such as int, float, bool, and str. However, we will likely change this in the near future to not specialize on inputs of primitive types.", "For example:", "Because integers are specialized, the torch.ops.aten.add.Tensor operations are all computed with the inlined constant 1, rather than arg1_1. Additionally, the times iterator used in the for loop is also \u201cinlined\u201d in the graph through the 3 repeated torch.ops.aten.add.Tensor calls, and the input arg2_1 is never used.", "As torch.export is a one-shot process for capturing a computation graph from a PyTorch program, it might ultimately run into untraceable parts of programs as it is nearly impossible to support tracing all PyTorch and Python features. In the case of torch.compile, an unsupported operation will cause a \u201cgraph break\u201d and the unsupported operation will be run with default Python evaluation. In contrast, torch.export will require users to provide additional information or rewrite parts of their code to make it traceable. As the tracing is based on TorchDynamo, which evaluates at the Python bytecode level, there will be significantly fewer rewrites required compared to previous tracing frameworks.", "When a graph break is encountered, ExportDB is a great resource for learning about the kinds of programs that are supported and unsupported, along with ways to rewrite programs to make them traceable.", "Graph breaks can also be encountered on data-dependent control flow (if\nx.shape[0] > 2) when shapes are not being specialized, as a tracing compiler cannot possibly deal with without generating code for a combinatorially exploding number of paths. In such cases, users will need to rewrite their code using special control flow operators (coming soon!).", "Data dependent behavior such as using the value inside of a tensor to construct another tensor, or using the value of a tensor to slice into another tensor, is also something the tracer cannot fully determine. Users will need to rewrite their code using the inline constraint APIs torch.export.constrain_as_size() and torch.export.constrain_as_value().", "When tracing, a META implementation (or \u201cmeta kernel\u201d) is required for all operators. This is used to reason about the input/output shapes for this operator.", "Note that the official API for registering custom meta kernels for custom ops is currently undergoing development. While the final API is being refined, you can refer to the documentation here.", "In the unfortunate case where your model uses an ATen operator that is does not have a meta kernel implementation yet, please file an issue.", "Additional Links for Export Users", "Deep Dive for PyTorch Developers", "export() takes an arbitrary Python callable (an nn.Module, a function or a method) and produces a traced graph representing only the Tensor computation of the function in an Ahead-of-Time (AOT) fashion, which can subsequently be executed with different outputs or serialized. The traced graph (1) produces a normalized operator set consisting only of functional Core ATen Operator Set and user specified custom operators, (2) has eliminated all Python control flow and data structures (except for certain conditions), and (3) has the set of shape constraints needed to show that this normalization and control flow elimination is sound for a future input.", "Soundness Guarantee", "While tracing, export() takes note of shape-related assumptions made by the user program and the underlying PyTorch operator kernels. The output ExportedProgram is considered valid only when these assumptions hold true.", "There are 2 types of assumptions made during tracing", "All assumptions must be validated at graph capture time for export() to succeed. Specifically:", "If any assumption can not be validated, a fatal error will be raised. When that happens, the error message will include suggested code needed to construct necessary constraints to validate the assumptions, for example export() would suggest following code for input constraints:", "This example means the program requires the dim 0 of input x to be less than or equal to 5 to be valid. You can inspect the constraints needed and then copy this exact function into your code to generated needed constraints to be passed into constraints argument.", "An ExportedProgram containing the traced callable.", "ExportedProgram", "Acceptable input/output types", "Acceptable types of inputs (for args and kwargs) and outputs include:", "dynamic_dim() constructs a Constraint object that describes the dynamism of a dimension index of tensor t. Constraint objects should be passed to constraints argument of export().", "A Constraint object that describes shape dynamism. It can be passed to export() so that export() does not assume static size of specified tensor, i.e. keeping it dynamic as a symbolic size rather than specializing according to size of example tracing input.", "Specifically dynamic_dim() can be used to express following types of dynamism.", "Size of a dimension is dynamic and unbounded:", "Size of a dimension is dynamic with a lower bound:", "Size of a dimension is dynamic with an upper bound:", "Size of a dimension is dynamic and it is always equal to size of another dynamic dimension:", "Hint export() about the constraint of an intermediate scalar value that represents shape of a tensor so that subsequent tensor constructors can be traced correctly because many operators need to make assumption about range of sizes.", "None", "For example, following program can not be traced soundly wihout using constrain_as_size() to give export() a hint about shape ranges:", "export() would give following error:", "Assuming the actual range of d can be between [3, 10], you can add a call to constrain_as_size() in the source code like this:", "With the additional hint, export() would be able to trace the program correctly by taking the else branch, resulting in following graph:", "Warning", "if your size is intended to be dynamic, do NOT test if sizes are equal to 0 or 1, these will SILENTLY report false and be bypassed", "Hint export() about the constraint of an intermediate scalar value so that subsequent branching behaviors that check on the range of aforementioned scalar value can be soundly traced.", "Warning", "(Note that if the intermediate scalar value will be used like a size, including being passed as size arg to a tensor factory or view, call constrain_as_size() instead.)", "None", "For example, following program can not be traced soundly:", "v is a data-dependent value, which is assumed to have a range of (-inf, inf). export() a hint about which branch to take would not be able to determine if the traced branching decision is correct or not. Thus export() would give following error:", "Assuming the actual range of v can be between [10, 200], you can add a call to constrain_as_value() in the source code like this:", "With the additional hint, export() would be able to trace the program correctly by taking the else branch, resulting in following graph:", "Warning", "Under active development, saved files may not be usable in newer versions of PyTorch.", "Saves an ExportedProgram to a file-like object. It can then be loaded using the Python API torch.export.load.", "Example:", "Warning", "Under active development, saved files may not be usable in newer versions of PyTorch.", "Loads an ExportedProgram previously saved with torch.export.save.", "An ExportedProgram object", "ExportedProgram", "Example:", "Warning", "Do not construct Constraint directly, use dynamic_dim() instead.", "This represents constraints on input tensor dimensions, e.g., requiring them to be fully polymorphic or within some range.", "Package of a program from export(). It contains an torch.fx.Graph that represents Tensor computation, a state_dict containing tensor values of all lifted parameters and buffers, and various metadata.", "You can call an ExportedProgram like the original callable traced by export() with the same calling convention.", "To perform transformations on the graph, use .module property to access an torch.fx.GraphModule. You can then use FX transformation to rewrite the graph. Afterwards, you can simply use export() again to construct a correct ExportedProgram.", "Returns a self contained GraphModule with all the parameters/buffers inlined.", "ExportGraphSignature models the input/output signature of Export Graph, which is a fx.Graph with stronger invariants gurantees.", "Export Graph is functional and does not access \u201cstates\u201d like parameters or buffers within the graph via getattr nodes. Instead, export() gurantees that parameters and buffers are lifted out of the graph as inputs. Similarly, any mutations to buffers are not included in the graph either, instead the updated values of mutated buffers are modeled as additional outputs of Export Graph.", "The ordering of all inputs and outputs are:", "e.g. If following module is exported:", "Resulting Graph would be:", "Resulting ExportGraphSignature would be:", "An enumeration."]}, {"name": "torch.export.ArgumentKind", "path": "export#torch.export.ArgumentKind", "type": "Traced Graph Export", "text": ["An enumeration."]}, {"name": "torch.export.ArgumentSpec", "path": "export#torch.export.ArgumentSpec", "type": "Traced Graph Export", "text": []}, {"name": "torch.export.constrain_as_size()", "path": "export#torch.export.constrain_as_size", "type": "Traced Graph Export", "text": ["Hint export() about the constraint of an intermediate scalar value that represents shape of a tensor so that subsequent tensor constructors can be traced correctly because many operators need to make assumption about range of sizes.", "None", "For example, following program can not be traced soundly wihout using constrain_as_size() to give export() a hint about shape ranges:", "export() would give following error:", "Assuming the actual range of d can be between [3, 10], you can add a call to constrain_as_size() in the source code like this:", "With the additional hint, export() would be able to trace the program correctly by taking the else branch, resulting in following graph:", "Warning", "if your size is intended to be dynamic, do NOT test if sizes are equal to 0 or 1, these will SILENTLY report false and be bypassed"]}, {"name": "torch.export.constrain_as_value()", "path": "export#torch.export.constrain_as_value", "type": "Traced Graph Export", "text": ["Hint export() about the constraint of an intermediate scalar value so that subsequent branching behaviors that check on the range of aforementioned scalar value can be soundly traced.", "Warning", "(Note that if the intermediate scalar value will be used like a size, including being passed as size arg to a tensor factory or view, call constrain_as_size() instead.)", "None", "For example, following program can not be traced soundly:", "v is a data-dependent value, which is assumed to have a range of (-inf, inf). export() a hint about which branch to take would not be able to determine if the traced branching decision is correct or not. Thus export() would give following error:", "Assuming the actual range of v can be between [10, 200], you can add a call to constrain_as_value() in the source code like this:", "With the additional hint, export() would be able to trace the program correctly by taking the else branch, resulting in following graph:"]}, {"name": "torch.export.Constraint", "path": "export#torch.export.Constraint", "type": "Traced Graph Export", "text": ["Warning", "Do not construct Constraint directly, use dynamic_dim() instead.", "This represents constraints on input tensor dimensions, e.g., requiring them to be fully polymorphic or within some range."]}, {"name": "torch.export.Dynamic shapes", "path": "torch.compiler_dynamic_shapes", "type": "Traced Graph Export", "text": ["Code: symbolic_shapes.py", "See also: The dynamic shapes manual", "Deep learning compilers commonly only work for static shapes, that is to say, they produced compiled programs which only work for a single specific configuration of input shapes, and must recompile if any input shape changes. This assumption works great for the majority of commonly run deep learning models today, but there are a few situations where it is insufficient:", "In supporting dynamic shapes, we chose not to support dynamic rank programs, e.g., programs whose inputs tensors change in dimensionality, as this pattern rarely occurs in real-world deep learning programs, and it avoids the need to reason inductively over symbolic lists of shapes.", "The default dynamic behavior in PyTorch 2.1 is:", "When considering how to add support for dynamic shapes to TorchDynamo and TorchInductor, we made a major design decision: in order to reuse decompositions and other preexisting code written in Python/C++ targeting the PyTorch API, we must be able to trace through dynamic shapes. Unlike a fully symbolic system which might capture both branches of a conditional, we always pick one branch and specialize our trace under the assumption that we only use this trace when we would have made the same choice for that branch in the future. To do this, we maintain a \u201chint\u201d for every symbolic size saying what its concrete value is at compile time (as TorchDynamo is a just-in-time compiler, it always knows what the actual input sizes are.) When we perform a condition on a tensor, we simply consult the hint to find out which branch to take.", "This greatly simplifies the symbolic shape formulas we produce, but means we have a much more involved system for managing guards. Consider, for example, the following program:", "The final IR we will compile with TorchInductor will either be torch.cat([x, y]).add(2) or torch.cat([x, y]).mul(2) (with the condition flattened away), but to determine which branch we are in, we would need to know the size of z, an intermediate. Because TorchDynamo must know upfront if a compiled trace is valid (we do not support bailouts, like some JIT compilers), we must be able to reduce z.size(0) as an expression in terms of the inputs, x.size(0) + y.size(0). This is done by writing meta functions for all operators in PyTorch which can propagate size information to the output of a tensor without actually performing computation on the node.", "Symbolic shapes workflow:", "Important files:", "Understanding the Python class hierarchy:", "C++ is fairly similar:", "When you write code that is traceable with make_fx, it must be able to deal with SymInt/SymFloat/SymBool flowing through it. The dynamic shapes manual gives some guidance for how to do this.", "Symbolic reasoning:", "To resolve control flow, we check the hint, aka actual value, of a symbolic integer to determine which branch to go. However, in some cases, we may not have a hint: so-called unbacked symbolic integers arise when a size variable emerges from a data-dependent operation like .nonzero() or .item(). It is illegal to perform control flow on these symbolic integers, so we must graph break on these operations.", "Naively implemented, this is too restrictive: most PyTorch programs will immediately fail if you try to do anything with unbacked symbolic integers. Here are the most important enhancements to make this actually work:", "In future versions of PT2 (beyond PT2.1), we will extend our reasoning system to infer that an unbacked symbolic integer is size-like based on usage. For example, if you pass the result of an .item() call to a factory function like torch.empty, we will automatically infer that the result is a size (because if it was not, it would fail.) This assumption would get validated at runtime, raising an error if it was not fulfilled."]}, {"name": "torch.export.dynamic_dim()", "path": "export#torch.export.dynamic_dim", "type": "Traced Graph Export", "text": ["dynamic_dim() constructs a Constraint object that describes the dynamism of a dimension index of tensor t. Constraint objects should be passed to constraints argument of export().", "A Constraint object that describes shape dynamism. It can be passed to export() so that export() does not assume static size of specified tensor, i.e. keeping it dynamic as a symbolic size rather than specializing according to size of example tracing input.", "Specifically dynamic_dim() can be used to express following types of dynamism.", "Size of a dimension is dynamic and unbounded:", "Size of a dimension is dynamic with a lower bound:", "Size of a dimension is dynamic with an upper bound:", "Size of a dimension is dynamic and it is always equal to size of another dynamic dimension:"]}, {"name": "torch.export.export()", "path": "export#torch.export.export", "type": "Traced Graph Export", "text": ["export() takes an arbitrary Python callable (an nn.Module, a function or a method) and produces a traced graph representing only the Tensor computation of the function in an Ahead-of-Time (AOT) fashion, which can subsequently be executed with different outputs or serialized. The traced graph (1) produces a normalized operator set consisting only of functional Core ATen Operator Set and user specified custom operators, (2) has eliminated all Python control flow and data structures (except for certain conditions), and (3) has the set of shape constraints needed to show that this normalization and control flow elimination is sound for a future input.", "Soundness Guarantee", "While tracing, export() takes note of shape-related assumptions made by the user program and the underlying PyTorch operator kernels. The output ExportedProgram is considered valid only when these assumptions hold true.", "There are 2 types of assumptions made during tracing", "All assumptions must be validated at graph capture time for export() to succeed. Specifically:", "If any assumption can not be validated, a fatal error will be raised. When that happens, the error message will include suggested code needed to construct necessary constraints to validate the assumptions, for example export() would suggest following code for input constraints:", "This example means the program requires the dim 0 of input x to be less than or equal to 5 to be valid. You can inspect the constraints needed and then copy this exact function into your code to generated needed constraints to be passed into constraints argument.", "An ExportedProgram containing the traced callable.", "ExportedProgram", "Acceptable input/output types", "Acceptable types of inputs (for args and kwargs) and outputs include:"]}, {"name": "torch.export.ExportBackwardSignature", "path": "export#torch.export.ExportBackwardSignature", "type": "Traced Graph Export", "text": []}, {"name": "torch.export.ExportDB", "path": "generated/exportdb/index", "type": "Traced Graph Export", "text": ["ExportDB is a centralized dataset of supported and unsupported export cases. It is targeted towards users who want to understand specifically what types of code are supported, the subtleties of export, and how to modify their existing code to be compatible with export. Note that this is not an exhaustive set of everything that is supported by exportdb, but it covers the most common and confusing use cases that users will run into.", "If you have a feature that you think needs a stronger guarantee from us to support in export please create an issue in the pytorch/pytorch repo wih a module:export tag.", "Tags", "Note", "Tags: torch.escape-hatch", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags:", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags:", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.closure, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.escape-hatch, torch.dynamic-value", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.escape-hatch, torch.dynamic-value", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags:", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.data-structure", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.assert", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, python.control-flow", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.map, torch.dynamic-shape", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.data-structure", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, python.assert, python.data-structure", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.data-structure, python.control-flow", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.closure", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.context-manager", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags:", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags:", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.control-flow", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.control-flow", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.builtin", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, python.builtin", "Support Level: NOT_SUPPORTED_YET", "Original source code:", "Result:", "Note", "Tags: python.builtin", "Support Level: NOT_SUPPORTED_YET", "Original source code:", "Result:", "You can rewrite the example above to something like the following:"]}, {"name": "torch.export.ExportDB.python.assert", "path": "generated/exportdb/python.assert", "type": "Traced Graph Export", "text": ["Note", "Tags: python.assert", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, python.assert, python.data-structure", "Support Level: SUPPORTED", "Original source code:", "Result:"]}, {"name": "torch.export.ExportDB.python.builtin", "path": "generated/exportdb/python.builtin", "type": "Traced Graph Export", "text": ["Note", "Tags: torch.dynamic-shape, python.builtin", "Support Level: NOT_SUPPORTED_YET", "Original source code:", "Result:", "Note", "Tags: python.builtin", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.builtin", "Support Level: NOT_SUPPORTED_YET", "Original source code:", "Result:", "You can rewrite the example above to something like the following:"]}, {"name": "torch.export.ExportDB.python.closure", "path": "generated/exportdb/python.closure", "type": "Traced Graph Export", "text": ["Note", "Tags: python.closure, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.closure", "Support Level: SUPPORTED", "Original source code:", "Result:"]}, {"name": "torch.export.ExportDB.python.context-manager", "path": "generated/exportdb/python.context-manager", "type": "Traced Graph Export", "text": ["Note", "Tags: python.context-manager", "Support Level: SUPPORTED", "Original source code:", "Result:"]}, {"name": "torch.export.ExportDB.python.control-flow", "path": "generated/exportdb/python.control-flow", "type": "Traced Graph Export", "text": ["Note", "Tags: torch.dynamic-shape, python.control-flow", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.data-structure, python.control-flow", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.control-flow", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.control-flow", "Support Level: SUPPORTED", "Original source code:", "Result:"]}, {"name": "torch.export.ExportDB.python.data-structure", "path": "generated/exportdb/python.data-structure", "type": "Traced Graph Export", "text": ["Note", "Tags: python.data-structure", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.data-structure", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, python.assert, python.data-structure", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.data-structure, python.control-flow", "Support Level: SUPPORTED", "Original source code:", "Result:"]}, {"name": "torch.export.ExportDB.torch.cond", "path": "generated/exportdb/torch.cond", "type": "Traced Graph Export", "text": ["Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.closure, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:"]}, {"name": "torch.export.ExportDB.torch.dynamic-shape", "path": "generated/exportdb/torch.dynamic-shape", "type": "Traced Graph Export", "text": ["Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, python.control-flow", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.map, torch.dynamic-shape", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, python.builtin", "Support Level: NOT_SUPPORTED_YET", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, python.assert, python.data-structure", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape", "Support Level: SUPPORTED", "Original source code:", "Result:"]}, {"name": "torch.export.ExportDB.torch.dynamic-value", "path": "generated/exportdb/torch.dynamic-value", "type": "Traced Graph Export", "text": ["Note", "Tags: torch.escape-hatch, torch.dynamic-value", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.escape-hatch, torch.dynamic-value", "Support Level: SUPPORTED", "Original source code:", "Result:"]}, {"name": "torch.export.ExportDB.torch.escape-hatch", "path": "generated/exportdb/torch.escape-hatch", "type": "Traced Graph Export", "text": ["Note", "Tags: torch.escape-hatch", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.escape-hatch, torch.dynamic-value", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.escape-hatch, torch.dynamic-value", "Support Level: SUPPORTED", "Original source code:", "Result:"]}, {"name": "torch.export.ExportDB.torch.map", "path": "generated/exportdb/torch.map", "type": "Traced Graph Export", "text": ["Note", "Tags: torch.map, torch.dynamic-shape", "Support Level: SUPPORTED", "Original source code:", "Result:"]}, {"name": "torch.export.ExportedProgram", "path": "export#torch.export.ExportedProgram", "type": "Traced Graph Export", "text": ["Package of a program from export(). It contains an torch.fx.Graph that represents Tensor computation, a state_dict containing tensor values of all lifted parameters and buffers, and various metadata.", "You can call an ExportedProgram like the original callable traced by export() with the same calling convention.", "To perform transformations on the graph, use .module property to access an torch.fx.GraphModule. You can then use FX transformation to rewrite the graph. Afterwards, you can simply use export() again to construct a correct ExportedProgram.", "Returns a self contained GraphModule with all the parameters/buffers inlined."]}, {"name": "torch.export.ExportedProgram.module()", "path": "export#torch.export.ExportedProgram.module", "type": "Traced Graph Export", "text": ["Returns a self contained GraphModule with all the parameters/buffers inlined."]}, {"name": "torch.export.ExportGraphSignature", "path": "export#torch.export.ExportGraphSignature", "type": "Traced Graph Export", "text": ["ExportGraphSignature models the input/output signature of Export Graph, which is a fx.Graph with stronger invariants gurantees.", "Export Graph is functional and does not access \u201cstates\u201d like parameters or buffers within the graph via getattr nodes. Instead, export() gurantees that parameters and buffers are lifted out of the graph as inputs. Similarly, any mutations to buffers are not included in the graph either, instead the updated values of mutated buffers are modeled as additional outputs of Export Graph.", "The ordering of all inputs and outputs are:", "e.g. If following module is exported:", "Resulting Graph would be:", "Resulting ExportGraphSignature would be:"]}, {"name": "torch.export.Fake tensor", "path": "torch.compiler_fake_tensor", "type": "Traced Graph Export", "text": ["Code: fake_tensor.py", "When doing Dynamo symbolic evaluation and compiler passes, we often want to be able to run tensor operations to understand what output sizes/dtypes/devices are, without actually running those operations (or trashing preexisting tensors), which would be slower (if you\u2019re doing a lot of compute) and take a lot of memory (it\u2019s bad if your compiler needs to use GPU memory while you are compiling the program). A fake tensor is like a real tensor in all respects, except that it doesn\u2019t actually have any data. For example, when we do Dynamo tracing, we need to trace through user Tensor code and answer questions about intermediates (e.g., if a user does a conditional on an intermediate tensor). Without fake tensor, we would not have accurate information for these queries.", "Similarly, suppose you want to store metadata for a tensor, e.g., on an FX IR node (meta[\u2018val\u2019]). You can instead store a fake tensor directly on the node, which will give you all the metadata you need for the tensor, including subtle stuff that you probably wouldn\u2019t have handled (e.g., aliasing relationships).", "All fake tensors are associated with a FakeTensorMode. Because fake tensor\u2019s primary use case is to do analysis on real tensors, the general workflow is you have a bunch of real tensors, you allocate a FakeTensorMode, and then you use from_real_tensor to convert all those real tensors into fake tensors, and then you do things to the fake tensors. In particular, the FakeTensorMode maintains a memo table persistently mapping tensors (and storages) to the same storages. If you fakeify the same tensor multiple times, you will get the same fake tensor; if you fakeify two tensors which alias each other, you will get two fake tensors which alias the same fake storage. FakeTensors are tensor subclasses, so if you do operations on them, you\u2019ll automatically get a fake tensor, but in general you will want to do operations on fake tensors (e.g., if you\u2019re running an FX pass) with the FakeTensorMode active; what a tensor operation will do is automatically turn on the fake tensor mode and try again.", "A fake tensor is represented as a __torch_dispatch__ tensor subclass of a meta tensor. This means under the hood, fake tensors are meta device tensors; they then use extra extensibility hooks, specifically dispatch_device, to lie about what the actual device of the tensor is. This was one of the more error-prone parts of fake tensors in the early days: sometimes, fake tensors were too good at lying about being CPU/CUDA whatever, and you\u2019d end up with a CPU kernel getting called with a fake tensor trying to dereference the data pointer, which obviously won\u2019t work. If you are segfaulting in fake tensor code, this is the first thing you should check: is the C++ backtrace in a CPU kernel (unexpected!) or a meta kernel (expected!) A meta kernel is like a real kernel, but all it does is allocate the outputs, it doesn\u2019t do any data compute.", "A tensor subclass has to define how to implement various operations. Here is the general fake tensor recipe:", "Non-PT2 usage (check out test/test_fake_tensor.py for more examples):", "Q: Why do you have real tensors as inputs?", "A: In a PT2 context, this is because you typically are compiling just-in-time, so for all the inputs to a graph you\u2019re compiling, you already have the \u201creal\u201d inputs, because you\u2019re compiling while you\u2019re executing the program.", "PT2 pre-AOTAutograd usage (this is unusual, you probably don\u2019t want to do this):", "detect_fake_mode will search a number of locations to try to find \u201cthe\u201d fake tensor mode associated with the lifecycle. Typically it will be pulled off of the tracing context.", "PT2 post-AOTAutograd usage:", "# Fake mode is enabled! example_inputs is typically fake already # TODO: we probably want to change this # Still do this to access fake mode fake_mode = detect_fake_mode(example_inputs) # But in general you don\u2019t have to turn it on", "Other useful stuff:", "When might you want to disable fake tensor mode? Usually you don\u2019t want to do this. One niche case where we\u2019ve found it useful is to implement constant propagation on fake tensors: in this case, we need to do some actual tensor computation even though we\u2019re in a fake tensor mode.", "Auto-convert or not? Originally, FakeTensorMode would not automatically fakeify real tensors if you tried to do compute on them inside a FakeTensorMode region. The motivation behind this was to prevent the following footgun:", "What should this code do? It would be surprising if we actually modified the metadata on the real tensor. But at the same time, there isn\u2019t any obvious opportunity to create a FakeTensor. So we conservatively decided to make this raise an error: \u201cInvoking operators with non-Fake Tensor inputs in FakeTensorMode is not yet supported. Please convert all Tensors to FakeTensors first.\u201d", "This error is pretty annoying in practice. For example, suppose you have a real nn.Module and you want to feed fake tensors through it. You need to somehow fakeify the nn.Module. This motivated FakeCopyMode.", "Eventually, we gave up and added automatic fakeification. However, this is still not yet enabled by default in many uses of FakeTensorMode.", "Metadata mutation on fake tensor If you have a fake tensor, and you t_() it, the metadata on the fake tensor changes. This is reasonable on its face, but sometimes you want to also store fake tensors as metadata on FX nodes; mutating a fake tensor is bad because this will invalidate old metadata!", "In fact, there is a fundamental tension here, which is that fake tensors maintain extremely accurate metadata about tensors, up to and including object identity. If object metadata changes over time in an FX graph, there is not actually any way to represent this change over time. Most of the time, our serious FX analyses are done on functionalized graphs, which don\u2019t have this, but occasionally you need to do an analysis on a non-functionalized graph. Maybe it was a mistake to put fake tensor in meta[\u2018val\u2019]", "Fake tensor uses both a subclass and a mode tensor subclass pattern, where FakeTensor.__torch_dispatch__ enables the FakeTensorMode associated with the fake tensor, and then redispatches (relying on FakeTensorMode to do the heavy lifting). If fake tensor operations get a subclass argument it doesn\u2019t recognize, it will return NotImplemented, giving the other subclass a chance to run first (hopefully desugaring into plain tensor operations), before it tries again. This can cause infinite loops.", "Unfortunately, there is a pretty complicated set of places where any given operator may be implemented. Some important cases to know about:", "Because fake tensors are used in situations that are very sensitive to the exact properties of a tensor, fake tensors do conversion very carefully, preserving leaf-ness, requires_grad\u2019ness, aliasing, and a whole host of other properties. The bulk of the heavy lifting is in MetaConverter.", "You would think fake tensors are fast because they don\u2019t do any tensor compute. But at small tensor sizes we are actually entirely overhead bound, and, well, fake tensor is in Python, and we often do a LOT of work to do a single tensor operation (because they are implemented as decompositions). So fake tensors are actually pretty slow in practice, especially when symbolic shapes are involved. There are two important fastpaths we currently have in fake tensor that make a big difference in practice:", "There is interest in sending fake tensors as user inputs into the PT2 stack, which would imply we would need to be able to create a fake tensor of a fake tensor. This isn\u2019t really supported right now, but maybe it would not be too difficult to do.", "Every FakeTensorMode contains a ShapeEnv, which tracks all symbolic shapes information. Their lifetimes are typically tied: they live and die together.", "Because FakeTensorMode has a ShapeEnv (but meta implementations do not), meta functions that are data-dependent and require allocating an unbacked SymInt live in fake tensor. Fake tensor also takes care of memoizing unbacked SymInts, so that, e.g., if you call nonzero() on the same fake tensor twice, you get the same symbolic size.", "Colab Tutorial On Using FakeTensor To Determine Max Batch Size"]}, {"name": "torch.export.IRs", "path": "torch.compiler_ir", "type": "Traced Graph Export", "text": ["PyTorch 2.0 offers two set of IRs for backends to interface with: Core Aten IR and Prims IR.", "Core aten ops is the core subset of aten operators that can be used to compose other operators. Core aten IR is fully functional, and there is no inplace or _out variants in this opset. In contrast to Prims IR, core aten ops reuses the existing aten ops in \u201cnative_functions.yaml\u201d, and it doesn\u2019t further decompose ops into explicit type promotion and broadcasting ops. This opset is designed to serve as the functional IR to interface with backends.", "Warning", "This opset is still under active development, more ops will be added in the future.", "Operator", "Schema", "aten._adaptive_avg_pool2d", "_adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor", "aten._adaptive_avg_pool2d_backward", "_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor", "aten._adaptive_avg_pool3d", "_adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -> Tensor", "aten._cdist_forward", "_cdist_forward(Tensor x1, Tensor x2, float p, int? compute_mode) -> Tensor", "aten._embedding_bag", "_embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)", "aten._local_scalar_dense", "_local_scalar_dense(Tensor self) -> Scalar", "aten._log_softmax", "_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor", "aten._native_batch_norm_legit", "_native_batch_norm_legit(Tensor input, Tensor? weight, Tensor? bias, Tensor(a!) running_mean, Tensor(b!) running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)", "aten._native_batch_norm_legit.no_stats", "_native_batch_norm_legit.no_stats(Tensor input, Tensor? weight, Tensor? bias, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)", "aten._native_batch_norm_legit_no_training", "_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)", "aten._pdist_forward", "_pdist_forward(Tensor self, float p=2) -> Tensor", "aten._softmax", "_softmax(Tensor self, int dim, bool half_to_float) -> Tensor", "aten._to_copy", "_to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -> Tensor", "aten.abs", "abs(Tensor self) -> Tensor", "aten.acos", "acos(Tensor self) -> Tensor", "aten.acosh", "acosh(Tensor self) -> Tensor", "aten.adaptive_avg_pool1d", "adaptive_avg_pool1d(Tensor self, int[1] output_size) -> Tensor", "aten.add.Scalar", "add.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor", "aten.add.Tensor", "add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor", "aten.addmm", "addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor", "aten.alias", "alias(Tensor(a) self) -> Tensor(a)", "aten.amax", "amax(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor", "aten.amin", "amin(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor", "aten.any", "any(Tensor self) -> Tensor", "aten.any.dim", "any.dim(Tensor self, int dim, bool keepdim=False) -> Tensor", "aten.arange.start_step", "arange.start_step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", "aten.argmax", "argmax(Tensor self, int? dim=None, bool keepdim=False) -> Tensor", "aten.argmin", "argmin(Tensor self, int? dim=None, bool keepdim=False) -> Tensor", "aten.as_strided", "as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)", "aten.asin", "asin(Tensor self) -> Tensor", "aten.asinh", "asinh(Tensor self) -> Tensor", "aten.atan", "atan(Tensor self) -> Tensor", "aten.atanh", "atanh(Tensor self) -> Tensor", "aten.avg_pool1d", "avg_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, bool ceil_mode=False, bool count_include_pad=True) -> Tensor", "aten.avg_pool2d", "avg_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor", "aten.avg_pool2d_backward", "avg_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor", "aten.avg_pool3d", "avg_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor", "aten.bitwise_and.Scalar", "bitwise_and.Scalar(Tensor self, Scalar other) -> Tensor", "aten.bitwise_and.Tensor", "bitwise_and.Tensor(Tensor self, Tensor other) -> Tensor", "aten.bitwise_not", "bitwise_not(Tensor self) -> Tensor", "aten.bitwise_or.Scalar", "bitwise_or.Scalar(Tensor self, Scalar other) -> Tensor", "aten.bitwise_or.Tensor", "bitwise_or.Tensor(Tensor self, Tensor other) -> Tensor", "aten.bitwise_xor.Scalar", "bitwise_xor.Scalar(Tensor self, Scalar other) -> Tensor", "aten.bitwise_xor.Tensor", "bitwise_xor.Tensor(Tensor self, Tensor other) -> Tensor", "aten.bmm", "bmm(Tensor self, Tensor mat2) -> Tensor", "aten.cat", "cat(Tensor[] tensors, int dim=0) -> Tensor", "aten.ceil", "ceil(Tensor self) -> Tensor", "aten.clamp", "clamp(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor", "aten.clamp.Tensor", "clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor", "aten.clone", "clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor", "aten.col2im", "col2im(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor", "aten.constant_pad_nd", "constant_pad_nd(Tensor self, SymInt[] pad, Scalar value=0) -> Tensor", "aten.convolution", "convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups) -> Tensor", "aten.convolution_backward", "convolution_backward(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)", "aten.cos", "cos(Tensor self) -> Tensor", "aten.cosh", "cosh(Tensor self) -> Tensor", "aten.cumsum", "cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor", "aten.div.Scalar", "div.Scalar(Tensor self, Scalar other) -> Tensor", "aten.div.Tensor", "div.Tensor(Tensor self, Tensor other) -> Tensor", "aten.embedding", "embedding(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -> Tensor", "aten.embedding_dense_backward", "embedding_dense_backward(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq) -> Tensor", "aten.empty.memory_format", "empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor", "aten.empty_strided", "empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", "aten.eq.Scalar", "eq.Scalar(Tensor self, Scalar other) -> Tensor", "aten.eq.Tensor", "eq.Tensor(Tensor self, Tensor other) -> Tensor", "aten.erf", "erf(Tensor self) -> Tensor", "aten.exp", "exp(Tensor self) -> Tensor", "aten.expand", "expand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -> Tensor(a)", "aten.fill.Scalar", "fill.Scalar(Tensor self, Scalar value) -> Tensor", "aten.flip", "flip(Tensor self, int[] dims) -> Tensor", "aten.floor", "floor(Tensor self) -> Tensor", "aten.fmod.Scalar", "fmod.Scalar(Tensor self, Scalar other) -> Tensor", "aten.fmod.Tensor", "fmod.Tensor(Tensor self, Tensor other) -> Tensor", "aten.full", "full(SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", "aten.gather", "gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor", "aten.ge.Scalar", "ge.Scalar(Tensor self, Scalar other) -> Tensor", "aten.ge.Tensor", "ge.Tensor(Tensor self, Tensor other) -> Tensor", "aten.gelu", "gelu(Tensor self, *, str approximate=\u2019none\u2019) -> Tensor", "aten.grid_sampler_2d", "grid_sampler_2d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor", "aten.gt.Scalar", "gt.Scalar(Tensor self, Scalar other) -> Tensor", "aten.gt.Tensor", "gt.Tensor(Tensor self, Tensor other) -> Tensor", "aten.hardtanh", "hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor", "aten.index.Tensor", "index.Tensor(Tensor self, Tensor?[] indices) -> Tensor", "aten.index_put", "index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor", "aten.index_select", "index_select(Tensor self, int dim, Tensor index) -> Tensor", "aten.isinf", "isinf(Tensor self) -> Tensor", "aten.isnan", "isnan(Tensor self) -> Tensor", "aten.le.Scalar", "le.Scalar(Tensor self, Scalar other) -> Tensor", "aten.le.Tensor", "le.Tensor(Tensor self, Tensor other) -> Tensor", "aten.leaky_relu", "leaky_relu(Tensor self, Scalar negative_slope=0.01) -> Tensor", "aten.log", "log(Tensor self) -> Tensor", "aten.logical_and", "logical_and(Tensor self, Tensor other) -> Tensor", "aten.logical_not", "logical_not(Tensor self) -> Tensor", "aten.logical_or", "logical_or(Tensor self, Tensor other) -> Tensor", "aten.logical_xor", "logical_xor(Tensor self, Tensor other) -> Tensor", "aten.lt.Scalar", "lt.Scalar(Tensor self, Scalar other) -> Tensor", "aten.lt.Tensor", "lt.Tensor(Tensor self, Tensor other) -> Tensor", "aten.max.dim", "max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)", "aten.max_pool2d_with_indices", "max_pool2d_with_indices(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)", "aten.max_pool2d_with_indices_backward", "max_pool2d_with_indices_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices) -> Tensor", "aten.max_pool3d_with_indices", "max_pool3d_with_indices(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)", "aten.maximum", "maximum(Tensor self, Tensor other) -> Tensor", "aten.mean", "mean(Tensor self, *, ScalarType? dtype=None) -> Tensor", "aten.mean.dim", "mean.dim(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor", "aten.min.dim", "min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)", "aten.minimum", "minimum(Tensor self, Tensor other) -> Tensor", "aten.mm", "mm(Tensor self, Tensor mat2) -> Tensor", "aten.mul.Scalar", "mul.Scalar(Tensor self, Scalar other) -> Tensor", "aten.mul.Tensor", "mul.Tensor(Tensor self, Tensor other) -> Tensor", "aten.native_dropout", "native_dropout(Tensor input, float p, bool? train) -> (Tensor, Tensor)", "aten.native_group_norm", "native_group_norm(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps) -> (Tensor, Tensor, Tensor)", "aten.native_group_norm_backward", "native_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask) -> (Tensor, Tensor, Tensor)", "aten.native_layer_norm", "native_layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)", "aten.native_layer_norm_backward", "native_layer_norm_backward(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -> (Tensor, Tensor, Tensor)", "aten.ne.Scalar", "ne.Scalar(Tensor self, Scalar other) -> Tensor", "aten.ne.Tensor", "ne.Tensor(Tensor self, Tensor other) -> Tensor", "aten.neg", "neg(Tensor self) -> Tensor", "aten.nonzero", "nonzero(Tensor self) -> Tensor", "aten.permute", "permute(Tensor(a) self, int[] dims) -> Tensor(a)", "aten.pixel_shuffle", "pixel_shuffle(Tensor self, int upscale_factor) -> Tensor", "aten.pow.Tensor_Scalar", "pow.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor", "aten.pow.Tensor_Tensor", "pow.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor", "aten.prod", "prod(Tensor self, *, ScalarType? dtype=None) -> Tensor", "aten.prod.dim_int", "prod.dim_int(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor", "aten.rand", "rand(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", "aten.randn", "randn(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", "aten.randperm", "randperm(SymInt n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", "aten.reciprocal", "reciprocal(Tensor self) -> Tensor", "aten.reflection_pad1d", "reflection_pad1d(Tensor self, SymInt[2] padding) -> Tensor", "aten.reflection_pad2d", "reflection_pad2d(Tensor self, SymInt[4] padding) -> Tensor", "aten.reflection_pad3d", "reflection_pad3d(Tensor self, SymInt[6] padding) -> Tensor", "aten.relu", "relu(Tensor self) -> Tensor", "aten.remainder.Scalar", "remainder.Scalar(Tensor self, Scalar other) -> Tensor", "aten.remainder.Tensor", "remainder.Tensor(Tensor self, Tensor other) -> Tensor", "aten.repeat", "repeat(Tensor self, SymInt[] repeats) -> Tensor", "aten.replication_pad2d", "replication_pad2d(Tensor self, SymInt[4] padding) -> Tensor", "aten.replication_pad3d", "replication_pad3d(Tensor self, SymInt[6] padding) -> Tensor", "aten.roll", "roll(Tensor self, SymInt[1] shifts, int[1] dims=[]) -> Tensor", "aten.round", "round(Tensor self) -> Tensor", "aten.rsqrt", "rsqrt(Tensor self) -> Tensor", "aten.scalar_tensor", "scalar_tensor(Scalar s, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", "aten.scatter.src", "scatter.src(Tensor self, int dim, Tensor index, Tensor src) -> Tensor", "aten.scatter.value", "scatter.value(Tensor self, int dim, Tensor index, Scalar value) -> Tensor", "aten.scatter_add", "scatter_add(Tensor self, int dim, Tensor index, Tensor src) -> Tensor", "aten.scatter_reduce.two", "scatter_reduce.two(Tensor self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True) -> Tensor", "aten.select.int", "select.int(Tensor(a) self, int dim, SymInt index) -> Tensor(a)", "aten.select_scatter", "select_scatter(Tensor self, Tensor src, int dim, SymInt index) -> Tensor", "aten.sigmoid", "sigmoid(Tensor self) -> Tensor", "aten.sign", "sign(Tensor self) -> Tensor", "aten.sin", "sin(Tensor self) -> Tensor", "aten.sinh", "sinh(Tensor self) -> Tensor", "aten.slice.Tensor", "slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)", "aten.slice_scatter", "slice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor", "aten.sort", "sort(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)", "aten.split_with_sizes", "split_with_sizes(Tensor(a -> *) self, SymInt[] split_sizes, int dim=0) -> Tensor(a)[]", "aten.sqrt", "sqrt(Tensor self) -> Tensor", "aten.squeeze.dim", "squeeze.dim(Tensor(a) self, int dim) -> Tensor(a)", "aten.squeeze.dims", "squeeze.dims(Tensor(a) self, int[] dim) -> Tensor(a)", "aten.sub.Scalar", "sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor", "aten.sub.Tensor", "sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor", "aten.sum.dim_IntList", "sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor", "aten.sym_numel", "sym_numel(Tensor self) -> SymInt", "aten.sym_size.int", "sym_size.int(Tensor self, int dim) -> SymInt", "aten.sym_storage_offset", "sym_storage_offset(Tensor self) -> SymInt", "aten.sym_stride.int", "sym_stride.int(Tensor self, int dim) -> SymInt", "aten.tan", "tan(Tensor self) -> Tensor", "aten.tanh", "tanh(Tensor self) -> Tensor", "aten.topk", "topk(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)", "aten.unsqueeze", "unsqueeze(Tensor(a) self, int dim) -> Tensor(a)", "aten.upsample_bilinear2d.vec", "upsample_bilinear2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor", "aten.upsample_nearest2d.vec", "upsample_nearest2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor", "aten.var.correction", "var.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> Tensor", "aten.var.dim", "var.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> Tensor", "aten.view", "view(Tensor(a) self, SymInt[] size) -> Tensor(a)", "aten.where.self", "where.self(Tensor condition, Tensor self, Tensor other) -> Tensor", "Prims IR is a set of primitive operators that can be used to compose other operators. Prims IR is a lower level opset than core aten IR, and it further decomposes ops into explicit type promotion and broadcasting ops: prims.convert_element_type and prims.broadcast_in_dim. This opset is designed to interface with compiler backends.", "Warning", "This opset is still under active development, more ops will be added in the future.", "Operator", "Schema", "prims.abs", "abs(Tensor self) -> Tensor", "prims.acos", "acos(Tensor self) -> Tensor", "prims.acosh", "acosh(Tensor self) -> Tensor", "prims.asin", "asin(Tensor self) -> Tensor", "prims.asinh", "asinh(Tensor self) -> Tensor", "prims.atan", "atan(Tensor self) -> Tensor", "prims.atanh", "atanh(Tensor self) -> Tensor", "prims.cos", "cos(Tensor self) -> Tensor", "prims.cosh", "cosh(Tensor self) -> Tensor", "prims.bessel_i0", "bessel_i0(Tensor self) -> Tensor", "prims.bessel_i0e", "bessel_i0e(Tensor self) -> Tensor", "prims.bessel_i1", "bessel_i1(Tensor self) -> Tensor", "prims.bessel_i1e", "bessel_i1e(Tensor self) -> Tensor", "prims.bessel_j0", "bessel_j0(Tensor self) -> Tensor", "prims.bessel_j1", "bessel_j1(Tensor self) -> Tensor", "prims.bitwise_not", "bitwise_not(Tensor self) -> Tensor", "prims.cbrt", "cbrt(Tensor self) -> Tensor", "prims.ceil", "ceil(Tensor self) -> Tensor", "prims.conj_physical", "conj_physical(Tensor self) -> Tensor", "prims.digamma", "digamma(Tensor self) -> Tensor", "prims.erf", "erf(Tensor self) -> Tensor", "prims.erf_inv", "erf_inv(Tensor self) -> Tensor", "prims.erfc", "erfc(Tensor self) -> Tensor", "prims.erfcx", "erfcx(Tensor self) -> Tensor", "prims.exp", "exp(Tensor self) -> Tensor", "prims.expm1", "expm1(Tensor self) -> Tensor", "prims.exp2", "exp2(Tensor self) -> Tensor", "prims.fill", "fill(Tensor self, Scalar value) -> Tensor", "prims.floor", "floor(Tensor self) -> Tensor", "prims.imag", "imag(Tensor self) -> Tensor", "prims.isfinite", "isfinite(Tensor self) -> Tensor", "prims.lgamma", "lgamma(Tensor self) -> Tensor", "prims.log", "log(Tensor self) -> Tensor", "prims.log1p", "log1p(Tensor self) -> Tensor", "prims.log2", "log2(Tensor self) -> Tensor", "prims.log10", "log10(Tensor self) -> Tensor", "prims.ndtri", "ndtri(Tensor self) -> Tensor", "prims.neg", "neg(Tensor self) -> Tensor", "prims.real", "real(Tensor self) -> Tensor", "prims.reciprocal", "reciprocal(Tensor self) -> Tensor", "prims.round", "round(Tensor self) -> Tensor", "prims.sign", "sign(Tensor self) -> Tensor", "prims.signbit", "signbit(Tensor self) -> Tensor", "prims.sin", "sin(Tensor self) -> Tensor", "prims.sinh", "sinh(Tensor self) -> Tensor", "prims.spherical_bessel_j0", "spherical_bessel_j0(Tensor self) -> Tensor", "prims.sqrt", "sqrt(Tensor self) -> Tensor", "prims.tan", "tan(Tensor self) -> Tensor", "prims.tanh", "tanh(Tensor self) -> Tensor", "prims.trunc", "trunc(Tensor self) -> Tensor", "prims.add", "add(Tensor self, Tensor other) -> Tensor", "prims.atan2", "atan2(Tensor self, Tensor other) -> Tensor", "prims.bitwise_and", "bitwise_and(Tensor self, Tensor other) -> Tensor", "prims.bitwise_or", "bitwise_or(Tensor self, Tensor other) -> Tensor", "prims.bitwise_xor", "bitwise_xor(Tensor self, Tensor other) -> Tensor", "prims.div", "div(Tensor self, Tensor other) -> Tensor", "prims.eq", "eq(Tensor self, Tensor other) -> Tensor", "prims.fmax", "fmax(Tensor self, Tensor other) -> Tensor", "prims.fmin", "fmin(Tensor self, Tensor other) -> Tensor", "prims.fmod", "fmod(Tensor self, Tensor other) -> Tensor", "prims.gcd", "gcd(Tensor self, Tensor other) -> Tensor", "prims.ge", "ge(Tensor self, Tensor other) -> Tensor", "prims.gt", "gt(Tensor self, Tensor other) -> Tensor", "prims.hypot", "hypot(Tensor self, Tensor other) -> Tensor", "prims.igamma", "igamma(Tensor self, Tensor other) -> Tensor", "prims.igammac", "igammac(Tensor self, Tensor other) -> Tensor", "prims.le", "le(Tensor self, Tensor other) -> Tensor", "prims.lt", "lt(Tensor self, Tensor other) -> Tensor", "prims.maximum", "maximum(Tensor self, Tensor other) -> Tensor", "prims.minimum", "minimum(Tensor self, Tensor other) -> Tensor", "prims.mul", "mul(Tensor self, Tensor other) -> Tensor", "prims.ne", "ne(Tensor self, Tensor other) -> Tensor", "prims.nextafter", "nextafter(Tensor self, Tensor other) -> Tensor", "prims.pow", "pow(Tensor self, Tensor other) -> Tensor", "prims.remainder", "remainder(Tensor self, Tensor other) -> Tensor", "prims.rsqrt", "rsqrt(Tensor self) -> Tensor", "prims.shift_left", "shift_left(Tensor self, Tensor other) -> Tensor", "prims.shift_right_arithmetic", "shift_right_arithmetic(Tensor self, Tensor other) -> Tensor", "prims.sub", "sub(Tensor self, Tensor other) -> Tensor", "prims.zeta", "zeta(Tensor self, Tensor other) -> Tensor", "prims.as_strided", "as_strided(Tensor(a!) a, SymInt[] size, SymInt[] stride, SymInt storage_offset) -> Tensor(a!)", "prims.broadcast_in_dim", "broadcast_in_dim(Tensor(a) a, SymInt[] shape, int[] broadcast_dimensions) -> Tensor(a)", "prims.collapse_view", "collapse_view(Tensor(a) a, int start, int end) -> Tensor(a)", "prims.conj", "conj(Tensor(a) a) -> Tensor(a)", "prims.slice", "slice(Tensor(a) a, SymInt[] start_indices, SymInt[] limit_indices, SymInt[]? strides=None) -> Tensor(a)", "prims.slice_in_dim", "slice_in_dim(Tensor(a) a, SymInt start_index, SymInt limit_index, int stride=1, int axis=0) -> Tensor(a)", "prims.split_dim", "split_dim(Tensor(a) a, int dim, SymInt outer_length) -> Tensor(a)", "prims.squeeze", "squeeze(Tensor(a) a, int[] dimensions) -> Tensor(a)", "prims.transpose", "transpose(Tensor(a) a, int[] permutation) -> Tensor(a)", "prims.view_of", "view_of(Tensor(a) a) -> Tensor", "prims.as_strided_scatter", "as_strided_scatter(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt storage_offset) -> Tensor", "prims.collapse", "collapse(Tensor a, int start, int end) -> Tensor", "prims.cat", "cat(Tensor[] tensors, int dim) -> Tensor", "prims.reshape", "reshape(Tensor a, SymInt[] shape) -> Tensor", "prims.rev", "rev(Tensor a, int[] dims) -> Tensor", "prims.where", "where(Tensor pred, Tensor a, Tensor b) -> Tensor", "prims.clone", "clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor", "prims.convert_element_type", "convert_element_type(Tensor a, ScalarType dtype) -> Tensor", "prims.device_put", "device_put(Tensor a, Device device) -> Tensor", "prims.item", "item(Tensor a) -> Scalar", "prims.maximum_value", "maximum_value(ScalarType dtype) -> Scalar", "prims.minimum_value", "minimum_value(ScalarType dtype) -> Scalar", "prims.copy_strided", "copy_strided(Tensor a, SymInt[] stride) -> Tensor", "prims.copy_to", "copy_to(Tensor(a!) a, Tensor b) -> Tensor(a!)", "prims.resize", "resize(Tensor(a!) a, SymInt[] shape) -> Tensor(a!)", "prims.amax", "amax(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -> Tensor", "prims.amin", "amin(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -> Tensor", "prims.prod", "prod(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -> Tensor", "prims.sum", "sum(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -> Tensor", "prims.xor_sum", "xor_sum(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -> Tensor", "prims.var", "var(Tensor inp, int[]? dims, *, float correction, ScalarType? output_dtype=None) -> Tensor", "prims.empty_strided", "empty_strided(SymInt[] shape, SymInt[] strides, *, ScalarType dtype, Device device, bool requires_grad) -> Tensor", "prims.empty_permuted", "empty_permuted(SymInt[] shape, int[] physical_layout, *, ScalarType dtype, Device device, bool requires_grad) -> Tensor", "prims.scalar_tensor", "scalar_tensor(Scalar s, *, ScalarType? dtype=None, Device? device=None) -> Tensor", "prims.iota", "iota(SymInt length, *, SymInt start, SymInt step, ScalarType dtype, Device device, bool requires_grad) -> Tensor", "prims.svd", "svd(Tensor A, *, bool full_matrices) -> (Tensor U, Tensor S, Tensor Vh)", "prims.normal", "normal(SymInt[] shape, *, Scalar mean, Scalar std, ScalarType dtype, Device device, bool requires_grad) -> Tensor", "prims.uniform", "uniform(SymInt[] shape, *, Scalar low, Scalar high, ScalarType dtype, Device device) -> Tensor", "prims.fft_r2c", "fft_r2c(Tensor self, *, int[] dim, bool onesided) -> Tensor", "prims.fft_c2c", "fft_c2c(Tensor self, *, int[] dim, bool forward) -> Tensor", "prims.fft_c2r", "fft_c2r(Tensor self, *, int[] dim, SymInt last_dim_size) -> Tensor"]}, {"name": "torch.export.load()", "path": "export#torch.export.load", "type": "Traced Graph Export", "text": ["Warning", "Under active development, saved files may not be usable in newer versions of PyTorch.", "Loads an ExportedProgram previously saved with torch.export.save.", "An ExportedProgram object", "ExportedProgram", "Example:"]}, {"name": "torch.export.ModuleCallEntry", "path": "export#torch.export.ModuleCallEntry", "type": "Traced Graph Export", "text": []}, {"name": "torch.export.ModuleCallSignature", "path": "export#torch.export.ModuleCallSignature", "type": "Traced Graph Export", "text": []}, {"name": "torch.export.save()", "path": "export#torch.export.save", "type": "Traced Graph Export", "text": ["Warning", "Under active development, saved files may not be usable in newer versions of PyTorch.", "Saves an ExportedProgram to a file-like object. It can then be loaded using the Python API torch.export.load.", "Example:"]}, {"name": "torch.export.TorchDynamo Deep Dive", "path": "torch.compiler_deepdive", "type": "Traced Graph Export", "text": ["Before you read this section, read torch.compiler.", "TorchDynamo is a Python-level Just-In-Time (JIT) compiler designed to make unmodified PyTorch programs faster. TorchDynamo hooks into the frame evaluation API in CPython (PEP 523) to dynamically modify Python bytecode right before it is executed. It rewrites Python bytecode to extract sequences of PyTorch operations into an FX Graph which is then compiled with a customizable backend. It creates this FX Graph through bytecode analysis and is designed to mix Python execution with compiled backends to get the best of both worlds \u2014 usability and performance.", "TorchDynamo makes it easy to experiment with different compiler backends to make PyTorch code faster with a single line decorator torch._dynamo.optimize() which is wrapped for convenience by torch.compile()", "The following diagram demonstrates how PyTorch works with torch.compile and without it:", "TorchInductor is one of the backends supported by TorchDynamo Graph into Triton for GPUs or C++/OpenMP for CPUs. We have a training performance dashboard that provides performance comparison for different training backends. You can read more in the TorchInductor post on PyTorch dev-discuss.", "For an in-depth overview, read the sections below, watch the deep-dive video, and check out the dev-discuss topics.", "Author: Jason Ansel and Kaichao You", "This section will go over some of the TorchDynamo internals and will demonstrate how TorchDynamo works under the hood.", "TorchDynamo operates just-in-time and specializes graphs based on dynamic properties. Below is a basic example of how to use TorchDynamo. One can decorate a function or a method using torchdynamo.optimize to enable TorchDynamo optimization:", "For example, the first graph above has the following guards:", "If any of those guards fail, the graph will be recaptured and recompiled. The interesting guard type there is TENSOR_MATCH, which checks the following torch.Tensor properties:", "The full specialization mode allows the backend compiler to assume an entirely static graph. Unfortunately, most backends require this. Operators which return dynamic shapes will trigger a graph break when not in dynamic shape mode.", "If you want to understand better what TorchDynamo is doing, you can set:", "This code triggers useful (but spammy) printouts.", "For example, the printouts for the first graph in the toy_example are:", "At the top you can see the FX graph. Next, you see the original bytecode of the function, followed by the modified bytecode generated by TorchDynamo. Finally, you see the guards which we covered above.", "In the modified bytecode, __compiled_fn_0 is the return value of my_compiler() (the compiled graph). __resume_at_30_1 and __resume_at_38_2 are both generated continuation functions that pick up execution after a graph break (at bytecode offsets 30 and 38). Each of these functions take the form:", "By generating this resume_at function, we force the remainder of the function to be executed in a new Python frame which recursively triggers TorchDynamo to restart its capture once execution reaches that point for the first time.", "To inspect the artifacts generated by TorchDynamo, there is an API torch._dynamo.eval_frame._debug_get_cache_entry_list that retrieves compiled code and guards out of a function\u2019s __code__ object. A compiled function can have several cache entries, and each cache entry consists a generated function to check guards, and a types.CodeType object to keep the code to be executed if the guarding conditions are satisfied.", "The compiled bytecode, printed by dis.dis(code), will call the result of the backend compiler function which is stored inside a global variable such as __compiled_fn_0 in the module containing the original function.", "The generated bytecodes are roughly equivalent to the following Python (converted manually for illustration purposes).", "Note that we pass a simple my_compiler function as the backend compiler, therefore the subgraph code __resume_at_38_2, __resume_at_30_1, and __compiled_fn_0._torchdynamo_orig_callable remain python code. However, if we use other backends like the built-in inductor, the subgraph code will be compiled CUDA kernels for GPU or C++ code for CPU."]}, {"name": "torch.export.Writing Graph Transformations on ATen IR", "path": "torch.compiler_transformations", "type": "Traced Graph Export", "text": ["Since the ATen IR sits at the FX Graph/GraphModule level, any transformations written for FX Graphs can be easily applied onto the ATen IR. If you\u2019re familiar with writing FX graph transformations, then this will be the same.", "The most direct way of writing transformations is by looping through the given graph and directly manipulating the nodes within the graph.", "For example, let\u2019s say we want to replace torch.ops.aten.add.Tensor() calls with torch.ops.aten.mul.Tensor() calls:", "We can also delete and append new nodes through FX utility functions that can be found in the Graph documentation. For example, if we want to insert a torch.ops.aten.relu.default() after the add call:", "In general, transformations can be roughly categorized into a couple of axis:", "Axis A: 1. Creating one-to-X mapping (eg. decomposition) 2. Creating many-to-one mapping (eg. fusion)", "Axis B: 1. Doing forwards iteration (eg. shape propagation) 2. Doing backwards iteration (eg. dead code elimination)", "Axis C: 1. Dependent on local node information (eg. out-variant conversion) 2. Dependent on global graph information (eg. memory planning)", "Our projection on the frequency of these use cases are: 1. A.1, B.1, C.1 2. A.2 3. B.2, C.2", "Although we can make all graph transformations through directly manipulating the graph, we also provide some helper utilities for some ease of use for the level 1 and 2 use-cases.", "For level 1 uses cases (creating one-to-X mappings, doing forwards iterations, and looking at local node information), we can utilize the Transformer class to execute each node and recreate a graph, except with the transformations specified.", "An example for one-to-one mappings, if we wanted to replace an op A with another op B, we can run the GraphModule, and very time we see op A, return op B.", "An example is:", "The super().call_function(target, args, kwargs, meta) call creates a call_function FX node, and returns the result of running the operator with the given arguments.", "If we wanted to do one-to-X mappings, like replacing op A with 2 other ops B and C, we would then make 2 calls to super().call_function to create 2 FX nodes, one with op B and another with op C, and return the result of running op C.", "For example:", "If we wanted to remove an op, we can just return the value passed into the function:", "An example of utilizing local node information is, if we wanted to convert all the scalars within the graph to tensors, we can run the given fx.GraphModule, and for every argument that contains a scalar, we convert it to a tensor. It might look something like:", "For creating many-to-one mappings, we can utilize FX\u2019s subgraph rewriter. Given a pattern, it creates a subgraph of operators matching to the pattern, and then replaces each matched subgraph with the replacement.", "Note:", "The pattern and replacement inputs must be callable functions or GraphModules containing the same operators that are used within the graph (ATen ops) so that the subgraph rewriter can find the correct pattern in the graph. Inputs to the pattern/replacement callables will be treated as wildcards when matching.", "An example:", "The subgraph rewriter returns a list of ReplacedPatterns:", "Note:", "The `PassManager <https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/infra/pass_manager.py>`__ is a class used to run multiple passes on a given graph module. When initializing a PassManager instance, we pass in a list of passes that we want to run and set a couple of flags. To run the collection of passes on a graph module, we can pass the graph module directly to the PassManager instance.", "An example:", "To add a common set of checks that are run after each pass, we can call the function set_checks(check: Callable) which takes in a callable function as input. If the run_checks_after_each_pass flag is set, the check will be called after each pass is run on the graph module.", "An example:", "There are a couple of common FX graph based partitioners we can use to partition the graph.", "For finding subgraphs within a graph that match a specific pattern, we can utilize FX\u2019s `SubgraphMatcher <https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/utils/matcher_utils.py>`__.", "Class Attributes:", "An example:", "The match function returns a list of InternalMatch:", "To find the largest subgraphs of nodes that support a specific invariant, we can utilize FX\u2019s `CapabilityBasedPartitioner <https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/infra/partitioner.py#L34>`__.", "Class Attributes", "The `OperatorSupportBase <https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/operator_support.py#LL28C1-L28C1>`__ class is used by the partitioner to determine if a specific node in the graph belongs in the partition. This is done by overriding the is_node_supported function. You can chain multiple OperatorSuppportBase by using `chain <https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/operator_support.py#L150>`__(which returns False if any of the OperatorSupportBase return False) and `any_chain <https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/operator_support.py#L164>`__ (which returns True if any of the OperatorSupportBase returns True).", "An example:"]}, {"name": "torch.eye", "path": "generated/torch.eye", "type": "Torch", "text": ["Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.", "A 2-D tensor with ones on the diagonal and zeros elsewhere", "Tensor", "Example:"]}, {"name": "torch.fake_quantize_per_channel_affine", "path": "generated/torch.fake_quantize_per_channel_affine", "type": "Torch", "text": ["Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.", "A newly fake_quantized per channel torch.float32 tensor", "Tensor", "Example:"]}, {"name": "torch.fake_quantize_per_tensor_affine", "path": "generated/torch.fake_quantize_per_tensor_affine", "type": "Torch", "text": ["Returns a new tensor with the data in input fake quantized using scale, zero_point, quant_min and quant_max.", "A newly fake_quantized torch.float32 tensor", "Tensor", "Example:"]}, {"name": "torch.fft", "path": "fft", "type": "Discrete Fourier Transforms", "text": ["Discrete Fourier transforms and related functions.", "Computes the one dimensional discrete Fourier transform of input.", "Computes the one dimensional inverse discrete Fourier transform of input.", "Computes the 2 dimensional discrete Fourier transform of input.", "Computes the 2 dimensional inverse discrete Fourier transform of input.", "Computes the N dimensional discrete Fourier transform of input.", "Computes the N dimensional inverse discrete Fourier transform of input.", "Computes the one dimensional Fourier transform of real-valued input.", "Computes the inverse of rfft().", "Computes the 2-dimensional discrete Fourier transform of real input.", "Computes the inverse of rfft2().", "Computes the N-dimensional discrete Fourier transform of real input.", "Computes the inverse of rfftn().", "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric input signal.", "Computes the inverse of hfft().", "Computes the 2-dimensional discrete Fourier transform of a Hermitian symmetric input signal.", "Computes the 2-dimensional inverse discrete Fourier transform of real input.", "Computes the n-dimensional discrete Fourier transform of a Hermitian symmetric input signal.", "Computes the N-dimensional inverse discrete Fourier transform of real input.", "Computes the discrete Fourier Transform sample frequencies for a signal of size n.", "Computes the sample frequencies for rfft() with a signal of size n.", "Reorders n-dimensional FFT data, as provided by fftn(), to have negative frequency terms first.", "Inverse of fftshift()."]}, {"name": "torch.fft.fft()", "path": "generated/torch.fft.fft#torch.fft.fft", "type": "Discrete Fourier Transforms", "text": ["Computes the one dimensional discrete Fourier transform of input.", "Note", "The Fourier domain representation of any real signal satisfies the Hermitian property: X[i] = conj(X[-i]). This function always returns both the positive and negative frequency terms even though, for real inputs, the negative frequencies are redundant. rfft() returns the more compact one-sided representation where only the positive frequencies are returned.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (fft()), these correspond to:", "Calling the backward transform (ifft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.fft.fft2()", "path": "generated/torch.fft.fft2#torch.fft.fft2", "type": "Discrete Fourier Transforms", "text": ["Computes the 2 dimensional discrete Fourier transform of input. Equivalent to fftn() but FFTs only the last two dimensions by default.", "Note", "The Fourier domain representation of any real signal satisfies the Hermitian property: X[i, j] = conj(X[-i, -j]). This function always returns all positive and negative frequency terms even though, for real inputs, half of these values are redundant. rfft2() returns the more compact one-sided representation where only the positive frequencies of the last dimension are returned.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (fft2()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (ifft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft2() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "The discrete Fourier transform is separable, so fft2() here is equivalent to two one-dimensional fft() calls:"]}, {"name": "torch.fft.fftfreq()", "path": "generated/torch.fft.fftfreq#torch.fft.fftfreq", "type": "Discrete Fourier Transforms", "text": ["Computes the discrete Fourier Transform sample frequencies for a signal of size n.", "Note", "By convention, fft() returns positive frequency terms first, followed by the negative frequencies in reverse order, so that f[-i] for all 0<i\u2264n/20 < i \\leq n/2 in Python gives the negative frequency terms. For an FFT of length n and with inputs spaced in length unit d, the frequencies are:", "Note", "For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. fftfreq() follows NumPy\u2019s convention of taking it to be negative.", "For even input, we can see the Nyquist frequency at f[2] is given as negative:"]}, {"name": "torch.fft.fftn()", "path": "generated/torch.fft.fftn#torch.fft.fftn", "type": "Discrete Fourier Transforms", "text": ["Computes the N dimensional discrete Fourier transform of input.", "Note", "The Fourier domain representation of any real signal satisfies the Hermitian property: X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n]). This function always returns all positive and negative frequency terms even though, for real inputs, half of these values are redundant. rfftn() returns the more compact one-sided representation where only the positive frequencies of the last dimension are returned.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (fftn()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (ifftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifftn() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "The discrete Fourier transform is separable, so fftn() here is equivalent to two one-dimensional fft() calls:"]}, {"name": "torch.fft.fftshift()", "path": "generated/torch.fft.fftshift#torch.fft.fftshift", "type": "Discrete Fourier Transforms", "text": ["Reorders n-dimensional FFT data, as provided by fftn(), to have negative frequency terms first.", "This performs a periodic shift of n-dimensional data such that the origin (0, ..., 0) is moved to the center of the tensor. Specifically, to input.shape[dim] // 2 in each selected dimension.", "Note", "By convention, the FFT returns positive frequency terms first, followed by the negative frequencies in reverse order, so that f[-i] for all 0<i\u2264n/20 < i \\leq n/2 in Python gives the negative frequency terms. fftshift() rearranges all frequencies into ascending order from negative to positive with the zero-frequency term in the center.", "Note", "For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. fftshift() always puts the Nyquist term at the 0-index. This is the same convention used by fftfreq().", "Also notice that the Nyquist frequency term at f[2] was moved to the beginning of the tensor.", "This also works for multi-dimensional transforms:", "fftshift() can also be useful for spatial data. If our data is defined on a centered grid ([-(N//2), (N-1)//2]) then we can use the standard FFT defined on an uncentered grid ([0, N)) by first applying an ifftshift().", "Similarly, we can convert the frequency domain components to centered convention by applying fftshift().", "The inverse transform, from centered Fourier space back to centered spatial data, can be performed by applying the inverse shifts in reverse order:"]}, {"name": "torch.fft.hfft()", "path": "generated/torch.fft.hfft#torch.fft.hfft", "type": "Discrete Fourier Transforms", "text": ["Computes the one dimensional discrete Fourier transform of a Hermitian symmetric input signal.", "Note", "hfft()/ihfft() are analogous to rfft()/irfft(). The real FFT expects a real signal in the time-domain and gives a Hermitian symmetry in the frequency-domain. The Hermitian FFT is the opposite; Hermitian symmetric in the time-domain and real-valued in the frequency-domain. For this reason, special care needs to be taken with the length argument n, in the same way as with irfft().", "Note", "Because the signal is Hermitian in the time-domain, the result will be real in the frequency domain. Note that some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in input[0] would result in one or more complex frequency terms which cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length n.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension. With default arguments, size of the transformed dimension should be (2^n + 1) as argument n defaults to even output size = 2 * (transformed_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (hfft()), these correspond to:", "Calling the backward transform (ihfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Taking a real-valued frequency signal and bringing it into the time domain gives Hermitian symmetric output:", "Note that T[1] == T[-1].conj() and T[2] == T[-2].conj() is redundant. We can thus compute the forward transform without considering negative frequencies:", "Like with irfft(), the output length must be given in order to recover an even length output:"]}, {"name": "torch.fft.hfft2()", "path": "generated/torch.fft.hfft2#torch.fft.hfft2", "type": "Discrete Fourier Transforms", "text": ["Computes the 2-dimensional discrete Fourier transform of a Hermitian symmetric input signal. Equivalent to hfftn() but only transforms the last two dimensions by default.", "input is interpreted as a one-sided Hermitian signal in the time domain. By the Hermitian property, the Fourier transform will be real-valued.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions. With default arguments, the size of last dimension should be (2^n + 1) as argument s defaults to even output size = 2 * (last_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (hfft2()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (ihfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft2() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Starting from a real frequency-space signal, we can generate a Hermitian-symmetric time-domain signal: >>> T = torch.rand(10, 9) >>> t = torch.fft.ihfft2(T)", "Without specifying the output length to hfftn(), the output will not round-trip properly because the input is odd-length in the last dimension:", "So, it is recommended to always pass the signal shape s."]}, {"name": "torch.fft.hfftn()", "path": "generated/torch.fft.hfftn#torch.fft.hfftn", "type": "Discrete Fourier Transforms", "text": ["Computes the n-dimensional discrete Fourier transform of a Hermitian symmetric input signal.", "input is interpreted as a one-sided Hermitian signal in the time domain. By the Hermitian property, the Fourier transform will be real-valued.", "Note", "hfftn()/ihfftn() are analogous to rfftn()/irfftn(). The real FFT expects a real signal in the time-domain and gives Hermitian symmetry in the frequency-domain. The Hermitian FFT is the opposite; Hermitian symmetric in the time-domain and real-valued in the frequency-domain. For this reason, special care needs to be taken with the shape argument s, in the same way as with irfftn().", "Note", "Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. It is recommended to always pass the signal shape s.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions. With default arguments, the size of last dimension should be (2^n + 1) as argument s defaults to even output size = 2 * (last_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (hfftn()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (ihfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfftn() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Starting from a real frequency-space signal, we can generate a Hermitian-symmetric time-domain signal: >>> T = torch.rand(10, 9) >>> t = torch.fft.ihfftn(T)", "Without specifying the output length to hfftn(), the output will not round-trip properly because the input is odd-length in the last dimension:", "So, it is recommended to always pass the signal shape s."]}, {"name": "torch.fft.ifft()", "path": "generated/torch.fft.ifft#torch.fft.ifft", "type": "Discrete Fourier Transforms", "text": ["Computes the one dimensional inverse discrete Fourier transform of input.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ifft()), these correspond to:", "Calling the forward transform (fft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.fft.ifft2()", "path": "generated/torch.fft.ifft2#torch.fft.ifft2", "type": "Discrete Fourier Transforms", "text": ["Computes the 2 dimensional inverse discrete Fourier transform of input. Equivalent to ifftn() but IFFTs only the last two dimensions by default.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ifft2()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (fft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft2() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "The discrete Fourier transform is separable, so ifft2() here is equivalent to two one-dimensional ifft() calls:"]}, {"name": "torch.fft.ifftn()", "path": "generated/torch.fft.ifftn#torch.fft.ifftn", "type": "Discrete Fourier Transforms", "text": ["Computes the N dimensional inverse discrete Fourier transform of input.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ifftn()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (fftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifftn() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "The discrete Fourier transform is separable, so ifftn() here is equivalent to two one-dimensional ifft() calls:"]}, {"name": "torch.fft.ifftshift()", "path": "generated/torch.fft.ifftshift#torch.fft.ifftshift", "type": "Discrete Fourier Transforms", "text": ["Inverse of fftshift().", "A round-trip through fftshift() and ifftshift() gives the same result:"]}, {"name": "torch.fft.ihfft()", "path": "generated/torch.fft.ihfft#torch.fft.ihfft", "type": "Discrete Fourier Transforms", "text": ["Computes the inverse of hfft().", "input must be a real-valued signal, interpreted in the Fourier domain. The IFFT of a real signal is Hermitian-symmetric, X[i] = conj(X[-i]). ihfft() represents this in the one-sided form where only the positive frequencies below the Nyquist frequency are included. To compute the full output, use ifft().", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ihfft()), these correspond to:", "Calling the forward transform (hfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Compare against the full output from ifft():"]}, {"name": "torch.fft.ihfft2()", "path": "generated/torch.fft.ihfft2#torch.fft.ihfft2", "type": "Discrete Fourier Transforms", "text": ["Computes the 2-dimensional inverse discrete Fourier transform of real input. Equivalent to ihfftn() but transforms only the two last dimensions by default.", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ihfft2()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (hfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft2() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Compared against the full output from ifft2(), the Hermitian time-space signal takes up only half the space.", "The discrete Fourier transform is separable, so ihfft2() here is equivalent to a combination of ifft() and ihfft():"]}, {"name": "torch.fft.ihfftn()", "path": "generated/torch.fft.ihfftn#torch.fft.ihfftn", "type": "Discrete Fourier Transforms", "text": ["Computes the N-dimensional inverse discrete Fourier transform of real input.", "input must be a real-valued signal, interpreted in the Fourier domain. The n-dimensional IFFT of a real signal is Hermitian-symmetric, X[i, j, ...] = conj(X[-i, -j, ...]). ihfftn() represents this in the one-sided form where only the positive frequencies below the Nyquist frequency are included in the last signal dimension. To compute the full output, use ifftn().", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ihfftn()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (hfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfftn() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Compared against the full output from ifftn(), we have all elements up to the Nyquist frequency.", "The discrete Fourier transform is separable, so ihfftn() here is equivalent to a combination of ihfft() and ifft():"]}, {"name": "torch.fft.irfft()", "path": "generated/torch.fft.irfft#torch.fft.irfft", "type": "Discrete Fourier Transforms", "text": ["Computes the inverse of rfft().", "input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfft(). By the Hermitian property, the output will be real-valued.", "Note", "Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length n.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension. With default arguments, size of the transformed dimension should be (2^n + 1) as argument n defaults to even output size = 2 * (transformed_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (irfft()), these correspond to:", "Calling the forward transform (rfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Without specifying the output length to irfft(), the output will not round-trip properly because the input is odd-length:", "So, it is recommended to always pass the signal length n:"]}, {"name": "torch.fft.irfft2()", "path": "generated/torch.fft.irfft2#torch.fft.irfft2", "type": "Discrete Fourier Transforms", "text": ["Computes the inverse of rfft2(). Equivalent to irfftn() but IFFTs only the last two dimensions by default.", "input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfft2(). By the Hermitian property, the output will be real-valued.", "Note", "Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape s.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions. With default arguments, the size of last dimension should be (2^n + 1) as argument s defaults to even output size = 2 * (last_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (irfft2()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (rfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft2() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Without specifying the output length to irfft2(), the output will not round-trip properly because the input is odd-length in the last dimension:", "So, it is recommended to always pass the signal shape s."]}, {"name": "torch.fft.irfftn()", "path": "generated/torch.fft.irfftn#torch.fft.irfftn", "type": "Discrete Fourier Transforms", "text": ["Computes the inverse of rfftn().", "input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfftn(). By the Hermitian property, the output will be real-valued.", "Note", "Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape s.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions. With default arguments, the size of last dimension should be (2^n + 1) as argument s defaults to even output size = 2 * (last_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (irfftn()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (rfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfftn() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Without specifying the output length to irfft(), the output will not round-trip properly because the input is odd-length in the last dimension:", "So, it is recommended to always pass the signal shape s."]}, {"name": "torch.fft.rfft()", "path": "generated/torch.fft.rfft#torch.fft.rfft", "type": "Discrete Fourier Transforms", "text": ["Computes the one dimensional Fourier transform of real-valued input.", "The FFT of a real signal is Hermitian-symmetric, X[i] = conj(X[-i]) so the output contains only the positive frequencies below the Nyquist frequency. To compute the full output, use fft()", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (rfft()), these correspond to:", "Calling the backward transform (irfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Compare against the full output from fft():", "Notice that the symmetric element T[-1] == T[1].conj() is omitted. At the Nyquist frequency T[-2] == T[2] is it\u2019s own symmetric pair, and therefore must always be real-valued."]}, {"name": "torch.fft.rfft2()", "path": "generated/torch.fft.rfft2#torch.fft.rfft2", "type": "Discrete Fourier Transforms", "text": ["Computes the 2-dimensional discrete Fourier transform of real input. Equivalent to rfftn() but FFTs only the last two dimensions by default.", "The FFT of a real signal is Hermitian-symmetric, X[i, j] = conj(X[-i, -j]), so the full fft2() output contains redundant information. rfft2() instead omits the negative frequencies in the last dimension.", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (rfft2()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (irfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft2() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Compared against the full output from fft2(), we have all elements up to the Nyquist frequency.", "The discrete Fourier transform is separable, so rfft2() here is equivalent to a combination of fft() and rfft():"]}, {"name": "torch.fft.rfftfreq()", "path": "generated/torch.fft.rfftfreq#torch.fft.rfftfreq", "type": "Discrete Fourier Transforms", "text": ["Computes the sample frequencies for rfft() with a signal of size n.", "Note", "rfft() returns Hermitian one-sided output, so only the positive frequency terms are returned. For a real FFT of length n and with inputs spaced in length unit d, the frequencies are:", "Note", "For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. Unlike fftfreq(), rfftfreq() always returns it as positive.", "Compared to the output from fftfreq(), we see that the Nyquist frequency at f[2] has changed sign: >>> torch.fft.fftfreq(4) tensor([ 0.0000, 0.2500, -0.5000, -0.2500])"]}, {"name": "torch.fft.rfftn()", "path": "generated/torch.fft.rfftn#torch.fft.rfftn", "type": "Discrete Fourier Transforms", "text": ["Computes the N-dimensional discrete Fourier transform of real input.", "The FFT of a real signal is Hermitian-symmetric, X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n]) so the full fftn() output contains redundant information. rfftn() instead omits the negative frequencies in the last dimension.", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (rfftn()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (irfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfftn() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Compared against the full output from fftn(), we have all elements up to the Nyquist frequency.", "The discrete Fourier transform is separable, so rfftn() here is equivalent to a combination of fft() and rfft():"]}, {"name": "torch.fft.torch.fft.fft", "path": "generated/torch.fft.fft", "type": "Discrete Fourier Transforms", "text": ["Computes the one dimensional discrete Fourier transform of input.", "Note", "The Fourier domain representation of any real signal satisfies the Hermitian property: X[i] = conj(X[-i]). This function always returns both the positive and negative frequency terms even though, for real inputs, the negative frequencies are redundant. rfft() returns the more compact one-sided representation where only the positive frequencies are returned.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (fft()), these correspond to:", "Calling the backward transform (ifft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.fft.torch.fft.fft2", "path": "generated/torch.fft.fft2", "type": "Discrete Fourier Transforms", "text": ["Computes the 2 dimensional discrete Fourier transform of input. Equivalent to fftn() but FFTs only the last two dimensions by default.", "Note", "The Fourier domain representation of any real signal satisfies the Hermitian property: X[i, j] = conj(X[-i, -j]). This function always returns all positive and negative frequency terms even though, for real inputs, half of these values are redundant. rfft2() returns the more compact one-sided representation where only the positive frequencies of the last dimension are returned.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (fft2()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (ifft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft2() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "The discrete Fourier transform is separable, so fft2() here is equivalent to two one-dimensional fft() calls:"]}, {"name": "torch.fft.torch.fft.fftfreq", "path": "generated/torch.fft.fftfreq", "type": "Discrete Fourier Transforms", "text": ["Computes the discrete Fourier Transform sample frequencies for a signal of size n.", "Note", "By convention, fft() returns positive frequency terms first, followed by the negative frequencies in reverse order, so that f[-i] for all 0<i\u2264n/20 < i \\leq n/2 in Python gives the negative frequency terms. For an FFT of length n and with inputs spaced in length unit d, the frequencies are:", "Note", "For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. fftfreq() follows NumPy\u2019s convention of taking it to be negative.", "For even input, we can see the Nyquist frequency at f[2] is given as negative:"]}, {"name": "torch.fft.torch.fft.fftn", "path": "generated/torch.fft.fftn", "type": "Discrete Fourier Transforms", "text": ["Computes the N dimensional discrete Fourier transform of input.", "Note", "The Fourier domain representation of any real signal satisfies the Hermitian property: X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n]). This function always returns all positive and negative frequency terms even though, for real inputs, half of these values are redundant. rfftn() returns the more compact one-sided representation where only the positive frequencies of the last dimension are returned.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (fftn()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (ifftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifftn() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "The discrete Fourier transform is separable, so fftn() here is equivalent to two one-dimensional fft() calls:"]}, {"name": "torch.fft.torch.fft.fftshift", "path": "generated/torch.fft.fftshift", "type": "Discrete Fourier Transforms", "text": ["Reorders n-dimensional FFT data, as provided by fftn(), to have negative frequency terms first.", "This performs a periodic shift of n-dimensional data such that the origin (0, ..., 0) is moved to the center of the tensor. Specifically, to input.shape[dim] // 2 in each selected dimension.", "Note", "By convention, the FFT returns positive frequency terms first, followed by the negative frequencies in reverse order, so that f[-i] for all 0<i\u2264n/20 < i \\leq n/2 in Python gives the negative frequency terms. fftshift() rearranges all frequencies into ascending order from negative to positive with the zero-frequency term in the center.", "Note", "For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. fftshift() always puts the Nyquist term at the 0-index. This is the same convention used by fftfreq().", "Also notice that the Nyquist frequency term at f[2] was moved to the beginning of the tensor.", "This also works for multi-dimensional transforms:", "fftshift() can also be useful for spatial data. If our data is defined on a centered grid ([-(N//2), (N-1)//2]) then we can use the standard FFT defined on an uncentered grid ([0, N)) by first applying an ifftshift().", "Similarly, we can convert the frequency domain components to centered convention by applying fftshift().", "The inverse transform, from centered Fourier space back to centered spatial data, can be performed by applying the inverse shifts in reverse order:"]}, {"name": "torch.fft.torch.fft.hfft", "path": "generated/torch.fft.hfft", "type": "Discrete Fourier Transforms", "text": ["Computes the one dimensional discrete Fourier transform of a Hermitian symmetric input signal.", "Note", "hfft()/ihfft() are analogous to rfft()/irfft(). The real FFT expects a real signal in the time-domain and gives a Hermitian symmetry in the frequency-domain. The Hermitian FFT is the opposite; Hermitian symmetric in the time-domain and real-valued in the frequency-domain. For this reason, special care needs to be taken with the length argument n, in the same way as with irfft().", "Note", "Because the signal is Hermitian in the time-domain, the result will be real in the frequency domain. Note that some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in input[0] would result in one or more complex frequency terms which cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length n.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension. With default arguments, size of the transformed dimension should be (2^n + 1) as argument n defaults to even output size = 2 * (transformed_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (hfft()), these correspond to:", "Calling the backward transform (ihfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Taking a real-valued frequency signal and bringing it into the time domain gives Hermitian symmetric output:", "Note that T[1] == T[-1].conj() and T[2] == T[-2].conj() is redundant. We can thus compute the forward transform without considering negative frequencies:", "Like with irfft(), the output length must be given in order to recover an even length output:"]}, {"name": "torch.fft.torch.fft.hfft2", "path": "generated/torch.fft.hfft2", "type": "Discrete Fourier Transforms", "text": ["Computes the 2-dimensional discrete Fourier transform of a Hermitian symmetric input signal. Equivalent to hfftn() but only transforms the last two dimensions by default.", "input is interpreted as a one-sided Hermitian signal in the time domain. By the Hermitian property, the Fourier transform will be real-valued.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions. With default arguments, the size of last dimension should be (2^n + 1) as argument s defaults to even output size = 2 * (last_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (hfft2()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (ihfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft2() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Starting from a real frequency-space signal, we can generate a Hermitian-symmetric time-domain signal: >>> T = torch.rand(10, 9) >>> t = torch.fft.ihfft2(T)", "Without specifying the output length to hfftn(), the output will not round-trip properly because the input is odd-length in the last dimension:", "So, it is recommended to always pass the signal shape s."]}, {"name": "torch.fft.torch.fft.hfftn", "path": "generated/torch.fft.hfftn", "type": "Discrete Fourier Transforms", "text": ["Computes the n-dimensional discrete Fourier transform of a Hermitian symmetric input signal.", "input is interpreted as a one-sided Hermitian signal in the time domain. By the Hermitian property, the Fourier transform will be real-valued.", "Note", "hfftn()/ihfftn() are analogous to rfftn()/irfftn(). The real FFT expects a real signal in the time-domain and gives Hermitian symmetry in the frequency-domain. The Hermitian FFT is the opposite; Hermitian symmetric in the time-domain and real-valued in the frequency-domain. For this reason, special care needs to be taken with the shape argument s, in the same way as with irfftn().", "Note", "Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. It is recommended to always pass the signal shape s.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions. With default arguments, the size of last dimension should be (2^n + 1) as argument s defaults to even output size = 2 * (last_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (hfftn()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (ihfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfftn() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Starting from a real frequency-space signal, we can generate a Hermitian-symmetric time-domain signal: >>> T = torch.rand(10, 9) >>> t = torch.fft.ihfftn(T)", "Without specifying the output length to hfftn(), the output will not round-trip properly because the input is odd-length in the last dimension:", "So, it is recommended to always pass the signal shape s."]}, {"name": "torch.fft.torch.fft.ifft", "path": "generated/torch.fft.ifft", "type": "Discrete Fourier Transforms", "text": ["Computes the one dimensional inverse discrete Fourier transform of input.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ifft()), these correspond to:", "Calling the forward transform (fft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.fft.torch.fft.ifft2", "path": "generated/torch.fft.ifft2", "type": "Discrete Fourier Transforms", "text": ["Computes the 2 dimensional inverse discrete Fourier transform of input. Equivalent to ifftn() but IFFTs only the last two dimensions by default.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ifft2()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (fft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft2() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "The discrete Fourier transform is separable, so ifft2() here is equivalent to two one-dimensional ifft() calls:"]}, {"name": "torch.fft.torch.fft.ifftn", "path": "generated/torch.fft.ifftn", "type": "Discrete Fourier Transforms", "text": ["Computes the N dimensional inverse discrete Fourier transform of input.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ifftn()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (fftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifftn() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "The discrete Fourier transform is separable, so ifftn() here is equivalent to two one-dimensional ifft() calls:"]}, {"name": "torch.fft.torch.fft.ifftshift", "path": "generated/torch.fft.ifftshift", "type": "Discrete Fourier Transforms", "text": ["Inverse of fftshift().", "A round-trip through fftshift() and ifftshift() gives the same result:"]}, {"name": "torch.fft.torch.fft.ihfft", "path": "generated/torch.fft.ihfft", "type": "Discrete Fourier Transforms", "text": ["Computes the inverse of hfft().", "input must be a real-valued signal, interpreted in the Fourier domain. The IFFT of a real signal is Hermitian-symmetric, X[i] = conj(X[-i]). ihfft() represents this in the one-sided form where only the positive frequencies below the Nyquist frequency are included. To compute the full output, use ifft().", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ihfft()), these correspond to:", "Calling the forward transform (hfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Compare against the full output from ifft():"]}, {"name": "torch.fft.torch.fft.ihfft2", "path": "generated/torch.fft.ihfft2", "type": "Discrete Fourier Transforms", "text": ["Computes the 2-dimensional inverse discrete Fourier transform of real input. Equivalent to ihfftn() but transforms only the two last dimensions by default.", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ihfft2()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (hfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft2() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Compared against the full output from ifft2(), the Hermitian time-space signal takes up only half the space.", "The discrete Fourier transform is separable, so ihfft2() here is equivalent to a combination of ifft() and ihfft():"]}, {"name": "torch.fft.torch.fft.ihfftn", "path": "generated/torch.fft.ihfftn", "type": "Discrete Fourier Transforms", "text": ["Computes the N-dimensional inverse discrete Fourier transform of real input.", "input must be a real-valued signal, interpreted in the Fourier domain. The n-dimensional IFFT of a real signal is Hermitian-symmetric, X[i, j, ...] = conj(X[-i, -j, ...]). ihfftn() represents this in the one-sided form where only the positive frequencies below the Nyquist frequency are included in the last signal dimension. To compute the full output, use ifftn().", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ihfftn()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (hfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfftn() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Compared against the full output from ifftn(), we have all elements up to the Nyquist frequency.", "The discrete Fourier transform is separable, so ihfftn() here is equivalent to a combination of ihfft() and ifft():"]}, {"name": "torch.fft.torch.fft.irfft", "path": "generated/torch.fft.irfft", "type": "Discrete Fourier Transforms", "text": ["Computes the inverse of rfft().", "input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfft(). By the Hermitian property, the output will be real-valued.", "Note", "Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length n.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension. With default arguments, size of the transformed dimension should be (2^n + 1) as argument n defaults to even output size = 2 * (transformed_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (irfft()), these correspond to:", "Calling the forward transform (rfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Without specifying the output length to irfft(), the output will not round-trip properly because the input is odd-length:", "So, it is recommended to always pass the signal length n:"]}, {"name": "torch.fft.torch.fft.irfft2", "path": "generated/torch.fft.irfft2", "type": "Discrete Fourier Transforms", "text": ["Computes the inverse of rfft2(). Equivalent to irfftn() but IFFTs only the last two dimensions by default.", "input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfft2(). By the Hermitian property, the output will be real-valued.", "Note", "Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape s.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions. With default arguments, the size of last dimension should be (2^n + 1) as argument s defaults to even output size = 2 * (last_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (irfft2()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (rfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft2() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Without specifying the output length to irfft2(), the output will not round-trip properly because the input is odd-length in the last dimension:", "So, it is recommended to always pass the signal shape s."]}, {"name": "torch.fft.torch.fft.irfftn", "path": "generated/torch.fft.irfftn", "type": "Discrete Fourier Transforms", "text": ["Computes the inverse of rfftn().", "input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfftn(). By the Hermitian property, the output will be real-valued.", "Note", "Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape s.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions. With default arguments, the size of last dimension should be (2^n + 1) as argument s defaults to even output size = 2 * (last_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (irfftn()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (rfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfftn() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Without specifying the output length to irfft(), the output will not round-trip properly because the input is odd-length in the last dimension:", "So, it is recommended to always pass the signal shape s."]}, {"name": "torch.fft.torch.fft.rfft", "path": "generated/torch.fft.rfft", "type": "Discrete Fourier Transforms", "text": ["Computes the one dimensional Fourier transform of real-valued input.", "The FFT of a real signal is Hermitian-symmetric, X[i] = conj(X[-i]) so the output contains only the positive frequencies below the Nyquist frequency. To compute the full output, use fft()", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (rfft()), these correspond to:", "Calling the backward transform (irfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Compare against the full output from fft():", "Notice that the symmetric element T[-1] == T[1].conj() is omitted. At the Nyquist frequency T[-2] == T[2] is it\u2019s own symmetric pair, and therefore must always be real-valued."]}, {"name": "torch.fft.torch.fft.rfft2", "path": "generated/torch.fft.rfft2", "type": "Discrete Fourier Transforms", "text": ["Computes the 2-dimensional discrete Fourier transform of real input. Equivalent to rfftn() but FFTs only the last two dimensions by default.", "The FFT of a real signal is Hermitian-symmetric, X[i, j] = conj(X[-i, -j]), so the full fft2() output contains redundant information. rfft2() instead omits the negative frequencies in the last dimension.", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (rfft2()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (irfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft2() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Compared against the full output from fft2(), we have all elements up to the Nyquist frequency.", "The discrete Fourier transform is separable, so rfft2() here is equivalent to a combination of fft() and rfft():"]}, {"name": "torch.fft.torch.fft.rfftfreq", "path": "generated/torch.fft.rfftfreq", "type": "Discrete Fourier Transforms", "text": ["Computes the sample frequencies for rfft() with a signal of size n.", "Note", "rfft() returns Hermitian one-sided output, so only the positive frequency terms are returned. For a real FFT of length n and with inputs spaced in length unit d, the frequencies are:", "Note", "For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. Unlike fftfreq(), rfftfreq() always returns it as positive.", "Compared to the output from fftfreq(), we see that the Nyquist frequency at f[2] has changed sign: >>> torch.fft.fftfreq(4) tensor([ 0.0000, 0.2500, -0.5000, -0.2500])"]}, {"name": "torch.fft.torch.fft.rfftn", "path": "generated/torch.fft.rfftn", "type": "Discrete Fourier Transforms", "text": ["Computes the N-dimensional discrete Fourier transform of real input.", "The FFT of a real signal is Hermitian-symmetric, X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n]) so the full fftn() output contains redundant information. rfftn() instead omits the negative frequencies in the last dimension.", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (rfftn()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (irfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfftn() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Compared against the full output from fftn(), we have all elements up to the Nyquist frequency.", "The discrete Fourier transform is separable, so rfftn() here is equivalent to a combination of fft() and rfft():"]}, {"name": "torch.fix", "path": "generated/torch.fix", "type": "Torch", "text": ["Alias for torch.trunc()"]}, {"name": "torch.flatten", "path": "generated/torch.flatten", "type": "Torch", "text": ["Flattens input by reshaping it into a one-dimensional tensor. If start_dim or end_dim are passed, only dimensions starting with start_dim and ending with end_dim are flattened. The order of elements in input is unchanged.", "Unlike NumPy\u2019s flatten, which always copies input\u2019s data, this function may return the original object, a view, or copy. If no dimensions are flattened, then the original object input is returned. Otherwise, if input can be viewed as the flattened shape, then that view is returned. Finally, only if the input cannot be viewed as the flattened shape is input\u2019s data copied. See torch.Tensor.view() for details on when a view will be returned.", "Note", "Flattening a zero-dimensional tensor will return a one-dimensional view.", "Example:"]}, {"name": "torch.flip", "path": "generated/torch.flip", "type": "Torch", "text": ["Reverse the order of an n-D tensor along given axis in dims.", "Note", "torch.flip makes a copy of input\u2019s data. This is different from NumPy\u2019s np.flip, which returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data, torch.flip is expected to be slower than np.flip.", "Example:"]}, {"name": "torch.fliplr", "path": "generated/torch.fliplr", "type": "Torch", "text": ["Flip tensor in the left/right direction, returning a new tensor.", "Flip the entries in each row in the left/right direction. Columns are preserved, but appear in a different order than before.", "Note", "Requires the tensor to be at least 2-D.", "Note", "torch.fliplr makes a copy of input\u2019s data. This is different from NumPy\u2019s np.fliplr, which returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data, torch.fliplr is expected to be slower than np.fliplr.", "input (Tensor) \u2013 Must be at least 2-dimensional.", "Example:"]}, {"name": "torch.flipud", "path": "generated/torch.flipud", "type": "Torch", "text": ["Flip tensor in the up/down direction, returning a new tensor.", "Flip the entries in each column in the up/down direction. Rows are preserved, but appear in a different order than before.", "Note", "Requires the tensor to be at least 1-D.", "Note", "torch.flipud makes a copy of input\u2019s data. This is different from NumPy\u2019s np.flipud, which returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data, torch.flipud is expected to be slower than np.flipud.", "input (Tensor) \u2013 Must be at least 1-dimensional.", "Example:"]}, {"name": "torch.float_power", "path": "generated/torch.float_power", "type": "Torch", "text": ["Raises input to the power of exponent, elementwise, in double precision. If neither input is complex returns a torch.float64 tensor, and if one or more inputs is complex returns a torch.complex128 tensor.", "Note", "This function always computes in double precision, unlike torch.pow(), which implements more typical type promotion. This is useful when the computation needs to be performed in a wider or more precise dtype, or the results of the computation may contain fractional values not representable in the input dtypes, like when an integer base is raised to a negative integer exponent.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.FloatStorage", "path": "storage#torch.FloatStorage", "type": "Storage", "text": []}, {"name": "torch.FloatStorage.dtype", "path": "storage#torch.FloatStorage.dtype", "type": "Storage", "text": []}, {"name": "torch.floor", "path": "generated/torch.floor", "type": "Torch", "text": ["Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.", "For integer inputs, follows the array-api convention of returning a copy of the input tensor.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.floor_divide", "path": "generated/torch.floor_divide", "type": "Torch", "text": ["Note", "Before PyTorch 1.13 torch.floor_divide() incorrectly performed truncation division. To restore the previous behavior use torch.div() with rounding_mode='trunc'.", "Computes input divided by other, elementwise, and floors the result.", "Supports broadcasting to a common shape, type promotion, and integer and float inputs.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.fmax", "path": "generated/torch.fmax", "type": "Torch", "text": ["Computes the element-wise maximum of input and other.", "This is like torch.maximum() except it handles NaNs differently: if exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the maximum. Only if both elements are NaN is NaN propagated.", "This function is a wrapper around C++\u2019s std::fmax and is similar to NumPy\u2019s fmax function.", "Supports broadcasting to a common shape, type promotion, and integer and floating-point inputs.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.fmin", "path": "generated/torch.fmin", "type": "Torch", "text": ["Computes the element-wise minimum of input and other.", "This is like torch.minimum() except it handles NaNs differently: if exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the minimum. Only if both elements are NaN is NaN propagated.", "This function is a wrapper around C++\u2019s std::fmin and is similar to NumPy\u2019s fmin function.", "Supports broadcasting to a common shape, type promotion, and integer and floating-point inputs.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.fmod", "path": "generated/torch.fmod", "type": "Torch", "text": ["Applies C++\u2019s std::fmod entrywise. The result has the same sign as the dividend input and its absolute value is less than that of other.", "This function may be defined in terms of torch.div() as", "Supports broadcasting to a common shape, type promotion, and integer and float inputs.", "Note", "When the divisor is zero, returns NaN for floating point dtypes on both CPU and GPU; raises RuntimeError for integer division by zero on CPU; Integer division by zero on GPU may return any value.", "Note", "Complex inputs are not supported. In some cases, it is not mathematically possible to satisfy the definition of a modulo operation with complex numbers.", "See also", "torch.remainder() which implements Python\u2019s modulus operator. This one is defined using division rounding down the result.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.frac", "path": "generated/torch.frac", "type": "Torch", "text": ["Computes the fractional portion of each element in input.", "Example:"]}, {"name": "torch.frexp", "path": "generated/torch.frexp", "type": "Torch", "text": ["Decomposes input into mantissa and exponent tensors such that input=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}.", "The range of mantissa is the open interval (-1, 1).", "Supports float inputs.", "input (Tensor) \u2013 the input tensor", "out (tuple, optional) \u2013 the output tensors", "Example:"]}, {"name": "torch.from_dlpack", "path": "generated/torch.from_dlpack", "type": "Torch", "text": ["Converts a tensor from an external library into a torch.Tensor.", "The returned PyTorch tensor will share the memory with the input tensor (which may have come from another library). Note that in-place operations will therefore also affect the data of the input tensor. This may lead to unexpected issues (e.g., other libraries may have read-only flags or immutable data structures), so the user should only do this if they know for sure that this is fine.", "ext_tensor (object with __dlpack__ attribute, or a DLPack capsule) \u2013 ", "The tensor or DLPack capsule to convert.", "If ext_tensor is a tensor (or ndarray) object, it must support the __dlpack__ protocol (i.e., have a ext_tensor.__dlpack__ method). Otherwise ext_tensor may be a DLPack capsule, which is an opaque PyCapsule instance, typically produced by a to_dlpack function or method.", "Tensor", "Examples:"]}, {"name": "torch.from_numpy", "path": "generated/torch.from_numpy", "type": "Torch", "text": ["Creates a Tensor from a numpy.ndarray.", "The returned tensor and ndarray share the same memory. Modifications to the tensor will be reflected in the ndarray and vice versa. The returned tensor is not resizable.", "It currently accepts ndarray with dtypes of numpy.float64, numpy.float32, numpy.float16, numpy.complex64, numpy.complex128, numpy.int64, numpy.int32, numpy.int16, numpy.int8, numpy.uint8, and numpy.bool.", "Warning", "Writing to a tensor created from a read-only NumPy array is not supported and will result in undefined behavior.", "Example:"]}, {"name": "torch.frombuffer", "path": "generated/torch.frombuffer", "type": "Torch", "text": ["Creates a 1-dimensional Tensor from an object that implements the Python buffer protocol.", "Skips the first offset bytes in the buffer, and interprets the rest of the raw bytes as a 1-dimensional tensor of type dtype with count elements.", "Note that either of the following must be true:", "1. count is a positive non-zero number, and the total number of bytes in the buffer is less than offset plus count times the size (in bytes) of dtype.", "2. count is negative, and the length (number of bytes) of the buffer subtracted by the offset is a multiple of the size (in bytes) of dtype.", "The returned tensor and buffer share the same memory. Modifications to the tensor will be reflected in the buffer and vice versa. The returned tensor is not resizable.", "Note", "This function increments the reference count for the object that owns the shared memory. Therefore, such memory will not be deallocated before the returned tensor goes out of scope.", "Warning", "This function\u2019s behavior is undefined when passed an object implementing the buffer protocol whose data is not on the CPU. Doing so is likely to cause a segmentation fault.", "Warning", "This function does not try to infer the dtype (hence, it is not optional). Passing a different dtype than its source may result in unexpected behavior.", "buffer (object) \u2013 a Python object that exposes the buffer interface.", "Example:"]}, {"name": "torch.full", "path": "generated/torch.full", "type": "Torch", "text": ["Creates a tensor of size size filled with fill_value. The tensor\u2019s dtype is inferred from fill_value.", "Example:"]}, {"name": "torch.full_like", "path": "generated/torch.full_like", "type": "Torch", "text": ["Returns a tensor with the same size as input filled with fill_value. torch.full_like(input, fill_value) is equivalent to torch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device)."]}, {"name": "torch.func", "path": "func", "type": "JAX-like Function Transforms", "text": ["torch.func, previously known as \u201cfunctorch\u201d, is JAX-like composable function transforms for PyTorch.", "Note", "This library is currently in beta. What this means is that the features generally work (unless otherwise documented) and we (the PyTorch team) are committed to bringing this library forward. However, the APIs may change under user feedback and we don\u2019t have full coverage over PyTorch operations.", "If you have suggestions on the API or use-cases you\u2019d like to be covered, please open an GitHub issue or reach out. We\u2019d love to hear about how you\u2019re using the library.", "There are a number of use cases that are tricky to do in PyTorch today:", "Composing vmap(), grad(), and vjp() transforms allows us to express the above without designing a separate subsystem for each. This idea of composable function transforms comes from the JAX framework."]}, {"name": "torch.func.functional_call()", "path": "generated/torch.func.functional_call#torch.func.functional_call", "type": "JAX-like Function Transforms", "text": ["Performs a functional call on the module by replacing the module parameters and buffers with the provided ones.", "Note", "If the module has active parametrizations, passing a value in the parameter_and_buffer_dicts argument with the name set to the regular parameter name will completely disable the parametrization. If you want to apply the parametrization function to the value passed please set the key as {submodule_name}.parametrizations.{parameter_name}.original.", "Note", "If the module performs in-place operations on parameters/buffers, these will be reflected in the parameter_and_buffer_dicts input.", "Example:", "Note", "If the module has tied weights, whether or not functional_call respects the tying is determined by the tie_weights flag.", "Example:", "An example of passing mutliple dictionaries", "And here is an example of applying the grad transform over the parameters of a model.", "Note", "If the user does not need grad tracking outside of grad transforms, they can detach all of the parameters for better performance and memory usage", "Example:", "This means that the user cannot call grad_weight.backward(). However, if they don\u2019t need autograd tracking outside of the transforms, this will result in less memory usage and faster speeds.", "the result of calling module.", "Any"]}, {"name": "torch.func.functionalize()", "path": "generated/torch.func.functionalize#torch.func.functionalize", "type": "JAX-like Function Transforms", "text": ["functionalize is a transform that can be used to remove (intermediate) mutations and aliasing from a function, while preserving the function\u2019s semantics.", "functionalize(func) returns a new function with the same semantics as func, but with all intermediate mutations removed. Every inplace operation performed on an intermediate tensor: intermediate.foo_() gets replaced by its out-of-place equivalent: intermediate_updated = intermediate.foo().", "functionalize is useful for shipping a pytorch program off to backends or compilers that aren\u2019t able to easily represent mutations or aliasing operators.", "Returns a new \u201cfunctionalized\u201d function. It takes the same inputs as func, and has the same behavior, but any mutations (and optionally aliasing) performed on intermeidate tensors in the function will be removed.", "Callable", "functionalize will also remove mutations (and views) that were performed on function inputs. However to preserve semantics, functionalize will \u201cfix up\u201d the mutations after the transform has finished running, by detecting if any tensor inputs \u201cshould have\u201d been mutated, and copying the new data back to the inputs if necessary.", "Example:", "Finally, a helpful mental model for understanding functionalization is that most user pytorch programs are writting with the public torch API. When executed, torch operators are generally decomposed into our internal C++ \u201cATen\u201d API. The logic for functionalization happens entirely at the level of ATen. Functionalization knows how to take every aliasing operator in ATen, and map it to its non-aliasing equivalent (e.g. tensor.view({-1}) -> at::view_copy(tensor, {-1})), and how to take every mutating operator in ATen, and map it to its non-mutating equivalent (e.g. tensor.add_(1) -> at::add(tensor, -1)), while tracking aliases and mutations out-of-line to know when to fix things up. Information about which ATen operators are aliasing or mutating all comes from https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/native_functions.yaml."]}, {"name": "torch.func.grad()", "path": "generated/torch.func.grad#torch.func.grad", "type": "JAX-like Function Transforms", "text": ["grad operator helps computing gradients of func with respect to the input(s) specified by argnums. This operator can be nested to compute higher-order gradients.", "Function to compute gradients with respect to its inputs. By default, the output of the function is the gradient tensor(s) with respect to the first argument. If specified has_aux equals True, tuple of gradients and output auxiliary objects is returned. If argnums is a tuple of integers, a tuple of output gradients with respect to each argnums value is returned.", "Callable", "Example of using grad:", "When composed with vmap, grad can be used to compute per-sample-gradients:", "Example of using grad with has_aux and argnums:", "Note", "Using PyTorch torch.no_grad together with grad.", "Case 1: Using torch.no_grad inside a function:", "In this case, grad(f)(x) will respect the inner torch.no_grad.", "Case 2: Using grad inside torch.no_grad context manager:", "In this case, grad will respect the inner torch.no_grad, but not the outer one. This is because grad is a \u201cfunction transform\u201d: its result should not depend on the result of a context manager outside of f."]}, {"name": "torch.func.grad_and_value()", "path": "generated/torch.func.grad_and_value#torch.func.grad_and_value", "type": "JAX-like Function Transforms", "text": ["Returns a function to compute a tuple of the gradient and primal, or forward, computation.", "Function to compute a tuple of gradients with respect to its inputs and the forward computation. By default, the output of the function is a tuple of the gradient tensor(s) with respect to the first argument and the primal computation. If specified has_aux equals True, tuple of gradients and tuple of the forward computation with output auxiliary objects is returned. If argnums is a tuple of integers, a tuple of a tuple of the output gradients with respect to each argnums value and the forward computation is returned.", "Callable", "See grad() for examples"]}, {"name": "torch.func.hessian()", "path": "generated/torch.func.hessian#torch.func.hessian", "type": "JAX-like Function Transforms", "text": ["Computes the Hessian of func with respect to the arg(s) at index argnum via a forward-over-reverse strategy.", "The forward-over-reverse strategy (composing jacfwd(jacrev(func))) is a good default for good performance. It is possible to compute Hessians through other compositions of jacfwd() and jacrev() like jacfwd(jacfwd(func)) or jacrev(jacrev(func)).", "Returns a function that takes in the same inputs as func and returns the Hessian of func with respect to the arg(s) at argnums.", "Note", "You may see this API error out with \u201cforward-mode AD not implemented for operator X\u201d. If so, please file a bug report and we will prioritize it. An alternative is to use jacrev(jacrev(func)), which has better operator coverage.", "A basic usage with a R^N -> R^1 function gives a N x N Hessian:"]}, {"name": "torch.func.jacfwd()", "path": "generated/torch.func.jacfwd#torch.func.jacfwd", "type": "JAX-like Function Transforms", "text": ["Computes the Jacobian of func with respect to the arg(s) at index argnum using forward-mode autodiff", "Returns a function that takes in the same inputs as func and returns the Jacobian of func with respect to the arg(s) at argnums. If has_aux is True, then the returned function instead returns a (jacobian, aux) tuple where jacobian is the Jacobian and aux is auxiliary objects returned by func.", "Note", "You may see this API error out with \u201cforward-mode AD not implemented for operator X\u201d. If so, please file a bug report and we will prioritize it. An alternative is to use jacrev(), which has better operator coverage.", "A basic usage with a pointwise, unary operation will give a diagonal array as the Jacobian", "jacfwd() can be composed with vmap to produce batched Jacobians:", "If you would like to compute the output of the function as well as the jacobian of the function, use the has_aux flag to return the output as an auxiliary object:", "Additionally, jacrev() can be composed with itself or jacrev() to produce Hessians", "By default, jacfwd() computes the Jacobian with respect to the first input. However, it can compute the Jacboian with respect to a different argument by using argnums:", "Additionally, passing a tuple to argnums will compute the Jacobian with respect to multiple arguments"]}, {"name": "torch.func.jacrev()", "path": "generated/torch.func.jacrev#torch.func.jacrev", "type": "JAX-like Function Transforms", "text": ["Computes the Jacobian of func with respect to the arg(s) at index argnum using reverse mode autodiff", "Note", "Using chunk_size=1 is equivalent to computing the jacobian row-by-row with a for-loop i.e. the constraints of vmap() are not applicable.", "Returns a function that takes in the same inputs as func and returns the Jacobian of func with respect to the arg(s) at argnums. If has_aux is True, then the returned function instead returns a (jacobian, aux) tuple where jacobian is the Jacobian and aux is auxiliary objects returned by func.", "A basic usage with a pointwise, unary operation will give a diagonal array as the Jacobian", "If you would like to compute the output of the function as well as the jacobian of the function, use the has_aux flag to return the output as an auxiliary object:", "jacrev() can be composed with vmap to produce batched Jacobians:", "Additionally, jacrev() can be composed with itself to produce Hessians", "By default, jacrev() computes the Jacobian with respect to the first input. However, it can compute the Jacboian with respect to a different argument by using argnums:", "Additionally, passing a tuple to argnums will compute the Jacobian with respect to multiple arguments", "Note", "Using PyTorch torch.no_grad together with jacrev. Case 1: Using torch.no_grad inside a function:", "In this case, jacrev(f)(x) will respect the inner torch.no_grad.", "Case 2: Using jacrev inside torch.no_grad context manager:", "In this case, jacrev will respect the inner torch.no_grad, but not the outer one. This is because jacrev is a \u201cfunction transform\u201d: its result should not depend on the result of a context manager outside of f."]}, {"name": "torch.func.jvp()", "path": "generated/torch.func.jvp#torch.func.jvp", "type": "JAX-like Function Transforms", "text": ["Standing for the Jacobian-vector product, returns a tuple containing the output of func(*primals) and the \u201cJacobian of func evaluated at primals\u201d times tangents. This is also known as forward-mode autodiff.", "Returns a (output, jvp_out) tuple containing the output of func evaluated at primals and the Jacobian-vector product. If has_aux is True, then instead returns a (output, jvp_out, aux) tuple.", "Note", "You may see this API error out with \u201cforward-mode AD not implemented for operator X\u201d. If so, please file a bug report and we will prioritize it.", "jvp is useful when you wish to compute gradients of a function R^1 -> R^N", "jvp() can support functions with multiple inputs by passing in the tangents for each of the inputs"]}, {"name": "torch.func.linearize()", "path": "generated/torch.func.linearize#torch.func.linearize", "type": "JAX-like Function Transforms", "text": ["Returns the value of func at primals and linear approximation at primals.", "Returns a (output, jvp_fn) tuple containing the output of func applied to primals and a function that computes the jvp of func evaluated at primals.", "Tuple[Any, Callable]", "linearize is useful if jvp is to be computed multiple times at primals. However, to achieve this, linearize saves intermediate computation and has higher memory requrements than directly applying jvp. So, if all the tangents are known, it maybe more efficient to compute vmap(jvp) instead of using linearize.", "Note", "linearize evaluates func twice. Please file an issue for an implementation with a single evaluation."]}, {"name": "torch.func.Migrating from functorch to torch.func", "path": "func.migrating", "type": "JAX-like Function Transforms", "text": ["torch.func, previously known as \u201cfunctorch\u201d, is JAX-like composable function transforms for PyTorch.", "functorch started as an out-of-tree library over at the pytorch/functorch repository. Our goal has always been to upstream functorch directly into PyTorch and provide it as a core PyTorch library.", "As the final step of the upstream, we\u2019ve decided to migrate from being a top level package (functorch) to being a part of PyTorch to reflect how the function transforms are integrated directly into PyTorch core. As of PyTorch 2.0, we are deprecating import functorch and ask that users migrate to the newest APIs, which we will maintain going forward. import functorch will be kept around to maintain backwards compatibility for a couple of releases.", "The following APIs are a drop-in replacement for the following functorch APIs. They are fully backwards compatible.", "functorch API", "PyTorch API (as of PyTorch 2.0)", "functorch.vmap", "torch.vmap() or torch.func.vmap()", "functorch.grad", "torch.func.grad()", "functorch.vjp", "torch.func.vjp()", "functorch.jvp", "torch.func.jvp()", "functorch.jacrev", "torch.func.jacrev()", "functorch.jacfwd", "torch.func.jacfwd()", "functorch.hessian", "torch.func.hessian()", "functorch.functionalize", "torch.func.functionalize()", "Furthermore, if you are using torch.autograd.functional APIs, please try out the torch.func equivalents instead. torch.func function transforms are more composable and more performant in many cases.", "torch.autograd.functional API", "torch.func API (as of PyTorch 2.0)", "torch.autograd.functional.vjp()", "torch.func.grad() or torch.func.vjp()", "torch.autograd.functional.jvp()", "torch.func.jvp()", "torch.autograd.functional.jacobian()", "torch.func.jacrev() or torch.func.jacfwd()", "torch.autograd.functional.hessian()", "torch.func.hessian()", "We\u2019ve changed the APIs to apply function transforms over NN modules to make them fit better into the PyTorch design philosophy. The new API is different, so please read this section carefully.", "torch.func.functional_call() is the replacement for functorch.make_functional and functorch.make_functional_with_buffers. However, it is not a drop-in replacement.", "If you\u2019re in a hurry, you can use helper functions in this gist that emulate the behavior of functorch.make_functional and functorch.make_functional_with_buffers. We recommend using torch.func.functional_call() directly because it is a more explicit and flexible API.", "Concretely, functorch.make_functional returns a functional module and parameters. The functional module accepts parameters and inputs to the model as arguments. torch.func.functional_call() allows one to call the forward pass of an existing module using new parameters and buffers and inputs.", "Here\u2019s an example of how to compute gradients of parameters of a model using functorch vs torch.func:", "And here\u2019s an example of how to compute jacobians of model parameters:", "Note that it is important for memory consumption that you should only carry around a single copy of your parameters. model.named_parameters() does not copy the parameters. If in your model training you update the parameters of the model in-place, then the nn.Module that is your model has the single copy of the parameters and everything is OK.", "However, if you want to carry your parameters around in a dictionary and update them out-of-place, then there are two copies of parameters: the one in the dictionary and the one in the model. In this case, you should change model to not hold memory by converting it to the meta device via model.to('meta').", "Please use torch.func.stack_module_state() instead of functorch.combine_state_for_ensemble torch.func.stack_module_state() returns two dictionaries, one of stacked parameters, and one of stacked buffers, that can then be used with torch.vmap() and torch.func.functional_call() for ensembling.", "For example, here is an example of how to ensemble over a very simple model:", "We are no longer supporting functorch.compile (also known as AOTAutograd) as a frontend for compilation in PyTorch; we have integrated AOTAutograd into PyTorch\u2019s compilation story. If you are a user, please use torch.compile() instead."]}, {"name": "torch.func.replace_all_batch_norm_modules_()", "path": "generated/torch.func.replace_all_batch_norm_modules_#torch.func.replace_all_batch_norm_modules_", "type": "JAX-like Function Transforms", "text": ["In place updates root by setting the running_mean and running_var to be None and setting track_running_stats to be False for any nn.BatchNorm module in root", "Module"]}, {"name": "torch.func.stack_module_state()", "path": "generated/torch.func.stack_module_state#torch.func.stack_module_state", "type": "JAX-like Function Transforms", "text": ["Prepares a list of torch.nn.Modules for ensembling with vmap().", "Given a list of M nn.Modules of the same class, returns two dictionaries that stack all of their parameters and buffers together, indexed by name. The stacked parameters are optimizable (i.e. they are new leaf nodes in the autograd history that are unrelated to the original parameters and can be passed directly to an optimizer).", "Here\u2019s an example of how to ensemble over a very simple model:", "When there\u2019s submodules, this follows state dict naming conventions", "Warning", "All of the modules being stacked together must be the same (except for the values of their parameters/buffers). For example, they should be in the same mode (training vs eval).", "Tuple[Dict[str, Any], Dict[str, Any]]"]}, {"name": "torch.func.torch.func API Reference", "path": "func.api", "type": "JAX-like Function Transforms", "text": ["vmap is the vectorizing map; vmap(func) returns a new function that maps func over some dimension of the inputs.", "grad operator helps computing gradients of func with respect to the input(s) specified by argnums.", "Returns a function to compute a tuple of the gradient and primal, or forward, computation.", "Standing for the vector-Jacobian product, returns a tuple containing the results of func applied to primals and a function that, when given cotangents, computes the reverse-mode Jacobian of func with respect to primals times cotangents.", "Standing for the Jacobian-vector product, returns a tuple containing the output of func(*primals) and the \"Jacobian of func evaluated at primals\" times tangents.", "Returns the value of func at primals and linear approximation at primals.", "Computes the Jacobian of func with respect to the arg(s) at index argnum using reverse mode autodiff", "Computes the Jacobian of func with respect to the arg(s) at index argnum using forward-mode autodiff", "Computes the Hessian of func with respect to the arg(s) at index argnum via a forward-over-reverse strategy.", "functionalize is a transform that can be used to remove (intermediate) mutations and aliasing from a function, while preserving the function's semantics.", "In general, you can transform over a function that calls a torch.nn.Module. For example, the following is an example of computing a jacobian of a function that takes three values and returns three values:", "However, if you want to do something like compute a jacobian over the parameters of the model, then there needs to be a way to construct a function where the parameters are the inputs to the function. That\u2019s what functional_call() is for: it accepts an nn.Module, the transformed parameters, and the inputs to the Module\u2019s forward pass. It returns the value of running the Module\u2019s forward pass with the replaced parameters.", "Here\u2019s how we would compute the Jacobian over the parameters", "Performs a functional call on the module by replacing the module parameters and buffers with the provided ones.", "Prepares a list of torch.nn.Modules for ensembling with vmap().", "In place updates root by setting the running_mean and running_var to be None and setting track_running_stats to be False for any nn.BatchNorm module in root", "If you\u2019re looking for information on fixing Batch Norm modules, please follow the guidance here"]}, {"name": "torch.func.torch.func API Reference.Patching Batch Norm", "path": "func.batch_norm", "type": "JAX-like Function Transforms", "text": ["Batch Norm requires in-place updates to running_mean and running_var of the same size as the input. Functorch does not support inplace update to a regular tensor that takes in a batched tensor (i.e. regular.add_(batched) is not allowed). So when vmapping over a batch of inputs to a single module, we end up with this error", "One of the best supported ways is to switch BatchNorm for GroupNorm. Options 1 and 2 support this", "All of these options assume that you don\u2019t need running stats. If you\u2019re using a module this means that it\u2019s assumed you won\u2019t use batch norm in evaluation mode. If you have a use case that involves running batch norm with vmap in evaluation mode, please file an issue", "If you want to change for GroupNorm, anywhere that you have BatchNorm, replace it with:", "Here C is the same C as in the original BatchNorm. G is the number of groups to break C into. As such, C % G == 0 and as a fallback, you can set C == G, meaning each channel will be treated separately.", "If you must use BatchNorm and you\u2019ve built the module yourself, you can change the module to not use running stats. In other words, anywhere that there\u2019s a BatchNorm module, set the track_running_stats flag to be False", "Some torchvision models, like resnet and regnet, can take in a norm_layer parameter. These are often defaulted to be BatchNorm2d if they\u2019ve been defaulted.", "Instead you can set it to be GroupNorm.", "Here, once again, c % g == 0 so as a fallback, set g = c.", "If you are attached to BatchNorm, be sure to use a version that doesn\u2019t use running stats", "functorch has added some functionality to allow for quick, in-place patching of the module to not use running stats. Changing the norm layer is more fragile, so we have not offered that. If you have a net where you want the BatchNorm to not use running stats, you can run replace_all_batch_norm_modules_ to update the module in-place to not use running stats", "When run under eval mode, the running_mean and running_var will not be updated. Therefore, vmap can support this mode"]}, {"name": "torch.func.torch.func API Reference.torch.func.functional_call", "path": "generated/torch.func.functional_call", "type": "JAX-like Function Transforms", "text": ["Performs a functional call on the module by replacing the module parameters and buffers with the provided ones.", "Note", "If the module has active parametrizations, passing a value in the parameter_and_buffer_dicts argument with the name set to the regular parameter name will completely disable the parametrization. If you want to apply the parametrization function to the value passed please set the key as {submodule_name}.parametrizations.{parameter_name}.original.", "Note", "If the module performs in-place operations on parameters/buffers, these will be reflected in the parameter_and_buffer_dicts input.", "Example:", "Note", "If the module has tied weights, whether or not functional_call respects the tying is determined by the tie_weights flag.", "Example:", "An example of passing mutliple dictionaries", "And here is an example of applying the grad transform over the parameters of a model.", "Note", "If the user does not need grad tracking outside of grad transforms, they can detach all of the parameters for better performance and memory usage", "Example:", "This means that the user cannot call grad_weight.backward(). However, if they don\u2019t need autograd tracking outside of the transforms, this will result in less memory usage and faster speeds.", "the result of calling module.", "Any"]}, {"name": "torch.func.torch.func API Reference.torch.func.functionalize", "path": "generated/torch.func.functionalize", "type": "JAX-like Function Transforms", "text": ["functionalize is a transform that can be used to remove (intermediate) mutations and aliasing from a function, while preserving the function\u2019s semantics.", "functionalize(func) returns a new function with the same semantics as func, but with all intermediate mutations removed. Every inplace operation performed on an intermediate tensor: intermediate.foo_() gets replaced by its out-of-place equivalent: intermediate_updated = intermediate.foo().", "functionalize is useful for shipping a pytorch program off to backends or compilers that aren\u2019t able to easily represent mutations or aliasing operators.", "Returns a new \u201cfunctionalized\u201d function. It takes the same inputs as func, and has the same behavior, but any mutations (and optionally aliasing) performed on intermeidate tensors in the function will be removed.", "Callable", "functionalize will also remove mutations (and views) that were performed on function inputs. However to preserve semantics, functionalize will \u201cfix up\u201d the mutations after the transform has finished running, by detecting if any tensor inputs \u201cshould have\u201d been mutated, and copying the new data back to the inputs if necessary.", "Example:", "Finally, a helpful mental model for understanding functionalization is that most user pytorch programs are writting with the public torch API. When executed, torch operators are generally decomposed into our internal C++ \u201cATen\u201d API. The logic for functionalization happens entirely at the level of ATen. Functionalization knows how to take every aliasing operator in ATen, and map it to its non-aliasing equivalent (e.g. tensor.view({-1}) -> at::view_copy(tensor, {-1})), and how to take every mutating operator in ATen, and map it to its non-mutating equivalent (e.g. tensor.add_(1) -> at::add(tensor, -1)), while tracking aliases and mutations out-of-line to know when to fix things up. Information about which ATen operators are aliasing or mutating all comes from https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/native_functions.yaml."]}, {"name": "torch.func.torch.func API Reference.torch.func.grad", "path": "generated/torch.func.grad", "type": "JAX-like Function Transforms", "text": ["grad operator helps computing gradients of func with respect to the input(s) specified by argnums. This operator can be nested to compute higher-order gradients.", "Function to compute gradients with respect to its inputs. By default, the output of the function is the gradient tensor(s) with respect to the first argument. If specified has_aux equals True, tuple of gradients and output auxiliary objects is returned. If argnums is a tuple of integers, a tuple of output gradients with respect to each argnums value is returned.", "Callable", "Example of using grad:", "When composed with vmap, grad can be used to compute per-sample-gradients:", "Example of using grad with has_aux and argnums:", "Note", "Using PyTorch torch.no_grad together with grad.", "Case 1: Using torch.no_grad inside a function:", "In this case, grad(f)(x) will respect the inner torch.no_grad.", "Case 2: Using grad inside torch.no_grad context manager:", "In this case, grad will respect the inner torch.no_grad, but not the outer one. This is because grad is a \u201cfunction transform\u201d: its result should not depend on the result of a context manager outside of f."]}, {"name": "torch.func.torch.func API Reference.torch.func.grad_and_value", "path": "generated/torch.func.grad_and_value", "type": "JAX-like Function Transforms", "text": ["Returns a function to compute a tuple of the gradient and primal, or forward, computation.", "Function to compute a tuple of gradients with respect to its inputs and the forward computation. By default, the output of the function is a tuple of the gradient tensor(s) with respect to the first argument and the primal computation. If specified has_aux equals True, tuple of gradients and tuple of the forward computation with output auxiliary objects is returned. If argnums is a tuple of integers, a tuple of a tuple of the output gradients with respect to each argnums value and the forward computation is returned.", "Callable", "See grad() for examples"]}, {"name": "torch.func.torch.func API Reference.torch.func.hessian", "path": "generated/torch.func.hessian", "type": "JAX-like Function Transforms", "text": ["Computes the Hessian of func with respect to the arg(s) at index argnum via a forward-over-reverse strategy.", "The forward-over-reverse strategy (composing jacfwd(jacrev(func))) is a good default for good performance. It is possible to compute Hessians through other compositions of jacfwd() and jacrev() like jacfwd(jacfwd(func)) or jacrev(jacrev(func)).", "Returns a function that takes in the same inputs as func and returns the Hessian of func with respect to the arg(s) at argnums.", "Note", "You may see this API error out with \u201cforward-mode AD not implemented for operator X\u201d. If so, please file a bug report and we will prioritize it. An alternative is to use jacrev(jacrev(func)), which has better operator coverage.", "A basic usage with a R^N -> R^1 function gives a N x N Hessian:"]}, {"name": "torch.func.torch.func API Reference.torch.func.jacfwd", "path": "generated/torch.func.jacfwd", "type": "JAX-like Function Transforms", "text": ["Computes the Jacobian of func with respect to the arg(s) at index argnum using forward-mode autodiff", "Returns a function that takes in the same inputs as func and returns the Jacobian of func with respect to the arg(s) at argnums. If has_aux is True, then the returned function instead returns a (jacobian, aux) tuple where jacobian is the Jacobian and aux is auxiliary objects returned by func.", "Note", "You may see this API error out with \u201cforward-mode AD not implemented for operator X\u201d. If so, please file a bug report and we will prioritize it. An alternative is to use jacrev(), which has better operator coverage.", "A basic usage with a pointwise, unary operation will give a diagonal array as the Jacobian", "jacfwd() can be composed with vmap to produce batched Jacobians:", "If you would like to compute the output of the function as well as the jacobian of the function, use the has_aux flag to return the output as an auxiliary object:", "Additionally, jacrev() can be composed with itself or jacrev() to produce Hessians", "By default, jacfwd() computes the Jacobian with respect to the first input. However, it can compute the Jacboian with respect to a different argument by using argnums:", "Additionally, passing a tuple to argnums will compute the Jacobian with respect to multiple arguments"]}, {"name": "torch.func.torch.func API Reference.torch.func.jacrev", "path": "generated/torch.func.jacrev", "type": "JAX-like Function Transforms", "text": ["Computes the Jacobian of func with respect to the arg(s) at index argnum using reverse mode autodiff", "Note", "Using chunk_size=1 is equivalent to computing the jacobian row-by-row with a for-loop i.e. the constraints of vmap() are not applicable.", "Returns a function that takes in the same inputs as func and returns the Jacobian of func with respect to the arg(s) at argnums. If has_aux is True, then the returned function instead returns a (jacobian, aux) tuple where jacobian is the Jacobian and aux is auxiliary objects returned by func.", "A basic usage with a pointwise, unary operation will give a diagonal array as the Jacobian", "If you would like to compute the output of the function as well as the jacobian of the function, use the has_aux flag to return the output as an auxiliary object:", "jacrev() can be composed with vmap to produce batched Jacobians:", "Additionally, jacrev() can be composed with itself to produce Hessians", "By default, jacrev() computes the Jacobian with respect to the first input. However, it can compute the Jacboian with respect to a different argument by using argnums:", "Additionally, passing a tuple to argnums will compute the Jacobian with respect to multiple arguments", "Note", "Using PyTorch torch.no_grad together with jacrev. Case 1: Using torch.no_grad inside a function:", "In this case, jacrev(f)(x) will respect the inner torch.no_grad.", "Case 2: Using jacrev inside torch.no_grad context manager:", "In this case, jacrev will respect the inner torch.no_grad, but not the outer one. This is because jacrev is a \u201cfunction transform\u201d: its result should not depend on the result of a context manager outside of f."]}, {"name": "torch.func.torch.func API Reference.torch.func.jvp", "path": "generated/torch.func.jvp", "type": "JAX-like Function Transforms", "text": ["Standing for the Jacobian-vector product, returns a tuple containing the output of func(*primals) and the \u201cJacobian of func evaluated at primals\u201d times tangents. This is also known as forward-mode autodiff.", "Returns a (output, jvp_out) tuple containing the output of func evaluated at primals and the Jacobian-vector product. If has_aux is True, then instead returns a (output, jvp_out, aux) tuple.", "Note", "You may see this API error out with \u201cforward-mode AD not implemented for operator X\u201d. If so, please file a bug report and we will prioritize it.", "jvp is useful when you wish to compute gradients of a function R^1 -> R^N", "jvp() can support functions with multiple inputs by passing in the tangents for each of the inputs"]}, {"name": "torch.func.torch.func API Reference.torch.func.linearize", "path": "generated/torch.func.linearize", "type": "JAX-like Function Transforms", "text": ["Returns the value of func at primals and linear approximation at primals.", "Returns a (output, jvp_fn) tuple containing the output of func applied to primals and a function that computes the jvp of func evaluated at primals.", "Tuple[Any, Callable]", "linearize is useful if jvp is to be computed multiple times at primals. However, to achieve this, linearize saves intermediate computation and has higher memory requrements than directly applying jvp. So, if all the tangents are known, it maybe more efficient to compute vmap(jvp) instead of using linearize.", "Note", "linearize evaluates func twice. Please file an issue for an implementation with a single evaluation."]}, {"name": "torch.func.torch.func API Reference.torch.func.replace_all_batch_norm_modules_", "path": "generated/torch.func.replace_all_batch_norm_modules_", "type": "JAX-like Function Transforms", "text": ["In place updates root by setting the running_mean and running_var to be None and setting track_running_stats to be False for any nn.BatchNorm module in root", "Module"]}, {"name": "torch.func.torch.func API Reference.torch.func.stack_module_state", "path": "generated/torch.func.stack_module_state", "type": "JAX-like Function Transforms", "text": ["Prepares a list of torch.nn.Modules for ensembling with vmap().", "Given a list of M nn.Modules of the same class, returns two dictionaries that stack all of their parameters and buffers together, indexed by name. The stacked parameters are optimizable (i.e. they are new leaf nodes in the autograd history that are unrelated to the original parameters and can be passed directly to an optimizer).", "Here\u2019s an example of how to ensemble over a very simple model:", "When there\u2019s submodules, this follows state dict naming conventions", "Warning", "All of the modules being stacked together must be the same (except for the values of their parameters/buffers). For example, they should be in the same mode (training vs eval).", "Tuple[Dict[str, Any], Dict[str, Any]]"]}, {"name": "torch.func.torch.func API Reference.torch.func.vjp", "path": "generated/torch.func.vjp", "type": "JAX-like Function Transforms", "text": ["Standing for the vector-Jacobian product, returns a tuple containing the results of func applied to primals and a function that, when given cotangents, computes the reverse-mode Jacobian of func with respect to primals times cotangents.", "Returns a (output, vjp_fn) tuple containing the output of func applied to primals and a function that computes the vjp of func with respect to all primals using the cotangents passed to the returned function. If has_aux is True, then instead returns a (output, vjp_fn, aux) tuple. The returned vjp_fn function will return a tuple of each VJP.", "When used in simple cases, vjp() behaves the same as grad()", "However, vjp() can support functions with multiple outputs by passing in the cotangents for each of the outputs", "vjp() can even support outputs being Python structs", "The function returned by vjp() will compute the partials with respect to each of the primals", "primals are the positional arguments for f. All kwargs use their default value", "Note", "Using PyTorch torch.no_grad together with vjp. Case 1: Using torch.no_grad inside a function:", "In this case, vjp(f)(x) will respect the inner torch.no_grad.", "Case 2: Using vjp inside torch.no_grad context manager:", "In this case, vjp will respect the inner torch.no_grad, but not the outer one. This is because vjp is a \u201cfunction transform\u201d: its result should not depend on the result of a context manager outside of f."]}, {"name": "torch.func.torch.func API Reference.torch.func.vmap", "path": "generated/torch.func.vmap", "type": "JAX-like Function Transforms", "text": ["vmap is the vectorizing map; vmap(func) returns a new function that maps func over some dimension of the inputs. Semantically, vmap pushes the map into PyTorch operations called by func, effectively vectorizing those operations.", "vmap is useful for handling batch dimensions: one can write a function func that runs on examples and then lift it to a function that can take batches of examples with vmap(func). vmap can also be used to compute batched gradients when composed with autograd.", "Note", "torch.vmap() is aliased to torch.func.vmap() for convenience. Use whichever one you\u2019d like.", "Returns a new \u201cbatched\u201d function. It takes the same inputs as func, except each input has an extra dimension at the index specified by in_dims. It takes returns the same outputs as func, except each output has an extra dimension at the index specified by out_dims.", "Callable", "One example of using vmap() is to compute batched dot products. PyTorch doesn\u2019t provide a batched torch.dot API; instead of unsuccessfully rummaging through docs, use vmap() to construct a new function.", "vmap() can be helpful in hiding batch dimensions, leading to a simpler model authoring experience.", "vmap() can also help vectorize computations that were previously difficult or impossible to batch. One example is higher-order gradient computation. The PyTorch autograd engine computes vjps (vector-Jacobian products). Computing a full Jacobian matrix for some function f: R^N -> R^N usually requires N calls to autograd.grad, one per Jacobian row. Using vmap(), we can vectorize the whole computation, computing the Jacobian in a single call to autograd.grad.", "vmap() can also be nested, producing an output with multiple batched dimensions", "If the inputs are not batched along the first dimension, in_dims specifies the dimension that each inputs are batched along as", "If there are multiple inputs each of which is batched along different dimensions, in_dims must be a tuple with the batch dimension for each input as", "If the input is a Python struct, in_dims must be a tuple containing a struct matching the shape of the input:", "By default, the output is batched along the first dimension. However, it can be batched along any dimension by using out_dims", "For any function that uses kwargs, the returned function will not batch the kwargs but will accept kwargs", "Note", "vmap does not provide general autobatching or handle variable-length sequences out of the box."]}, {"name": "torch.func.torch.func Whirlwind Tour", "path": "func.whirlwind_tour", "type": "JAX-like Function Transforms", "text": ["torch.func, previously known as functorch, is a library for JAX-like composable function transforms in PyTorch.", "There are a number of use cases that are tricky to do in PyTorch today: - computing per-sample-gradients (or other per-sample quantities)", "Composing vmap(), grad(), vjp(), and jvp() transforms allows us to express the above without designing a separate subsystem for each.", "grad(func) is our gradient computation transform. It returns a new function that computes the gradients of func. It assumes func returns a single-element Tensor and by default it computes the gradients of the output of func w.r.t. to the first input.", "Note: vmap() imposes restrictions on the code that it can be used on. For more details, please see UX Limitations.", "vmap(func)(*inputs) is a transform that adds a dimension to all Tensor operations in func. vmap(func) returns a new function that maps func over some dimension (default: 0) of each Tensor in inputs.", "vmap is useful for hiding batch dimensions: one can write a function func that runs on examples and then lift it to a function that can take batches of examples with vmap(func), leading to a simpler modeling experience:", "When composed with grad(), vmap() can be used to compute per-sample-gradients:", "The vjp() transform applies func to inputs and returns a new function that computes the vector-Jacobian product (vjp) given some cotangents Tensors.", "The jvp() transforms computes Jacobian-vector-products and is also known as \u201cforward-mode AD\u201d. It is not a higher-order function unlike most other transforms, but it returns the outputs of func(inputs) as well as the jvps.", "The jacrev() transform returns a new function that takes in x and returns the Jacobian of the function with respect to x using reverse-mode AD.", "jacrev() can be composed with vmap() to produce batched jacobians:", "jacfwd() is a drop-in replacement for jacrev that computes Jacobians using forward-mode AD:", "Composing jacrev() with itself or jacfwd() can produce hessians:", "hessian() is a convenience function that combines jacfwd and jacrev:"]}, {"name": "torch.func.UX Limitations", "path": "func.ux_limitations", "type": "JAX-like Function Transforms", "text": ["torch.func, like JAX, has restrictions around what can be transformed. In general, JAX\u2019s limitations are that transforms only work with pure functions: that is, functions where the output is completely determined by the input and that do not involve side effects (like mutation).", "We have a similar guarantee: our transforms work well with pure functions. However, we do support certain in-place operations. On one hand, writing code compatible with function transforms may involve changing how you write PyTorch code, on the other hand, you may find that our transforms let you express things that were previously difficult to express in PyTorch.", "All torch.func transforms share a limitation in that a function should not assign to global variables. Instead, all outputs to a function must be returned from the function. This restriction comes from how torch.func is implemented: each transform wraps Tensor inputs in special torch.func Tensor subclasses that facilitate the transform.", "So, instead of the following:", "Please rewrite f to return intermediate:", "If you are trying to use a torch.autograd API like torch.autograd.grad or torch.autograd.backward inside of a function being transformed by vmap() or one of torch.func\u2019s AD transforms (vjp(), jvp(), jacrev(), jacfwd()), the transform may not be able to transform over it. If it is unable to do so, you\u2019ll receive an error message.", "This is a fundamental design limitation in how PyTorch\u2019s AD support is implemented and the reason why we designed the torch.func library. Please instead use the torch.func equivalents of the torch.autograd APIs: - torch.autograd.grad, Tensor.backward -> torch.func.vjp or torch.func.grad - torch.autograd.functional.jvp -> torch.func.jvp - torch.autograd.functional.jacobian -> torch.func.jacrev or torch.func.jacfwd - torch.autograd.functional.hessian -> torch.func.hessian", "Note", "vmap() is our most restrictive transform. The grad-related transforms (grad(), vjp(), jvp()) do not have these limitations. jacfwd() (and hessian(), which is implemented with jacfwd()) is a composition of vmap() and jvp() so it also has these limitations.", "vmap(func) is a transform that returns a function that maps func over some new dimension of each input Tensor. The mental model for vmap is that it is like running a for-loop: for pure functions (i.e. in the absence of side effects), vmap(f)(x) is equivalent to:", "In the presence of side effects, vmap() no longer acts like it is running a for-loop. For example, the following function:", "will print \u201chello!\u201d once and pop only one element from lst.", "vmap() executes f a single time, so all side effects only happen once.", "This is a consequence of how vmap is implemented. torch.func has a special, internal BatchedTensor class. vmap(f)(*inputs) takes all Tensor inputs, turns them into BatchedTensors, and calls f(*batched_tensor_inputs). BatchedTensor overrides the PyTorch API to produce batched (i.e. vectorized) behavior for each PyTorch operator.", "You might be here due to receiving an error about vmap-incompatible in-place operations. vmap() will raise an error if it encounters an unsupported PyTorch in-place operation and it will succeed otherwise. Unsupported operations are those that would cause a Tensor with more elements to be written to a Tensor with fewer elements. Here\u2019s an example of how this can occur:", "x is a Tensor with one element, y is a Tensor with three elements. x + y has three elements (due to broadcasting), but attempting to write three elements back into x, which only has one element, raises an error due to attempting to write three elements into a Tensor with a single element.", "There is no problem if the Tensor being written to is batched under vmap() (i.e. it is being vmapped over).", "One common fix for this is to replace calls to factory functions with their \u201cnew_*\u201d equivalent. For example:", "To see why this helps, consider the following.", "Inside of vmap(), result is a Tensor of shape [3, 3]. However, although vec looks like it has shape [3], vec actually has underlying shape [2, 3]. It is not possible to copy vec into result.diagonal(), which has shape [3], because it has too many elements.", "Replacing torch.zeros() with Tensor.new_zeros() makes it so that result has an underlying Tensor of shape [2, 3, 3], so it is now possible to copy vec, which has underlying shape [2, 3], into result.diagonal().", "vmap() doesn\u2019t support the out= keyword argument in PyTorch operations. It will error out gracefully if it encounters that in your code.", "This is not a fundamental limitation; we could theoretically support this in the future but we have chosen not to for now.", "We don\u2019t yet support vmap over data-dependent control flow. Data-dependent control flow is when the condition of an if-statement, while-loop, or for-loop is a Tensor that is being vmap\u2019ed over. For example, the following will raise an error message:", "However, any control flow that is not dependent on the values in vmap\u2019ed tensors will work:", "JAX supports transforming over data-dependent control flow using special control flow operators (e.g. jax.lax.cond, jax.lax.while_loop). We\u2019re investigating adding equivalents of those to PyTorch.", "We do not (and will not) support vmap over a user-defined function that calls .item() on a Tensor. For example, the following will raise an error message:", "Please try to rewrite your code to not use .item() calls.", "You may also encounter an error message about using .item() but you might not have used it. In those cases, it is possible that PyTorch internally is calling .item() \u2013 please file an issue on GitHub and we\u2019ll fix PyTorch internals.", "vmap(f) requires that f applied to every \u201cexample\u201d in your input returns a Tensor with the same shape. Operations such as torch.nonzero, torch.is_nonzero are not supported and will error as a result.", "To see why, consider the following example:", "torch.nonzero(xs[0]) returns a Tensor of shape 2; but torch.nonzero(xs[1]) returns a Tensor of shape 1. We are unable to construct a single Tensor as an output; the output would need to be a ragged Tensor (and PyTorch does not yet have the concept of a ragged Tensor).", "The user\u2019s intention when calling a random operation can be unclear. Specifically, some users may want the random behavior to be the same across batches while others may want it to differ across batches. To address this, vmap takes a randomness flag.", "The flag can only be passed to vmap and can take on 3 values, \u201cerror,\u201d \u201cdifferent,\u201d or \u201csame,\u201d defaulting to error. Under \u201cerror\u201d mode, any call to a random function will produce an error asking the user to use one of the other two flags based on their use case.", "Under \u201cdifferent\u201d randomness, elements in a batch produce different random values. For instance,", "Under \u201csame\u201d randomness, elements in a batch produce same random values. For instance,", "Warning", "Our system only determine the randomness behavior of PyTorch operators and cannot control the behavior of other libraries, like numpy. This is similar to JAX\u2019s limitations with their solutions", "Note", "Multiple vmap calls using either type of supported randomness will not produce the same results. Like with standard PyTorch, a user can get randomness reproducibility through either using torch.manual_seed() outside of vmap or by using generators.", "Note", "Finally, our randomness differs from JAX because we aren\u2019t using a stateless PRNG, in part because PyTorch doesn\u2019t have full support for a stateless PRNG. Instead, we\u2019ve introduced a flag system to allow for the most common forms of randomness that we see. If your use case does not fit these forms of randomness, please file an issue."]}, {"name": "torch.func.vjp()", "path": "generated/torch.func.vjp#torch.func.vjp", "type": "JAX-like Function Transforms", "text": ["Standing for the vector-Jacobian product, returns a tuple containing the results of func applied to primals and a function that, when given cotangents, computes the reverse-mode Jacobian of func with respect to primals times cotangents.", "Returns a (output, vjp_fn) tuple containing the output of func applied to primals and a function that computes the vjp of func with respect to all primals using the cotangents passed to the returned function. If has_aux is True, then instead returns a (output, vjp_fn, aux) tuple. The returned vjp_fn function will return a tuple of each VJP.", "When used in simple cases, vjp() behaves the same as grad()", "However, vjp() can support functions with multiple outputs by passing in the cotangents for each of the outputs", "vjp() can even support outputs being Python structs", "The function returned by vjp() will compute the partials with respect to each of the primals", "primals are the positional arguments for f. All kwargs use their default value", "Note", "Using PyTorch torch.no_grad together with vjp. Case 1: Using torch.no_grad inside a function:", "In this case, vjp(f)(x) will respect the inner torch.no_grad.", "Case 2: Using vjp inside torch.no_grad context manager:", "In this case, vjp will respect the inner torch.no_grad, but not the outer one. This is because vjp is a \u201cfunction transform\u201d: its result should not depend on the result of a context manager outside of f."]}, {"name": "torch.func.vmap()", "path": "generated/torch.func.vmap#torch.func.vmap", "type": "JAX-like Function Transforms", "text": ["vmap is the vectorizing map; vmap(func) returns a new function that maps func over some dimension of the inputs. Semantically, vmap pushes the map into PyTorch operations called by func, effectively vectorizing those operations.", "vmap is useful for handling batch dimensions: one can write a function func that runs on examples and then lift it to a function that can take batches of examples with vmap(func). vmap can also be used to compute batched gradients when composed with autograd.", "Note", "torch.vmap() is aliased to torch.func.vmap() for convenience. Use whichever one you\u2019d like.", "Returns a new \u201cbatched\u201d function. It takes the same inputs as func, except each input has an extra dimension at the index specified by in_dims. It takes returns the same outputs as func, except each output has an extra dimension at the index specified by out_dims.", "Callable", "One example of using vmap() is to compute batched dot products. PyTorch doesn\u2019t provide a batched torch.dot API; instead of unsuccessfully rummaging through docs, use vmap() to construct a new function.", "vmap() can be helpful in hiding batch dimensions, leading to a simpler model authoring experience.", "vmap() can also help vectorize computations that were previously difficult or impossible to batch. One example is higher-order gradient computation. The PyTorch autograd engine computes vjps (vector-Jacobian products). Computing a full Jacobian matrix for some function f: R^N -> R^N usually requires N calls to autograd.grad, one per Jacobian row. Using vmap(), we can vectorize the whole computation, computing the Jacobian in a single call to autograd.grad.", "vmap() can also be nested, producing an output with multiple batched dimensions", "If the inputs are not batched along the first dimension, in_dims specifies the dimension that each inputs are batched along as", "If there are multiple inputs each of which is batched along different dimensions, in_dims must be a tuple with the batch dimension for each input as", "If the input is a Python struct, in_dims must be a tuple containing a struct matching the shape of the input:", "By default, the output is batched along the first dimension. However, it can be batched along any dimension by using out_dims", "For any function that uses kwargs, the returned function will not batch the kwargs but will accept kwargs", "Note", "vmap does not provide general autobatching or handle variable-length sequences out of the box."]}, {"name": "torch.futures", "path": "futures", "type": "Miscellaneous", "text": ["This package provides a Future type that encapsulates an asynchronous execution and a set of utility functions to simplify operations on Future objects. Currently, the Future type is primarily used by the Distributed RPC Framework.", "Wrapper around a torch._C.Future which encapsulates an asynchronous execution of a callable, e.g. rpc_async(). It also exposes a set of APIs to add callback functions and set results.", "Warning", "GPU support is a beta feature, subject to changes.", "Append the given callback function to this Future, which will be run when the Future is completed. Multiple callbacks can be added to the same Future, but the order in which they will be executed cannot be guaranteed. The callback must take one argument, which is the reference to this Future. The callback function can use the value() method to get the value. Note that if this Future is already completed, the given callback will be run inline.", "We recommend that you use the then() method as it provides a way to synchronize after your callback has completed. add_done_callback can be cheaper if your callback does not return anything. But both then() and add_done_callback use the same callback registration API under the hood.", "With respect to GPU tensors, this method behaves in the same way as then().", "callback (Future) \u2013 a Callable that takes in one argument, which is the reference to this Future.", "Note", "Note that if the callback function throws, either through the original future being completed with an exception and calling fut.wait(), or through other code in the callback, error handling must be carefully taken care of. For example, if this callback later completes additional futures, those futures are not marked as completed with an error and the user is responsible for handling completion/waiting on those futures independently.", "Return True if this Future is done. A Future is done if it has a result or an exception.", "If the value contains tensors that reside on GPUs, Future.done() will return True even if the asynchronous kernels that are populating those tensors haven\u2019t yet completed running on the device, because at such stage the result is already usable, provided one performs the appropriate synchronizations (see wait()).", "bool", "Set an exception for this Future, which will mark this Future as completed with an error and trigger all attached callbacks. Note that when calling wait()/value() on this Future, the exception set here will be raised inline.", "result (BaseException) \u2013 the exception for this Future.", "Set the result for this Future, which will mark this Future as completed and trigger all attached callbacks. Note that a Future cannot be marked completed twice.", "If the result contains tensors that reside on GPUs, this method can be called even if the asynchronous kernels that are populating those tensors haven\u2019t yet completed running on the device, provided that the streams on which those kernels were enqueued are set as the current ones when this method is called. Put simply, it\u2019s safe to call this method immediately after launching those kernels, without any additional synchronization, as long as one doesn\u2019t change streams in between. This method will record events on all the relevant current streams and will use them to ensure proper scheduling for all the consumers of this Future.", "result (object) \u2013 the result object of this Future.", "Append the given callback function to this Future, which will be run when the Future is completed. Multiple callbacks can be added to the same Future, but the order in which they will be executed cannot be guaranteed (to enforce a certain order consider chaining: fut.then(cb1).then(cb2)). The callback must take one argument, which is the reference to this Future. The callback function can use the value() method to get the value. Note that if this Future is already completed, the given callback will be run immediately inline.", "If the Future\u2019s value contains tensors that reside on GPUs, the callback might be invoked while the async kernels that are populating those tensors haven\u2019t yet finished executing on the device. However, the callback will be invoked with some dedicated streams set as current (fetched from a global pool) which will be synchronized with those kernels. Hence any operation performed by the callback on these tensors will be scheduled on the device after the kernels complete. In other words, as long as the callback doesn\u2019t switch streams, it can safely manipulate the result without any additional synchronization. This is similar to the non-blocking behavior of wait().", "Similarly, if the callback returns a value that contains tensors that reside on a GPU, it can do so even if the kernels that are producing these tensors are still running on the device, as long as the callback didn\u2019t change streams during its execution. If one wants to change streams, one must be careful to re-synchronize them with the original streams, that is, those that were current when the callback was invoked.", "callback (Callable) \u2013 a Callable that takes this Future as the only argument.", "A new Future object that holds the return value of the callback and will be marked as completed when the given callback finishes.", "Future[S]", "Note", "Note that if the callback function throws, either through the original future being completed with an exception and calling fut.wait(), or through other code in the callback, the future returned by then will be marked appropriately with the encountered error. However, if this callback later completes additional futures, those futures are not marked as completed with an error and the user is responsible for handling completion/waiting on those futures independently.", "Obtain the value of an already-completed future.", "This method should only be called after a call to wait() has completed, or inside a callback function passed to then(). In other cases this Future may not yet hold a value and calling value() could fail.", "If the value contains tensors that reside on GPUs, then this method will not perform any additional synchronization. This should be done beforehand, separately, through a call to wait() (except within callbacks, for which it\u2019s already being taken care of by then()).", "The value held by this Future. If the function (callback or RPC) creating the value has thrown an error, this value() method will also throw an error.", "T", "Block until the value of this Future is ready.", "If the value contains tensors that reside on GPUs, then an additional synchronization is performed with the kernels (executing on the device) which may be asynchronously populating those tensors. Such sync is non-blocking, which means that wait() will insert the necessary instructions in the current streams to ensure that further operations enqueued on those streams will be properly scheduled after the async kernels but, once that is done, wait() will return, even if those kernels are still running. No further synchronization is required when accessing and using the values, as long as one doesn\u2019t change streams.", "The value held by this Future. If the function (callback or RPC) creating the value has thrown an error, this wait method will also throw an error.", "T", "Collects the provided Future objects into a single combined Future that is completed when all of the sub-futures are completed.", "futures (list) \u2013 a list of Future objects.", "Returns a Future object to a list of the passed in Futures.", "Future[List[Future]]", "Waits for all provided futures to be complete, and returns the list of completed values. If any of the futures encounters an error, the method will exit early and report the error not waiting for other futures to complete.", "futures (list) \u2013 a list of Future object.", "A list of the completed Future results. This method will throw an error if wait on any Future throws.", "List"]}, {"name": "torch.futures.collect_all()", "path": "futures#torch.futures.collect_all", "type": "Miscellaneous", "text": ["Collects the provided Future objects into a single combined Future that is completed when all of the sub-futures are completed.", "futures (list) \u2013 a list of Future objects.", "Returns a Future object to a list of the passed in Futures.", "Future[List[Future]]"]}, {"name": "torch.futures.Future", "path": "futures#torch.futures.Future", "type": "Miscellaneous", "text": ["Wrapper around a torch._C.Future which encapsulates an asynchronous execution of a callable, e.g. rpc_async(). It also exposes a set of APIs to add callback functions and set results.", "Warning", "GPU support is a beta feature, subject to changes.", "Append the given callback function to this Future, which will be run when the Future is completed. Multiple callbacks can be added to the same Future, but the order in which they will be executed cannot be guaranteed. The callback must take one argument, which is the reference to this Future. The callback function can use the value() method to get the value. Note that if this Future is already completed, the given callback will be run inline.", "We recommend that you use the then() method as it provides a way to synchronize after your callback has completed. add_done_callback can be cheaper if your callback does not return anything. But both then() and add_done_callback use the same callback registration API under the hood.", "With respect to GPU tensors, this method behaves in the same way as then().", "callback (Future) \u2013 a Callable that takes in one argument, which is the reference to this Future.", "Note", "Note that if the callback function throws, either through the original future being completed with an exception and calling fut.wait(), or through other code in the callback, error handling must be carefully taken care of. For example, if this callback later completes additional futures, those futures are not marked as completed with an error and the user is responsible for handling completion/waiting on those futures independently.", "Return True if this Future is done. A Future is done if it has a result or an exception.", "If the value contains tensors that reside on GPUs, Future.done() will return True even if the asynchronous kernels that are populating those tensors haven\u2019t yet completed running on the device, because at such stage the result is already usable, provided one performs the appropriate synchronizations (see wait()).", "bool", "Set an exception for this Future, which will mark this Future as completed with an error and trigger all attached callbacks. Note that when calling wait()/value() on this Future, the exception set here will be raised inline.", "result (BaseException) \u2013 the exception for this Future.", "Set the result for this Future, which will mark this Future as completed and trigger all attached callbacks. Note that a Future cannot be marked completed twice.", "If the result contains tensors that reside on GPUs, this method can be called even if the asynchronous kernels that are populating those tensors haven\u2019t yet completed running on the device, provided that the streams on which those kernels were enqueued are set as the current ones when this method is called. Put simply, it\u2019s safe to call this method immediately after launching those kernels, without any additional synchronization, as long as one doesn\u2019t change streams in between. This method will record events on all the relevant current streams and will use them to ensure proper scheduling for all the consumers of this Future.", "result (object) \u2013 the result object of this Future.", "Append the given callback function to this Future, which will be run when the Future is completed. Multiple callbacks can be added to the same Future, but the order in which they will be executed cannot be guaranteed (to enforce a certain order consider chaining: fut.then(cb1).then(cb2)). The callback must take one argument, which is the reference to this Future. The callback function can use the value() method to get the value. Note that if this Future is already completed, the given callback will be run immediately inline.", "If the Future\u2019s value contains tensors that reside on GPUs, the callback might be invoked while the async kernels that are populating those tensors haven\u2019t yet finished executing on the device. However, the callback will be invoked with some dedicated streams set as current (fetched from a global pool) which will be synchronized with those kernels. Hence any operation performed by the callback on these tensors will be scheduled on the device after the kernels complete. In other words, as long as the callback doesn\u2019t switch streams, it can safely manipulate the result without any additional synchronization. This is similar to the non-blocking behavior of wait().", "Similarly, if the callback returns a value that contains tensors that reside on a GPU, it can do so even if the kernels that are producing these tensors are still running on the device, as long as the callback didn\u2019t change streams during its execution. If one wants to change streams, one must be careful to re-synchronize them with the original streams, that is, those that were current when the callback was invoked.", "callback (Callable) \u2013 a Callable that takes this Future as the only argument.", "A new Future object that holds the return value of the callback and will be marked as completed when the given callback finishes.", "Future[S]", "Note", "Note that if the callback function throws, either through the original future being completed with an exception and calling fut.wait(), or through other code in the callback, the future returned by then will be marked appropriately with the encountered error. However, if this callback later completes additional futures, those futures are not marked as completed with an error and the user is responsible for handling completion/waiting on those futures independently.", "Obtain the value of an already-completed future.", "This method should only be called after a call to wait() has completed, or inside a callback function passed to then(). In other cases this Future may not yet hold a value and calling value() could fail.", "If the value contains tensors that reside on GPUs, then this method will not perform any additional synchronization. This should be done beforehand, separately, through a call to wait() (except within callbacks, for which it\u2019s already being taken care of by then()).", "The value held by this Future. If the function (callback or RPC) creating the value has thrown an error, this value() method will also throw an error.", "T", "Block until the value of this Future is ready.", "If the value contains tensors that reside on GPUs, then an additional synchronization is performed with the kernels (executing on the device) which may be asynchronously populating those tensors. Such sync is non-blocking, which means that wait() will insert the necessary instructions in the current streams to ensure that further operations enqueued on those streams will be properly scheduled after the async kernels but, once that is done, wait() will return, even if those kernels are still running. No further synchronization is required when accessing and using the values, as long as one doesn\u2019t change streams.", "The value held by this Future. If the function (callback or RPC) creating the value has thrown an error, this wait method will also throw an error.", "T"]}, {"name": "torch.futures.Future.add_done_callback()", "path": "futures#torch.futures.Future.add_done_callback", "type": "Miscellaneous", "text": ["Append the given callback function to this Future, which will be run when the Future is completed. Multiple callbacks can be added to the same Future, but the order in which they will be executed cannot be guaranteed. The callback must take one argument, which is the reference to this Future. The callback function can use the value() method to get the value. Note that if this Future is already completed, the given callback will be run inline.", "We recommend that you use the then() method as it provides a way to synchronize after your callback has completed. add_done_callback can be cheaper if your callback does not return anything. But both then() and add_done_callback use the same callback registration API under the hood.", "With respect to GPU tensors, this method behaves in the same way as then().", "callback (Future) \u2013 a Callable that takes in one argument, which is the reference to this Future.", "Note", "Note that if the callback function throws, either through the original future being completed with an exception and calling fut.wait(), or through other code in the callback, error handling must be carefully taken care of. For example, if this callback later completes additional futures, those futures are not marked as completed with an error and the user is responsible for handling completion/waiting on those futures independently."]}, {"name": "torch.futures.Future.done()", "path": "futures#torch.futures.Future.done", "type": "Miscellaneous", "text": ["Return True if this Future is done. A Future is done if it has a result or an exception.", "If the value contains tensors that reside on GPUs, Future.done() will return True even if the asynchronous kernels that are populating those tensors haven\u2019t yet completed running on the device, because at such stage the result is already usable, provided one performs the appropriate synchronizations (see wait()).", "bool"]}, {"name": "torch.futures.Future.set_exception()", "path": "futures#torch.futures.Future.set_exception", "type": "Miscellaneous", "text": ["Set an exception for this Future, which will mark this Future as completed with an error and trigger all attached callbacks. Note that when calling wait()/value() on this Future, the exception set here will be raised inline.", "result (BaseException) \u2013 the exception for this Future."]}, {"name": "torch.futures.Future.set_result()", "path": "futures#torch.futures.Future.set_result", "type": "Miscellaneous", "text": ["Set the result for this Future, which will mark this Future as completed and trigger all attached callbacks. Note that a Future cannot be marked completed twice.", "If the result contains tensors that reside on GPUs, this method can be called even if the asynchronous kernels that are populating those tensors haven\u2019t yet completed running on the device, provided that the streams on which those kernels were enqueued are set as the current ones when this method is called. Put simply, it\u2019s safe to call this method immediately after launching those kernels, without any additional synchronization, as long as one doesn\u2019t change streams in between. This method will record events on all the relevant current streams and will use them to ensure proper scheduling for all the consumers of this Future.", "result (object) \u2013 the result object of this Future."]}, {"name": "torch.futures.Future.then()", "path": "futures#torch.futures.Future.then", "type": "Miscellaneous", "text": ["Append the given callback function to this Future, which will be run when the Future is completed. Multiple callbacks can be added to the same Future, but the order in which they will be executed cannot be guaranteed (to enforce a certain order consider chaining: fut.then(cb1).then(cb2)). The callback must take one argument, which is the reference to this Future. The callback function can use the value() method to get the value. Note that if this Future is already completed, the given callback will be run immediately inline.", "If the Future\u2019s value contains tensors that reside on GPUs, the callback might be invoked while the async kernels that are populating those tensors haven\u2019t yet finished executing on the device. However, the callback will be invoked with some dedicated streams set as current (fetched from a global pool) which will be synchronized with those kernels. Hence any operation performed by the callback on these tensors will be scheduled on the device after the kernels complete. In other words, as long as the callback doesn\u2019t switch streams, it can safely manipulate the result without any additional synchronization. This is similar to the non-blocking behavior of wait().", "Similarly, if the callback returns a value that contains tensors that reside on a GPU, it can do so even if the kernels that are producing these tensors are still running on the device, as long as the callback didn\u2019t change streams during its execution. If one wants to change streams, one must be careful to re-synchronize them with the original streams, that is, those that were current when the callback was invoked.", "callback (Callable) \u2013 a Callable that takes this Future as the only argument.", "A new Future object that holds the return value of the callback and will be marked as completed when the given callback finishes.", "Future[S]", "Note", "Note that if the callback function throws, either through the original future being completed with an exception and calling fut.wait(), or through other code in the callback, the future returned by then will be marked appropriately with the encountered error. However, if this callback later completes additional futures, those futures are not marked as completed with an error and the user is responsible for handling completion/waiting on those futures independently."]}, {"name": "torch.futures.Future.value()", "path": "futures#torch.futures.Future.value", "type": "Miscellaneous", "text": ["Obtain the value of an already-completed future.", "This method should only be called after a call to wait() has completed, or inside a callback function passed to then(). In other cases this Future may not yet hold a value and calling value() could fail.", "If the value contains tensors that reside on GPUs, then this method will not perform any additional synchronization. This should be done beforehand, separately, through a call to wait() (except within callbacks, for which it\u2019s already being taken care of by then()).", "The value held by this Future. If the function (callback or RPC) creating the value has thrown an error, this value() method will also throw an error.", "T"]}, {"name": "torch.futures.Future.wait()", "path": "futures#torch.futures.Future.wait", "type": "Miscellaneous", "text": ["Block until the value of this Future is ready.", "If the value contains tensors that reside on GPUs, then an additional synchronization is performed with the kernels (executing on the device) which may be asynchronously populating those tensors. Such sync is non-blocking, which means that wait() will insert the necessary instructions in the current streams to ensure that further operations enqueued on those streams will be properly scheduled after the async kernels but, once that is done, wait() will return, even if those kernels are still running. No further synchronization is required when accessing and using the values, as long as one doesn\u2019t change streams.", "The value held by this Future. If the function (callback or RPC) creating the value has thrown an error, this wait method will also throw an error.", "T"]}, {"name": "torch.futures.wait_all()", "path": "futures#torch.futures.wait_all", "type": "Miscellaneous", "text": ["Waits for all provided futures to be complete, and returns the list of completed values. If any of the futures encounters an error, the method will exit early and report the error not waiting for other futures to complete.", "futures (list) \u2013 a list of Future object.", "A list of the completed Future results. This method will throw an error if wait on any Future throws.", "List"]}, {"name": "torch.fx", "path": "fx", "type": "FX", "text": ["FX is a toolkit for developers to use to transform nn.Module instances. FX consists of three main components: a symbolic tracer, an intermediate representation, and Python code generation. A demonstration of these components in action:", "The symbolic tracer performs \u201csymbolic execution\u201d of the Python code. It feeds fake values, called Proxies, through the code. Operations on theses Proxies are recorded. More information about symbolic tracing can be found in the symbolic_trace() and Tracer documentation.", "The intermediate representation is the container for the operations that were recorded during symbolic tracing. It consists of a list of Nodes that represent function inputs, callsites (to functions, methods, or torch.nn.Module instances), and return values. More information about the IR can be found in the documentation for Graph. The IR is the format on which transformations are applied.", "Python code generation is what makes FX a Python-to-Python (or Module-to-Module) transformation toolkit. For each Graph IR, we can create valid Python code matching the Graph\u2019s semantics. This functionality is wrapped up in GraphModule, which is a torch.nn.Module instance that holds a Graph as well as a forward method generated from the Graph.", "Taken together, this pipeline of components (symbolic tracing -> intermediate representation -> transforms -> Python code generation) constitutes the Python-to-Python transformation pipeline of FX. In addition, these components can be used separately. For example, symbolic tracing can be used in isolation to capture a form of the code for analysis (and not transformation) purposes. Code generation can be used for programmatically generating models, for example from a config file. There are many uses for FX!", "Several example transformations can be found at the examples repository.", "What is an FX transform? Essentially, it\u2019s a function that looks like this.", "Your transform will take in a torch.nn.Module, acquire a Graph from it, do some modifications, and return a new torch.nn.Module. You should think of the torch.nn.Module that your FX transform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another FX transform, you can pass it to TorchScript, or you can run it. Ensuring that the inputs and outputs of your FX transform are a torch.nn.Module will allow for composability.", "Note", "It is also possible to modify an existing GraphModule instead of creating a new one, like so:", "Note that you MUST call GraphModule.recompile() to bring the generated forward() method on the GraphModule in sync with the modified Graph.", "Given that you\u2019ve passed in a torch.nn.Module that has been traced into a Graph, there are now two primary approaches you can take to building a new Graph.", "Full treatment of the semantics of graphs can be found in the Graph documentation, but we are going to cover the basics here. A Graph is a data structure that represents a method on a GraphModule. The information that this requires is:", "All three of these concepts are represented with Node instances. Let\u2019s see what we mean by that with a short example:", "Here we define a module MyModule for demonstration purposes, instantiate it, symbolically trace it, then call the Graph.print_tabular() method to print out a table showing the nodes of this Graph:", "opcode", "name", "target", "args", "kwargs", "placeholder", "x", "x", "()", "{}", "get_attr", "linear_weight", "linear.weight", "()", "{}", "call_function", "add_1", "<built-in function add>", "(x, linear_weight)", "{}", "call_module", "linear_1", "linear", "(add_1,)", "{}", "call_method", "relu_1", "relu", "(linear_1,)", "{}", "call_function", "sum_1", "<built-in method sum \u2026>", "(relu_1,)", "{\u2018dim\u2019: -1}", "call_function", "topk_1", "<built-in method topk \u2026>", "(sum_1, 3)", "{}", "output", "output", "output", "(topk_1,)", "{}", "We can use this information to answer the questions we posed above.", "Given that we now know the basics of how code is represented in FX, we can now explore how we would edit a Graph.", "One approach to building this new Graph is to directly manipulate your old one. To aid in this, we can simply take the Graph we obtain from symbolic tracing and modify it. For example, let\u2019s say we desire to replace torch.add() calls with torch.mul() calls.", "We can also do more involved Graph rewrites, such as deleting or appending nodes. To aid in these transformations, FX has utility functions for transforming the graph that can be found in the Graph documentation. An example of using these APIs to append a torch.relu() call can be found below.", "For simple transformations that only consist of substitutions, you can also make use of the subgraph rewriter.", "FX also provides another level of automation on top of direct graph manipulation. The replace_pattern() API is essentially a \u201cfind/replace\u201d tool for editing Graphs. It allows you to specify a pattern and replacement function and it will trace through those functions, find instances of the group of operations in the pattern graph, and replace those instances with copies of the replacement graph. This can help to greatly automate tedious graph manipulation code, which can get unwieldy as the transformations get more complex.", "Another way of manipulating Graphs is by reusing the Proxy machinery used in symbolic tracing. For example, let\u2019s imagine that we wanted to write a transformation that decomposed PyTorch functions into smaller operations. It would transform every F.relu(x) call into (x > 0) * x. One possibility would be to perform the requisite graph rewriting to insert the comparison and multiplication after the F.relu, and then clean up the original F.relu. However, we can automate this process by using Proxy objects to automatically record operations into the Graph.", "To use this method, we write the operations that we want inserted as regular PyTorch code and invoke that code with Proxy objects as arguments. These Proxy objects will capture the operations that are performed on them and append them to the Graph.", "In addition to avoiding explicit graph manipulation, using Proxys also allows you to specify your rewrite rules as native Python code. For transformations that require a large amount of rewrite rules (such as vmap or grad), this can often improve readability and maintainability of the rules. Note that while calling Proxy we also passed a tracer pointing to the underlying variable graph. This is done so if in case the operations in graph are n-ary (e.g. add is a binary operator) the call to Proxy does not create multiple instances of a graph tracer which can lead to unexpected runtime errors. We recommend this method of using Proxy especially when the underlying operators can not be safely assumed to be unary.", "A worked example of using Proxys for Graph manipulation can be found here.", "A useful code organizational pattern in FX is to loop over all the Nodes in a Graph and execute them. This can be used for several things including runtime analysis of values flowing through the graph or transformation of the code via retracing with Proxys. For example, suppose we want to run a GraphModule and record the torch.Tensor shape and dtype properties on the nodes as we see them at runtime. That might look like:", "As you can see, a full interpreter for FX is not that complicated but it can be very useful. To ease using this pattern, we provide the Interpreter class, which encompasses the above logic in a way that certain aspects of the interpreter\u2019s execution can be overridden via method overrides.", "In addition to executing operations, we can also generate a new Graph by feeding Proxy values through an interpreter. Similarly, we provide the Transformer class to encompass this pattern. Transformer behaves similarly to Interpreter, but instead of calling the run method to get a concrete output value from the Module, you would call the Transformer.transform() method to return a new GraphModule which was subject to any transformation rules you installed as overridden methods.", "Often in the course of authoring transformations, our code will not be quite right. In this case, we may need to do some debugging. The key is to work backwards: first, check the results of invoking the generated module to prove or disprove correctness. Then, inspect and debug the generated code. Then, debug the process of transformations that led to the generated code.", "If you\u2019re not familiar with debuggers, please see the auxiliary section Available Debuggers.", "Because the output of most deep learning modules consists of floating point torch.Tensor instances, checking for equivalence between the results of two torch.nn.Module is not as straightforward as doing a simple equality check. To motivate this, let\u2019s use an example:", "Here, we\u2019ve tried to check equality of the values of two deep learning models with the == equality operator. However, this is not well- defined both due to the issue of that operator returning a tensor and not a bool, but also because comparison of floating point values should use a margin of error (or epsilon) to account for the non-commutativity of floating point operations (see here for more details). We can use torch.allclose() instead, which will give us an approximate comparison taking into account a relative and absolute tolerance threshold:", "This is the first tool in our toolbox to check if transformed modules are behaving as we expect compared to a reference implementation.", "Because FX generates the forward() function on GraphModules, using traditional debugging techniques like print statements or pdb is not as straightforward. Luckily, we have several techniques we can use for debugging the generated code.", "Invoke pdb to step into the running program. Although the code that represents the Graph is not in any source file, we can still step into it manually using pdb when the forward pass is invoked.", "If you\u2019d like to run the same code multiple times, then it can be a bit tedious to step to the right code with pdb. In that case, one approach is to simply copy-paste the generated forward pass into your code and examine it from there.", "GraphModule.to_folder() is a method in GraphModule that allows you to dump out the generated FX code to a folder. Although copying the forward pass into the code often suffices as in Print the Generated Code, it may be easier to examine modules and parameters using to_folder.", "After running the above example, we can then look at the code within foo/module.py and modify it as desired (e.g. adding print statements or using pdb) to debug the generated code.", "Now that we\u2019ve identified that a transformation is creating incorrect code, it\u2019s time to debug the transformation itself. First, we\u2019ll check the Limitations of Symbolic Tracing section in the documentation. Once we verify that tracing is working as expected, the goal becomes figuring out what went wrong during our GraphModule transformation. There may be a quick answer in Writing Transformations, but, if not, there are several ways to examine our traced module:", "Using the utility functions above, we can compare our traced Module before and after we\u2019ve applied our transformations. Sometimes, a simple visual comparison is enough to trace down a bug. If it\u2019s still not clear what\u2019s going wrong, a debugger like pdb can be a good next step.", "Going off of the example above, consider the following code:", "Using the above example, let\u2019s say that the call to print(traced) showed us that there was an error in our transforms. We want to find what goes wrong using a debugger. We start a pdb session. We can see what\u2019s happening during the transform by breaking on transform_graph(traced), then pressing s to \u201cstep into\u201d the call to transform_graph(traced).", "We may also have good luck by editing the print_tabular method to print different attributes of the Nodes in the Graph. (For example, we might want to see the Node\u2019s input_nodes and users.)", "The most common Python debugger is pdb. You can start your program in \u201cdebug mode\u201d with pdb by typing python -m pdb FILENAME.py into the command line, where FILENAME is the name of the file you want to debug. After that, you can use the pdb debugger commands to move through your running program stepwise. It\u2019s common to set a breakpoint (b LINE-NUMBER) when you start pdb, then call c to run the program until that point. This prevents you from having to step through each line of execution (using s or n) to get to the part of the code you want to examine. Alternatively, you can write import pdb; pdb.set_trace() before the line you want to break at. If you add pdb.set_trace(), your program will automatically start in debug mode when you run it. (In other words, you can just type python FILENAME.py into the command line instead of python -m pdb FILENAME.py.) Once you\u2019re running your file in debug mode, you can step through the code and examine your program\u2019s internal state using certain commands. There are many excellent tutorials on pdb online, including RealPython\u2019s \u201cPython Debugging With Pdb\u201d.", "IDEs like PyCharm or VSCode usually have a debugger built in. In your IDE, you can choose to either a) use pdb by pulling up a terminal window in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the built-in debugger (usually a graphical wrapper around pdb).", "FX uses a system of symbolic tracing (a.k.a symbolic execution) to capture the semantics of programs in a transformable/analyzable form. The system is tracing in that it executes the program (really a torch.nn.Module or function) to record operations. It is symbolic in that the data flowing through the program during this execution is not real data, but rather symbols (Proxy in FX parlance).", "Although symbolic tracing works for most neural net code, it has some limitations.", "The main limitation of symbolic tracing is it does not currently support dynamic control flow. That is, loops or if statements where the condition may depend on the input values of the program.", "For example, let\u2019s examine the following program:", "The condition to the if statement relies on the value of x.sum(), which relies on the value of x, a function input. Since x can change (i.e. if you pass a new input tensor to the traced function), this is dynamic control flow. The traceback walks back up through your code to show you where this situation happens.", "On the other hand, so-called static control flow is supported. Static control flow is loops or if statements whose value cannot change across invocations. Typically, in PyTorch programs, this control flow arises for code making decisions about a model\u2019s architecture based on hyper-parameters. As a concrete example:", "The if-statement if self.do_activation does not depend on any function inputs, thus it is static. do_activation can be considered to be a hyper-parameter, and the traces of different instances of MyModule with different values for that parameter have different code. This is a valid pattern that is supported by symbolic tracing.", "Many instances of dynamic control flow are semantically static control flow. These instances can be made to support symbolic tracing by removing the data dependencies on input values, for example by moving values to Module attributes or by binding concrete values to arguments during symbolic tracing:", "In the case of truly dynamic control flow, the sections of the program that contain this code can be traced as calls to the Method (see Customizing Tracing with the Tracer class) or function (see wrap()) rather than tracing through them.", "FX uses __torch_function__ as the mechanism by which it intercepts calls (see the technical overview for more information about this). Some functions, such as builtin Python functions or those in the math module, are not covered by __torch_function__, but we would still like to capture them in symbolic tracing. For example:", "The error tells us that the built-in function len is not supported. We can make it so that functions like this are recorded in the trace as direct calls using the wrap() API:", "The Tracer class is the class that underlies the implementation of symbolic_trace. The behavior of tracing can be customized by subclassing Tracer, like so:", "Leaf Modules are the modules that appear as calls in the symbolic trace rather than being traced through. The default set of leaf modules is the set of standard torch.nn module instances. For example:", "The set of leaf modules can be customized by overriding Tracer.is_leaf_module().", "Tensor constructors (e.g. torch.zeros, torch.ones, torch.rand, torch.randn, torch.sparse_coo_tensor) are currently not traceable.", "Type annotations", "Gotcha around training flag and submodules", "Symbolic tracing API", "Given an nn.Module or function instance root, this function will return a GraphModule constructed by recording operations seen while tracing through root.", "concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures.", "For example:", "FX can typically not trace through this due to the presence of control flow. However, we can use concrete_args to specialize on the value of b to trace through this:", "Note that although you can still pass in different values of b, they will be ignored.", "We can also use concrete_args to eliminate data-structure handling from our function. This will use pytrees to flatten your input. To avoid overspecializing, pass in fx.PH for values that shouldn\u2019t be specialized. For example:", "a Module created from the recorded operations from root.", "GraphModule", "Note", "Backwards-compatibility for this API is guaranteed.", "This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d. A \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being traced through:", "This function can also equivalently be used as a decorator:", "A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of \u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace rather than traced through.", "fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the graph when it\u2019s called", "Note", "Backwards-compatibility for this API is guaranteed.", "GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a graph attribute, as well as code and forward attributes generated from that graph.", "Warning", "When graph is reassigned, code and forward will be automatically regenerated. However, if you edit the contents of the graph without reassigning the graph attribute itself, you must call recompile() to update the generated code.", "Note", "Backwards-compatibility for this API is guaranteed.", "Construct a GraphModule.", "Note", "Backwards-compatibility for this API is guaranteed.", "Adds the given submodule to self.", "This installs empty Modules where none exist yet if they are subpaths of target.", "this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute)", "bool", "Note", "Backwards-compatibility for this API is guaranteed.", "Return the Python code generated from the Graph underlying this GraphModule.", "Deletes all unused submodules from self.", "A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node", "This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule.", "Note", "Backwards-compatibility for this API is guaranteed.", "Deletes the given submodule from self.", "The module will not be deleted if target is not a valid target.", "target (str) \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)", "submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule.", "bool", "Note", "Backwards-compatibility for this API is guaranteed.", "Return the Graph underlying this GraphModule", "Return the Python code generated for current GraphModule and its children GraphModules", "Warning", "This API is experimental and is NOT backward-compatible.", "Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date.", "Note", "Backwards-compatibility for this API is guaranteed.", "PythonCode", "imported with from <folder> import <module_name>", "Args:", "folder (Union[str, os.PathLike]): The folder to write the code out to", "writing out the code", "Warning", "This API is experimental and is NOT backward-compatible.", "Graph is the main data structure used in the FX Intermediate Representation. It consists of a series of Node s, each representing callsites (or other syntactic constructs). The list of Node s, taken together, constitute a valid Python function.", "For example, the following code", "Will produce the following Graph:", "For the semantics of operations represented in the Graph, please see Node.", "Note", "Backwards-compatibility for this API is guaranteed.", "Construct an empty Graph.", "Note", "Backwards-compatibility for this API is guaranteed.", "Insert a call_function Node into the Graph. A call_function node represents a call to a Python callable, specified by the_function.", "The newly created and inserted call_function node.", "Node", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node().", "Note", "Backwards-compatibility for this API is guaranteed.", "Insert a call_method Node into the Graph. A call_method node represents a call to a given method on the 0th element of args.", "The newly created and inserted call_method node.", "Node", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node().", "Note", "Backwards-compatibility for this API is guaranteed.", "Insert a call_module Node into the Graph. A call_module node represents a call to the forward() function of a Module in the Module hierarchy.", "The newly-created and inserted call_module node.", "Node", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node().", "Note", "Backwards-compatibility for this API is guaranteed.", "Create a Node and add it to the Graph at the current insert-point. Note that the current insert-point can be set via Graph.inserting_before() and Graph.inserting_after().", "The newly-created and inserted node.", "Node", "Note", "Backwards-compatibility for this API is guaranteed.", "Remove all dead code from the graph, based on each node\u2019s number of users, and whether the nodes have any side effects. The graph must be topologically sorted before calling.", "Whether the graph was changed as a result of the pass.", "bool", "Example:", "Before dead code is eliminated, a from a = x + 1 below has no users and thus can be eliminated from the graph without having an effect.", "After dead code is eliminated, a = x + 1 has been removed, and the rest of forward remains.", "Warning", "Dead code elimination has some heuristics to avoid removing side-effectful nodes (see Node.is_impure) but in general coverage is very bad, so you should assume that this method is not sound to call unless you know that your FX graph consists entirely of functional operations.", "Note", "Backwards-compatibility for this API is guaranteed.", "Erases a Node from the Graph. Throws an exception if there are still users of that node in the Graph.", "to_erase (Node) \u2013 The Node to erase from the Graph.", "Note", "Backwards-compatibility for this API is guaranteed.", "Insert a get_attr node into the Graph. A get_attr Node represents the fetch of an attribute from the Module hierarchy.", "The newly-created and inserted get_attr node.", "Node", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node.", "Note", "Backwards-compatibility for this API is guaranteed.", "Copy all nodes from a given graph into self.", "The value in self that is now equivalent to the output value in g, if g had an output node. None otherwise.", "Optional[Union[Tuple[Any, \u2026], List[Any], Dict[str, Any], slice, range, Node, str, int, float, bool, complex, dtype, Tensor, device, memory_format, layout, OpOverload]]", "Note", "Backwards-compatibility for this API is guaranteed.", "When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits:", "Args:", "the beginning of the entire graph.", "A resource manager that will restore the insert point on __exit__.", "Note", "Backwards-compatibility for this API is guaranteed.", "When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits:", "Args:", "the beginning of the entire graph.", "A resource manager that will restore the insert point on __exit__.", "Note", "Backwards-compatibility for this API is guaranteed.", "Runs various checks on this Graph to make sure it is well-formed. In particular: - Checks Nodes have correct ownership (owned by this graph) - Checks Nodes appear in topological order - If this Graph has an owning GraphModule, checks that targets exist in that GraphModule", "Note", "Backwards-compatibility for this API is guaranteed.", "Copy a node from one graph into another. arg_transform needs to transform arguments from the graph of node to the graph of self. Example:", "Node", "Note", "Backwards-compatibility for this API is guaranteed.", "Get the list of Nodes that constitute this Graph.", "Note that this Node list representation is a doubly-linked list. Mutations during iteration (e.g. delete a Node, add a Node) are safe.", "A doubly-linked list of Nodes. Note that reversed can be called on this list to switch iteration order.", "Register a transformer function when python code is generated", "a function that returns a code transformer to be registered. This function is called by on_generate_code to obtain the code transformer.", "This function is also given as its input the currently registered code transformer (or None if nothing is registered), in case it is not desirable to overwrite it. This is useful to chain code transformers together.", "a context manager that when used in a with statement, to automatically restore the previously registered code transformer.", "Example:", "This function can also be used as a context manager, with the benefit to automatically restores the previously registered code transformer:", "Warning", "This API is experimental and is NOT backward-compatible.", "Insert an output Node into the Graph. An output node represents a return statement in Python code. result is the value that should be returned.", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node.", "Note", "Backwards-compatibility for this API is guaranteed.", "Insert a placeholder node into the Graph. A placeholder represents a function input.", "Node", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node.", "Note", "Backwards-compatibility for this API is guaranteed.", "Prints the intermediate representation of the graph in tabular format. Note that this API requires the tabulate module to be installed.", "Note", "Backwards-compatibility for this API is guaranteed.", "Processes args so that they can be passed to the FX graph.", "Warning", "This API is experimental and is NOT backward-compatible.", "Warning", "This API is experimental and is NOT backward-compatible.", "Turn this Graph into valid Python code.", "root_module (str) \u2013 The name of the root module on which to look-up qualified name targets. This is usually \u2018self\u2019.", "src: the Python source code representing the object globals: a dictionary of global names in src -> the objects that they reference.", "A PythonCode object, consisting of two fields", "Note", "Backwards-compatibility for this API is guaranteed.", "Warning", "This API is experimental and is NOT backward-compatible.", "Node is the data structure that represents individual operations within a Graph. For the most part, Nodes represent callsites to various entities, such as operators, methods, and Modules (some exceptions include nodes that specify function inputs and outputs). Each Node has a function specified by its op property. The Node semantics for each value of op are as follows:", "Note", "Backwards-compatibility for this API is guaranteed.", "Return all Nodes that are inputs to this Node. This is equivalent to iterating over args and kwargs and only collecting the values that are Nodes.", "List of Nodes that appear in the args and kwargs of this Node, in that order.", "Insert x after this node in the list of nodes in the graph. Equivalent to self.next.prepend(x)", "x (Node) \u2013 The node to put after this node. Must be a member of the same graph.", "Note", "Backwards-compatibility for this API is guaranteed.", "The tuple of arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information.", "Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment.", "Return a descriptive string representation of self.", "This method can be used with no arguments as a debugging utility.", "This function is also used internally in the __str__ method of Graph. Together, the strings in placeholder_names and maybe_return_typename make up the signature of the autogenerated forward function in this Graph\u2019s surrounding GraphModule. placeholder_names and maybe_return_typename should not be used otherwise.", "in the __str__ method of Graph, and 2) self is a placeholder Node, return None. Otherwise, return a descriptive string representation of the current Node.", "str", "Note", "Backwards-compatibility for this API is guaranteed.", "Returns whether this op is impure, i.e. if its op is a placeholder or output, or if a call_function or call_module which is impure.", "If the op is impure or not.", "bool", "Warning", "This API is experimental and is NOT backward-compatible.", "The dict of keyword arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information.", "Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment.", "Returns the next Node in the linked list of Nodes.", "The next Node in the linked list of Nodes.", "Returns normalized arguments to Python targets. This means that args/kwargs will be matched up to the module/functional\u2019s signature and return exclusively kwargs in positional order if normalize_to_only_use_kwargs is true. Also populates default values. Does not support positional-only parameters or varargs parameters.", "Supports module calls.", "May require arg_types and kwarg_types in order to disambiguate overloads.", "Returns NamedTuple ArgsKwargsPair, or None if not successful.", "Optional[ArgsKwargsPair]", "Warning", "This API is experimental and is NOT backward-compatible.", "Insert x before this node in the list of nodes in the graph. Example:", "x (Node) \u2013 The node to put before this node. Must be a member of the same graph.", "Note", "Backwards-compatibility for this API is guaranteed.", "Returns the previous Node in the linked list of Nodes.", "The previous Node in the linked list of Nodes.", "Replace all uses of self in the Graph with the Node replace_with.", "The list of Nodes on which this change was made.", "List[Node]", "Note", "Backwards-compatibility for this API is guaranteed.", "Loop through input nodes of self, and replace all instances of old_input with new_input.", "Note", "Backwards-compatibility for this API is guaranteed.", "Return the Python stack trace that was recorded during tracing, if any. When traced with fx.Tracer, this property is usually populated by Tracer.create_proxy. To record stack traces during tracing for debug purposes, set record_stack_traces = True on the Tracer instance. When traced with dynamo, this property will be populated by default by OutputGraph.create_proxy.", "stack_trace would have the innermost frame at the end of the string.", "Update an existing positional argument to contain the new value arg. After calling, self.args[idx] == arg.", "Note", "Backwards-compatibility for this API is guaranteed.", "Update an existing keyword argument to contain the new value arg. After calling, self.kwargs[key] == arg.", "Note", "Backwards-compatibility for this API is guaranteed.", "Tracer is the class that implements the symbolic tracing functionality of torch.fx.symbolic_trace. A call to symbolic_trace(m) is equivalent to Tracer().trace(m).", "Tracer can be subclassed to override various behaviors of the tracing process. The different behaviors that can be overridden are described in the docstrings of the methods on this class.", "Note", "Backwards-compatibility for this API is guaranteed.", "Method that specifies the behavior of this Tracer when it encounters a call to an nn.Module instance.", "By default, the behavior is to check if the called module is a leaf module via is_leaf_module. If it is, emit a call_module node referring to m in the Graph. Otherwise, call the Module normally, tracing through the operations in its forward function.", "This method can be overridden to\u2013for example\u2013create nested traced GraphModules, or any other behavior you would want while tracing across Module boundaries.", "The return value from the Module call. In the case that a call_module node was emitted, this is a Proxy value. Otherwise, it is whatever value was returned from the Module invocation.", "Any", "Note", "Backwards-compatibility for this API is guaranteed.", "A method to specify the behavior of tracing when preparing values to be used as arguments to nodes in the Graph.", "By default, the behavior includes:", "Given a non-Proxy Tensor object, emit IR for various cases:", "This method can be overridden to support more types.", "a (Any) \u2013 The value to be emitted as an Argument in the Graph.", "The value a converted into the appropriate Argument", "Optional[Union[Tuple[Any, \u2026], List[Any], Dict[str, Any], slice, range, Node, str, int, float, bool, complex, dtype, Tensor, device, memory_format, layout, OpOverload]]", "Note", "Backwards-compatibility for this API is guaranteed.", "Create placeholder nodes corresponding to the signature of the root Module. This method introspects root\u2019s signature and emits those nodes accordingly, also supporting *args and **kwargs.", "Warning", "This API is experimental and is NOT backward-compatible.", "Inserts a graph node given target, args, kwargs, and name.", "This method can be overridden to do extra checking, validation, or modification of values used in node creation. For example, one might want to disallow in-place operations from being recorded.", "Note", "Backwards-compatibility for this API is guaranteed.", "Node", "Create a Node from the given arguments, then return the Node wrapped in a Proxy object.", "If kind = \u2018placeholder\u2019, then we\u2019re creating a Node that represents the parameter of a function. If we need to encode a default parameter, we use the args tuple. args is otherwise empty for placeholder Nodes.", "Note", "Backwards-compatibility for this API is guaranteed.", "Method that specifies the behavior of this Tracer when we call getattr on a call to an nn.Module instance.", "By default, the behavior is to return a proxy value for the attribute. It also stores the proxy value in the parameter_proxy_cache, so that future calls will reuse the proxy rather than creating a new one.", "This method can be overridden to \u2013for example\u2013 not return proxies when querying parameters.", "The return value from the getattr call.", "Warning", "This API is experimental and is NOT backward-compatible.", "A method to specify whether a given nn.Module is a \u201cleaf\u201d module.", "Leaf modules are the atomic units that appear in the IR, referenced by call_module calls. By default, Modules in the PyTorch standard library namespace (torch.nn) are leaf modules. All other modules are traced through and their constituent ops are recorded, unless specified otherwise via this parameter.", "bool", "Note", "Backwards-compatibility for this API is guaranteed.", "when used in control flow. Normally we don\u2019t know what to do because we don\u2019t know the value of the proxy, but a custom tracer can attach more information to the graph node using create_node and can choose to return an iterator.", "Note", "Backwards-compatibility for this API is guaranteed.", "Iterator", "This is what happens when ** is called on a proxy. This should return an iterator it ** is suppose to work in your custom tracer.", "Note", "Backwards-compatibility for this API is guaranteed.", "Any", "Helper method to find the qualified name of mod in the Module hierarchy of root. For example, if root has a submodule named foo, which has a submodule named bar, passing bar into this function will return the string \u201cfoo.bar\u201d.", "mod (str) \u2013 The Module to retrieve the qualified name for.", "str", "Note", "Backwards-compatibility for this API is guaranteed.", "Note", "Backwards-compatibility for this API is guaranteed.", "Proxy", "when used in control flow. Normally we don\u2019t know what to do because we don\u2019t know the value of the proxy, but a custom tracer can attach more information to the graph node using create_node and can choose to return a value.", "Note", "Backwards-compatibility for this API is guaranteed.", "bool", "Trace root and return the corresponding FX Graph representation. root can either be an nn.Module instance or a Python callable.", "Note that after this call, self.root may be different from the root passed in here. For example, when a free function is passed to trace(), we will create an nn.Module instance to use as the root and add embedded constants to.", "A Graph representing the semantics of the passed-in root.", "Graph", "Note", "Backwards-compatibility for this API is guaranteed.", "Proxy objects are Node wrappers that flow through the program during symbolic tracing and record all the operations (torch function calls, method calls, operators) that they touch into the growing FX Graph.", "If you\u2019re doing graph transforms, you can wrap your own Proxy method around a raw Node so that you can use the overloaded operators to add additional things to a Graph.", "Proxy objects cannot be iterated. In other words, the symbolic tracer will throw an error if a Proxy is used in a loop or as an *args/**kwargs function argument.", "There are two main ways around this: 1. Factor out the untraceable logic into a top-level function and use fx.wrap on it. 2. If the control flow is static (i.e. the loop trip count is based on some hyperparameter), the code can be kept in its original position and refactored into something like:", "For a more detailed description into the Proxy internals, check out the \u201cProxy\u201d section in torch/fx/OVERVIEW.md", "Note", "Backwards-compatibility for this API is guaranteed.", "An Interpreter executes an FX graph Node-by-Node. This pattern can be useful for many things, including writing code transformations as well as analysis passes.", "Methods in the Interpreter class can be overridden to customize the behavior of execution. The map of overrideable methods in terms of call hierarchy:", "Suppose we want to swap all instances of torch.neg with torch.sigmoid and vice versa (including their Tensor method equivalents). We could subclass Interpreter like so:", "Note", "Backwards-compatibility for this API is guaranteed.", "Run module via interpretation and return the result. This uses the \u201cboxed\u201d calling convention, where you pass a list of arguments, which will be cleared by the interpreter. This ensures that input tensors are promptly deallocated.", "Note", "Backwards-compatibility for this API is guaranteed.", "Execute a call_function node and return the result.", "Any", "Any: The value returned by the function invocation", "Note", "Backwards-compatibility for this API is guaranteed.", "Execute a call_method node and return the result.", "Any", "Any: The value returned by the method invocation", "Note", "Backwards-compatibility for this API is guaranteed.", "Execute a call_module node and return the result.", "Any", "Any: The value returned by the module invocation", "Note", "Backwards-compatibility for this API is guaranteed.", "Fetch the concrete values of args and kwargs of node n from the current execution environment.", "n (Node) \u2013 The node for which args and kwargs should be fetched.", "args and kwargs with concrete values for n.", "Tuple[Tuple, Dict]", "Note", "Backwards-compatibility for this API is guaranteed.", "Fetch an attribute from the Module hierarchy of self.module.", "target (str) \u2013 The fully-qualified name of the attribute to fetch", "The value of the attribute.", "Any", "Note", "Backwards-compatibility for this API is guaranteed.", "Execute a get_attr node. Will retrieve an attribute value from the Module hierarchy of self.module.", "The value of the attribute that was retrieved", "Any", "Note", "Backwards-compatibility for this API is guaranteed.", "Recursively descend through args and look up the concrete value for each Node in the current execution environment.", "Optional[Union[Tuple[Any, \u2026], List[Any], Dict[str, Any], slice, range, Node, str, int, float, bool, complex, dtype, Tensor, device, memory_format, layout, OpOverload]]", "Note", "Backwards-compatibility for this API is guaranteed.", "Execute an output node. This really just retrieves the value referenced by the output node and returns it.", "The return value referenced by the output node", "Any", "Note", "Backwards-compatibility for this API is guaranteed.", "Execute a placeholder node. Note that this is stateful: Interpreter maintains an internal iterator over arguments passed to run and this method returns next() on that iterator.", "The argument value that was retrieved.", "Any", "Note", "Backwards-compatibility for this API is guaranteed.", "Run module via interpretation and return the result.", "The value returned from executing the Module", "Any", "Note", "Backwards-compatibility for this API is guaranteed.", "Run a specific node n and return the result. Calls into placeholder, get_attr, call_function, call_method, call_module, or output depending on node.op", "n (Node) \u2013 The Node to execute", "The result of executing n", "Any", "Note", "Backwards-compatibility for this API is guaranteed.", "Transformer is a special type of interpreter that produces a new Module. It exposes a transform() method that returns the transformed Module. Transformer does not require arguments to run, as Interpreter does. Transformer works entirely symbolically.", "Suppose we want to swap all instances of torch.neg with torch.sigmoid and vice versa (including their Tensor method equivalents). We could subclass Transformer like so:", "module (GraphModule) \u2013 The Module to be transformed.", "Note", "Backwards-compatibility for this API is guaranteed.", "Note", "Backwards-compatibility for this API is guaranteed.", "Any", "Note", "Backwards-compatibility for this API is guaranteed.", "Any", "Execute a get_attr node. In Transformer, this is overridden to insert a new get_attr node into the output graph.", "Proxy", "Note", "Backwards-compatibility for this API is guaranteed.", "Execute a placeholder node. In Transformer, this is overridden to insert a new placeholder into the output graph.", "Proxy", "Note", "Backwards-compatibility for this API is guaranteed.", "Transform self.module and return the transformed GraphModule.", "Note", "Backwards-compatibility for this API is guaranteed.", "GraphModule", "Matches all possible non-overlapping sets of operators and their data dependencies (pattern) in the Graph of a GraphModule (gm), then replaces each of these matched subgraphs with another subgraph (replacement).", "A list of Match objects representing the places in the original graph that pattern was matched to. The list is empty if there are no matches. Match is defined as:", "List[Match]", "Examples:", "The above code will first match pattern in the forward method of traced_module. Pattern-matching is done based on use-def relationships, not node names. For example, if you had p = torch.cat([a, b]) in pattern, you could match m = torch.cat([a, b]) in the original forward function, despite the variable names being different (p vs m).", "The return statement in pattern is matched based on its value only; it may or may not match to the return statement in the larger graph. In other words, the pattern doesn\u2019t have to extend to the end of the larger graph.", "When the pattern is matched, it will be removed from the larger function and replaced by replacement. If there are multiple matches for pattern in the larger function, each non-overlapping match will be replaced. In the case of a match overlap, the first found match in the set of overlapping matches will be replaced. (\u201cFirst\u201d here being defined as the first in a topological ordering of the Nodes\u2019 use-def relationships. In most cases, the first Node is the parameter that appears directly after self, while the last Node is whatever the function returns.)", "One important thing to note is that the parameters of the pattern Callable must be used in the Callable itself, and the parameters of the replacement Callable must match the pattern. The first rule is why, in the above code block, the forward function has parameters x, w1, w2, but the pattern function only has parameters w1, w2. pattern doesn\u2019t use x, so it shouldn\u2019t specify x as a parameter. As an example of the second rule, consider replacing", "with", "In this case, replacement needs the same number of parameters as pattern (both x and y), even though the parameter y isn\u2019t used in replacement.", "After calling subgraph_rewriter.replace_pattern, the generated Python code looks like this:", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Graph", "path": "fx#torch.fx.Graph", "type": "FX", "text": ["Graph is the main data structure used in the FX Intermediate Representation. It consists of a series of Node s, each representing callsites (or other syntactic constructs). The list of Node s, taken together, constitute a valid Python function.", "For example, the following code", "Will produce the following Graph:", "For the semantics of operations represented in the Graph, please see Node.", "Note", "Backwards-compatibility for this API is guaranteed.", "Construct an empty Graph.", "Note", "Backwards-compatibility for this API is guaranteed.", "Insert a call_function Node into the Graph. A call_function node represents a call to a Python callable, specified by the_function.", "The newly created and inserted call_function node.", "Node", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node().", "Note", "Backwards-compatibility for this API is guaranteed.", "Insert a call_method Node into the Graph. A call_method node represents a call to a given method on the 0th element of args.", "The newly created and inserted call_method node.", "Node", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node().", "Note", "Backwards-compatibility for this API is guaranteed.", "Insert a call_module Node into the Graph. A call_module node represents a call to the forward() function of a Module in the Module hierarchy.", "The newly-created and inserted call_module node.", "Node", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node().", "Note", "Backwards-compatibility for this API is guaranteed.", "Create a Node and add it to the Graph at the current insert-point. Note that the current insert-point can be set via Graph.inserting_before() and Graph.inserting_after().", "The newly-created and inserted node.", "Node", "Note", "Backwards-compatibility for this API is guaranteed.", "Remove all dead code from the graph, based on each node\u2019s number of users, and whether the nodes have any side effects. The graph must be topologically sorted before calling.", "Whether the graph was changed as a result of the pass.", "bool", "Example:", "Before dead code is eliminated, a from a = x + 1 below has no users and thus can be eliminated from the graph without having an effect.", "After dead code is eliminated, a = x + 1 has been removed, and the rest of forward remains.", "Warning", "Dead code elimination has some heuristics to avoid removing side-effectful nodes (see Node.is_impure) but in general coverage is very bad, so you should assume that this method is not sound to call unless you know that your FX graph consists entirely of functional operations.", "Note", "Backwards-compatibility for this API is guaranteed.", "Erases a Node from the Graph. Throws an exception if there are still users of that node in the Graph.", "to_erase (Node) \u2013 The Node to erase from the Graph.", "Note", "Backwards-compatibility for this API is guaranteed.", "Insert a get_attr node into the Graph. A get_attr Node represents the fetch of an attribute from the Module hierarchy.", "The newly-created and inserted get_attr node.", "Node", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node.", "Note", "Backwards-compatibility for this API is guaranteed.", "Copy all nodes from a given graph into self.", "The value in self that is now equivalent to the output value in g, if g had an output node. None otherwise.", "Optional[Union[Tuple[Any, \u2026], List[Any], Dict[str, Any], slice, range, Node, str, int, float, bool, complex, dtype, Tensor, device, memory_format, layout, OpOverload]]", "Note", "Backwards-compatibility for this API is guaranteed.", "When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits:", "Args:", "the beginning of the entire graph.", "A resource manager that will restore the insert point on __exit__.", "Note", "Backwards-compatibility for this API is guaranteed.", "When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits:", "Args:", "the beginning of the entire graph.", "A resource manager that will restore the insert point on __exit__.", "Note", "Backwards-compatibility for this API is guaranteed.", "Runs various checks on this Graph to make sure it is well-formed. In particular: - Checks Nodes have correct ownership (owned by this graph) - Checks Nodes appear in topological order - If this Graph has an owning GraphModule, checks that targets exist in that GraphModule", "Note", "Backwards-compatibility for this API is guaranteed.", "Copy a node from one graph into another. arg_transform needs to transform arguments from the graph of node to the graph of self. Example:", "Node", "Note", "Backwards-compatibility for this API is guaranteed.", "Get the list of Nodes that constitute this Graph.", "Note that this Node list representation is a doubly-linked list. Mutations during iteration (e.g. delete a Node, add a Node) are safe.", "A doubly-linked list of Nodes. Note that reversed can be called on this list to switch iteration order.", "Register a transformer function when python code is generated", "a function that returns a code transformer to be registered. This function is called by on_generate_code to obtain the code transformer.", "This function is also given as its input the currently registered code transformer (or None if nothing is registered), in case it is not desirable to overwrite it. This is useful to chain code transformers together.", "a context manager that when used in a with statement, to automatically restore the previously registered code transformer.", "Example:", "This function can also be used as a context manager, with the benefit to automatically restores the previously registered code transformer:", "Warning", "This API is experimental and is NOT backward-compatible.", "Insert an output Node into the Graph. An output node represents a return statement in Python code. result is the value that should be returned.", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node.", "Note", "Backwards-compatibility for this API is guaranteed.", "Insert a placeholder node into the Graph. A placeholder represents a function input.", "Node", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node.", "Note", "Backwards-compatibility for this API is guaranteed.", "Prints the intermediate representation of the graph in tabular format. Note that this API requires the tabulate module to be installed.", "Note", "Backwards-compatibility for this API is guaranteed.", "Processes args so that they can be passed to the FX graph.", "Warning", "This API is experimental and is NOT backward-compatible.", "Warning", "This API is experimental and is NOT backward-compatible.", "Turn this Graph into valid Python code.", "root_module (str) \u2013 The name of the root module on which to look-up qualified name targets. This is usually \u2018self\u2019.", "src: the Python source code representing the object globals: a dictionary of global names in src -> the objects that they reference.", "A PythonCode object, consisting of two fields", "Note", "Backwards-compatibility for this API is guaranteed.", "Warning", "This API is experimental and is NOT backward-compatible."]}, {"name": "torch.fx.Graph.__init__()", "path": "fx#torch.fx.Graph.__init__", "type": "FX", "text": ["Construct an empty Graph.", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Graph.call_function()", "path": "fx#torch.fx.Graph.call_function", "type": "FX", "text": ["Insert a call_function Node into the Graph. A call_function node represents a call to a Python callable, specified by the_function.", "The newly created and inserted call_function node.", "Node", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node().", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Graph.call_method()", "path": "fx#torch.fx.Graph.call_method", "type": "FX", "text": ["Insert a call_method Node into the Graph. A call_method node represents a call to a given method on the 0th element of args.", "The newly created and inserted call_method node.", "Node", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node().", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Graph.call_module()", "path": "fx#torch.fx.Graph.call_module", "type": "FX", "text": ["Insert a call_module Node into the Graph. A call_module node represents a call to the forward() function of a Module in the Module hierarchy.", "The newly-created and inserted call_module node.", "Node", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node().", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Graph.create_node()", "path": "fx#torch.fx.Graph.create_node", "type": "FX", "text": ["Create a Node and add it to the Graph at the current insert-point. Note that the current insert-point can be set via Graph.inserting_before() and Graph.inserting_after().", "The newly-created and inserted node.", "Node", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Graph.eliminate_dead_code()", "path": "fx#torch.fx.Graph.eliminate_dead_code", "type": "FX", "text": ["Remove all dead code from the graph, based on each node\u2019s number of users, and whether the nodes have any side effects. The graph must be topologically sorted before calling.", "Whether the graph was changed as a result of the pass.", "bool", "Example:", "Before dead code is eliminated, a from a = x + 1 below has no users and thus can be eliminated from the graph without having an effect.", "After dead code is eliminated, a = x + 1 has been removed, and the rest of forward remains.", "Warning", "Dead code elimination has some heuristics to avoid removing side-effectful nodes (see Node.is_impure) but in general coverage is very bad, so you should assume that this method is not sound to call unless you know that your FX graph consists entirely of functional operations.", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Graph.erase_node()", "path": "fx#torch.fx.Graph.erase_node", "type": "FX", "text": ["Erases a Node from the Graph. Throws an exception if there are still users of that node in the Graph.", "to_erase (Node) \u2013 The Node to erase from the Graph.", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Graph.get_attr()", "path": "fx#torch.fx.Graph.get_attr", "type": "FX", "text": ["Insert a get_attr node into the Graph. A get_attr Node represents the fetch of an attribute from the Module hierarchy.", "The newly-created and inserted get_attr node.", "Node", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node.", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Graph.graph_copy()", "path": "fx#torch.fx.Graph.graph_copy", "type": "FX", "text": ["Copy all nodes from a given graph into self.", "The value in self that is now equivalent to the output value in g, if g had an output node. None otherwise.", "Optional[Union[Tuple[Any, \u2026], List[Any], Dict[str, Any], slice, range, Node, str, int, float, bool, complex, dtype, Tensor, device, memory_format, layout, OpOverload]]", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Graph.inserting_after()", "path": "fx#torch.fx.Graph.inserting_after", "type": "FX", "text": ["When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits:", "Args:", "the beginning of the entire graph.", "A resource manager that will restore the insert point on __exit__.", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Graph.inserting_before()", "path": "fx#torch.fx.Graph.inserting_before", "type": "FX", "text": ["When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits:", "Args:", "the beginning of the entire graph.", "A resource manager that will restore the insert point on __exit__.", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Graph.lint()", "path": "fx#torch.fx.Graph.lint", "type": "FX", "text": ["Runs various checks on this Graph to make sure it is well-formed. In particular: - Checks Nodes have correct ownership (owned by this graph) - Checks Nodes appear in topological order - If this Graph has an owning GraphModule, checks that targets exist in that GraphModule", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Graph.node_copy()", "path": "fx#torch.fx.Graph.node_copy", "type": "FX", "text": ["Copy a node from one graph into another. arg_transform needs to transform arguments from the graph of node to the graph of self. Example:", "Node", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Graph.nodes", "path": "fx#torch.fx.Graph.nodes", "type": "FX", "text": ["Get the list of Nodes that constitute this Graph.", "Note that this Node list representation is a doubly-linked list. Mutations during iteration (e.g. delete a Node, add a Node) are safe.", "A doubly-linked list of Nodes. Note that reversed can be called on this list to switch iteration order."]}, {"name": "torch.fx.Graph.on_generate_code()", "path": "fx#torch.fx.Graph.on_generate_code", "type": "FX", "text": ["Register a transformer function when python code is generated", "a function that returns a code transformer to be registered. This function is called by on_generate_code to obtain the code transformer.", "This function is also given as its input the currently registered code transformer (or None if nothing is registered), in case it is not desirable to overwrite it. This is useful to chain code transformers together.", "a context manager that when used in a with statement, to automatically restore the previously registered code transformer.", "Example:", "This function can also be used as a context manager, with the benefit to automatically restores the previously registered code transformer:", "Warning", "This API is experimental and is NOT backward-compatible."]}, {"name": "torch.fx.Graph.output()", "path": "fx#torch.fx.Graph.output", "type": "FX", "text": ["Insert an output Node into the Graph. An output node represents a return statement in Python code. result is the value that should be returned.", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node.", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Graph.placeholder()", "path": "fx#torch.fx.Graph.placeholder", "type": "FX", "text": ["Insert a placeholder node into the Graph. A placeholder represents a function input.", "Node", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node.", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Graph.print_tabular()", "path": "fx#torch.fx.Graph.print_tabular", "type": "FX", "text": ["Prints the intermediate representation of the graph in tabular format. Note that this API requires the tabulate module to be installed.", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Graph.process_inputs()", "path": "fx#torch.fx.Graph.process_inputs", "type": "FX", "text": ["Processes args so that they can be passed to the FX graph.", "Warning", "This API is experimental and is NOT backward-compatible."]}, {"name": "torch.fx.Graph.process_outputs()", "path": "fx#torch.fx.Graph.process_outputs", "type": "FX", "text": ["Warning", "This API is experimental and is NOT backward-compatible."]}, {"name": "torch.fx.Graph.python_code()", "path": "fx#torch.fx.Graph.python_code", "type": "FX", "text": ["Turn this Graph into valid Python code.", "root_module (str) \u2013 The name of the root module on which to look-up qualified name targets. This is usually \u2018self\u2019.", "src: the Python source code representing the object globals: a dictionary of global names in src -> the objects that they reference.", "A PythonCode object, consisting of two fields", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Graph.set_codegen()", "path": "fx#torch.fx.Graph.set_codegen", "type": "FX", "text": ["Warning", "This API is experimental and is NOT backward-compatible."]}, {"name": "torch.fx.GraphModule", "path": "fx#torch.fx.GraphModule", "type": "FX", "text": ["GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a graph attribute, as well as code and forward attributes generated from that graph.", "Warning", "When graph is reassigned, code and forward will be automatically regenerated. However, if you edit the contents of the graph without reassigning the graph attribute itself, you must call recompile() to update the generated code.", "Note", "Backwards-compatibility for this API is guaranteed.", "Construct a GraphModule.", "Note", "Backwards-compatibility for this API is guaranteed.", "Adds the given submodule to self.", "This installs empty Modules where none exist yet if they are subpaths of target.", "this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute)", "bool", "Note", "Backwards-compatibility for this API is guaranteed.", "Return the Python code generated from the Graph underlying this GraphModule.", "Deletes all unused submodules from self.", "A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node", "This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule.", "Note", "Backwards-compatibility for this API is guaranteed.", "Deletes the given submodule from self.", "The module will not be deleted if target is not a valid target.", "target (str) \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)", "submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule.", "bool", "Note", "Backwards-compatibility for this API is guaranteed.", "Return the Graph underlying this GraphModule", "Return the Python code generated for current GraphModule and its children GraphModules", "Warning", "This API is experimental and is NOT backward-compatible.", "Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date.", "Note", "Backwards-compatibility for this API is guaranteed.", "PythonCode", "imported with from <folder> import <module_name>", "Args:", "folder (Union[str, os.PathLike]): The folder to write the code out to", "writing out the code", "Warning", "This API is experimental and is NOT backward-compatible."]}, {"name": "torch.fx.GraphModule.__init__()", "path": "fx#torch.fx.GraphModule.__init__", "type": "FX", "text": ["Construct a GraphModule.", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.GraphModule.add_submodule()", "path": "fx#torch.fx.GraphModule.add_submodule", "type": "FX", "text": ["Adds the given submodule to self.", "This installs empty Modules where none exist yet if they are subpaths of target.", "this method to return True, each object in the chain denoted by target must either a) not exist yet, or b) reference an nn.Module (not a parameter or other attribute)", "bool", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.GraphModule.code", "path": "fx#torch.fx.GraphModule.code", "type": "FX", "text": ["Return the Python code generated from the Graph underlying this GraphModule."]}, {"name": "torch.fx.GraphModule.delete_all_unused_submodules()", "path": "fx#torch.fx.GraphModule.delete_all_unused_submodules", "type": "FX", "text": ["Deletes all unused submodules from self.", "A Module is considered \u201cused\u201d if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a call_module node 3. It has a non-Module attribute that is used from a get_attr node", "This method can be called to clean up an nn.Module without manually calling delete_submodule on each unused submodule.", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.GraphModule.delete_submodule()", "path": "fx#torch.fx.GraphModule.delete_submodule", "type": "FX", "text": ["Deletes the given submodule from self.", "The module will not be deleted if target is not a valid target.", "target (str) \u2013 The fully-qualified string name of the new submodule (See example in nn.Module.get_submodule for how to specify a fully-qualified string.)", "submodule we want to delete. A return value of False means that the target was not a valid reference to a submodule.", "bool", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.GraphModule.graph", "path": "fx#torch.fx.GraphModule.graph", "type": "FX", "text": ["Return the Graph underlying this GraphModule"]}, {"name": "torch.fx.GraphModule.print_readable()", "path": "fx#torch.fx.GraphModule.print_readable", "type": "FX", "text": ["Return the Python code generated for current GraphModule and its children GraphModules", "Warning", "This API is experimental and is NOT backward-compatible."]}, {"name": "torch.fx.GraphModule.recompile()", "path": "fx#torch.fx.GraphModule.recompile", "type": "FX", "text": ["Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date.", "Note", "Backwards-compatibility for this API is guaranteed.", "PythonCode"]}, {"name": "torch.fx.GraphModule.to_folder()", "path": "fx#torch.fx.GraphModule.to_folder", "type": "FX", "text": ["imported with from <folder> import <module_name>", "Args:", "folder (Union[str, os.PathLike]): The folder to write the code out to", "writing out the code", "Warning", "This API is experimental and is NOT backward-compatible."]}, {"name": "torch.fx.Interpreter", "path": "fx#torch.fx.Interpreter", "type": "FX", "text": ["An Interpreter executes an FX graph Node-by-Node. This pattern can be useful for many things, including writing code transformations as well as analysis passes.", "Methods in the Interpreter class can be overridden to customize the behavior of execution. The map of overrideable methods in terms of call hierarchy:", "Suppose we want to swap all instances of torch.neg with torch.sigmoid and vice versa (including their Tensor method equivalents). We could subclass Interpreter like so:", "Note", "Backwards-compatibility for this API is guaranteed.", "Run module via interpretation and return the result. This uses the \u201cboxed\u201d calling convention, where you pass a list of arguments, which will be cleared by the interpreter. This ensures that input tensors are promptly deallocated.", "Note", "Backwards-compatibility for this API is guaranteed.", "Execute a call_function node and return the result.", "Any", "Any: The value returned by the function invocation", "Note", "Backwards-compatibility for this API is guaranteed.", "Execute a call_method node and return the result.", "Any", "Any: The value returned by the method invocation", "Note", "Backwards-compatibility for this API is guaranteed.", "Execute a call_module node and return the result.", "Any", "Any: The value returned by the module invocation", "Note", "Backwards-compatibility for this API is guaranteed.", "Fetch the concrete values of args and kwargs of node n from the current execution environment.", "n (Node) \u2013 The node for which args and kwargs should be fetched.", "args and kwargs with concrete values for n.", "Tuple[Tuple, Dict]", "Note", "Backwards-compatibility for this API is guaranteed.", "Fetch an attribute from the Module hierarchy of self.module.", "target (str) \u2013 The fully-qualified name of the attribute to fetch", "The value of the attribute.", "Any", "Note", "Backwards-compatibility for this API is guaranteed.", "Execute a get_attr node. Will retrieve an attribute value from the Module hierarchy of self.module.", "The value of the attribute that was retrieved", "Any", "Note", "Backwards-compatibility for this API is guaranteed.", "Recursively descend through args and look up the concrete value for each Node in the current execution environment.", "Optional[Union[Tuple[Any, \u2026], List[Any], Dict[str, Any], slice, range, Node, str, int, float, bool, complex, dtype, Tensor, device, memory_format, layout, OpOverload]]", "Note", "Backwards-compatibility for this API is guaranteed.", "Execute an output node. This really just retrieves the value referenced by the output node and returns it.", "The return value referenced by the output node", "Any", "Note", "Backwards-compatibility for this API is guaranteed.", "Execute a placeholder node. Note that this is stateful: Interpreter maintains an internal iterator over arguments passed to run and this method returns next() on that iterator.", "The argument value that was retrieved.", "Any", "Note", "Backwards-compatibility for this API is guaranteed.", "Run module via interpretation and return the result.", "The value returned from executing the Module", "Any", "Note", "Backwards-compatibility for this API is guaranteed.", "Run a specific node n and return the result. Calls into placeholder, get_attr, call_function, call_method, call_module, or output depending on node.op", "n (Node) \u2013 The Node to execute", "The result of executing n", "Any", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Interpreter.boxed_run()", "path": "fx#torch.fx.Interpreter.boxed_run", "type": "FX", "text": ["Run module via interpretation and return the result. This uses the \u201cboxed\u201d calling convention, where you pass a list of arguments, which will be cleared by the interpreter. This ensures that input tensors are promptly deallocated.", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Interpreter.call_function()", "path": "fx#torch.fx.Interpreter.call_function", "type": "FX", "text": ["Execute a call_function node and return the result.", "Any", "Any: The value returned by the function invocation", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Interpreter.call_method()", "path": "fx#torch.fx.Interpreter.call_method", "type": "FX", "text": ["Execute a call_method node and return the result.", "Any", "Any: The value returned by the method invocation", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Interpreter.call_module()", "path": "fx#torch.fx.Interpreter.call_module", "type": "FX", "text": ["Execute a call_module node and return the result.", "Any", "Any: The value returned by the module invocation", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Interpreter.fetch_args_kwargs_from_env()", "path": "fx#torch.fx.Interpreter.fetch_args_kwargs_from_env", "type": "FX", "text": ["Fetch the concrete values of args and kwargs of node n from the current execution environment.", "n (Node) \u2013 The node for which args and kwargs should be fetched.", "args and kwargs with concrete values for n.", "Tuple[Tuple, Dict]", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Interpreter.fetch_attr()", "path": "fx#torch.fx.Interpreter.fetch_attr", "type": "FX", "text": ["Fetch an attribute from the Module hierarchy of self.module.", "target (str) \u2013 The fully-qualified name of the attribute to fetch", "The value of the attribute.", "Any", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Interpreter.get_attr()", "path": "fx#torch.fx.Interpreter.get_attr", "type": "FX", "text": ["Execute a get_attr node. Will retrieve an attribute value from the Module hierarchy of self.module.", "The value of the attribute that was retrieved", "Any", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Interpreter.map_nodes_to_values()", "path": "fx#torch.fx.Interpreter.map_nodes_to_values", "type": "FX", "text": ["Recursively descend through args and look up the concrete value for each Node in the current execution environment.", "Optional[Union[Tuple[Any, \u2026], List[Any], Dict[str, Any], slice, range, Node, str, int, float, bool, complex, dtype, Tensor, device, memory_format, layout, OpOverload]]", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Interpreter.output()", "path": "fx#torch.fx.Interpreter.output", "type": "FX", "text": ["Execute an output node. This really just retrieves the value referenced by the output node and returns it.", "The return value referenced by the output node", "Any", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Interpreter.placeholder()", "path": "fx#torch.fx.Interpreter.placeholder", "type": "FX", "text": ["Execute a placeholder node. Note that this is stateful: Interpreter maintains an internal iterator over arguments passed to run and this method returns next() on that iterator.", "The argument value that was retrieved.", "Any", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Interpreter.run()", "path": "fx#torch.fx.Interpreter.run", "type": "FX", "text": ["Run module via interpretation and return the result.", "The value returned from executing the Module", "Any", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Interpreter.run_node()", "path": "fx#torch.fx.Interpreter.run_node", "type": "FX", "text": ["Run a specific node n and return the result. Calls into placeholder, get_attr, call_function, call_method, call_module, or output depending on node.op", "n (Node) \u2013 The Node to execute", "The result of executing n", "Any", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Node", "path": "fx#torch.fx.Node", "type": "FX", "text": ["Node is the data structure that represents individual operations within a Graph. For the most part, Nodes represent callsites to various entities, such as operators, methods, and Modules (some exceptions include nodes that specify function inputs and outputs). Each Node has a function specified by its op property. The Node semantics for each value of op are as follows:", "Note", "Backwards-compatibility for this API is guaranteed.", "Return all Nodes that are inputs to this Node. This is equivalent to iterating over args and kwargs and only collecting the values that are Nodes.", "List of Nodes that appear in the args and kwargs of this Node, in that order.", "Insert x after this node in the list of nodes in the graph. Equivalent to self.next.prepend(x)", "x (Node) \u2013 The node to put after this node. Must be a member of the same graph.", "Note", "Backwards-compatibility for this API is guaranteed.", "The tuple of arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information.", "Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment.", "Return a descriptive string representation of self.", "This method can be used with no arguments as a debugging utility.", "This function is also used internally in the __str__ method of Graph. Together, the strings in placeholder_names and maybe_return_typename make up the signature of the autogenerated forward function in this Graph\u2019s surrounding GraphModule. placeholder_names and maybe_return_typename should not be used otherwise.", "in the __str__ method of Graph, and 2) self is a placeholder Node, return None. Otherwise, return a descriptive string representation of the current Node.", "str", "Note", "Backwards-compatibility for this API is guaranteed.", "Returns whether this op is impure, i.e. if its op is a placeholder or output, or if a call_function or call_module which is impure.", "If the op is impure or not.", "bool", "Warning", "This API is experimental and is NOT backward-compatible.", "The dict of keyword arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information.", "Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment.", "Returns the next Node in the linked list of Nodes.", "The next Node in the linked list of Nodes.", "Returns normalized arguments to Python targets. This means that args/kwargs will be matched up to the module/functional\u2019s signature and return exclusively kwargs in positional order if normalize_to_only_use_kwargs is true. Also populates default values. Does not support positional-only parameters or varargs parameters.", "Supports module calls.", "May require arg_types and kwarg_types in order to disambiguate overloads.", "Returns NamedTuple ArgsKwargsPair, or None if not successful.", "Optional[ArgsKwargsPair]", "Warning", "This API is experimental and is NOT backward-compatible.", "Insert x before this node in the list of nodes in the graph. Example:", "x (Node) \u2013 The node to put before this node. Must be a member of the same graph.", "Note", "Backwards-compatibility for this API is guaranteed.", "Returns the previous Node in the linked list of Nodes.", "The previous Node in the linked list of Nodes.", "Replace all uses of self in the Graph with the Node replace_with.", "The list of Nodes on which this change was made.", "List[Node]", "Note", "Backwards-compatibility for this API is guaranteed.", "Loop through input nodes of self, and replace all instances of old_input with new_input.", "Note", "Backwards-compatibility for this API is guaranteed.", "Return the Python stack trace that was recorded during tracing, if any. When traced with fx.Tracer, this property is usually populated by Tracer.create_proxy. To record stack traces during tracing for debug purposes, set record_stack_traces = True on the Tracer instance. When traced with dynamo, this property will be populated by default by OutputGraph.create_proxy.", "stack_trace would have the innermost frame at the end of the string.", "Update an existing positional argument to contain the new value arg. After calling, self.args[idx] == arg.", "Note", "Backwards-compatibility for this API is guaranteed.", "Update an existing keyword argument to contain the new value arg. After calling, self.kwargs[key] == arg.", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Node.all_input_nodes", "path": "fx#torch.fx.Node.all_input_nodes", "type": "FX", "text": ["Return all Nodes that are inputs to this Node. This is equivalent to iterating over args and kwargs and only collecting the values that are Nodes.", "List of Nodes that appear in the args and kwargs of this Node, in that order."]}, {"name": "torch.fx.Node.append()", "path": "fx#torch.fx.Node.append", "type": "FX", "text": ["Insert x after this node in the list of nodes in the graph. Equivalent to self.next.prepend(x)", "x (Node) \u2013 The node to put after this node. Must be a member of the same graph.", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Node.args", "path": "fx#torch.fx.Node.args", "type": "FX", "text": ["The tuple of arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information.", "Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment."]}, {"name": "torch.fx.Node.format_node()", "path": "fx#torch.fx.Node.format_node", "type": "FX", "text": ["Return a descriptive string representation of self.", "This method can be used with no arguments as a debugging utility.", "This function is also used internally in the __str__ method of Graph. Together, the strings in placeholder_names and maybe_return_typename make up the signature of the autogenerated forward function in this Graph\u2019s surrounding GraphModule. placeholder_names and maybe_return_typename should not be used otherwise.", "in the __str__ method of Graph, and 2) self is a placeholder Node, return None. Otherwise, return a descriptive string representation of the current Node.", "str", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Node.is_impure()", "path": "fx#torch.fx.Node.is_impure", "type": "FX", "text": ["Returns whether this op is impure, i.e. if its op is a placeholder or output, or if a call_function or call_module which is impure.", "If the op is impure or not.", "bool", "Warning", "This API is experimental and is NOT backward-compatible."]}, {"name": "torch.fx.Node.kwargs", "path": "fx#torch.fx.Node.kwargs", "type": "FX", "text": ["The dict of keyword arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information.", "Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment."]}, {"name": "torch.fx.Node.next", "path": "fx#torch.fx.Node.next", "type": "FX", "text": ["Returns the next Node in the linked list of Nodes.", "The next Node in the linked list of Nodes."]}, {"name": "torch.fx.Node.normalized_arguments()", "path": "fx#torch.fx.Node.normalized_arguments", "type": "FX", "text": ["Returns normalized arguments to Python targets. This means that args/kwargs will be matched up to the module/functional\u2019s signature and return exclusively kwargs in positional order if normalize_to_only_use_kwargs is true. Also populates default values. Does not support positional-only parameters or varargs parameters.", "Supports module calls.", "May require arg_types and kwarg_types in order to disambiguate overloads.", "Returns NamedTuple ArgsKwargsPair, or None if not successful.", "Optional[ArgsKwargsPair]", "Warning", "This API is experimental and is NOT backward-compatible."]}, {"name": "torch.fx.Node.prepend()", "path": "fx#torch.fx.Node.prepend", "type": "FX", "text": ["Insert x before this node in the list of nodes in the graph. Example:", "x (Node) \u2013 The node to put before this node. Must be a member of the same graph.", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Node.prev", "path": "fx#torch.fx.Node.prev", "type": "FX", "text": ["Returns the previous Node in the linked list of Nodes.", "The previous Node in the linked list of Nodes."]}, {"name": "torch.fx.Node.replace_all_uses_with()", "path": "fx#torch.fx.Node.replace_all_uses_with", "type": "FX", "text": ["Replace all uses of self in the Graph with the Node replace_with.", "The list of Nodes on which this change was made.", "List[Node]", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Node.replace_input_with()", "path": "fx#torch.fx.Node.replace_input_with", "type": "FX", "text": ["Loop through input nodes of self, and replace all instances of old_input with new_input.", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Node.stack_trace", "path": "fx#torch.fx.Node.stack_trace", "type": "FX", "text": ["Return the Python stack trace that was recorded during tracing, if any. When traced with fx.Tracer, this property is usually populated by Tracer.create_proxy. To record stack traces during tracing for debug purposes, set record_stack_traces = True on the Tracer instance. When traced with dynamo, this property will be populated by default by OutputGraph.create_proxy.", "stack_trace would have the innermost frame at the end of the string."]}, {"name": "torch.fx.Node.update_arg()", "path": "fx#torch.fx.Node.update_arg", "type": "FX", "text": ["Update an existing positional argument to contain the new value arg. After calling, self.args[idx] == arg.", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Node.update_kwarg()", "path": "fx#torch.fx.Node.update_kwarg", "type": "FX", "text": ["Update an existing keyword argument to contain the new value arg. After calling, self.kwargs[key] == arg.", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Proxy", "path": "fx#torch.fx.Proxy", "type": "FX", "text": ["Proxy objects are Node wrappers that flow through the program during symbolic tracing and record all the operations (torch function calls, method calls, operators) that they touch into the growing FX Graph.", "If you\u2019re doing graph transforms, you can wrap your own Proxy method around a raw Node so that you can use the overloaded operators to add additional things to a Graph.", "Proxy objects cannot be iterated. In other words, the symbolic tracer will throw an error if a Proxy is used in a loop or as an *args/**kwargs function argument.", "There are two main ways around this: 1. Factor out the untraceable logic into a top-level function and use fx.wrap on it. 2. If the control flow is static (i.e. the loop trip count is based on some hyperparameter), the code can be kept in its original position and refactored into something like:", "For a more detailed description into the Proxy internals, check out the \u201cProxy\u201d section in torch/fx/OVERVIEW.md", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.replace_pattern()", "path": "fx#torch.fx.replace_pattern", "type": "FX", "text": ["Matches all possible non-overlapping sets of operators and their data dependencies (pattern) in the Graph of a GraphModule (gm), then replaces each of these matched subgraphs with another subgraph (replacement).", "A list of Match objects representing the places in the original graph that pattern was matched to. The list is empty if there are no matches. Match is defined as:", "List[Match]", "Examples:", "The above code will first match pattern in the forward method of traced_module. Pattern-matching is done based on use-def relationships, not node names. For example, if you had p = torch.cat([a, b]) in pattern, you could match m = torch.cat([a, b]) in the original forward function, despite the variable names being different (p vs m).", "The return statement in pattern is matched based on its value only; it may or may not match to the return statement in the larger graph. In other words, the pattern doesn\u2019t have to extend to the end of the larger graph.", "When the pattern is matched, it will be removed from the larger function and replaced by replacement. If there are multiple matches for pattern in the larger function, each non-overlapping match will be replaced. In the case of a match overlap, the first found match in the set of overlapping matches will be replaced. (\u201cFirst\u201d here being defined as the first in a topological ordering of the Nodes\u2019 use-def relationships. In most cases, the first Node is the parameter that appears directly after self, while the last Node is whatever the function returns.)", "One important thing to note is that the parameters of the pattern Callable must be used in the Callable itself, and the parameters of the replacement Callable must match the pattern. The first rule is why, in the above code block, the forward function has parameters x, w1, w2, but the pattern function only has parameters w1, w2. pattern doesn\u2019t use x, so it shouldn\u2019t specify x as a parameter. As an example of the second rule, consider replacing", "with", "In this case, replacement needs the same number of parameters as pattern (both x and y), even though the parameter y isn\u2019t used in replacement.", "After calling subgraph_rewriter.replace_pattern, the generated Python code looks like this:", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.symbolic_trace()", "path": "fx#torch.fx.symbolic_trace", "type": "FX", "text": ["Symbolic tracing API", "Given an nn.Module or function instance root, this function will return a GraphModule constructed by recording operations seen while tracing through root.", "concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures.", "For example:", "FX can typically not trace through this due to the presence of control flow. However, we can use concrete_args to specialize on the value of b to trace through this:", "Note that although you can still pass in different values of b, they will be ignored.", "We can also use concrete_args to eliminate data-structure handling from our function. This will use pytrees to flatten your input. To avoid overspecializing, pass in fx.PH for values that shouldn\u2019t be specialized. For example:", "a Module created from the recorded operations from root.", "GraphModule", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Tracer", "path": "fx#torch.fx.Tracer", "type": "FX", "text": ["Tracer is the class that implements the symbolic tracing functionality of torch.fx.symbolic_trace. A call to symbolic_trace(m) is equivalent to Tracer().trace(m).", "Tracer can be subclassed to override various behaviors of the tracing process. The different behaviors that can be overridden are described in the docstrings of the methods on this class.", "Note", "Backwards-compatibility for this API is guaranteed.", "Method that specifies the behavior of this Tracer when it encounters a call to an nn.Module instance.", "By default, the behavior is to check if the called module is a leaf module via is_leaf_module. If it is, emit a call_module node referring to m in the Graph. Otherwise, call the Module normally, tracing through the operations in its forward function.", "This method can be overridden to\u2013for example\u2013create nested traced GraphModules, or any other behavior you would want while tracing across Module boundaries.", "The return value from the Module call. In the case that a call_module node was emitted, this is a Proxy value. Otherwise, it is whatever value was returned from the Module invocation.", "Any", "Note", "Backwards-compatibility for this API is guaranteed.", "A method to specify the behavior of tracing when preparing values to be used as arguments to nodes in the Graph.", "By default, the behavior includes:", "Given a non-Proxy Tensor object, emit IR for various cases:", "This method can be overridden to support more types.", "a (Any) \u2013 The value to be emitted as an Argument in the Graph.", "The value a converted into the appropriate Argument", "Optional[Union[Tuple[Any, \u2026], List[Any], Dict[str, Any], slice, range, Node, str, int, float, bool, complex, dtype, Tensor, device, memory_format, layout, OpOverload]]", "Note", "Backwards-compatibility for this API is guaranteed.", "Create placeholder nodes corresponding to the signature of the root Module. This method introspects root\u2019s signature and emits those nodes accordingly, also supporting *args and **kwargs.", "Warning", "This API is experimental and is NOT backward-compatible.", "Inserts a graph node given target, args, kwargs, and name.", "This method can be overridden to do extra checking, validation, or modification of values used in node creation. For example, one might want to disallow in-place operations from being recorded.", "Note", "Backwards-compatibility for this API is guaranteed.", "Node", "Create a Node from the given arguments, then return the Node wrapped in a Proxy object.", "If kind = \u2018placeholder\u2019, then we\u2019re creating a Node that represents the parameter of a function. If we need to encode a default parameter, we use the args tuple. args is otherwise empty for placeholder Nodes.", "Note", "Backwards-compatibility for this API is guaranteed.", "Method that specifies the behavior of this Tracer when we call getattr on a call to an nn.Module instance.", "By default, the behavior is to return a proxy value for the attribute. It also stores the proxy value in the parameter_proxy_cache, so that future calls will reuse the proxy rather than creating a new one.", "This method can be overridden to \u2013for example\u2013 not return proxies when querying parameters.", "The return value from the getattr call.", "Warning", "This API is experimental and is NOT backward-compatible.", "A method to specify whether a given nn.Module is a \u201cleaf\u201d module.", "Leaf modules are the atomic units that appear in the IR, referenced by call_module calls. By default, Modules in the PyTorch standard library namespace (torch.nn) are leaf modules. All other modules are traced through and their constituent ops are recorded, unless specified otherwise via this parameter.", "bool", "Note", "Backwards-compatibility for this API is guaranteed.", "when used in control flow. Normally we don\u2019t know what to do because we don\u2019t know the value of the proxy, but a custom tracer can attach more information to the graph node using create_node and can choose to return an iterator.", "Note", "Backwards-compatibility for this API is guaranteed.", "Iterator", "This is what happens when ** is called on a proxy. This should return an iterator it ** is suppose to work in your custom tracer.", "Note", "Backwards-compatibility for this API is guaranteed.", "Any", "Helper method to find the qualified name of mod in the Module hierarchy of root. For example, if root has a submodule named foo, which has a submodule named bar, passing bar into this function will return the string \u201cfoo.bar\u201d.", "mod (str) \u2013 The Module to retrieve the qualified name for.", "str", "Note", "Backwards-compatibility for this API is guaranteed.", "Note", "Backwards-compatibility for this API is guaranteed.", "Proxy", "when used in control flow. Normally we don\u2019t know what to do because we don\u2019t know the value of the proxy, but a custom tracer can attach more information to the graph node using create_node and can choose to return a value.", "Note", "Backwards-compatibility for this API is guaranteed.", "bool", "Trace root and return the corresponding FX Graph representation. root can either be an nn.Module instance or a Python callable.", "Note that after this call, self.root may be different from the root passed in here. For example, when a free function is passed to trace(), we will create an nn.Module instance to use as the root and add embedded constants to.", "A Graph representing the semantics of the passed-in root.", "Graph", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Tracer.call_module()", "path": "fx#torch.fx.Tracer.call_module", "type": "FX", "text": ["Method that specifies the behavior of this Tracer when it encounters a call to an nn.Module instance.", "By default, the behavior is to check if the called module is a leaf module via is_leaf_module. If it is, emit a call_module node referring to m in the Graph. Otherwise, call the Module normally, tracing through the operations in its forward function.", "This method can be overridden to\u2013for example\u2013create nested traced GraphModules, or any other behavior you would want while tracing across Module boundaries.", "The return value from the Module call. In the case that a call_module node was emitted, this is a Proxy value. Otherwise, it is whatever value was returned from the Module invocation.", "Any", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Tracer.create_arg()", "path": "fx#torch.fx.Tracer.create_arg", "type": "FX", "text": ["A method to specify the behavior of tracing when preparing values to be used as arguments to nodes in the Graph.", "By default, the behavior includes:", "Given a non-Proxy Tensor object, emit IR for various cases:", "This method can be overridden to support more types.", "a (Any) \u2013 The value to be emitted as an Argument in the Graph.", "The value a converted into the appropriate Argument", "Optional[Union[Tuple[Any, \u2026], List[Any], Dict[str, Any], slice, range, Node, str, int, float, bool, complex, dtype, Tensor, device, memory_format, layout, OpOverload]]", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Tracer.create_args_for_root()", "path": "fx#torch.fx.Tracer.create_args_for_root", "type": "FX", "text": ["Create placeholder nodes corresponding to the signature of the root Module. This method introspects root\u2019s signature and emits those nodes accordingly, also supporting *args and **kwargs.", "Warning", "This API is experimental and is NOT backward-compatible."]}, {"name": "torch.fx.Tracer.create_node()", "path": "fx#torch.fx.Tracer.create_node", "type": "FX", "text": ["Inserts a graph node given target, args, kwargs, and name.", "This method can be overridden to do extra checking, validation, or modification of values used in node creation. For example, one might want to disallow in-place operations from being recorded.", "Note", "Backwards-compatibility for this API is guaranteed.", "Node"]}, {"name": "torch.fx.Tracer.create_proxy()", "path": "fx#torch.fx.Tracer.create_proxy", "type": "FX", "text": ["Create a Node from the given arguments, then return the Node wrapped in a Proxy object.", "If kind = \u2018placeholder\u2019, then we\u2019re creating a Node that represents the parameter of a function. If we need to encode a default parameter, we use the args tuple. args is otherwise empty for placeholder Nodes.", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Tracer.getattr()", "path": "fx#torch.fx.Tracer.getattr", "type": "FX", "text": ["Method that specifies the behavior of this Tracer when we call getattr on a call to an nn.Module instance.", "By default, the behavior is to return a proxy value for the attribute. It also stores the proxy value in the parameter_proxy_cache, so that future calls will reuse the proxy rather than creating a new one.", "This method can be overridden to \u2013for example\u2013 not return proxies when querying parameters.", "The return value from the getattr call.", "Warning", "This API is experimental and is NOT backward-compatible."]}, {"name": "torch.fx.Tracer.is_leaf_module()", "path": "fx#torch.fx.Tracer.is_leaf_module", "type": "FX", "text": ["A method to specify whether a given nn.Module is a \u201cleaf\u201d module.", "Leaf modules are the atomic units that appear in the IR, referenced by call_module calls. By default, Modules in the PyTorch standard library namespace (torch.nn) are leaf modules. All other modules are traced through and their constituent ops are recorded, unless specified otherwise via this parameter.", "bool", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Tracer.iter()", "path": "fx#torch.fx.Tracer.iter", "type": "FX", "text": ["when used in control flow. Normally we don\u2019t know what to do because we don\u2019t know the value of the proxy, but a custom tracer can attach more information to the graph node using create_node and can choose to return an iterator.", "Note", "Backwards-compatibility for this API is guaranteed.", "Iterator"]}, {"name": "torch.fx.Tracer.keys()", "path": "fx#torch.fx.Tracer.keys", "type": "FX", "text": ["This is what happens when ** is called on a proxy. This should return an iterator it ** is suppose to work in your custom tracer.", "Note", "Backwards-compatibility for this API is guaranteed.", "Any"]}, {"name": "torch.fx.Tracer.path_of_module()", "path": "fx#torch.fx.Tracer.path_of_module", "type": "FX", "text": ["Helper method to find the qualified name of mod in the Module hierarchy of root. For example, if root has a submodule named foo, which has a submodule named bar, passing bar into this function will return the string \u201cfoo.bar\u201d.", "mod (str) \u2013 The Module to retrieve the qualified name for.", "str", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Tracer.proxy()", "path": "fx#torch.fx.Tracer.proxy", "type": "FX", "text": ["Note", "Backwards-compatibility for this API is guaranteed.", "Proxy"]}, {"name": "torch.fx.Tracer.to_bool()", "path": "fx#torch.fx.Tracer.to_bool", "type": "FX", "text": ["when used in control flow. Normally we don\u2019t know what to do because we don\u2019t know the value of the proxy, but a custom tracer can attach more information to the graph node using create_node and can choose to return a value.", "Note", "Backwards-compatibility for this API is guaranteed.", "bool"]}, {"name": "torch.fx.Tracer.trace()", "path": "fx#torch.fx.Tracer.trace", "type": "FX", "text": ["Trace root and return the corresponding FX Graph representation. root can either be an nn.Module instance or a Python callable.", "Note that after this call, self.root may be different from the root passed in here. For example, when a free function is passed to trace(), we will create an nn.Module instance to use as the root and add embedded constants to.", "A Graph representing the semantics of the passed-in root.", "Graph", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Transformer", "path": "fx#torch.fx.Transformer", "type": "FX", "text": ["Transformer is a special type of interpreter that produces a new Module. It exposes a transform() method that returns the transformed Module. Transformer does not require arguments to run, as Interpreter does. Transformer works entirely symbolically.", "Suppose we want to swap all instances of torch.neg with torch.sigmoid and vice versa (including their Tensor method equivalents). We could subclass Transformer like so:", "module (GraphModule) \u2013 The Module to be transformed.", "Note", "Backwards-compatibility for this API is guaranteed.", "Note", "Backwards-compatibility for this API is guaranteed.", "Any", "Note", "Backwards-compatibility for this API is guaranteed.", "Any", "Execute a get_attr node. In Transformer, this is overridden to insert a new get_attr node into the output graph.", "Proxy", "Note", "Backwards-compatibility for this API is guaranteed.", "Execute a placeholder node. In Transformer, this is overridden to insert a new placeholder into the output graph.", "Proxy", "Note", "Backwards-compatibility for this API is guaranteed.", "Transform self.module and return the transformed GraphModule.", "Note", "Backwards-compatibility for this API is guaranteed.", "GraphModule"]}, {"name": "torch.fx.Transformer.call_function()", "path": "fx#torch.fx.Transformer.call_function", "type": "FX", "text": ["Note", "Backwards-compatibility for this API is guaranteed.", "Any"]}, {"name": "torch.fx.Transformer.call_module()", "path": "fx#torch.fx.Transformer.call_module", "type": "FX", "text": ["Note", "Backwards-compatibility for this API is guaranteed.", "Any"]}, {"name": "torch.fx.Transformer.get_attr()", "path": "fx#torch.fx.Transformer.get_attr", "type": "FX", "text": ["Execute a get_attr node. In Transformer, this is overridden to insert a new get_attr node into the output graph.", "Proxy", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Transformer.placeholder()", "path": "fx#torch.fx.Transformer.placeholder", "type": "FX", "text": ["Execute a placeholder node. In Transformer, this is overridden to insert a new placeholder into the output graph.", "Proxy", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.fx.Transformer.transform()", "path": "fx#torch.fx.Transformer.transform", "type": "FX", "text": ["Transform self.module and return the transformed GraphModule.", "Note", "Backwards-compatibility for this API is guaranteed.", "GraphModule"]}, {"name": "torch.fx.wrap()", "path": "fx#torch.fx.wrap", "type": "FX", "text": ["This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d. A \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being traced through:", "This function can also equivalently be used as a decorator:", "A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of \u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace rather than traced through.", "fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the graph when it\u2019s called", "Note", "Backwards-compatibility for this API is guaranteed."]}, {"name": "torch.gather", "path": "generated/torch.gather", "type": "Torch", "text": ["Gathers values along an axis specified by dim.", "For a 3-D tensor the output is specified by:", "input and index must have the same number of dimensions. It is also required that index.size(d) <= input.size(d) for all dimensions d != dim. out will have the same shape as index. Note that input and index do not broadcast against each other.", "Example:"]}, {"name": "torch.gcd", "path": "generated/torch.gcd", "type": "Torch", "text": ["Computes the element-wise greatest common divisor (GCD) of input and other.", "Both input and other must have integer types.", "Note", "This defines gcd(0,0)=0gcd(0, 0) = 0.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.ge", "path": "generated/torch.ge", "type": "Torch", "text": ["Computes input\u2265other\\text{input} \\geq \\text{other} element-wise.", "The second argument can be a number or a tensor whose shape is broadcastable with the first argument.", "out (Tensor, optional) \u2013 the output tensor.", "A boolean tensor that is True where input is greater than or equal to other and False elsewhere", "Example:"]}, {"name": "torch.Generator", "path": "generated/torch.generator#torch.Generator", "type": "Torch", "text": ["Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers. Used as a keyword argument in many In-place random sampling functions.", "device (torch.device, optional) \u2013 the desired device for the generator.", "An torch.Generator object.", "Generator", "Example:", "Generator.device -> device", "Gets the current device of the generator.", "Example:", "Returns the Generator state as a torch.ByteTensor.", "A torch.ByteTensor which contains all the necessary bits to restore a Generator to a specific point in time.", "Tensor", "Example:", "Returns the initial seed for generating random numbers.", "Example:", "Sets the seed for generating random numbers. Returns a torch.Generator object. Any 32-bit integer is a valid seed.", "seed (int) \u2013 The desired seed. Value must be within the inclusive range [-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError is raised. Negative inputs are remapped to positive values with the formula 0xffff_ffff_ffff_ffff + seed.", "An torch.Generator object.", "Generator", "Example:", "Gets a non-deterministic random number from std::random_device or the current time and uses it to seed a Generator.", "Example:", "Sets the Generator state.", "new_state (torch.ByteTensor) \u2013 The desired state.", "Example:"]}, {"name": "torch.Generator.device", "path": "generated/torch.generator#torch.Generator.device", "type": "Torch", "text": ["Generator.device -> device", "Gets the current device of the generator.", "Example:"]}, {"name": "torch.Generator.get_state()", "path": "generated/torch.generator#torch.Generator.get_state", "type": "Torch", "text": ["Returns the Generator state as a torch.ByteTensor.", "A torch.ByteTensor which contains all the necessary bits to restore a Generator to a specific point in time.", "Tensor", "Example:"]}, {"name": "torch.Generator.initial_seed()", "path": "generated/torch.generator#torch.Generator.initial_seed", "type": "Torch", "text": ["Returns the initial seed for generating random numbers.", "Example:"]}, {"name": "torch.Generator.manual_seed()", "path": "generated/torch.generator#torch.Generator.manual_seed", "type": "Torch", "text": ["Sets the seed for generating random numbers. Returns a torch.Generator object. Any 32-bit integer is a valid seed.", "seed (int) \u2013 The desired seed. Value must be within the inclusive range [-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError is raised. Negative inputs are remapped to positive values with the formula 0xffff_ffff_ffff_ffff + seed.", "An torch.Generator object.", "Generator", "Example:"]}, {"name": "torch.Generator.seed()", "path": "generated/torch.generator#torch.Generator.seed", "type": "Torch", "text": ["Gets a non-deterministic random number from std::random_device or the current time and uses it to seed a Generator.", "Example:"]}, {"name": "torch.Generator.set_state()", "path": "generated/torch.generator#torch.Generator.set_state", "type": "Torch", "text": ["Sets the Generator state.", "new_state (torch.ByteTensor) \u2013 The desired state.", "Example:"]}, {"name": "torch.geqrf", "path": "generated/torch.geqrf", "type": "Torch", "text": ["This is a low-level function for calling LAPACK\u2019s geqrf directly. This function returns a namedtuple (a, tau) as defined in LAPACK documentation for geqrf .", "Computes a QR decomposition of input. Both Q and R matrices are stored in the same output tensor a. The elements of R are stored on and above the diagonal. Elementary reflectors (or Householder vectors) implicitly defining matrix Q are stored below the diagonal. The results of this function can be used together with torch.linalg.householder_product() to obtain the Q matrix or with torch.ormqr(), which uses an implicit representation of the Q matrix, for an efficient matrix-matrix multiplication.", "See LAPACK documentation for geqrf for further details.", "Note", "See also torch.linalg.qr(), which computes Q and R matrices, and torch.linalg.lstsq() with the driver=\"gels\" option for a function that can solve matrix equations using a QR decomposition.", "input (Tensor) \u2013 the input matrix", "out (tuple, optional) \u2013 the output tuple of (Tensor, Tensor). Ignored if None. Default: None."]}, {"name": "torch.ger", "path": "generated/torch.ger", "type": "Torch", "text": ["Alias of torch.outer().", "Warning", "This function is deprecated and will be removed in a future PyTorch release. Use torch.outer() instead."]}, {"name": "torch.get_default_dtype", "path": "generated/torch.get_default_dtype", "type": "Torch", "text": ["Get the current default floating point torch.dtype.", "Example:"]}, {"name": "torch.get_deterministic_debug_mode", "path": "generated/torch.get_deterministic_debug_mode", "type": "Torch", "text": ["Returns the current value of the debug mode for deterministic operations. Refer to torch.set_deterministic_debug_mode() documentation for more details.", "int"]}, {"name": "torch.get_float32_matmul_precision", "path": "generated/torch.get_float32_matmul_precision", "type": "Torch", "text": ["Returns the current value of float32 matrix multiplication precision. Refer to torch.set_float32_matmul_precision() documentation for more details.", "str"]}, {"name": "torch.get_num_interop_threads", "path": "generated/torch.get_num_interop_threads", "type": "Torch", "text": ["Returns the number of threads used for inter-op parallelism on CPU (e.g. in JIT interpreter)"]}, {"name": "torch.get_num_threads", "path": "generated/torch.get_num_threads", "type": "Torch", "text": ["Returns the number of threads used for parallelizing CPU operations"]}, {"name": "torch.get_rng_state", "path": "generated/torch.get_rng_state", "type": "Torch", "text": ["Returns the random number generator state as a torch.ByteTensor.", "Tensor"]}, {"name": "torch.gradient", "path": "generated/torch.gradient", "type": "Torch", "text": ["Estimates the gradient of a function g:Rn\u2192Rg : \\mathbb{R}^n \\rightarrow \\mathbb{R} in one or more dimensions using the second-order accurate central differences method and either first or second order estimates at the boundaries.", "The gradient of gg is estimated using samples. By default, when spacing is not specified, the samples are entirely described by input, and the mapping of input coordinates to an output is the same as the tensor\u2019s mapping of indices to values. For example, for a three-dimensional input the function described is g:R3\u2192Rg : \\mathbb{R}^3 \\rightarrow \\mathbb{R}, and g(1,2,3)==input[1,2,3]g(1, 2, 3)\\ == input[1, 2, 3].", "When spacing is specified, it modifies the relationship between input and input coordinates. This is detailed in the \u201cKeyword Arguments\u201d section below.", "The gradient is estimated by estimating each partial derivative of gg independently. This estimation is accurate if gg is in C3C^3 (it has at least 3 continuous derivatives), and the estimation can be improved by providing closer samples. Mathematically, the value at each interior point of a partial derivative is estimated using Taylor\u2019s theorem with remainder. Letting xx be an interior point with x\u2212hlx-h_l and x+hrx+h_r be points neighboring it to the left and right respectively, f(x+hr)f(x+h_r) and f(x\u2212hl)f(x-h_l) can be estimated using:", "Using the fact that f\u2208C3f \\in C^3 and solving the linear system, we derive:", "Note", "We estimate the gradient of functions in complex domain g:Cn\u2192Cg : \\mathbb{C}^n \\rightarrow \\mathbb{C} in the same way.", "The value of each partial derivative at the boundary points is computed differently. See edge_order below.", "input (Tensor) \u2013 the tensor that represents the values of the function", "Examples:"]}, {"name": "torch.greater", "path": "generated/torch.greater", "type": "Torch", "text": ["Alias for torch.gt()."]}, {"name": "torch.greater_equal", "path": "generated/torch.greater_equal", "type": "Torch", "text": ["Alias for torch.ge()."]}, {"name": "torch.gt", "path": "generated/torch.gt", "type": "Torch", "text": ["Computes input>other\\text{input} > \\text{other} element-wise.", "The second argument can be a number or a tensor whose shape is broadcastable with the first argument.", "out (Tensor, optional) \u2013 the output tensor.", "A boolean tensor that is True where input is greater than other and False elsewhere", "Example:"]}, {"name": "torch.HalfStorage", "path": "storage#torch.HalfStorage", "type": "Storage", "text": []}, {"name": "torch.HalfStorage.dtype", "path": "storage#torch.HalfStorage.dtype", "type": "Storage", "text": []}, {"name": "torch.hamming_window", "path": "generated/torch.hamming_window", "type": "Torch", "text": ["Hamming window function.", "where NN is the full window size.", "The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NN in above formula is in fact window_length+1\\text{window\\_length} + 1. Also, we always have torch.hamming_window(L, periodic=True) equal to torch.hamming_window(L + 1, periodic=False)[:-1]).", "Note", "If window_length =1=1, the returned window contains a single value 1.", "Note", "This is a generalized version of torch.hann_window().", "A 1-D tensor of size (window_length,)(\\text{window\\_length},) containing the window.", "Tensor"]}, {"name": "torch.hann_window", "path": "generated/torch.hann_window", "type": "Torch", "text": ["Hann window function.", "where NN is the full window size.", "The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NN in above formula is in fact window_length+1\\text{window\\_length} + 1. Also, we always have torch.hann_window(L, periodic=True) equal to torch.hann_window(L + 1, periodic=False)[:-1]).", "Note", "If window_length =1=1, the returned window contains a single value 1.", "A 1-D tensor of size (window_length,)(\\text{window\\_length},) containing the window", "Tensor"]}, {"name": "torch.heaviside", "path": "generated/torch.heaviside", "type": "Torch", "text": ["Computes the Heaviside step function for each element in input. The Heaviside step function is defined as:", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.histc", "path": "generated/torch.histc", "type": "Torch", "text": ["Computes the histogram of a tensor.", "The elements are sorted into equal width bins between min and max. If min and max are both zero, the minimum and maximum values of the data are used.", "Elements lower than min and higher than max and NaN elements are ignored.", "out (Tensor, optional) \u2013 the output tensor.", "Histogram represented as a tensor", "Tensor", "Example:"]}, {"name": "torch.histogram", "path": "generated/torch.histogram", "type": "Torch", "text": ["Computes a histogram of the values in a tensor.", "bins can be an integer or a 1D tensor.", "If bins is an int, it specifies the number of equal-width bins. By default, the lower and upper range of the bins is determined by the minimum and maximum elements of the input tensor. The range argument can be provided to specify a range for the bins.", "If bins is a 1D tensor, it specifies the sequence of bin edges including the rightmost edge. It should contain at least 2 elements and its elements should be increasing.", "1D Tensor containing the values of the histogram. bin_edges(Tensor): 1D Tensor containing the edges of the histogram bins.", "hist (Tensor)", "Example:"]}, {"name": "torch.histogramdd", "path": "generated/torch.histogramdd", "type": "Torch", "text": ["Computes a multi-dimensional histogram of the values in a tensor.", "Interprets the elements of an input tensor whose innermost dimension has size N as a collection of N-dimensional points. Maps each of the points into a set of N-dimensional bins and returns the number of points (or total weight) in each bin.", "input must be a tensor with at least 2 dimensions. If input has shape (M, N), each of its M rows defines a point in N-dimensional space. If input has three or more dimensions, all but the last dimension are flattened.", "Each dimension is independently associated with its own strictly increasing sequence of bin edges. Bin edges may be specified explicitly by passing a sequence of 1D tensors. Alternatively, bin edges may be constructed automatically by passing a sequence of integers specifying the number of equal-width bins in each dimension.", "corresponding to its dimension", "into which the point falls", "bins can be a sequence of N 1D tensors, a sequence of N ints, or a single int.", "If bins is a sequence of N 1D tensors, it explicitly specifies the N sequences of bin edges. Each 1D tensor should contain a strictly increasing sequence with at least one element. A sequence of K bin edges defines K-1 bins, explicitly specifying the left and right edges of all bins. Every bin is exclusive of its left edge. Only the rightmost bin is inclusive of its right edge.", "If bins is a sequence of N ints, it specifies the number of equal-width bins in each dimension. By default, the leftmost and rightmost bin edges in each dimension are determined by the minimum and maximum elements of the input tensor in the corresponding dimension. The range argument can be provided to manually specify the leftmost and rightmost bin edges in each dimension.", "If bins is an int, it specifies the number of equal-width bins for all dimensions.", "Note", "See also torch.histogram(), which specifically computes 1D histograms. While torch.histogramdd() infers the dimensionality of its bins and binned values from the shape of input, torch.histogram() accepts and flattens input of any shape.", "N-dimensional Tensor containing the values of the histogram. bin_edges(Tensor[]): sequence of N 1D Tensors containing the bin edges.", "hist (Tensor)"]}, {"name": "torch.hsplit", "path": "generated/torch.hsplit", "type": "Torch", "text": ["Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections. Each split is a view of input.", "If input is one dimensional this is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is zero), and if input has two or more dimensions it\u2019s equivalent to calling torch.tensor_split(input, indices_or_sections, dim=1) (the split dimension is 1), except that if indices_or_sections is an integer it must evenly divide the split dimension or a runtime error will be thrown.", "This function is based on NumPy\u2019s numpy.hsplit()."]}, {"name": "torch.hspmm()", "path": "generated/torch.hspmm#torch.hspmm", "type": "Sparse Tensors", "text": ["Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2. The result is a (1 + 1)-dimensional hybrid COO matrix.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.hstack", "path": "generated/torch.hstack", "type": "Torch", "text": ["Stack tensors in sequence horizontally (column wise).", "This is equivalent to concatenation along the first axis for 1-D tensors, and along the second axis for all other tensors.", "tensors (sequence of Tensors) \u2013 sequence of tensors to concatenate", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.hub", "path": "hub", "type": "Miscellaneous", "text": ["Pytorch Hub is a pre-trained model repository designed to facilitate research reproducibility.", "Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights) to a GitHub repository by adding a simple hubconf.py file;", "hubconf.py can have multiple entrypoints. Each entrypoint is defined as a python function (example: a pre-trained model you want to publish).", "Here is a code snippet specifies an entrypoint for resnet18 model if we expand the implementation in pytorch/vision/hubconf.py. In most case importing the right function in hubconf.py is sufficient. Here we just want to use the expanded version as an example to show how it works. You can see the full script in pytorch/vision repo", "Pytorch Hub provides convenient APIs to explore all available models in hub through torch.hub.list(), show docstring and examples through torch.hub.help() and load the pre-trained models using torch.hub.load().", "List all callable entrypoints available in the repo specified by github.", "trust_repo (bool, str or None) \u2013 ", "\"check\", True, False or None. This parameter was introduced in v1.12 and helps ensuring that users only run code from repos that they trust.", "Default is None and will eventually change to \"check\" in v2.0.", "The available callables entrypoint", "list", "Show the docstring of entrypoint model.", "trust_repo (bool, str or None) \u2013 ", "\"check\", True, False or None. This parameter was introduced in v1.12 and helps ensuring that users only run code from repos that they trust.", "Default is None and will eventually change to \"check\" in v2.0.", "Load a model from a github repo or a local directory.", "Note: Loading a model is the typical use case, but this can also be used to for loading other objects such as tokenizers, loss functions, etc.", "If source is \u2018github\u2019, repo_or_dir is expected to be of the form repo_owner/repo_name[:ref] with an optional ref (a tag or a branch).", "If source is \u2018local\u2019, repo_or_dir is expected to be a path to a local directory.", "trust_repo (bool, str or None) \u2013 ", "\"check\", True, False or None. This parameter was introduced in v1.12 and helps ensuring that users only run code from repos that they trust.", "Default is None and will eventually change to \"check\" in v2.0.", "The output of the model callable when called with the given *args and **kwargs.", "Download object at the given URL to a local path.", "Loads the Torch serialized object at the given URL.", "If downloaded file is a zip file, it will be automatically decompressed.", "If the object is already present in model_dir, it\u2019s deserialized and returned. The default value of model_dir is <hub_dir>/checkpoints where hub_dir is the directory returned by get_dir().", "Dict[str, Any]", "Note that *args and **kwargs in torch.hub.load() are used to instantiate a model. After you have loaded a model, how can you find out what you can do with the model? A suggested workflow is", "To help users explore without referring to documentation back and forth, we strongly recommend repo owners make function help messages clear and succinct. It\u2019s also helpful to include a minimal working example.", "The locations are used in the order of", "Get the Torch Hub cache directory used for storing downloaded models & weights.", "If set_dir() is not called, default path is $TORCH_HOME/hub where environment variable $TORCH_HOME defaults to $XDG_CACHE_HOME/torch. $XDG_CACHE_HOME follows the X Design Group specification of the Linux filesystem layout, with a default value ~/.cache if the environment variable is not set.", "Optionally set the Torch Hub directory used to save downloaded models & weights.", "d (str) \u2013 path to a local folder to save downloaded models & weights.", "By default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the directory returned by get_dir().", "Users can force a reload by calling hub.load(..., force_reload=True). This will delete the existing GitHub folder and downloaded weights, reinitialize a fresh download. This is useful when updates are published to the same branch, users can keep up with the latest release.", "Torch hub works by importing the package as if it was installed. There are some side effects introduced by importing in Python. For example, you can see new items in Python caches sys.modules and sys.path_importer_cache which is normal Python behavior. This also means that you may have import errors when importing different models from different repos, if the repos have the same sub-package names (typically, a model subpackage). A workaround for these kinds of import errors is to remove the offending sub-package from the sys.modules dict; more details can be found in this GitHub issue.", "A known limitation that is worth mentioning here: users CANNOT load two different branches of the same repo in the same python process. It\u2019s just like installing two packages with the same name in Python, which is not good. Cache might join the party and give you surprises if you actually try that. Of course it\u2019s totally fine to load them in separate processes."]}, {"name": "torch.hub.download_url_to_file()", "path": "hub#torch.hub.download_url_to_file", "type": "Miscellaneous", "text": ["Download object at the given URL to a local path."]}, {"name": "torch.hub.get_dir()", "path": "hub#torch.hub.get_dir", "type": "Miscellaneous", "text": ["Get the Torch Hub cache directory used for storing downloaded models & weights.", "If set_dir() is not called, default path is $TORCH_HOME/hub where environment variable $TORCH_HOME defaults to $XDG_CACHE_HOME/torch. $XDG_CACHE_HOME follows the X Design Group specification of the Linux filesystem layout, with a default value ~/.cache if the environment variable is not set."]}, {"name": "torch.hub.help()", "path": "hub#torch.hub.help", "type": "Miscellaneous", "text": ["Show the docstring of entrypoint model.", "trust_repo (bool, str or None) \u2013 ", "\"check\", True, False or None. This parameter was introduced in v1.12 and helps ensuring that users only run code from repos that they trust.", "Default is None and will eventually change to \"check\" in v2.0."]}, {"name": "torch.hub.list()", "path": "hub#torch.hub.list", "type": "Miscellaneous", "text": ["List all callable entrypoints available in the repo specified by github.", "trust_repo (bool, str or None) \u2013 ", "\"check\", True, False or None. This parameter was introduced in v1.12 and helps ensuring that users only run code from repos that they trust.", "Default is None and will eventually change to \"check\" in v2.0.", "The available callables entrypoint", "list"]}, {"name": "torch.hub.load()", "path": "hub#torch.hub.load", "type": "Miscellaneous", "text": ["Load a model from a github repo or a local directory.", "Note: Loading a model is the typical use case, but this can also be used to for loading other objects such as tokenizers, loss functions, etc.", "If source is \u2018github\u2019, repo_or_dir is expected to be of the form repo_owner/repo_name[:ref] with an optional ref (a tag or a branch).", "If source is \u2018local\u2019, repo_or_dir is expected to be a path to a local directory.", "trust_repo (bool, str or None) \u2013 ", "\"check\", True, False or None. This parameter was introduced in v1.12 and helps ensuring that users only run code from repos that they trust.", "Default is None and will eventually change to \"check\" in v2.0.", "The output of the model callable when called with the given *args and **kwargs."]}, {"name": "torch.hub.load_state_dict_from_url()", "path": "hub#torch.hub.load_state_dict_from_url", "type": "Miscellaneous", "text": ["Loads the Torch serialized object at the given URL.", "If downloaded file is a zip file, it will be automatically decompressed.", "If the object is already present in model_dir, it\u2019s deserialized and returned. The default value of model_dir is <hub_dir>/checkpoints where hub_dir is the directory returned by get_dir().", "Dict[str, Any]"]}, {"name": "torch.hub.set_dir()", "path": "hub#torch.hub.set_dir", "type": "Miscellaneous", "text": ["Optionally set the Torch Hub directory used to save downloaded models & weights.", "d (str) \u2013 path to a local folder to save downloaded models & weights."]}, {"name": "torch.hypot", "path": "generated/torch.hypot", "type": "Torch", "text": ["Given the legs of a right triangle, return its hypotenuse.", "The shapes of input and other must be broadcastable.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.i0", "path": "generated/torch.i0", "type": "Torch", "text": ["Alias for torch.special.i0()."]}, {"name": "torch.igamma", "path": "generated/torch.igamma", "type": "Torch", "text": ["Alias for torch.special.gammainc()."]}, {"name": "torch.igammac", "path": "generated/torch.igammac", "type": "Torch", "text": ["Alias for torch.special.gammaincc()."]}, {"name": "torch.imag", "path": "generated/torch.imag", "type": "Torch", "text": ["Returns a new tensor containing imaginary values of the self tensor. The returned tensor and self share the same underlying storage.", "Warning", "imag() is only supported for tensors with complex dtypes.", "input (Tensor) \u2013 the input tensor.", "Example:"]}, {"name": "torch.index_add", "path": "generated/torch.index_add", "type": "Torch", "text": ["See index_add_() for function description."]}, {"name": "torch.index_copy", "path": "generated/torch.index_copy", "type": "Torch", "text": ["See index_add_() for function description."]}, {"name": "torch.index_reduce", "path": "generated/torch.index_reduce", "type": "Torch", "text": ["See index_reduce_() for function description."]}, {"name": "torch.index_select", "path": "generated/torch.index_select", "type": "Torch", "text": ["Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.", "The returned tensor has the same number of dimensions as the original tensor (input). The dimth dimension has the same size as the length of index; other dimensions have the same size as in the original tensor.", "Note", "The returned tensor does not use the same storage as the original tensor. If out has a different shape than expected, we silently change it to the correct shape, reallocating the underlying storage if necessary.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.inference_mode", "path": "generated/torch.inference_mode#torch.inference_mode", "type": "Torch", "text": ["Context-manager that enables or disables inference mode", "InferenceMode is a new context manager analogous to no_grad to be used when you are certain your operations will have no interactions with autograd (e.g., model training). Code run under this mode gets better performance by disabling view tracking and version counter bumps. Note that unlike some other mechanisms that locally enable or disable grad, entering inference_mode also disables to forward-mode AD.", "This context manager is thread local; it will not affect computation in other threads.", "Also functions as a decorator.", "Note", "Inference mode is one of several mechanisms that can enable or disable gradients locally see Locally disabling gradient computation for more information on how they compare.", "mode (bool or function) \u2013 Either a boolean flag whether to enable or disable inference mode or a Python function to decorate with inference mode enabled"]}, {"name": "torch.initial_seed", "path": "generated/torch.initial_seed", "type": "Torch", "text": ["Returns the initial seed for generating random numbers as a Python long.", "int"]}, {"name": "torch.inner", "path": "generated/torch.inner", "type": "Torch", "text": ["Computes the dot product for 1D tensors. For higher dimensions, sums the product of elements from input and other along their last dimension.", "Note", "If either input or other is a scalar, the result is equivalent to torch.mul(input, other).", "If both input and other are non-scalars, the size of their last dimension must match and the result is equivalent to torch.tensordot(input, other, dims=([-1], [-1]))", "out (Tensor, optional) \u2013 Optional output tensor to write result into. The output shape is input.shape[:-1] + other.shape[:-1].", "Example:"]}, {"name": "torch.IntStorage", "path": "storage#torch.IntStorage", "type": "Storage", "text": []}, {"name": "torch.IntStorage.dtype", "path": "storage#torch.IntStorage.dtype", "type": "Storage", "text": []}, {"name": "torch.inverse", "path": "generated/torch.inverse", "type": "Torch", "text": ["Alias for torch.linalg.inv()"]}, {"name": "torch.is_complex", "path": "generated/torch.is_complex", "type": "Torch", "text": ["Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.", "input (Tensor) \u2013 the input tensor."]}, {"name": "torch.is_conj", "path": "generated/torch.is_conj", "type": "Torch", "text": ["Returns True if the input is a conjugated tensor, i.e. its conjugate bit is set to True.", "input (Tensor) \u2013 the input tensor."]}, {"name": "torch.is_deterministic_algorithms_warn_only_enabled", "path": "generated/torch.is_deterministic_algorithms_warn_only_enabled", "type": "Torch", "text": ["Returns True if the global deterministic flag is set to warn only. Refer to torch.use_deterministic_algorithms() documentation for more details.", "bool"]}, {"name": "torch.is_floating_point", "path": "generated/torch.is_floating_point", "type": "Torch", "text": ["Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.", "input (Tensor) \u2013 the input tensor."]}, {"name": "torch.is_grad_enabled", "path": "generated/torch.is_grad_enabled", "type": "Torch", "text": ["Returns True if grad mode is currently enabled."]}, {"name": "torch.is_inference_mode_enabled", "path": "generated/torch.is_inference_mode_enabled", "type": "Torch", "text": ["Returns True if inference mode is currently enabled."]}, {"name": "torch.is_nonzero", "path": "generated/torch.is_nonzero", "type": "Torch", "text": ["Returns True if the input is a single element tensor which is not equal to zero after type conversions. i.e. not equal to torch.tensor([0.]) or torch.tensor([0]) or torch.tensor([False]). Throws a RuntimeError if torch.numel() != 1 (even in case of sparse tensors).", "input (Tensor) \u2013 the input tensor.", "Examples:"]}, {"name": "torch.is_storage", "path": "generated/torch.is_storage", "type": "Torch", "text": ["Returns True if obj is a PyTorch storage object.", "obj (Object) \u2013 Object to test"]}, {"name": "torch.is_tensor", "path": "generated/torch.is_tensor", "type": "Torch", "text": ["Returns True if obj is a PyTorch tensor.", "Note that this function is simply doing isinstance(obj, Tensor). Using that isinstance check is better for typechecking with mypy, and more explicit - so it\u2019s recommended to use that instead of is_tensor.", "obj (Object) \u2013 Object to test", "Example:"]}, {"name": "torch.is_warn_always_enabled", "path": "generated/torch.is_warn_always_enabled", "type": "Torch", "text": ["Returns True if the global warn_always flag is turned on. Refer to torch.set_warn_always() documentation for more details.", "bool"]}, {"name": "torch.isclose", "path": "generated/torch.isclose", "type": "Torch", "text": ["Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other. Closeness is defined as:", "where input and other are finite. Where input and/or other are nonfinite they are close if and only if they are equal, with NaNs being considered equal to each other when equal_nan is True.", "Examples:"]}, {"name": "torch.isfinite", "path": "generated/torch.isfinite", "type": "Torch", "text": ["Returns a new tensor with boolean elements representing if each element is finite or not.", "Real values are finite when they are not NaN, negative infinity, or infinity. Complex values are finite when both their real and imaginary parts are finite.", "input (Tensor) \u2013 the input tensor.", "A boolean tensor that is True where input is finite and False elsewhere", "Example:"]}, {"name": "torch.isin", "path": "generated/torch.isin", "type": "Torch", "text": ["Tests if each element of elements is in test_elements. Returns a boolean tensor of the same shape as elements that is True for elements in test_elements and False otherwise.", "Note", "One of elements or test_elements can be a scalar, but not both.", "A boolean tensor of the same shape as elements that is True for elements in test_elements and False otherwise"]}, {"name": "torch.isinf", "path": "generated/torch.isinf", "type": "Torch", "text": ["Tests if each element of input is infinite (positive or negative infinity) or not.", "Note", "Complex values are infinite when their real or imaginary part is infinite.", "input (Tensor) \u2013 the input tensor.", "A boolean tensor that is True where input is infinite and False elsewhere", "Example:"]}, {"name": "torch.isnan", "path": "generated/torch.isnan", "type": "Torch", "text": ["Returns a new tensor with boolean elements representing if each element of input is NaN or not. Complex values are considered NaN when either their real and/or imaginary part is NaN.", "input (Tensor) \u2013 the input tensor.", "A boolean tensor that is True where input is NaN and False elsewhere", "Example:"]}, {"name": "torch.isneginf", "path": "generated/torch.isneginf", "type": "Torch", "text": ["Tests if each element of input is negative infinity or not.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.isposinf", "path": "generated/torch.isposinf", "type": "Torch", "text": ["Tests if each element of input is positive infinity or not.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.isreal", "path": "generated/torch.isreal", "type": "Torch", "text": ["Returns a new tensor with boolean elements representing if each element of input is real-valued or not. All real-valued types are considered real. Complex values are considered real when their imaginary part is 0.", "input (Tensor) \u2013 the input tensor.", "A boolean tensor that is True where input is real and False elsewhere", "Example:"]}, {"name": "torch.istft", "path": "generated/torch.istft", "type": "Torch", "text": ["Inverse short time Fourier Transform. This is expected to be the inverse of stft().", "It has the same parameters (+ additional optional parameter of length) and it should return the least squares estimation of the original signal. The algorithm will check using the NOLA condition ( nonzero overlap).", "Important consideration in the parameters window and center so that the envelop created by the summation of all the windows is never zero at certain point in time. Specifically, \u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times hop\\_length] \\cancel{=} 0.", "Since stft() discards elements at the end of the signal if they do not fit in a frame, istft may return a shorter signal than the original signal (can occur if center is False since the signal isn\u2019t padded). If length is given in the arguments and is longer than expected, istft will pad zeros to the end of the returned signal.", "If center is True, then there will be padding e.g. 'constant', 'reflect', etc. Left padding can be trimmed off exactly because they can be calculated but right padding cannot be calculated without additional information.", "Example: Suppose the last window is: [17, 18, 0, 0, 0] vs [18, 0, 0, 0, 0]", "The n_fft, hop_length, win_length are all the same which prevents the calculation of right padding. These additional values could be zeros or a reflection of the signal so providing length could be useful. If length is None then padding will be aggressively removed (some loss of signal).", "[1] D. W. Griffin and J. S. Lim, \u201cSignal estimation from modified short-time Fourier transform,\u201d IEEE Trans. ASSP, vol.32, no.2, pp.236-243, Apr. 1984.", "input (Tensor) \u2013 ", "The input tensor. Expected to be in the format of stft(), output. That is a complex tensor of shape (B?, N, T) where", "Changed in version 2.0: Real datatype inputs are no longer supported. Input must now have a complex datatype, as returned by stft(..., return_complex=True).", "B? is an optional batch dimension from the input tensor.", "Tensor"]}, {"name": "torch.jit.annotate()", "path": "generated/torch.jit.annotate#torch.jit.annotate", "type": "Torch Script", "text": ["This method is a pass-through function that returns the_value, used to hint TorchScript compiler the type of the_value. It is a no-op when running outside of TorchScript.", "Though TorchScript can infer correct type for most Python expressions, there are some cases where type inference can be wrong, including:", "Note that annotate() does not help in __init__ method of torch.nn.Module subclasses because it is executed in eager mode. To annotate types of torch.nn.Module attributes, use Annotate() instead.", "Example:", "the_value is passed back as return value."]}, {"name": "torch.jit.Attribute", "path": "generated/torch.jit.attribute#torch.jit.Attribute", "type": "Torch Script", "text": ["This method is a pass-through function that returns value, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type of type. Note that torch.jit.Attribute should only be used in __init__ method of jit.ScriptModule subclasses.", "Though TorchScript can infer correct type for most Python expressions, there are some cases where type inference can be wrong, including:", "In eager mode, it is simply a pass-through function that returns value without other implications.", "Example:", "Note: it\u2019s now preferred to instead use type annotations instead of torch.jit.Attribute:", "Returns value", "Return number of occurrences of value.", "Return first index of value.", "Raises ValueError if the value is not present.", "Alias for field number 1", "Alias for field number 0"]}, {"name": "torch.jit.Attribute.count()", "path": "generated/torch.jit.attribute#torch.jit.Attribute.count", "type": "Torch Script", "text": ["Return number of occurrences of value."]}, {"name": "torch.jit.Attribute.index()", "path": "generated/torch.jit.attribute#torch.jit.Attribute.index", "type": "Torch Script", "text": ["Return first index of value.", "Raises ValueError if the value is not present."]}, {"name": "torch.jit.Attribute.type", "path": "generated/torch.jit.attribute#torch.jit.Attribute.type", "type": "Torch Script", "text": ["Alias for field number 1"]}, {"name": "torch.jit.Attribute.value", "path": "generated/torch.jit.attribute#torch.jit.Attribute.value", "type": "Torch Script", "text": ["Alias for field number 0"]}, {"name": "torch.jit.enable_onednn_fusion()", "path": "generated/torch.jit.enable_onednn_fusion#torch.jit.enable_onednn_fusion", "type": "Torch Script", "text": ["Enables or disables onednn JIT fusion based on the parameter enabled."]}, {"name": "torch.jit.export()", "path": "jit#torch.jit.export", "type": "Torch Script", "text": ["This decorator indicates that a method on an nn.Module is used as an entry point into a ScriptModule and should be compiled.", "forward implicitly is assumed to be an entry point, so it does not need this decorator. Functions and methods called from forward are compiled as they are seen by the compiler, so they do not need this decorator either.", "Example (using @torch.jit.export on a method):"]}, {"name": "torch.jit.fork()", "path": "generated/torch.jit.fork#torch.jit.fork", "type": "Torch Script", "text": ["Creates an asynchronous task executing func and a reference to the value of the result of this execution. fork will return immediately, so the return value of func may not have been computed yet. To force completion of the task and access the return value invoke torch.jit.wait on the Future. fork invoked with a func which returns T is typed as torch.jit.Future[T]. fork calls can be arbitrarily nested, and may be invoked with positional and keyword arguments. Asynchronous execution will only occur when run in TorchScript. If run in pure python, fork will not execute in parallel. fork will also not execute in parallel when invoked while tracing, however the fork and wait calls will be captured in the exported IR Graph.", "Warning", "fork tasks will execute non-deterministically. We recommend only spawning parallel fork tasks for pure functions that do not modify their inputs, module attributes, or global state.", "a reference to the execution of func. The value T can only be accessed by forcing completion of func through torch.jit.wait.", "torch.jit.Future[T]", "Example (fork a free function):", "Example (fork a module method):"]}, {"name": "torch.jit.freeze()", "path": "generated/torch.jit.freeze#torch.jit.freeze", "type": "Torch Script", "text": ["Freezing a ScriptModule will clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph. By default, forward will be preserved, as well as attributes & methods specified in preserved_attrs. Additionally, any attribute that is modified within a preserved method will be preserved.", "Freezing currently only accepts ScriptModules that are in eval mode.", "Freezing applies generic optimization that will speed up your model regardless of machine. To further optimize using server-specific settings, run optimize_for_inference after freezing.", "Frozen ScriptModule.", "Example (Freezing a simple module with a Parameter):", "Example (Freezing a module with preserved attributes)", "Note", "Freezing submodule attributes is also supported: frozen_module = torch.jit.freeze(scripted_module, preserved_attrs=[\u201csubmodule.version\u201d])", "Note", "If you\u2019re not sure why an attribute is not being inlined as a constant, you can run dump_alias_db on frozen_module.forward.graph to see if freezing has detected the attribute is being modified.", "Note", "Because freezing makes weights constants and removes module hierarchy, to and other nn.Module methods to manipulate device or dtype no longer work. As a workaround, You can remap devices by specifying map_location in torch.jit.load, however device-specific logic may have been baked into the model."]}, {"name": "torch.jit.ignore()", "path": "generated/torch.jit.ignore#torch.jit.ignore", "type": "Torch Script", "text": ["This decorator indicates to the compiler that a function or method should be ignored and left as a Python function. This allows you to leave code in your model that is not yet TorchScript compatible. If called from TorchScript, ignored functions will dispatch the call to the Python interpreter. Models with ignored functions cannot be exported; use @torch.jit.unused instead.", "Example (using @torch.jit.ignore on a method):", "Example (using @torch.jit.ignore(drop=True) on a method):"]}, {"name": "torch.jit.is_scripting()", "path": "jit_language_reference#torch.jit.is_scripting", "type": "Torch Script", "text": ["Function that returns True when in compilation and False otherwise. This is useful especially with the @unused decorator to leave code in your model that is not yet TorchScript compatible. .. testcode:", "bool"]}, {"name": "torch.jit.is_tracing()", "path": "jit_language_reference#torch.jit.is_tracing", "type": "Torch Script", "text": ["Returns True in tracing (if a function is called during the tracing of code with torch.jit.trace) and False otherwise."]}, {"name": "torch.jit.isinstance()", "path": "generated/torch.jit.isinstance#torch.jit.isinstance", "type": "Torch Script", "text": ["This function provides for container type refinement in TorchScript. It can refine parameterized containers of the List, Dict, Tuple, and Optional types. E.g. List[str], Dict[str, List[torch.Tensor]], Optional[Tuple[int,str,int]]. It can also refine basic types such as bools and ints that are available in TorchScript.", "False otherwise with no new type refinement", "bool", "Example (using torch.jit.isinstance for type refinement): .. testcode:"]}, {"name": "torch.jit.load()", "path": "generated/torch.jit.load#torch.jit.load", "type": "Torch Script", "text": ["Load a ScriptModule or ScriptFunction previously saved with torch.jit.save", "All previously saved modules, no matter their device, are first loaded onto CPU, and then are moved to the devices they were saved from. If this fails (e.g. because the run time system doesn\u2019t have certain devices), an exception is raised.", "A ScriptModule object.", "Example:"]}, {"name": "torch.jit.onednn_fusion_enabled()", "path": "generated/torch.jit.onednn_fusion_enabled#torch.jit.onednn_fusion_enabled", "type": "Torch Script", "text": ["Returns whether onednn JIT fusion is enabled"]}, {"name": "torch.jit.optimize_for_inference()", "path": "generated/torch.jit.optimize_for_inference#torch.jit.optimize_for_inference", "type": "Torch Script", "text": ["Performs a set of optimization passes to optimize a model for the purposes of inference. If the model is not already frozen, optimize_for_inference will invoke torch.jit.freeze automatically.", "In addition to generic optimizations that should speed up your model regardless of environment, prepare for inference will also bake in build specific settings such as the presence of CUDNN or MKLDNN, and may in the future make transformations which speed things up on one machine but slow things down on another. Accordingly, serialization is not implemented following invoking optimize_for_inference and is not guaranteed.", "This is still in prototype, and may have the potential to slow down your model. Primary use cases that have been targeted so far have been vision models on cpu and gpu to a lesser extent.", "Example (optimizing a module with Conv->Batchnorm):", "ScriptModule"]}, {"name": "torch.jit.save()", "path": "generated/torch.jit.save#torch.jit.save", "type": "Torch Script", "text": ["Save an offline version of this module for use in a separate process. The saved module serializes all of the methods, submodules, parameters, and attributes of this module. It can be loaded into the C++ API using torch::jit::load(filename) or into the Python API with torch.jit.load.", "To be able to save a module, it must not make any calls to native Python functions. This means that all submodules must be subclasses of ScriptModule as well.", "Danger", "All modules, no matter their device, are always loaded onto the CPU during loading. This is different from torch.load()\u2019s semantics and may change in the future.", "Note", "torch.jit.save attempts to preserve the behavior of some operators across versions. For example, dividing two integer tensors in PyTorch 1.5 performed floor division, and if the module containing that code is saved in PyTorch 1.5 and loaded in PyTorch 1.6 its division behavior will be preserved. The same module saved in PyTorch 1.6 will fail to load in PyTorch 1.5, however, since the behavior of division changed in 1.6, and 1.5 does not know how to replicate the 1.6 behavior.", "Example:"]}, {"name": "torch.jit.script()", "path": "generated/torch.jit.script#torch.jit.script", "type": "Torch Script", "text": ["Scripting a function or nn.Module will inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return a ScriptModule or ScriptFunction. TorchScript itself is a subset of the Python language, so not all features in Python work, but we provide enough functionality to compute on tensors and do control-dependent operations. For a complete guide, see the TorchScript Language Reference.", "Scripting a dictionary or list copies the data inside it into a TorchScript instance than can be subsequently passed by reference between Python and TorchScript with zero copy overhead.", "and as a decorator @torch.jit.script for TorchScript Classes and functions.", "If obj is nn.Module, script returns a ScriptModule object. The returned ScriptModule will have the same set of sub-modules and parameters as the original nn.Module. If obj is a standalone function, a ScriptFunction will be returned. If obj is a dict, then script returns an instance of torch._C.ScriptDict. If obj is a list, then script returns an instance of torch._C.ScriptList.", "The @torch.jit.script decorator will construct a ScriptFunction by compiling the body of the function.", "Example (scripting a function):", "Example inputs can be used to annotate a function arguments.", "Example (annotating a function before scripting):", "Scripting an nn.Module by default will compile the forward method and recursively compile any methods, submodules, and functions called by forward. If a nn.Module only uses features supported in TorchScript, no changes to the original module code should be necessary. script will construct ScriptModule that has copies of the attributes, parameters, and methods of the original module.", "Example (scripting a simple module with a Parameter):", "Example (scripting a module with traced submodules):", "To compile a method other than forward (and recursively compile anything it calls), add the @torch.jit.export decorator to the method. To opt out of compilation use @torch.jit.ignore or @torch.jit.unused.", "Example (an exported and ignored method in a module):", "Example ( Annotating forward of nn.Module using example_inputs):"]}, {"name": "torch.jit.script_if_tracing()", "path": "generated/torch.jit.script_if_tracing#torch.jit.script_if_tracing", "type": "Torch Script", "text": ["Compiles fn when it is first called during tracing. torch.jit.script has a non-negligible start up time when it is first called due to lazy-initializations of many compiler builtins. Therefore you should not use it in library code. However, you may want to have parts of your library work in tracing even if they use control flow. In these cases, you should use @torch.jit.script_if_tracing to substitute for torch.jit.script.", "fn \u2013 A function to compile.", "If called during tracing, a ScriptFunction created by torch.jit.script is returned. Otherwise, the original function fn is returned."]}, {"name": "torch.jit.ScriptFunction", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction", "type": "Torch Script", "text": ["Functionally equivalent to a ScriptModule, but represents a single function and does not have any attributes or Parameters."]}, {"name": "torch.jit.ScriptFunction.get_debug_state()", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction.get_debug_state", "type": "Torch Script", "text": []}, {"name": "torch.jit.ScriptFunction.save()", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction.save", "type": "Torch Script", "text": []}, {"name": "torch.jit.ScriptFunction.save_to_buffer()", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction.save_to_buffer", "type": "Torch Script", "text": []}, {"name": "torch.jit.ScriptModule", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule", "type": "Torch Script", "text": ["A wrapper around C++ torch::jit::Module. ScriptModules contain methods, attributes, parameters, and constants. These can be accessed the same way as on a normal nn.Module.", "Adds a child module to the current module.", "The module can be accessed as an attribute using the given name.", "Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).", "fn (Module -> None) \u2013 function to be applied to each submodule", "self", "Module", "Example:", "Casts all floating point parameters and buffers to bfloat16 datatype.", "Note", "This method modifies the module in-place.", "self", "Module", "Returns an iterator over module buffers.", "recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.", "torch.Tensor \u2013 module buffer", "Iterator[Tensor]", "Example:", "Returns an iterator over immediate children modules.", "Module \u2013 a child module", "Iterator[Module]", "Returns a pretty-printed representation (as valid Python syntax) of the internal graph for the forward method. See Inspecting Code for details.", "Returns a tuple of:", "[0] a pretty-printed representation (as valid Python syntax) of the internal graph for the forward method. See code. [1] a ConstMap following the CONSTANT.cN format of the output in [0]. The indices in the [0] output are keys to the underlying constant\u2019s values.", "See Inspecting Code for details.", "Compile this Module\u2019s forward using torch.compile().", "This Module\u2019s __call__ method is compiled and all arguments are passed as-is to torch.compile().", "See torch.compile() for details on the arguments for this function.", "Moves all model parameters and buffers to the CPU.", "Note", "This method modifies the module in-place.", "self", "Module", "Moves all model parameters and buffers to the GPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.", "Note", "This method modifies the module in-place.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module", "Casts all floating point parameters and buffers to double datatype.", "Note", "This method modifies the module in-place.", "self", "Module", "Sets the module in evaluation mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "This is equivalent with self.train(False).", "See Locally disabling gradient computation for a comparison between .eval() and several similar mechanisms that may be confused with it.", "self", "Module", "Set the extra representation of the module", "To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.", "str", "Casts all floating point parameters and buffers to float datatype.", "Note", "This method modifies the module in-place.", "self", "Module", "Returns the buffer given by target if it exists, otherwise throws an error.", "See the docstring for get_submodule for a more detailed explanation of this method\u2019s functionality as well as how to correctly specify target.", "target (str) \u2013 The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.)", "The buffer referenced by target", "torch.Tensor", "AttributeError \u2013 If the target string references an invalid path or resolves to something that is not a buffer", "Returns any extra state to include in the module\u2019s state_dict. Implement this and a corresponding set_extra_state() for your module if you need to store extra state. This function is called when building the module\u2019s state_dict().", "Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.", "Any extra state to store in the module\u2019s state_dict", "object", "Returns the parameter given by target if it exists, otherwise throws an error.", "See the docstring for get_submodule for a more detailed explanation of this method\u2019s functionality as well as how to correctly specify target.", "target (str) \u2013 The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.)", "The Parameter referenced by target", "torch.nn.Parameter", "AttributeError \u2013 If the target string references an invalid path or resolves to something that is not an nn.Parameter", "Returns the submodule given by target if it exists, otherwise throws an error.", "For example, let\u2019s say you have an nn.Module A that looks like this:", "(The diagram shows an nn.Module A. A has a nested submodule net_b, which itself has two submodules net_c and linear. net_c then has a submodule conv.)", "To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\"). To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\").", "The runtime of get_submodule is bounded by the degree of module nesting in target. A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used.", "target (str) \u2013 The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)", "The submodule referenced by target", "torch.nn.Module", "AttributeError \u2013 If the target string references an invalid path or resolves to something that is not an nn.Module", "Returns a string representation of the internal graph for the forward method. See Interpreting Graphs for details.", "Casts all floating point parameters and buffers to half datatype.", "Note", "This method modifies the module in-place.", "self", "Module", "Returns a string representation of the internal graph for the forward method. This graph will be preprocessed to inline all function and method calls. See Interpreting Graphs for details.", "Moves all model parameters and buffers to the IPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on IPU while being optimized.", "Note", "This method modifies the module in-place.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module", "Copies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.", "Warning", "If assign is True the optimizer must be created after the call to load_state_dict.", "NamedTuple with missing_keys and unexpected_keys fields", "Note", "If a parameter or buffer is registered as None and its corresponding key exists in state_dict, load_state_dict() will raise a RuntimeError.", "Returns an iterator over all modules in the network.", "Module \u2013 a module in the network", "Iterator[Module]", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:", "Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.", "(str, torch.Tensor) \u2013 Tuple containing the name and buffer", "Iterator[Tuple[str, Tensor]]", "Example:", "Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.", "(str, Module) \u2013 Tuple containing a name and child module", "Iterator[Tuple[str, Module]]", "Example:", "Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.", "(str, Module) \u2013 Tuple of name and module", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:", "Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.", "(str, Parameter) \u2013 Tuple containing the name and parameter", "Iterator[Tuple[str, Parameter]]", "Example:", "Returns an iterator over module parameters.", "This is typically passed to an optimizer.", "recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.", "Parameter \u2013 module parameter", "Iterator[Parameter]", "Example:", "Registers a backward hook on the module.", "This function is deprecated in favor of register_full_backward_hook() and the behavior of this function will change in future versions.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Adds a buffer to the module.", "This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict.", "Buffers can be accessed as attributes using given names.", "Example:", "Registers a forward hook on the module.", "The hook will be called every time after forward() has computed an output.", "If with_kwargs is False or not specified, the input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called. The hook should have the following signature:", "If with_kwargs is True, the forward hook will be passed the kwargs given to the forward function and be expected to return the output possibly modified. The hook should have the following signature:", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Registers a forward pre-hook on the module.", "The hook will be called every time before forward() is invoked.", "If with_kwargs is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature:", "If with_kwargs is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature:", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Registers a backward hook on the module.", "The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature:", "The grad_input and grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in grad_input and grad_output will be None for all non-Tensor arguments.", "For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module\u2019s forward function.", "Warning", "Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Registers a backward pre-hook on the module.", "The hook will be called every time the gradients for the module are computed. The hook should have the following signature:", "The grad_output is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of grad_output in subsequent computations. Entries in grad_output will be None for all non-Tensor arguments.", "For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module\u2019s forward function.", "Warning", "Modifying inputs inplace is not allowed when using backward hooks and will raise an error.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Registers a post hook to be run after module\u2019s load_state_dict is called.", "hook(module, incompatible_keys) -> None", "The module argument is the current module that this hook is registered on, and the incompatible_keys argument is a NamedTuple consisting of attributes missing_keys and unexpected_keys. missing_keys is a list of str containing the missing keys and unexpected_keys is a list of str containing the unexpected keys.", "The given incompatible_keys can be modified inplace if needed.", "Note that the checks performed when calling load_state_dict() with strict=True are affected by modifications the hook makes to missing_keys or unexpected_keys, as expected. Additions to either set of keys will result in an error being thrown when strict=True, and clearing out both missing and unexpected keys will avoid an error.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Alias for add_module().", "Adds a parameter to the module.", "The parameter can be accessed as an attribute using given name.", "These hooks will be called with arguments: self, prefix, and keep_vars before calling state_dict on self. The registered hooks can be used to perform pre-processing before the state_dict call is made.", "Change if autograd should record operations on parameters in this module.", "This method sets the parameters\u2019 requires_grad attributes in-place.", "This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).", "See Locally disabling gradient computation for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it.", "requires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.", "self", "Module", "See torch.jit.save witch accepts a file-like object. This function, torch.save(), converts the object to a string, treating it as a path. DO NOT confuse these two functions when it comes to the \u2018f\u2019 parameter functionality.", "This function is called from load_state_dict() to handle any extra state found within the state_dict. Implement this function and a corresponding get_extra_state() for your module if you need to store extra state within its state_dict.", "state (dict) \u2013 Extra state from the state_dict", "See torch.Tensor.share_memory_()", "T", "Returns a dictionary containing references to the whole state of the module.", "Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included.", "Note", "The returned object is a shallow copy. It contains references to the module\u2019s parameters and buffers.", "Warning", "Currently state_dict() also accepts positional arguments for destination, prefix and keep_vars in order. However, this is being deprecated and keyword arguments will be enforced in future releases.", "Warning", "Please avoid the use of argument destination as it is not designed for end-users.", "a dictionary containing a whole state of the module", "dict", "Example:", "Moves and/or casts the parameters and buffers.", "This can be called as", "Its signature is similar to torch.Tensor.to(), but only accepts floating point or complex dtypes. In addition, this method will only cast the floating point or complex parameters and buffers to dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.", "See below for examples.", "Note", "This method modifies the module in-place.", "self", "Module", "Examples:", "Moves the parameters and buffers to the specified device without copying storage.", "self", "Module", "Sets the module in training mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "mode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.", "self", "Module", "Casts all parameters and buffers to dst_type.", "Note", "This method modifies the module in-place.", "dst_type (type or string) \u2013 the desired type", "self", "Module", "Moves all model parameters and buffers to the XPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.", "Note", "This method modifies the module in-place.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module", "Resets gradients of all model parameters. See similar function under torch.optim.Optimizer for more context.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See torch.optim.Optimizer.zero_grad() for details."]}, {"name": "torch.jit.ScriptModule.add_module()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.add_module", "type": "Torch Script", "text": ["Adds a child module to the current module.", "The module can be accessed as an attribute using the given name."]}, {"name": "torch.jit.ScriptModule.apply()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.apply", "type": "Torch Script", "text": ["Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).", "fn (Module -> None) \u2013 function to be applied to each submodule", "self", "Module", "Example:"]}, {"name": "torch.jit.ScriptModule.bfloat16()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.bfloat16", "type": "Torch Script", "text": ["Casts all floating point parameters and buffers to bfloat16 datatype.", "Note", "This method modifies the module in-place.", "self", "Module"]}, {"name": "torch.jit.ScriptModule.buffers()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.buffers", "type": "Torch Script", "text": ["Returns an iterator over module buffers.", "recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.", "torch.Tensor \u2013 module buffer", "Iterator[Tensor]", "Example:"]}, {"name": "torch.jit.ScriptModule.children()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.children", "type": "Torch Script", "text": ["Returns an iterator over immediate children modules.", "Module \u2013 a child module", "Iterator[Module]"]}, {"name": "torch.jit.ScriptModule.code", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.code", "type": "Torch Script", "text": ["Returns a pretty-printed representation (as valid Python syntax) of the internal graph for the forward method. See Inspecting Code for details."]}, {"name": "torch.jit.ScriptModule.code_with_constants", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.code_with_constants", "type": "Torch Script", "text": ["Returns a tuple of:", "[0] a pretty-printed representation (as valid Python syntax) of the internal graph for the forward method. See code. [1] a ConstMap following the CONSTANT.cN format of the output in [0]. The indices in the [0] output are keys to the underlying constant\u2019s values.", "See Inspecting Code for details."]}, {"name": "torch.jit.ScriptModule.compile()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.compile", "type": "Torch Script", "text": ["Compile this Module\u2019s forward using torch.compile().", "This Module\u2019s __call__ method is compiled and all arguments are passed as-is to torch.compile().", "See torch.compile() for details on the arguments for this function."]}, {"name": "torch.jit.ScriptModule.cpu()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.cpu", "type": "Torch Script", "text": ["Moves all model parameters and buffers to the CPU.", "Note", "This method modifies the module in-place.", "self", "Module"]}, {"name": "torch.jit.ScriptModule.cuda()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.cuda", "type": "Torch Script", "text": ["Moves all model parameters and buffers to the GPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.", "Note", "This method modifies the module in-place.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module"]}, {"name": "torch.jit.ScriptModule.double()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.double", "type": "Torch Script", "text": ["Casts all floating point parameters and buffers to double datatype.", "Note", "This method modifies the module in-place.", "self", "Module"]}, {"name": "torch.jit.ScriptModule.eval()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.eval", "type": "Torch Script", "text": ["Sets the module in evaluation mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "This is equivalent with self.train(False).", "See Locally disabling gradient computation for a comparison between .eval() and several similar mechanisms that may be confused with it.", "self", "Module"]}, {"name": "torch.jit.ScriptModule.extra_repr()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.extra_repr", "type": "Torch Script", "text": ["Set the extra representation of the module", "To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.", "str"]}, {"name": "torch.jit.ScriptModule.float()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.float", "type": "Torch Script", "text": ["Casts all floating point parameters and buffers to float datatype.", "Note", "This method modifies the module in-place.", "self", "Module"]}, {"name": "torch.jit.ScriptModule.get_buffer()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.get_buffer", "type": "Torch Script", "text": ["Returns the buffer given by target if it exists, otherwise throws an error.", "See the docstring for get_submodule for a more detailed explanation of this method\u2019s functionality as well as how to correctly specify target.", "target (str) \u2013 The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.)", "The buffer referenced by target", "torch.Tensor", "AttributeError \u2013 If the target string references an invalid path or resolves to something that is not a buffer"]}, {"name": "torch.jit.ScriptModule.get_extra_state()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.get_extra_state", "type": "Torch Script", "text": ["Returns any extra state to include in the module\u2019s state_dict. Implement this and a corresponding set_extra_state() for your module if you need to store extra state. This function is called when building the module\u2019s state_dict().", "Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.", "Any extra state to store in the module\u2019s state_dict", "object"]}, {"name": "torch.jit.ScriptModule.get_parameter()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.get_parameter", "type": "Torch Script", "text": ["Returns the parameter given by target if it exists, otherwise throws an error.", "See the docstring for get_submodule for a more detailed explanation of this method\u2019s functionality as well as how to correctly specify target.", "target (str) \u2013 The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.)", "The Parameter referenced by target", "torch.nn.Parameter", "AttributeError \u2013 If the target string references an invalid path or resolves to something that is not an nn.Parameter"]}, {"name": "torch.jit.ScriptModule.get_submodule()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.get_submodule", "type": "Torch Script", "text": ["Returns the submodule given by target if it exists, otherwise throws an error.", "For example, let\u2019s say you have an nn.Module A that looks like this:", "(The diagram shows an nn.Module A. A has a nested submodule net_b, which itself has two submodules net_c and linear. net_c then has a submodule conv.)", "To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\"). To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\").", "The runtime of get_submodule is bounded by the degree of module nesting in target. A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used.", "target (str) \u2013 The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)", "The submodule referenced by target", "torch.nn.Module", "AttributeError \u2013 If the target string references an invalid path or resolves to something that is not an nn.Module"]}, {"name": "torch.jit.ScriptModule.graph", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.graph", "type": "Torch Script", "text": ["Returns a string representation of the internal graph for the forward method. See Interpreting Graphs for details."]}, {"name": "torch.jit.ScriptModule.half()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.half", "type": "Torch Script", "text": ["Casts all floating point parameters and buffers to half datatype.", "Note", "This method modifies the module in-place.", "self", "Module"]}, {"name": "torch.jit.ScriptModule.inlined_graph", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.inlined_graph", "type": "Torch Script", "text": ["Returns a string representation of the internal graph for the forward method. This graph will be preprocessed to inline all function and method calls. See Interpreting Graphs for details."]}, {"name": "torch.jit.ScriptModule.ipu()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.ipu", "type": "Torch Script", "text": ["Moves all model parameters and buffers to the IPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on IPU while being optimized.", "Note", "This method modifies the module in-place.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module"]}, {"name": "torch.jit.ScriptModule.load_state_dict()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.load_state_dict", "type": "Torch Script", "text": ["Copies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.", "Warning", "If assign is True the optimizer must be created after the call to load_state_dict.", "NamedTuple with missing_keys and unexpected_keys fields", "Note", "If a parameter or buffer is registered as None and its corresponding key exists in state_dict, load_state_dict() will raise a RuntimeError."]}, {"name": "torch.jit.ScriptModule.modules()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.modules", "type": "Torch Script", "text": ["Returns an iterator over all modules in the network.", "Module \u2013 a module in the network", "Iterator[Module]", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:"]}, {"name": "torch.jit.ScriptModule.named_buffers()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_buffers", "type": "Torch Script", "text": ["Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.", "(str, torch.Tensor) \u2013 Tuple containing the name and buffer", "Iterator[Tuple[str, Tensor]]", "Example:"]}, {"name": "torch.jit.ScriptModule.named_children()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_children", "type": "Torch Script", "text": ["Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.", "(str, Module) \u2013 Tuple containing a name and child module", "Iterator[Tuple[str, Module]]", "Example:"]}, {"name": "torch.jit.ScriptModule.named_modules()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_modules", "type": "Torch Script", "text": ["Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.", "(str, Module) \u2013 Tuple of name and module", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:"]}, {"name": "torch.jit.ScriptModule.named_parameters()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_parameters", "type": "Torch Script", "text": ["Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.", "(str, Parameter) \u2013 Tuple containing the name and parameter", "Iterator[Tuple[str, Parameter]]", "Example:"]}, {"name": "torch.jit.ScriptModule.parameters()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.parameters", "type": "Torch Script", "text": ["Returns an iterator over module parameters.", "This is typically passed to an optimizer.", "recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.", "Parameter \u2013 module parameter", "Iterator[Parameter]", "Example:"]}, {"name": "torch.jit.ScriptModule.register_backward_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_backward_hook", "type": "Torch Script", "text": ["Registers a backward hook on the module.", "This function is deprecated in favor of register_full_backward_hook() and the behavior of this function will change in future versions.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.jit.ScriptModule.register_buffer()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_buffer", "type": "Torch Script", "text": ["Adds a buffer to the module.", "This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict.", "Buffers can be accessed as attributes using given names.", "Example:"]}, {"name": "torch.jit.ScriptModule.register_forward_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_forward_hook", "type": "Torch Script", "text": ["Registers a forward hook on the module.", "The hook will be called every time after forward() has computed an output.", "If with_kwargs is False or not specified, the input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called. The hook should have the following signature:", "If with_kwargs is True, the forward hook will be passed the kwargs given to the forward function and be expected to return the output possibly modified. The hook should have the following signature:", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.jit.ScriptModule.register_forward_pre_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_forward_pre_hook", "type": "Torch Script", "text": ["Registers a forward pre-hook on the module.", "The hook will be called every time before forward() is invoked.", "If with_kwargs is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature:", "If with_kwargs is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature:", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.jit.ScriptModule.register_full_backward_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_full_backward_hook", "type": "Torch Script", "text": ["Registers a backward hook on the module.", "The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature:", "The grad_input and grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in grad_input and grad_output will be None for all non-Tensor arguments.", "For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module\u2019s forward function.", "Warning", "Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.jit.ScriptModule.register_full_backward_pre_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_full_backward_pre_hook", "type": "Torch Script", "text": ["Registers a backward pre-hook on the module.", "The hook will be called every time the gradients for the module are computed. The hook should have the following signature:", "The grad_output is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of grad_output in subsequent computations. Entries in grad_output will be None for all non-Tensor arguments.", "For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module\u2019s forward function.", "Warning", "Modifying inputs inplace is not allowed when using backward hooks and will raise an error.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.jit.ScriptModule.register_load_state_dict_post_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_load_state_dict_post_hook", "type": "Torch Script", "text": ["Registers a post hook to be run after module\u2019s load_state_dict is called.", "hook(module, incompatible_keys) -> None", "The module argument is the current module that this hook is registered on, and the incompatible_keys argument is a NamedTuple consisting of attributes missing_keys and unexpected_keys. missing_keys is a list of str containing the missing keys and unexpected_keys is a list of str containing the unexpected keys.", "The given incompatible_keys can be modified inplace if needed.", "Note that the checks performed when calling load_state_dict() with strict=True are affected by modifications the hook makes to missing_keys or unexpected_keys, as expected. Additions to either set of keys will result in an error being thrown when strict=True, and clearing out both missing and unexpected keys will avoid an error.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.jit.ScriptModule.register_module()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_module", "type": "Torch Script", "text": ["Alias for add_module()."]}, {"name": "torch.jit.ScriptModule.register_parameter()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_parameter", "type": "Torch Script", "text": ["Adds a parameter to the module.", "The parameter can be accessed as an attribute using given name."]}, {"name": "torch.jit.ScriptModule.register_state_dict_pre_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_state_dict_pre_hook", "type": "Torch Script", "text": ["These hooks will be called with arguments: self, prefix, and keep_vars before calling state_dict on self. The registered hooks can be used to perform pre-processing before the state_dict call is made."]}, {"name": "torch.jit.ScriptModule.requires_grad_()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.requires_grad_", "type": "Torch Script", "text": ["Change if autograd should record operations on parameters in this module.", "This method sets the parameters\u2019 requires_grad attributes in-place.", "This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).", "See Locally disabling gradient computation for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it.", "requires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.", "self", "Module"]}, {"name": "torch.jit.ScriptModule.save()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.save", "type": "Torch Script", "text": ["See torch.jit.save witch accepts a file-like object. This function, torch.save(), converts the object to a string, treating it as a path. DO NOT confuse these two functions when it comes to the \u2018f\u2019 parameter functionality."]}, {"name": "torch.jit.ScriptModule.set_extra_state()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.set_extra_state", "type": "Torch Script", "text": ["This function is called from load_state_dict() to handle any extra state found within the state_dict. Implement this function and a corresponding get_extra_state() for your module if you need to store extra state within its state_dict.", "state (dict) \u2013 Extra state from the state_dict"]}, {"name": "torch.jit.ScriptModule.share_memory()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.share_memory", "type": "Torch Script", "text": ["See torch.Tensor.share_memory_()", "T"]}, {"name": "torch.jit.ScriptModule.state_dict()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.state_dict", "type": "Torch Script", "text": ["Returns a dictionary containing references to the whole state of the module.", "Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included.", "Note", "The returned object is a shallow copy. It contains references to the module\u2019s parameters and buffers.", "Warning", "Currently state_dict() also accepts positional arguments for destination, prefix and keep_vars in order. However, this is being deprecated and keyword arguments will be enforced in future releases.", "Warning", "Please avoid the use of argument destination as it is not designed for end-users.", "a dictionary containing a whole state of the module", "dict", "Example:"]}, {"name": "torch.jit.ScriptModule.to()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.to", "type": "Torch Script", "text": ["Moves and/or casts the parameters and buffers.", "This can be called as", "Its signature is similar to torch.Tensor.to(), but only accepts floating point or complex dtypes. In addition, this method will only cast the floating point or complex parameters and buffers to dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.", "See below for examples.", "Note", "This method modifies the module in-place.", "self", "Module", "Examples:"]}, {"name": "torch.jit.ScriptModule.to_empty()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.to_empty", "type": "Torch Script", "text": ["Moves the parameters and buffers to the specified device without copying storage.", "self", "Module"]}, {"name": "torch.jit.ScriptModule.train()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.train", "type": "Torch Script", "text": ["Sets the module in training mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "mode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.", "self", "Module"]}, {"name": "torch.jit.ScriptModule.type()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.type", "type": "Torch Script", "text": ["Casts all parameters and buffers to dst_type.", "Note", "This method modifies the module in-place.", "dst_type (type or string) \u2013 the desired type", "self", "Module"]}, {"name": "torch.jit.ScriptModule.xpu()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.xpu", "type": "Torch Script", "text": ["Moves all model parameters and buffers to the XPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.", "Note", "This method modifies the module in-place.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module"]}, {"name": "torch.jit.ScriptModule.zero_grad()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.zero_grad", "type": "Torch Script", "text": ["Resets gradients of all model parameters. See similar function under torch.optim.Optimizer for more context.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See torch.optim.Optimizer.zero_grad() for details."]}, {"name": "torch.jit.set_fusion_strategy()", "path": "generated/torch.jit.set_fusion_strategy#torch.jit.set_fusion_strategy", "type": "Torch Script", "text": ["Sets the type and number of specializations that can occur during fusion.", "Usage: provide a list of pairs (type, depth) where type is one of \u201cSTATIC\u201d or \u201cDYNAMIC\u201d and depth is an integer.", "In STATIC fusion, fused ops are compiled to have fixed input shapes. The shape is determined based on some initial profiling runs. In DYNAMIC fusion, fused ops are compiled to have variable input shapes, so that multiple shapes are possible.", "In both cases, we also recompile on new striding behavior, device, or dtype.", "When an input doesn\u2019t match the format required by the specialized compiled op, it will run a fallback function. Fallback functions are recursively be compiled and specialized based on the observed tensor shapes. Since compilation can be slow, the \u201cdepth\u201d parameter is provided to limit the number of specializations that can be compiled, before giving up on recompiling and falling back to a completely un-fused, un-specialized implementation.", "The list of (type, depth) pairs controls the type of specializations and the number of specializations. For example: [(\u201cSTATIC\u201d, 2), (\u201cDYNAMIC\u201d, 2)] indicates that the first two specializations will use static fusions, the following two specializations will use dynamic fusion, and any inputs that satisfy none of the 4 options will run an unfused implementation.", "NB: in the future, if more as more fusion backends are added there may be more granular apis for specific fusers."]}, {"name": "torch.jit.strict_fusion", "path": "generated/torch.jit.strict_fusion#torch.jit.strict_fusion", "type": "Torch Script", "text": ["This class errors if not all nodes have been fused in inference, or symbolically differentiated in training.", "Example:", "Forcing fusion of additions."]}, {"name": "torch.jit.trace()", "path": "generated/torch.jit.trace#torch.jit.trace", "type": "Torch Script", "text": ["Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation. Tracing is ideal for code that operates only on Tensors and lists, dictionaries, and tuples of Tensors.", "Using torch.jit.trace and torch.jit.trace_module, you can turn an existing module or Python function into a TorchScript ScriptFunction or ScriptModule. You must provide example inputs, and we run the function, recording the operations performed on all the tensors.", "This module also contains any parameters that the original module had as well.", "Warning", "Tracing only correctly records functions and modules which are not data dependent (e.g., do not have conditionals on data in tensors) and do not have any untracked external dependencies (e.g., perform input/output or access global variables). Tracing only records operations done when the given function is run on the given tensors. Therefore, the returned ScriptModule will always run the same traced graph on any input. This has some important implications when your module is expected to run different sets of operations, depending on the input and/or the module state. For example,", "In cases like these, tracing would not be appropriate and scripting is a better choice. If you trace such models, you may silently get incorrect results on subsequent invocations of the model. The tracer will try to emit warnings when doing something that may cause an incorrect trace to be produced.", "func (callable or torch.nn.Module) \u2013 A Python function or torch.nn.Module that will be run with example_inputs. func arguments and return values must be tensors or (possibly nested) tuples that contain tensors. When a module is passed torch.jit.trace, only the forward method is run and traced (see torch.jit.trace for details).", "If func is nn.Module or forward of nn.Module, trace returns a ScriptModule object with a single forward method containing the traced code. The returned ScriptModule will have the same set of sub-modules and parameters as the original nn.Module. If func is a standalone function, trace returns ScriptFunction.", "Example (tracing a function):", "Example (tracing an existing module):"]}, {"name": "torch.jit.trace_module()", "path": "generated/torch.jit.trace_module#torch.jit.trace_module", "type": "Torch Script", "text": ["Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation. When a module is passed to torch.jit.trace, only the forward method is run and traced. With trace_module, you can specify a dictionary of method names to example inputs to trace (see the inputs) argument below.", "See torch.jit.trace for more information on tracing.", "A ScriptModule object with a single forward method containing the traced code. When func is a torch.nn.Module, the returned ScriptModule will have the same set of sub-modules and parameters as func.", "Example (tracing a module with multiple methods):"]}, {"name": "torch.jit.unused()", "path": "generated/torch.jit.unused#torch.jit.unused", "type": "Torch Script", "text": ["This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception. This allows you to leave code in your model that is not yet TorchScript compatible and still export your model.", "Example (using @torch.jit.unused on a method):"]}, {"name": "torch.jit.wait()", "path": "generated/torch.jit.wait#torch.jit.wait", "type": "Torch Script", "text": ["Forces completion of a torch.jit.Future[T] asynchronous task, returning the result of the task. See fork() for docs and examples. :param future: an asynchronous task reference, created through torch.jit.fork :type future: torch.jit.Future[T]", "the return value of the the completed task", "T"]}, {"name": "torch.kaiser_window", "path": "generated/torch.kaiser_window", "type": "Torch", "text": ["Computes the Kaiser window with window length window_length and shape parameter beta.", "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and N = L - 1 if periodic is False and L if periodic is True, where L is the window_length. This function computes:", "Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling torch.kaiser_window(L + 1, B, periodic=False)[:-1]). The periodic argument is intended as a helpful shorthand to produce a periodic window as input to functions like torch.stft().", "Note", "If window_length is one, then the returned window is a single element tensor containing a one."]}, {"name": "torch.kron", "path": "generated/torch.kron", "type": "Torch", "text": ["Computes the Kronecker product, denoted by \u2297\\otimes, of input and other.", "If input is a (a0\u00d7a1\u00d7\u22ef\u00d7an)(a_0 \\times a_1 \\times \\dots \\times a_n) tensor and other is a (b0\u00d7b1\u00d7\u22ef\u00d7bn)(b_0 \\times b_1 \\times \\dots \\times b_n) tensor, the result will be a (a0\u2217b0\u00d7a1\u2217b1\u00d7\u22ef\u00d7an\u2217bn)(a_0*b_0 \\times a_1*b_1 \\times \\dots \\times a_n*b_n) tensor with the following entries:", "where kt=it\u2217bt+jtk_t = i_t * b_t + j_t for 0\u2264t\u2264n0 \\leq t \\leq n. If one tensor has fewer dimensions than the other it is unsqueezed until it has the same number of dimensions.", "Supports real-valued and complex-valued inputs.", "Note", "This function generalizes the typical definition of the Kronecker product for two matrices to two tensors, as described above. When input is a (m\u00d7n)(m \\times n) matrix and other is a (p\u00d7q)(p \\times q) matrix, the result will be a (p\u2217m\u00d7q\u2217n)(p*m \\times q*n) block matrix:", "where input is A\\mathbf{A} and other is B\\mathbf{B}.", "out (Tensor, optional) \u2013 The output tensor. Ignored if None. Default: None", "Examples:"]}, {"name": "torch.kthvalue", "path": "generated/torch.kthvalue", "type": "Torch", "text": ["Returns a namedtuple (values, indices) where values is the k th smallest element of each row of the input tensor in the given dimension dim. And indices is the index location of each element found.", "If dim is not given, the last dimension of the input is chosen.", "If keepdim is True, both the values and indices tensors are the same size as input, except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in both the values and indices tensors having 1 fewer dimension than the input tensor.", "Note", "When input is a CUDA tensor and there are multiple valid k th values, this function may nondeterministically return indices for any of them.", "out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers", "Example:"]}, {"name": "torch.layout", "path": "tensor_attributes#torch.layout", "type": "Miscellaneous", "text": []}, {"name": "torch.lcm", "path": "generated/torch.lcm", "type": "Torch", "text": ["Computes the element-wise least common multiple (LCM) of input and other.", "Both input and other must have integer types.", "Note", "This defines lcm(0,0)=0lcm(0, 0) = 0 and lcm(0,a)=0lcm(0, a) = 0.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.ldexp", "path": "generated/torch.ldexp", "type": "Torch", "text": ["Multiplies input by 2 ** other.", "Typically this function is used to construct floating point numbers by multiplying mantissas in input with integral powers of two created from the exponents in other.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.le", "path": "generated/torch.le", "type": "Torch", "text": ["Computes input\u2264other\\text{input} \\leq \\text{other} element-wise.", "The second argument can be a number or a tensor whose shape is broadcastable with the first argument.", "out (Tensor, optional) \u2013 the output tensor.", "A boolean tensor that is True where input is less than or equal to other and False elsewhere", "Example:"]}, {"name": "torch.lerp", "path": "generated/torch.lerp", "type": "Torch", "text": ["Does a linear interpolation of two tensors start (given by input) and end based on a scalar or tensor weight and returns the resulting out tensor.", "The shapes of start and end must be broadcastable. If weight is a tensor, then the shapes of weight, start, and end must be broadcastable.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.less", "path": "generated/torch.less", "type": "Torch", "text": ["Alias for torch.lt()."]}, {"name": "torch.less_equal", "path": "generated/torch.less_equal", "type": "Torch", "text": ["Alias for torch.le()."]}, {"name": "torch.lgamma", "path": "generated/torch.lgamma", "type": "Torch", "text": ["Computes the natural logarithm of the absolute value of the gamma function on input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.library", "path": "library", "type": "Miscellaneous", "text": ["Python operator registration API provides capabilities for extending PyTorch\u2019s core library of operators with user defined operators. Currently, this can be done in two ways:", "Creating new libraries", "Lets you to register new operators and kernels for various backends and functionalities by specifying the appropriate dispatch keys. For example,", "Extending existing C++ libraries (e.g., aten)", "This may come in handy to fill up spotty operator support for a feature implemented through a dispatch key. For example.,", "A tutorial that walks you through some examples on how to use this API is available on Google Colab.", "Warning", "Dispatcher is a complicated PyTorch concept and having a sound understanding of Dispatcher is crucial to be able to do anything advanced with this API. This blog post is a good starting point to learn about Dispatcher.", "A class to create libraries that can be used to register new operators or override operators in existing libraries from Python. A user can optionally pass in a dispatch keyname if they only want to register kernels corresponding to only one specific dispatch key.", "To create a library to override operators in an existing library (with name ns), set the kind to \u201cIMPL\u201d. To create a new library (with name ns) to register new operators, set the kind to \u201cDEF\u201d. To create a fragment of a possibly existing library to register operators (and bypass the limitation that there is only one library for a given namespace), set the kind to \u201cFRAGMENT\u201d.", "Defines a new operator and its semantics in the ns namespace.", "name of the operator as inferred from the schema.", "Registers the function implementation for an operator defined in the library.", "A dummy function to pass to Library.impl in order to register a fallthrough.", "We have also added some function decorators to make it convenient to register functions for operators:"]}, {"name": "torch.library.fallthrough_kernel()", "path": "library#torch.library.fallthrough_kernel", "type": "Miscellaneous", "text": ["A dummy function to pass to Library.impl in order to register a fallthrough."]}, {"name": "torch.library.Library", "path": "library#torch.library.Library", "type": "Miscellaneous", "text": ["A class to create libraries that can be used to register new operators or override operators in existing libraries from Python. A user can optionally pass in a dispatch keyname if they only want to register kernels corresponding to only one specific dispatch key.", "To create a library to override operators in an existing library (with name ns), set the kind to \u201cIMPL\u201d. To create a new library (with name ns) to register new operators, set the kind to \u201cDEF\u201d. To create a fragment of a possibly existing library to register operators (and bypass the limitation that there is only one library for a given namespace), set the kind to \u201cFRAGMENT\u201d.", "Defines a new operator and its semantics in the ns namespace.", "name of the operator as inferred from the schema.", "Registers the function implementation for an operator defined in the library."]}, {"name": "torch.library.Library.define()", "path": "library#torch.library.Library.define", "type": "Miscellaneous", "text": ["Defines a new operator and its semantics in the ns namespace.", "name of the operator as inferred from the schema."]}, {"name": "torch.library.Library.impl()", "path": "library#torch.library.Library.impl", "type": "Miscellaneous", "text": ["Registers the function implementation for an operator defined in the library."]}, {"name": "torch.linalg", "path": "linalg", "type": "Linear Algebra", "text": ["Common linear algebra operations.", "See Linear algebra (torch.linalg) for some common numerical edge-cases.", "Computes a vector or matrix norm.", "Computes a vector norm.", "Computes a matrix norm.", "Alias for torch.diagonal() with defaults dim1= -2, dim2= -1.", "Computes the determinant of a square matrix.", "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.", "Computes the condition number of a matrix with respect to a matrix norm.", "Computes the numerical rank of a matrix.", "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.", "Computes the QR decomposition of a matrix.", "Computes the LU decomposition with partial pivoting of a matrix.", "Computes a compact representation of the LU factorization with partial pivoting of a matrix.", "Computes the eigenvalue decomposition of a square matrix if it exists.", "Computes the eigenvalues of a square matrix.", "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.", "Computes the eigenvalues of a complex Hermitian or real symmetric matrix.", "Computes the singular value decomposition (SVD) of a matrix.", "Computes the singular values of a matrix.", "Computes the solution of a square system of linear equations with a unique solution.", "Computes the solution of a triangular system of linear equations with a unique solution.", "Computes the solution of a square system of linear equations with a unique solution given an LU decomposition.", "Computes a solution to the least squares problem of a system of linear equations.", "Computes the inverse of a square matrix if it exists.", "Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.", "Computes the matrix exponential of a square matrix.", "Computes the n-th power of a square matrix for an integer n.", "Computes the cross product of two 3-dimensional vectors.", "Alias for torch.matmul()", "Computes the dot product of two batches of vectors along a dimension.", "Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.", "Computes the first n columns of a product of Householder matrices.", "Computes the multiplicative inverse of torch.tensordot().", "Computes the solution X to the system torch.tensordot(A, X) = B.", "Generates a Vandermonde matrix.", "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.", "Computes the inverse of a square matrix if it is invertible.", "A version of solve() that does not perform error checks unless check_errors= True.", "This is a version of lu_factor() that does not perform error checks unless check_errors= True.", "Computes a compact representation of the LDL factorization of a Hermitian or symmetric (possibly indefinite) matrix.", "This is a version of ldl_factor() that does not perform error checks unless check_errors= True.", "Computes the solution of a system of linear equations using the LDL factorization."]}, {"name": "torch.linalg.cholesky()", "path": "generated/torch.linalg.cholesky#torch.linalg.cholesky", "type": "Linear Algebra", "text": ["Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix A\u2208Kn\u00d7nA \\in \\mathbb{K}^{n \\times n} is defined as", "where LL is a lower triangular matrix with real positive diagonal (even in the complex case) and LHL^{\\text{H}} is the conjugate transpose when LL is complex, and the transpose when LL is real-valued.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "Note", "When inputs are on a CUDA device, this function synchronizes that device with the CPU.", "See also", "torch.linalg.cholesky_ex() for a version of this operation that skips the (slow) error checking by default and instead returns the debug information. This makes it a faster way to check if a matrix is positive-definite.", "torch.linalg.eigh() for a different decomposition of a Hermitian matrix. The eigenvalue decomposition gives more information about the matrix but it slower to compute than the Cholesky decomposition.", "A (Tensor) \u2013 tensor of shape (*, n, n) where * is zero or more batch dimensions consisting of symmetric or Hermitian positive-definite matrices.", "RuntimeError \u2013 if the A matrix or any matrix in a batched A is not Hermitian (resp. symmetric) positive-definite. If A is a batch of matrices, the error message will include the batch index of the first matrix that fails to meet this condition.", "Examples:"]}, {"name": "torch.linalg.cholesky_ex()", "path": "generated/torch.linalg.cholesky_ex#torch.linalg.cholesky_ex", "type": "Linear Algebra", "text": ["Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.", "This function skips the (slow) error checking and error message construction of torch.linalg.cholesky(), instead directly returning the LAPACK error codes as part of a named tuple (L, info). This makes this function a faster way to check if a matrix is positive-definite, and it provides an opportunity to handle decomposition errors more gracefully or performantly than torch.linalg.cholesky() does.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "If A is not a Hermitian positive-definite matrix, or if it\u2019s a batch of matrices and one or more of them is not a Hermitian positive-definite matrix, then info stores a positive integer for the corresponding matrix. The positive integer indicates the order of the leading minor that is not positive-definite, and the decomposition could not be completed. info filled with zeros indicates that the decomposition was successful. If check_errors=True and info contains positive integers, then a RuntimeError is thrown.", "Note", "When the inputs are on a CUDA device, this function synchronizes only when check_errors= True.", "Warning", "This function is \u201cexperimental\u201d and it may change in a future PyTorch release.", "See also", "torch.linalg.cholesky() is a NumPy compatible variant that always checks for errors.", "A (Tensor) \u2013 the Hermitian n times n matrix or the batch of such matrices of size (*, n, n) where * is one or more batch dimensions.", "Examples:"]}, {"name": "torch.linalg.cond()", "path": "generated/torch.linalg.cond#torch.linalg.cond", "type": "Linear Algebra", "text": ["Computes the condition number of a matrix with respect to a matrix norm.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, the condition number \u03ba\\kappa of a matrix A\u2208Kn\u00d7nA \\in \\mathbb{K}^{n \\times n} is defined as", "The condition number of A measures the numerical stability of the linear system AX = B with respect to a matrix norm.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "p defines the matrix norm that is computed. The following norms are supported:", "p", "matrix norm", "None", "2-norm (largest singular value)", "\u2018fro\u2019", "Frobenius norm", "\u2018nuc\u2019", "nuclear norm", "inf", "max(sum(abs(x), dim=1))", "-inf", "min(sum(abs(x), dim=1))", "1", "max(sum(abs(x), dim=0))", "-1", "min(sum(abs(x), dim=0))", "2", "largest singular value", "-2", "smallest singular value", "where inf refers to float(\u2018inf\u2019), NumPy\u2019s inf object, or any equivalent object.", "For p is one of (\u2018fro\u2019, \u2018nuc\u2019, inf, -inf, 1, -1), this function uses torch.linalg.norm() and torch.linalg.inv(). As such, in this case, the matrix (or every matrix in the batch) A has to be square and invertible.", "For p in (2, -2), this function can be computed in terms of the singular values \u03c31\u2265\u2026\u2265\u03c3n\\sigma_1 \\geq \\ldots \\geq \\sigma_n", "In these cases, it is computed using torch.linalg.svdvals(). For these norms, the matrix (or every matrix in the batch) A may have any shape.", "Note", "When inputs are on a CUDA device, this function synchronizes that device with the CPU if p is one of (\u2018fro\u2019, \u2018nuc\u2019, inf, -inf, 1, -1).", "See also", "torch.linalg.solve() for a function that solves linear systems of square matrices.", "torch.linalg.lstsq() for a function that solves linear systems of general matrices.", "out (Tensor, optional) \u2013 output tensor. Ignored if None. Default: None.", "A real-valued tensor, even when A is complex.", "RuntimeError \u2013 if p is one of (\u2018fro\u2019, \u2018nuc\u2019, inf, -inf, 1, -1) and the A matrix or any matrix in the batch A is not square or invertible.", "Examples:"]}, {"name": "torch.linalg.cross()", "path": "generated/torch.linalg.cross#torch.linalg.cross", "type": "Linear Algebra", "text": ["Computes the cross product of two 3-dimensional vectors.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of vectors, for which it computes the product along the dimension dim. It broadcasts over the batch dimensions.", "out (Tensor, optional) \u2013 the output tensor. Ignored if None. Default: None."]}, {"name": "torch.linalg.det()", "path": "generated/torch.linalg.det#torch.linalg.det", "type": "Linear Algebra", "text": ["Computes the determinant of a square matrix.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "See also", "torch.linalg.slogdet() computes the sign and natural logarithm of the absolute value of the determinant of square matrices.", "A (Tensor) \u2013 tensor of shape (*, n, n) where * is zero or more batch dimensions.", "out (Tensor, optional) \u2013 output tensor. Ignored if None. Default: None.", "Examples:"]}, {"name": "torch.linalg.diagonal()", "path": "generated/torch.linalg.diagonal#torch.linalg.diagonal", "type": "Linear Algebra", "text": ["Alias for torch.diagonal() with defaults dim1= -2, dim2= -1."]}, {"name": "torch.linalg.eig()", "path": "generated/torch.linalg.eig#torch.linalg.eig", "type": "Linear Algebra", "text": ["Computes the eigenvalue decomposition of a square matrix if it exists.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, the eigenvalue decomposition of a square matrix A\u2208Kn\u00d7nA \\in \\mathbb{K}^{n \\times n} (if it exists) is defined as", "This decomposition exists if and only if AA is diagonalizable. This is the case when all its eigenvalues are different.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "Note", "The eigenvalues and eigenvectors of a real matrix may be complex.", "Note", "When inputs are on a CUDA device, this function synchronizes that device with the CPU.", "Warning", "This function assumes that A is diagonalizable (for example, when all the eigenvalues are different). If it is not diagonalizable, the returned eigenvalues will be correct but A\u2260Vdiag\u2061(\u039b)V\u22121A \\neq V \\operatorname{diag}(\\Lambda)V^{-1}.", "Warning", "The returned eigenvectors are normalized to have norm 1. Even then, the eigenvectors of a matrix are not unique, nor are they continuous with respect to A. Due to this lack of uniqueness, different hardware and software may compute different eigenvectors.", "This non-uniqueness is caused by the fact that multiplying an eigenvector by by ei\u03d5,\u03d5\u2208Re^{i \\phi}, \\phi \\in \\mathbb{R} produces another set of valid eigenvectors of the matrix. For this reason, the loss function shall not depend on the phase of the eigenvectors, as this quantity is not well-defined. This is checked when computing the gradients of this function. As such, when inputs are on a CUDA device, this function synchronizes that device with the CPU when computing the gradients. This is checked when computing the gradients of this function. As such, when inputs are on a CUDA device, the computation of the gradients of this function synchronizes that device with the CPU.", "Warning", "Gradients computed using the eigenvectors tensor will only be finite when A has distinct eigenvalues. Furthermore, if the distance between any two eigenvalues is close to zero, the gradient will be numerically unstable, as it depends on the eigenvalues \u03bbi\\lambda_i through the computation of 1min\u2061i\u2260j\u03bbi\u2212\u03bbj\\frac{1}{\\min_{i \\neq j} \\lambda_i - \\lambda_j}.", "See also", "torch.linalg.eigvals() computes only the eigenvalues. Unlike torch.linalg.eig(), the gradients of eigvals() are always numerically stable.", "torch.linalg.eigh() for a (faster) function that computes the eigenvalue decomposition for Hermitian and symmetric matrices.", "torch.linalg.svd() for a function that computes another type of spectral decomposition that works on matrices of any shape.", "torch.linalg.qr() for another (much faster) decomposition that works on matrices of any shape.", "A (Tensor) \u2013 tensor of shape (*, n, n) where * is zero or more batch dimensions consisting of diagonalizable matrices.", "out (tuple, optional) \u2013 output tuple of two tensors. Ignored if None. Default: None.", "A named tuple (eigenvalues, eigenvectors) which corresponds to \u039b\\Lambda and VV above.", "eigenvalues and eigenvectors will always be complex-valued, even when A is real. The eigenvectors will be given by the columns of eigenvectors.", "Examples:"]}, {"name": "torch.linalg.eigh()", "path": "generated/torch.linalg.eigh#torch.linalg.eigh", "type": "Linear Algebra", "text": ["Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, the eigenvalue decomposition of a complex Hermitian or real symmetric matrix A\u2208Kn\u00d7nA \\in \\mathbb{K}^{n \\times n} is defined as", "where QHQ^{\\text{H}} is the conjugate transpose when QQ is complex, and the transpose when QQ is real-valued. QQ is orthogonal in the real case and unitary in the complex case.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "A is assumed to be Hermitian (resp. symmetric), but this is not checked internally, instead:", "The eigenvalues are returned in ascending order.", "Note", "When inputs are on a CUDA device, this function synchronizes that device with the CPU.", "Note", "The eigenvalues of real symmetric or complex Hermitian matrices are always real.", "Warning", "The eigenvectors of a symmetric matrix are not unique, nor are they continuous with respect to A. Due to this lack of uniqueness, different hardware and software may compute different eigenvectors.", "This non-uniqueness is caused by the fact that multiplying an eigenvector by -1 in the real case or by ei\u03d5,\u03d5\u2208Re^{i \\phi}, \\phi \\in \\mathbb{R} in the complex case produces another set of valid eigenvectors of the matrix. For this reason, the loss function shall not depend on the phase of the eigenvectors, as this quantity is not well-defined. This is checked for complex inputs when computing the gradients of this function. As such, when inputs are complex and are on a CUDA device, the computation of the gradients of this function synchronizes that device with the CPU.", "Warning", "Gradients computed using the eigenvectors tensor will only be finite when A has distinct eigenvalues. Furthermore, if the distance between any two eigenvalues is close to zero, the gradient will be numerically unstable, as it depends on the eigenvalues \u03bbi\\lambda_i through the computation of 1min\u2061i\u2260j\u03bbi\u2212\u03bbj\\frac{1}{\\min_{i \\neq j} \\lambda_i - \\lambda_j}.", "Warning", "User may see pytorch crashes if running eigh on CUDA devices with CUDA versions before 12.1 update 1 with large ill-conditioned matrices as inputs. Refer to Linear Algebra Numerical Stability for more details. If this is the case, user may (1) tune their matrix inputs to be less ill-conditioned, or (2) use torch.backends.cuda.preferred_linalg_library() to try other supported backends.", "See also", "torch.linalg.eigvalsh() computes only the eigenvalues of a Hermitian matrix. Unlike torch.linalg.eigh(), the gradients of eigvalsh() are always numerically stable.", "torch.linalg.cholesky() for a different decomposition of a Hermitian matrix. The Cholesky decomposition gives less information about the matrix but is much faster to compute than the eigenvalue decomposition.", "torch.linalg.eig() for a (slower) function that computes the eigenvalue decomposition of a not necessarily Hermitian square matrix.", "torch.linalg.svd() for a (slower) function that computes the more general SVD decomposition of matrices of any shape.", "torch.linalg.qr() for another (much faster) decomposition that works on general matrices.", "out (tuple, optional) \u2013 output tuple of two tensors. Ignored if None. Default: None.", "A named tuple (eigenvalues, eigenvectors) which corresponds to \u039b\\Lambda and QQ above.", "eigenvalues will always be real-valued, even when A is complex. It will also be ordered in ascending order.", "eigenvectors will have the same dtype as A and will contain the eigenvectors as its columns."]}, {"name": "torch.linalg.eigvals()", "path": "generated/torch.linalg.eigvals#torch.linalg.eigvals", "type": "Linear Algebra", "text": ["Computes the eigenvalues of a square matrix.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, the eigenvalues of a square matrix A\u2208Kn\u00d7nA \\in \\mathbb{K}^{n \\times n} are defined as the roots (counted with multiplicity) of the polynomial p of degree n given by", "where In\\mathrm{I}_n is the n-dimensional identity matrix.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "Note", "The eigenvalues of a real matrix may be complex, as the roots of a real polynomial may be complex.", "The eigenvalues of a matrix are always well-defined, even when the matrix is not diagonalizable.", "Note", "When inputs are on a CUDA device, this function synchronizes that device with the CPU.", "See also", "torch.linalg.eig() computes the full eigenvalue decomposition.", "A (Tensor) \u2013 tensor of shape (*, n, n) where * is zero or more batch dimensions.", "out (Tensor, optional) \u2013 output tensor. Ignored if None. Default: None.", "A complex-valued tensor containing the eigenvalues even when A is real.", "Examples:"]}, {"name": "torch.linalg.eigvalsh()", "path": "generated/torch.linalg.eigvalsh#torch.linalg.eigvalsh", "type": "Linear Algebra", "text": ["Computes the eigenvalues of a complex Hermitian or real symmetric matrix.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, the eigenvalues of a complex Hermitian or real symmetric matrix A\u2208Kn\u00d7nA \\in \\mathbb{K}^{n \\times n} are defined as the roots (counted with multiplicity) of the polynomial p of degree n given by", "where In\\mathrm{I}_n is the n-dimensional identity matrix. The eigenvalues of a real symmetric or complex Hermitian matrix are always real.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "The eigenvalues are returned in ascending order.", "A is assumed to be Hermitian (resp. symmetric), but this is not checked internally, instead:", "Note", "When inputs are on a CUDA device, this function synchronizes that device with the CPU.", "See also", "torch.linalg.eigh() computes the full eigenvalue decomposition.", "out (Tensor, optional) \u2013 output tensor. Ignored if None. Default: None.", "A real-valued tensor containing the eigenvalues even when A is complex. The eigenvalues are returned in ascending order.", "Examples:"]}, {"name": "torch.linalg.householder_product()", "path": "generated/torch.linalg.householder_product#torch.linalg.householder_product", "type": "Linear Algebra", "text": ["Computes the first n columns of a product of Householder matrices.", "Let K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, and let V\u2208Km\u00d7nV \\in \\mathbb{K}^{m \\times n} be a matrix with columns vi\u2208Kmv_i \\in \\mathbb{K}^m for i=1,\u2026,mi=1,\\ldots,m with m\u2265nm \\geq n. Denote by wiw_i the vector resulting from zeroing out the first i\u22121i-1 components of viv_i and setting to 1 the ii-th. For a vector \u03c4\u2208Kk\\tau \\in \\mathbb{K}^k with k\u2264nk \\leq n, this function computes the first nn columns of the matrix", "where Im\\mathrm{I}_m is the m-dimensional identity matrix and wHw^{\\text{H}} is the conjugate transpose when ww is complex, and the transpose when ww is real-valued. The output matrix is the same size as the input matrix A.", "See Representation of Orthogonal or Unitary Matrices for further details.", "Supports inputs of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if the inputs are batches of matrices then the output has the same batch dimensions.", "See also", "torch.geqrf() can be used together with this function to form the Q from the qr() decomposition.", "torch.ormqr() is a related function that computes the matrix multiplication of a product of Householder matrices with another matrix. However, that function is not supported by autograd.", "Warning", "Gradient computations are only well-defined if taui\u22601\u2223\u2223vi\u2223\u22232tau_i \\neq \\frac{1}{||v_i||^2}. If this condition is not met, no error will be thrown, but the gradient produced may contain NaN.", "out (Tensor, optional) \u2013 output tensor. Ignored if None. Default: None.", "RuntimeError \u2013 if A doesn\u2019t satisfy the requirement m >= n, or tau doesn\u2019t satisfy the requirement n >= k.", "Examples:"]}, {"name": "torch.linalg.inv()", "path": "generated/torch.linalg.inv#torch.linalg.inv", "type": "Linear Algebra", "text": ["Computes the inverse of a square matrix if it exists. Throws a RuntimeError if the matrix is not invertible.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, for a matrix A\u2208Kn\u00d7nA \\in \\mathbb{K}^{n \\times n}, its inverse matrix A\u22121\u2208Kn\u00d7nA^{-1} \\in \\mathbb{K}^{n \\times n} (if it exists) is defined as", "where In\\mathrm{I}_n is the n-dimensional identity matrix.", "The inverse matrix exists if and only if AA is invertible. In this case, the inverse is unique.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "Note", "When inputs are on a CUDA device, this function synchronizes that device with the CPU.", "Note", "Consider using torch.linalg.solve() if possible for multiplying a matrix on the left by the inverse, as:", "It is always preferred to use solve() when possible, as it is faster and more numerically stable than computing the inverse explicitly.", "See also", "torch.linalg.pinv() computes the pseudoinverse (Moore-Penrose inverse) of matrices of any shape.", "torch.linalg.solve() computes A.inv() @ B with a numerically stable algorithm.", "A (Tensor) \u2013 tensor of shape (*, n, n) where * is zero or more batch dimensions consisting of invertible matrices.", "out (Tensor, optional) \u2013 output tensor. Ignored if None. Default: None.", "RuntimeError \u2013 if the matrix A or any matrix in the batch of matrices A is not invertible.", "Examples:"]}, {"name": "torch.linalg.inv_ex()", "path": "generated/torch.linalg.inv_ex#torch.linalg.inv_ex", "type": "Linear Algebra", "text": ["Computes the inverse of a square matrix if it is invertible.", "Returns a namedtuple (inverse, info). inverse contains the result of inverting A and info stores the LAPACK error codes.", "If A is not an invertible matrix, or if it\u2019s a batch of matrices and one or more of them is not an invertible matrix, then info stores a positive integer for the corresponding matrix. The positive integer indicates the diagonal element of the LU decomposition of the input matrix that is exactly zero. info filled with zeros indicates that the inversion was successful. If check_errors=True and info contains positive integers, then a RuntimeError is thrown.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "Note", "When the inputs are on a CUDA device, this function synchronizes only when check_errors= True.", "Warning", "This function is \u201cexperimental\u201d and it may change in a future PyTorch release.", "See also", "torch.linalg.inv() is a NumPy compatible variant that always checks for errors.", "out (tuple, optional) \u2013 tuple of two tensors to write the output to. Ignored if None. Default: None.", "Examples:"]}, {"name": "torch.linalg.ldl_factor()", "path": "generated/torch.linalg.ldl_factor#torch.linalg.ldl_factor", "type": "Linear Algebra", "text": ["Computes a compact representation of the LDL factorization of a Hermitian or symmetric (possibly indefinite) matrix.", "When A is complex valued it can be Hermitian (hermitian= True) or symmetric (hermitian= False).", "The factorization is of the form the form A=LDLTA = L D L^T. If hermitian is True then transpose operation is the conjugate transpose.", "LL (or UU) and DD are stored in compact form in LD. They follow the format specified by LAPACK\u2019s sytrf function. These tensors may be used in torch.linalg.ldl_solve() to solve linear systems.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "Note", "When inputs are on a CUDA device, this function synchronizes that device with the CPU. For a version of this function that does not synchronize, see torch.linalg.ldl_factor_ex().", "A (Tensor) \u2013 tensor of shape (*, n, n) where * is zero or more batch dimensions consisting of symmetric or Hermitian matrices.", "A named tuple (LD, pivots).", "Examples:"]}, {"name": "torch.linalg.ldl_factor_ex()", "path": "generated/torch.linalg.ldl_factor_ex#torch.linalg.ldl_factor_ex", "type": "Linear Algebra", "text": ["This is a version of ldl_factor() that does not perform error checks unless check_errors= True. It also returns the info tensor returned by LAPACK\u2019s sytrf. info stores integer error codes from the backend library. A positive integer indicates the diagonal element of DD that is zero. Division by 0 will occur if the result is used for solving a system of linear equations. info filled with zeros indicates that the factorization was successful. If check_errors=True and info contains positive integers, then a RuntimeError is thrown.", "Note", "When the inputs are on a CUDA device, this function synchronizes only when check_errors= True.", "Warning", "This function is \u201cexperimental\u201d and it may change in a future PyTorch release.", "A (Tensor) \u2013 tensor of shape (*, n, n) where * is zero or more batch dimensions consisting of symmetric or Hermitian matrices.", "A named tuple (LD, pivots, info).", "Examples:"]}, {"name": "torch.linalg.ldl_solve()", "path": "generated/torch.linalg.ldl_solve#torch.linalg.ldl_solve", "type": "Linear Algebra", "text": ["Computes the solution of a system of linear equations using the LDL factorization.", "LD and pivots are the compact representation of the LDL factorization and are expected to be computed by torch.linalg.ldl_factor_ex(). hermitian argument to this function should be the same as the corresponding arguments in torch.linalg.ldl_factor_ex().", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "Warning", "This function is \u201cexperimental\u201d and it may change in a future PyTorch release.", "Examples:"]}, {"name": "torch.linalg.lstsq()", "path": "generated/torch.linalg.lstsq#torch.linalg.lstsq", "type": "Linear Algebra", "text": ["Computes a solution to the least squares problem of a system of linear equations.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, the least squares problem for a linear system AX=BAX = B with A\u2208Km\u00d7n,B\u2208Km\u00d7kA \\in \\mathbb{K}^{m \\times n}, B \\in \\mathbb{K}^{m \\times k} is defined as", "where \u2225\u2212\u2225F\\|-\\|_F denotes the Frobenius norm.", "Supports inputs of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if the inputs are batches of matrices then the output has the same batch dimensions.", "driver chooses the backend function that will be used. For CPU inputs the valid values are \u2018gels\u2019, \u2018gelsy\u2019, \u2018gelsd, \u2018gelss\u2019. To choose the best driver on CPU consider:", "If A is well-conditioned (its condition number is not too large), or you do not mind some precision loss.", "If A is not well-conditioned.", "For CUDA input, the only valid driver is \u2018gels\u2019, which assumes that A is full-rank.", "See also the full description of these drivers", "rcond is used to determine the effective rank of the matrices in A when driver is one of (\u2018gelsy\u2019, \u2018gelsd\u2019, \u2018gelss\u2019). In this case, if \u03c3i\\sigma_i are the singular values of A in decreasing order, \u03c3i\\sigma_i will be rounded down to zero if \u03c3i\u2264rcond\u22c5\u03c31\\sigma_i \\leq \\text{rcond} \\cdot \\sigma_1. If rcond= None (default), rcond is set to the machine precision of the dtype of A times max(m, n).", "This function returns the solution to the problem and some extra information in a named tuple of four tensors (solution, residuals, rank, singular_values). For inputs A, B of shape (*, m, n), (*, m, k) respectively, it contains", "Note", "This function computes X = A.pinverse() @ B in a faster and more numerically stable way than performing the computations separately.", "Warning", "The default value of rcond may change in a future PyTorch release. It is therefore recommended to use a fixed value to avoid potential breaking changes.", "driver (str, optional) \u2013 name of the LAPACK/MAGMA method to be used. If None, \u2018gelsy\u2019 is used for CPU inputs and \u2018gels\u2019 for CUDA inputs. Default: None.", "A named tuple (solution, residuals, rank, singular_values).", "Examples:"]}, {"name": "torch.linalg.lu()", "path": "generated/torch.linalg.lu#torch.linalg.lu", "type": "Linear Algebra", "text": ["Computes the LU decomposition with partial pivoting of a matrix.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, the LU decomposition with partial pivoting of a matrix A\u2208Km\u00d7nA \\in \\mathbb{K}^{m \\times n} is defined as", "where k = min(m,n), PP is a permutation matrix, LL is lower triangular with ones on the diagonal and UU is upper triangular.", "If pivot= False and A is on GPU, then the LU decomposition without pivoting is computed", "When pivot= False, the returned matrix P will be empty. The LU decomposition without pivoting may not exist if any of the principal minors of A is singular. In this case, the output matrix may contain inf or NaN.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "See also", "torch.linalg.solve() solves a system of linear equations using the LU decomposition with partial pivoting.", "Warning", "The LU decomposition is almost never unique, as often there are different permutation matrices that can yield different LU decompositions. As such, different platforms, like SciPy, or inputs on different devices, may produce different valid decompositions.", "Warning", "Gradient computations are only supported if the input matrix is full-rank. If this condition is not met, no error will be thrown, but the gradient may not be finite. This is because the LU decomposition with pivoting is not differentiable at these points.", "out (tuple, optional) \u2013 output tuple of three tensors. Ignored if None. Default: None.", "A named tuple (P, L, U).", "Examples:"]}, {"name": "torch.linalg.lu_factor()", "path": "generated/torch.linalg.lu_factor#torch.linalg.lu_factor", "type": "Linear Algebra", "text": ["Computes a compact representation of the LU factorization with partial pivoting of a matrix.", "This function computes a compact representation of the decomposition given by torch.linalg.lu(). If the matrix is square, this representation may be used in torch.linalg.lu_solve() to solve system of linear equations that share the matrix A.", "The returned decomposition is represented as a named tuple (LU, pivots). The LU matrix has the same shape as the input matrix A. Its upper and lower triangular parts encode the non-constant elements of L and U of the LU decomposition of A.", "The returned permutation matrix is represented by a 1-indexed vector. pivots[i] == j represents that in the i-th step of the algorithm, the i-th row was permuted with the j-1-th row.", "On CUDA, one may use pivot= False. In this case, this function returns the LU decomposition without pivoting if it exists.", "Supports inputs of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if the inputs are batches of matrices then the output has the same batch dimensions.", "Note", "When inputs are on a CUDA device, this function synchronizes that device with the CPU. For a version of this function that does not synchronize, see torch.linalg.lu_factor_ex().", "Warning", "The LU decomposition is almost never unique, as often there are different permutation matrices that can yield different LU decompositions. As such, different platforms, like SciPy, or inputs on different devices, may produce different valid decompositions.", "Gradient computations are only supported if the input matrix is full-rank. If this condition is not met, no error will be thrown, but the gradient may not be finite. This is because the LU decomposition with pivoting is not differentiable at these points.", "See also", "torch.linalg.lu_solve() solves a system of linear equations given the output of this function provided the input matrix was square and invertible.", "torch.lu_unpack() unpacks the tensors returned by lu_factor() into the three matrices P, L, U that form the decomposition.", "torch.linalg.lu() computes the LU decomposition with partial pivoting of a possibly non-square matrix. It is a composition of lu_factor() and torch.lu_unpack().", "torch.linalg.solve() solves a system of linear equations. It is a composition of lu_factor() and lu_solve().", "A (Tensor) \u2013 tensor of shape (*, m, n) where * is zero or more batch dimensions.", "A named tuple (LU, pivots).", "RuntimeError \u2013 if the A matrix is not invertible or any matrix in a batched A is not invertible.", "Examples:"]}, {"name": "torch.linalg.lu_factor_ex()", "path": "generated/torch.linalg.lu_factor_ex#torch.linalg.lu_factor_ex", "type": "Linear Algebra", "text": ["This is a version of lu_factor() that does not perform error checks unless check_errors= True. It also returns the info tensor returned by LAPACK\u2019s getrf.", "Note", "When the inputs are on a CUDA device, this function synchronizes only when check_errors= True.", "Warning", "This function is \u201cexperimental\u201d and it may change in a future PyTorch release.", "A (Tensor) \u2013 tensor of shape (*, m, n) where * is zero or more batch dimensions.", "A named tuple (LU, pivots, info)."]}, {"name": "torch.linalg.lu_solve()", "path": "generated/torch.linalg.lu_solve#torch.linalg.lu_solve", "type": "Linear Algebra", "text": ["Computes the solution of a square system of linear equations with a unique solution given an LU decomposition.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, this function computes the solution X\u2208Kn\u00d7kX \\in \\mathbb{K}^{n \\times k} of the linear system associated to A\u2208Kn\u00d7n,B\u2208Kn\u00d7kA \\in \\mathbb{K}^{n \\times n}, B \\in \\mathbb{K}^{n \\times k}, which is defined as", "where AA is given factorized as returned by lu_factor().", "If left= False, this function returns the matrix X\u2208Kn\u00d7kX \\in \\mathbb{K}^{n \\times k} that solves the system", "If adjoint= True (and left= True), given an LU factorization of :math:`A this function function returns the X\u2208Kn\u00d7kX \\in \\mathbb{K}^{n \\times k} that solves the system", "where AHA^{\\text{H}} is the conjugate transpose when AA is complex, and the transpose when AA is real-valued. The left= False case is analogous.", "Supports inputs of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if the inputs are batches of matrices then the output has the same batch dimensions.", "Examples:"]}, {"name": "torch.linalg.matmul()", "path": "generated/torch.linalg.matmul#torch.linalg.matmul", "type": "Linear Algebra", "text": ["Alias for torch.matmul()"]}, {"name": "torch.linalg.matrix_exp()", "path": "generated/torch.linalg.matrix_exp#torch.linalg.matrix_exp", "type": "Linear Algebra", "text": ["Computes the matrix exponential of a square matrix.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, this function computes the matrix exponential of A\u2208Kn\u00d7nA \\in \\mathbb{K}^{n \\times n}, which is defined as", "If the matrix AA has eigenvalues \u03bbi\u2208C\\lambda_i \\in \\mathbb{C}, the matrix matrix_exp(A)\\mathrm{matrix\\_exp}(A) has eigenvalues e\u03bbi\u2208Ce^{\\lambda_i} \\in \\mathbb{C}.", "Supports input of bfloat16, float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "A (Tensor) \u2013 tensor of shape (*, n, n) where * is zero or more batch dimensions.", "Example:"]}, {"name": "torch.linalg.matrix_norm()", "path": "generated/torch.linalg.matrix_norm#torch.linalg.matrix_norm", "type": "Linear Algebra", "text": ["Computes a matrix norm.", "If A is complex valued, it computes the norm of A.abs()", "Support input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices: the norm will be computed over the dimensions specified by the 2-tuple dim and the other dimensions will be treated as batch dimensions. The output will have the same batch dimensions.", "ord defines the matrix norm that is computed. The following norms are supported:", "ord", "matrix norm", "\u2018fro\u2019 (default)", "Frobenius norm", "\u2018nuc\u2019", "nuclear norm", "inf", "max(sum(abs(x), dim=1))", "-inf", "min(sum(abs(x), dim=1))", "1", "max(sum(abs(x), dim=0))", "-1", "min(sum(abs(x), dim=0))", "2", "largest singular value", "-2", "smallest singular value", "where inf refers to float(\u2018inf\u2019), NumPy\u2019s inf object, or any equivalent object.", "A real-valued tensor, even when A is complex.", "Examples:"]}, {"name": "torch.linalg.matrix_power()", "path": "generated/torch.linalg.matrix_power#torch.linalg.matrix_power", "type": "Linear Algebra", "text": ["Computes the n-th power of a square matrix for an integer n.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "If n= 0, it returns the identity matrix (or batch) of the same shape as A. If n is negative, it returns the inverse of each matrix (if invertible) raised to the power of abs(n).", "Note", "Consider using torch.linalg.solve() if possible for multiplying a matrix on the left by a negative power as, if n> 0:", "It is always preferred to use solve() when possible, as it is faster and more numerically stable than computing A\u2212nA^{-n} explicitly.", "See also", "torch.linalg.solve() computes A.inverse() @ B with a numerically stable algorithm.", "out (Tensor, optional) \u2013 output tensor. Ignored if None. Default: None.", "RuntimeError \u2013 if n< 0 and the matrix A or any matrix in the batch of matrices A is not invertible.", "Examples:"]}, {"name": "torch.linalg.matrix_rank()", "path": "generated/torch.linalg.matrix_rank#torch.linalg.matrix_rank", "type": "Linear Algebra", "text": ["Computes the numerical rank of a matrix.", "The matrix rank is computed as the number of singular values (or eigenvalues in absolute value when hermitian= True) that are greater than max\u2061(atol,\u03c31\u2217rtol)\\max(\\text{atol}, \\sigma_1 * \\text{rtol}) threshold, where \u03c31\\sigma_1 is the largest singular value (or eigenvalue).", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "If hermitian= True, A is assumed to be Hermitian if complex or symmetric if real, but this is not checked internally. Instead, just the lower triangular part of the matrix is used in the computations.", "If rtol is not specified and A is a matrix of dimensions (m, n), the relative tolerance is set to be rtol=max\u2061(m,n)\u03b5\\text{rtol} = \\max(m, n) \\varepsilon and \u03b5\\varepsilon is the epsilon value for the dtype of A (see finfo). If rtol is not specified and atol is specified to be larger than zero then rtol is set to zero.", "If atol or rtol is a torch.Tensor, its shape must be broadcastable to that of the singular values of A as returned by torch.linalg.svdvals().", "Note", "This function has NumPy compatible variant linalg.matrix_rank(A, tol, hermitian=False). However, use of the positional argument tol is deprecated in favor of atol and rtol.", "Note", "The matrix rank is computed using a singular value decomposition torch.linalg.svdvals() if hermitian= False (default) and the eigenvalue decomposition torch.linalg.eigvalsh() when hermitian= True. When inputs are on a CUDA device, this function synchronizes that device with the CPU.", "Examples:"]}, {"name": "torch.linalg.multi_dot()", "path": "generated/torch.linalg.multi_dot#torch.linalg.multi_dot", "type": "Linear Algebra", "text": ["Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.", "Supports inputs of float, double, cfloat and cdouble dtypes. This function does not support batched inputs.", "Every tensor in tensors must be 2D, except for the first and last which may be 1D. If the first tensor is a 1D vector of shape (n,) it is treated as a row vector of shape (1, n), similarly if the last tensor is a 1D vector of shape (n,) it is treated as a column vector of shape (n, 1).", "If the first and last tensors are matrices, the output will be a matrix. However, if either is a 1D vector, then the output will be a 1D vector.", "Differences with numpy.linalg.multi_dot:", "Warning", "This function does not broadcast.", "Note", "This function is implemented by chaining torch.mm() calls after computing the optimal matrix multiplication order.", "Note", "The cost of multiplying two matrices with shapes (a, b) and (b, c) is a * b * c. Given matrices A, B, C with shapes (10, 100), (100, 5), (5, 50) respectively, we can calculate the cost of different multiplication orders as follows:", "In this case, multiplying A and B first followed by C is 10 times faster.", "tensors (Sequence[Tensor]) \u2013 two or more tensors to multiply. The first and last tensors may be 1D or 2D. Every other tensor must be 2D.", "out (Tensor, optional) \u2013 output tensor. Ignored if None. Default: None.", "Examples:"]}, {"name": "torch.linalg.norm()", "path": "generated/torch.linalg.norm#torch.linalg.norm", "type": "Linear Algebra", "text": ["Computes a vector or matrix norm.", "Supports input of float, double, cfloat and cdouble dtypes.", "Whether this function computes a vector or matrix norm is determined as follows:", "ord defines the norm that is computed. The following norms are supported:", "ord", "norm for matrices", "norm for vectors", "None (default)", "Frobenius norm", "2-norm (see below)", "\u2018fro\u2019", "Frobenius norm", "\u2013 not supported \u2013", "\u2018nuc\u2019", "nuclear norm", "\u2013 not supported \u2013", "inf", "max(sum(abs(x), dim=1))", "max(abs(x))", "-inf", "min(sum(abs(x), dim=1))", "min(abs(x))", "0", "\u2013 not supported \u2013", "sum(x != 0)", "1", "max(sum(abs(x), dim=0))", "as below", "-1", "min(sum(abs(x), dim=0))", "as below", "2", "largest singular value", "as below", "-2", "smallest singular value", "as below", "other int or float", "\u2013 not supported \u2013", "sum(abs(x)^{ord})^{(1 / ord)}", "where inf refers to float(\u2018inf\u2019), NumPy\u2019s inf object, or any equivalent object.", "See also", "torch.linalg.vector_norm() computes a vector norm.", "torch.linalg.matrix_norm() computes a matrix norm.", "The above functions are often clearer and more flexible than using torch.linalg.norm(). For example, torch.linalg.norm(A, ord=1, dim=(0, 1)) always computes a matrix norm, but with torch.linalg.vector_norm(A, ord=1, dim=(0, 1)) it is possible to compute a vector norm over the two dimensions.", "A real-valued tensor, even when A is complex.", "Examples:", "Using the dim argument to compute vector norms:", "Using the dim argument to compute matrix norms:"]}, {"name": "torch.linalg.pinv()", "path": "generated/torch.linalg.pinv#torch.linalg.pinv", "type": "Linear Algebra", "text": ["Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.", "The pseudoinverse may be defined algebraically but it is more computationally convenient to understand it through the SVD", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "If hermitian= True, A is assumed to be Hermitian if complex or symmetric if real, but this is not checked internally. Instead, just the lower triangular part of the matrix is used in the computations.", "The singular values (or the norm of the eigenvalues when hermitian= True) that are below max\u2061(atol,\u03c31\u22c5rtol)\\max(\\text{atol}, \\sigma_1 \\cdot \\text{rtol}) threshold are treated as zero and discarded in the computation, where \u03c31\\sigma_1 is the largest singular value (or eigenvalue).", "If rtol is not specified and A is a matrix of dimensions (m, n), the relative tolerance is set to be rtol=max\u2061(m,n)\u03b5\\text{rtol} = \\max(m, n) \\varepsilon and \u03b5\\varepsilon is the epsilon value for the dtype of A (see finfo). If rtol is not specified and atol is specified to be larger than zero then rtol is set to zero.", "If atol or rtol is a torch.Tensor, its shape must be broadcastable to that of the singular values of A as returned by torch.linalg.svd().", "Note", "This function uses torch.linalg.svd() if hermitian= False and torch.linalg.eigh() if hermitian= True. For CUDA inputs, this function synchronizes that device with the CPU.", "Note", "Consider using torch.linalg.lstsq() if possible for multiplying a matrix on the left by the pseudoinverse, as:", "It is always preferred to use lstsq() when possible, as it is faster and more numerically stable than computing the pseudoinverse explicitly.", "Note", "This function has NumPy compatible variant linalg.pinv(A, rcond, hermitian=False). However, use of the positional argument rcond is deprecated in favor of rtol.", "Warning", "This function uses internally torch.linalg.svd() (or torch.linalg.eigh() when hermitian= True), so its derivative has the same problems as those of these functions. See the warnings in torch.linalg.svd() and torch.linalg.eigh() for more details.", "See also", "torch.linalg.inv() computes the inverse of a square matrix.", "torch.linalg.lstsq() computes A.pinv() @ B with a numerically stable algorithm.", "Examples:"]}, {"name": "torch.linalg.qr()", "path": "generated/torch.linalg.qr#torch.linalg.qr", "type": "Linear Algebra", "text": ["Computes the QR decomposition of a matrix.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, the full QR decomposition of a matrix A\u2208Km\u00d7nA \\in \\mathbb{K}^{m \\times n} is defined as", "where QQ is orthogonal in the real case and unitary in the complex case, and RR is upper triangular with real diagonal (even in the complex case).", "When m > n (tall matrix), as R is upper triangular, its last m - n rows are zero. In this case, we can drop the last m - n columns of Q to form the reduced QR decomposition:", "The reduced QR decomposition agrees with the full QR decomposition when n >= m (wide matrix).", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "The parameter mode chooses between the full and reduced QR decomposition. If A has shape (*, m, n), denoting k = min(m, n)", "Differences with numpy.linalg.qr:", "Warning", "The elements in the diagonal of R are not necessarily positive. As such, the returned QR decomposition is only unique up to the sign of the diagonal of R. Therefore, different platforms, like NumPy, or inputs on different devices, may produce different valid decompositions.", "Warning", "The QR decomposition is only well-defined if the first k = min(m, n) columns of every matrix in A are linearly independent. If this condition is not met, no error will be thrown, but the QR produced may be incorrect and its autodiff may fail or produce incorrect results.", "out (tuple, optional) \u2013 output tuple of two tensors. Ignored if None. Default: None.", "A named tuple (Q, R).", "Examples:"]}, {"name": "torch.linalg.slogdet()", "path": "generated/torch.linalg.slogdet#torch.linalg.slogdet", "type": "Linear Algebra", "text": ["Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.", "For complex A, it returns the sign and the natural logarithm of the modulus of the determinant, that is, a logarithmic polar decomposition of the determinant.", "The determinant can be recovered as sign * exp(logabsdet). When a matrix has a determinant of zero, it returns (0, -inf).", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "See also", "torch.linalg.det() computes the determinant of square matrices.", "A (Tensor) \u2013 tensor of shape (*, n, n) where * is zero or more batch dimensions.", "out (tuple, optional) \u2013 output tuple of two tensors. Ignored if None. Default: None.", "A named tuple (sign, logabsdet).", "sign will have the same dtype as A.", "logabsdet will always be real-valued, even when A is complex.", "Examples:"]}, {"name": "torch.linalg.solve()", "path": "generated/torch.linalg.solve#torch.linalg.solve", "type": "Linear Algebra", "text": ["Computes the solution of a square system of linear equations with a unique solution.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, this function computes the solution X\u2208Kn\u00d7kX \\in \\mathbb{K}^{n \\times k} of the linear system associated to A\u2208Kn\u00d7n,B\u2208Kn\u00d7kA \\in \\mathbb{K}^{n \\times n}, B \\in \\mathbb{K}^{n \\times k}, which is defined as", "If left= False, this function returns the matrix X\u2208Kn\u00d7kX \\in \\mathbb{K}^{n \\times k} that solves the system", "This system of linear equations has one solution if and only if AA is invertible. This function assumes that AA is invertible.", "Supports inputs of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if the inputs are batches of matrices then the output has the same batch dimensions.", "Letting * be zero or more batch dimensions,", "Note", "This function computes X = A.inverse() @ B in a faster and more numerically stable way than performing the computations separately.", "Note", "It is possible to compute the solution of the system XA=BXA = B by passing the inputs A and B transposed and transposing the output returned by this function.", "Note", "When inputs are on a CUDA device, this function synchronizes that device with the CPU.", "See also", "torch.linalg.solve_triangular() computes the solution of a triangular system of linear equations with a unique solution.", "RuntimeError \u2013 if the A matrix is not invertible or any matrix in a batched A is not invertible.", "Examples:"]}, {"name": "torch.linalg.solve_ex()", "path": "generated/torch.linalg.solve_ex#torch.linalg.solve_ex", "type": "Linear Algebra", "text": ["A version of solve() that does not perform error checks unless check_errors= True. It also returns the info tensor returned by LAPACK\u2019s getrf.", "Note", "When the inputs are on a CUDA device, this function synchronizes only when check_errors= True.", "Warning", "This function is \u201cexperimental\u201d and it may change in a future PyTorch release.", "A (Tensor) \u2013 tensor of shape (*, n, n) where * is zero or more batch dimensions.", "A named tuple (result, info).", "Examples:"]}, {"name": "torch.linalg.solve_triangular()", "path": "generated/torch.linalg.solve_triangular#torch.linalg.solve_triangular", "type": "Linear Algebra", "text": ["Computes the solution of a triangular system of linear equations with a unique solution.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, this function computes the solution X\u2208Kn\u00d7kX \\in \\mathbb{K}^{n \\times k} of the linear system associated to the triangular matrix A\u2208Kn\u00d7nA \\in \\mathbb{K}^{n \\times n} without zeros on the diagonal (that is, it is invertible) and the rectangular matrix , B\u2208Kn\u00d7kB \\in \\mathbb{K}^{n \\times k}, which is defined as", "The argument upper signals whether AA is upper or lower triangular.", "If left= False, this function returns the matrix X\u2208Kn\u00d7kX \\in \\mathbb{K}^{n \\times k} that solves the system", "If upper= True (resp. False) just the upper (resp. lower) triangular half of A will be accessed. The elements below the main diagonal will be considered to be zero and will not be accessed.", "If unitriangular= True, the diagonal of A is assumed to be ones and will not be accessed.", "The result may contain NaN s if the diagonal of A contains zeros or elements that are very close to zero and unitriangular= False (default) or if the input matrix has very small eigenvalues.", "Supports inputs of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if the inputs are batches of matrices then the output has the same batch dimensions.", "See also", "torch.linalg.solve() computes the solution of a general square system of linear equations with a unique solution.", "Examples:"]}, {"name": "torch.linalg.svd()", "path": "generated/torch.linalg.svd#torch.linalg.svd", "type": "Linear Algebra", "text": ["Computes the singular value decomposition (SVD) of a matrix.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, the full SVD of a matrix A\u2208Km\u00d7nA \\in \\mathbb{K}^{m \\times n}, if k = min(m,n), is defined as", "where diag\u2061(S)\u2208Km\u00d7n\\operatorname{diag}(S) \\in \\mathbb{K}^{m \\times n}, VHV^{\\text{H}} is the conjugate transpose when VV is complex, and the transpose when VV is real-valued. The matrices UU, VV (and thus VHV^{\\text{H}}) are orthogonal in the real case, and unitary in the complex case.", "When m > n (resp. m < n) we can drop the last m - n (resp. n - m) columns of U (resp. V) to form the reduced SVD:", "where diag\u2061(S)\u2208Kk\u00d7k\\operatorname{diag}(S) \\in \\mathbb{K}^{k \\times k}. In this case, UU and VV also have orthonormal columns.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "The returned decomposition is a named tuple (U, S, Vh) which corresponds to UU, SS, VHV^{\\text{H}} above.", "The singular values are returned in descending order.", "The parameter full_matrices chooses between the full (default) and reduced SVD.", "The driver kwarg may be used in CUDA with a cuSOLVER backend to choose the algorithm used to compute the SVD. The choice of a driver is a trade-off between accuracy and speed.", "If A is well-conditioned (its condition number is not too large), or you do not mind some precision loss.", "By default (driver= None), we call \u2018gesvdj\u2019 and, if it fails, we fallback to \u2018gesvd\u2019.", "Differences with numpy.linalg.svd:", "Note", "When full_matrices= True, the gradients with respect to U[\u2026, :, min(m, n):] and Vh[\u2026, min(m, n):, :] will be ignored, as those vectors can be arbitrary bases of the corresponding subspaces.", "Warning", "The returned tensors U and V are not unique, nor are they continuous with respect to A. Due to this lack of uniqueness, different hardware and software may compute different singular vectors.", "This non-uniqueness is caused by the fact that multiplying any pair of singular vectors uk,vku_k, v_k by -1 in the real case or by ei\u03d5,\u03d5\u2208Re^{i \\phi}, \\phi \\in \\mathbb{R} in the complex case produces another two valid singular vectors of the matrix. For this reason, the loss function shall not depend on this ei\u03d5e^{i \\phi} quantity, as it is not well-defined. This is checked for complex inputs when computing the gradients of this function. As such, when inputs are complex and are on a CUDA device, the computation of the gradients of this function synchronizes that device with the CPU.", "Warning", "Gradients computed using U or Vh will only be finite when A does not have repeated singular values. If A is rectangular, additionally, zero must also not be one of its singular values. Furthermore, if the distance between any two singular values is close to zero, the gradient will be numerically unstable, as it depends on the singular values \u03c3i\\sigma_i through the computation of 1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}. In the rectangular case, the gradient will also be numerically unstable when A has small singular values, as it also depends on the computation of 1\u03c3i\\frac{1}{\\sigma_i}.", "See also", "torch.linalg.svdvals() computes only the singular values. Unlike torch.linalg.svd(), the gradients of svdvals() are always numerically stable.", "torch.linalg.eig() for a function that computes another type of spectral decomposition of a matrix. The eigendecomposition works just on square matrices.", "torch.linalg.eigh() for a (faster) function that computes the eigenvalue decomposition for Hermitian and symmetric matrices.", "torch.linalg.qr() for another (much faster) decomposition that works on general matrices.", "A named tuple (U, S, Vh) which corresponds to UU, SS, VHV^{\\text{H}} above.", "S will always be real-valued, even when A is complex. It will also be ordered in descending order.", "U and Vh will have the same dtype as A. The left / right singular vectors will be given by the columns of U and the rows of Vh respectively.", "Examples:"]}, {"name": "torch.linalg.svdvals()", "path": "generated/torch.linalg.svdvals#torch.linalg.svdvals", "type": "Linear Algebra", "text": ["Computes the singular values of a matrix.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "The singular values are returned in descending order.", "Note", "This function is equivalent to NumPy\u2019s linalg.svd(A, compute_uv=False).", "Note", "When inputs are on a CUDA device, this function synchronizes that device with the CPU.", "See also", "torch.linalg.svd() computes the full singular value decomposition.", "A (Tensor) \u2013 tensor of shape (*, m, n) where * is zero or more batch dimensions.", "A real-valued tensor, even when A is complex.", "Examples:"]}, {"name": "torch.linalg.tensorinv()", "path": "generated/torch.linalg.tensorinv#torch.linalg.tensorinv", "type": "Linear Algebra", "text": ["Computes the multiplicative inverse of torch.tensordot().", "If m is the product of the first ind dimensions of A and n is the product of the rest of the dimensions, this function expects m and n to be equal. If this is the case, it computes a tensor X such that tensordot(A, X, ind) is the identity matrix in dimension m. X will have the shape of A but with the first ind dimensions pushed back to the end", "Supports input of float, double, cfloat and cdouble dtypes.", "Note", "When A is a 2-dimensional tensor and ind= 1, this function computes the (multiplicative) inverse of A (see torch.linalg.inv()).", "Note", "Consider using torch.linalg.tensorsolve() if possible for multiplying a tensor on the left by the tensor inverse, as:", "It is always preferred to use tensorsolve() when possible, as it is faster and more numerically stable than computing the pseudoinverse explicitly.", "See also", "torch.linalg.tensorsolve() computes torch.tensordot(tensorinv(A), B).", "out (Tensor, optional) \u2013 output tensor. Ignored if None. Default: None.", "RuntimeError \u2013 if the reshaped A is not invertible or the product of the first ind dimensions is not equal to the product of the rest.", "Examples:"]}, {"name": "torch.linalg.tensorsolve()", "path": "generated/torch.linalg.tensorsolve#torch.linalg.tensorsolve", "type": "Linear Algebra", "text": ["Computes the solution X to the system torch.tensordot(A, X) = B.", "If m is the product of the first B.ndim dimensions of A and n is the product of the rest of the dimensions, this function expects m and n to be equal.", "The returned tensor x satisfies tensordot(A, x, dims=x.ndim) == B. x has shape A[B.ndim:].", "If dims is specified, A will be reshaped as", "Supports inputs of float, double, cfloat and cdouble dtypes.", "See also", "torch.linalg.tensorinv() computes the multiplicative inverse of torch.tensordot().", "out (Tensor, optional) \u2013 output tensor. Ignored if None. Default: None.", "RuntimeError \u2013 if the reshaped A.view(m, m) with m as above is not invertible or the product of the first ind dimensions is not equal to the product of the rest of the dimensions.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.cholesky", "path": "generated/torch.linalg.cholesky", "type": "Linear Algebra", "text": ["Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix A\u2208Kn\u00d7nA \\in \\mathbb{K}^{n \\times n} is defined as", "where LL is a lower triangular matrix with real positive diagonal (even in the complex case) and LHL^{\\text{H}} is the conjugate transpose when LL is complex, and the transpose when LL is real-valued.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "Note", "When inputs are on a CUDA device, this function synchronizes that device with the CPU.", "See also", "torch.linalg.cholesky_ex() for a version of this operation that skips the (slow) error checking by default and instead returns the debug information. This makes it a faster way to check if a matrix is positive-definite.", "torch.linalg.eigh() for a different decomposition of a Hermitian matrix. The eigenvalue decomposition gives more information about the matrix but it slower to compute than the Cholesky decomposition.", "A (Tensor) \u2013 tensor of shape (*, n, n) where * is zero or more batch dimensions consisting of symmetric or Hermitian positive-definite matrices.", "RuntimeError \u2013 if the A matrix or any matrix in a batched A is not Hermitian (resp. symmetric) positive-definite. If A is a batch of matrices, the error message will include the batch index of the first matrix that fails to meet this condition.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.cholesky_ex", "path": "generated/torch.linalg.cholesky_ex", "type": "Linear Algebra", "text": ["Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.", "This function skips the (slow) error checking and error message construction of torch.linalg.cholesky(), instead directly returning the LAPACK error codes as part of a named tuple (L, info). This makes this function a faster way to check if a matrix is positive-definite, and it provides an opportunity to handle decomposition errors more gracefully or performantly than torch.linalg.cholesky() does.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "If A is not a Hermitian positive-definite matrix, or if it\u2019s a batch of matrices and one or more of them is not a Hermitian positive-definite matrix, then info stores a positive integer for the corresponding matrix. The positive integer indicates the order of the leading minor that is not positive-definite, and the decomposition could not be completed. info filled with zeros indicates that the decomposition was successful. If check_errors=True and info contains positive integers, then a RuntimeError is thrown.", "Note", "When the inputs are on a CUDA device, this function synchronizes only when check_errors= True.", "Warning", "This function is \u201cexperimental\u201d and it may change in a future PyTorch release.", "See also", "torch.linalg.cholesky() is a NumPy compatible variant that always checks for errors.", "A (Tensor) \u2013 the Hermitian n times n matrix or the batch of such matrices of size (*, n, n) where * is one or more batch dimensions.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.cond", "path": "generated/torch.linalg.cond", "type": "Linear Algebra", "text": ["Computes the condition number of a matrix with respect to a matrix norm.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, the condition number \u03ba\\kappa of a matrix A\u2208Kn\u00d7nA \\in \\mathbb{K}^{n \\times n} is defined as", "The condition number of A measures the numerical stability of the linear system AX = B with respect to a matrix norm.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "p defines the matrix norm that is computed. The following norms are supported:", "p", "matrix norm", "None", "2-norm (largest singular value)", "\u2018fro\u2019", "Frobenius norm", "\u2018nuc\u2019", "nuclear norm", "inf", "max(sum(abs(x), dim=1))", "-inf", "min(sum(abs(x), dim=1))", "1", "max(sum(abs(x), dim=0))", "-1", "min(sum(abs(x), dim=0))", "2", "largest singular value", "-2", "smallest singular value", "where inf refers to float(\u2018inf\u2019), NumPy\u2019s inf object, or any equivalent object.", "For p is one of (\u2018fro\u2019, \u2018nuc\u2019, inf, -inf, 1, -1), this function uses torch.linalg.norm() and torch.linalg.inv(). As such, in this case, the matrix (or every matrix in the batch) A has to be square and invertible.", "For p in (2, -2), this function can be computed in terms of the singular values \u03c31\u2265\u2026\u2265\u03c3n\\sigma_1 \\geq \\ldots \\geq \\sigma_n", "In these cases, it is computed using torch.linalg.svdvals(). For these norms, the matrix (or every matrix in the batch) A may have any shape.", "Note", "When inputs are on a CUDA device, this function synchronizes that device with the CPU if p is one of (\u2018fro\u2019, \u2018nuc\u2019, inf, -inf, 1, -1).", "See also", "torch.linalg.solve() for a function that solves linear systems of square matrices.", "torch.linalg.lstsq() for a function that solves linear systems of general matrices.", "out (Tensor, optional) \u2013 output tensor. Ignored if None. Default: None.", "A real-valued tensor, even when A is complex.", "RuntimeError \u2013 if p is one of (\u2018fro\u2019, \u2018nuc\u2019, inf, -inf, 1, -1) and the A matrix or any matrix in the batch A is not square or invertible.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.cross", "path": "generated/torch.linalg.cross", "type": "Linear Algebra", "text": ["Computes the cross product of two 3-dimensional vectors.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of vectors, for which it computes the product along the dimension dim. It broadcasts over the batch dimensions.", "out (Tensor, optional) \u2013 the output tensor. Ignored if None. Default: None."]}, {"name": "torch.linalg.torch.linalg.det", "path": "generated/torch.linalg.det", "type": "Linear Algebra", "text": ["Computes the determinant of a square matrix.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "See also", "torch.linalg.slogdet() computes the sign and natural logarithm of the absolute value of the determinant of square matrices.", "A (Tensor) \u2013 tensor of shape (*, n, n) where * is zero or more batch dimensions.", "out (Tensor, optional) \u2013 output tensor. Ignored if None. Default: None.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.diagonal", "path": "generated/torch.linalg.diagonal", "type": "Linear Algebra", "text": ["Alias for torch.diagonal() with defaults dim1= -2, dim2= -1."]}, {"name": "torch.linalg.torch.linalg.eig", "path": "generated/torch.linalg.eig", "type": "Linear Algebra", "text": ["Computes the eigenvalue decomposition of a square matrix if it exists.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, the eigenvalue decomposition of a square matrix A\u2208Kn\u00d7nA \\in \\mathbb{K}^{n \\times n} (if it exists) is defined as", "This decomposition exists if and only if AA is diagonalizable. This is the case when all its eigenvalues are different.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "Note", "The eigenvalues and eigenvectors of a real matrix may be complex.", "Note", "When inputs are on a CUDA device, this function synchronizes that device with the CPU.", "Warning", "This function assumes that A is diagonalizable (for example, when all the eigenvalues are different). If it is not diagonalizable, the returned eigenvalues will be correct but A\u2260Vdiag\u2061(\u039b)V\u22121A \\neq V \\operatorname{diag}(\\Lambda)V^{-1}.", "Warning", "The returned eigenvectors are normalized to have norm 1. Even then, the eigenvectors of a matrix are not unique, nor are they continuous with respect to A. Due to this lack of uniqueness, different hardware and software may compute different eigenvectors.", "This non-uniqueness is caused by the fact that multiplying an eigenvector by by ei\u03d5,\u03d5\u2208Re^{i \\phi}, \\phi \\in \\mathbb{R} produces another set of valid eigenvectors of the matrix. For this reason, the loss function shall not depend on the phase of the eigenvectors, as this quantity is not well-defined. This is checked when computing the gradients of this function. As such, when inputs are on a CUDA device, this function synchronizes that device with the CPU when computing the gradients. This is checked when computing the gradients of this function. As such, when inputs are on a CUDA device, the computation of the gradients of this function synchronizes that device with the CPU.", "Warning", "Gradients computed using the eigenvectors tensor will only be finite when A has distinct eigenvalues. Furthermore, if the distance between any two eigenvalues is close to zero, the gradient will be numerically unstable, as it depends on the eigenvalues \u03bbi\\lambda_i through the computation of 1min\u2061i\u2260j\u03bbi\u2212\u03bbj\\frac{1}{\\min_{i \\neq j} \\lambda_i - \\lambda_j}.", "See also", "torch.linalg.eigvals() computes only the eigenvalues. Unlike torch.linalg.eig(), the gradients of eigvals() are always numerically stable.", "torch.linalg.eigh() for a (faster) function that computes the eigenvalue decomposition for Hermitian and symmetric matrices.", "torch.linalg.svd() for a function that computes another type of spectral decomposition that works on matrices of any shape.", "torch.linalg.qr() for another (much faster) decomposition that works on matrices of any shape.", "A (Tensor) \u2013 tensor of shape (*, n, n) where * is zero or more batch dimensions consisting of diagonalizable matrices.", "out (tuple, optional) \u2013 output tuple of two tensors. Ignored if None. Default: None.", "A named tuple (eigenvalues, eigenvectors) which corresponds to \u039b\\Lambda and VV above.", "eigenvalues and eigenvectors will always be complex-valued, even when A is real. The eigenvectors will be given by the columns of eigenvectors.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.eigh", "path": "generated/torch.linalg.eigh", "type": "Linear Algebra", "text": ["Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, the eigenvalue decomposition of a complex Hermitian or real symmetric matrix A\u2208Kn\u00d7nA \\in \\mathbb{K}^{n \\times n} is defined as", "where QHQ^{\\text{H}} is the conjugate transpose when QQ is complex, and the transpose when QQ is real-valued. QQ is orthogonal in the real case and unitary in the complex case.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "A is assumed to be Hermitian (resp. symmetric), but this is not checked internally, instead:", "The eigenvalues are returned in ascending order.", "Note", "When inputs are on a CUDA device, this function synchronizes that device with the CPU.", "Note", "The eigenvalues of real symmetric or complex Hermitian matrices are always real.", "Warning", "The eigenvectors of a symmetric matrix are not unique, nor are they continuous with respect to A. Due to this lack of uniqueness, different hardware and software may compute different eigenvectors.", "This non-uniqueness is caused by the fact that multiplying an eigenvector by -1 in the real case or by ei\u03d5,\u03d5\u2208Re^{i \\phi}, \\phi \\in \\mathbb{R} in the complex case produces another set of valid eigenvectors of the matrix. For this reason, the loss function shall not depend on the phase of the eigenvectors, as this quantity is not well-defined. This is checked for complex inputs when computing the gradients of this function. As such, when inputs are complex and are on a CUDA device, the computation of the gradients of this function synchronizes that device with the CPU.", "Warning", "Gradients computed using the eigenvectors tensor will only be finite when A has distinct eigenvalues. Furthermore, if the distance between any two eigenvalues is close to zero, the gradient will be numerically unstable, as it depends on the eigenvalues \u03bbi\\lambda_i through the computation of 1min\u2061i\u2260j\u03bbi\u2212\u03bbj\\frac{1}{\\min_{i \\neq j} \\lambda_i - \\lambda_j}.", "Warning", "User may see pytorch crashes if running eigh on CUDA devices with CUDA versions before 12.1 update 1 with large ill-conditioned matrices as inputs. Refer to Linear Algebra Numerical Stability for more details. If this is the case, user may (1) tune their matrix inputs to be less ill-conditioned, or (2) use torch.backends.cuda.preferred_linalg_library() to try other supported backends.", "See also", "torch.linalg.eigvalsh() computes only the eigenvalues of a Hermitian matrix. Unlike torch.linalg.eigh(), the gradients of eigvalsh() are always numerically stable.", "torch.linalg.cholesky() for a different decomposition of a Hermitian matrix. The Cholesky decomposition gives less information about the matrix but is much faster to compute than the eigenvalue decomposition.", "torch.linalg.eig() for a (slower) function that computes the eigenvalue decomposition of a not necessarily Hermitian square matrix.", "torch.linalg.svd() for a (slower) function that computes the more general SVD decomposition of matrices of any shape.", "torch.linalg.qr() for another (much faster) decomposition that works on general matrices.", "out (tuple, optional) \u2013 output tuple of two tensors. Ignored if None. Default: None.", "A named tuple (eigenvalues, eigenvectors) which corresponds to \u039b\\Lambda and QQ above.", "eigenvalues will always be real-valued, even when A is complex. It will also be ordered in ascending order.", "eigenvectors will have the same dtype as A and will contain the eigenvectors as its columns."]}, {"name": "torch.linalg.torch.linalg.eigvals", "path": "generated/torch.linalg.eigvals", "type": "Linear Algebra", "text": ["Computes the eigenvalues of a square matrix.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, the eigenvalues of a square matrix A\u2208Kn\u00d7nA \\in \\mathbb{K}^{n \\times n} are defined as the roots (counted with multiplicity) of the polynomial p of degree n given by", "where In\\mathrm{I}_n is the n-dimensional identity matrix.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "Note", "The eigenvalues of a real matrix may be complex, as the roots of a real polynomial may be complex.", "The eigenvalues of a matrix are always well-defined, even when the matrix is not diagonalizable.", "Note", "When inputs are on a CUDA device, this function synchronizes that device with the CPU.", "See also", "torch.linalg.eig() computes the full eigenvalue decomposition.", "A (Tensor) \u2013 tensor of shape (*, n, n) where * is zero or more batch dimensions.", "out (Tensor, optional) \u2013 output tensor. Ignored if None. Default: None.", "A complex-valued tensor containing the eigenvalues even when A is real.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.eigvalsh", "path": "generated/torch.linalg.eigvalsh", "type": "Linear Algebra", "text": ["Computes the eigenvalues of a complex Hermitian or real symmetric matrix.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, the eigenvalues of a complex Hermitian or real symmetric matrix A\u2208Kn\u00d7nA \\in \\mathbb{K}^{n \\times n} are defined as the roots (counted with multiplicity) of the polynomial p of degree n given by", "where In\\mathrm{I}_n is the n-dimensional identity matrix. The eigenvalues of a real symmetric or complex Hermitian matrix are always real.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "The eigenvalues are returned in ascending order.", "A is assumed to be Hermitian (resp. symmetric), but this is not checked internally, instead:", "Note", "When inputs are on a CUDA device, this function synchronizes that device with the CPU.", "See also", "torch.linalg.eigh() computes the full eigenvalue decomposition.", "out (Tensor, optional) \u2013 output tensor. Ignored if None. Default: None.", "A real-valued tensor containing the eigenvalues even when A is complex. The eigenvalues are returned in ascending order.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.householder_product", "path": "generated/torch.linalg.householder_product", "type": "Linear Algebra", "text": ["Computes the first n columns of a product of Householder matrices.", "Let K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, and let V\u2208Km\u00d7nV \\in \\mathbb{K}^{m \\times n} be a matrix with columns vi\u2208Kmv_i \\in \\mathbb{K}^m for i=1,\u2026,mi=1,\\ldots,m with m\u2265nm \\geq n. Denote by wiw_i the vector resulting from zeroing out the first i\u22121i-1 components of viv_i and setting to 1 the ii-th. For a vector \u03c4\u2208Kk\\tau \\in \\mathbb{K}^k with k\u2264nk \\leq n, this function computes the first nn columns of the matrix", "where Im\\mathrm{I}_m is the m-dimensional identity matrix and wHw^{\\text{H}} is the conjugate transpose when ww is complex, and the transpose when ww is real-valued. The output matrix is the same size as the input matrix A.", "See Representation of Orthogonal or Unitary Matrices for further details.", "Supports inputs of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if the inputs are batches of matrices then the output has the same batch dimensions.", "See also", "torch.geqrf() can be used together with this function to form the Q from the qr() decomposition.", "torch.ormqr() is a related function that computes the matrix multiplication of a product of Householder matrices with another matrix. However, that function is not supported by autograd.", "Warning", "Gradient computations are only well-defined if taui\u22601\u2223\u2223vi\u2223\u22232tau_i \\neq \\frac{1}{||v_i||^2}. If this condition is not met, no error will be thrown, but the gradient produced may contain NaN.", "out (Tensor, optional) \u2013 output tensor. Ignored if None. Default: None.", "RuntimeError \u2013 if A doesn\u2019t satisfy the requirement m >= n, or tau doesn\u2019t satisfy the requirement n >= k.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.inv", "path": "generated/torch.linalg.inv", "type": "Linear Algebra", "text": ["Computes the inverse of a square matrix if it exists. Throws a RuntimeError if the matrix is not invertible.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, for a matrix A\u2208Kn\u00d7nA \\in \\mathbb{K}^{n \\times n}, its inverse matrix A\u22121\u2208Kn\u00d7nA^{-1} \\in \\mathbb{K}^{n \\times n} (if it exists) is defined as", "where In\\mathrm{I}_n is the n-dimensional identity matrix.", "The inverse matrix exists if and only if AA is invertible. In this case, the inverse is unique.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "Note", "When inputs are on a CUDA device, this function synchronizes that device with the CPU.", "Note", "Consider using torch.linalg.solve() if possible for multiplying a matrix on the left by the inverse, as:", "It is always preferred to use solve() when possible, as it is faster and more numerically stable than computing the inverse explicitly.", "See also", "torch.linalg.pinv() computes the pseudoinverse (Moore-Penrose inverse) of matrices of any shape.", "torch.linalg.solve() computes A.inv() @ B with a numerically stable algorithm.", "A (Tensor) \u2013 tensor of shape (*, n, n) where * is zero or more batch dimensions consisting of invertible matrices.", "out (Tensor, optional) \u2013 output tensor. Ignored if None. Default: None.", "RuntimeError \u2013 if the matrix A or any matrix in the batch of matrices A is not invertible.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.inv_ex", "path": "generated/torch.linalg.inv_ex", "type": "Linear Algebra", "text": ["Computes the inverse of a square matrix if it is invertible.", "Returns a namedtuple (inverse, info). inverse contains the result of inverting A and info stores the LAPACK error codes.", "If A is not an invertible matrix, or if it\u2019s a batch of matrices and one or more of them is not an invertible matrix, then info stores a positive integer for the corresponding matrix. The positive integer indicates the diagonal element of the LU decomposition of the input matrix that is exactly zero. info filled with zeros indicates that the inversion was successful. If check_errors=True and info contains positive integers, then a RuntimeError is thrown.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "Note", "When the inputs are on a CUDA device, this function synchronizes only when check_errors= True.", "Warning", "This function is \u201cexperimental\u201d and it may change in a future PyTorch release.", "See also", "torch.linalg.inv() is a NumPy compatible variant that always checks for errors.", "out (tuple, optional) \u2013 tuple of two tensors to write the output to. Ignored if None. Default: None.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.ldl_factor", "path": "generated/torch.linalg.ldl_factor", "type": "Linear Algebra", "text": ["Computes a compact representation of the LDL factorization of a Hermitian or symmetric (possibly indefinite) matrix.", "When A is complex valued it can be Hermitian (hermitian= True) or symmetric (hermitian= False).", "The factorization is of the form the form A=LDLTA = L D L^T. If hermitian is True then transpose operation is the conjugate transpose.", "LL (or UU) and DD are stored in compact form in LD. They follow the format specified by LAPACK\u2019s sytrf function. These tensors may be used in torch.linalg.ldl_solve() to solve linear systems.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "Note", "When inputs are on a CUDA device, this function synchronizes that device with the CPU. For a version of this function that does not synchronize, see torch.linalg.ldl_factor_ex().", "A (Tensor) \u2013 tensor of shape (*, n, n) where * is zero or more batch dimensions consisting of symmetric or Hermitian matrices.", "A named tuple (LD, pivots).", "Examples:"]}, {"name": "torch.linalg.torch.linalg.ldl_factor_ex", "path": "generated/torch.linalg.ldl_factor_ex", "type": "Linear Algebra", "text": ["This is a version of ldl_factor() that does not perform error checks unless check_errors= True. It also returns the info tensor returned by LAPACK\u2019s sytrf. info stores integer error codes from the backend library. A positive integer indicates the diagonal element of DD that is zero. Division by 0 will occur if the result is used for solving a system of linear equations. info filled with zeros indicates that the factorization was successful. If check_errors=True and info contains positive integers, then a RuntimeError is thrown.", "Note", "When the inputs are on a CUDA device, this function synchronizes only when check_errors= True.", "Warning", "This function is \u201cexperimental\u201d and it may change in a future PyTorch release.", "A (Tensor) \u2013 tensor of shape (*, n, n) where * is zero or more batch dimensions consisting of symmetric or Hermitian matrices.", "A named tuple (LD, pivots, info).", "Examples:"]}, {"name": "torch.linalg.torch.linalg.ldl_solve", "path": "generated/torch.linalg.ldl_solve", "type": "Linear Algebra", "text": ["Computes the solution of a system of linear equations using the LDL factorization.", "LD and pivots are the compact representation of the LDL factorization and are expected to be computed by torch.linalg.ldl_factor_ex(). hermitian argument to this function should be the same as the corresponding arguments in torch.linalg.ldl_factor_ex().", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "Warning", "This function is \u201cexperimental\u201d and it may change in a future PyTorch release.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.lstsq", "path": "generated/torch.linalg.lstsq", "type": "Linear Algebra", "text": ["Computes a solution to the least squares problem of a system of linear equations.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, the least squares problem for a linear system AX=BAX = B with A\u2208Km\u00d7n,B\u2208Km\u00d7kA \\in \\mathbb{K}^{m \\times n}, B \\in \\mathbb{K}^{m \\times k} is defined as", "where \u2225\u2212\u2225F\\|-\\|_F denotes the Frobenius norm.", "Supports inputs of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if the inputs are batches of matrices then the output has the same batch dimensions.", "driver chooses the backend function that will be used. For CPU inputs the valid values are \u2018gels\u2019, \u2018gelsy\u2019, \u2018gelsd, \u2018gelss\u2019. To choose the best driver on CPU consider:", "If A is well-conditioned (its condition number is not too large), or you do not mind some precision loss.", "If A is not well-conditioned.", "For CUDA input, the only valid driver is \u2018gels\u2019, which assumes that A is full-rank.", "See also the full description of these drivers", "rcond is used to determine the effective rank of the matrices in A when driver is one of (\u2018gelsy\u2019, \u2018gelsd\u2019, \u2018gelss\u2019). In this case, if \u03c3i\\sigma_i are the singular values of A in decreasing order, \u03c3i\\sigma_i will be rounded down to zero if \u03c3i\u2264rcond\u22c5\u03c31\\sigma_i \\leq \\text{rcond} \\cdot \\sigma_1. If rcond= None (default), rcond is set to the machine precision of the dtype of A times max(m, n).", "This function returns the solution to the problem and some extra information in a named tuple of four tensors (solution, residuals, rank, singular_values). For inputs A, B of shape (*, m, n), (*, m, k) respectively, it contains", "Note", "This function computes X = A.pinverse() @ B in a faster and more numerically stable way than performing the computations separately.", "Warning", "The default value of rcond may change in a future PyTorch release. It is therefore recommended to use a fixed value to avoid potential breaking changes.", "driver (str, optional) \u2013 name of the LAPACK/MAGMA method to be used. If None, \u2018gelsy\u2019 is used for CPU inputs and \u2018gels\u2019 for CUDA inputs. Default: None.", "A named tuple (solution, residuals, rank, singular_values).", "Examples:"]}, {"name": "torch.linalg.torch.linalg.lu", "path": "generated/torch.linalg.lu", "type": "Linear Algebra", "text": ["Computes the LU decomposition with partial pivoting of a matrix.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, the LU decomposition with partial pivoting of a matrix A\u2208Km\u00d7nA \\in \\mathbb{K}^{m \\times n} is defined as", "where k = min(m,n), PP is a permutation matrix, LL is lower triangular with ones on the diagonal and UU is upper triangular.", "If pivot= False and A is on GPU, then the LU decomposition without pivoting is computed", "When pivot= False, the returned matrix P will be empty. The LU decomposition without pivoting may not exist if any of the principal minors of A is singular. In this case, the output matrix may contain inf or NaN.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "See also", "torch.linalg.solve() solves a system of linear equations using the LU decomposition with partial pivoting.", "Warning", "The LU decomposition is almost never unique, as often there are different permutation matrices that can yield different LU decompositions. As such, different platforms, like SciPy, or inputs on different devices, may produce different valid decompositions.", "Warning", "Gradient computations are only supported if the input matrix is full-rank. If this condition is not met, no error will be thrown, but the gradient may not be finite. This is because the LU decomposition with pivoting is not differentiable at these points.", "out (tuple, optional) \u2013 output tuple of three tensors. Ignored if None. Default: None.", "A named tuple (P, L, U).", "Examples:"]}, {"name": "torch.linalg.torch.linalg.lu_factor", "path": "generated/torch.linalg.lu_factor", "type": "Linear Algebra", "text": ["Computes a compact representation of the LU factorization with partial pivoting of a matrix.", "This function computes a compact representation of the decomposition given by torch.linalg.lu(). If the matrix is square, this representation may be used in torch.linalg.lu_solve() to solve system of linear equations that share the matrix A.", "The returned decomposition is represented as a named tuple (LU, pivots). The LU matrix has the same shape as the input matrix A. Its upper and lower triangular parts encode the non-constant elements of L and U of the LU decomposition of A.", "The returned permutation matrix is represented by a 1-indexed vector. pivots[i] == j represents that in the i-th step of the algorithm, the i-th row was permuted with the j-1-th row.", "On CUDA, one may use pivot= False. In this case, this function returns the LU decomposition without pivoting if it exists.", "Supports inputs of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if the inputs are batches of matrices then the output has the same batch dimensions.", "Note", "When inputs are on a CUDA device, this function synchronizes that device with the CPU. For a version of this function that does not synchronize, see torch.linalg.lu_factor_ex().", "Warning", "The LU decomposition is almost never unique, as often there are different permutation matrices that can yield different LU decompositions. As such, different platforms, like SciPy, or inputs on different devices, may produce different valid decompositions.", "Gradient computations are only supported if the input matrix is full-rank. If this condition is not met, no error will be thrown, but the gradient may not be finite. This is because the LU decomposition with pivoting is not differentiable at these points.", "See also", "torch.linalg.lu_solve() solves a system of linear equations given the output of this function provided the input matrix was square and invertible.", "torch.lu_unpack() unpacks the tensors returned by lu_factor() into the three matrices P, L, U that form the decomposition.", "torch.linalg.lu() computes the LU decomposition with partial pivoting of a possibly non-square matrix. It is a composition of lu_factor() and torch.lu_unpack().", "torch.linalg.solve() solves a system of linear equations. It is a composition of lu_factor() and lu_solve().", "A (Tensor) \u2013 tensor of shape (*, m, n) where * is zero or more batch dimensions.", "A named tuple (LU, pivots).", "RuntimeError \u2013 if the A matrix is not invertible or any matrix in a batched A is not invertible.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.lu_factor_ex", "path": "generated/torch.linalg.lu_factor_ex", "type": "Linear Algebra", "text": ["This is a version of lu_factor() that does not perform error checks unless check_errors= True. It also returns the info tensor returned by LAPACK\u2019s getrf.", "Note", "When the inputs are on a CUDA device, this function synchronizes only when check_errors= True.", "Warning", "This function is \u201cexperimental\u201d and it may change in a future PyTorch release.", "A (Tensor) \u2013 tensor of shape (*, m, n) where * is zero or more batch dimensions.", "A named tuple (LU, pivots, info)."]}, {"name": "torch.linalg.torch.linalg.lu_solve", "path": "generated/torch.linalg.lu_solve", "type": "Linear Algebra", "text": ["Computes the solution of a square system of linear equations with a unique solution given an LU decomposition.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, this function computes the solution X\u2208Kn\u00d7kX \\in \\mathbb{K}^{n \\times k} of the linear system associated to A\u2208Kn\u00d7n,B\u2208Kn\u00d7kA \\in \\mathbb{K}^{n \\times n}, B \\in \\mathbb{K}^{n \\times k}, which is defined as", "where AA is given factorized as returned by lu_factor().", "If left= False, this function returns the matrix X\u2208Kn\u00d7kX \\in \\mathbb{K}^{n \\times k} that solves the system", "If adjoint= True (and left= True), given an LU factorization of :math:`A this function function returns the X\u2208Kn\u00d7kX \\in \\mathbb{K}^{n \\times k} that solves the system", "where AHA^{\\text{H}} is the conjugate transpose when AA is complex, and the transpose when AA is real-valued. The left= False case is analogous.", "Supports inputs of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if the inputs are batches of matrices then the output has the same batch dimensions.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.matmul", "path": "generated/torch.linalg.matmul", "type": "Linear Algebra", "text": ["Alias for torch.matmul()"]}, {"name": "torch.linalg.torch.linalg.matrix_exp", "path": "generated/torch.linalg.matrix_exp", "type": "Linear Algebra", "text": ["Computes the matrix exponential of a square matrix.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, this function computes the matrix exponential of A\u2208Kn\u00d7nA \\in \\mathbb{K}^{n \\times n}, which is defined as", "If the matrix AA has eigenvalues \u03bbi\u2208C\\lambda_i \\in \\mathbb{C}, the matrix matrix_exp(A)\\mathrm{matrix\\_exp}(A) has eigenvalues e\u03bbi\u2208Ce^{\\lambda_i} \\in \\mathbb{C}.", "Supports input of bfloat16, float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "A (Tensor) \u2013 tensor of shape (*, n, n) where * is zero or more batch dimensions.", "Example:"]}, {"name": "torch.linalg.torch.linalg.matrix_norm", "path": "generated/torch.linalg.matrix_norm", "type": "Linear Algebra", "text": ["Computes a matrix norm.", "If A is complex valued, it computes the norm of A.abs()", "Support input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices: the norm will be computed over the dimensions specified by the 2-tuple dim and the other dimensions will be treated as batch dimensions. The output will have the same batch dimensions.", "ord defines the matrix norm that is computed. The following norms are supported:", "ord", "matrix norm", "\u2018fro\u2019 (default)", "Frobenius norm", "\u2018nuc\u2019", "nuclear norm", "inf", "max(sum(abs(x), dim=1))", "-inf", "min(sum(abs(x), dim=1))", "1", "max(sum(abs(x), dim=0))", "-1", "min(sum(abs(x), dim=0))", "2", "largest singular value", "-2", "smallest singular value", "where inf refers to float(\u2018inf\u2019), NumPy\u2019s inf object, or any equivalent object.", "A real-valued tensor, even when A is complex.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.matrix_power", "path": "generated/torch.linalg.matrix_power", "type": "Linear Algebra", "text": ["Computes the n-th power of a square matrix for an integer n.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "If n= 0, it returns the identity matrix (or batch) of the same shape as A. If n is negative, it returns the inverse of each matrix (if invertible) raised to the power of abs(n).", "Note", "Consider using torch.linalg.solve() if possible for multiplying a matrix on the left by a negative power as, if n> 0:", "It is always preferred to use solve() when possible, as it is faster and more numerically stable than computing A\u2212nA^{-n} explicitly.", "See also", "torch.linalg.solve() computes A.inverse() @ B with a numerically stable algorithm.", "out (Tensor, optional) \u2013 output tensor. Ignored if None. Default: None.", "RuntimeError \u2013 if n< 0 and the matrix A or any matrix in the batch of matrices A is not invertible.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.matrix_rank", "path": "generated/torch.linalg.matrix_rank", "type": "Linear Algebra", "text": ["Computes the numerical rank of a matrix.", "The matrix rank is computed as the number of singular values (or eigenvalues in absolute value when hermitian= True) that are greater than max\u2061(atol,\u03c31\u2217rtol)\\max(\\text{atol}, \\sigma_1 * \\text{rtol}) threshold, where \u03c31\\sigma_1 is the largest singular value (or eigenvalue).", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "If hermitian= True, A is assumed to be Hermitian if complex or symmetric if real, but this is not checked internally. Instead, just the lower triangular part of the matrix is used in the computations.", "If rtol is not specified and A is a matrix of dimensions (m, n), the relative tolerance is set to be rtol=max\u2061(m,n)\u03b5\\text{rtol} = \\max(m, n) \\varepsilon and \u03b5\\varepsilon is the epsilon value for the dtype of A (see finfo). If rtol is not specified and atol is specified to be larger than zero then rtol is set to zero.", "If atol or rtol is a torch.Tensor, its shape must be broadcastable to that of the singular values of A as returned by torch.linalg.svdvals().", "Note", "This function has NumPy compatible variant linalg.matrix_rank(A, tol, hermitian=False). However, use of the positional argument tol is deprecated in favor of atol and rtol.", "Note", "The matrix rank is computed using a singular value decomposition torch.linalg.svdvals() if hermitian= False (default) and the eigenvalue decomposition torch.linalg.eigvalsh() when hermitian= True. When inputs are on a CUDA device, this function synchronizes that device with the CPU.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.multi_dot", "path": "generated/torch.linalg.multi_dot", "type": "Linear Algebra", "text": ["Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.", "Supports inputs of float, double, cfloat and cdouble dtypes. This function does not support batched inputs.", "Every tensor in tensors must be 2D, except for the first and last which may be 1D. If the first tensor is a 1D vector of shape (n,) it is treated as a row vector of shape (1, n), similarly if the last tensor is a 1D vector of shape (n,) it is treated as a column vector of shape (n, 1).", "If the first and last tensors are matrices, the output will be a matrix. However, if either is a 1D vector, then the output will be a 1D vector.", "Differences with numpy.linalg.multi_dot:", "Warning", "This function does not broadcast.", "Note", "This function is implemented by chaining torch.mm() calls after computing the optimal matrix multiplication order.", "Note", "The cost of multiplying two matrices with shapes (a, b) and (b, c) is a * b * c. Given matrices A, B, C with shapes (10, 100), (100, 5), (5, 50) respectively, we can calculate the cost of different multiplication orders as follows:", "In this case, multiplying A and B first followed by C is 10 times faster.", "tensors (Sequence[Tensor]) \u2013 two or more tensors to multiply. The first and last tensors may be 1D or 2D. Every other tensor must be 2D.", "out (Tensor, optional) \u2013 output tensor. Ignored if None. Default: None.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.norm", "path": "generated/torch.linalg.norm", "type": "Linear Algebra", "text": ["Computes a vector or matrix norm.", "Supports input of float, double, cfloat and cdouble dtypes.", "Whether this function computes a vector or matrix norm is determined as follows:", "ord defines the norm that is computed. The following norms are supported:", "ord", "norm for matrices", "norm for vectors", "None (default)", "Frobenius norm", "2-norm (see below)", "\u2018fro\u2019", "Frobenius norm", "\u2013 not supported \u2013", "\u2018nuc\u2019", "nuclear norm", "\u2013 not supported \u2013", "inf", "max(sum(abs(x), dim=1))", "max(abs(x))", "-inf", "min(sum(abs(x), dim=1))", "min(abs(x))", "0", "\u2013 not supported \u2013", "sum(x != 0)", "1", "max(sum(abs(x), dim=0))", "as below", "-1", "min(sum(abs(x), dim=0))", "as below", "2", "largest singular value", "as below", "-2", "smallest singular value", "as below", "other int or float", "\u2013 not supported \u2013", "sum(abs(x)^{ord})^{(1 / ord)}", "where inf refers to float(\u2018inf\u2019), NumPy\u2019s inf object, or any equivalent object.", "See also", "torch.linalg.vector_norm() computes a vector norm.", "torch.linalg.matrix_norm() computes a matrix norm.", "The above functions are often clearer and more flexible than using torch.linalg.norm(). For example, torch.linalg.norm(A, ord=1, dim=(0, 1)) always computes a matrix norm, but with torch.linalg.vector_norm(A, ord=1, dim=(0, 1)) it is possible to compute a vector norm over the two dimensions.", "A real-valued tensor, even when A is complex.", "Examples:", "Using the dim argument to compute vector norms:", "Using the dim argument to compute matrix norms:"]}, {"name": "torch.linalg.torch.linalg.pinv", "path": "generated/torch.linalg.pinv", "type": "Linear Algebra", "text": ["Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.", "The pseudoinverse may be defined algebraically but it is more computationally convenient to understand it through the SVD", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "If hermitian= True, A is assumed to be Hermitian if complex or symmetric if real, but this is not checked internally. Instead, just the lower triangular part of the matrix is used in the computations.", "The singular values (or the norm of the eigenvalues when hermitian= True) that are below max\u2061(atol,\u03c31\u22c5rtol)\\max(\\text{atol}, \\sigma_1 \\cdot \\text{rtol}) threshold are treated as zero and discarded in the computation, where \u03c31\\sigma_1 is the largest singular value (or eigenvalue).", "If rtol is not specified and A is a matrix of dimensions (m, n), the relative tolerance is set to be rtol=max\u2061(m,n)\u03b5\\text{rtol} = \\max(m, n) \\varepsilon and \u03b5\\varepsilon is the epsilon value for the dtype of A (see finfo). If rtol is not specified and atol is specified to be larger than zero then rtol is set to zero.", "If atol or rtol is a torch.Tensor, its shape must be broadcastable to that of the singular values of A as returned by torch.linalg.svd().", "Note", "This function uses torch.linalg.svd() if hermitian= False and torch.linalg.eigh() if hermitian= True. For CUDA inputs, this function synchronizes that device with the CPU.", "Note", "Consider using torch.linalg.lstsq() if possible for multiplying a matrix on the left by the pseudoinverse, as:", "It is always preferred to use lstsq() when possible, as it is faster and more numerically stable than computing the pseudoinverse explicitly.", "Note", "This function has NumPy compatible variant linalg.pinv(A, rcond, hermitian=False). However, use of the positional argument rcond is deprecated in favor of rtol.", "Warning", "This function uses internally torch.linalg.svd() (or torch.linalg.eigh() when hermitian= True), so its derivative has the same problems as those of these functions. See the warnings in torch.linalg.svd() and torch.linalg.eigh() for more details.", "See also", "torch.linalg.inv() computes the inverse of a square matrix.", "torch.linalg.lstsq() computes A.pinv() @ B with a numerically stable algorithm.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.qr", "path": "generated/torch.linalg.qr", "type": "Linear Algebra", "text": ["Computes the QR decomposition of a matrix.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, the full QR decomposition of a matrix A\u2208Km\u00d7nA \\in \\mathbb{K}^{m \\times n} is defined as", "where QQ is orthogonal in the real case and unitary in the complex case, and RR is upper triangular with real diagonal (even in the complex case).", "When m > n (tall matrix), as R is upper triangular, its last m - n rows are zero. In this case, we can drop the last m - n columns of Q to form the reduced QR decomposition:", "The reduced QR decomposition agrees with the full QR decomposition when n >= m (wide matrix).", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "The parameter mode chooses between the full and reduced QR decomposition. If A has shape (*, m, n), denoting k = min(m, n)", "Differences with numpy.linalg.qr:", "Warning", "The elements in the diagonal of R are not necessarily positive. As such, the returned QR decomposition is only unique up to the sign of the diagonal of R. Therefore, different platforms, like NumPy, or inputs on different devices, may produce different valid decompositions.", "Warning", "The QR decomposition is only well-defined if the first k = min(m, n) columns of every matrix in A are linearly independent. If this condition is not met, no error will be thrown, but the QR produced may be incorrect and its autodiff may fail or produce incorrect results.", "out (tuple, optional) \u2013 output tuple of two tensors. Ignored if None. Default: None.", "A named tuple (Q, R).", "Examples:"]}, {"name": "torch.linalg.torch.linalg.slogdet", "path": "generated/torch.linalg.slogdet", "type": "Linear Algebra", "text": ["Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.", "For complex A, it returns the sign and the natural logarithm of the modulus of the determinant, that is, a logarithmic polar decomposition of the determinant.", "The determinant can be recovered as sign * exp(logabsdet). When a matrix has a determinant of zero, it returns (0, -inf).", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "See also", "torch.linalg.det() computes the determinant of square matrices.", "A (Tensor) \u2013 tensor of shape (*, n, n) where * is zero or more batch dimensions.", "out (tuple, optional) \u2013 output tuple of two tensors. Ignored if None. Default: None.", "A named tuple (sign, logabsdet).", "sign will have the same dtype as A.", "logabsdet will always be real-valued, even when A is complex.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.solve", "path": "generated/torch.linalg.solve", "type": "Linear Algebra", "text": ["Computes the solution of a square system of linear equations with a unique solution.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, this function computes the solution X\u2208Kn\u00d7kX \\in \\mathbb{K}^{n \\times k} of the linear system associated to A\u2208Kn\u00d7n,B\u2208Kn\u00d7kA \\in \\mathbb{K}^{n \\times n}, B \\in \\mathbb{K}^{n \\times k}, which is defined as", "If left= False, this function returns the matrix X\u2208Kn\u00d7kX \\in \\mathbb{K}^{n \\times k} that solves the system", "This system of linear equations has one solution if and only if AA is invertible. This function assumes that AA is invertible.", "Supports inputs of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if the inputs are batches of matrices then the output has the same batch dimensions.", "Letting * be zero or more batch dimensions,", "Note", "This function computes X = A.inverse() @ B in a faster and more numerically stable way than performing the computations separately.", "Note", "It is possible to compute the solution of the system XA=BXA = B by passing the inputs A and B transposed and transposing the output returned by this function.", "Note", "When inputs are on a CUDA device, this function synchronizes that device with the CPU.", "See also", "torch.linalg.solve_triangular() computes the solution of a triangular system of linear equations with a unique solution.", "RuntimeError \u2013 if the A matrix is not invertible or any matrix in a batched A is not invertible.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.solve_ex", "path": "generated/torch.linalg.solve_ex", "type": "Linear Algebra", "text": ["A version of solve() that does not perform error checks unless check_errors= True. It also returns the info tensor returned by LAPACK\u2019s getrf.", "Note", "When the inputs are on a CUDA device, this function synchronizes only when check_errors= True.", "Warning", "This function is \u201cexperimental\u201d and it may change in a future PyTorch release.", "A (Tensor) \u2013 tensor of shape (*, n, n) where * is zero or more batch dimensions.", "A named tuple (result, info).", "Examples:"]}, {"name": "torch.linalg.torch.linalg.solve_triangular", "path": "generated/torch.linalg.solve_triangular", "type": "Linear Algebra", "text": ["Computes the solution of a triangular system of linear equations with a unique solution.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, this function computes the solution X\u2208Kn\u00d7kX \\in \\mathbb{K}^{n \\times k} of the linear system associated to the triangular matrix A\u2208Kn\u00d7nA \\in \\mathbb{K}^{n \\times n} without zeros on the diagonal (that is, it is invertible) and the rectangular matrix , B\u2208Kn\u00d7kB \\in \\mathbb{K}^{n \\times k}, which is defined as", "The argument upper signals whether AA is upper or lower triangular.", "If left= False, this function returns the matrix X\u2208Kn\u00d7kX \\in \\mathbb{K}^{n \\times k} that solves the system", "If upper= True (resp. False) just the upper (resp. lower) triangular half of A will be accessed. The elements below the main diagonal will be considered to be zero and will not be accessed.", "If unitriangular= True, the diagonal of A is assumed to be ones and will not be accessed.", "The result may contain NaN s if the diagonal of A contains zeros or elements that are very close to zero and unitriangular= False (default) or if the input matrix has very small eigenvalues.", "Supports inputs of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if the inputs are batches of matrices then the output has the same batch dimensions.", "See also", "torch.linalg.solve() computes the solution of a general square system of linear equations with a unique solution.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.svd", "path": "generated/torch.linalg.svd", "type": "Linear Algebra", "text": ["Computes the singular value decomposition (SVD) of a matrix.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, the full SVD of a matrix A\u2208Km\u00d7nA \\in \\mathbb{K}^{m \\times n}, if k = min(m,n), is defined as", "where diag\u2061(S)\u2208Km\u00d7n\\operatorname{diag}(S) \\in \\mathbb{K}^{m \\times n}, VHV^{\\text{H}} is the conjugate transpose when VV is complex, and the transpose when VV is real-valued. The matrices UU, VV (and thus VHV^{\\text{H}}) are orthogonal in the real case, and unitary in the complex case.", "When m > n (resp. m < n) we can drop the last m - n (resp. n - m) columns of U (resp. V) to form the reduced SVD:", "where diag\u2061(S)\u2208Kk\u00d7k\\operatorname{diag}(S) \\in \\mathbb{K}^{k \\times k}. In this case, UU and VV also have orthonormal columns.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "The returned decomposition is a named tuple (U, S, Vh) which corresponds to UU, SS, VHV^{\\text{H}} above.", "The singular values are returned in descending order.", "The parameter full_matrices chooses between the full (default) and reduced SVD.", "The driver kwarg may be used in CUDA with a cuSOLVER backend to choose the algorithm used to compute the SVD. The choice of a driver is a trade-off between accuracy and speed.", "If A is well-conditioned (its condition number is not too large), or you do not mind some precision loss.", "By default (driver= None), we call \u2018gesvdj\u2019 and, if it fails, we fallback to \u2018gesvd\u2019.", "Differences with numpy.linalg.svd:", "Note", "When full_matrices= True, the gradients with respect to U[\u2026, :, min(m, n):] and Vh[\u2026, min(m, n):, :] will be ignored, as those vectors can be arbitrary bases of the corresponding subspaces.", "Warning", "The returned tensors U and V are not unique, nor are they continuous with respect to A. Due to this lack of uniqueness, different hardware and software may compute different singular vectors.", "This non-uniqueness is caused by the fact that multiplying any pair of singular vectors uk,vku_k, v_k by -1 in the real case or by ei\u03d5,\u03d5\u2208Re^{i \\phi}, \\phi \\in \\mathbb{R} in the complex case produces another two valid singular vectors of the matrix. For this reason, the loss function shall not depend on this ei\u03d5e^{i \\phi} quantity, as it is not well-defined. This is checked for complex inputs when computing the gradients of this function. As such, when inputs are complex and are on a CUDA device, the computation of the gradients of this function synchronizes that device with the CPU.", "Warning", "Gradients computed using U or Vh will only be finite when A does not have repeated singular values. If A is rectangular, additionally, zero must also not be one of its singular values. Furthermore, if the distance between any two singular values is close to zero, the gradient will be numerically unstable, as it depends on the singular values \u03c3i\\sigma_i through the computation of 1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}. In the rectangular case, the gradient will also be numerically unstable when A has small singular values, as it also depends on the computation of 1\u03c3i\\frac{1}{\\sigma_i}.", "See also", "torch.linalg.svdvals() computes only the singular values. Unlike torch.linalg.svd(), the gradients of svdvals() are always numerically stable.", "torch.linalg.eig() for a function that computes another type of spectral decomposition of a matrix. The eigendecomposition works just on square matrices.", "torch.linalg.eigh() for a (faster) function that computes the eigenvalue decomposition for Hermitian and symmetric matrices.", "torch.linalg.qr() for another (much faster) decomposition that works on general matrices.", "A named tuple (U, S, Vh) which corresponds to UU, SS, VHV^{\\text{H}} above.", "S will always be real-valued, even when A is complex. It will also be ordered in descending order.", "U and Vh will have the same dtype as A. The left / right singular vectors will be given by the columns of U and the rows of Vh respectively.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.svdvals", "path": "generated/torch.linalg.svdvals", "type": "Linear Algebra", "text": ["Computes the singular values of a matrix.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.", "The singular values are returned in descending order.", "Note", "This function is equivalent to NumPy\u2019s linalg.svd(A, compute_uv=False).", "Note", "When inputs are on a CUDA device, this function synchronizes that device with the CPU.", "See also", "torch.linalg.svd() computes the full singular value decomposition.", "A (Tensor) \u2013 tensor of shape (*, m, n) where * is zero or more batch dimensions.", "A real-valued tensor, even when A is complex.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.tensorinv", "path": "generated/torch.linalg.tensorinv", "type": "Linear Algebra", "text": ["Computes the multiplicative inverse of torch.tensordot().", "If m is the product of the first ind dimensions of A and n is the product of the rest of the dimensions, this function expects m and n to be equal. If this is the case, it computes a tensor X such that tensordot(A, X, ind) is the identity matrix in dimension m. X will have the shape of A but with the first ind dimensions pushed back to the end", "Supports input of float, double, cfloat and cdouble dtypes.", "Note", "When A is a 2-dimensional tensor and ind= 1, this function computes the (multiplicative) inverse of A (see torch.linalg.inv()).", "Note", "Consider using torch.linalg.tensorsolve() if possible for multiplying a tensor on the left by the tensor inverse, as:", "It is always preferred to use tensorsolve() when possible, as it is faster and more numerically stable than computing the pseudoinverse explicitly.", "See also", "torch.linalg.tensorsolve() computes torch.tensordot(tensorinv(A), B).", "out (Tensor, optional) \u2013 output tensor. Ignored if None. Default: None.", "RuntimeError \u2013 if the reshaped A is not invertible or the product of the first ind dimensions is not equal to the product of the rest.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.tensorsolve", "path": "generated/torch.linalg.tensorsolve", "type": "Linear Algebra", "text": ["Computes the solution X to the system torch.tensordot(A, X) = B.", "If m is the product of the first B.ndim dimensions of A and n is the product of the rest of the dimensions, this function expects m and n to be equal.", "The returned tensor x satisfies tensordot(A, x, dims=x.ndim) == B. x has shape A[B.ndim:].", "If dims is specified, A will be reshaped as", "Supports inputs of float, double, cfloat and cdouble dtypes.", "See also", "torch.linalg.tensorinv() computes the multiplicative inverse of torch.tensordot().", "out (Tensor, optional) \u2013 output tensor. Ignored if None. Default: None.", "RuntimeError \u2013 if the reshaped A.view(m, m) with m as above is not invertible or the product of the first ind dimensions is not equal to the product of the rest of the dimensions.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.vander", "path": "generated/torch.linalg.vander", "type": "Linear Algebra", "text": ["Generates a Vandermonde matrix.", "Returns the Vandermonde matrix VV", "for N > 1. If N= None, then N = x.size(-1) so that the output is a square matrix.", "Supports inputs of float, double, cfloat, cdouble, and integral dtypes. Also supports batches of vectors, and if x is a batch of vectors then the output has the same batch dimensions.", "Differences with numpy.vander:", "x (Tensor) \u2013 tensor of shape (*, n) where * is zero or more batch dimensions consisting of vectors.", "N (int, optional) \u2013 Number of columns in the output. Default: x.size(-1)", "Example:"]}, {"name": "torch.linalg.torch.linalg.vecdot", "path": "generated/torch.linalg.vecdot", "type": "Linear Algebra", "text": ["Computes the dot product of two batches of vectors along a dimension.", "In symbols, this function computes", "over the dimension dim where xi\u203e\\overline{x_i} denotes the conjugate for complex vectors, and it is the identity for real vectors.", "Supports input of half, bfloat16, float, double, cfloat, cdouble and integral dtypes. It also supports broadcasting.", "Examples:"]}, {"name": "torch.linalg.torch.linalg.vector_norm", "path": "generated/torch.linalg.vector_norm", "type": "Linear Algebra", "text": ["Computes a vector norm.", "If x is complex valued, it computes the norm of x.abs()", "Supports input of float, double, cfloat and cdouble dtypes.", "This function does not necessarily treat multidimensional x as a batch of vectors, instead:", "This behavior is for consistency with torch.linalg.norm().", "ord defines the vector norm that is computed. The following norms are supported:", "ord", "vector norm", "2 (default)", "2-norm (see below)", "inf", "max(abs(x))", "-inf", "min(abs(x))", "0", "sum(x != 0)", "other int or float", "sum(abs(x)^{ord})^{(1 / ord)}", "where inf refers to float(\u2018inf\u2019), NumPy\u2019s inf object, or any equivalent object.", "dtype may be used to perform the computation in a more precise dtype. It is semantically equivalent to calling linalg.vector_norm(x.to(dtype)) but it is faster in some cases.", "See also", "torch.linalg.matrix_norm() computes a matrix norm.", "A real-valued tensor, even when x is complex.", "Examples:"]}, {"name": "torch.linalg.vander()", "path": "generated/torch.linalg.vander#torch.linalg.vander", "type": "Linear Algebra", "text": ["Generates a Vandermonde matrix.", "Returns the Vandermonde matrix VV", "for N > 1. If N= None, then N = x.size(-1) so that the output is a square matrix.", "Supports inputs of float, double, cfloat, cdouble, and integral dtypes. Also supports batches of vectors, and if x is a batch of vectors then the output has the same batch dimensions.", "Differences with numpy.vander:", "x (Tensor) \u2013 tensor of shape (*, n) where * is zero or more batch dimensions consisting of vectors.", "N (int, optional) \u2013 Number of columns in the output. Default: x.size(-1)", "Example:"]}, {"name": "torch.linalg.vecdot()", "path": "generated/torch.linalg.vecdot#torch.linalg.vecdot", "type": "Linear Algebra", "text": ["Computes the dot product of two batches of vectors along a dimension.", "In symbols, this function computes", "over the dimension dim where xi\u203e\\overline{x_i} denotes the conjugate for complex vectors, and it is the identity for real vectors.", "Supports input of half, bfloat16, float, double, cfloat, cdouble and integral dtypes. It also supports broadcasting.", "Examples:"]}, {"name": "torch.linalg.vector_norm()", "path": "generated/torch.linalg.vector_norm#torch.linalg.vector_norm", "type": "Linear Algebra", "text": ["Computes a vector norm.", "If x is complex valued, it computes the norm of x.abs()", "Supports input of float, double, cfloat and cdouble dtypes.", "This function does not necessarily treat multidimensional x as a batch of vectors, instead:", "This behavior is for consistency with torch.linalg.norm().", "ord defines the vector norm that is computed. The following norms are supported:", "ord", "vector norm", "2 (default)", "2-norm (see below)", "inf", "max(abs(x))", "-inf", "min(abs(x))", "0", "sum(x != 0)", "other int or float", "sum(abs(x)^{ord})^{(1 / ord)}", "where inf refers to float(\u2018inf\u2019), NumPy\u2019s inf object, or any equivalent object.", "dtype may be used to perform the computation in a more precise dtype. It is semantically equivalent to calling linalg.vector_norm(x.to(dtype)) but it is faster in some cases.", "See also", "torch.linalg.matrix_norm() computes a matrix norm.", "A real-valued tensor, even when x is complex.", "Examples:"]}, {"name": "torch.linspace", "path": "generated/torch.linspace", "type": "Torch", "text": ["Creates a one-dimensional tensor of size steps whose values are evenly spaced from start to end, inclusive. That is, the value are:", "From PyTorch 1.11 linspace requires the steps argument. Use steps=100 to restore the previous behavior.", "Example:"]}, {"name": "torch.load", "path": "generated/torch.load", "type": "Torch", "text": ["Loads an object saved with torch.save() from a file.", "torch.load() uses Python\u2019s unpickling facilities but treats storages, which underlie tensors, specially. They are first deserialized on the CPU and are then moved to the device they were saved from. If this fails (e.g. because the run time system doesn\u2019t have certain devices), an exception is raised. However, storages can be dynamically remapped to an alternative set of devices using the map_location argument.", "If map_location is a callable, it will be called once for each serialized storage with two arguments: storage and location. The storage argument will be the initial deserialization of the storage, residing on the CPU. Each serialized storage has a location tag associated with it which identifies the device it was saved from, and this tag is the second argument passed to map_location. The builtin location tags are 'cpu' for CPU tensors and 'cuda:device_id' (e.g. 'cuda:2') for CUDA tensors. map_location should return either None or a storage. If map_location returns a storage, it will be used as the final deserialized object, already moved to the right device. Otherwise, torch.load() will fall back to the default behavior, as if map_location wasn\u2019t specified.", "If map_location is a torch.device object or a string containing a device tag, it indicates the location where all tensors should be loaded.", "Otherwise, if map_location is a dict, it will be used to remap location tags appearing in the file (keys), to ones that specify where to put the storages (values).", "User extensions can register their own location tags and tagging and deserialization methods using torch.serialization.register_package().", "Any", "Warning", "torch.load() unless weights_only parameter is set to True, uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Never load data that could have come from an untrusted source in an unsafe mode, or that could have been tampered with. Only load data you trust.", "Note", "When you call torch.load() on a file which contains GPU tensors, those tensors will be loaded to GPU by default. You can call torch.load(.., map_location='cpu') and then load_state_dict() to avoid GPU RAM surge when loading a model checkpoint.", "Note", "By default, we decode byte strings as utf-8. This is to avoid a common error case UnicodeDecodeError: 'ascii' codec can't decode byte 0x... when loading files saved by Python 2 in Python 3. If this default is incorrect, you may use an extra encoding keyword argument to specify how these objects should be loaded, e.g., encoding='latin1' decodes them to strings using latin1 encoding, and encoding='bytes' keeps them as byte arrays which can be decoded later with byte_array.decode(...)."]}, {"name": "torch.lobpcg", "path": "generated/torch.lobpcg", "type": "Torch", "text": ["Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive definite generalized eigenvalue problem using matrix-free LOBPCG methods.", "This function is a front-end to the following LOBPCG algorithms selectable via method argument:", "method=\u201dbasic\u201d - the LOBPCG method introduced by Andrew Knyazev, see [Knyazev2001]. A less robust method, may fail when Cholesky is applied to singular input.", "method=\u201dortho\u201d - the LOBPCG method with orthogonal basis selection [StathopoulosEtal2002]. A robust method.", "Supported inputs are dense, sparse, and batches of dense matrices.", "Note", "In general, the basic method spends least time per iteration. However, the robust methods converge much faster and are more stable. So, the usage of the basic method is generally not recommended but there exist cases where the usage of the basic method may be preferred.", "Warning", "The backward method does not support sparse and complex inputs. It works only when B is not provided (i.e. B == None). We are actively working on extensions, and the details of the algorithms are going to be published promptly.", "Warning", "While it is assumed that A is symmetric, A.grad is not. To make sure that A.grad is symmetric, so that A - t * A.grad is symmetric in first-order optimization routines, prior to running lobpcg we do the following symmetrization map: A -> (A + A.t()) / 2. The map is performed only when the A requires gradients.", "tracker (callable, optional) \u2013 ", "a function for tracing the iteration process. When specified, it is called at each iteration step with LOBPCG instance as an argument. The LOBPCG instance holds the full state of the iteration process in the following attributes:", "iparams, fparams, bparams - dictionaries of integer, float, and boolean valued input parameters, respectively", "ivars, fvars, bvars, tvars - dictionaries of integer, float, boolean, and Tensor valued iteration variables, respectively.", "A, B, iK - input Tensor arguments.", "E, X, S, R - iteration Tensor variables.", "For instance:", "ivars[\u201cistep\u201d] - the current iteration step X - the current approximation of eigenvectors E - the current approximation of eigenvalues R - the current residual ivars[\u201cconverged_count\u201d] - the current number of converged eigenpairs tvars[\u201crerr\u201d] - the current state of convergence criteria", "Note that when tracker stores Tensor objects from the LOBPCG instance, it must make copies of these.", "If tracker sets bvars[\u201cforce_stop\u201d] = True, the iteration process will be hard-stopped.", "tensor of eigenvalues of size (\u2217,k)(*, k)", "X (Tensor): tensor of eigenvectors of size (\u2217,m,k)(*, m, k)", "E (Tensor)", "[Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal Preconditioned Eigensolver: Locally Optimal Block Preconditioned Conjugate Gradient Method. SIAM J. Sci. Comput., 23(2), 517-541. (25 pages) https://epubs.siam.org/doi/abs/10.1137/S1064827500366124", "[StathopoulosEtal2002] Andreas Stathopoulos and Kesheng Wu. (2002) A Block Orthogonalization Procedure with Constant Synchronization Requirements. SIAM J. Sci. Comput., 23(6), 2165-2182. (18 pages) https://epubs.siam.org/doi/10.1137/S1064827500370883", "[DuerschEtal2018] Jed A. Duersch, Meiyue Shao, Chao Yang, Ming Gu. (2018) A Robust and Efficient Implementation of LOBPCG. SIAM J. Sci. Comput., 40(5), C655-C676. (22 pages) https://epubs.siam.org/doi/abs/10.1137/17M1129830"]}, {"name": "torch.log", "path": "generated/torch.log", "type": "Torch", "text": ["Returns a new tensor with the natural logarithm of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.log10", "path": "generated/torch.log10", "type": "Torch", "text": ["Returns a new tensor with the logarithm to the base 10 of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.log1p", "path": "generated/torch.log1p", "type": "Torch", "text": ["Returns a new tensor with the natural logarithm of (1 + input).", "Note", "This function is more accurate than torch.log() for small values of input", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.log2", "path": "generated/torch.log2", "type": "Torch", "text": ["Returns a new tensor with the logarithm to the base 2 of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.logaddexp", "path": "generated/torch.logaddexp", "type": "Torch", "text": ["Logarithm of the sum of exponentiations of the inputs.", "Calculates pointwise log\u2061(ex+ey)\\log\\left(e^x + e^y\\right). This function is useful in statistics where the calculated probabilities of events may be so small as to exceed the range of normal floating point numbers. In such cases the logarithm of the calculated probability is stored. This function allows adding probabilities stored in such a fashion.", "This op should be disambiguated with torch.logsumexp() which performs a reduction on a single tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.logaddexp2", "path": "generated/torch.logaddexp2", "type": "Torch", "text": ["Logarithm of the sum of exponentiations of the inputs in base-2.", "Calculates pointwise log\u20612(2x+2y)\\log_2\\left(2^x + 2^y\\right). See torch.logaddexp() for more details.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.logcumsumexp", "path": "generated/torch.logcumsumexp", "type": "Torch", "text": ["Returns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim.", "For summation index jj given by dim and other indices ii, the result is", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.logdet", "path": "generated/torch.logdet", "type": "Torch", "text": ["Calculates log determinant of a square matrix or batches of square matrices.", "It returns -inf if the input has a determinant of zero, and NaN if it has a negative determinant.", "Note", "Backward through logdet() internally uses SVD results when input is not invertible. In this case, double backward through logdet() will be unstable in when input doesn\u2019t have distinct singular values. See torch.linalg.svd() for details.", "See also", "torch.linalg.slogdet() computes the sign (resp. angle) and natural logarithm of the absolute value of the determinant of real-valued (resp. complex) square matrices.", "input (Tensor) \u2013 the input tensor of size (*, n, n) where * is zero or more batch dimensions.", "Example:"]}, {"name": "torch.logical_and", "path": "generated/torch.logical_and", "type": "Torch", "text": ["Computes the element-wise logical AND of the given input tensors. Zeros are treated as False and nonzeros are treated as True.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.logical_not", "path": "generated/torch.logical_not", "type": "Torch", "text": ["Computes the element-wise logical NOT of the given input tensor. If not specified, the output tensor will have the bool dtype. If the input tensor is not a bool tensor, zeros are treated as False and non-zeros are treated as True.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.logical_or", "path": "generated/torch.logical_or", "type": "Torch", "text": ["Computes the element-wise logical OR of the given input tensors. Zeros are treated as False and nonzeros are treated as True.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.logical_xor", "path": "generated/torch.logical_xor", "type": "Torch", "text": ["Computes the element-wise logical XOR of the given input tensors. Zeros are treated as False and nonzeros are treated as True.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.logit", "path": "generated/torch.logit", "type": "Torch", "text": ["Alias for torch.special.logit()."]}, {"name": "torch.logspace", "path": "generated/torch.logspace", "type": "Torch", "text": ["Creates a one-dimensional tensor of size steps whose values are evenly spaced from basestart{{\\text{{base}}}}^{{\\text{{start}}}} to baseend{{\\text{{base}}}}^{{\\text{{end}}}}, inclusive, on a logarithmic scale with base base. That is, the values are:", "From PyTorch 1.11 logspace requires the steps argument. Use steps=100 to restore the previous behavior.", "Example:"]}, {"name": "torch.logsumexp", "path": "generated/torch.logsumexp", "type": "Torch", "text": ["Returns the log of summed exponentials of each row of the input tensor in the given dimension dim. The computation is numerically stabilized.", "For summation index jj given by dim and other indices ii, the result is", "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.LongStorage", "path": "storage#torch.LongStorage", "type": "Storage", "text": []}, {"name": "torch.LongStorage.dtype", "path": "storage#torch.LongStorage.dtype", "type": "Storage", "text": []}, {"name": "torch.lt", "path": "generated/torch.lt", "type": "Torch", "text": ["Computes input<other\\text{input} < \\text{other} element-wise.", "The second argument can be a number or a tensor whose shape is broadcastable with the first argument.", "out (Tensor, optional) \u2013 the output tensor.", "A boolean tensor that is True where input is less than other and False elsewhere", "Example:"]}, {"name": "torch.lu", "path": "generated/torch.lu", "type": "Torch", "text": ["Computes the LU factorization of a matrix or batches of matrices A. Returns a tuple containing the LU factorization and pivots of A. Pivoting is done if pivot is set to True.", "Warning", "torch.lu() is deprecated in favor of torch.linalg.lu_factor() and torch.linalg.lu_factor_ex(). torch.lu() will be removed in a future PyTorch release. LU, pivots, info = torch.lu(A, compute_pivots) should be replaced with", "LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True) should be replaced with", "Note", "Warning", "The gradients of this function will only be finite when A is full rank. This is because the LU decomposition is just differentiable at full rank matrices. Furthermore, if A is close to not being full rank, the gradient will be numerically unstable as it depends on the computation of L\u22121L^{-1} and U\u22121U^{-1}.", "A tuple of tensors containing", "(Tensor, IntTensor, IntTensor (optional))", "Example:"]}, {"name": "torch.lu_solve", "path": "generated/torch.lu_solve", "type": "Torch", "text": ["Returns the LU solve of the linear system Ax=bAx = b using the partially pivoted LU factorization of A from lu_factor().", "This function supports float, double, cfloat and cdouble dtypes for input.", "Warning", "torch.lu_solve() is deprecated in favor of torch.linalg.lu_solve(). torch.lu_solve() will be removed in a future PyTorch release. X = torch.lu_solve(B, LU, pivots) should be replaced with", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.lu_unpack", "path": "generated/torch.lu_unpack", "type": "Torch", "text": ["Unpacks the LU decomposition returned by lu_factor() into the P, L, U matrices.", "See also", "lu() returns the matrices from the LU decomposition. Its gradient formula is more efficient than that of doing lu_factor() followed by lu_unpack().", "out (tuple, optional) \u2013 output tuple of three tensors. Ignored if None.", "A namedtuple (P, L, U)", "Examples:"]}, {"name": "torch.manual_seed", "path": "generated/torch.manual_seed", "type": "Torch", "text": ["Sets the seed for generating random numbers. Returns a torch.Generator object.", "seed (int) \u2013 The desired seed. Value must be within the inclusive range [-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError is raised. Negative inputs are remapped to positive values with the formula 0xffff_ffff_ffff_ffff + seed.", "Generator"]}, {"name": "torch.masked", "path": "masked", "type": "Miscellaneous", "text": ["Warning", "The PyTorch API of masked tensors is in the prototype stage and may or may not change in the future.", "MaskedTensor serves as an extension to torch.Tensor that provides the user with the ability to:", "\u201cSpecified\u201d and \u201cunspecified\u201d have a long history in PyTorch without formal semantics and certainly without consistency; indeed, MaskedTensor was born out of a build up of issues that the vanilla torch.Tensor class could not properly address. Thus, a primary goal of MaskedTensor is to become the source of truth for said \u201cspecified\u201d and \u201cunspecified\u201d values in PyTorch where they are a first class citizen instead of an afterthought. In turn, this should further unlock sparsity\u2019s potential, enable safer and more consistent operators, and provide a smoother and more intuitive experience for users and developers alike.", "A MaskedTensor is a tensor subclass that consists of 1) an input (data), and 2) a mask. The mask tells us which entries from the input should be included or ignored.", "By way of example, suppose that we wanted to mask out all values that are equal to 0 (represented by the gray) and take the max:", "On top is the vanilla tensor example while the bottom is MaskedTensor where all the 0\u2019s are masked out. This clearly yields a different result depending on whether we have the mask, but this flexible structure allows the user to systematically ignore any elements they\u2019d like during computation.", "There are already a number of existing tutorials that we\u2019ve written to help users onboard, such as:", "Unary operators are operators that only contain only a single input. Applying them to MaskedTensors is relatively straightforward: if the data is masked out at a given index, we apply the operator, otherwise we\u2019ll continue to mask out the data.", "The available unary operators are:", "Computes the absolute value of each element in input.", "Alias for torch.abs()", "Computes the inverse cosine of each element in input.", "Alias for torch.acos().", "Returns a new tensor with the inverse hyperbolic cosine of the elements of input.", "Alias for torch.acosh().", "Computes the element-wise angle (in radians) of the given input tensor.", "Returns a new tensor with the arcsine of the elements of input.", "Alias for torch.asin().", "Returns a new tensor with the inverse hyperbolic sine of the elements of input.", "Alias for torch.asinh().", "Returns a new tensor with the arctangent of the elements of input.", "Alias for torch.atan().", "Returns a new tensor with the inverse hyperbolic tangent of the elements of input.", "Alias for torch.atanh().", "Computes the bitwise NOT of the given input tensor.", "Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.", "Clamps all elements in input into the range [ min, max ].", "Alias for torch.clamp().", "Computes the element-wise conjugate of the given input tensor.", "Returns a new tensor with the cosine of the elements of input.", "Returns a new tensor with the hyperbolic cosine of the elements of input.", "Returns a new tensor with each of the elements of input converted from angles in degrees to radians.", "Alias for torch.special.digamma().", "Alias for torch.special.erf().", "Alias for torch.special.erfc().", "Alias for torch.special.erfinv().", "Returns a new tensor with the exponential of the elements of the input tensor input.", "Alias for torch.special.exp2().", "Alias for torch.special.expm1().", "Alias for torch.trunc()", "Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.", "Computes the fractional portion of each element in input.", "Computes the natural logarithm of the absolute value of the gamma function on input.", "Returns a new tensor with the natural logarithm of the elements of input.", "Returns a new tensor with the logarithm to the base 10 of the elements of input.", "Returns a new tensor with the natural logarithm of (1 + input).", "Returns a new tensor with the logarithm to the base 2 of the elements of input.", "Alias for torch.special.logit().", "Alias for torch.special.i0().", "Returns a new tensor with boolean elements representing if each element of input is NaN or not.", "Replaces NaN, positive infinity, and negative infinity values in input with the values specified by nan, posinf, and neginf, respectively.", "Returns a new tensor with the negative of the elements of input.", "Alias for torch.neg()", "Returns input.", "Takes the power of each element in input with exponent and returns a tensor with the result.", "Returns a new tensor with each of the elements of input converted from angles in radians to degrees.", "Returns a new tensor with the reciprocal of the elements of input", "Rounds elements of input to the nearest integer.", "Returns a new tensor with the reciprocal of the square-root of each of the elements of input.", "Alias for torch.special.expit().", "Returns a new tensor with the signs of the elements of input.", "This function is an extension of torch.sign() to complex tensors.", "Tests if each element of input has its sign bit set or not.", "Returns a new tensor with the sine of the elements of input.", "Alias for torch.special.sinc().", "Returns a new tensor with the hyperbolic sine of the elements of input.", "Returns a new tensor with the square-root of the elements of input.", "Returns a new tensor with the square of the elements of input.", "Returns a new tensor with the tangent of the elements of input.", "Returns a new tensor with the hyperbolic tangent of the elements of input.", "Returns a new tensor with the truncated integer values of the elements of input.", "The available inplace unary operators are all of the above except:", "Computes the element-wise angle (in radians) of the given input tensor.", "Returns input.", "Tests if each element of input has its sign bit set or not.", "Returns a new tensor with boolean elements representing if each element of input is NaN or not.", "As you may have seen in the tutorial, MaskedTensor also has binary operations implemented with the caveat that the masks in the two MaskedTensors must match or else an error will be raised. As noted in the error, if you need support for a particular operator or have proposed semantics for how they should behave instead, please open an issue on GitHub. For now, we have decided to go with the most conservative implementation to ensure that users know exactly what is going on and are being intentional about their decisions with masked semantics.", "The available binary operators are:", "Adds other, scaled by alpha, to input.", "Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i} with consideration of the quadrant.", "Alias for torch.atan2().", "Computes the bitwise AND of input and other.", "Computes the bitwise OR of input and other.", "Computes the bitwise XOR of input and other.", "Computes the left arithmetic shift of input by other bits.", "Computes the right arithmetic shift of input by other bits.", "Divides each element of the input input by the corresponding element of other.", "Alias for torch.div().", "Applies C++'s std::fmod entrywise.", "Logarithm of the sum of exponentiations of the inputs.", "Logarithm of the sum of exponentiations of the inputs in base-2.", "Multiplies input by other.", "Alias for torch.mul().", "Return the next floating-point value after input towards other, elementwise.", "Computes Python's modulus operation entrywise.", "Subtracts other, scaled by alpha, from input.", "Alias for torch.sub().", "Alias for torch.div() with rounding_mode=None.", "Computes element-wise equality", "Computes input\u2260other\\text{input} \\neq \\text{other} element-wise.", "Computes input\u2264other\\text{input} \\leq \\text{other} element-wise.", "Computes input\u2265other\\text{input} \\geq \\text{other} element-wise.", "Alias for torch.gt().", "Alias for torch.ge().", "Computes input>other\\text{input} > \\text{other} element-wise.", "Alias for torch.le().", "Computes input<other\\text{input} < \\text{other} element-wise.", "Alias for torch.lt().", "Computes the element-wise maximum of input and other.", "Computes the element-wise minimum of input and other.", "Computes the element-wise maximum of input and other.", "Computes the element-wise minimum of input and other.", "Alias for torch.ne().", "The available inplace binary operators are all of the above except:", "Logarithm of the sum of exponentiations of the inputs.", "Logarithm of the sum of exponentiations of the inputs in base-2.", "True if two tensors have the same size and elements, False otherwise.", "Computes the element-wise minimum of input and other.", "Computes the element-wise minimum of input and other.", "Computes the element-wise maximum of input and other.", "The following reductions are available (with autograd support). For more information, the Overview tutorial details some examples of reductions, while the Advanced semantics tutorial has some further in-depth discussions about how we decided on certain reduction semantics.", "Returns the sum of all elements in the input tensor.", "Returns the mean value of all elements in the input tensor.", "Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.", "Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.", "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension", "Returns the indices of the maximum value of all elements in the input tensor.", "Returns the product of all elements in the input tensor.", "Tests if all elements in input evaluate to True.", "Returns the matrix norm or vector norm of a given tensor.", "Calculates the variance over the dimensions specified by dim.", "Calculates the standard deviation over the dimensions specified by dim.", "We\u2019ve included a number of view and select functions as well; intuitively, these operators will apply to both the data and the mask and then wrap the result in a MaskedTensor. For a quick example, consider select():", "The following ops are currently supported:", "Returns a 1-dimensional view of each input tensor with zero dimensions.", "Broadcasts the given tensors according to Broadcasting semantics.", "Broadcasts input to the shape shape.", "Concatenates the given sequence of seq tensors in the given dimension.", "Attempts to split a tensor into the specified number of chunks.", "Creates a new tensor by horizontally stacking the tensors in tensors.", "Splits input, a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections.", "Flattens input by reshaping it into a one-dimensional tensor.", "Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.", "Stack tensors in sequence horizontally (column wise).", "Computes the Kronecker product, denoted by \u2297\\otimes, of input and other.", "Creates grids of coordinates specified by the 1D inputs in attr:tensors.", "Returns a new tensor that is a narrowed version of input tensor.", "Return a contiguous flattened tensor.", "Slices the input tensor along the selected dimension at the given index.", "Splits the tensor into chunks.", "Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.", "Returns a tensor that is a transposed version of input.", "Splits input, a tensor with two or more dimensions, into multiple tensors vertically according to indices_or_sections.", "Stack tensors in sequence vertically (row wise).", "Tensor.expand", "Returns a new view of the self tensor with singleton dimensions expanded to a larger size.", "Tensor.expand_as", "Expand this tensor to the same size as other.", "Tensor.reshape", "Returns a tensor with the same data and number of elements as self but with the specified shape.", "Tensor.reshape_as", "Returns this tensor as the same shape as other.", "Tensor.view", "Returns a new tensor with the same data as the self tensor but of a different shape."]}, {"name": "torch.masked_select", "path": "generated/torch.masked_select", "type": "Torch", "text": ["Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.", "The shapes of the mask tensor and the input tensor don\u2019t need to match, but they must be broadcastable.", "Note", "The returned tensor does not use the same storage as the original tensor", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.matmul", "path": "generated/torch.matmul", "type": "Torch", "text": ["Matrix product of two tensors.", "The behavior depends on the dimensionality of the tensors as follows:", "If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N > 2), then a batched matrix multiply is returned. If the first argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after. If the second argument is 1-dimensional, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after. The non-matrix (i.e. batch) dimensions are broadcasted (and thus must be broadcastable). For example, if input is a (j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n) tensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n) tensor.", "Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs are broadcastable, and not the matrix dimensions. For example, if input is a (j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p) tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the matrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p) tensor.", "This operation has support for arguments with sparse layouts. In particular the matrix-matrix (both arguments 2-dimensional) supports sparse arguments with the same restrictions as torch.mm()", "Warning", "Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported, or may not have autograd support. If you notice missing functionality please open a feature request.", "This operator supports TensorFloat32.", "On certain ROCm devices, when using float16 inputs this module will use different precision for backward.", "Note", "The 1-dimensional dot product version of this function does not support an out parameter.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.matrix_exp", "path": "generated/torch.matrix_exp", "type": "Torch", "text": ["Alias for torch.linalg.matrix_exp()."]}, {"name": "torch.matrix_power", "path": "generated/torch.matrix_power", "type": "Torch", "text": ["Alias for torch.linalg.matrix_power()"]}, {"name": "torch.max", "path": "generated/torch.max", "type": "Torch", "text": ["Returns the maximum value of all elements in the input tensor.", "Warning", "This function produces deterministic (sub)gradients unlike max(dim=0)", "input (Tensor) \u2013 the input tensor.", "Example:", "Returns a namedtuple (values, indices) where values is the maximum value of each row of the input tensor in the given dimension dim. And indices is the index location of each maximum value found (argmax).", "If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensors having 1 fewer dimension than input.", "Note", "If there are multiple maximal values in a reduced row then the indices of the first maximal value are returned.", "out (tuple, optional) \u2013 the result tuple of two output tensors (max, max_indices)", "Example:", "See torch.maximum()."]}, {"name": "torch.maximum", "path": "generated/torch.maximum", "type": "Torch", "text": ["Computes the element-wise maximum of input and other.", "Note", "If one of the elements being compared is a NaN, then that element is returned. maximum() is not supported for tensors with complex dtypes.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.mean", "path": "generated/torch.mean", "type": "Torch", "text": ["Returns the mean value of all elements in the input tensor.", "input (Tensor) \u2013 the input tensor.", "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.", "Example:", "Returns the mean value of each row of the input tensor in the given dimension dim. If dim is a list of dimensions, reduce over all of them.", "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).", "See also", "torch.nanmean() computes the mean value of non-NaN elements.", "Example:"]}, {"name": "torch.median", "path": "generated/torch.median", "type": "Torch", "text": ["Returns the median of the values in input.", "Note", "The median is not unique for input tensors with an even number of elements. In this case the lower of the two medians is returned. To compute the mean of both medians, use torch.quantile() with q=0.5 instead.", "Warning", "This function produces deterministic (sub)gradients unlike median(dim=0)", "input (Tensor) \u2013 the input tensor.", "Example:", "Returns a namedtuple (values, indices) where values contains the median of each row of input in the dimension dim, and indices contains the index of the median values found in the dimension dim.", "By default, dim is the last dimension of the input tensor.", "If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the outputs tensor having 1 fewer dimension than input.", "Note", "The median is not unique for input tensors with an even number of elements in the dimension dim. In this case the lower of the two medians is returned. To compute the mean of both medians in input, use torch.quantile() with q=0.5 instead.", "Warning", "indices does not necessarily contain the first occurrence of each median value found, unless it is unique. The exact implementation details are device-specific. Do not expect the same result when run on CPU and GPU in general. For the same reason do not expect the gradients to be deterministic.", "out ((Tensor, Tensor), optional) \u2013 The first tensor will be populated with the median values and the second tensor, which must have dtype long, with their indices in the dimension dim of input.", "Example:"]}, {"name": "torch.memory_format", "path": "tensor_attributes#torch.memory_format", "type": "Miscellaneous", "text": []}, {"name": "torch.meshgrid", "path": "generated/torch.meshgrid", "type": "Torch", "text": ["Creates grids of coordinates specified by the 1D inputs in attr:tensors.", "This is helpful when you want to visualize data over some range of inputs. See below for a plotting example.", "Given NN 1D tensors T0\u2026TN\u22121T_0 \\ldots T_{N-1} as inputs with corresponding sizes S0\u2026SN\u22121S_0 \\ldots S_{N-1}, this creates NN N-dimensional tensors G0\u2026GN\u22121G_0 \\ldots G_{N-1}, each with shape (S0,...,SN\u22121)(S_0, ..., S_{N-1}) where the output GiG_i is constructed by expanding TiT_i to the result shape.", "Note", "0D inputs are treated equivalently to 1D inputs of a single element.", "Warning", "torch.meshgrid(*tensors) currently has the same behavior as calling numpy.meshgrid(*arrays, indexing=\u2019ij\u2019).", "In the future torch.meshgrid will transition to indexing=\u2019xy\u2019 as the default.", "https://github.com/pytorch/pytorch/issues/50276 tracks this issue with the goal of migrating to NumPy\u2019s behavior.", "See also", "torch.cartesian_prod() has the same effect but it collects the data in a tensor of vectors.", "indexing (Optional[str]) \u2013 ", "(str, optional): the indexing mode, either \u201cxy\u201d or \u201cij\u201d, defaults to \u201cij\u201d. See warning for future changes.", "If \u201cxy\u201d is selected, the first dimension corresponds to the cardinality of the second input and the second dimension corresponds to the cardinality of the first input.", "If \u201cij\u201d is selected, the dimensions are in the same order as the cardinality of the inputs.", "If the input has NN tensors of size S0\u2026SN\u22121S_0 \\ldots S_{N-1}, then the output will also have NN tensors, where each tensor is of shape (S0,...,SN\u22121)(S_0, ..., S_{N-1}).", "seq (sequence of Tensors)", "Example:"]}, {"name": "torch.min", "path": "generated/torch.min", "type": "Torch", "text": ["Returns the minimum value of all elements in the input tensor.", "Warning", "This function produces deterministic (sub)gradients unlike min(dim=0)", "input (Tensor) \u2013 the input tensor.", "Example:", "Returns a namedtuple (values, indices) where values is the minimum value of each row of the input tensor in the given dimension dim. And indices is the index location of each minimum value found (argmin).", "If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensors having 1 fewer dimension than input.", "Note", "If there are multiple minimal values in a reduced row then the indices of the first minimal value are returned.", "out (tuple, optional) \u2013 the tuple of two output tensors (min, min_indices)", "Example:", "See torch.minimum()."]}, {"name": "torch.minimum", "path": "generated/torch.minimum", "type": "Torch", "text": ["Computes the element-wise minimum of input and other.", "Note", "If one of the elements being compared is a NaN, then that element is returned. minimum() is not supported for tensors with complex dtypes.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.mm", "path": "generated/torch.mm", "type": "Torch", "text": ["Performs a matrix multiplication of the matrices input and mat2.", "If input is a (n\u00d7m)(n \\times m) tensor, mat2 is a (m\u00d7p)(m \\times p) tensor, out will be a (n\u00d7p)(n \\times p) tensor.", "Note", "This function does not broadcast. For broadcasting matrix products, see torch.matmul().", "Supports strided and sparse 2-D tensors as inputs, autograd with respect to strided inputs.", "This operation has support for arguments with sparse layouts. If out is provided it\u2019s layout will be used. Otherwise, the result layout will be deduced from that of input.", "Warning", "Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported, or may not have autograd support. If you notice missing functionality please open a feature request.", "This operator supports TensorFloat32.", "On certain ROCm devices, when using float16 inputs this module will use different precision for backward.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.mode", "path": "generated/torch.mode", "type": "Torch", "text": ["Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e. a value which appears most often in that row, and indices is the index location of each mode value found.", "By default, dim is the last dimension of the input tensor.", "If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensors having 1 fewer dimension than input.", "Note", "This function is not defined for torch.cuda.Tensor yet.", "out (tuple, optional) \u2013 the result tuple of two output tensors (values, indices)", "Example:"]}, {"name": "torch.monitor", "path": "monitor", "type": "Monitor", "text": ["Warning", "This module is a prototype release, and its interfaces and functionality may change without warning in future PyTorch releases.", "torch.monitor provides an interface for logging events and counters from PyTorch.", "The stat interfaces are designed to be used for tracking high level metrics that are periodically logged out to be used for monitoring system performance. Since the stats aggregate with a specific window size you can log to them from critical loops with minimal performance impact.", "For more infrequent events or values such as loss, accuracy, usage tracking the event interface can be directly used.", "Event handlers can be registered to handle the events and pass them to an external event sink.", "These are types of aggregations that can be used to accumulate stats.", "Members:", "VALUE returns the last value to be added.", "MEAN computes the arithmetic mean of all the added values.", "COUNT returns the total number of added values.", "SUM returns the sum of the added values.", "MAX returns the max of the added values.", "MIN returns the min of the added values.", "Stat is used to compute summary statistics in a performant way over fixed intervals. Stat logs the statistics as an Event once every window_size duration. When the window closes the stats are logged via the event handlers as a torch.monitor.Stat event.", "window_size should be set to something relatively high to avoid a huge number of events being logged. Ex: 60s. Stat uses millisecond precision.", "If max_samples is set, the stat will cap the number of samples per window by discarding add calls once max_samples adds have occurred. If it\u2019s not set, all add calls during the window will be included. This is an optional field to make aggregations more directly comparable across windows when the number of samples might vary.", "When the Stat is destructed it will log any remaining data even if the window hasn\u2019t elapsed.", "Constructs the Stat.", "Adds a value to the stat to be aggregated according to the configured stat type and aggregations.", "Number of data points that have currently been collected. Resets once the event has been logged.", "Returns the current value of the stat, primarily for testing purposes. If the stat has logged and no additional values have been added this will be zero.", "The name of the stat that was set during creation.", "data_value_t is one of str, float, int, bool.", "Event represents a specific typed event to be logged. This can represent high-level data points such as loss or accuracy per epoch or more low-level aggregations such as through the Stats provided through this library.", "All Events of the same type should have the same name so downstream handlers can correctly process them.", "Constructs the Event.", "The structured data contained within the Event.", "The name of the Event.", "The timestamp when the Event happened.", "EventHandlerHandle is a wrapper type returned by register_event_handler used to unregister the handler via unregister_event_handler. This cannot be directly initialized.", "log_event logs the specified event to all of the registered event handlers. It\u2019s up to the event handlers to log the event out to the corresponding event sink.", "If there are no event handlers registered this method is a no-op.", "register_event_handler registers a callback to be called whenever an event is logged via log_event. These handlers should avoid blocking the main thread since that may interfere with training as they run during the log_event call.", "unregister_event_handler unregisters the EventHandlerHandle returned after calling register_event_handler. After this returns the event handler will no longer receive events.", "TensorboardEventHandler is an event handler that will write known events to the provided SummaryWriter.", "This currently only supports torch.monitor.Stat events which are logged as scalars.", "Constructs the TensorboardEventHandler."]}, {"name": "torch.monitor.Aggregation", "path": "monitor#torch.monitor.Aggregation", "type": "Monitor", "text": ["These are types of aggregations that can be used to accumulate stats.", "Members:", "VALUE returns the last value to be added.", "MEAN computes the arithmetic mean of all the added values.", "COUNT returns the total number of added values.", "SUM returns the sum of the added values.", "MAX returns the max of the added values.", "MIN returns the min of the added values."]}, {"name": "torch.monitor.Aggregation.name", "path": "monitor#torch.monitor.Aggregation.name", "type": "Monitor", "text": []}, {"name": "torch.monitor.data_value_t", "path": "monitor#torch.monitor.data_value_t", "type": "Monitor", "text": ["data_value_t is one of str, float, int, bool."]}, {"name": "torch.monitor.Event", "path": "monitor#torch.monitor.Event", "type": "Monitor", "text": ["Event represents a specific typed event to be logged. This can represent high-level data points such as loss or accuracy per epoch or more low-level aggregations such as through the Stats provided through this library.", "All Events of the same type should have the same name so downstream handlers can correctly process them.", "Constructs the Event.", "The structured data contained within the Event.", "The name of the Event.", "The timestamp when the Event happened."]}, {"name": "torch.monitor.Event.__init__()", "path": "monitor#torch.monitor.Event.__init__", "type": "Monitor", "text": ["Constructs the Event."]}, {"name": "torch.monitor.Event.data", "path": "monitor#torch.monitor.Event.data", "type": "Monitor", "text": ["The structured data contained within the Event."]}, {"name": "torch.monitor.Event.name", "path": "monitor#torch.monitor.Event.name", "type": "Monitor", "text": ["The name of the Event."]}, {"name": "torch.monitor.Event.timestamp", "path": "monitor#torch.monitor.Event.timestamp", "type": "Monitor", "text": ["The timestamp when the Event happened."]}, {"name": "torch.monitor.EventHandlerHandle", "path": "monitor#torch.monitor.EventHandlerHandle", "type": "Monitor", "text": ["EventHandlerHandle is a wrapper type returned by register_event_handler used to unregister the handler via unregister_event_handler. This cannot be directly initialized."]}, {"name": "torch.monitor.log_event()", "path": "monitor#torch.monitor.log_event", "type": "Monitor", "text": ["log_event logs the specified event to all of the registered event handlers. It\u2019s up to the event handlers to log the event out to the corresponding event sink.", "If there are no event handlers registered this method is a no-op."]}, {"name": "torch.monitor.register_event_handler()", "path": "monitor#torch.monitor.register_event_handler", "type": "Monitor", "text": ["register_event_handler registers a callback to be called whenever an event is logged via log_event. These handlers should avoid blocking the main thread since that may interfere with training as they run during the log_event call."]}, {"name": "torch.monitor.Stat", "path": "monitor#torch.monitor.Stat", "type": "Monitor", "text": ["Stat is used to compute summary statistics in a performant way over fixed intervals. Stat logs the statistics as an Event once every window_size duration. When the window closes the stats are logged via the event handlers as a torch.monitor.Stat event.", "window_size should be set to something relatively high to avoid a huge number of events being logged. Ex: 60s. Stat uses millisecond precision.", "If max_samples is set, the stat will cap the number of samples per window by discarding add calls once max_samples adds have occurred. If it\u2019s not set, all add calls during the window will be included. This is an optional field to make aggregations more directly comparable across windows when the number of samples might vary.", "When the Stat is destructed it will log any remaining data even if the window hasn\u2019t elapsed.", "Constructs the Stat.", "Adds a value to the stat to be aggregated according to the configured stat type and aggregations.", "Number of data points that have currently been collected. Resets once the event has been logged.", "Returns the current value of the stat, primarily for testing purposes. If the stat has logged and no additional values have been added this will be zero.", "The name of the stat that was set during creation."]}, {"name": "torch.monitor.Stat.__init__()", "path": "monitor#torch.monitor.Stat.__init__", "type": "Monitor", "text": ["Constructs the Stat."]}, {"name": "torch.monitor.Stat.add()", "path": "monitor#torch.monitor.Stat.add", "type": "Monitor", "text": ["Adds a value to the stat to be aggregated according to the configured stat type and aggregations."]}, {"name": "torch.monitor.Stat.count", "path": "monitor#torch.monitor.Stat.count", "type": "Monitor", "text": ["Number of data points that have currently been collected. Resets once the event has been logged."]}, {"name": "torch.monitor.Stat.get()", "path": "monitor#torch.monitor.Stat.get", "type": "Monitor", "text": ["Returns the current value of the stat, primarily for testing purposes. If the stat has logged and no additional values have been added this will be zero."]}, {"name": "torch.monitor.Stat.name", "path": "monitor#torch.monitor.Stat.name", "type": "Monitor", "text": ["The name of the stat that was set during creation."]}, {"name": "torch.monitor.TensorboardEventHandler", "path": "monitor#torch.monitor.TensorboardEventHandler", "type": "Monitor", "text": ["TensorboardEventHandler is an event handler that will write known events to the provided SummaryWriter.", "This currently only supports torch.monitor.Stat events which are logged as scalars.", "Constructs the TensorboardEventHandler."]}, {"name": "torch.monitor.TensorboardEventHandler.__init__()", "path": "monitor#torch.monitor.TensorboardEventHandler.__init__", "type": "Monitor", "text": ["Constructs the TensorboardEventHandler."]}, {"name": "torch.monitor.unregister_event_handler()", "path": "monitor#torch.monitor.unregister_event_handler", "type": "Monitor", "text": ["unregister_event_handler unregisters the EventHandlerHandle returned after calling register_event_handler. After this returns the event handler will no longer receive events."]}, {"name": "torch.moveaxis", "path": "generated/torch.moveaxis", "type": "Torch", "text": ["Alias for torch.movedim().", "This function is equivalent to NumPy\u2019s moveaxis function.", "Examples:"]}, {"name": "torch.movedim", "path": "generated/torch.movedim", "type": "Torch", "text": ["Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.", "Other dimensions of input that are not explicitly moved remain in their original order and appear at the positions not specified in destination.", "Examples:"]}, {"name": "torch.mps", "path": "mps", "type": "MPS", "text": ["This package enables an interface for accessing MPS (Metal Performance Shaders) backend in Python. Metal is Apple\u2019s API for programming metal GPU (graphics processor unit). Using MPS means that increased performance can be achieved, by running work on the metal GPU(s). See https://developer.apple.com/documentation/metalperformanceshaders for more details.", "Waits for all kernels in all streams on a MPS device to complete.", "Returns the random number generator state as a ByteTensor.", "Sets the random number generator state.", "Sets the seed for generating random numbers.", "Sets the seed for generating random numbers to a random number.", "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU applications.", "Set memory fraction for limiting process's memory allocation on MPS device.", "Returns the current GPU memory occupied by tensors in bytes.", "Returns total GPU memory allocated by Metal driver for the process in bytes.", "profiler.start", "Start OS Signpost tracing from MPS backend.", "profiler.stop", "Stops generating OS Signpost tracing from MPS backend.", "profiler.profile", "Context Manager to enabling generating OS Signpost tracing from MPS backend.", "event.Event", "Wrapper around an MPS event."]}, {"name": "torch.mps.current_allocated_memory()", "path": "generated/torch.mps.current_allocated_memory#torch.mps.current_allocated_memory", "type": "MPS", "text": ["Returns the current GPU memory occupied by tensors in bytes.", "Note", "The returned size does not include cached allocations in memory pools of MPSAllocator.", "int"]}, {"name": "torch.mps.driver_allocated_memory()", "path": "generated/torch.mps.driver_allocated_memory#torch.mps.driver_allocated_memory", "type": "MPS", "text": ["Returns total GPU memory allocated by Metal driver for the process in bytes.", "Note", "The returned size includes cached allocations in MPSAllocator pools as well as allocations from MPS/MPSGraph frameworks.", "int"]}, {"name": "torch.mps.empty_cache()", "path": "generated/torch.mps.empty_cache#torch.mps.empty_cache", "type": "MPS", "text": ["Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU applications."]}, {"name": "torch.mps.Event", "path": "generated/torch.mps.event.event", "type": "MPS", "text": ["Wrapper around an MPS event.", "MPS events are synchronization markers that can be used to monitor the device\u2019s progress, to accurately measure timing, and to synchronize MPS streams.", "enable_timing (bool, optional) \u2013 indicates if the event should measure time (default: False)", "Returns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded.", "Returns True if all work currently captured by event has completed.", "Records the event in the default stream.", "Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes.", "Makes all future work submitted to the default stream wait for this event."]}, {"name": "torch.mps.event.Event", "path": "generated/torch.mps.event.event#torch.mps.event.Event", "type": "MPS", "text": ["Wrapper around an MPS event.", "MPS events are synchronization markers that can be used to monitor the device\u2019s progress, to accurately measure timing, and to synchronize MPS streams.", "enable_timing (bool, optional) \u2013 indicates if the event should measure time (default: False)", "Returns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded.", "Returns True if all work currently captured by event has completed.", "Records the event in the default stream.", "Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes.", "Makes all future work submitted to the default stream wait for this event."]}, {"name": "torch.mps.event.Event.elapsed_time()", "path": "generated/torch.mps.event.event#torch.mps.event.Event.elapsed_time", "type": "MPS", "text": ["Returns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded."]}, {"name": "torch.mps.event.Event.query()", "path": "generated/torch.mps.event.event#torch.mps.event.Event.query", "type": "MPS", "text": ["Returns True if all work currently captured by event has completed."]}, {"name": "torch.mps.event.Event.record()", "path": "generated/torch.mps.event.event#torch.mps.event.Event.record", "type": "MPS", "text": ["Records the event in the default stream."]}, {"name": "torch.mps.event.Event.synchronize()", "path": "generated/torch.mps.event.event#torch.mps.event.Event.synchronize", "type": "MPS", "text": ["Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes."]}, {"name": "torch.mps.event.Event.wait()", "path": "generated/torch.mps.event.event#torch.mps.event.Event.wait", "type": "MPS", "text": ["Makes all future work submitted to the default stream wait for this event."]}, {"name": "torch.mps.get_rng_state()", "path": "generated/torch.mps.get_rng_state#torch.mps.get_rng_state", "type": "MPS", "text": ["Returns the random number generator state as a ByteTensor.", "Tensor"]}, {"name": "torch.mps.manual_seed()", "path": "generated/torch.mps.manual_seed#torch.mps.manual_seed", "type": "MPS", "text": ["Sets the seed for generating random numbers.", "seed (int) \u2013 The desired seed."]}, {"name": "torch.mps.profiler.profile()", "path": "generated/torch.mps.profiler.profile#torch.mps.profiler.profile", "type": "MPS", "text": ["Context Manager to enabling generating OS Signpost tracing from MPS backend."]}, {"name": "torch.mps.profiler.start()", "path": "generated/torch.mps.profiler.start#torch.mps.profiler.start", "type": "MPS", "text": ["Start OS Signpost tracing from MPS backend.", "The generated OS Signposts could be recorded and viewed in XCode Instruments Logging tool."]}, {"name": "torch.mps.profiler.stop()", "path": "generated/torch.mps.profiler.stop#torch.mps.profiler.stop", "type": "MPS", "text": ["Stops generating OS Signpost tracing from MPS backend."]}, {"name": "torch.mps.seed()", "path": "generated/torch.mps.seed#torch.mps.seed", "type": "MPS", "text": ["Sets the seed for generating random numbers to a random number."]}, {"name": "torch.mps.set_per_process_memory_fraction()", "path": "generated/torch.mps.set_per_process_memory_fraction#torch.mps.set_per_process_memory_fraction", "type": "MPS", "text": ["Set memory fraction for limiting process\u2019s memory allocation on MPS device. The allowed value equals the fraction multiplied by recommended maximum device memory (obtained from Metal API device.recommendedMaxWorkingSetSize). If trying to allocate more than the allowed value in a process, it will raise an out of memory error in allocator.", "fraction (float) \u2013 Range: 0~2. Allowed memory equals total_memory * fraction.", "Note", "Passing 0 to fraction means unlimited allocations (may cause system failure if out of memory). Passing fraction greater than 1.0 allows limits beyond the value returned from device.recommendedMaxWorkingSetSize."]}, {"name": "torch.mps.set_rng_state()", "path": "generated/torch.mps.set_rng_state#torch.mps.set_rng_state", "type": "MPS", "text": ["Sets the random number generator state.", "new_state (torch.ByteTensor) \u2013 The desired state"]}, {"name": "torch.mps.synchronize()", "path": "generated/torch.mps.synchronize#torch.mps.synchronize", "type": "MPS", "text": ["Waits for all kernels in all streams on a MPS device to complete."]}, {"name": "torch.mps.torch.mps.current_allocated_memory", "path": "generated/torch.mps.current_allocated_memory", "type": "MPS", "text": ["Returns the current GPU memory occupied by tensors in bytes.", "Note", "The returned size does not include cached allocations in memory pools of MPSAllocator.", "int"]}, {"name": "torch.mps.torch.mps.driver_allocated_memory", "path": "generated/torch.mps.driver_allocated_memory", "type": "MPS", "text": ["Returns total GPU memory allocated by Metal driver for the process in bytes.", "Note", "The returned size includes cached allocations in MPSAllocator pools as well as allocations from MPS/MPSGraph frameworks.", "int"]}, {"name": "torch.mps.torch.mps.empty_cache", "path": "generated/torch.mps.empty_cache", "type": "MPS", "text": ["Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU applications."]}, {"name": "torch.mps.torch.mps.get_rng_state", "path": "generated/torch.mps.get_rng_state", "type": "MPS", "text": ["Returns the random number generator state as a ByteTensor.", "Tensor"]}, {"name": "torch.mps.torch.mps.manual_seed", "path": "generated/torch.mps.manual_seed", "type": "MPS", "text": ["Sets the seed for generating random numbers.", "seed (int) \u2013 The desired seed."]}, {"name": "torch.mps.torch.mps.profiler.profile", "path": "generated/torch.mps.profiler.profile", "type": "MPS", "text": ["Context Manager to enabling generating OS Signpost tracing from MPS backend."]}, {"name": "torch.mps.torch.mps.profiler.start", "path": "generated/torch.mps.profiler.start", "type": "MPS", "text": ["Start OS Signpost tracing from MPS backend.", "The generated OS Signposts could be recorded and viewed in XCode Instruments Logging tool."]}, {"name": "torch.mps.torch.mps.profiler.stop", "path": "generated/torch.mps.profiler.stop", "type": "MPS", "text": ["Stops generating OS Signpost tracing from MPS backend."]}, {"name": "torch.mps.torch.mps.seed", "path": "generated/torch.mps.seed", "type": "MPS", "text": ["Sets the seed for generating random numbers to a random number."]}, {"name": "torch.mps.torch.mps.set_per_process_memory_fraction", "path": "generated/torch.mps.set_per_process_memory_fraction", "type": "MPS", "text": ["Set memory fraction for limiting process\u2019s memory allocation on MPS device. The allowed value equals the fraction multiplied by recommended maximum device memory (obtained from Metal API device.recommendedMaxWorkingSetSize). If trying to allocate more than the allowed value in a process, it will raise an out of memory error in allocator.", "fraction (float) \u2013 Range: 0~2. Allowed memory equals total_memory * fraction.", "Note", "Passing 0 to fraction means unlimited allocations (may cause system failure if out of memory). Passing fraction greater than 1.0 allows limits beyond the value returned from device.recommendedMaxWorkingSetSize."]}, {"name": "torch.mps.torch.mps.set_rng_state", "path": "generated/torch.mps.set_rng_state", "type": "MPS", "text": ["Sets the random number generator state.", "new_state (torch.ByteTensor) \u2013 The desired state"]}, {"name": "torch.mps.torch.mps.synchronize", "path": "generated/torch.mps.synchronize", "type": "MPS", "text": ["Waits for all kernels in all streams on a MPS device to complete."]}, {"name": "torch.msort", "path": "generated/torch.msort", "type": "Torch", "text": ["Sorts the elements of the input tensor along its first dimension in ascending order by value.", "Note", "torch.msort(t) is equivalent to torch.sort(t, dim=0)[0]. See also torch.sort().", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.mul", "path": "generated/torch.mul", "type": "Torch", "text": ["Multiplies input by other.", "Supports broadcasting to a common shape, type promotion, and integer, float, and complex inputs.", "out (Tensor, optional) \u2013 the output tensor.", "Examples:"]}, {"name": "torch.multinomial", "path": "generated/torch.multinomial", "type": "Torch", "text": ["Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.", "Note", "The rows of input do not need to sum to one (in which case we use the values as weights), but must be non-negative, finite and have a non-zero sum.", "Indices are ordered from left to right according to when each was sampled (first samples are placed in first column).", "If input is a vector, out is a vector of size num_samples.", "If input is a matrix with m rows, out is an matrix of shape (m\u00d7num_samples)(m \\times \\text{num\\_samples}).", "If replacement is True, samples are drawn with replacement.", "If not, they are drawn without replacement, which means that when a sample index is drawn for a row, it cannot be drawn again for that row.", "Note", "When drawn without replacement, num_samples must be lower than number of non-zero elements in input (or the min number of non-zero elements in each row of input if it is a matrix).", "Example:"]}, {"name": "torch.multiply", "path": "generated/torch.multiply", "type": "Torch", "text": ["Alias for torch.mul()."]}, {"name": "torch.multiprocessing.get_all_sharing_strategies()", "path": "multiprocessing#torch.multiprocessing.get_all_sharing_strategies", "type": "Miscellaneous", "text": ["Returns a set of sharing strategies supported on a current system."]}, {"name": "torch.multiprocessing.get_sharing_strategy()", "path": "multiprocessing#torch.multiprocessing.get_sharing_strategy", "type": "Miscellaneous", "text": ["Returns the current strategy for sharing CPU tensors."]}, {"name": "torch.multiprocessing.set_sharing_strategy()", "path": "multiprocessing#torch.multiprocessing.set_sharing_strategy", "type": "Miscellaneous", "text": ["Sets the strategy for sharing CPU tensors.", "new_strategy (str) \u2013 Name of the selected strategy. Should be one of the values returned by get_all_sharing_strategies()."]}, {"name": "torch.multiprocessing.spawn()", "path": "multiprocessing#torch.multiprocessing.spawn", "type": "Miscellaneous", "text": ["Spawns nprocs processes that run fn with args.", "If one of the processes exits with a non-zero exit status, the remaining processes are killed and an exception is raised with the cause of termination. In the case an exception was caught in the child process, it is forwarded and its traceback is included in the exception raised in the parent process.", "fn (function) \u2013 ", "Function is called as the entrypoint of the spawned process. This function must be defined at the top level of a module so it can be pickled and spawned. This is a requirement imposed by multiprocessing.", "The function is called as fn(i, *args), where i is the process index and args is the passed through tuple of arguments.", "None if join is True, ProcessContext if join is False"]}, {"name": "torch.multiprocessing.SpawnContext", "path": "multiprocessing#torch.multiprocessing.SpawnContext", "type": "Miscellaneous", "text": ["Returned by spawn() when called with join=False.", "Tries to join one or more processes in this spawn context. If one of them exited with a non-zero exit status, this function kills the remaining processes and raises an exception with the cause of the first process exiting.", "Returns True if all processes have been joined successfully, False if there are more processes that need to be joined.", "timeout (float) \u2013 Wait this long before giving up on waiting."]}, {"name": "torch.multiprocessing.SpawnContext.join()", "path": "multiprocessing#torch.multiprocessing.SpawnContext.join", "type": "Miscellaneous", "text": ["Tries to join one or more processes in this spawn context. If one of them exited with a non-zero exit status, this function kills the remaining processes and raises an exception with the cause of the first process exiting.", "Returns True if all processes have been joined successfully, False if there are more processes that need to be joined.", "timeout (float) \u2013 Wait this long before giving up on waiting."]}, {"name": "torch.mv", "path": "generated/torch.mv", "type": "Torch", "text": ["Performs a matrix-vector product of the matrix input and the vector vec.", "If input is a (n\u00d7m)(n \\times m) tensor, vec is a 1-D tensor of size mm, out will be 1-D of size nn.", "Note", "This function does not broadcast.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.mvlgamma", "path": "generated/torch.mvlgamma", "type": "Torch", "text": ["Alias for torch.special.multigammaln()."]}, {"name": "torch.nan_to_num", "path": "generated/torch.nan_to_num", "type": "Torch", "text": ["Replaces NaN, positive infinity, and negative infinity values in input with the values specified by nan, posinf, and neginf, respectively. By default, NaNs are replaced with zero, positive infinity is replaced with the greatest finite value representable by input\u2019s dtype, and negative infinity is replaced with the least finite value representable by input\u2019s dtype.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.nanmean", "path": "generated/torch.nanmean", "type": "Torch", "text": ["Computes the mean of all non-NaN elements along the specified dimensions.", "This function is identical to torch.mean() when there are no NaN values in the input tensor. In the presence of NaN, torch.mean() will propagate the NaN to the output whereas torch.nanmean() will ignore the NaN values (torch.nanmean(a) is equivalent to torch.mean(a[~a.isnan()])).", "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).", "See also", "torch.mean() computes the mean value, propagating NaN.", "Example:"]}, {"name": "torch.nanmedian", "path": "generated/torch.nanmedian", "type": "Torch", "text": ["Returns the median of the values in input, ignoring NaN values.", "This function is identical to torch.median() when there are no NaN values in input. When input has one or more NaN values, torch.median() will always return NaN, while this function will return the median of the non-NaN elements in input. If all the elements in input are NaN it will also return NaN.", "input (Tensor) \u2013 the input tensor.", "Example:", "Returns a namedtuple (values, indices) where values contains the median of each row of input in the dimension dim, ignoring NaN values, and indices contains the index of the median values found in the dimension dim.", "This function is identical to torch.median() when there are no NaN values in a reduced row. When a reduced row has one or more NaN values, torch.median() will always reduce it to NaN, while this function will reduce it to the median of the non-NaN elements. If all the elements in a reduced row are NaN then it will be reduced to NaN, too.", "out ((Tensor, Tensor), optional) \u2013 The first tensor will be populated with the median values and the second tensor, which must have dtype long, with their indices in the dimension dim of input.", "Example:"]}, {"name": "torch.nanquantile", "path": "generated/torch.nanquantile", "type": "Torch", "text": ["This is a variant of torch.quantile() that \u201cignores\u201d NaN values, computing the quantiles q as if NaN values in input did not exist. If all values in a reduced row are NaN then the quantiles for that reduction will be NaN. See the documentation for torch.quantile().", "Example:"]}, {"name": "torch.nansum", "path": "generated/torch.nansum", "type": "Torch", "text": ["Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.", "input (Tensor) \u2013 the input tensor.", "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.", "Example:", "Returns the sum of each row of the input tensor in the given dimension dim, treating Not a Numbers (NaNs) as zero. If dim is a list of dimensions, reduce over all of them.", "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).", "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.", "Example:"]}, {"name": "torch.narrow", "path": "generated/torch.narrow", "type": "Torch", "text": ["Returns a new tensor that is a narrowed version of input tensor. The dimension dim is input from start to start + length. The returned tensor and input tensor share the same underlying storage.", "Example:"]}, {"name": "torch.narrow_copy", "path": "generated/torch.narrow_copy", "type": "Torch", "text": ["Same as Tensor.narrow() except this returns a copy rather than shared storage. This is primarily for sparse tensors, which do not have a shared-storage narrow method.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "See also", "torch.narrow() for a non copy variant"]}, {"name": "torch.ne", "path": "generated/torch.ne", "type": "Torch", "text": ["Computes input\u2260other\\text{input} \\neq \\text{other} element-wise.", "The second argument can be a number or a tensor whose shape is broadcastable with the first argument.", "out (Tensor, optional) \u2013 the output tensor.", "A boolean tensor that is True where input is not equal to other and False elsewhere", "Example:"]}, {"name": "torch.neg", "path": "generated/torch.neg", "type": "Torch", "text": ["Returns a new tensor with the negative of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.negative", "path": "generated/torch.negative", "type": "Torch", "text": ["Alias for torch.neg()"]}, {"name": "torch.nested", "path": "nested", "type": "Miscellaneous", "text": ["Warning", "The PyTorch API of nested tensors is in prototype stage and will change in the near future.", "NestedTensor allows the user to pack a list of Tensors into a single, efficient datastructure.", "The only constraint on the input Tensors is that their dimension must match.", "This enables more efficient metadata representations and access to purpose built kernels.", "One application of NestedTensors is to express sequential data in various domains. While the conventional approach is to pad variable length sequences, NestedTensor enables users to bypass padding. The API for calling operations on a nested tensor is no different from that of a regular torch.Tensor, which should allow seamless integration with existing models, with the main difference being construction of the inputs.", "As this is a prototype feature, the operations supported are still limited. However, we welcome issues, feature requests and contributions. More information on contributing can be found in this Readme.", "Construction is straightforward and involves passing a list of Tensors to the torch.nested.nested_tensor constructor.", "Data type, device and whether gradients are required can be chosen via the usual keyword arguments.", "In the vein of torch.as_tensor, torch.nested.as_nested_tensor can be used to preserve autograd history from the tensors passed to the constructor. For more information, refer to the section on Nested tensor constructor and conversion functions.", "In order to form a valid NestedTensor all the passed Tensors need to match in dimension, but none of the other attributes need to.", "If one of the dimensions doesn\u2019t match, the constructor throws an error.", "Note that the passed Tensors are being copied into a contiguous piece of memory. The resulting NestedTensor allocates new memory to store them and does not keep a reference.", "At this moment we only support one level of nesting, i.e. a simple, flat list of Tensors. In the future we can add support for multiple levels of nesting, such as a list that consists entirely of lists of Tensors. Note that for this extension it is important to maintain an even level of nesting across entries so that the resulting NestedTensor has a well defined dimension. If you have a need for this feature, please feel encouraged to open a feature request so that we can track it and plan accordingly.", "Even though a NestedTensor does not support .size() (or .shape), it supports .size(i) if dimension i is regular.", "If all dimensions are regular, the NestedTensor is intended to be semantically indistinguishable from a regular torch.Tensor.", "In the future we might make it easier to detect this condition and convert seamlessly.", "Please open a feature request if you have a need for this (or any other related feature for that matter).", "unbind allows you to retrieve a view of the constituents.", "Note that nt.unbind()[0] is not a copy, but rather a slice of the underlying memory, which represents the first entry or constituent of the NestedTensor.", "The following functions are related to nested tensors:", "Constructs a nested tensor with no autograd history (also known as a \u201cleaf tensor\u201d, see Autograd mechanics) from tensor_list a list of tensors.", "Example:", "Constructs a nested tensor preserving autograd history from tensor_list a list of tensors.", "Note", "Tensors within the list are always copied by this function due to current nested tensor semantics.", "tensor_list (List[Tensor]) \u2013 a list of tensors with the same ndim", "Tensor", "Example:", "Returns a new (non-nested) Tensor by padding the input nested tensor. The leading entries will be filled with the nested data, while the trailing entries will be padded.", "Warning", "to_padded_tensor() always copies the underlying data, since the nested and the non-nested tensors differ in memory layout.", "padding (float) \u2013 The padding value for the trailing entries.", "Example:", "In this section, we summarize the operations that are currently supported on NestedTensor and any constraints they have.", "PyTorch operation", "Constraints", "torch.matmul()", "Supports matrix multiplication between two (>= 3d) nested tensors where the last two dimensions are matrix dimensions and the leading (batch) dimensions have the same size (i.e. no broadcasting support for batch dimensions yet).", "torch.bmm()", "Supports batch matrix multiplication of two 3-d nested tensors.", "torch.nn.Linear()", "Supports 3-d nested input and a dense 2-d weight matrix.", "torch.nn.functional.softmax()", "Supports softmax along all dims except dim=0.", "torch.nn.Dropout()", "Behavior is the same as on regular tensors.", "torch.Tensor.masked_fill()", "Behavior is the same as on regular tensors.", "torch.relu()", "Behavior is the same as on regular tensors.", "torch.gelu()", "Behavior is the same as on regular tensors.", "torch.silu()", "Behavior is the same as on regular tensors.", "torch.abs()", "Behavior is the same as on regular tensors.", "torch.sgn()", "Behavior is the same as on regular tensors.", "torch.logical_not()", "Behavior is the same as on regular tensors.", "torch.neg()", "Behavior is the same as on regular tensors.", "torch.sub()", "Supports elementwise subtraction of two nested tensors.", "torch.add()", "Supports elementwise addition of two nested tensors. Supports addition of a scalar to a nested tensor.", "torch.mul()", "Supports elementwise multiplication of two nested tensors. Supports multiplication of a nested tensor by a scalar.", "torch.select()", "Supports selecting along all dimensions.", "torch.clone()", "Behavior is the same as on regular tensors.", "torch.detach()", "Behavior is the same as on regular tensors.", "torch.unbind()", "Supports unbinding along dim=0 only.", "torch.reshape()", "Supports reshaping with size of dim=0 preserved (i.e. number of tensors nested cannot be changed). Unlike regular tensors, a size of -1 here means that the existing size is inherited. In particular, the only valid size for a irregular dimension is -1. Size inference is not implemented yet and hence for new dimensions the size cannot be -1.", "torch.Tensor.reshape_as()", "Similar constraint as for reshape.", "torch.transpose()", "Supports transposing of all dims except dim=0.", "torch.Tensor.view()", "Rules for the new shape are similar to that of reshape.", "torch.empty_like()", "Behavior is analogous to that of regular tensors; returns a new empty nested tensor (i.e. with uninitialized values) matching the nested structure of the input.", "torch.randn_like()", "Behavior is analogous to that of regular tensors; returns a new nested tensor with values randomly initialized according to a standard normal distribution matching the nested structure of the input.", "torch.zeros_like()", "Behavior is analogous to that of regular tensors; returns a new nested tensor with all zero values matching the nested structure of the input.", "torch.nn.LayerNorm()", "The normalized_shape argument is restricted to not extend into the irregular dimensions of the NestedTensor."]}, {"name": "torch.nested.as_nested_tensor()", "path": "nested#torch.nested.as_nested_tensor", "type": "Miscellaneous", "text": ["Constructs a nested tensor preserving autograd history from tensor_list a list of tensors.", "Note", "Tensors within the list are always copied by this function due to current nested tensor semantics.", "tensor_list (List[Tensor]) \u2013 a list of tensors with the same ndim", "Tensor", "Example:"]}, {"name": "torch.nested.nested_tensor()", "path": "nested#torch.nested.nested_tensor", "type": "Miscellaneous", "text": ["Constructs a nested tensor with no autograd history (also known as a \u201cleaf tensor\u201d, see Autograd mechanics) from tensor_list a list of tensors.", "Example:"]}, {"name": "torch.nested.to_padded_tensor()", "path": "nested#torch.nested.to_padded_tensor", "type": "Miscellaneous", "text": ["Returns a new (non-nested) Tensor by padding the input nested tensor. The leading entries will be filled with the nested data, while the trailing entries will be padded.", "Warning", "to_padded_tensor() always copies the underlying data, since the nested and the non-nested tensors differ in memory layout.", "padding (float) \u2013 The padding value for the trailing entries.", "Example:"]}, {"name": "torch.nextafter", "path": "generated/torch.nextafter", "type": "Torch", "text": ["Return the next floating-point value after input towards other, elementwise.", "The shapes of input and other must be broadcastable.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.nn", "path": "nn", "type": "Neuro Network", "text": ["These are the basic building blocks for graphs:", "torch.nn", "A kind of Tensor that is to be considered a module parameter.", "A parameter that is not initialized.", "A buffer that is not initialized.", "Base class for all neural network modules.", "A sequential container.", "Holds submodules in a list.", "Holds submodules in a dictionary.", "Holds parameters in a list.", "Holds parameters in a dictionary.", "Global Hooks For Module", "Registers a forward pre-hook common to all modules.", "Registers a global forward hook for all the modules", "Registers a backward hook common to all the modules.", "Registers a backward pre-hook common to all the modules.", "Registers a backward hook common to all the modules.", "Registers a buffer registration hook common to all modules.", "Registers a module registration hook common to all modules.", "Registers a parameter registration hook common to all modules.", "nn.Conv1d", "Applies a 1D convolution over an input signal composed of several input planes.", "nn.Conv2d", "Applies a 2D convolution over an input signal composed of several input planes.", "nn.Conv3d", "Applies a 3D convolution over an input signal composed of several input planes.", "nn.ConvTranspose1d", "Applies a 1D transposed convolution operator over an input image composed of several input planes.", "nn.ConvTranspose2d", "Applies a 2D transposed convolution operator over an input image composed of several input planes.", "nn.ConvTranspose3d", "Applies a 3D transposed convolution operator over an input image composed of several input planes.", "nn.LazyConv1d", "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size(1).", "nn.LazyConv2d", "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size(1).", "nn.LazyConv3d", "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size(1).", "nn.LazyConvTranspose1d", "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size(1).", "nn.LazyConvTranspose2d", "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size(1).", "nn.LazyConvTranspose3d", "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size(1).", "nn.Unfold", "Extracts sliding local blocks from a batched input tensor.", "nn.Fold", "Combines an array of sliding local blocks into a large containing tensor.", "nn.MaxPool1d", "Applies a 1D max pooling over an input signal composed of several input planes.", "nn.MaxPool2d", "Applies a 2D max pooling over an input signal composed of several input planes.", "nn.MaxPool3d", "Applies a 3D max pooling over an input signal composed of several input planes.", "nn.MaxUnpool1d", "Computes a partial inverse of MaxPool1d.", "nn.MaxUnpool2d", "Computes a partial inverse of MaxPool2d.", "nn.MaxUnpool3d", "Computes a partial inverse of MaxPool3d.", "nn.AvgPool1d", "Applies a 1D average pooling over an input signal composed of several input planes.", "nn.AvgPool2d", "Applies a 2D average pooling over an input signal composed of several input planes.", "nn.AvgPool3d", "Applies a 3D average pooling over an input signal composed of several input planes.", "nn.FractionalMaxPool2d", "Applies a 2D fractional max pooling over an input signal composed of several input planes.", "nn.FractionalMaxPool3d", "Applies a 3D fractional max pooling over an input signal composed of several input planes.", "nn.LPPool1d", "Applies a 1D power-average pooling over an input signal composed of several input planes.", "nn.LPPool2d", "Applies a 2D power-average pooling over an input signal composed of several input planes.", "nn.AdaptiveMaxPool1d", "Applies a 1D adaptive max pooling over an input signal composed of several input planes.", "nn.AdaptiveMaxPool2d", "Applies a 2D adaptive max pooling over an input signal composed of several input planes.", "nn.AdaptiveMaxPool3d", "Applies a 3D adaptive max pooling over an input signal composed of several input planes.", "nn.AdaptiveAvgPool1d", "Applies a 1D adaptive average pooling over an input signal composed of several input planes.", "nn.AdaptiveAvgPool2d", "Applies a 2D adaptive average pooling over an input signal composed of several input planes.", "nn.AdaptiveAvgPool3d", "Applies a 3D adaptive average pooling over an input signal composed of several input planes.", "nn.ReflectionPad1d", "Pads the input tensor using the reflection of the input boundary.", "nn.ReflectionPad2d", "Pads the input tensor using the reflection of the input boundary.", "nn.ReflectionPad3d", "Pads the input tensor using the reflection of the input boundary.", "nn.ReplicationPad1d", "Pads the input tensor using replication of the input boundary.", "nn.ReplicationPad2d", "Pads the input tensor using replication of the input boundary.", "nn.ReplicationPad3d", "Pads the input tensor using replication of the input boundary.", "nn.ZeroPad1d", "Pads the input tensor boundaries with zero.", "nn.ZeroPad2d", "Pads the input tensor boundaries with zero.", "nn.ZeroPad3d", "Pads the input tensor boundaries with zero.", "nn.ConstantPad1d", "Pads the input tensor boundaries with a constant value.", "nn.ConstantPad2d", "Pads the input tensor boundaries with a constant value.", "nn.ConstantPad3d", "Pads the input tensor boundaries with a constant value.", "nn.ELU", "Applies the Exponential Linear Unit (ELU) function, element-wise, as described in the paper: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs).", "nn.Hardshrink", "Applies the Hard Shrinkage (Hardshrink) function element-wise.", "nn.Hardsigmoid", "Applies the Hardsigmoid function element-wise.", "nn.Hardtanh", "Applies the HardTanh function element-wise.", "nn.Hardswish", "Applies the Hardswish function, element-wise, as described in the paper: Searching for MobileNetV3.", "nn.LeakyReLU", "Applies the element-wise function:", "nn.LogSigmoid", "Applies the element-wise function:", "nn.MultiheadAttention", "Allows the model to jointly attend to information from different representation subspaces as described in the paper: Attention Is All You Need.", "nn.PReLU", "Applies the element-wise function:", "nn.ReLU", "Applies the rectified linear unit function element-wise:", "nn.ReLU6", "Applies the element-wise function:", "nn.RReLU", "Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper:", "nn.SELU", "Applied element-wise, as:", "nn.CELU", "Applies the element-wise function:", "nn.GELU", "Applies the Gaussian Error Linear Units function:", "nn.Sigmoid", "Applies the element-wise function:", "nn.SiLU", "Applies the Sigmoid Linear Unit (SiLU) function, element-wise.", "nn.Mish", "Applies the Mish function, element-wise.", "nn.Softplus", "Applies the Softplus function Softplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x)) element-wise.", "nn.Softshrink", "Applies the soft shrinkage function elementwise:", "nn.Softsign", "Applies the element-wise function:", "nn.Tanh", "Applies the Hyperbolic Tangent (Tanh) function element-wise.", "nn.Tanhshrink", "Applies the element-wise function:", "nn.Threshold", "Thresholds each element of the input Tensor.", "nn.GLU", "Applies the gated linear unit function GLU(a,b)=a\u2297\u03c3(b){GLU}(a, b)= a \\otimes \\sigma(b) where aa is the first half of the input matrices and bb is the second half.", "nn.Softmin", "Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0, 1] and sum to 1.", "nn.Softmax", "Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1.", "nn.Softmax2d", "Applies SoftMax over features to each spatial location.", "nn.LogSoftmax", "Applies the log\u2061(Softmax(x))\\log(\\text{Softmax}(x)) function to an n-dimensional input Tensor.", "nn.AdaptiveLogSoftmaxWithLoss", "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou.", "nn.BatchNorm1d", "Applies Batch Normalization over a 2D or 3D input as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .", "nn.BatchNorm2d", "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .", "nn.BatchNorm3d", "Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .", "nn.LazyBatchNorm1d", "A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size(1).", "nn.LazyBatchNorm2d", "A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size(1).", "nn.LazyBatchNorm3d", "A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size(1).", "nn.GroupNorm", "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization", "nn.SyncBatchNorm", "Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .", "nn.InstanceNorm1d", "Applies Instance Normalization over a 2D (unbatched) or 3D (batched) input as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.", "nn.InstanceNorm2d", "Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.", "nn.InstanceNorm3d", "Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.", "nn.LazyInstanceNorm1d", "A torch.nn.InstanceNorm1d module with lazy initialization of the num_features argument of the InstanceNorm1d that is inferred from the input.size(1).", "nn.LazyInstanceNorm2d", "A torch.nn.InstanceNorm2d module with lazy initialization of the num_features argument of the InstanceNorm2d that is inferred from the input.size(1).", "nn.LazyInstanceNorm3d", "A torch.nn.InstanceNorm3d module with lazy initialization of the num_features argument of the InstanceNorm3d that is inferred from the input.size(1).", "nn.LayerNorm", "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization", "nn.LocalResponseNorm", "Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.", "nn.RNNBase", "Base class for RNN modules (RNN, LSTM, GRU).", "nn.RNN", "Applies a multi-layer Elman RNN with tanh\u2061\\tanh or ReLU\\text{ReLU} non-linearity to an input sequence.", "nn.LSTM", "Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.", "nn.GRU", "Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.", "nn.RNNCell", "An Elman RNN cell with tanh or ReLU non-linearity.", "nn.LSTMCell", "A long short-term memory (LSTM) cell.", "nn.GRUCell", "A gated recurrent unit (GRU) cell", "nn.Transformer", "A transformer model.", "nn.TransformerEncoder", "TransformerEncoder is a stack of N encoder layers.", "nn.TransformerDecoder", "TransformerDecoder is a stack of N decoder layers", "nn.TransformerEncoderLayer", "TransformerEncoderLayer is made up of self-attn and feedforward network.", "nn.TransformerDecoderLayer", "TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.", "nn.Identity", "A placeholder identity operator that is argument-insensitive.", "nn.Linear", "Applies a linear transformation to the incoming data: y=xAT+by = xA^T + b", "nn.Bilinear", "Applies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + b", "nn.LazyLinear", "A torch.nn.Linear module where in_features is inferred.", "nn.Dropout", "During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.", "nn.Dropout1d", "Randomly zero out entire channels (a channel is a 1D feature map, e.g., the jj-th channel of the ii-th sample in the batched input is a 1D tensor input[i,j]\\text{input}[i, j]).", "nn.Dropout2d", "Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jj-th channel of the ii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]).", "nn.Dropout3d", "Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jj-th channel of the ii-th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]).", "nn.AlphaDropout", "Applies Alpha Dropout over the input.", "nn.FeatureAlphaDropout", "Randomly masks out entire channels (a channel is a feature map, e.g.", "nn.Embedding", "A simple lookup table that stores embeddings of a fixed dictionary and size.", "nn.EmbeddingBag", "Computes sums or means of 'bags' of embeddings, without instantiating the intermediate embeddings.", "nn.CosineSimilarity", "Returns cosine similarity between x1x_1 and x2x_2, computed along dim.", "nn.PairwiseDistance", "Computes the pairwise distance between input vectors, or between columns of input matrices.", "nn.L1Loss", "Creates a criterion that measures the mean absolute error (MAE) between each element in the input xx and target yy.", "nn.MSELoss", "Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input xx and target yy.", "nn.CrossEntropyLoss", "This criterion computes the cross entropy loss between input logits and target.", "nn.CTCLoss", "The Connectionist Temporal Classification loss.", "nn.NLLLoss", "The negative log likelihood loss.", "nn.PoissonNLLLoss", "Negative log likelihood loss with Poisson distribution of target.", "nn.GaussianNLLLoss", "Gaussian negative log likelihood loss.", "nn.KLDivLoss", "The Kullback-Leibler divergence loss.", "nn.BCELoss", "Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities:", "nn.BCEWithLogitsLoss", "This loss combines a Sigmoid layer and the BCELoss in one single class.", "nn.MarginRankingLoss", "Creates a criterion that measures the loss given inputs x1x1, x2x2, two 1D mini-batch or 0D Tensors, and a label 1D mini-batch or 0D Tensor yy (containing 1 or -1).", "nn.HingeEmbeddingLoss", "Measures the loss given an input tensor xx and a labels tensor yy (containing 1 or -1).", "nn.MultiLabelMarginLoss", "Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input xx (a 2D mini-batch Tensor) and output yy (which is a 2D Tensor of target class indices).", "nn.HuberLoss", "Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.", "nn.SmoothL1Loss", "Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.", "nn.SoftMarginLoss", "Creates a criterion that optimizes a two-class classification logistic loss between input tensor xx and target tensor yy (containing 1 or -1).", "nn.MultiLabelSoftMarginLoss", "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input xx and target yy of size (N,C)(N, C).", "nn.CosineEmbeddingLoss", "Creates a criterion that measures the loss given input tensors x1x_1, x2x_2 and a Tensor label yy with values 1 or -1.", "nn.MultiMarginLoss", "Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input xx (a 2D mini-batch Tensor) and output yy (which is a 1D tensor of target class indices, 0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-1):", "nn.TripletMarginLoss", "Creates a criterion that measures the triplet loss given an input tensors x1x1, x2x2, x3x3 and a margin with a value greater than 00.", "nn.TripletMarginWithDistanceLoss", "Creates a criterion that measures the triplet loss given input tensors aa, pp, and nn (representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\"distance function\") used to compute the relationship between the anchor and positive example (\"positive distance\") and the anchor and negative example (\"negative distance\").", "nn.PixelShuffle", "Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r), where r is an upscale factor.", "nn.PixelUnshuffle", "Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W), where r is a downscale factor.", "nn.Upsample", "Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.", "nn.UpsamplingNearest2d", "Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels.", "nn.UpsamplingBilinear2d", "Applies a 2D bilinear upsampling to an input signal composed of several input channels.", "nn.ChannelShuffle", "Divide the channels in a tensor of shape (\u2217,C,H,W)(*, C , H, W) into g groups and rearrange them as (\u2217,Cg,g,H,W)(*, C \\frac g, g, H, W), while keeping the original tensor shape.", "nn.DataParallel", "Implements data parallelism at the module level.", "nn.parallel.DistributedDataParallel", "Implements distributed data parallelism that is based on torch.distributed package at the module level.", "From the torch.nn.utils module", "Clips gradient norm of an iterable of parameters.", "Clips gradient of an iterable of parameters at specified value.", "Convert parameters to one vector", "Convert one vector to the parameters", "prune.BasePruningMethod", "Abstract base class for creation of new pruning techniques.", "prune.PruningContainer", "Container holding a sequence of pruning methods for iterative pruning.", "prune.Identity", "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.", "prune.RandomUnstructured", "Prune (currently unpruned) units in a tensor at random.", "prune.L1Unstructured", "Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.", "prune.RandomStructured", "Prune entire (currently unpruned) channels in a tensor at random.", "prune.LnStructured", "Prune entire (currently unpruned) channels in a tensor based on their Ln-norm.", "prune.CustomFromMask", "prune.identity", "Applies pruning reparametrization to the tensor corresponding to the parameter called name in module without actually pruning any units.", "prune.random_unstructured", "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units selected at random.", "prune.l1_unstructured", "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units with the lowest L1-norm.", "prune.random_structured", "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim selected at random.", "prune.ln_structured", "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim with the lowest Ln-norm.", "prune.global_unstructured", "Globally prunes tensors corresponding to all parameters in parameters by applying the specified pruning_method.", "prune.custom_from_mask", "Prunes tensor corresponding to parameter called name in module by applying the pre-computed mask in mask.", "prune.remove", "Removes the pruning reparameterization from a module and the pruning method from the forward hook.", "prune.is_pruned", "Check whether module is pruned by looking for forward_pre_hooks in its modules that inherit from the BasePruningMethod.", "Applies weight normalization to a parameter in the given module.", "Removes the weight normalization reparameterization from a module.", "Applies spectral normalization to a parameter in the given module.", "Removes the spectral normalization reparameterization from a module.", "Given a module class object and args / kwargs, instantiates the module without initializing parameters / buffers.", "Parametrizations implemented using the new parametrization functionality in torch.nn.utils.parameterize.register_parametrization().", "parametrizations.orthogonal", "Applies an orthogonal or unitary parametrization to a matrix or a batch of matrices.", "parametrizations.spectral_norm", "Applies spectral normalization to a parameter in the given module.", "Utility functions to parametrize Tensors on existing Modules. Note that these functions can be used to parametrize a given Parameter or Buffer given a specific function that maps from an input space to the parametrized space. They are not parameterizations that would transform an object into a parameter. See the Parametrizations tutorial for more information on how to implement your own parametrizations.", "parametrize.register_parametrization", "Adds a parametrization to a tensor in a module.", "parametrize.remove_parametrizations", "Removes the parametrizations on a tensor in a module.", "parametrize.cached", "Context manager that enables the caching system within parametrizations registered with register_parametrization().", "parametrize.is_parametrized", "Returns True if module has an active parametrization.", "parametrize.ParametrizationList", "A sequential container that holds and manages the original or original0, original1, .", "Utility functions to calls a given Module in a stateless manner.", "stateless.functional_call", "Performs a functional call on the module by replacing the module parameters and buffers with the provided ones.", "Utility functions in other modules", "nn.utils.rnn.PackedSequence", "Holds the data and list of batch_sizes of a packed sequence.", "nn.utils.rnn.pack_padded_sequence", "Packs a Tensor containing padded sequences of variable length.", "nn.utils.rnn.pad_packed_sequence", "Pads a packed batch of variable length sequences.", "nn.utils.rnn.pad_sequence", "Pad a list of variable length Tensors with padding_value", "nn.utils.rnn.pack_sequence", "Packs a list of variable length Tensors", "nn.utils.rnn.unpack_sequence", "Unpacks PackedSequence into a list of variable length Tensors", "nn.utils.rnn.unpad_sequence", "Unpad padded Tensor into a list of variable length Tensors", "nn.Flatten", "Flattens a contiguous range of dims into a tensor.", "nn.Unflatten", "Unflattens a tensor dim expanding it to a desired shape.", "Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than floating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to the Quantization documentation.", "nn.modules.lazy.LazyModuleMixin", "A mixin for modules that lazily initialize parameters, also known as \"lazy modules.\""]}, {"name": "torch.nn.AdaptiveAvgPool1d", "path": "generated/torch.nn.adaptiveavgpool1d", "type": "Neuro Network", "text": ["Applies a 1D adaptive average pooling over an input signal composed of several input planes.", "The output size is LoutL_{out}, for any input size. The number of output features is equal to the number of input planes.", "output_size (Union[int, Tuple[int]]) \u2013 the target output size LoutL_{out}."]}, {"name": "torch.nn.AdaptiveAvgPool2d", "path": "generated/torch.nn.adaptiveavgpool2d", "type": "Neuro Network", "text": ["Applies a 2D adaptive average pooling over an input signal composed of several input planes.", "The output is of size H x W, for any input size. The number of output features is equal to the number of input planes.", "output_size (Union[int, None, Tuple[Optional[int], Optional[int]]]) \u2013 the target output size of the image of the form H x W. Can be a tuple (H, W) or a single H for a square image H x H. H and W can be either a int, or None which means the size will be the same as that of the input."]}, {"name": "torch.nn.AdaptiveAvgPool3d", "path": "generated/torch.nn.adaptiveavgpool3d", "type": "Neuro Network", "text": ["Applies a 3D adaptive average pooling over an input signal composed of several input planes.", "The output is of size D x H x W, for any input size. The number of output features is equal to the number of input planes.", "output_size (Union[int, None, Tuple[Optional[int], Optional[int], Optional[int]]]) \u2013 the target output size of the form D x H x W. Can be a tuple (D, H, W) or a single number D for a cube D x D x D. D, H and W can be either a int, or None which means the size will be the same as that of the input."]}, {"name": "torch.nn.AdaptiveLogSoftmaxWithLoss", "path": "generated/torch.nn.adaptivelogsoftmaxwithloss", "type": "Neuro Network", "text": ["Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou.", "Adaptive softmax is an approximate strategy for training models with large output spaces. It is most effective when the label distribution is highly imbalanced, for example in natural language modelling, where the word frequency distribution approximately follows the Zipf\u2019s law.", "Adaptive softmax partitions the labels into several clusters, according to their frequency. These clusters may contain different number of targets each. Additionally, clusters containing less frequent labels assign lower dimensional embeddings to those labels, which speeds up the computation. For each minibatch, only clusters for which at least one target is present are evaluated.", "The idea is that the clusters which are accessed frequently (like the first one, containing most frequent labels), should also be cheap to compute \u2013 that is, contain a small number of assigned labels.", "We highly recommend taking a look at the original paper for more details.", "Warning", "Labels passed as inputs to this module should be sorted according to their frequency. This means that the most frequent label should be represented by the index 0, and the least frequent label should be represented by the index n_classes - 1.", "Note", "This module returns a NamedTuple with output and loss fields. See further documentation for details.", "Note", "To compute log-probabilities for all classes, the log_prob method can be used.", "NamedTuple with output and loss fields", "Computes log probabilities for all n_classes\\texttt{n\\_classes}", "input (Tensor) \u2013 a minibatch of examples", "log-probabilities of for each class cc in range 0<=c<=n_classes0 <= c <= \\texttt{n\\_classes}, where n_classes\\texttt{n\\_classes} is a parameter passed to AdaptiveLogSoftmaxWithLoss constructor.", "Tensor", "This is equivalent to self.log_prob(input).argmax(dim=1), but is more efficient in some cases.", "input (Tensor) \u2013 a minibatch of examples", "a class with the highest probability for each example", "output (Tensor)"]}, {"name": "torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob()", "path": "generated/torch.nn.adaptivelogsoftmaxwithloss#torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob", "type": "Neuro Network", "text": ["Computes log probabilities for all n_classes\\texttt{n\\_classes}", "input (Tensor) \u2013 a minibatch of examples", "log-probabilities of for each class cc in range 0<=c<=n_classes0 <= c <= \\texttt{n\\_classes}, where n_classes\\texttt{n\\_classes} is a parameter passed to AdaptiveLogSoftmaxWithLoss constructor.", "Tensor"]}, {"name": "torch.nn.AdaptiveLogSoftmaxWithLoss.predict()", "path": "generated/torch.nn.adaptivelogsoftmaxwithloss#torch.nn.AdaptiveLogSoftmaxWithLoss.predict", "type": "Neuro Network", "text": ["This is equivalent to self.log_prob(input).argmax(dim=1), but is more efficient in some cases.", "input (Tensor) \u2013 a minibatch of examples", "a class with the highest probability for each example", "output (Tensor)"]}, {"name": "torch.nn.AdaptiveMaxPool1d", "path": "generated/torch.nn.adaptivemaxpool1d", "type": "Neuro Network", "text": ["Applies a 1D adaptive max pooling over an input signal composed of several input planes.", "The output size is LoutL_{out}, for any input size. The number of output features is equal to the number of input planes."]}, {"name": "torch.nn.AdaptiveMaxPool2d", "path": "generated/torch.nn.adaptivemaxpool2d", "type": "Neuro Network", "text": ["Applies a 2D adaptive max pooling over an input signal composed of several input planes.", "The output is of size Hout\u00d7WoutH_{out} \\times W_{out}, for any input size. The number of output features is equal to the number of input planes."]}, {"name": "torch.nn.AdaptiveMaxPool3d", "path": "generated/torch.nn.adaptivemaxpool3d", "type": "Neuro Network", "text": ["Applies a 3D adaptive max pooling over an input signal composed of several input planes.", "The output is of size Dout\u00d7Hout\u00d7WoutD_{out} \\times H_{out} \\times W_{out}, for any input size. The number of output features is equal to the number of input planes."]}, {"name": "torch.nn.AlphaDropout", "path": "generated/torch.nn.alphadropout", "type": "Neuro Network", "text": ["Applies Alpha Dropout over the input.", "Alpha Dropout is a type of Dropout that maintains the self-normalizing property. For an input with zero mean and unit standard deviation, the output of Alpha Dropout maintains the original mean and standard deviation of the input. Alpha Dropout goes hand-in-hand with SELU activation function, which ensures that the outputs have zero mean and unit standard deviation.", "During training, it randomly masks some of the elements of the input tensor with probability p using samples from a bernoulli distribution. The elements to masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit standard deviation.", "During evaluation the module simply computes an identity function.", "More details can be found in the paper Self-Normalizing Neural Networks .", "Examples:"]}, {"name": "torch.nn.AvgPool1d", "path": "generated/torch.nn.avgpool1d", "type": "Neuro Network", "text": ["Applies a 1D average pooling over an input signal composed of several input planes.", "In the simplest case, the output value of the layer with input size (N,C,L)(N, C, L), output (N,C,Lout)(N, C, L_{out}) and kernel_size kk can be precisely described as:", "If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points.", "Note", "When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding or the input. Sliding windows that would start in the right padded region are ignored.", "The parameters kernel_size, stride, padding can each be an int or a one-element tuple.", "Output: (N,C,Lout)(N, C, L_{out}) or (C,Lout)(C, L_{out}), where", "Examples:"]}, {"name": "torch.nn.AvgPool2d", "path": "generated/torch.nn.avgpool2d", "type": "Neuro Network", "text": ["Applies a 2D average pooling over an input signal composed of several input planes.", "In the simplest case, the output value of the layer with input size (N,C,H,W)(N, C, H, W), output (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) and kernel_size (kH,kW)(kH, kW) can be precisely described as:", "If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points.", "Note", "When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding or the input. Sliding windows that would start in the right padded region are ignored.", "The parameters kernel_size, stride, padding can either be:", "Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) or (C,Hout,Wout)(C, H_{out}, W_{out}), where", "Examples:"]}, {"name": "torch.nn.AvgPool3d", "path": "generated/torch.nn.avgpool3d", "type": "Neuro Network", "text": ["Applies a 3D average pooling over an input signal composed of several input planes.", "In the simplest case, the output value of the layer with input size (N,C,D,H,W)(N, C, D, H, W), output (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out}) and kernel_size (kD,kH,kW)(kD, kH, kW) can be precisely described as:", "If padding is non-zero, then the input is implicitly zero-padded on all three sides for padding number of points.", "Note", "When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding or the input. Sliding windows that would start in the right padded region are ignored.", "The parameters kernel_size, stride can either be:", "Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out}) or (C,Dout,Hout,Wout)(C, D_{out}, H_{out}, W_{out}), where", "Examples:"]}, {"name": "torch.nn.BasePruningMethod", "path": "generated/torch.nn.utils.prune.basepruningmethod", "type": "Neuro Network", "text": ["Abstract base class for creation of new pruning techniques.", "Provides a skeleton for customization requiring the overriding of methods such as compute_mask() and apply().", "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the default_mask according to the specific pruning method recipe.", "mask to apply to t, of same dims as t", "mask (torch.Tensor)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.BatchNorm1d", "path": "generated/torch.nn.batchnorm1d", "type": "Neuro Network", "text": ["Applies Batch Normalization over a 2D or 3D input as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .", "The mean and standard-deviation are calculated per-dimension over the mini-batches and \u03b3\\gamma and \u03b2\\beta are learnable parameter vectors of size C (where C is the number of features or channels of the input). By default, the elements of \u03b3\\gamma are set to 1 and the elements of \u03b2\\beta are set to 0. At train time in the forward pass, the standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False). However, the value stored in the moving average of the standard-deviation is calculated via the unbiased estimator, equivalent to torch.var(input, unbiased=True).", "Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.", "If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.", "Note", "This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t, where x^\\hat{x} is the estimated statistic and xtx_t is the new observed value.", "Because the Batch Normalization is done over the C dimension, computing statistics on (N, L) slices, it\u2019s common terminology to call this Temporal Batch Normalization.", "Examples:"]}, {"name": "torch.nn.BatchNorm2d", "path": "generated/torch.nn.batchnorm2d", "type": "Neuro Network", "text": ["Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .", "The mean and standard-deviation are calculated per-dimension over the mini-batches and \u03b3\\gamma and \u03b2\\beta are learnable parameter vectors of size C (where C is the input size). By default, the elements of \u03b3\\gamma are set to 1 and the elements of \u03b2\\beta are set to 0. At train time in the forward pass, the standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False). However, the value stored in the moving average of the standard-deviation is calculated via the unbiased estimator, equivalent to torch.var(input, unbiased=True).", "Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.", "If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.", "Note", "This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t, where x^\\hat{x} is the estimated statistic and xtx_t is the new observed value.", "Because the Batch Normalization is done over the C dimension, computing statistics on (N, H, W) slices, it\u2019s common terminology to call this Spatial Batch Normalization.", "Examples:"]}, {"name": "torch.nn.BatchNorm3d", "path": "generated/torch.nn.batchnorm3d", "type": "Neuro Network", "text": ["Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .", "The mean and standard-deviation are calculated per-dimension over the mini-batches and \u03b3\\gamma and \u03b2\\beta are learnable parameter vectors of size C (where C is the input size). By default, the elements of \u03b3\\gamma are set to 1 and the elements of \u03b2\\beta are set to 0. At train time in the forward pass, the standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False). However, the value stored in the moving average of the standard-deviation is calculated via the unbiased estimator, equivalent to torch.var(input, unbiased=True).", "Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.", "If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.", "Note", "This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t, where x^\\hat{x} is the estimated statistic and xtx_t is the new observed value.", "Because the Batch Normalization is done over the C dimension, computing statistics on (N, D, H, W) slices, it\u2019s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization.", "Examples:"]}, {"name": "torch.nn.BCELoss", "path": "generated/torch.nn.bceloss", "type": "Neuro Network", "text": ["Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities:", "The unreduced (i.e. with reduction set to 'none') loss can be described as:", "where NN is the batch size. If reduction is not 'none' (default 'mean'), then", "This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets yy should be numbers between 0 and 1.", "Notice that if xnx_n is either 0 or 1, one of the log terms would be mathematically undefined in the above loss equation. PyTorch chooses to set log\u2061(0)=\u2212\u221e\\log (0) = -\\infty, since lim\u2061x\u21920log\u2061(x)=\u2212\u221e\\lim_{x\\to 0} \\log (x) = -\\infty. However, an infinite term in the loss equation is not desirable for several reasons.", "For one, if either yn=0y_n = 0 or (1\u2212yn)=0(1 - y_n) = 0, then we would be multiplying 0 with infinity. Secondly, if we have an infinite loss value, then we would also have an infinite term in our gradient, since lim\u2061x\u21920ddxlog\u2061(x)=\u221e\\lim_{x\\to 0} \\frac{d}{dx} \\log (x) = \\infty. This would make BCELoss\u2019s backward method nonlinear with respect to xnx_n, and using it for things like linear regression would not be straight-forward.", "Our solution is that BCELoss clamps its log function outputs to be greater than or equal to -100. This way, we can always have a finite loss value and a linear backward method.", "Examples:"]}, {"name": "torch.nn.BCEWithLogitsLoss", "path": "generated/torch.nn.bcewithlogitsloss", "type": "Neuro Network", "text": ["This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.", "The unreduced (i.e. with reduction set to 'none') loss can be described as:", "where NN is the batch size. If reduction is not 'none' (default 'mean'), then", "This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets t[i] should be numbers between 0 and 1.", "It\u2019s possible to trade off recall and precision by adding weights to positive examples. In the case of multi-label classification the loss can be described as:", "where cc is the class number (c>1c > 1 for multi-label binary classification, c=1c = 1 for single-label binary classification), nn is the number of the sample in the batch and pcp_c is the weight of the positive answer for the class cc.", "pc>1p_c > 1 increases the recall, pc<1p_c < 1 increases the precision.", "For example, if a dataset contains 100 positive and 300 negative examples of a single class, then pos_weight for the class should be equal to 300100=3\\frac{300}{100}=3. The loss would act as if the dataset contains 3\u00d7100=3003\\times 100=300 positive examples.", "Examples:", "Examples:"]}, {"name": "torch.nn.Bilinear", "path": "generated/torch.nn.bilinear", "type": "Neuro Network", "text": ["Applies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + b", "Examples:"]}, {"name": "torch.nn.CELU", "path": "generated/torch.nn.celu", "type": "Neuro Network", "text": ["Applies the element-wise function:", "More details can be found in the paper Continuously Differentiable Exponential Linear Units .", "Examples:"]}, {"name": "torch.nn.ChannelShuffle", "path": "generated/torch.nn.channelshuffle", "type": "Neuro Network", "text": ["Divide the channels in a tensor of shape (\u2217,C,H,W)(*, C , H, W) into g groups and rearrange them as (\u2217,Cg,g,H,W)(*, C \\frac g, g, H, W), while keeping the original tensor shape.", "groups (int) \u2013 number of groups to divide channels in.", "Examples:"]}, {"name": "torch.nn.ConstantPad1d", "path": "generated/torch.nn.constantpad1d", "type": "Neuro Network", "text": ["Pads the input tensor boundaries with a constant value.", "For N-dimensional padding, use torch.nn.functional.pad().", "padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in both boundaries. If a 2-tuple, uses (padding_left\\text{padding\\_left}, padding_right\\text{padding\\_right})", "Output: (C,Wout)(C, W_{out}) or (N,C,Wout)(N, C, W_{out}), where", "Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}", "Examples:"]}, {"name": "torch.nn.ConstantPad2d", "path": "generated/torch.nn.constantpad2d", "type": "Neuro Network", "text": ["Pads the input tensor boundaries with a constant value.", "For N-dimensional padding, use torch.nn.functional.pad().", "padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}, padding_right\\text{padding\\_right}, padding_top\\text{padding\\_top}, padding_bottom\\text{padding\\_bottom})", "Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) or (C,Hout,Wout)(C, H_{out}, W_{out}), where", "Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}", "Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}", "Examples:"]}, {"name": "torch.nn.ConstantPad3d", "path": "generated/torch.nn.constantpad3d", "type": "Neuro Network", "text": ["Pads the input tensor boundaries with a constant value.", "For N-dimensional padding, use torch.nn.functional.pad().", "padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding_left\\text{padding\\_left}, padding_right\\text{padding\\_right}, padding_top\\text{padding\\_top}, padding_bottom\\text{padding\\_bottom}, padding_front\\text{padding\\_front}, padding_back\\text{padding\\_back})", "Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out}) or (C,Dout,Hout,Wout)(C, D_{out}, H_{out}, W_{out}), where", "Dout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\_front} + \\text{padding\\_back}", "Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}", "Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}", "Examples:"]}, {"name": "torch.nn.Conv1d", "path": "generated/torch.nn.conv1d", "type": "Neuro Network", "text": ["Applies a 1D convolution over an input signal composed of several input planes.", "In the simplest case, the output value of the layer with input size (N,Cin,L)(N, C_{\\text{in}}, L) and output (N,Cout,Lout)(N, C_{\\text{out}}, L_{\\text{out}}) can be precisely described as:", "where \u22c6\\star is the valid cross-correlation operator, NN is a batch size, CC denotes a number of channels, LL is a length of signal sequence.", "This module supports TensorFloat32.", "On certain ROCm devices, when using float16 inputs this module will use different precision for backward.", "groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,", "Note", "When groups == in_channels and out_channels == K * in_channels, where K is a positive integer, this operation is also known as a \u201cdepthwise convolution\u201d.", "In other words, for an input of size (N,Cin,Lin)(N, C_{in}, L_{in}), a depthwise convolution with a depthwise multiplier K can be performed with the arguments (Cin=Cin,Cout=Cin\u00d7K,...,groups=Cin)(C_\\text{in}=C_\\text{in}, C_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in}).", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Note", "padding='valid' is the same as no padding. padding='same' pads the input so the output has the shape as the input. However, this mode doesn\u2019t support any stride values other than 1.", "Note", "This module supports complex data types i.e. complex32, complex64, complex128.", "Output: (N,Cout,Lout)(N, C_{out}, L_{out}) or (Cout,Lout)(C_{out}, L_{out}), where", "Examples:"]}, {"name": "torch.nn.Conv2d", "path": "generated/torch.nn.conv2d", "type": "Neuro Network", "text": ["Applies a 2D convolution over an input signal composed of several input planes.", "In the simplest case, the output value of the layer with input size (N,Cin,H,W)(N, C_{\\text{in}}, H, W) and output (N,Cout,Hout,Wout)(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}}) can be precisely described as:", "where \u22c6\\star is the valid 2D cross-correlation operator, NN is a batch size, CC denotes a number of channels, HH is a height of input planes in pixels, and WW is width in pixels.", "This module supports TensorFloat32.", "On certain ROCm devices, when using float16 inputs this module will use different precision for backward.", "groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,", "The parameters kernel_size, stride, padding, dilation can either be:", "Note", "When groups == in_channels and out_channels == K * in_channels, where K is a positive integer, this operation is also known as a \u201cdepthwise convolution\u201d.", "In other words, for an input of size (N,Cin,Lin)(N, C_{in}, L_{in}), a depthwise convolution with a depthwise multiplier K can be performed with the arguments (Cin=Cin,Cout=Cin\u00d7K,...,groups=Cin)(C_\\text{in}=C_\\text{in}, C_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in}).", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Note", "padding='valid' is the same as no padding. padding='same' pads the input so the output has the shape as the input. However, this mode doesn\u2019t support any stride values other than 1.", "Note", "This module supports complex data types i.e. complex32, complex64, complex128.", "Output: (N,Cout,Hout,Wout)(N, C_{out}, H_{out}, W_{out}) or (Cout,Hout,Wout)(C_{out}, H_{out}, W_{out}), where"]}, {"name": "torch.nn.Conv3d", "path": "generated/torch.nn.conv3d", "type": "Neuro Network", "text": ["Applies a 3D convolution over an input signal composed of several input planes.", "In the simplest case, the output value of the layer with input size (N,Cin,D,H,W)(N, C_{in}, D, H, W) and output (N,Cout,Dout,Hout,Wout)(N, C_{out}, D_{out}, H_{out}, W_{out}) can be precisely described as:", "where \u22c6\\star is the valid 3D cross-correlation operator", "This module supports TensorFloat32.", "On certain ROCm devices, when using float16 inputs this module will use different precision for backward.", "groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,", "The parameters kernel_size, stride, padding, dilation can either be:", "Note", "When groups == in_channels and out_channels == K * in_channels, where K is a positive integer, this operation is also known as a \u201cdepthwise convolution\u201d.", "In other words, for an input of size (N,Cin,Lin)(N, C_{in}, L_{in}), a depthwise convolution with a depthwise multiplier K can be performed with the arguments (Cin=Cin,Cout=Cin\u00d7K,...,groups=Cin)(C_\\text{in}=C_\\text{in}, C_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in}).", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Note", "padding='valid' is the same as no padding. padding='same' pads the input so the output has the shape as the input. However, this mode doesn\u2019t support any stride values other than 1.", "Note", "This module supports complex data types i.e. complex32, complex64, complex128.", "Output: (N,Cout,Dout,Hout,Wout)(N, C_{out}, D_{out}, H_{out}, W_{out}) or (Cout,Dout,Hout,Wout)(C_{out}, D_{out}, H_{out}, W_{out}), where", "Examples:"]}, {"name": "torch.nn.ConvTranspose1d", "path": "generated/torch.nn.convtranspose1d", "type": "Neuro Network", "text": ["Applies a 1D transposed convolution operator over an input image composed of several input planes.", "This module can be seen as the gradient of Conv1d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation as it does not compute a true inverse of convolution). For more information, see the visualizations here and the Deconvolutional Networks paper.", "This module supports TensorFloat32.", "On certain ROCm devices, when using float16 inputs this module will use different precision for backward.", "groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,", "Note", "The padding argument effectively adds dilation * (kernel_size - 1) - padding amount of zero padding to both sizes of the input. This is set so that when a Conv1d and a ConvTranspose1d are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when stride > 1, Conv1d maps multiple input shapes to the same output shape. output_padding is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that output_padding is only used to find output shape, but does not actually add zero-padding to output.", "Note", "In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic =\nTrue. Please see the notes on Reproducibility for background.", "Output: (N,Cout,Lout)(N, C_{out}, L_{out}) or (Cout,Lout)(C_{out}, L_{out}), where"]}, {"name": "torch.nn.ConvTranspose2d", "path": "generated/torch.nn.convtranspose2d", "type": "Neuro Network", "text": ["Applies a 2D transposed convolution operator over an input image composed of several input planes.", "This module can be seen as the gradient of Conv2d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation as it does not compute a true inverse of convolution). For more information, see the visualizations here and the Deconvolutional Networks paper.", "This module supports TensorFloat32.", "On certain ROCm devices, when using float16 inputs this module will use different precision for backward.", "groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,", "The parameters kernel_size, stride, padding, output_padding can either be:", "Note", "The padding argument effectively adds dilation * (kernel_size - 1) - padding amount of zero padding to both sizes of the input. This is set so that when a Conv2d and a ConvTranspose2d are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when stride > 1, Conv2d maps multiple input shapes to the same output shape. output_padding is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that output_padding is only used to find output shape, but does not actually add zero-padding to output.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Examples:"]}, {"name": "torch.nn.ConvTranspose3d", "path": "generated/torch.nn.convtranspose3d", "type": "Neuro Network", "text": ["Applies a 3D transposed convolution operator over an input image composed of several input planes. The transposed convolution operator multiplies each input value element-wise by a learnable kernel, and sums over the outputs from all input feature planes.", "This module can be seen as the gradient of Conv3d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation as it does not compute a true inverse of convolution). For more information, see the visualizations here and the Deconvolutional Networks paper.", "This module supports TensorFloat32.", "On certain ROCm devices, when using float16 inputs this module will use different precision for backward.", "groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,", "The parameters kernel_size, stride, padding, output_padding can either be:", "Note", "The padding argument effectively adds dilation * (kernel_size - 1) - padding amount of zero padding to both sizes of the input. This is set so that when a Conv3d and a ConvTranspose3d are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when stride > 1, Conv3d maps multiple input shapes to the same output shape. output_padding is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that output_padding is only used to find output shape, but does not actually add zero-padding to output.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Examples:"]}, {"name": "torch.nn.CosineEmbeddingLoss", "path": "generated/torch.nn.cosineembeddingloss", "type": "Neuro Network", "text": ["Creates a criterion that measures the loss given input tensors x1x_1, x2x_2 and a Tensor label yy with values 1 or -1. This is used for measuring whether two inputs are similar or dissimilar, using the cosine similarity, and is typically used for learning nonlinear embeddings or semi-supervised learning.", "The loss function for each sample is:"]}, {"name": "torch.nn.CosineSimilarity", "path": "generated/torch.nn.cosinesimilarity", "type": "Neuro Network", "text": ["Returns cosine similarity between x1x_1 and x2x_2, computed along dim.", "and broadcastable with x1 at other dimensions."]}, {"name": "torch.nn.CrossEntropyLoss", "path": "generated/torch.nn.crossentropyloss", "type": "Neuro Network", "text": ["This criterion computes the cross entropy loss between input logits and target.", "It is useful when training a classification problem with C classes. If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set.", "The input is expected to contain the unnormalized logits for each class (which do not need to be positive or sum to 1, in general). input has to be a Tensor of size (C)(C) for unbatched input, (minibatch,C)(minibatch, C) or (minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K) with K\u22651K \\geq 1 for the K-dimensional case. The last being useful for higher dimension inputs, such as computing cross entropy loss per-pixel for 2D images.", "The target that this criterion expects should contain either:", "Class indices in the range [0,C)[0, C) where CC is the number of classes; if ignore_index is specified, this loss also accepts this class index (this index may not necessarily be in the class range). The unreduced (i.e. with reduction set to 'none') loss for this case can be described as:", "where xx is the input, yy is the target, ww is the weight, CC is the number of classes, and NN spans the minibatch dimension as well as d1,...,dkd_1, ..., d_k for the K-dimensional case. If reduction is not 'none' (default 'mean'), then", "Note that this case is equivalent to applying LogSoftmax on an input, followed by NLLLoss.", "Probabilities for each class; useful when labels beyond a single class per minibatch item are required, such as for blended labels, label smoothing, etc. The unreduced (i.e. with reduction set to 'none') loss for this case can be described as:", "where xx is the input, yy is the target, ww is the weight, CC is the number of classes, and NN spans the minibatch dimension as well as d1,...,dkd_1, ..., d_k for the K-dimensional case. If reduction is not 'none' (default 'mean'), then", "Note", "The performance of this criterion is generally better when target contains class indices, as this allows for optimized computation. Consider providing target as class probabilities only when a single class label per minibatch item is too restrictive.", "where:", "Examples:"]}, {"name": "torch.nn.CTCLoss", "path": "generated/torch.nn.ctcloss", "type": "Neuro Network", "text": ["The Connectionist Temporal Classification loss.", "Calculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the probability of possible alignments of input to target, producing a loss value which is differentiable with respect to each input node. The alignment of input to target is assumed to be \u201cmany-to-one\u201d, which limits the length of the target sequence such that it must be \u2264\\leq the input length.", "Examples:", "A. Graves et al.: Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks: https://www.cs.toronto.edu/~graves/icml_2006.pdf", "Note", "In order to use CuDNN, the following must be satisfied: targets must be in concatenated format, all input_lengths must be T. blank=0blank=0, target_lengths \u2264256\\leq 256, the integer arguments must be of dtype torch.int32.", "The regular implementation uses the (more common in PyTorch) torch.long dtype.", "Note", "In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic =\nTrue. Please see the notes on Reproducibility for background."]}, {"name": "torch.nn.CustomFromMask", "path": "generated/torch.nn.utils.prune.customfrommask", "type": "Neuro Network", "text": ["Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.DataParallel", "path": "generated/torch.nn.dataparallel", "type": "Neuro Network", "text": ["Implements data parallelism at the module level.", "This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension (other objects will be copied once per device). In the forward pass, the module is replicated on each device, and each replica handles a portion of the input. During the backwards pass, gradients from each replica are summed into the original module.", "The batch size should be larger than the number of GPUs used.", "Warning", "It is recommended to use DistributedDataParallel, instead of this class, to do multi-GPU training, even if there is only a single node. See: Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel and Distributed Data Parallel.", "Arbitrary positional and keyword inputs are allowed to be passed into DataParallel but some types are specially handled. tensors will be scattered on dim specified (default 0). tuple, list and dict types will be shallow copied. The other types will be shared among different threads and can be corrupted if written to in the model\u2019s forward pass.", "The parallelized module must have its parameters and buffers on device_ids[0] before running this DataParallel module.", "Warning", "In each forward, module is replicated on each device, so any updates to the running module in forward will be lost. For example, if module has a counter attribute that is incremented in each forward, it will always stay at the initial value because the update is done on the replicas which are destroyed after forward. However, DataParallel guarantees that the replica on device[0] will have its parameters and buffers sharing storage with the base parallelized module. So in-place updates to the parameters or buffers on device[0] will be recorded. E.g., BatchNorm2d and spectral_norm() rely on this behavior to update the buffers.", "Warning", "Forward and backward hooks defined on module and its submodules will be invoked len(device_ids) times, each with inputs located on a particular device. Particularly, the hooks are only guaranteed to be executed in correct order with respect to operations on corresponding devices. For example, it is not guaranteed that hooks set via register_forward_pre_hook() be executed before all len(device_ids) forward() calls, but that each such hook be executed before the corresponding forward() call of that device.", "Warning", "When module returns a scalar (i.e., 0-dimensional tensor) in forward(), this wrapper will return a vector of length equal to number of devices used in data parallelism, containing the result from each device.", "Note", "There is a subtlety in using the pack sequence -> recurrent network -> unpack sequence pattern in a Module wrapped in DataParallel. See My recurrent network doesn\u2019t work with data parallelism section in FAQ for details.", "module (Module) \u2013 the module to be parallelized", "Example:"]}, {"name": "torch.nn.DistributedDataParallel", "path": "generated/torch.nn.parallel.distributeddataparallel", "type": "Neuro Network", "text": ["Implements distributed data parallelism that is based on torch.distributed package at the module level.", "This container provides data parallelism by synchronizing gradients across each model replica. The devices to synchronize across are specified by the input process_group, which is the entire world by default. Note that DistributedDataParallel does not chunk or otherwise shard the input across participating GPUs; the user is responsible for defining how to do so, for example through the use of a DistributedSampler.", "See also: Basics and Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel. The same constraints on input as in torch.nn.DataParallel apply.", "Creation of this class requires that torch.distributed to be already initialized, by calling torch.distributed.init_process_group().", "DistributedDataParallel is proven to be significantly faster than torch.nn.DataParallel for single-node multi-GPU data parallel training.", "To use DistributedDataParallel on a host with N GPUs, you should spawn up N processes, ensuring that each process exclusively works on a single GPU from 0 to N-1. This can be done by either setting CUDA_VISIBLE_DEVICES for every process or by calling:", "where i is from 0 to N-1. In each process, you should refer the following to construct this module:", "In order to spawn up multiple processes per node, you can use either torch.distributed.launch or torch.multiprocessing.spawn.", "Note", "Please refer to PyTorch Distributed Overview for a brief introduction to all features related to distributed training.", "Note", "DistributedDataParallel can be used in conjunction with torch.distributed.optim.ZeroRedundancyOptimizer to reduce per-rank optimizer states memory footprint. Please refer to ZeroRedundancyOptimizer recipe for more details.", "Note", "nccl backend is currently the fastest and highly recommended backend when using GPUs. This applies to both single-node and multi-node distributed training.", "Note", "This module also supports mixed-precision distributed training. This means that your model can have different types of parameters such as mixed types of fp16 and fp32, the gradient reduction on these mixed types of parameters will just work fine.", "Note", "If you use torch.save on one process to checkpoint the module, and torch.load on some other processes to recover it, make sure that map_location is configured properly for every process. Without map_location, torch.load would recover the module to devices where the module was saved from.", "Note", "When a model is trained on M nodes with batch=N, the gradient will be M times smaller when compared to the same model trained on a single node with batch=M*N if the loss is summed (NOT averaged as usual) across instances in a batch (because the gradients between different nodes are averaged). You should take this into consideration when you want to obtain a mathematically equivalent training process compared to the local training counterpart. But in most cases, you can just treat a DistributedDataParallel wrapped model, a DataParallel wrapped model and an ordinary model on a single GPU as the same (E.g. using the same learning rate for equivalent batch size).", "Note", "Parameters are never broadcast between processes. The module performs an all-reduce step on gradients and assumes that they will be modified by the optimizer in all processes in the same way. Buffers (e.g. BatchNorm stats) are broadcast from the module in process of rank 0, to all other replicas in the system in every iteration.", "Note", "If you are using DistributedDataParallel in conjunction with the Distributed RPC Framework, you should always use torch.distributed.autograd.backward() to compute gradients and torch.distributed.optim.DistributedOptimizer for optimizing parameters.", "Example:", "Note", "DistributedDataParallel currently offers limited support for gradient checkpointing with torch.utils.checkpoint(). DDP will work as expected when there are no unused parameters in the model and each layer is checkpointed at most once (make sure you are not passing find_unused_parameters=True to DDP). We currently do not support the case where a layer is checkpointed multiple times, or when there unused parameters in the checkpointed model.", "Note", "To let a non-DDP model load a state dict from a DDP model, consume_prefix_in_state_dict_if_present() needs to be applied to strip the prefix \u201cmodule.\u201d in the DDP state dict before loading.", "Warning", "Constructor, forward method, and differentiation of the output (or a function of the output of this module) are distributed synchronization points. Take that into account in case different processes might be executing different code.", "Warning", "This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later. Same applies to buffers.", "Warning", "This module assumes all parameters are registered in the model of each distributed processes are in the same order. The module itself will conduct gradient allreduce following the reverse order of the registered parameters of the model. In other words, it is users\u2019 responsibility to ensure that each distributed process has the exact same model and thus the exact same parameter registration order.", "Warning", "This module allows parameters with non-rowmajor-contiguous strides. For example, your model may contain some parameters whose torch.memory_format is torch.contiguous_format and others whose format is torch.channels_last. However, corresponding parameters in different processes must have the same strides.", "Warning", "This module doesn\u2019t work with torch.autograd.grad() (i.e. it will only work if gradients are to be accumulated in .grad attributes of parameters).", "Warning", "If you plan on using this module with a nccl backend or a gloo backend (that uses Infiniband), together with a DataLoader that uses multiple workers, please change the multiprocessing start method to forkserver (Python 3 only) or spawn. Unfortunately Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will likely experience deadlocks if you don\u2019t change this setting.", "Warning", "You should never try to change your model\u2019s parameters after wrapping up your model with DistributedDataParallel. Because, when wrapping up your model with DistributedDataParallel, the constructor of DistributedDataParallel will register the additional gradient reduction functions on all the parameters of the model itself at the time of construction. If you change the model\u2019s parameters afterwards, gradient reduction functions no longer match the correct set of parameters.", "Warning", "Using DistributedDataParallel in conjunction with the Distributed RPC Framework is experimental and subject to change.", "device_ids (list of int or torch.device) \u2013 ", "CUDA devices. 1) For single-device modules, device_ids can contain exactly one device id, which represents the only CUDA device where the input module corresponding to this process resides. Alternatively, device_ids can also be None. 2) For multi-device modules and CPU modules, device_ids must be None.", "When device_ids is None for both cases, both the input data for the forward pass and the actual module must be placed on the correct device. (default: None)", "static_graph (bool) \u2013 ", "When set to True, DDP knows the trained graph is static. Static graph means 1) The set of used and unused parameters will not change during the whole training loop; in this case, it does not matter whether users set find_unused_parameters = True or not. 2) How the graph is trained will not change during the whole training loop (meaning there is no control flow depending on iterations). When static_graph is set to be True, DDP will support cases that can not be supported in the past: 1) Reentrant backwards. 2) Activation checkpointing multiple times. 3) Activation checkpointing when model has unused parameters. 4) There are model parameters that are outside of forward function. 5) Potentially improve performance when there are unused parameters, as DDP will not search graph in each iteration to detect unused parameters when static_graph is set to be True. To check whether you can set static_graph to be True, one way is to check ddp logging data at the end of your previous model training, if ddp_logging_data.get(\"can_set_static_graph\") == True, mostly you can set static_graph = True as well.", "module (Module) \u2013 the module to be parallelized.", "Example:", "A context manager to be used in conjunction with an instance of torch.nn.parallel.DistributedDataParallel to be able to train with uneven inputs across participating processes.", "This context manager will keep track of already-joined DDP processes, and \u201cshadow\u201d the forward and backward passes by inserting collective communication operations to match with the ones created by non-joined DDP processes. This will ensure each collective call has a corresponding call by already-joined DDP processes, preventing hangs or errors that would otherwise happen when training with uneven inputs across processes. Alternatively, if the flag throw_on_early_termination is specified to be True, all trainers will throw an error once one rank runs out of inputs, allowing these errors to be caught and handled according to application logic.", "Once all DDP processes have joined, the context manager will broadcast the model corresponding to the last joined process to all processes to ensure the model is the same across all processes (which is guaranteed by DDP).", "To use this to enable training with uneven inputs across processes, simply wrap this context manager around your training loop. No further modifications to the model or data loading is required.", "Warning", "If the model or training loop this context manager is wrapped around has additional distributed collective operations, such as SyncBatchNorm in the model\u2019s forward pass, then the flag throw_on_early_termination must be enabled. This is because this context manager is not aware of non-DDP collective communication. This flag will cause all ranks to throw when any one rank exhausts inputs, allowing these errors to be caught and recovered from across all ranks.", "Example:", "Returns the DDP join hook, which enables training on uneven inputs by shadowing the collective communications in the forward and backward passes.", "kwargs (dict) \u2013 a dict containing any keyword arguments to modify the behavior of the join hook at run time; all Joinable instances sharing the same join context manager are forwarded the same value for kwargs.", "If True, then gradients are divided by the initial world size that DDP was launched with. If False, then gradients are divided by the effective world size (i.e. the number of non-joined processes), meaning that the uneven inputs contribute more toward the global gradient. Typically, this should be set to True if the degree of unevenness is small but can be set to False in extreme cases for possibly better results. Default is True.", "A context manager to disable gradient synchronizations across DDP processes. Within this context, gradients will be accumulated on module variables, which will later be synchronized in the first forward-backward pass exiting the context.", "Example:", "Warning", "The forward pass should be included inside the context manager, or else gradients will still be synchronized.", "Registers a communication hook which is an enhancement that provides a flexible hook to users where they can specify how DDP aggregates gradients across multiple workers.", "This hook would be very useful for researchers to try out new ideas. For example, this hook can be used to implement several algorithms like GossipGrad and gradient compression which involve different communication strategies for parameter syncs while running Distributed DataParallel training.", "state (object) \u2013 ", "Passed to the hook to maintain any state information during the training process. Examples include error feedback in gradient compression, peers to communicate with next in GossipGrad, etc.", "It is locally stored by each worker and shared by all the gradient tensors on the worker.", "hook (Callable) \u2013 ", "Callable with the following signature: hook(state: object, bucket: dist.GradBucket) -> torch.futures.Future[torch.Tensor]:", "This function is called once the bucket is ready. The hook can perform whatever processing is needed and return a Future indicating completion of any async work (ex: allreduce). If the hook doesn\u2019t perform any communication, it still must return a completed Future. The Future should hold the new value of grad bucket\u2019s tensors. Once a bucket is ready, c10d reducer would call this hook and use the tensors returned by the Future and copy grads to individual parameters. Note that the future\u2019s return type must be a single tensor.", "We also provide an API called get_future to retrieve a Future associated with the completion of c10d.ProcessGroup.Work. get_future is currently supported for NCCL and also supported for most operations on GLOO and MPI, except for peer to peer operations (send/recv).", "Warning", "Grad bucket\u2019s tensors will not be predivided by world_size. User is responsible to divide by the world_size in case of operations like allreduce.", "Warning", "DDP communication hook can only be registered once and should be registered before calling backward.", "Warning", "The Future object that hook returns should contain a single tensor that has the same shape with the tensors inside grad bucket.", "Warning", "get_future API supports NCCL, and partially GLOO and MPI backends (no support for peer-to-peer operations like send/recv) and will return a torch.futures.Future.", "Below is an example of a noop hook that returns the same tensor.", "Below is an example of a Parallel SGD algorithm where gradients are encoded before allreduce, and then decoded after allreduce."]}, {"name": "torch.nn.Dropout", "path": "generated/torch.nn.dropout", "type": "Neuro Network", "text": ["During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call.", "This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper Improving neural networks by preventing co-adaptation of feature detectors .", "Furthermore, the outputs are scaled by a factor of 11\u2212p\\frac{1}{1-p} during training. This means that during evaluation the module simply computes an identity function.", "Examples:"]}, {"name": "torch.nn.Dropout1d", "path": "generated/torch.nn.dropout1d", "type": "Neuro Network", "text": ["Randomly zero out entire channels (a channel is a 1D feature map, e.g., the jj-th channel of the ii-th sample in the batched input is a 1D tensor input[i,j]\\text{input}[i, j]). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution.", "Usually the input comes from nn.Conv1d modules.", "As described in the paper Efficient Object Localization Using Convolutional Networks , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease.", "In this case, nn.Dropout1d() will help promote independence between feature maps and should be used instead.", "Examples:"]}, {"name": "torch.nn.Dropout2d", "path": "generated/torch.nn.dropout2d", "type": "Neuro Network", "text": ["Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jj-th channel of the ii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution.", "Usually the input comes from nn.Conv2d modules.", "As described in the paper Efficient Object Localization Using Convolutional Networks , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease.", "In this case, nn.Dropout2d() will help promote independence between feature maps and should be used instead.", "Warning", "Due to historical reasons, this class will perform 1D channel-wise dropout for 3D inputs (as done by nn.Dropout1d). Thus, it currently does NOT support inputs without a batch dimension of shape (C,H,W)(C, H, W). This behavior will change in a future release to interpret 3D inputs as no-batch-dim inputs. To maintain the old behavior, switch to nn.Dropout1d.", "Examples:"]}, {"name": "torch.nn.Dropout3d", "path": "generated/torch.nn.dropout3d", "type": "Neuro Network", "text": ["Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jj-th channel of the ii-th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution.", "Usually the input comes from nn.Conv3d modules.", "As described in the paper Efficient Object Localization Using Convolutional Networks , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease.", "In this case, nn.Dropout3d() will help promote independence between feature maps and should be used instead.", "Examples:"]}, {"name": "torch.nn.ELU", "path": "generated/torch.nn.elu", "type": "Neuro Network", "text": ["Applies the Exponential Linear Unit (ELU) function, element-wise, as described in the paper: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs).", "ELU is defined as:", "Examples:"]}, {"name": "torch.nn.Embedding", "path": "generated/torch.nn.embedding", "type": "Neuro Network", "text": ["A simple lookup table that stores embeddings of a fixed dictionary and size.", "This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.", "weight (Tensor) \u2013 the learnable weights of the module of shape (num_embeddings, embedding_dim) initialized from N(0,1)\\mathcal{N}(0, 1)", "Note", "Keep in mind that only a limited number of optimizers support sparse gradients: currently it\u2019s optim.SGD (CUDA and CPU), optim.SparseAdam (CUDA and CPU) and optim.Adagrad (CPU)", "Note", "When max_norm is not None, Embedding\u2019s forward method will modify the weight tensor in-place. Since tensors needed for gradient computations cannot be modified in-place, performing a differentiable operation on Embedding.weight before calling Embedding\u2019s forward method requires cloning Embedding.weight when max_norm is not None. For example:", "Examples:", "Creates Embedding instance from given 2-dimensional FloatTensor.", "Examples:"]}, {"name": "torch.nn.Embedding.from_pretrained()", "path": "generated/torch.nn.embedding#torch.nn.Embedding.from_pretrained", "type": "Neuro Network", "text": ["Creates Embedding instance from given 2-dimensional FloatTensor.", "Examples:"]}, {"name": "torch.nn.EmbeddingBag", "path": "generated/torch.nn.embeddingbag", "type": "Neuro Network", "text": ["Computes sums or means of \u2018bags\u2019 of embeddings, without instantiating the intermediate embeddings.", "For bags of constant length, no per_sample_weights, no indices equal to padding_idx, and with 2D inputs, this class", "However, EmbeddingBag is much more time and memory efficient than using a chain of these operations.", "EmbeddingBag also supports per-sample weights as an argument to the forward pass. This scales the output of the Embedding before performing a weighted reduction as specified by mode. If per_sample_weights is passed, the only supported mode is \"sum\", which computes a weighted sum according to per_sample_weights.", "weight (Tensor) \u2013 the learnable weights of the module of shape (num_embeddings, embedding_dim) initialized from N(0,1)\\mathcal{N}(0, 1).", "Examples:", "Forward pass of EmbeddingBag.", "Tensor output shape of (B, embedding_dim).", "Tensor", "Note", "A few notes about input and offsets:", "Creates EmbeddingBag instance from given 2-dimensional FloatTensor.", "EmbeddingBag", "Examples:"]}, {"name": "torch.nn.EmbeddingBag.forward()", "path": "generated/torch.nn.embeddingbag#torch.nn.EmbeddingBag.forward", "type": "Neuro Network", "text": ["Forward pass of EmbeddingBag.", "Tensor output shape of (B, embedding_dim).", "Tensor", "Note", "A few notes about input and offsets:"]}, {"name": "torch.nn.EmbeddingBag.from_pretrained()", "path": "generated/torch.nn.embeddingbag#torch.nn.EmbeddingBag.from_pretrained", "type": "Neuro Network", "text": ["Creates EmbeddingBag instance from given 2-dimensional FloatTensor.", "EmbeddingBag", "Examples:"]}, {"name": "torch.nn.FeatureAlphaDropout", "path": "generated/torch.nn.featurealphadropout", "type": "Neuro Network", "text": ["Randomly masks out entire channels (a channel is a feature map, e.g. the jj-th channel of the ii-th sample in the batch input is a tensor input[i,j]\\text{input}[i, j]) of the input tensor). Instead of setting activations to zero, as in regular Dropout, the activations are set to the negative saturation value of the SELU activation function. More details can be found in the paper Self-Normalizing Neural Networks .", "Each element will be masked independently for each sample on every forward call with probability p using samples from a Bernoulli distribution. The elements to be masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit variance.", "Usually the input comes from nn.AlphaDropout modules.", "As described in the paper Efficient Object Localization Using Convolutional Networks , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease.", "In this case, nn.AlphaDropout() will help promote independence between feature maps and should be used instead.", "Examples:"]}, {"name": "torch.nn.Flatten", "path": "generated/torch.nn.flatten", "type": "Neuro Network", "text": ["Flattens a contiguous range of dims into a tensor. For use with Sequential. See torch.flatten() for details."]}, {"name": "torch.nn.Fold", "path": "generated/torch.nn.fold", "type": "Neuro Network", "text": ["Combines an array of sliding local blocks into a large containing tensor.", "Consider a batched input tensor containing sliding local blocks, e.g., patches of images, of shape (N,C\u00d7\u220f(kernel_size),L)(N, C \\times \\prod(\\text{kernel\\_size}), L), where NN is batch dimension, C\u00d7\u220f(kernel_size)C \\times \\prod(\\text{kernel\\_size}) is the number of values within a block (a block has \u220f(kernel_size)\\prod(\\text{kernel\\_size}) spatial locations each containing a CC-channeled vector), and LL is the total number of blocks. (This is exactly the same specification as the output shape of Unfold.) This operation combines these local blocks into the large output tensor of shape (N,C,output_size[0],output_size[1],\u2026)(N, C, \\text{output\\_size}[0], \\text{output\\_size}[1], \\dots) by summing the overlapping values. Similar to Unfold, the arguments must satisfy", "where dd is over all spatial dimensions.", "The padding, stride and dilation arguments specify how the sliding blocks are retrieved.", "Note", "Fold calculates each combined value in the resulting large tensor by summing all values from all containing blocks. Unfold extracts the values in the local blocks by copying from the large tensor. So, if the blocks overlap, they are not inverses of each other.", "In general, folding and unfolding operations are related as follows. Consider Fold and Unfold instances created with the same parameters:", "Then for any (supported) input tensor the following equality holds:", "where divisor is a tensor that depends only on the shape and dtype of the input:", "When the divisor tensor contains no zero elements, then fold and unfold operations are inverses of each other (up to constant divisor).", "Warning", "Currently, only unbatched (3D) or batched (4D) image-like output tensors are supported.", "Examples:"]}, {"name": "torch.nn.FractionalMaxPool2d", "path": "generated/torch.nn.fractionalmaxpool2d", "type": "Neuro Network", "text": ["Applies a 2D fractional max pooling over an input signal composed of several input planes.", "Fractional MaxPooling is described in detail in the paper Fractional MaxPooling by Ben Graham", "The max-pooling operation is applied in kH\u00d7kWkH \\times kW regions by a stochastic step size determined by the target output size. The number of output features is equal to the number of input planes.", "Note", "Exactly one of output_size or output_ratio must be defined."]}, {"name": "torch.nn.FractionalMaxPool3d", "path": "generated/torch.nn.fractionalmaxpool3d", "type": "Neuro Network", "text": ["Applies a 3D fractional max pooling over an input signal composed of several input planes.", "Fractional MaxPooling is described in detail in the paper Fractional MaxPooling by Ben Graham", "The max-pooling operation is applied in kT\u00d7kH\u00d7kWkT \\times kH \\times kW regions by a stochastic step size determined by the target output size. The number of output features is equal to the number of input planes.", "Note", "Exactly one of output_size or output_ratio must be defined."]}, {"name": "torch.nn.functional", "path": "nn.functional", "type": "NN Functions", "text": ["Applies a 1D convolution over an input signal composed of several input planes.", "Applies a 2D convolution over an input image composed of several input planes.", "Applies a 3D convolution over an input image composed of several input planes.", "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \"deconvolution\".", "Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \"deconvolution\".", "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \"deconvolution\"", "Extracts sliding local blocks from a batched input tensor.", "Combines an array of sliding local blocks into a large containing tensor.", "Applies a 1D average pooling over an input signal composed of several input planes.", "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kW regions by step size sH\u00d7sWsH \\times sW steps.", "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sW steps.", "Applies a 1D max pooling over an input signal composed of several input planes.", "Applies a 2D max pooling over an input signal composed of several input planes.", "Applies a 3D max pooling over an input signal composed of several input planes.", "Computes a partial inverse of MaxPool1d.", "Computes a partial inverse of MaxPool2d.", "Computes a partial inverse of MaxPool3d.", "Applies a 1D power-average pooling over an input signal composed of several input planes.", "Applies a 2D power-average pooling over an input signal composed of several input planes.", "Applies a 1D adaptive max pooling over an input signal composed of several input planes.", "Applies a 2D adaptive max pooling over an input signal composed of several input planes.", "Applies a 3D adaptive max pooling over an input signal composed of several input planes.", "Applies a 1D adaptive average pooling over an input signal composed of several input planes.", "Applies a 2D adaptive average pooling over an input signal composed of several input planes.", "Applies a 3D adaptive average pooling over an input signal composed of several input planes.", "Applies 2D fractional max pooling over an input signal composed of several input planes.", "Applies 3D fractional max pooling over an input signal composed of several input planes.", "Computes scaled dot product attention on query, key and value tensors, using an optional attention mask if passed, and applying dropout if a probability greater than 0.0 is specified.", "Thresholds each element of the input Tensor.", "In-place version of threshold().", "Applies the rectified linear unit function element-wise.", "In-place version of relu().", "Applies the HardTanh function element-wise.", "In-place version of hardtanh().", "Applies the hardswish function, element-wise, as described in the paper:", "Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6).", "Applies the Exponential Linear Unit (ELU) function element-wise.", "In-place version of elu().", "Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.", "Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1)).", "Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)", "In-place version of leaky_relu().", "Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x) where weight is a learnable parameter.", "Randomized leaky ReLU.", "In-place version of rrelu().", "The gated linear unit.", "When the approximate argument is 'none', it applies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)", "Applies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)", "Applies the hard shrinkage function element-wise", "Applies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)", "Applies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}", "Applies element-wise, the function Softplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x)).", "Applies a softmin function.", "Applies a softmax function.", "Applies the soft shrinkage function elementwise", "Samples from the Gumbel-Softmax distribution (Link 1 Link 2) and optionally discretizes.", "Applies a softmax followed by a logarithm.", "Applies element-wise, Tanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}", "Applies the element-wise function Sigmoid(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}", "Applies the element-wise function", "Applies the Sigmoid Linear Unit (SiLU) function, element-wise.", "Applies the Mish function, element-wise.", "Applies Batch Normalization for each channel across a batch of data.", "Applies Group Normalization for last certain number of dimensions.", "Applies Instance Normalization for each channel in each data sample in a batch.", "Applies Layer Normalization for last certain number of dimensions.", "Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.", "Performs LpL_p normalization of inputs over specified dimension.", "Applies a linear transformation to the incoming data: y=xAT+by = xA^T + b.", "Applies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + b", "During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.", "Applies alpha dropout to the input.", "Randomly masks out entire channels (a channel is a feature map, e.g.", "Randomly zero out entire channels (a channel is a 1D feature map, e.g., the jj-th channel of the ii-th sample in the batched input is a 1D tensor input[i,j]\\text{input}[i, j]) of the input tensor).", "Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jj-th channel of the ii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]) of the input tensor).", "Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jj-th channel of the ii-th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]) of the input tensor).", "A simple lookup table that looks up embeddings in a fixed dictionary and size.", "Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.", "Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.", "See torch.nn.PairwiseDistance for details", "Returns cosine similarity between x1 and x2, computed along dim.", "Computes the p-norm distance between every pair of row vectors in the input.", "Function that measures the Binary Cross Entropy between the target and input probabilities.", "Function that measures Binary Cross Entropy between target and input logits.", "Poisson negative log likelihood loss.", "See CosineEmbeddingLoss for details.", "This criterion computes the cross entropy loss between input logits and target.", "The Connectionist Temporal Classification loss.", "Gaussian negative log likelihood loss.", "See HingeEmbeddingLoss for details.", "The Kullback-Leibler divergence Loss", "Function that takes the mean element-wise absolute value difference.", "Measures the element-wise mean squared error.", "See MarginRankingLoss for details.", "See MultiLabelMarginLoss for details.", "See MultiLabelSoftMarginLoss for details.", "See MultiMarginLoss for details.", "The negative log likelihood loss.", "Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.", "Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.", "See SoftMarginLoss for details.", "See TripletMarginLoss for details", "See TripletMarginWithDistanceLoss for details.", "Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r), where r is the upscale_factor.", "Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W), where r is the downscale_factor.", "Pads tensor.", "Down/up samples the input to either the given size or the given scale_factor", "Upsamples the input to either the given size or the given scale_factor", "Upsamples the input, using nearest neighbours' pixel values.", "Upsamples the input, using bilinear upsampling.", "Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.", "Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta.", "torch.nn.parallel.data_parallel", "Evaluates module(input) in parallel across the GPUs given in device_ids."]}, {"name": "torch.nn.functional.adaptive_avg_pool1d()", "path": "generated/torch.nn.functional.adaptive_avg_pool1d#torch.nn.functional.adaptive_avg_pool1d", "type": "NN Functions", "text": ["Applies a 1D adaptive average pooling over an input signal composed of several input planes.", "See AdaptiveAvgPool1d for details and output shape.", "output_size \u2013 the target output size (single integer)"]}, {"name": "torch.nn.functional.adaptive_avg_pool2d()", "path": "generated/torch.nn.functional.adaptive_avg_pool2d#torch.nn.functional.adaptive_avg_pool2d", "type": "NN Functions", "text": ["Applies a 2D adaptive average pooling over an input signal composed of several input planes.", "See AdaptiveAvgPool2d for details and output shape.", "output_size (None) \u2013 the target output size (single integer or double-integer tuple)", "Tensor"]}, {"name": "torch.nn.functional.adaptive_avg_pool3d()", "path": "generated/torch.nn.functional.adaptive_avg_pool3d#torch.nn.functional.adaptive_avg_pool3d", "type": "NN Functions", "text": ["Applies a 3D adaptive average pooling over an input signal composed of several input planes.", "See AdaptiveAvgPool3d for details and output shape.", "output_size (None) \u2013 the target output size (single integer or triple-integer tuple)", "Tensor"]}, {"name": "torch.nn.functional.adaptive_max_pool1d()", "path": "generated/torch.nn.functional.adaptive_max_pool1d#torch.nn.functional.adaptive_max_pool1d", "type": "NN Functions", "text": ["Applies a 1D adaptive max pooling over an input signal composed of several input planes.", "See AdaptiveMaxPool1d for details and output shape."]}, {"name": "torch.nn.functional.adaptive_max_pool2d()", "path": "generated/torch.nn.functional.adaptive_max_pool2d#torch.nn.functional.adaptive_max_pool2d", "type": "NN Functions", "text": ["Applies a 2D adaptive max pooling over an input signal composed of several input planes.", "See AdaptiveMaxPool2d for details and output shape."]}, {"name": "torch.nn.functional.adaptive_max_pool3d()", "path": "generated/torch.nn.functional.adaptive_max_pool3d#torch.nn.functional.adaptive_max_pool3d", "type": "NN Functions", "text": ["Applies a 3D adaptive max pooling over an input signal composed of several input planes.", "See AdaptiveMaxPool3d for details and output shape."]}, {"name": "torch.nn.functional.affine_grid()", "path": "generated/torch.nn.functional.affine_grid#torch.nn.functional.affine_grid", "type": "NN Functions", "text": ["Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta.", "Note", "This function is often used in conjunction with grid_sample() to build Spatial Transformer Networks .", "output Tensor of size (N\u00d7H\u00d7W\u00d72N \\times H \\times W \\times 2)", "output (Tensor)", "Warning", "When align_corners = True, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by grid_sample() will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was align_corners = True. Since then, the default behavior has been changed to align_corners = False, in order to bring it in line with the default for interpolate().", "Warning", "When align_corners = True, 2D affine transforms on 1D data and 3D affine transforms on 2D data (that is, when one of the spatial dimensions has unit size) are ill-defined, and not an intended use case. This is not a problem when align_corners = False. Up to version 1.2.0, all grid points along a unit dimension were considered arbitrarily to be at -1. From version 1.3.0, under align_corners = True all grid points along a unit dimension are considered to be at 0 (the center of the input image)."]}, {"name": "torch.nn.functional.alpha_dropout()", "path": "generated/torch.nn.functional.alpha_dropout#torch.nn.functional.alpha_dropout", "type": "NN Functions", "text": ["Applies alpha dropout to the input.", "See AlphaDropout for details.", "Tensor"]}, {"name": "torch.nn.functional.avg_pool1d()", "path": "generated/torch.nn.functional.avg_pool1d#torch.nn.functional.avg_pool1d", "type": "NN Functions", "text": ["Applies a 1D average pooling over an input signal composed of several input planes.", "See AvgPool1d for details and output shape.", "Examples:"]}, {"name": "torch.nn.functional.avg_pool2d()", "path": "generated/torch.nn.functional.avg_pool2d#torch.nn.functional.avg_pool2d", "type": "NN Functions", "text": ["Applies 2D average-pooling operation in kH\u00d7kWkH \\times kW regions by step size sH\u00d7sWsH \\times sW steps. The number of output features is equal to the number of input planes.", "See AvgPool2d for details and output shape."]}, {"name": "torch.nn.functional.avg_pool3d()", "path": "generated/torch.nn.functional.avg_pool3d#torch.nn.functional.avg_pool3d", "type": "NN Functions", "text": ["Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sW steps. The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac{\\text{input planes}}{sT}\\rfloor.", "See AvgPool3d for details and output shape."]}, {"name": "torch.nn.functional.batch_norm()", "path": "generated/torch.nn.functional.batch_norm#torch.nn.functional.batch_norm", "type": "NN Functions", "text": ["Applies Batch Normalization for each channel across a batch of data.", "See BatchNorm1d, BatchNorm2d, BatchNorm3d for details.", "Tensor"]}, {"name": "torch.nn.functional.bilinear()", "path": "generated/torch.nn.functional.bilinear#torch.nn.functional.bilinear", "type": "NN Functions", "text": ["Applies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + b", "Shape:"]}, {"name": "torch.nn.functional.binary_cross_entropy()", "path": "generated/torch.nn.functional.binary_cross_entropy#torch.nn.functional.binary_cross_entropy", "type": "NN Functions", "text": ["Function that measures the Binary Cross Entropy between the target and input probabilities.", "See BCELoss for details.", "Tensor", "Examples:"]}, {"name": "torch.nn.functional.binary_cross_entropy_with_logits()", "path": "generated/torch.nn.functional.binary_cross_entropy_with_logits#torch.nn.functional.binary_cross_entropy_with_logits", "type": "NN Functions", "text": ["Function that measures Binary Cross Entropy between target and input logits.", "See BCEWithLogitsLoss for details.", "Tensor", "Examples:"]}, {"name": "torch.nn.functional.celu()", "path": "generated/torch.nn.functional.celu#torch.nn.functional.celu", "type": "NN Functions", "text": ["Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1)).", "See CELU for more details.", "Tensor"]}, {"name": "torch.nn.functional.conv1d()", "path": "generated/torch.nn.functional.conv1d#torch.nn.functional.conv1d", "type": "NN Functions", "text": ["Applies a 1D convolution over an input signal composed of several input planes.", "This operator supports TensorFloat32.", "See Conv1d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Note", "This operator supports complex data types i.e. complex32, complex64, complex128.", "padding \u2013 ", "implicit paddings on both sides of the input. Can be a string {\u2018valid\u2019, \u2018same\u2019}, single number or a one-element tuple (padW,). Default: 0 padding='valid' is the same as no padding. padding='same' pads the input so the output has the same shape as the input. However, this mode doesn\u2019t support any stride values other than 1.", "Warning", "For padding='same', if the weight is even-length and dilation is odd in any dimension, a full pad() operation may be needed internally. Lowering performance.", "Examples:"]}, {"name": "torch.nn.functional.conv2d()", "path": "generated/torch.nn.functional.conv2d#torch.nn.functional.conv2d", "type": "NN Functions", "text": ["Applies a 2D convolution over an input image composed of several input planes.", "This operator supports TensorFloat32.", "See Conv2d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Note", "This operator supports complex data types i.e. complex32, complex64, complex128.", "padding \u2013 ", "implicit paddings on both sides of the input. Can be a string {\u2018valid\u2019, \u2018same\u2019}, single number or a tuple (padH, padW). Default: 0 padding='valid' is the same as no padding. padding='same' pads the input so the output has the same shape as the input. However, this mode doesn\u2019t support any stride values other than 1.", "Warning", "For padding='same', if the weight is even-length and dilation is odd in any dimension, a full pad() operation may be needed internally. Lowering performance.", "Examples:"]}, {"name": "torch.nn.functional.conv3d()", "path": "generated/torch.nn.functional.conv3d#torch.nn.functional.conv3d", "type": "NN Functions", "text": ["Applies a 3D convolution over an input image composed of several input planes.", "This operator supports TensorFloat32.", "See Conv3d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Note", "This operator supports complex data types i.e. complex32, complex64, complex128.", "padding \u2013 ", "implicit paddings on both sides of the input. Can be a string {\u2018valid\u2019, \u2018same\u2019}, single number or a tuple (padT, padH, padW). Default: 0 padding='valid' is the same as no padding. padding='same' pads the input so the output has the same shape as the input. However, this mode doesn\u2019t support any stride values other than 1.", "Warning", "For padding='same', if the weight is even-length and dilation is odd in any dimension, a full pad() operation may be needed internally. Lowering performance.", "Examples:"]}, {"name": "torch.nn.functional.conv_transpose1d()", "path": "generated/torch.nn.functional.conv_transpose1d#torch.nn.functional.conv_transpose1d", "type": "NN Functions", "text": ["Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.", "This operator supports TensorFloat32.", "See ConvTranspose1d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Examples:"]}, {"name": "torch.nn.functional.conv_transpose2d()", "path": "generated/torch.nn.functional.conv_transpose2d#torch.nn.functional.conv_transpose2d", "type": "NN Functions", "text": ["Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.", "This operator supports TensorFloat32.", "See ConvTranspose2d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Examples:"]}, {"name": "torch.nn.functional.conv_transpose3d()", "path": "generated/torch.nn.functional.conv_transpose3d#torch.nn.functional.conv_transpose3d", "type": "NN Functions", "text": ["Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d", "This operator supports TensorFloat32.", "See ConvTranspose3d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Examples:"]}, {"name": "torch.nn.functional.cosine_embedding_loss()", "path": "generated/torch.nn.functional.cosine_embedding_loss#torch.nn.functional.cosine_embedding_loss", "type": "NN Functions", "text": ["See CosineEmbeddingLoss for details.", "Tensor"]}, {"name": "torch.nn.functional.cosine_similarity()", "path": "generated/torch.nn.functional.cosine_similarity#torch.nn.functional.cosine_similarity", "type": "NN Functions", "text": ["Returns cosine similarity between x1 and x2, computed along dim. x1 and x2 must be broadcastable to a common shape. dim refers to the dimension in this common shape. Dimension dim of the output is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension.", "Supports type promotion.", "Example:"]}, {"name": "torch.nn.functional.cross_entropy()", "path": "generated/torch.nn.functional.cross_entropy#torch.nn.functional.cross_entropy", "type": "NN Functions", "text": ["This criterion computes the cross entropy loss between input logits and target.", "See CrossEntropyLoss for details.", "Tensor", "where:", "Examples:"]}, {"name": "torch.nn.functional.ctc_loss()", "path": "generated/torch.nn.functional.ctc_loss#torch.nn.functional.ctc_loss", "type": "NN Functions", "text": ["The Connectionist Temporal Classification loss.", "See CTCLoss for details.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.", "Tensor", "Example:"]}, {"name": "torch.nn.functional.dropout()", "path": "generated/torch.nn.functional.dropout#torch.nn.functional.dropout", "type": "NN Functions", "text": ["During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.", "See Dropout for details.", "Tensor"]}, {"name": "torch.nn.functional.dropout1d()", "path": "generated/torch.nn.functional.dropout1d#torch.nn.functional.dropout1d", "type": "NN Functions", "text": ["Randomly zero out entire channels (a channel is a 1D feature map, e.g., the jj-th channel of the ii-th sample in the batched input is a 1D tensor input[i,j]\\text{input}[i, j]) of the input tensor). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution.", "See Dropout1d for details.", "Tensor"]}, {"name": "torch.nn.functional.dropout2d()", "path": "generated/torch.nn.functional.dropout2d#torch.nn.functional.dropout2d", "type": "NN Functions", "text": ["Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jj-th channel of the ii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]) of the input tensor). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution.", "See Dropout2d for details.", "Tensor"]}, {"name": "torch.nn.functional.dropout3d()", "path": "generated/torch.nn.functional.dropout3d#torch.nn.functional.dropout3d", "type": "NN Functions", "text": ["Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jj-th channel of the ii-th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]) of the input tensor). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution.", "See Dropout3d for details.", "Tensor"]}, {"name": "torch.nn.functional.elu()", "path": "generated/torch.nn.functional.elu#torch.nn.functional.elu", "type": "NN Functions", "text": ["Applies the Exponential Linear Unit (ELU) function element-wise.", "See ELU for more details.", "Tensor"]}, {"name": "torch.nn.functional.elu_()", "path": "generated/torch.nn.functional.elu_#torch.nn.functional.elu_", "type": "NN Functions", "text": ["In-place version of elu()."]}, {"name": "torch.nn.functional.embedding()", "path": "generated/torch.nn.functional.embedding#torch.nn.functional.embedding", "type": "NN Functions", "text": ["A simple lookup table that looks up embeddings in a fixed dictionary and size.", "This module is often used to retrieve word embeddings using indices. The input to the module is a list of indices, and the embedding matrix, and the output is the corresponding word embeddings.", "See torch.nn.Embedding for more details.", "Note", "Note that the analytical gradients of this function with respect to entries in weight at the row specified by padding_idx are expected to differ from the numerical ones.", "Note", "Note that :class:`torch.nn.Embedding differs from this function in that it initializes the row of weight specified by padding_idx to all zeros on construction.", "Tensor", "Examples:"]}, {"name": "torch.nn.functional.embedding_bag()", "path": "generated/torch.nn.functional.embedding_bag#torch.nn.functional.embedding_bag", "type": "NN Functions", "text": ["Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.", "See torch.nn.EmbeddingBag for more details.", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.", "Tensor", "input (LongTensor) and offsets (LongTensor, optional)", "Examples:"]}, {"name": "torch.nn.functional.feature_alpha_dropout()", "path": "generated/torch.nn.functional.feature_alpha_dropout#torch.nn.functional.feature_alpha_dropout", "type": "NN Functions", "text": ["Randomly masks out entire channels (a channel is a feature map, e.g. the jj-th channel of the ii-th sample in the batch input is a tensor input[i,j]\\text{input}[i, j]) of the input tensor). Instead of setting activations to zero, as in regular Dropout, the activations are set to the negative saturation value of the SELU activation function.", "Each element will be masked independently on every forward call with probability p using samples from a Bernoulli distribution. The elements to be masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit variance.", "See FeatureAlphaDropout for details.", "Tensor"]}, {"name": "torch.nn.functional.fold()", "path": "generated/torch.nn.functional.fold#torch.nn.functional.fold", "type": "NN Functions", "text": ["Combines an array of sliding local blocks into a large containing tensor.", "Warning", "Currently, only unbatched (3D) or batched (4D) image-like output tensors are supported.", "See torch.nn.Fold for details", "Tensor"]}, {"name": "torch.nn.functional.fractional_max_pool2d()", "path": "generated/torch.nn.functional.fractional_max_pool2d#torch.nn.functional.fractional_max_pool2d", "type": "NN Functions", "text": ["Applies 2D fractional max pooling over an input signal composed of several input planes.", "Fractional MaxPooling is described in detail in the paper Fractional MaxPooling by Ben Graham", "The max-pooling operation is applied in kH\u00d7kWkH \\times kW regions by a stochastic step size determined by the target output size. The number of output features is equal to the number of input planes."]}, {"name": "torch.nn.functional.fractional_max_pool3d()", "path": "generated/torch.nn.functional.fractional_max_pool3d#torch.nn.functional.fractional_max_pool3d", "type": "NN Functions", "text": ["Applies 3D fractional max pooling over an input signal composed of several input planes.", "Fractional MaxPooling is described in detail in the paper Fractional MaxPooling by Ben Graham", "The max-pooling operation is applied in kT\u00d7kH\u00d7kWkT \\times kH \\times kW regions by a stochastic step size determined by the target output size. The number of output features is equal to the number of input planes."]}, {"name": "torch.nn.functional.gaussian_nll_loss()", "path": "generated/torch.nn.functional.gaussian_nll_loss#torch.nn.functional.gaussian_nll_loss", "type": "NN Functions", "text": ["Gaussian negative log likelihood loss.", "See GaussianNLLLoss for details.", "Tensor"]}, {"name": "torch.nn.functional.gelu()", "path": "generated/torch.nn.functional.gelu#torch.nn.functional.gelu", "type": "NN Functions", "text": ["When the approximate argument is \u2018none\u2019, it applies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)", "where \u03a6(x)\\Phi(x) is the Cumulative Distribution Function for Gaussian Distribution.", "When the approximate argument is \u2018tanh\u2019, Gelu is estimated with", "See Gaussian Error Linear Units (GELUs)."]}, {"name": "torch.nn.functional.glu()", "path": "generated/torch.nn.functional.glu#torch.nn.functional.glu", "type": "NN Functions", "text": ["The gated linear unit. Computes:", "where input is split in half along dim to form a and b, \u03c3\\sigma is the sigmoid function and \u2297\\otimes is the element-wise product between matrices.", "See Language Modeling with Gated Convolutional Networks.", "Tensor"]}, {"name": "torch.nn.functional.grid_sample()", "path": "generated/torch.nn.functional.grid_sample#torch.nn.functional.grid_sample", "type": "NN Functions", "text": ["Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.", "Currently, only spatial (4-D) and volumetric (5-D) input are supported.", "In the spatial (4-D) case, for input with shape (N,C,Hin,Win)(N, C, H_\\text{in}, W_\\text{in}) and grid with shape (N,Hout,Wout,2)(N, H_\\text{out}, W_\\text{out}, 2), the output will have shape (N,C,Hout,Wout)(N, C, H_\\text{out}, W_\\text{out}).", "For each output location output[n, :, h, w], the size-2 vector grid[n, h, w] specifies input pixel locations x and y, which are used to interpolate the output value output[n, :, h, w]. In the case of 5D inputs, grid[n, d, h, w] specifies the x, y, z pixel locations for interpolating output[n, :, d, h, w]. mode argument specifies nearest or bilinear interpolation method to sample the input pixels.", "grid specifies the sampling pixel locations normalized by the input spatial dimensions. Therefore, it should have most values in the range of [-1, 1]. For example, values x = -1, y = -1 is the left-top pixel of input, and values x = 1, y = 1 is the right-bottom pixel of input.", "If grid has values outside the range of [-1, 1], the corresponding outputs are handled as defined by padding_mode. Options are", "Note", "This function is often used in conjunction with affine_grid() to build Spatial Transformer Networks .", "Note", "When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on Reproducibility for background.", "Note", "NaN values in grid would be interpreted as -1.", "output Tensor", "output (Tensor)", "Warning", "When align_corners = True, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by grid_sample() will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was align_corners = True. Since then, the default behavior has been changed to align_corners = False, in order to bring it in line with the default for interpolate().", "Note", "mode='bicubic' is implemented using the cubic convolution algorithm with \u03b1=\u22120.75\\alpha=-0.75. The constant \u03b1\\alpha might be different from packages to packages. For example, PIL and OpenCV use -0.5 and -0.75 respectively. This algorithm may \u201covershoot\u201d the range of values it\u2019s interpolating. For example, it may produce negative values or values greater than 255 when interpolating input in [0, 255]. Clamp the results with torch.clamp() to ensure they are within the valid range."]}, {"name": "torch.nn.functional.group_norm()", "path": "generated/torch.nn.functional.group_norm#torch.nn.functional.group_norm", "type": "NN Functions", "text": ["Applies Group Normalization for last certain number of dimensions.", "See GroupNorm for details.", "Tensor"]}, {"name": "torch.nn.functional.gumbel_softmax()", "path": "generated/torch.nn.functional.gumbel_softmax#torch.nn.functional.gumbel_softmax", "type": "NN Functions", "text": ["Samples from the Gumbel-Softmax distribution (Link 1 Link 2) and optionally discretizes.", "Sampled tensor of same shape as logits from the Gumbel-Softmax distribution. If hard=True, the returned samples will be one-hot, otherwise they will be probability distributions that sum to 1 across dim.", "Tensor", "Note", "This function is here for legacy reasons, may be removed from nn.Functional in the future.", "Note", "The main trick for hard is to do y_hard - y_soft.detach() + y_soft", "It achieves two things: - makes the output value exactly one-hot (since we add then subtract y_soft value) - makes the gradient equal to y_soft gradient (since we strip all other gradients)"]}, {"name": "torch.nn.functional.hardshrink()", "path": "generated/torch.nn.functional.hardshrink#torch.nn.functional.hardshrink", "type": "NN Functions", "text": ["Applies the hard shrinkage function element-wise", "See Hardshrink for more details."]}, {"name": "torch.nn.functional.hardsigmoid()", "path": "generated/torch.nn.functional.hardsigmoid#torch.nn.functional.hardsigmoid", "type": "NN Functions", "text": ["Applies the element-wise function", "inplace (bool) \u2013 If set to True, will do this operation in-place. Default: False", "Tensor", "See Hardsigmoid for more details."]}, {"name": "torch.nn.functional.hardswish()", "path": "generated/torch.nn.functional.hardswish#torch.nn.functional.hardswish", "type": "NN Functions", "text": ["Applies the hardswish function, element-wise, as described in the paper:", "Searching for MobileNetV3.", "See Hardswish for more details.", "Tensor"]}, {"name": "torch.nn.functional.hardtanh()", "path": "generated/torch.nn.functional.hardtanh#torch.nn.functional.hardtanh", "type": "NN Functions", "text": ["Applies the HardTanh function element-wise. See Hardtanh for more details.", "Tensor"]}, {"name": "torch.nn.functional.hardtanh_()", "path": "generated/torch.nn.functional.hardtanh_#torch.nn.functional.hardtanh_", "type": "NN Functions", "text": ["In-place version of hardtanh()."]}, {"name": "torch.nn.functional.hinge_embedding_loss()", "path": "generated/torch.nn.functional.hinge_embedding_loss#torch.nn.functional.hinge_embedding_loss", "type": "NN Functions", "text": ["See HingeEmbeddingLoss for details.", "Tensor"]}, {"name": "torch.nn.functional.huber_loss()", "path": "generated/torch.nn.functional.huber_loss#torch.nn.functional.huber_loss", "type": "NN Functions", "text": ["Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.", "See HuberLoss for details.", "Tensor"]}, {"name": "torch.nn.functional.instance_norm()", "path": "generated/torch.nn.functional.instance_norm#torch.nn.functional.instance_norm", "type": "NN Functions", "text": ["Applies Instance Normalization for each channel in each data sample in a batch.", "See InstanceNorm1d, InstanceNorm2d, InstanceNorm3d for details.", "Tensor"]}, {"name": "torch.nn.functional.interpolate()", "path": "generated/torch.nn.functional.interpolate#torch.nn.functional.interpolate", "type": "NN Functions", "text": ["Down/up samples the input to either the given size or the given scale_factor", "The algorithm used for interpolation is determined by mode.", "Currently temporal, spatial and volumetric sampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape.", "The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width.", "The modes available for resizing are: nearest, linear (3D-only), bilinear, bicubic (4D-only), trilinear (5D-only), area, nearest-exact", "Tensor", "Note", "With mode='bicubic', it\u2019s possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call result.clamp(min=0, max=255) if you want to reduce the overshoot when displaying the image.", "Note", "Mode mode='nearest-exact' matches Scikit-Image and PIL nearest neighbours interpolation algorithms and fixes known issues with mode='nearest'. This mode is introduced to keep backward compatibility. Mode mode='nearest' matches buggy OpenCV\u2019s INTER_NEAREST interpolation algorithm.", "Note", "The gradients for the dtype float16 on CUDA may be inaccurate in the upsample operation when using modes ['linear', 'bilinear', 'bicubic', 'trilinear', 'area']. For more details, please refer to the discussion in issue#104157.", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information."]}, {"name": "torch.nn.functional.kl_div()", "path": "generated/torch.nn.functional.kl_div#torch.nn.functional.kl_div", "type": "NN Functions", "text": ["The Kullback-Leibler divergence Loss", "See KLDivLoss for details.", "Tensor", "Note", "size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction.", "Warning", "reduction = 'mean' doesn\u2019t return the true kl divergence value, please use reduction = 'batchmean' which aligns with KL math definition."]}, {"name": "torch.nn.functional.l1_loss()", "path": "generated/torch.nn.functional.l1_loss#torch.nn.functional.l1_loss", "type": "NN Functions", "text": ["Function that takes the mean element-wise absolute value difference.", "See L1Loss for details.", "Tensor"]}, {"name": "torch.nn.functional.layer_norm()", "path": "generated/torch.nn.functional.layer_norm#torch.nn.functional.layer_norm", "type": "NN Functions", "text": ["Applies Layer Normalization for last certain number of dimensions.", "See LayerNorm for details.", "Tensor"]}, {"name": "torch.nn.functional.leaky_relu()", "path": "generated/torch.nn.functional.leaky_relu#torch.nn.functional.leaky_relu", "type": "NN Functions", "text": ["Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)", "See LeakyReLU for more details.", "Tensor"]}, {"name": "torch.nn.functional.leaky_relu_()", "path": "generated/torch.nn.functional.leaky_relu_#torch.nn.functional.leaky_relu_", "type": "NN Functions", "text": ["In-place version of leaky_relu()."]}, {"name": "torch.nn.functional.linear()", "path": "generated/torch.nn.functional.linear#torch.nn.functional.linear", "type": "NN Functions", "text": ["Applies a linear transformation to the incoming data: y=xAT+by = xA^T + b.", "This operation supports 2-D weight with sparse layout", "Warning", "Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported, or may not have autograd support. If you notice missing functionality please open a feature request.", "This operator supports TensorFloat32.", "Shape:"]}, {"name": "torch.nn.functional.local_response_norm()", "path": "generated/torch.nn.functional.local_response_norm#torch.nn.functional.local_response_norm", "type": "NN Functions", "text": ["Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels.", "See LocalResponseNorm for details.", "Tensor"]}, {"name": "torch.nn.functional.log_softmax()", "path": "generated/torch.nn.functional.log_softmax#torch.nn.functional.log_softmax", "type": "NN Functions", "text": ["Applies a softmax followed by a logarithm.", "While mathematically equivalent to log(softmax(x)), doing these two operations separately is slower and numerically unstable. This function uses an alternative formulation to compute the output and gradient correctly.", "See LogSoftmax for more details.", "Tensor"]}, {"name": "torch.nn.functional.logsigmoid()", "path": "generated/torch.nn.functional.logsigmoid#torch.nn.functional.logsigmoid", "type": "NN Functions", "text": ["Applies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)", "See LogSigmoid for more details."]}, {"name": "torch.nn.functional.lp_pool1d()", "path": "generated/torch.nn.functional.lp_pool1d#torch.nn.functional.lp_pool1d", "type": "NN Functions", "text": ["Applies a 1D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of p is zero, the gradient is set to zero as well.", "See LPPool1d for details.", "Tensor"]}, {"name": "torch.nn.functional.lp_pool2d()", "path": "generated/torch.nn.functional.lp_pool2d#torch.nn.functional.lp_pool2d", "type": "NN Functions", "text": ["Applies a 2D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of p is zero, the gradient is set to zero as well.", "See LPPool2d for details.", "Tensor"]}, {"name": "torch.nn.functional.margin_ranking_loss()", "path": "generated/torch.nn.functional.margin_ranking_loss#torch.nn.functional.margin_ranking_loss", "type": "NN Functions", "text": ["See MarginRankingLoss for details.", "Tensor"]}, {"name": "torch.nn.functional.max_pool1d()", "path": "generated/torch.nn.functional.max_pool1d#torch.nn.functional.max_pool1d", "type": "NN Functions", "text": ["Applies a 1D max pooling over an input signal composed of several input planes.", "Note", "The order of ceil_mode and return_indices is different from what seen in MaxPool1d, and will change in a future release.", "See MaxPool1d for details."]}, {"name": "torch.nn.functional.max_pool2d()", "path": "generated/torch.nn.functional.max_pool2d#torch.nn.functional.max_pool2d", "type": "NN Functions", "text": ["Applies a 2D max pooling over an input signal composed of several input planes.", "Note", "The order of ceil_mode and return_indices is different from what seen in MaxPool2d, and will change in a future release.", "See MaxPool2d for details."]}, {"name": "torch.nn.functional.max_pool3d()", "path": "generated/torch.nn.functional.max_pool3d#torch.nn.functional.max_pool3d", "type": "NN Functions", "text": ["Applies a 3D max pooling over an input signal composed of several input planes.", "Note", "The order of ceil_mode and return_indices is different from what seen in MaxPool3d, and will change in a future release.", "See MaxPool3d for details."]}, {"name": "torch.nn.functional.max_unpool1d()", "path": "generated/torch.nn.functional.max_unpool1d#torch.nn.functional.max_unpool1d", "type": "NN Functions", "text": ["Computes a partial inverse of MaxPool1d.", "See MaxUnpool1d for details.", "Tensor"]}, {"name": "torch.nn.functional.max_unpool2d()", "path": "generated/torch.nn.functional.max_unpool2d#torch.nn.functional.max_unpool2d", "type": "NN Functions", "text": ["Computes a partial inverse of MaxPool2d.", "See MaxUnpool2d for details.", "Tensor"]}, {"name": "torch.nn.functional.max_unpool3d()", "path": "generated/torch.nn.functional.max_unpool3d#torch.nn.functional.max_unpool3d", "type": "NN Functions", "text": ["Computes a partial inverse of MaxPool3d.", "See MaxUnpool3d for details.", "Tensor"]}, {"name": "torch.nn.functional.mish()", "path": "generated/torch.nn.functional.mish#torch.nn.functional.mish", "type": "NN Functions", "text": ["Applies the Mish function, element-wise. Mish: A Self Regularized Non-Monotonic Neural Activation Function.", "Note", "See Mish: A Self Regularized Non-Monotonic Neural Activation Function", "See Mish for more details.", "Tensor"]}, {"name": "torch.nn.functional.mse_loss()", "path": "generated/torch.nn.functional.mse_loss#torch.nn.functional.mse_loss", "type": "NN Functions", "text": ["Measures the element-wise mean squared error.", "See MSELoss for details.", "Tensor"]}, {"name": "torch.nn.functional.multi_margin_loss()", "path": "generated/torch.nn.functional.multi_margin_loss#torch.nn.functional.multi_margin_loss", "type": "NN Functions", "text": ["See MultiMarginLoss for details.", "Tensor"]}, {"name": "torch.nn.functional.multilabel_margin_loss()", "path": "generated/torch.nn.functional.multilabel_margin_loss#torch.nn.functional.multilabel_margin_loss", "type": "NN Functions", "text": ["See MultiLabelMarginLoss for details.", "Tensor"]}, {"name": "torch.nn.functional.multilabel_soft_margin_loss()", "path": "generated/torch.nn.functional.multilabel_soft_margin_loss#torch.nn.functional.multilabel_soft_margin_loss", "type": "NN Functions", "text": ["See MultiLabelSoftMarginLoss for details.", "Tensor"]}, {"name": "torch.nn.functional.nll_loss()", "path": "generated/torch.nn.functional.nll_loss#torch.nn.functional.nll_loss", "type": "NN Functions", "text": ["The negative log likelihood loss.", "See NLLLoss for details.", "Tensor", "Example:"]}, {"name": "torch.nn.functional.normalize()", "path": "generated/torch.nn.functional.normalize#torch.nn.functional.normalize", "type": "NN Functions", "text": ["Performs LpL_p normalization of inputs over specified dimension.", "For a tensor input of sizes (n0,...,ndim,...,nk)(n_0, ..., n_{dim}, ..., n_k), each ndimn_{dim} -element vector vv along dimension dim is transformed as", "With the default arguments it uses the Euclidean norm over vectors along dimension 11 for normalization.", "Tensor"]}, {"name": "torch.nn.functional.one_hot()", "path": "generated/torch.nn.functional.one_hot#torch.nn.functional.one_hot", "type": "NN Functions", "text": ["Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.", "See also One-hot on Wikipedia .", "LongTensor that has one more dimension with 1 values at the index of last dimension indicated by the input, and 0 everywhere else."]}, {"name": "torch.nn.functional.pad()", "path": "generated/torch.nn.functional.pad#torch.nn.functional.pad", "type": "NN Functions", "text": ["Pads tensor.", "The padding size by which to pad some dimensions of input are described starting from the last dimension and moving forward. \u230alen(pad)2\u230b\\left\\lfloor\\frac{\\text{len(pad)}}{2}\\right\\rfloor dimensions of input will be padded. For example, to pad only the last dimension of the input tensor, then pad has the form (padding_left,padding_right)(\\text{padding\\_left}, \\text{padding\\_right}); to pad the last 2 dimensions of the input tensor, then use (padding_left,padding_right,(\\text{padding\\_left}, \\text{padding\\_right}, padding_top,padding_bottom)\\text{padding\\_top}, \\text{padding\\_bottom}); to pad the last 3 dimensions, use (padding_left,padding_right,(\\text{padding\\_left}, \\text{padding\\_right}, padding_top,padding_bottom\\text{padding\\_top}, \\text{padding\\_bottom} padding_front,padding_back)\\text{padding\\_front}, \\text{padding\\_back}).", "See torch.nn.CircularPad2d, torch.nn.ConstantPad2d, torch.nn.ReflectionPad2d, and torch.nn.ReplicationPad2d for concrete examples on how each of the padding modes works. Constant padding is implemented for arbitrary dimensions. Circular, replicate and reflection padding are implemented for padding the last 3 dimensions of a 4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor, or the last dimension of a 2D or 3D input tensor.", "Note", "When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on Reproducibility for background.", "Examples:"]}, {"name": "torch.nn.functional.pairwise_distance()", "path": "generated/torch.nn.functional.pairwise_distance#torch.nn.functional.pairwise_distance", "type": "NN Functions", "text": ["See torch.nn.PairwiseDistance for details"]}, {"name": "torch.nn.functional.pdist()", "path": "generated/torch.nn.functional.pdist#torch.nn.functional.pdist", "type": "NN Functions", "text": ["Computes the p-norm distance between every pair of row vectors in the input. This is identical to the upper triangular portion, excluding the diagonal, of torch.norm(input[:, None] - input, dim=2, p=p). This function will be faster if the rows are contiguous.", "If input has shape N\u00d7MN \\times M then the output will have shape 12N(N\u22121)\\frac{1}{2} N (N - 1).", "This function is equivalent to scipy.spatial.distance.pdist(input,\n'minkowski', p=p) if p\u2208(0,\u221e)p \\in (0, \\infty). When p=0p = 0 it is equivalent to scipy.spatial.distance.pdist(input, 'hamming') * M. When p=\u221ep = \\infty, the closest scipy function is scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max())."]}, {"name": "torch.nn.functional.pixel_shuffle()", "path": "generated/torch.nn.functional.pixel_shuffle#torch.nn.functional.pixel_shuffle", "type": "NN Functions", "text": ["Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r), where r is the upscale_factor.", "See PixelShuffle for details.", "Examples:"]}, {"name": "torch.nn.functional.pixel_unshuffle()", "path": "generated/torch.nn.functional.pixel_unshuffle#torch.nn.functional.pixel_unshuffle", "type": "NN Functions", "text": ["Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W), where r is the downscale_factor.", "See PixelUnshuffle for details.", "Examples:"]}, {"name": "torch.nn.functional.poisson_nll_loss()", "path": "generated/torch.nn.functional.poisson_nll_loss#torch.nn.functional.poisson_nll_loss", "type": "NN Functions", "text": ["Poisson negative log likelihood loss.", "See PoissonNLLLoss for details.", "Tensor"]}, {"name": "torch.nn.functional.prelu()", "path": "generated/torch.nn.functional.prelu#torch.nn.functional.prelu", "type": "NN Functions", "text": ["Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x) where weight is a learnable parameter.", "Note", "weight is expected to be a scalar or 1-D tensor. If weight is 1-D, its size must match the number of input channels, determined by input.size(1) when input.dim() >= 2, otherwise 1. In the 1-D case, note that when input has dim > 2, weight can be expanded to the shape of input in a way that is not possible using normal broadcasting semantics.", "See PReLU for more details."]}, {"name": "torch.nn.functional.relu()", "path": "generated/torch.nn.functional.relu#torch.nn.functional.relu", "type": "NN Functions", "text": ["Applies the rectified linear unit function element-wise. See ReLU for more details.", "Tensor"]}, {"name": "torch.nn.functional.relu6()", "path": "generated/torch.nn.functional.relu6#torch.nn.functional.relu6", "type": "NN Functions", "text": ["Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6).", "See ReLU6 for more details.", "Tensor"]}, {"name": "torch.nn.functional.relu_()", "path": "generated/torch.nn.functional.relu_#torch.nn.functional.relu_", "type": "NN Functions", "text": ["In-place version of relu()."]}, {"name": "torch.nn.functional.rrelu()", "path": "generated/torch.nn.functional.rrelu#torch.nn.functional.rrelu", "type": "NN Functions", "text": ["Randomized leaky ReLU.", "See RReLU for more details.", "Tensor"]}, {"name": "torch.nn.functional.rrelu_()", "path": "generated/torch.nn.functional.rrelu_#torch.nn.functional.rrelu_", "type": "NN Functions", "text": ["In-place version of rrelu()."]}, {"name": "torch.nn.functional.scaled_dot_product_attention()", "path": "generated/torch.nn.functional.scaled_dot_product_attention#torch.nn.functional.scaled_dot_product_attention", "type": "NN Functions", "text": ["Computes scaled dot product attention on query, key and value tensors, using an optional attention mask if passed, and applying dropout if a probability greater than 0.0 is specified.", "Warning", "This function is beta and subject to change.", "Note", "There are currently three supported implementations of scaled dot product attention:", "The function may call optimized kernels for improved performance when using the CUDA backend. For all other backends, the PyTorch implementation will be used.", "All implementations are enabled by default. Scaled dot product attention attempts to automatically select the most optimal implementation based on the inputs. In order to provide more fine-grained control over what implementation is used, the following functions are provided for enabling and disabling implementations. The context manager is the preferred mechanism:", "Each of the fused kernels has specific input limitations. If the user requires the use of a specific fused implementation, disable the PyTorch C++ implementation using torch.backends.cuda.sdp_kernel(). In the event that a fused implementation is not available, an error will be raised with the reasons why the fused implementation cannot run.", "Due to the nature of fusing floating point operations, the output of this function may be different depending on what backend kernel is chosen. The c++ implementation supports torch.float64 and can be used when higher precision is required. For more information please see Numerical accuracy", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Attention output; shape (N,...,L,Ev)(N, ..., L, Ev).", "output (Tensor)", "Examples:"]}, {"name": "torch.nn.functional.selu()", "path": "generated/torch.nn.functional.selu#torch.nn.functional.selu", "type": "NN Functions", "text": ["Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.", "See SELU for more details.", "Tensor"]}, {"name": "torch.nn.functional.sigmoid()", "path": "generated/torch.nn.functional.sigmoid#torch.nn.functional.sigmoid", "type": "NN Functions", "text": ["Applies the element-wise function Sigmoid(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}", "See Sigmoid for more details."]}, {"name": "torch.nn.functional.silu()", "path": "generated/torch.nn.functional.silu#torch.nn.functional.silu", "type": "NN Functions", "text": ["Applies the Sigmoid Linear Unit (SiLU) function, element-wise. The SiLU function is also known as the swish function.", "Note", "See Gaussian Error Linear Units (GELUs) where the SiLU (Sigmoid Linear Unit) was originally coined, and see Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning and Swish: a Self-Gated Activation Function where the SiLU was experimented with later.", "See SiLU for more details.", "Tensor"]}, {"name": "torch.nn.functional.smooth_l1_loss()", "path": "generated/torch.nn.functional.smooth_l1_loss#torch.nn.functional.smooth_l1_loss", "type": "NN Functions", "text": ["Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.", "See SmoothL1Loss for details.", "Tensor"]}, {"name": "torch.nn.functional.soft_margin_loss()", "path": "generated/torch.nn.functional.soft_margin_loss#torch.nn.functional.soft_margin_loss", "type": "NN Functions", "text": ["See SoftMarginLoss for details.", "Tensor"]}, {"name": "torch.nn.functional.softmax()", "path": "generated/torch.nn.functional.softmax#torch.nn.functional.softmax", "type": "NN Functions", "text": ["Applies a softmax function.", "Softmax is defined as:", "Softmax(xi)=exp\u2061(xi)\u2211jexp\u2061(xj)\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}", "It is applied to all slices along dim, and will re-scale them so that the elements lie in the range [0, 1] and sum to 1.", "See Softmax for more details.", "Tensor", "Note", "This function doesn\u2019t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use log_softmax instead (it\u2019s faster and has better numerical properties)."]}, {"name": "torch.nn.functional.softmin()", "path": "generated/torch.nn.functional.softmin#torch.nn.functional.softmin", "type": "NN Functions", "text": ["Applies a softmin function.", "Note that Softmin(x)=Softmax(\u2212x)\\text{Softmin}(x) = \\text{Softmax}(-x). See softmax definition for mathematical formula.", "See Softmin for more details.", "Tensor"]}, {"name": "torch.nn.functional.softplus()", "path": "generated/torch.nn.functional.softplus#torch.nn.functional.softplus", "type": "NN Functions", "text": ["Applies element-wise, the function Softplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x)).", "For numerical stability the implementation reverts to the linear function when input\u00d7\u03b2>thresholdinput \\times \\beta > threshold.", "See Softplus for more details."]}, {"name": "torch.nn.functional.softshrink()", "path": "generated/torch.nn.functional.softshrink#torch.nn.functional.softshrink", "type": "NN Functions", "text": ["Applies the soft shrinkage function elementwise", "See Softshrink for more details."]}, {"name": "torch.nn.functional.softsign()", "path": "generated/torch.nn.functional.softsign#torch.nn.functional.softsign", "type": "NN Functions", "text": ["Applies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}", "See Softsign for more details."]}, {"name": "torch.nn.functional.tanh()", "path": "generated/torch.nn.functional.tanh#torch.nn.functional.tanh", "type": "NN Functions", "text": ["Applies element-wise, Tanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}", "See Tanh for more details."]}, {"name": "torch.nn.functional.tanhshrink()", "path": "generated/torch.nn.functional.tanhshrink#torch.nn.functional.tanhshrink", "type": "NN Functions", "text": ["Applies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)", "See Tanhshrink for more details."]}, {"name": "torch.nn.functional.threshold()", "path": "generated/torch.nn.functional.threshold#torch.nn.functional.threshold", "type": "NN Functions", "text": ["Thresholds each element of the input Tensor.", "See Threshold for more details.", "Tensor"]}, {"name": "torch.nn.functional.threshold_()", "path": "generated/torch.nn.functional.threshold_#torch.nn.functional.threshold_", "type": "NN Functions", "text": ["In-place version of threshold()."]}, {"name": "torch.nn.functional.torch.nn.functional.adaptive_avg_pool1d", "path": "generated/torch.nn.functional.adaptive_avg_pool1d", "type": "NN Functions", "text": ["Applies a 1D adaptive average pooling over an input signal composed of several input planes.", "See AdaptiveAvgPool1d for details and output shape.", "output_size \u2013 the target output size (single integer)"]}, {"name": "torch.nn.functional.torch.nn.functional.adaptive_avg_pool2d", "path": "generated/torch.nn.functional.adaptive_avg_pool2d", "type": "NN Functions", "text": ["Applies a 2D adaptive average pooling over an input signal composed of several input planes.", "See AdaptiveAvgPool2d for details and output shape.", "output_size (None) \u2013 the target output size (single integer or double-integer tuple)", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.adaptive_avg_pool3d", "path": "generated/torch.nn.functional.adaptive_avg_pool3d", "type": "NN Functions", "text": ["Applies a 3D adaptive average pooling over an input signal composed of several input planes.", "See AdaptiveAvgPool3d for details and output shape.", "output_size (None) \u2013 the target output size (single integer or triple-integer tuple)", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.adaptive_max_pool1d", "path": "generated/torch.nn.functional.adaptive_max_pool1d", "type": "NN Functions", "text": ["Applies a 1D adaptive max pooling over an input signal composed of several input planes.", "See AdaptiveMaxPool1d for details and output shape."]}, {"name": "torch.nn.functional.torch.nn.functional.adaptive_max_pool2d", "path": "generated/torch.nn.functional.adaptive_max_pool2d", "type": "NN Functions", "text": ["Applies a 2D adaptive max pooling over an input signal composed of several input planes.", "See AdaptiveMaxPool2d for details and output shape."]}, {"name": "torch.nn.functional.torch.nn.functional.adaptive_max_pool3d", "path": "generated/torch.nn.functional.adaptive_max_pool3d", "type": "NN Functions", "text": ["Applies a 3D adaptive max pooling over an input signal composed of several input planes.", "See AdaptiveMaxPool3d for details and output shape."]}, {"name": "torch.nn.functional.torch.nn.functional.affine_grid", "path": "generated/torch.nn.functional.affine_grid", "type": "NN Functions", "text": ["Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta.", "Note", "This function is often used in conjunction with grid_sample() to build Spatial Transformer Networks .", "output Tensor of size (N\u00d7H\u00d7W\u00d72N \\times H \\times W \\times 2)", "output (Tensor)", "Warning", "When align_corners = True, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by grid_sample() will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was align_corners = True. Since then, the default behavior has been changed to align_corners = False, in order to bring it in line with the default for interpolate().", "Warning", "When align_corners = True, 2D affine transforms on 1D data and 3D affine transforms on 2D data (that is, when one of the spatial dimensions has unit size) are ill-defined, and not an intended use case. This is not a problem when align_corners = False. Up to version 1.2.0, all grid points along a unit dimension were considered arbitrarily to be at -1. From version 1.3.0, under align_corners = True all grid points along a unit dimension are considered to be at 0 (the center of the input image)."]}, {"name": "torch.nn.functional.torch.nn.functional.alpha_dropout", "path": "generated/torch.nn.functional.alpha_dropout", "type": "NN Functions", "text": ["Applies alpha dropout to the input.", "See AlphaDropout for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.avg_pool1d", "path": "generated/torch.nn.functional.avg_pool1d", "type": "NN Functions", "text": ["Applies a 1D average pooling over an input signal composed of several input planes.", "See AvgPool1d for details and output shape.", "Examples:"]}, {"name": "torch.nn.functional.torch.nn.functional.avg_pool2d", "path": "generated/torch.nn.functional.avg_pool2d", "type": "NN Functions", "text": ["Applies 2D average-pooling operation in kH\u00d7kWkH \\times kW regions by step size sH\u00d7sWsH \\times sW steps. The number of output features is equal to the number of input planes.", "See AvgPool2d for details and output shape."]}, {"name": "torch.nn.functional.torch.nn.functional.avg_pool3d", "path": "generated/torch.nn.functional.avg_pool3d", "type": "NN Functions", "text": ["Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sW steps. The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac{\\text{input planes}}{sT}\\rfloor.", "See AvgPool3d for details and output shape."]}, {"name": "torch.nn.functional.torch.nn.functional.batch_norm", "path": "generated/torch.nn.functional.batch_norm", "type": "NN Functions", "text": ["Applies Batch Normalization for each channel across a batch of data.", "See BatchNorm1d, BatchNorm2d, BatchNorm3d for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.bilinear", "path": "generated/torch.nn.functional.bilinear", "type": "NN Functions", "text": ["Applies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + b", "Shape:"]}, {"name": "torch.nn.functional.torch.nn.functional.binary_cross_entropy", "path": "generated/torch.nn.functional.binary_cross_entropy", "type": "NN Functions", "text": ["Function that measures the Binary Cross Entropy between the target and input probabilities.", "See BCELoss for details.", "Tensor", "Examples:"]}, {"name": "torch.nn.functional.torch.nn.functional.binary_cross_entropy_with_logits", "path": "generated/torch.nn.functional.binary_cross_entropy_with_logits", "type": "NN Functions", "text": ["Function that measures Binary Cross Entropy between target and input logits.", "See BCEWithLogitsLoss for details.", "Tensor", "Examples:"]}, {"name": "torch.nn.functional.torch.nn.functional.celu", "path": "generated/torch.nn.functional.celu", "type": "NN Functions", "text": ["Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1)).", "See CELU for more details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.conv1d", "path": "generated/torch.nn.functional.conv1d", "type": "NN Functions", "text": ["Applies a 1D convolution over an input signal composed of several input planes.", "This operator supports TensorFloat32.", "See Conv1d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Note", "This operator supports complex data types i.e. complex32, complex64, complex128.", "padding \u2013 ", "implicit paddings on both sides of the input. Can be a string {\u2018valid\u2019, \u2018same\u2019}, single number or a one-element tuple (padW,). Default: 0 padding='valid' is the same as no padding. padding='same' pads the input so the output has the same shape as the input. However, this mode doesn\u2019t support any stride values other than 1.", "Warning", "For padding='same', if the weight is even-length and dilation is odd in any dimension, a full pad() operation may be needed internally. Lowering performance.", "Examples:"]}, {"name": "torch.nn.functional.torch.nn.functional.conv2d", "path": "generated/torch.nn.functional.conv2d", "type": "NN Functions", "text": ["Applies a 2D convolution over an input image composed of several input planes.", "This operator supports TensorFloat32.", "See Conv2d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Note", "This operator supports complex data types i.e. complex32, complex64, complex128.", "padding \u2013 ", "implicit paddings on both sides of the input. Can be a string {\u2018valid\u2019, \u2018same\u2019}, single number or a tuple (padH, padW). Default: 0 padding='valid' is the same as no padding. padding='same' pads the input so the output has the same shape as the input. However, this mode doesn\u2019t support any stride values other than 1.", "Warning", "For padding='same', if the weight is even-length and dilation is odd in any dimension, a full pad() operation may be needed internally. Lowering performance.", "Examples:"]}, {"name": "torch.nn.functional.torch.nn.functional.conv3d", "path": "generated/torch.nn.functional.conv3d", "type": "NN Functions", "text": ["Applies a 3D convolution over an input image composed of several input planes.", "This operator supports TensorFloat32.", "See Conv3d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Note", "This operator supports complex data types i.e. complex32, complex64, complex128.", "padding \u2013 ", "implicit paddings on both sides of the input. Can be a string {\u2018valid\u2019, \u2018same\u2019}, single number or a tuple (padT, padH, padW). Default: 0 padding='valid' is the same as no padding. padding='same' pads the input so the output has the same shape as the input. However, this mode doesn\u2019t support any stride values other than 1.", "Warning", "For padding='same', if the weight is even-length and dilation is odd in any dimension, a full pad() operation may be needed internally. Lowering performance.", "Examples:"]}, {"name": "torch.nn.functional.torch.nn.functional.conv_transpose1d", "path": "generated/torch.nn.functional.conv_transpose1d", "type": "NN Functions", "text": ["Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.", "This operator supports TensorFloat32.", "See ConvTranspose1d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Examples:"]}, {"name": "torch.nn.functional.torch.nn.functional.conv_transpose2d", "path": "generated/torch.nn.functional.conv_transpose2d", "type": "NN Functions", "text": ["Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.", "This operator supports TensorFloat32.", "See ConvTranspose2d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Examples:"]}, {"name": "torch.nn.functional.torch.nn.functional.conv_transpose3d", "path": "generated/torch.nn.functional.conv_transpose3d", "type": "NN Functions", "text": ["Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d", "This operator supports TensorFloat32.", "See ConvTranspose3d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Examples:"]}, {"name": "torch.nn.functional.torch.nn.functional.cosine_embedding_loss", "path": "generated/torch.nn.functional.cosine_embedding_loss", "type": "NN Functions", "text": ["See CosineEmbeddingLoss for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.cosine_similarity", "path": "generated/torch.nn.functional.cosine_similarity", "type": "NN Functions", "text": ["Returns cosine similarity between x1 and x2, computed along dim. x1 and x2 must be broadcastable to a common shape. dim refers to the dimension in this common shape. Dimension dim of the output is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension.", "Supports type promotion.", "Example:"]}, {"name": "torch.nn.functional.torch.nn.functional.cross_entropy", "path": "generated/torch.nn.functional.cross_entropy", "type": "NN Functions", "text": ["This criterion computes the cross entropy loss between input logits and target.", "See CrossEntropyLoss for details.", "Tensor", "where:", "Examples:"]}, {"name": "torch.nn.functional.torch.nn.functional.ctc_loss", "path": "generated/torch.nn.functional.ctc_loss", "type": "NN Functions", "text": ["The Connectionist Temporal Classification loss.", "See CTCLoss for details.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.", "Tensor", "Example:"]}, {"name": "torch.nn.functional.torch.nn.functional.dropout", "path": "generated/torch.nn.functional.dropout", "type": "NN Functions", "text": ["During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.", "See Dropout for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.dropout1d", "path": "generated/torch.nn.functional.dropout1d", "type": "NN Functions", "text": ["Randomly zero out entire channels (a channel is a 1D feature map, e.g., the jj-th channel of the ii-th sample in the batched input is a 1D tensor input[i,j]\\text{input}[i, j]) of the input tensor). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution.", "See Dropout1d for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.dropout2d", "path": "generated/torch.nn.functional.dropout2d", "type": "NN Functions", "text": ["Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jj-th channel of the ii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]) of the input tensor). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution.", "See Dropout2d for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.dropout3d", "path": "generated/torch.nn.functional.dropout3d", "type": "NN Functions", "text": ["Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jj-th channel of the ii-th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]) of the input tensor). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution.", "See Dropout3d for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.elu", "path": "generated/torch.nn.functional.elu", "type": "NN Functions", "text": ["Applies the Exponential Linear Unit (ELU) function element-wise.", "See ELU for more details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.elu_", "path": "generated/torch.nn.functional.elu_", "type": "NN Functions", "text": ["In-place version of elu()."]}, {"name": "torch.nn.functional.torch.nn.functional.embedding", "path": "generated/torch.nn.functional.embedding", "type": "NN Functions", "text": ["A simple lookup table that looks up embeddings in a fixed dictionary and size.", "This module is often used to retrieve word embeddings using indices. The input to the module is a list of indices, and the embedding matrix, and the output is the corresponding word embeddings.", "See torch.nn.Embedding for more details.", "Note", "Note that the analytical gradients of this function with respect to entries in weight at the row specified by padding_idx are expected to differ from the numerical ones.", "Note", "Note that :class:`torch.nn.Embedding differs from this function in that it initializes the row of weight specified by padding_idx to all zeros on construction.", "Tensor", "Examples:"]}, {"name": "torch.nn.functional.torch.nn.functional.embedding_bag", "path": "generated/torch.nn.functional.embedding_bag", "type": "NN Functions", "text": ["Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.", "See torch.nn.EmbeddingBag for more details.", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.", "Tensor", "input (LongTensor) and offsets (LongTensor, optional)", "Examples:"]}, {"name": "torch.nn.functional.torch.nn.functional.feature_alpha_dropout", "path": "generated/torch.nn.functional.feature_alpha_dropout", "type": "NN Functions", "text": ["Randomly masks out entire channels (a channel is a feature map, e.g. the jj-th channel of the ii-th sample in the batch input is a tensor input[i,j]\\text{input}[i, j]) of the input tensor). Instead of setting activations to zero, as in regular Dropout, the activations are set to the negative saturation value of the SELU activation function.", "Each element will be masked independently on every forward call with probability p using samples from a Bernoulli distribution. The elements to be masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit variance.", "See FeatureAlphaDropout for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.fold", "path": "generated/torch.nn.functional.fold", "type": "NN Functions", "text": ["Combines an array of sliding local blocks into a large containing tensor.", "Warning", "Currently, only unbatched (3D) or batched (4D) image-like output tensors are supported.", "See torch.nn.Fold for details", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.fractional_max_pool2d", "path": "generated/torch.nn.functional.fractional_max_pool2d", "type": "NN Functions", "text": ["Applies 2D fractional max pooling over an input signal composed of several input planes.", "Fractional MaxPooling is described in detail in the paper Fractional MaxPooling by Ben Graham", "The max-pooling operation is applied in kH\u00d7kWkH \\times kW regions by a stochastic step size determined by the target output size. The number of output features is equal to the number of input planes."]}, {"name": "torch.nn.functional.torch.nn.functional.fractional_max_pool3d", "path": "generated/torch.nn.functional.fractional_max_pool3d", "type": "NN Functions", "text": ["Applies 3D fractional max pooling over an input signal composed of several input planes.", "Fractional MaxPooling is described in detail in the paper Fractional MaxPooling by Ben Graham", "The max-pooling operation is applied in kT\u00d7kH\u00d7kWkT \\times kH \\times kW regions by a stochastic step size determined by the target output size. The number of output features is equal to the number of input planes."]}, {"name": "torch.nn.functional.torch.nn.functional.gaussian_nll_loss", "path": "generated/torch.nn.functional.gaussian_nll_loss", "type": "NN Functions", "text": ["Gaussian negative log likelihood loss.", "See GaussianNLLLoss for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.gelu", "path": "generated/torch.nn.functional.gelu", "type": "NN Functions", "text": ["When the approximate argument is \u2018none\u2019, it applies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)", "where \u03a6(x)\\Phi(x) is the Cumulative Distribution Function for Gaussian Distribution.", "When the approximate argument is \u2018tanh\u2019, Gelu is estimated with", "See Gaussian Error Linear Units (GELUs)."]}, {"name": "torch.nn.functional.torch.nn.functional.glu", "path": "generated/torch.nn.functional.glu", "type": "NN Functions", "text": ["The gated linear unit. Computes:", "where input is split in half along dim to form a and b, \u03c3\\sigma is the sigmoid function and \u2297\\otimes is the element-wise product between matrices.", "See Language Modeling with Gated Convolutional Networks.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.grid_sample", "path": "generated/torch.nn.functional.grid_sample", "type": "NN Functions", "text": ["Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.", "Currently, only spatial (4-D) and volumetric (5-D) input are supported.", "In the spatial (4-D) case, for input with shape (N,C,Hin,Win)(N, C, H_\\text{in}, W_\\text{in}) and grid with shape (N,Hout,Wout,2)(N, H_\\text{out}, W_\\text{out}, 2), the output will have shape (N,C,Hout,Wout)(N, C, H_\\text{out}, W_\\text{out}).", "For each output location output[n, :, h, w], the size-2 vector grid[n, h, w] specifies input pixel locations x and y, which are used to interpolate the output value output[n, :, h, w]. In the case of 5D inputs, grid[n, d, h, w] specifies the x, y, z pixel locations for interpolating output[n, :, d, h, w]. mode argument specifies nearest or bilinear interpolation method to sample the input pixels.", "grid specifies the sampling pixel locations normalized by the input spatial dimensions. Therefore, it should have most values in the range of [-1, 1]. For example, values x = -1, y = -1 is the left-top pixel of input, and values x = 1, y = 1 is the right-bottom pixel of input.", "If grid has values outside the range of [-1, 1], the corresponding outputs are handled as defined by padding_mode. Options are", "Note", "This function is often used in conjunction with affine_grid() to build Spatial Transformer Networks .", "Note", "When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on Reproducibility for background.", "Note", "NaN values in grid would be interpreted as -1.", "output Tensor", "output (Tensor)", "Warning", "When align_corners = True, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by grid_sample() will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was align_corners = True. Since then, the default behavior has been changed to align_corners = False, in order to bring it in line with the default for interpolate().", "Note", "mode='bicubic' is implemented using the cubic convolution algorithm with \u03b1=\u22120.75\\alpha=-0.75. The constant \u03b1\\alpha might be different from packages to packages. For example, PIL and OpenCV use -0.5 and -0.75 respectively. This algorithm may \u201covershoot\u201d the range of values it\u2019s interpolating. For example, it may produce negative values or values greater than 255 when interpolating input in [0, 255]. Clamp the results with torch.clamp() to ensure they are within the valid range."]}, {"name": "torch.nn.functional.torch.nn.functional.group_norm", "path": "generated/torch.nn.functional.group_norm", "type": "NN Functions", "text": ["Applies Group Normalization for last certain number of dimensions.", "See GroupNorm for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.gumbel_softmax", "path": "generated/torch.nn.functional.gumbel_softmax", "type": "NN Functions", "text": ["Samples from the Gumbel-Softmax distribution (Link 1 Link 2) and optionally discretizes.", "Sampled tensor of same shape as logits from the Gumbel-Softmax distribution. If hard=True, the returned samples will be one-hot, otherwise they will be probability distributions that sum to 1 across dim.", "Tensor", "Note", "This function is here for legacy reasons, may be removed from nn.Functional in the future.", "Note", "The main trick for hard is to do y_hard - y_soft.detach() + y_soft", "It achieves two things: - makes the output value exactly one-hot (since we add then subtract y_soft value) - makes the gradient equal to y_soft gradient (since we strip all other gradients)"]}, {"name": "torch.nn.functional.torch.nn.functional.hardshrink", "path": "generated/torch.nn.functional.hardshrink", "type": "NN Functions", "text": ["Applies the hard shrinkage function element-wise", "See Hardshrink for more details."]}, {"name": "torch.nn.functional.torch.nn.functional.hardsigmoid", "path": "generated/torch.nn.functional.hardsigmoid", "type": "NN Functions", "text": ["Applies the element-wise function", "inplace (bool) \u2013 If set to True, will do this operation in-place. Default: False", "Tensor", "See Hardsigmoid for more details."]}, {"name": "torch.nn.functional.torch.nn.functional.hardswish", "path": "generated/torch.nn.functional.hardswish", "type": "NN Functions", "text": ["Applies the hardswish function, element-wise, as described in the paper:", "Searching for MobileNetV3.", "See Hardswish for more details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.hardtanh", "path": "generated/torch.nn.functional.hardtanh", "type": "NN Functions", "text": ["Applies the HardTanh function element-wise. See Hardtanh for more details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.hardtanh_", "path": "generated/torch.nn.functional.hardtanh_", "type": "NN Functions", "text": ["In-place version of hardtanh()."]}, {"name": "torch.nn.functional.torch.nn.functional.hinge_embedding_loss", "path": "generated/torch.nn.functional.hinge_embedding_loss", "type": "NN Functions", "text": ["See HingeEmbeddingLoss for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.huber_loss", "path": "generated/torch.nn.functional.huber_loss", "type": "NN Functions", "text": ["Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.", "See HuberLoss for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.instance_norm", "path": "generated/torch.nn.functional.instance_norm", "type": "NN Functions", "text": ["Applies Instance Normalization for each channel in each data sample in a batch.", "See InstanceNorm1d, InstanceNorm2d, InstanceNorm3d for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.interpolate", "path": "generated/torch.nn.functional.interpolate", "type": "NN Functions", "text": ["Down/up samples the input to either the given size or the given scale_factor", "The algorithm used for interpolation is determined by mode.", "Currently temporal, spatial and volumetric sampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape.", "The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width.", "The modes available for resizing are: nearest, linear (3D-only), bilinear, bicubic (4D-only), trilinear (5D-only), area, nearest-exact", "Tensor", "Note", "With mode='bicubic', it\u2019s possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call result.clamp(min=0, max=255) if you want to reduce the overshoot when displaying the image.", "Note", "Mode mode='nearest-exact' matches Scikit-Image and PIL nearest neighbours interpolation algorithms and fixes known issues with mode='nearest'. This mode is introduced to keep backward compatibility. Mode mode='nearest' matches buggy OpenCV\u2019s INTER_NEAREST interpolation algorithm.", "Note", "The gradients for the dtype float16 on CUDA may be inaccurate in the upsample operation when using modes ['linear', 'bilinear', 'bicubic', 'trilinear', 'area']. For more details, please refer to the discussion in issue#104157.", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information."]}, {"name": "torch.nn.functional.torch.nn.functional.kl_div", "path": "generated/torch.nn.functional.kl_div", "type": "NN Functions", "text": ["The Kullback-Leibler divergence Loss", "See KLDivLoss for details.", "Tensor", "Note", "size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction.", "Warning", "reduction = 'mean' doesn\u2019t return the true kl divergence value, please use reduction = 'batchmean' which aligns with KL math definition."]}, {"name": "torch.nn.functional.torch.nn.functional.l1_loss", "path": "generated/torch.nn.functional.l1_loss", "type": "NN Functions", "text": ["Function that takes the mean element-wise absolute value difference.", "See L1Loss for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.layer_norm", "path": "generated/torch.nn.functional.layer_norm", "type": "NN Functions", "text": ["Applies Layer Normalization for last certain number of dimensions.", "See LayerNorm for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.leaky_relu", "path": "generated/torch.nn.functional.leaky_relu", "type": "NN Functions", "text": ["Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)", "See LeakyReLU for more details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.leaky_relu_", "path": "generated/torch.nn.functional.leaky_relu_", "type": "NN Functions", "text": ["In-place version of leaky_relu()."]}, {"name": "torch.nn.functional.torch.nn.functional.linear", "path": "generated/torch.nn.functional.linear", "type": "NN Functions", "text": ["Applies a linear transformation to the incoming data: y=xAT+by = xA^T + b.", "This operation supports 2-D weight with sparse layout", "Warning", "Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported, or may not have autograd support. If you notice missing functionality please open a feature request.", "This operator supports TensorFloat32.", "Shape:"]}, {"name": "torch.nn.functional.torch.nn.functional.local_response_norm", "path": "generated/torch.nn.functional.local_response_norm", "type": "NN Functions", "text": ["Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels.", "See LocalResponseNorm for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.log_softmax", "path": "generated/torch.nn.functional.log_softmax", "type": "NN Functions", "text": ["Applies a softmax followed by a logarithm.", "While mathematically equivalent to log(softmax(x)), doing these two operations separately is slower and numerically unstable. This function uses an alternative formulation to compute the output and gradient correctly.", "See LogSoftmax for more details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.logsigmoid", "path": "generated/torch.nn.functional.logsigmoid", "type": "NN Functions", "text": ["Applies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)", "See LogSigmoid for more details."]}, {"name": "torch.nn.functional.torch.nn.functional.lp_pool1d", "path": "generated/torch.nn.functional.lp_pool1d", "type": "NN Functions", "text": ["Applies a 1D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of p is zero, the gradient is set to zero as well.", "See LPPool1d for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.lp_pool2d", "path": "generated/torch.nn.functional.lp_pool2d", "type": "NN Functions", "text": ["Applies a 2D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of p is zero, the gradient is set to zero as well.", "See LPPool2d for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.margin_ranking_loss", "path": "generated/torch.nn.functional.margin_ranking_loss", "type": "NN Functions", "text": ["See MarginRankingLoss for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.max_pool1d", "path": "generated/torch.nn.functional.max_pool1d", "type": "NN Functions", "text": ["Applies a 1D max pooling over an input signal composed of several input planes.", "Note", "The order of ceil_mode and return_indices is different from what seen in MaxPool1d, and will change in a future release.", "See MaxPool1d for details."]}, {"name": "torch.nn.functional.torch.nn.functional.max_pool2d", "path": "generated/torch.nn.functional.max_pool2d", "type": "NN Functions", "text": ["Applies a 2D max pooling over an input signal composed of several input planes.", "Note", "The order of ceil_mode and return_indices is different from what seen in MaxPool2d, and will change in a future release.", "See MaxPool2d for details."]}, {"name": "torch.nn.functional.torch.nn.functional.max_pool3d", "path": "generated/torch.nn.functional.max_pool3d", "type": "NN Functions", "text": ["Applies a 3D max pooling over an input signal composed of several input planes.", "Note", "The order of ceil_mode and return_indices is different from what seen in MaxPool3d, and will change in a future release.", "See MaxPool3d for details."]}, {"name": "torch.nn.functional.torch.nn.functional.max_unpool1d", "path": "generated/torch.nn.functional.max_unpool1d", "type": "NN Functions", "text": ["Computes a partial inverse of MaxPool1d.", "See MaxUnpool1d for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.max_unpool2d", "path": "generated/torch.nn.functional.max_unpool2d", "type": "NN Functions", "text": ["Computes a partial inverse of MaxPool2d.", "See MaxUnpool2d for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.max_unpool3d", "path": "generated/torch.nn.functional.max_unpool3d", "type": "NN Functions", "text": ["Computes a partial inverse of MaxPool3d.", "See MaxUnpool3d for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.mish", "path": "generated/torch.nn.functional.mish", "type": "NN Functions", "text": ["Applies the Mish function, element-wise. Mish: A Self Regularized Non-Monotonic Neural Activation Function.", "Note", "See Mish: A Self Regularized Non-Monotonic Neural Activation Function", "See Mish for more details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.mse_loss", "path": "generated/torch.nn.functional.mse_loss", "type": "NN Functions", "text": ["Measures the element-wise mean squared error.", "See MSELoss for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.multi_margin_loss", "path": "generated/torch.nn.functional.multi_margin_loss", "type": "NN Functions", "text": ["See MultiMarginLoss for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.multilabel_margin_loss", "path": "generated/torch.nn.functional.multilabel_margin_loss", "type": "NN Functions", "text": ["See MultiLabelMarginLoss for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.multilabel_soft_margin_loss", "path": "generated/torch.nn.functional.multilabel_soft_margin_loss", "type": "NN Functions", "text": ["See MultiLabelSoftMarginLoss for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.nll_loss", "path": "generated/torch.nn.functional.nll_loss", "type": "NN Functions", "text": ["The negative log likelihood loss.", "See NLLLoss for details.", "Tensor", "Example:"]}, {"name": "torch.nn.functional.torch.nn.functional.normalize", "path": "generated/torch.nn.functional.normalize", "type": "NN Functions", "text": ["Performs LpL_p normalization of inputs over specified dimension.", "For a tensor input of sizes (n0,...,ndim,...,nk)(n_0, ..., n_{dim}, ..., n_k), each ndimn_{dim} -element vector vv along dimension dim is transformed as", "With the default arguments it uses the Euclidean norm over vectors along dimension 11 for normalization.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.one_hot", "path": "generated/torch.nn.functional.one_hot", "type": "NN Functions", "text": ["Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.", "See also One-hot on Wikipedia .", "LongTensor that has one more dimension with 1 values at the index of last dimension indicated by the input, and 0 everywhere else."]}, {"name": "torch.nn.functional.torch.nn.functional.pad", "path": "generated/torch.nn.functional.pad", "type": "NN Functions", "text": ["Pads tensor.", "The padding size by which to pad some dimensions of input are described starting from the last dimension and moving forward. \u230alen(pad)2\u230b\\left\\lfloor\\frac{\\text{len(pad)}}{2}\\right\\rfloor dimensions of input will be padded. For example, to pad only the last dimension of the input tensor, then pad has the form (padding_left,padding_right)(\\text{padding\\_left}, \\text{padding\\_right}); to pad the last 2 dimensions of the input tensor, then use (padding_left,padding_right,(\\text{padding\\_left}, \\text{padding\\_right}, padding_top,padding_bottom)\\text{padding\\_top}, \\text{padding\\_bottom}); to pad the last 3 dimensions, use (padding_left,padding_right,(\\text{padding\\_left}, \\text{padding\\_right}, padding_top,padding_bottom\\text{padding\\_top}, \\text{padding\\_bottom} padding_front,padding_back)\\text{padding\\_front}, \\text{padding\\_back}).", "See torch.nn.CircularPad2d, torch.nn.ConstantPad2d, torch.nn.ReflectionPad2d, and torch.nn.ReplicationPad2d for concrete examples on how each of the padding modes works. Constant padding is implemented for arbitrary dimensions. Circular, replicate and reflection padding are implemented for padding the last 3 dimensions of a 4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor, or the last dimension of a 2D or 3D input tensor.", "Note", "When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on Reproducibility for background.", "Examples:"]}, {"name": "torch.nn.functional.torch.nn.functional.pairwise_distance", "path": "generated/torch.nn.functional.pairwise_distance", "type": "NN Functions", "text": ["See torch.nn.PairwiseDistance for details"]}, {"name": "torch.nn.functional.torch.nn.functional.pdist", "path": "generated/torch.nn.functional.pdist", "type": "NN Functions", "text": ["Computes the p-norm distance between every pair of row vectors in the input. This is identical to the upper triangular portion, excluding the diagonal, of torch.norm(input[:, None] - input, dim=2, p=p). This function will be faster if the rows are contiguous.", "If input has shape N\u00d7MN \\times M then the output will have shape 12N(N\u22121)\\frac{1}{2} N (N - 1).", "This function is equivalent to scipy.spatial.distance.pdist(input,\n'minkowski', p=p) if p\u2208(0,\u221e)p \\in (0, \\infty). When p=0p = 0 it is equivalent to scipy.spatial.distance.pdist(input, 'hamming') * M. When p=\u221ep = \\infty, the closest scipy function is scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max())."]}, {"name": "torch.nn.functional.torch.nn.functional.pixel_shuffle", "path": "generated/torch.nn.functional.pixel_shuffle", "type": "NN Functions", "text": ["Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r), where r is the upscale_factor.", "See PixelShuffle for details.", "Examples:"]}, {"name": "torch.nn.functional.torch.nn.functional.pixel_unshuffle", "path": "generated/torch.nn.functional.pixel_unshuffle", "type": "NN Functions", "text": ["Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W), where r is the downscale_factor.", "See PixelUnshuffle for details.", "Examples:"]}, {"name": "torch.nn.functional.torch.nn.functional.poisson_nll_loss", "path": "generated/torch.nn.functional.poisson_nll_loss", "type": "NN Functions", "text": ["Poisson negative log likelihood loss.", "See PoissonNLLLoss for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.prelu", "path": "generated/torch.nn.functional.prelu", "type": "NN Functions", "text": ["Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x) where weight is a learnable parameter.", "Note", "weight is expected to be a scalar or 1-D tensor. If weight is 1-D, its size must match the number of input channels, determined by input.size(1) when input.dim() >= 2, otherwise 1. In the 1-D case, note that when input has dim > 2, weight can be expanded to the shape of input in a way that is not possible using normal broadcasting semantics.", "See PReLU for more details."]}, {"name": "torch.nn.functional.torch.nn.functional.relu", "path": "generated/torch.nn.functional.relu", "type": "NN Functions", "text": ["Applies the rectified linear unit function element-wise. See ReLU for more details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.relu6", "path": "generated/torch.nn.functional.relu6", "type": "NN Functions", "text": ["Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6).", "See ReLU6 for more details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.relu_", "path": "generated/torch.nn.functional.relu_", "type": "NN Functions", "text": ["In-place version of relu()."]}, {"name": "torch.nn.functional.torch.nn.functional.rrelu", "path": "generated/torch.nn.functional.rrelu", "type": "NN Functions", "text": ["Randomized leaky ReLU.", "See RReLU for more details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.rrelu_", "path": "generated/torch.nn.functional.rrelu_", "type": "NN Functions", "text": ["In-place version of rrelu()."]}, {"name": "torch.nn.functional.torch.nn.functional.scaled_dot_product_attention", "path": "generated/torch.nn.functional.scaled_dot_product_attention", "type": "NN Functions", "text": ["Computes scaled dot product attention on query, key and value tensors, using an optional attention mask if passed, and applying dropout if a probability greater than 0.0 is specified.", "Warning", "This function is beta and subject to change.", "Note", "There are currently three supported implementations of scaled dot product attention:", "The function may call optimized kernels for improved performance when using the CUDA backend. For all other backends, the PyTorch implementation will be used.", "All implementations are enabled by default. Scaled dot product attention attempts to automatically select the most optimal implementation based on the inputs. In order to provide more fine-grained control over what implementation is used, the following functions are provided for enabling and disabling implementations. The context manager is the preferred mechanism:", "Each of the fused kernels has specific input limitations. If the user requires the use of a specific fused implementation, disable the PyTorch C++ implementation using torch.backends.cuda.sdp_kernel(). In the event that a fused implementation is not available, an error will be raised with the reasons why the fused implementation cannot run.", "Due to the nature of fusing floating point operations, the output of this function may be different depending on what backend kernel is chosen. The c++ implementation supports torch.float64 and can be used when higher precision is required. For more information please see Numerical accuracy", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Attention output; shape (N,...,L,Ev)(N, ..., L, Ev).", "output (Tensor)", "Examples:"]}, {"name": "torch.nn.functional.torch.nn.functional.selu", "path": "generated/torch.nn.functional.selu", "type": "NN Functions", "text": ["Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.", "See SELU for more details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.sigmoid", "path": "generated/torch.nn.functional.sigmoid", "type": "NN Functions", "text": ["Applies the element-wise function Sigmoid(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}", "See Sigmoid for more details."]}, {"name": "torch.nn.functional.torch.nn.functional.silu", "path": "generated/torch.nn.functional.silu", "type": "NN Functions", "text": ["Applies the Sigmoid Linear Unit (SiLU) function, element-wise. The SiLU function is also known as the swish function.", "Note", "See Gaussian Error Linear Units (GELUs) where the SiLU (Sigmoid Linear Unit) was originally coined, and see Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning and Swish: a Self-Gated Activation Function where the SiLU was experimented with later.", "See SiLU for more details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.smooth_l1_loss", "path": "generated/torch.nn.functional.smooth_l1_loss", "type": "NN Functions", "text": ["Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.", "See SmoothL1Loss for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.soft_margin_loss", "path": "generated/torch.nn.functional.soft_margin_loss", "type": "NN Functions", "text": ["See SoftMarginLoss for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.softmax", "path": "generated/torch.nn.functional.softmax", "type": "NN Functions", "text": ["Applies a softmax function.", "Softmax is defined as:", "Softmax(xi)=exp\u2061(xi)\u2211jexp\u2061(xj)\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}", "It is applied to all slices along dim, and will re-scale them so that the elements lie in the range [0, 1] and sum to 1.", "See Softmax for more details.", "Tensor", "Note", "This function doesn\u2019t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use log_softmax instead (it\u2019s faster and has better numerical properties)."]}, {"name": "torch.nn.functional.torch.nn.functional.softmin", "path": "generated/torch.nn.functional.softmin", "type": "NN Functions", "text": ["Applies a softmin function.", "Note that Softmin(x)=Softmax(\u2212x)\\text{Softmin}(x) = \\text{Softmax}(-x). See softmax definition for mathematical formula.", "See Softmin for more details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.softplus", "path": "generated/torch.nn.functional.softplus", "type": "NN Functions", "text": ["Applies element-wise, the function Softplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x)).", "For numerical stability the implementation reverts to the linear function when input\u00d7\u03b2>thresholdinput \\times \\beta > threshold.", "See Softplus for more details."]}, {"name": "torch.nn.functional.torch.nn.functional.softshrink", "path": "generated/torch.nn.functional.softshrink", "type": "NN Functions", "text": ["Applies the soft shrinkage function elementwise", "See Softshrink for more details."]}, {"name": "torch.nn.functional.torch.nn.functional.softsign", "path": "generated/torch.nn.functional.softsign", "type": "NN Functions", "text": ["Applies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}", "See Softsign for more details."]}, {"name": "torch.nn.functional.torch.nn.functional.tanh", "path": "generated/torch.nn.functional.tanh", "type": "NN Functions", "text": ["Applies element-wise, Tanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}", "See Tanh for more details."]}, {"name": "torch.nn.functional.torch.nn.functional.tanhshrink", "path": "generated/torch.nn.functional.tanhshrink", "type": "NN Functions", "text": ["Applies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)", "See Tanhshrink for more details."]}, {"name": "torch.nn.functional.torch.nn.functional.threshold", "path": "generated/torch.nn.functional.threshold", "type": "NN Functions", "text": ["Thresholds each element of the input Tensor.", "See Threshold for more details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.threshold_", "path": "generated/torch.nn.functional.threshold_", "type": "NN Functions", "text": ["In-place version of threshold()."]}, {"name": "torch.nn.functional.torch.nn.functional.torch.nn.parallel.data_parallel", "path": "generated/torch.nn.functional.torch.nn.parallel.data_parallel", "type": "NN Functions", "text": ["Evaluates module(input) in parallel across the GPUs given in device_ids.", "This is the functional version of the DataParallel module.", "a Tensor containing the result of module(input) located on output_device", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.triplet_margin_loss", "path": "generated/torch.nn.functional.triplet_margin_loss", "type": "NN Functions", "text": ["See TripletMarginLoss for details", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.triplet_margin_with_distance_loss", "path": "generated/torch.nn.functional.triplet_margin_with_distance_loss", "type": "NN Functions", "text": ["See TripletMarginWithDistanceLoss for details.", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.unfold", "path": "generated/torch.nn.functional.unfold", "type": "NN Functions", "text": ["Extracts sliding local blocks from a batched input tensor.", "Warning", "Currently, only 4-D input tensors (batched image-like tensors) are supported.", "Warning", "More than one element of the unfolded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensor, please clone it first.", "See torch.nn.Unfold for details", "Tensor"]}, {"name": "torch.nn.functional.torch.nn.functional.upsample", "path": "generated/torch.nn.functional.upsample", "type": "NN Functions", "text": ["Upsamples the input to either the given size or the given scale_factor", "Warning", "This function is deprecated in favor of torch.nn.functional.interpolate(). This is equivalent with nn.functional.interpolate(...).", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.", "The algorithm used for upsampling is determined by mode.", "Currently temporal, spatial and volumetric upsampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape.", "The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width.", "The modes available for upsampling are: nearest, linear (3D-only), bilinear, bicubic (4D-only), trilinear (5D-only)", "Note", "With mode='bicubic', it\u2019s possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call result.clamp(min=0, max=255) if you want to reduce the overshoot when displaying the image.", "Warning", "With align_corners = True, the linearly interpolating modes (linear, bilinear, and trilinear) don\u2019t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See Upsample for concrete examples on how this affects the outputs."]}, {"name": "torch.nn.functional.torch.nn.functional.upsample_bilinear", "path": "generated/torch.nn.functional.upsample_bilinear", "type": "NN Functions", "text": ["Upsamples the input, using bilinear upsampling.", "Warning", "This function is deprecated in favor of torch.nn.functional.interpolate(). This is equivalent with nn.functional.interpolate(..., mode='bilinear', align_corners=True).", "Expected inputs are spatial (4 dimensional). Use upsample_trilinear fo volumetric (5 dimensional) inputs.", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information."]}, {"name": "torch.nn.functional.torch.nn.functional.upsample_nearest", "path": "generated/torch.nn.functional.upsample_nearest", "type": "NN Functions", "text": ["Upsamples the input, using nearest neighbours\u2019 pixel values.", "Warning", "This function is deprecated in favor of torch.nn.functional.interpolate(). This is equivalent with nn.functional.interpolate(..., mode='nearest').", "Currently spatial and volumetric upsampling are supported (i.e. expected inputs are 4 or 5 dimensional).", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information."]}, {"name": "torch.nn.functional.triplet_margin_loss()", "path": "generated/torch.nn.functional.triplet_margin_loss#torch.nn.functional.triplet_margin_loss", "type": "NN Functions", "text": ["See TripletMarginLoss for details", "Tensor"]}, {"name": "torch.nn.functional.triplet_margin_with_distance_loss()", "path": "generated/torch.nn.functional.triplet_margin_with_distance_loss#torch.nn.functional.triplet_margin_with_distance_loss", "type": "NN Functions", "text": ["See TripletMarginWithDistanceLoss for details.", "Tensor"]}, {"name": "torch.nn.functional.unfold()", "path": "generated/torch.nn.functional.unfold#torch.nn.functional.unfold", "type": "NN Functions", "text": ["Extracts sliding local blocks from a batched input tensor.", "Warning", "Currently, only 4-D input tensors (batched image-like tensors) are supported.", "Warning", "More than one element of the unfolded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensor, please clone it first.", "See torch.nn.Unfold for details", "Tensor"]}, {"name": "torch.nn.functional.upsample()", "path": "generated/torch.nn.functional.upsample#torch.nn.functional.upsample", "type": "NN Functions", "text": ["Upsamples the input to either the given size or the given scale_factor", "Warning", "This function is deprecated in favor of torch.nn.functional.interpolate(). This is equivalent with nn.functional.interpolate(...).", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.", "The algorithm used for upsampling is determined by mode.", "Currently temporal, spatial and volumetric upsampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape.", "The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width.", "The modes available for upsampling are: nearest, linear (3D-only), bilinear, bicubic (4D-only), trilinear (5D-only)", "Note", "With mode='bicubic', it\u2019s possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call result.clamp(min=0, max=255) if you want to reduce the overshoot when displaying the image.", "Warning", "With align_corners = True, the linearly interpolating modes (linear, bilinear, and trilinear) don\u2019t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See Upsample for concrete examples on how this affects the outputs."]}, {"name": "torch.nn.functional.upsample_bilinear()", "path": "generated/torch.nn.functional.upsample_bilinear#torch.nn.functional.upsample_bilinear", "type": "NN Functions", "text": ["Upsamples the input, using bilinear upsampling.", "Warning", "This function is deprecated in favor of torch.nn.functional.interpolate(). This is equivalent with nn.functional.interpolate(..., mode='bilinear', align_corners=True).", "Expected inputs are spatial (4 dimensional). Use upsample_trilinear fo volumetric (5 dimensional) inputs.", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information."]}, {"name": "torch.nn.functional.upsample_nearest()", "path": "generated/torch.nn.functional.upsample_nearest#torch.nn.functional.upsample_nearest", "type": "NN Functions", "text": ["Upsamples the input, using nearest neighbours\u2019 pixel values.", "Warning", "This function is deprecated in favor of torch.nn.functional.interpolate(). This is equivalent with nn.functional.interpolate(..., mode='nearest').", "Currently spatial and volumetric upsampling are supported (i.e. expected inputs are 4 or 5 dimensional).", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information."]}, {"name": "torch.nn.GaussianNLLLoss", "path": "generated/torch.nn.gaussiannllloss", "type": "Neuro Network", "text": ["Gaussian negative log likelihood loss.", "The targets are treated as samples from Gaussian distributions with expectations and variances predicted by the neural network. For a target tensor modelled as having Gaussian distribution with a tensor of expectations input and a tensor of positive variances var the loss is:", "where eps is used for stability. By default, the constant term of the loss function is omitted unless full is True. If var is not the same size as input (due to a homoscedastic assumption), it must either have a final dimension of 1 or have one fewer dimension (with all other sizes being the same) for correct broadcasting.", "Note", "The clamping of var is ignored with respect to autograd, and so the gradients are unaffected by it.", "Nix, D. A. and Weigend, A. S., \u201cEstimating the mean and variance of the target probability distribution\u201d, Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN\u201994), Orlando, FL, USA, 1994, pp. 55-60 vol.1, doi: 10.1109/ICNN.1994.374138."]}, {"name": "torch.nn.GELU", "path": "generated/torch.nn.gelu", "type": "Neuro Network", "text": ["Applies the Gaussian Error Linear Units function:", "where \u03a6(x)\\Phi(x) is the Cumulative Distribution Function for Gaussian Distribution.", "When the approximate argument is \u2018tanh\u2019, Gelu is estimated with:", "approximate (str, optional) \u2013 the gelu approximation algorithm to use: 'none' | 'tanh'. Default: 'none'", "Examples:"]}, {"name": "torch.nn.GLU", "path": "generated/torch.nn.glu", "type": "Neuro Network", "text": ["Applies the gated linear unit function GLU(a,b)=a\u2297\u03c3(b){GLU}(a, b)= a \\otimes \\sigma(b) where aa is the first half of the input matrices and bb is the second half.", "dim (int) \u2013 the dimension on which to split the input. Default: -1", "Examples:"]}, {"name": "torch.nn.GroupNorm", "path": "generated/torch.nn.groupnorm", "type": "Neuro Network", "text": ["Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization", "The input channels are separated into num_groups groups, each containing num_channels / num_groups channels. num_channels must be divisible by num_groups. The mean and standard-deviation are calculated separately over the each group. \u03b3\\gamma and \u03b2\\beta are learnable per-channel affine transform parameter vectors of size num_channels if affine is True. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False).", "This layer uses statistics computed from input data in both training and evaluation modes.", "Examples:"]}, {"name": "torch.nn.GRU", "path": "generated/torch.nn.gru", "type": "Neuro Network", "text": ["Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.", "For each element in the input sequence, each layer computes the following function:", "where hth_t is the hidden state at time t, xtx_t is the input at time t, h(t\u22121)h_{(t-1)} is the hidden state of the layer at time t-1 or the initial hidden state at time 0, and rtr_t, ztz_t, ntn_t are the reset, update, and new gates, respectively. \u03c3\\sigma is the sigmoid function, and \u2217* is the Hadamard product.", "In a multilayer GRU, the input xt(l)x^{(l)}_t of the ll -th layer (l\u22652l \\ge 2) is the hidden state ht(l\u22121)h^{(l-1)}_t of the previous layer multiplied by dropout \u03b4t(l\u22121)\\delta^{(l-1)}_t where each \u03b4t(l\u22121)\\delta^{(l-1)}_t is a Bernoulli random variable which is 00 with probability dropout.", "where:", "Note", "All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}) where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}", "Note", "For bidirectional GRUs, forward and backward are directions 0 and 1 respectively. Example of splitting the output layers when batch_first=False: output.view(seq_len, batch, num_directions, hidden_size).", "Note", "batch_first argument is ignored for unbatched inputs.", "Note", "The calculation of new gate ntn_t subtly differs from the original paper and other frameworks. In the original implementation, the Hadamard product (\u2217)(*) between rtr_t and the previous hidden state h(t\u22121)h_{(t-1)} is done before the multiplication with the weight matrix W and addition of bias:", "This is in contrast to PyTorch implementation, which is done after Whnh(t\u22121)W_{hn} h_{(t-1)}", "This implementation differs on purpose for efficiency.", "Note", "If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance.", "Examples:"]}, {"name": "torch.nn.GRUCell", "path": "generated/torch.nn.grucell", "type": "Neuro Network", "text": ["A gated recurrent unit (GRU) cell", "where \u03c3\\sigma is the sigmoid function, and \u2217* is the Hadamard product.", "Note", "All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}) where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}", "On certain ROCm devices, when using float16 inputs this module will use different precision for backward.", "Examples:"]}, {"name": "torch.nn.Hardshrink", "path": "generated/torch.nn.hardshrink", "type": "Neuro Network", "text": ["Applies the Hard Shrinkage (Hardshrink) function element-wise.", "Hardshrink is defined as:", "lambd (float) \u2013 the \u03bb\\lambda value for the Hardshrink formulation. Default: 0.5", "Examples:"]}, {"name": "torch.nn.Hardsigmoid", "path": "generated/torch.nn.hardsigmoid", "type": "Neuro Network", "text": ["Applies the Hardsigmoid function element-wise.", "Hardsigmoid is defined as:", "inplace (bool) \u2013 can optionally do the operation in-place. Default: False", "Examples:"]}, {"name": "torch.nn.Hardswish", "path": "generated/torch.nn.hardswish", "type": "Neuro Network", "text": ["Applies the Hardswish function, element-wise, as described in the paper: Searching for MobileNetV3.", "Hardswish is defined as:", "inplace (bool) \u2013 can optionally do the operation in-place. Default: False", "Examples:"]}, {"name": "torch.nn.Hardtanh", "path": "generated/torch.nn.hardtanh", "type": "Neuro Network", "text": ["Applies the HardTanh function element-wise.", "HardTanh is defined as:", "Keyword arguments min_value and max_value have been deprecated in favor of min_val and max_val.", "Examples:"]}, {"name": "torch.nn.HingeEmbeddingLoss", "path": "generated/torch.nn.hingeembeddingloss", "type": "Neuro Network", "text": ["Measures the loss given an input tensor xx and a labels tensor yy (containing 1 or -1). This is usually used for measuring whether two inputs are similar or dissimilar, e.g. using the L1 pairwise distance as xx, and is typically used for learning nonlinear embeddings or semi-supervised learning.", "The loss function for nn-th sample in the mini-batch is", "and the total loss functions is", "where L={l1,\u2026,lN}\u22a4L = \\{l_1,\\dots,l_N\\}^\\top."]}, {"name": "torch.nn.HuberLoss", "path": "generated/torch.nn.huberloss", "type": "Neuro Network", "text": ["Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. This loss combines advantages of both L1Loss and MSELoss; the delta-scaled L1 region makes the loss less sensitive to outliers than MSELoss, while the L2 region provides smoothness over L1Loss near 0. See Huber loss for more information.", "For a batch of size NN, the unreduced loss can be described as:", "with", "If reduction is not none, then:", "Note", "When delta is set to 1, this loss is equivalent to SmoothL1Loss. In general, this loss differs from SmoothL1Loss by a factor of delta (AKA beta in Smooth L1). See SmoothL1Loss for additional discussion on the differences in behavior between the two losses."]}, {"name": "torch.nn.Identity", "path": "generated/torch.nn.identity", "type": "Neuro Network", "text": ["A placeholder identity operator that is argument-insensitive.", "Examples:"]}, {"name": "torch.nn.Identity", "path": "generated/torch.nn.utils.prune.identity", "type": "Neuro Network", "text": ["Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.", "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.init", "path": "nn.init", "type": "Parameter Initializations", "text": ["Warning", "All the functions in this module are intended to be used to initialize neural network parameters, so they all run in torch.no_grad() mode and will not be taken into account by autograd.", "Return the recommended gain value for the given nonlinearity function. The values are as follows:", "nonlinearity", "gain", "Linear / Identity", "11", "Conv{1,2,3}D", "11", "Sigmoid", "11", "Tanh", "53\\frac{5}{3}", "ReLU", "2\\sqrt{2}", "Leaky Relu", "21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}", "SELU", "34\\frac{3}{4}", "Warning", "In order to implement Self-Normalizing Neural Networks , you should use nonlinearity='linear' instead of nonlinearity='selu'. This gives the initial weights a variance of 1 / N, which is necessary to induce a stable fixed point in the forward pass. In contrast, the default gain for SELU sacrifices the normalization effect for more stable gradient flow in rectangular layers.", "Fills the input Tensor with values drawn from the uniform distribution U(a,b)\\mathcal{U}(a, b).", "Tensor", "Fills the input Tensor with values drawn from the normal distribution N(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2).", "Tensor", "Fills the input Tensor with the value val\\text{val}.", "Tensor", "Fills the input Tensor with the scalar value 1.", "tensor (Tensor) \u2013 an n-dimensional torch.Tensor", "Tensor", "Fills the input Tensor with the scalar value 0.", "tensor (Tensor) \u2013 an n-dimensional torch.Tensor", "Tensor", "Fills the 2-dimensional input Tensor with the identity matrix. Preserves the identity of the inputs in Linear layers, where as many inputs are preserved as possible.", "tensor \u2013 a 2-dimensional torch.Tensor", "Fills the {3, 4, 5}-dimensional input Tensor with the Dirac delta function. Preserves the identity of the inputs in Convolutional layers, where as many input channels are preserved as possible. In case of groups>1, each group of channels preserves identity", "Fills the input Tensor with values according to the method described in Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), using a uniform distribution. The resulting tensor will have values sampled from U(\u2212a,a)\\mathcal{U}(-a, a) where", "Also known as Glorot initialization.", "Tensor", "Fills the input Tensor with values according to the method described in Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), using a normal distribution. The resulting tensor will have values sampled from N(0,std2)\\mathcal{N}(0, \\text{std}^2) where", "Also known as Glorot initialization.", "Tensor", "Fills the input Tensor with values according to the method described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - He, K. et al. (2015), using a uniform distribution. The resulting tensor will have values sampled from U(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound}) where", "Also known as He initialization.", "Fills the input Tensor with values according to the method described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - He, K. et al. (2015), using a normal distribution. The resulting tensor will have values sampled from N(0,std2)\\mathcal{N}(0, \\text{std}^2) where", "Also known as He initialization.", "Fills the input Tensor with values drawn from a truncated normal distribution. The values are effectively drawn from the normal distribution N(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2) with values outside [a,b][a, b] redrawn until they are within the bounds. The method used for generating the random values works best when a\u2264mean\u2264ba \\leq \\text{mean} \\leq b.", "Tensor", "Fills the input Tensor with a (semi) orthogonal matrix, as described in Exact solutions to the nonlinear dynamics of learning in deep linear neural networks - Saxe, A. et al. (2013). The input tensor must have at least 2 dimensions, and for tensors with more than 2 dimensions the trailing dimensions are flattened.", "Fills the 2D input Tensor as a sparse matrix, where the non-zero elements will be drawn from the normal distribution N(0,0.01)\\mathcal{N}(0, 0.01), as described in Deep learning via Hessian-free optimization - Martens, J. (2010)."]}, {"name": "torch.nn.init.calculate_gain()", "path": "nn.init#torch.nn.init.calculate_gain", "type": "Parameter Initializations", "text": ["Return the recommended gain value for the given nonlinearity function. The values are as follows:", "nonlinearity", "gain", "Linear / Identity", "11", "Conv{1,2,3}D", "11", "Sigmoid", "11", "Tanh", "53\\frac{5}{3}", "ReLU", "2\\sqrt{2}", "Leaky Relu", "21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}", "SELU", "34\\frac{3}{4}", "Warning", "In order to implement Self-Normalizing Neural Networks , you should use nonlinearity='linear' instead of nonlinearity='selu'. This gives the initial weights a variance of 1 / N, which is necessary to induce a stable fixed point in the forward pass. In contrast, the default gain for SELU sacrifices the normalization effect for more stable gradient flow in rectangular layers."]}, {"name": "torch.nn.init.constant_()", "path": "nn.init#torch.nn.init.constant_", "type": "Parameter Initializations", "text": ["Fills the input Tensor with the value val\\text{val}.", "Tensor"]}, {"name": "torch.nn.init.dirac_()", "path": "nn.init#torch.nn.init.dirac_", "type": "Parameter Initializations", "text": ["Fills the {3, 4, 5}-dimensional input Tensor with the Dirac delta function. Preserves the identity of the inputs in Convolutional layers, where as many input channels are preserved as possible. In case of groups>1, each group of channels preserves identity"]}, {"name": "torch.nn.init.eye_()", "path": "nn.init#torch.nn.init.eye_", "type": "Parameter Initializations", "text": ["Fills the 2-dimensional input Tensor with the identity matrix. Preserves the identity of the inputs in Linear layers, where as many inputs are preserved as possible.", "tensor \u2013 a 2-dimensional torch.Tensor"]}, {"name": "torch.nn.init.kaiming_normal_()", "path": "nn.init#torch.nn.init.kaiming_normal_", "type": "Parameter Initializations", "text": ["Fills the input Tensor with values according to the method described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - He, K. et al. (2015), using a normal distribution. The resulting tensor will have values sampled from N(0,std2)\\mathcal{N}(0, \\text{std}^2) where", "Also known as He initialization."]}, {"name": "torch.nn.init.kaiming_uniform_()", "path": "nn.init#torch.nn.init.kaiming_uniform_", "type": "Parameter Initializations", "text": ["Fills the input Tensor with values according to the method described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - He, K. et al. (2015), using a uniform distribution. The resulting tensor will have values sampled from U(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound}) where", "Also known as He initialization."]}, {"name": "torch.nn.init.normal_()", "path": "nn.init#torch.nn.init.normal_", "type": "Parameter Initializations", "text": ["Fills the input Tensor with values drawn from the normal distribution N(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2).", "Tensor"]}, {"name": "torch.nn.init.ones_()", "path": "nn.init#torch.nn.init.ones_", "type": "Parameter Initializations", "text": ["Fills the input Tensor with the scalar value 1.", "tensor (Tensor) \u2013 an n-dimensional torch.Tensor", "Tensor"]}, {"name": "torch.nn.init.orthogonal_()", "path": "nn.init#torch.nn.init.orthogonal_", "type": "Parameter Initializations", "text": ["Fills the input Tensor with a (semi) orthogonal matrix, as described in Exact solutions to the nonlinear dynamics of learning in deep linear neural networks - Saxe, A. et al. (2013). The input tensor must have at least 2 dimensions, and for tensors with more than 2 dimensions the trailing dimensions are flattened."]}, {"name": "torch.nn.init.sparse_()", "path": "nn.init#torch.nn.init.sparse_", "type": "Parameter Initializations", "text": ["Fills the 2D input Tensor as a sparse matrix, where the non-zero elements will be drawn from the normal distribution N(0,0.01)\\mathcal{N}(0, 0.01), as described in Deep learning via Hessian-free optimization - Martens, J. (2010)."]}, {"name": "torch.nn.init.trunc_normal_()", "path": "nn.init#torch.nn.init.trunc_normal_", "type": "Parameter Initializations", "text": ["Fills the input Tensor with values drawn from a truncated normal distribution. The values are effectively drawn from the normal distribution N(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2) with values outside [a,b][a, b] redrawn until they are within the bounds. The method used for generating the random values works best when a\u2264mean\u2264ba \\leq \\text{mean} \\leq b.", "Tensor"]}, {"name": "torch.nn.init.uniform_()", "path": "nn.init#torch.nn.init.uniform_", "type": "Parameter Initializations", "text": ["Fills the input Tensor with values drawn from the uniform distribution U(a,b)\\mathcal{U}(a, b).", "Tensor"]}, {"name": "torch.nn.init.xavier_normal_()", "path": "nn.init#torch.nn.init.xavier_normal_", "type": "Parameter Initializations", "text": ["Fills the input Tensor with values according to the method described in Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), using a normal distribution. The resulting tensor will have values sampled from N(0,std2)\\mathcal{N}(0, \\text{std}^2) where", "Also known as Glorot initialization.", "Tensor"]}, {"name": "torch.nn.init.xavier_uniform_()", "path": "nn.init#torch.nn.init.xavier_uniform_", "type": "Parameter Initializations", "text": ["Fills the input Tensor with values according to the method described in Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), using a uniform distribution. The resulting tensor will have values sampled from U(\u2212a,a)\\mathcal{U}(-a, a) where", "Also known as Glorot initialization.", "Tensor"]}, {"name": "torch.nn.init.zeros_()", "path": "nn.init#torch.nn.init.zeros_", "type": "Parameter Initializations", "text": ["Fills the input Tensor with the scalar value 0.", "tensor (Tensor) \u2013 an n-dimensional torch.Tensor", "Tensor"]}, {"name": "torch.nn.InstanceNorm1d", "path": "generated/torch.nn.instancenorm1d", "type": "Neuro Network", "text": ["Applies Instance Normalization over a 2D (unbatched) or 3D (batched) input as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.", "The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. \u03b3\\gamma and \u03b2\\beta are learnable parameter vectors of size C (where C is the number of features or channels of the input) if affine is True. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False).", "By default, this layer uses instance statistics computed from input data in both training and evaluation modes.", "If track_running_stats is set to True, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.", "Note", "This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t, where x^\\hat{x} is the estimated statistic and xtx_t is the new observed value.", "Note", "InstanceNorm1d and LayerNorm are very similar, but have some subtle differences. InstanceNorm1d is applied on each channel of channeled data like multidimensional time series, but LayerNorm is usually applied on entire sample and often in NLP tasks. Additionally, LayerNorm applies elementwise affine transform, while InstanceNorm1d usually don\u2019t apply affine transform.", "Examples:"]}, {"name": "torch.nn.InstanceNorm2d", "path": "generated/torch.nn.instancenorm2d", "type": "Neuro Network", "text": ["Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.", "The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. \u03b3\\gamma and \u03b2\\beta are learnable parameter vectors of size C (where C is the input size) if affine is True. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False).", "By default, this layer uses instance statistics computed from input data in both training and evaluation modes.", "If track_running_stats is set to True, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.", "Note", "This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t, where x^\\hat{x} is the estimated statistic and xtx_t is the new observed value.", "Note", "InstanceNorm2d and LayerNorm are very similar, but have some subtle differences. InstanceNorm2d is applied on each channel of channeled data like RGB images, but LayerNorm is usually applied on entire sample and often in NLP tasks. Additionally, LayerNorm applies elementwise affine transform, while InstanceNorm2d usually don\u2019t apply affine transform.", "Examples:"]}, {"name": "torch.nn.InstanceNorm3d", "path": "generated/torch.nn.instancenorm3d", "type": "Neuro Network", "text": ["Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.", "The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. \u03b3\\gamma and \u03b2\\beta are learnable parameter vectors of size C (where C is the input size) if affine is True. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False).", "By default, this layer uses instance statistics computed from input data in both training and evaluation modes.", "If track_running_stats is set to True, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.", "Note", "This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t, where x^\\hat{x} is the estimated statistic and xtx_t is the new observed value.", "Note", "InstanceNorm3d and LayerNorm are very similar, but have some subtle differences. InstanceNorm3d is applied on each channel of channeled data like 3D models with RGB color, but LayerNorm is usually applied on entire sample and often in NLP tasks. Additionally, LayerNorm applies elementwise affine transform, while InstanceNorm3d usually don\u2019t apply affine transform.", "Examples:"]}, {"name": "torch.nn.KLDivLoss", "path": "generated/torch.nn.kldivloss", "type": "Neuro Network", "text": ["The Kullback-Leibler divergence loss.", "For tensors of the same shape ypred,ytruey_{\\text{pred}},\\ y_{\\text{true}}, where ypredy_{\\text{pred}} is the input and ytruey_{\\text{true}} is the target, we define the pointwise KL-divergence as", "To avoid underflow issues when computing this quantity, this loss expects the argument input in the log-space. The argument target may also be provided in the log-space if log_target= True.", "To summarise, this function is roughly equivalent to computing", "and then reducing this result depending on the argument reduction as", "Note", "As all the other losses in PyTorch, this function expects the first argument, input, to be the output of the model (e.g. the neural network) and the second, target, to be the observations in the dataset. This differs from the standard mathematical notation KL(P\u2223\u2223Q)KL(P\\ ||\\ Q) where PP denotes the distribution of the observations and QQ denotes the model.", "Warning", "reduction= \u201cmean\u201d doesn\u2019t return the true KL divergence value, please use reduction= \u201cbatchmean\u201d which aligns with the mathematical definition.", "Examples:"]}, {"name": "torch.nn.L1Loss", "path": "generated/torch.nn.l1loss", "type": "Neuro Network", "text": ["Creates a criterion that measures the mean absolute error (MAE) between each element in the input xx and target yy.", "The unreduced (i.e. with reduction set to 'none') loss can be described as:", "where NN is the batch size. If reduction is not 'none' (default 'mean'), then:", "xx and yy are tensors of arbitrary shapes with a total of nn elements each.", "The sum operation still operates over all the elements, and divides by nn.", "The division by nn can be avoided if one sets reduction = 'sum'.", "Supports real-valued and complex-valued inputs.", "Examples:"]}, {"name": "torch.nn.L1Unstructured", "path": "generated/torch.nn.utils.prune.l1unstructured", "type": "Neuro Network", "text": ["Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.", "amount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.", "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.LayerNorm", "path": "generated/torch.nn.layernorm", "type": "Neuro Network", "text": ["Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization", "The mean and standard-deviation are calculated over the last D dimensions, where D is the dimension of normalized_shape. For example, if normalized_shape is (3, 5) (a 2-dimensional shape), the mean and standard-deviation are computed over the last 2 dimensions of the input (i.e. input.mean((-2, -1))). \u03b3\\gamma and \u03b2\\beta are learnable affine transform parameters of normalized_shape if elementwise_affine is True. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False).", "Note", "Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the affine option, Layer Normalization applies per-element scale and bias with elementwise_affine.", "This layer uses statistics computed from input data in both training and evaluation modes.", "normalized_shape (int or list or torch.Size) \u2013 ", "input shape from an expected input of size", "If a single integer is used, it is treated as a singleton list, and this module will normalize over the last dimension which is expected to be of that specific size.", "Examples:"]}, {"name": "torch.nn.LazyBatchNorm1d", "path": "generated/torch.nn.lazybatchnorm1d", "type": "Neuro Network", "text": ["A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size(1). The attributes that will be lazily initialized are weight, bias, running_mean and running_var.", "Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation on lazy modules and their limitations.", "alias of BatchNorm1d"]}, {"name": "torch.nn.LazyBatchNorm1d.cls_to_become", "path": "generated/torch.nn.lazybatchnorm1d#torch.nn.LazyBatchNorm1d.cls_to_become", "type": "Neuro Network", "text": ["alias of BatchNorm1d"]}, {"name": "torch.nn.LazyBatchNorm2d", "path": "generated/torch.nn.lazybatchnorm2d", "type": "Neuro Network", "text": ["A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size(1). The attributes that will be lazily initialized are weight, bias, running_mean and running_var.", "Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation on lazy modules and their limitations.", "alias of BatchNorm2d"]}, {"name": "torch.nn.LazyBatchNorm2d.cls_to_become", "path": "generated/torch.nn.lazybatchnorm2d#torch.nn.LazyBatchNorm2d.cls_to_become", "type": "Neuro Network", "text": ["alias of BatchNorm2d"]}, {"name": "torch.nn.LazyBatchNorm3d", "path": "generated/torch.nn.lazybatchnorm3d", "type": "Neuro Network", "text": ["A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size(1). The attributes that will be lazily initialized are weight, bias, running_mean and running_var.", "Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation on lazy modules and their limitations.", "alias of BatchNorm3d"]}, {"name": "torch.nn.LazyBatchNorm3d.cls_to_become", "path": "generated/torch.nn.lazybatchnorm3d#torch.nn.LazyBatchNorm3d.cls_to_become", "type": "Neuro Network", "text": ["alias of BatchNorm3d"]}, {"name": "torch.nn.LazyConv1d", "path": "generated/torch.nn.lazyconv1d", "type": "Neuro Network", "text": ["A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size(1). The attributes that will be lazily initialized are weight and bias.", "Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation on lazy modules and their limitations.", "See also", "torch.nn.Conv1d and torch.nn.modules.lazy.LazyModuleMixin", "alias of Conv1d"]}, {"name": "torch.nn.LazyConv1d.cls_to_become", "path": "generated/torch.nn.lazyconv1d#torch.nn.LazyConv1d.cls_to_become", "type": "Neuro Network", "text": ["alias of Conv1d"]}, {"name": "torch.nn.LazyConv2d", "path": "generated/torch.nn.lazyconv2d", "type": "Neuro Network", "text": ["A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size(1). The attributes that will be lazily initialized are weight and bias.", "Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation on lazy modules and their limitations.", "See also", "torch.nn.Conv2d and torch.nn.modules.lazy.LazyModuleMixin", "alias of Conv2d"]}, {"name": "torch.nn.LazyConv2d.cls_to_become", "path": "generated/torch.nn.lazyconv2d#torch.nn.LazyConv2d.cls_to_become", "type": "Neuro Network", "text": ["alias of Conv2d"]}, {"name": "torch.nn.LazyConv3d", "path": "generated/torch.nn.lazyconv3d", "type": "Neuro Network", "text": ["A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size(1). The attributes that will be lazily initialized are weight and bias.", "Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation on lazy modules and their limitations.", "See also", "torch.nn.Conv3d and torch.nn.modules.lazy.LazyModuleMixin", "alias of Conv3d"]}, {"name": "torch.nn.LazyConv3d.cls_to_become", "path": "generated/torch.nn.lazyconv3d#torch.nn.LazyConv3d.cls_to_become", "type": "Neuro Network", "text": ["alias of Conv3d"]}, {"name": "torch.nn.LazyConvTranspose1d", "path": "generated/torch.nn.lazyconvtranspose1d", "type": "Neuro Network", "text": ["A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size(1). The attributes that will be lazily initialized are weight and bias.", "Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation on lazy modules and their limitations.", "See also", "torch.nn.ConvTranspose1d and torch.nn.modules.lazy.LazyModuleMixin", "alias of ConvTranspose1d"]}, {"name": "torch.nn.LazyConvTranspose1d.cls_to_become", "path": "generated/torch.nn.lazyconvtranspose1d#torch.nn.LazyConvTranspose1d.cls_to_become", "type": "Neuro Network", "text": ["alias of ConvTranspose1d"]}, {"name": "torch.nn.LazyConvTranspose2d", "path": "generated/torch.nn.lazyconvtranspose2d", "type": "Neuro Network", "text": ["A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size(1). The attributes that will be lazily initialized are weight and bias.", "Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation on lazy modules and their limitations.", "See also", "torch.nn.ConvTranspose2d and torch.nn.modules.lazy.LazyModuleMixin", "alias of ConvTranspose2d"]}, {"name": "torch.nn.LazyConvTranspose2d.cls_to_become", "path": "generated/torch.nn.lazyconvtranspose2d#torch.nn.LazyConvTranspose2d.cls_to_become", "type": "Neuro Network", "text": ["alias of ConvTranspose2d"]}, {"name": "torch.nn.LazyConvTranspose3d", "path": "generated/torch.nn.lazyconvtranspose3d", "type": "Neuro Network", "text": ["A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size(1). The attributes that will be lazily initialized are weight and bias.", "Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation on lazy modules and their limitations.", "See also", "torch.nn.ConvTranspose3d and torch.nn.modules.lazy.LazyModuleMixin", "alias of ConvTranspose3d"]}, {"name": "torch.nn.LazyConvTranspose3d.cls_to_become", "path": "generated/torch.nn.lazyconvtranspose3d#torch.nn.LazyConvTranspose3d.cls_to_become", "type": "Neuro Network", "text": ["alias of ConvTranspose3d"]}, {"name": "torch.nn.LazyInstanceNorm1d", "path": "generated/torch.nn.lazyinstancenorm1d", "type": "Neuro Network", "text": ["A torch.nn.InstanceNorm1d module with lazy initialization of the num_features argument of the InstanceNorm1d that is inferred from the input.size(1). The attributes that will be lazily initialized are weight, bias, running_mean and running_var.", "Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation on lazy modules and their limitations.", "alias of InstanceNorm1d"]}, {"name": "torch.nn.LazyInstanceNorm1d.cls_to_become", "path": "generated/torch.nn.lazyinstancenorm1d#torch.nn.LazyInstanceNorm1d.cls_to_become", "type": "Neuro Network", "text": ["alias of InstanceNorm1d"]}, {"name": "torch.nn.LazyInstanceNorm2d", "path": "generated/torch.nn.lazyinstancenorm2d", "type": "Neuro Network", "text": ["A torch.nn.InstanceNorm2d module with lazy initialization of the num_features argument of the InstanceNorm2d that is inferred from the input.size(1). The attributes that will be lazily initialized are weight, bias, running_mean and running_var.", "Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation on lazy modules and their limitations.", "alias of InstanceNorm2d"]}, {"name": "torch.nn.LazyInstanceNorm2d.cls_to_become", "path": "generated/torch.nn.lazyinstancenorm2d#torch.nn.LazyInstanceNorm2d.cls_to_become", "type": "Neuro Network", "text": ["alias of InstanceNorm2d"]}, {"name": "torch.nn.LazyInstanceNorm3d", "path": "generated/torch.nn.lazyinstancenorm3d", "type": "Neuro Network", "text": ["A torch.nn.InstanceNorm3d module with lazy initialization of the num_features argument of the InstanceNorm3d that is inferred from the input.size(1). The attributes that will be lazily initialized are weight, bias, running_mean and running_var.", "Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation on lazy modules and their limitations.", "alias of InstanceNorm3d"]}, {"name": "torch.nn.LazyInstanceNorm3d.cls_to_become", "path": "generated/torch.nn.lazyinstancenorm3d#torch.nn.LazyInstanceNorm3d.cls_to_become", "type": "Neuro Network", "text": ["alias of InstanceNorm3d"]}, {"name": "torch.nn.LazyLinear", "path": "generated/torch.nn.lazylinear", "type": "Neuro Network", "text": ["A torch.nn.Linear module where in_features is inferred.", "In this module, the weight and bias are of torch.nn.UninitializedParameter class. They will be initialized after the first call to forward is done and the module will become a regular torch.nn.Linear module. The in_features argument of the Linear is inferred from the input.shape[-1].", "Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation on lazy modules and their limitations.", "alias of Linear"]}, {"name": "torch.nn.LazyLinear.cls_to_become", "path": "generated/torch.nn.lazylinear#torch.nn.LazyLinear.cls_to_become", "type": "Neuro Network", "text": ["alias of Linear"]}, {"name": "torch.nn.LazyModuleMixin", "path": "generated/torch.nn.modules.lazy.lazymodulemixin", "type": "Neuro Network", "text": ["A mixin for modules that lazily initialize parameters, also known as \u201clazy modules.\u201d", "Modules that lazily initialize parameters, or \u201clazy modules\u201d, derive the shapes of their parameters from the first input(s) to their forward method. Until that first forward they contain torch.nn.UninitializedParameter s that should not be accessed or used, and afterward they contain regular torch.nn.Parameter s. Lazy modules are convenient since they don\u2019t require computing some module arguments, like the in_features argument of a typical torch.nn.Linear.", "After construction, networks with lazy modules should first be converted to the desired dtype and placed on the expected device. This is because lazy modules only perform shape inference so the usual dtype and device placement behavior applies. The lazy modules should then perform \u201cdry runs\u201d to initialize all the components in the module. These \u201cdry runs\u201d send inputs of the correct size, dtype, and device through the network and to each one of its lazy modules. After this the network can be used as usual.", "A final caveat when using lazy modules is that the order of initialization of a network\u2019s parameters may change, since the lazy modules are always initialized after other modules. For example, if the LazyMLP class defined above had a torch.nn.LazyLinear module first and then a regular torch.nn.Linear second, the second module would be initialized on construction and the first module would be initialized during the first dry run. This can cause the parameters of a network using lazy modules to be initialized differently than the parameters of a network without lazy modules as the order of parameter initializations, which often depends on a stateful random number generator, is different. Check Reproducibility for more details.", "Lazy modules can be serialized with a state dict like other modules. For example:", "Lazy modules can load regular torch.nn.Parameter s (i.e. you can serialize/deserialize initialized LazyModules and they will remain initialized)", "Note, however, that the loaded parameters will not be replaced when doing a \u201cdry run\u201d if they are initialized when the state is loaded. This prevents using initialized modules in different contexts.", "Check if a module has parameters that are not initialized", "Initialize parameters according to the input batch properties. This adds an interface to isolate parameter initialization from the forward pass when doing parameter shape inference."]}, {"name": "torch.nn.LeakyReLU", "path": "generated/torch.nn.leakyrelu", "type": "Neuro Network", "text": ["Applies the element-wise function:", "or", "Examples:"]}, {"name": "torch.nn.Linear", "path": "generated/torch.nn.linear", "type": "Neuro Network", "text": ["Applies a linear transformation to the incoming data: y=xAT+by = xA^T + b", "This module supports TensorFloat32.", "On certain ROCm devices, when using float16 inputs this module will use different precision for backward.", "Examples:"]}, {"name": "torch.nn.LnStructured", "path": "generated/torch.nn.utils.prune.lnstructured", "type": "Neuro Network", "text": ["Prune entire (currently unpruned) channels in a tensor based on their Ln-norm.", "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a mask to apply on top of the default_mask by zeroing out the channels along the specified dim with the lowest Ln-norm.", "mask to apply to t, of same dims as t", "mask (torch.Tensor)", "IndexError \u2013 if self.dim >= len(t.shape)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.LocalResponseNorm", "path": "generated/torch.nn.localresponsenorm", "type": "Neuro Network", "text": ["Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels.", "Examples:"]}, {"name": "torch.nn.LogSigmoid", "path": "generated/torch.nn.logsigmoid", "type": "Neuro Network", "text": ["Applies the element-wise function:", "Examples:"]}, {"name": "torch.nn.LogSoftmax", "path": "generated/torch.nn.logsoftmax", "type": "Neuro Network", "text": ["Applies the log\u2061(Softmax(x))\\log(\\text{Softmax}(x)) function to an n-dimensional input Tensor. The LogSoftmax formulation can be simplified as:", "dim (int) \u2013 A dimension along which LogSoftmax will be computed.", "a Tensor of the same dimension and shape as the input with values in the range [-inf, 0)", "None", "Examples:"]}, {"name": "torch.nn.LPPool1d", "path": "generated/torch.nn.lppool1d", "type": "Neuro Network", "text": ["Applies a 1D power-average pooling over an input signal composed of several input planes.", "On each window, the function computed is:", "Note", "If the sum to the power of p is zero, the gradient of this function is not defined. This implementation will set the gradient to zero in this case.", "Output: (N,C,Lout)(N, C, L_{out}) or (C,Lout)(C, L_{out}), where"]}, {"name": "torch.nn.LPPool2d", "path": "generated/torch.nn.lppool2d", "type": "Neuro Network", "text": ["Applies a 2D power-average pooling over an input signal composed of several input planes.", "On each window, the function computed is:", "The parameters kernel_size, stride can either be:", "Note", "If the sum to the power of p is zero, the gradient of this function is not defined. This implementation will set the gradient to zero in this case.", "Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}), where", "Examples:"]}, {"name": "torch.nn.LSTM", "path": "generated/torch.nn.lstm", "type": "Neuro Network", "text": ["Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.", "For each element in the input sequence, each layer computes the following function:", "where hth_t is the hidden state at time t, ctc_t is the cell state at time t, xtx_t is the input at time t, ht\u22121h_{t-1} is the hidden state of the layer at time t-1 or the initial hidden state at time 0, and iti_t, ftf_t, gtg_t, oto_t are the input, forget, cell, and output gates, respectively. \u03c3\\sigma is the sigmoid function, and \u2299\\odot is the Hadamard product.", "In a multilayer LSTM, the input xt(l)x^{(l)}_t of the ll -th layer (l\u22652l \\ge 2) is the hidden state ht(l\u22121)h^{(l-1)}_t of the previous layer multiplied by dropout \u03b4t(l\u22121)\\delta^{(l-1)}_t where each \u03b4t(l\u22121)\\delta^{(l-1)}_t is a Bernoulli random variable which is 00 with probability dropout.", "If proj_size > 0 is specified, LSTM with projections will be used. This changes the LSTM cell in the following way. First, the dimension of hth_t will be changed from hidden_size to proj_size (dimensions of WhiW_{hi} will be changed accordingly). Second, the output hidden state of each layer will be multiplied by a learnable projection matrix: ht=Whrhth_t = W_{hr}h_t. Note that as a consequence of this, the output of LSTM network will be of different shape as well. See Inputs/Outputs sections below for exact dimensions of all variables. You can find more details in https://arxiv.org/abs/1402.1128.", "where:", "Note", "All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}) where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}", "Note", "For bidirectional LSTMs, forward and backward are directions 0 and 1 respectively. Example of splitting the output layers when batch_first=False: output.view(seq_len, batch, num_directions, hidden_size).", "Note", "For bidirectional LSTMs, h_n is not equivalent to the last element of output; the former contains the final forward and reverse hidden states, while the latter contains the final forward hidden state and the initial reverse hidden state.", "Note", "batch_first argument is ignored for unbatched inputs.", "Note", "proj_size should be smaller than hidden_size.", "Warning", "There are known non-determinism issues for RNN functions on some versions of cuDNN and CUDA. You can enforce deterministic behavior by setting the following environment variables:", "On CUDA 10.1, set environment variable CUDA_LAUNCH_BLOCKING=1. This may affect performance.", "On CUDA 10.2 or later, set environment variable (note the leading colon symbol) CUBLAS_WORKSPACE_CONFIG=:16:8 or CUBLAS_WORKSPACE_CONFIG=:4096:2.", "See the cuDNN 8 Release Notes for more information.", "Note", "If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance.", "Examples:"]}, {"name": "torch.nn.LSTMCell", "path": "generated/torch.nn.lstmcell", "type": "Neuro Network", "text": ["A long short-term memory (LSTM) cell.", "where \u03c3\\sigma is the sigmoid function, and \u2217* is the Hadamard product.", "c_0 of shape (batch, hidden_size) or (hidden_size): tensor containing the initial cell state", "If (h_0, c_0) is not provided, both h_0 and c_0 default to zero.", "Note", "All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}) where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}", "On certain ROCm devices, when using float16 inputs this module will use different precision for backward.", "Examples:"]}, {"name": "torch.nn.MarginRankingLoss", "path": "generated/torch.nn.marginrankingloss", "type": "Neuro Network", "text": ["Creates a criterion that measures the loss given inputs x1x1, x2x2, two 1D mini-batch or 0D Tensors, and a label 1D mini-batch or 0D Tensor yy (containing 1 or -1).", "If y=1y = 1 then it assumed the first input should be ranked higher (have a larger value) than the second input, and vice-versa for y=\u22121y = -1.", "The loss function for each pair of samples in the mini-batch is:", "Examples:"]}, {"name": "torch.nn.MaxPool1d", "path": "generated/torch.nn.maxpool1d", "type": "Neuro Network", "text": ["Applies a 1D max pooling over an input signal composed of several input planes.", "In the simplest case, the output value of the layer with input size (N,C,L)(N, C, L) and output (N,C,Lout)(N, C, L_{out}) can be precisely described as:", "If padding is non-zero, then the input is implicitly padded with negative infinity on both sides for padding number of points. dilation is the stride between the elements within the sliding window. This link has a nice visualization of the pooling parameters.", "Note", "When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding or the input. Sliding windows that would start in the right padded region are ignored.", "Output: (N,C,Lout)(N, C, L_{out}) or (C,Lout)(C, L_{out}), where", "Examples:"]}, {"name": "torch.nn.MaxPool2d", "path": "generated/torch.nn.maxpool2d", "type": "Neuro Network", "text": ["Applies a 2D max pooling over an input signal composed of several input planes.", "In the simplest case, the output value of the layer with input size (N,C,H,W)(N, C, H, W), output (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) and kernel_size (kH,kW)(kH, kW) can be precisely described as:", "If padding is non-zero, then the input is implicitly padded with negative infinity on both sides for padding number of points. dilation controls the spacing between the kernel points. It is harder to describe, but this link has a nice visualization of what dilation does.", "Note", "When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding or the input. Sliding windows that would start in the right padded region are ignored.", "The parameters kernel_size, stride, padding, dilation can either be:", "Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) or (C,Hout,Wout)(C, H_{out}, W_{out}), where", "Examples:"]}, {"name": "torch.nn.MaxPool3d", "path": "generated/torch.nn.maxpool3d", "type": "Neuro Network", "text": ["Applies a 3D max pooling over an input signal composed of several input planes.", "In the simplest case, the output value of the layer with input size (N,C,D,H,W)(N, C, D, H, W), output (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out}) and kernel_size (kD,kH,kW)(kD, kH, kW) can be precisely described as:", "If padding is non-zero, then the input is implicitly padded with negative infinity on both sides for padding number of points. dilation controls the spacing between the kernel points. It is harder to describe, but this link has a nice visualization of what dilation does.", "Note", "When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding or the input. Sliding windows that would start in the right padded region are ignored.", "The parameters kernel_size, stride, padding, dilation can either be:", "Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out}) or (C,Dout,Hout,Wout)(C, D_{out}, H_{out}, W_{out}), where", "Examples:"]}, {"name": "torch.nn.MaxUnpool1d", "path": "generated/torch.nn.maxunpool1d", "type": "Neuro Network", "text": ["Computes a partial inverse of MaxPool1d.", "MaxPool1d is not fully invertible, since the non-maximal values are lost.", "MaxUnpool1d takes in as input the output of MaxPool1d including the indices of the maximal values and computes a partial inverse in which all non-maximal values are set to zero.", "Note", "This operation may behave nondeterministically when the input indices has repeat values. See https://github.com/pytorch/pytorch/issues/80827 and Reproducibility for more information.", "Note", "MaxPool1d can map several input sizes to the same output sizes. Hence, the inversion process can get ambiguous. To accommodate this, you can provide the needed output size as an additional argument output_size in the forward call. See the Inputs and Example below.", "Output: (N,C,Hout)(N, C, H_{out}) or (C,Hout)(C, H_{out}), where", "or as given by output_size in the call operator", "Example:"]}, {"name": "torch.nn.MaxUnpool2d", "path": "generated/torch.nn.maxunpool2d", "type": "Neuro Network", "text": ["Computes a partial inverse of MaxPool2d.", "MaxPool2d is not fully invertible, since the non-maximal values are lost.", "MaxUnpool2d takes in as input the output of MaxPool2d including the indices of the maximal values and computes a partial inverse in which all non-maximal values are set to zero.", "Note", "This operation may behave nondeterministically when the input indices has repeat values. See https://github.com/pytorch/pytorch/issues/80827 and Reproducibility for more information.", "Note", "MaxPool2d can map several input sizes to the same output sizes. Hence, the inversion process can get ambiguous. To accommodate this, you can provide the needed output size as an additional argument output_size in the forward call. See the Inputs and Example below.", "Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) or (C,Hout,Wout)(C, H_{out}, W_{out}), where", "or as given by output_size in the call operator", "Example:"]}, {"name": "torch.nn.MaxUnpool3d", "path": "generated/torch.nn.maxunpool3d", "type": "Neuro Network", "text": ["Computes a partial inverse of MaxPool3d.", "MaxPool3d is not fully invertible, since the non-maximal values are lost. MaxUnpool3d takes in as input the output of MaxPool3d including the indices of the maximal values and computes a partial inverse in which all non-maximal values are set to zero.", "Note", "This operation may behave nondeterministically when the input indices has repeat values. See https://github.com/pytorch/pytorch/issues/80827 and Reproducibility for more information.", "Note", "MaxPool3d can map several input sizes to the same output sizes. Hence, the inversion process can get ambiguous. To accommodate this, you can provide the needed output size as an additional argument output_size in the forward call. See the Inputs section below.", "Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out}) or (C,Dout,Hout,Wout)(C, D_{out}, H_{out}, W_{out}), where", "or as given by output_size in the call operator", "Example:"]}, {"name": "torch.nn.Mish", "path": "generated/torch.nn.mish", "type": "Neuro Network", "text": ["Applies the Mish function, element-wise. Mish: A Self Regularized Non-Monotonic Neural Activation Function.", "Note", "See Mish: A Self Regularized Non-Monotonic Neural Activation Function", "Examples:"]}, {"name": "torch.nn.Module", "path": "generated/torch.nn.module", "type": "Neuro Network", "text": ["Base class for all neural network modules.", "Your models should also subclass this class.", "Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:", "Submodules assigned in this way will be registered, and will have their parameters converted too when you call to(), etc.", "Note", "As per the example above, an __init__() call to the parent class must be made before assignment on the child.", "training (bool) \u2013 Boolean represents whether this module is in training or evaluation mode.", "Adds a child module to the current module.", "The module can be accessed as an attribute using the given name.", "Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).", "fn (Module -> None) \u2013 function to be applied to each submodule", "self", "Module", "Example:", "Casts all floating point parameters and buffers to bfloat16 datatype.", "Note", "This method modifies the module in-place.", "self", "Module", "Returns an iterator over module buffers.", "recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.", "torch.Tensor \u2013 module buffer", "Iterator[Tensor]", "Example:", "Returns an iterator over immediate children modules.", "Module \u2013 a child module", "Iterator[Module]", "Compile this Module\u2019s forward using torch.compile().", "This Module\u2019s __call__ method is compiled and all arguments are passed as-is to torch.compile().", "See torch.compile() for details on the arguments for this function.", "Moves all model parameters and buffers to the CPU.", "Note", "This method modifies the module in-place.", "self", "Module", "Moves all model parameters and buffers to the GPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.", "Note", "This method modifies the module in-place.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module", "Casts all floating point parameters and buffers to double datatype.", "Note", "This method modifies the module in-place.", "self", "Module", "Sets the module in evaluation mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "This is equivalent with self.train(False).", "See Locally disabling gradient computation for a comparison between .eval() and several similar mechanisms that may be confused with it.", "self", "Module", "Set the extra representation of the module", "To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.", "str", "Casts all floating point parameters and buffers to float datatype.", "Note", "This method modifies the module in-place.", "self", "Module", "Defines the computation performed at every call.", "Should be overridden by all subclasses.", "Note", "Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.", "Returns the buffer given by target if it exists, otherwise throws an error.", "See the docstring for get_submodule for a more detailed explanation of this method\u2019s functionality as well as how to correctly specify target.", "target (str) \u2013 The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.)", "The buffer referenced by target", "torch.Tensor", "AttributeError \u2013 If the target string references an invalid path or resolves to something that is not a buffer", "Returns any extra state to include in the module\u2019s state_dict. Implement this and a corresponding set_extra_state() for your module if you need to store extra state. This function is called when building the module\u2019s state_dict().", "Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.", "Any extra state to store in the module\u2019s state_dict", "object", "Returns the parameter given by target if it exists, otherwise throws an error.", "See the docstring for get_submodule for a more detailed explanation of this method\u2019s functionality as well as how to correctly specify target.", "target (str) \u2013 The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.)", "The Parameter referenced by target", "torch.nn.Parameter", "AttributeError \u2013 If the target string references an invalid path or resolves to something that is not an nn.Parameter", "Returns the submodule given by target if it exists, otherwise throws an error.", "For example, let\u2019s say you have an nn.Module A that looks like this:", "(The diagram shows an nn.Module A. A has a nested submodule net_b, which itself has two submodules net_c and linear. net_c then has a submodule conv.)", "To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\"). To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\").", "The runtime of get_submodule is bounded by the degree of module nesting in target. A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used.", "target (str) \u2013 The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)", "The submodule referenced by target", "torch.nn.Module", "AttributeError \u2013 If the target string references an invalid path or resolves to something that is not an nn.Module", "Casts all floating point parameters and buffers to half datatype.", "Note", "This method modifies the module in-place.", "self", "Module", "Moves all model parameters and buffers to the IPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on IPU while being optimized.", "Note", "This method modifies the module in-place.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module", "Copies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.", "Warning", "If assign is True the optimizer must be created after the call to load_state_dict.", "NamedTuple with missing_keys and unexpected_keys fields", "Note", "If a parameter or buffer is registered as None and its corresponding key exists in state_dict, load_state_dict() will raise a RuntimeError.", "Returns an iterator over all modules in the network.", "Module \u2013 a module in the network", "Iterator[Module]", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:", "Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.", "(str, torch.Tensor) \u2013 Tuple containing the name and buffer", "Iterator[Tuple[str, Tensor]]", "Example:", "Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.", "(str, Module) \u2013 Tuple containing a name and child module", "Iterator[Tuple[str, Module]]", "Example:", "Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.", "(str, Module) \u2013 Tuple of name and module", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:", "Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.", "(str, Parameter) \u2013 Tuple containing the name and parameter", "Iterator[Tuple[str, Parameter]]", "Example:", "Returns an iterator over module parameters.", "This is typically passed to an optimizer.", "recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.", "Parameter \u2013 module parameter", "Iterator[Parameter]", "Example:", "Registers a backward hook on the module.", "This function is deprecated in favor of register_full_backward_hook() and the behavior of this function will change in future versions.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Adds a buffer to the module.", "This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict.", "Buffers can be accessed as attributes using given names.", "Example:", "Registers a forward hook on the module.", "The hook will be called every time after forward() has computed an output.", "If with_kwargs is False or not specified, the input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called. The hook should have the following signature:", "If with_kwargs is True, the forward hook will be passed the kwargs given to the forward function and be expected to return the output possibly modified. The hook should have the following signature:", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Registers a forward pre-hook on the module.", "The hook will be called every time before forward() is invoked.", "If with_kwargs is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature:", "If with_kwargs is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature:", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Registers a backward hook on the module.", "The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature:", "The grad_input and grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in grad_input and grad_output will be None for all non-Tensor arguments.", "For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module\u2019s forward function.", "Warning", "Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Registers a backward pre-hook on the module.", "The hook will be called every time the gradients for the module are computed. The hook should have the following signature:", "The grad_output is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of grad_output in subsequent computations. Entries in grad_output will be None for all non-Tensor arguments.", "For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module\u2019s forward function.", "Warning", "Modifying inputs inplace is not allowed when using backward hooks and will raise an error.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Registers a post hook to be run after module\u2019s load_state_dict is called.", "hook(module, incompatible_keys) -> None", "The module argument is the current module that this hook is registered on, and the incompatible_keys argument is a NamedTuple consisting of attributes missing_keys and unexpected_keys. missing_keys is a list of str containing the missing keys and unexpected_keys is a list of str containing the unexpected keys.", "The given incompatible_keys can be modified inplace if needed.", "Note that the checks performed when calling load_state_dict() with strict=True are affected by modifications the hook makes to missing_keys or unexpected_keys, as expected. Additions to either set of keys will result in an error being thrown when strict=True, and clearing out both missing and unexpected keys will avoid an error.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Alias for add_module().", "Adds a parameter to the module.", "The parameter can be accessed as an attribute using given name.", "These hooks will be called with arguments: self, prefix, and keep_vars before calling state_dict on self. The registered hooks can be used to perform pre-processing before the state_dict call is made.", "Change if autograd should record operations on parameters in this module.", "This method sets the parameters\u2019 requires_grad attributes in-place.", "This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).", "See Locally disabling gradient computation for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it.", "requires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.", "self", "Module", "This function is called from load_state_dict() to handle any extra state found within the state_dict. Implement this function and a corresponding get_extra_state() for your module if you need to store extra state within its state_dict.", "state (dict) \u2013 Extra state from the state_dict", "See torch.Tensor.share_memory_()", "T", "Returns a dictionary containing references to the whole state of the module.", "Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included.", "Note", "The returned object is a shallow copy. It contains references to the module\u2019s parameters and buffers.", "Warning", "Currently state_dict() also accepts positional arguments for destination, prefix and keep_vars in order. However, this is being deprecated and keyword arguments will be enforced in future releases.", "Warning", "Please avoid the use of argument destination as it is not designed for end-users.", "a dictionary containing a whole state of the module", "dict", "Example:", "Moves and/or casts the parameters and buffers.", "This can be called as", "Its signature is similar to torch.Tensor.to(), but only accepts floating point or complex dtypes. In addition, this method will only cast the floating point or complex parameters and buffers to dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.", "See below for examples.", "Note", "This method modifies the module in-place.", "self", "Module", "Examples:", "Moves the parameters and buffers to the specified device without copying storage.", "self", "Module", "Sets the module in training mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "mode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.", "self", "Module", "Casts all parameters and buffers to dst_type.", "Note", "This method modifies the module in-place.", "dst_type (type or string) \u2013 the desired type", "self", "Module", "Moves all model parameters and buffers to the XPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.", "Note", "This method modifies the module in-place.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module", "Resets gradients of all model parameters. See similar function under torch.optim.Optimizer for more context.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See torch.optim.Optimizer.zero_grad() for details."]}, {"name": "torch.nn.Module.add_module()", "path": "generated/torch.nn.module#torch.nn.Module.add_module", "type": "Neuro Network", "text": ["Adds a child module to the current module.", "The module can be accessed as an attribute using the given name."]}, {"name": "torch.nn.Module.apply()", "path": "generated/torch.nn.module#torch.nn.Module.apply", "type": "Neuro Network", "text": ["Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).", "fn (Module -> None) \u2013 function to be applied to each submodule", "self", "Module", "Example:"]}, {"name": "torch.nn.Module.bfloat16()", "path": "generated/torch.nn.module#torch.nn.Module.bfloat16", "type": "Neuro Network", "text": ["Casts all floating point parameters and buffers to bfloat16 datatype.", "Note", "This method modifies the module in-place.", "self", "Module"]}, {"name": "torch.nn.Module.buffers()", "path": "generated/torch.nn.module#torch.nn.Module.buffers", "type": "Neuro Network", "text": ["Returns an iterator over module buffers.", "recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.", "torch.Tensor \u2013 module buffer", "Iterator[Tensor]", "Example:"]}, {"name": "torch.nn.Module.children()", "path": "generated/torch.nn.module#torch.nn.Module.children", "type": "Neuro Network", "text": ["Returns an iterator over immediate children modules.", "Module \u2013 a child module", "Iterator[Module]"]}, {"name": "torch.nn.Module.compile()", "path": "generated/torch.nn.module#torch.nn.Module.compile", "type": "Neuro Network", "text": ["Compile this Module\u2019s forward using torch.compile().", "This Module\u2019s __call__ method is compiled and all arguments are passed as-is to torch.compile().", "See torch.compile() for details on the arguments for this function."]}, {"name": "torch.nn.Module.cpu()", "path": "generated/torch.nn.module#torch.nn.Module.cpu", "type": "Neuro Network", "text": ["Moves all model parameters and buffers to the CPU.", "Note", "This method modifies the module in-place.", "self", "Module"]}, {"name": "torch.nn.Module.cuda()", "path": "generated/torch.nn.module#torch.nn.Module.cuda", "type": "Neuro Network", "text": ["Moves all model parameters and buffers to the GPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.", "Note", "This method modifies the module in-place.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module"]}, {"name": "torch.nn.Module.double()", "path": "generated/torch.nn.module#torch.nn.Module.double", "type": "Neuro Network", "text": ["Casts all floating point parameters and buffers to double datatype.", "Note", "This method modifies the module in-place.", "self", "Module"]}, {"name": "torch.nn.Module.eval()", "path": "generated/torch.nn.module#torch.nn.Module.eval", "type": "Neuro Network", "text": ["Sets the module in evaluation mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "This is equivalent with self.train(False).", "See Locally disabling gradient computation for a comparison between .eval() and several similar mechanisms that may be confused with it.", "self", "Module"]}, {"name": "torch.nn.Module.extra_repr()", "path": "generated/torch.nn.module#torch.nn.Module.extra_repr", "type": "Neuro Network", "text": ["Set the extra representation of the module", "To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.", "str"]}, {"name": "torch.nn.Module.float()", "path": "generated/torch.nn.module#torch.nn.Module.float", "type": "Neuro Network", "text": ["Casts all floating point parameters and buffers to float datatype.", "Note", "This method modifies the module in-place.", "self", "Module"]}, {"name": "torch.nn.Module.forward()", "path": "generated/torch.nn.module#torch.nn.Module.forward", "type": "Neuro Network", "text": ["Defines the computation performed at every call.", "Should be overridden by all subclasses.", "Note", "Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them."]}, {"name": "torch.nn.Module.get_buffer()", "path": "generated/torch.nn.module#torch.nn.Module.get_buffer", "type": "Neuro Network", "text": ["Returns the buffer given by target if it exists, otherwise throws an error.", "See the docstring for get_submodule for a more detailed explanation of this method\u2019s functionality as well as how to correctly specify target.", "target (str) \u2013 The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.)", "The buffer referenced by target", "torch.Tensor", "AttributeError \u2013 If the target string references an invalid path or resolves to something that is not a buffer"]}, {"name": "torch.nn.Module.get_extra_state()", "path": "generated/torch.nn.module#torch.nn.Module.get_extra_state", "type": "Neuro Network", "text": ["Returns any extra state to include in the module\u2019s state_dict. Implement this and a corresponding set_extra_state() for your module if you need to store extra state. This function is called when building the module\u2019s state_dict().", "Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.", "Any extra state to store in the module\u2019s state_dict", "object"]}, {"name": "torch.nn.Module.get_parameter()", "path": "generated/torch.nn.module#torch.nn.Module.get_parameter", "type": "Neuro Network", "text": ["Returns the parameter given by target if it exists, otherwise throws an error.", "See the docstring for get_submodule for a more detailed explanation of this method\u2019s functionality as well as how to correctly specify target.", "target (str) \u2013 The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.)", "The Parameter referenced by target", "torch.nn.Parameter", "AttributeError \u2013 If the target string references an invalid path or resolves to something that is not an nn.Parameter"]}, {"name": "torch.nn.Module.get_submodule()", "path": "generated/torch.nn.module#torch.nn.Module.get_submodule", "type": "Neuro Network", "text": ["Returns the submodule given by target if it exists, otherwise throws an error.", "For example, let\u2019s say you have an nn.Module A that looks like this:", "(The diagram shows an nn.Module A. A has a nested submodule net_b, which itself has two submodules net_c and linear. net_c then has a submodule conv.)", "To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\"). To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\").", "The runtime of get_submodule is bounded by the degree of module nesting in target. A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used.", "target (str) \u2013 The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)", "The submodule referenced by target", "torch.nn.Module", "AttributeError \u2013 If the target string references an invalid path or resolves to something that is not an nn.Module"]}, {"name": "torch.nn.Module.half()", "path": "generated/torch.nn.module#torch.nn.Module.half", "type": "Neuro Network", "text": ["Casts all floating point parameters and buffers to half datatype.", "Note", "This method modifies the module in-place.", "self", "Module"]}, {"name": "torch.nn.Module.ipu()", "path": "generated/torch.nn.module#torch.nn.Module.ipu", "type": "Neuro Network", "text": ["Moves all model parameters and buffers to the IPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on IPU while being optimized.", "Note", "This method modifies the module in-place.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module"]}, {"name": "torch.nn.Module.load_state_dict()", "path": "generated/torch.nn.module#torch.nn.Module.load_state_dict", "type": "Neuro Network", "text": ["Copies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.", "Warning", "If assign is True the optimizer must be created after the call to load_state_dict.", "NamedTuple with missing_keys and unexpected_keys fields", "Note", "If a parameter or buffer is registered as None and its corresponding key exists in state_dict, load_state_dict() will raise a RuntimeError."]}, {"name": "torch.nn.Module.modules()", "path": "generated/torch.nn.module#torch.nn.Module.modules", "type": "Neuro Network", "text": ["Returns an iterator over all modules in the network.", "Module \u2013 a module in the network", "Iterator[Module]", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:"]}, {"name": "torch.nn.Module.named_buffers()", "path": "generated/torch.nn.module#torch.nn.Module.named_buffers", "type": "Neuro Network", "text": ["Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.", "(str, torch.Tensor) \u2013 Tuple containing the name and buffer", "Iterator[Tuple[str, Tensor]]", "Example:"]}, {"name": "torch.nn.Module.named_children()", "path": "generated/torch.nn.module#torch.nn.Module.named_children", "type": "Neuro Network", "text": ["Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.", "(str, Module) \u2013 Tuple containing a name and child module", "Iterator[Tuple[str, Module]]", "Example:"]}, {"name": "torch.nn.Module.named_modules()", "path": "generated/torch.nn.module#torch.nn.Module.named_modules", "type": "Neuro Network", "text": ["Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.", "(str, Module) \u2013 Tuple of name and module", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:"]}, {"name": "torch.nn.Module.named_parameters()", "path": "generated/torch.nn.module#torch.nn.Module.named_parameters", "type": "Neuro Network", "text": ["Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.", "(str, Parameter) \u2013 Tuple containing the name and parameter", "Iterator[Tuple[str, Parameter]]", "Example:"]}, {"name": "torch.nn.Module.parameters()", "path": "generated/torch.nn.module#torch.nn.Module.parameters", "type": "Neuro Network", "text": ["Returns an iterator over module parameters.", "This is typically passed to an optimizer.", "recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.", "Parameter \u2013 module parameter", "Iterator[Parameter]", "Example:"]}, {"name": "torch.nn.Module.register_backward_hook()", "path": "generated/torch.nn.module#torch.nn.Module.register_backward_hook", "type": "Neuro Network", "text": ["Registers a backward hook on the module.", "This function is deprecated in favor of register_full_backward_hook() and the behavior of this function will change in future versions.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.Module.register_buffer()", "path": "generated/torch.nn.module#torch.nn.Module.register_buffer", "type": "Neuro Network", "text": ["Adds a buffer to the module.", "This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict.", "Buffers can be accessed as attributes using given names.", "Example:"]}, {"name": "torch.nn.Module.register_forward_hook()", "path": "generated/torch.nn.module#torch.nn.Module.register_forward_hook", "type": "Neuro Network", "text": ["Registers a forward hook on the module.", "The hook will be called every time after forward() has computed an output.", "If with_kwargs is False or not specified, the input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called. The hook should have the following signature:", "If with_kwargs is True, the forward hook will be passed the kwargs given to the forward function and be expected to return the output possibly modified. The hook should have the following signature:", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.Module.register_forward_pre_hook()", "path": "generated/torch.nn.module#torch.nn.Module.register_forward_pre_hook", "type": "Neuro Network", "text": ["Registers a forward pre-hook on the module.", "The hook will be called every time before forward() is invoked.", "If with_kwargs is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature:", "If with_kwargs is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature:", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.Module.register_full_backward_hook()", "path": "generated/torch.nn.module#torch.nn.Module.register_full_backward_hook", "type": "Neuro Network", "text": ["Registers a backward hook on the module.", "The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature:", "The grad_input and grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in grad_input and grad_output will be None for all non-Tensor arguments.", "For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module\u2019s forward function.", "Warning", "Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.Module.register_full_backward_pre_hook()", "path": "generated/torch.nn.module#torch.nn.Module.register_full_backward_pre_hook", "type": "Neuro Network", "text": ["Registers a backward pre-hook on the module.", "The hook will be called every time the gradients for the module are computed. The hook should have the following signature:", "The grad_output is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of grad_output in subsequent computations. Entries in grad_output will be None for all non-Tensor arguments.", "For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module\u2019s forward function.", "Warning", "Modifying inputs inplace is not allowed when using backward hooks and will raise an error.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.Module.register_load_state_dict_post_hook()", "path": "generated/torch.nn.module#torch.nn.Module.register_load_state_dict_post_hook", "type": "Neuro Network", "text": ["Registers a post hook to be run after module\u2019s load_state_dict is called.", "hook(module, incompatible_keys) -> None", "The module argument is the current module that this hook is registered on, and the incompatible_keys argument is a NamedTuple consisting of attributes missing_keys and unexpected_keys. missing_keys is a list of str containing the missing keys and unexpected_keys is a list of str containing the unexpected keys.", "The given incompatible_keys can be modified inplace if needed.", "Note that the checks performed when calling load_state_dict() with strict=True are affected by modifications the hook makes to missing_keys or unexpected_keys, as expected. Additions to either set of keys will result in an error being thrown when strict=True, and clearing out both missing and unexpected keys will avoid an error.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.Module.register_module()", "path": "generated/torch.nn.module#torch.nn.Module.register_module", "type": "Neuro Network", "text": ["Alias for add_module()."]}, {"name": "torch.nn.Module.register_parameter()", "path": "generated/torch.nn.module#torch.nn.Module.register_parameter", "type": "Neuro Network", "text": ["Adds a parameter to the module.", "The parameter can be accessed as an attribute using given name."]}, {"name": "torch.nn.Module.register_state_dict_pre_hook()", "path": "generated/torch.nn.module#torch.nn.Module.register_state_dict_pre_hook", "type": "Neuro Network", "text": ["These hooks will be called with arguments: self, prefix, and keep_vars before calling state_dict on self. The registered hooks can be used to perform pre-processing before the state_dict call is made."]}, {"name": "torch.nn.Module.requires_grad_()", "path": "generated/torch.nn.module#torch.nn.Module.requires_grad_", "type": "Neuro Network", "text": ["Change if autograd should record operations on parameters in this module.", "This method sets the parameters\u2019 requires_grad attributes in-place.", "This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).", "See Locally disabling gradient computation for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it.", "requires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.", "self", "Module"]}, {"name": "torch.nn.Module.set_extra_state()", "path": "generated/torch.nn.module#torch.nn.Module.set_extra_state", "type": "Neuro Network", "text": ["This function is called from load_state_dict() to handle any extra state found within the state_dict. Implement this function and a corresponding get_extra_state() for your module if you need to store extra state within its state_dict.", "state (dict) \u2013 Extra state from the state_dict"]}, {"name": "torch.nn.Module.share_memory()", "path": "generated/torch.nn.module#torch.nn.Module.share_memory", "type": "Neuro Network", "text": ["See torch.Tensor.share_memory_()", "T"]}, {"name": "torch.nn.Module.state_dict()", "path": "generated/torch.nn.module#torch.nn.Module.state_dict", "type": "Neuro Network", "text": ["Returns a dictionary containing references to the whole state of the module.", "Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included.", "Note", "The returned object is a shallow copy. It contains references to the module\u2019s parameters and buffers.", "Warning", "Currently state_dict() also accepts positional arguments for destination, prefix and keep_vars in order. However, this is being deprecated and keyword arguments will be enforced in future releases.", "Warning", "Please avoid the use of argument destination as it is not designed for end-users.", "a dictionary containing a whole state of the module", "dict", "Example:"]}, {"name": "torch.nn.Module.to()", "path": "generated/torch.nn.module#torch.nn.Module.to", "type": "Neuro Network", "text": ["Moves and/or casts the parameters and buffers.", "This can be called as", "Its signature is similar to torch.Tensor.to(), but only accepts floating point or complex dtypes. In addition, this method will only cast the floating point or complex parameters and buffers to dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.", "See below for examples.", "Note", "This method modifies the module in-place.", "self", "Module", "Examples:"]}, {"name": "torch.nn.Module.to_empty()", "path": "generated/torch.nn.module#torch.nn.Module.to_empty", "type": "Neuro Network", "text": ["Moves the parameters and buffers to the specified device without copying storage.", "self", "Module"]}, {"name": "torch.nn.Module.train()", "path": "generated/torch.nn.module#torch.nn.Module.train", "type": "Neuro Network", "text": ["Sets the module in training mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "mode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.", "self", "Module"]}, {"name": "torch.nn.Module.type()", "path": "generated/torch.nn.module#torch.nn.Module.type", "type": "Neuro Network", "text": ["Casts all parameters and buffers to dst_type.", "Note", "This method modifies the module in-place.", "dst_type (type or string) \u2013 the desired type", "self", "Module"]}, {"name": "torch.nn.Module.xpu()", "path": "generated/torch.nn.module#torch.nn.Module.xpu", "type": "Neuro Network", "text": ["Moves all model parameters and buffers to the XPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.", "Note", "This method modifies the module in-place.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module"]}, {"name": "torch.nn.Module.zero_grad()", "path": "generated/torch.nn.module#torch.nn.Module.zero_grad", "type": "Neuro Network", "text": ["Resets gradients of all model parameters. See similar function under torch.optim.Optimizer for more context.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See torch.optim.Optimizer.zero_grad() for details."]}, {"name": "torch.nn.ModuleDict", "path": "generated/torch.nn.moduledict", "type": "Neuro Network", "text": ["Holds submodules in a dictionary.", "ModuleDict can be indexed like a regular Python dictionary, but modules it contains are properly registered, and will be visible by all Module methods.", "ModuleDict is an ordered dictionary that respects", "Note that update() with other unordered mapping types (e.g., Python\u2019s plain dict before Python version 3.6) does not preserve the order of the merged mapping.", "modules (iterable, optional) \u2013 a mapping (dictionary) of (string: module) or an iterable of key-value pairs of type (string, module)", "Example:", "Remove all items from the ModuleDict.", "Return an iterable of the ModuleDict key/value pairs.", "Iterable[Tuple[str, Module]]", "Return an iterable of the ModuleDict keys.", "Iterable[str]", "Remove key from the ModuleDict and return its module.", "key (str) \u2013 key to pop from the ModuleDict", "Module", "Update the ModuleDict with the key-value pairs from a mapping or an iterable, overwriting existing keys.", "Note", "If modules is an OrderedDict, a ModuleDict, or an iterable of key-value pairs, the order of new elements in it is preserved.", "modules (iterable) \u2013 a mapping (dictionary) from string to Module, or an iterable of key-value pairs of type (string, Module)", "Return an iterable of the ModuleDict values.", "Iterable[Module]"]}, {"name": "torch.nn.ModuleDict.clear()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.clear", "type": "Neuro Network", "text": ["Remove all items from the ModuleDict."]}, {"name": "torch.nn.ModuleDict.items()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.items", "type": "Neuro Network", "text": ["Return an iterable of the ModuleDict key/value pairs.", "Iterable[Tuple[str, Module]]"]}, {"name": "torch.nn.ModuleDict.keys()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.keys", "type": "Neuro Network", "text": ["Return an iterable of the ModuleDict keys.", "Iterable[str]"]}, {"name": "torch.nn.ModuleDict.pop()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.pop", "type": "Neuro Network", "text": ["Remove key from the ModuleDict and return its module.", "key (str) \u2013 key to pop from the ModuleDict", "Module"]}, {"name": "torch.nn.ModuleDict.update()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.update", "type": "Neuro Network", "text": ["Update the ModuleDict with the key-value pairs from a mapping or an iterable, overwriting existing keys.", "Note", "If modules is an OrderedDict, a ModuleDict, or an iterable of key-value pairs, the order of new elements in it is preserved.", "modules (iterable) \u2013 a mapping (dictionary) from string to Module, or an iterable of key-value pairs of type (string, Module)"]}, {"name": "torch.nn.ModuleDict.values()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.values", "type": "Neuro Network", "text": ["Return an iterable of the ModuleDict values.", "Iterable[Module]"]}, {"name": "torch.nn.ModuleList", "path": "generated/torch.nn.modulelist", "type": "Neuro Network", "text": ["Holds submodules in a list.", "ModuleList can be indexed like a regular Python list, but modules it contains are properly registered, and will be visible by all Module methods.", "modules (iterable, optional) \u2013 an iterable of modules to add", "Example:", "Appends a given module to the end of the list.", "module (nn.Module) \u2013 module to append", "ModuleList", "Appends modules from a Python iterable to the end of the list.", "modules (iterable) \u2013 iterable of modules to append", "ModuleList", "Insert a given module before a given index in the list."]}, {"name": "torch.nn.ModuleList.append()", "path": "generated/torch.nn.modulelist#torch.nn.ModuleList.append", "type": "Neuro Network", "text": ["Appends a given module to the end of the list.", "module (nn.Module) \u2013 module to append", "ModuleList"]}, {"name": "torch.nn.ModuleList.extend()", "path": "generated/torch.nn.modulelist#torch.nn.ModuleList.extend", "type": "Neuro Network", "text": ["Appends modules from a Python iterable to the end of the list.", "modules (iterable) \u2013 iterable of modules to append", "ModuleList"]}, {"name": "torch.nn.ModuleList.insert()", "path": "generated/torch.nn.modulelist#torch.nn.ModuleList.insert", "type": "Neuro Network", "text": ["Insert a given module before a given index in the list."]}, {"name": "torch.nn.modules.lazy.LazyModuleMixin", "path": "generated/torch.nn.modules.lazy.lazymodulemixin#torch.nn.modules.lazy.LazyModuleMixin", "type": "Neuro Network", "text": ["A mixin for modules that lazily initialize parameters, also known as \u201clazy modules.\u201d", "Modules that lazily initialize parameters, or \u201clazy modules\u201d, derive the shapes of their parameters from the first input(s) to their forward method. Until that first forward they contain torch.nn.UninitializedParameter s that should not be accessed or used, and afterward they contain regular torch.nn.Parameter s. Lazy modules are convenient since they don\u2019t require computing some module arguments, like the in_features argument of a typical torch.nn.Linear.", "After construction, networks with lazy modules should first be converted to the desired dtype and placed on the expected device. This is because lazy modules only perform shape inference so the usual dtype and device placement behavior applies. The lazy modules should then perform \u201cdry runs\u201d to initialize all the components in the module. These \u201cdry runs\u201d send inputs of the correct size, dtype, and device through the network and to each one of its lazy modules. After this the network can be used as usual.", "A final caveat when using lazy modules is that the order of initialization of a network\u2019s parameters may change, since the lazy modules are always initialized after other modules. For example, if the LazyMLP class defined above had a torch.nn.LazyLinear module first and then a regular torch.nn.Linear second, the second module would be initialized on construction and the first module would be initialized during the first dry run. This can cause the parameters of a network using lazy modules to be initialized differently than the parameters of a network without lazy modules as the order of parameter initializations, which often depends on a stateful random number generator, is different. Check Reproducibility for more details.", "Lazy modules can be serialized with a state dict like other modules. For example:", "Lazy modules can load regular torch.nn.Parameter s (i.e. you can serialize/deserialize initialized LazyModules and they will remain initialized)", "Note, however, that the loaded parameters will not be replaced when doing a \u201cdry run\u201d if they are initialized when the state is loaded. This prevents using initialized modules in different contexts.", "Check if a module has parameters that are not initialized", "Initialize parameters according to the input batch properties. This adds an interface to isolate parameter initialization from the forward pass when doing parameter shape inference."]}, {"name": "torch.nn.modules.lazy.LazyModuleMixin.has_uninitialized_params()", "path": "generated/torch.nn.modules.lazy.lazymodulemixin#torch.nn.modules.lazy.LazyModuleMixin.has_uninitialized_params", "type": "Neuro Network", "text": ["Check if a module has parameters that are not initialized"]}, {"name": "torch.nn.modules.lazy.LazyModuleMixin.initialize_parameters()", "path": "generated/torch.nn.modules.lazy.lazymodulemixin#torch.nn.modules.lazy.LazyModuleMixin.initialize_parameters", "type": "Neuro Network", "text": ["Initialize parameters according to the input batch properties. This adds an interface to isolate parameter initialization from the forward pass when doing parameter shape inference."]}, {"name": "torch.nn.modules.module.register_module_backward_hook()", "path": "generated/torch.nn.modules.module.register_module_backward_hook#torch.nn.modules.module.register_module_backward_hook", "type": "Neuro Network", "text": ["Registers a backward hook common to all the modules.", "This function is deprecated in favor of torch.nn.modules.module.register_module_full_backward_hook() and the behavior of this function will change in future versions.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.modules.module.register_module_buffer_registration_hook()", "path": "generated/torch.nn.modules.module.register_module_buffer_registration_hook#torch.nn.modules.module.register_module_buffer_registration_hook", "type": "Neuro Network", "text": ["Registers a buffer registration hook common to all modules.", "Warning", "This adds global state to the nn.Module module", "The hook will be called every time register_buffer() is invoked. It should have the following signature:", "The hook can modify the input or return a single modified value in the hook.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.modules.module.register_module_forward_hook()", "path": "generated/torch.nn.modules.module.register_module_forward_hook#torch.nn.modules.module.register_module_forward_hook", "type": "Neuro Network", "text": ["Registers a global forward hook for all the modules", "Warning", "This adds global state to the nn.module module and it is only intended for debugging/profiling purposes.", "The hook will be called every time after forward() has computed an output. It should have the following signature:", "The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "This hook will be executed before specific module hooks registered with register_forward_hook."]}, {"name": "torch.nn.modules.module.register_module_forward_pre_hook()", "path": "generated/torch.nn.modules.module.register_module_forward_pre_hook#torch.nn.modules.module.register_module_forward_pre_hook", "type": "Neuro Network", "text": ["Registers a forward pre-hook common to all modules.", "Warning", "This adds global state to the nn.module module and it is only intended for debugging/profiling purposes.", "The hook will be called every time before forward() is invoked. It should have the following signature:", "The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).", "This hook has precedence over the specific module hooks registered with register_forward_pre_hook.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.modules.module.register_module_full_backward_hook()", "path": "generated/torch.nn.modules.module.register_module_full_backward_hook#torch.nn.modules.module.register_module_full_backward_hook", "type": "Neuro Network", "text": ["Registers a backward hook common to all the modules.", "Warning", "This adds global state to the nn.module module and it is only intended for debugging/profiling purposes.", "The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature:", "The grad_input and grad_output are tuples. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments will not appear in the hook. Entries in grad_input and grad_output will be None for all non-Tensor arguments.", "For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module\u2019s forward function.", "Global hooks are called before hooks registered with register_backward_hook", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.modules.module.register_module_full_backward_pre_hook()", "path": "generated/torch.nn.modules.module.register_module_full_backward_pre_hook#torch.nn.modules.module.register_module_full_backward_pre_hook", "type": "Neuro Network", "text": ["Registers a backward pre-hook common to all the modules.", "Warning", "This adds global state to the nn.module module and it is only intended for debugging/profiling purposes.", "The hook will be called every time the gradients for the module are computed. The hook should have the following signature:", "The grad_output is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of grad_output in subsequent computations. Entries in grad_output will be None for all non-Tensor arguments.", "For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module\u2019s forward function.", "Global hooks are called before hooks registered with register_backward_pre_hook", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.modules.module.register_module_module_registration_hook()", "path": "generated/torch.nn.modules.module.register_module_module_registration_hook#torch.nn.modules.module.register_module_module_registration_hook", "type": "Neuro Network", "text": ["Registers a module registration hook common to all modules.", "Warning", "This adds global state to the nn.Module module", "The hook will be called every time register_module() is invoked. It should have the following signature:", "The hook can modify the input or return a single modified value in the hook.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.modules.module.register_module_parameter_registration_hook()", "path": "generated/torch.nn.modules.module.register_module_parameter_registration_hook#torch.nn.modules.module.register_module_parameter_registration_hook", "type": "Neuro Network", "text": ["Registers a parameter registration hook common to all modules.", "Warning", "This adds global state to the nn.Module module", "The hook will be called every time register_parameter() is invoked. It should have the following signature:", "The hook can modify the input or return a single modified value in the hook.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.MSELoss", "path": "generated/torch.nn.mseloss", "type": "Neuro Network", "text": ["Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input xx and target yy.", "The unreduced (i.e. with reduction set to 'none') loss can be described as:", "where NN is the batch size. If reduction is not 'none' (default 'mean'), then:", "xx and yy are tensors of arbitrary shapes with a total of nn elements each.", "The mean operation still operates over all the elements, and divides by nn.", "The division by nn can be avoided if one sets reduction = 'sum'.", "Examples:"]}, {"name": "torch.nn.MultiheadAttention", "path": "generated/torch.nn.multiheadattention", "type": "Neuro Network", "text": ["Allows the model to jointly attend to information from different representation subspaces as described in the paper: Attention Is All You Need.", "Multi-Head Attention is defined as:", "where headi=Attention(QWiQ,KWiK,VWiV)head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V).", "nn.MultiHeadAttention will use the optimized implementations of scaled_dot_product_attention() when possible.", "In addition to support for the new scaled_dot_product_attention() function, for speeding up Inference, MHA will use fastpath inference with support for Nested Tensors, iff:", "If the optimized inference fastpath implementation is in use, a NestedTensor can be passed for query/key/value to represent padding more efficiently than using a padding mask. In this case, a NestedTensor will be returned, and an additional speedup proportional to the fraction of the input that is padding can be expected.", "Examples:", "Tuple[Tensor, Optional[Tensor]]", "Note", "batch_first argument is ignored for unbatched inputs.", "Determine mask type and combine masks if necessary. If only one mask is provided, that mask and the corresponding mask type will be returned. If both masks are provided, they will be both expanded to shape (batch_size, num_heads, seq_len, seq_len), combined with logical or and mask type 2 will be returned :param attn_mask: attention mask of shape (seq_len, seq_len), mask type 0 :param key_padding_mask: padding mask of shape (batch_size, seq_len), mask type 1 :param query: query embeddings of shape (batch_size, seq_len, embed_dim)", "merged mask mask_type: merged mask type (0, 1, or 2)", "merged_mask"]}, {"name": "torch.nn.MultiheadAttention.forward()", "path": "generated/torch.nn.multiheadattention#torch.nn.MultiheadAttention.forward", "type": "Neuro Network", "text": ["Tuple[Tensor, Optional[Tensor]]", "Note", "batch_first argument is ignored for unbatched inputs."]}, {"name": "torch.nn.MultiheadAttention.merge_masks()", "path": "generated/torch.nn.multiheadattention#torch.nn.MultiheadAttention.merge_masks", "type": "Neuro Network", "text": ["Determine mask type and combine masks if necessary. If only one mask is provided, that mask and the corresponding mask type will be returned. If both masks are provided, they will be both expanded to shape (batch_size, num_heads, seq_len, seq_len), combined with logical or and mask type 2 will be returned :param attn_mask: attention mask of shape (seq_len, seq_len), mask type 0 :param key_padding_mask: padding mask of shape (batch_size, seq_len), mask type 1 :param query: query embeddings of shape (batch_size, seq_len, embed_dim)", "merged mask mask_type: merged mask type (0, 1, or 2)", "merged_mask"]}, {"name": "torch.nn.MultiLabelMarginLoss", "path": "generated/torch.nn.multilabelmarginloss", "type": "Neuro Network", "text": ["Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input xx (a 2D mini-batch Tensor) and output yy (which is a 2D Tensor of target class indices). For each sample in the mini-batch:", "where x\u2208{0,\u22ef,x.size(0)\u22121}x \\in \\left\\{0, \\; \\cdots , \\; \\text{x.size}(0) - 1\\right\\}, y\u2208{0,\u22ef,y.size(0)\u22121}y \\in \\left\\{0, \\; \\cdots , \\; \\text{y.size}(0) - 1\\right\\}, 0\u2264y[j]\u2264x.size(0)\u221210 \\leq y[j] \\leq \\text{x.size}(0)-1, and i\u2260y[j]i \\neq y[j] for all ii and jj.", "yy and xx must have the same size.", "The criterion only considers a contiguous block of non-negative targets that starts at the front.", "This allows for different samples to have variable amounts of target classes.", "Examples:"]}, {"name": "torch.nn.MultiLabelSoftMarginLoss", "path": "generated/torch.nn.multilabelsoftmarginloss", "type": "Neuro Network", "text": ["Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input xx and target yy of size (N,C)(N, C). For each sample in the minibatch:", "where i\u2208{0,\u22ef,x.nElement()\u22121}i \\in \\left\\{0, \\; \\cdots , \\; \\text{x.nElement}() - 1\\right\\}, y[i]\u2208{0,1}y[i] \\in \\left\\{0, \\; 1\\right\\}."]}, {"name": "torch.nn.MultiMarginLoss", "path": "generated/torch.nn.multimarginloss", "type": "Neuro Network", "text": ["Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input xx (a 2D mini-batch Tensor) and output yy (which is a 1D tensor of target class indices, 0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-1):", "For each mini-batch sample, the loss in terms of the 1D input xx and scalar output yy is:", "where i\u2208{0,\u22ef,x.size(0)\u22121}i \\in \\left\\{0, \\; \\cdots , \\; \\text{x.size}(0) - 1\\right\\} and i\u2260yi \\neq y.", "Optionally, you can give non-equal weighting on the classes by passing a 1D weight tensor into the constructor.", "The loss function then becomes:", "Examples:"]}, {"name": "torch.nn.NLLLoss", "path": "generated/torch.nn.nllloss", "type": "Neuro Network", "text": ["The negative log likelihood loss. It is useful to train a classification problem with C classes.", "If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set.", "The input given through a forward call is expected to contain log-probabilities of each class. input has to be a Tensor of size either (minibatch,C)(minibatch, C) or (minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K) with K\u22651K \\geq 1 for the K-dimensional case. The latter is useful for higher dimension inputs, such as computing NLL loss per-pixel for 2D images.", "Obtaining log-probabilities in a neural network is easily achieved by adding a LogSoftmax layer in the last layer of your network. You may use CrossEntropyLoss instead, if you prefer not to add an extra layer.", "The target that this loss expects should be a class index in the range [0,C\u22121][0, C-1] where C = number of classes; if ignore_index is specified, this loss also accepts this class index (this index may not necessarily be in the class range).", "The unreduced (i.e. with reduction set to 'none') loss can be described as:", "where xx is the input, yy is the target, ww is the weight, and NN is the batch size. If reduction is not 'none' (default 'mean'), then", "Examples:"]}, {"name": "torch.nn.PackedSequence", "path": "generated/torch.nn.utils.rnn.packedsequence", "type": "Neuro Network", "text": ["Holds the data and list of batch_sizes of a packed sequence.", "All RNN modules accept packed sequences as inputs.", "Note", "Instances of this class should never be created manually. They are meant to be instantiated by functions like pack_padded_sequence().", "Batch sizes represent the number elements at each sequence step in the batch, not the varying sequence lengths passed to pack_padded_sequence(). For instance, given data abc and x the PackedSequence would contain data axbc with batch_sizes=[2,1,1].", "Note", "data can be on arbitrary device and of arbitrary dtype. sorted_indices and unsorted_indices must be torch.int64 tensors on the same device as data.", "However, batch_sizes should always be a CPU torch.int64 tensor.", "This invariant is maintained throughout PackedSequence class, and all functions that construct a :class:PackedSequence in PyTorch (i.e., they only pass in tensors conforming to this constraint).", "Alias for field number 1", "Return number of occurrences of value.", "Alias for field number 0", "Return first index of value.", "Raises ValueError if the value is not present.", "Returns true if self.data stored on a gpu", "Returns true if self.data stored on in pinned memory", "Alias for field number 2", "Performs dtype and/or device conversion on self.data.", "It has similar signature as torch.Tensor.to(), except optional arguments like non_blocking and copy should be passed as kwargs, not args, or they will not apply to the index tensors.", "Note", "If the self.data Tensor already has the correct torch.dtype and torch.device, then self is returned. Otherwise, returns a copy with the desired configuration.", "Alias for field number 3"]}, {"name": "torch.nn.PairwiseDistance", "path": "generated/torch.nn.pairwisedistance", "type": "Neuro Network", "text": ["Computes the pairwise distance between input vectors, or between columns of input matrices.", "Distances are computed using p-norm, with constant eps added to avoid division by zero if p is negative, i.e.:", "where ee is the vector of ones and the p-norm is given by."]}, {"name": "torch.nn.parallel.data_parallel()", "path": "generated/torch.nn.functional.torch.nn.parallel.data_parallel#torch.nn.parallel.data_parallel", "type": "NN Functions", "text": ["Evaluates module(input) in parallel across the GPUs given in device_ids.", "This is the functional version of the DataParallel module.", "a Tensor containing the result of module(input) located on output_device", "Tensor"]}, {"name": "torch.nn.parallel.DistributedDataParallel", "path": "generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel", "type": "Neuro Network", "text": ["Implements distributed data parallelism that is based on torch.distributed package at the module level.", "This container provides data parallelism by synchronizing gradients across each model replica. The devices to synchronize across are specified by the input process_group, which is the entire world by default. Note that DistributedDataParallel does not chunk or otherwise shard the input across participating GPUs; the user is responsible for defining how to do so, for example through the use of a DistributedSampler.", "See also: Basics and Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel. The same constraints on input as in torch.nn.DataParallel apply.", "Creation of this class requires that torch.distributed to be already initialized, by calling torch.distributed.init_process_group().", "DistributedDataParallel is proven to be significantly faster than torch.nn.DataParallel for single-node multi-GPU data parallel training.", "To use DistributedDataParallel on a host with N GPUs, you should spawn up N processes, ensuring that each process exclusively works on a single GPU from 0 to N-1. This can be done by either setting CUDA_VISIBLE_DEVICES for every process or by calling:", "where i is from 0 to N-1. In each process, you should refer the following to construct this module:", "In order to spawn up multiple processes per node, you can use either torch.distributed.launch or torch.multiprocessing.spawn.", "Note", "Please refer to PyTorch Distributed Overview for a brief introduction to all features related to distributed training.", "Note", "DistributedDataParallel can be used in conjunction with torch.distributed.optim.ZeroRedundancyOptimizer to reduce per-rank optimizer states memory footprint. Please refer to ZeroRedundancyOptimizer recipe for more details.", "Note", "nccl backend is currently the fastest and highly recommended backend when using GPUs. This applies to both single-node and multi-node distributed training.", "Note", "This module also supports mixed-precision distributed training. This means that your model can have different types of parameters such as mixed types of fp16 and fp32, the gradient reduction on these mixed types of parameters will just work fine.", "Note", "If you use torch.save on one process to checkpoint the module, and torch.load on some other processes to recover it, make sure that map_location is configured properly for every process. Without map_location, torch.load would recover the module to devices where the module was saved from.", "Note", "When a model is trained on M nodes with batch=N, the gradient will be M times smaller when compared to the same model trained on a single node with batch=M*N if the loss is summed (NOT averaged as usual) across instances in a batch (because the gradients between different nodes are averaged). You should take this into consideration when you want to obtain a mathematically equivalent training process compared to the local training counterpart. But in most cases, you can just treat a DistributedDataParallel wrapped model, a DataParallel wrapped model and an ordinary model on a single GPU as the same (E.g. using the same learning rate for equivalent batch size).", "Note", "Parameters are never broadcast between processes. The module performs an all-reduce step on gradients and assumes that they will be modified by the optimizer in all processes in the same way. Buffers (e.g. BatchNorm stats) are broadcast from the module in process of rank 0, to all other replicas in the system in every iteration.", "Note", "If you are using DistributedDataParallel in conjunction with the Distributed RPC Framework, you should always use torch.distributed.autograd.backward() to compute gradients and torch.distributed.optim.DistributedOptimizer for optimizing parameters.", "Example:", "Note", "DistributedDataParallel currently offers limited support for gradient checkpointing with torch.utils.checkpoint(). DDP will work as expected when there are no unused parameters in the model and each layer is checkpointed at most once (make sure you are not passing find_unused_parameters=True to DDP). We currently do not support the case where a layer is checkpointed multiple times, or when there unused parameters in the checkpointed model.", "Note", "To let a non-DDP model load a state dict from a DDP model, consume_prefix_in_state_dict_if_present() needs to be applied to strip the prefix \u201cmodule.\u201d in the DDP state dict before loading.", "Warning", "Constructor, forward method, and differentiation of the output (or a function of the output of this module) are distributed synchronization points. Take that into account in case different processes might be executing different code.", "Warning", "This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later. Same applies to buffers.", "Warning", "This module assumes all parameters are registered in the model of each distributed processes are in the same order. The module itself will conduct gradient allreduce following the reverse order of the registered parameters of the model. In other words, it is users\u2019 responsibility to ensure that each distributed process has the exact same model and thus the exact same parameter registration order.", "Warning", "This module allows parameters with non-rowmajor-contiguous strides. For example, your model may contain some parameters whose torch.memory_format is torch.contiguous_format and others whose format is torch.channels_last. However, corresponding parameters in different processes must have the same strides.", "Warning", "This module doesn\u2019t work with torch.autograd.grad() (i.e. it will only work if gradients are to be accumulated in .grad attributes of parameters).", "Warning", "If you plan on using this module with a nccl backend or a gloo backend (that uses Infiniband), together with a DataLoader that uses multiple workers, please change the multiprocessing start method to forkserver (Python 3 only) or spawn. Unfortunately Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will likely experience deadlocks if you don\u2019t change this setting.", "Warning", "You should never try to change your model\u2019s parameters after wrapping up your model with DistributedDataParallel. Because, when wrapping up your model with DistributedDataParallel, the constructor of DistributedDataParallel will register the additional gradient reduction functions on all the parameters of the model itself at the time of construction. If you change the model\u2019s parameters afterwards, gradient reduction functions no longer match the correct set of parameters.", "Warning", "Using DistributedDataParallel in conjunction with the Distributed RPC Framework is experimental and subject to change.", "device_ids (list of int or torch.device) \u2013 ", "CUDA devices. 1) For single-device modules, device_ids can contain exactly one device id, which represents the only CUDA device where the input module corresponding to this process resides. Alternatively, device_ids can also be None. 2) For multi-device modules and CPU modules, device_ids must be None.", "When device_ids is None for both cases, both the input data for the forward pass and the actual module must be placed on the correct device. (default: None)", "static_graph (bool) \u2013 ", "When set to True, DDP knows the trained graph is static. Static graph means 1) The set of used and unused parameters will not change during the whole training loop; in this case, it does not matter whether users set find_unused_parameters = True or not. 2) How the graph is trained will not change during the whole training loop (meaning there is no control flow depending on iterations). When static_graph is set to be True, DDP will support cases that can not be supported in the past: 1) Reentrant backwards. 2) Activation checkpointing multiple times. 3) Activation checkpointing when model has unused parameters. 4) There are model parameters that are outside of forward function. 5) Potentially improve performance when there are unused parameters, as DDP will not search graph in each iteration to detect unused parameters when static_graph is set to be True. To check whether you can set static_graph to be True, one way is to check ddp logging data at the end of your previous model training, if ddp_logging_data.get(\"can_set_static_graph\") == True, mostly you can set static_graph = True as well.", "module (Module) \u2013 the module to be parallelized.", "Example:", "A context manager to be used in conjunction with an instance of torch.nn.parallel.DistributedDataParallel to be able to train with uneven inputs across participating processes.", "This context manager will keep track of already-joined DDP processes, and \u201cshadow\u201d the forward and backward passes by inserting collective communication operations to match with the ones created by non-joined DDP processes. This will ensure each collective call has a corresponding call by already-joined DDP processes, preventing hangs or errors that would otherwise happen when training with uneven inputs across processes. Alternatively, if the flag throw_on_early_termination is specified to be True, all trainers will throw an error once one rank runs out of inputs, allowing these errors to be caught and handled according to application logic.", "Once all DDP processes have joined, the context manager will broadcast the model corresponding to the last joined process to all processes to ensure the model is the same across all processes (which is guaranteed by DDP).", "To use this to enable training with uneven inputs across processes, simply wrap this context manager around your training loop. No further modifications to the model or data loading is required.", "Warning", "If the model or training loop this context manager is wrapped around has additional distributed collective operations, such as SyncBatchNorm in the model\u2019s forward pass, then the flag throw_on_early_termination must be enabled. This is because this context manager is not aware of non-DDP collective communication. This flag will cause all ranks to throw when any one rank exhausts inputs, allowing these errors to be caught and recovered from across all ranks.", "Example:", "Returns the DDP join hook, which enables training on uneven inputs by shadowing the collective communications in the forward and backward passes.", "kwargs (dict) \u2013 a dict containing any keyword arguments to modify the behavior of the join hook at run time; all Joinable instances sharing the same join context manager are forwarded the same value for kwargs.", "If True, then gradients are divided by the initial world size that DDP was launched with. If False, then gradients are divided by the effective world size (i.e. the number of non-joined processes), meaning that the uneven inputs contribute more toward the global gradient. Typically, this should be set to True if the degree of unevenness is small but can be set to False in extreme cases for possibly better results. Default is True.", "A context manager to disable gradient synchronizations across DDP processes. Within this context, gradients will be accumulated on module variables, which will later be synchronized in the first forward-backward pass exiting the context.", "Example:", "Warning", "The forward pass should be included inside the context manager, or else gradients will still be synchronized.", "Registers a communication hook which is an enhancement that provides a flexible hook to users where they can specify how DDP aggregates gradients across multiple workers.", "This hook would be very useful for researchers to try out new ideas. For example, this hook can be used to implement several algorithms like GossipGrad and gradient compression which involve different communication strategies for parameter syncs while running Distributed DataParallel training.", "state (object) \u2013 ", "Passed to the hook to maintain any state information during the training process. Examples include error feedback in gradient compression, peers to communicate with next in GossipGrad, etc.", "It is locally stored by each worker and shared by all the gradient tensors on the worker.", "hook (Callable) \u2013 ", "Callable with the following signature: hook(state: object, bucket: dist.GradBucket) -> torch.futures.Future[torch.Tensor]:", "This function is called once the bucket is ready. The hook can perform whatever processing is needed and return a Future indicating completion of any async work (ex: allreduce). If the hook doesn\u2019t perform any communication, it still must return a completed Future. The Future should hold the new value of grad bucket\u2019s tensors. Once a bucket is ready, c10d reducer would call this hook and use the tensors returned by the Future and copy grads to individual parameters. Note that the future\u2019s return type must be a single tensor.", "We also provide an API called get_future to retrieve a Future associated with the completion of c10d.ProcessGroup.Work. get_future is currently supported for NCCL and also supported for most operations on GLOO and MPI, except for peer to peer operations (send/recv).", "Warning", "Grad bucket\u2019s tensors will not be predivided by world_size. User is responsible to divide by the world_size in case of operations like allreduce.", "Warning", "DDP communication hook can only be registered once and should be registered before calling backward.", "Warning", "The Future object that hook returns should contain a single tensor that has the same shape with the tensors inside grad bucket.", "Warning", "get_future API supports NCCL, and partially GLOO and MPI backends (no support for peer-to-peer operations like send/recv) and will return a torch.futures.Future.", "Below is an example of a noop hook that returns the same tensor.", "Below is an example of a Parallel SGD algorithm where gradients are encoded before allreduce, and then decoded after allreduce."]}, {"name": "torch.nn.parallel.DistributedDataParallel.join()", "path": "generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel.join", "type": "Neuro Network", "text": ["A context manager to be used in conjunction with an instance of torch.nn.parallel.DistributedDataParallel to be able to train with uneven inputs across participating processes.", "This context manager will keep track of already-joined DDP processes, and \u201cshadow\u201d the forward and backward passes by inserting collective communication operations to match with the ones created by non-joined DDP processes. This will ensure each collective call has a corresponding call by already-joined DDP processes, preventing hangs or errors that would otherwise happen when training with uneven inputs across processes. Alternatively, if the flag throw_on_early_termination is specified to be True, all trainers will throw an error once one rank runs out of inputs, allowing these errors to be caught and handled according to application logic.", "Once all DDP processes have joined, the context manager will broadcast the model corresponding to the last joined process to all processes to ensure the model is the same across all processes (which is guaranteed by DDP).", "To use this to enable training with uneven inputs across processes, simply wrap this context manager around your training loop. No further modifications to the model or data loading is required.", "Warning", "If the model or training loop this context manager is wrapped around has additional distributed collective operations, such as SyncBatchNorm in the model\u2019s forward pass, then the flag throw_on_early_termination must be enabled. This is because this context manager is not aware of non-DDP collective communication. This flag will cause all ranks to throw when any one rank exhausts inputs, allowing these errors to be caught and recovered from across all ranks.", "Example:"]}, {"name": "torch.nn.parallel.DistributedDataParallel.join_hook()", "path": "generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel.join_hook", "type": "Neuro Network", "text": ["Returns the DDP join hook, which enables training on uneven inputs by shadowing the collective communications in the forward and backward passes.", "kwargs (dict) \u2013 a dict containing any keyword arguments to modify the behavior of the join hook at run time; all Joinable instances sharing the same join context manager are forwarded the same value for kwargs.", "If True, then gradients are divided by the initial world size that DDP was launched with. If False, then gradients are divided by the effective world size (i.e. the number of non-joined processes), meaning that the uneven inputs contribute more toward the global gradient. Typically, this should be set to True if the degree of unevenness is small but can be set to False in extreme cases for possibly better results. Default is True."]}, {"name": "torch.nn.parallel.DistributedDataParallel.no_sync()", "path": "generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel.no_sync", "type": "Neuro Network", "text": ["A context manager to disable gradient synchronizations across DDP processes. Within this context, gradients will be accumulated on module variables, which will later be synchronized in the first forward-backward pass exiting the context.", "Example:", "Warning", "The forward pass should be included inside the context manager, or else gradients will still be synchronized."]}, {"name": "torch.nn.parallel.DistributedDataParallel.register_comm_hook()", "path": "generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel.register_comm_hook", "type": "Neuro Network", "text": ["Registers a communication hook which is an enhancement that provides a flexible hook to users where they can specify how DDP aggregates gradients across multiple workers.", "This hook would be very useful for researchers to try out new ideas. For example, this hook can be used to implement several algorithms like GossipGrad and gradient compression which involve different communication strategies for parameter syncs while running Distributed DataParallel training.", "state (object) \u2013 ", "Passed to the hook to maintain any state information during the training process. Examples include error feedback in gradient compression, peers to communicate with next in GossipGrad, etc.", "It is locally stored by each worker and shared by all the gradient tensors on the worker.", "hook (Callable) \u2013 ", "Callable with the following signature: hook(state: object, bucket: dist.GradBucket) -> torch.futures.Future[torch.Tensor]:", "This function is called once the bucket is ready. The hook can perform whatever processing is needed and return a Future indicating completion of any async work (ex: allreduce). If the hook doesn\u2019t perform any communication, it still must return a completed Future. The Future should hold the new value of grad bucket\u2019s tensors. Once a bucket is ready, c10d reducer would call this hook and use the tensors returned by the Future and copy grads to individual parameters. Note that the future\u2019s return type must be a single tensor.", "We also provide an API called get_future to retrieve a Future associated with the completion of c10d.ProcessGroup.Work. get_future is currently supported for NCCL and also supported for most operations on GLOO and MPI, except for peer to peer operations (send/recv).", "Warning", "Grad bucket\u2019s tensors will not be predivided by world_size. User is responsible to divide by the world_size in case of operations like allreduce.", "Warning", "DDP communication hook can only be registered once and should be registered before calling backward.", "Warning", "The Future object that hook returns should contain a single tensor that has the same shape with the tensors inside grad bucket.", "Warning", "get_future API supports NCCL, and partially GLOO and MPI backends (no support for peer-to-peer operations like send/recv) and will return a torch.futures.Future.", "Below is an example of a noop hook that returns the same tensor.", "Below is an example of a Parallel SGD algorithm where gradients are encoded before allreduce, and then decoded after allreduce."]}, {"name": "torch.nn.Parameter", "path": "generated/torch.nn.parameter.parameter", "type": "Neuro Network", "text": ["A kind of Tensor that is to be considered a module parameter.", "Parameters are Tensor subclasses, that have a very special property when used with Module s - when they\u2019re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in parameters() iterator. Assigning a Tensor doesn\u2019t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as Parameter, these temporaries would get registered too."]}, {"name": "torch.nn.parameter.Parameter", "path": "generated/torch.nn.parameter.parameter#torch.nn.parameter.Parameter", "type": "Neuro Network", "text": ["A kind of Tensor that is to be considered a module parameter.", "Parameters are Tensor subclasses, that have a very special property when used with Module s - when they\u2019re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in parameters() iterator. Assigning a Tensor doesn\u2019t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as Parameter, these temporaries would get registered too."]}, {"name": "torch.nn.parameter.UninitializedBuffer", "path": "generated/torch.nn.parameter.uninitializedbuffer#torch.nn.parameter.UninitializedBuffer", "type": "Neuro Network", "text": ["A buffer that is not initialized.", "Uninitialized Buffer is a a special case of torch.Tensor where the shape of the data is still unknown.", "Unlike a torch.Tensor, uninitialized parameters hold no data and attempting to access some properties, like their shape, will throw a runtime error. The only operations that can be performed on a uninitialized parameter are changing its datatype, moving it to a different device and converting it to a regular torch.Tensor.", "The default device or dtype to use when the buffer is materialized can be set during construction using e.g. device='cuda'."]}, {"name": "torch.nn.parameter.UninitializedParameter", "path": "generated/torch.nn.parameter.uninitializedparameter#torch.nn.parameter.UninitializedParameter", "type": "Neuro Network", "text": ["A parameter that is not initialized.", "Uninitialized Parameters are a a special case of torch.nn.Parameter where the shape of the data is still unknown.", "Unlike a torch.nn.Parameter, uninitialized parameters hold no data and attempting to access some properties, like their shape, will throw a runtime error. The only operations that can be performed on a uninitialized parameter are changing its datatype, moving it to a different device and converting it to a regular torch.nn.Parameter.", "The default device or dtype to use when the parameter is materialized can be set during construction using e.g. device='cuda'.", "alias of Parameter"]}, {"name": "torch.nn.parameter.UninitializedParameter.cls_to_become", "path": "generated/torch.nn.parameter.uninitializedparameter#torch.nn.parameter.UninitializedParameter.cls_to_become", "type": "Neuro Network", "text": ["alias of Parameter"]}, {"name": "torch.nn.ParameterDict", "path": "generated/torch.nn.parameterdict", "type": "Neuro Network", "text": ["Holds parameters in a dictionary.", "ParameterDict can be indexed like a regular Python dictionary, but Parameters it contains are properly registered, and will be visible by all Module methods. Other objects are treated as would be done by a regular Python dictionary", "ParameterDict is an ordered dictionary. update() with other unordered mapping types (e.g., Python\u2019s plain dict) does not preserve the order of the merged mapping. On the other hand, OrderedDict or another ParameterDict will preserve their ordering.", "Note that the constructor, assigning an element of the dictionary and the update() method will convert any Tensor into Parameter.", "values (iterable, optional) \u2013 a mapping (dictionary) of (string : Any) or an iterable of key-value pairs of type (string, Any)", "Example:", "Remove all items from the ParameterDict.", "Returns a copy of this ParameterDict instance.", "ParameterDict", "Return a new ParameterDict with the keys provided", "ParameterDict", "Return the parameter associated with key if present. Otherwise return default if provided, None if not.", "Any", "Return an iterable of the ParameterDict key/value pairs.", "Iterable[Tuple[str, Any]]", "Return an iterable of the ParameterDict keys.", "Iterable[str]", "Remove key from the ParameterDict and return its parameter.", "key (str) \u2013 key to pop from the ParameterDict", "Any", "Remove and return the last inserted (key, parameter) pair from the ParameterDict", "Tuple[str, Any]", "If key is in the ParameterDict, return its value. If not, insert key with a parameter default and return default. default defaults to None.", "Any", "Update the ParameterDict with the key-value pairs from a mapping or an iterable, overwriting existing keys.", "Note", "If parameters is an OrderedDict, a ParameterDict, or an iterable of key-value pairs, the order of new elements in it is preserved.", "parameters (iterable) \u2013 a mapping (dictionary) from string to Parameter, or an iterable of key-value pairs of type (string, Parameter)", "Return an iterable of the ParameterDict values.", "Iterable[Any]"]}, {"name": "torch.nn.ParameterDict.clear()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.clear", "type": "Neuro Network", "text": ["Remove all items from the ParameterDict."]}, {"name": "torch.nn.ParameterDict.copy()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.copy", "type": "Neuro Network", "text": ["Returns a copy of this ParameterDict instance.", "ParameterDict"]}, {"name": "torch.nn.ParameterDict.fromkeys()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.fromkeys", "type": "Neuro Network", "text": ["Return a new ParameterDict with the keys provided", "ParameterDict"]}, {"name": "torch.nn.ParameterDict.get()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.get", "type": "Neuro Network", "text": ["Return the parameter associated with key if present. Otherwise return default if provided, None if not.", "Any"]}, {"name": "torch.nn.ParameterDict.items()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.items", "type": "Neuro Network", "text": ["Return an iterable of the ParameterDict key/value pairs.", "Iterable[Tuple[str, Any]]"]}, {"name": "torch.nn.ParameterDict.keys()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.keys", "type": "Neuro Network", "text": ["Return an iterable of the ParameterDict keys.", "Iterable[str]"]}, {"name": "torch.nn.ParameterDict.pop()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.pop", "type": "Neuro Network", "text": ["Remove key from the ParameterDict and return its parameter.", "key (str) \u2013 key to pop from the ParameterDict", "Any"]}, {"name": "torch.nn.ParameterDict.popitem()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.popitem", "type": "Neuro Network", "text": ["Remove and return the last inserted (key, parameter) pair from the ParameterDict", "Tuple[str, Any]"]}, {"name": "torch.nn.ParameterDict.setdefault()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.setdefault", "type": "Neuro Network", "text": ["If key is in the ParameterDict, return its value. If not, insert key with a parameter default and return default. default defaults to None.", "Any"]}, {"name": "torch.nn.ParameterDict.update()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.update", "type": "Neuro Network", "text": ["Update the ParameterDict with the key-value pairs from a mapping or an iterable, overwriting existing keys.", "Note", "If parameters is an OrderedDict, a ParameterDict, or an iterable of key-value pairs, the order of new elements in it is preserved.", "parameters (iterable) \u2013 a mapping (dictionary) from string to Parameter, or an iterable of key-value pairs of type (string, Parameter)"]}, {"name": "torch.nn.ParameterDict.values()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.values", "type": "Neuro Network", "text": ["Return an iterable of the ParameterDict values.", "Iterable[Any]"]}, {"name": "torch.nn.ParameterList", "path": "generated/torch.nn.parameterlist", "type": "Neuro Network", "text": ["Holds parameters in a list.", "ParameterList can be used like a regular Python list, but Tensors that are Parameter are properly registered, and will be visible by all Module methods.", "Note that the constructor, assigning an element of the list, the append() method and the extend() method will convert any Tensor into Parameter.", "parameters (iterable, optional) \u2013 an iterable of elements to add to the list.", "Example:", "Appends a given value at the end of the list.", "value (Any) \u2013 value to append", "ParameterList", "Appends values from a Python iterable to the end of the list.", "values (iterable) \u2013 iterable of values to append", "ParameterList"]}, {"name": "torch.nn.ParameterList.append()", "path": "generated/torch.nn.parameterlist#torch.nn.ParameterList.append", "type": "Neuro Network", "text": ["Appends a given value at the end of the list.", "value (Any) \u2013 value to append", "ParameterList"]}, {"name": "torch.nn.ParameterList.extend()", "path": "generated/torch.nn.parameterlist#torch.nn.ParameterList.extend", "type": "Neuro Network", "text": ["Appends values from a Python iterable to the end of the list.", "values (iterable) \u2013 iterable of values to append", "ParameterList"]}, {"name": "torch.nn.ParametrizationList", "path": "generated/torch.nn.utils.parametrize.parametrizationlist", "type": "Neuro Network", "text": ["A sequential container that holds and manages the original or original0, original1, \u2026 parameters or buffers of a parametrized torch.nn.Module.", "It is the type of module.parametrizations[tensor_name] when module[tensor_name] has been parametrized with register_parametrization().", "If the first registered parametrization has a right_inverse that returns one tensor or does not have a right_inverse (in which case we assume that right_inverse is the identity), it will hold the tensor under the name original. If it has a right_inverse that returns more than one tensor, these will be registered as original0, original1, \u2026", "Warning", "This class is used internally by register_parametrization(). It is documented here for completeness. It shall not be instantiated by the user.", "Calls the methods right_inverse (see register_parametrization()) of the parametrizations in the inverse order they were registered in. Then, it stores the result in self.original if right_inverse outputs one tensor or in self.original0, self.original1, \u2026 if it outputs several.", "value (Tensor) \u2013 Value to which initialize the module"]}, {"name": "torch.nn.PixelShuffle", "path": "generated/torch.nn.pixelshuffle", "type": "Neuro Network", "text": ["Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r), where r is an upscale factor.", "This is useful for implementing efficient sub-pixel convolution with a stride of 1/r1/r.", "See the paper: Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network by Shi et. al (2016) for more details.", "upscale_factor (int) \u2013 factor to increase spatial resolution by", "Examples:"]}, {"name": "torch.nn.PixelUnshuffle", "path": "generated/torch.nn.pixelunshuffle", "type": "Neuro Network", "text": ["Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W), where r is a downscale factor.", "See the paper: Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network by Shi et. al (2016) for more details.", "downscale_factor (int) \u2013 factor to decrease spatial resolution by", "Examples:"]}, {"name": "torch.nn.PoissonNLLLoss", "path": "generated/torch.nn.poissonnllloss", "type": "Neuro Network", "text": ["Negative log likelihood loss with Poisson distribution of target.", "The loss can be described as:", "The last term can be omitted or approximated with Stirling formula. The approximation is used for target values more than 1. For targets less or equal to 1 zeros are added to the loss.", "full (bool, optional) \u2013 ", "whether to compute full loss, i. e. to add the Stirling approximation term", "Examples:"]}, {"name": "torch.nn.PReLU", "path": "generated/torch.nn.prelu", "type": "Neuro Network", "text": ["Applies the element-wise function:", "or", "Here aa is a learnable parameter. When called without arguments, nn.PReLU() uses a single parameter aa across all input channels. If called with nn.PReLU(nChannels), a separate aa is used for each input channel.", "Note", "weight decay should not be used when learning aa for good performance.", "Note", "Channel dim is the 2nd dim of input. When input has dims < 2, then there is no channel dim and the number of channels = 1.", "weight (Tensor) \u2013 the learnable weights of shape (num_parameters).", "Examples:"]}, {"name": "torch.nn.PruningContainer", "path": "generated/torch.nn.utils.prune.pruningcontainer", "type": "Neuro Network", "text": ["Container holding a sequence of pruning methods for iterative pruning. Keeps track of the order in which pruning methods are applied and handles combining successive pruning calls.", "Accepts as argument an instance of a BasePruningMethod or an iterable of them.", "Adds a child pruning method to the container.", "method (subclass of BasePruningMethod) \u2013 child pruning method to be added to the container.", "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Applies the latest method by computing the new partial masks and returning its combination with the default_mask. The new partial mask should be computed on the entries or channels that were not zeroed out by the default_mask. Which portions of the tensor t the new mask will be calculated from depends on the PRUNING_TYPE (handled by the type handler):", "new mask that combines the effects of the default_mask and the new mask from the current pruning method (of same dimensions as default_mask and t).", "mask (torch.Tensor)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.RandomStructured", "path": "generated/torch.nn.utils.prune.randomstructured", "type": "Neuro Network", "text": ["Prune entire (currently unpruned) channels in a tensor at random.", "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the default_mask by randomly zeroing out channels along the specified dim of the tensor.", "mask to apply to t, of same dims as t", "mask (torch.Tensor)", "IndexError \u2013 if self.dim >= len(t.shape)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.RandomUnstructured", "path": "generated/torch.nn.utils.prune.randomunstructured", "type": "Neuro Network", "text": ["Prune (currently unpruned) units in a tensor at random.", "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.ReflectionPad1d", "path": "generated/torch.nn.reflectionpad1d", "type": "Neuro Network", "text": ["Pads the input tensor using the reflection of the input boundary.", "For N-dimensional padding, use torch.nn.functional.pad().", "padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 2-tuple, uses (padding_left\\text{padding\\_left}, padding_right\\text{padding\\_right})", "Output: (C,Wout)(C, W_{out}) or (N,C,Wout)(N, C, W_{out}), where", "Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}", "Examples:"]}, {"name": "torch.nn.ReflectionPad2d", "path": "generated/torch.nn.reflectionpad2d", "type": "Neuro Network", "text": ["Pads the input tensor using the reflection of the input boundary.", "For N-dimensional padding, use torch.nn.functional.pad().", "padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}, padding_right\\text{padding\\_right}, padding_top\\text{padding\\_top}, padding_bottom\\text{padding\\_bottom})", "Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) or (C,Hout,Wout)(C, H_{out}, W_{out}) where", "Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}", "Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}", "Examples:"]}, {"name": "torch.nn.ReflectionPad3d", "path": "generated/torch.nn.reflectionpad3d", "type": "Neuro Network", "text": ["Pads the input tensor using the reflection of the input boundary.", "For N-dimensional padding, use torch.nn.functional.pad().", "padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding_left\\text{padding\\_left}, padding_right\\text{padding\\_right}, padding_top\\text{padding\\_top}, padding_bottom\\text{padding\\_bottom}, padding_front\\text{padding\\_front}, padding_back\\text{padding\\_back})", "Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out}) or (C,Dout,Hout,Wout)(C, D_{out}, H_{out}, W_{out}), where", "Dout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\_front} + \\text{padding\\_back}", "Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}", "Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}", "Examples:"]}, {"name": "torch.nn.ReLU", "path": "generated/torch.nn.relu", "type": "Neuro Network", "text": ["Applies the rectified linear unit function element-wise:", "ReLU(x)=(x)+=max\u2061(0,x)\\text{ReLU}(x) = (x)^+ = \\max(0, x)", "inplace (bool) \u2013 can optionally do the operation in-place. Default: False", "Examples:"]}, {"name": "torch.nn.ReLU6", "path": "generated/torch.nn.relu6", "type": "Neuro Network", "text": ["Applies the element-wise function:", "inplace (bool) \u2013 can optionally do the operation in-place. Default: False", "Examples:"]}, {"name": "torch.nn.ReplicationPad1d", "path": "generated/torch.nn.replicationpad1d", "type": "Neuro Network", "text": ["Pads the input tensor using replication of the input boundary.", "For N-dimensional padding, use torch.nn.functional.pad().", "padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 2-tuple, uses (padding_left\\text{padding\\_left}, padding_right\\text{padding\\_right})", "Output: (C,Wout)(C, W_{out}) or (N,C,Wout)(N, C, W_{out}), where", "Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}", "Examples:"]}, {"name": "torch.nn.ReplicationPad2d", "path": "generated/torch.nn.replicationpad2d", "type": "Neuro Network", "text": ["Pads the input tensor using replication of the input boundary.", "For N-dimensional padding, use torch.nn.functional.pad().", "padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}, padding_right\\text{padding\\_right}, padding_top\\text{padding\\_top}, padding_bottom\\text{padding\\_bottom})", "Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) or (C,Hout,Wout)(C, H_{out}, W_{out}), where", "Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}", "Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}", "Examples:"]}, {"name": "torch.nn.ReplicationPad3d", "path": "generated/torch.nn.replicationpad3d", "type": "Neuro Network", "text": ["Pads the input tensor using replication of the input boundary.", "For N-dimensional padding, use torch.nn.functional.pad().", "padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding_left\\text{padding\\_left}, padding_right\\text{padding\\_right}, padding_top\\text{padding\\_top}, padding_bottom\\text{padding\\_bottom}, padding_front\\text{padding\\_front}, padding_back\\text{padding\\_back})", "Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out}) or (C,Dout,Hout,Wout)(C, D_{out}, H_{out}, W_{out}), where", "Dout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\_front} + \\text{padding\\_back}", "Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}", "Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}", "Examples:"]}, {"name": "torch.nn.RNN", "path": "generated/torch.nn.rnn", "type": "Neuro Network", "text": ["Applies a multi-layer Elman RNN with tanh\u2061\\tanh or ReLU\\text{ReLU} non-linearity to an input sequence.", "For each element in the input sequence, each layer computes the following function:", "where hth_t is the hidden state at time t, xtx_t is the input at time t, and h(t\u22121)h_{(t-1)} is the hidden state of the previous layer at time t-1 or the initial hidden state at time 0. If nonlinearity is 'relu', then ReLU\\text{ReLU} is used instead of tanh\u2061\\tanh.", "where:", "Note", "All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}) where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}", "Note", "For bidirectional RNNs, forward and backward are directions 0 and 1 respectively. Example of splitting the output layers when batch_first=False: output.view(seq_len, batch, num_directions, hidden_size).", "Note", "batch_first argument is ignored for unbatched inputs.", "Warning", "There are known non-determinism issues for RNN functions on some versions of cuDNN and CUDA. You can enforce deterministic behavior by setting the following environment variables:", "On CUDA 10.1, set environment variable CUDA_LAUNCH_BLOCKING=1. This may affect performance.", "On CUDA 10.2 or later, set environment variable (note the leading colon symbol) CUBLAS_WORKSPACE_CONFIG=:16:8 or CUBLAS_WORKSPACE_CONFIG=:4096:2.", "See the cuDNN 8 Release Notes for more information.", "Note", "If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance.", "Examples:"]}, {"name": "torch.nn.RNNBase", "path": "generated/torch.nn.rnnbase", "type": "Neuro Network", "text": ["Base class for RNN modules (RNN, LSTM, GRU).", "Implements aspects of RNNs shared by the RNN, LSTM, and GRU classes, such as module initialization and utility methods for parameter storage management.", "Note", "The forward method is not implemented by the RNNBase class.", "Note", "LSTM and GRU classes override some methods implemented by RNNBase.", "Resets parameter data pointer so that they can use faster code paths.", "Right now, this works only if the module is on the GPU and cuDNN is enabled. Otherwise, it\u2019s a no-op."]}, {"name": "torch.nn.RNNBase.flatten_parameters()", "path": "generated/torch.nn.rnnbase#torch.nn.RNNBase.flatten_parameters", "type": "Neuro Network", "text": ["Resets parameter data pointer so that they can use faster code paths.", "Right now, this works only if the module is on the GPU and cuDNN is enabled. Otherwise, it\u2019s a no-op."]}, {"name": "torch.nn.RNNCell", "path": "generated/torch.nn.rnncell", "type": "Neuro Network", "text": ["An Elman RNN cell with tanh or ReLU non-linearity.", "If nonlinearity is \u2018relu\u2019, then ReLU is used in place of tanh.", "Note", "All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}) where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}", "Examples:"]}, {"name": "torch.nn.RReLU", "path": "generated/torch.nn.rrelu", "type": "Neuro Network", "text": ["Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper:", "Empirical Evaluation of Rectified Activations in Convolutional Network.", "The function is defined as:", "where aa is randomly sampled from uniform distribution U(lower,upper)\\mathcal{U}(\\text{lower}, \\text{upper}) during training while during evaluation aa is fixed with a=lower+upper2a = \\frac{\\text{lower} + \\text{upper}}{2}.", "See: https://arxiv.org/pdf/1505.00853.pdf", "Examples:"]}, {"name": "torch.nn.SELU", "path": "generated/torch.nn.selu", "type": "Neuro Network", "text": ["Applied element-wise, as:", "with \u03b1=1.6732632423543772848170429916717\\alpha = 1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946\\text{scale} = 1.0507009873554804934193349852946.", "Warning", "When using kaiming_normal or kaiming_normal_ for initialisation, nonlinearity='linear' should be used instead of nonlinearity='selu' in order to get Self-Normalizing Neural Networks. See torch.nn.init.calculate_gain() for more information.", "More details can be found in the paper Self-Normalizing Neural Networks .", "inplace (bool, optional) \u2013 can optionally do the operation in-place. Default: False", "Examples:"]}, {"name": "torch.nn.Sequential", "path": "generated/torch.nn.sequential", "type": "Neuro Network", "text": ["A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an OrderedDict of modules can be passed in. The forward() method of Sequential accepts any input and forwards it to the first module it contains. It then \u201cchains\u201d outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.", "The value a Sequential provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the Sequential applies to each of the modules it stores (which are each a registered submodule of the Sequential).", "What\u2019s the difference between a Sequential and a torch.nn.ModuleList? A ModuleList is exactly what it sounds like\u2013a list for storing Module s! On the other hand, the layers in a Sequential are connected in a cascading way.", "Example:", "Appends a given module to the end.", "module (nn.Module) \u2013 module to append", "Sequential"]}, {"name": "torch.nn.Sequential.append()", "path": "generated/torch.nn.sequential#torch.nn.Sequential.append", "type": "Neuro Network", "text": ["Appends a given module to the end.", "module (nn.Module) \u2013 module to append", "Sequential"]}, {"name": "torch.nn.Sigmoid", "path": "generated/torch.nn.sigmoid", "type": "Neuro Network", "text": ["Applies the element-wise function:", "Examples:"]}, {"name": "torch.nn.SiLU", "path": "generated/torch.nn.silu", "type": "Neuro Network", "text": ["Applies the Sigmoid Linear Unit (SiLU) function, element-wise. The SiLU function is also known as the swish function.", "Note", "See Gaussian Error Linear Units (GELUs) where the SiLU (Sigmoid Linear Unit) was originally coined, and see Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning and Swish: a Self-Gated Activation Function where the SiLU was experimented with later.", "Examples:"]}, {"name": "torch.nn.SmoothL1Loss", "path": "generated/torch.nn.smoothl1loss", "type": "Neuro Network", "text": ["Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. It is less sensitive to outliers than torch.nn.MSELoss and in some cases prevents exploding gradients (e.g. see the paper Fast R-CNN by Ross Girshick).", "For a batch of size NN, the unreduced loss can be described as:", "with", "If reduction is not none, then:", "Note", "Smooth L1 loss can be seen as exactly L1Loss, but with the \u2223x\u2212y\u2223<beta|x - y| < beta portion replaced with a quadratic function such that its slope is 1 at \u2223x\u2212y\u2223=beta|x - y| = beta. The quadratic segment smooths the L1 loss near \u2223x\u2212y\u2223=0|x - y| = 0.", "Note", "Smooth L1 loss is closely related to HuberLoss, being equivalent to huber(x,y)/betahuber(x, y) / beta (note that Smooth L1\u2019s beta hyper-parameter is also known as delta for Huber). This leads to the following differences:"]}, {"name": "torch.nn.SoftMarginLoss", "path": "generated/torch.nn.softmarginloss", "type": "Neuro Network", "text": ["Creates a criterion that optimizes a two-class classification logistic loss between input tensor xx and target tensor yy (containing 1 or -1)."]}, {"name": "torch.nn.Softmax", "path": "generated/torch.nn.softmax", "type": "Neuro Network", "text": ["Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1.", "Softmax is defined as:", "When the input Tensor is a sparse tensor then the unspecified values are treated as -inf.", "a Tensor of the same dimension and shape as the input with values in the range [0, 1]", "dim (int) \u2013 A dimension along which Softmax will be computed (so every slice along dim will sum to 1).", "None", "Note", "This module doesn\u2019t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use LogSoftmax instead (it\u2019s faster and has better numerical properties).", "Examples:"]}, {"name": "torch.nn.Softmax2d", "path": "generated/torch.nn.softmax2d", "type": "Neuro Network", "text": ["Applies SoftMax over features to each spatial location.", "When given an image of Channels x Height x Width, it will apply Softmax to each location (Channels,hi,wj)(Channels, h_i, w_j)", "a Tensor of the same dimension and shape as the input with values in the range [0, 1]", "None", "Examples:"]}, {"name": "torch.nn.Softmin", "path": "generated/torch.nn.softmin", "type": "Neuro Network", "text": ["Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0, 1] and sum to 1.", "Softmin is defined as:", "dim (int) \u2013 A dimension along which Softmin will be computed (so every slice along dim will sum to 1).", "a Tensor of the same dimension and shape as the input, with values in the range [0, 1]", "None", "Examples:"]}, {"name": "torch.nn.Softplus", "path": "generated/torch.nn.softplus", "type": "Neuro Network", "text": ["Applies the Softplus function Softplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x)) element-wise.", "SoftPlus is a smooth approximation to the ReLU function and can be used to constrain the output of a machine to always be positive.", "For numerical stability the implementation reverts to the linear function when input\u00d7\u03b2>thresholdinput \\times \\beta > threshold.", "Examples:"]}, {"name": "torch.nn.Softshrink", "path": "generated/torch.nn.softshrink", "type": "Neuro Network", "text": ["Applies the soft shrinkage function elementwise:", "lambd (float) \u2013 the \u03bb\\lambda (must be no less than zero) value for the Softshrink formulation. Default: 0.5", "Examples:"]}, {"name": "torch.nn.Softsign", "path": "generated/torch.nn.softsign", "type": "Neuro Network", "text": ["Applies the element-wise function:", "Examples:"]}, {"name": "torch.nn.SyncBatchNorm", "path": "generated/torch.nn.syncbatchnorm", "type": "Neuro Network", "text": ["Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .", "The mean and standard-deviation are calculated per-dimension over all mini-batches of the same process groups. \u03b3\\gamma and \u03b2\\beta are learnable parameter vectors of size C (where C is the input size). By default, the elements of \u03b3\\gamma are sampled from U(0,1)\\mathcal{U}(0, 1) and the elements of \u03b2\\beta are set to 0. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False).", "Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.", "If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.", "Note", "This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t, where x^\\hat{x} is the estimated statistic and xtx_t is the new observed value.", "Because the Batch Normalization is done for each channel in the C dimension, computing statistics on (N, +) slices, it\u2019s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization.", "Currently SyncBatchNorm only supports DistributedDataParallel (DDP) with single GPU per process. Use torch.nn.SyncBatchNorm.convert_sync_batchnorm() to convert BatchNorm*D layer to SyncBatchNorm before wrapping Network with DDP.", "Note", "Synchronization of batchnorm statistics occurs only while training, i.e. synchronization is disabled when model.eval() is set or if self.training is otherwise False.", "Examples:", "Helper function to convert all BatchNorm*D layers in the model to torch.nn.SyncBatchNorm layers.", "The original module with the converted torch.nn.SyncBatchNorm layers. If the original module is a BatchNorm*D layer, a new torch.nn.SyncBatchNorm layer object will be returned instead.", "Example:"]}, {"name": "torch.nn.SyncBatchNorm.convert_sync_batchnorm()", "path": "generated/torch.nn.syncbatchnorm#torch.nn.SyncBatchNorm.convert_sync_batchnorm", "type": "Neuro Network", "text": ["Helper function to convert all BatchNorm*D layers in the model to torch.nn.SyncBatchNorm layers.", "The original module with the converted torch.nn.SyncBatchNorm layers. If the original module is a BatchNorm*D layer, a new torch.nn.SyncBatchNorm layer object will be returned instead.", "Example:"]}, {"name": "torch.nn.Tanh", "path": "generated/torch.nn.tanh", "type": "Neuro Network", "text": ["Applies the Hyperbolic Tangent (Tanh) function element-wise.", "Tanh is defined as:", "Examples:"]}, {"name": "torch.nn.Tanhshrink", "path": "generated/torch.nn.tanhshrink", "type": "Neuro Network", "text": ["Applies the element-wise function:", "Examples:"]}, {"name": "torch.nn.Threshold", "path": "generated/torch.nn.threshold", "type": "Neuro Network", "text": ["Thresholds each element of the input Tensor.", "Threshold is defined as:", "Examples:"]}, {"name": "torch.nn.torch.nn.modules.module.register_module_backward_hook", "path": "generated/torch.nn.modules.module.register_module_backward_hook", "type": "Neuro Network", "text": ["Registers a backward hook common to all the modules.", "This function is deprecated in favor of torch.nn.modules.module.register_module_full_backward_hook() and the behavior of this function will change in future versions.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.torch.nn.modules.module.register_module_buffer_registration_hook", "path": "generated/torch.nn.modules.module.register_module_buffer_registration_hook", "type": "Neuro Network", "text": ["Registers a buffer registration hook common to all modules.", "Warning", "This adds global state to the nn.Module module", "The hook will be called every time register_buffer() is invoked. It should have the following signature:", "The hook can modify the input or return a single modified value in the hook.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.torch.nn.modules.module.register_module_forward_hook", "path": "generated/torch.nn.modules.module.register_module_forward_hook", "type": "Neuro Network", "text": ["Registers a global forward hook for all the modules", "Warning", "This adds global state to the nn.module module and it is only intended for debugging/profiling purposes.", "The hook will be called every time after forward() has computed an output. It should have the following signature:", "The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "This hook will be executed before specific module hooks registered with register_forward_hook."]}, {"name": "torch.nn.torch.nn.modules.module.register_module_forward_pre_hook", "path": "generated/torch.nn.modules.module.register_module_forward_pre_hook", "type": "Neuro Network", "text": ["Registers a forward pre-hook common to all modules.", "Warning", "This adds global state to the nn.module module and it is only intended for debugging/profiling purposes.", "The hook will be called every time before forward() is invoked. It should have the following signature:", "The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).", "This hook has precedence over the specific module hooks registered with register_forward_pre_hook.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.torch.nn.modules.module.register_module_full_backward_hook", "path": "generated/torch.nn.modules.module.register_module_full_backward_hook", "type": "Neuro Network", "text": ["Registers a backward hook common to all the modules.", "Warning", "This adds global state to the nn.module module and it is only intended for debugging/profiling purposes.", "The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature:", "The grad_input and grad_output are tuples. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments will not appear in the hook. Entries in grad_input and grad_output will be None for all non-Tensor arguments.", "For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module\u2019s forward function.", "Global hooks are called before hooks registered with register_backward_hook", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.torch.nn.modules.module.register_module_full_backward_pre_hook", "path": "generated/torch.nn.modules.module.register_module_full_backward_pre_hook", "type": "Neuro Network", "text": ["Registers a backward pre-hook common to all the modules.", "Warning", "This adds global state to the nn.module module and it is only intended for debugging/profiling purposes.", "The hook will be called every time the gradients for the module are computed. The hook should have the following signature:", "The grad_output is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of grad_output in subsequent computations. Entries in grad_output will be None for all non-Tensor arguments.", "For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module\u2019s forward function.", "Global hooks are called before hooks registered with register_backward_pre_hook", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.torch.nn.modules.module.register_module_module_registration_hook", "path": "generated/torch.nn.modules.module.register_module_module_registration_hook", "type": "Neuro Network", "text": ["Registers a module registration hook common to all modules.", "Warning", "This adds global state to the nn.Module module", "The hook will be called every time register_module() is invoked. It should have the following signature:", "The hook can modify the input or return a single modified value in the hook.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.torch.nn.modules.module.register_module_parameter_registration_hook", "path": "generated/torch.nn.modules.module.register_module_parameter_registration_hook", "type": "Neuro Network", "text": ["Registers a parameter registration hook common to all modules.", "Warning", "This adds global state to the nn.Module module", "The hook will be called every time register_parameter() is invoked. It should have the following signature:", "The hook can modify the input or return a single modified value in the hook.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.torch.nn.utils.clip_grad_norm_", "path": "generated/torch.nn.utils.clip_grad_norm_", "type": "Neuro Network", "text": ["Clips gradient norm of an iterable of parameters.", "The norm is computed over all gradients together, as if they were concatenated into a single vector. Gradients are modified in-place.", "Total norm of the parameter gradients (viewed as a single vector).", "Tensor"]}, {"name": "torch.nn.torch.nn.utils.clip_grad_value_", "path": "generated/torch.nn.utils.clip_grad_value_", "type": "Neuro Network", "text": ["Clips gradient of an iterable of parameters at specified value.", "Gradients are modified in-place."]}, {"name": "torch.nn.torch.nn.utils.parameters_to_vector", "path": "generated/torch.nn.utils.parameters_to_vector", "type": "Neuro Network", "text": ["Convert parameters to one vector", "parameters (Iterable[Tensor]) \u2013 an iterator of Tensors that are the parameters of a model.", "The parameters represented by a single vector", "Tensor"]}, {"name": "torch.nn.torch.nn.utils.parametrizations.orthogonal", "path": "generated/torch.nn.utils.parametrizations.orthogonal", "type": "Neuro Network", "text": ["Applies an orthogonal or unitary parametrization to a matrix or a batch of matrices.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, the parametrized matrix Q\u2208Km\u00d7nQ \\in \\mathbb{K}^{m \\times n} is orthogonal as", "where QHQ^{\\text{H}} is the conjugate transpose when QQ is complex and the transpose when QQ is real-valued, and In\\mathrm{I}_n is the n-dimensional identity matrix. In plain words, QQ will have orthonormal columns whenever m\u2265nm \\geq n and orthonormal rows otherwise.", "If the tensor has more than two dimensions, we consider it as a batch of matrices of shape (\u2026, m, n).", "The matrix QQ may be parametrized via three different orthogonal_map in terms of the original tensor:", "\"matrix_exp\"/\"cayley\" often make the parametrized weight converge faster than \"householder\", but they are slower to compute for very thin or very wide matrices.", "If use_trivialization=True (default), the parametrization implements the \u201cDynamic Trivialization Framework\u201d, where an extra matrix B\u2208Kn\u00d7nB \\in \\mathbb{K}^{n \\times n} is stored under module.parametrizations.weight[0].base. This helps the convergence of the parametrized layer at the expense of some extra memory use. See Trivializations for Gradient-Based Optimization on Manifolds .", "Initial value of QQ: If the original tensor is not parametrized and use_trivialization=True (default), the initial value of QQ is that of the original tensor if it is orthogonal (or unitary in the complex case) and it is orthogonalized via the QR decomposition otherwise (see torch.linalg.qr()). Same happens when it is not parametrized and orthogonal_map=\"householder\" even when use_trivialization=False. Otherwise, the initial value is the result of the composition of all the registered parametrizations applied to the original tensor.", "Note", "This function is implemented using the parametrization functionality in register_parametrization().", "The original module with an orthogonal parametrization registered to the specified weight", "Module", "Example:"]}, {"name": "torch.nn.torch.nn.utils.parametrizations.spectral_norm", "path": "generated/torch.nn.utils.parametrizations.spectral_norm", "type": "Neuro Network", "text": ["Applies spectral normalization to a parameter in the given module.", "When applied on a vector, it simplifies to", "Spectral normalization stabilizes the training of discriminators (critics) in Generative Adversarial Networks (GANs) by reducing the Lipschitz constant of the model. \u03c3\\sigma is approximated performing one iteration of the power method every time the weight is accessed. If the dimension of the weight tensor is greater than 2, it is reshaped to 2D in power iteration method to get spectral norm.", "See Spectral Normalization for Generative Adversarial Networks .", "Note", "This function is implemented using the parametrization functionality in register_parametrization(). It is a reimplementation of torch.nn.utils.spectral_norm().", "Note", "When this constraint is registered, the singular vectors associated to the largest singular value are estimated rather than sampled at random. These are then updated performing n_power_iterations of the power method whenever the tensor is accessed with the module on training mode.", "Note", "If the _SpectralNorm module, i.e., module.parametrization.weight[idx], is in training mode on removal, it will perform another power iteration. If you\u2019d like to avoid this iteration, set the module to eval mode before its removal.", "The original module with a new parametrization registered to the specified weight", "Module", "Example:"]}, {"name": "torch.nn.torch.nn.utils.parametrize.cached", "path": "generated/torch.nn.utils.parametrize.cached", "type": "Neuro Network", "text": ["Context manager that enables the caching system within parametrizations registered with register_parametrization().", "The value of the parametrized objects is computed and cached the first time they are required when this context manager is active. The cached values are discarded when leaving the context manager.", "This is useful when using a parametrized parameter more than once in the forward pass. An example of this is when parametrizing the recurrent kernel of an RNN or when sharing weights.", "The simplest way to activate the cache is by wrapping the forward pass of the neural network", "in training and evaluation. One may also wrap the parts of the modules that use several times the parametrized tensors. For example, the loop of an RNN with a parametrized recurrent kernel:"]}, {"name": "torch.nn.torch.nn.utils.parametrize.is_parametrized", "path": "generated/torch.nn.utils.parametrize.is_parametrized", "type": "Neuro Network", "text": ["Returns True if module has an active parametrization.", "If the argument tensor_name is specified, returns True if module[tensor_name] is parametrized.", "bool"]}, {"name": "torch.nn.torch.nn.utils.parametrize.register_parametrization", "path": "generated/torch.nn.utils.parametrize.register_parametrization", "type": "Neuro Network", "text": ["Adds a parametrization to a tensor in a module.", "Assume that tensor_name=\"weight\" for simplicity. When accessing module.weight, the module will return the parametrized version parametrization(module.weight). If the original tensor requires a gradient, the backward pass will differentiate through parametrization, and the optimizer will update the tensor accordingly.", "The first time that a module registers a parametrization, this function will add an attribute parametrizations to the module of type ParametrizationList.", "The list of parametrizations on the tensor weight will be accessible under module.parametrizations.weight.", "The original tensor will be accessible under module.parametrizations.weight.original.", "Parametrizations may be concatenated by registering several parametrizations on the same attribute.", "The training mode of a registered parametrization is updated on registration to match the training mode of the host module", "Parametrized parameters and buffers have an inbuilt caching system that can be activated using the context manager cached().", "A parametrization may optionally implement a method with signature", "This method is called on the unparametrized tensor when the first parametrization is registered to compute the initial value of the original tensor. If this method is not implemented, the original tensor will be just the unparametrized tensor.", "If all the parametrizations registered on a tensor implement right_inverse it is possible to initialize a parametrized tensor by assigning to it, as shown in the example below.", "It is possible for the first parametrization to depend on several inputs. This may be implemented returning a tuple of tensors from right_inverse (see the example implementation of a RankOne parametrization below).", "In this case, the unconstrained tensors are also located under module.parametrizations.weight with names original0, original1,\u2026", "Note", "If unsafe=False (default) both the forward and right_inverse methods will be called once to perform a number of consistency checks. If unsafe=True, then right_inverse will be called if the tensor is not parametrized, and nothing will be called otherwise.", "Note", "In most situations, right_inverse will be a function such that forward(right_inverse(X)) == X (see right inverse). Sometimes, when the parametrization is not surjective, it may be reasonable to relax this.", "Warning", "If a parametrization depends on several inputs, register_parametrization() will register a number of new parameters. If such parametrization is registered after the optimizer is created, these new parameters will need to be added manually to the optimizer. See torch.Optimizer.add_param_group().", "unsafe (bool) \u2013 a boolean flag that denotes whether the parametrization may change the dtype and shape of the tensor. Default: False Warning: the parametrization is not checked for consistency upon registration. Enable this flag at your own risk.", "ValueError \u2013 if the module does not have a parameter or a buffer named tensor_name", "Module"]}, {"name": "torch.nn.torch.nn.utils.parametrize.remove_parametrizations", "path": "generated/torch.nn.utils.parametrize.remove_parametrizations", "type": "Neuro Network", "text": ["Removes the parametrizations on a tensor in a module.", "module", "Module"]}, {"name": "torch.nn.torch.nn.utils.prune.custom_from_mask", "path": "generated/torch.nn.utils.prune.custom_from_mask", "type": "Neuro Network", "text": ["Prunes tensor corresponding to parameter called name in module by applying the pre-computed mask in mask. Modifies module in place (and also return the modified module) by:", "modified (i.e. pruned) version of the input module", "module (nn.Module)"]}, {"name": "torch.nn.torch.nn.utils.prune.global_unstructured", "path": "generated/torch.nn.utils.prune.global_unstructured", "type": "Neuro Network", "text": ["Globally prunes tensors corresponding to all parameters in parameters by applying the specified pruning_method. Modifies modules in place by:", "TypeError \u2013 if PRUNING_TYPE != 'unstructured'", "Note", "Since global structured pruning doesn\u2019t make much sense unless the norm is normalized by the size of the parameter, we now limit the scope of global pruning to unstructured methods."]}, {"name": "torch.nn.torch.nn.utils.prune.is_pruned", "path": "generated/torch.nn.utils.prune.is_pruned", "type": "Neuro Network", "text": ["Check whether module is pruned by looking for forward_pre_hooks in its modules that inherit from the BasePruningMethod.", "module (nn.Module) \u2013 object that is either pruned or unpruned", "binary answer to whether module is pruned."]}, {"name": "torch.nn.torch.nn.utils.prune.l1_unstructured", "path": "generated/torch.nn.utils.prune.l1_unstructured", "type": "Neuro Network", "text": ["Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units with the lowest L1-norm. Modifies module in place (and also return the modified module) by:", "modified (i.e. pruned) version of the input module", "module (nn.Module)"]}, {"name": "torch.nn.torch.nn.utils.prune.ln_structured", "path": "generated/torch.nn.utils.prune.ln_structured", "type": "Neuro Network", "text": ["Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim with the lowest Ln-norm. Modifies module in place (and also return the modified module) by:", "modified (i.e. pruned) version of the input module", "module (nn.Module)"]}, {"name": "torch.nn.torch.nn.utils.prune.random_structured", "path": "generated/torch.nn.utils.prune.random_structured", "type": "Neuro Network", "text": ["Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim selected at random. Modifies module in place (and also return the modified module) by:", "modified (i.e. pruned) version of the input module", "module (nn.Module)"]}, {"name": "torch.nn.torch.nn.utils.prune.random_unstructured", "path": "generated/torch.nn.utils.prune.random_unstructured", "type": "Neuro Network", "text": ["Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units selected at random. Modifies module in place (and also return the modified module) by:", "modified (i.e. pruned) version of the input module", "module (nn.Module)"]}, {"name": "torch.nn.torch.nn.utils.prune.remove", "path": "generated/torch.nn.utils.prune.remove", "type": "Neuro Network", "text": ["Removes the pruning reparameterization from a module and the pruning method from the forward hook. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.torch.nn.utils.remove_spectral_norm", "path": "generated/torch.nn.utils.remove_spectral_norm", "type": "Neuro Network", "text": ["Removes the spectral normalization reparameterization from a module.", "T_module"]}, {"name": "torch.nn.torch.nn.utils.remove_weight_norm", "path": "generated/torch.nn.utils.remove_weight_norm", "type": "Neuro Network", "text": ["Removes the weight normalization reparameterization from a module.", "T_module"]}, {"name": "torch.nn.torch.nn.utils.rnn.pack_padded_sequence", "path": "generated/torch.nn.utils.rnn.pack_padded_sequence", "type": "Neuro Network", "text": ["Packs a Tensor containing padded sequences of variable length.", "input can be of size T x B x * where T is the length of the longest sequence (equal to lengths[0]), B is the batch size, and * is any number of dimensions (including 0). If batch_first is True, B x T x * input is expected.", "For unsorted sequences, use enforce_sorted = False. If enforce_sorted is True, the sequences should be sorted by length in a decreasing order, i.e. input[:,0] should be the longest sequence, and input[:,B-1] the shortest one. enforce_sorted = True is only necessary for ONNX export.", "Note", "This function accepts any input that has at least two dimensions. You can apply it to pack the labels, and use the output of the RNN with them to compute the loss directly. A Tensor can be retrieved from a PackedSequence object by accessing its .data attribute.", "a PackedSequence object", "PackedSequence"]}, {"name": "torch.nn.torch.nn.utils.rnn.pack_sequence", "path": "generated/torch.nn.utils.rnn.pack_sequence", "type": "Neuro Network", "text": ["Packs a list of variable length Tensors", "Consecutive call of the next functions: pad_sequence, pack_padded_sequence.", "sequences should be a list of Tensors of size L x *, where L is the length of a sequence and * is any number of trailing dimensions, including zero.", "For unsorted sequences, use enforce_sorted = False. If enforce_sorted is True, the sequences should be sorted in the order of decreasing length. enforce_sorted = True is only necessary for ONNX export.", "a PackedSequence object", "PackedSequence"]}, {"name": "torch.nn.torch.nn.utils.rnn.pad_packed_sequence", "path": "generated/torch.nn.utils.rnn.pad_packed_sequence", "type": "Neuro Network", "text": ["Pads a packed batch of variable length sequences.", "It is an inverse operation to pack_padded_sequence().", "The returned Tensor\u2019s data will be of size T x B x *, where T is the length of the longest sequence and B is the batch size. If batch_first is True, the data will be transposed into B x T x * format.", "Note", "total_length is useful to implement the pack sequence -> recurrent network -> unpack sequence pattern in a Module wrapped in DataParallel. See this FAQ section for details.", "Tuple of Tensor containing the padded sequence, and a Tensor containing the list of lengths of each sequence in the batch. Batch elements will be re-ordered as they were ordered originally when the batch was passed to pack_padded_sequence or pack_sequence.", "Tuple[Tensor, Tensor]"]}, {"name": "torch.nn.torch.nn.utils.rnn.pad_sequence", "path": "generated/torch.nn.utils.rnn.pad_sequence", "type": "Neuro Network", "text": ["Pad a list of variable length Tensors with padding_value", "pad_sequence stacks a list of Tensors along a new dimension, and pads them to equal length. For example, if the input is a list of sequences with size L x * and batch_first is False, the output is of size T x B x *.", "B is batch size. It is equal to the number of elements in sequences. T is length of the longest sequence. L is length of the sequence. * is any number of trailing dimensions, including none.", "Note", "This function returns a Tensor of size T x B x * or B x T x * where T is the length of the longest sequence. This function assumes trailing dimensions and type of all the Tensors in sequences are same.", "Tensor of size T x B x * if batch_first is False. Tensor of size B x T x * otherwise", "Tensor"]}, {"name": "torch.nn.torch.nn.utils.rnn.unpack_sequence", "path": "generated/torch.nn.utils.rnn.unpack_sequence", "type": "Neuro Network", "text": ["Unpacks PackedSequence into a list of variable length Tensors", "packed_sequences should be a PackedSequence object.", "packed_sequences (PackedSequence) \u2013 A PackedSequence object.", "a list of Tensor objects", "List[Tensor]"]}, {"name": "torch.nn.torch.nn.utils.rnn.unpad_sequence", "path": "generated/torch.nn.utils.rnn.unpad_sequence", "type": "Neuro Network", "text": ["Unpad padded Tensor into a list of variable length Tensors", "unpad_sequence unstacks padded Tensor into a list of variable length Tensors.", "a list of Tensor objects", "List[Tensor]"]}, {"name": "torch.nn.torch.nn.utils.skip_init", "path": "generated/torch.nn.utils.skip_init", "type": "Neuro Network", "text": ["Given a module class object and args / kwargs, instantiates the module without initializing parameters / buffers. This can be useful if initialization is slow or if custom initialization will be performed, making the default initialization unnecessary. There are some caveats to this, due to the way this function is implemented:", "1. The module must accept a device arg in its constructor that is passed to any parameters or buffers created during construction.", "2. The module must not perform any computation on parameters in its constructor except initialization (i.e. functions from torch.nn.init).", "If these conditions are satisfied, the module can be instantiated with parameter / buffer values uninitialized, as if having been created using torch.empty().", "Instantiated module with uninitialized parameters / buffers", "Example:"]}, {"name": "torch.nn.torch.nn.utils.spectral_norm", "path": "generated/torch.nn.utils.spectral_norm", "type": "Neuro Network", "text": ["Applies spectral normalization to a parameter in the given module.", "Spectral normalization stabilizes the training of discriminators (critics) in Generative Adversarial Networks (GANs) by rescaling the weight tensor with spectral norm \u03c3\\sigma of the weight matrix calculated using power iteration method. If the dimension of the weight tensor is greater than 2, it is reshaped to 2D in power iteration method to get spectral norm. This is implemented via a hook that calculates spectral norm and rescales weight before every forward() call.", "See Spectral Normalization for Generative Adversarial Networks .", "The original module with the spectral norm hook", "T_module", "Note", "This function has been reimplemented as torch.nn.utils.parametrizations.spectral_norm() using the new parametrization functionality in torch.nn.utils.parametrize.register_parametrization(). Please use the newer version. This function will be deprecated in a future version of PyTorch.", "Example:"]}, {"name": "torch.nn.torch.nn.utils.stateless.functional_call", "path": "generated/torch.nn.utils.stateless.functional_call", "type": "Neuro Network", "text": ["Performs a functional call on the module by replacing the module parameters and buffers with the provided ones.", "Warning", "This API is deprecated as of PyTorch 2.0 and will be removed in a future version of PyTorch. Please use torch.func.functional_call() instead, which is a drop-in replacement for this API.", "Note", "If the module has active parametrizations, passing a value in the parameters_and_buffers argument with the name set to the regular parameter name will completely disable the parametrization. If you want to apply the parametrization function to the value passed please set the key as {submodule_name}.parametrizations.{parameter_name}.original.", "Note", "If the module performs in-place operations on parameters/buffers, these will be reflected in the parameters_and_buffers input.", "Example:", "Note", "If the module has tied weights, whether or not functional_call respects the tying is determined by the tie_weights flag.", "Example:", "the result of calling module.", "Any"]}, {"name": "torch.nn.torch.nn.utils.vector_to_parameters", "path": "generated/torch.nn.utils.vector_to_parameters", "type": "Neuro Network", "text": ["Convert one vector to the parameters"]}, {"name": "torch.nn.torch.nn.utils.weight_norm", "path": "generated/torch.nn.utils.weight_norm", "type": "Neuro Network", "text": ["Applies weight normalization to a parameter in the given module.", "Weight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. This replaces the parameter specified by name (e.g. 'weight') with two parameters: one specifying the magnitude (e.g. 'weight_g') and one specifying the direction (e.g. 'weight_v'). Weight normalization is implemented via a hook that recomputes the weight tensor from the magnitude and direction before every forward() call.", "By default, with dim=0, the norm is computed independently per output channel/plane. To compute a norm over the entire weight tensor, use dim=None.", "See https://arxiv.org/abs/1602.07868", "Warning", "This function is deprecated. Use torch.nn.utils.parametrizations.weight_norm() which uses the modern parametrization API. The new weight_norm is compatible with state_dict generated from old weight_norm.", "Migration guide:", "The original module with the weight norm hook", "T_module", "Example:"]}, {"name": "torch.nn.Transformer", "path": "generated/torch.nn.transformer", "type": "Neuro Network", "text": ["A transformer model. User is able to modify the attributes as needed. The architecture is based on the paper \u201cAttention Is All You Need\u201d. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010.", "Note: A full example to apply nn.Transformer module for the word language model is available in https://github.com/pytorch/examples/tree/master/word_language_model", "Take in and process masked source/target sequences.", "Tensor", "Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked positions. If a BoolTensor is provided, positions with True are not allowed to attend while False values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight. [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by the attention. If a BoolTensor is provided, the positions with the value of True will be ignored while the position with the value of False will be unchanged.", "Note: Due to the multi-head attention architecture in the transformer model, the output sequence length of a transformer is same as the input sequence (i.e. target) length of the decoder.", "where S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number", "Generate a square causal mask for the sequence. The masked positions are filled with float(\u2018-inf\u2019). Unmasked positions are filled with float(0.0).", "Tensor"]}, {"name": "torch.nn.Transformer.forward()", "path": "generated/torch.nn.transformer#torch.nn.Transformer.forward", "type": "Neuro Network", "text": ["Take in and process masked source/target sequences.", "Tensor", "Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked positions. If a BoolTensor is provided, positions with True are not allowed to attend while False values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight. [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by the attention. If a BoolTensor is provided, the positions with the value of True will be ignored while the position with the value of False will be unchanged.", "Note: Due to the multi-head attention architecture in the transformer model, the output sequence length of a transformer is same as the input sequence (i.e. target) length of the decoder.", "where S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number"]}, {"name": "torch.nn.Transformer.generate_square_subsequent_mask()", "path": "generated/torch.nn.transformer#torch.nn.Transformer.generate_square_subsequent_mask", "type": "Neuro Network", "text": ["Generate a square causal mask for the sequence. The masked positions are filled with float(\u2018-inf\u2019). Unmasked positions are filled with float(0.0).", "Tensor"]}, {"name": "torch.nn.TransformerDecoder", "path": "generated/torch.nn.transformerdecoder", "type": "Neuro Network", "text": ["TransformerDecoder is a stack of N decoder layers", "Pass the inputs (and mask) through the decoder layer in turn.", "Tensor", "see the docs in Transformer class."]}, {"name": "torch.nn.TransformerDecoder.forward()", "path": "generated/torch.nn.transformerdecoder#torch.nn.TransformerDecoder.forward", "type": "Neuro Network", "text": ["Pass the inputs (and mask) through the decoder layer in turn.", "Tensor", "see the docs in Transformer class."]}, {"name": "torch.nn.TransformerDecoderLayer", "path": "generated/torch.nn.transformerdecoderlayer", "type": "Neuro Network", "text": ["TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network. This standard decoder layer is based on the paper \u201cAttention Is All You Need\u201d. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application.", "Pass the inputs (and mask) through the decoder layer.", "Tensor", "see the docs in Transformer class."]}, {"name": "torch.nn.TransformerDecoderLayer.forward()", "path": "generated/torch.nn.transformerdecoderlayer#torch.nn.TransformerDecoderLayer.forward", "type": "Neuro Network", "text": ["Pass the inputs (and mask) through the decoder layer.", "Tensor", "see the docs in Transformer class."]}, {"name": "torch.nn.TransformerEncoder", "path": "generated/torch.nn.transformerencoder", "type": "Neuro Network", "text": ["TransformerEncoder is a stack of N encoder layers. Users can build the BERT(https://arxiv.org/abs/1810.04805) model with corresponding parameters.", "Pass the input through the encoder layers in turn.", "Tensor", "see the docs in Transformer class."]}, {"name": "torch.nn.TransformerEncoder.forward()", "path": "generated/torch.nn.transformerencoder#torch.nn.TransformerEncoder.forward", "type": "Neuro Network", "text": ["Pass the input through the encoder layers in turn.", "Tensor", "see the docs in Transformer class."]}, {"name": "torch.nn.TransformerEncoderLayer", "path": "generated/torch.nn.transformerencoderlayer", "type": "Neuro Network", "text": ["TransformerEncoderLayer is made up of self-attn and feedforward network. This standard encoder layer is based on the paper \u201cAttention Is All You Need\u201d. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application.", "TransformerEncoderLayer can handle either traditional torch.tensor inputs, or Nested Tensor inputs. Derived classes are expected to similarly accept both input formats. (Not all combinations of inputs are currently supported by TransformerEncoderLayer while Nested Tensor is in prototype state.)", "If you are implementing a custom layer, you may derive it either from the Module or TransformerEncoderLayer class. If your custom layer supports both torch.Tensors and Nested Tensors inputs, make its implementation a derived class of TransformerEncoderLayer. If your custom Layer supports only torch.Tensor inputs, derive its implementation from Module.", "forward() will use a special optimized implementation described in FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness if all of the following conditions are met:", "If the optimized implementation is in use, a NestedTensor can be passed for src to represent padding more efficiently than using a padding mask. In this case, a NestedTensor will be returned, and an additional speedup proportional to the fraction of the input that is padding can be expected.", "Pass the input through the encoder layer.", "Tensor", "see the docs in Transformer class."]}, {"name": "torch.nn.TransformerEncoderLayer.forward()", "path": "generated/torch.nn.transformerencoderlayer#torch.nn.TransformerEncoderLayer.forward", "type": "Neuro Network", "text": ["Pass the input through the encoder layer.", "Tensor", "see the docs in Transformer class."]}, {"name": "torch.nn.TripletMarginLoss", "path": "generated/torch.nn.tripletmarginloss", "type": "Neuro Network", "text": ["Creates a criterion that measures the triplet loss given an input tensors x1x1, x2x2, x3x3 and a margin with a value greater than 00. This is used for measuring a relative similarity between samples. A triplet is composed by a, p and n (i.e., anchor, positive examples and negative examples respectively). The shapes of all input tensors should be (N,D)(N, D).", "The distance swap is described in detail in the paper Learning shallow convolutional feature descriptors with triplet losses by V. Balntas, E. Riba et al.", "The loss function for each sample in the mini-batch is:", "where", "The norm is calculated using the specified p value and a small constant \u03b5\\varepsilon is added for numerical stability.", "See also TripletMarginWithDistanceLoss, which computes the triplet margin loss for input tensors using a custom distance function.", "Examples:"]}, {"name": "torch.nn.TripletMarginWithDistanceLoss", "path": "generated/torch.nn.tripletmarginwithdistanceloss", "type": "Neuro Network", "text": ["Creates a criterion that measures the triplet loss given input tensors aa, pp, and nn (representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\u201cdistance function\u201d) used to compute the relationship between the anchor and positive example (\u201cpositive distance\u201d) and the anchor and negative example (\u201cnegative distance\u201d).", "The unreduced loss (i.e., with reduction set to 'none') can be described as:", "where NN is the batch size; dd is a nonnegative, real-valued function quantifying the closeness of two tensors, referred to as the distance_function; and marginmargin is a nonnegative margin representing the minimum difference between the positive and negative distances that is required for the loss to be 0. The input tensors have NN elements each and can be of any shape that the distance function can handle.", "If reduction is not 'none' (default 'mean'), then:", "See also TripletMarginLoss, which computes the triplet loss for input tensors using the lpl_p distance as the distance function.", "Examples:", "V. Balntas, et al.: Learning shallow convolutional feature descriptors with triplet losses: http://www.bmva.org/bmvc/2016/papers/paper119/index.html"]}, {"name": "torch.nn.Unflatten", "path": "generated/torch.nn.unflatten", "type": "Neuro Network", "text": ["Unflattens a tensor dim expanding it to a desired shape. For use with Sequential."]}, {"name": "torch.nn.Unfold", "path": "generated/torch.nn.unfold", "type": "Neuro Network", "text": ["Extracts sliding local blocks from a batched input tensor.", "Consider a batched input tensor of shape (N,C,\u2217)(N, C, *), where NN is the batch dimension, CC is the channel dimension, and \u2217* represent arbitrary spatial dimensions. This operation flattens each sliding kernel_size-sized block within the spatial dimensions of input into a column (i.e., last dimension) of a 3-D output tensor of shape (N,C\u00d7\u220f(kernel_size),L)(N, C \\times \\prod(\\text{kernel\\_size}), L), where C\u00d7\u220f(kernel_size)C \\times \\prod(\\text{kernel\\_size}) is the total number of values within each block (a block has \u220f(kernel_size)\\prod(\\text{kernel\\_size}) spatial locations each containing a CC-channeled vector), and LL is the total number of such blocks:", "where spatial_size\\text{spatial\\_size} is formed by the spatial dimensions of input (\u2217* above), and dd is over all spatial dimensions.", "Therefore, indexing output at the last dimension (column dimension) gives all values within a certain block.", "The padding, stride and dilation arguments specify how the sliding blocks are retrieved.", "Note", "Fold calculates each combined value in the resulting large tensor by summing all values from all containing blocks. Unfold extracts the values in the local blocks by copying from the large tensor. So, if the blocks overlap, they are not inverses of each other.", "In general, folding and unfolding operations are related as follows. Consider Fold and Unfold instances created with the same parameters:", "Then for any (supported) input tensor the following equality holds:", "where divisor is a tensor that depends only on the shape and dtype of the input:", "When the divisor tensor contains no zero elements, then fold and unfold operations are inverses of each other (up to constant divisor).", "Warning", "Currently, only 4-D input tensors (batched image-like tensors) are supported.", "Examples:"]}, {"name": "torch.nn.UninitializedBuffer", "path": "generated/torch.nn.parameter.uninitializedbuffer", "type": "Neuro Network", "text": ["A buffer that is not initialized.", "Uninitialized Buffer is a a special case of torch.Tensor where the shape of the data is still unknown.", "Unlike a torch.Tensor, uninitialized parameters hold no data and attempting to access some properties, like their shape, will throw a runtime error. The only operations that can be performed on a uninitialized parameter are changing its datatype, moving it to a different device and converting it to a regular torch.Tensor.", "The default device or dtype to use when the buffer is materialized can be set during construction using e.g. device='cuda'."]}, {"name": "torch.nn.UninitializedParameter", "path": "generated/torch.nn.parameter.uninitializedparameter", "type": "Neuro Network", "text": ["A parameter that is not initialized.", "Uninitialized Parameters are a a special case of torch.nn.Parameter where the shape of the data is still unknown.", "Unlike a torch.nn.Parameter, uninitialized parameters hold no data and attempting to access some properties, like their shape, will throw a runtime error. The only operations that can be performed on a uninitialized parameter are changing its datatype, moving it to a different device and converting it to a regular torch.nn.Parameter.", "The default device or dtype to use when the parameter is materialized can be set during construction using e.g. device='cuda'.", "alias of Parameter"]}, {"name": "torch.nn.Upsample", "path": "generated/torch.nn.upsample", "type": "Neuro Network", "text": ["Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.", "The input data is assumed to be of the form minibatch x channels x [optional depth] x [optional height] x width. Hence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor.", "The algorithms available for upsampling are nearest neighbor and linear, bilinear, bicubic and trilinear for 3D, 4D and 5D input Tensor, respectively.", "One can either give a scale_factor or the target output size to calculate the output size. (You cannot give both, as it is ambiguous)", "Warning", "With align_corners = True, the linearly interpolating modes (linear, bilinear, bicubic, and trilinear) don\u2019t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See below for concrete examples on how this affects the outputs.", "Note", "If you want downsampling/general resizing, you should use interpolate().", "Examples:"]}, {"name": "torch.nn.UpsamplingBilinear2d", "path": "generated/torch.nn.upsamplingbilinear2d", "type": "Neuro Network", "text": ["Applies a 2D bilinear upsampling to an input signal composed of several input channels.", "To specify the scale, it takes either the size or the scale_factor as it\u2019s constructor argument.", "When size is given, it is the output size of the image (h, w).", "Warning", "This class is deprecated in favor of interpolate(). It is equivalent to nn.functional.interpolate(..., mode='bilinear', align_corners=True).", "Examples:"]}, {"name": "torch.nn.UpsamplingNearest2d", "path": "generated/torch.nn.upsamplingnearest2d", "type": "Neuro Network", "text": ["Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels.", "To specify the scale, it takes either the size or the scale_factor as it\u2019s constructor argument.", "When size is given, it is the output size of the image (h, w).", "Warning", "This class is deprecated in favor of interpolate().", "Examples:"]}, {"name": "torch.nn.utils.clip_grad_norm_()", "path": "generated/torch.nn.utils.clip_grad_norm_#torch.nn.utils.clip_grad_norm_", "type": "Neuro Network", "text": ["Clips gradient norm of an iterable of parameters.", "The norm is computed over all gradients together, as if they were concatenated into a single vector. Gradients are modified in-place.", "Total norm of the parameter gradients (viewed as a single vector).", "Tensor"]}, {"name": "torch.nn.utils.clip_grad_value_()", "path": "generated/torch.nn.utils.clip_grad_value_#torch.nn.utils.clip_grad_value_", "type": "Neuro Network", "text": ["Clips gradient of an iterable of parameters at specified value.", "Gradients are modified in-place."]}, {"name": "torch.nn.utils.parameters_to_vector()", "path": "generated/torch.nn.utils.parameters_to_vector#torch.nn.utils.parameters_to_vector", "type": "Neuro Network", "text": ["Convert parameters to one vector", "parameters (Iterable[Tensor]) \u2013 an iterator of Tensors that are the parameters of a model.", "The parameters represented by a single vector", "Tensor"]}, {"name": "torch.nn.utils.parametrizations.orthogonal()", "path": "generated/torch.nn.utils.parametrizations.orthogonal#torch.nn.utils.parametrizations.orthogonal", "type": "Neuro Network", "text": ["Applies an orthogonal or unitary parametrization to a matrix or a batch of matrices.", "Letting K\\mathbb{K} be R\\mathbb{R} or C\\mathbb{C}, the parametrized matrix Q\u2208Km\u00d7nQ \\in \\mathbb{K}^{m \\times n} is orthogonal as", "where QHQ^{\\text{H}} is the conjugate transpose when QQ is complex and the transpose when QQ is real-valued, and In\\mathrm{I}_n is the n-dimensional identity matrix. In plain words, QQ will have orthonormal columns whenever m\u2265nm \\geq n and orthonormal rows otherwise.", "If the tensor has more than two dimensions, we consider it as a batch of matrices of shape (\u2026, m, n).", "The matrix QQ may be parametrized via three different orthogonal_map in terms of the original tensor:", "\"matrix_exp\"/\"cayley\" often make the parametrized weight converge faster than \"householder\", but they are slower to compute for very thin or very wide matrices.", "If use_trivialization=True (default), the parametrization implements the \u201cDynamic Trivialization Framework\u201d, where an extra matrix B\u2208Kn\u00d7nB \\in \\mathbb{K}^{n \\times n} is stored under module.parametrizations.weight[0].base. This helps the convergence of the parametrized layer at the expense of some extra memory use. See Trivializations for Gradient-Based Optimization on Manifolds .", "Initial value of QQ: If the original tensor is not parametrized and use_trivialization=True (default), the initial value of QQ is that of the original tensor if it is orthogonal (or unitary in the complex case) and it is orthogonalized via the QR decomposition otherwise (see torch.linalg.qr()). Same happens when it is not parametrized and orthogonal_map=\"householder\" even when use_trivialization=False. Otherwise, the initial value is the result of the composition of all the registered parametrizations applied to the original tensor.", "Note", "This function is implemented using the parametrization functionality in register_parametrization().", "The original module with an orthogonal parametrization registered to the specified weight", "Module", "Example:"]}, {"name": "torch.nn.utils.parametrizations.spectral_norm()", "path": "generated/torch.nn.utils.parametrizations.spectral_norm#torch.nn.utils.parametrizations.spectral_norm", "type": "Neuro Network", "text": ["Applies spectral normalization to a parameter in the given module.", "When applied on a vector, it simplifies to", "Spectral normalization stabilizes the training of discriminators (critics) in Generative Adversarial Networks (GANs) by reducing the Lipschitz constant of the model. \u03c3\\sigma is approximated performing one iteration of the power method every time the weight is accessed. If the dimension of the weight tensor is greater than 2, it is reshaped to 2D in power iteration method to get spectral norm.", "See Spectral Normalization for Generative Adversarial Networks .", "Note", "This function is implemented using the parametrization functionality in register_parametrization(). It is a reimplementation of torch.nn.utils.spectral_norm().", "Note", "When this constraint is registered, the singular vectors associated to the largest singular value are estimated rather than sampled at random. These are then updated performing n_power_iterations of the power method whenever the tensor is accessed with the module on training mode.", "Note", "If the _SpectralNorm module, i.e., module.parametrization.weight[idx], is in training mode on removal, it will perform another power iteration. If you\u2019d like to avoid this iteration, set the module to eval mode before its removal.", "The original module with a new parametrization registered to the specified weight", "Module", "Example:"]}, {"name": "torch.nn.utils.parametrize.cached()", "path": "generated/torch.nn.utils.parametrize.cached#torch.nn.utils.parametrize.cached", "type": "Neuro Network", "text": ["Context manager that enables the caching system within parametrizations registered with register_parametrization().", "The value of the parametrized objects is computed and cached the first time they are required when this context manager is active. The cached values are discarded when leaving the context manager.", "This is useful when using a parametrized parameter more than once in the forward pass. An example of this is when parametrizing the recurrent kernel of an RNN or when sharing weights.", "The simplest way to activate the cache is by wrapping the forward pass of the neural network", "in training and evaluation. One may also wrap the parts of the modules that use several times the parametrized tensors. For example, the loop of an RNN with a parametrized recurrent kernel:"]}, {"name": "torch.nn.utils.parametrize.is_parametrized()", "path": "generated/torch.nn.utils.parametrize.is_parametrized#torch.nn.utils.parametrize.is_parametrized", "type": "Neuro Network", "text": ["Returns True if module has an active parametrization.", "If the argument tensor_name is specified, returns True if module[tensor_name] is parametrized.", "bool"]}, {"name": "torch.nn.utils.parametrize.ParametrizationList", "path": "generated/torch.nn.utils.parametrize.parametrizationlist#torch.nn.utils.parametrize.ParametrizationList", "type": "Neuro Network", "text": ["A sequential container that holds and manages the original or original0, original1, \u2026 parameters or buffers of a parametrized torch.nn.Module.", "It is the type of module.parametrizations[tensor_name] when module[tensor_name] has been parametrized with register_parametrization().", "If the first registered parametrization has a right_inverse that returns one tensor or does not have a right_inverse (in which case we assume that right_inverse is the identity), it will hold the tensor under the name original. If it has a right_inverse that returns more than one tensor, these will be registered as original0, original1, \u2026", "Warning", "This class is used internally by register_parametrization(). It is documented here for completeness. It shall not be instantiated by the user.", "Calls the methods right_inverse (see register_parametrization()) of the parametrizations in the inverse order they were registered in. Then, it stores the result in self.original if right_inverse outputs one tensor or in self.original0, self.original1, \u2026 if it outputs several.", "value (Tensor) \u2013 Value to which initialize the module"]}, {"name": "torch.nn.utils.parametrize.ParametrizationList.right_inverse()", "path": "generated/torch.nn.utils.parametrize.parametrizationlist#torch.nn.utils.parametrize.ParametrizationList.right_inverse", "type": "Neuro Network", "text": ["Calls the methods right_inverse (see register_parametrization()) of the parametrizations in the inverse order they were registered in. Then, it stores the result in self.original if right_inverse outputs one tensor or in self.original0, self.original1, \u2026 if it outputs several.", "value (Tensor) \u2013 Value to which initialize the module"]}, {"name": "torch.nn.utils.parametrize.register_parametrization()", "path": "generated/torch.nn.utils.parametrize.register_parametrization#torch.nn.utils.parametrize.register_parametrization", "type": "Neuro Network", "text": ["Adds a parametrization to a tensor in a module.", "Assume that tensor_name=\"weight\" for simplicity. When accessing module.weight, the module will return the parametrized version parametrization(module.weight). If the original tensor requires a gradient, the backward pass will differentiate through parametrization, and the optimizer will update the tensor accordingly.", "The first time that a module registers a parametrization, this function will add an attribute parametrizations to the module of type ParametrizationList.", "The list of parametrizations on the tensor weight will be accessible under module.parametrizations.weight.", "The original tensor will be accessible under module.parametrizations.weight.original.", "Parametrizations may be concatenated by registering several parametrizations on the same attribute.", "The training mode of a registered parametrization is updated on registration to match the training mode of the host module", "Parametrized parameters and buffers have an inbuilt caching system that can be activated using the context manager cached().", "A parametrization may optionally implement a method with signature", "This method is called on the unparametrized tensor when the first parametrization is registered to compute the initial value of the original tensor. If this method is not implemented, the original tensor will be just the unparametrized tensor.", "If all the parametrizations registered on a tensor implement right_inverse it is possible to initialize a parametrized tensor by assigning to it, as shown in the example below.", "It is possible for the first parametrization to depend on several inputs. This may be implemented returning a tuple of tensors from right_inverse (see the example implementation of a RankOne parametrization below).", "In this case, the unconstrained tensors are also located under module.parametrizations.weight with names original0, original1,\u2026", "Note", "If unsafe=False (default) both the forward and right_inverse methods will be called once to perform a number of consistency checks. If unsafe=True, then right_inverse will be called if the tensor is not parametrized, and nothing will be called otherwise.", "Note", "In most situations, right_inverse will be a function such that forward(right_inverse(X)) == X (see right inverse). Sometimes, when the parametrization is not surjective, it may be reasonable to relax this.", "Warning", "If a parametrization depends on several inputs, register_parametrization() will register a number of new parameters. If such parametrization is registered after the optimizer is created, these new parameters will need to be added manually to the optimizer. See torch.Optimizer.add_param_group().", "unsafe (bool) \u2013 a boolean flag that denotes whether the parametrization may change the dtype and shape of the tensor. Default: False Warning: the parametrization is not checked for consistency upon registration. Enable this flag at your own risk.", "ValueError \u2013 if the module does not have a parameter or a buffer named tensor_name", "Module"]}, {"name": "torch.nn.utils.parametrize.remove_parametrizations()", "path": "generated/torch.nn.utils.parametrize.remove_parametrizations#torch.nn.utils.parametrize.remove_parametrizations", "type": "Neuro Network", "text": ["Removes the parametrizations on a tensor in a module.", "module", "Module"]}, {"name": "torch.nn.utils.prune.BasePruningMethod", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod", "type": "Neuro Network", "text": ["Abstract base class for creation of new pruning techniques.", "Provides a skeleton for customization requiring the overriding of methods such as compute_mask() and apply().", "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the default_mask according to the specific pruning method recipe.", "mask to apply to t, of same dims as t", "mask (torch.Tensor)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.BasePruningMethod.apply()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.apply", "type": "Neuro Network", "text": ["Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask."]}, {"name": "torch.nn.utils.prune.BasePruningMethod.apply_mask()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.apply_mask", "type": "Neuro Network", "text": ["Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"]}, {"name": "torch.nn.utils.prune.BasePruningMethod.compute_mask()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.compute_mask", "type": "Neuro Network", "text": ["Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the default_mask according to the specific pruning method recipe.", "mask to apply to t, of same dims as t", "mask (torch.Tensor)"]}, {"name": "torch.nn.utils.prune.BasePruningMethod.prune()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.prune", "type": "Neuro Network", "text": ["Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t."]}, {"name": "torch.nn.utils.prune.BasePruningMethod.remove()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.remove", "type": "Neuro Network", "text": ["Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.custom_from_mask()", "path": "generated/torch.nn.utils.prune.custom_from_mask#torch.nn.utils.prune.custom_from_mask", "type": "Neuro Network", "text": ["Prunes tensor corresponding to parameter called name in module by applying the pre-computed mask in mask. Modifies module in place (and also return the modified module) by:", "modified (i.e. pruned) version of the input module", "module (nn.Module)"]}, {"name": "torch.nn.utils.prune.CustomFromMask", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask", "type": "Neuro Network", "text": ["Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.CustomFromMask.apply()", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask.apply", "type": "Neuro Network", "text": ["Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask."]}, {"name": "torch.nn.utils.prune.CustomFromMask.apply_mask()", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask.apply_mask", "type": "Neuro Network", "text": ["Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"]}, {"name": "torch.nn.utils.prune.CustomFromMask.prune()", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask.prune", "type": "Neuro Network", "text": ["Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t."]}, {"name": "torch.nn.utils.prune.CustomFromMask.remove()", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask.remove", "type": "Neuro Network", "text": ["Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.global_unstructured()", "path": "generated/torch.nn.utils.prune.global_unstructured#torch.nn.utils.prune.global_unstructured", "type": "Neuro Network", "text": ["Globally prunes tensors corresponding to all parameters in parameters by applying the specified pruning_method. Modifies modules in place by:", "TypeError \u2013 if PRUNING_TYPE != 'unstructured'", "Note", "Since global structured pruning doesn\u2019t make much sense unless the norm is normalized by the size of the parameter, we now limit the scope of global pruning to unstructured methods."]}, {"name": "torch.nn.utils.prune.Identity", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity", "type": "Neuro Network", "text": ["Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.", "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.Identity.apply()", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity.apply", "type": "Neuro Network", "text": ["Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask."]}, {"name": "torch.nn.utils.prune.Identity.apply_mask()", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity.apply_mask", "type": "Neuro Network", "text": ["Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"]}, {"name": "torch.nn.utils.prune.Identity.prune()", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity.prune", "type": "Neuro Network", "text": ["Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t."]}, {"name": "torch.nn.utils.prune.Identity.remove()", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity.remove", "type": "Neuro Network", "text": ["Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.is_pruned()", "path": "generated/torch.nn.utils.prune.is_pruned#torch.nn.utils.prune.is_pruned", "type": "Neuro Network", "text": ["Check whether module is pruned by looking for forward_pre_hooks in its modules that inherit from the BasePruningMethod.", "module (nn.Module) \u2013 object that is either pruned or unpruned", "binary answer to whether module is pruned."]}, {"name": "torch.nn.utils.prune.l1_unstructured()", "path": "generated/torch.nn.utils.prune.l1_unstructured#torch.nn.utils.prune.l1_unstructured", "type": "Neuro Network", "text": ["Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units with the lowest L1-norm. Modifies module in place (and also return the modified module) by:", "modified (i.e. pruned) version of the input module", "module (nn.Module)"]}, {"name": "torch.nn.utils.prune.L1Unstructured", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured", "type": "Neuro Network", "text": ["Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.", "amount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.", "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.L1Unstructured.apply()", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured.apply", "type": "Neuro Network", "text": ["Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask."]}, {"name": "torch.nn.utils.prune.L1Unstructured.apply_mask()", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured.apply_mask", "type": "Neuro Network", "text": ["Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"]}, {"name": "torch.nn.utils.prune.L1Unstructured.prune()", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured.prune", "type": "Neuro Network", "text": ["Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t."]}, {"name": "torch.nn.utils.prune.L1Unstructured.remove()", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured.remove", "type": "Neuro Network", "text": ["Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.ln_structured()", "path": "generated/torch.nn.utils.prune.ln_structured#torch.nn.utils.prune.ln_structured", "type": "Neuro Network", "text": ["Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim with the lowest Ln-norm. Modifies module in place (and also return the modified module) by:", "modified (i.e. pruned) version of the input module", "module (nn.Module)"]}, {"name": "torch.nn.utils.prune.LnStructured", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured", "type": "Neuro Network", "text": ["Prune entire (currently unpruned) channels in a tensor based on their Ln-norm.", "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a mask to apply on top of the default_mask by zeroing out the channels along the specified dim with the lowest Ln-norm.", "mask to apply to t, of same dims as t", "mask (torch.Tensor)", "IndexError \u2013 if self.dim >= len(t.shape)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.LnStructured.apply()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.apply", "type": "Neuro Network", "text": ["Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask."]}, {"name": "torch.nn.utils.prune.LnStructured.apply_mask()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.apply_mask", "type": "Neuro Network", "text": ["Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"]}, {"name": "torch.nn.utils.prune.LnStructured.compute_mask()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.compute_mask", "type": "Neuro Network", "text": ["Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a mask to apply on top of the default_mask by zeroing out the channels along the specified dim with the lowest Ln-norm.", "mask to apply to t, of same dims as t", "mask (torch.Tensor)", "IndexError \u2013 if self.dim >= len(t.shape)"]}, {"name": "torch.nn.utils.prune.LnStructured.prune()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.prune", "type": "Neuro Network", "text": ["Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t."]}, {"name": "torch.nn.utils.prune.LnStructured.remove()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.remove", "type": "Neuro Network", "text": ["Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.PruningContainer", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer", "type": "Neuro Network", "text": ["Container holding a sequence of pruning methods for iterative pruning. Keeps track of the order in which pruning methods are applied and handles combining successive pruning calls.", "Accepts as argument an instance of a BasePruningMethod or an iterable of them.", "Adds a child pruning method to the container.", "method (subclass of BasePruningMethod) \u2013 child pruning method to be added to the container.", "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Applies the latest method by computing the new partial masks and returning its combination with the default_mask. The new partial mask should be computed on the entries or channels that were not zeroed out by the default_mask. Which portions of the tensor t the new mask will be calculated from depends on the PRUNING_TYPE (handled by the type handler):", "new mask that combines the effects of the default_mask and the new mask from the current pruning method (of same dimensions as default_mask and t).", "mask (torch.Tensor)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.PruningContainer.add_pruning_method()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.add_pruning_method", "type": "Neuro Network", "text": ["Adds a child pruning method to the container.", "method (subclass of BasePruningMethod) \u2013 child pruning method to be added to the container."]}, {"name": "torch.nn.utils.prune.PruningContainer.apply()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.apply", "type": "Neuro Network", "text": ["Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask."]}, {"name": "torch.nn.utils.prune.PruningContainer.apply_mask()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.apply_mask", "type": "Neuro Network", "text": ["Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"]}, {"name": "torch.nn.utils.prune.PruningContainer.compute_mask()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.compute_mask", "type": "Neuro Network", "text": ["Applies the latest method by computing the new partial masks and returning its combination with the default_mask. The new partial mask should be computed on the entries or channels that were not zeroed out by the default_mask. Which portions of the tensor t the new mask will be calculated from depends on the PRUNING_TYPE (handled by the type handler):", "new mask that combines the effects of the default_mask and the new mask from the current pruning method (of same dimensions as default_mask and t).", "mask (torch.Tensor)"]}, {"name": "torch.nn.utils.prune.PruningContainer.prune()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.prune", "type": "Neuro Network", "text": ["Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t."]}, {"name": "torch.nn.utils.prune.PruningContainer.remove()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.remove", "type": "Neuro Network", "text": ["Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.random_structured()", "path": "generated/torch.nn.utils.prune.random_structured#torch.nn.utils.prune.random_structured", "type": "Neuro Network", "text": ["Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim selected at random. Modifies module in place (and also return the modified module) by:", "modified (i.e. pruned) version of the input module", "module (nn.Module)"]}, {"name": "torch.nn.utils.prune.random_unstructured()", "path": "generated/torch.nn.utils.prune.random_unstructured#torch.nn.utils.prune.random_unstructured", "type": "Neuro Network", "text": ["Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units selected at random. Modifies module in place (and also return the modified module) by:", "modified (i.e. pruned) version of the input module", "module (nn.Module)"]}, {"name": "torch.nn.utils.prune.RandomStructured", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured", "type": "Neuro Network", "text": ["Prune entire (currently unpruned) channels in a tensor at random.", "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the default_mask by randomly zeroing out channels along the specified dim of the tensor.", "mask to apply to t, of same dims as t", "mask (torch.Tensor)", "IndexError \u2013 if self.dim >= len(t.shape)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.RandomStructured.apply()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.apply", "type": "Neuro Network", "text": ["Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask."]}, {"name": "torch.nn.utils.prune.RandomStructured.apply_mask()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.apply_mask", "type": "Neuro Network", "text": ["Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"]}, {"name": "torch.nn.utils.prune.RandomStructured.compute_mask()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.compute_mask", "type": "Neuro Network", "text": ["Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the default_mask by randomly zeroing out channels along the specified dim of the tensor.", "mask to apply to t, of same dims as t", "mask (torch.Tensor)", "IndexError \u2013 if self.dim >= len(t.shape)"]}, {"name": "torch.nn.utils.prune.RandomStructured.prune()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.prune", "type": "Neuro Network", "text": ["Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t."]}, {"name": "torch.nn.utils.prune.RandomStructured.remove()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.remove", "type": "Neuro Network", "text": ["Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.RandomUnstructured", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured", "type": "Neuro Network", "text": ["Prune (currently unpruned) units in a tensor at random.", "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.RandomUnstructured.apply()", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured.apply", "type": "Neuro Network", "text": ["Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask."]}, {"name": "torch.nn.utils.prune.RandomUnstructured.apply_mask()", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured.apply_mask", "type": "Neuro Network", "text": ["Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"]}, {"name": "torch.nn.utils.prune.RandomUnstructured.prune()", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured.prune", "type": "Neuro Network", "text": ["Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t."]}, {"name": "torch.nn.utils.prune.RandomUnstructured.remove()", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured.remove", "type": "Neuro Network", "text": ["Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.remove()", "path": "generated/torch.nn.utils.prune.remove#torch.nn.utils.prune.remove", "type": "Neuro Network", "text": ["Removes the pruning reparameterization from a module and the pruning method from the forward hook. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.remove_spectral_norm()", "path": "generated/torch.nn.utils.remove_spectral_norm#torch.nn.utils.remove_spectral_norm", "type": "Neuro Network", "text": ["Removes the spectral normalization reparameterization from a module.", "T_module"]}, {"name": "torch.nn.utils.remove_weight_norm()", "path": "generated/torch.nn.utils.remove_weight_norm#torch.nn.utils.remove_weight_norm", "type": "Neuro Network", "text": ["Removes the weight normalization reparameterization from a module.", "T_module"]}, {"name": "torch.nn.utils.rnn.pack_padded_sequence()", "path": "generated/torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence", "type": "Neuro Network", "text": ["Packs a Tensor containing padded sequences of variable length.", "input can be of size T x B x * where T is the length of the longest sequence (equal to lengths[0]), B is the batch size, and * is any number of dimensions (including 0). If batch_first is True, B x T x * input is expected.", "For unsorted sequences, use enforce_sorted = False. If enforce_sorted is True, the sequences should be sorted by length in a decreasing order, i.e. input[:,0] should be the longest sequence, and input[:,B-1] the shortest one. enforce_sorted = True is only necessary for ONNX export.", "Note", "This function accepts any input that has at least two dimensions. You can apply it to pack the labels, and use the output of the RNN with them to compute the loss directly. A Tensor can be retrieved from a PackedSequence object by accessing its .data attribute.", "a PackedSequence object", "PackedSequence"]}, {"name": "torch.nn.utils.rnn.pack_sequence()", "path": "generated/torch.nn.utils.rnn.pack_sequence#torch.nn.utils.rnn.pack_sequence", "type": "Neuro Network", "text": ["Packs a list of variable length Tensors", "Consecutive call of the next functions: pad_sequence, pack_padded_sequence.", "sequences should be a list of Tensors of size L x *, where L is the length of a sequence and * is any number of trailing dimensions, including zero.", "For unsorted sequences, use enforce_sorted = False. If enforce_sorted is True, the sequences should be sorted in the order of decreasing length. enforce_sorted = True is only necessary for ONNX export.", "a PackedSequence object", "PackedSequence"]}, {"name": "torch.nn.utils.rnn.PackedSequence", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence", "type": "Neuro Network", "text": ["Holds the data and list of batch_sizes of a packed sequence.", "All RNN modules accept packed sequences as inputs.", "Note", "Instances of this class should never be created manually. They are meant to be instantiated by functions like pack_padded_sequence().", "Batch sizes represent the number elements at each sequence step in the batch, not the varying sequence lengths passed to pack_padded_sequence(). For instance, given data abc and x the PackedSequence would contain data axbc with batch_sizes=[2,1,1].", "Note", "data can be on arbitrary device and of arbitrary dtype. sorted_indices and unsorted_indices must be torch.int64 tensors on the same device as data.", "However, batch_sizes should always be a CPU torch.int64 tensor.", "This invariant is maintained throughout PackedSequence class, and all functions that construct a :class:PackedSequence in PyTorch (i.e., they only pass in tensors conforming to this constraint).", "Alias for field number 1", "Return number of occurrences of value.", "Alias for field number 0", "Return first index of value.", "Raises ValueError if the value is not present.", "Returns true if self.data stored on a gpu", "Returns true if self.data stored on in pinned memory", "Alias for field number 2", "Performs dtype and/or device conversion on self.data.", "It has similar signature as torch.Tensor.to(), except optional arguments like non_blocking and copy should be passed as kwargs, not args, or they will not apply to the index tensors.", "Note", "If the self.data Tensor already has the correct torch.dtype and torch.device, then self is returned. Otherwise, returns a copy with the desired configuration.", "Alias for field number 3"]}, {"name": "torch.nn.utils.rnn.PackedSequence.batch_sizes", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.batch_sizes", "type": "Neuro Network", "text": ["Alias for field number 1"]}, {"name": "torch.nn.utils.rnn.PackedSequence.count()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.count", "type": "Neuro Network", "text": ["Return number of occurrences of value."]}, {"name": "torch.nn.utils.rnn.PackedSequence.data", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.data", "type": "Neuro Network", "text": ["Alias for field number 0"]}, {"name": "torch.nn.utils.rnn.PackedSequence.index()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.index", "type": "Neuro Network", "text": ["Return first index of value.", "Raises ValueError if the value is not present."]}, {"name": "torch.nn.utils.rnn.PackedSequence.is_cuda", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.is_cuda", "type": "Neuro Network", "text": ["Returns true if self.data stored on a gpu"]}, {"name": "torch.nn.utils.rnn.PackedSequence.is_pinned()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.is_pinned", "type": "Neuro Network", "text": ["Returns true if self.data stored on in pinned memory"]}, {"name": "torch.nn.utils.rnn.PackedSequence.sorted_indices", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.sorted_indices", "type": "Neuro Network", "text": ["Alias for field number 2"]}, {"name": "torch.nn.utils.rnn.PackedSequence.to()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.to", "type": "Neuro Network", "text": ["Performs dtype and/or device conversion on self.data.", "It has similar signature as torch.Tensor.to(), except optional arguments like non_blocking and copy should be passed as kwargs, not args, or they will not apply to the index tensors.", "Note", "If the self.data Tensor already has the correct torch.dtype and torch.device, then self is returned. Otherwise, returns a copy with the desired configuration."]}, {"name": "torch.nn.utils.rnn.PackedSequence.unsorted_indices", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.unsorted_indices", "type": "Neuro Network", "text": ["Alias for field number 3"]}, {"name": "torch.nn.utils.rnn.pad_packed_sequence()", "path": "generated/torch.nn.utils.rnn.pad_packed_sequence#torch.nn.utils.rnn.pad_packed_sequence", "type": "Neuro Network", "text": ["Pads a packed batch of variable length sequences.", "It is an inverse operation to pack_padded_sequence().", "The returned Tensor\u2019s data will be of size T x B x *, where T is the length of the longest sequence and B is the batch size. If batch_first is True, the data will be transposed into B x T x * format.", "Note", "total_length is useful to implement the pack sequence -> recurrent network -> unpack sequence pattern in a Module wrapped in DataParallel. See this FAQ section for details.", "Tuple of Tensor containing the padded sequence, and a Tensor containing the list of lengths of each sequence in the batch. Batch elements will be re-ordered as they were ordered originally when the batch was passed to pack_padded_sequence or pack_sequence.", "Tuple[Tensor, Tensor]"]}, {"name": "torch.nn.utils.rnn.pad_sequence()", "path": "generated/torch.nn.utils.rnn.pad_sequence#torch.nn.utils.rnn.pad_sequence", "type": "Neuro Network", "text": ["Pad a list of variable length Tensors with padding_value", "pad_sequence stacks a list of Tensors along a new dimension, and pads them to equal length. For example, if the input is a list of sequences with size L x * and batch_first is False, the output is of size T x B x *.", "B is batch size. It is equal to the number of elements in sequences. T is length of the longest sequence. L is length of the sequence. * is any number of trailing dimensions, including none.", "Note", "This function returns a Tensor of size T x B x * or B x T x * where T is the length of the longest sequence. This function assumes trailing dimensions and type of all the Tensors in sequences are same.", "Tensor of size T x B x * if batch_first is False. Tensor of size B x T x * otherwise", "Tensor"]}, {"name": "torch.nn.utils.rnn.unpack_sequence()", "path": "generated/torch.nn.utils.rnn.unpack_sequence#torch.nn.utils.rnn.unpack_sequence", "type": "Neuro Network", "text": ["Unpacks PackedSequence into a list of variable length Tensors", "packed_sequences should be a PackedSequence object.", "packed_sequences (PackedSequence) \u2013 A PackedSequence object.", "a list of Tensor objects", "List[Tensor]"]}, {"name": "torch.nn.utils.rnn.unpad_sequence()", "path": "generated/torch.nn.utils.rnn.unpad_sequence#torch.nn.utils.rnn.unpad_sequence", "type": "Neuro Network", "text": ["Unpad padded Tensor into a list of variable length Tensors", "unpad_sequence unstacks padded Tensor into a list of variable length Tensors.", "a list of Tensor objects", "List[Tensor]"]}, {"name": "torch.nn.utils.skip_init()", "path": "generated/torch.nn.utils.skip_init#torch.nn.utils.skip_init", "type": "Neuro Network", "text": ["Given a module class object and args / kwargs, instantiates the module without initializing parameters / buffers. This can be useful if initialization is slow or if custom initialization will be performed, making the default initialization unnecessary. There are some caveats to this, due to the way this function is implemented:", "1. The module must accept a device arg in its constructor that is passed to any parameters or buffers created during construction.", "2. The module must not perform any computation on parameters in its constructor except initialization (i.e. functions from torch.nn.init).", "If these conditions are satisfied, the module can be instantiated with parameter / buffer values uninitialized, as if having been created using torch.empty().", "Instantiated module with uninitialized parameters / buffers", "Example:"]}, {"name": "torch.nn.utils.spectral_norm()", "path": "generated/torch.nn.utils.spectral_norm#torch.nn.utils.spectral_norm", "type": "Neuro Network", "text": ["Applies spectral normalization to a parameter in the given module.", "Spectral normalization stabilizes the training of discriminators (critics) in Generative Adversarial Networks (GANs) by rescaling the weight tensor with spectral norm \u03c3\\sigma of the weight matrix calculated using power iteration method. If the dimension of the weight tensor is greater than 2, it is reshaped to 2D in power iteration method to get spectral norm. This is implemented via a hook that calculates spectral norm and rescales weight before every forward() call.", "See Spectral Normalization for Generative Adversarial Networks .", "The original module with the spectral norm hook", "T_module", "Note", "This function has been reimplemented as torch.nn.utils.parametrizations.spectral_norm() using the new parametrization functionality in torch.nn.utils.parametrize.register_parametrization(). Please use the newer version. This function will be deprecated in a future version of PyTorch.", "Example:"]}, {"name": "torch.nn.utils.stateless.functional_call()", "path": "generated/torch.nn.utils.stateless.functional_call#torch.nn.utils.stateless.functional_call", "type": "Neuro Network", "text": ["Performs a functional call on the module by replacing the module parameters and buffers with the provided ones.", "Warning", "This API is deprecated as of PyTorch 2.0 and will be removed in a future version of PyTorch. Please use torch.func.functional_call() instead, which is a drop-in replacement for this API.", "Note", "If the module has active parametrizations, passing a value in the parameters_and_buffers argument with the name set to the regular parameter name will completely disable the parametrization. If you want to apply the parametrization function to the value passed please set the key as {submodule_name}.parametrizations.{parameter_name}.original.", "Note", "If the module performs in-place operations on parameters/buffers, these will be reflected in the parameters_and_buffers input.", "Example:", "Note", "If the module has tied weights, whether or not functional_call respects the tying is determined by the tie_weights flag.", "Example:", "the result of calling module.", "Any"]}, {"name": "torch.nn.utils.vector_to_parameters()", "path": "generated/torch.nn.utils.vector_to_parameters#torch.nn.utils.vector_to_parameters", "type": "Neuro Network", "text": ["Convert one vector to the parameters"]}, {"name": "torch.nn.utils.weight_norm()", "path": "generated/torch.nn.utils.weight_norm#torch.nn.utils.weight_norm", "type": "Neuro Network", "text": ["Applies weight normalization to a parameter in the given module.", "Weight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. This replaces the parameter specified by name (e.g. 'weight') with two parameters: one specifying the magnitude (e.g. 'weight_g') and one specifying the direction (e.g. 'weight_v'). Weight normalization is implemented via a hook that recomputes the weight tensor from the magnitude and direction before every forward() call.", "By default, with dim=0, the norm is computed independently per output channel/plane. To compute a norm over the entire weight tensor, use dim=None.", "See https://arxiv.org/abs/1602.07868", "Warning", "This function is deprecated. Use torch.nn.utils.parametrizations.weight_norm() which uses the modern parametrization API. The new weight_norm is compatible with state_dict generated from old weight_norm.", "Migration guide:", "The original module with the weight norm hook", "T_module", "Example:"]}, {"name": "torch.nn.ZeroPad1d", "path": "generated/torch.nn.zeropad1d", "type": "Neuro Network", "text": ["Pads the input tensor boundaries with zero.", "For N-dimensional padding, use torch.nn.functional.pad().", "padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in both boundaries. If a 2-tuple, uses (padding_left\\text{padding\\_left}, padding_right\\text{padding\\_right})", "Output: (C,Wout)(C, W_{out}) or (N,C,Wout)(N, C, W_{out}), where", "Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}", "Examples:"]}, {"name": "torch.nn.ZeroPad2d", "path": "generated/torch.nn.zeropad2d", "type": "Neuro Network", "text": ["Pads the input tensor boundaries with zero.", "For N-dimensional padding, use torch.nn.functional.pad().", "padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}, padding_right\\text{padding\\_right}, padding_top\\text{padding\\_top}, padding_bottom\\text{padding\\_bottom})", "Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) or (C,Hout,Wout)(C, H_{out}, W_{out}), where", "Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}", "Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}", "Examples:"]}, {"name": "torch.nn.ZeroPad3d", "path": "generated/torch.nn.zeropad3d", "type": "Neuro Network", "text": ["Pads the input tensor boundaries with zero.", "For N-dimensional padding, use torch.nn.functional.pad().", "padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding_left\\text{padding\\_left}, padding_right\\text{padding\\_right}, padding_top\\text{padding\\_top}, padding_bottom\\text{padding\\_bottom}, padding_front\\text{padding\\_front}, padding_back\\text{padding\\_back})", "Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out}) or (C,Dout,Hout,Wout)(C, D_{out}, H_{out}, W_{out}), where", "Dout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\_front} + \\text{padding\\_back}", "Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}", "Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}", "Examples:"]}, {"name": "torch.no_grad", "path": "generated/torch.no_grad#torch.no_grad", "type": "Torch", "text": ["Context-manager that disables gradient calculation.", "Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True.", "In this mode, the result of every computation will have requires_grad=False, even when the inputs have requires_grad=True. There is an exception! All factory functions, or functions that create a new Tensor and take a requires_grad kwarg, will NOT be affected by this mode.", "This context manager is thread local; it will not affect computation in other threads.", "Also functions as a decorator.", "Note", "No-grad is one of several mechanisms that can enable or disable gradients locally see Locally disabling gradient computation for more information on how they compare.", "Note", "This API does not apply to forward-mode AD. If you want to disable forward AD for a computation, you can unpack your dual tensors."]}, {"name": "torch.nonzero", "path": "generated/torch.nonzero", "type": "Torch", "text": ["Note", "torch.nonzero(..., as_tuple=False) (default) returns a 2-D tensor where each row is the index for a nonzero value.", "torch.nonzero(..., as_tuple=True) returns a tuple of 1-D index tensors, allowing for advanced indexing, so x[x.nonzero(as_tuple=True)] gives all nonzero values of tensor x. Of the returned tuple, each index tensor contains nonzero indices for a certain dimension.", "See below for more details on the two behaviors.", "When input is on CUDA, torch.nonzero() causes host-device synchronization.", "When as_tuple is False (default):", "Returns a tensor containing the indices of all non-zero elements of input. Each row in the result contains the indices of a non-zero element in input. The result is sorted lexicographically, with the last index changing the fastest (C-style).", "If input has nn dimensions, then the resulting indices tensor out is of size (z\u00d7n)(z \\times n), where zz is the total number of non-zero elements in the input tensor.", "When as_tuple is True:", "Returns a tuple of 1-D tensors, one for each dimension in input, each containing the indices (in that dimension) of all non-zero elements of input .", "If input has nn dimensions, then the resulting tuple contains nn tensors of size zz, where zz is the total number of non-zero elements in the input tensor.", "As a special case, when input has zero dimensions and a nonzero scalar value, it is treated as a one-dimensional tensor with one element.", "input (Tensor) \u2013 the input tensor.", "out (LongTensor, optional) \u2013 the output tensor containing indices", "If as_tuple is False, the output tensor containing indices. If as_tuple is True, one 1-D tensor for each dimension, containing the indices of each nonzero element along that dimension.", "LongTensor or tuple of LongTensor", "Example:"]}, {"name": "torch.norm", "path": "generated/torch.norm", "type": "Torch", "text": ["Returns the matrix norm or vector norm of a given tensor.", "Warning", "torch.norm is deprecated and may be removed in a future PyTorch release. Its documentation and behavior may be incorrect, and it is no longer actively maintained.", "Use torch.linalg.vector_norm() when computing vector norms and torch.linalg.matrix_norm() when computing matrix norms. For a function with a similar behavior as this one see torch.linalg.norm(). Note, however, the signature for these functions is slightly different than the signature for torch.norm.", "p (int, float, inf, -inf, 'fro', 'nuc', optional) \u2013 ", "the order of norm. Default: 'fro' The following norms can be calculated:", "ord", "matrix norm", "vector norm", "\u2019fro\u2019", "Frobenius norm", "\u2013", "\u2018nuc\u2019", "nuclear norm", "\u2013", "Number", "\u2013", "sum(abs(x)**ord)**(1./ord)", "The vector norm can be calculated across any number of dimensions. The corresponding dimensions of input are flattened into one dimension, and the norm is calculated on the flattened dimension.", "Frobenius norm produces the same result as p=2 in all cases except when dim is a list of three or more dims, in which case Frobenius norm throws an error.", "Nuclear norm can only be calculated across exactly two dimensions.", "Note", "Even though p='fro' supports any number of dimensions, the true mathematical definition of Frobenius norm only applies to tensors with exactly two dimensions. torch.linalg.matrix_norm() with ord='fro' aligns with the mathematical definition, since it can only be applied across exactly two dimensions.", "Example:"]}, {"name": "torch.normal", "path": "generated/torch.normal", "type": "Torch", "text": ["Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.", "The mean is a tensor with the mean of each output element\u2019s normal distribution", "The std is a tensor with the standard deviation of each output element\u2019s normal distribution", "The shapes of mean and std don\u2019t need to match, but the total number of elements in each tensor need to be the same.", "Note", "When the shapes do not match, the shape of mean is used as the shape for the returned output tensor", "Note", "When std is a CUDA tensor, this function synchronizes its device with the CPU.", "Example:", "Similar to the function above, but the means are shared among all drawn elements.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Similar to the function above, but the standard deviations are shared among all drawn elements.", "out (Tensor, optional) \u2013 the output tensor", "Example:", "Similar to the function above, but the means and standard deviations are shared among all drawn elements. The resulting tensor has size given by size.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.not_equal", "path": "generated/torch.not_equal", "type": "Torch", "text": ["Alias for torch.ne()."]}, {"name": "torch.numel", "path": "generated/torch.numel", "type": "Torch", "text": ["Returns the total number of elements in the input tensor.", "input (Tensor) \u2013 the input tensor.", "Example:"]}, {"name": "torch.ones", "path": "generated/torch.ones", "type": "Torch", "text": ["Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.", "size (int...) \u2013 a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.", "Example:"]}, {"name": "torch.ones_like", "path": "generated/torch.ones_like", "type": "Torch", "text": ["Returns a tensor filled with the scalar value 1, with the same size as input. torch.ones_like(input) is equivalent to torch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).", "Warning", "As of 0.4, this function does not support an out keyword. As an alternative, the old torch.ones_like(input, out=output) is equivalent to torch.ones(input.size(), out=output).", "input (Tensor) \u2013 the size of input will determine size of the output tensor.", "Example:"]}, {"name": "torch.onnx", "path": "onnx", "type": "ONNX", "text": ["Open Neural Network eXchange (ONNX) is an open standard format for representing machine learning models. The torch.onnx module captures the computation graph from a native PyTorch torch.nn.Module model and converts it into an ONNX graph.", "The exported model can be consumed by any of the many runtimes that support ONNX, including Microsoft\u2019s ONNX Runtime.", "There are two flavors of ONNX exporter API that you can use, as listed below:", "The TorchDynamo-based ONNX exporter is the newest (and Beta) exporter for PyTorch 2.0 and newer", "TorchDynamo engine is leveraged to hook into Python\u2019s frame evaluation API and dynamically rewrite its bytecode into an FX Graph. The resulting FX Graph is then polished before it is finally translated into an ONNX graph.", "The main advantage of this approach is that the FX graph is captured using bytecode analysis that preserves the dynamic nature of the model instead of using traditional static tracing techniques.", "Learn more about the TorchDynamo-based ONNX Exporter", "The TorchScript-based ONNX exporter is available since PyTorch 1.2.0", "TorchScript is leveraged to trace (through torch.jit.trace()) the model and capture a static computation graph.", "As a consequence, the resulting graph has a couple limitations:", "As an attempt to support the static tracing limitations, the exporter also supports TorchScript scripting (through torch.jit.script()), which adds support for data-dependent control-flow, for example. However, TorchScript itself is a subset of the Python language, so not all features in Python are supported, such as in-place operations.", "Learn more about the TorchScript-based ONNX Exporter", "The ONNX exporter is a community project and we welcome contributions. We follow the PyTorch guidelines for contributions, but you might also be interested in reading our development wiki."]}, {"name": "torch.onnx.DiagnosticOptions", "path": "onnx_dynamo#torch.onnx.DiagnosticOptions", "type": "ONNX", "text": ["Options for diagnostic context."]}, {"name": "torch.onnx.disable_log()", "path": "onnx_torchscript#torch.onnx.disable_log", "type": "ONNX", "text": ["Disables ONNX logging."]}, {"name": "torch.onnx.dynamo_export()", "path": "onnx_dynamo#torch.onnx.dynamo_export", "type": "ONNX", "text": ["Export a torch.nn.Module to an ONNX graph.", "An in-memory representation of the exported ONNX model.", "ExportOutput", "Example 1 - Simplest export", "Example 2 - Exporting with dynamic shapes", "By printing input dynamic dimensions we can see the input shape is no longer (2,2,2)"]}, {"name": "torch.onnx.enable_fake_mode()", "path": "onnx_dynamo#torch.onnx.enable_fake_mode", "type": "ONNX", "text": ["Enable fake mode for the duration of the context.", "Internally it instantiates a torch._subclasses.fake_tensor.FakeTensorMode context manager that converts user input and model parameters into torch._subclasses.fake_tensor.FakeTensor.", "A torch._subclasses.fake_tensor.FakeTensor is a torch.Tensor with the ability to run PyTorch code without having to actually do computation through tensors allocated on a meta device. Because there is no actual data being allocated on the device, this API allows for exporting large models without the actual memory footprint needed for executing it.", "It is highly recommended to enable fake mode when exporting models that are too large to fit into memory.", "A ONNXFakeContext object that must be passed to dynamo_export() through the ExportOptions.fake_context argument.", "Example:", "Warning", "This API is experimental and is NOT backward-compatible."]}, {"name": "torch.onnx.enable_log()", "path": "onnx_torchscript#torch.onnx.enable_log", "type": "ONNX", "text": ["Enables ONNX logging."]}, {"name": "torch.onnx.export()", "path": "onnx_torchscript#torch.onnx.export", "type": "ONNX", "text": ["Exports a model into ONNX format.", "If model is not a torch.jit.ScriptModule nor a torch.jit.ScriptFunction, this runs model once in order to convert it to a TorchScript graph to be exported (the equivalent of torch.jit.trace()). Thus this has the same limited support for dynamic control flow as torch.jit.trace().", "args (tuple or torch.Tensor) \u2013 ", "args can be structured either as:", "ONLY A TUPLE OF ARGUMENTS:", "The tuple should contain model inputs such that model(*args) is a valid invocation of the model. Any non-Tensor arguments will be hard-coded into the exported model; any Tensor arguments will become inputs of the exported model, in the order they occur in the tuple.", "A TENSOR:", "This is equivalent to a 1-ary tuple of that Tensor.", "A TUPLE OF ARGUMENTS ENDING WITH A DICTIONARY OF NAMED ARGUMENTS:", "All but the last element of the tuple will be passed as non-keyword arguments, and named arguments will be set from the last element. If a named argument is not present in the dictionary, it is assigned the default value, or None if a default value is not provided.", "Note", "If a dictionary is the last element of the args tuple, it will be interpreted as containing named arguments. In order to pass a dict as the last non-keyword arg, provide an empty dict as the last element of the args tuple. For example, instead of:", "Write:", "training (enum, default TrainingMode.EVAL) \u2013 ", "False and in training mode if model.training is True.", "which might interfere with training.", "operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013 ", "(in the default opset domain).", "to standard ONNX ops in the default opset domain. If unable to do so (e.g. because support has not been added to convert a particular torch op to ONNX), fall back to exporting the op into a custom opset domain without conversion. Applies to custom ops as well as ATen ops. For the exported model to be usable, the runtime must support these non-standard ops.", "are exported as ATen ops (in opset domain \u201corg.pytorch.aten\u201d). ATen is PyTorch\u2019s built-in tensor library, so this instructs the runtime to use PyTorch\u2019s implementation of these ops.", "Warning", "Models exported this way are probably runnable only by Caffe2.", "This may be useful if the numeric differences in implementations of operators are causing large differences in behavior between PyTorch and Caffe2 (which is more common on untrained models).", "(in the TorchScript namespace \u201caten\u201d) as a regular ONNX op. If we are unable to do so (e.g. because support has not been added to convert a particular torch op to ONNX), fall back to exporting an ATen op. See documentation on OperatorExportTypes.ONNX_ATEN for context. For example:", "Assuming aten::triu is not supported in ONNX, this will be exported as:", "If PyTorch was built with Caffe2 (i.e. with BUILD_CAFFE2=1), then Caffe2-specific behavior will be enabled, including special support for ops are produced by the modules described in Quantization.", "Warning", "Models exported this way are probably runnable only by Caffe2.", "dynamic_axes (dict[string, dict[int, string]] or dict[string, list(int)], default empty dict) \u2013 ", "By default the exported model will have the shapes of all input and output tensors set to exactly match those given in args. To specify axes of tensors as dynamic (i.e. known only at run-time), set dynamic_axes to a dict with schema:", "output_names.", "list, each element is an axis index.", "For example:", "Produces:", "While:", "Produces:", "keep_initializers_as_inputs (bool, default None) \u2013 ", "If True, all the initializers (typically corresponding to parameters) in the exported graph will also be added as inputs to the graph. If False, then initializers are not added as inputs to the graph, and only the non-parameter inputs are added as inputs. This may allow for better optimizations (e.g. constant folding) by backends/runtimes.", "If True, deduplicate_initializers pass will not be executed. This means initializers with duplicated values will not be deduplicated and will be treated as distinct inputs to the graph. This allows different input initializers to be supplied at the runtime following export.", "If opset_version < 9, initializers MUST be part of graph inputs and this argument will be ignored and the behavior will be equivalent to setting this argument to True.", "If None, then the behavior is chosen automatically as follows:", "to setting this argument to False.", "custom_opsets (dict[str, int], default empty dict) \u2013 ", "A dict with schema:", "If a custom opset is referenced by model but not mentioned in this dictionary, the opset version is set to 1. Only custom opset domain name and version should be indicated through this argument.", "export_modules_as_functions (bool or set of type of nn.Module, default False) \u2013 ", "Flag to enable exporting all nn.Module forward calls as local functions in ONNX. Or a set to indicate the particular types of modules to export as local functions in ONNX. This feature requires opset_version >= 15, otherwise the export will fail. This is because opset_version < 15 implies IR version < 8, which means no local function support. Module variables will be exported as function attributes. There are two categories of function attributes.", "1. Annotated attributes: class variables that have type annotations via PEP 526-style will be exported as attributes. Annotated attributes are not used inside the subgraph of ONNX local function because they are not created by PyTorch JIT tracing, but they may be used by consumers to determine whether or not to replace the function with a particular fused kernel.", "2. Inferred attributes: variables that are used by operators inside the module. Attribute names will have prefix \u201cinferred::\u201d. This is to differentiate from predefined attributes retrieved from python module annotations. Inferred attributes are used inside the subgraph of ONNX local function.", "only if the type of the nn.Module is found in the set."]}, {"name": "torch.onnx.export_to_pretty_string()", "path": "onnx_torchscript#torch.onnx.export_to_pretty_string", "type": "ONNX", "text": ["Similar to export(), but returns a text representation of the ONNX model. Only differences in args listed below. All other args are the same as export().", "A UTF-8 str containing a human-readable representation of the ONNX model."]}, {"name": "torch.onnx.ExportOptions", "path": "onnx_dynamo#torch.onnx.ExportOptions", "type": "ONNX", "text": ["Options to influence the TorchDynamo ONNX exporter."]}, {"name": "torch.onnx.ExportOutput", "path": "onnx_dynamo#torch.onnx.ExportOutput", "type": "ONNX", "text": ["An in-memory representation of a PyTorch model that has been exported to ONNX.", "Converts the PyTorch model inputs to exported ONNX model inputs format.", "Due to design differences, input/output format between PyTorch model and exported ONNX model are often not the same. E.g., None is allowed for PyTorch model, but are not supported by ONNX. Nested constructs of tensors are allowed for PyTorch model, but only flattened tensors are supported by ONNX, etc.", "The actual adapting steps are associated with each individual export. It depends on the PyTorch model, the particular set of model_args and model_kwargs used for the export, and export options.", "This method replays the adapting steps recorded during export.", "A sequence of tensors converted from PyTorch model inputs.", "Sequence[Union[Tensor, int, float, bool]]", "Example:", "Warning", "This API is experimental and is NOT backward-compatible.", "Converts the PyTorch model outputs to exported ONNX model outputs format.", "Due to design differences, input/output format between PyTorch model and exported ONNX model are often not the same. E.g., None is allowed for PyTorch model, but are not supported by ONNX. Nested constructs of tensors are allowed for PyTorch model, but only flattened tensors are supported by ONNX, etc.", "The actual adapting steps are associated with each individual export. It depends on the PyTorch model, the particular set of model_args and model_kwargs used for the export, and export options.", "This method replays the adapting steps recorded during export.", "model_outputs (Any) \u2013 The PyTorch model outputs.", "PyTorch model outputs in exported ONNX model outputs format.", "Sequence[Union[Tensor, int, float, bool]]", "Example:", "Warning", "This API is experimental and is NOT backward-compatible.", "The diagnostic context associated with the export.", "The fake context associated with the export.", "The exported ONNX model as an onnx.ModelProto.", "Saves the in-memory ONNX model to destination using specified serializer.", "Saves the export diagnostics as a SARIF log to the specified destination path.", "destination (str) \u2013 The destination to save the diagnostics SARIF log. It must have a .sarif extension.", "ValueError \u2013 If the destination path does not end with .sarif extension."]}, {"name": "torch.onnx.ExportOutput.adapt_torch_inputs_to_onnx()", "path": "onnx_dynamo#torch.onnx.ExportOutput.adapt_torch_inputs_to_onnx", "type": "ONNX", "text": ["Converts the PyTorch model inputs to exported ONNX model inputs format.", "Due to design differences, input/output format between PyTorch model and exported ONNX model are often not the same. E.g., None is allowed for PyTorch model, but are not supported by ONNX. Nested constructs of tensors are allowed for PyTorch model, but only flattened tensors are supported by ONNX, etc.", "The actual adapting steps are associated with each individual export. It depends on the PyTorch model, the particular set of model_args and model_kwargs used for the export, and export options.", "This method replays the adapting steps recorded during export.", "A sequence of tensors converted from PyTorch model inputs.", "Sequence[Union[Tensor, int, float, bool]]", "Example:", "Warning", "This API is experimental and is NOT backward-compatible."]}, {"name": "torch.onnx.ExportOutput.adapt_torch_outputs_to_onnx()", "path": "onnx_dynamo#torch.onnx.ExportOutput.adapt_torch_outputs_to_onnx", "type": "ONNX", "text": ["Converts the PyTorch model outputs to exported ONNX model outputs format.", "Due to design differences, input/output format between PyTorch model and exported ONNX model are often not the same. E.g., None is allowed for PyTorch model, but are not supported by ONNX. Nested constructs of tensors are allowed for PyTorch model, but only flattened tensors are supported by ONNX, etc.", "The actual adapting steps are associated with each individual export. It depends on the PyTorch model, the particular set of model_args and model_kwargs used for the export, and export options.", "This method replays the adapting steps recorded during export.", "model_outputs (Any) \u2013 The PyTorch model outputs.", "PyTorch model outputs in exported ONNX model outputs format.", "Sequence[Union[Tensor, int, float, bool]]", "Example:", "Warning", "This API is experimental and is NOT backward-compatible."]}, {"name": "torch.onnx.ExportOutput.diagnostic_context", "path": "onnx_dynamo#torch.onnx.ExportOutput.diagnostic_context", "type": "ONNX", "text": ["The diagnostic context associated with the export."]}, {"name": "torch.onnx.ExportOutput.fake_context", "path": "onnx_dynamo#torch.onnx.ExportOutput.fake_context", "type": "ONNX", "text": ["The fake context associated with the export."]}, {"name": "torch.onnx.ExportOutput.model_proto", "path": "onnx_dynamo#torch.onnx.ExportOutput.model_proto", "type": "ONNX", "text": ["The exported ONNX model as an onnx.ModelProto."]}, {"name": "torch.onnx.ExportOutput.save()", "path": "onnx_dynamo#torch.onnx.ExportOutput.save", "type": "ONNX", "text": ["Saves the in-memory ONNX model to destination using specified serializer."]}, {"name": "torch.onnx.ExportOutput.save_diagnostics()", "path": "onnx_dynamo#torch.onnx.ExportOutput.save_diagnostics", "type": "ONNX", "text": ["Saves the export diagnostics as a SARIF log to the specified destination path.", "destination (str) \u2013 The destination to save the diagnostics SARIF log. It must have a .sarif extension.", "ValueError \u2013 If the destination path does not end with .sarif extension."]}, {"name": "torch.onnx.ExportOutputSerializer", "path": "onnx_dynamo#torch.onnx.ExportOutputSerializer", "type": "ONNX", "text": ["Protocol for serializing an ONNX graph into a specific format (e.g. Protobuf). Note that this is an advanced usage scenario.", "Protocol method that must be implemented for serialization.", "A simple serializer that writes the exported onnx.ModelProto in Protobuf format to destination:"]}, {"name": "torch.onnx.ExportOutputSerializer.serialize()", "path": "onnx_dynamo#torch.onnx.ExportOutputSerializer.serialize", "type": "ONNX", "text": ["Protocol method that must be implemented for serialization.", "A simple serializer that writes the exported onnx.ModelProto in Protobuf format to destination:"]}, {"name": "torch.onnx.is_in_onnx_export()", "path": "onnx_torchscript#torch.onnx.is_in_onnx_export", "type": "ONNX", "text": ["Returns whether it is in the middle of ONNX export.", "bool"]}, {"name": "torch.onnx.is_onnxrt_backend_supported()", "path": "onnx_dynamo_onnxruntime_backend#torch.onnx.is_onnxrt_backend_supported", "type": "ONNX", "text": ["Returns True if ONNX Runtime dependencies are installed and usable to support TorchDynamo backend integration; False otherwise.", "Example:", "bool"]}, {"name": "torch.onnx.JitScalarType", "path": "generated/torch.onnx.jitscalartype#torch.onnx.JitScalarType", "type": "ONNX", "text": ["Scalar types defined in torch.", "Use JitScalarType to convert from torch and JIT scalar types to ONNX scalar types.", "Convert a JitScalarType to a torch dtype.", "dtype", "Convert a torch dtype to JitScalarType.", "A \u201cRuntimeError: INTERNAL ASSERT FAILED at \u201c../aten/src/ATen/core/jit_type_base.h\u201d can be raised in several scenarios where shape info is not present. Instead use from_value API which is safer.", "dtype (Optional[dtype]) \u2013 A torch.dtype to create a JitScalarType from", "JitScalarType", "OnnxExporterError \u2013 if dtype is not a valid torch.dtype or if it is None.", "JitScalarType", "Create a JitScalarType from an value\u2019s scalar type.", "JitScalarType.", "JitScalarType", "Return whether this JitScalarType is compatible with ONNX.", "bool", "Convert a JitScalarType to an ONNX data type.", "TensorProtoDataType", "Convert a JitScalarType to a JIT scalar type name.", "Literal[\u2018Byte\u2019, \u2018Char\u2019, \u2018Double\u2019, \u2018Float\u2019, \u2018Half\u2019, \u2018Int\u2019, \u2018Long\u2019, \u2018Short\u2019, \u2018Bool\u2019, \u2018ComplexHalf\u2019, \u2018ComplexFloat\u2019, \u2018ComplexDouble\u2019, \u2018QInt8\u2019, \u2018QUInt8\u2019, \u2018QInt32\u2019, \u2018BFloat16\u2019, \u2018Float8E5M2\u2019, \u2018Float8E4M3FN\u2019, \u2018Undefined\u2019]", "Convert a JitScalarType to a torch type name.", "Literal[\u2018bool\u2019, \u2018uint8_t\u2019, \u2018int8_t\u2019, \u2018double\u2019, \u2018float\u2019, \u2018half\u2019, \u2018int\u2019, \u2018int64_t\u2019, \u2018int16_t\u2019, \u2018complex32\u2019, \u2018complex64\u2019, \u2018complex128\u2019, \u2018qint8\u2019, \u2018quint8\u2019, \u2018qint32\u2019, \u2018bfloat16\u2019, \u2018float8_e5m2\u2019, \u2018float8_e4m3fn\u2019]"]}, {"name": "torch.onnx.JitScalarType.dtype()", "path": "generated/torch.onnx.jitscalartype#torch.onnx.JitScalarType.dtype", "type": "ONNX", "text": ["Convert a JitScalarType to a torch dtype.", "dtype"]}, {"name": "torch.onnx.JitScalarType.from_dtype()", "path": "generated/torch.onnx.jitscalartype#torch.onnx.JitScalarType.from_dtype", "type": "ONNX", "text": ["Convert a torch dtype to JitScalarType.", "A \u201cRuntimeError: INTERNAL ASSERT FAILED at \u201c../aten/src/ATen/core/jit_type_base.h\u201d can be raised in several scenarios where shape info is not present. Instead use from_value API which is safer.", "dtype (Optional[dtype]) \u2013 A torch.dtype to create a JitScalarType from", "JitScalarType", "OnnxExporterError \u2013 if dtype is not a valid torch.dtype or if it is None.", "JitScalarType"]}, {"name": "torch.onnx.JitScalarType.from_value()", "path": "generated/torch.onnx.jitscalartype#torch.onnx.JitScalarType.from_value", "type": "ONNX", "text": ["Create a JitScalarType from an value\u2019s scalar type.", "JitScalarType.", "JitScalarType"]}, {"name": "torch.onnx.JitScalarType.onnx_compatible()", "path": "generated/torch.onnx.jitscalartype#torch.onnx.JitScalarType.onnx_compatible", "type": "ONNX", "text": ["Return whether this JitScalarType is compatible with ONNX.", "bool"]}, {"name": "torch.onnx.JitScalarType.onnx_type()", "path": "generated/torch.onnx.jitscalartype#torch.onnx.JitScalarType.onnx_type", "type": "ONNX", "text": ["Convert a JitScalarType to an ONNX data type.", "TensorProtoDataType"]}, {"name": "torch.onnx.JitScalarType.scalar_name()", "path": "generated/torch.onnx.jitscalartype#torch.onnx.JitScalarType.scalar_name", "type": "ONNX", "text": ["Convert a JitScalarType to a JIT scalar type name.", "Literal[\u2018Byte\u2019, \u2018Char\u2019, \u2018Double\u2019, \u2018Float\u2019, \u2018Half\u2019, \u2018Int\u2019, \u2018Long\u2019, \u2018Short\u2019, \u2018Bool\u2019, \u2018ComplexHalf\u2019, \u2018ComplexFloat\u2019, \u2018ComplexDouble\u2019, \u2018QInt8\u2019, \u2018QUInt8\u2019, \u2018QInt32\u2019, \u2018BFloat16\u2019, \u2018Float8E5M2\u2019, \u2018Float8E4M3FN\u2019, \u2018Undefined\u2019]"]}, {"name": "torch.onnx.JitScalarType.torch_name()", "path": "generated/torch.onnx.jitscalartype#torch.onnx.JitScalarType.torch_name", "type": "ONNX", "text": ["Convert a JitScalarType to a torch type name.", "Literal[\u2018bool\u2019, \u2018uint8_t\u2019, \u2018int8_t\u2019, \u2018double\u2019, \u2018float\u2019, \u2018half\u2019, \u2018int\u2019, \u2018int64_t\u2019, \u2018int16_t\u2019, \u2018complex32\u2019, \u2018complex64\u2019, \u2018complex128\u2019, \u2018qint8\u2019, \u2018quint8\u2019, \u2018qint32\u2019, \u2018bfloat16\u2019, \u2018float8_e5m2\u2019, \u2018float8_e4m3fn\u2019]"]}, {"name": "torch.onnx.ONNX Backend for TorchDynamo", "path": "onnx_dynamo_onnxruntime_backend", "type": "ONNX", "text": ["For a quick overview of torch.compiler, see torch.compiler.", "Warning", "The ONNX backend for torch.compile is a rapidly evolving beta technology.", "Returns True if ONNX Runtime dependencies are installed and usable to support TorchDynamo backend integration; False otherwise.", "Example:", "bool"]}, {"name": "torch.onnx.OnnxExporterError", "path": "onnx_dynamo#torch.onnx.OnnxExporterError", "type": "ONNX", "text": ["Raised when an ONNX exporter error occurs.", "This exception is thrown when there\u2019s an error during the ONNX export process. It encapsulates the ExportOutput object generated until the failure, allowing access to the partial export results and associated metadata."]}, {"name": "torch.onnx.OnnxRegistry", "path": "onnx_dynamo#torch.onnx.OnnxRegistry", "type": "ONNX", "text": ["Registry for ONNX functions.", "The registry maintains a mapping from qualified names to symbolic functions under a fixed opset version. It supports registering custom onnx-script functions and for dispatcher to dispatch calls to the appropriate function.", "Returns a list of ONNXFunctions for the given op: torch.ops.<namespace>.<op_name>.<overload>.", "The list is ordered by the time of registration. The custom operators should be in the second half of the list.", "A list of ONNXFunctions corresponding to the given name, or None if the name is not in the registry.", "Optional[List[ONNXFunction]]", "Returns whether the given op is registered: torch.ops.<namespace>.<op_name>.<overload>.", "True if the given op is registered, otherwise False.", "bool", "The ONNX opset version the exporter should target. Defaults to the latest supported ONNX opset version: 18. The default version will increment over time as ONNX continues to evolve.", "Registers a custom operator: torch.ops.<namespace>.<op_name>.<overload>.", "ValueError \u2013 If the name is not in the form of \u2018namespace::op\u2019."]}, {"name": "torch.onnx.OnnxRegistry.get_op_functions()", "path": "onnx_dynamo#torch.onnx.OnnxRegistry.get_op_functions", "type": "ONNX", "text": ["Returns a list of ONNXFunctions for the given op: torch.ops.<namespace>.<op_name>.<overload>.", "The list is ordered by the time of registration. The custom operators should be in the second half of the list.", "A list of ONNXFunctions corresponding to the given name, or None if the name is not in the registry.", "Optional[List[ONNXFunction]]"]}, {"name": "torch.onnx.OnnxRegistry.is_registered_op()", "path": "onnx_dynamo#torch.onnx.OnnxRegistry.is_registered_op", "type": "ONNX", "text": ["Returns whether the given op is registered: torch.ops.<namespace>.<op_name>.<overload>.", "True if the given op is registered, otherwise False.", "bool"]}, {"name": "torch.onnx.OnnxRegistry.opset_version", "path": "onnx_dynamo#torch.onnx.OnnxRegistry.opset_version", "type": "ONNX", "text": ["The ONNX opset version the exporter should target. Defaults to the latest supported ONNX opset version: 18. The default version will increment over time as ONNX continues to evolve."]}, {"name": "torch.onnx.OnnxRegistry.register_op()", "path": "onnx_dynamo#torch.onnx.OnnxRegistry.register_op", "type": "ONNX", "text": ["Registers a custom operator: torch.ops.<namespace>.<op_name>.<overload>.", "ValueError \u2013 If the name is not in the form of \u2018namespace::op\u2019."]}, {"name": "torch.onnx.register_custom_op_symbolic()", "path": "onnx_torchscript#torch.onnx.register_custom_op_symbolic", "type": "ONNX", "text": ["Registers a symbolic function for a custom operator.", "When the user registers symbolic for custom/contrib ops, it is highly recommended to add shape inference for that operator via setType API, otherwise the exported graph may have incorrect shape inference in some extreme cases. An example of setType is test_aten_embedding_2 in test_operators.py.", "See \u201cCustom Operators\u201d in the module documentation for an example usage."]}, {"name": "torch.onnx.select_model_mode_for_export()", "path": "onnx_torchscript#torch.onnx.select_model_mode_for_export", "type": "ONNX", "text": ["A context manager to temporarily set the training mode of model to mode, resetting it when we exit the with-block."]}, {"name": "torch.onnx.TorchDynamo-based ONNX Exporter", "path": "onnx_dynamo", "type": "ONNX", "text": ["Warning", "The ONNX exporter for TorchDynamo is a rapidly evolving beta technology.", "The ONNX exporter leverages TorchDynamo engine to hook into Python\u2019s frame evaluation API and dynamically rewrite its bytecode into an FX Graph. The resulting FX Graph is then polished before it is finally translated into an ONNX graph.", "The main advantage of this approach is that the FX graph is captured using bytecode analysis that preserves the dynamic nature of the model instead of using traditional static tracing techniques.", "The exporter is designed to be modular and extensible. It is composed of the following components:", "The ONNX exporter depends on extra Python packages:", "They can be installed through pip:", "See below a demonstration of exporter API in action with a simple Multilayer Perceptron (MLP) as example:", "As the code above shows, all you need is to provide torch.onnx.dynamo_export() with an instance of the model and its input. The exporter will then return an instance of torch.onnx.ExportOutput that contains the exported ONNX graph along with extra information.", "The in-memory model available through export_output.model_proto is an onnx.ModelProto object in compliance with the ONNX IR spec. The ONNX model may then be serialized into a Protobuf file using the torch.onnx.ExportOutput.save() API.", "You can view the exported model using Netron.", "Note that each layer is represented in a rectangular box with a f icon in the top right corner.", "By expanding it, the function body is shown.", "The function body is a sequence of ONNX operators or other functions.", "ONNX diagnostics goes beyond regular logs through the adoption of Static Analysis Results Interchange Format (aka SARIF) to help users debug and improve their model using a GUI, such as Visual Studio Code\u2019s SARIF Viewer.", "The main advantages are:", "ONNX Diagnostic SARIF Rules", "Export a torch.nn.Module to an ONNX graph.", "An in-memory representation of the exported ONNX model.", "ExportOutput", "Example 1 - Simplest export", "Example 2 - Exporting with dynamic shapes", "By printing input dynamic dimensions we can see the input shape is no longer (2,2,2)", "Options to influence the TorchDynamo ONNX exporter.", "Enable fake mode for the duration of the context.", "Internally it instantiates a torch._subclasses.fake_tensor.FakeTensorMode context manager that converts user input and model parameters into torch._subclasses.fake_tensor.FakeTensor.", "A torch._subclasses.fake_tensor.FakeTensor is a torch.Tensor with the ability to run PyTorch code without having to actually do computation through tensors allocated on a meta device. Because there is no actual data being allocated on the device, this API allows for exporting large models without the actual memory footprint needed for executing it.", "It is highly recommended to enable fake mode when exporting models that are too large to fit into memory.", "A ONNXFakeContext object that must be passed to dynamo_export() through the ExportOptions.fake_context argument.", "Example:", "Warning", "This API is experimental and is NOT backward-compatible.", "An in-memory representation of a PyTorch model that has been exported to ONNX.", "Converts the PyTorch model inputs to exported ONNX model inputs format.", "Due to design differences, input/output format between PyTorch model and exported ONNX model are often not the same. E.g., None is allowed for PyTorch model, but are not supported by ONNX. Nested constructs of tensors are allowed for PyTorch model, but only flattened tensors are supported by ONNX, etc.", "The actual adapting steps are associated with each individual export. It depends on the PyTorch model, the particular set of model_args and model_kwargs used for the export, and export options.", "This method replays the adapting steps recorded during export.", "A sequence of tensors converted from PyTorch model inputs.", "Sequence[Union[Tensor, int, float, bool]]", "Example:", "Warning", "This API is experimental and is NOT backward-compatible.", "Converts the PyTorch model outputs to exported ONNX model outputs format.", "Due to design differences, input/output format between PyTorch model and exported ONNX model are often not the same. E.g., None is allowed for PyTorch model, but are not supported by ONNX. Nested constructs of tensors are allowed for PyTorch model, but only flattened tensors are supported by ONNX, etc.", "The actual adapting steps are associated with each individual export. It depends on the PyTorch model, the particular set of model_args and model_kwargs used for the export, and export options.", "This method replays the adapting steps recorded during export.", "model_outputs (Any) \u2013 The PyTorch model outputs.", "PyTorch model outputs in exported ONNX model outputs format.", "Sequence[Union[Tensor, int, float, bool]]", "Example:", "Warning", "This API is experimental and is NOT backward-compatible.", "The diagnostic context associated with the export.", "The fake context associated with the export.", "The exported ONNX model as an onnx.ModelProto.", "Saves the in-memory ONNX model to destination using specified serializer.", "Saves the export diagnostics as a SARIF log to the specified destination path.", "destination (str) \u2013 The destination to save the diagnostics SARIF log. It must have a .sarif extension.", "ValueError \u2013 If the destination path does not end with .sarif extension.", "Protocol for serializing an ONNX graph into a specific format (e.g. Protobuf). Note that this is an advanced usage scenario.", "Protocol method that must be implemented for serialization.", "A simple serializer that writes the exported onnx.ModelProto in Protobuf format to destination:", "Raised when an ONNX exporter error occurs.", "This exception is thrown when there\u2019s an error during the ONNX export process. It encapsulates the ExportOutput object generated until the failure, allowing access to the partial export results and associated metadata.", "Registry for ONNX functions.", "The registry maintains a mapping from qualified names to symbolic functions under a fixed opset version. It supports registering custom onnx-script functions and for dispatcher to dispatch calls to the appropriate function.", "Returns a list of ONNXFunctions for the given op: torch.ops.<namespace>.<op_name>.<overload>.", "The list is ordered by the time of registration. The custom operators should be in the second half of the list.", "A list of ONNXFunctions corresponding to the given name, or None if the name is not in the registry.", "Optional[List[ONNXFunction]]", "Returns whether the given op is registered: torch.ops.<namespace>.<op_name>.<overload>.", "True if the given op is registered, otherwise False.", "bool", "The ONNX opset version the exporter should target. Defaults to the latest supported ONNX opset version: 18. The default version will increment over time as ONNX continues to evolve.", "Registers a custom operator: torch.ops.<namespace>.<op_name>.<overload>.", "ValueError \u2013 If the name is not in the form of \u2018namespace::op\u2019.", "Options for diagnostic context."]}, {"name": "torch.onnx.TorchDynamo-based ONNX Exporter.FXE0007:fx-graph-to-onnx", "path": "generated/onnx_dynamo_diagnostics_rules/fxe0007%3afx-graph-to-onnx", "type": "ONNX", "text": ["This diagnostic tracks the transformation process from an FX Graph (in FX IR) to an ONNX Graph (in ONNX IR).", "To enable a detailed view of the graph transformation in progress within this diagnostic, switch to the DEBUG mode."]}, {"name": "torch.onnx.TorchDynamo-based ONNX Exporter.FXE0008:fx-node-to-onnx", "path": "generated/onnx_dynamo_diagnostics_rules/fxe0008%3afx-node-to-onnx", "type": "ONNX", "text": ["This diagnostic tracks the transformation process from an FX Node to ONNX Operators.", "The process of converting FX Node to ONNX Node involves dealing with six distinct node types:", "For a granular understanding of how each node type is transformed, refer to the implementation details in FxOnnxInterpreter."]}, {"name": "torch.onnx.TorchDynamo-based ONNX Exporter.FXE0010:fx-pass", "path": "generated/onnx_dynamo_diagnostics_rules/fxe0010%3afx-pass", "type": "ONNX", "text": ["This diagnostic tracks the FX passes executed during the ONNX export process prior to converting from FX IR (Intermediate Representation) to ONNX IR.", "Under the scope of ONNX export, an FX pass refers to a specific transformation applied to the FX GraphModule. The primary aim of these passes is to streamline the graph into a format that aligns more with the ONNX IR. Moreover, these passes work to substitute unsupported FX IR features with those recognized and endorsed by ONNX IR. Common transformations include, but aren\u2019t limited to, decomposition, functionalization and type promotion.", "For those who are interested in a comprehensive log detailing the modifications made during these passes, there are a couple of options:", "However, it\u2019s noteworthy that by default, such detailed logging is turned off. The primary reason being its considerable impact on performance.", "For an in-depth understanding of each specific pass, please refer to the directory: torch/onnx/_internal/fx/passes."]}, {"name": "torch.onnx.TorchDynamo-based ONNX Exporter.FXE0011:no-symbolic-function-for-call-function", "path": "generated/onnx_dynamo_diagnostics_rules/fxe0011%3ano-symbolic-function-for-call-function", "type": "ONNX", "text": ["This error occurs when the ONNX converter is unable to find a corresponding symbolic function to convert a \u201ccall_function\u201d node in the input graph to its equivalence in ONNX. The \u201ccall_function\u201d node represents a normalized function call in PyTorch, such as \u201ctorch.aten.ops.add\u201d.", "To resolve this error, you can try one of the following:", "TODO: Replace above link once docs for dynamo_export custom op registration are available."]}, {"name": "torch.onnx.TorchDynamo-based ONNX Exporter.FXE0012:unsupported-fx-node-analysis", "path": "generated/onnx_dynamo_diagnostics_rules/fxe0012%3aunsupported-fx-node-analysis", "type": "ONNX", "text": ["This error indicates that an FX graph contains one or more unsupported nodes. The error message is typically accompanied by a list of the unsupported nodes found during analysis.", "To resolve this error, you can try resolving each individual unsupported node error by following the suggestions by its diagnostic. Typically, options include:"]}, {"name": "torch.onnx.TorchDynamo-based ONNX Exporter.FXE0013:op-level-debugging", "path": "generated/onnx_dynamo_diagnostics_rules/fxe0013%3aop-level-debugging", "type": "ONNX", "text": ["This warning message indicates that during op level debugging, certain symbolic functions have failed to match the results of torch ops when using real tensors generated from fake tensors. It is important to note that the symbolic functions may not necessarily be incorrect, as the validation process is non-deterministic and should only be used as a reference.", "There are two categories of warnings that can be triggered:"]}, {"name": "torch.onnx.TorchDynamo-based ONNX Exporter.FXE0014:find-opschema-matched-symbolic-function", "path": "generated/onnx_dynamo_diagnostics_rules/fxe0014%3afind-opschema-matched-symbolic-function", "type": "ONNX", "text": ["When an ATen/Custom operator is registered and needs to be dispatched to an OnnxFunction, the input/attribute dtypes of the ATen/Custom operator are compared with the input/attribute dtypes of the OnnxFunction opschemas to find a match. However, if a perfect/exact match is not found, the dispatcher will attempt to find the nearest match with the highest number of input/attribute dtypes matching the OnnxFunction opschemas, while issuing a warning.", "There are two types of level that can be triggered in this rule:", "Here are some suggestions based on the WARNING situation:"]}, {"name": "torch.onnx.TorchDynamo-based ONNX Exporter.FXE0015:fx-node-insert-type-promotion", "path": "generated/onnx_dynamo_diagnostics_rules/fxe0015%3afx-node-insert-type-promotion", "type": "ONNX", "text": ["This diagnostic monitors the node-level type promotion insertion process. In PyTorch, there is an automatic process called implicit type promotion, where the input types of an operator are promoted to a common type. The determination of the common type is based on the type promotion rule specific to each operator. To learn more about PyTorch\u2019s type promotion rules, refer to the elementwise_dtypes doc and torch._refs ops.", "However, implicit type promotion is not supported in ONNX. Therefore, to replicate the PyTorch behavior, we need to explicitly insert cast nodes. This diagnostic tracks the process of node-level type promotion insertion.", "The type promotion rules used by this process can be found in torch/onnx/_internal/fx/passes/type_promotion.py. To update or add new type promotion rules, please refer to the [Note: Update type promotion rule] section."]}, {"name": "torch.onnx.TorchDynamo-based ONNX Exporter.FXE0016:find-operator-overloads-in-onnx-registry", "path": "generated/onnx_dynamo_diagnostics_rules/fxe0016%3afind-operator-overloads-in-onnx-registry", "type": "ONNX", "text": ["The operator overload name serves the purpose of verifying whether a PyTorch operator is registered in the ONNX registry. If it\u2019s not found, the dispatcher takes a fallback approach and tries to locate the default overload of the PyTorch operator in the registry. If even the default overload is absent, it signifies that the operator is officially unsupported.", "There are three types of level that can be triggered in this rule:", "Here are some suggestions based on the WARNING situation:", "Here are some suggestions based on the ERROR situation:"]}, {"name": "torch.onnx.TorchScript-based ONNX Exporter", "path": "onnx_torchscript", "type": "ONNX", "text": ["Note", "To export an ONNX model using TorchDynamo instead of TorchScript, see torch.onnx.dynamo_export().", "Avoiding Pitfalls", "Limitations", "Unsupported Tensor Indexing Patterns", "Adding support for operators", "ATen operators", "torch.autograd.Functions", "Custom operators", "Python API", "Here is a simple script which exports a pretrained AlexNet to an ONNX file named alexnet.onnx. The call to torch.onnx.export runs the model once to trace its execution and then exports the traced model to the specified file:", "The resulting alexnet.onnx file contains a binary protocol buffer which contains both the network structure and parameters of the model you exported (in this case, AlexNet). The argument verbose=True causes the exporter to print out a human-readable representation of the model:", "You can also verify the output using the ONNX library, which you can install using pip:", "Then, you can run:", "You can also run the exported model with one of the many runtimes that support ONNX. For example after installing ONNX Runtime, you can load and run the model:", "Here is a more involved tutorial on exporting a model and running it with ONNX Runtime.", "Internally, torch.onnx.export() requires a torch.jit.ScriptModule rather than a torch.nn.Module. If the passed-in model is not already a ScriptModule, export() will use tracing to convert it to one:", "Scripting: Compiling a model via scripting preserves dynamic control flow and is valid for inputs of different sizes. To use scripting:", "See Introduction to TorchScript and TorchScript for more details, including how to compose tracing and scripting to suit the particular requirements of different models.", "PyTorch models can be written using NumPy or Python types and functions, but during tracing, any variables of NumPy or Python types (rather than torch.Tensor) are converted to constants, which will produce the wrong result if those values should change depending on the inputs.", "For example, rather than using numpy functions on numpy.ndarrays:", "Use torch operators on torch.Tensors:", "And rather than use torch.Tensor.item() (which converts a Tensor to a Python built-in number):", "Use torch\u2019s support for implicit casting of single-element tensors:", "Using the Tensor.data field can produce an incorrect trace and therefore an incorrect ONNX graph. Use torch.Tensor.detach() instead. (Work is ongoing to remove Tensor.data entirely).", "In tracing mode, shapes obtained from tensor.shape are traced as tensors, and share the same memory. This might cause a mismatch the final output values. As a workaround, avoid the use of inplace operations in these scenarios. For example, in the model:", "real_seq_length and seq_length share the same memory in tracing mode. This could be avoided by rewriting the inplace operation:", "Only torch.Tensors, numeric types that can be trivially converted to torch.Tensors (e.g. float, int), and tuples and lists of those types are supported as model inputs or outputs. Dict and str inputs and outputs are accepted in tracing mode, but:", "Due to differences in implementations of operators, running the exported model on different runtimes may produce different results from each other or from PyTorch. Normally these differences are numerically small, so this should only be a concern if your application is sensitive to these small differences.", "Tensor indexing patterns that cannot be exported are listed below. If you are experiencing issues exporting a model that does not include any of the unsupported patterns below, please double check that you are exporting with the latest opset_version.", "When indexing into a tensor for reading, the following patterns are not supported:", "When indexing into a Tensor for writing, the following patterns are not supported:", "When exporting a model that includes unsupported operators, you\u2019ll see an error message like:", "When that happens, there are a few things you can do:", "If you decided to implement a symbolic function (we hope you will contribute it back to PyTorch!), here is how you can get started:", "A \u201csymbolic function\u201d is a function that decomposes a PyTorch operator into a composition of a series of ONNX operators.", "During export, each node (which contains a PyTorch operator) in the TorchScript graph is visited by the exporter in topological order. Upon visiting a node, the exporter looks for a registered symbolic functions for that operator. Symbolic functions are implemented in Python. A symbolic function for an op named foo would look something like:", "The torch._C types are Python wrappers around the types defined in C++ in ir.h.", "The process for adding a symbolic function depends on the type of operator.", "ATen is PyTorch\u2019s built-in tensor library. If the operator is an ATen operator (shows up in the TorchScript graph with the prefix aten::), make sure it is not supported already.", "Visit the auto generated list of supported TorchScript operators for details on which operator are supported in each opset_version.", "If the operator is not in the list above:", "Here is an example of handling missing symbolic function for the ELU operator.", "If we run the following code:", "We see something like:", "Since we see aten::elu in the graph, we know this is an ATen operator.", "We check the ONNX operator list, and confirm that Elu is standardized in ONNX.", "We find a signature for elu in torch/nn/functional.pyi:", "We add the following lines to symbolic_opset9.py:", "Now PyTorch is able to export models containing the aten::elu operator!", "See the torch/onnx/symbolic_opset*.py files for more examples.", "If the operator is a sub-class of torch.autograd.Function, there are three ways to export it.", "You can add a static method named symbolic to your function class. It should return ONNX operators that represent the function\u2019s behavior in ONNX. For example:", "In cases where a static symbolic method is not provided for its subsequent torch.autograd.Function or where a function to register prim::PythonOp as custom symbolic functions is not provided, torch.onnx.export() tries to inline the graph that corresponds to that torch.autograd.Function such that this function is broken down into individual operators that were used within the function. The export should be successful as long as these individual operators are supported. For example:", "There is no static symbolic method present for this model, yet it is exported as follows:", "If you need to avoid inlining of torch.autograd.Function, you should export models with operator_export_type set to ONNX_FALLTHROUGH or ONNX_ATEN_FALLBACK.", "You can export your model with custom operators that includes a combination of many standard ONNX ops, or are driven by self-defined C++ backend.", "If an operator is not a standard ONNX op, but can be composed of multiple existing ONNX ops, you can utilize ONNX-script to create an external ONNX function to support the operator. You can export it by following this example:", "The example above exports it as a custom operator in the \u201connx-script\u201d opset. When exporting a custom operator, you can specify the custom domain version using the custom_opsets dictionary at export. If not specified, the custom opset version defaults to 1.", "NOTE: Be careful to align the opset version mentioned in the above example, and make sure they are consumed in exporter step. The example usage of how to write a onnx-script function is a beta version in terms of the active development on onnx-script. Please follow the latest ONNX-script", "If a model uses a custom operator implemented in C++ as described in Extending TorchScript with Custom C++ Operators, you can export it by following this example:", "The example above exports it as a custom operator in the \u201ccustom_domain\u201d opset. When exporting a custom operator, you can specify the custom domain version using the custom_opsets dictionary at export. If not specified, the custom opset version defaults to 1.", "The runtime that consumes the model needs to support the custom op. See Caffe2 custom ops, ONNX Runtime custom ops, or your runtime of choice\u2019s documentation.", "When export fails due to an unconvertible ATen op, there may in fact be more than one such op but the error message only mentions the first. To discover all of the unconvertible ops in one go you can:", "The set is approximated because some ops may be removed during the conversion process and don\u2019t need to be converted. Some other ops may have partial support that will fail conversion with particular inputs, but this should give you a general idea of what ops are not supported. Please feel free to open GitHub Issues for op support requests.", "Q: I have exported my LSTM model, but its input size seems to be fixed?", "The tracer records the shapes of the example inputs. If the model should accept inputs of dynamic shapes, set dynamic_axes when calling torch.onnx.export().", "Q: How to export models containing loops?", "See Tracing vs Scripting.", "Q: How to export models with primitive type inputs (e.g. int, float)?", "Support for primitive numeric type inputs was added in PyTorch 1.9. However, the exporter does not support models with str inputs.", "Q: Does ONNX support implicit scalar datatype casting?", "The ONNX standard does not, but the exporter will try to handle that part. Scalars are exported as constant tensors. The exporter will figure out the right data type for scalars. In rare cases when it is unable to do so, you will need to manually specify the datatype with e.g. dtype=torch.float32. If you see any errors, please [create a GitHub issue](https://github.com/pytorch/pytorch/issues).", "Q: Are lists of Tensors exportable to ONNX?", "Yes, for opset_version >= 11, since ONNX introduced the Sequence type in opset 11.", "Exports a model into ONNX format.", "If model is not a torch.jit.ScriptModule nor a torch.jit.ScriptFunction, this runs model once in order to convert it to a TorchScript graph to be exported (the equivalent of torch.jit.trace()). Thus this has the same limited support for dynamic control flow as torch.jit.trace().", "args (tuple or torch.Tensor) \u2013 ", "args can be structured either as:", "ONLY A TUPLE OF ARGUMENTS:", "The tuple should contain model inputs such that model(*args) is a valid invocation of the model. Any non-Tensor arguments will be hard-coded into the exported model; any Tensor arguments will become inputs of the exported model, in the order they occur in the tuple.", "A TENSOR:", "This is equivalent to a 1-ary tuple of that Tensor.", "A TUPLE OF ARGUMENTS ENDING WITH A DICTIONARY OF NAMED ARGUMENTS:", "All but the last element of the tuple will be passed as non-keyword arguments, and named arguments will be set from the last element. If a named argument is not present in the dictionary, it is assigned the default value, or None if a default value is not provided.", "Note", "If a dictionary is the last element of the args tuple, it will be interpreted as containing named arguments. In order to pass a dict as the last non-keyword arg, provide an empty dict as the last element of the args tuple. For example, instead of:", "Write:", "training (enum, default TrainingMode.EVAL) \u2013 ", "False and in training mode if model.training is True.", "which might interfere with training.", "operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013 ", "(in the default opset domain).", "to standard ONNX ops in the default opset domain. If unable to do so (e.g. because support has not been added to convert a particular torch op to ONNX), fall back to exporting the op into a custom opset domain without conversion. Applies to custom ops as well as ATen ops. For the exported model to be usable, the runtime must support these non-standard ops.", "are exported as ATen ops (in opset domain \u201corg.pytorch.aten\u201d). ATen is PyTorch\u2019s built-in tensor library, so this instructs the runtime to use PyTorch\u2019s implementation of these ops.", "Warning", "Models exported this way are probably runnable only by Caffe2.", "This may be useful if the numeric differences in implementations of operators are causing large differences in behavior between PyTorch and Caffe2 (which is more common on untrained models).", "(in the TorchScript namespace \u201caten\u201d) as a regular ONNX op. If we are unable to do so (e.g. because support has not been added to convert a particular torch op to ONNX), fall back to exporting an ATen op. See documentation on OperatorExportTypes.ONNX_ATEN for context. For example:", "Assuming aten::triu is not supported in ONNX, this will be exported as:", "If PyTorch was built with Caffe2 (i.e. with BUILD_CAFFE2=1), then Caffe2-specific behavior will be enabled, including special support for ops are produced by the modules described in Quantization.", "Warning", "Models exported this way are probably runnable only by Caffe2.", "dynamic_axes (dict[string, dict[int, string]] or dict[string, list(int)], default empty dict) \u2013 ", "By default the exported model will have the shapes of all input and output tensors set to exactly match those given in args. To specify axes of tensors as dynamic (i.e. known only at run-time), set dynamic_axes to a dict with schema:", "output_names.", "list, each element is an axis index.", "For example:", "Produces:", "While:", "Produces:", "keep_initializers_as_inputs (bool, default None) \u2013 ", "If True, all the initializers (typically corresponding to parameters) in the exported graph will also be added as inputs to the graph. If False, then initializers are not added as inputs to the graph, and only the non-parameter inputs are added as inputs. This may allow for better optimizations (e.g. constant folding) by backends/runtimes.", "If True, deduplicate_initializers pass will not be executed. This means initializers with duplicated values will not be deduplicated and will be treated as distinct inputs to the graph. This allows different input initializers to be supplied at the runtime following export.", "If opset_version < 9, initializers MUST be part of graph inputs and this argument will be ignored and the behavior will be equivalent to setting this argument to True.", "If None, then the behavior is chosen automatically as follows:", "to setting this argument to False.", "custom_opsets (dict[str, int], default empty dict) \u2013 ", "A dict with schema:", "If a custom opset is referenced by model but not mentioned in this dictionary, the opset version is set to 1. Only custom opset domain name and version should be indicated through this argument.", "export_modules_as_functions (bool or set of type of nn.Module, default False) \u2013 ", "Flag to enable exporting all nn.Module forward calls as local functions in ONNX. Or a set to indicate the particular types of modules to export as local functions in ONNX. This feature requires opset_version >= 15, otherwise the export will fail. This is because opset_version < 15 implies IR version < 8, which means no local function support. Module variables will be exported as function attributes. There are two categories of function attributes.", "1. Annotated attributes: class variables that have type annotations via PEP 526-style will be exported as attributes. Annotated attributes are not used inside the subgraph of ONNX local function because they are not created by PyTorch JIT tracing, but they may be used by consumers to determine whether or not to replace the function with a particular fused kernel.", "2. Inferred attributes: variables that are used by operators inside the module. Attribute names will have prefix \u201cinferred::\u201d. This is to differentiate from predefined attributes retrieved from python module annotations. Inferred attributes are used inside the subgraph of ONNX local function.", "only if the type of the nn.Module is found in the set.", "Similar to export(), but returns a text representation of the ONNX model. Only differences in args listed below. All other args are the same as export().", "A UTF-8 str containing a human-readable representation of the ONNX model.", "Registers a symbolic function for a custom operator.", "When the user registers symbolic for custom/contrib ops, it is highly recommended to add shape inference for that operator via setType API, otherwise the exported graph may have incorrect shape inference in some extreme cases. An example of setType is test_aten_embedding_2 in test_operators.py.", "See \u201cCustom Operators\u201d in the module documentation for an example usage.", "Unregisters symbolic_name.", "See \u201cCustom Operators\u201d in the module documentation for an example usage.", "A context manager to temporarily set the training mode of model to mode, resetting it when we exit the with-block.", "Returns whether it is in the middle of ONNX export.", "bool", "Enables ONNX logging.", "Disables ONNX logging.", "Find all mismatches between the original model and the exported model.", "Experimental. The API is subject to change.", "This tool helps debug the mismatch between the original PyTorch model and exported ONNX model. It binary searches the model graph to find the minimal subgraph that exhibits the mismatch.", "A GraphInfo object that contains the mismatch information.", "Example:", "Scalar types defined in torch.", "GraphInfo contains validation information of a TorchScript graph and its converted ONNX graph.", "Options for ONNX export verification."]}, {"name": "torch.onnx.TorchScript-based ONNX Exporter.GraphInfo", "path": "generated/torch.onnx.verification.graphinfo", "type": "ONNX", "text": ["GraphInfo contains validation information of a TorchScript graph and its converted ONNX graph.", "Return a list of all leaf GraphInfo objects that have mismatch.", "List[GraphInfo]", "Clear states and results of previous verification.", "Return the number of nodes in the subgraph excluding those in _EXCLUDED_NODE_KINDS.", "int", "Return the set of node kinds in the subgraph excluding those in _EXCLUDED_NODE_KINDS.", "Set[str]", "Export the subgraph to ONNX along with the input/output data for repro.", "The repro directory will contain the following files:", "The path to the exported repro directory.", "str", "Find all mismatches between the TorchScript IR graph and the exported onnx model.", "Binary searches the model graph to find the minimal subgraph that exhibits the mismatch. A GraphInfo object is created for each subgraph, recording the test inputs and export options, as well as the validation results.", "options (Optional[VerificationOptions]) \u2013 The verification options.", "Find the GraphInfo object with the given id.", "Optional[GraphInfo]", "Return True if the subgraph has output mismatch between torch and ONNX.", "bool", "Pretty print details of the mismatch between torch and ONNX.", "graph (bool) \u2013 If True, print the ATen JIT graph and ONNX graph.", "Pretty print GraphInfo tree.", "Each node represents a subgraph, showing the number of nodes in the subgraph and a check mark if the subgraph has output mismatch between torch and ONNX.", "The id of the subgraph is shown under the node. The GraphInfo object for any subgraph can be retrieved by calling graph_info.find_partition(id).", "Example:", "Verify the export from TorchScript IR graph to ONNX.", "Export the TorchScript IR graph to ONNX, with the inputs, parameters and export options recorded in this object. Then verify the exported ONNX graph against the original TorchScript IR graph under the provided verification options.", "options (VerificationOptions) \u2013 The verification options.", "The AssertionError raised during the verification. Returns None if no error is raised. onnx_graph: The exported ONNX graph in TorchScript IR format. onnx_outs: The outputs from running exported ONNX model under the onnx backend in options. pt_outs: The outputs from running the TorchScript IR graph.", "error"]}, {"name": "torch.onnx.TorchScript-based ONNX Exporter.JitScalarType", "path": "generated/torch.onnx.jitscalartype", "type": "ONNX", "text": ["Scalar types defined in torch.", "Use JitScalarType to convert from torch and JIT scalar types to ONNX scalar types.", "Convert a JitScalarType to a torch dtype.", "dtype", "Convert a torch dtype to JitScalarType.", "A \u201cRuntimeError: INTERNAL ASSERT FAILED at \u201c../aten/src/ATen/core/jit_type_base.h\u201d can be raised in several scenarios where shape info is not present. Instead use from_value API which is safer.", "dtype (Optional[dtype]) \u2013 A torch.dtype to create a JitScalarType from", "JitScalarType", "OnnxExporterError \u2013 if dtype is not a valid torch.dtype or if it is None.", "JitScalarType", "Create a JitScalarType from an value\u2019s scalar type.", "JitScalarType.", "JitScalarType", "Return whether this JitScalarType is compatible with ONNX.", "bool", "Convert a JitScalarType to an ONNX data type.", "TensorProtoDataType", "Convert a JitScalarType to a JIT scalar type name.", "Literal[\u2018Byte\u2019, \u2018Char\u2019, \u2018Double\u2019, \u2018Float\u2019, \u2018Half\u2019, \u2018Int\u2019, \u2018Long\u2019, \u2018Short\u2019, \u2018Bool\u2019, \u2018ComplexHalf\u2019, \u2018ComplexFloat\u2019, \u2018ComplexDouble\u2019, \u2018QInt8\u2019, \u2018QUInt8\u2019, \u2018QInt32\u2019, \u2018BFloat16\u2019, \u2018Float8E5M2\u2019, \u2018Float8E4M3FN\u2019, \u2018Undefined\u2019]", "Convert a JitScalarType to a torch type name.", "Literal[\u2018bool\u2019, \u2018uint8_t\u2019, \u2018int8_t\u2019, \u2018double\u2019, \u2018float\u2019, \u2018half\u2019, \u2018int\u2019, \u2018int64_t\u2019, \u2018int16_t\u2019, \u2018complex32\u2019, \u2018complex64\u2019, \u2018complex128\u2019, \u2018qint8\u2019, \u2018quint8\u2019, \u2018qint32\u2019, \u2018bfloat16\u2019, \u2018float8_e5m2\u2019, \u2018float8_e4m3fn\u2019]"]}, {"name": "torch.onnx.TorchScript-based ONNX Exporter.VerificationOptions", "path": "generated/torch.onnx.verification.verificationoptions", "type": "ONNX", "text": ["Options for ONNX export verification."]}, {"name": "torch.onnx.unregister_custom_op_symbolic()", "path": "onnx_torchscript#torch.onnx.unregister_custom_op_symbolic", "type": "ONNX", "text": ["Unregisters symbolic_name.", "See \u201cCustom Operators\u201d in the module documentation for an example usage."]}, {"name": "torch.onnx.verification.find_mismatch()", "path": "onnx_torchscript#torch.onnx.verification.find_mismatch", "type": "ONNX", "text": ["Find all mismatches between the original model and the exported model.", "Experimental. The API is subject to change.", "This tool helps debug the mismatch between the original PyTorch model and exported ONNX model. It binary searches the model graph to find the minimal subgraph that exhibits the mismatch.", "A GraphInfo object that contains the mismatch information.", "Example:"]}, {"name": "torch.onnx.verification.GraphInfo", "path": "generated/torch.onnx.verification.graphinfo#torch.onnx.verification.GraphInfo", "type": "ONNX", "text": ["GraphInfo contains validation information of a TorchScript graph and its converted ONNX graph.", "Return a list of all leaf GraphInfo objects that have mismatch.", "List[GraphInfo]", "Clear states and results of previous verification.", "Return the number of nodes in the subgraph excluding those in _EXCLUDED_NODE_KINDS.", "int", "Return the set of node kinds in the subgraph excluding those in _EXCLUDED_NODE_KINDS.", "Set[str]", "Export the subgraph to ONNX along with the input/output data for repro.", "The repro directory will contain the following files:", "The path to the exported repro directory.", "str", "Find all mismatches between the TorchScript IR graph and the exported onnx model.", "Binary searches the model graph to find the minimal subgraph that exhibits the mismatch. A GraphInfo object is created for each subgraph, recording the test inputs and export options, as well as the validation results.", "options (Optional[VerificationOptions]) \u2013 The verification options.", "Find the GraphInfo object with the given id.", "Optional[GraphInfo]", "Return True if the subgraph has output mismatch between torch and ONNX.", "bool", "Pretty print details of the mismatch between torch and ONNX.", "graph (bool) \u2013 If True, print the ATen JIT graph and ONNX graph.", "Pretty print GraphInfo tree.", "Each node represents a subgraph, showing the number of nodes in the subgraph and a check mark if the subgraph has output mismatch between torch and ONNX.", "The id of the subgraph is shown under the node. The GraphInfo object for any subgraph can be retrieved by calling graph_info.find_partition(id).", "Example:", "Verify the export from TorchScript IR graph to ONNX.", "Export the TorchScript IR graph to ONNX, with the inputs, parameters and export options recorded in this object. Then verify the exported ONNX graph against the original TorchScript IR graph under the provided verification options.", "options (VerificationOptions) \u2013 The verification options.", "The AssertionError raised during the verification. Returns None if no error is raised. onnx_graph: The exported ONNX graph in TorchScript IR format. onnx_outs: The outputs from running exported ONNX model under the onnx backend in options. pt_outs: The outputs from running the TorchScript IR graph.", "error"]}, {"name": "torch.onnx.verification.GraphInfo.all_mismatch_leaf_graph_info()", "path": "generated/torch.onnx.verification.graphinfo#torch.onnx.verification.GraphInfo.all_mismatch_leaf_graph_info", "type": "ONNX", "text": ["Return a list of all leaf GraphInfo objects that have mismatch.", "List[GraphInfo]"]}, {"name": "torch.onnx.verification.GraphInfo.clear()", "path": "generated/torch.onnx.verification.graphinfo#torch.onnx.verification.GraphInfo.clear", "type": "ONNX", "text": ["Clear states and results of previous verification."]}, {"name": "torch.onnx.verification.GraphInfo.essential_node_count()", "path": "generated/torch.onnx.verification.graphinfo#torch.onnx.verification.GraphInfo.essential_node_count", "type": "ONNX", "text": ["Return the number of nodes in the subgraph excluding those in _EXCLUDED_NODE_KINDS.", "int"]}, {"name": "torch.onnx.verification.GraphInfo.essential_node_kinds()", "path": "generated/torch.onnx.verification.graphinfo#torch.onnx.verification.GraphInfo.essential_node_kinds", "type": "ONNX", "text": ["Return the set of node kinds in the subgraph excluding those in _EXCLUDED_NODE_KINDS.", "Set[str]"]}, {"name": "torch.onnx.verification.GraphInfo.export_repro()", "path": "generated/torch.onnx.verification.graphinfo#torch.onnx.verification.GraphInfo.export_repro", "type": "ONNX", "text": ["Export the subgraph to ONNX along with the input/output data for repro.", "The repro directory will contain the following files:", "The path to the exported repro directory.", "str"]}, {"name": "torch.onnx.verification.GraphInfo.find_mismatch()", "path": "generated/torch.onnx.verification.graphinfo#torch.onnx.verification.GraphInfo.find_mismatch", "type": "ONNX", "text": ["Find all mismatches between the TorchScript IR graph and the exported onnx model.", "Binary searches the model graph to find the minimal subgraph that exhibits the mismatch. A GraphInfo object is created for each subgraph, recording the test inputs and export options, as well as the validation results.", "options (Optional[VerificationOptions]) \u2013 The verification options."]}, {"name": "torch.onnx.verification.GraphInfo.find_partition()", "path": "generated/torch.onnx.verification.graphinfo#torch.onnx.verification.GraphInfo.find_partition", "type": "ONNX", "text": ["Find the GraphInfo object with the given id.", "Optional[GraphInfo]"]}, {"name": "torch.onnx.verification.GraphInfo.has_mismatch()", "path": "generated/torch.onnx.verification.graphinfo#torch.onnx.verification.GraphInfo.has_mismatch", "type": "ONNX", "text": ["Return True if the subgraph has output mismatch between torch and ONNX.", "bool"]}, {"name": "torch.onnx.verification.GraphInfo.pretty_print_mismatch()", "path": "generated/torch.onnx.verification.graphinfo#torch.onnx.verification.GraphInfo.pretty_print_mismatch", "type": "ONNX", "text": ["Pretty print details of the mismatch between torch and ONNX.", "graph (bool) \u2013 If True, print the ATen JIT graph and ONNX graph."]}, {"name": "torch.onnx.verification.GraphInfo.pretty_print_tree()", "path": "generated/torch.onnx.verification.graphinfo#torch.onnx.verification.GraphInfo.pretty_print_tree", "type": "ONNX", "text": ["Pretty print GraphInfo tree.", "Each node represents a subgraph, showing the number of nodes in the subgraph and a check mark if the subgraph has output mismatch between torch and ONNX.", "The id of the subgraph is shown under the node. The GraphInfo object for any subgraph can be retrieved by calling graph_info.find_partition(id).", "Example:"]}, {"name": "torch.onnx.verification.GraphInfo.verify_export()", "path": "generated/torch.onnx.verification.graphinfo#torch.onnx.verification.GraphInfo.verify_export", "type": "ONNX", "text": ["Verify the export from TorchScript IR graph to ONNX.", "Export the TorchScript IR graph to ONNX, with the inputs, parameters and export options recorded in this object. Then verify the exported ONNX graph against the original TorchScript IR graph under the provided verification options.", "options (VerificationOptions) \u2013 The verification options.", "The AssertionError raised during the verification. Returns None if no error is raised. onnx_graph: The exported ONNX graph in TorchScript IR format. onnx_outs: The outputs from running exported ONNX model under the onnx backend in options. pt_outs: The outputs from running the TorchScript IR graph.", "error"]}, {"name": "torch.onnx.verification.VerificationOptions", "path": "generated/torch.onnx.verification.verificationoptions#torch.onnx.verification.VerificationOptions", "type": "ONNX", "text": ["Options for ONNX export verification."]}, {"name": "torch.optim", "path": "optim", "type": "Optimization", "text": ["torch.optim is a package implementing various optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can also be easily integrated in the future.", "To use torch.optim you have to construct an optimizer object that will hold the current state and will update the parameters based on the computed gradients.", "To construct an Optimizer you have to give it an iterable containing the parameters (all should be Variable s) to optimize. Then, you can specify optimizer-specific options such as the learning rate, weight decay, etc.", "Example:", "Optimizer s also support specifying per-parameter options. To do this, instead of passing an iterable of Variable s, pass in an iterable of dict s. Each of them will define a separate parameter group, and should contain a params key, containing a list of parameters belonging to it. Other keys should match the keyword arguments accepted by the optimizers, and will be used as optimization options for this group.", "Note", "You can still pass options as keyword arguments. They will be used as defaults, in the groups that didn\u2019t override them. This is useful when you only want to vary a single option, while keeping all others consistent between parameter groups.", "For example, this is very useful when one wants to specify per-layer learning rates:", "This means that model.base\u2019s parameters will use the default learning rate of 1e-2, model.classifier\u2019s parameters will use a learning rate of 1e-3, and a momentum of 0.9 will be used for all parameters.", "All optimizers implement a step() method, that updates the parameters. It can be used in two ways:", "This is a simplified version supported by most optimizers. The function can be called once the gradients are computed using e.g. backward().", "Example:", "Some optimization algorithms such as Conjugate Gradient and LBFGS need to reevaluate the function multiple times, so you have to pass in a closure that allows them to recompute your model. The closure should clear the gradients, compute the loss, and return it.", "Example:", "Base class for all optimizers.", "Warning", "Parameters need to be specified as collections that have a deterministic ordering that is consistent between runs. Examples of objects that don\u2019t satisfy those properties are sets and iterators over values of dictionaries.", "Optimizer.add_param_group", "Add a param group to the Optimizer s param_groups.", "Optimizer.load_state_dict", "Loads the optimizer state.", "Optimizer.state_dict", "Returns the state of the optimizer as a dict.", "Optimizer.step", "Performs a single optimization step (parameter update).", "Optimizer.zero_grad", "Resets the gradients of all optimized torch.Tensor s.", "Implements Adadelta algorithm.", "Implements Adagrad algorithm.", "Implements Adam algorithm.", "Implements AdamW algorithm.", "SparseAdam implements a masked version of the Adam algorithm suitable for sparse gradients.", "Implements Adamax algorithm (a variant of Adam based on infinity norm).", "Implements Averaged Stochastic Gradient Descent.", "Implements L-BFGS algorithm, heavily inspired by minFunc.", "Implements NAdam algorithm.", "Implements RAdam algorithm.", "Implements RMSprop algorithm.", "Implements the resilient backpropagation algorithm.", "Implements stochastic gradient descent (optionally with momentum).", "Many of our algorithms have various implementations optimized for performance, readability and/or generality, so we attempt to default to the generally fastest implementation for the current device if no particular implementation has been specified by the user.", "We have 3 major categories of implementations: for-loop, foreach (multi-tensor), and fused. The most straightforward implementations are for-loops over the parameters with big chunks of computation. For-looping is usually slower than our foreach implementations, which combine parameters into a multi-tensor and run the big chunks of computation all at once, thereby saving many sequential kernel calls. A few of our optimizers have even faster fused implementations, which fuse the big chunks of computation into one kernel. We can think of foreach implementations as fusing horizontally and fused implementations as fusing vertically on top of that.", "In general, the performance ordering of the 3 implementations is fused > foreach > for-loop. So when applicable, we default to foreach over for-loop. Applicable means the foreach implementation is available, the user has not specified any implementation-specific kwargs (e.g., fused, foreach, differentiable), and all tensors are native and on CUDA. Note that while fused should be even faster than foreach, the implementations are newer and we would like to give them more bake-in time before flipping the switch everywhere. You are welcome to try them out though!", "Below is a table showing the available and default implementations of each algorithm:", "Algorithm", "Default", "Has foreach?", "Has fused?", "foreach", "yes", "no", "foreach", "yes", "no", "foreach", "yes", "yes", "foreach", "yes", "yes", "for-loop", "no", "no", "foreach", "yes", "no", "foreach", "yes", "no", "for-loop", "no", "no", "foreach", "yes", "no", "foreach", "yes", "no", "foreach", "yes", "no", "foreach", "yes", "no", "foreach", "yes", "no", "torch.optim.lr_scheduler provides several methods to adjust the learning rate based on the number of epochs. torch.optim.lr_scheduler.ReduceLROnPlateau allows dynamic learning rate reducing based on some validation measurements.", "Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you should write your code this way:", "Example:", "Most learning rate schedulers can be called back-to-back (also referred to as chaining schedulers). The result is that each scheduler is applied one after the other on the learning rate obtained by the one preceding it.", "Example:", "In many places in the documentation, we will use the following template to refer to schedulers algorithms.", "Warning", "Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before the optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way. If you use the learning rate scheduler (calling scheduler.step()) before the optimizer\u2019s update (calling optimizer.step()), this will skip the first value of the learning rate schedule. If you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check if you are calling scheduler.step() at the wrong time.", "lr_scheduler.LambdaLR", "Sets the learning rate of each parameter group to the initial lr times a given function.", "lr_scheduler.MultiplicativeLR", "Multiply the learning rate of each parameter group by the factor given in the specified function.", "lr_scheduler.StepLR", "Decays the learning rate of each parameter group by gamma every step_size epochs.", "lr_scheduler.MultiStepLR", "Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones.", "lr_scheduler.ConstantLR", "Decays the learning rate of each parameter group by a small constant factor until the number of epoch reaches a pre-defined milestone: total_iters.", "lr_scheduler.LinearLR", "Decays the learning rate of each parameter group by linearly changing small multiplicative factor until the number of epoch reaches a pre-defined milestone: total_iters.", "lr_scheduler.ExponentialLR", "Decays the learning rate of each parameter group by gamma every epoch.", "lr_scheduler.PolynomialLR", "Decays the learning rate of each parameter group using a polynomial function in the given total_iters.", "lr_scheduler.CosineAnnealingLR", "Set the learning rate of each parameter group using a cosine annealing schedule, where \u03b7max\\eta_{max} is set to the initial lr and TcurT_{cur} is the number of epochs since the last restart in SGDR:", "lr_scheduler.ChainedScheduler", "Chains list of learning rate schedulers.", "lr_scheduler.SequentialLR", "Receives the list of schedulers that is expected to be called sequentially during optimization process and milestone points that provides exact intervals to reflect which scheduler is supposed to be called at a given epoch.", "lr_scheduler.ReduceLROnPlateau", "Reduce learning rate when a metric has stopped improving.", "lr_scheduler.CyclicLR", "Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR).", "lr_scheduler.OneCycleLR", "Sets the learning rate of each parameter group according to the 1cycle learning rate policy.", "lr_scheduler.CosineAnnealingWarmRestarts", "Set the learning rate of each parameter group using a cosine annealing schedule, where \u03b7max\\eta_{max} is set to the initial lr, TcurT_{cur} is the number of epochs since the last restart and TiT_{i} is the number of epochs between two warm restarts in SGDR:", "torch.optim.swa_utils implements Stochastic Weight Averaging (SWA) and Exponential Moving Average (EMA). In particular, the torch.optim.swa_utils.AveragedModel class implements SWA and EMA models, torch.optim.swa_utils.SWALR implements the SWA learning rate scheduler and torch.optim.swa_utils.update_bn() is a utility function used to update SWA/EMA batch normalization statistics at the end of training.", "SWA has been proposed in Averaging Weights Leads to Wider Optima and Better Generalization.", "EMA is a widely known technique to reduce the training time by reducing the number of weight updates needed. It is a variation of Polyak averaging, but using exponential weights instead of equal weights across iterations.", "The AveragedModel class serves to compute the weights of the SWA or EMA model.", "You can create an SWA averaged model by running:", "EMA models are constructed by specifying the multi_avg_fn argument as follows:", "Decay is a parameter between 0 and 1 that controls how fast the averaged parameters are decayed. If not provided to get_ema_multi_avg_fn, the default is 0.999.", "get_ema_multi_avg_fn returns a function that applies the following EMA equation to the weights:", "where alpha is the EMA decay.", "Here the model model can be an arbitrary torch.nn.Module object. averaged_model will keep track of the running averages of the parameters of the model. To update these averages, you should use the update_parameters() function after the optimizer.step():", "For SWA and EMA, this call is usually done right after the optimizer step(). In the case of SWA, this is usually skipped for some numbers of steps at the beginning of the training.", "By default, torch.optim.swa_utils.AveragedModel computes a running equal average of the parameters that you provide, but you can also use custom averaging functions with the avg_fn or multi_avg_fn parameters:", "In the following example ema_model computes an exponential moving average using the avg_fn parameter:", "In the following example ema_model computes an exponential moving average using the more efficient multi_avg_fn parameter:", "Typically, in SWA the learning rate is set to a high constant value. SWALR is a learning rate scheduler that anneals the learning rate to a fixed value, and then keeps it constant. For example, the following code creates a scheduler that linearly anneals the learning rate from its initial value to 0.05 in 5 epochs within each parameter group:", "You can also use cosine annealing to a fixed value instead of linear annealing by setting anneal_strategy=\"cos\".", "update_bn() is a utility function that allows to compute the batchnorm statistics for the SWA model on a given dataloader loader at the end of training:", "update_bn() applies the swa_model to every element in the dataloader and computes the activation statistics for each batch normalization layer in the model.", "Warning", "update_bn() assumes that each batch in the dataloader loader is either a tensors or a list of tensors where the first element is the tensor that the network swa_model should be applied to. If your dataloader has a different structure, you can update the batch normalization statistics of the swa_model by doing a forward pass with the swa_model on each element of the dataset.", "In the example below, swa_model is the SWA model that accumulates the averages of the weights. We train the model for a total of 300 epochs and we switch to the SWA learning rate schedule and start to collect SWA averages of the parameters at epoch 160:", "In the example below, ema_model is the EMA model that accumulates the exponentially-decayed averages of the weights with a decay rate of 0.999. We train the model for a total of 300 epochs and start to collect EMA averages immediately."]}, {"name": "torch.optim.Adadelta", "path": "generated/torch.optim.adadelta", "type": "Optimization", "text": ["Implements Adadelta algorithm.", "For further details regarding the algorithm we refer to ADADELTA: An Adaptive Learning Rate Method.", "Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options.", "Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict().", "Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]", "Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss.", "Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.Adadelta.add_param_group()", "path": "generated/torch.optim.adadelta#torch.optim.Adadelta.add_param_group", "type": "Optimization", "text": ["Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options."]}, {"name": "torch.optim.Adadelta.load_state_dict()", "path": "generated/torch.optim.adadelta#torch.optim.Adadelta.load_state_dict", "type": "Optimization", "text": ["Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.Adadelta.register_load_state_dict_post_hook()", "path": "generated/torch.optim.adadelta#torch.optim.Adadelta.register_load_state_dict_post_hook", "type": "Optimization", "text": ["Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.Adadelta.register_load_state_dict_pre_hook()", "path": "generated/torch.optim.adadelta#torch.optim.Adadelta.register_load_state_dict_pre_hook", "type": "Optimization", "text": ["Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.Adadelta.register_state_dict_post_hook()", "path": "generated/torch.optim.adadelta#torch.optim.Adadelta.register_state_dict_post_hook", "type": "Optimization", "text": ["Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.Adadelta.register_state_dict_pre_hook()", "path": "generated/torch.optim.adadelta#torch.optim.Adadelta.register_state_dict_pre_hook", "type": "Optimization", "text": ["Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.Adadelta.register_step_post_hook()", "path": "generated/torch.optim.adadelta#torch.optim.Adadelta.register_step_post_hook", "type": "Optimization", "text": ["Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.Adadelta.register_step_pre_hook()", "path": "generated/torch.optim.adadelta#torch.optim.Adadelta.register_step_pre_hook", "type": "Optimization", "text": ["Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.Adadelta.state_dict()", "path": "generated/torch.optim.adadelta#torch.optim.Adadelta.state_dict", "type": "Optimization", "text": ["Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]"]}, {"name": "torch.optim.Adadelta.step()", "path": "generated/torch.optim.adadelta#torch.optim.Adadelta.step", "type": "Optimization", "text": ["Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.Adadelta.zero_grad()", "path": "generated/torch.optim.adadelta#torch.optim.Adadelta.zero_grad", "type": "Optimization", "text": ["Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.Adagrad", "path": "generated/torch.optim.adagrad", "type": "Optimization", "text": ["Implements Adagrad algorithm.", "For further details regarding the algorithm we refer to Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.", "Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options.", "Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict().", "Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]", "Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss.", "Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.Adagrad.add_param_group()", "path": "generated/torch.optim.adagrad#torch.optim.Adagrad.add_param_group", "type": "Optimization", "text": ["Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options."]}, {"name": "torch.optim.Adagrad.load_state_dict()", "path": "generated/torch.optim.adagrad#torch.optim.Adagrad.load_state_dict", "type": "Optimization", "text": ["Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.Adagrad.register_load_state_dict_post_hook()", "path": "generated/torch.optim.adagrad#torch.optim.Adagrad.register_load_state_dict_post_hook", "type": "Optimization", "text": ["Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.Adagrad.register_load_state_dict_pre_hook()", "path": "generated/torch.optim.adagrad#torch.optim.Adagrad.register_load_state_dict_pre_hook", "type": "Optimization", "text": ["Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.Adagrad.register_state_dict_post_hook()", "path": "generated/torch.optim.adagrad#torch.optim.Adagrad.register_state_dict_post_hook", "type": "Optimization", "text": ["Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.Adagrad.register_state_dict_pre_hook()", "path": "generated/torch.optim.adagrad#torch.optim.Adagrad.register_state_dict_pre_hook", "type": "Optimization", "text": ["Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.Adagrad.register_step_post_hook()", "path": "generated/torch.optim.adagrad#torch.optim.Adagrad.register_step_post_hook", "type": "Optimization", "text": ["Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.Adagrad.register_step_pre_hook()", "path": "generated/torch.optim.adagrad#torch.optim.Adagrad.register_step_pre_hook", "type": "Optimization", "text": ["Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.Adagrad.state_dict()", "path": "generated/torch.optim.adagrad#torch.optim.Adagrad.state_dict", "type": "Optimization", "text": ["Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]"]}, {"name": "torch.optim.Adagrad.step()", "path": "generated/torch.optim.adagrad#torch.optim.Adagrad.step", "type": "Optimization", "text": ["Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.Adagrad.zero_grad()", "path": "generated/torch.optim.adagrad#torch.optim.Adagrad.zero_grad", "type": "Optimization", "text": ["Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.Adam", "path": "generated/torch.optim.adam", "type": "Optimization", "text": ["Implements Adam algorithm.", "For further details regarding the algorithm we refer to Adam: A Method for Stochastic Optimization.", "Note", "The foreach and fused implementations are typically faster than the for-loop, single-tensor implementation. Thus, if the user has not specified BOTH flags (i.e., when foreach = fused = None), we will attempt defaulting to the foreach implementation when the tensors are all on CUDA. For example, if the user specifies True for fused but nothing for foreach, we will run the fused implementation. If the user specifies False for foreach but nothing for fused (or False for fused but nothing for foreach), we will run the for-loop implementation. If the user specifies True for both foreach and fused, we will prioritize fused over foreach, as it is typically faster. We attempt to use the fastest, so the hierarchy goes fused -> foreach -> for-loop. HOWEVER, since the fused implementation is relatively new, we want to give it sufficient bake-in time, so we default to foreach and NOT fused when the user has not specified either flag.", "Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options.", "Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict().", "Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]", "Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss.", "Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.Adam.add_param_group()", "path": "generated/torch.optim.adam#torch.optim.Adam.add_param_group", "type": "Optimization", "text": ["Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options."]}, {"name": "torch.optim.Adam.load_state_dict()", "path": "generated/torch.optim.adam#torch.optim.Adam.load_state_dict", "type": "Optimization", "text": ["Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.Adam.register_load_state_dict_post_hook()", "path": "generated/torch.optim.adam#torch.optim.Adam.register_load_state_dict_post_hook", "type": "Optimization", "text": ["Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.Adam.register_load_state_dict_pre_hook()", "path": "generated/torch.optim.adam#torch.optim.Adam.register_load_state_dict_pre_hook", "type": "Optimization", "text": ["Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.Adam.register_state_dict_post_hook()", "path": "generated/torch.optim.adam#torch.optim.Adam.register_state_dict_post_hook", "type": "Optimization", "text": ["Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.Adam.register_state_dict_pre_hook()", "path": "generated/torch.optim.adam#torch.optim.Adam.register_state_dict_pre_hook", "type": "Optimization", "text": ["Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.Adam.register_step_post_hook()", "path": "generated/torch.optim.adam#torch.optim.Adam.register_step_post_hook", "type": "Optimization", "text": ["Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.Adam.register_step_pre_hook()", "path": "generated/torch.optim.adam#torch.optim.Adam.register_step_pre_hook", "type": "Optimization", "text": ["Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.Adam.state_dict()", "path": "generated/torch.optim.adam#torch.optim.Adam.state_dict", "type": "Optimization", "text": ["Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]"]}, {"name": "torch.optim.Adam.step()", "path": "generated/torch.optim.adam#torch.optim.Adam.step", "type": "Optimization", "text": ["Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.Adam.zero_grad()", "path": "generated/torch.optim.adam#torch.optim.Adam.zero_grad", "type": "Optimization", "text": ["Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.Adamax", "path": "generated/torch.optim.adamax", "type": "Optimization", "text": ["Implements Adamax algorithm (a variant of Adam based on infinity norm).", "For further details regarding the algorithm we refer to Adam: A Method for Stochastic Optimization.", "Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options.", "Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict().", "Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]", "Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss.", "Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.Adamax.add_param_group()", "path": "generated/torch.optim.adamax#torch.optim.Adamax.add_param_group", "type": "Optimization", "text": ["Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options."]}, {"name": "torch.optim.Adamax.load_state_dict()", "path": "generated/torch.optim.adamax#torch.optim.Adamax.load_state_dict", "type": "Optimization", "text": ["Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.Adamax.register_load_state_dict_post_hook()", "path": "generated/torch.optim.adamax#torch.optim.Adamax.register_load_state_dict_post_hook", "type": "Optimization", "text": ["Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.Adamax.register_load_state_dict_pre_hook()", "path": "generated/torch.optim.adamax#torch.optim.Adamax.register_load_state_dict_pre_hook", "type": "Optimization", "text": ["Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.Adamax.register_state_dict_post_hook()", "path": "generated/torch.optim.adamax#torch.optim.Adamax.register_state_dict_post_hook", "type": "Optimization", "text": ["Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.Adamax.register_state_dict_pre_hook()", "path": "generated/torch.optim.adamax#torch.optim.Adamax.register_state_dict_pre_hook", "type": "Optimization", "text": ["Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.Adamax.register_step_post_hook()", "path": "generated/torch.optim.adamax#torch.optim.Adamax.register_step_post_hook", "type": "Optimization", "text": ["Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.Adamax.register_step_pre_hook()", "path": "generated/torch.optim.adamax#torch.optim.Adamax.register_step_pre_hook", "type": "Optimization", "text": ["Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.Adamax.state_dict()", "path": "generated/torch.optim.adamax#torch.optim.Adamax.state_dict", "type": "Optimization", "text": ["Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]"]}, {"name": "torch.optim.Adamax.step()", "path": "generated/torch.optim.adamax#torch.optim.Adamax.step", "type": "Optimization", "text": ["Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.Adamax.zero_grad()", "path": "generated/torch.optim.adamax#torch.optim.Adamax.zero_grad", "type": "Optimization", "text": ["Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.AdamW", "path": "generated/torch.optim.adamw", "type": "Optimization", "text": ["Implements AdamW algorithm.", "For further details regarding the algorithm we refer to Decoupled Weight Decay Regularization.", "Note", "The foreach and fused implementations are typically faster than the for-loop, single-tensor implementation. Thus, if the user has not specified BOTH flags (i.e., when foreach = fused = None), we will attempt defaulting to the foreach implementation when the tensors are all on CUDA. For example, if the user specifies True for fused but nothing for foreach, we will run the fused implementation. If the user specifies False for foreach but nothing for fused (or False for fused but nothing for foreach), we will run the for-loop implementation. If the user specifies True for both foreach and fused, we will prioritize fused over foreach, as it is typically faster. We attempt to use the fastest, so the hierarchy goes fused -> foreach -> for-loop. HOWEVER, since the fused implementation is relatively new, we want to give it sufficient bake-in time, so we default to foreach and NOT fused when the user has not specified either flag.", "Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options.", "Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict().", "Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]", "Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss.", "Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.AdamW.add_param_group()", "path": "generated/torch.optim.adamw#torch.optim.AdamW.add_param_group", "type": "Optimization", "text": ["Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options."]}, {"name": "torch.optim.AdamW.load_state_dict()", "path": "generated/torch.optim.adamw#torch.optim.AdamW.load_state_dict", "type": "Optimization", "text": ["Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.AdamW.register_load_state_dict_post_hook()", "path": "generated/torch.optim.adamw#torch.optim.AdamW.register_load_state_dict_post_hook", "type": "Optimization", "text": ["Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.AdamW.register_load_state_dict_pre_hook()", "path": "generated/torch.optim.adamw#torch.optim.AdamW.register_load_state_dict_pre_hook", "type": "Optimization", "text": ["Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.AdamW.register_state_dict_post_hook()", "path": "generated/torch.optim.adamw#torch.optim.AdamW.register_state_dict_post_hook", "type": "Optimization", "text": ["Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.AdamW.register_state_dict_pre_hook()", "path": "generated/torch.optim.adamw#torch.optim.AdamW.register_state_dict_pre_hook", "type": "Optimization", "text": ["Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.AdamW.register_step_post_hook()", "path": "generated/torch.optim.adamw#torch.optim.AdamW.register_step_post_hook", "type": "Optimization", "text": ["Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.AdamW.register_step_pre_hook()", "path": "generated/torch.optim.adamw#torch.optim.AdamW.register_step_pre_hook", "type": "Optimization", "text": ["Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.AdamW.state_dict()", "path": "generated/torch.optim.adamw#torch.optim.AdamW.state_dict", "type": "Optimization", "text": ["Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]"]}, {"name": "torch.optim.AdamW.step()", "path": "generated/torch.optim.adamw#torch.optim.AdamW.step", "type": "Optimization", "text": ["Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.AdamW.zero_grad()", "path": "generated/torch.optim.adamw#torch.optim.AdamW.zero_grad", "type": "Optimization", "text": ["Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.ASGD", "path": "generated/torch.optim.asgd", "type": "Optimization", "text": ["Implements Averaged Stochastic Gradient Descent.", "It has been proposed in Acceleration of stochastic approximation by averaging.", "Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options.", "Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict().", "Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]", "Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss.", "Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.ASGD.add_param_group()", "path": "generated/torch.optim.asgd#torch.optim.ASGD.add_param_group", "type": "Optimization", "text": ["Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options."]}, {"name": "torch.optim.ASGD.load_state_dict()", "path": "generated/torch.optim.asgd#torch.optim.ASGD.load_state_dict", "type": "Optimization", "text": ["Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.ASGD.register_load_state_dict_post_hook()", "path": "generated/torch.optim.asgd#torch.optim.ASGD.register_load_state_dict_post_hook", "type": "Optimization", "text": ["Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.ASGD.register_load_state_dict_pre_hook()", "path": "generated/torch.optim.asgd#torch.optim.ASGD.register_load_state_dict_pre_hook", "type": "Optimization", "text": ["Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.ASGD.register_state_dict_post_hook()", "path": "generated/torch.optim.asgd#torch.optim.ASGD.register_state_dict_post_hook", "type": "Optimization", "text": ["Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.ASGD.register_state_dict_pre_hook()", "path": "generated/torch.optim.asgd#torch.optim.ASGD.register_state_dict_pre_hook", "type": "Optimization", "text": ["Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.ASGD.register_step_post_hook()", "path": "generated/torch.optim.asgd#torch.optim.ASGD.register_step_post_hook", "type": "Optimization", "text": ["Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.ASGD.register_step_pre_hook()", "path": "generated/torch.optim.asgd#torch.optim.ASGD.register_step_pre_hook", "type": "Optimization", "text": ["Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.ASGD.state_dict()", "path": "generated/torch.optim.asgd#torch.optim.ASGD.state_dict", "type": "Optimization", "text": ["Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]"]}, {"name": "torch.optim.ASGD.step()", "path": "generated/torch.optim.asgd#torch.optim.ASGD.step", "type": "Optimization", "text": ["Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.ASGD.zero_grad()", "path": "generated/torch.optim.asgd#torch.optim.ASGD.zero_grad", "type": "Optimization", "text": ["Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.ChainedScheduler", "path": "generated/torch.optim.lr_scheduler.chainedscheduler", "type": "Optimization", "text": ["Chains list of learning rate schedulers. It takes a list of chainable learning rate schedulers and performs consecutive step() functions belonging to them by just one call.", "schedulers (list) \u2013 List of chained schedulers.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer. The wrapped scheduler states will also be saved."]}, {"name": "torch.optim.ConstantLR", "path": "generated/torch.optim.lr_scheduler.constantlr", "type": "Optimization", "text": ["Decays the learning rate of each parameter group by a small constant factor until the number of epoch reaches a pre-defined milestone: total_iters. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.CosineAnnealingLR", "path": "generated/torch.optim.lr_scheduler.cosineannealinglr", "type": "Optimization", "text": ["Set the learning rate of each parameter group using a cosine annealing schedule, where \u03b7max\\eta_{max} is set to the initial lr and TcurT_{cur} is the number of epochs since the last restart in SGDR:", "When last_epoch=-1, sets initial lr as lr. Notice that because the schedule is defined recursively, the learning rate can be simultaneously modified outside this scheduler by other operators. If the learning rate is set solely by this scheduler, the learning rate at each step becomes:", "It has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts. Note that this only implements the cosine annealing part of SGDR, and not the restarts.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.CosineAnnealingWarmRestarts", "path": "generated/torch.optim.lr_scheduler.cosineannealingwarmrestarts", "type": "Optimization", "text": ["Set the learning rate of each parameter group using a cosine annealing schedule, where \u03b7max\\eta_{max} is set to the initial lr, TcurT_{cur} is the number of epochs since the last restart and TiT_{i} is the number of epochs between two warm restarts in SGDR:", "When Tcur=TiT_{cur}=T_{i}, set \u03b7t=\u03b7min\\eta_t = \\eta_{min}. When Tcur=0T_{cur}=0 after restart, set \u03b7t=\u03b7max\\eta_t=\\eta_{max}.", "It has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer.", "Step could be called after every batch update", "This function can be called in an interleaved way."]}, {"name": "torch.optim.CyclicLR", "path": "generated/torch.optim.lr_scheduler.cycliclr", "type": "Optimization", "text": ["Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). The policy cycles the learning rate between two boundaries with a constant frequency, as detailed in the paper Cyclical Learning Rates for Training Neural Networks. The distance between the two boundaries can be scaled on a per-iteration or per-cycle basis.", "Cyclical learning rate policy changes the learning rate after every batch. step should be called after a batch has been used for training.", "This class has three built-in policies, as put forth in the paper:", "This implementation was adapted from the github repo: bckenstler/CLR", "Return last computed learning rate by current scheduler.", "Calculates the learning rate at batch index. This function treats self.last_epoch as the last batch index.", "If self.cycle_momentum is True, this function has a side effect of updating the optimizer\u2019s momentum.", "Display the current learning rate."]}, {"name": "torch.optim.ExponentialLR", "path": "generated/torch.optim.lr_scheduler.exponentiallr", "type": "Optimization", "text": ["Decays the learning rate of each parameter group by gamma every epoch. When last_epoch=-1, sets initial lr as lr.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.LambdaLR", "path": "generated/torch.optim.lr_scheduler.lambdalr", "type": "Optimization", "text": ["Sets the learning rate of each parameter group to the initial lr times a given function. When last_epoch=-1, sets initial lr as lr.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "When saving or loading the scheduler, please make sure to also save or load the state of the optimizer.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas.", "When saving or loading the scheduler, please make sure to also save or load the state of the optimizer."]}, {"name": "torch.optim.LBFGS", "path": "generated/torch.optim.lbfgs", "type": "Optimization", "text": ["Implements L-BFGS algorithm, heavily inspired by minFunc.", "Warning", "This optimizer doesn\u2019t support per-parameter options and parameter groups (there can be only one).", "Warning", "Right now all parameters have to be on a single device. This will be improved in the future.", "Note", "This is a very memory intensive optimizer (it requires additional param_bytes * (history_size + 1) bytes). If it doesn\u2019t fit in memory try reducing the history size, or use a different algorithm.", "Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options.", "Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict().", "Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]", "Performs a single optimization step.", "closure (Callable) \u2013 A closure that reevaluates the model and returns the loss.", "Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.LBFGS.add_param_group()", "path": "generated/torch.optim.lbfgs#torch.optim.LBFGS.add_param_group", "type": "Optimization", "text": ["Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options."]}, {"name": "torch.optim.LBFGS.load_state_dict()", "path": "generated/torch.optim.lbfgs#torch.optim.LBFGS.load_state_dict", "type": "Optimization", "text": ["Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.LBFGS.register_load_state_dict_post_hook()", "path": "generated/torch.optim.lbfgs#torch.optim.LBFGS.register_load_state_dict_post_hook", "type": "Optimization", "text": ["Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.LBFGS.register_load_state_dict_pre_hook()", "path": "generated/torch.optim.lbfgs#torch.optim.LBFGS.register_load_state_dict_pre_hook", "type": "Optimization", "text": ["Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.LBFGS.register_state_dict_post_hook()", "path": "generated/torch.optim.lbfgs#torch.optim.LBFGS.register_state_dict_post_hook", "type": "Optimization", "text": ["Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.LBFGS.register_state_dict_pre_hook()", "path": "generated/torch.optim.lbfgs#torch.optim.LBFGS.register_state_dict_pre_hook", "type": "Optimization", "text": ["Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.LBFGS.register_step_post_hook()", "path": "generated/torch.optim.lbfgs#torch.optim.LBFGS.register_step_post_hook", "type": "Optimization", "text": ["Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.LBFGS.register_step_pre_hook()", "path": "generated/torch.optim.lbfgs#torch.optim.LBFGS.register_step_pre_hook", "type": "Optimization", "text": ["Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.LBFGS.state_dict()", "path": "generated/torch.optim.lbfgs#torch.optim.LBFGS.state_dict", "type": "Optimization", "text": ["Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]"]}, {"name": "torch.optim.LBFGS.step()", "path": "generated/torch.optim.lbfgs#torch.optim.LBFGS.step", "type": "Optimization", "text": ["Performs a single optimization step.", "closure (Callable) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.LBFGS.zero_grad()", "path": "generated/torch.optim.lbfgs#torch.optim.LBFGS.zero_grad", "type": "Optimization", "text": ["Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.LinearLR", "path": "generated/torch.optim.lr_scheduler.linearlr", "type": "Optimization", "text": ["Decays the learning rate of each parameter group by linearly changing small multiplicative factor until the number of epoch reaches a pre-defined milestone: total_iters. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.lr_scheduler.ChainedScheduler", "path": "generated/torch.optim.lr_scheduler.chainedscheduler#torch.optim.lr_scheduler.ChainedScheduler", "type": "Optimization", "text": ["Chains list of learning rate schedulers. It takes a list of chainable learning rate schedulers and performs consecutive step() functions belonging to them by just one call.", "schedulers (list) \u2013 List of chained schedulers.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer. The wrapped scheduler states will also be saved."]}, {"name": "torch.optim.lr_scheduler.ChainedScheduler.get_last_lr()", "path": "generated/torch.optim.lr_scheduler.chainedscheduler#torch.optim.lr_scheduler.ChainedScheduler.get_last_lr", "type": "Optimization", "text": ["Return last computed learning rate by current scheduler."]}, {"name": "torch.optim.lr_scheduler.ChainedScheduler.load_state_dict()", "path": "generated/torch.optim.lr_scheduler.chainedscheduler#torch.optim.lr_scheduler.ChainedScheduler.load_state_dict", "type": "Optimization", "text": ["Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.lr_scheduler.ChainedScheduler.print_lr()", "path": "generated/torch.optim.lr_scheduler.chainedscheduler#torch.optim.lr_scheduler.ChainedScheduler.print_lr", "type": "Optimization", "text": ["Display the current learning rate."]}, {"name": "torch.optim.lr_scheduler.ChainedScheduler.state_dict()", "path": "generated/torch.optim.lr_scheduler.chainedscheduler#torch.optim.lr_scheduler.ChainedScheduler.state_dict", "type": "Optimization", "text": ["Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer. The wrapped scheduler states will also be saved."]}, {"name": "torch.optim.lr_scheduler.ConstantLR", "path": "generated/torch.optim.lr_scheduler.constantlr#torch.optim.lr_scheduler.ConstantLR", "type": "Optimization", "text": ["Decays the learning rate of each parameter group by a small constant factor until the number of epoch reaches a pre-defined milestone: total_iters. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.lr_scheduler.ConstantLR.get_last_lr()", "path": "generated/torch.optim.lr_scheduler.constantlr#torch.optim.lr_scheduler.ConstantLR.get_last_lr", "type": "Optimization", "text": ["Return last computed learning rate by current scheduler."]}, {"name": "torch.optim.lr_scheduler.ConstantLR.load_state_dict()", "path": "generated/torch.optim.lr_scheduler.constantlr#torch.optim.lr_scheduler.ConstantLR.load_state_dict", "type": "Optimization", "text": ["Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.lr_scheduler.ConstantLR.print_lr()", "path": "generated/torch.optim.lr_scheduler.constantlr#torch.optim.lr_scheduler.ConstantLR.print_lr", "type": "Optimization", "text": ["Display the current learning rate."]}, {"name": "torch.optim.lr_scheduler.ConstantLR.state_dict()", "path": "generated/torch.optim.lr_scheduler.constantlr#torch.optim.lr_scheduler.ConstantLR.state_dict", "type": "Optimization", "text": ["Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.lr_scheduler.CosineAnnealingLR", "path": "generated/torch.optim.lr_scheduler.cosineannealinglr#torch.optim.lr_scheduler.CosineAnnealingLR", "type": "Optimization", "text": ["Set the learning rate of each parameter group using a cosine annealing schedule, where \u03b7max\\eta_{max} is set to the initial lr and TcurT_{cur} is the number of epochs since the last restart in SGDR:", "When last_epoch=-1, sets initial lr as lr. Notice that because the schedule is defined recursively, the learning rate can be simultaneously modified outside this scheduler by other operators. If the learning rate is set solely by this scheduler, the learning rate at each step becomes:", "It has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts. Note that this only implements the cosine annealing part of SGDR, and not the restarts.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.lr_scheduler.CosineAnnealingLR.get_last_lr()", "path": "generated/torch.optim.lr_scheduler.cosineannealinglr#torch.optim.lr_scheduler.CosineAnnealingLR.get_last_lr", "type": "Optimization", "text": ["Return last computed learning rate by current scheduler."]}, {"name": "torch.optim.lr_scheduler.CosineAnnealingLR.load_state_dict()", "path": "generated/torch.optim.lr_scheduler.cosineannealinglr#torch.optim.lr_scheduler.CosineAnnealingLR.load_state_dict", "type": "Optimization", "text": ["Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.lr_scheduler.CosineAnnealingLR.print_lr()", "path": "generated/torch.optim.lr_scheduler.cosineannealinglr#torch.optim.lr_scheduler.CosineAnnealingLR.print_lr", "type": "Optimization", "text": ["Display the current learning rate."]}, {"name": "torch.optim.lr_scheduler.CosineAnnealingLR.state_dict()", "path": "generated/torch.optim.lr_scheduler.cosineannealinglr#torch.optim.lr_scheduler.CosineAnnealingLR.state_dict", "type": "Optimization", "text": ["Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts", "path": "generated/torch.optim.lr_scheduler.cosineannealingwarmrestarts#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts", "type": "Optimization", "text": ["Set the learning rate of each parameter group using a cosine annealing schedule, where \u03b7max\\eta_{max} is set to the initial lr, TcurT_{cur} is the number of epochs since the last restart and TiT_{i} is the number of epochs between two warm restarts in SGDR:", "When Tcur=TiT_{cur}=T_{i}, set \u03b7t=\u03b7min\\eta_t = \\eta_{min}. When Tcur=0T_{cur}=0 after restart, set \u03b7t=\u03b7max\\eta_t=\\eta_{max}.", "It has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer.", "Step could be called after every batch update", "This function can be called in an interleaved way."]}, {"name": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.get_last_lr()", "path": "generated/torch.optim.lr_scheduler.cosineannealingwarmrestarts#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.get_last_lr", "type": "Optimization", "text": ["Return last computed learning rate by current scheduler."]}, {"name": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.load_state_dict()", "path": "generated/torch.optim.lr_scheduler.cosineannealingwarmrestarts#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.load_state_dict", "type": "Optimization", "text": ["Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.print_lr()", "path": "generated/torch.optim.lr_scheduler.cosineannealingwarmrestarts#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.print_lr", "type": "Optimization", "text": ["Display the current learning rate."]}, {"name": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.state_dict()", "path": "generated/torch.optim.lr_scheduler.cosineannealingwarmrestarts#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.state_dict", "type": "Optimization", "text": ["Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step()", "path": "generated/torch.optim.lr_scheduler.cosineannealingwarmrestarts#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step", "type": "Optimization", "text": ["Step could be called after every batch update", "This function can be called in an interleaved way."]}, {"name": "torch.optim.lr_scheduler.CyclicLR", "path": "generated/torch.optim.lr_scheduler.cycliclr#torch.optim.lr_scheduler.CyclicLR", "type": "Optimization", "text": ["Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). The policy cycles the learning rate between two boundaries with a constant frequency, as detailed in the paper Cyclical Learning Rates for Training Neural Networks. The distance between the two boundaries can be scaled on a per-iteration or per-cycle basis.", "Cyclical learning rate policy changes the learning rate after every batch. step should be called after a batch has been used for training.", "This class has three built-in policies, as put forth in the paper:", "This implementation was adapted from the github repo: bckenstler/CLR", "Return last computed learning rate by current scheduler.", "Calculates the learning rate at batch index. This function treats self.last_epoch as the last batch index.", "If self.cycle_momentum is True, this function has a side effect of updating the optimizer\u2019s momentum.", "Display the current learning rate."]}, {"name": "torch.optim.lr_scheduler.CyclicLR.get_last_lr()", "path": "generated/torch.optim.lr_scheduler.cycliclr#torch.optim.lr_scheduler.CyclicLR.get_last_lr", "type": "Optimization", "text": ["Return last computed learning rate by current scheduler."]}, {"name": "torch.optim.lr_scheduler.CyclicLR.get_lr()", "path": "generated/torch.optim.lr_scheduler.cycliclr#torch.optim.lr_scheduler.CyclicLR.get_lr", "type": "Optimization", "text": ["Calculates the learning rate at batch index. This function treats self.last_epoch as the last batch index.", "If self.cycle_momentum is True, this function has a side effect of updating the optimizer\u2019s momentum."]}, {"name": "torch.optim.lr_scheduler.CyclicLR.print_lr()", "path": "generated/torch.optim.lr_scheduler.cycliclr#torch.optim.lr_scheduler.CyclicLR.print_lr", "type": "Optimization", "text": ["Display the current learning rate."]}, {"name": "torch.optim.lr_scheduler.ExponentialLR", "path": "generated/torch.optim.lr_scheduler.exponentiallr#torch.optim.lr_scheduler.ExponentialLR", "type": "Optimization", "text": ["Decays the learning rate of each parameter group by gamma every epoch. When last_epoch=-1, sets initial lr as lr.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.lr_scheduler.ExponentialLR.get_last_lr()", "path": "generated/torch.optim.lr_scheduler.exponentiallr#torch.optim.lr_scheduler.ExponentialLR.get_last_lr", "type": "Optimization", "text": ["Return last computed learning rate by current scheduler."]}, {"name": "torch.optim.lr_scheduler.ExponentialLR.load_state_dict()", "path": "generated/torch.optim.lr_scheduler.exponentiallr#torch.optim.lr_scheduler.ExponentialLR.load_state_dict", "type": "Optimization", "text": ["Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.lr_scheduler.ExponentialLR.print_lr()", "path": "generated/torch.optim.lr_scheduler.exponentiallr#torch.optim.lr_scheduler.ExponentialLR.print_lr", "type": "Optimization", "text": ["Display the current learning rate."]}, {"name": "torch.optim.lr_scheduler.ExponentialLR.state_dict()", "path": "generated/torch.optim.lr_scheduler.exponentiallr#torch.optim.lr_scheduler.ExponentialLR.state_dict", "type": "Optimization", "text": ["Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.lr_scheduler.LambdaLR", "path": "generated/torch.optim.lr_scheduler.lambdalr#torch.optim.lr_scheduler.LambdaLR", "type": "Optimization", "text": ["Sets the learning rate of each parameter group to the initial lr times a given function. When last_epoch=-1, sets initial lr as lr.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "When saving or loading the scheduler, please make sure to also save or load the state of the optimizer.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas.", "When saving or loading the scheduler, please make sure to also save or load the state of the optimizer."]}, {"name": "torch.optim.lr_scheduler.LambdaLR.get_last_lr()", "path": "generated/torch.optim.lr_scheduler.lambdalr#torch.optim.lr_scheduler.LambdaLR.get_last_lr", "type": "Optimization", "text": ["Return last computed learning rate by current scheduler."]}, {"name": "torch.optim.lr_scheduler.LambdaLR.load_state_dict()", "path": "generated/torch.optim.lr_scheduler.lambdalr#torch.optim.lr_scheduler.LambdaLR.load_state_dict", "type": "Optimization", "text": ["Loads the schedulers state.", "When saving or loading the scheduler, please make sure to also save or load the state of the optimizer.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.lr_scheduler.LambdaLR.print_lr()", "path": "generated/torch.optim.lr_scheduler.lambdalr#torch.optim.lr_scheduler.LambdaLR.print_lr", "type": "Optimization", "text": ["Display the current learning rate."]}, {"name": "torch.optim.lr_scheduler.LambdaLR.state_dict()", "path": "generated/torch.optim.lr_scheduler.lambdalr#torch.optim.lr_scheduler.LambdaLR.state_dict", "type": "Optimization", "text": ["Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas.", "When saving or loading the scheduler, please make sure to also save or load the state of the optimizer."]}, {"name": "torch.optim.lr_scheduler.LinearLR", "path": "generated/torch.optim.lr_scheduler.linearlr#torch.optim.lr_scheduler.LinearLR", "type": "Optimization", "text": ["Decays the learning rate of each parameter group by linearly changing small multiplicative factor until the number of epoch reaches a pre-defined milestone: total_iters. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.lr_scheduler.LinearLR.get_last_lr()", "path": "generated/torch.optim.lr_scheduler.linearlr#torch.optim.lr_scheduler.LinearLR.get_last_lr", "type": "Optimization", "text": ["Return last computed learning rate by current scheduler."]}, {"name": "torch.optim.lr_scheduler.LinearLR.load_state_dict()", "path": "generated/torch.optim.lr_scheduler.linearlr#torch.optim.lr_scheduler.LinearLR.load_state_dict", "type": "Optimization", "text": ["Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.lr_scheduler.LinearLR.print_lr()", "path": "generated/torch.optim.lr_scheduler.linearlr#torch.optim.lr_scheduler.LinearLR.print_lr", "type": "Optimization", "text": ["Display the current learning rate."]}, {"name": "torch.optim.lr_scheduler.LinearLR.state_dict()", "path": "generated/torch.optim.lr_scheduler.linearlr#torch.optim.lr_scheduler.LinearLR.state_dict", "type": "Optimization", "text": ["Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.lr_scheduler.MultiplicativeLR", "path": "generated/torch.optim.lr_scheduler.multiplicativelr#torch.optim.lr_scheduler.MultiplicativeLR", "type": "Optimization", "text": ["Multiply the learning rate of each parameter group by the factor given in the specified function. When last_epoch=-1, sets initial lr as lr.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas."]}, {"name": "torch.optim.lr_scheduler.MultiplicativeLR.get_last_lr()", "path": "generated/torch.optim.lr_scheduler.multiplicativelr#torch.optim.lr_scheduler.MultiplicativeLR.get_last_lr", "type": "Optimization", "text": ["Return last computed learning rate by current scheduler."]}, {"name": "torch.optim.lr_scheduler.MultiplicativeLR.load_state_dict()", "path": "generated/torch.optim.lr_scheduler.multiplicativelr#torch.optim.lr_scheduler.MultiplicativeLR.load_state_dict", "type": "Optimization", "text": ["Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.lr_scheduler.MultiplicativeLR.print_lr()", "path": "generated/torch.optim.lr_scheduler.multiplicativelr#torch.optim.lr_scheduler.MultiplicativeLR.print_lr", "type": "Optimization", "text": ["Display the current learning rate."]}, {"name": "torch.optim.lr_scheduler.MultiplicativeLR.state_dict()", "path": "generated/torch.optim.lr_scheduler.multiplicativelr#torch.optim.lr_scheduler.MultiplicativeLR.state_dict", "type": "Optimization", "text": ["Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas."]}, {"name": "torch.optim.lr_scheduler.MultiStepLR", "path": "generated/torch.optim.lr_scheduler.multisteplr#torch.optim.lr_scheduler.MultiStepLR", "type": "Optimization", "text": ["Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.lr_scheduler.MultiStepLR.get_last_lr()", "path": "generated/torch.optim.lr_scheduler.multisteplr#torch.optim.lr_scheduler.MultiStepLR.get_last_lr", "type": "Optimization", "text": ["Return last computed learning rate by current scheduler."]}, {"name": "torch.optim.lr_scheduler.MultiStepLR.load_state_dict()", "path": "generated/torch.optim.lr_scheduler.multisteplr#torch.optim.lr_scheduler.MultiStepLR.load_state_dict", "type": "Optimization", "text": ["Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.lr_scheduler.MultiStepLR.print_lr()", "path": "generated/torch.optim.lr_scheduler.multisteplr#torch.optim.lr_scheduler.MultiStepLR.print_lr", "type": "Optimization", "text": ["Display the current learning rate."]}, {"name": "torch.optim.lr_scheduler.MultiStepLR.state_dict()", "path": "generated/torch.optim.lr_scheduler.multisteplr#torch.optim.lr_scheduler.MultiStepLR.state_dict", "type": "Optimization", "text": ["Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.lr_scheduler.OneCycleLR", "path": "generated/torch.optim.lr_scheduler.onecyclelr#torch.optim.lr_scheduler.OneCycleLR", "type": "Optimization", "text": ["Sets the learning rate of each parameter group according to the 1cycle learning rate policy. The 1cycle policy anneals the learning rate from an initial learning rate to some maximum learning rate and then from that maximum learning rate to some minimum learning rate much lower than the initial learning rate. This policy was initially described in the paper Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates.", "The 1cycle learning rate policy changes the learning rate after every batch. step should be called after a batch has been used for training.", "This scheduler is not chainable.", "Note also that the total number of steps in the cycle can be determined in one of two ways (listed in order of precedence):", "You must either provide a value for total_steps or provide a value for both epochs and steps_per_epoch.", "The default behaviour of this scheduler follows the fastai implementation of 1cycle, which claims that \u201cunpublished work has shown even better results by using only two phases\u201d. To mimic the behaviour of the original paper instead, set three_phase=True.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.lr_scheduler.OneCycleLR.get_last_lr()", "path": "generated/torch.optim.lr_scheduler.onecyclelr#torch.optim.lr_scheduler.OneCycleLR.get_last_lr", "type": "Optimization", "text": ["Return last computed learning rate by current scheduler."]}, {"name": "torch.optim.lr_scheduler.OneCycleLR.load_state_dict()", "path": "generated/torch.optim.lr_scheduler.onecyclelr#torch.optim.lr_scheduler.OneCycleLR.load_state_dict", "type": "Optimization", "text": ["Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.lr_scheduler.OneCycleLR.print_lr()", "path": "generated/torch.optim.lr_scheduler.onecyclelr#torch.optim.lr_scheduler.OneCycleLR.print_lr", "type": "Optimization", "text": ["Display the current learning rate."]}, {"name": "torch.optim.lr_scheduler.OneCycleLR.state_dict()", "path": "generated/torch.optim.lr_scheduler.onecyclelr#torch.optim.lr_scheduler.OneCycleLR.state_dict", "type": "Optimization", "text": ["Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.lr_scheduler.PolynomialLR", "path": "generated/torch.optim.lr_scheduler.polynomiallr#torch.optim.lr_scheduler.PolynomialLR", "type": "Optimization", "text": ["Decays the learning rate of each parameter group using a polynomial function in the given total_iters. When last_epoch=-1, sets initial lr as lr.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.lr_scheduler.PolynomialLR.get_last_lr()", "path": "generated/torch.optim.lr_scheduler.polynomiallr#torch.optim.lr_scheduler.PolynomialLR.get_last_lr", "type": "Optimization", "text": ["Return last computed learning rate by current scheduler."]}, {"name": "torch.optim.lr_scheduler.PolynomialLR.load_state_dict()", "path": "generated/torch.optim.lr_scheduler.polynomiallr#torch.optim.lr_scheduler.PolynomialLR.load_state_dict", "type": "Optimization", "text": ["Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.lr_scheduler.PolynomialLR.print_lr()", "path": "generated/torch.optim.lr_scheduler.polynomiallr#torch.optim.lr_scheduler.PolynomialLR.print_lr", "type": "Optimization", "text": ["Display the current learning rate."]}, {"name": "torch.optim.lr_scheduler.PolynomialLR.state_dict()", "path": "generated/torch.optim.lr_scheduler.polynomiallr#torch.optim.lr_scheduler.PolynomialLR.state_dict", "type": "Optimization", "text": ["Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.lr_scheduler.ReduceLROnPlateau", "path": "generated/torch.optim.lr_scheduler.reducelronplateau#torch.optim.lr_scheduler.ReduceLROnPlateau", "type": "Optimization", "text": ["Reduce learning rate when a metric has stopped improving. Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This scheduler reads a metrics quantity and if no improvement is seen for a \u2018patience\u2019 number of epochs, the learning rate is reduced."]}, {"name": "torch.optim.lr_scheduler.SequentialLR", "path": "generated/torch.optim.lr_scheduler.sequentiallr#torch.optim.lr_scheduler.SequentialLR", "type": "Optimization", "text": ["Receives the list of schedulers that is expected to be called sequentially during optimization process and milestone points that provides exact intervals to reflect which scheduler is supposed to be called at a given epoch.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer. The wrapped scheduler states will also be saved."]}, {"name": "torch.optim.lr_scheduler.SequentialLR.get_last_lr()", "path": "generated/torch.optim.lr_scheduler.sequentiallr#torch.optim.lr_scheduler.SequentialLR.get_last_lr", "type": "Optimization", "text": ["Return last computed learning rate by current scheduler."]}, {"name": "torch.optim.lr_scheduler.SequentialLR.load_state_dict()", "path": "generated/torch.optim.lr_scheduler.sequentiallr#torch.optim.lr_scheduler.SequentialLR.load_state_dict", "type": "Optimization", "text": ["Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.lr_scheduler.SequentialLR.print_lr()", "path": "generated/torch.optim.lr_scheduler.sequentiallr#torch.optim.lr_scheduler.SequentialLR.print_lr", "type": "Optimization", "text": ["Display the current learning rate."]}, {"name": "torch.optim.lr_scheduler.SequentialLR.state_dict()", "path": "generated/torch.optim.lr_scheduler.sequentiallr#torch.optim.lr_scheduler.SequentialLR.state_dict", "type": "Optimization", "text": ["Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer. The wrapped scheduler states will also be saved."]}, {"name": "torch.optim.lr_scheduler.StepLR", "path": "generated/torch.optim.lr_scheduler.steplr#torch.optim.lr_scheduler.StepLR", "type": "Optimization", "text": ["Decays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.lr_scheduler.StepLR.get_last_lr()", "path": "generated/torch.optim.lr_scheduler.steplr#torch.optim.lr_scheduler.StepLR.get_last_lr", "type": "Optimization", "text": ["Return last computed learning rate by current scheduler."]}, {"name": "torch.optim.lr_scheduler.StepLR.load_state_dict()", "path": "generated/torch.optim.lr_scheduler.steplr#torch.optim.lr_scheduler.StepLR.load_state_dict", "type": "Optimization", "text": ["Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.lr_scheduler.StepLR.print_lr()", "path": "generated/torch.optim.lr_scheduler.steplr#torch.optim.lr_scheduler.StepLR.print_lr", "type": "Optimization", "text": ["Display the current learning rate."]}, {"name": "torch.optim.lr_scheduler.StepLR.state_dict()", "path": "generated/torch.optim.lr_scheduler.steplr#torch.optim.lr_scheduler.StepLR.state_dict", "type": "Optimization", "text": ["Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.MultiplicativeLR", "path": "generated/torch.optim.lr_scheduler.multiplicativelr", "type": "Optimization", "text": ["Multiply the learning rate of each parameter group by the factor given in the specified function. When last_epoch=-1, sets initial lr as lr.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas."]}, {"name": "torch.optim.MultiStepLR", "path": "generated/torch.optim.lr_scheduler.multisteplr", "type": "Optimization", "text": ["Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.NAdam", "path": "generated/torch.optim.nadam", "type": "Optimization", "text": ["Implements NAdam algorithm.", "For further details regarding the algorithm we refer to Incorporating Nesterov Momentum into Adam.", "Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options.", "Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict().", "Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]", "Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss.", "Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.NAdam.add_param_group()", "path": "generated/torch.optim.nadam#torch.optim.NAdam.add_param_group", "type": "Optimization", "text": ["Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options."]}, {"name": "torch.optim.NAdam.load_state_dict()", "path": "generated/torch.optim.nadam#torch.optim.NAdam.load_state_dict", "type": "Optimization", "text": ["Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.NAdam.register_load_state_dict_post_hook()", "path": "generated/torch.optim.nadam#torch.optim.NAdam.register_load_state_dict_post_hook", "type": "Optimization", "text": ["Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.NAdam.register_load_state_dict_pre_hook()", "path": "generated/torch.optim.nadam#torch.optim.NAdam.register_load_state_dict_pre_hook", "type": "Optimization", "text": ["Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.NAdam.register_state_dict_post_hook()", "path": "generated/torch.optim.nadam#torch.optim.NAdam.register_state_dict_post_hook", "type": "Optimization", "text": ["Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.NAdam.register_state_dict_pre_hook()", "path": "generated/torch.optim.nadam#torch.optim.NAdam.register_state_dict_pre_hook", "type": "Optimization", "text": ["Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.NAdam.register_step_post_hook()", "path": "generated/torch.optim.nadam#torch.optim.NAdam.register_step_post_hook", "type": "Optimization", "text": ["Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.NAdam.register_step_pre_hook()", "path": "generated/torch.optim.nadam#torch.optim.NAdam.register_step_pre_hook", "type": "Optimization", "text": ["Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.NAdam.state_dict()", "path": "generated/torch.optim.nadam#torch.optim.NAdam.state_dict", "type": "Optimization", "text": ["Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]"]}, {"name": "torch.optim.NAdam.step()", "path": "generated/torch.optim.nadam#torch.optim.NAdam.step", "type": "Optimization", "text": ["Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.NAdam.zero_grad()", "path": "generated/torch.optim.nadam#torch.optim.NAdam.zero_grad", "type": "Optimization", "text": ["Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.OneCycleLR", "path": "generated/torch.optim.lr_scheduler.onecyclelr", "type": "Optimization", "text": ["Sets the learning rate of each parameter group according to the 1cycle learning rate policy. The 1cycle policy anneals the learning rate from an initial learning rate to some maximum learning rate and then from that maximum learning rate to some minimum learning rate much lower than the initial learning rate. This policy was initially described in the paper Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates.", "The 1cycle learning rate policy changes the learning rate after every batch. step should be called after a batch has been used for training.", "This scheduler is not chainable.", "Note also that the total number of steps in the cycle can be determined in one of two ways (listed in order of precedence):", "You must either provide a value for total_steps or provide a value for both epochs and steps_per_epoch.", "The default behaviour of this scheduler follows the fastai implementation of 1cycle, which claims that \u201cunpublished work has shown even better results by using only two phases\u201d. To mimic the behaviour of the original paper instead, set three_phase=True.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.Optimizer", "path": "optim#torch.optim.Optimizer", "type": "Optimization", "text": ["Base class for all optimizers.", "Warning", "Parameters need to be specified as collections that have a deterministic ordering that is consistent between runs. Examples of objects that don\u2019t satisfy those properties are sets and iterators over values of dictionaries."]}, {"name": "torch.optim.Optimizer.add_param_group()", "path": "generated/torch.optim.optimizer.add_param_group#torch.optim.Optimizer.add_param_group", "type": "Optimization", "text": ["Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options."]}, {"name": "torch.optim.Optimizer.load_state_dict()", "path": "generated/torch.optim.optimizer.load_state_dict#torch.optim.Optimizer.load_state_dict", "type": "Optimization", "text": ["Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.Optimizer.state_dict()", "path": "generated/torch.optim.optimizer.state_dict#torch.optim.Optimizer.state_dict", "type": "Optimization", "text": ["Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]"]}, {"name": "torch.optim.Optimizer.step()", "path": "generated/torch.optim.optimizer.step#torch.optim.Optimizer.step", "type": "Optimization", "text": ["Performs a single optimization step (parameter update).", "closure (Callable) \u2013 A closure that reevaluates the model and returns the loss. Optional for most optimizers.", "Note", "Unless otherwise specified, this function should not modify the .grad field of the parameters."]}, {"name": "torch.optim.Optimizer.zero_grad()", "path": "generated/torch.optim.optimizer.zero_grad#torch.optim.Optimizer.zero_grad", "type": "Optimization", "text": ["Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.PolynomialLR", "path": "generated/torch.optim.lr_scheduler.polynomiallr", "type": "Optimization", "text": ["Decays the learning rate of each parameter group using a polynomial function in the given total_iters. When last_epoch=-1, sets initial lr as lr.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.RAdam", "path": "generated/torch.optim.radam", "type": "Optimization", "text": ["Implements RAdam algorithm.", "For further details regarding the algorithm we refer to On the variance of the adaptive learning rate and beyond.", "This implementation uses the same weight_decay implementation as Adam (were the weight_decay is applied to the gradient) and not the one from AdamW (were weight_decay is applied to the update). This is different from the author\u2019s implementation.", "Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options.", "Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict().", "Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]", "Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss.", "Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.RAdam.add_param_group()", "path": "generated/torch.optim.radam#torch.optim.RAdam.add_param_group", "type": "Optimization", "text": ["Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options."]}, {"name": "torch.optim.RAdam.load_state_dict()", "path": "generated/torch.optim.radam#torch.optim.RAdam.load_state_dict", "type": "Optimization", "text": ["Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.RAdam.register_load_state_dict_post_hook()", "path": "generated/torch.optim.radam#torch.optim.RAdam.register_load_state_dict_post_hook", "type": "Optimization", "text": ["Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.RAdam.register_load_state_dict_pre_hook()", "path": "generated/torch.optim.radam#torch.optim.RAdam.register_load_state_dict_pre_hook", "type": "Optimization", "text": ["Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.RAdam.register_state_dict_post_hook()", "path": "generated/torch.optim.radam#torch.optim.RAdam.register_state_dict_post_hook", "type": "Optimization", "text": ["Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.RAdam.register_state_dict_pre_hook()", "path": "generated/torch.optim.radam#torch.optim.RAdam.register_state_dict_pre_hook", "type": "Optimization", "text": ["Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.RAdam.register_step_post_hook()", "path": "generated/torch.optim.radam#torch.optim.RAdam.register_step_post_hook", "type": "Optimization", "text": ["Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.RAdam.register_step_pre_hook()", "path": "generated/torch.optim.radam#torch.optim.RAdam.register_step_pre_hook", "type": "Optimization", "text": ["Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.RAdam.state_dict()", "path": "generated/torch.optim.radam#torch.optim.RAdam.state_dict", "type": "Optimization", "text": ["Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]"]}, {"name": "torch.optim.RAdam.step()", "path": "generated/torch.optim.radam#torch.optim.RAdam.step", "type": "Optimization", "text": ["Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.RAdam.zero_grad()", "path": "generated/torch.optim.radam#torch.optim.RAdam.zero_grad", "type": "Optimization", "text": ["Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.ReduceLROnPlateau", "path": "generated/torch.optim.lr_scheduler.reducelronplateau", "type": "Optimization", "text": ["Reduce learning rate when a metric has stopped improving. Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This scheduler reads a metrics quantity and if no improvement is seen for a \u2018patience\u2019 number of epochs, the learning rate is reduced."]}, {"name": "torch.optim.RMSprop", "path": "generated/torch.optim.rmsprop", "type": "Optimization", "text": ["Implements RMSprop algorithm.", "For further details regarding the algorithm we refer to lecture notes by G. Hinton. and centered version Generating Sequences With Recurrent Neural Networks. The implementation here takes the square root of the gradient average before adding epsilon (note that TensorFlow interchanges these two operations). The effective learning rate is thus \u03b3/(v+\u03f5)\\gamma/(\\sqrt{v} + \\epsilon) where \u03b3\\gamma is the scheduled learning rate and vv is the weighted moving average of the squared gradient.", "Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options.", "Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict().", "Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]", "Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss.", "Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.RMSprop.add_param_group()", "path": "generated/torch.optim.rmsprop#torch.optim.RMSprop.add_param_group", "type": "Optimization", "text": ["Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options."]}, {"name": "torch.optim.RMSprop.load_state_dict()", "path": "generated/torch.optim.rmsprop#torch.optim.RMSprop.load_state_dict", "type": "Optimization", "text": ["Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.RMSprop.register_load_state_dict_post_hook()", "path": "generated/torch.optim.rmsprop#torch.optim.RMSprop.register_load_state_dict_post_hook", "type": "Optimization", "text": ["Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.RMSprop.register_load_state_dict_pre_hook()", "path": "generated/torch.optim.rmsprop#torch.optim.RMSprop.register_load_state_dict_pre_hook", "type": "Optimization", "text": ["Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.RMSprop.register_state_dict_post_hook()", "path": "generated/torch.optim.rmsprop#torch.optim.RMSprop.register_state_dict_post_hook", "type": "Optimization", "text": ["Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.RMSprop.register_state_dict_pre_hook()", "path": "generated/torch.optim.rmsprop#torch.optim.RMSprop.register_state_dict_pre_hook", "type": "Optimization", "text": ["Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.RMSprop.register_step_post_hook()", "path": "generated/torch.optim.rmsprop#torch.optim.RMSprop.register_step_post_hook", "type": "Optimization", "text": ["Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.RMSprop.register_step_pre_hook()", "path": "generated/torch.optim.rmsprop#torch.optim.RMSprop.register_step_pre_hook", "type": "Optimization", "text": ["Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.RMSprop.state_dict()", "path": "generated/torch.optim.rmsprop#torch.optim.RMSprop.state_dict", "type": "Optimization", "text": ["Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]"]}, {"name": "torch.optim.RMSprop.step()", "path": "generated/torch.optim.rmsprop#torch.optim.RMSprop.step", "type": "Optimization", "text": ["Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.RMSprop.zero_grad()", "path": "generated/torch.optim.rmsprop#torch.optim.RMSprop.zero_grad", "type": "Optimization", "text": ["Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.Rprop", "path": "generated/torch.optim.rprop", "type": "Optimization", "text": ["Implements the resilient backpropagation algorithm.", "For further details regarding the algorithm we refer to the paper A Direct Adaptive Method for Faster Backpropagation Learning: The RPROP Algorithm.", "Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options.", "Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict().", "Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]", "Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss.", "Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.Rprop.add_param_group()", "path": "generated/torch.optim.rprop#torch.optim.Rprop.add_param_group", "type": "Optimization", "text": ["Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options."]}, {"name": "torch.optim.Rprop.load_state_dict()", "path": "generated/torch.optim.rprop#torch.optim.Rprop.load_state_dict", "type": "Optimization", "text": ["Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.Rprop.register_load_state_dict_post_hook()", "path": "generated/torch.optim.rprop#torch.optim.Rprop.register_load_state_dict_post_hook", "type": "Optimization", "text": ["Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.Rprop.register_load_state_dict_pre_hook()", "path": "generated/torch.optim.rprop#torch.optim.Rprop.register_load_state_dict_pre_hook", "type": "Optimization", "text": ["Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.Rprop.register_state_dict_post_hook()", "path": "generated/torch.optim.rprop#torch.optim.Rprop.register_state_dict_post_hook", "type": "Optimization", "text": ["Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.Rprop.register_state_dict_pre_hook()", "path": "generated/torch.optim.rprop#torch.optim.Rprop.register_state_dict_pre_hook", "type": "Optimization", "text": ["Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.Rprop.register_step_post_hook()", "path": "generated/torch.optim.rprop#torch.optim.Rprop.register_step_post_hook", "type": "Optimization", "text": ["Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.Rprop.register_step_pre_hook()", "path": "generated/torch.optim.rprop#torch.optim.Rprop.register_step_pre_hook", "type": "Optimization", "text": ["Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.Rprop.state_dict()", "path": "generated/torch.optim.rprop#torch.optim.Rprop.state_dict", "type": "Optimization", "text": ["Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]"]}, {"name": "torch.optim.Rprop.step()", "path": "generated/torch.optim.rprop#torch.optim.Rprop.step", "type": "Optimization", "text": ["Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.Rprop.zero_grad()", "path": "generated/torch.optim.rprop#torch.optim.Rprop.zero_grad", "type": "Optimization", "text": ["Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.SequentialLR", "path": "generated/torch.optim.lr_scheduler.sequentiallr", "type": "Optimization", "text": ["Receives the list of schedulers that is expected to be called sequentially during optimization process and milestone points that provides exact intervals to reflect which scheduler is supposed to be called at a given epoch.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer. The wrapped scheduler states will also be saved."]}, {"name": "torch.optim.SGD", "path": "generated/torch.optim.sgd", "type": "Optimization", "text": ["Implements stochastic gradient descent (optionally with momentum).", "Nesterov momentum is based on the formula from On the importance of initialization and momentum in deep learning.", "Note", "The implementation of SGD with Momentum/Nesterov subtly differs from Sutskever et. al. and implementations in some other frameworks.", "Considering the specific case of Momentum, the update can be written as", "where pp, gg, vv and \u03bc\\mu denote the parameters, gradient, velocity, and momentum respectively.", "This is in contrast to Sutskever et. al. and other frameworks which employ an update of the form", "The Nesterov version is analogously modified.", "Moreover, the initial value of the momentum buffer is set to the gradient value at the first step. This is in contrast to some other frameworks that initialize it to all zeros.", "Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options.", "Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict().", "Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]", "Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss.", "Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.SGD.add_param_group()", "path": "generated/torch.optim.sgd#torch.optim.SGD.add_param_group", "type": "Optimization", "text": ["Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options."]}, {"name": "torch.optim.SGD.load_state_dict()", "path": "generated/torch.optim.sgd#torch.optim.SGD.load_state_dict", "type": "Optimization", "text": ["Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.SGD.register_load_state_dict_post_hook()", "path": "generated/torch.optim.sgd#torch.optim.SGD.register_load_state_dict_post_hook", "type": "Optimization", "text": ["Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.SGD.register_load_state_dict_pre_hook()", "path": "generated/torch.optim.sgd#torch.optim.SGD.register_load_state_dict_pre_hook", "type": "Optimization", "text": ["Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.SGD.register_state_dict_post_hook()", "path": "generated/torch.optim.sgd#torch.optim.SGD.register_state_dict_post_hook", "type": "Optimization", "text": ["Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.SGD.register_state_dict_pre_hook()", "path": "generated/torch.optim.sgd#torch.optim.SGD.register_state_dict_pre_hook", "type": "Optimization", "text": ["Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.SGD.register_step_post_hook()", "path": "generated/torch.optim.sgd#torch.optim.SGD.register_step_post_hook", "type": "Optimization", "text": ["Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.SGD.register_step_pre_hook()", "path": "generated/torch.optim.sgd#torch.optim.SGD.register_step_pre_hook", "type": "Optimization", "text": ["Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.SGD.state_dict()", "path": "generated/torch.optim.sgd#torch.optim.SGD.state_dict", "type": "Optimization", "text": ["Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]"]}, {"name": "torch.optim.SGD.step()", "path": "generated/torch.optim.sgd#torch.optim.SGD.step", "type": "Optimization", "text": ["Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.SGD.zero_grad()", "path": "generated/torch.optim.sgd#torch.optim.SGD.zero_grad", "type": "Optimization", "text": ["Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.SparseAdam", "path": "generated/torch.optim.sparseadam", "type": "Optimization", "text": ["SparseAdam implements a masked version of the Adam algorithm suitable for sparse gradients. Currently, due to implementation constraints (explained below), SparseAdam is only intended for a narrow subset of use cases, specifically parameters of a dense layout with gradients of a sparse layout. This occurs in a special case where the module backwards produces grads already in a sparse layout. One example NN module that behaves as such is nn.Embedding(sparse=True).", "SparseAdam approximates the Adam algorithm by masking out the parameter and moment updates corresponding to the zero values in the gradients. Whereas the Adam algorithm will update the first moment, the second moment, and the parameters based on all values of the gradients, SparseAdam only updates the moments and parameters corresponding to the non-zero values of the gradients.", "A simplified way of thinking about the intended implementation is as such:", "In actuality, we use sparse layout Tensors to optimize this approximation, which means the more gradients that are masked by not being materialized, the more performant the optimization. Since we rely on using sparse layout tensors, we infer that any materialized value in the sparse layout is non-zero and we do NOT actually verify that all values are not zero! It is important to not conflate a semantically sparse tensor (a tensor where many of its values are zeros) with a sparse layout tensor (a tensor where .is_sparse returns True). The SparseAdam approximation is intended for semantically sparse tensors and the sparse layout is only a implementation detail. A clearer implementation would be to use MaskedTensors, but those are experimental.", "Note", "If you suspect your gradients are semantically sparse (but do not have sparse layout), this variant may not be the best for you. Ideally, you want to avoid materializing anything that is suspected to be sparse in the first place, since needing to convert all your grads from dense layout to sparse layout may outweigh the performance gain. Here, using Adam may be the best alternative, unless you can easily rig up your module to output sparse grads similar to nn.Embedding(sparse=True). If you insist on converting your grads, you can do so by manually overriding your parameters\u2019 .grad fields with their sparse equivalents before calling .step().", "Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options.", "Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict().", "Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle", "Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]", "Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss.", "Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.SparseAdam.add_param_group()", "path": "generated/torch.optim.sparseadam#torch.optim.SparseAdam.add_param_group", "type": "Optimization", "text": ["Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options."]}, {"name": "torch.optim.SparseAdam.load_state_dict()", "path": "generated/torch.optim.sparseadam#torch.optim.SparseAdam.load_state_dict", "type": "Optimization", "text": ["Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.SparseAdam.register_load_state_dict_post_hook()", "path": "generated/torch.optim.sparseadam#torch.optim.SparseAdam.register_load_state_dict_post_hook", "type": "Optimization", "text": ["Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "The hook will be called with argument self after calling load_state_dict on self. The registered hook can be used to perform post-processing after load_state_dict has loaded the state_dict.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.SparseAdam.register_load_state_dict_pre_hook()", "path": "generated/torch.optim.sparseadam#torch.optim.SparseAdam.register_load_state_dict_pre_hook", "type": "Optimization", "text": ["Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used and the state_dict argument is a shallow copy of the state_dict the user passed in to load_state_dict. The hook may modify the state_dict inplace or optionally return a new one. If a state_dict is returned, it will be used to be loaded into the optimizer.", "The hook will be called with argument self and state_dict before calling load_state_dict on self. The registered hook can be used to perform pre-processing before the load_state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.SparseAdam.register_state_dict_post_hook()", "path": "generated/torch.optim.sparseadam#torch.optim.SparseAdam.register_state_dict_post_hook", "type": "Optimization", "text": ["Register a state dict post-hook which will be called after state_dict() is called. It should have the following signature:", "The hook will be called with arguments self and state_dict after generating a state_dict on self. The hook may modify the state_dict inplace or optionally return a new one. The registered hook can be used to perform post-processing on the state_dict before it is returned.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.SparseAdam.register_state_dict_pre_hook()", "path": "generated/torch.optim.sparseadam#torch.optim.SparseAdam.register_state_dict_pre_hook", "type": "Optimization", "text": ["Register a state dict pre-hook which will be called before state_dict() is called. It should have the following signature:", "The optimizer argument is the optimizer instance being used. The hook will be called with argument self before calling state_dict on self. The registered hook can be used to perform pre-processing before the state_dict call is made.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemoveableHandle"]}, {"name": "torch.optim.SparseAdam.register_step_post_hook()", "path": "generated/torch.optim.sparseadam#torch.optim.SparseAdam.register_step_post_hook", "type": "Optimization", "text": ["Register an optimizer step post hook which will be called after optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.SparseAdam.register_step_pre_hook()", "path": "generated/torch.optim.sparseadam#torch.optim.SparseAdam.register_step_pre_hook", "type": "Optimization", "text": ["Register an optimizer step pre hook which will be called before optimizer step. It should have the following signature:", "The optimizer argument is the optimizer instance being used. If args and kwargs are modified by the pre-hook, then the transformed values are returned as a tuple containing the new_args and new_kwargs.", "hook (Callable) \u2013 The user defined hook to be registered.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.optim.SparseAdam.state_dict()", "path": "generated/torch.optim.sparseadam#torch.optim.SparseAdam.state_dict", "type": "Optimization", "text": ["Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]"]}, {"name": "torch.optim.SparseAdam.step()", "path": "generated/torch.optim.sparseadam#torch.optim.SparseAdam.step", "type": "Optimization", "text": ["Performs a single optimization step.", "closure (Callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.SparseAdam.zero_grad()", "path": "generated/torch.optim.sparseadam#torch.optim.SparseAdam.zero_grad", "type": "Optimization", "text": ["Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.StepLR", "path": "generated/torch.optim.lr_scheduler.steplr", "type": "Optimization", "text": ["Decays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.", "Return last computed learning rate by current scheduler.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Display the current learning rate.", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer."]}, {"name": "torch.optim.torch.optim.Optimizer.add_param_group", "path": "generated/torch.optim.optimizer.add_param_group", "type": "Optimization", "text": ["Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 Specifies what Tensors should be optimized along with group specific optimization options."]}, {"name": "torch.optim.torch.optim.Optimizer.load_state_dict", "path": "generated/torch.optim.optimizer.load_state_dict", "type": "Optimization", "text": ["Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.torch.optim.Optimizer.state_dict", "path": "generated/torch.optim.optimizer.state_dict", "type": "Optimization", "text": ["Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. state is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter.", "parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group.", "NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group params (int IDs) and the optimizer param_groups (actual nn.Parameter s) in order to match state WITHOUT additional verification.", "A returned state dict might look something like:", "Dict[str, Any]"]}, {"name": "torch.optim.torch.optim.Optimizer.step", "path": "generated/torch.optim.optimizer.step", "type": "Optimization", "text": ["Performs a single optimization step (parameter update).", "closure (Callable) \u2013 A closure that reevaluates the model and returns the loss. Optional for most optimizers.", "Note", "Unless otherwise specified, this function should not modify the .grad field of the parameters."]}, {"name": "torch.optim.torch.optim.Optimizer.zero_grad", "path": "generated/torch.optim.optimizer.zero_grad", "type": "Optimization", "text": ["Resets the gradients of all optimized torch.Tensor s.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.orgqr", "path": "generated/torch.orgqr", "type": "Torch", "text": ["Alias for torch.linalg.householder_product()."]}, {"name": "torch.ormqr", "path": "generated/torch.ormqr", "type": "Torch", "text": ["Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.", "Multiplies a m\u00d7nm \\times n matrix C (given by other) with a matrix Q, where Q is represented using Householder reflectors (input, tau). See Representation of Orthogonal or Unitary Matrices for further details.", "If left is True then op(Q) times C is computed, otherwise the result is C times op(Q). When left is True, the implicit matrix Q has size m\u00d7mm \\times m. It has size n\u00d7nn \\times n otherwise. If transpose is True then op is the conjugate transpose operation, otherwise it\u2019s a no-op.", "Supports inputs of float, double, cfloat and cdouble dtypes. Also supports batched inputs, and, if the input is batched, the output is batched with the same dimensions.", "See also", "torch.geqrf() can be used to form the Householder representation (input, tau) of matrix Q from the QR decomposition.", "Note", "This function supports backward but it is only fast when (input, tau) do not require gradients and/or tau.size(-1) is very small. ``", "out (Tensor, optional) \u2013 the output Tensor. Ignored if None. Default: None."]}, {"name": "torch.outer", "path": "generated/torch.outer", "type": "Torch", "text": ["Outer product of input and vec2. If input is a vector of size nn and vec2 is a vector of size mm, then out must be a matrix of size (n\u00d7m)(n \\times m).", "Note", "This function does not broadcast.", "out (Tensor, optional) \u2013 optional output matrix", "Example:"]}, {"name": "torch.overrides", "path": "torch.overrides", "type": "Miscellaneous", "text": ["This module exposes various helper functions for the __torch_function__ protocol. See Extending torch Python API for more details on the __torch_function__ protocol.", "Return public functions that cannot be overridden by __torch_function__.", "A tuple of functions that are publicly available in the torch API but cannot be overridden with __torch_function__. Mostly this is because none of the arguments of these functions are tensors or tensor-likes.", "Set[Callable]", "List functions that are overridable via __torch_function__", "A dictionary that maps namespaces that contain overridable functions to functions in that namespace that can be overridden.", "Dict[Any, List[Callable]]", "Get a human readable string name for a function passed to __torch_function__", "f (Callable) \u2013 Function to resolve the name of.", "Name of the function; if eval\u2019ed it should give back the input function.", "str", "Return a dict containing dummy overrides for all overridable functions", "A dictionary that maps overridable functions in the PyTorch API to lambda functions that have the same signature as the real function and unconditionally return -1. These lambda functions are useful for testing API coverage for a type that defines __torch_function__.", "Dict[Callable, Callable]", "Implement a function with checks for __torch_function__ overrides.", "See torch::autograd::handle_torch_function for the equivalent of this function in the C++ implementation.", "Result from calling implementation or an __torch_function__ method, as appropriate.", "object", ":raises TypeError : if no implementation is found.:", "Check for __torch_function__ implementations in the elements of an iterable or if a __torch_function__ mode is enabled. Considers exact Tensor s and Parameter s non-dispatchable. Use this to guard a call to handle_torch_function(); don\u2019t use it to test if something is Tensor-like, use is_tensor_like() instead. :param relevant_args: Iterable or arguments to check for __torch_function__ methods. :type relevant_args: iterable", "True if any of the elements of relevant_args have __torch_function__ implementations, False otherwise.", "bool", "See also", "Checks if something is a Tensor-like, including an exact Tensor.", "Returns True if the passed-in input is a Tensor-like.", "Currently, this occurs whenever there\u2019s a __torch_function__ attribute on the type of the input.", "A subclass of tensor is generally a Tensor-like.", "Built-in or user types aren\u2019t usually Tensor-like.", "But, they can be made Tensor-like by implementing __torch_function__.", "Returns True if the function passed in is a handler for a method or property belonging to torch.Tensor, as passed into __torch_function__.", "Note", "For properties, their __get__ method must be passed in.", "This may be needed, in particular, for the following reasons:", "bool", "Wraps a given function with __torch_function__ -related functionality.", "dispatcher (Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function.", "Note", "This decorator may reduce the performance of your code. Generally, it\u2019s enough to express your code as a series of functions that, themselves, support __torch_function__. If you find yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a low-level library and you also need it to work for Tensor-likes, then this function is available."]}, {"name": "torch.overrides.get_ignored_functions()", "path": "torch.overrides#torch.overrides.get_ignored_functions", "type": "Miscellaneous", "text": ["Return public functions that cannot be overridden by __torch_function__.", "A tuple of functions that are publicly available in the torch API but cannot be overridden with __torch_function__. Mostly this is because none of the arguments of these functions are tensors or tensor-likes.", "Set[Callable]"]}, {"name": "torch.overrides.get_overridable_functions()", "path": "torch.overrides#torch.overrides.get_overridable_functions", "type": "Miscellaneous", "text": ["List functions that are overridable via __torch_function__", "A dictionary that maps namespaces that contain overridable functions to functions in that namespace that can be overridden.", "Dict[Any, List[Callable]]"]}, {"name": "torch.overrides.get_testing_overrides()", "path": "torch.overrides#torch.overrides.get_testing_overrides", "type": "Miscellaneous", "text": ["Return a dict containing dummy overrides for all overridable functions", "A dictionary that maps overridable functions in the PyTorch API to lambda functions that have the same signature as the real function and unconditionally return -1. These lambda functions are useful for testing API coverage for a type that defines __torch_function__.", "Dict[Callable, Callable]"]}, {"name": "torch.overrides.handle_torch_function()", "path": "torch.overrides#torch.overrides.handle_torch_function", "type": "Miscellaneous", "text": ["Implement a function with checks for __torch_function__ overrides.", "See torch::autograd::handle_torch_function for the equivalent of this function in the C++ implementation.", "Result from calling implementation or an __torch_function__ method, as appropriate.", "object", ":raises TypeError : if no implementation is found.:"]}, {"name": "torch.overrides.has_torch_function()", "path": "torch.overrides#torch.overrides.has_torch_function", "type": "Miscellaneous", "text": ["Check for __torch_function__ implementations in the elements of an iterable or if a __torch_function__ mode is enabled. Considers exact Tensor s and Parameter s non-dispatchable. Use this to guard a call to handle_torch_function(); don\u2019t use it to test if something is Tensor-like, use is_tensor_like() instead. :param relevant_args: Iterable or arguments to check for __torch_function__ methods. :type relevant_args: iterable", "True if any of the elements of relevant_args have __torch_function__ implementations, False otherwise.", "bool", "See also", "Checks if something is a Tensor-like, including an exact Tensor."]}, {"name": "torch.overrides.is_tensor_like()", "path": "torch.overrides#torch.overrides.is_tensor_like", "type": "Miscellaneous", "text": ["Returns True if the passed-in input is a Tensor-like.", "Currently, this occurs whenever there\u2019s a __torch_function__ attribute on the type of the input.", "A subclass of tensor is generally a Tensor-like.", "Built-in or user types aren\u2019t usually Tensor-like.", "But, they can be made Tensor-like by implementing __torch_function__."]}, {"name": "torch.overrides.is_tensor_method_or_property()", "path": "torch.overrides#torch.overrides.is_tensor_method_or_property", "type": "Miscellaneous", "text": ["Returns True if the function passed in is a handler for a method or property belonging to torch.Tensor, as passed into __torch_function__.", "Note", "For properties, their __get__ method must be passed in.", "This may be needed, in particular, for the following reasons:", "bool"]}, {"name": "torch.overrides.resolve_name()", "path": "torch.overrides#torch.overrides.resolve_name", "type": "Miscellaneous", "text": ["Get a human readable string name for a function passed to __torch_function__", "f (Callable) \u2013 Function to resolve the name of.", "Name of the function; if eval\u2019ed it should give back the input function.", "str"]}, {"name": "torch.overrides.wrap_torch_function()", "path": "torch.overrides#torch.overrides.wrap_torch_function", "type": "Miscellaneous", "text": ["Wraps a given function with __torch_function__ -related functionality.", "dispatcher (Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function.", "Note", "This decorator may reduce the performance of your code. Generally, it\u2019s enough to express your code as a series of functions that, themselves, support __torch_function__. If you find yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a low-level library and you also need it to work for Tensor-likes, then this function is available."]}, {"name": "torch.package", "path": "package", "type": "Package", "text": ["torch.package adds support for creating packages containing both artifacts and arbitrary PyTorch code. These packages can be saved, shared, used to load and execute models at a later date or on a different machine, and can even be deployed to production using torch::deploy.", "This document contains tutorials, how-to guides, explanations, and an API reference that will help you learn more about torch.package and how to use it.", "Warning", "This module depends on the pickle module which is not secure. Only unpackage data you trust.", "It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Never unpackage data that could have come from an untrusted source, or that could have been tampered with.", "For more information, review the documentation for the pickle module.", "Tutorials", "How do I\u2026", "Explanation", "A tutorial that guides you through packaging and unpackaging a simple model is available on Colab. After completing this exercise, you will be familiar with the basic API for creating and using Torch packages.", "The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should work for exploring the contents. Some common ways to interact with ZIP files:", "PackageImporter provides a file_structure() method, which will return a printable and queryable Directory object. The Directory object is a simple directory structure that you can use to explore the current contents of a torch.package.", "The Directory object itself is directly printable and will print out a file tree representation. To filter what is returned, use the glob-style include and exclude filtering arguments.", "Output:", "You can also query Directory objects with the has_file() method.", "Say there is a given module foo, and you want to know why your PackageExporter is pulling in foo as a dependency.", "PackageExporter.get_rdeps() will return all modules that directly depend on foo.", "If you would like to see how a given module src depends on foo, the PackageExporter.all_paths() method will return a DOT-formatted graph showing all the dependency paths between src and foo.", "If you would just like to see the whole dependency graph of your PackageExporter, you can use PackageExporter.dependency_graph_string().", "PackageExporter exposes three methods, save_pickle, save_text and save_binary that allow you to save Python objects, text, and binary data to a package.", "PackageImporter exposes complementary methods named load_pickle, load_text and load_binary that allow you to load Python objects, text and binary data from a package.", "torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method __reduce_package__ on a class and by defining a corresponding de-packaging function. This is similar to defining __reduce__ for Python\u2019s normal pickling process.", "Steps:", "A PackageImporter will add the attribute __torch_package__ to every module that it initializes. Your code can check for the presence of this attribute to determine whether it is executing in a packaged context or not.", "Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a torch.package.", "Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to hard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring your code so that it behaves the same way no matter how it was loaded.", "PackageExporter offers a save_source_string() method that allows one to save arbitrary Python source code to a module of your choosing.", "PackageImporter implements the importlib.resources API for accessing resources from inside a package.", "The importlib.resources API allows access to resources from within packaged code.", "Using importlib.resources is the recommended way to access package contents from within packaged code, since it complies with the Python standard. However, it is also possible to access the parent PackageImporter instance itself from within packaged code.", "To tell if an object\u2019s code is from a torch.package, use the torch.package.is_from_package() function. Note: if an object is from a package but its definition is from a module marked extern or from stdlib, this check will return False.", "To re-export an object that was previously imported by a PackageImporter, you must make the new PackageExporter aware of the original PackageImporter so that it can find source code for your object\u2019s dependencies.", "To package a TorchScript model, use the same save_pickle and load_pickle APIs as you would with any other object. Saving TorchScript objects that are attributes or submodules is supported as well with no extra work.", "A torch.package file is a ZIP archive which conventionally uses the .pt extension. Inside the ZIP archive, there are two kinds of files:", "As an example, this is what a fully packaged ResNet model from torchvision looks like:", "The .data/ directory is owned by torch.package, and its contents are considered to be a private implementation detail. The torch.package format makes no guarantees about the contents of .data/, but any changes made will be backward compatible (that is, newer version of PyTorch will always be able to load older torch.packages).", "Currently, the .data/ directory contains the following items:", "All other files in the archive were put there by a user. The layout is identical to a Python regular package. For a deeper dive in how Python packaging works, please consult this essay (it\u2019s slightly out of date, so double-check implementation details with the Python reference documentation).", "When you issue a save_pickle(obj, ...) call, PackageExporter will pickle the object normally. Then, it uses the pickletools standard library module to parse the pickle bytecode.", "In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like:", "The dependency resolver will gather up all GLOBAL ops and mark them as dependencies of your pickled object. For more information about pickling and the pickle format, please consult the Python docs.", "When a Python module is identified as a dependency, torch.package walks the module\u2019s python AST representation and looks for import statements with full support for the standard forms: from x import y, import z, from w import v as u, etc. When one of these import statements are encountered, torch.package registers the imported modules as dependencies that are then themselves parsed in the same AST walking way.", "Note: AST parsing has limited support for the __import__(...) syntax and does not support importlib.import_module calls. In general, you should not expect dynamic imports to be detected by torch.package.", "torch.package automatically finds the Python modules that your code and objects depend on. This process is called dependency resolution. For each module that the dependency resolver finds, you must specify an action to take.", "The allowed actions are:", "Finally, there is one more important action that is not technically part of torch.package:", "Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from a module and leave the rest out. This is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a module, so that\u2019s what torch.package uses.", "Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like \"foo.**\"). You associate a pattern with an action using methods on PackageExporter, e.g.", "If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined, and the first action will be taken.", "If a module is intern-ed, it will be placed into the package.", "This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet from torchvision, you will need to intern the module torchvision.models.resnet.", "On package import, when your packaged code tries to import an intern-ed module, PackageImporter will look inside your package for that module. If it can\u2019t find that module, an error will be raised. This ensures that each PackageImporter is isolated from the loading environment\u2014even if you have my_interned_module available in both your package and the loading environment, PackageImporter will only use the version in your package.", "Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if you attempt to intern them. These kinds of modules need to be mock-ed or extern-ed.", "If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this list on package_exporter.extern_modules.", "On package import, when the packaged code tries to import an extern-ed module, PackageImporter will use the default Python importer to find that module, as if you did importlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised.", "In this way, you can depend on third-party libraries like numpy and scipy from within your package without having to package them too.", "Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility for your package, try to limit your use of extern.", "If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve objects from it (so that from my_mocked_module import foo will not error), but any use of that object will raise a NotImplementedError.", "mock should be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want available for use in non-packaged contents. For example, initialization/configuration code, or code only used for debugging/training.", "Warning: In general, mock should be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code, which may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.", "The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some guidelines for writing code with clean dependencies (which are also generally good practices!):", "Include only what you use. Do not leave unused imports in your code. The dependency resolver is not smart enough to tell that they are indeed unused, and will try to process them.", "Qualify your imports. For example, instead of writing import foo and later using foo.bar.baz, prefer to write from foo.bar import baz. This more precisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all of foo.", "Split up large files with unrelated functionality into smaller ones. If your utils module contains a hodge-podge of unrelated functionality, any module that depends on utils will need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define single-purpose modules that can be packaged independently of one another.", "Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buck glob().", "A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a separator string, e.g. foo.bar.baz.", "A pattern contains one or more segments. Segments can be:", "Examples:", "When specifying actions, you can pass multiple patterns, e.g.", "A module will match against this action if it matches any of the patterns.", "You can also specify patterns to exclude, e.g.", "A module will not match against this action if it matches any of the exclude patterns. In this example, we are mocking all modules except torchvision and its submodules.", "When a module could potentially match against multiple actions, the first action defined will be taken.", "Python makes it really easy to bind objects and run code at module-level scope. This is generally fine\u2014after all, functions and classes are bound to names this way. However, things become more complicated when you define an object at module scope with the intention of mutating it, introducing mutable global state.", "Mutable global state is quite useful\u2014it can reduce boilerplate, allow for open registration into tables, etc. But unless employed very carefully, it can cause complications when used with torch.package.", "Every PackageImporter creates an independent environment for its contents. This is nice because it means we load multiple packages and ensure they are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug errors.", "Any class that you import from a PackageImporter will be a version of the class specific to that importer. For example:", "In this example, MyClass and imported_MyClass are not the same type. In this specific example, MyClass and imported_MyClass have exactly the same implementation, so you might think it\u2019s okay to consider them the same class. But consider the situation where imported_MyClass is coming from an older package with an entirely different implementation of MyClass \u2014 in that case, it\u2019s unsafe to consider them the same class.", "Under the hood, each importer has a prefix that allows it to uniquely identify classes:", "That means you should not expect isinstance checks to work when one of the arguments is from a package and the other is not. If you need this functionality, consider the following options:", "Each PackageImporter instance creates an independent, isolated environment for its modules and objects. Modules in a package can only import other packaged modules, or modules marked extern. If you use multiple PackageImporter instances to load a single package, you will get multiple independent environments that do not interact.", "This is achieved by extending Python\u2019s import infrastructure with a custom importer. PackageImporter provides the same core API as the importlib importer; namely, it implements the import_module and __import__ methods.", "When you invoke PackageImporter.import_module(), PackageImporter will construct and return a new module, much as the system importer does. However, PackageImporter patches the returned module to use self (i.e. that PackageImporter instance) to fulfill future import requests by looking in the package rather than searching the user\u2019s Python environment.", "To avoid confusion (\u201cis this foo.bar object the one from my package, or the one from my Python environment?\u201d), PackageImporter mangles the __name__ and __file__ of all imported modules, by adding a mangle prefix to them.", "For __name__, a name like torchvision.models.resnet18 becomes <torch_package_0>.torchvision.models.resnet18.", "For __file__, a name like torchvision/models/resnet18.py becomes <torch_package_0>.torchvision/modules/resnet18.py.", "Name mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print statements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consult mangling.md in torch/package/.", "This exception is raised when there is an issue with exporting a package. PackageExporter will attempt to gather up all the errors and present them to you at once.", "This is an exception that is thrown when a mock or extern is marked as allow_empty=False, and is not matched with any module during packaging.", "Exporters allow you to write packages of code, pickled Python data, and arbitrary binary and text resources into a self-contained package.", "Imports can load this code in a hermetic way, such that code is loaded from the package rather than the normal Python import system. This allows for the packaging of PyTorch model code and data so that it can be run on a server or used in the future for transfer learning.", "The code contained in packages is copied file-by-file from the original source when it is created, and the file format is a specially organized zip file. Future users of the package can unzip the package, and edit the code in order to perform custom modifications to it.", "The importer for packages ensures that code in the module can only be loaded from within the package, except for modules explicitly listed as external using extern(). The file extern_modules in the zip archive lists all the modules that a package externally depends on. This prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing a locally-installed package, but then fails when the package is copied to another machine.", "When source code is added to the package, the exporter can optionally scan it for further code dependencies (dependencies=True). It looks for import statements, resolves relative references to qualified module names, and performs an action specified by the user (See: extern(), mock(), and intern()).", "Create an exporter.", "Given a module, add it to the dependency graph according to patterns specified by the user.", "that has all paths from src to dst.", "A dot representation containing all paths from src to dst. (https://graphviz.org/doc/info/lang.html)", "str", "Write the package to the filesystem. Any calls after close() are now invalid. It is preferable to use resource guard syntax instead:", "Return all modules that are currently denied.", "A list containing the names of modules which will be denied in this package.", "List[str]", "Blocklist modules who names match the given glob patterns from the list of modules the package can import. If a dependency on any matching packages is found, a PackagingError is raised.", "Returns digraph string representation of dependencies in package.", "A string representation of dependencies in package.", "str", "Include module in the list of external modules the package can import. This will prevent dependency discovery from saving it in the package. The importer will load an external module directly from the standard import system. Code for extern modules must also exist in the process loading the package.", "Return all modules that are currently externed.", "A list containing the names of modules which will be externed in this package.", "List[str]", "Return a list of all modules which depend on the module module_name.", "A list containing the names of modules which depend on module_name.", "List[str]", "Get an id. This id is guaranteed to only be handed out once for this package.", "str", "Specify modules that should be packaged. A module must match some intern pattern in order to be included in the package and have its dependencies processed recursively.", "Return all modules that are currently interned.", "A list containing the names of modules which will be interned in this package.", "List[str]", "Replace some required modules with a mock implementation. Mocked modules will return a fake object for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes find files that are imported by model files but whose functionality is never used (e.g. custom serialization code or training helpers). Use this function to mock this functionality out without having to modify the original code.", "include (Union[List[str], str]) \u2013 ", "A string e.g. \"my_package.my_subpackage\", or list of strings for the names of the modules to be mocked out. Strings can also be a glob-style pattern string that may match multiple modules. Any required dependencies that match this pattern string will be mocked out automatically.", "'torch.**' \u2013 matches torch and all submodules of torch, e.g. 'torch.nn' and 'torch.nn.functional'", "'torch.*' \u2013 matches 'torch.nn' or 'torch.functional', but not 'torch.nn.functional'", "Return all modules that are currently mocked.", "A list containing the names of modules which will be mocked in this package.", "List[str]", "Registers an extern hook on the exporter.", "The hook will be called each time a module matches against an extern() pattern. It should have the following signature:", "Hooks will be called in order of registration.", "A handle that can be used to remove the added hook by calling handle.remove().", "torch.utils.hooks.RemovableHandle", "Registers an intern hook on the exporter.", "The hook will be called each time a module matches against an intern() pattern. It should have the following signature:", "Hooks will be called in order of registration.", "A handle that can be used to remove the added hook by calling handle.remove().", "torch.utils.hooks.RemovableHandle", "Registers a mock hook on the exporter.", "The hook will be called each time a module matches against a mock() pattern. It should have the following signature:", "Hooks will be called in order of registration.", "A handle that can be used to remove the added hook by calling handle.remove().", "torch.utils.hooks.RemovableHandle", "Save raw bytes to the package.", "Save the code for module into the package. Code for the module is resolved using the importers path to find the module object, and then using its __file__ attribute to find the source code.", "Save a python object to the archive using pickle. Equivalent to torch.save() but saving into the archive rather than a stand-alone file. Standard pickle does not save the code, only the objects. If dependencies is true, this method will also scan the pickled objects for which modules are required to reconstruct them and save the relevant code.", "To be able to save an object where type(obj).__name__ is my_module.MyObject, my_module.MyObject must resolve to the class of the object according to the importer order. When saving objects that have previously been packaged, the importer\u2019s import_module method will need to be present in the importer list for this to work.", "Adds the local file system file_or_directory to the source package to provide the code for module_name.", "Adds src as the source code for module_name in the exported package.", "Save text data to the package.", "Importers allow you to load code written to packages by PackageExporter. Code is loaded in a hermetic way, using files from the package rather than the normal python import system. This allows for the packaging of PyTorch model code and data so that it can be run on a server or used in the future for transfer learning.", "The importer for packages ensures that code in the module can only be loaded from within the package, except for modules explicitly listed as external during export. The file extern_modules in the zip archive lists all the modules that a package externally depends on. This prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing a locally-installed package, but then fails when the package is copied to another machine.", "Open file_or_buffer for importing. This checks that the imported package only requires modules allowed by module_allowed", "ImportError \u2013 If the package will use a disallowed module.", "Returns a file structure representation of package\u2019s zipfile.", "Directory", "Directory", "Returns internal identifier that torch.package uses to distinguish PackageImporter instances. Looks like:", "Load a module from the package if it hasn\u2019t already been loaded, and then return the module. Modules are loaded locally to the importer and will appear in self.modules rather than sys.modules.", "The (possibly already) loaded module.", "types.ModuleType", "Load raw bytes.", "The loaded data.", "bytes", "Unpickles the resource from the package, loading any modules that are needed to construct the objects using import_module().", "The unpickled object.", "Any", "Load a string.", "The loaded text.", "str", "Returns the version of python that was used to create this package.", "Note: this function is experimental and not Forward Compatible. The plan is to move this into a lock file later on.", "Optional[str] a python version e.g. 3.8.9 or None if no version was stored with this package", "A file structure representation. Organized as Directory nodes that have lists of their Directory children. Directories for a package are created by calling PackageImporter.file_structure().", "Checks if a file is present in a Directory.", "filename (str) \u2013 Path of file to search for.", "If a Directory contains the specified file.", "bool"]}, {"name": "torch.package.Directory", "path": "package#torch.package.Directory", "type": "Package", "text": ["A file structure representation. Organized as Directory nodes that have lists of their Directory children. Directories for a package are created by calling PackageImporter.file_structure().", "Checks if a file is present in a Directory.", "filename (str) \u2013 Path of file to search for.", "If a Directory contains the specified file.", "bool"]}, {"name": "torch.package.Directory.has_file()", "path": "package#torch.package.Directory.has_file", "type": "Package", "text": ["Checks if a file is present in a Directory.", "filename (str) \u2013 Path of file to search for.", "If a Directory contains the specified file.", "bool"]}, {"name": "torch.package.EmptyMatchError", "path": "package#torch.package.EmptyMatchError", "type": "Package", "text": ["This is an exception that is thrown when a mock or extern is marked as allow_empty=False, and is not matched with any module during packaging."]}, {"name": "torch.package.PackageExporter", "path": "package#torch.package.PackageExporter", "type": "Package", "text": ["Exporters allow you to write packages of code, pickled Python data, and arbitrary binary and text resources into a self-contained package.", "Imports can load this code in a hermetic way, such that code is loaded from the package rather than the normal Python import system. This allows for the packaging of PyTorch model code and data so that it can be run on a server or used in the future for transfer learning.", "The code contained in packages is copied file-by-file from the original source when it is created, and the file format is a specially organized zip file. Future users of the package can unzip the package, and edit the code in order to perform custom modifications to it.", "The importer for packages ensures that code in the module can only be loaded from within the package, except for modules explicitly listed as external using extern(). The file extern_modules in the zip archive lists all the modules that a package externally depends on. This prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing a locally-installed package, but then fails when the package is copied to another machine.", "When source code is added to the package, the exporter can optionally scan it for further code dependencies (dependencies=True). It looks for import statements, resolves relative references to qualified module names, and performs an action specified by the user (See: extern(), mock(), and intern()).", "Create an exporter.", "Given a module, add it to the dependency graph according to patterns specified by the user.", "that has all paths from src to dst.", "A dot representation containing all paths from src to dst. (https://graphviz.org/doc/info/lang.html)", "str", "Write the package to the filesystem. Any calls after close() are now invalid. It is preferable to use resource guard syntax instead:", "Return all modules that are currently denied.", "A list containing the names of modules which will be denied in this package.", "List[str]", "Blocklist modules who names match the given glob patterns from the list of modules the package can import. If a dependency on any matching packages is found, a PackagingError is raised.", "Returns digraph string representation of dependencies in package.", "A string representation of dependencies in package.", "str", "Include module in the list of external modules the package can import. This will prevent dependency discovery from saving it in the package. The importer will load an external module directly from the standard import system. Code for extern modules must also exist in the process loading the package.", "Return all modules that are currently externed.", "A list containing the names of modules which will be externed in this package.", "List[str]", "Return a list of all modules which depend on the module module_name.", "A list containing the names of modules which depend on module_name.", "List[str]", "Get an id. This id is guaranteed to only be handed out once for this package.", "str", "Specify modules that should be packaged. A module must match some intern pattern in order to be included in the package and have its dependencies processed recursively.", "Return all modules that are currently interned.", "A list containing the names of modules which will be interned in this package.", "List[str]", "Replace some required modules with a mock implementation. Mocked modules will return a fake object for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes find files that are imported by model files but whose functionality is never used (e.g. custom serialization code or training helpers). Use this function to mock this functionality out without having to modify the original code.", "include (Union[List[str], str]) \u2013 ", "A string e.g. \"my_package.my_subpackage\", or list of strings for the names of the modules to be mocked out. Strings can also be a glob-style pattern string that may match multiple modules. Any required dependencies that match this pattern string will be mocked out automatically.", "'torch.**' \u2013 matches torch and all submodules of torch, e.g. 'torch.nn' and 'torch.nn.functional'", "'torch.*' \u2013 matches 'torch.nn' or 'torch.functional', but not 'torch.nn.functional'", "Return all modules that are currently mocked.", "A list containing the names of modules which will be mocked in this package.", "List[str]", "Registers an extern hook on the exporter.", "The hook will be called each time a module matches against an extern() pattern. It should have the following signature:", "Hooks will be called in order of registration.", "A handle that can be used to remove the added hook by calling handle.remove().", "torch.utils.hooks.RemovableHandle", "Registers an intern hook on the exporter.", "The hook will be called each time a module matches against an intern() pattern. It should have the following signature:", "Hooks will be called in order of registration.", "A handle that can be used to remove the added hook by calling handle.remove().", "torch.utils.hooks.RemovableHandle", "Registers a mock hook on the exporter.", "The hook will be called each time a module matches against a mock() pattern. It should have the following signature:", "Hooks will be called in order of registration.", "A handle that can be used to remove the added hook by calling handle.remove().", "torch.utils.hooks.RemovableHandle", "Save raw bytes to the package.", "Save the code for module into the package. Code for the module is resolved using the importers path to find the module object, and then using its __file__ attribute to find the source code.", "Save a python object to the archive using pickle. Equivalent to torch.save() but saving into the archive rather than a stand-alone file. Standard pickle does not save the code, only the objects. If dependencies is true, this method will also scan the pickled objects for which modules are required to reconstruct them and save the relevant code.", "To be able to save an object where type(obj).__name__ is my_module.MyObject, my_module.MyObject must resolve to the class of the object according to the importer order. When saving objects that have previously been packaged, the importer\u2019s import_module method will need to be present in the importer list for this to work.", "Adds the local file system file_or_directory to the source package to provide the code for module_name.", "Adds src as the source code for module_name in the exported package.", "Save text data to the package."]}, {"name": "torch.package.PackageExporter.__init__()", "path": "package#torch.package.PackageExporter.__init__", "type": "Package", "text": ["Create an exporter."]}, {"name": "torch.package.PackageExporter.add_dependency()", "path": "package#torch.package.PackageExporter.add_dependency", "type": "Package", "text": ["Given a module, add it to the dependency graph according to patterns specified by the user."]}, {"name": "torch.package.PackageExporter.all_paths()", "path": "package#torch.package.PackageExporter.all_paths", "type": "Package", "text": ["that has all paths from src to dst.", "A dot representation containing all paths from src to dst. (https://graphviz.org/doc/info/lang.html)", "str"]}, {"name": "torch.package.PackageExporter.close()", "path": "package#torch.package.PackageExporter.close", "type": "Package", "text": ["Write the package to the filesystem. Any calls after close() are now invalid. It is preferable to use resource guard syntax instead:"]}, {"name": "torch.package.PackageExporter.denied_modules()", "path": "package#torch.package.PackageExporter.denied_modules", "type": "Package", "text": ["Return all modules that are currently denied.", "A list containing the names of modules which will be denied in this package.", "List[str]"]}, {"name": "torch.package.PackageExporter.deny()", "path": "package#torch.package.PackageExporter.deny", "type": "Package", "text": ["Blocklist modules who names match the given glob patterns from the list of modules the package can import. If a dependency on any matching packages is found, a PackagingError is raised."]}, {"name": "torch.package.PackageExporter.dependency_graph_string()", "path": "package#torch.package.PackageExporter.dependency_graph_string", "type": "Package", "text": ["Returns digraph string representation of dependencies in package.", "A string representation of dependencies in package.", "str"]}, {"name": "torch.package.PackageExporter.extern()", "path": "package#torch.package.PackageExporter.extern", "type": "Package", "text": ["Include module in the list of external modules the package can import. This will prevent dependency discovery from saving it in the package. The importer will load an external module directly from the standard import system. Code for extern modules must also exist in the process loading the package."]}, {"name": "torch.package.PackageExporter.externed_modules()", "path": "package#torch.package.PackageExporter.externed_modules", "type": "Package", "text": ["Return all modules that are currently externed.", "A list containing the names of modules which will be externed in this package.", "List[str]"]}, {"name": "torch.package.PackageExporter.get_rdeps()", "path": "package#torch.package.PackageExporter.get_rdeps", "type": "Package", "text": ["Return a list of all modules which depend on the module module_name.", "A list containing the names of modules which depend on module_name.", "List[str]"]}, {"name": "torch.package.PackageExporter.get_unique_id()", "path": "package#torch.package.PackageExporter.get_unique_id", "type": "Package", "text": ["Get an id. This id is guaranteed to only be handed out once for this package.", "str"]}, {"name": "torch.package.PackageExporter.intern()", "path": "package#torch.package.PackageExporter.intern", "type": "Package", "text": ["Specify modules that should be packaged. A module must match some intern pattern in order to be included in the package and have its dependencies processed recursively."]}, {"name": "torch.package.PackageExporter.interned_modules()", "path": "package#torch.package.PackageExporter.interned_modules", "type": "Package", "text": ["Return all modules that are currently interned.", "A list containing the names of modules which will be interned in this package.", "List[str]"]}, {"name": "torch.package.PackageExporter.mock()", "path": "package#torch.package.PackageExporter.mock", "type": "Package", "text": ["Replace some required modules with a mock implementation. Mocked modules will return a fake object for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes find files that are imported by model files but whose functionality is never used (e.g. custom serialization code or training helpers). Use this function to mock this functionality out without having to modify the original code.", "include (Union[List[str], str]) \u2013 ", "A string e.g. \"my_package.my_subpackage\", or list of strings for the names of the modules to be mocked out. Strings can also be a glob-style pattern string that may match multiple modules. Any required dependencies that match this pattern string will be mocked out automatically.", "'torch.**' \u2013 matches torch and all submodules of torch, e.g. 'torch.nn' and 'torch.nn.functional'", "'torch.*' \u2013 matches 'torch.nn' or 'torch.functional', but not 'torch.nn.functional'"]}, {"name": "torch.package.PackageExporter.mocked_modules()", "path": "package#torch.package.PackageExporter.mocked_modules", "type": "Package", "text": ["Return all modules that are currently mocked.", "A list containing the names of modules which will be mocked in this package.", "List[str]"]}, {"name": "torch.package.PackageExporter.register_extern_hook()", "path": "package#torch.package.PackageExporter.register_extern_hook", "type": "Package", "text": ["Registers an extern hook on the exporter.", "The hook will be called each time a module matches against an extern() pattern. It should have the following signature:", "Hooks will be called in order of registration.", "A handle that can be used to remove the added hook by calling handle.remove().", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.package.PackageExporter.register_intern_hook()", "path": "package#torch.package.PackageExporter.register_intern_hook", "type": "Package", "text": ["Registers an intern hook on the exporter.", "The hook will be called each time a module matches against an intern() pattern. It should have the following signature:", "Hooks will be called in order of registration.", "A handle that can be used to remove the added hook by calling handle.remove().", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.package.PackageExporter.register_mock_hook()", "path": "package#torch.package.PackageExporter.register_mock_hook", "type": "Package", "text": ["Registers a mock hook on the exporter.", "The hook will be called each time a module matches against a mock() pattern. It should have the following signature:", "Hooks will be called in order of registration.", "A handle that can be used to remove the added hook by calling handle.remove().", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.package.PackageExporter.save_binary()", "path": "package#torch.package.PackageExporter.save_binary", "type": "Package", "text": ["Save raw bytes to the package."]}, {"name": "torch.package.PackageExporter.save_module()", "path": "package#torch.package.PackageExporter.save_module", "type": "Package", "text": ["Save the code for module into the package. Code for the module is resolved using the importers path to find the module object, and then using its __file__ attribute to find the source code."]}, {"name": "torch.package.PackageExporter.save_pickle()", "path": "package#torch.package.PackageExporter.save_pickle", "type": "Package", "text": ["Save a python object to the archive using pickle. Equivalent to torch.save() but saving into the archive rather than a stand-alone file. Standard pickle does not save the code, only the objects. If dependencies is true, this method will also scan the pickled objects for which modules are required to reconstruct them and save the relevant code.", "To be able to save an object where type(obj).__name__ is my_module.MyObject, my_module.MyObject must resolve to the class of the object according to the importer order. When saving objects that have previously been packaged, the importer\u2019s import_module method will need to be present in the importer list for this to work."]}, {"name": "torch.package.PackageExporter.save_source_file()", "path": "package#torch.package.PackageExporter.save_source_file", "type": "Package", "text": ["Adds the local file system file_or_directory to the source package to provide the code for module_name."]}, {"name": "torch.package.PackageExporter.save_source_string()", "path": "package#torch.package.PackageExporter.save_source_string", "type": "Package", "text": ["Adds src as the source code for module_name in the exported package."]}, {"name": "torch.package.PackageExporter.save_text()", "path": "package#torch.package.PackageExporter.save_text", "type": "Package", "text": ["Save text data to the package."]}, {"name": "torch.package.PackageImporter", "path": "package#torch.package.PackageImporter", "type": "Package", "text": ["Importers allow you to load code written to packages by PackageExporter. Code is loaded in a hermetic way, using files from the package rather than the normal python import system. This allows for the packaging of PyTorch model code and data so that it can be run on a server or used in the future for transfer learning.", "The importer for packages ensures that code in the module can only be loaded from within the package, except for modules explicitly listed as external during export. The file extern_modules in the zip archive lists all the modules that a package externally depends on. This prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing a locally-installed package, but then fails when the package is copied to another machine.", "Open file_or_buffer for importing. This checks that the imported package only requires modules allowed by module_allowed", "ImportError \u2013 If the package will use a disallowed module.", "Returns a file structure representation of package\u2019s zipfile.", "Directory", "Directory", "Returns internal identifier that torch.package uses to distinguish PackageImporter instances. Looks like:", "Load a module from the package if it hasn\u2019t already been loaded, and then return the module. Modules are loaded locally to the importer and will appear in self.modules rather than sys.modules.", "The (possibly already) loaded module.", "types.ModuleType", "Load raw bytes.", "The loaded data.", "bytes", "Unpickles the resource from the package, loading any modules that are needed to construct the objects using import_module().", "The unpickled object.", "Any", "Load a string.", "The loaded text.", "str", "Returns the version of python that was used to create this package.", "Note: this function is experimental and not Forward Compatible. The plan is to move this into a lock file later on.", "Optional[str] a python version e.g. 3.8.9 or None if no version was stored with this package"]}, {"name": "torch.package.PackageImporter.__init__()", "path": "package#torch.package.PackageImporter.__init__", "type": "Package", "text": ["Open file_or_buffer for importing. This checks that the imported package only requires modules allowed by module_allowed", "ImportError \u2013 If the package will use a disallowed module."]}, {"name": "torch.package.PackageImporter.file_structure()", "path": "package#torch.package.PackageImporter.file_structure", "type": "Package", "text": ["Returns a file structure representation of package\u2019s zipfile.", "Directory", "Directory"]}, {"name": "torch.package.PackageImporter.id()", "path": "package#torch.package.PackageImporter.id", "type": "Package", "text": ["Returns internal identifier that torch.package uses to distinguish PackageImporter instances. Looks like:"]}, {"name": "torch.package.PackageImporter.import_module()", "path": "package#torch.package.PackageImporter.import_module", "type": "Package", "text": ["Load a module from the package if it hasn\u2019t already been loaded, and then return the module. Modules are loaded locally to the importer and will appear in self.modules rather than sys.modules.", "The (possibly already) loaded module.", "types.ModuleType"]}, {"name": "torch.package.PackageImporter.load_binary()", "path": "package#torch.package.PackageImporter.load_binary", "type": "Package", "text": ["Load raw bytes.", "The loaded data.", "bytes"]}, {"name": "torch.package.PackageImporter.load_pickle()", "path": "package#torch.package.PackageImporter.load_pickle", "type": "Package", "text": ["Unpickles the resource from the package, loading any modules that are needed to construct the objects using import_module().", "The unpickled object.", "Any"]}, {"name": "torch.package.PackageImporter.load_text()", "path": "package#torch.package.PackageImporter.load_text", "type": "Package", "text": ["Load a string.", "The loaded text.", "str"]}, {"name": "torch.package.PackageImporter.python_version()", "path": "package#torch.package.PackageImporter.python_version", "type": "Package", "text": ["Returns the version of python that was used to create this package.", "Note: this function is experimental and not Forward Compatible. The plan is to move this into a lock file later on.", "Optional[str] a python version e.g. 3.8.9 or None if no version was stored with this package"]}, {"name": "torch.package.PackagingError", "path": "package#torch.package.PackagingError", "type": "Package", "text": ["This exception is raised when there is an issue with exporting a package. PackageExporter will attempt to gather up all the errors and present them to you at once."]}, {"name": "torch.pca_lowrank", "path": "generated/torch.pca_lowrank", "type": "Torch", "text": ["Performs linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix.", "This function returns a namedtuple (U, S, V) which is the nearly optimal approximation of a singular value decomposition of a centered matrix AA such that A=Udiag(S)VTA = U diag(S) V^T.", "Note", "The relation of (U, S, V) to PCA is as follows:", "Note", "Different from the standard SVD, the size of returned matrices depend on the specified rank and q values as follows:", "Note", "To obtain repeatable results, reset the seed for the pseudorandom number generator", "Tuple[Tensor, Tensor, Tensor]", "References:"]}, {"name": "torch.permute", "path": "generated/torch.permute", "type": "Torch", "text": ["Returns a view of the original tensor input with its dimensions permuted."]}, {"name": "torch.pinverse", "path": "generated/torch.pinverse", "type": "Torch", "text": ["Alias for torch.linalg.pinv()"]}, {"name": "torch.poisson", "path": "generated/torch.poisson", "type": "Torch", "text": ["Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,", "input must be non-negative.", "input (Tensor) \u2013 the input tensor containing the rates of the Poisson distribution", "generator (torch.Generator, optional) \u2013 a pseudorandom number generator for sampling", "Example:"]}, {"name": "torch.polar", "path": "generated/torch.polar", "type": "Torch", "text": ["Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value abs and angle angle.", "Note", "torch.polar is similar to std::polar and does not compute the polar decomposition of a complex tensor like Python\u2019s cmath.polar and SciPy\u2019s linalg.polar do. The behavior of this function is undefined if abs is negative or NaN, or if angle is infinite.", "out (Tensor) \u2013 If the inputs are torch.float32, must be torch.complex64. If the inputs are torch.float64, must be torch.complex128.", "Example:"]}, {"name": "torch.polygamma", "path": "generated/torch.polygamma", "type": "Torch", "text": ["Alias for torch.special.polygamma()."]}, {"name": "torch.positive", "path": "generated/torch.positive", "type": "Torch", "text": ["Returns input. Throws a runtime error if input is a bool tensor.", "input (Tensor) \u2013 the input tensor.", "Example:"]}, {"name": "torch.pow", "path": "generated/torch.pow", "type": "Torch", "text": ["Takes the power of each element in input with exponent and returns a tensor with the result.", "exponent can be either a single float number or a Tensor with the same number of elements as input.", "When exponent is a scalar value, the operation applied is:", "When exponent is a tensor, the operation applied is:", "When exponent is a tensor, the shapes of input and exponent must be broadcastable.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "self is a scalar float value, and exponent is a tensor. The returned tensor out is of the same shape as exponent", "The operation applied is:", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.prod", "path": "generated/torch.prod", "type": "Torch", "text": ["Returns the product of all elements in the input tensor.", "input (Tensor) \u2013 the input tensor.", "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.", "Example:", "Returns the product of each row of the input tensor in the given dimension dim.", "If keepdim is True, the output tensor is of the same size as input except in the dimension dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension than input.", "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.", "Example:"]}, {"name": "torch.profiler", "path": "profiler", "type": "Profiler", "text": ["PyTorch Profiler is a tool that allows the collection of performance metrics during training and inference. Profiler\u2019s context manager API can be used to better understand what model operators are the most expensive, examine their input shapes and stack traces, study device kernel activity and visualize the execution trace.", "Note", "An earlier version of the API in torch.autograd module is considered legacy and will be deprecated.", "Low-level profiler wrap the autograd profile", "Note", "This API is experimental and subject to change in the future.", "Enabling shape and stack tracing results in additional overhead. When record_shapes=True is specified, profiler will temporarily hold references to the tensors; that may further prevent certain optimizations that depend on the reference count and introduce extra tensor copies.", "Adds a user defined metadata with a string key and a string value into the trace file", "Adds a user defined metadata with a string key and a valid json value into the trace file", "Returns the list of unaggregated profiler events, to be used in the trace callback or after the profiling is finished", "Exports the collected trace in Chrome JSON format.", "Extract the memory information from the memory profile collected tree for a given device, and export a timeline plot consisting of [times, [sizes by category]], where times are timestamps and sizes are memory usage for each category. The memory timeline plot will be saved a JSON (by default) or gzipped JSON.", "Input: (path of file, device) Output: File written as JSON or gzipped JSON", "Save stack traces in a file in a format suitable for visualization.", "Note", "Example of using FlameGraph tool:", "Averages events, grouping them by operator name and (optionally) input shapes and stack.", "Note", "To use shape/stack functionality make sure to set record_shapes/with_stack when creating profiler context manager.", "Profiler context manager.", "use_cuda (bool) \u2013 ", "Deprecated since version 1.8.1: use activities instead.", "Note", "Use schedule() to generate the callable schedule. Non-default schedules are useful when profiling long training jobs and allow the user to obtain multiple traces at the different iterations of the training process. The default schedule simply records all the events continuously for the duration of the context manager.", "Note", "Use tensorboard_trace_handler() to generate result files for TensorBoard:", "on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name)", "After profiling, result files can be found in the specified directory. Use the command:", "tensorboard --logdir dir_name", "to see the results in TensorBoard. For more information, see PyTorch Profiler TensorBoard Plugin", "Note", "Enabling shape and stack tracing results in additional overhead. When record_shapes=True is specified, profiler will temporarily hold references to the tensors; that may further prevent certain optimizations that depend on the reference count and introduce extra tensor copies.", "Examples:", "Using the profiler\u2019s schedule, on_trace_ready and step functions:", "Signals the profiler that the next profiling step has started.", "Profiler actions that can be taken at the specified intervals", "Members:", "CPU", "XPU", "MTIA", "CUDA", "Returns a callable that can be used as profiler schedule argument. The profiler will skip the first skip_first steps, then wait for wait steps, then do the warmup for the next warmup steps, then do the active recording for the next active steps and then repeat the cycle starting with wait steps. The optional number of cycles is specified with the repeat parameter, the zero value means that the cycles will continue until the profiling is finished.", "Callable", "Outputs tracing files to directory of dir_name, then that directory can be directly delivered to tensorboard as logdir. worker_name should be unique for each worker in distributed scenario, it will be set to \u2018[hostname]_[pid]\u2019 by default.", "Check if ITT feature is available or not", "Describe an instantaneous event that occurred at some point.", "msg (str) \u2013 ASCII message to associate with the event.", "Pushes a range onto a stack of nested range span. Returns zero-based depth of the range that is started.", "msg (str) \u2013 ASCII message to associate with range", "Pops a range off of a stack of nested range spans. Returns the zero-based depth of the range that is ended."]}, {"name": "torch.profiler._KinetoProfile", "path": "profiler#torch.profiler._KinetoProfile", "type": "Profiler", "text": ["Low-level profiler wrap the autograd profile", "Note", "This API is experimental and subject to change in the future.", "Enabling shape and stack tracing results in additional overhead. When record_shapes=True is specified, profiler will temporarily hold references to the tensors; that may further prevent certain optimizations that depend on the reference count and introduce extra tensor copies.", "Adds a user defined metadata with a string key and a string value into the trace file", "Adds a user defined metadata with a string key and a valid json value into the trace file", "Returns the list of unaggregated profiler events, to be used in the trace callback or after the profiling is finished", "Exports the collected trace in Chrome JSON format.", "Extract the memory information from the memory profile collected tree for a given device, and export a timeline plot consisting of [times, [sizes by category]], where times are timestamps and sizes are memory usage for each category. The memory timeline plot will be saved a JSON (by default) or gzipped JSON.", "Input: (path of file, device) Output: File written as JSON or gzipped JSON", "Save stack traces in a file in a format suitable for visualization.", "Note", "Example of using FlameGraph tool:", "Averages events, grouping them by operator name and (optionally) input shapes and stack.", "Note", "To use shape/stack functionality make sure to set record_shapes/with_stack when creating profiler context manager."]}, {"name": "torch.profiler._KinetoProfile.add_metadata()", "path": "profiler#torch.profiler._KinetoProfile.add_metadata", "type": "Profiler", "text": ["Adds a user defined metadata with a string key and a string value into the trace file"]}, {"name": "torch.profiler._KinetoProfile.add_metadata_json()", "path": "profiler#torch.profiler._KinetoProfile.add_metadata_json", "type": "Profiler", "text": ["Adds a user defined metadata with a string key and a valid json value into the trace file"]}, {"name": "torch.profiler._KinetoProfile.events()", "path": "profiler#torch.profiler._KinetoProfile.events", "type": "Profiler", "text": ["Returns the list of unaggregated profiler events, to be used in the trace callback or after the profiling is finished"]}, {"name": "torch.profiler._KinetoProfile.export_chrome_trace()", "path": "profiler#torch.profiler._KinetoProfile.export_chrome_trace", "type": "Profiler", "text": ["Exports the collected trace in Chrome JSON format."]}, {"name": "torch.profiler._KinetoProfile.export_memory_timeline()", "path": "profiler#torch.profiler._KinetoProfile.export_memory_timeline", "type": "Profiler", "text": ["Extract the memory information from the memory profile collected tree for a given device, and export a timeline plot consisting of [times, [sizes by category]], where times are timestamps and sizes are memory usage for each category. The memory timeline plot will be saved a JSON (by default) or gzipped JSON.", "Input: (path of file, device) Output: File written as JSON or gzipped JSON"]}, {"name": "torch.profiler._KinetoProfile.export_stacks()", "path": "profiler#torch.profiler._KinetoProfile.export_stacks", "type": "Profiler", "text": ["Save stack traces in a file in a format suitable for visualization.", "Note", "Example of using FlameGraph tool:"]}, {"name": "torch.profiler._KinetoProfile.key_averages()", "path": "profiler#torch.profiler._KinetoProfile.key_averages", "type": "Profiler", "text": ["Averages events, grouping them by operator name and (optionally) input shapes and stack.", "Note", "To use shape/stack functionality make sure to set record_shapes/with_stack when creating profiler context manager."]}, {"name": "torch.profiler.itt.is_available()", "path": "profiler#torch.profiler.itt.is_available", "type": "Profiler", "text": ["Check if ITT feature is available or not"]}, {"name": "torch.profiler.itt.mark()", "path": "profiler#torch.profiler.itt.mark", "type": "Profiler", "text": ["Describe an instantaneous event that occurred at some point.", "msg (str) \u2013 ASCII message to associate with the event."]}, {"name": "torch.profiler.itt.range_pop()", "path": "profiler#torch.profiler.itt.range_pop", "type": "Profiler", "text": ["Pops a range off of a stack of nested range spans. Returns the zero-based depth of the range that is ended."]}, {"name": "torch.profiler.itt.range_push()", "path": "profiler#torch.profiler.itt.range_push", "type": "Profiler", "text": ["Pushes a range onto a stack of nested range span. Returns zero-based depth of the range that is started.", "msg (str) \u2013 ASCII message to associate with range"]}, {"name": "torch.profiler.profile", "path": "profiler#torch.profiler.profile", "type": "Profiler", "text": ["Profiler context manager.", "use_cuda (bool) \u2013 ", "Deprecated since version 1.8.1: use activities instead.", "Note", "Use schedule() to generate the callable schedule. Non-default schedules are useful when profiling long training jobs and allow the user to obtain multiple traces at the different iterations of the training process. The default schedule simply records all the events continuously for the duration of the context manager.", "Note", "Use tensorboard_trace_handler() to generate result files for TensorBoard:", "on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name)", "After profiling, result files can be found in the specified directory. Use the command:", "tensorboard --logdir dir_name", "to see the results in TensorBoard. For more information, see PyTorch Profiler TensorBoard Plugin", "Note", "Enabling shape and stack tracing results in additional overhead. When record_shapes=True is specified, profiler will temporarily hold references to the tensors; that may further prevent certain optimizations that depend on the reference count and introduce extra tensor copies.", "Examples:", "Using the profiler\u2019s schedule, on_trace_ready and step functions:", "Signals the profiler that the next profiling step has started."]}, {"name": "torch.profiler.profile.step()", "path": "profiler#torch.profiler.profile.step", "type": "Profiler", "text": ["Signals the profiler that the next profiling step has started."]}, {"name": "torch.profiler.ProfilerAction", "path": "profiler#torch.profiler.ProfilerAction", "type": "Profiler", "text": ["Profiler actions that can be taken at the specified intervals"]}, {"name": "torch.profiler.ProfilerActivity", "path": "profiler#torch.profiler.ProfilerActivity", "type": "Profiler", "text": ["Members:", "CPU", "XPU", "MTIA", "CUDA"]}, {"name": "torch.profiler.ProfilerActivity.name", "path": "profiler#torch.profiler.ProfilerActivity.name", "type": "Profiler", "text": []}, {"name": "torch.profiler.schedule()", "path": "profiler#torch.profiler.schedule", "type": "Profiler", "text": ["Returns a callable that can be used as profiler schedule argument. The profiler will skip the first skip_first steps, then wait for wait steps, then do the warmup for the next warmup steps, then do the active recording for the next active steps and then repeat the cycle starting with wait steps. The optional number of cycles is specified with the repeat parameter, the zero value means that the cycles will continue until the profiling is finished.", "Callable"]}, {"name": "torch.profiler.tensorboard_trace_handler()", "path": "profiler#torch.profiler.tensorboard_trace_handler", "type": "Profiler", "text": ["Outputs tracing files to directory of dir_name, then that directory can be directly delivered to tensorboard as logdir. worker_name should be unique for each worker in distributed scenario, it will be set to \u2018[hostname]_[pid]\u2019 by default."]}, {"name": "torch.promote_types", "path": "generated/torch.promote_types", "type": "Torch", "text": ["Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2. See type promotion documentation for more information on the type promotion logic.", "Example:"]}, {"name": "torch.QInt32Storage", "path": "storage#torch.QInt32Storage", "type": "Storage", "text": []}, {"name": "torch.QInt32Storage.dtype", "path": "storage#torch.QInt32Storage.dtype", "type": "Storage", "text": []}, {"name": "torch.QInt8Storage", "path": "storage#torch.QInt8Storage", "type": "Storage", "text": []}, {"name": "torch.QInt8Storage.dtype", "path": "storage#torch.QInt8Storage.dtype", "type": "Storage", "text": []}, {"name": "torch.qr", "path": "generated/torch.qr", "type": "Torch", "text": ["Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q R with QQ being an orthogonal matrix or batch of orthogonal matrices and RR being an upper triangular matrix or batch of upper triangular matrices.", "If some is True, then this function returns the thin (reduced) QR factorization. Otherwise, if some is False, this function returns the complete QR factorization.", "Warning", "torch.qr() is deprecated in favor of torch.linalg.qr() and will be removed in a future PyTorch release. The boolean parameter some has been replaced with a string parameter mode.", "Q, R = torch.qr(A) should be replaced with", "Q, R = torch.qr(A, some=False) should be replaced with", "Warning", "If you plan to backpropagate through QR, note that the current backward implementation is only well-defined when the first min\u2061(input.size(\u22121),input.size(\u22122))\\min(input.size(-1), input.size(-2)) columns of input are linearly independent. This behavior will probably change once QR supports pivoting.", "Note", "This function uses LAPACK for CPU inputs and MAGMA for CUDA inputs, and may produce different (valid) decompositions on different device types or different platforms.", "some (bool, optional) \u2013 ", "Set to True for reduced QR decomposition and False for complete QR decomposition. If k = min(m, n) then:", "out (tuple, optional) \u2013 tuple of Q and R tensors. The dimensions of Q and R are detailed in the description of some above.", "Example:"]}, {"name": "torch.quantile", "path": "generated/torch.quantile", "type": "Torch", "text": ["Computes the q-th quantiles of each row of the input tensor along the dimension dim.", "To compute the quantile, we map q in [0, 1] to the range of indices [0, n] to find the location of the quantile in the sorted input. If the quantile lies between two data points a < b with indices i and j in the sorted order, result is computed according to the given interpolation method as follows:", "If q is a 1D tensor, the first dimension of the output represents the quantiles and has size equal to the size of q, the remaining dimensions are what remains from the reduction.", "Note", "By default dim is None resulting in the input tensor being flattened before computation.", "Example:"]}, {"name": "torch.quantize_per_channel", "path": "generated/torch.quantize_per_channel", "type": "Torch", "text": ["Converts a float tensor to a per-channel quantized tensor with given scales and zero points.", "A newly quantized tensor", "Tensor", "Example:"]}, {"name": "torch.quantize_per_tensor", "path": "generated/torch.quantize_per_tensor", "type": "Torch", "text": ["Converts a float tensor to a quantized tensor with given scale and zero point.", "A newly quantized tensor or list of quantized tensors.", "Tensor", "Example:"]}, {"name": "torch.quantized_batch_norm", "path": "generated/torch.quantized_batch_norm", "type": "Torch", "text": ["Applies batch normalization on a 4D (NCHW) quantized tensor.", "A quantized tensor with batch normalization applied.", "Tensor", "Example:"]}, {"name": "torch.quantized_max_pool1d", "path": "generated/torch.quantized_max_pool1d", "type": "Torch", "text": ["Applies a 1D max pooling over an input quantized tensor composed of several input planes.", "A quantized tensor with max_pool1d applied.", "Tensor", "Example:"]}, {"name": "torch.quantized_max_pool2d", "path": "generated/torch.quantized_max_pool2d", "type": "Torch", "text": ["Applies a 2D max pooling over an input quantized tensor composed of several input planes.", "A quantized tensor with max_pool2d applied.", "Tensor", "Example:"]}, {"name": "torch.quasirandom.SobolEngine", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine", "type": "Torch", "text": ["The torch.quasirandom.SobolEngine is an engine for generating (scrambled) Sobol sequences. Sobol sequences are an example of low discrepancy quasi-random sequences.", "This implementation of an engine for Sobol sequences is capable of sampling sequences up to a maximum dimension of 21201. It uses direction numbers from https://web.maths.unsw.edu.au/~fkuo/sobol/ obtained using the search criterion D(6) up to the dimension 21201. This is the recommended choice by the authors.", "Examples:", "Function to draw a sequence of n points from a Sobol sequence. Note that the samples are dependent on the previous samples. The size of the result is (n,dimension)(n, dimension).", "Tensor", "Function to draw a sequence of 2**m points from a Sobol sequence. Note that the samples are dependent on the previous samples. The size of the result is (2\u2217\u2217m,dimension)(2**m, dimension).", "Tensor", "Function to fast-forward the state of the SobolEngine by n steps. This is equivalent to drawing n samples without using the samples.", "n (Int) \u2013 The number of steps to fast-forward by.", "Function to reset the SobolEngine to base state."]}, {"name": "torch.quasirandom.SobolEngine.draw()", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine.draw", "type": "Torch", "text": ["Function to draw a sequence of n points from a Sobol sequence. Note that the samples are dependent on the previous samples. The size of the result is (n,dimension)(n, dimension).", "Tensor"]}, {"name": "torch.quasirandom.SobolEngine.draw_base2()", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine.draw_base2", "type": "Torch", "text": ["Function to draw a sequence of 2**m points from a Sobol sequence. Note that the samples are dependent on the previous samples. The size of the result is (2\u2217\u2217m,dimension)(2**m, dimension).", "Tensor"]}, {"name": "torch.quasirandom.SobolEngine.fast_forward()", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine.fast_forward", "type": "Torch", "text": ["Function to fast-forward the state of the SobolEngine by n steps. This is equivalent to drawing n samples without using the samples.", "n (Int) \u2013 The number of steps to fast-forward by."]}, {"name": "torch.quasirandom.SobolEngine.reset()", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine.reset", "type": "Torch", "text": ["Function to reset the SobolEngine to base state."]}, {"name": "torch.QUInt2x4Storage", "path": "storage#torch.QUInt2x4Storage", "type": "Storage", "text": []}, {"name": "torch.QUInt2x4Storage.dtype", "path": "storage#torch.QUInt2x4Storage.dtype", "type": "Storage", "text": []}, {"name": "torch.QUInt4x2Storage", "path": "storage#torch.QUInt4x2Storage", "type": "Storage", "text": []}, {"name": "torch.QUInt4x2Storage.dtype", "path": "storage#torch.QUInt4x2Storage.dtype", "type": "Storage", "text": []}, {"name": "torch.QUInt8Storage", "path": "storage#torch.QUInt8Storage", "type": "Storage", "text": []}, {"name": "torch.QUInt8Storage.dtype", "path": "storage#torch.QUInt8Storage.dtype", "type": "Storage", "text": []}, {"name": "torch.rad2deg", "path": "generated/torch.rad2deg", "type": "Torch", "text": ["Returns a new tensor with each of the elements of input converted from angles in radians to degrees.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.rand", "path": "generated/torch.rand", "type": "Torch", "text": ["Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)", "The shape of the tensor is defined by the variable argument size.", "size (int...) \u2013 a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.", "Example:"]}, {"name": "torch.rand_like", "path": "generated/torch.rand_like", "type": "Torch", "text": ["Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1). torch.rand_like(input) is equivalent to torch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).", "input (Tensor) \u2013 the size of input will determine size of the output tensor."]}, {"name": "torch.randint", "path": "generated/torch.randint", "type": "Torch", "text": ["Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).", "The shape of the tensor is defined by the variable argument size.", "Note", "With the global dtype default (torch.float32), this function returns a tensor with dtype torch.int64.", "Example:"]}, {"name": "torch.randint_like", "path": "generated/torch.randint_like", "type": "Torch", "text": ["Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive)."]}, {"name": "torch.randn", "path": "generated/torch.randn", "type": "Torch", "text": ["Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).", "The shape of the tensor is defined by the variable argument size.", "size (int...) \u2013 a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.", "Example:"]}, {"name": "torch.randn_like", "path": "generated/torch.randn_like", "type": "Torch", "text": ["Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like(input) is equivalent to torch.randn(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).", "input (Tensor) \u2013 the size of input will determine size of the output tensor."]}, {"name": "torch.random", "path": "random", "type": "Miscellaneous", "text": ["Forks the RNG, so that when you return, the RNG is reset to the state that it was previously in.", "Generator", "Returns the random number generator state as a torch.ByteTensor.", "Tensor", "Returns the initial seed for generating random numbers as a Python long.", "int", "Sets the seed for generating random numbers. Returns a torch.Generator object.", "seed (int) \u2013 The desired seed. Value must be within the inclusive range [-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError is raised. Negative inputs are remapped to positive values with the formula 0xffff_ffff_ffff_ffff + seed.", "Sets the seed for generating random numbers to a non-deterministic random number. Returns a 64 bit number used to seed the RNG.", "int", "Sets the random number generator state.", "new_state (torch.ByteTensor) \u2013 The desired state"]}, {"name": "torch.random.fork_rng()", "path": "random#torch.random.fork_rng", "type": "Miscellaneous", "text": ["Forks the RNG, so that when you return, the RNG is reset to the state that it was previously in.", "Generator"]}, {"name": "torch.random.get_rng_state()", "path": "random#torch.random.get_rng_state", "type": "Miscellaneous", "text": ["Returns the random number generator state as a torch.ByteTensor.", "Tensor"]}, {"name": "torch.random.initial_seed()", "path": "random#torch.random.initial_seed", "type": "Miscellaneous", "text": ["Returns the initial seed for generating random numbers as a Python long.", "int"]}, {"name": "torch.random.manual_seed()", "path": "random#torch.random.manual_seed", "type": "Miscellaneous", "text": ["Sets the seed for generating random numbers. Returns a torch.Generator object.", "seed (int) \u2013 The desired seed. Value must be within the inclusive range [-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError is raised. Negative inputs are remapped to positive values with the formula 0xffff_ffff_ffff_ffff + seed."]}, {"name": "torch.random.seed()", "path": "random#torch.random.seed", "type": "Miscellaneous", "text": ["Sets the seed for generating random numbers to a non-deterministic random number. Returns a 64 bit number used to seed the RNG.", "int"]}, {"name": "torch.random.set_rng_state()", "path": "random#torch.random.set_rng_state", "type": "Miscellaneous", "text": ["Sets the random number generator state.", "new_state (torch.ByteTensor) \u2013 The desired state"]}, {"name": "torch.randperm", "path": "generated/torch.randperm", "type": "Torch", "text": ["Returns a random permutation of integers from 0 to n - 1.", "n (int) \u2013 the upper bound (exclusive)", "Example:"]}, {"name": "torch.range", "path": "generated/torch.range", "type": "Torch", "text": ["Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1 with values from start to end with step step. Step is the gap between two values in the tensor.", "Warning", "This function is deprecated and will be removed in a future release because its behavior is inconsistent with Python\u2019s range builtin. Instead, use torch.arange(), which produces values in [start, end).", "Example:"]}, {"name": "torch.ravel", "path": "generated/torch.ravel", "type": "Torch", "text": ["Return a contiguous flattened tensor. A copy is made only if needed.", "input (Tensor) \u2013 the input tensor.", "Example:"]}, {"name": "torch.real", "path": "generated/torch.real", "type": "Torch", "text": ["Returns a new tensor containing real values of the self tensor. The returned tensor and self share the same underlying storage.", "input (Tensor) \u2013 the input tensor.", "Example:"]}, {"name": "torch.reciprocal", "path": "generated/torch.reciprocal", "type": "Torch", "text": ["Returns a new tensor with the reciprocal of the elements of input", "Note", "Unlike NumPy\u2019s reciprocal, torch.reciprocal supports integral inputs. Integral inputs to reciprocal are automatically promoted to the default scalar type.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.remainder", "path": "generated/torch.remainder", "type": "Torch", "text": ["Computes Python\u2019s modulus operation entrywise. The result has the same sign as the divisor other and its absolute value is less than that of other.", "It may also be defined in terms of torch.div() as", "Supports broadcasting to a common shape, type promotion, and integer and float inputs.", "Note", "Complex inputs are not supported. In some cases, it is not mathematically possible to satisfy the definition of a modulo operation with complex numbers. See torch.fmod() for how division by zero is handled.", "See also", "torch.fmod() which implements C++\u2019s std::fmod. This one is defined in terms of division rounding towards zero.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.renorm", "path": "generated/torch.renorm", "type": "Torch", "text": ["Returns a tensor where each sub-tensor of input along dimension dim is normalized such that the p-norm of the sub-tensor is lower than the value maxnorm", "Note", "If the norm of a row is lower than maxnorm, the row is unchanged", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.repeat_interleave", "path": "generated/torch.repeat_interleave", "type": "Torch", "text": ["Repeat elements of a tensor.", "Warning", "This is different from torch.Tensor.repeat() but similar to numpy.repeat.", "output_size (int, optional) \u2013 Total output size for the given axis ( e.g. sum of repeats). If given, it will avoid stream synchronization needed to calculate output shape of the tensor.", "Repeated tensor which has the same shape as input, except along the given axis.", "Tensor", "Example:", "If the repeats is tensor([n1, n2, n3, \u2026]), then the output will be tensor([0, 0, \u2026, 1, 1, \u2026, 2, 2, \u2026, \u2026]) where 0 appears n1 times, 1 appears n2 times, 2 appears n3 times, etc.", "Repeats 0 repeats[0] times, 1 repeats[1] times, 2 repeats[2] times, etc.", "repeats (Tensor) \u2013 The number of repetitions for each element.", "Repeated tensor of size sum(repeats).", "Tensor", "Example:"]}, {"name": "torch.reshape", "path": "generated/torch.reshape", "type": "Torch", "text": ["Returns a tensor with the same data and number of elements as input, but with the specified shape. When possible, the returned tensor will be a view of input. Otherwise, it will be a copy. Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs. viewing behavior.", "See torch.Tensor.view() on when it is possible to return a view.", "A single dimension may be -1, in which case it\u2019s inferred from the remaining dimensions and the number of elements in input.", "Example:"]}, {"name": "torch.resolve_conj", "path": "generated/torch.resolve_conj", "type": "Torch", "text": ["Returns a new tensor with materialized conjugation if input\u2019s conjugate bit is set to True, else returns input. The output tensor will always have its conjugate bit set to False.", "input (Tensor) \u2013 the input tensor.", "Example:"]}, {"name": "torch.resolve_neg", "path": "generated/torch.resolve_neg", "type": "Torch", "text": ["Returns a new tensor with materialized negation if input\u2019s negative bit is set to True, else returns input. The output tensor will always have its negative bit set to False.", "input (Tensor) \u2013 the input tensor.", "Example:"]}, {"name": "torch.result_type", "path": "generated/torch.result_type", "type": "Torch", "text": ["Returns the torch.dtype that would result from performing an arithmetic operation on the provided input tensors. See type promotion documentation for more information on the type promotion logic.", "Example:"]}, {"name": "torch.roll", "path": "generated/torch.roll", "type": "Torch", "text": ["Roll the tensor input along the given dimension(s). Elements that are shifted beyond the last position are re-introduced at the first position. If dims is None, the tensor will be flattened before rolling and then restored to the original shape.", "Example:"]}, {"name": "torch.rot90", "path": "generated/torch.rot90", "type": "Torch", "text": ["Rotate an n-D tensor by 90 degrees in the plane specified by dims axis. Rotation direction is from the first towards the second axis if k > 0, and from the second towards the first for k < 0.", "Example:"]}, {"name": "torch.round", "path": "generated/torch.round", "type": "Torch", "text": ["Rounds elements of input to the nearest integer.", "For integer inputs, follows the array-api convention of returning a copy of the input tensor. The return type of output is same as that of input\u2019s dtype.", "Note", "This function implements the \u201cround half to even\u201d to break ties when a number is equidistant from two integers (e.g. round(2.5) is 2).", "When the :attr:`decimals` argument is specified the algorithm used is similar to NumPy\u2019s around. This algorithm is fast but inexact and it can easily overflow for low precision dtypes. Eg. round(tensor([10000], dtype=torch.float16), decimals=3) is inf.", "See also", "torch.ceil(), which rounds up. torch.floor(), which rounds down. torch.trunc(), which rounds towards zero.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.row_stack", "path": "generated/torch.row_stack", "type": "Torch", "text": ["Alias of torch.vstack()."]}, {"name": "torch.rsqrt", "path": "generated/torch.rsqrt", "type": "Torch", "text": ["Returns a new tensor with the reciprocal of the square-root of each of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.save", "path": "generated/torch.save", "type": "Torch", "text": ["Saves an object to a disk file.", "See also: Saving and loading tensors", "Note", "A common PyTorch convention is to save tensors using .pt file extension.", "Note", "PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves views for more details.", "Note", "The 1.6 release of PyTorch switched torch.save to use a new zipfile-based file format. torch.load still retains the ability to load files in the old format. If for any reason you want torch.save to use the old format, pass the kwarg _use_new_zipfile_serialization=False."]}, {"name": "torch.scatter", "path": "generated/torch.scatter", "type": "Torch", "text": ["Out-of-place version of torch.Tensor.scatter_()"]}, {"name": "torch.scatter_add", "path": "generated/torch.scatter_add", "type": "Torch", "text": ["Out-of-place version of torch.Tensor.scatter_add_()"]}, {"name": "torch.scatter_reduce", "path": "generated/torch.scatter_reduce", "type": "Torch", "text": ["Out-of-place version of torch.Tensor.scatter_reduce_()"]}, {"name": "torch.searchsorted", "path": "generated/torch.searchsorted", "type": "Torch", "text": ["Find the indices from the innermost dimension of sorted_sequence such that, if the corresponding values in values were inserted before the indices, when sorted, the order of the corresponding innermost dimension within sorted_sequence would be preserved. Return a new tensor with the same size as values. If right is False or side is \u2018left (default), then the left boundary of sorted_sequence is closed. More formally, the returned index satisfies the following rules:", "sorted_sequence", "right", "returned index satisfies", "1-D", "False", "sorted_sequence[i-1] < values[m][n]...[l][x] <= sorted_sequence[i]", "1-D", "True", "sorted_sequence[i-1] <= values[m][n]...[l][x] < sorted_sequence[i]", "N-D", "False", "sorted_sequence[m][n]...[l][i-1] < values[m][n]...[l][x] <= sorted_sequence[m][n]...[l][i]", "N-D", "True", "sorted_sequence[m][n]...[l][i-1] <= values[m][n]...[l][x] < sorted_sequence[m][n]...[l][i]", "Example:"]}, {"name": "torch.seed", "path": "generated/torch.seed", "type": "Torch", "text": ["Sets the seed for generating random numbers to a non-deterministic random number. Returns a 64 bit number used to seed the RNG.", "int"]}, {"name": "torch.select", "path": "generated/torch.select", "type": "Torch", "text": ["Slices the input tensor along the selected dimension at the given index. This function returns a view of the original tensor with the given dimension removed.", "Note", "If input is a sparse tensor and returning a view of the tensor is not possible, a RuntimeError exception is raised. In this is the case, consider using torch.select_copy() function.", "Note", "select() is equivalent to slicing. For example, tensor.select(0, index) is equivalent to tensor[index] and tensor.select(2, index) is equivalent to tensor[:,:,index]."]}, {"name": "torch.select_scatter", "path": "generated/torch.select_scatter", "type": "Torch", "text": ["Embeds the values of the src tensor into input at the given index. This function returns a tensor with fresh storage; it does not create a view.", "Note", "src must be of the proper size in order to be embedded into input. Specifically, it should have the same shape as torch.select(input, dim, index)", "Example:"]}, {"name": "torch.set_default_device", "path": "generated/torch.set_default_device", "type": "Torch", "text": ["Sets the default torch.Tensor to be allocated on device. This does not affect factory function calls which are called with an explicit device argument. Factory calls will be performed as if they were passed device as an argument.", "To only temporarily change the default device instead of setting it globally, use with torch.device(device): instead.", "The default device is initially cpu. If you set the default tensor device to another device (e.g., cuda) without a device index, tensors will be allocated on whatever the current device for the device type, even after torch.cuda.set_device() is called.", "Warning", "This function imposes a slight performance cost on every Python call to the torch API (not just factory functions). If this is causing problems for you, please comment on https://github.com/pytorch/pytorch/issues/92701", "device (device or string) \u2013 the device to set as default", "Example:"]}, {"name": "torch.set_default_dtype", "path": "generated/torch.set_default_dtype", "type": "Torch", "text": ["Sets the default floating point dtype to d. Supports torch.float32 and torch.float64 as inputs. Other dtypes may be accepted without complaint but are not supported and are unlikely to work as expected.", "When PyTorch is initialized its default floating point dtype is torch.float32, and the intent of set_default_dtype(torch.float64) is to facilitate NumPy-like type inference. The default floating point dtype is used to:", "d (torch.dtype) \u2013 the floating point dtype to make the default. Either torch.float32 or torch.float64."]}, {"name": "torch.set_default_tensor_type", "path": "generated/torch.set_default_tensor_type", "type": "Torch", "text": ["Sets the default torch.Tensor type to floating point tensor type t. This type will also be used as default floating point type for type inference in torch.tensor().", "The default floating point tensor type is initially torch.FloatTensor.", "t (type or string) \u2013 the floating point tensor type or its name", "Example:"]}, {"name": "torch.set_deterministic_debug_mode", "path": "generated/torch.set_deterministic_debug_mode", "type": "Torch", "text": ["Sets the debug mode for deterministic operations.", "Note", "This is an alternative interface for torch.use_deterministic_algorithms(). Refer to that function\u2019s documentation for details about affected operations.", "debug_mode (str or int) \u2013 If \u201cdefault\u201d or 0, don\u2019t error or warn on nondeterministic operations. If \u201cwarn\u201d or 1, warn on nondeterministic operations. If \u201cerror\u201d or 2, error on nondeterministic operations."]}, {"name": "torch.set_float32_matmul_precision", "path": "generated/torch.set_float32_matmul_precision", "type": "Torch", "text": ["Sets the internal precision of float32 matrix multiplications.", "Running float32 matrix multiplications in lower precision may significantly increase performance, and in some programs the loss of precision has a negligible impact.", "Supports three settings:", "When using \u201chigh\u201d precision, float32 multiplications may use a bfloat16-based algorithm that is more complicated than simply truncating to some smaller number mantissa bits (e.g. 10 for TensorFloat32, 8 for bfloat16). Refer to [Henry2019] for a complete description of this algorithm. To briefly explain here, the first step is to realize that we can perfectly encode a single float32 number as the sum of three bfloat16 numbers (because float32 has 24 mantissa bits while bfloat16 has 8, and both have the same number of exponent bits). This means that the product of two float32 numbers can be exactly given by the sum of nine products of bfloat16 numbers. We can then trade accuracy for speed by dropping some of these products. The \u201chigh\u201d precision algorithm specifically keeps only the three most significant products, which conveniently excludes all of the products involving the last 8 mantissa bits of either input. This means that we can represent our inputs as the sum of two bfloat16 numbers rather than three. Because bfloat16 fused-multiply-add (FMA) instructions are typically >10x faster than float32 ones, it\u2019s faster to do three multiplications and 2 additions with bfloat16 precision than it is to do a single multiplication with float32 precision.", "http://arxiv.org/abs/1904.06376", "Note", "This does not change the output dtype of float32 matrix multiplications, it controls how the internal computation of the matrix multiplication is performed.", "Note", "This does not change the precision of convolution operations. Other flags, like torch.backends.cudnn.allow_tf32, may control the precision of convolution operations.", "Note", "This flag currently only affects one native device type: CUDA. If \u201chigh\u201d or \u201cmedium\u201d are set then the TensorFloat32 datatype will be used when computing float32 matrix multiplications, equivalent to setting torch.backends.cuda.matmul.allow_tf32 = True. When \u201chighest\u201d (the default) is set then the float32 datatype is used for internal computations, equivalent to setting torch.backends.cuda.matmul.allow_tf32 = False.", "precision (str) \u2013 can be set to \u201chighest\u201d (default), \u201chigh\u201d, or \u201cmedium\u201d (see above)."]}, {"name": "torch.set_flush_denormal", "path": "generated/torch.set_flush_denormal", "type": "Torch", "text": ["Disables denormal floating numbers on CPU.", "Returns True if your system supports flushing denormal numbers and it successfully configures flush denormal mode. set_flush_denormal() is only supported on x86 architectures supporting SSE3.", "mode (bool) \u2013 Controls whether to enable flush denormal mode or not", "Example:"]}, {"name": "torch.set_grad_enabled", "path": "generated/torch.set_grad_enabled#torch.set_grad_enabled", "type": "Torch", "text": ["Context-manager that sets gradient calculation on or off.", "set_grad_enabled will enable or disable grads based on its argument mode. It can be used as a context-manager or as a function.", "This context manager is thread local; it will not affect computation in other threads.", "mode (bool) \u2013 Flag whether to enable grad (True), or disable (False). This can be used to conditionally enable gradients.", "Note", "set_grad_enabled is one of several mechanisms that can enable or disable gradients locally see Locally disabling gradient computation for more information on how they compare.", "Note", "This API does not apply to forward-mode AD."]}, {"name": "torch.set_num_interop_threads", "path": "generated/torch.set_num_interop_threads", "type": "Torch", "text": ["Sets the number of threads used for interop parallelism (e.g. in JIT interpreter) on CPU.", "Warning", "Can only be called once and before any inter-op parallel work is started (e.g. JIT execution)."]}, {"name": "torch.set_num_threads", "path": "generated/torch.set_num_threads", "type": "Torch", "text": ["Sets the number of threads used for intraop parallelism on CPU.", "Warning", "To ensure that the correct number of threads is used, set_num_threads must be called before running eager, JIT or autograd code."]}, {"name": "torch.set_printoptions", "path": "generated/torch.set_printoptions", "type": "Torch", "text": ["Set options for printing. Items shamelessly taken from NumPy", "Example:"]}, {"name": "torch.set_rng_state", "path": "generated/torch.set_rng_state", "type": "Torch", "text": ["Sets the random number generator state.", "new_state (torch.ByteTensor) \u2013 The desired state"]}, {"name": "torch.set_warn_always", "path": "generated/torch.set_warn_always", "type": "Torch", "text": ["When this flag is False (default) then some PyTorch warnings may only appear once per process. This helps avoid excessive warning information. Setting it to True causes these warnings to always appear, which may be helpful when debugging.", "b (bool) \u2013 If True, force warnings to always be emitted If False, set to the default behaviour"]}, {"name": "torch.sgn", "path": "generated/torch.sgn", "type": "Torch", "text": ["This function is an extension of torch.sign() to complex tensors. It computes a new tensor whose elements have the same angles as the corresponding elements of input and absolute values (i.e. magnitudes) of one for complex tensors and is equivalent to torch.sign() for non-complex tensors.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.ShortStorage", "path": "storage#torch.ShortStorage", "type": "Storage", "text": []}, {"name": "torch.ShortStorage.dtype", "path": "storage#torch.ShortStorage.dtype", "type": "Storage", "text": []}, {"name": "torch.sigmoid", "path": "generated/torch.sigmoid", "type": "Torch", "text": ["Alias for torch.special.expit()."]}, {"name": "torch.sign", "path": "generated/torch.sign", "type": "Torch", "text": ["Returns a new tensor with the signs of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.signal", "path": "signal", "type": "SciPy-like Signal", "text": ["The torch.signal module, modeled after SciPy\u2019s signal module.", "Computes the Bartlett window.", "Computes the Blackman window.", "Computes a window with a simple cosine waveform.", "Computes a window with an exponential waveform.", "Computes a window with a gaussian waveform.", "Computes the general cosine window.", "Computes the general Hamming window.", "Computes the Hamming window.", "Computes the Hann window.", "Computes the Kaiser window.", "Computes the minimum 4-term Blackman-Harris window according to Nuttall."]}, {"name": "torch.signal.torch.signal.windows.bartlett", "path": "generated/torch.signal.windows.bartlett", "type": "SciPy-like Signal", "text": ["Computes the Bartlett window.", "The Bartlett window is defined as follows:", "The window is normalized to 1 (maximum value is 1). However, the 1 doesn\u2019t appear if M is even and sym is True.", "M (int) \u2013 the length of the window. In other words, the number of points of the returned window.", "Tensor", "Examples:"]}, {"name": "torch.signal.torch.signal.windows.blackman", "path": "generated/torch.signal.windows.blackman", "type": "SciPy-like Signal", "text": ["Computes the Blackman window.", "The Blackman window is defined as follows:", "The window is normalized to 1 (maximum value is 1). However, the 1 doesn\u2019t appear if M is even and sym is True.", "M (int) \u2013 the length of the window. In other words, the number of points of the returned window.", "Tensor", "Examples:"]}, {"name": "torch.signal.torch.signal.windows.cosine", "path": "generated/torch.signal.windows.cosine", "type": "SciPy-like Signal", "text": ["Computes a window with a simple cosine waveform. Also known as the sine window.", "The cosine window is defined as follows:", "The window is normalized to 1 (maximum value is 1). However, the 1 doesn\u2019t appear if M is even and sym is True.", "M (int) \u2013 the length of the window. In other words, the number of points of the returned window.", "Tensor", "Examples:"]}, {"name": "torch.signal.torch.signal.windows.exponential", "path": "generated/torch.signal.windows.exponential", "type": "SciPy-like Signal", "text": ["Computes a window with an exponential waveform. Also known as Poisson window.", "The exponential window is defined as follows:", "where c is the center of the window.", "The window is normalized to 1 (maximum value is 1). However, the 1 doesn\u2019t appear if M is even and sym is True.", "M (int) \u2013 the length of the window. In other words, the number of points of the returned window.", "Tensor", "Examples:"]}, {"name": "torch.signal.torch.signal.windows.gaussian", "path": "generated/torch.signal.windows.gaussian", "type": "SciPy-like Signal", "text": ["Computes a window with a gaussian waveform.", "The gaussian window is defined as follows:", "The window is normalized to 1 (maximum value is 1). However, the 1 doesn\u2019t appear if M is even and sym is True.", "M (int) \u2013 the length of the window. In other words, the number of points of the returned window.", "Tensor", "Examples:"]}, {"name": "torch.signal.torch.signal.windows.general_cosine", "path": "generated/torch.signal.windows.general_cosine", "type": "SciPy-like Signal", "text": ["Computes the general cosine window.", "The general cosine window is defined as follows:", "The window is normalized to 1 (maximum value is 1). However, the 1 doesn\u2019t appear if M is even and sym is True.", "M (int) \u2013 the length of the window. In other words, the number of points of the returned window.", "Tensor", "Examples:"]}, {"name": "torch.signal.torch.signal.windows.general_hamming", "path": "generated/torch.signal.windows.general_hamming", "type": "SciPy-like Signal", "text": ["Computes the general Hamming window.", "The general Hamming window is defined as follows:", "The window is normalized to 1 (maximum value is 1). However, the 1 doesn\u2019t appear if M is even and sym is True.", "M (int) \u2013 the length of the window. In other words, the number of points of the returned window.", "Tensor", "Examples:"]}, {"name": "torch.signal.torch.signal.windows.hamming", "path": "generated/torch.signal.windows.hamming", "type": "SciPy-like Signal", "text": ["Computes the Hamming window.", "The Hamming window is defined as follows:", "The window is normalized to 1 (maximum value is 1). However, the 1 doesn\u2019t appear if M is even and sym is True.", "M (int) \u2013 the length of the window. In other words, the number of points of the returned window.", "Tensor", "Examples:"]}, {"name": "torch.signal.torch.signal.windows.hann", "path": "generated/torch.signal.windows.hann", "type": "SciPy-like Signal", "text": ["Computes the Hann window.", "The Hann window is defined as follows:", "The window is normalized to 1 (maximum value is 1). However, the 1 doesn\u2019t appear if M is even and sym is True.", "M (int) \u2013 the length of the window. In other words, the number of points of the returned window.", "Tensor", "Examples:"]}, {"name": "torch.signal.torch.signal.windows.kaiser", "path": "generated/torch.signal.windows.kaiser", "type": "SciPy-like Signal", "text": ["Computes the Kaiser window.", "The Kaiser window is defined as follows:", "where I_0 is the zeroth order modified Bessel function of the first kind (see torch.special.i0()), and N = M - 1 if sym else M.", "The window is normalized to 1 (maximum value is 1). However, the 1 doesn\u2019t appear if M is even and sym is True.", "M (int) \u2013 the length of the window. In other words, the number of points of the returned window.", "Tensor", "Examples:"]}, {"name": "torch.signal.torch.signal.windows.nuttall", "path": "generated/torch.signal.windows.nuttall", "type": "SciPy-like Signal", "text": ["Computes the minimum 4-term Blackman-Harris window according to Nuttall.", "where z_n = 2 \u03c0 n/ M.", "The window is normalized to 1 (maximum value is 1). However, the 1 doesn\u2019t appear if M is even and sym is True.", "M (int) \u2013 the length of the window. In other words, the number of points of the returned window.", "Tensor", "References:", "Examples:"]}, {"name": "torch.signal.windows.bartlett()", "path": "generated/torch.signal.windows.bartlett#torch.signal.windows.bartlett", "type": "SciPy-like Signal", "text": ["Computes the Bartlett window.", "The Bartlett window is defined as follows:", "The window is normalized to 1 (maximum value is 1). However, the 1 doesn\u2019t appear if M is even and sym is True.", "M (int) \u2013 the length of the window. In other words, the number of points of the returned window.", "Tensor", "Examples:"]}, {"name": "torch.signal.windows.blackman()", "path": "generated/torch.signal.windows.blackman#torch.signal.windows.blackman", "type": "SciPy-like Signal", "text": ["Computes the Blackman window.", "The Blackman window is defined as follows:", "The window is normalized to 1 (maximum value is 1). However, the 1 doesn\u2019t appear if M is even and sym is True.", "M (int) \u2013 the length of the window. In other words, the number of points of the returned window.", "Tensor", "Examples:"]}, {"name": "torch.signal.windows.cosine()", "path": "generated/torch.signal.windows.cosine#torch.signal.windows.cosine", "type": "SciPy-like Signal", "text": ["Computes a window with a simple cosine waveform. Also known as the sine window.", "The cosine window is defined as follows:", "The window is normalized to 1 (maximum value is 1). However, the 1 doesn\u2019t appear if M is even and sym is True.", "M (int) \u2013 the length of the window. In other words, the number of points of the returned window.", "Tensor", "Examples:"]}, {"name": "torch.signal.windows.exponential()", "path": "generated/torch.signal.windows.exponential#torch.signal.windows.exponential", "type": "SciPy-like Signal", "text": ["Computes a window with an exponential waveform. Also known as Poisson window.", "The exponential window is defined as follows:", "where c is the center of the window.", "The window is normalized to 1 (maximum value is 1). However, the 1 doesn\u2019t appear if M is even and sym is True.", "M (int) \u2013 the length of the window. In other words, the number of points of the returned window.", "Tensor", "Examples:"]}, {"name": "torch.signal.windows.gaussian()", "path": "generated/torch.signal.windows.gaussian#torch.signal.windows.gaussian", "type": "SciPy-like Signal", "text": ["Computes a window with a gaussian waveform.", "The gaussian window is defined as follows:", "The window is normalized to 1 (maximum value is 1). However, the 1 doesn\u2019t appear if M is even and sym is True.", "M (int) \u2013 the length of the window. In other words, the number of points of the returned window.", "Tensor", "Examples:"]}, {"name": "torch.signal.windows.general_cosine()", "path": "generated/torch.signal.windows.general_cosine#torch.signal.windows.general_cosine", "type": "SciPy-like Signal", "text": ["Computes the general cosine window.", "The general cosine window is defined as follows:", "The window is normalized to 1 (maximum value is 1). However, the 1 doesn\u2019t appear if M is even and sym is True.", "M (int) \u2013 the length of the window. In other words, the number of points of the returned window.", "Tensor", "Examples:"]}, {"name": "torch.signal.windows.general_hamming()", "path": "generated/torch.signal.windows.general_hamming#torch.signal.windows.general_hamming", "type": "SciPy-like Signal", "text": ["Computes the general Hamming window.", "The general Hamming window is defined as follows:", "The window is normalized to 1 (maximum value is 1). However, the 1 doesn\u2019t appear if M is even and sym is True.", "M (int) \u2013 the length of the window. In other words, the number of points of the returned window.", "Tensor", "Examples:"]}, {"name": "torch.signal.windows.hamming()", "path": "generated/torch.signal.windows.hamming#torch.signal.windows.hamming", "type": "SciPy-like Signal", "text": ["Computes the Hamming window.", "The Hamming window is defined as follows:", "The window is normalized to 1 (maximum value is 1). However, the 1 doesn\u2019t appear if M is even and sym is True.", "M (int) \u2013 the length of the window. In other words, the number of points of the returned window.", "Tensor", "Examples:"]}, {"name": "torch.signal.windows.hann()", "path": "generated/torch.signal.windows.hann#torch.signal.windows.hann", "type": "SciPy-like Signal", "text": ["Computes the Hann window.", "The Hann window is defined as follows:", "The window is normalized to 1 (maximum value is 1). However, the 1 doesn\u2019t appear if M is even and sym is True.", "M (int) \u2013 the length of the window. In other words, the number of points of the returned window.", "Tensor", "Examples:"]}, {"name": "torch.signal.windows.kaiser()", "path": "generated/torch.signal.windows.kaiser#torch.signal.windows.kaiser", "type": "SciPy-like Signal", "text": ["Computes the Kaiser window.", "The Kaiser window is defined as follows:", "where I_0 is the zeroth order modified Bessel function of the first kind (see torch.special.i0()), and N = M - 1 if sym else M.", "The window is normalized to 1 (maximum value is 1). However, the 1 doesn\u2019t appear if M is even and sym is True.", "M (int) \u2013 the length of the window. In other words, the number of points of the returned window.", "Tensor", "Examples:"]}, {"name": "torch.signal.windows.nuttall()", "path": "generated/torch.signal.windows.nuttall#torch.signal.windows.nuttall", "type": "SciPy-like Signal", "text": ["Computes the minimum 4-term Blackman-Harris window according to Nuttall.", "where z_n = 2 \u03c0 n/ M.", "The window is normalized to 1 (maximum value is 1). However, the 1 doesn\u2019t appear if M is even and sym is True.", "M (int) \u2013 the length of the window. In other words, the number of points of the returned window.", "Tensor", "References:", "Examples:"]}, {"name": "torch.signbit", "path": "generated/torch.signbit", "type": "Torch", "text": ["Tests if each element of input has its sign bit set or not.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Note", "signbit handles signed zeros, so negative zero (-0) returns True."]}, {"name": "torch.sin", "path": "generated/torch.sin", "type": "Torch", "text": ["Returns a new tensor with the sine of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.sinc", "path": "generated/torch.sinc", "type": "Torch", "text": ["Alias for torch.special.sinc()."]}, {"name": "torch.sinh", "path": "generated/torch.sinh", "type": "Torch", "text": ["Returns a new tensor with the hyperbolic sine of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Note", "When input is on the CPU, the implementation of torch.sinh may use the Sleef library, which rounds very large results to infinity or negative infinity. See here for details."]}, {"name": "torch.slice_scatter", "path": "generated/torch.slice_scatter", "type": "Torch", "text": ["Embeds the values of the src tensor into input at the given dimension. This function returns a tensor with fresh storage; it does not create a view.", "Example:"]}, {"name": "torch.slogdet", "path": "generated/torch.slogdet", "type": "Torch", "text": ["Alias for torch.linalg.slogdet()"]}, {"name": "torch.smm()", "path": "generated/torch.smm#torch.smm", "type": "Sparse Tensors", "text": ["Performs a matrix multiplication of the sparse matrix input with the dense matrix mat."]}, {"name": "torch.softmax", "path": "generated/torch.softmax", "type": "Torch", "text": ["Alias for torch.nn.functional.softmax()."]}, {"name": "torch.sort", "path": "generated/torch.sort", "type": "Torch", "text": ["Sorts the elements of the input tensor along a given dimension in ascending order by value.", "If dim is not given, the last dimension of the input is chosen.", "If descending is True then the elements are sorted in descending order by value.", "If stable is True then the sorting routine becomes stable, preserving the order of equivalent elements.", "A namedtuple of (values, indices) is returned, where the values are the sorted values and indices are the indices of the elements in the original input tensor.", "out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers", "Example:"]}, {"name": "torch.sparse", "path": "sparse", "type": "Sparse Tensors", "text": ["Warning", "The PyTorch API of sparse tensors is in beta and may change in the near future. We highly welcome feature requests, bug reports and general suggestions as GitHub issues.", "By default PyTorch stores torch.Tensor stores elements contiguously physical memory. This leads to efficient implementations of various array processing algorithms that require fast access to elements.", "Now, some users might decide to represent data such as graph adjacency matrices, pruned weights or points clouds by Tensors whose elements are mostly zero valued. We recognize these are important applications and aim to provide performance optimizations for these use cases via sparse storage formats.", "Various sparse storage formats such as COO, CSR/CSC, semi-structured, LIL, etc. have been developed over the years. While they differ in exact layouts, they all compress data through efficient representation of zero valued elements. We call the uncompressed values specified in contrast to unspecified, compressed elements.", "By compressing repeat zeros sparse storage formats aim to save memory and computational resources on various CPUs and GPUs. Especially for high degrees of sparsity or highly structured sparsity this can have significant performance implications. As such sparse storage formats can be seen as a performance optimization.", "Like many other performance optimization sparse storage formats are not always advantageous. When trying sparse formats for your use case you might find your execution time to increase rather than decrease.", "Please feel encouraged to open a GitHub issue if you analytically expected to see a stark increase in performance but measured a degradation instead. This helps us prioritize the implementation of efficient kernels and wider performance optimizations.", "We make it easy to try different sparsity layouts, and convert between them, without being opinionated on what\u2019s best for your particular application.", "We want it to be straightforward to construct a sparse Tensor from a given dense Tensor by providing conversion routines for each layout.", "In the next example we convert a 2D Tensor with default dense (strided) layout to a 2D Tensor backed by the COO memory layout. Only values and indices of non-zero elements are stored in this case.", "PyTorch currently supports COO, CSR, CSC, BSR, and BSC.", "We also have a prototype implementation to support :ref: semi-structured sparsity<sparse-semi-structured-docs>. Please see the references for more details.", "Note that we provide slight generalizations of these formats.", "Batching: Devices such as GPUs require batching for optimal performance and thus we support batch dimensions.", "We currently offer a very simple version of batching where each component of a sparse format itself is batched. This also requires the same number of specified elements per batch entry. In this example we construct a 3D (batched) CSR Tensor from a 3D dense Tensor.", "Dense dimensions: On the other hand, some data such as Graph embeddings might be better viewed as sparse collections of vectors instead of scalars.", "In this example we create a 3D Hybrid COO Tensor with 2 sparse and 1 dense dimension from a 3D strided Tensor. If an entire row in the 3D strided Tensor is zero, it is not stored. If however any of the values in the row are non-zero, they are stored entirely. This reduces the number of indices since we need one index one per row instead of one per element. But it also increases the amount of storage for the values. Since only rows that are entirely zero can be emitted and the presence of any non-zero valued elements cause the entire row to be stored.", "Fundamentally, operations on Tensor with sparse storage formats behave the same as operations on Tensor with strided (or other) storage formats. The particularities of storage, that is the physical layout of the data, influences the performance of an operation but should not influence the semantics.", "We are actively increasing operator coverage for sparse tensors. Users should not expect support same level of support as for dense Tensors yet. See our operator documentation for a list.", "As shown in the example above, we don\u2019t support non-zero preserving unary operators such as cos. The output of a non-zero preserving unary operation will not be able to take advantage of sparse storage formats to the same extent as the input and potentially result in a catastrophic increase in memory. We instead rely on the user to explicitly convert to a dense Tensor first and then run the operation.", "We are aware that some users want to ignore compressed zeros for operations such as cos instead of preserving the exact semantics of the operation. For this we can point to torch.masked and its MaskedTensor, which is in turn also backed and powered by sparse storage formats and kernels.", "Also note that, for now, the user doesn\u2019t have a choice of the output layout. For example, adding a sparse Tensor to a regular strided Tensor results in a strided Tensor. Some users might prefer for this to stay a sparse layout, because they know the result will still be sufficiently sparse.", "We acknowledge that access to kernels that can efficiently produce different output layouts can be very useful. A subsequent operation might significantly benefit from receiving a particular layout. We are working on an API to control the result layout and recognize it is an important feature to plan a more optimal path of execution for any given model.", "Warning", "Sparse semi-sturctured tensors are currently a prototype feature and subject to change. Please feel free to open an issue to report a bug or if you have feedback to share.", "Semi-Structured sparsity is a sparse data layout that was first introduced in NVIDIA\u2019s Ampere architecture. It is also referred to as fine-grained structured sparsity or 2:4 structured sparsity.", "This sparse layout stores n elements out of every 2n elements, with n being determined by the width of the Tensor\u2019s data type (dtype). The most frequently used dtype is float16, where n=2, thus the term \u201c2:4 structured sparsity.\u201d", "Semi-structured sparsity is explained in greater detail in this NVIDIA blog post.", "In PyTorch, semi-structured sparsity is implemented via a Tensor subclass. By subclassing, we can override __torch_dispatch__ , allowing us to use faster sparse kernels when performing matrix multiplication. We can also store the tensor in it\u2019s compressed form inside the subclass to reduce memory overhead.", "In this compressed form, the sparse tensor is stored by retaining only the specified elements and some metadata, which encodes the mask.", "Note", "The specified elements and metadata mask of a semi-structured sparse tensor are stored together in a single flat compressed tensor. They are appended to each other to form a contiguous chunk of memory.", "compressed tensor = [ specified elements of original tensor | metadata_mask ]", "For an original tensor of size (r, c) we expect the first m * k // 2 elements to be the kept elements and the rest of the tensor is metadata.", "In order to make it easier for the user to view the specified elements and mask, one can use .indices() and .values() to access the mask and specified elements respectively.", "For 2:4 sparse tensors, the metadata overhead is minor - just 2 bits per specified element.", "Note", "It\u2019s important to note that torch.float32 is only supported for 1:2 sparsity. Therefore, it does not follow the same formula as above.", "Here, we break down how to calculate the compression ratio ( size dense / size sparse) of a 2:4 sparse tensor.", "Let (r, c) = tensor.shape and e = bitwidth(tensor.dtype), so e = 16 for torch.float16 and torch.bfloat16 and e = 8 for torch.int8.", "Using these calculations, we can determine the total memory footprint for both the original dense and the new sparse representation.", "This gives us a simple formula for the compression ratio, which is dependent only on the bitwidth of the tensor datatype.", "By using this formula, we find that the compression ratio is 56.25% for torch.float16 or torch.bfloat16, and 62.5% for torch.int8.", "You can transform a dense tensor into a sparse semi-structured tensor by simply using the torch.to_sparse_semi_structured function.", "Please also note that we only support CUDA tensors since hardware compatibility for semi-structured sparsity is limited to NVIDIA GPUs.", "The following datatypes are supported for semi-structured sparsity. Note that each datatype has its own shape constraints and compression factor.", "PyTorch dtype", "Shape Constraints", "Compression Factor", "Sparsity Pattern", "torch.float16", "Tensor must be 2D and (r, c) must both be a positive multiple of 64", "9/16", "2:4", "torch.bfloat16", "Tensor must be 2D and (r, c) must both be a positive multiple of 64", "9/16", "2:4", "torch.int8", "Tensor must be 2D and (r, c) must both be a positive multiple of 128", "10/16", "2:4", "To construct a semi-structured sparse tensor, start by creating a regular dense tensor that adheres to a 2:4 (or semi-structured) sparse format. To do this we tile a small 1x4 strip to create a 16x16 dense float16 tensor. Afterwards, we can call to_sparse_semi_structured function to compress it for accelerated inference.", "Currently, the following operations are supported for semi-structured sparse tensors:", "To use these ops, simply pass the output of to_sparse_semi_structured(tensor) instead of using tensor once your tensor has 0s in a semi-structured sparse format, like this:", "You can accelerate the linear layers in your model if the weights are already semi-structured sparse with just a few lines of code:", "PyTorch implements the so-called Coordinate format, or COO format, as one of the storage formats for implementing sparse tensors. In COO format, the specified elements are stored as tuples of element indices and the corresponding values. In particular,", "where ndim is the dimensionality of the tensor and nse is the number of specified elements.", "Note", "The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant overhead from storing other tensor data).", "The memory consumption of a strided tensor is at least product(<tensor shape>) * <size of element type in bytes>.", "For example, the memory consumption of a 10 000 x 10 000 tensor with 100 000 non-zero 32-bit floating point numbers is at least (2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor layout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using the default strided tensor layout. Notice the 200 fold memory saving from using the COO storage format.", "A sparse COO tensor can be constructed by providing the two tensors of indices and values, as well as the size of the sparse tensor (when it cannot be inferred from the indices and values tensors) to a function torch.sparse_coo_tensor().", "Suppose we want to define a sparse tensor with the entry 3 at location (0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2). Unspecified elements are assumed to have the same value, fill value, which is zero by default. We would then write:", "Note that the input i is NOT a list of index tuples. If you want to write your indices this way, you should transpose before passing them to the sparse constructor:", "An empty sparse COO tensor can be constructed by specifying its size only:", "PyTorch implements an extension of sparse tensors with scalar values to sparse tensors with (contiguous) tensor values. Such tensors are called hybrid tensors.", "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing the values tensor to be a multi-dimensional tensor so that we have:", "Note", "We use (M + K)-dimensional tensor to denote a N-dimensional sparse hybrid tensor, where M and K are the numbers of sparse and dense dimensions, respectively, such that M + K == N holds.", "Suppose we want to create a (2 + 1)-dimensional tensor with the entry [3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry [7, 8] at location (1, 2). We would write", "In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following invariants:", "Note", "Dense dimensions always follow sparse dimensions, that is, mixing of dense and sparse dimensions is not supported.", "Note", "To be sure that a constructed sparse tensor has consistent indices, values, and size, the invariant checks can be enabled per tensor creation via check_invariants=True keyword argument, or globally using torch.sparse.check_sparse_tensor_invariants context manager instance. By default, the sparse tensor invariants checks are disabled.", "PyTorch sparse COO tensor format permits sparse uncoalesced tensors, where there may be duplicate coordinates in the indices; in this case, the interpretation is that the value at that index is the sum of all duplicate value entries. For example, one can specify multiple values, 3 and 4, for the same index 1, that leads to an 1-D uncoalesced tensor:", "while the coalescing process will accumulate the multi-valued elements into a single value using summation:", "In general, the output of torch.Tensor.coalesce() method is a sparse tensor with the following properties:", "Note", "For the most part, you shouldn\u2019t have to care whether or not a sparse tensor is coalesced or not, as most operations will work identically given a sparse coalesced or uncoalesced tensor.", "However, some operations can be implemented more efficiently on uncoalesced tensors, and some on coalesced tensors.", "For instance, addition of sparse COO tensors is implemented by simply concatenating the indices and values tensors:", "If you repeatedly perform an operation that can produce duplicate entries (e.g., torch.Tensor.add()), you should occasionally coalesce your sparse tensors to prevent them from growing too large.", "On the other hand, the lexicographical ordering of indices can be advantageous for implementing algorithms that involve many element selection operations, such as slicing or matrix products.", "Let\u2019s consider the following example:", "As mentioned above, a sparse COO tensor is a torch.Tensor instance and to distinguish it from the Tensor instances that use some other layout, on can use torch.Tensor.is_sparse or torch.Tensor.layout properties:", "The number of sparse and dense dimensions can be acquired using methods torch.Tensor.sparse_dim() and torch.Tensor.dense_dim(), respectively. For instance:", "If s is a sparse COO tensor then its COO format data can be acquired using methods torch.Tensor.indices() and torch.Tensor.values().", "Note", "Currently, one can acquire the COO format data only when the tensor instance is coalesced:", "For acquiring the COO format data of an uncoalesced tensor, use torch.Tensor._values() and torch.Tensor._indices():", "Warning", "Calling torch.Tensor._values() will return a detached tensor. To track gradients, torch.Tensor.coalesce().values() must be used instead.", "Constructing a new sparse COO tensor results a tensor that is not coalesced:", "but one can construct a coalesced copy of a sparse COO tensor using the torch.Tensor.coalesce() method:", "When working with uncoalesced sparse COO tensors, one must take into an account the additive nature of uncoalesced data: the values of the same indices are the terms of a sum that evaluation gives the value of the corresponding tensor element. For example, the scalar multiplication on a sparse uncoalesced tensor could be implemented by multiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation, say, a square root, cannot be implemented by applying the operation to uncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not hold in general.", "Slicing (with positive step) of a sparse COO tensor is supported only for dense dimensions. Indexing is supported for both sparse and dense dimensions:", "In PyTorch, the fill value of a sparse tensor cannot be specified explicitly and is assumed to be zero in general. However, there exists operations that may interpret the fill value differently. For instance, torch.sparse.softmax() computes the softmax with the assumption that the fill value is negative infinity.", "Sparse Compressed Tensors represents a class of sparse tensors that have a common feature of compressing the indices of a certain dimension using an encoding that enables certain optimizations on linear algebra kernels of sparse compressed tensors. This encoding is based on the Compressed Sparse Row (CSR) format that PyTorch sparse compressed tensors extend with the support of sparse tensor batches, allowing multi-dimensional tensor values, and storing sparse tensor values in dense blocks.", "Note", "We use (B + M + K)-dimensional tensor to denote a N-dimensional sparse compressed hybrid tensor, where B, M, and K are the numbers of batch, sparse, and dense dimensions, respectively, such that B + M + K == N holds. The number of sparse dimensions for sparse compressed tensors is always two, M == 2.", "Note", "We say that an indices tensor compressed_indices uses CSR encoding if the following invariants are satisfied:", "To be sure that a constructed sparse tensor has consistent indices, values, and size, the invariant checks can be enabled per tensor creation via check_invariants=True keyword argument, or globally using torch.sparse.check_sparse_tensor_invariants context manager instance. By default, the sparse tensor invariants checks are disabled.", "Note", "The generalization of sparse compressed layouts to N-dimensional tensors can lead to some confusion regarding the count of specified elements. When a sparse compressed tensor contains batch dimensions the number of specified elements will correspond to the number of such elements per-batch. When a sparse compressed tensor has dense dimensions the element considered is now the K-dimensional array. Also for block sparse compressed layouts the 2-D block is considered as the element being specified. Take as an example a 3-dimensional block sparse tensor, with one batch dimension of length b, and a block shape of p, q. If this tensor has n specified elements, then in fact we have n blocks specified per batch. This tensor would have values with shape (b, n, p, q). This interpretation of the number of specified elements comes from all sparse compressed layouts being derived from the compression of a 2-dimensional matrix. Batch dimensions are treated as stacking of sparse matrices, dense dimensions change the meaning of the element from a simple scalar value to an array with its own dimensions.", "The primary advantage of the CSR format over the COO format is better use of storage and much faster computation operations such as sparse matrix-vector multiplication using MKL and MAGMA backends.", "In the simplest case, a (0 + 2 + 0)-dimensional sparse CSR tensor consists of three 1-D tensors: crow_indices, col_indices and values:", "Note", "The index tensors crow_indices and col_indices should have element type either torch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix operations, use torch.int32. This is as a result of the default linking of pytorch being with MKL LP64, which uses 32 bit integer indexing.", "In the general case, the (B + 2 + K)-dimensional sparse CSR tensor consists of two (B + 1)-dimensional index tensors crow_indices and col_indices, and of (1 + K)-dimensional values tensor such that", "while the shape of the sparse CSR tensor is (*batchsize, nrows,\nncols, *densesize) where len(batchsize) == B and len(densesize) == K.", "Note", "The batches of sparse CSR tensors are dependent: the number of specified elements in all batches must be the same. This somewhat artificial constraint allows efficient storage of the indices of different CSR batches.", "Note", "The number of sparse and dense dimensions can be acquired using torch.Tensor.sparse_dim() and torch.Tensor.dense_dim() methods. The batch dimensions can be computed from the tensor shape: batchsize = tensor.shape[:-tensor.sparse_dim() -\ntensor.dense_dim()].", "Note", "The memory consumption of a sparse CSR tensor is at least (nrows * 8 + (8 + <size of element type in bytes> *\nprod(densesize)) * nse) * prod(batchsize) bytes (plus a constant overhead from storing other tensor data).", "With the same example data of the note in sparse COO format introduction, the memory consumption of a 10 000 x 10 000 tensor with 100 000 non-zero 32-bit floating point numbers is at least (10000 * 8 + (8 + 4 * 1) * 100 000) * 1 = 1 280 000 bytes when using CSR tensor layout. Notice the 1.6 and 310 fold savings from using CSR storage format compared to using the COO and strided formats, respectively.", "Sparse CSR tensors can be directly constructed by using the torch.sparse_csr_tensor() function. The user must supply the row and column indices and values tensors separately where the row indices must be specified using the CSR compression encoding. The size argument is optional and will be deduced from the crow_indices and col_indices if it is not present.", "Note", "The values of sparse dimensions in deduced size is computed from the size of crow_indices and the maximal index value in col_indices. If the number of columns needs to be larger than in the deduced size then the size argument must be specified explicitly.", "The simplest way of constructing a 2-D sparse CSR tensor from a strided or sparse COO tensor is to use torch.Tensor.to_sparse_csr() method. Any zeros in the (strided) tensor will be interpreted as missing values in the sparse tensor:", "The sparse matrix-vector multiplication can be performed with the tensor.matmul() method. This is currently the only math operation supported on CSR tensors.", "The sparse CSC (Compressed Sparse Column) tensor format implements the CSC format for storage of 2 dimensional tensors with an extension to supporting batches of sparse CSC tensors and values being multi-dimensional tensors.", "Note", "Sparse CSC tensor is essentially a transpose of the sparse CSR tensor when the transposition is about swapping the sparse dimensions.", "Similarly to sparse CSR tensors, a sparse CSC tensor consists of three tensors: ccol_indices, row_indices and values:", "Sparse CSC tensors can be directly constructed by using the torch.sparse_csc_tensor() function. The user must supply the row and column indices and values tensors separately where the column indices must be specified using the CSR compression encoding. The size argument is optional and will be deduced from the row_indices and ccol_indices tensors if it is not present.", "Note", "The sparse CSC tensor constructor function has the compressed column indices argument before the row indices argument.", "The (0 + 2 + 0)-dimensional sparse CSC tensors can be constructed from any two-dimensional tensor using torch.Tensor.to_sparse_csc() method. Any zeros in the (strided) tensor will be interpreted as missing values in the sparse tensor:", "The sparse BSR (Block compressed Sparse Row) tensor format implements the BSR format for storage of two-dimensional tensors with an extension to supporting batches of sparse BSR tensors and values being blocks of multi-dimensional tensors.", "A sparse BSR tensor consists of three tensors: crow_indices, col_indices and values:", "Sparse BSR tensors can be directly constructed by using the torch.sparse_bsr_tensor() function. The user must supply the row and column block indices and values tensors separately where the row block indices must be specified using the CSR compression encoding. The size argument is optional and will be deduced from the crow_indices and col_indices tensors if it is not present.", "The (0 + 2 + 0)-dimensional sparse BSR tensors can be constructed from any two-dimensional tensor using torch.Tensor.to_sparse_bsr() method that also requires the specification of the values block size:", "The sparse BSC (Block compressed Sparse Column) tensor format implements the BSC format for storage of two-dimensional tensors with an extension to supporting batches of sparse BSC tensors and values being blocks of multi-dimensional tensors.", "A sparse BSC tensor consists of three tensors: ccol_indices, row_indices and values:", "Sparse BSC tensors can be directly constructed by using the torch.sparse_bsc_tensor() function. The user must supply the row and column block indices and values tensors separately where the column block indices must be specified using the CSR compression encoding. The size argument is optional and will be deduced from the ccol_indices and row_indices tensors if it is not present.", "All sparse compressed tensors \u2014 CSR, CSC, BSR, and BSC tensors \u2014 are conceptionally very similar in that their indices data is split into two parts: so-called compressed indices that use the CSR encoding, and so-called plain indices that are orthogonal to the compressed indices. This allows various tools on these tensors to share the same implementations that are parameterized by tensor layout.", "Sparse CSR, CSC, BSR, and CSC tensors can be constructed by using torch.sparse_compressed_tensor() function that have the same interface as the above discussed constructor functions torch.sparse_csr_tensor(), torch.sparse_csc_tensor(), torch.sparse_bsr_tensor(), and torch.sparse_bsc_tensor(), respectively, but with an extra required layout argument. The following example illustrates a method of constructing CSR and CSC tensors using the same input data by specifying the corresponding layout parameter to the torch.sparse_compressed_tensor() function:", "The following table summarizes supported Linear Algebra operations on sparse matrices where the operands layouts may vary. Here T[layout] denotes a tensor with a given layout. Similarly, M[layout] denotes a matrix (2-D PyTorch tensor), and V[layout] denotes a vector (1-D PyTorch tensor). In addition, f denotes a scalar (float or 0-D PyTorch tensor), * is element-wise multiplication, and @ is matrix multiplication.", "PyTorch operation", "Sparse grad?", "Layout signature", "torch.mv()", "no", "M[sparse_coo] @ V[strided] -> V[strided]", "torch.mv()", "no", "M[sparse_csr] @ V[strided] -> V[strided]", "torch.matmul()", "no", "M[sparse_coo] @ M[strided] -> M[strided]", "torch.matmul()", "no", "M[sparse_csr] @ M[strided] -> M[strided]", "torch.matmul()", "no", "M[SparseSemiStructured] @ M[strided] -> M[strided]", "torch.matmul()", "no", "M[strided] @ M[SparseSemiStructured] -> M[strided]", "torch.mm()", "no", "M[sparse_coo] @ M[strided] -> M[strided]", "torch.mm()", "no", "M[SparseSemiStructured] @ M[strided] -> M[strided]", "torch.mm()", "no", "M[strided] @ M[SparseSemiStructured] -> M[strided]", "torch.sparse.mm()", "yes", "M[sparse_coo] @ M[strided] -> M[strided]", "torch.smm()", "no", "M[sparse_coo] @ M[strided] -> M[sparse_coo]", "torch.hspmm()", "no", "M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo]", "torch.bmm()", "no", "T[sparse_coo] @ T[strided] -> T[strided]", "torch.addmm()", "no", "f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided]", "torch.addmm()", "no", "f * M[strided] + f * (M[SparseSemiStructured] @ M[strided]) -> M[strided]", "torch.addmm()", "no", "f * M[strided] + f * (M[strided] @ M[SparseSemiStructured]) -> M[strided]", "torch.sparse.addmm()", "yes", "f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided]", "torch.sspaddmm()", "no", "f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo]", "torch.lobpcg()", "no", "GENEIG(M[sparse_coo]) -> M[strided], M[strided]", "torch.pca_lowrank()", "yes", "PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided]", "torch.svd_lowrank()", "yes", "SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided]", "where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports backward with respect to sparse matrix argument. All PyTorch operations, except torch.smm(), support backward with respect to strided matrix arguments.", "Note", "Currently, PyTorch does not support matrix multiplication with the layout signature M[strided] @ M[sparse_coo]. However, applications can still compute this using the matrix relation D @\nS == (S.t() @ D.t()).t().", "The following Tensor methods are related to sparse tensors:", "Tensor.is_sparse", "Is True if the Tensor uses sparse COO storage layout, False otherwise.", "Tensor.is_sparse_csr", "Is True if the Tensor uses sparse CSR storage layout, False otherwise.", "Tensor.dense_dim", "Return the number of dense dimensions in a sparse tensor self.", "Tensor.sparse_dim", "Return the number of sparse dimensions in a sparse tensor self.", "Tensor.sparse_mask", "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask.", "Tensor.to_sparse", "Returns a sparse copy of the tensor.", "Tensor.to_sparse_coo", "Convert a tensor to coordinate format.", "Tensor.to_sparse_csr", "Convert a tensor to compressed row storage format (CSR).", "Tensor.to_sparse_csc", "Convert a tensor to compressed column storage (CSC) format.", "Tensor.to_sparse_bsr", "Convert a tensor to a block sparse row (BSR) storage format of given blocksize.", "Tensor.to_sparse_bsc", "Convert a tensor to a block sparse column (BSC) storage format of given blocksize.", "Tensor.to_dense", "Creates a strided copy of self if self is not a strided tensor, otherwise returns self.", "Tensor.values", "Return the values tensor of a sparse COO tensor.", "The following Tensor methods are specific to sparse COO tensors:", "Tensor.coalesce", "Returns a coalesced copy of self if self is an uncoalesced tensor.", "Tensor.sparse_resize_", "Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions.", "Tensor.sparse_resize_and_clear_", "Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions.", "Tensor.is_coalesced", "Returns True if self is a sparse COO tensor that is coalesced, False otherwise.", "Tensor.indices", "Return the indices tensor of a sparse COO tensor.", "The following methods are specific to sparse CSR tensors and sparse BSR tensors:", "Tensor.crow_indices", "Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr.", "Tensor.col_indices", "Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr.", "The following methods are specific to sparse CSC tensors and sparse BSC tensors:", "Tensor.row_indices", "Tensor.ccol_indices", "The following Tensor methods support sparse COO tensors:", "add() add_() addmm() addmm_() any() asin() asin_() arcsin() arcsin_() bmm() clone() deg2rad() deg2rad_() detach() detach_() dim() div() div_() floor_divide() floor_divide_() get_device() index_select() isnan() log1p() log1p_() mm() mul() mul_() mv() narrow_copy() neg() neg_() negative() negative_() numel() rad2deg() rad2deg_() resize_as_() size() pow() sqrt() square() smm() sspaddmm() sub() sub_() t() t_() transpose() transpose_() zero_()", "Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.", "Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices.", "Constructs a sparse tensor in CSC (Compressed Sparse Column) with specified values at the given ccol_indices and row_indices.", "Constructs a sparse tensor in BSR (Block Compressed Sparse Row)) with specified 2-dimensional blocks at the given crow_indices and col_indices.", "Constructs a sparse tensor in BSC (Block Compressed Sparse Column)) with specified 2-dimensional blocks at the given ccol_indices and row_indices.", "Constructs a sparse tensor in Compressed Sparse format - CSR, CSC, BSR, or BSC - with specified values at the given compressed_indices and plain_indices.", "sparse.sum", "Returns the sum of each row of the sparse tensor input in the given dimensions dim.", "sparse.addmm", "This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse COO matrix mat1.", "sparse.sampled_addmm", "Performs a matrix multiplication of the dense matrices mat1 and mat2 at the locations specified by the sparsity pattern of input.", "sparse.mm", "Performs a matrix multiplication of the sparse matrix mat1", "Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.", "Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.", "Performs a matrix multiplication of the sparse matrix input with the dense matrix mat.", "sparse.softmax", "Applies a softmax function.", "sparse.log_softmax", "Applies a softmax function followed by logarithm.", "sparse.spdiags", "Creates a sparse 2D tensor by placing the values from rows of diagonals along specified diagonals of the output", "The following torch functions support sparse tensors:", "cat() dstack() empty() empty_like() hstack() index_select() is_complex() is_floating_point() is_nonzero() is_same_size() is_signed() is_tensor() lobpcg() mm() native_norm() pca_lowrank() select() stack() svd_lowrank() unsqueeze() vstack() zeros() zeros_like()", "To manage checking sparse tensor invariants, see:", "sparse.check_sparse_tensor_invariants", "A tool to control checking sparse tensor invariants.", "To use sparse tensors with gradcheck() function, see:", "sparse.as_sparse_gradcheck", "Decorator for torch.autograd.gradcheck or its functools.partial variants that extends the gradcheck function with support to input functions that operate on or/and return sparse tensors.", "We aim to support all zero-preserving unary functions.", "If you find that we are missing a zero-preserving unary function that you need, please feel encouraged to open an issue for a feature request. As always please kindly try the search function first before opening an issue.", "The following operators currently support sparse COO/CSR/CSC/BSR/CSR tensor inputs.", "abs() asin() asinh() atan() atanh() ceil() conj_physical() floor() log1p() neg() round() sin() sinh() sign() sgn() signbit() tan() tanh() trunc() expm1() sqrt() angle() isinf() isposinf() isneginf() isnan() erf() erfinv()"]}, {"name": "torch.sparse.addmm()", "path": "generated/torch.sparse.addmm#torch.sparse.addmm", "type": "Sparse Tensors", "text": ["This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse COO matrix mat1. When mat1 is a COO tensor it must have sparse_dim = 2. When inputs are COO tensors, this function also supports backward for both inputs.", "Supports both CSR and COO storage formats.", "Note", "This function doesn\u2019t support computing derivaties with respect to CSR matrices."]}, {"name": "torch.sparse.as_sparse_gradcheck()", "path": "generated/torch.sparse.as_sparse_gradcheck#torch.sparse.as_sparse_gradcheck", "type": "Sparse Tensors", "text": ["Decorator for torch.autograd.gradcheck or its functools.partial variants that extends the gradcheck function with support to input functions that operate on or/and return sparse tensors.", "The specified gradcheck function itself is guaranteed to operate on strided tensors only.", "For example:"]}, {"name": "torch.sparse.check_sparse_tensor_invariants", "path": "generated/torch.sparse.check_sparse_tensor_invariants", "type": "Sparse Tensors", "text": ["A tool to control checking sparse tensor invariants.", "The following options exists to manage sparsr tensor invariants checking in sparse tensor construction:", "Using a context manager:", "Using a procedural approach:", "Using function decoration:", "Using check_invariants keyword argument in sparse tensor constructor call. For example:", "Disable sparse tensor invariants checking in sparse tensor constructors.", "See torch.sparse.check_sparse_tensor_invariants.enable() for more information.", "Enable sparse tensor invariants checking in sparse tensor constructors.", "Note", "By default, the sparse tensor invariants checks are disabled. Use torch.sparse.check_sparse_tensor_invariants.is_enabled() to retrieve the current state of sparse tensor invariants checking.", "Note", "The sparse tensor invariants check flag is effective to all sparse tensor constructors, both in Python and ATen.", "The flag can be locally overridden by the check_invariants optional argument of the sparse tensor constructor functions.", "Returns True if the sparse tensor invariants checking is enabled.", "Note", "Use torch.sparse.check_sparse_tensor_invariants.enable() or torch.sparse.check_sparse_tensor_invariants.disable() to manage the state of the sparse tensor invariants checks."]}, {"name": "torch.sparse.check_sparse_tensor_invariants.disable()", "path": "generated/torch.sparse.check_sparse_tensor_invariants#torch.sparse.check_sparse_tensor_invariants.disable", "type": "Sparse Tensors", "text": ["Disable sparse tensor invariants checking in sparse tensor constructors.", "See torch.sparse.check_sparse_tensor_invariants.enable() for more information."]}, {"name": "torch.sparse.check_sparse_tensor_invariants.enable()", "path": "generated/torch.sparse.check_sparse_tensor_invariants#torch.sparse.check_sparse_tensor_invariants.enable", "type": "Sparse Tensors", "text": ["Enable sparse tensor invariants checking in sparse tensor constructors.", "Note", "By default, the sparse tensor invariants checks are disabled. Use torch.sparse.check_sparse_tensor_invariants.is_enabled() to retrieve the current state of sparse tensor invariants checking.", "Note", "The sparse tensor invariants check flag is effective to all sparse tensor constructors, both in Python and ATen.", "The flag can be locally overridden by the check_invariants optional argument of the sparse tensor constructor functions."]}, {"name": "torch.sparse.check_sparse_tensor_invariants.is_enabled()", "path": "generated/torch.sparse.check_sparse_tensor_invariants#torch.sparse.check_sparse_tensor_invariants.is_enabled", "type": "Sparse Tensors", "text": ["Returns True if the sparse tensor invariants checking is enabled.", "Note", "Use torch.sparse.check_sparse_tensor_invariants.enable() or torch.sparse.check_sparse_tensor_invariants.disable() to manage the state of the sparse tensor invariants checks."]}, {"name": "torch.sparse.log_softmax()", "path": "generated/torch.sparse.log_softmax#torch.sparse.log_softmax", "type": "Sparse Tensors", "text": ["Applies a softmax function followed by logarithm.", "See softmax for more details."]}, {"name": "torch.sparse.mm()", "path": "generated/torch.sparse.mm#torch.sparse.mm", "type": "Sparse Tensors", "text": ["Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2. Similar to torch.mm(), if mat1 is a (n\u00d7m)(n \\times m) tensor, mat2 is a (m\u00d7p)(m \\times p) tensor, out will be a (n\u00d7p)(n \\times p) tensor. When mat1 is a COO tensor it must have sparse_dim = 2. When inputs are COO tensors, this function also supports backward for both inputs.", "Supports both CSR and COO storage formats.", "Note", "This function doesn\u2019t support computing derivaties with respect to CSR matrices.", "This function also additionally accepts an optional reduce argument that allows specification of an optional reduction operation, mathematically performs the following operation:", "where \u2a01\\bigoplus defines the reduce operator. reduce is implemented only for CSR storage format on CPU device.", "The format of the output tensor of this function follows: - sparse x sparse -> sparse - sparse x dense -> dense", "Example:"]}, {"name": "torch.sparse.sampled_addmm()", "path": "generated/torch.sparse.sampled_addmm#torch.sparse.sampled_addmm", "type": "Sparse Tensors", "text": ["Performs a matrix multiplication of the dense matrices mat1 and mat2 at the locations specified by the sparsity pattern of input. The matrix input is added to the final result.", "Mathematically this performs the following operation:", "where spy(input)\\text{spy}(\\text{input}) is the sparsity pattern matrix of input, alpha and beta are the scaling factors. spy(input)\\text{spy}(\\text{input}) has value 1 at the positions where input has non-zero values, and 0 elsewhere.", "Note", "input must be a sparse CSR tensor. mat1 and mat2 must be dense tensors.", "Examples:"]}, {"name": "torch.sparse.softmax()", "path": "generated/torch.sparse.softmax#torch.sparse.softmax", "type": "Sparse Tensors", "text": ["Applies a softmax function.", "Softmax is defined as:", "Softmax(xi)=exp(xi)\u2211jexp(xj)\\text{Softmax}(x_{i}) = \\frac{exp(x_i)}{\\sum_j exp(x_j)}", "where i,ji, j run over sparse tensor indices and unspecified entries are ignores. This is equivalent to defining unspecified entries as negative infinity so that exp(xk)=0exp(x_k) = 0 when the entry with index kk has not specified.", "It is applied to all slices along dim, and will re-scale them so that the elements lie in the range [0, 1] and sum to 1."]}, {"name": "torch.sparse.spdiags()", "path": "generated/torch.sparse.spdiags#torch.sparse.spdiags", "type": "Sparse Tensors", "text": ["Creates a sparse 2D tensor by placing the values from rows of diagonals along specified diagonals of the output", "The offsets tensor controls which diagonals are set.", "The number of rows in diagonals must match the length of offsets, and an offset may not be repeated.", "layout (torch.layout, optional) \u2013 The desired layout of the returned tensor. torch.sparse_coo, torch.sparse_csc and torch.sparse_csr are supported. Default: torch.sparse_coo", "Examples:", "Set the main and first two lower diagonals of a matrix:", "Change the output layout:", "Set partial diagonals of a large output:", "Note", "When setting the values along a given diagonal the index into the diagonal and the index into the row of diagonals is taken as the column index in the output. This has the effect that when setting a diagonal with a positive offset k the first value along that diagonal will be the value in position k of the row of diagonals", "Specifying a positive offset:"]}, {"name": "torch.sparse.sum()", "path": "generated/torch.sparse.sum#torch.sparse.sum", "type": "Sparse Tensors", "text": ["Returns the sum of each row of the sparse tensor input in the given dimensions dim. If dim is a list of dimensions, reduce over all of them. When sum over all sparse_dim, this method returns a dense tensor instead of a sparse tensor.", "All summed dim are squeezed (see torch.squeeze()), resulting an output tensor having dim fewer dimensions than input.", "During backward, only gradients at nnz locations of input will propagate back. Note that the gradients of input is coalesced.", "Tensor", "Example:"]}, {"name": "torch.sparse.torch.hspmm", "path": "generated/torch.hspmm", "type": "Sparse Tensors", "text": ["Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2. The result is a (1 + 1)-dimensional hybrid COO matrix.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.sparse.torch.smm", "path": "generated/torch.smm", "type": "Sparse Tensors", "text": ["Performs a matrix multiplication of the sparse matrix input with the dense matrix mat."]}, {"name": "torch.sparse.torch.sparse.addmm", "path": "generated/torch.sparse.addmm", "type": "Sparse Tensors", "text": ["This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse COO matrix mat1. When mat1 is a COO tensor it must have sparse_dim = 2. When inputs are COO tensors, this function also supports backward for both inputs.", "Supports both CSR and COO storage formats.", "Note", "This function doesn\u2019t support computing derivaties with respect to CSR matrices."]}, {"name": "torch.sparse.torch.sparse.as_sparse_gradcheck", "path": "generated/torch.sparse.as_sparse_gradcheck", "type": "Sparse Tensors", "text": ["Decorator for torch.autograd.gradcheck or its functools.partial variants that extends the gradcheck function with support to input functions that operate on or/and return sparse tensors.", "The specified gradcheck function itself is guaranteed to operate on strided tensors only.", "For example:"]}, {"name": "torch.sparse.torch.sparse.log_softmax", "path": "generated/torch.sparse.log_softmax", "type": "Sparse Tensors", "text": ["Applies a softmax function followed by logarithm.", "See softmax for more details."]}, {"name": "torch.sparse.torch.sparse.mm", "path": "generated/torch.sparse.mm", "type": "Sparse Tensors", "text": ["Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2. Similar to torch.mm(), if mat1 is a (n\u00d7m)(n \\times m) tensor, mat2 is a (m\u00d7p)(m \\times p) tensor, out will be a (n\u00d7p)(n \\times p) tensor. When mat1 is a COO tensor it must have sparse_dim = 2. When inputs are COO tensors, this function also supports backward for both inputs.", "Supports both CSR and COO storage formats.", "Note", "This function doesn\u2019t support computing derivaties with respect to CSR matrices.", "This function also additionally accepts an optional reduce argument that allows specification of an optional reduction operation, mathematically performs the following operation:", "where \u2a01\\bigoplus defines the reduce operator. reduce is implemented only for CSR storage format on CPU device.", "The format of the output tensor of this function follows: - sparse x sparse -> sparse - sparse x dense -> dense", "Example:"]}, {"name": "torch.sparse.torch.sparse.sampled_addmm", "path": "generated/torch.sparse.sampled_addmm", "type": "Sparse Tensors", "text": ["Performs a matrix multiplication of the dense matrices mat1 and mat2 at the locations specified by the sparsity pattern of input. The matrix input is added to the final result.", "Mathematically this performs the following operation:", "where spy(input)\\text{spy}(\\text{input}) is the sparsity pattern matrix of input, alpha and beta are the scaling factors. spy(input)\\text{spy}(\\text{input}) has value 1 at the positions where input has non-zero values, and 0 elsewhere.", "Note", "input must be a sparse CSR tensor. mat1 and mat2 must be dense tensors.", "Examples:"]}, {"name": "torch.sparse.torch.sparse.softmax", "path": "generated/torch.sparse.softmax", "type": "Sparse Tensors", "text": ["Applies a softmax function.", "Softmax is defined as:", "Softmax(xi)=exp(xi)\u2211jexp(xj)\\text{Softmax}(x_{i}) = \\frac{exp(x_i)}{\\sum_j exp(x_j)}", "where i,ji, j run over sparse tensor indices and unspecified entries are ignores. This is equivalent to defining unspecified entries as negative infinity so that exp(xk)=0exp(x_k) = 0 when the entry with index kk has not specified.", "It is applied to all slices along dim, and will re-scale them so that the elements lie in the range [0, 1] and sum to 1."]}, {"name": "torch.sparse.torch.sparse.spdiags", "path": "generated/torch.sparse.spdiags", "type": "Sparse Tensors", "text": ["Creates a sparse 2D tensor by placing the values from rows of diagonals along specified diagonals of the output", "The offsets tensor controls which diagonals are set.", "The number of rows in diagonals must match the length of offsets, and an offset may not be repeated.", "layout (torch.layout, optional) \u2013 The desired layout of the returned tensor. torch.sparse_coo, torch.sparse_csc and torch.sparse_csr are supported. Default: torch.sparse_coo", "Examples:", "Set the main and first two lower diagonals of a matrix:", "Change the output layout:", "Set partial diagonals of a large output:", "Note", "When setting the values along a given diagonal the index into the diagonal and the index into the row of diagonals is taken as the column index in the output. This has the effect that when setting a diagonal with a positive offset k the first value along that diagonal will be the value in position k of the row of diagonals", "Specifying a positive offset:"]}, {"name": "torch.sparse.torch.sparse.sum", "path": "generated/torch.sparse.sum", "type": "Sparse Tensors", "text": ["Returns the sum of each row of the sparse tensor input in the given dimensions dim. If dim is a list of dimensions, reduce over all of them. When sum over all sparse_dim, this method returns a dense tensor instead of a sparse tensor.", "All summed dim are squeezed (see torch.squeeze()), resulting an output tensor having dim fewer dimensions than input.", "During backward, only gradients at nnz locations of input will propagate back. Note that the gradients of input is coalesced.", "Tensor", "Example:"]}, {"name": "torch.sparse.torch.sparse_compressed_tensor", "path": "generated/torch.sparse_compressed_tensor", "type": "Sparse Tensors", "text": ["Constructs a sparse tensor in Compressed Sparse format - CSR, CSC, BSR, or BSC - with specified values at the given compressed_indices and plain_indices. Sparse matrix multiplication operations in Compressed Sparse format are typically faster than that for sparse tensors in COO format. Make you have a look at the note on the data type of the indices.", "Note", "If the device argument is not specified the device of the given values and indices tensor(s) must match. If, however, the argument is specified the input Tensors will be converted to the given device and in turn determine the device of the constructed sparse tensor."]}, {"name": "torch.sparse.torch.sspaddmm", "path": "generated/torch.sspaddmm", "type": "Sparse Tensors", "text": ["Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.", "Note: This function is equivalent to torch.addmm(), except input and mat1 are sparse."]}, {"name": "torch.sparse.torch.Tensor.ccol_indices", "path": "generated/torch.tensor.ccol_indices", "type": "Sparse Tensors", "text": []}, {"name": "torch.sparse.torch.Tensor.coalesce", "path": "generated/torch.tensor.coalesce", "type": "Sparse Tensors", "text": ["Returns a coalesced copy of self if self is an uncoalesced tensor.", "Returns self if self is a coalesced tensor.", "Warning", "Throws an error if self is not a sparse COO tensor."]}, {"name": "torch.sparse.torch.Tensor.col_indices", "path": "generated/torch.tensor.col_indices", "type": "Sparse Tensors", "text": ["Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The col_indices tensor is strictly of shape (self.nnz()) and of type int32 or int64. When using MKL routines such as sparse matrix multiplication, it is necessary to use int32 indexing in order to avoid downcasting and potentially losing information."]}, {"name": "torch.sparse.torch.Tensor.crow_indices", "path": "generated/torch.tensor.crow_indices", "type": "Sparse Tensors", "text": ["Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The crow_indices tensor is strictly of shape (self.size(0) + 1) and of type int32 or int64. When using MKL routines such as sparse matrix multiplication, it is necessary to use int32 indexing in order to avoid downcasting and potentially losing information."]}, {"name": "torch.sparse.torch.Tensor.is_coalesced", "path": "generated/torch.tensor.is_coalesced", "type": "Sparse Tensors", "text": ["Returns True if self is a sparse COO tensor that is coalesced, False otherwise.", "Warning", "Throws an error if self is not a sparse COO tensor.", "See coalesce() and uncoalesced tensors."]}, {"name": "torch.sparse.torch.Tensor.is_sparse_csr", "path": "generated/torch.tensor.is_sparse_csr", "type": "Sparse Tensors", "text": ["Is True if the Tensor uses sparse CSR storage layout, False otherwise."]}, {"name": "torch.sparse.torch.Tensor.row_indices", "path": "generated/torch.tensor.row_indices", "type": "Sparse Tensors", "text": []}, {"name": "torch.sparse.torch.Tensor.sparse_resize_", "path": "generated/torch.tensor.sparse_resize_", "type": "Sparse Tensors", "text": ["Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions.", "Note", "If the number of specified elements in self is zero, then size, sparse_dim, and dense_dim can be any size and positive integers such that len(size) == sparse_dim +\ndense_dim.", "If self specifies one or more elements, however, then each dimension in size must not be smaller than the corresponding dimension of self, sparse_dim must equal the number of sparse dimensions in self, and dense_dim must equal the number of dense dimensions in self.", "Warning", "Throws an error if self is not a sparse tensor."]}, {"name": "torch.sparse.torch.Tensor.sparse_resize_and_clear_", "path": "generated/torch.tensor.sparse_resize_and_clear_", "type": "Sparse Tensors", "text": ["Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions."]}, {"name": "torch.sparse.torch.Tensor.to_sparse_coo", "path": "generated/torch.tensor.to_sparse_coo", "type": "Sparse Tensors", "text": ["Convert a tensor to coordinate format.", "Examples:"]}, {"name": "torch.sparse_bsc_tensor", "path": "generated/torch.sparse_bsc_tensor", "type": "Torch", "text": ["Constructs a sparse tensor in BSC (Block Compressed Sparse Column)) with specified 2-dimensional blocks at the given ccol_indices and row_indices. Sparse matrix multiplication operations in BSC format are typically faster than that for sparse tensors in COO format. Make you have a look at the note on the data type of the indices.", "Note", "If the device argument is not specified the device of the given values and indices tensor(s) must match. If, however, the argument is specified the input Tensors will be converted to the given device and in turn determine the device of the constructed sparse tensor."]}, {"name": "torch.sparse_bsr_tensor", "path": "generated/torch.sparse_bsr_tensor", "type": "Torch", "text": ["Constructs a sparse tensor in BSR (Block Compressed Sparse Row)) with specified 2-dimensional blocks at the given crow_indices and col_indices. Sparse matrix multiplication operations in BSR format are typically faster than that for sparse tensors in COO format. Make you have a look at the note on the data type of the indices.", "Note", "If the device argument is not specified the device of the given values and indices tensor(s) must match. If, however, the argument is specified the input Tensors will be converted to the given device and in turn determine the device of the constructed sparse tensor."]}, {"name": "torch.sparse_compressed_tensor()", "path": "generated/torch.sparse_compressed_tensor#torch.sparse_compressed_tensor", "type": "Sparse Tensors", "text": ["Constructs a sparse tensor in Compressed Sparse format - CSR, CSC, BSR, or BSC - with specified values at the given compressed_indices and plain_indices. Sparse matrix multiplication operations in Compressed Sparse format are typically faster than that for sparse tensors in COO format. Make you have a look at the note on the data type of the indices.", "Note", "If the device argument is not specified the device of the given values and indices tensor(s) must match. If, however, the argument is specified the input Tensors will be converted to the given device and in turn determine the device of the constructed sparse tensor."]}, {"name": "torch.sparse_coo_tensor", "path": "generated/torch.sparse_coo_tensor", "type": "Torch", "text": ["Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.", "Note", "This function returns an uncoalesced tensor when is_coalesced is unspecified or None.", "Note", "If the device argument is not specified the device of the given values and indices tensor(s) must match. If, however, the argument is specified the input Tensors will be converted to the given device and in turn determine the device of the constructed sparse tensor.", "Example:"]}, {"name": "torch.sparse_csc_tensor", "path": "generated/torch.sparse_csc_tensor", "type": "Torch", "text": ["Constructs a sparse tensor in CSC (Compressed Sparse Column) with specified values at the given ccol_indices and row_indices. Sparse matrix multiplication operations in CSC format are typically faster than that for sparse tensors in COO format. Make you have a look at the note on the data type of the indices.", "Note", "If the device argument is not specified the device of the given values and indices tensor(s) must match. If, however, the argument is specified the input Tensors will be converted to the given device and in turn determine the device of the constructed sparse tensor."]}, {"name": "torch.sparse_csr_tensor", "path": "generated/torch.sparse_csr_tensor", "type": "Torch", "text": ["Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. Sparse matrix multiplication operations in CSR format are typically faster than that for sparse tensors in COO format. Make you have a look at the note on the data type of the indices.", "Note", "If the device argument is not specified the device of the given values and indices tensor(s) must match. If, however, the argument is specified the input Tensors will be converted to the given device and in turn determine the device of the constructed sparse tensor."]}, {"name": "torch.special", "path": "special", "type": "SciPy-like Special", "text": ["The torch.special module, modeled after SciPy\u2019s special module.", "Airy function Ai(input)\\text{Ai}\\left(\\text{input}\\right).", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Bessel function of the first kind of order 00.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Bessel function of the first kind of order 11.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Computes the logarithmic derivative of the gamma function on input.", "input (Tensor) \u2013 the tensor to compute the digamma function on", "out (Tensor, optional) \u2013 the output tensor.", "Note", "This function is similar to SciPy\u2019s scipy.special.digamma.", "Note", "From PyTorch 1.8 onwards, the digamma function returns -Inf for 0. Previously it returned NaN for 0.", "Example:", "Computes the entropy on input (as defined below), elementwise.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Computes the error function of input. The error function is defined as follows:", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Computes the complementary error function of input. The complementary error function is defined as follows:", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Computes the scaled complementary error function for each element of input. The scaled complementary error function is defined as follows:", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Computes the inverse error function of input. The inverse error function is defined in the range (\u22121,1)(-1, 1) as:", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Computes the base two exponential function of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Computes the expit (also known as the logistic sigmoid function) of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Computes the exponential of the elements minus 1 of input.", "Note", "This function provides greater precision than exp(x) - 1 for small values of x.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Computes the regularized lower incomplete gamma function:", "where both inputi\\text{input}_i and otheri\\text{other}_i are weakly positive and at least one is strictly positive. If both are zero or either is negative then outi=nan\\text{out}_i=\\text{nan}. \u0393(\u22c5)\\Gamma(\\cdot) in the equation above is the gamma function,", "See torch.special.gammaincc() and torch.special.gammaln() for related functions.", "Supports broadcasting to a common shape and float inputs.", "Note", "The backward pass with respect to input is not yet supported. Please open an issue on PyTorch\u2019s Github to request it.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Computes the regularized upper incomplete gamma function:", "where both inputi\\text{input}_i and otheri\\text{other}_i are weakly positive and at least one is strictly positive. If both are zero or either is negative then outi=nan\\text{out}_i=\\text{nan}. \u0393(\u22c5)\\Gamma(\\cdot) in the equation above is the gamma function,", "See torch.special.gammainc() and torch.special.gammaln() for related functions.", "Supports broadcasting to a common shape and float inputs.", "Note", "The backward pass with respect to input is not yet supported. Please open an issue on PyTorch\u2019s Github to request it.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Computes the natural logarithm of the absolute value of the gamma function on input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Computes the zeroth order modified Bessel function of the first kind for each element of input.", "input (Tensor) \u2013 the input tensor", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below) for each element of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Computes the first order modified Bessel function of the first kind (as defined below) for each element of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Computes the exponentially scaled first order modified Bessel function of the first kind (as defined below) for each element of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Alias for torch.log1p().", "Computes the log of the area under the standard Gaussian probability density function, integrated from minus infinity to input, elementwise.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Computes softmax followed by a logarithm.", "While mathematically equivalent to log(softmax(x)), doing these two operations separately is slower and numerically unstable. This function is computed as:", "Returns a new tensor with the logit of the elements of input. input is clamped to [eps, 1 - eps] when eps is not None. When eps is None and input < 0 or input > 1, the function will yields NaN.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Alias for torch.logsumexp().", "Computes the multivariate log-gamma function with dimension pp element-wise, given by", "where C=log\u2061(\u03c0)\u22c5p(p\u22121)4C = \\log(\\pi) \\cdot \\frac{p (p - 1)}{4} and \u0393(\u2212)\\Gamma(-) is the Gamma function.", "All elements must be greater than p\u221212\\frac{p - 1}{2}, otherwise the behavior is undefiend.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Computes the area under the standard Gaussian probability density function, integrated from minus infinity to input, elementwise.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Computes the argument, x, for which the area under the Gaussian probability density function (integrated from minus infinity to x) is equal to input, elementwise.", "Note", "Also known as quantile function for Normal Distribution.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Computes the nthn^{th} derivative of the digamma function on input. n\u22650n \\geq 0 is called the order of the polygamma function.", "Note", "This function is implemented only for nonnegative integers n\u22650n \\geq 0.", "out (Tensor, optional) \u2013 the output tensor.", "Alias for torch.special.digamma().", "Alias for torch.round().", "Scaled modified Bessel function of the second kind of order 00.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Scaled modified Bessel function of the second kind of order 11.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Computes the normalized sinc of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Computes the softmax function.", "Softmax is defined as:", "Softmax(xi)=exp\u2061(xi)\u2211jexp\u2061(xj)\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}", "It is applied to all slices along dim, and will re-scale them so that the elements lie in the range [0, 1] and sum to 1.", "Spherical Bessel function of the first kind of order 00.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Computes input * log1p(other) with the following cases.", "Similar to SciPy\u2019s scipy.special.xlog1py.", "Note", "At least one of input or other must be a tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Computes input * log(other) with the following cases.", "Similar to SciPy\u2019s scipy.special.xlogy.", "Note", "At least one of input or other must be a tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Computes the Hurwitz zeta function, elementwise.", "Note", "The Riemann zeta function corresponds to the case when q = 1", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.special.airy_ai()", "path": "special#torch.special.airy_ai", "type": "SciPy-like Special", "text": ["Airy function Ai(input)\\text{Ai}\\left(\\text{input}\\right).", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.special.bessel_j0()", "path": "special#torch.special.bessel_j0", "type": "SciPy-like Special", "text": ["Bessel function of the first kind of order 00.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.special.bessel_j1()", "path": "special#torch.special.bessel_j1", "type": "SciPy-like Special", "text": ["Bessel function of the first kind of order 11.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.special.digamma()", "path": "special#torch.special.digamma", "type": "SciPy-like Special", "text": ["Computes the logarithmic derivative of the gamma function on input.", "input (Tensor) \u2013 the tensor to compute the digamma function on", "out (Tensor, optional) \u2013 the output tensor.", "Note", "This function is similar to SciPy\u2019s scipy.special.digamma.", "Note", "From PyTorch 1.8 onwards, the digamma function returns -Inf for 0. Previously it returned NaN for 0.", "Example:"]}, {"name": "torch.special.entr()", "path": "special#torch.special.entr", "type": "SciPy-like Special", "text": ["Computes the entropy on input (as defined below), elementwise.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.special.erf()", "path": "special#torch.special.erf", "type": "SciPy-like Special", "text": ["Computes the error function of input. The error function is defined as follows:", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.special.erfc()", "path": "special#torch.special.erfc", "type": "SciPy-like Special", "text": ["Computes the complementary error function of input. The complementary error function is defined as follows:", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.special.erfcx()", "path": "special#torch.special.erfcx", "type": "SciPy-like Special", "text": ["Computes the scaled complementary error function for each element of input. The scaled complementary error function is defined as follows:", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.special.erfinv()", "path": "special#torch.special.erfinv", "type": "SciPy-like Special", "text": ["Computes the inverse error function of input. The inverse error function is defined in the range (\u22121,1)(-1, 1) as:", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.special.exp2()", "path": "special#torch.special.exp2", "type": "SciPy-like Special", "text": ["Computes the base two exponential function of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.special.expit()", "path": "special#torch.special.expit", "type": "SciPy-like Special", "text": ["Computes the expit (also known as the logistic sigmoid function) of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.special.expm1()", "path": "special#torch.special.expm1", "type": "SciPy-like Special", "text": ["Computes the exponential of the elements minus 1 of input.", "Note", "This function provides greater precision than exp(x) - 1 for small values of x.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.special.gammainc()", "path": "special#torch.special.gammainc", "type": "SciPy-like Special", "text": ["Computes the regularized lower incomplete gamma function:", "where both inputi\\text{input}_i and otheri\\text{other}_i are weakly positive and at least one is strictly positive. If both are zero or either is negative then outi=nan\\text{out}_i=\\text{nan}. \u0393(\u22c5)\\Gamma(\\cdot) in the equation above is the gamma function,", "See torch.special.gammaincc() and torch.special.gammaln() for related functions.", "Supports broadcasting to a common shape and float inputs.", "Note", "The backward pass with respect to input is not yet supported. Please open an issue on PyTorch\u2019s Github to request it.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.special.gammaincc()", "path": "special#torch.special.gammaincc", "type": "SciPy-like Special", "text": ["Computes the regularized upper incomplete gamma function:", "where both inputi\\text{input}_i and otheri\\text{other}_i are weakly positive and at least one is strictly positive. If both are zero or either is negative then outi=nan\\text{out}_i=\\text{nan}. \u0393(\u22c5)\\Gamma(\\cdot) in the equation above is the gamma function,", "See torch.special.gammainc() and torch.special.gammaln() for related functions.", "Supports broadcasting to a common shape and float inputs.", "Note", "The backward pass with respect to input is not yet supported. Please open an issue on PyTorch\u2019s Github to request it.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.special.gammaln()", "path": "special#torch.special.gammaln", "type": "SciPy-like Special", "text": ["Computes the natural logarithm of the absolute value of the gamma function on input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.special.i0()", "path": "special#torch.special.i0", "type": "SciPy-like Special", "text": ["Computes the zeroth order modified Bessel function of the first kind for each element of input.", "input (Tensor) \u2013 the input tensor", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.special.i0e()", "path": "special#torch.special.i0e", "type": "SciPy-like Special", "text": ["Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below) for each element of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.special.i1()", "path": "special#torch.special.i1", "type": "SciPy-like Special", "text": ["Computes the first order modified Bessel function of the first kind (as defined below) for each element of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.special.i1e()", "path": "special#torch.special.i1e", "type": "SciPy-like Special", "text": ["Computes the exponentially scaled first order modified Bessel function of the first kind (as defined below) for each element of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.special.log1p()", "path": "special#torch.special.log1p", "type": "SciPy-like Special", "text": ["Alias for torch.log1p()."]}, {"name": "torch.special.log_ndtr()", "path": "special#torch.special.log_ndtr", "type": "SciPy-like Special", "text": ["Computes the log of the area under the standard Gaussian probability density function, integrated from minus infinity to input, elementwise.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.special.log_softmax()", "path": "special#torch.special.log_softmax", "type": "SciPy-like Special", "text": ["Computes softmax followed by a logarithm.", "While mathematically equivalent to log(softmax(x)), doing these two operations separately is slower and numerically unstable. This function is computed as:"]}, {"name": "torch.special.logit()", "path": "special#torch.special.logit", "type": "SciPy-like Special", "text": ["Returns a new tensor with the logit of the elements of input. input is clamped to [eps, 1 - eps] when eps is not None. When eps is None and input < 0 or input > 1, the function will yields NaN.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.special.logsumexp()", "path": "special#torch.special.logsumexp", "type": "SciPy-like Special", "text": ["Alias for torch.logsumexp()."]}, {"name": "torch.special.multigammaln()", "path": "special#torch.special.multigammaln", "type": "SciPy-like Special", "text": ["Computes the multivariate log-gamma function with dimension pp element-wise, given by", "where C=log\u2061(\u03c0)\u22c5p(p\u22121)4C = \\log(\\pi) \\cdot \\frac{p (p - 1)}{4} and \u0393(\u2212)\\Gamma(-) is the Gamma function.", "All elements must be greater than p\u221212\\frac{p - 1}{2}, otherwise the behavior is undefiend.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.special.ndtr()", "path": "special#torch.special.ndtr", "type": "SciPy-like Special", "text": ["Computes the area under the standard Gaussian probability density function, integrated from minus infinity to input, elementwise.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.special.ndtri()", "path": "special#torch.special.ndtri", "type": "SciPy-like Special", "text": ["Computes the argument, x, for which the area under the Gaussian probability density function (integrated from minus infinity to x) is equal to input, elementwise.", "Note", "Also known as quantile function for Normal Distribution.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.special.polygamma()", "path": "special#torch.special.polygamma", "type": "SciPy-like Special", "text": ["Computes the nthn^{th} derivative of the digamma function on input. n\u22650n \\geq 0 is called the order of the polygamma function.", "Note", "This function is implemented only for nonnegative integers n\u22650n \\geq 0.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.special.psi()", "path": "special#torch.special.psi", "type": "SciPy-like Special", "text": ["Alias for torch.special.digamma()."]}, {"name": "torch.special.round()", "path": "special#torch.special.round", "type": "SciPy-like Special", "text": ["Alias for torch.round()."]}, {"name": "torch.special.scaled_modified_bessel_k0()", "path": "special#torch.special.scaled_modified_bessel_k0", "type": "SciPy-like Special", "text": ["Scaled modified Bessel function of the second kind of order 00.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.special.scaled_modified_bessel_k1()", "path": "special#torch.special.scaled_modified_bessel_k1", "type": "SciPy-like Special", "text": ["Scaled modified Bessel function of the second kind of order 11.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.special.sinc()", "path": "special#torch.special.sinc", "type": "SciPy-like Special", "text": ["Computes the normalized sinc of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.special.softmax()", "path": "special#torch.special.softmax", "type": "SciPy-like Special", "text": ["Computes the softmax function.", "Softmax is defined as:", "Softmax(xi)=exp\u2061(xi)\u2211jexp\u2061(xj)\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}", "It is applied to all slices along dim, and will re-scale them so that the elements lie in the range [0, 1] and sum to 1."]}, {"name": "torch.special.spherical_bessel_j0()", "path": "special#torch.special.spherical_bessel_j0", "type": "SciPy-like Special", "text": ["Spherical Bessel function of the first kind of order 00.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.special.xlog1py()", "path": "special#torch.special.xlog1py", "type": "SciPy-like Special", "text": ["Computes input * log1p(other) with the following cases.", "Similar to SciPy\u2019s scipy.special.xlog1py.", "Note", "At least one of input or other must be a tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.special.xlogy()", "path": "special#torch.special.xlogy", "type": "SciPy-like Special", "text": ["Computes input * log(other) with the following cases.", "Similar to SciPy\u2019s scipy.special.xlogy.", "Note", "At least one of input or other must be a tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.special.zeta()", "path": "special#torch.special.zeta", "type": "SciPy-like Special", "text": ["Computes the Hurwitz zeta function, elementwise.", "Note", "The Riemann zeta function corresponds to the case when q = 1", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.split", "path": "generated/torch.split", "type": "Torch", "text": ["Splits the tensor into chunks. Each chunk is a view of the original tensor.", "If split_size_or_sections is an integer type, then tensor will be split into equally sized chunks (if possible). Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by split_size.", "If split_size_or_sections is a list, then tensor will be split into len(split_size_or_sections) chunks with sizes in dim according to split_size_or_sections.", "Tuple[Tensor, \u2026]", "Example:"]}, {"name": "torch.sqrt", "path": "generated/torch.sqrt", "type": "Torch", "text": ["Returns a new tensor with the square-root of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.square", "path": "generated/torch.square", "type": "Torch", "text": ["Returns a new tensor with the square of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.squeeze", "path": "generated/torch.squeeze", "type": "Torch", "text": ["Returns a tensor with all specified dimensions of input of size 1 removed.", "For example, if input is of shape: (A\u00d71\u00d7B\u00d7C\u00d71\u00d7D)(A \\times 1 \\times B \\times C \\times 1 \\times D) then the input.squeeze() will be of shape: (A\u00d7B\u00d7C\u00d7D)(A \\times B \\times C \\times D).", "When dim is given, a squeeze operation is done only in the given dimension(s). If input is of shape: (A\u00d71\u00d7B)(A \\times 1 \\times B), squeeze(input, 0) leaves the tensor unchanged, but squeeze(input, 1) will squeeze the tensor to the shape (A\u00d7B)(A \\times B).", "Note", "The returned tensor shares the storage with the input tensor, so changing the contents of one will change the contents of the other.", "Warning", "If the tensor has a batch dimension of size 1, then squeeze(input) will also remove the batch dimension, which can lead to unexpected errors. Consider specifying only the dims you wish to be squeezed.", "dim (int or tuple of ints, optional) \u2013 ", "only in the specified dimensions.", "Changed in version 2.0: dim now accepts tuples of dimensions.", "Example:"]}, {"name": "torch.sspaddmm()", "path": "generated/torch.sspaddmm#torch.sspaddmm", "type": "Sparse Tensors", "text": ["Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.", "Note: This function is equivalent to torch.addmm(), except input and mat1 are sparse."]}, {"name": "torch.stack", "path": "generated/torch.stack", "type": "Torch", "text": ["Concatenates a sequence of tensors along a new dimension.", "All tensors need to be of the same size.", "See also", "torch.cat() concatenates the given sequence along an existing dimension.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.std", "path": "generated/torch.std", "type": "Torch", "text": ["Calculates the standard deviation over the dimensions specified by dim. dim can be a single dimension, list of dimensions, or None to reduce over all dimensions.", "The standard deviation (\u03c3\\sigma) is calculated as", "where xx is the sample set of elements, x\u02c9\\bar{x} is the sample mean, NN is the number of samples and \u03b4N\\delta N is the correction.", "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).", "correction (int) \u2013 ", "difference between the sample size and sample degrees of freedom. Defaults to Bessel\u2019s correction, correction=1.", "Changed in version 2.0: Previously this argument was called unbiased and was a boolean with True corresponding to correction=1 and False being correction=0."]}, {"name": "torch.std_mean", "path": "generated/torch.std_mean", "type": "Torch", "text": ["Calculates the standard deviation and mean over the dimensions specified by dim. dim can be a single dimension, list of dimensions, or None to reduce over all dimensions.", "The standard deviation (\u03c3\\sigma) is calculated as", "where xx is the sample set of elements, x\u02c9\\bar{x} is the sample mean, NN is the number of samples and \u03b4N\\delta N is the correction.", "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).", "correction (int) \u2013 ", "difference between the sample size and sample degrees of freedom. Defaults to Bessel\u2019s correction, correction=1.", "Changed in version 2.0: Previously this argument was called unbiased and was a boolean with True corresponding to correction=1 and False being correction=0.", "A tuple (std, mean) containing the standard deviation and mean."]}, {"name": "torch.stft", "path": "generated/torch.stft", "type": "Torch", "text": ["Short-time Fourier transform (STFT).", "Warning", "From version 1.8.0, return_complex must always be given explicitly for real inputs and return_complex=False has been deprecated. Strongly prefer return_complex=True as in a future pytorch release, this function will only return complex tensors.", "Note that torch.view_as_real() can be used to recover a real tensor with an extra last dimension for real and imaginary components.", "The STFT computes the Fourier transform of short overlapping windows of the input. This giving frequency components of the signal as they change over time. The interface of this function is modeled after (but not a drop-in replacement for) librosa stft function.", "Ignoring the optional batch dimension, this method computes the following expression:", "where mm is the index of the sliding window, and \u03c9\\omega is the frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft} for onesided=False, or 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 1 for onesided=True.", "Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T) if return_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N \\times T \\times 2). Where \u2217* is the optional batch size of input, NN is the number of frequencies where STFT is applied and TT is the total number of frames used.", "Warning", "This function changed signature at version 0.4.1. Calling with the previous signature may cause error or return incorrect result.", "return_complex (bool, optional) \u2013 ", "whether to return a complex tensor, or a real tensor with an extra last dimension for the real and imaginary components.", "Changed in version 2.0: return_complex is now a required argument for real inputs, as the default is being transitioned to True.", "Deprecated since version 2.0: return_complex=False is deprecated, instead use return_complex=True Note that calling torch.view_as_real() on the output will recover the deprecated output format.", "Tensor"]}, {"name": "torch.Storage", "path": "storage", "type": "Storage", "text": ["torch.Storage is an alias for the storage class that corresponds with the default data type (torch.get_default_dtype()). For instance, if the default data type is torch.float, torch.Storage resolves to torch.FloatStorage.", "The torch.<type>Storage and torch.cuda.<type>Storage classes, like torch.FloatStorage, torch.IntStorage, etc., are not actually ever instantiated. Calling their constructors creates a torch.TypedStorage with the appropriate torch.dtype and torch.device. torch.<type>Storage classes have all of the same class methods that torch.TypedStorage has.", "A torch.TypedStorage is a contiguous, one-dimensional array of elements of a particular torch.dtype. It can be given any torch.dtype, and the internal data will be interpreted appropriately. torch.TypedStorage contains a torch.UntypedStorage which holds the data as an untyped array of bytes.", "Every strided torch.Tensor contains a torch.TypedStorage, which stores all of the data that the torch.Tensor views.", "Warning", "All storage classes except for torch.UntypedStorage will be removed in the future, and torch.UntypedStorage will be used in all cases.", "Casts this storage to bfloat16 type", "Casts this storage to bool type", "Casts this storage to byte type", "Casts this storage to char type", "Returns a copy of this storage", "Casts this storage to complex double type", "Casts this storage to complex float type", "Returns a CPU copy of this storage if it\u2019s not already on the CPU", "Returns a copy of this object in CUDA memory.", "If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.", "T", "Casts this storage to double type", "Casts this storage to float type", "If shared is True, then memory is shared between all processes. All changes are written to the file. If shared is False, then the changes on the storage do not affect the file.", "size is the number of elements in the storage. If shared is False, then the file must contain at least size * sizeof(Type) bytes (Type is the type of storage). If shared is True the file will be created if needed.", "int", "Casts this storage to half type", "Returns a copy of this object in HPU memory.", "If this object is already in HPU memory and on the correct device, then no copy is performed and the original object is returned.", "T", "Casts this storage to int type", "Determine whether the CPU TypedStorage is already pinned on device.", "device (str or torch.device) \u2013 The device to pin memory on. Default: 'cuda'", "A boolean variable.", "Casts this storage to long type", "Copies the CPU TypedStorage to pinned memory, if it\u2019s not already pinned.", "device (str or torch.device) \u2013 The device to pin memory on. Default: 'cuda'.", "A pinned CPU storage.", "Moves the storage to shared memory.", "This is a no-op for storages already in shared memory and for CUDA storages, which do not need to be moved for sharing across processes. Storages in shared memory cannot be resized.", "Returns: self", "Casts this storage to short type", "Returns a list containing the elements of this storage", "Returns the type if dtype is not provided, else casts this object to the specified type.", "If this is already of the correct type, no copy is performed and the original object is returned.", "Union[T, str]", "Returns the internal torch.UntypedStorage", "Casts this storage to bfloat16 type", "Casts this storage to bool type", "Casts this storage to byte type", "Swaps bytes in underlying data", "Casts this storage to char type", "Returns a copy of this storage", "Casts this storage to complex double type", "Casts this storage to complex float type", "Returns a CPU copy of this storage if it\u2019s not already on the CPU", "Returns a copy of this object in CUDA memory.", "If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.", "Casts this storage to double type", "Casts this storage to float type", "If shared is True, then memory is shared between all processes. All changes are written to the file. If shared is False, then the changes on the storage do not affect the file.", "size is the number of elements in the storage. If shared is False, then the file must contain at least size * sizeof(Type) bytes (Type is the type of storage). If shared is True the file will be created if needed.", "int", "Casts this storage to half type", "Returns a copy of this object in HPU memory.", "If this object is already in HPU memory and on the correct device, then no copy is performed and the original object is returned.", "Casts this storage to int type", "Determine whether the CPU storage is already pinned on device.", "device (str or torch.device) \u2013 The device to pin memory on. Default: 'cuda'.", "A boolean variable.", "Casts this storage to long type", "Returns a MPS copy of this storage if it\u2019s not already on the MPS", "Copies the CPU storage to pinned memory, if it\u2019s not already pinned.", "device (str or torch.device) \u2013 The device to pin memory on. Default: 'cuda'.", "A pinned CPU storage.", "Casts this storage to short type", "int", "Returns a list containing the elements of this storage", "Returns the type if dtype is not provided, else casts this object to the specified type.", "If this is already of the correct type, no copy is performed and the original object is returned."]}, {"name": "torch.sub", "path": "generated/torch.sub", "type": "Torch", "text": ["Subtracts other, scaled by alpha, from input.", "Supports broadcasting to a common shape, type promotion, and integer, float, and complex inputs.", "Example:"]}, {"name": "torch.subtract", "path": "generated/torch.subtract", "type": "Torch", "text": ["Alias for torch.sub()."]}, {"name": "torch.sum", "path": "generated/torch.sum", "type": "Torch", "text": ["Returns the sum of all elements in the input tensor.", "input (Tensor) \u2013 the input tensor.", "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.", "Example:", "Returns the sum of each row of the input tensor in the given dimension dim. If dim is a list of dimensions, reduce over all of them.", "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).", "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.", "Example:"]}, {"name": "torch.svd", "path": "generated/torch.svd", "type": "Torch", "text": ["Computes the singular value decomposition of either a matrix or batch of matrices input. The singular value decomposition is represented as a namedtuple (U, S, V), such that input =Udiag(S)VH= U \\text{diag}(S) V^{\\text{H}}. where VHV^{\\text{H}} is the transpose of V for real inputs, and the conjugate transpose of V for complex inputs. If input is a batch of matrices, then U, S, and V are also batched with the same batch dimensions as input.", "If some is True (default), the method returns the reduced singular value decomposition. In this case, if the last two dimensions of input are m and n, then the returned U and V matrices will contain only min(n, m) orthonormal columns.", "If compute_uv is False, the returned U and V will be zero-filled matrices of shape (m, m) and (n, n) respectively, and the same device as input. The argument some has no effect when compute_uv is False.", "Supports input of float, double, cfloat and cdouble data types. The dtypes of U and V are the same as input\u2019s. S will always be real-valued, even if input is complex.", "Warning", "torch.svd() is deprecated in favor of torch.linalg.svd() and will be removed in a future PyTorch release.", "U, S, V = torch.svd(A, some=some, compute_uv=True) (default) should be replaced with", "_, S, _ = torch.svd(A, some=some, compute_uv=False) should be replaced with", "Note", "Differences with torch.linalg.svd():", "Note", "The singular values are returned in descending order. If input is a batch of matrices, then the singular values of each matrix in the batch are returned in descending order.", "Note", "The S tensor can only be used to compute gradients if compute_uv is True.", "Note", "When some is False, the gradients on U[\u2026, :, min(m, n):] and V[\u2026, :, min(m, n):] will be ignored in the backward pass, as those vectors can be arbitrary bases of the corresponding subspaces.", "Note", "The implementation of torch.linalg.svd() on CPU uses LAPACK\u2019s routine ?gesdd (a divide-and-conquer algorithm) instead of ?gesvd for speed. Analogously, on GPU, it uses cuSOLVER\u2019s routines gesvdj and gesvdjBatched on CUDA 10.1.243 and later, and MAGMA\u2019s routine gesdd on earlier versions of CUDA.", "Note", "The returned U will not be contiguous. The matrix (or batch of matrices) will be represented as a column-major matrix (i.e. Fortran-contiguous).", "Warning", "The gradients with respect to U and V will only be finite when the input does not have zero nor repeated singular values.", "Warning", "If the distance between any two singular values is close to zero, the gradients with respect to U and V will be numerically unstable, as they depends on 1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}. The same happens when the matrix has small singular values, as these gradients also depend on S\u207b\u00b9.", "Warning", "For complex-valued input the singular value decomposition is not unique, as U and V may be multiplied by an arbitrary phase factor ei\u03d5e^{i \\phi} on every column. The same happens when input has repeated singular values, where one may multiply the columns of the spanning subspace in U and V by a rotation matrix and the resulting vectors will span the same subspace. Different platforms, like NumPy, or inputs on different device types, may produce different U and V tensors.", "out (tuple, optional) \u2013 the output tuple of tensors", "Example:"]}, {"name": "torch.svd_lowrank", "path": "generated/torch.svd_lowrank", "type": "Torch", "text": ["Return the singular value decomposition (U, S, V) of a matrix, batches of matrices, or a sparse matrix AA such that A\u2248Udiag(S)VTA \\approx U diag(S) V^T. In case MM is given, then SVD is computed for the matrix A\u2212MA - M.", "Note", "The implementation is based on the Algorithm 5.1 from Halko et al, 2009.", "Note", "To obtain repeatable results, reset the seed for the pseudorandom number generator", "Note", "The input is assumed to be a low-rank matrix.", "Note", "In general, use the full-rank SVD implementation torch.linalg.svd() for dense matrices due to its 10-fold higher performance characteristics. The low-rank SVD will be useful for huge sparse matrices that torch.linalg.svd() cannot handle.", "A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)", "q (int, optional): a slightly overestimated rank of A.", "conduct; niter must be a nonnegative integer, and defaults to 2", "(\u2217,1,n)(*, 1, n).", "Tuple[Tensor, Tensor, Tensor]"]}, {"name": "torch.swapaxes", "path": "generated/torch.swapaxes", "type": "Torch", "text": ["Alias for torch.transpose().", "This function is equivalent to NumPy\u2019s swapaxes function.", "Examples:"]}, {"name": "torch.swapdims", "path": "generated/torch.swapdims", "type": "Torch", "text": ["Alias for torch.transpose().", "This function is equivalent to NumPy\u2019s swapaxes function.", "Examples:"]}, {"name": "torch.sym_float", "path": "generated/torch.sym_float", "type": "Torch", "text": ["SymInt-aware utility for float casting.", "a (SymInt, SymFloat, or object) \u2013 Object to cast"]}, {"name": "torch.sym_int", "path": "generated/torch.sym_int", "type": "Torch", "text": ["SymInt-aware utility for int casting.", "a (SymInt, SymFloat, or object) \u2013 Object to cast"]}, {"name": "torch.sym_max", "path": "generated/torch.sym_max", "type": "Torch", "text": ["SymInt-aware utility for max()."]}, {"name": "torch.sym_min", "path": "generated/torch.sym_min", "type": "Torch", "text": ["SymInt-aware utility for max()."]}, {"name": "torch.sym_not", "path": "generated/torch.sym_not", "type": "Torch", "text": ["SymInt-aware utility for logical negation.", "a (SymBool or bool) \u2013 Object to negate"]}, {"name": "torch.SymBool", "path": "torch#torch.SymBool", "type": "Torch", "text": ["Like an bool (including magic methods), but redirects all operations on the wrapped node. This is used in particular to symbolically record operations in the symbolic shape workflow.", "Unlike regular bools, regular boolean operators will force extra guards instead of symbolically evaluate. Use the bitwise operators instead to handle this."]}, {"name": "torch.SymFloat", "path": "torch#torch.SymFloat", "type": "Torch", "text": ["Like an float (including magic methods), but redirects all operations on the wrapped node. This is used in particular to symbolically record operations in the symbolic shape workflow."]}, {"name": "torch.SymInt", "path": "torch#torch.SymInt", "type": "Torch", "text": ["Like an int (including magic methods), but redirects all operations on the wrapped node. This is used in particular to symbolically record operations in the symbolic shape workflow."]}, {"name": "torch.t", "path": "generated/torch.t", "type": "Torch", "text": ["Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.", "0-D and 1-D tensors are returned as is. When input is a 2-D tensor this is equivalent to transpose(input, 0, 1).", "input (Tensor) \u2013 the input tensor.", "Example:", "See also torch.transpose()."]}, {"name": "torch.Tag", "path": "torch#torch.Tag", "type": "Torch", "text": ["Members:", "core", "data_dependent_output", "dynamic_output_shape", "generated", "inplace_view", "nondeterministic_bitwise", "nondeterministic_seeded", "pointwise", "view_copy"]}, {"name": "torch.Tag.name", "path": "torch#torch.Tag.name", "type": "Torch", "text": []}, {"name": "torch.take", "path": "generated/torch.take", "type": "Torch", "text": ["Returns a new tensor with the elements of input at the given indices. The input tensor is treated as if it were viewed as a 1-D tensor. The result takes the same shape as the indices.", "Example:"]}, {"name": "torch.take_along_dim", "path": "generated/torch.take_along_dim", "type": "Torch", "text": ["Selects values from input at the 1-dimensional indices from indices along the given dim.", "Functions that return indices along a dimension, like torch.argmax() and torch.argsort(), are designed to work with this function. See the examples below.", "Note", "This function is similar to NumPy\u2019s take_along_axis. See also torch.gather().", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.tan", "path": "generated/torch.tan", "type": "Torch", "text": ["Returns a new tensor with the tangent of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.tanh", "path": "generated/torch.tanh", "type": "Torch", "text": ["Returns a new tensor with the hyperbolic tangent of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.Tensor", "path": "tensors", "type": "Tensor", "text": ["A torch.Tensor is a multi-dimensional matrix containing elements of a single data type.", "Torch defines 10 tensor types with CPU and GPU variants which are as follows:", "Data type", "dtype", "CPU tensor", "GPU tensor", "32-bit floating point", "torch.float32 or torch.float", "torch.FloatTensor", "torch.cuda.FloatTensor", "64-bit floating point", "torch.float64 or torch.double", "torch.DoubleTensor", "torch.cuda.DoubleTensor", "16-bit floating point 1", "torch.float16 or torch.half", "torch.HalfTensor", "torch.cuda.HalfTensor", "16-bit floating point 2", "torch.bfloat16", "torch.BFloat16Tensor", "torch.cuda.BFloat16Tensor", "32-bit complex", "torch.complex32 or torch.chalf", "64-bit complex", "torch.complex64 or torch.cfloat", "128-bit complex", "torch.complex128 or torch.cdouble", "8-bit integer (unsigned)", "torch.uint8", "torch.ByteTensor", "torch.cuda.ByteTensor", "8-bit integer (signed)", "torch.int8", "torch.CharTensor", "torch.cuda.CharTensor", "16-bit integer (signed)", "torch.int16 or torch.short", "torch.ShortTensor", "torch.cuda.ShortTensor", "32-bit integer (signed)", "torch.int32 or torch.int", "torch.IntTensor", "torch.cuda.IntTensor", "64-bit integer (signed)", "torch.int64 or torch.long", "torch.LongTensor", "torch.cuda.LongTensor", "Boolean", "torch.bool", "torch.BoolTensor", "torch.cuda.BoolTensor", "quantized 8-bit integer (unsigned)", "torch.quint8", "torch.ByteTensor", "/", "quantized 8-bit integer (signed)", "torch.qint8", "torch.CharTensor", "/", "quantized 32-bit integer (signed)", "torch.qint32", "torch.IntTensor", "/", "quantized 4-bit integer (unsigned) 3", "torch.quint4x2", "torch.ByteTensor", "/", "Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10 significand bits. Useful when precision is important at the expense of range.", "Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7 significand bits. Useful when range is important, since it has the same number of exponent bits as float32", "quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator.", "torch.Tensor is an alias for the default tensor type (torch.FloatTensor).", "A tensor can be constructed from a Python list or sequence using the torch.tensor() constructor:", "Warning", "torch.tensor() always copies data. If you have a Tensor data and just want to change its requires_grad flag, use requires_grad_() or detach() to avoid a copy. If you have a numpy array and want to avoid a copy, use torch.as_tensor().", "A tensor of specific data type can be constructed by passing a torch.dtype and/or a torch.device to a constructor or tensor creation op:", "For more information about building Tensors, see Creation Ops", "The contents of a tensor can be accessed and modified using Python\u2019s indexing and slicing notation:", "Use torch.Tensor.item() to get a Python number from a tensor containing a single value:", "For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops", "A tensor can be created with requires_grad=True so that torch.autograd records operations on them for automatic differentiation.", "Each tensor has an associated torch.Storage, which holds its data. The tensor class also provides multi-dimensional, strided view of a storage and defines numeric operations on it.", "Note", "For more information on tensor views, see Tensor Views.", "Note", "For more information on the torch.dtype, torch.device, and torch.layout attributes of a torch.Tensor, see Tensor Attributes.", "Note", "Methods which mutate a tensor are marked with an underscore suffix. For example, torch.FloatTensor.abs_() computes the absolute value in-place and returns the modified tensor, while torch.FloatTensor.abs() computes the result in a new tensor.", "Note", "To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using to() method on the tensor.", "Warning", "Current implementation of torch.Tensor introduces memory overhead, thus it might lead to unexpectedly high memory usage in the applications with many tiny tensors. If this is your case, consider using one large structure.", "There are a few main ways to create a tensor, depending on your use case.", "Returns a view of this tensor with its dimensions reversed.", "If n is the number of dimensions in x, x.T is equivalent to x.permute(n-1, n-2, ..., 0).", "Warning", "The use of Tensor.T() on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider mT to transpose batches of matrices or x.permute(*torch.arange(x.ndim - 1, -1, -1)) to reverse the dimensions of a tensor.", "Returns a view of a matrix (2-D tensor) conjugated and transposed.", "x.H is equivalent to x.transpose(0, 1).conj() for complex matrices and x.transpose(0, 1) for real matrices.", "See also", "mH: An attribute that also works on batches of matrices.", "Returns a view of this tensor with the last two dimensions transposed.", "x.mT is equivalent to x.transpose(-2, -1).", "Accessing this property is equivalent to calling adjoint().", "Tensor.new_tensor", "Returns a new Tensor with data as the tensor data.", "Tensor.new_full", "Returns a Tensor of size size filled with fill_value.", "Tensor.new_empty", "Returns a Tensor of size size filled with uninitialized data.", "Tensor.new_ones", "Returns a Tensor of size size filled with 1.", "Tensor.new_zeros", "Returns a Tensor of size size filled with 0.", "Tensor.is_cuda", "Is True if the Tensor is stored on the GPU, False otherwise.", "Tensor.is_quantized", "Is True if the Tensor is quantized, False otherwise.", "Tensor.is_meta", "Is True if the Tensor is a meta tensor, False otherwise.", "Tensor.device", "Is the torch.device where this Tensor is.", "Tensor.grad", "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self.", "Tensor.ndim", "Alias for dim()", "Tensor.real", "Returns a new tensor containing real values of the self tensor for a complex-valued input tensor.", "Tensor.imag", "Returns a new tensor containing imaginary values of the self tensor.", "Tensor.nbytes", "Returns the number of bytes consumed by the \"view\" of elements of the Tensor if the Tensor does not use sparse storage layout.", "Tensor.itemsize", "Alias for element_size()", "Tensor.abs", "See torch.abs()", "Tensor.abs_", "In-place version of abs()", "Tensor.absolute", "Alias for abs()", "Tensor.absolute_", "In-place version of absolute() Alias for abs_()", "Tensor.acos", "See torch.acos()", "Tensor.acos_", "In-place version of acos()", "Tensor.arccos", "See torch.arccos()", "Tensor.arccos_", "In-place version of arccos()", "Tensor.add", "Add a scalar or tensor to self tensor.", "Tensor.add_", "In-place version of add()", "Tensor.addbmm", "See torch.addbmm()", "Tensor.addbmm_", "In-place version of addbmm()", "Tensor.addcdiv", "See torch.addcdiv()", "Tensor.addcdiv_", "In-place version of addcdiv()", "Tensor.addcmul", "See torch.addcmul()", "Tensor.addcmul_", "In-place version of addcmul()", "Tensor.addmm", "See torch.addmm()", "Tensor.addmm_", "In-place version of addmm()", "Tensor.sspaddmm", "See torch.sspaddmm()", "Tensor.addmv", "See torch.addmv()", "Tensor.addmv_", "In-place version of addmv()", "Tensor.addr", "See torch.addr()", "Tensor.addr_", "In-place version of addr()", "Tensor.adjoint", "Alias for adjoint()", "Tensor.allclose", "See torch.allclose()", "Tensor.amax", "See torch.amax()", "Tensor.amin", "See torch.amin()", "Tensor.aminmax", "See torch.aminmax()", "Tensor.angle", "See torch.angle()", "Tensor.apply_", "Applies the function callable to each element in the tensor, replacing each element with the value returned by callable.", "Tensor.argmax", "See torch.argmax()", "Tensor.argmin", "See torch.argmin()", "Tensor.argsort", "See torch.argsort()", "Tensor.argwhere", "See torch.argwhere()", "Tensor.asin", "See torch.asin()", "Tensor.asin_", "In-place version of asin()", "Tensor.arcsin", "See torch.arcsin()", "Tensor.arcsin_", "In-place version of arcsin()", "Tensor.as_strided", "See torch.as_strided()", "Tensor.atan", "See torch.atan()", "Tensor.atan_", "In-place version of atan()", "Tensor.arctan", "See torch.arctan()", "Tensor.arctan_", "In-place version of arctan()", "Tensor.atan2", "See torch.atan2()", "Tensor.atan2_", "In-place version of atan2()", "Tensor.arctan2", "See torch.arctan2()", "Tensor.arctan2_", "atan2_(other) -> Tensor", "Tensor.all", "See torch.all()", "Tensor.any", "See torch.any()", "Tensor.backward", "Computes the gradient of current tensor wrt graph leaves.", "Tensor.baddbmm", "See torch.baddbmm()", "Tensor.baddbmm_", "In-place version of baddbmm()", "Tensor.bernoulli", "Returns a result tensor where each result[i]\\texttt{result[i]} is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]}).", "Tensor.bernoulli_", "Fills each location of self with an independent sample from Bernoulli(p)\\text{Bernoulli}(\\texttt{p}).", "Tensor.bfloat16", "self.bfloat16() is equivalent to self.to(torch.bfloat16).", "Tensor.bincount", "See torch.bincount()", "Tensor.bitwise_not", "See torch.bitwise_not()", "Tensor.bitwise_not_", "In-place version of bitwise_not()", "Tensor.bitwise_and", "See torch.bitwise_and()", "Tensor.bitwise_and_", "In-place version of bitwise_and()", "Tensor.bitwise_or", "See torch.bitwise_or()", "Tensor.bitwise_or_", "In-place version of bitwise_or()", "Tensor.bitwise_xor", "See torch.bitwise_xor()", "Tensor.bitwise_xor_", "In-place version of bitwise_xor()", "Tensor.bitwise_left_shift", "See torch.bitwise_left_shift()", "Tensor.bitwise_left_shift_", "In-place version of bitwise_left_shift()", "Tensor.bitwise_right_shift", "See torch.bitwise_right_shift()", "Tensor.bitwise_right_shift_", "In-place version of bitwise_right_shift()", "Tensor.bmm", "See torch.bmm()", "Tensor.bool", "self.bool() is equivalent to self.to(torch.bool).", "Tensor.byte", "self.byte() is equivalent to self.to(torch.uint8).", "Tensor.broadcast_to", "See torch.broadcast_to().", "Tensor.cauchy_", "Fills the tensor with numbers drawn from the Cauchy distribution:", "Tensor.ceil", "See torch.ceil()", "Tensor.ceil_", "In-place version of ceil()", "Tensor.char", "self.char() is equivalent to self.to(torch.int8).", "Tensor.cholesky", "See torch.cholesky()", "Tensor.cholesky_inverse", "See torch.cholesky_inverse()", "Tensor.cholesky_solve", "See torch.cholesky_solve()", "Tensor.chunk", "See torch.chunk()", "Tensor.clamp", "See torch.clamp()", "Tensor.clamp_", "In-place version of clamp()", "Tensor.clip", "Alias for clamp().", "Tensor.clip_", "Alias for clamp_().", "Tensor.clone", "See torch.clone()", "Tensor.contiguous", "Returns a contiguous in memory tensor containing the same data as self tensor.", "Tensor.copy_", "Copies the elements from src into self tensor and returns self.", "Tensor.conj", "See torch.conj()", "Tensor.conj_physical", "See torch.conj_physical()", "Tensor.conj_physical_", "In-place version of conj_physical()", "Tensor.resolve_conj", "See torch.resolve_conj()", "Tensor.resolve_neg", "See torch.resolve_neg()", "Tensor.copysign", "See torch.copysign()", "Tensor.copysign_", "In-place version of copysign()", "Tensor.cos", "See torch.cos()", "Tensor.cos_", "In-place version of cos()", "Tensor.cosh", "See torch.cosh()", "Tensor.cosh_", "In-place version of cosh()", "Tensor.corrcoef", "See torch.corrcoef()", "Tensor.count_nonzero", "See torch.count_nonzero()", "Tensor.cov", "See torch.cov()", "Tensor.acosh", "See torch.acosh()", "Tensor.acosh_", "In-place version of acosh()", "Tensor.arccosh", "acosh() -> Tensor", "Tensor.arccosh_", "acosh_() -> Tensor", "Tensor.cpu", "Returns a copy of this object in CPU memory.", "Tensor.cross", "See torch.cross()", "Tensor.cuda", "Returns a copy of this object in CUDA memory.", "Tensor.logcumsumexp", "See torch.logcumsumexp()", "Tensor.cummax", "See torch.cummax()", "Tensor.cummin", "See torch.cummin()", "Tensor.cumprod", "See torch.cumprod()", "Tensor.cumprod_", "In-place version of cumprod()", "Tensor.cumsum", "See torch.cumsum()", "Tensor.cumsum_", "In-place version of cumsum()", "Tensor.chalf", "self.chalf() is equivalent to self.to(torch.complex32).", "Tensor.cfloat", "self.cfloat() is equivalent to self.to(torch.complex64).", "Tensor.cdouble", "self.cdouble() is equivalent to self.to(torch.complex128).", "Tensor.data_ptr", "Returns the address of the first element of self tensor.", "Tensor.deg2rad", "See torch.deg2rad()", "Tensor.dequantize", "Given a quantized Tensor, dequantize it and return the dequantized float Tensor.", "Tensor.det", "See torch.det()", "Tensor.dense_dim", "Return the number of dense dimensions in a sparse tensor self.", "Tensor.detach", "Returns a new Tensor, detached from the current graph.", "Tensor.detach_", "Detaches the Tensor from the graph that created it, making it a leaf.", "Tensor.diag", "See torch.diag()", "Tensor.diag_embed", "See torch.diag_embed()", "Tensor.diagflat", "See torch.diagflat()", "Tensor.diagonal", "See torch.diagonal()", "Tensor.diagonal_scatter", "See torch.diagonal_scatter()", "Tensor.fill_diagonal_", "Fill the main diagonal of a tensor that has at least 2-dimensions.", "Tensor.fmax", "See torch.fmax()", "Tensor.fmin", "See torch.fmin()", "Tensor.diff", "See torch.diff()", "Tensor.digamma", "See torch.digamma()", "Tensor.digamma_", "In-place version of digamma()", "Tensor.dim", "Returns the number of dimensions of self tensor.", "Tensor.dim_order", "Returns a tuple of int describing the dim order or physical layout of self.", "Tensor.dist", "See torch.dist()", "Tensor.div", "See torch.div()", "Tensor.div_", "In-place version of div()", "Tensor.divide", "See torch.divide()", "Tensor.divide_", "In-place version of divide()", "Tensor.dot", "See torch.dot()", "Tensor.double", "self.double() is equivalent to self.to(torch.float64).", "Tensor.dsplit", "See torch.dsplit()", "Tensor.element_size", "Returns the size in bytes of an individual element.", "Tensor.eq", "See torch.eq()", "Tensor.eq_", "In-place version of eq()", "Tensor.equal", "See torch.equal()", "Tensor.erf", "See torch.erf()", "Tensor.erf_", "In-place version of erf()", "Tensor.erfc", "See torch.erfc()", "Tensor.erfc_", "In-place version of erfc()", "Tensor.erfinv", "See torch.erfinv()", "Tensor.erfinv_", "In-place version of erfinv()", "Tensor.exp", "See torch.exp()", "Tensor.exp_", "In-place version of exp()", "Tensor.expm1", "See torch.expm1()", "Tensor.expm1_", "In-place version of expm1()", "Tensor.expand", "Returns a new view of the self tensor with singleton dimensions expanded to a larger size.", "Tensor.expand_as", "Expand this tensor to the same size as other.", "Tensor.exponential_", "Fills self tensor with elements drawn from the exponential distribution:", "Tensor.fix", "See torch.fix().", "Tensor.fix_", "In-place version of fix()", "Tensor.fill_", "Fills self tensor with the specified value.", "Tensor.flatten", "See torch.flatten()", "Tensor.flip", "See torch.flip()", "Tensor.fliplr", "See torch.fliplr()", "Tensor.flipud", "See torch.flipud()", "Tensor.float", "self.float() is equivalent to self.to(torch.float32).", "Tensor.float_power", "See torch.float_power()", "Tensor.float_power_", "In-place version of float_power()", "Tensor.floor", "See torch.floor()", "Tensor.floor_", "In-place version of floor()", "Tensor.floor_divide", "See torch.floor_divide()", "Tensor.floor_divide_", "In-place version of floor_divide()", "Tensor.fmod", "See torch.fmod()", "Tensor.fmod_", "In-place version of fmod()", "Tensor.frac", "See torch.frac()", "Tensor.frac_", "In-place version of frac()", "Tensor.frexp", "See torch.frexp()", "Tensor.gather", "See torch.gather()", "Tensor.gcd", "See torch.gcd()", "Tensor.gcd_", "In-place version of gcd()", "Tensor.ge", "See torch.ge().", "Tensor.ge_", "In-place version of ge().", "Tensor.greater_equal", "See torch.greater_equal().", "Tensor.greater_equal_", "In-place version of greater_equal().", "Tensor.geometric_", "Fills self tensor with elements drawn from the geometric distribution:", "Tensor.geqrf", "See torch.geqrf()", "Tensor.ger", "See torch.ger()", "Tensor.get_device", "For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.", "Tensor.gt", "See torch.gt().", "Tensor.gt_", "In-place version of gt().", "Tensor.greater", "See torch.greater().", "Tensor.greater_", "In-place version of greater().", "Tensor.half", "self.half() is equivalent to self.to(torch.float16).", "Tensor.hardshrink", "See torch.nn.functional.hardshrink()", "Tensor.heaviside", "See torch.heaviside()", "Tensor.histc", "See torch.histc()", "Tensor.histogram", "See torch.histogram()", "Tensor.hsplit", "See torch.hsplit()", "Tensor.hypot", "See torch.hypot()", "Tensor.hypot_", "In-place version of hypot()", "Tensor.i0", "See torch.i0()", "Tensor.i0_", "In-place version of i0()", "Tensor.igamma", "See torch.igamma()", "Tensor.igamma_", "In-place version of igamma()", "Tensor.igammac", "See torch.igammac()", "Tensor.igammac_", "In-place version of igammac()", "Tensor.index_add_", "Accumulate the elements of alpha times source into the self tensor by adding to the indices in the order given in index.", "Tensor.index_add", "Out-of-place version of torch.Tensor.index_add_().", "Tensor.index_copy_", "Copies the elements of tensor into the self tensor by selecting the indices in the order given in index.", "Tensor.index_copy", "Out-of-place version of torch.Tensor.index_copy_().", "Tensor.index_fill_", "Fills the elements of the self tensor with value value by selecting the indices in the order given in index.", "Tensor.index_fill", "Out-of-place version of torch.Tensor.index_fill_().", "Tensor.index_put_", "Puts values from the tensor values into the tensor self using the indices specified in indices (which is a tuple of Tensors).", "Tensor.index_put", "Out-place version of index_put_().", "Tensor.index_reduce_", "Accumulate the elements of source into the self tensor by accumulating to the indices in the order given in index using the reduction given by the reduce argument.", "Tensor.index_reduce", "Tensor.index_select", "See torch.index_select()", "Tensor.indices", "Return the indices tensor of a sparse COO tensor.", "Tensor.inner", "See torch.inner().", "Tensor.int", "self.int() is equivalent to self.to(torch.int32).", "Tensor.int_repr", "Given a quantized Tensor, self.int_repr() returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.", "Tensor.inverse", "See torch.inverse()", "Tensor.isclose", "See torch.isclose()", "Tensor.isfinite", "See torch.isfinite()", "Tensor.isinf", "See torch.isinf()", "Tensor.isposinf", "See torch.isposinf()", "Tensor.isneginf", "See torch.isneginf()", "Tensor.isnan", "See torch.isnan()", "Tensor.is_contiguous", "Returns True if self tensor is contiguous in memory in the order specified by memory format.", "Tensor.is_complex", "Returns True if the data type of self is a complex data type.", "Tensor.is_conj", "Returns True if the conjugate bit of self is set to true.", "Tensor.is_floating_point", "Returns True if the data type of self is a floating point data type.", "Tensor.is_inference", "See torch.is_inference()", "Tensor.is_leaf", "All Tensors that have requires_grad which is False will be leaf Tensors by convention.", "Tensor.is_pinned", "Returns true if this tensor resides in pinned memory.", "Tensor.is_set_to", "Returns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride).", "Tensor.is_shared", "Checks if tensor is in shared memory.", "Tensor.is_signed", "Returns True if the data type of self is a signed data type.", "Tensor.is_sparse", "Is True if the Tensor uses sparse COO storage layout, False otherwise.", "Tensor.istft", "See torch.istft()", "Tensor.isreal", "See torch.isreal()", "Tensor.item", "Returns the value of this tensor as a standard Python number.", "Tensor.kthvalue", "See torch.kthvalue()", "Tensor.lcm", "See torch.lcm()", "Tensor.lcm_", "In-place version of lcm()", "Tensor.ldexp", "See torch.ldexp()", "Tensor.ldexp_", "In-place version of ldexp()", "Tensor.le", "See torch.le().", "Tensor.le_", "In-place version of le().", "Tensor.less_equal", "See torch.less_equal().", "Tensor.less_equal_", "In-place version of less_equal().", "Tensor.lerp", "See torch.lerp()", "Tensor.lerp_", "In-place version of lerp()", "Tensor.lgamma", "See torch.lgamma()", "Tensor.lgamma_", "In-place version of lgamma()", "Tensor.log", "See torch.log()", "Tensor.log_", "In-place version of log()", "Tensor.logdet", "See torch.logdet()", "Tensor.log10", "See torch.log10()", "Tensor.log10_", "In-place version of log10()", "Tensor.log1p", "See torch.log1p()", "Tensor.log1p_", "In-place version of log1p()", "Tensor.log2", "See torch.log2()", "Tensor.log2_", "In-place version of log2()", "Tensor.log_normal_", "Fills self tensor with numbers samples from the log-normal distribution parameterized by the given mean \u03bc\\mu and standard deviation \u03c3\\sigma.", "Tensor.logaddexp", "See torch.logaddexp()", "Tensor.logaddexp2", "See torch.logaddexp2()", "Tensor.logsumexp", "See torch.logsumexp()", "Tensor.logical_and", "See torch.logical_and()", "Tensor.logical_and_", "In-place version of logical_and()", "Tensor.logical_not", "See torch.logical_not()", "Tensor.logical_not_", "In-place version of logical_not()", "Tensor.logical_or", "See torch.logical_or()", "Tensor.logical_or_", "In-place version of logical_or()", "Tensor.logical_xor", "See torch.logical_xor()", "Tensor.logical_xor_", "In-place version of logical_xor()", "Tensor.logit", "See torch.logit()", "Tensor.logit_", "In-place version of logit()", "Tensor.long", "self.long() is equivalent to self.to(torch.int64).", "Tensor.lt", "See torch.lt().", "Tensor.lt_", "In-place version of lt().", "Tensor.less", "lt(other) -> Tensor", "Tensor.less_", "In-place version of less().", "Tensor.lu", "See torch.lu()", "Tensor.lu_solve", "See torch.lu_solve()", "Tensor.as_subclass", "Makes a cls instance with the same data pointer as self.", "Tensor.map_", "Applies callable for each element in self tensor and the given tensor and stores the results in self tensor.", "Tensor.masked_scatter_", "Copies elements from source into self tensor at positions where the mask is True.", "Tensor.masked_scatter", "Out-of-place version of torch.Tensor.masked_scatter_()", "Tensor.masked_fill_", "Fills elements of self tensor with value where mask is True.", "Tensor.masked_fill", "Out-of-place version of torch.Tensor.masked_fill_()", "Tensor.masked_select", "See torch.masked_select()", "Tensor.matmul", "See torch.matmul()", "Tensor.matrix_power", "Note", "matrix_power() is deprecated, use torch.linalg.matrix_power() instead.", "Tensor.matrix_exp", "See torch.matrix_exp()", "Tensor.max", "See torch.max()", "Tensor.maximum", "See torch.maximum()", "Tensor.mean", "See torch.mean()", "Tensor.nanmean", "See torch.nanmean()", "Tensor.median", "See torch.median()", "Tensor.nanmedian", "See torch.nanmedian()", "Tensor.min", "See torch.min()", "Tensor.minimum", "See torch.minimum()", "Tensor.mm", "See torch.mm()", "Tensor.smm", "See torch.smm()", "Tensor.mode", "See torch.mode()", "Tensor.movedim", "See torch.movedim()", "Tensor.moveaxis", "See torch.moveaxis()", "Tensor.msort", "See torch.msort()", "Tensor.mul", "See torch.mul().", "Tensor.mul_", "In-place version of mul().", "Tensor.multiply", "See torch.multiply().", "Tensor.multiply_", "In-place version of multiply().", "Tensor.multinomial", "See torch.multinomial()", "Tensor.mv", "See torch.mv()", "Tensor.mvlgamma", "See torch.mvlgamma()", "Tensor.mvlgamma_", "In-place version of mvlgamma()", "Tensor.nansum", "See torch.nansum()", "Tensor.narrow", "See torch.narrow().", "Tensor.narrow_copy", "See torch.narrow_copy().", "Tensor.ndimension", "Alias for dim()", "Tensor.nan_to_num", "See torch.nan_to_num().", "Tensor.nan_to_num_", "In-place version of nan_to_num().", "Tensor.ne", "See torch.ne().", "Tensor.ne_", "In-place version of ne().", "Tensor.not_equal", "See torch.not_equal().", "Tensor.not_equal_", "In-place version of not_equal().", "Tensor.neg", "See torch.neg()", "Tensor.neg_", "In-place version of neg()", "Tensor.negative", "See torch.negative()", "Tensor.negative_", "In-place version of negative()", "Tensor.nelement", "Alias for numel()", "Tensor.nextafter", "See torch.nextafter()", "Tensor.nextafter_", "In-place version of nextafter()", "Tensor.nonzero", "See torch.nonzero()", "Tensor.norm", "See torch.norm()", "Tensor.normal_", "Fills self tensor with elements samples from the normal distribution parameterized by mean and std.", "Tensor.numel", "See torch.numel()", "Tensor.numpy", "Returns the tensor as a NumPy ndarray.", "Tensor.orgqr", "See torch.orgqr()", "Tensor.ormqr", "See torch.ormqr()", "Tensor.outer", "See torch.outer().", "Tensor.permute", "See torch.permute()", "Tensor.pin_memory", "Copies the tensor to pinned memory, if it's not already pinned.", "Tensor.pinverse", "See torch.pinverse()", "Tensor.polygamma", "See torch.polygamma()", "Tensor.polygamma_", "In-place version of polygamma()", "Tensor.positive", "See torch.positive()", "Tensor.pow", "See torch.pow()", "Tensor.pow_", "In-place version of pow()", "Tensor.prod", "See torch.prod()", "Tensor.put_", "Copies the elements from source into the positions specified by index.", "Tensor.qr", "See torch.qr()", "Tensor.qscheme", "Returns the quantization scheme of a given QTensor.", "Tensor.quantile", "See torch.quantile()", "Tensor.nanquantile", "See torch.nanquantile()", "Tensor.q_scale", "Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().", "Tensor.q_zero_point", "Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().", "Tensor.q_per_channel_scales", "Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer.", "Tensor.q_per_channel_zero_points", "Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer.", "Tensor.q_per_channel_axis", "Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.", "Tensor.rad2deg", "See torch.rad2deg()", "Tensor.random_", "Fills self tensor with numbers sampled from the discrete uniform distribution over [from, to - 1].", "Tensor.ravel", "see torch.ravel()", "Tensor.reciprocal", "See torch.reciprocal()", "Tensor.reciprocal_", "In-place version of reciprocal()", "Tensor.record_stream", "Ensures that the tensor memory is not reused for another tensor until all current work queued on stream are complete.", "Tensor.register_hook", "Registers a backward hook.", "Tensor.register_post_accumulate_grad_hook", "Registers a backward hook that runs after grad accumulation.", "Tensor.remainder", "See torch.remainder()", "Tensor.remainder_", "In-place version of remainder()", "Tensor.renorm", "See torch.renorm()", "Tensor.renorm_", "In-place version of renorm()", "Tensor.repeat", "Repeats this tensor along the specified dimensions.", "Tensor.repeat_interleave", "See torch.repeat_interleave().", "Tensor.requires_grad", "Is True if gradients need to be computed for this Tensor, False otherwise.", "Tensor.requires_grad_", "Change if autograd should record operations on this tensor: sets this tensor's requires_grad attribute in-place.", "Tensor.reshape", "Returns a tensor with the same data and number of elements as self but with the specified shape.", "Tensor.reshape_as", "Returns this tensor as the same shape as other.", "Tensor.resize_", "Resizes self tensor to the specified size.", "Tensor.resize_as_", "Resizes the self tensor to be the same size as the specified tensor.", "Tensor.retain_grad", "Enables this Tensor to have their grad populated during backward().", "Tensor.retains_grad", "Is True if this Tensor is non-leaf and its grad is enabled to be populated during backward(), False otherwise.", "Tensor.roll", "See torch.roll()", "Tensor.rot90", "See torch.rot90()", "Tensor.round", "See torch.round()", "Tensor.round_", "In-place version of round()", "Tensor.rsqrt", "See torch.rsqrt()", "Tensor.rsqrt_", "In-place version of rsqrt()", "Tensor.scatter", "Out-of-place version of torch.Tensor.scatter_()", "Tensor.scatter_", "Writes all values from the tensor src into self at the indices specified in the index tensor.", "Tensor.scatter_add_", "Adds all values from the tensor src into self at the indices specified in the index tensor in a similar fashion as scatter_().", "Tensor.scatter_add", "Out-of-place version of torch.Tensor.scatter_add_()", "Tensor.scatter_reduce_", "Reduces all values from the src tensor to the indices specified in the index tensor in the self tensor using the applied reduction defined via the reduce argument (\"sum\", \"prod\", \"mean\", \"amax\", \"amin\").", "Tensor.scatter_reduce", "Out-of-place version of torch.Tensor.scatter_reduce_()", "Tensor.select", "See torch.select()", "Tensor.select_scatter", "See torch.select_scatter()", "Tensor.set_", "Sets the underlying storage, size, and strides.", "Tensor.share_memory_", "Moves the underlying storage to shared memory.", "Tensor.short", "self.short() is equivalent to self.to(torch.int16).", "Tensor.sigmoid", "See torch.sigmoid()", "Tensor.sigmoid_", "In-place version of sigmoid()", "Tensor.sign", "See torch.sign()", "Tensor.sign_", "In-place version of sign()", "Tensor.signbit", "See torch.signbit()", "Tensor.sgn", "See torch.sgn()", "Tensor.sgn_", "In-place version of sgn()", "Tensor.sin", "See torch.sin()", "Tensor.sin_", "In-place version of sin()", "Tensor.sinc", "See torch.sinc()", "Tensor.sinc_", "In-place version of sinc()", "Tensor.sinh", "See torch.sinh()", "Tensor.sinh_", "In-place version of sinh()", "Tensor.asinh", "See torch.asinh()", "Tensor.asinh_", "In-place version of asinh()", "Tensor.arcsinh", "See torch.arcsinh()", "Tensor.arcsinh_", "In-place version of arcsinh()", "Tensor.shape", "Returns the size of the self tensor.", "Tensor.size", "Returns the size of the self tensor.", "Tensor.slogdet", "See torch.slogdet()", "Tensor.slice_scatter", "See torch.slice_scatter()", "Tensor.softmax", "Alias for torch.nn.functional.softmax().", "Tensor.sort", "See torch.sort()", "Tensor.split", "See torch.split()", "Tensor.sparse_mask", "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask.", "Tensor.sparse_dim", "Return the number of sparse dimensions in a sparse tensor self.", "Tensor.sqrt", "See torch.sqrt()", "Tensor.sqrt_", "In-place version of sqrt()", "Tensor.square", "See torch.square()", "Tensor.square_", "In-place version of square()", "Tensor.squeeze", "See torch.squeeze()", "Tensor.squeeze_", "In-place version of squeeze()", "Tensor.std", "See torch.std()", "Tensor.stft", "See torch.stft()", "Tensor.storage", "Returns the underlying TypedStorage.", "Tensor.untyped_storage", "Returns the underlying UntypedStorage.", "Tensor.storage_offset", "Returns self tensor's offset in the underlying storage in terms of number of storage elements (not bytes).", "Tensor.storage_type", "Returns the type of the underlying storage.", "Tensor.stride", "Returns the stride of self tensor.", "Tensor.sub", "See torch.sub().", "Tensor.sub_", "In-place version of sub()", "Tensor.subtract", "See torch.subtract().", "Tensor.subtract_", "In-place version of subtract().", "Tensor.sum", "See torch.sum()", "Tensor.sum_to_size", "Sum this tensor to size.", "Tensor.svd", "See torch.svd()", "Tensor.swapaxes", "See torch.swapaxes()", "Tensor.swapdims", "See torch.swapdims()", "Tensor.t", "See torch.t()", "Tensor.t_", "In-place version of t()", "Tensor.tensor_split", "See torch.tensor_split()", "Tensor.tile", "See torch.tile()", "Tensor.to", "Performs Tensor dtype and/or device conversion.", "Tensor.to_mkldnn", "Returns a copy of the tensor in torch.mkldnn layout.", "Tensor.take", "See torch.take()", "Tensor.take_along_dim", "See torch.take_along_dim()", "Tensor.tan", "See torch.tan()", "Tensor.tan_", "In-place version of tan()", "Tensor.tanh", "See torch.tanh()", "Tensor.tanh_", "In-place version of tanh()", "Tensor.atanh", "See torch.atanh()", "Tensor.atanh_", "In-place version of atanh()", "Tensor.arctanh", "See torch.arctanh()", "Tensor.arctanh_", "In-place version of arctanh()", "Tensor.tolist", "Returns the tensor as a (nested) list.", "Tensor.topk", "See torch.topk()", "Tensor.to_dense", "Creates a strided copy of self if self is not a strided tensor, otherwise returns self.", "Tensor.to_sparse", "Returns a sparse copy of the tensor.", "Tensor.to_sparse_csr", "Convert a tensor to compressed row storage format (CSR).", "Tensor.to_sparse_csc", "Convert a tensor to compressed column storage (CSC) format.", "Tensor.to_sparse_bsr", "Convert a tensor to a block sparse row (BSR) storage format of given blocksize.", "Tensor.to_sparse_bsc", "Convert a tensor to a block sparse column (BSC) storage format of given blocksize.", "Tensor.trace", "See torch.trace()", "Tensor.transpose", "See torch.transpose()", "Tensor.transpose_", "In-place version of transpose()", "Tensor.triangular_solve", "See torch.triangular_solve()", "Tensor.tril", "See torch.tril()", "Tensor.tril_", "In-place version of tril()", "Tensor.triu", "See torch.triu()", "Tensor.triu_", "In-place version of triu()", "Tensor.true_divide", "See torch.true_divide()", "Tensor.true_divide_", "In-place version of true_divide_()", "Tensor.trunc", "See torch.trunc()", "Tensor.trunc_", "In-place version of trunc()", "Tensor.type", "Returns the type if dtype is not provided, else casts this object to the specified type.", "Tensor.type_as", "Returns this tensor cast to the type of the given tensor.", "Tensor.unbind", "See torch.unbind()", "Tensor.unflatten", "See torch.unflatten().", "Tensor.unfold", "Returns a view of the original tensor which contains all slices of size size from self tensor in the dimension dimension.", "Tensor.uniform_", "Fills self tensor with numbers sampled from the continuous uniform distribution:", "Tensor.unique", "Returns the unique elements of the input tensor.", "Tensor.unique_consecutive", "Eliminates all but the first element from every consecutive group of equivalent elements.", "Tensor.unsqueeze", "See torch.unsqueeze()", "Tensor.unsqueeze_", "In-place version of unsqueeze()", "Tensor.values", "Return the values tensor of a sparse COO tensor.", "Tensor.var", "See torch.var()", "Tensor.vdot", "See torch.vdot()", "Tensor.view", "Returns a new tensor with the same data as the self tensor but of a different shape.", "Tensor.view_as", "View this tensor as the same size as other.", "Tensor.vsplit", "See torch.vsplit()", "Tensor.where", "self.where(condition, y) is equivalent to torch.where(condition, self, y).", "Tensor.xlogy", "See torch.xlogy()", "Tensor.xlogy_", "In-place version of xlogy()", "Tensor.zero_", "Fills self tensor with zeros."]}, {"name": "torch.tensor", "path": "generated/torch.tensor", "type": "Torch", "text": ["Constructs a tensor with no autograd history (also known as a \u201cleaf tensor\u201d, see Autograd mechanics) by copying data.", "Warning", "When working with tensors prefer using torch.Tensor.clone(), torch.Tensor.detach(), and torch.Tensor.requires_grad_() for readability. Letting t be a tensor, torch.tensor(t) is equivalent to t.clone().detach(), and torch.tensor(t, requires_grad=True) is equivalent to t.clone().detach().requires_grad_(True).", "See also", "torch.as_tensor() preserves autograd history and avoids copies where possible. torch.from_numpy() creates a tensor that shares storage with a NumPy array.", "data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple, NumPy ndarray, scalar, and other types.", "Example:"]}, {"name": "torch.Tensor.abs()", "path": "generated/torch.tensor.abs#torch.Tensor.abs", "type": "Tensor", "text": ["See torch.abs()"]}, {"name": "torch.Tensor.abs_()", "path": "generated/torch.tensor.abs_#torch.Tensor.abs_", "type": "Tensor", "text": ["In-place version of abs()"]}, {"name": "torch.Tensor.absolute()", "path": "generated/torch.tensor.absolute#torch.Tensor.absolute", "type": "Tensor", "text": ["Alias for abs()"]}, {"name": "torch.Tensor.absolute_()", "path": "generated/torch.tensor.absolute_#torch.Tensor.absolute_", "type": "Tensor", "text": ["In-place version of absolute() Alias for abs_()"]}, {"name": "torch.Tensor.acos()", "path": "generated/torch.tensor.acos#torch.Tensor.acos", "type": "Tensor", "text": ["See torch.acos()"]}, {"name": "torch.Tensor.acos_()", "path": "generated/torch.tensor.acos_#torch.Tensor.acos_", "type": "Tensor", "text": ["In-place version of acos()"]}, {"name": "torch.Tensor.acosh()", "path": "generated/torch.tensor.acosh#torch.Tensor.acosh", "type": "Tensor", "text": ["See torch.acosh()"]}, {"name": "torch.Tensor.acosh_()", "path": "generated/torch.tensor.acosh_#torch.Tensor.acosh_", "type": "Tensor", "text": ["In-place version of acosh()"]}, {"name": "torch.Tensor.add()", "path": "generated/torch.tensor.add#torch.Tensor.add", "type": "Tensor", "text": ["Add a scalar or tensor to self tensor. If both alpha and other are specified, each element of other is scaled by alpha before being used.", "When other is a tensor, the shape of other must be broadcastable with the shape of the underlying tensor", "See torch.add()"]}, {"name": "torch.Tensor.add_()", "path": "generated/torch.tensor.add_#torch.Tensor.add_", "type": "Tensor", "text": ["In-place version of add()"]}, {"name": "torch.Tensor.addbmm()", "path": "generated/torch.tensor.addbmm#torch.Tensor.addbmm", "type": "Tensor", "text": ["See torch.addbmm()"]}, {"name": "torch.Tensor.addbmm_()", "path": "generated/torch.tensor.addbmm_#torch.Tensor.addbmm_", "type": "Tensor", "text": ["In-place version of addbmm()"]}, {"name": "torch.Tensor.addcdiv()", "path": "generated/torch.tensor.addcdiv#torch.Tensor.addcdiv", "type": "Tensor", "text": ["See torch.addcdiv()"]}, {"name": "torch.Tensor.addcdiv_()", "path": "generated/torch.tensor.addcdiv_#torch.Tensor.addcdiv_", "type": "Tensor", "text": ["In-place version of addcdiv()"]}, {"name": "torch.Tensor.addcmul()", "path": "generated/torch.tensor.addcmul#torch.Tensor.addcmul", "type": "Tensor", "text": ["See torch.addcmul()"]}, {"name": "torch.Tensor.addcmul_()", "path": "generated/torch.tensor.addcmul_#torch.Tensor.addcmul_", "type": "Tensor", "text": ["In-place version of addcmul()"]}, {"name": "torch.Tensor.addmm()", "path": "generated/torch.tensor.addmm#torch.Tensor.addmm", "type": "Tensor", "text": ["See torch.addmm()"]}, {"name": "torch.Tensor.addmm_()", "path": "generated/torch.tensor.addmm_#torch.Tensor.addmm_", "type": "Tensor", "text": ["In-place version of addmm()"]}, {"name": "torch.Tensor.addmv()", "path": "generated/torch.tensor.addmv#torch.Tensor.addmv", "type": "Tensor", "text": ["See torch.addmv()"]}, {"name": "torch.Tensor.addmv_()", "path": "generated/torch.tensor.addmv_#torch.Tensor.addmv_", "type": "Tensor", "text": ["In-place version of addmv()"]}, {"name": "torch.Tensor.addr()", "path": "generated/torch.tensor.addr#torch.Tensor.addr", "type": "Tensor", "text": ["See torch.addr()"]}, {"name": "torch.Tensor.addr_()", "path": "generated/torch.tensor.addr_#torch.Tensor.addr_", "type": "Tensor", "text": ["In-place version of addr()"]}, {"name": "torch.Tensor.adjoint()", "path": "generated/torch.tensor.adjoint#torch.Tensor.adjoint", "type": "Tensor", "text": ["Alias for adjoint()"]}, {"name": "torch.Tensor.align_as()", "path": "named_tensor#torch.Tensor.align_as", "type": "Miscellaneous", "text": ["Permutes the dimensions of the self tensor to match the dimension order in the other tensor, adding size-one dims for any new names.", "This operation is useful for explicit broadcasting by names (see examples).", "All of the dims of self must be named in order to use this method. The resulting tensor is a view on the original tensor.", "All dimension names of self must be present in other.names. other may contain named dimensions that are not in self.names; the output tensor has a size-one dimension for each of those new names.", "To align a tensor to a specific order, use align_to().", "Examples:", "Warning", "The named tensor API is experimental and subject to change."]}, {"name": "torch.Tensor.align_to()", "path": "named_tensor#torch.Tensor.align_to", "type": "Miscellaneous", "text": ["Permutes the dimensions of the self tensor to match the order specified in names, adding size-one dims for any new names.", "All of the dims of self must be named in order to use this method. The resulting tensor is a view on the original tensor.", "All dimension names of self must be present in names. names may contain additional names that are not in self.names; the output tensor has a size-one dimension for each of those new names.", "names may contain up to one Ellipsis (...). The Ellipsis is expanded to be equal to all dimension names of self that are not mentioned in names, in the order that they appear in self.", "Python 2 does not support Ellipsis but one may use a string literal instead ('...').", "names (iterable of str) \u2013 The desired dimension ordering of the output tensor. May contain up to one Ellipsis that is expanded to all unmentioned dim names of self.", "Examples:", "Warning", "The named tensor API is experimental and subject to change."]}, {"name": "torch.Tensor.all()", "path": "generated/torch.tensor.all#torch.Tensor.all", "type": "Tensor", "text": ["See torch.all()"]}, {"name": "torch.Tensor.allclose()", "path": "generated/torch.tensor.allclose#torch.Tensor.allclose", "type": "Tensor", "text": ["See torch.allclose()"]}, {"name": "torch.Tensor.amax()", "path": "generated/torch.tensor.amax#torch.Tensor.amax", "type": "Tensor", "text": ["See torch.amax()"]}, {"name": "torch.Tensor.amin()", "path": "generated/torch.tensor.amin#torch.Tensor.amin", "type": "Tensor", "text": ["See torch.amin()"]}, {"name": "torch.Tensor.aminmax()", "path": "generated/torch.tensor.aminmax#torch.Tensor.aminmax", "type": "Tensor", "text": ["See torch.aminmax()"]}, {"name": "torch.Tensor.angle()", "path": "generated/torch.tensor.angle#torch.Tensor.angle", "type": "Tensor", "text": ["See torch.angle()"]}, {"name": "torch.Tensor.any()", "path": "generated/torch.tensor.any#torch.Tensor.any", "type": "Tensor", "text": ["See torch.any()"]}, {"name": "torch.Tensor.apply_()", "path": "generated/torch.tensor.apply_#torch.Tensor.apply_", "type": "Tensor", "text": ["Applies the function callable to each element in the tensor, replacing each element with the value returned by callable.", "Note", "This function only works with CPU tensors and should not be used in code sections that require high performance."]}, {"name": "torch.Tensor.arccos()", "path": "generated/torch.tensor.arccos#torch.Tensor.arccos", "type": "Tensor", "text": ["See torch.arccos()"]}, {"name": "torch.Tensor.arccos_()", "path": "generated/torch.tensor.arccos_#torch.Tensor.arccos_", "type": "Tensor", "text": ["In-place version of arccos()"]}, {"name": "torch.Tensor.arccosh()", "path": "generated/torch.tensor.arccosh#torch.Tensor.arccosh", "type": "Tensor", "text": ["acosh() -> Tensor", "See torch.arccosh()"]}, {"name": "torch.Tensor.arccosh_()", "path": "generated/torch.tensor.arccosh_#torch.Tensor.arccosh_", "type": "Tensor", "text": ["acosh_() -> Tensor", "In-place version of arccosh()"]}, {"name": "torch.Tensor.arcsin()", "path": "generated/torch.tensor.arcsin#torch.Tensor.arcsin", "type": "Tensor", "text": ["See torch.arcsin()"]}, {"name": "torch.Tensor.arcsin_()", "path": "generated/torch.tensor.arcsin_#torch.Tensor.arcsin_", "type": "Tensor", "text": ["In-place version of arcsin()"]}, {"name": "torch.Tensor.arcsinh()", "path": "generated/torch.tensor.arcsinh#torch.Tensor.arcsinh", "type": "Tensor", "text": ["See torch.arcsinh()"]}, {"name": "torch.Tensor.arcsinh_()", "path": "generated/torch.tensor.arcsinh_#torch.Tensor.arcsinh_", "type": "Tensor", "text": ["In-place version of arcsinh()"]}, {"name": "torch.Tensor.arctan()", "path": "generated/torch.tensor.arctan#torch.Tensor.arctan", "type": "Tensor", "text": ["See torch.arctan()"]}, {"name": "torch.Tensor.arctan2()", "path": "generated/torch.tensor.arctan2#torch.Tensor.arctan2", "type": "Tensor", "text": ["See torch.arctan2()"]}, {"name": "torch.Tensor.arctan2_()", "path": "generated/torch.tensor.arctan2_#torch.Tensor.arctan2_", "type": "Tensor", "text": ["atan2_(other) -> Tensor", "In-place version of arctan2()"]}, {"name": "torch.Tensor.arctan_()", "path": "generated/torch.tensor.arctan_#torch.Tensor.arctan_", "type": "Tensor", "text": ["In-place version of arctan()"]}, {"name": "torch.Tensor.arctanh()", "path": "generated/torch.tensor.arctanh#torch.Tensor.arctanh", "type": "Tensor", "text": ["See torch.arctanh()"]}, {"name": "torch.Tensor.arctanh_()", "path": "generated/torch.tensor.arctanh_#torch.Tensor.arctanh_", "type": "Tensor", "text": ["In-place version of arctanh()"]}, {"name": "torch.Tensor.argmax()", "path": "generated/torch.tensor.argmax#torch.Tensor.argmax", "type": "Tensor", "text": ["See torch.argmax()"]}, {"name": "torch.Tensor.argmin()", "path": "generated/torch.tensor.argmin#torch.Tensor.argmin", "type": "Tensor", "text": ["See torch.argmin()"]}, {"name": "torch.Tensor.argsort()", "path": "generated/torch.tensor.argsort#torch.Tensor.argsort", "type": "Tensor", "text": ["See torch.argsort()"]}, {"name": "torch.Tensor.argwhere()", "path": "generated/torch.tensor.argwhere#torch.Tensor.argwhere", "type": "Tensor", "text": ["See torch.argwhere()"]}, {"name": "torch.Tensor.as_strided()", "path": "generated/torch.tensor.as_strided#torch.Tensor.as_strided", "type": "Tensor", "text": ["See torch.as_strided()"]}, {"name": "torch.Tensor.as_subclass()", "path": "generated/torch.tensor.as_subclass#torch.Tensor.as_subclass", "type": "Tensor", "text": ["Makes a cls instance with the same data pointer as self. Changes in the output mirror changes in self, and the output stays attached to the autograd graph. cls must be a subclass of Tensor."]}, {"name": "torch.Tensor.asin()", "path": "generated/torch.tensor.asin#torch.Tensor.asin", "type": "Tensor", "text": ["See torch.asin()"]}, {"name": "torch.Tensor.asin_()", "path": "generated/torch.tensor.asin_#torch.Tensor.asin_", "type": "Tensor", "text": ["In-place version of asin()"]}, {"name": "torch.Tensor.asinh()", "path": "generated/torch.tensor.asinh#torch.Tensor.asinh", "type": "Tensor", "text": ["See torch.asinh()"]}, {"name": "torch.Tensor.asinh_()", "path": "generated/torch.tensor.asinh_#torch.Tensor.asinh_", "type": "Tensor", "text": ["In-place version of asinh()"]}, {"name": "torch.Tensor.atan()", "path": "generated/torch.tensor.atan#torch.Tensor.atan", "type": "Tensor", "text": ["See torch.atan()"]}, {"name": "torch.Tensor.atan2()", "path": "generated/torch.tensor.atan2#torch.Tensor.atan2", "type": "Tensor", "text": ["See torch.atan2()"]}, {"name": "torch.Tensor.atan2_()", "path": "generated/torch.tensor.atan2_#torch.Tensor.atan2_", "type": "Tensor", "text": ["In-place version of atan2()"]}, {"name": "torch.Tensor.atan_()", "path": "generated/torch.tensor.atan_#torch.Tensor.atan_", "type": "Tensor", "text": ["In-place version of atan()"]}, {"name": "torch.Tensor.atanh()", "path": "generated/torch.tensor.atanh#torch.Tensor.atanh", "type": "Tensor", "text": ["See torch.atanh()"]}, {"name": "torch.Tensor.atanh_()", "path": "generated/torch.tensor.atanh_#torch.Tensor.atanh_", "type": "Tensor", "text": ["In-place version of atanh()"]}, {"name": "torch.Tensor.backward()", "path": "generated/torch.tensor.backward#torch.Tensor.backward", "type": "Tensor", "text": ["Computes the gradient of current tensor wrt graph leaves.", "The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying gradient. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. self.", "This function accumulates gradients in the leaves - you might need to zero .grad attributes or set them to None before calling it. See Default gradient layouts for details on the memory layout of accumulated gradients.", "Note", "If you run any forward ops, create gradient, and/or call backward in a user-specified CUDA stream context, see Stream semantics of backward passes.", "Note", "When inputs are provided and a given input is not a leaf, the current implementation will call its grad_fn (though it is not strictly needed to get this gradients). It is an implementation detail on which the user should not rely. See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details."]}, {"name": "torch.Tensor.baddbmm()", "path": "generated/torch.tensor.baddbmm#torch.Tensor.baddbmm", "type": "Tensor", "text": ["See torch.baddbmm()"]}, {"name": "torch.Tensor.baddbmm_()", "path": "generated/torch.tensor.baddbmm_#torch.Tensor.baddbmm_", "type": "Tensor", "text": ["In-place version of baddbmm()"]}, {"name": "torch.Tensor.bernoulli()", "path": "generated/torch.tensor.bernoulli#torch.Tensor.bernoulli", "type": "Tensor", "text": ["Returns a result tensor where each result[i]\\texttt{result[i]} is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]}). self must have floating point dtype, and the result will have the same dtype.", "See torch.bernoulli()"]}, {"name": "torch.Tensor.bernoulli_()", "path": "generated/torch.tensor.bernoulli_#torch.Tensor.bernoulli_", "type": "Tensor", "text": ["Fills each location of self with an independent sample from Bernoulli(p)\\text{Bernoulli}(\\texttt{p}). self can have integral dtype.", "p should either be a scalar or tensor containing probabilities to be used for drawing the binary random number.", "If it is a tensor, the ith\\text{i}^{th} element of self tensor will be set to a value sampled from Bernoulli(p_tensor[i])\\text{Bernoulli}(\\texttt{p\\_tensor[i]}). In this case p must have floating point dtype.", "See also bernoulli() and torch.bernoulli()"]}, {"name": "torch.Tensor.bfloat16()", "path": "generated/torch.tensor.bfloat16#torch.Tensor.bfloat16", "type": "Tensor", "text": ["self.bfloat16() is equivalent to self.to(torch.bfloat16). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.bincount()", "path": "generated/torch.tensor.bincount#torch.Tensor.bincount", "type": "Tensor", "text": ["See torch.bincount()"]}, {"name": "torch.Tensor.bitwise_and()", "path": "generated/torch.tensor.bitwise_and#torch.Tensor.bitwise_and", "type": "Tensor", "text": ["See torch.bitwise_and()"]}, {"name": "torch.Tensor.bitwise_and_()", "path": "generated/torch.tensor.bitwise_and_#torch.Tensor.bitwise_and_", "type": "Tensor", "text": ["In-place version of bitwise_and()"]}, {"name": "torch.Tensor.bitwise_left_shift()", "path": "generated/torch.tensor.bitwise_left_shift#torch.Tensor.bitwise_left_shift", "type": "Tensor", "text": ["See torch.bitwise_left_shift()"]}, {"name": "torch.Tensor.bitwise_left_shift_()", "path": "generated/torch.tensor.bitwise_left_shift_#torch.Tensor.bitwise_left_shift_", "type": "Tensor", "text": ["In-place version of bitwise_left_shift()"]}, {"name": "torch.Tensor.bitwise_not()", "path": "generated/torch.tensor.bitwise_not#torch.Tensor.bitwise_not", "type": "Tensor", "text": ["See torch.bitwise_not()"]}, {"name": "torch.Tensor.bitwise_not_()", "path": "generated/torch.tensor.bitwise_not_#torch.Tensor.bitwise_not_", "type": "Tensor", "text": ["In-place version of bitwise_not()"]}, {"name": "torch.Tensor.bitwise_or()", "path": "generated/torch.tensor.bitwise_or#torch.Tensor.bitwise_or", "type": "Tensor", "text": ["See torch.bitwise_or()"]}, {"name": "torch.Tensor.bitwise_or_()", "path": "generated/torch.tensor.bitwise_or_#torch.Tensor.bitwise_or_", "type": "Tensor", "text": ["In-place version of bitwise_or()"]}, {"name": "torch.Tensor.bitwise_right_shift()", "path": "generated/torch.tensor.bitwise_right_shift#torch.Tensor.bitwise_right_shift", "type": "Tensor", "text": ["See torch.bitwise_right_shift()"]}, {"name": "torch.Tensor.bitwise_right_shift_()", "path": "generated/torch.tensor.bitwise_right_shift_#torch.Tensor.bitwise_right_shift_", "type": "Tensor", "text": ["In-place version of bitwise_right_shift()"]}, {"name": "torch.Tensor.bitwise_xor()", "path": "generated/torch.tensor.bitwise_xor#torch.Tensor.bitwise_xor", "type": "Tensor", "text": ["See torch.bitwise_xor()"]}, {"name": "torch.Tensor.bitwise_xor_()", "path": "generated/torch.tensor.bitwise_xor_#torch.Tensor.bitwise_xor_", "type": "Tensor", "text": ["In-place version of bitwise_xor()"]}, {"name": "torch.Tensor.bmm()", "path": "generated/torch.tensor.bmm#torch.Tensor.bmm", "type": "Tensor", "text": ["See torch.bmm()"]}, {"name": "torch.Tensor.bool()", "path": "generated/torch.tensor.bool#torch.Tensor.bool", "type": "Tensor", "text": ["self.bool() is equivalent to self.to(torch.bool). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.broadcast_to()", "path": "generated/torch.tensor.broadcast_to#torch.Tensor.broadcast_to", "type": "Tensor", "text": ["See torch.broadcast_to()."]}, {"name": "torch.Tensor.byte()", "path": "generated/torch.tensor.byte#torch.Tensor.byte", "type": "Tensor", "text": ["self.byte() is equivalent to self.to(torch.uint8). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.cauchy_()", "path": "generated/torch.tensor.cauchy_#torch.Tensor.cauchy_", "type": "Tensor", "text": ["Fills the tensor with numbers drawn from the Cauchy distribution:"]}, {"name": "torch.Tensor.ccol_indices()", "path": "generated/torch.tensor.ccol_indices#torch.Tensor.ccol_indices", "type": "Sparse Tensors", "text": []}, {"name": "torch.Tensor.cdouble()", "path": "generated/torch.tensor.cdouble#torch.Tensor.cdouble", "type": "Tensor", "text": ["self.cdouble() is equivalent to self.to(torch.complex128). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.ceil()", "path": "generated/torch.tensor.ceil#torch.Tensor.ceil", "type": "Tensor", "text": ["See torch.ceil()"]}, {"name": "torch.Tensor.ceil_()", "path": "generated/torch.tensor.ceil_#torch.Tensor.ceil_", "type": "Tensor", "text": ["In-place version of ceil()"]}, {"name": "torch.Tensor.cfloat()", "path": "generated/torch.tensor.cfloat#torch.Tensor.cfloat", "type": "Tensor", "text": ["self.cfloat() is equivalent to self.to(torch.complex64). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.chalf()", "path": "generated/torch.tensor.chalf#torch.Tensor.chalf", "type": "Tensor", "text": ["self.chalf() is equivalent to self.to(torch.complex32). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.char()", "path": "generated/torch.tensor.char#torch.Tensor.char", "type": "Tensor", "text": ["self.char() is equivalent to self.to(torch.int8). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.cholesky()", "path": "generated/torch.tensor.cholesky#torch.Tensor.cholesky", "type": "Tensor", "text": ["See torch.cholesky()"]}, {"name": "torch.Tensor.cholesky_inverse()", "path": "generated/torch.tensor.cholesky_inverse#torch.Tensor.cholesky_inverse", "type": "Tensor", "text": ["See torch.cholesky_inverse()"]}, {"name": "torch.Tensor.cholesky_solve()", "path": "generated/torch.tensor.cholesky_solve#torch.Tensor.cholesky_solve", "type": "Tensor", "text": ["See torch.cholesky_solve()"]}, {"name": "torch.Tensor.chunk()", "path": "generated/torch.tensor.chunk#torch.Tensor.chunk", "type": "Tensor", "text": ["See torch.chunk()"]}, {"name": "torch.Tensor.clamp()", "path": "generated/torch.tensor.clamp#torch.Tensor.clamp", "type": "Tensor", "text": ["See torch.clamp()"]}, {"name": "torch.Tensor.clamp_()", "path": "generated/torch.tensor.clamp_#torch.Tensor.clamp_", "type": "Tensor", "text": ["In-place version of clamp()"]}, {"name": "torch.Tensor.clip()", "path": "generated/torch.tensor.clip#torch.Tensor.clip", "type": "Tensor", "text": ["Alias for clamp()."]}, {"name": "torch.Tensor.clip_()", "path": "generated/torch.tensor.clip_#torch.Tensor.clip_", "type": "Tensor", "text": ["Alias for clamp_()."]}, {"name": "torch.Tensor.clone()", "path": "generated/torch.tensor.clone#torch.Tensor.clone", "type": "Tensor", "text": ["See torch.clone()"]}, {"name": "torch.Tensor.coalesce()", "path": "generated/torch.tensor.coalesce#torch.Tensor.coalesce", "type": "Sparse Tensors", "text": ["Returns a coalesced copy of self if self is an uncoalesced tensor.", "Returns self if self is a coalesced tensor.", "Warning", "Throws an error if self is not a sparse COO tensor."]}, {"name": "torch.Tensor.col_indices()", "path": "generated/torch.tensor.col_indices#torch.Tensor.col_indices", "type": "Sparse Tensors", "text": ["Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The col_indices tensor is strictly of shape (self.nnz()) and of type int32 or int64. When using MKL routines such as sparse matrix multiplication, it is necessary to use int32 indexing in order to avoid downcasting and potentially losing information."]}, {"name": "torch.Tensor.conj()", "path": "generated/torch.tensor.conj#torch.Tensor.conj", "type": "Tensor", "text": ["See torch.conj()"]}, {"name": "torch.Tensor.conj_physical()", "path": "generated/torch.tensor.conj_physical#torch.Tensor.conj_physical", "type": "Tensor", "text": ["See torch.conj_physical()"]}, {"name": "torch.Tensor.conj_physical_()", "path": "generated/torch.tensor.conj_physical_#torch.Tensor.conj_physical_", "type": "Tensor", "text": ["In-place version of conj_physical()"]}, {"name": "torch.Tensor.contiguous()", "path": "generated/torch.tensor.contiguous#torch.Tensor.contiguous", "type": "Tensor", "text": ["Returns a contiguous in memory tensor containing the same data as self tensor. If self tensor is already in the specified memory format, this function returns the self tensor.", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.contiguous_format."]}, {"name": "torch.Tensor.copy_()", "path": "generated/torch.tensor.copy_#torch.Tensor.copy_", "type": "Tensor", "text": ["Copies the elements from src into self tensor and returns self.", "The src tensor must be broadcastable with the self tensor. It may be of a different data type or reside on a different device."]}, {"name": "torch.Tensor.copysign()", "path": "generated/torch.tensor.copysign#torch.Tensor.copysign", "type": "Tensor", "text": ["See torch.copysign()"]}, {"name": "torch.Tensor.copysign_()", "path": "generated/torch.tensor.copysign_#torch.Tensor.copysign_", "type": "Tensor", "text": ["In-place version of copysign()"]}, {"name": "torch.Tensor.corrcoef()", "path": "generated/torch.tensor.corrcoef#torch.Tensor.corrcoef", "type": "Tensor", "text": ["See torch.corrcoef()"]}, {"name": "torch.Tensor.cos()", "path": "generated/torch.tensor.cos#torch.Tensor.cos", "type": "Tensor", "text": ["See torch.cos()"]}, {"name": "torch.Tensor.cos_()", "path": "generated/torch.tensor.cos_#torch.Tensor.cos_", "type": "Tensor", "text": ["In-place version of cos()"]}, {"name": "torch.Tensor.cosh()", "path": "generated/torch.tensor.cosh#torch.Tensor.cosh", "type": "Tensor", "text": ["See torch.cosh()"]}, {"name": "torch.Tensor.cosh_()", "path": "generated/torch.tensor.cosh_#torch.Tensor.cosh_", "type": "Tensor", "text": ["In-place version of cosh()"]}, {"name": "torch.Tensor.count_nonzero()", "path": "generated/torch.tensor.count_nonzero#torch.Tensor.count_nonzero", "type": "Tensor", "text": ["See torch.count_nonzero()"]}, {"name": "torch.Tensor.cov()", "path": "generated/torch.tensor.cov#torch.Tensor.cov", "type": "Tensor", "text": ["See torch.cov()"]}, {"name": "torch.Tensor.cpu()", "path": "generated/torch.tensor.cpu#torch.Tensor.cpu", "type": "Tensor", "text": ["Returns a copy of this object in CPU memory.", "If this object is already in CPU memory and on the correct device, then no copy is performed and the original object is returned.", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.cross()", "path": "generated/torch.tensor.cross#torch.Tensor.cross", "type": "Tensor", "text": ["See torch.cross()"]}, {"name": "torch.Tensor.crow_indices()", "path": "generated/torch.tensor.crow_indices#torch.Tensor.crow_indices", "type": "Sparse Tensors", "text": ["Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The crow_indices tensor is strictly of shape (self.size(0) + 1) and of type int32 or int64. When using MKL routines such as sparse matrix multiplication, it is necessary to use int32 indexing in order to avoid downcasting and potentially losing information."]}, {"name": "torch.Tensor.cuda()", "path": "generated/torch.tensor.cuda#torch.Tensor.cuda", "type": "Tensor", "text": ["Returns a copy of this object in CUDA memory.", "If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned."]}, {"name": "torch.Tensor.cummax()", "path": "generated/torch.tensor.cummax#torch.Tensor.cummax", "type": "Tensor", "text": ["See torch.cummax()"]}, {"name": "torch.Tensor.cummin()", "path": "generated/torch.tensor.cummin#torch.Tensor.cummin", "type": "Tensor", "text": ["See torch.cummin()"]}, {"name": "torch.Tensor.cumprod()", "path": "generated/torch.tensor.cumprod#torch.Tensor.cumprod", "type": "Tensor", "text": ["See torch.cumprod()"]}, {"name": "torch.Tensor.cumprod_()", "path": "generated/torch.tensor.cumprod_#torch.Tensor.cumprod_", "type": "Tensor", "text": ["In-place version of cumprod()"]}, {"name": "torch.Tensor.cumsum()", "path": "generated/torch.tensor.cumsum#torch.Tensor.cumsum", "type": "Tensor", "text": ["See torch.cumsum()"]}, {"name": "torch.Tensor.cumsum_()", "path": "generated/torch.tensor.cumsum_#torch.Tensor.cumsum_", "type": "Tensor", "text": ["In-place version of cumsum()"]}, {"name": "torch.Tensor.data_ptr()", "path": "generated/torch.tensor.data_ptr#torch.Tensor.data_ptr", "type": "Tensor", "text": ["Returns the address of the first element of self tensor."]}, {"name": "torch.Tensor.deg2rad()", "path": "generated/torch.tensor.deg2rad#torch.Tensor.deg2rad", "type": "Tensor", "text": ["See torch.deg2rad()"]}, {"name": "torch.Tensor.dense_dim()", "path": "generated/torch.tensor.dense_dim#torch.Tensor.dense_dim", "type": "Tensor", "text": ["Return the number of dense dimensions in a sparse tensor self.", "Note", "Returns len(self.shape) if self is not a sparse tensor.", "See also Tensor.sparse_dim() and hybrid tensors."]}, {"name": "torch.Tensor.dequantize()", "path": "generated/torch.tensor.dequantize#torch.Tensor.dequantize", "type": "Tensor", "text": ["Given a quantized Tensor, dequantize it and return the dequantized float Tensor."]}, {"name": "torch.Tensor.det()", "path": "generated/torch.tensor.det#torch.Tensor.det", "type": "Tensor", "text": ["See torch.det()"]}, {"name": "torch.Tensor.detach()", "path": "generated/torch.tensor.detach#torch.Tensor.detach", "type": "Tensor", "text": ["Returns a new Tensor, detached from the current graph.", "The result will never require gradient.", "This method also affects forward mode AD gradients and the result will never have forward mode AD gradients.", "Note", "Returned Tensor shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks. IMPORTANT NOTE: Previously, in-place size / stride / storage changes (such as resize_ / resize_as_ / set_ / transpose_) to the returned tensor also update the original tensor. Now, these in-place changes will not update the original tensor anymore, and will instead trigger an error. For sparse tensors: In-place indices / values changes (such as zero_ / copy_ / add_) to the returned tensor will not update the original tensor anymore, and will instead trigger an error."]}, {"name": "torch.Tensor.detach_()", "path": "generated/torch.tensor.detach_#torch.Tensor.detach_", "type": "Tensor", "text": ["Detaches the Tensor from the graph that created it, making it a leaf. Views cannot be detached in-place.", "This method also affects forward mode AD gradients and the result will never have forward mode AD gradients."]}, {"name": "torch.Tensor.device", "path": "generated/torch.tensor.device#torch.Tensor.device", "type": "Tensor", "text": ["Is the torch.device where this Tensor is."]}, {"name": "torch.Tensor.diag()", "path": "generated/torch.tensor.diag#torch.Tensor.diag", "type": "Tensor", "text": ["See torch.diag()"]}, {"name": "torch.Tensor.diag_embed()", "path": "generated/torch.tensor.diag_embed#torch.Tensor.diag_embed", "type": "Tensor", "text": ["See torch.diag_embed()"]}, {"name": "torch.Tensor.diagflat()", "path": "generated/torch.tensor.diagflat#torch.Tensor.diagflat", "type": "Tensor", "text": ["See torch.diagflat()"]}, {"name": "torch.Tensor.diagonal()", "path": "generated/torch.tensor.diagonal#torch.Tensor.diagonal", "type": "Tensor", "text": ["See torch.diagonal()"]}, {"name": "torch.Tensor.diagonal_scatter()", "path": "generated/torch.tensor.diagonal_scatter#torch.Tensor.diagonal_scatter", "type": "Tensor", "text": ["See torch.diagonal_scatter()"]}, {"name": "torch.Tensor.diff()", "path": "generated/torch.tensor.diff#torch.Tensor.diff", "type": "Tensor", "text": ["See torch.diff()"]}, {"name": "torch.Tensor.digamma()", "path": "generated/torch.tensor.digamma#torch.Tensor.digamma", "type": "Tensor", "text": ["See torch.digamma()"]}, {"name": "torch.Tensor.digamma_()", "path": "generated/torch.tensor.digamma_#torch.Tensor.digamma_", "type": "Tensor", "text": ["In-place version of digamma()"]}, {"name": "torch.Tensor.dim()", "path": "generated/torch.tensor.dim#torch.Tensor.dim", "type": "Tensor", "text": ["Returns the number of dimensions of self tensor."]}, {"name": "torch.Tensor.dim_order()", "path": "generated/torch.tensor.dim_order#torch.Tensor.dim_order", "type": "Tensor", "text": ["Returns a tuple of int describing the dim order or physical layout of self.", "None \u2013 ", "Dim order represents how dimensions are laid out in memory, starting from the outermost to the innermost dimension.", "Warning", "The dim_order tensor API is experimental and subject to change."]}, {"name": "torch.Tensor.dist()", "path": "generated/torch.tensor.dist#torch.Tensor.dist", "type": "Tensor", "text": ["See torch.dist()"]}, {"name": "torch.Tensor.div()", "path": "generated/torch.tensor.div#torch.Tensor.div", "type": "Tensor", "text": ["See torch.div()"]}, {"name": "torch.Tensor.div_()", "path": "generated/torch.tensor.div_#torch.Tensor.div_", "type": "Tensor", "text": ["In-place version of div()"]}, {"name": "torch.Tensor.divide()", "path": "generated/torch.tensor.divide#torch.Tensor.divide", "type": "Tensor", "text": ["See torch.divide()"]}, {"name": "torch.Tensor.divide_()", "path": "generated/torch.tensor.divide_#torch.Tensor.divide_", "type": "Tensor", "text": ["In-place version of divide()"]}, {"name": "torch.Tensor.dot()", "path": "generated/torch.tensor.dot#torch.Tensor.dot", "type": "Tensor", "text": ["See torch.dot()"]}, {"name": "torch.Tensor.double()", "path": "generated/torch.tensor.double#torch.Tensor.double", "type": "Tensor", "text": ["self.double() is equivalent to self.to(torch.float64). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.dsplit()", "path": "generated/torch.tensor.dsplit#torch.Tensor.dsplit", "type": "Tensor", "text": ["See torch.dsplit()"]}, {"name": "torch.Tensor.element_size()", "path": "generated/torch.tensor.element_size#torch.Tensor.element_size", "type": "Tensor", "text": ["Returns the size in bytes of an individual element.", "Example:"]}, {"name": "torch.Tensor.eq()", "path": "generated/torch.tensor.eq#torch.Tensor.eq", "type": "Tensor", "text": ["See torch.eq()"]}, {"name": "torch.Tensor.eq_()", "path": "generated/torch.tensor.eq_#torch.Tensor.eq_", "type": "Tensor", "text": ["In-place version of eq()"]}, {"name": "torch.Tensor.equal()", "path": "generated/torch.tensor.equal#torch.Tensor.equal", "type": "Tensor", "text": ["See torch.equal()"]}, {"name": "torch.Tensor.erf()", "path": "generated/torch.tensor.erf#torch.Tensor.erf", "type": "Tensor", "text": ["See torch.erf()"]}, {"name": "torch.Tensor.erf_()", "path": "generated/torch.tensor.erf_#torch.Tensor.erf_", "type": "Tensor", "text": ["In-place version of erf()"]}, {"name": "torch.Tensor.erfc()", "path": "generated/torch.tensor.erfc#torch.Tensor.erfc", "type": "Tensor", "text": ["See torch.erfc()"]}, {"name": "torch.Tensor.erfc_()", "path": "generated/torch.tensor.erfc_#torch.Tensor.erfc_", "type": "Tensor", "text": ["In-place version of erfc()"]}, {"name": "torch.Tensor.erfinv()", "path": "generated/torch.tensor.erfinv#torch.Tensor.erfinv", "type": "Tensor", "text": ["See torch.erfinv()"]}, {"name": "torch.Tensor.erfinv_()", "path": "generated/torch.tensor.erfinv_#torch.Tensor.erfinv_", "type": "Tensor", "text": ["In-place version of erfinv()"]}, {"name": "torch.Tensor.exp()", "path": "generated/torch.tensor.exp#torch.Tensor.exp", "type": "Tensor", "text": ["See torch.exp()"]}, {"name": "torch.Tensor.exp_()", "path": "generated/torch.tensor.exp_#torch.Tensor.exp_", "type": "Tensor", "text": ["In-place version of exp()"]}, {"name": "torch.Tensor.expand()", "path": "generated/torch.tensor.expand#torch.Tensor.expand", "type": "Tensor", "text": ["Returns a new view of the self tensor with singleton dimensions expanded to a larger size.", "Passing -1 as the size for a dimension means not changing the size of that dimension.", "Tensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front. For the new dimensions, the size cannot be set to -1.", "Expanding a tensor does not allocate new memory, but only creates a new view on the existing tensor where a dimension of size one is expanded to a larger size by setting the stride to 0. Any dimension of size 1 can be expanded to an arbitrary value without allocating new memory.", "*sizes (torch.Size or int...) \u2013 the desired expanded size", "Warning", "More than one element of an expanded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.", "Example:"]}, {"name": "torch.Tensor.expand_as()", "path": "generated/torch.tensor.expand_as#torch.Tensor.expand_as", "type": "Tensor", "text": ["Expand this tensor to the same size as other. self.expand_as(other) is equivalent to self.expand(other.size()).", "Please see expand() for more information about expand.", "other (torch.Tensor) \u2013 The result tensor has the same size as other."]}, {"name": "torch.Tensor.expm1()", "path": "generated/torch.tensor.expm1#torch.Tensor.expm1", "type": "Tensor", "text": ["See torch.expm1()"]}, {"name": "torch.Tensor.expm1_()", "path": "generated/torch.tensor.expm1_#torch.Tensor.expm1_", "type": "Tensor", "text": ["In-place version of expm1()"]}, {"name": "torch.Tensor.exponential_()", "path": "generated/torch.tensor.exponential_#torch.Tensor.exponential_", "type": "Tensor", "text": ["Fills self tensor with elements drawn from the exponential distribution:"]}, {"name": "torch.Tensor.fill_()", "path": "generated/torch.tensor.fill_#torch.Tensor.fill_", "type": "Tensor", "text": ["Fills self tensor with the specified value."]}, {"name": "torch.Tensor.fill_diagonal_()", "path": "generated/torch.tensor.fill_diagonal_#torch.Tensor.fill_diagonal_", "type": "Tensor", "text": ["Fill the main diagonal of a tensor that has at least 2-dimensions. When dims>2, all dimensions of input must be of equal length. This function modifies the input tensor in-place, and returns the input tensor.", "Example:"]}, {"name": "torch.Tensor.fix()", "path": "generated/torch.tensor.fix#torch.Tensor.fix", "type": "Tensor", "text": ["See torch.fix()."]}, {"name": "torch.Tensor.fix_()", "path": "generated/torch.tensor.fix_#torch.Tensor.fix_", "type": "Tensor", "text": ["In-place version of fix()"]}, {"name": "torch.Tensor.flatten()", "path": "generated/torch.tensor.flatten#torch.Tensor.flatten", "type": "Tensor", "text": ["See torch.flatten()"]}, {"name": "torch.Tensor.flip()", "path": "generated/torch.tensor.flip#torch.Tensor.flip", "type": "Tensor", "text": ["See torch.flip()"]}, {"name": "torch.Tensor.fliplr()", "path": "generated/torch.tensor.fliplr#torch.Tensor.fliplr", "type": "Tensor", "text": ["See torch.fliplr()"]}, {"name": "torch.Tensor.flipud()", "path": "generated/torch.tensor.flipud#torch.Tensor.flipud", "type": "Tensor", "text": ["See torch.flipud()"]}, {"name": "torch.Tensor.float()", "path": "generated/torch.tensor.float#torch.Tensor.float", "type": "Tensor", "text": ["self.float() is equivalent to self.to(torch.float32). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.float_power()", "path": "generated/torch.tensor.float_power#torch.Tensor.float_power", "type": "Tensor", "text": ["See torch.float_power()"]}, {"name": "torch.Tensor.float_power_()", "path": "generated/torch.tensor.float_power_#torch.Tensor.float_power_", "type": "Tensor", "text": ["In-place version of float_power()"]}, {"name": "torch.Tensor.floor()", "path": "generated/torch.tensor.floor#torch.Tensor.floor", "type": "Tensor", "text": ["See torch.floor()"]}, {"name": "torch.Tensor.floor_()", "path": "generated/torch.tensor.floor_#torch.Tensor.floor_", "type": "Tensor", "text": ["In-place version of floor()"]}, {"name": "torch.Tensor.floor_divide()", "path": "generated/torch.tensor.floor_divide#torch.Tensor.floor_divide", "type": "Tensor", "text": ["See torch.floor_divide()"]}, {"name": "torch.Tensor.floor_divide_()", "path": "generated/torch.tensor.floor_divide_#torch.Tensor.floor_divide_", "type": "Tensor", "text": ["In-place version of floor_divide()"]}, {"name": "torch.Tensor.fmax()", "path": "generated/torch.tensor.fmax#torch.Tensor.fmax", "type": "Tensor", "text": ["See torch.fmax()"]}, {"name": "torch.Tensor.fmin()", "path": "generated/torch.tensor.fmin#torch.Tensor.fmin", "type": "Tensor", "text": ["See torch.fmin()"]}, {"name": "torch.Tensor.fmod()", "path": "generated/torch.tensor.fmod#torch.Tensor.fmod", "type": "Tensor", "text": ["See torch.fmod()"]}, {"name": "torch.Tensor.fmod_()", "path": "generated/torch.tensor.fmod_#torch.Tensor.fmod_", "type": "Tensor", "text": ["In-place version of fmod()"]}, {"name": "torch.Tensor.frac()", "path": "generated/torch.tensor.frac#torch.Tensor.frac", "type": "Tensor", "text": ["See torch.frac()"]}, {"name": "torch.Tensor.frac_()", "path": "generated/torch.tensor.frac_#torch.Tensor.frac_", "type": "Tensor", "text": ["In-place version of frac()"]}, {"name": "torch.Tensor.frexp()", "path": "generated/torch.tensor.frexp#torch.Tensor.frexp", "type": "Tensor", "text": ["See torch.frexp()"]}, {"name": "torch.Tensor.gather()", "path": "generated/torch.tensor.gather#torch.Tensor.gather", "type": "Tensor", "text": ["See torch.gather()"]}, {"name": "torch.Tensor.gcd()", "path": "generated/torch.tensor.gcd#torch.Tensor.gcd", "type": "Tensor", "text": ["See torch.gcd()"]}, {"name": "torch.Tensor.gcd_()", "path": "generated/torch.tensor.gcd_#torch.Tensor.gcd_", "type": "Tensor", "text": ["In-place version of gcd()"]}, {"name": "torch.Tensor.ge()", "path": "generated/torch.tensor.ge#torch.Tensor.ge", "type": "Tensor", "text": ["See torch.ge()."]}, {"name": "torch.Tensor.ge_()", "path": "generated/torch.tensor.ge_#torch.Tensor.ge_", "type": "Tensor", "text": ["In-place version of ge()."]}, {"name": "torch.Tensor.geometric_()", "path": "generated/torch.tensor.geometric_#torch.Tensor.geometric_", "type": "Tensor", "text": ["Fills self tensor with elements drawn from the geometric distribution:"]}, {"name": "torch.Tensor.geqrf()", "path": "generated/torch.tensor.geqrf#torch.Tensor.geqrf", "type": "Tensor", "text": ["See torch.geqrf()"]}, {"name": "torch.Tensor.ger()", "path": "generated/torch.tensor.ger#torch.Tensor.ger", "type": "Tensor", "text": ["See torch.ger()"]}, {"name": "torch.Tensor.get_device()", "path": "generated/torch.tensor.get_device#torch.Tensor.get_device", "type": "Tensor", "text": ["For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides. For CPU tensors, this function returns -1.", "Example:"]}, {"name": "torch.Tensor.grad", "path": "generated/torch.tensor.grad#torch.Tensor.grad", "type": "Tensor", "text": ["This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. The attribute will then contain the gradients computed and future calls to backward() will accumulate (add) gradients into it."]}, {"name": "torch.Tensor.greater()", "path": "generated/torch.tensor.greater#torch.Tensor.greater", "type": "Tensor", "text": ["See torch.greater()."]}, {"name": "torch.Tensor.greater_()", "path": "generated/torch.tensor.greater_#torch.Tensor.greater_", "type": "Tensor", "text": ["In-place version of greater()."]}, {"name": "torch.Tensor.greater_equal()", "path": "generated/torch.tensor.greater_equal#torch.Tensor.greater_equal", "type": "Tensor", "text": ["See torch.greater_equal()."]}, {"name": "torch.Tensor.greater_equal_()", "path": "generated/torch.tensor.greater_equal_#torch.Tensor.greater_equal_", "type": "Tensor", "text": ["In-place version of greater_equal()."]}, {"name": "torch.Tensor.gt()", "path": "generated/torch.tensor.gt#torch.Tensor.gt", "type": "Tensor", "text": ["See torch.gt()."]}, {"name": "torch.Tensor.gt_()", "path": "generated/torch.tensor.gt_#torch.Tensor.gt_", "type": "Tensor", "text": ["In-place version of gt()."]}, {"name": "torch.Tensor.H", "path": "tensors#torch.Tensor.H", "type": "Tensor", "text": ["Returns a view of a matrix (2-D tensor) conjugated and transposed.", "x.H is equivalent to x.transpose(0, 1).conj() for complex matrices and x.transpose(0, 1) for real matrices.", "See also", "mH: An attribute that also works on batches of matrices."]}, {"name": "torch.Tensor.half()", "path": "generated/torch.tensor.half#torch.Tensor.half", "type": "Tensor", "text": ["self.half() is equivalent to self.to(torch.float16). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.hardshrink()", "path": "generated/torch.tensor.hardshrink#torch.Tensor.hardshrink", "type": "Tensor", "text": ["See torch.nn.functional.hardshrink()"]}, {"name": "torch.Tensor.heaviside()", "path": "generated/torch.tensor.heaviside#torch.Tensor.heaviside", "type": "Tensor", "text": ["See torch.heaviside()"]}, {"name": "torch.Tensor.histc()", "path": "generated/torch.tensor.histc#torch.Tensor.histc", "type": "Tensor", "text": ["See torch.histc()"]}, {"name": "torch.Tensor.histogram()", "path": "generated/torch.tensor.histogram#torch.Tensor.histogram", "type": "Tensor", "text": ["See torch.histogram()"]}, {"name": "torch.Tensor.hsplit()", "path": "generated/torch.tensor.hsplit#torch.Tensor.hsplit", "type": "Tensor", "text": ["See torch.hsplit()"]}, {"name": "torch.Tensor.hypot()", "path": "generated/torch.tensor.hypot#torch.Tensor.hypot", "type": "Tensor", "text": ["See torch.hypot()"]}, {"name": "torch.Tensor.hypot_()", "path": "generated/torch.tensor.hypot_#torch.Tensor.hypot_", "type": "Tensor", "text": ["In-place version of hypot()"]}, {"name": "torch.Tensor.i0()", "path": "generated/torch.tensor.i0#torch.Tensor.i0", "type": "Tensor", "text": ["See torch.i0()"]}, {"name": "torch.Tensor.i0_()", "path": "generated/torch.tensor.i0_#torch.Tensor.i0_", "type": "Tensor", "text": ["In-place version of i0()"]}, {"name": "torch.Tensor.igamma()", "path": "generated/torch.tensor.igamma#torch.Tensor.igamma", "type": "Tensor", "text": ["See torch.igamma()"]}, {"name": "torch.Tensor.igamma_()", "path": "generated/torch.tensor.igamma_#torch.Tensor.igamma_", "type": "Tensor", "text": ["In-place version of igamma()"]}, {"name": "torch.Tensor.igammac()", "path": "generated/torch.tensor.igammac#torch.Tensor.igammac", "type": "Tensor", "text": ["See torch.igammac()"]}, {"name": "torch.Tensor.igammac_()", "path": "generated/torch.tensor.igammac_#torch.Tensor.igammac_", "type": "Tensor", "text": ["In-place version of igammac()"]}, {"name": "torch.Tensor.imag", "path": "generated/torch.tensor.imag#torch.Tensor.imag", "type": "Tensor", "text": ["Returns a new tensor containing imaginary values of the self tensor. The returned tensor and self share the same underlying storage.", "Warning", "imag() is only supported for tensors with complex dtypes."]}, {"name": "torch.Tensor.index_add()", "path": "generated/torch.tensor.index_add#torch.Tensor.index_add", "type": "Tensor", "text": ["Out-of-place version of torch.Tensor.index_add_()."]}, {"name": "torch.Tensor.index_add_()", "path": "generated/torch.tensor.index_add_#torch.Tensor.index_add_", "type": "Tensor", "text": ["Accumulate the elements of alpha times source into the self tensor by adding to the indices in the order given in index. For example, if dim == 0, index[i] == j, and alpha=-1, then the ith row of source is subtracted from the jth row of self.", "The dimth dimension of source must have the same size as the length of index (which must be a vector), and all other dimensions must match self, or an error will be raised.", "For a 3-D tensor the output is given as:", "Note", "This operation may behave nondeterministically when given tensors on a CUDA device. See Reproducibility for more information.", "alpha (Number) \u2013 the scalar multiplier for source", "Example:"]}, {"name": "torch.Tensor.index_copy()", "path": "generated/torch.tensor.index_copy#torch.Tensor.index_copy", "type": "Tensor", "text": ["Out-of-place version of torch.Tensor.index_copy_()."]}, {"name": "torch.Tensor.index_copy_()", "path": "generated/torch.tensor.index_copy_#torch.Tensor.index_copy_", "type": "Tensor", "text": ["Copies the elements of tensor into the self tensor by selecting the indices in the order given in index. For example, if dim == 0 and index[i] == j, then the ith row of tensor is copied to the jth row of self.", "The dimth dimension of tensor must have the same size as the length of index (which must be a vector), and all other dimensions must match self, or an error will be raised.", "Note", "If index contains duplicate entries, multiple elements from tensor will be copied to the same index of self. The result is nondeterministic since it depends on which copy occurs last.", "Example:"]}, {"name": "torch.Tensor.index_fill()", "path": "generated/torch.tensor.index_fill#torch.Tensor.index_fill", "type": "Tensor", "text": ["Out-of-place version of torch.Tensor.index_fill_()."]}, {"name": "torch.Tensor.index_fill_()", "path": "generated/torch.tensor.index_fill_#torch.Tensor.index_fill_", "type": "Tensor", "text": ["Fills the elements of the self tensor with value value by selecting the indices in the order given in index."]}, {"name": "torch.Tensor.index_put()", "path": "generated/torch.tensor.index_put#torch.Tensor.index_put", "type": "Tensor", "text": ["Out-place version of index_put_()."]}, {"name": "torch.Tensor.index_put_()", "path": "generated/torch.tensor.index_put_#torch.Tensor.index_put_", "type": "Tensor", "text": ["Puts values from the tensor values into the tensor self using the indices specified in indices (which is a tuple of Tensors). The expression tensor.index_put_(indices, values) is equivalent to tensor[indices] = values. Returns self.", "If accumulate is True, the elements in values are added to self. If accumulate is False, the behavior is undefined if indices contain duplicate elements."]}, {"name": "torch.Tensor.index_reduce()", "path": "generated/torch.tensor.index_reduce#torch.Tensor.index_reduce", "type": "Tensor", "text": []}, {"name": "torch.Tensor.index_reduce_()", "path": "generated/torch.tensor.index_reduce_#torch.Tensor.index_reduce_", "type": "Tensor", "text": ["Accumulate the elements of source into the self tensor by accumulating to the indices in the order given in index using the reduction given by the reduce argument. For example, if dim == 0, index[i] == j, reduce == prod and include_self == True then the ith row of source is multiplied by the jth row of self. If include_self=\"True\", the values in the self tensor are included in the reduction, otherwise, rows in the self tensor that are accumulated to are treated as if they were filled with the reduction identites.", "The dimth dimension of source must have the same size as the length of index (which must be a vector), and all other dimensions must match self, or an error will be raised.", "For a 3-D tensor with reduce=\"prod\" and include_self=True the output is given as:", "Note", "This operation may behave nondeterministically when given tensors on a CUDA device. See Reproducibility for more information.", "Note", "This function only supports floating point tensors.", "Warning", "This function is in beta and may change in the near future.", "include_self (bool) \u2013 whether the elements from the self tensor are included in the reduction", "Example:"]}, {"name": "torch.Tensor.index_select()", "path": "generated/torch.tensor.index_select#torch.Tensor.index_select", "type": "Tensor", "text": ["See torch.index_select()"]}, {"name": "torch.Tensor.indices()", "path": "generated/torch.tensor.indices#torch.Tensor.indices", "type": "Tensor", "text": ["Return the indices tensor of a sparse COO tensor.", "Warning", "Throws an error if self is not a sparse COO tensor.", "See also Tensor.values().", "Note", "This method can only be called on a coalesced sparse tensor. See Tensor.coalesce() for details."]}, {"name": "torch.Tensor.inner()", "path": "generated/torch.tensor.inner#torch.Tensor.inner", "type": "Tensor", "text": ["See torch.inner()."]}, {"name": "torch.Tensor.int()", "path": "generated/torch.tensor.int#torch.Tensor.int", "type": "Tensor", "text": ["self.int() is equivalent to self.to(torch.int32). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.int_repr()", "path": "generated/torch.tensor.int_repr#torch.Tensor.int_repr", "type": "Tensor", "text": ["Given a quantized Tensor, self.int_repr() returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor."]}, {"name": "torch.Tensor.inverse()", "path": "generated/torch.tensor.inverse#torch.Tensor.inverse", "type": "Tensor", "text": ["See torch.inverse()"]}, {"name": "torch.Tensor.is_coalesced()", "path": "generated/torch.tensor.is_coalesced#torch.Tensor.is_coalesced", "type": "Sparse Tensors", "text": ["Returns True if self is a sparse COO tensor that is coalesced, False otherwise.", "Warning", "Throws an error if self is not a sparse COO tensor.", "See coalesce() and uncoalesced tensors."]}, {"name": "torch.Tensor.is_complex()", "path": "generated/torch.tensor.is_complex#torch.Tensor.is_complex", "type": "Tensor", "text": ["Returns True if the data type of self is a complex data type."]}, {"name": "torch.Tensor.is_conj()", "path": "generated/torch.tensor.is_conj#torch.Tensor.is_conj", "type": "Tensor", "text": ["Returns True if the conjugate bit of self is set to true."]}, {"name": "torch.Tensor.is_contiguous()", "path": "generated/torch.tensor.is_contiguous#torch.Tensor.is_contiguous", "type": "Tensor", "text": ["Returns True if self tensor is contiguous in memory in the order specified by memory format.", "memory_format (torch.memory_format, optional) \u2013 Specifies memory allocation order. Default: torch.contiguous_format."]}, {"name": "torch.Tensor.is_cuda", "path": "generated/torch.tensor.is_cuda#torch.Tensor.is_cuda", "type": "Tensor", "text": ["Is True if the Tensor is stored on the GPU, False otherwise."]}, {"name": "torch.Tensor.is_floating_point()", "path": "generated/torch.tensor.is_floating_point#torch.Tensor.is_floating_point", "type": "Tensor", "text": ["Returns True if the data type of self is a floating point data type."]}, {"name": "torch.Tensor.is_inference()", "path": "generated/torch.tensor.is_inference#torch.Tensor.is_inference", "type": "Tensor", "text": ["See torch.is_inference()"]}, {"name": "torch.Tensor.is_leaf", "path": "generated/torch.tensor.is_leaf#torch.Tensor.is_leaf", "type": "Tensor", "text": ["All Tensors that have requires_grad which is False will be leaf Tensors by convention.", "For Tensors that have requires_grad which is True, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so grad_fn is None.", "Only leaf Tensors will have their grad populated during a call to backward(). To get grad populated for non-leaf Tensors, you can use retain_grad().", "Example:"]}, {"name": "torch.Tensor.is_meta", "path": "generated/torch.tensor.is_meta#torch.Tensor.is_meta", "type": "Tensor", "text": ["Is True if the Tensor is a meta tensor, False otherwise. Meta tensors are like normal tensors, but they carry no data."]}, {"name": "torch.Tensor.is_pinned()", "path": "generated/torch.tensor.is_pinned#torch.Tensor.is_pinned", "type": "Tensor", "text": ["Returns true if this tensor resides in pinned memory."]}, {"name": "torch.Tensor.is_quantized", "path": "generated/torch.tensor.is_quantized#torch.Tensor.is_quantized", "type": "Tensor", "text": ["Is True if the Tensor is quantized, False otherwise."]}, {"name": "torch.Tensor.is_set_to()", "path": "generated/torch.tensor.is_set_to#torch.Tensor.is_set_to", "type": "Tensor", "text": ["Returns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride)."]}, {"name": "torch.Tensor.is_shared()", "path": "generated/torch.tensor.is_shared#torch.Tensor.is_shared", "type": "Tensor", "text": ["Checks if tensor is in shared memory.", "This is always True for CUDA tensors."]}, {"name": "torch.Tensor.is_signed()", "path": "generated/torch.tensor.is_signed#torch.Tensor.is_signed", "type": "Tensor", "text": ["Returns True if the data type of self is a signed data type."]}, {"name": "torch.Tensor.is_sparse", "path": "generated/torch.tensor.is_sparse#torch.Tensor.is_sparse", "type": "Tensor", "text": ["Is True if the Tensor uses sparse COO storage layout, False otherwise."]}, {"name": "torch.Tensor.is_sparse_csr", "path": "generated/torch.tensor.is_sparse_csr#torch.Tensor.is_sparse_csr", "type": "Sparse Tensors", "text": ["Is True if the Tensor uses sparse CSR storage layout, False otherwise."]}, {"name": "torch.Tensor.isclose()", "path": "generated/torch.tensor.isclose#torch.Tensor.isclose", "type": "Tensor", "text": ["See torch.isclose()"]}, {"name": "torch.Tensor.isfinite()", "path": "generated/torch.tensor.isfinite#torch.Tensor.isfinite", "type": "Tensor", "text": ["See torch.isfinite()"]}, {"name": "torch.Tensor.isinf()", "path": "generated/torch.tensor.isinf#torch.Tensor.isinf", "type": "Tensor", "text": ["See torch.isinf()"]}, {"name": "torch.Tensor.isnan()", "path": "generated/torch.tensor.isnan#torch.Tensor.isnan", "type": "Tensor", "text": ["See torch.isnan()"]}, {"name": "torch.Tensor.isneginf()", "path": "generated/torch.tensor.isneginf#torch.Tensor.isneginf", "type": "Tensor", "text": ["See torch.isneginf()"]}, {"name": "torch.Tensor.isposinf()", "path": "generated/torch.tensor.isposinf#torch.Tensor.isposinf", "type": "Tensor", "text": ["See torch.isposinf()"]}, {"name": "torch.Tensor.isreal()", "path": "generated/torch.tensor.isreal#torch.Tensor.isreal", "type": "Tensor", "text": ["See torch.isreal()"]}, {"name": "torch.Tensor.istft()", "path": "generated/torch.tensor.istft#torch.Tensor.istft", "type": "Tensor", "text": ["See torch.istft()"]}, {"name": "torch.Tensor.item()", "path": "generated/torch.tensor.item#torch.Tensor.item", "type": "Tensor", "text": ["Returns the value of this tensor as a standard Python number. This only works for tensors with one element. For other cases, see tolist().", "This operation is not differentiable.", "Example:"]}, {"name": "torch.Tensor.itemsize", "path": "generated/torch.tensor.itemsize#torch.Tensor.itemsize", "type": "Tensor", "text": ["Alias for element_size()"]}, {"name": "torch.Tensor.kthvalue()", "path": "generated/torch.tensor.kthvalue#torch.Tensor.kthvalue", "type": "Tensor", "text": ["See torch.kthvalue()"]}, {"name": "torch.Tensor.lcm()", "path": "generated/torch.tensor.lcm#torch.Tensor.lcm", "type": "Tensor", "text": ["See torch.lcm()"]}, {"name": "torch.Tensor.lcm_()", "path": "generated/torch.tensor.lcm_#torch.Tensor.lcm_", "type": "Tensor", "text": ["In-place version of lcm()"]}, {"name": "torch.Tensor.ldexp()", "path": "generated/torch.tensor.ldexp#torch.Tensor.ldexp", "type": "Tensor", "text": ["See torch.ldexp()"]}, {"name": "torch.Tensor.ldexp_()", "path": "generated/torch.tensor.ldexp_#torch.Tensor.ldexp_", "type": "Tensor", "text": ["In-place version of ldexp()"]}, {"name": "torch.Tensor.le()", "path": "generated/torch.tensor.le#torch.Tensor.le", "type": "Tensor", "text": ["See torch.le()."]}, {"name": "torch.Tensor.le_()", "path": "generated/torch.tensor.le_#torch.Tensor.le_", "type": "Tensor", "text": ["In-place version of le()."]}, {"name": "torch.Tensor.lerp()", "path": "generated/torch.tensor.lerp#torch.Tensor.lerp", "type": "Tensor", "text": ["See torch.lerp()"]}, {"name": "torch.Tensor.lerp_()", "path": "generated/torch.tensor.lerp_#torch.Tensor.lerp_", "type": "Tensor", "text": ["In-place version of lerp()"]}, {"name": "torch.Tensor.less()", "path": "generated/torch.tensor.less#torch.Tensor.less", "type": "Tensor", "text": ["lt(other) -> Tensor", "See torch.less()."]}, {"name": "torch.Tensor.less_()", "path": "generated/torch.tensor.less_#torch.Tensor.less_", "type": "Tensor", "text": ["In-place version of less()."]}, {"name": "torch.Tensor.less_equal()", "path": "generated/torch.tensor.less_equal#torch.Tensor.less_equal", "type": "Tensor", "text": ["See torch.less_equal()."]}, {"name": "torch.Tensor.less_equal_()", "path": "generated/torch.tensor.less_equal_#torch.Tensor.less_equal_", "type": "Tensor", "text": ["In-place version of less_equal()."]}, {"name": "torch.Tensor.lgamma()", "path": "generated/torch.tensor.lgamma#torch.Tensor.lgamma", "type": "Tensor", "text": ["See torch.lgamma()"]}, {"name": "torch.Tensor.lgamma_()", "path": "generated/torch.tensor.lgamma_#torch.Tensor.lgamma_", "type": "Tensor", "text": ["In-place version of lgamma()"]}, {"name": "torch.Tensor.log()", "path": "generated/torch.tensor.log#torch.Tensor.log", "type": "Tensor", "text": ["See torch.log()"]}, {"name": "torch.Tensor.log10()", "path": "generated/torch.tensor.log10#torch.Tensor.log10", "type": "Tensor", "text": ["See torch.log10()"]}, {"name": "torch.Tensor.log10_()", "path": "generated/torch.tensor.log10_#torch.Tensor.log10_", "type": "Tensor", "text": ["In-place version of log10()"]}, {"name": "torch.Tensor.log1p()", "path": "generated/torch.tensor.log1p#torch.Tensor.log1p", "type": "Tensor", "text": ["See torch.log1p()"]}, {"name": "torch.Tensor.log1p_()", "path": "generated/torch.tensor.log1p_#torch.Tensor.log1p_", "type": "Tensor", "text": ["In-place version of log1p()"]}, {"name": "torch.Tensor.log2()", "path": "generated/torch.tensor.log2#torch.Tensor.log2", "type": "Tensor", "text": ["See torch.log2()"]}, {"name": "torch.Tensor.log2_()", "path": "generated/torch.tensor.log2_#torch.Tensor.log2_", "type": "Tensor", "text": ["In-place version of log2()"]}, {"name": "torch.Tensor.log_()", "path": "generated/torch.tensor.log_#torch.Tensor.log_", "type": "Tensor", "text": ["In-place version of log()"]}, {"name": "torch.Tensor.log_normal_()", "path": "generated/torch.tensor.log_normal_#torch.Tensor.log_normal_", "type": "Tensor", "text": ["Fills self tensor with numbers samples from the log-normal distribution parameterized by the given mean \u03bc\\mu and standard deviation \u03c3\\sigma. Note that mean and std are the mean and standard deviation of the underlying normal distribution, and not of the returned distribution:"]}, {"name": "torch.Tensor.logaddexp()", "path": "generated/torch.tensor.logaddexp#torch.Tensor.logaddexp", "type": "Tensor", "text": ["See torch.logaddexp()"]}, {"name": "torch.Tensor.logaddexp2()", "path": "generated/torch.tensor.logaddexp2#torch.Tensor.logaddexp2", "type": "Tensor", "text": ["See torch.logaddexp2()"]}, {"name": "torch.Tensor.logcumsumexp()", "path": "generated/torch.tensor.logcumsumexp#torch.Tensor.logcumsumexp", "type": "Tensor", "text": ["See torch.logcumsumexp()"]}, {"name": "torch.Tensor.logdet()", "path": "generated/torch.tensor.logdet#torch.Tensor.logdet", "type": "Tensor", "text": ["See torch.logdet()"]}, {"name": "torch.Tensor.logical_and()", "path": "generated/torch.tensor.logical_and#torch.Tensor.logical_and", "type": "Tensor", "text": ["See torch.logical_and()"]}, {"name": "torch.Tensor.logical_and_()", "path": "generated/torch.tensor.logical_and_#torch.Tensor.logical_and_", "type": "Tensor", "text": ["In-place version of logical_and()"]}, {"name": "torch.Tensor.logical_not()", "path": "generated/torch.tensor.logical_not#torch.Tensor.logical_not", "type": "Tensor", "text": ["See torch.logical_not()"]}, {"name": "torch.Tensor.logical_not_()", "path": "generated/torch.tensor.logical_not_#torch.Tensor.logical_not_", "type": "Tensor", "text": ["In-place version of logical_not()"]}, {"name": "torch.Tensor.logical_or()", "path": "generated/torch.tensor.logical_or#torch.Tensor.logical_or", "type": "Tensor", "text": ["See torch.logical_or()"]}, {"name": "torch.Tensor.logical_or_()", "path": "generated/torch.tensor.logical_or_#torch.Tensor.logical_or_", "type": "Tensor", "text": ["In-place version of logical_or()"]}, {"name": "torch.Tensor.logical_xor()", "path": "generated/torch.tensor.logical_xor#torch.Tensor.logical_xor", "type": "Tensor", "text": ["See torch.logical_xor()"]}, {"name": "torch.Tensor.logical_xor_()", "path": "generated/torch.tensor.logical_xor_#torch.Tensor.logical_xor_", "type": "Tensor", "text": ["In-place version of logical_xor()"]}, {"name": "torch.Tensor.logit()", "path": "generated/torch.tensor.logit#torch.Tensor.logit", "type": "Tensor", "text": ["See torch.logit()"]}, {"name": "torch.Tensor.logit_()", "path": "generated/torch.tensor.logit_#torch.Tensor.logit_", "type": "Tensor", "text": ["In-place version of logit()"]}, {"name": "torch.Tensor.logsumexp()", "path": "generated/torch.tensor.logsumexp#torch.Tensor.logsumexp", "type": "Tensor", "text": ["See torch.logsumexp()"]}, {"name": "torch.Tensor.long()", "path": "generated/torch.tensor.long#torch.Tensor.long", "type": "Tensor", "text": ["self.long() is equivalent to self.to(torch.int64). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.lt()", "path": "generated/torch.tensor.lt#torch.Tensor.lt", "type": "Tensor", "text": ["See torch.lt()."]}, {"name": "torch.Tensor.lt_()", "path": "generated/torch.tensor.lt_#torch.Tensor.lt_", "type": "Tensor", "text": ["In-place version of lt()."]}, {"name": "torch.Tensor.lu()", "path": "generated/torch.tensor.lu#torch.Tensor.lu", "type": "Tensor", "text": ["See torch.lu()"]}, {"name": "torch.Tensor.lu_solve()", "path": "generated/torch.tensor.lu_solve#torch.Tensor.lu_solve", "type": "Tensor", "text": ["See torch.lu_solve()"]}, {"name": "torch.Tensor.map_()", "path": "generated/torch.tensor.map_#torch.Tensor.map_", "type": "Tensor", "text": ["Applies callable for each element in self tensor and the given tensor and stores the results in self tensor. self tensor and the given tensor must be broadcastable.", "The callable should have the signature:"]}, {"name": "torch.Tensor.masked_fill()", "path": "generated/torch.tensor.masked_fill#torch.Tensor.masked_fill", "type": "Tensor", "text": ["Out-of-place version of torch.Tensor.masked_fill_()"]}, {"name": "torch.Tensor.masked_fill_()", "path": "generated/torch.tensor.masked_fill_#torch.Tensor.masked_fill_", "type": "Tensor", "text": ["Fills elements of self tensor with value where mask is True. The shape of mask must be broadcastable with the shape of the underlying tensor."]}, {"name": "torch.Tensor.masked_scatter()", "path": "generated/torch.tensor.masked_scatter#torch.Tensor.masked_scatter", "type": "Tensor", "text": ["Out-of-place version of torch.Tensor.masked_scatter_()", "Note", "The inputs self and mask broadcast."]}, {"name": "torch.Tensor.masked_scatter_()", "path": "generated/torch.tensor.masked_scatter_#torch.Tensor.masked_scatter_", "type": "Tensor", "text": ["Copies elements from source into self tensor at positions where the mask is True. Elements from source are copied into self starting at position 0 of source and continuing in order one-by-one for each occurrence of mask being True. The shape of mask must be broadcastable with the shape of the underlying tensor. The source should have at least as many elements as the number of ones in mask.", "Note", "The mask operates on the self tensor, not on the given source tensor."]}, {"name": "torch.Tensor.masked_select()", "path": "generated/torch.tensor.masked_select#torch.Tensor.masked_select", "type": "Tensor", "text": ["See torch.masked_select()"]}, {"name": "torch.Tensor.matmul()", "path": "generated/torch.tensor.matmul#torch.Tensor.matmul", "type": "Tensor", "text": ["See torch.matmul()"]}, {"name": "torch.Tensor.matrix_exp()", "path": "generated/torch.tensor.matrix_exp#torch.Tensor.matrix_exp", "type": "Tensor", "text": ["See torch.matrix_exp()"]}, {"name": "torch.Tensor.matrix_power()", "path": "generated/torch.tensor.matrix_power#torch.Tensor.matrix_power", "type": "Tensor", "text": ["Note", "matrix_power() is deprecated, use torch.linalg.matrix_power() instead.", "Alias for torch.linalg.matrix_power()"]}, {"name": "torch.Tensor.max()", "path": "generated/torch.tensor.max#torch.Tensor.max", "type": "Tensor", "text": ["See torch.max()"]}, {"name": "torch.Tensor.maximum()", "path": "generated/torch.tensor.maximum#torch.Tensor.maximum", "type": "Tensor", "text": ["See torch.maximum()"]}, {"name": "torch.Tensor.mean()", "path": "generated/torch.tensor.mean#torch.Tensor.mean", "type": "Tensor", "text": ["See torch.mean()"]}, {"name": "torch.Tensor.median()", "path": "generated/torch.tensor.median#torch.Tensor.median", "type": "Tensor", "text": ["See torch.median()"]}, {"name": "torch.Tensor.mH", "path": "tensors#torch.Tensor.mH", "type": "Tensor", "text": ["Accessing this property is equivalent to calling adjoint()."]}, {"name": "torch.Tensor.min()", "path": "generated/torch.tensor.min#torch.Tensor.min", "type": "Tensor", "text": ["See torch.min()"]}, {"name": "torch.Tensor.minimum()", "path": "generated/torch.tensor.minimum#torch.Tensor.minimum", "type": "Tensor", "text": ["See torch.minimum()"]}, {"name": "torch.Tensor.mm()", "path": "generated/torch.tensor.mm#torch.Tensor.mm", "type": "Tensor", "text": ["See torch.mm()"]}, {"name": "torch.Tensor.mode()", "path": "generated/torch.tensor.mode#torch.Tensor.mode", "type": "Tensor", "text": ["See torch.mode()"]}, {"name": "torch.Tensor.moveaxis()", "path": "generated/torch.tensor.moveaxis#torch.Tensor.moveaxis", "type": "Tensor", "text": ["See torch.moveaxis()"]}, {"name": "torch.Tensor.movedim()", "path": "generated/torch.tensor.movedim#torch.Tensor.movedim", "type": "Tensor", "text": ["See torch.movedim()"]}, {"name": "torch.Tensor.msort()", "path": "generated/torch.tensor.msort#torch.Tensor.msort", "type": "Tensor", "text": ["See torch.msort()"]}, {"name": "torch.Tensor.mT", "path": "tensors#torch.Tensor.mT", "type": "Tensor", "text": ["Returns a view of this tensor with the last two dimensions transposed.", "x.mT is equivalent to x.transpose(-2, -1)."]}, {"name": "torch.Tensor.mul()", "path": "generated/torch.tensor.mul#torch.Tensor.mul", "type": "Tensor", "text": ["See torch.mul()."]}, {"name": "torch.Tensor.mul_()", "path": "generated/torch.tensor.mul_#torch.Tensor.mul_", "type": "Tensor", "text": ["In-place version of mul()."]}, {"name": "torch.Tensor.multinomial()", "path": "generated/torch.tensor.multinomial#torch.Tensor.multinomial", "type": "Tensor", "text": ["See torch.multinomial()"]}, {"name": "torch.Tensor.multiply()", "path": "generated/torch.tensor.multiply#torch.Tensor.multiply", "type": "Tensor", "text": ["See torch.multiply()."]}, {"name": "torch.Tensor.multiply_()", "path": "generated/torch.tensor.multiply_#torch.Tensor.multiply_", "type": "Tensor", "text": ["In-place version of multiply()."]}, {"name": "torch.Tensor.mv()", "path": "generated/torch.tensor.mv#torch.Tensor.mv", "type": "Tensor", "text": ["See torch.mv()"]}, {"name": "torch.Tensor.mvlgamma()", "path": "generated/torch.tensor.mvlgamma#torch.Tensor.mvlgamma", "type": "Tensor", "text": ["See torch.mvlgamma()"]}, {"name": "torch.Tensor.mvlgamma_()", "path": "generated/torch.tensor.mvlgamma_#torch.Tensor.mvlgamma_", "type": "Tensor", "text": ["In-place version of mvlgamma()"]}, {"name": "torch.Tensor.names", "path": "named_tensor#torch.Tensor.names", "type": "Miscellaneous", "text": ["Stores names for each of this tensor\u2019s dimensions.", "names[idx] corresponds to the name of tensor dimension idx. Names are either a string if the dimension is named or None if the dimension is unnamed.", "Dimension names may contain characters or underscore. Furthermore, a dimension name must be a valid Python variable name (i.e., does not start with underscore).", "Tensors may not have two named dimensions with the same name.", "Warning", "The named tensor API is experimental and subject to change."]}, {"name": "torch.Tensor.nan_to_num()", "path": "generated/torch.tensor.nan_to_num#torch.Tensor.nan_to_num", "type": "Tensor", "text": ["See torch.nan_to_num()."]}, {"name": "torch.Tensor.nan_to_num_()", "path": "generated/torch.tensor.nan_to_num_#torch.Tensor.nan_to_num_", "type": "Tensor", "text": ["In-place version of nan_to_num()."]}, {"name": "torch.Tensor.nanmean()", "path": "generated/torch.tensor.nanmean#torch.Tensor.nanmean", "type": "Tensor", "text": ["See torch.nanmean()"]}, {"name": "torch.Tensor.nanmedian()", "path": "generated/torch.tensor.nanmedian#torch.Tensor.nanmedian", "type": "Tensor", "text": ["See torch.nanmedian()"]}, {"name": "torch.Tensor.nanquantile()", "path": "generated/torch.tensor.nanquantile#torch.Tensor.nanquantile", "type": "Tensor", "text": ["See torch.nanquantile()"]}, {"name": "torch.Tensor.nansum()", "path": "generated/torch.tensor.nansum#torch.Tensor.nansum", "type": "Tensor", "text": ["See torch.nansum()"]}, {"name": "torch.Tensor.narrow()", "path": "generated/torch.tensor.narrow#torch.Tensor.narrow", "type": "Tensor", "text": ["See torch.narrow()."]}, {"name": "torch.Tensor.narrow_copy()", "path": "generated/torch.tensor.narrow_copy#torch.Tensor.narrow_copy", "type": "Tensor", "text": ["See torch.narrow_copy()."]}, {"name": "torch.Tensor.nbytes", "path": "generated/torch.tensor.nbytes#torch.Tensor.nbytes", "type": "Tensor", "text": ["Returns the number of bytes consumed by the \u201cview\u201d of elements of the Tensor if the Tensor does not use sparse storage layout. Defined to be numel() * element_size()"]}, {"name": "torch.Tensor.ndim", "path": "generated/torch.tensor.ndim#torch.Tensor.ndim", "type": "Tensor", "text": ["Alias for dim()"]}, {"name": "torch.Tensor.ndimension()", "path": "generated/torch.tensor.ndimension#torch.Tensor.ndimension", "type": "Tensor", "text": ["Alias for dim()"]}, {"name": "torch.Tensor.ne()", "path": "generated/torch.tensor.ne#torch.Tensor.ne", "type": "Tensor", "text": ["See torch.ne()."]}, {"name": "torch.Tensor.ne_()", "path": "generated/torch.tensor.ne_#torch.Tensor.ne_", "type": "Tensor", "text": ["In-place version of ne()."]}, {"name": "torch.Tensor.neg()", "path": "generated/torch.tensor.neg#torch.Tensor.neg", "type": "Tensor", "text": ["See torch.neg()"]}, {"name": "torch.Tensor.neg_()", "path": "generated/torch.tensor.neg_#torch.Tensor.neg_", "type": "Tensor", "text": ["In-place version of neg()"]}, {"name": "torch.Tensor.negative()", "path": "generated/torch.tensor.negative#torch.Tensor.negative", "type": "Tensor", "text": ["See torch.negative()"]}, {"name": "torch.Tensor.negative_()", "path": "generated/torch.tensor.negative_#torch.Tensor.negative_", "type": "Tensor", "text": ["In-place version of negative()"]}, {"name": "torch.Tensor.nelement()", "path": "generated/torch.tensor.nelement#torch.Tensor.nelement", "type": "Tensor", "text": ["Alias for numel()"]}, {"name": "torch.Tensor.new_empty()", "path": "generated/torch.tensor.new_empty#torch.Tensor.new_empty", "type": "Tensor", "text": ["Returns a Tensor of size size filled with uninitialized data. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.", "size (int...) \u2013 a list, tuple, or torch.Size of integers defining the shape of the output tensor.", "Example:"]}, {"name": "torch.Tensor.new_full()", "path": "generated/torch.tensor.new_full#torch.Tensor.new_full", "type": "Tensor", "text": ["Returns a Tensor of size size filled with fill_value. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.", "fill_value (scalar) \u2013 the number to fill the output tensor with.", "Example:"]}, {"name": "torch.Tensor.new_ones()", "path": "generated/torch.tensor.new_ones#torch.Tensor.new_ones", "type": "Tensor", "text": ["Returns a Tensor of size size filled with 1. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.", "size (int...) \u2013 a list, tuple, or torch.Size of integers defining the shape of the output tensor.", "Example:"]}, {"name": "torch.Tensor.new_tensor()", "path": "generated/torch.tensor.new_tensor#torch.Tensor.new_tensor", "type": "Tensor", "text": ["Returns a new Tensor with data as the tensor data. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.", "Warning", "new_tensor() always copies data. If you have a Tensor data and want to avoid a copy, use torch.Tensor.requires_grad_() or torch.Tensor.detach(). If you have a numpy array and want to avoid a copy, use torch.from_numpy().", "Warning", "When data is a tensor x, new_tensor() reads out \u2018the data\u2019 from whatever it is passed, and constructs a leaf variable. Therefore tensor.new_tensor(x) is equivalent to x.clone().detach() and tensor.new_tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True). The equivalents using clone() and detach() are recommended.", "data (array_like) \u2013 The returned Tensor copies data.", "Example:"]}, {"name": "torch.Tensor.new_zeros()", "path": "generated/torch.tensor.new_zeros#torch.Tensor.new_zeros", "type": "Tensor", "text": ["Returns a Tensor of size size filled with 0. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.", "size (int...) \u2013 a list, tuple, or torch.Size of integers defining the shape of the output tensor.", "Example:"]}, {"name": "torch.Tensor.nextafter()", "path": "generated/torch.tensor.nextafter#torch.Tensor.nextafter", "type": "Tensor", "text": ["See torch.nextafter()"]}, {"name": "torch.Tensor.nextafter_()", "path": "generated/torch.tensor.nextafter_#torch.Tensor.nextafter_", "type": "Tensor", "text": ["In-place version of nextafter()"]}, {"name": "torch.Tensor.nonzero()", "path": "generated/torch.tensor.nonzero#torch.Tensor.nonzero", "type": "Tensor", "text": ["See torch.nonzero()"]}, {"name": "torch.Tensor.norm()", "path": "generated/torch.tensor.norm#torch.Tensor.norm", "type": "Tensor", "text": ["See torch.norm()"]}, {"name": "torch.Tensor.normal_()", "path": "generated/torch.tensor.normal_#torch.Tensor.normal_", "type": "Tensor", "text": ["Fills self tensor with elements samples from the normal distribution parameterized by mean and std."]}, {"name": "torch.Tensor.not_equal()", "path": "generated/torch.tensor.not_equal#torch.Tensor.not_equal", "type": "Tensor", "text": ["See torch.not_equal()."]}, {"name": "torch.Tensor.not_equal_()", "path": "generated/torch.tensor.not_equal_#torch.Tensor.not_equal_", "type": "Tensor", "text": ["In-place version of not_equal()."]}, {"name": "torch.Tensor.numel()", "path": "generated/torch.tensor.numel#torch.Tensor.numel", "type": "Tensor", "text": ["See torch.numel()"]}, {"name": "torch.Tensor.numpy()", "path": "generated/torch.tensor.numpy#torch.Tensor.numpy", "type": "Tensor", "text": ["Returns the tensor as a NumPy ndarray.", "If force is False (the default), the conversion is performed only if the tensor is on the CPU, does not require grad, does not have its conjugate bit set, and is a dtype and layout that NumPy supports. The returned ndarray and the tensor will share their storage, so changes to the tensor will be reflected in the ndarray and vice versa.", "If force is True this is equivalent to calling t.detach().cpu().resolve_conj().resolve_neg().numpy(). If the tensor isn\u2019t on the CPU or the conjugate or negative bit is set, the tensor won\u2019t share its storage with the returned ndarray. Setting force to True can be a useful shorthand.", "force (bool) \u2013 if True, the ndarray may be a copy of the tensor instead of always sharing memory, defaults to False."]}, {"name": "torch.Tensor.orgqr()", "path": "generated/torch.tensor.orgqr#torch.Tensor.orgqr", "type": "Tensor", "text": ["See torch.orgqr()"]}, {"name": "torch.Tensor.ormqr()", "path": "generated/torch.tensor.ormqr#torch.Tensor.ormqr", "type": "Tensor", "text": ["See torch.ormqr()"]}, {"name": "torch.Tensor.outer()", "path": "generated/torch.tensor.outer#torch.Tensor.outer", "type": "Tensor", "text": ["See torch.outer()."]}, {"name": "torch.Tensor.permute()", "path": "generated/torch.tensor.permute#torch.Tensor.permute", "type": "Tensor", "text": ["See torch.permute()"]}, {"name": "torch.Tensor.pin_memory()", "path": "generated/torch.tensor.pin_memory#torch.Tensor.pin_memory", "type": "Tensor", "text": ["Copies the tensor to pinned memory, if it\u2019s not already pinned."]}, {"name": "torch.Tensor.pinverse()", "path": "generated/torch.tensor.pinverse#torch.Tensor.pinverse", "type": "Tensor", "text": ["See torch.pinverse()"]}, {"name": "torch.Tensor.polygamma()", "path": "generated/torch.tensor.polygamma#torch.Tensor.polygamma", "type": "Tensor", "text": ["See torch.polygamma()"]}, {"name": "torch.Tensor.polygamma_()", "path": "generated/torch.tensor.polygamma_#torch.Tensor.polygamma_", "type": "Tensor", "text": ["In-place version of polygamma()"]}, {"name": "torch.Tensor.positive()", "path": "generated/torch.tensor.positive#torch.Tensor.positive", "type": "Tensor", "text": ["See torch.positive()"]}, {"name": "torch.Tensor.pow()", "path": "generated/torch.tensor.pow#torch.Tensor.pow", "type": "Tensor", "text": ["See torch.pow()"]}, {"name": "torch.Tensor.pow_()", "path": "generated/torch.tensor.pow_#torch.Tensor.pow_", "type": "Tensor", "text": ["In-place version of pow()"]}, {"name": "torch.Tensor.prod()", "path": "generated/torch.tensor.prod#torch.Tensor.prod", "type": "Tensor", "text": ["See torch.prod()"]}, {"name": "torch.Tensor.put_()", "path": "generated/torch.tensor.put_#torch.Tensor.put_", "type": "Tensor", "text": ["Copies the elements from source into the positions specified by index. For the purpose of indexing, the self tensor is treated as if it were a 1-D tensor.", "index and source need to have the same number of elements, but not necessarily the same shape.", "If accumulate is True, the elements in source are added to self. If accumulate is False, the behavior is undefined if index contain duplicate elements.", "Example:"]}, {"name": "torch.Tensor.q_per_channel_axis()", "path": "generated/torch.tensor.q_per_channel_axis#torch.Tensor.q_per_channel_axis", "type": "Tensor", "text": ["Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied."]}, {"name": "torch.Tensor.q_per_channel_scales()", "path": "generated/torch.tensor.q_per_channel_scales#torch.Tensor.q_per_channel_scales", "type": "Tensor", "text": ["Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor."]}, {"name": "torch.Tensor.q_per_channel_zero_points()", "path": "generated/torch.tensor.q_per_channel_zero_points#torch.Tensor.q_per_channel_zero_points", "type": "Tensor", "text": ["Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor."]}, {"name": "torch.Tensor.q_scale()", "path": "generated/torch.tensor.q_scale#torch.Tensor.q_scale", "type": "Tensor", "text": ["Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer()."]}, {"name": "torch.Tensor.q_zero_point()", "path": "generated/torch.tensor.q_zero_point#torch.Tensor.q_zero_point", "type": "Tensor", "text": ["Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer()."]}, {"name": "torch.Tensor.qr()", "path": "generated/torch.tensor.qr#torch.Tensor.qr", "type": "Tensor", "text": ["See torch.qr()"]}, {"name": "torch.Tensor.qscheme()", "path": "generated/torch.tensor.qscheme#torch.Tensor.qscheme", "type": "Tensor", "text": ["Returns the quantization scheme of a given QTensor."]}, {"name": "torch.Tensor.quantile()", "path": "generated/torch.tensor.quantile#torch.Tensor.quantile", "type": "Tensor", "text": ["See torch.quantile()"]}, {"name": "torch.Tensor.rad2deg()", "path": "generated/torch.tensor.rad2deg#torch.Tensor.rad2deg", "type": "Tensor", "text": ["See torch.rad2deg()"]}, {"name": "torch.Tensor.random_()", "path": "generated/torch.tensor.random_#torch.Tensor.random_", "type": "Tensor", "text": ["Fills self tensor with numbers sampled from the discrete uniform distribution over [from, to - 1]. If not specified, the values are usually only bounded by self tensor\u2019s data type. However, for floating point types, if unspecified, range will be [0, 2^mantissa] to ensure that every value is representable. For example, torch.tensor(1, dtype=torch.double).random_() will be uniform in [0, 2^53]."]}, {"name": "torch.Tensor.ravel()", "path": "generated/torch.tensor.ravel#torch.Tensor.ravel", "type": "Tensor", "text": ["see torch.ravel()"]}, {"name": "torch.Tensor.real", "path": "generated/torch.tensor.real#torch.Tensor.real", "type": "Tensor", "text": ["Returns a new tensor containing real values of the self tensor for a complex-valued input tensor. The returned tensor and self share the same underlying storage.", "Returns self if self is a real-valued tensor tensor."]}, {"name": "torch.Tensor.reciprocal()", "path": "generated/torch.tensor.reciprocal#torch.Tensor.reciprocal", "type": "Tensor", "text": ["See torch.reciprocal()"]}, {"name": "torch.Tensor.reciprocal_()", "path": "generated/torch.tensor.reciprocal_#torch.Tensor.reciprocal_", "type": "Tensor", "text": ["In-place version of reciprocal()"]}, {"name": "torch.Tensor.record_stream()", "path": "generated/torch.tensor.record_stream#torch.Tensor.record_stream", "type": "Tensor", "text": ["Ensures that the tensor memory is not reused for another tensor until all current work queued on stream are complete.", "Note", "The caching allocator is aware of only the stream where a tensor was allocated. Due to the awareness, it already correctly manages the life cycle of tensors on only one stream. But if a tensor is used on a stream different from the stream of origin, the allocator might reuse the memory unexpectedly. Calling this method lets the allocator know which streams have used the tensor."]}, {"name": "torch.Tensor.refine_names()", "path": "named_tensor#torch.Tensor.refine_names", "type": "Miscellaneous", "text": ["Refines the dimension names of self according to names.", "Refining is a special case of renaming that \u201clifts\u201d unnamed dimensions. A None dim can be refined to have any name; a named dim can only be refined to have the same name.", "Because named tensors can coexist with unnamed tensors, refining names gives a nice way to write named-tensor-aware code that works with both named and unnamed tensors.", "names may contain up to one Ellipsis (...). The Ellipsis is expanded greedily; it is expanded in-place to fill names to the same length as self.dim() using names from the corresponding indices of self.names.", "Python 2 does not support Ellipsis but one may use a string literal instead ('...').", "names (iterable of str) \u2013 The desired names of the output tensor. May contain up to one Ellipsis.", "Examples:", "Warning", "The named tensor API is experimental and subject to change."]}, {"name": "torch.Tensor.register_hook()", "path": "generated/torch.tensor.register_hook#torch.Tensor.register_hook", "type": "Tensor", "text": ["Registers a backward hook.", "The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature:", "The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of grad.", "This function returns a handle with a method handle.remove() that removes the hook from the module.", "Note", "See Backward Hooks execution for more information on how when this hook is executed, and how its execution is ordered relative to other hooks.", "Example:"]}, {"name": "torch.Tensor.register_post_accumulate_grad_hook()", "path": "generated/torch.tensor.register_post_accumulate_grad_hook#torch.Tensor.register_post_accumulate_grad_hook", "type": "Tensor", "text": ["Registers a backward hook that runs after grad accumulation.", "The hook will be called after all gradients for a tensor have been accumulated, meaning that the .grad field has been updated on that tensor. The post accumulate grad hook is ONLY applicable for leaf tensors (tensors without a .grad_fn field). Registering this hook on a non-leaf tensor will error!", "The hook should have the following signature:", "Note that, unlike other autograd hooks, this hook operates on the tensor that requires grad and not the grad itself. The hook can in-place modify and access its Tensor argument, including its .grad field.", "This function returns a handle with a method handle.remove() that removes the hook from the module.", "Note", "See Backward Hooks execution for more information on how when this hook is executed, and how its execution is ordered relative to other hooks. Since this hook runs during the backward pass, it will run in no_grad mode (unless create_graph is True). You can use torch.enable_grad() to re-enable autograd within the hook if you need it.", "Example:"]}, {"name": "torch.Tensor.remainder()", "path": "generated/torch.tensor.remainder#torch.Tensor.remainder", "type": "Tensor", "text": ["See torch.remainder()"]}, {"name": "torch.Tensor.remainder_()", "path": "generated/torch.tensor.remainder_#torch.Tensor.remainder_", "type": "Tensor", "text": ["In-place version of remainder()"]}, {"name": "torch.Tensor.rename()", "path": "named_tensor#torch.Tensor.rename", "type": "Miscellaneous", "text": ["Renames dimension names of self.", "There are two main usages:", "self.rename(**rename_map) returns a view on tensor that has dims renamed as specified in the mapping rename_map.", "self.rename(*names) returns a view on tensor, renaming all dimensions positionally using names. Use self.rename(None) to drop names on a tensor.", "One cannot specify both positional args names and keyword args rename_map.", "Examples:", "Warning", "The named tensor API is experimental and subject to change."]}, {"name": "torch.Tensor.rename_()", "path": "named_tensor#torch.Tensor.rename_", "type": "Miscellaneous", "text": ["In-place version of rename()."]}, {"name": "torch.Tensor.renorm()", "path": "generated/torch.tensor.renorm#torch.Tensor.renorm", "type": "Tensor", "text": ["See torch.renorm()"]}, {"name": "torch.Tensor.renorm_()", "path": "generated/torch.tensor.renorm_#torch.Tensor.renorm_", "type": "Tensor", "text": ["In-place version of renorm()"]}, {"name": "torch.Tensor.repeat()", "path": "generated/torch.tensor.repeat#torch.Tensor.repeat", "type": "Tensor", "text": ["Repeats this tensor along the specified dimensions.", "Unlike expand(), this function copies the tensor\u2019s data.", "Warning", "repeat() behaves differently from numpy.repeat, but is more similar to numpy.tile. For the operator similar to numpy.repeat, see torch.repeat_interleave().", "sizes (torch.Size or int...) \u2013 The number of times to repeat this tensor along each dimension", "Example:"]}, {"name": "torch.Tensor.repeat_interleave()", "path": "generated/torch.tensor.repeat_interleave#torch.Tensor.repeat_interleave", "type": "Tensor", "text": ["See torch.repeat_interleave()."]}, {"name": "torch.Tensor.requires_grad", "path": "generated/torch.tensor.requires_grad#torch.Tensor.requires_grad", "type": "Tensor", "text": ["Is True if gradients need to be computed for this Tensor, False otherwise.", "Note", "The fact that gradients need to be computed for a Tensor do not mean that the grad attribute will be populated, see is_leaf for more details."]}, {"name": "torch.Tensor.requires_grad_()", "path": "generated/torch.tensor.requires_grad_#torch.Tensor.requires_grad_", "type": "Tensor", "text": ["Change if autograd should record operations on this tensor: sets this tensor\u2019s requires_grad attribute in-place. Returns this tensor.", "requires_grad_()\u2019s main use case is to tell autograd to begin recording operations on a Tensor tensor. If tensor has requires_grad=False (because it was obtained through a DataLoader, or required preprocessing or initialization), tensor.requires_grad_() makes it so that autograd will begin to record operations on tensor.", "requires_grad (bool) \u2013 If autograd should record operations on this tensor. Default: True.", "Example:"]}, {"name": "torch.Tensor.reshape()", "path": "generated/torch.tensor.reshape#torch.Tensor.reshape", "type": "Tensor", "text": ["Returns a tensor with the same data and number of elements as self but with the specified shape. This method returns a view if shape is compatible with the current shape. See torch.Tensor.view() on when it is possible to return a view.", "See torch.reshape()", "shape (tuple of ints or int...) \u2013 the desired shape"]}, {"name": "torch.Tensor.reshape_as()", "path": "generated/torch.tensor.reshape_as#torch.Tensor.reshape_as", "type": "Tensor", "text": ["Returns this tensor as the same shape as other. self.reshape_as(other) is equivalent to self.reshape(other.sizes()). This method returns a view if other.sizes() is compatible with the current shape. See torch.Tensor.view() on when it is possible to return a view.", "Please see reshape() for more information about reshape.", "other (torch.Tensor) \u2013 The result tensor has the same shape as other."]}, {"name": "torch.Tensor.resize_()", "path": "generated/torch.tensor.resize_#torch.Tensor.resize_", "type": "Tensor", "text": ["Resizes self tensor to the specified size. If the number of elements is larger than the current storage size, then the underlying storage is resized to fit the new number of elements. If the number of elements is smaller, the underlying storage is not changed. Existing elements are preserved but any new memory is uninitialized.", "Warning", "This is a low-level method. The storage is reinterpreted as C-contiguous, ignoring the current strides (unless the target size equals the current size, in which case the tensor is left unchanged). For most purposes, you will instead want to use view(), which checks for contiguity, or reshape(), which copies data if needed. To change the size in-place with custom strides, see set_().", "Note", "If torch.use_deterministic_algorithms() is set to True, new elements are initialized to prevent nondeterministic behavior from using the result as an input to an operation. Floating point and complex values are set to NaN, and integer values are set to the maximum value.", "Example:"]}, {"name": "torch.Tensor.resize_as_()", "path": "generated/torch.tensor.resize_as_#torch.Tensor.resize_as_", "type": "Tensor", "text": ["Resizes the self tensor to be the same size as the specified tensor. This is equivalent to self.resize_(tensor.size()).", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of Tensor. Default: torch.contiguous_format. Note that memory format of self is going to be unaffected if self.size() matches tensor.size()."]}, {"name": "torch.Tensor.resolve_conj()", "path": "generated/torch.tensor.resolve_conj#torch.Tensor.resolve_conj", "type": "Tensor", "text": ["See torch.resolve_conj()"]}, {"name": "torch.Tensor.resolve_neg()", "path": "generated/torch.tensor.resolve_neg#torch.Tensor.resolve_neg", "type": "Tensor", "text": ["See torch.resolve_neg()"]}, {"name": "torch.Tensor.retain_grad()", "path": "generated/torch.tensor.retain_grad#torch.Tensor.retain_grad", "type": "Tensor", "text": ["Enables this Tensor to have their grad populated during backward(). This is a no-op for leaf tensors."]}, {"name": "torch.Tensor.retains_grad", "path": "generated/torch.tensor.retains_grad#torch.Tensor.retains_grad", "type": "Tensor", "text": ["Is True if this Tensor is non-leaf and its grad is enabled to be populated during backward(), False otherwise."]}, {"name": "torch.Tensor.roll()", "path": "generated/torch.tensor.roll#torch.Tensor.roll", "type": "Tensor", "text": ["See torch.roll()"]}, {"name": "torch.Tensor.rot90()", "path": "generated/torch.tensor.rot90#torch.Tensor.rot90", "type": "Tensor", "text": ["See torch.rot90()"]}, {"name": "torch.Tensor.round()", "path": "generated/torch.tensor.round#torch.Tensor.round", "type": "Tensor", "text": ["See torch.round()"]}, {"name": "torch.Tensor.round_()", "path": "generated/torch.tensor.round_#torch.Tensor.round_", "type": "Tensor", "text": ["In-place version of round()"]}, {"name": "torch.Tensor.row_indices()", "path": "generated/torch.tensor.row_indices#torch.Tensor.row_indices", "type": "Sparse Tensors", "text": []}, {"name": "torch.Tensor.rsqrt()", "path": "generated/torch.tensor.rsqrt#torch.Tensor.rsqrt", "type": "Tensor", "text": ["See torch.rsqrt()"]}, {"name": "torch.Tensor.rsqrt_()", "path": "generated/torch.tensor.rsqrt_#torch.Tensor.rsqrt_", "type": "Tensor", "text": ["In-place version of rsqrt()"]}, {"name": "torch.Tensor.scatter()", "path": "generated/torch.tensor.scatter#torch.Tensor.scatter", "type": "Tensor", "text": ["Out-of-place version of torch.Tensor.scatter_()"]}, {"name": "torch.Tensor.scatter_()", "path": "generated/torch.tensor.scatter_#torch.Tensor.scatter_", "type": "Tensor", "text": ["Writes all values from the tensor src into self at the indices specified in the index tensor. For each value in src, its output index is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim.", "For a 3-D tensor, self is updated as:", "This is the reverse operation of the manner described in gather().", "self, index and src (if it is a Tensor) should all have the same number of dimensions. It is also required that index.size(d) <= src.size(d) for all dimensions d, and that index.size(d) <= self.size(d) for all dimensions d != dim. Note that index and src do not broadcast.", "Moreover, as for gather(), the values of index must be between 0 and self.size(dim) - 1 inclusive.", "Warning", "When indices are not unique, the behavior is non-deterministic (one of the values from src will be picked arbitrarily) and the gradient will be incorrect (it will be propagated to all locations in the source that correspond to the same index)!", "Note", "The backward pass is implemented only for src.shape == index.shape.", "Additionally accepts an optional reduce argument that allows specification of an optional reduction operation, which is applied to all values in the tensor src into self at the indices specified in the index. For each value in src, the reduction operation is applied to an index in self which is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim.", "Given a 3-D tensor and reduction using the multiplication operation, self is updated as:", "Reducing with the addition operation is the same as using scatter_add_().", "Warning", "The reduce argument with Tensor src is deprecated and will be removed in a future PyTorch release. Please use scatter_reduce_() instead for more reduction options.", "Example:"]}, {"name": "torch.Tensor.scatter_add()", "path": "generated/torch.tensor.scatter_add#torch.Tensor.scatter_add", "type": "Tensor", "text": ["Out-of-place version of torch.Tensor.scatter_add_()"]}, {"name": "torch.Tensor.scatter_add_()", "path": "generated/torch.tensor.scatter_add_#torch.Tensor.scatter_add_", "type": "Tensor", "text": ["Adds all values from the tensor src into self at the indices specified in the index tensor in a similar fashion as scatter_(). For each value in src, it is added to an index in self which is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim.", "For a 3-D tensor, self is updated as:", "self, index and src should have same number of dimensions. It is also required that index.size(d) <= src.size(d) for all dimensions d, and that index.size(d) <= self.size(d) for all dimensions d != dim. Note that index and src do not broadcast.", "Note", "This operation may behave nondeterministically when given tensors on a CUDA device. See Reproducibility for more information.", "Note", "The backward pass is implemented only for src.shape == index.shape.", "Example:"]}, {"name": "torch.Tensor.scatter_reduce()", "path": "generated/torch.tensor.scatter_reduce#torch.Tensor.scatter_reduce", "type": "Tensor", "text": ["Out-of-place version of torch.Tensor.scatter_reduce_()"]}, {"name": "torch.Tensor.scatter_reduce_()", "path": "generated/torch.tensor.scatter_reduce_#torch.Tensor.scatter_reduce_", "type": "Tensor", "text": ["Reduces all values from the src tensor to the indices specified in the index tensor in the self tensor using the applied reduction defined via the reduce argument (\"sum\", \"prod\", \"mean\", \"amax\", \"amin\"). For each value in src, it is reduced to an index in self which is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim. If include_self=\"True\", the values in the self tensor are included in the reduction.", "self, index and src should all have the same number of dimensions. It is also required that index.size(d) <= src.size(d) for all dimensions d, and that index.size(d) <= self.size(d) for all dimensions d != dim. Note that index and src do not broadcast.", "For a 3-D tensor with reduce=\"sum\" and include_self=True the output is given as:", "Note", "This operation may behave nondeterministically when given tensors on a CUDA device. See Reproducibility for more information.", "Note", "The backward pass is implemented only for src.shape == index.shape.", "Warning", "This function is in beta and may change in the near future.", "Example:"]}, {"name": "torch.Tensor.select()", "path": "generated/torch.tensor.select#torch.Tensor.select", "type": "Tensor", "text": ["See torch.select()"]}, {"name": "torch.Tensor.select_scatter()", "path": "generated/torch.tensor.select_scatter#torch.Tensor.select_scatter", "type": "Tensor", "text": ["See torch.select_scatter()"]}, {"name": "torch.Tensor.set_()", "path": "generated/torch.tensor.set_#torch.Tensor.set_", "type": "Tensor", "text": ["Sets the underlying storage, size, and strides. If source is a tensor, self tensor will share the same storage and have the same size and strides as source. Changes to elements in one tensor will be reflected in the other.", "If source is a Storage, the method sets the underlying storage, offset, size, and stride."]}, {"name": "torch.Tensor.sgn()", "path": "generated/torch.tensor.sgn#torch.Tensor.sgn", "type": "Tensor", "text": ["See torch.sgn()"]}, {"name": "torch.Tensor.sgn_()", "path": "generated/torch.tensor.sgn_#torch.Tensor.sgn_", "type": "Tensor", "text": ["In-place version of sgn()"]}, {"name": "torch.Tensor.shape", "path": "generated/torch.tensor.shape#torch.Tensor.shape", "type": "Tensor", "text": ["Returns the size of the self tensor. Alias for size.", "See also Tensor.size().", "Example:"]}, {"name": "torch.Tensor.share_memory_()", "path": "generated/torch.tensor.share_memory_#torch.Tensor.share_memory_", "type": "Tensor", "text": ["Moves the underlying storage to shared memory.", "This is a no-op if the underlying storage is already in shared memory and for CUDA tensors. Tensors in shared memory cannot be resized."]}, {"name": "torch.Tensor.short()", "path": "generated/torch.tensor.short#torch.Tensor.short", "type": "Tensor", "text": ["self.short() is equivalent to self.to(torch.int16). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.sigmoid()", "path": "generated/torch.tensor.sigmoid#torch.Tensor.sigmoid", "type": "Tensor", "text": ["See torch.sigmoid()"]}, {"name": "torch.Tensor.sigmoid_()", "path": "generated/torch.tensor.sigmoid_#torch.Tensor.sigmoid_", "type": "Tensor", "text": ["In-place version of sigmoid()"]}, {"name": "torch.Tensor.sign()", "path": "generated/torch.tensor.sign#torch.Tensor.sign", "type": "Tensor", "text": ["See torch.sign()"]}, {"name": "torch.Tensor.sign_()", "path": "generated/torch.tensor.sign_#torch.Tensor.sign_", "type": "Tensor", "text": ["In-place version of sign()"]}, {"name": "torch.Tensor.signbit()", "path": "generated/torch.tensor.signbit#torch.Tensor.signbit", "type": "Tensor", "text": ["See torch.signbit()"]}, {"name": "torch.Tensor.sin()", "path": "generated/torch.tensor.sin#torch.Tensor.sin", "type": "Tensor", "text": ["See torch.sin()"]}, {"name": "torch.Tensor.sin_()", "path": "generated/torch.tensor.sin_#torch.Tensor.sin_", "type": "Tensor", "text": ["In-place version of sin()"]}, {"name": "torch.Tensor.sinc()", "path": "generated/torch.tensor.sinc#torch.Tensor.sinc", "type": "Tensor", "text": ["See torch.sinc()"]}, {"name": "torch.Tensor.sinc_()", "path": "generated/torch.tensor.sinc_#torch.Tensor.sinc_", "type": "Tensor", "text": ["In-place version of sinc()"]}, {"name": "torch.Tensor.sinh()", "path": "generated/torch.tensor.sinh#torch.Tensor.sinh", "type": "Tensor", "text": ["See torch.sinh()"]}, {"name": "torch.Tensor.sinh_()", "path": "generated/torch.tensor.sinh_#torch.Tensor.sinh_", "type": "Tensor", "text": ["In-place version of sinh()"]}, {"name": "torch.Tensor.size()", "path": "generated/torch.tensor.size#torch.Tensor.size", "type": "Tensor", "text": ["Returns the size of the self tensor. If dim is not specified, the returned value is a torch.Size, a subclass of tuple. If dim is specified, returns an int holding the size of that dimension.", "dim (int, optional) \u2013 The dimension for which to retrieve the size.", "Example:"]}, {"name": "torch.Tensor.slice_scatter()", "path": "generated/torch.tensor.slice_scatter#torch.Tensor.slice_scatter", "type": "Tensor", "text": ["See torch.slice_scatter()"]}, {"name": "torch.Tensor.slogdet()", "path": "generated/torch.tensor.slogdet#torch.Tensor.slogdet", "type": "Tensor", "text": ["See torch.slogdet()"]}, {"name": "torch.Tensor.smm()", "path": "generated/torch.tensor.smm#torch.Tensor.smm", "type": "Tensor", "text": ["See torch.smm()"]}, {"name": "torch.Tensor.softmax()", "path": "generated/torch.tensor.softmax#torch.Tensor.softmax", "type": "Tensor", "text": ["Alias for torch.nn.functional.softmax()."]}, {"name": "torch.Tensor.sort()", "path": "generated/torch.tensor.sort#torch.Tensor.sort", "type": "Tensor", "text": ["See torch.sort()"]}, {"name": "torch.Tensor.sparse_dim()", "path": "generated/torch.tensor.sparse_dim#torch.Tensor.sparse_dim", "type": "Tensor", "text": ["Return the number of sparse dimensions in a sparse tensor self.", "Note", "Returns 0 if self is not a sparse tensor.", "See also Tensor.dense_dim() and hybrid tensors."]}, {"name": "torch.Tensor.sparse_mask()", "path": "generated/torch.tensor.sparse_mask#torch.Tensor.sparse_mask", "type": "Tensor", "text": ["Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. The values of mask sparse tensor are ignored. self and mask tensors must have the same shape.", "Note", "The returned sparse tensor might contain duplicate values if mask is not coalesced. It is therefore advisable to pass mask.coalesce() if such behavior is not desired.", "Note", "The returned sparse tensor has the same indices as the sparse tensor mask, even when the corresponding values in self are zeros.", "mask (Tensor) \u2013 a sparse tensor whose indices are used as a filter", "Example:"]}, {"name": "torch.Tensor.sparse_resize_()", "path": "generated/torch.tensor.sparse_resize_#torch.Tensor.sparse_resize_", "type": "Sparse Tensors", "text": ["Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions.", "Note", "If the number of specified elements in self is zero, then size, sparse_dim, and dense_dim can be any size and positive integers such that len(size) == sparse_dim +\ndense_dim.", "If self specifies one or more elements, however, then each dimension in size must not be smaller than the corresponding dimension of self, sparse_dim must equal the number of sparse dimensions in self, and dense_dim must equal the number of dense dimensions in self.", "Warning", "Throws an error if self is not a sparse tensor."]}, {"name": "torch.Tensor.sparse_resize_and_clear_()", "path": "generated/torch.tensor.sparse_resize_and_clear_#torch.Tensor.sparse_resize_and_clear_", "type": "Sparse Tensors", "text": ["Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions."]}, {"name": "torch.Tensor.split()", "path": "generated/torch.tensor.split#torch.Tensor.split", "type": "Tensor", "text": ["See torch.split()"]}, {"name": "torch.Tensor.sqrt()", "path": "generated/torch.tensor.sqrt#torch.Tensor.sqrt", "type": "Tensor", "text": ["See torch.sqrt()"]}, {"name": "torch.Tensor.sqrt_()", "path": "generated/torch.tensor.sqrt_#torch.Tensor.sqrt_", "type": "Tensor", "text": ["In-place version of sqrt()"]}, {"name": "torch.Tensor.square()", "path": "generated/torch.tensor.square#torch.Tensor.square", "type": "Tensor", "text": ["See torch.square()"]}, {"name": "torch.Tensor.square_()", "path": "generated/torch.tensor.square_#torch.Tensor.square_", "type": "Tensor", "text": ["In-place version of square()"]}, {"name": "torch.Tensor.squeeze()", "path": "generated/torch.tensor.squeeze#torch.Tensor.squeeze", "type": "Tensor", "text": ["See torch.squeeze()"]}, {"name": "torch.Tensor.squeeze_()", "path": "generated/torch.tensor.squeeze_#torch.Tensor.squeeze_", "type": "Tensor", "text": ["In-place version of squeeze()"]}, {"name": "torch.Tensor.sspaddmm()", "path": "generated/torch.tensor.sspaddmm#torch.Tensor.sspaddmm", "type": "Tensor", "text": ["See torch.sspaddmm()"]}, {"name": "torch.Tensor.std()", "path": "generated/torch.tensor.std#torch.Tensor.std", "type": "Tensor", "text": ["See torch.std()"]}, {"name": "torch.Tensor.stft()", "path": "generated/torch.tensor.stft#torch.Tensor.stft", "type": "Tensor", "text": ["See torch.stft()", "Warning", "This function changed signature at version 0.4.1. Calling with the previous signature may cause error or return incorrect result."]}, {"name": "torch.Tensor.storage()", "path": "generated/torch.tensor.storage#torch.Tensor.storage", "type": "Tensor", "text": ["Returns the underlying TypedStorage.", "Warning", "TypedStorage is deprecated. It will be removed in the future, and UntypedStorage will be the only storage class. To access the UntypedStorage directly, use Tensor.untyped_storage()."]}, {"name": "torch.Tensor.storage_offset()", "path": "generated/torch.tensor.storage_offset#torch.Tensor.storage_offset", "type": "Tensor", "text": ["Returns self tensor\u2019s offset in the underlying storage in terms of number of storage elements (not bytes).", "Example:"]}, {"name": "torch.Tensor.storage_type()", "path": "generated/torch.tensor.storage_type#torch.Tensor.storage_type", "type": "Tensor", "text": ["Returns the type of the underlying storage."]}, {"name": "torch.Tensor.stride()", "path": "generated/torch.tensor.stride#torch.Tensor.stride", "type": "Tensor", "text": ["Returns the stride of self tensor.", "Stride is the jump necessary to go from one element to the next one in the specified dimension dim. A tuple of all strides is returned when no argument is passed in. Otherwise, an integer value is returned as the stride in the particular dimension dim.", "dim (int, optional) \u2013 the desired dimension in which stride is required", "Example:"]}, {"name": "torch.Tensor.sub()", "path": "generated/torch.tensor.sub#torch.Tensor.sub", "type": "Tensor", "text": ["See torch.sub()."]}, {"name": "torch.Tensor.sub_()", "path": "generated/torch.tensor.sub_#torch.Tensor.sub_", "type": "Tensor", "text": ["In-place version of sub()"]}, {"name": "torch.Tensor.subtract()", "path": "generated/torch.tensor.subtract#torch.Tensor.subtract", "type": "Tensor", "text": ["See torch.subtract()."]}, {"name": "torch.Tensor.subtract_()", "path": "generated/torch.tensor.subtract_#torch.Tensor.subtract_", "type": "Tensor", "text": ["In-place version of subtract()."]}, {"name": "torch.Tensor.sum()", "path": "generated/torch.tensor.sum#torch.Tensor.sum", "type": "Tensor", "text": ["See torch.sum()"]}, {"name": "torch.Tensor.sum_to_size()", "path": "generated/torch.tensor.sum_to_size#torch.Tensor.sum_to_size", "type": "Tensor", "text": ["Sum this tensor to size. size must be broadcastable to this tensor size.", "size (int...) \u2013 a sequence of integers defining the shape of the output tensor."]}, {"name": "torch.Tensor.svd()", "path": "generated/torch.tensor.svd#torch.Tensor.svd", "type": "Tensor", "text": ["See torch.svd()"]}, {"name": "torch.Tensor.swapaxes()", "path": "generated/torch.tensor.swapaxes#torch.Tensor.swapaxes", "type": "Tensor", "text": ["See torch.swapaxes()"]}, {"name": "torch.Tensor.swapdims()", "path": "generated/torch.tensor.swapdims#torch.Tensor.swapdims", "type": "Tensor", "text": ["See torch.swapdims()"]}, {"name": "torch.Tensor.T", "path": "tensors#torch.Tensor.T", "type": "Tensor", "text": ["Returns a view of this tensor with its dimensions reversed.", "If n is the number of dimensions in x, x.T is equivalent to x.permute(n-1, n-2, ..., 0).", "Warning", "The use of Tensor.T() on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider mT to transpose batches of matrices or x.permute(*torch.arange(x.ndim - 1, -1, -1)) to reverse the dimensions of a tensor."]}, {"name": "torch.Tensor.t()", "path": "generated/torch.tensor.t#torch.Tensor.t", "type": "Tensor", "text": ["See torch.t()"]}, {"name": "torch.Tensor.t_()", "path": "generated/torch.tensor.t_#torch.Tensor.t_", "type": "Tensor", "text": ["In-place version of t()"]}, {"name": "torch.Tensor.take()", "path": "generated/torch.tensor.take#torch.Tensor.take", "type": "Tensor", "text": ["See torch.take()"]}, {"name": "torch.Tensor.take_along_dim()", "path": "generated/torch.tensor.take_along_dim#torch.Tensor.take_along_dim", "type": "Tensor", "text": ["See torch.take_along_dim()"]}, {"name": "torch.Tensor.tan()", "path": "generated/torch.tensor.tan#torch.Tensor.tan", "type": "Tensor", "text": ["See torch.tan()"]}, {"name": "torch.Tensor.tan_()", "path": "generated/torch.tensor.tan_#torch.Tensor.tan_", "type": "Tensor", "text": ["In-place version of tan()"]}, {"name": "torch.Tensor.tanh()", "path": "generated/torch.tensor.tanh#torch.Tensor.tanh", "type": "Tensor", "text": ["See torch.tanh()"]}, {"name": "torch.Tensor.tanh_()", "path": "generated/torch.tensor.tanh_#torch.Tensor.tanh_", "type": "Tensor", "text": ["In-place version of tanh()"]}, {"name": "torch.Tensor.tensor_split()", "path": "generated/torch.tensor.tensor_split#torch.Tensor.tensor_split", "type": "Tensor", "text": ["See torch.tensor_split()"]}, {"name": "torch.Tensor.tile()", "path": "generated/torch.tensor.tile#torch.Tensor.tile", "type": "Tensor", "text": ["See torch.tile()"]}, {"name": "torch.Tensor.to()", "path": "generated/torch.tensor.to#torch.Tensor.to", "type": "Tensor", "text": ["Performs Tensor dtype and/or device conversion. A torch.dtype and torch.device are inferred from the arguments of self.to(*args, **kwargs).", "Note", "If the self Tensor already has the correct torch.dtype and torch.device, then self is returned. Otherwise, the returned tensor is a copy of self with the desired torch.dtype and torch.device.", "Here are the ways to call to:", "Returns a Tensor with the specified dtype", "memory_format (torch.memory_format, optional): the desired memory format of returned Tensor. Default: torch.preserve_format.", "Returns a Tensor with the specified device and (optional) dtype. If dtype is None it is inferred to be self.dtype. When non_blocking, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When copy is set, a new Tensor is created even when the Tensor already matches the desired conversion.", "memory_format (torch.memory_format, optional): the desired memory format of returned Tensor. Default: torch.preserve_format.", "Returns a Tensor with same torch.dtype and torch.device as the Tensor other. When non_blocking, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When copy is set, a new Tensor is created even when the Tensor already matches the desired conversion.", "Example:"]}, {"name": "torch.Tensor.to_dense()", "path": "generated/torch.tensor.to_dense#torch.Tensor.to_dense", "type": "Tensor", "text": ["Creates a strided copy of self if self is not a strided tensor, otherwise returns self.", "Example:"]}, {"name": "torch.Tensor.to_mkldnn()", "path": "generated/torch.tensor.to_mkldnn#torch.Tensor.to_mkldnn", "type": "Tensor", "text": ["Returns a copy of the tensor in torch.mkldnn layout."]}, {"name": "torch.Tensor.to_sparse()", "path": "generated/torch.tensor.to_sparse#torch.Tensor.to_sparse", "type": "Tensor", "text": ["Returns a sparse copy of the tensor. PyTorch supports sparse tensors in coordinate format.", "sparseDims (int, optional) \u2013 the number of sparse dimensions to include in the new sparse tensor", "Example:", "Returns a sparse tensor with the specified layout and blocksize. If the self is strided, the number of dense dimensions could be specified, and a hybrid sparse tensor will be created, with dense_dim dense dimensions and self.dim() - 2 - dense_dim batch dimension.", "Note", "If the self layout and blocksize parameters match with the specified layout and blocksize, return self. Otherwise, return a sparse tensor copy of self.", "Example:"]}, {"name": "torch.Tensor.to_sparse_bsc()", "path": "generated/torch.tensor.to_sparse_bsc#torch.Tensor.to_sparse_bsc", "type": "Tensor", "text": ["Convert a tensor to a block sparse column (BSC) storage format of given blocksize. If the self is strided, then the number of dense dimensions could be specified, and a hybrid BSC tensor will be created, with dense_dim dense dimensions and self.dim() - 2 - dense_dim batch dimension.", "Example:"]}, {"name": "torch.Tensor.to_sparse_bsr()", "path": "generated/torch.tensor.to_sparse_bsr#torch.Tensor.to_sparse_bsr", "type": "Tensor", "text": ["Convert a tensor to a block sparse row (BSR) storage format of given blocksize. If the self is strided, then the number of dense dimensions could be specified, and a hybrid BSR tensor will be created, with dense_dim dense dimensions and self.dim() - 2 - dense_dim batch dimension.", "Example:"]}, {"name": "torch.Tensor.to_sparse_coo()", "path": "generated/torch.tensor.to_sparse_coo#torch.Tensor.to_sparse_coo", "type": "Sparse Tensors", "text": ["Convert a tensor to coordinate format.", "Examples:"]}, {"name": "torch.Tensor.to_sparse_csc()", "path": "generated/torch.tensor.to_sparse_csc#torch.Tensor.to_sparse_csc", "type": "Tensor", "text": ["Convert a tensor to compressed column storage (CSC) format. Except for strided tensors, only works with 2D tensors. If the self is strided, then the number of dense dimensions could be specified, and a hybrid CSC tensor will be created, with dense_dim dense dimensions and self.dim() - 2 - dense_dim batch dimension.", "dense_dim (int, optional) \u2013 Number of dense dimensions of the resulting CSC tensor. This argument should be used only if self is a strided tensor, and must be a value between 0 and dimension of self tensor minus two.", "Example:"]}, {"name": "torch.Tensor.to_sparse_csr()", "path": "generated/torch.tensor.to_sparse_csr#torch.Tensor.to_sparse_csr", "type": "Tensor", "text": ["Convert a tensor to compressed row storage format (CSR). Except for strided tensors, only works with 2D tensors. If the self is strided, then the number of dense dimensions could be specified, and a hybrid CSR tensor will be created, with dense_dim dense dimensions and self.dim() - 2 - dense_dim batch dimension.", "dense_dim (int, optional) \u2013 Number of dense dimensions of the resulting CSR tensor. This argument should be used only if self is a strided tensor, and must be a value between 0 and dimension of self tensor minus two.", "Example:"]}, {"name": "torch.Tensor.tolist()", "path": "generated/torch.tensor.tolist#torch.Tensor.tolist", "type": "Tensor", "text": ["Returns the tensor as a (nested) list. For scalars, a standard Python number is returned, just like with item(). Tensors are automatically moved to the CPU first if necessary.", "This operation is not differentiable.", "Examples:"]}, {"name": "torch.Tensor.topk()", "path": "generated/torch.tensor.topk#torch.Tensor.topk", "type": "Tensor", "text": ["See torch.topk()"]}, {"name": "torch.Tensor.torch.Tensor.abs", "path": "generated/torch.tensor.abs", "type": "Tensor", "text": ["See torch.abs()"]}, {"name": "torch.Tensor.torch.Tensor.abs_", "path": "generated/torch.tensor.abs_", "type": "Tensor", "text": ["In-place version of abs()"]}, {"name": "torch.Tensor.torch.Tensor.absolute", "path": "generated/torch.tensor.absolute", "type": "Tensor", "text": ["Alias for abs()"]}, {"name": "torch.Tensor.torch.Tensor.absolute_", "path": "generated/torch.tensor.absolute_", "type": "Tensor", "text": ["In-place version of absolute() Alias for abs_()"]}, {"name": "torch.Tensor.torch.Tensor.acos", "path": "generated/torch.tensor.acos", "type": "Tensor", "text": ["See torch.acos()"]}, {"name": "torch.Tensor.torch.Tensor.acos_", "path": "generated/torch.tensor.acos_", "type": "Tensor", "text": ["In-place version of acos()"]}, {"name": "torch.Tensor.torch.Tensor.acosh", "path": "generated/torch.tensor.acosh", "type": "Tensor", "text": ["See torch.acosh()"]}, {"name": "torch.Tensor.torch.Tensor.acosh_", "path": "generated/torch.tensor.acosh_", "type": "Tensor", "text": ["In-place version of acosh()"]}, {"name": "torch.Tensor.torch.Tensor.add", "path": "generated/torch.tensor.add", "type": "Tensor", "text": ["Add a scalar or tensor to self tensor. If both alpha and other are specified, each element of other is scaled by alpha before being used.", "When other is a tensor, the shape of other must be broadcastable with the shape of the underlying tensor", "See torch.add()"]}, {"name": "torch.Tensor.torch.Tensor.add_", "path": "generated/torch.tensor.add_", "type": "Tensor", "text": ["In-place version of add()"]}, {"name": "torch.Tensor.torch.Tensor.addbmm", "path": "generated/torch.tensor.addbmm", "type": "Tensor", "text": ["See torch.addbmm()"]}, {"name": "torch.Tensor.torch.Tensor.addbmm_", "path": "generated/torch.tensor.addbmm_", "type": "Tensor", "text": ["In-place version of addbmm()"]}, {"name": "torch.Tensor.torch.Tensor.addcdiv", "path": "generated/torch.tensor.addcdiv", "type": "Tensor", "text": ["See torch.addcdiv()"]}, {"name": "torch.Tensor.torch.Tensor.addcdiv_", "path": "generated/torch.tensor.addcdiv_", "type": "Tensor", "text": ["In-place version of addcdiv()"]}, {"name": "torch.Tensor.torch.Tensor.addcmul", "path": "generated/torch.tensor.addcmul", "type": "Tensor", "text": ["See torch.addcmul()"]}, {"name": "torch.Tensor.torch.Tensor.addcmul_", "path": "generated/torch.tensor.addcmul_", "type": "Tensor", "text": ["In-place version of addcmul()"]}, {"name": "torch.Tensor.torch.Tensor.addmm", "path": "generated/torch.tensor.addmm", "type": "Tensor", "text": ["See torch.addmm()"]}, {"name": "torch.Tensor.torch.Tensor.addmm_", "path": "generated/torch.tensor.addmm_", "type": "Tensor", "text": ["In-place version of addmm()"]}, {"name": "torch.Tensor.torch.Tensor.addmv", "path": "generated/torch.tensor.addmv", "type": "Tensor", "text": ["See torch.addmv()"]}, {"name": "torch.Tensor.torch.Tensor.addmv_", "path": "generated/torch.tensor.addmv_", "type": "Tensor", "text": ["In-place version of addmv()"]}, {"name": "torch.Tensor.torch.Tensor.addr", "path": "generated/torch.tensor.addr", "type": "Tensor", "text": ["See torch.addr()"]}, {"name": "torch.Tensor.torch.Tensor.addr_", "path": "generated/torch.tensor.addr_", "type": "Tensor", "text": ["In-place version of addr()"]}, {"name": "torch.Tensor.torch.Tensor.adjoint", "path": "generated/torch.tensor.adjoint", "type": "Tensor", "text": ["Alias for adjoint()"]}, {"name": "torch.Tensor.torch.Tensor.all", "path": "generated/torch.tensor.all", "type": "Tensor", "text": ["See torch.all()"]}, {"name": "torch.Tensor.torch.Tensor.allclose", "path": "generated/torch.tensor.allclose", "type": "Tensor", "text": ["See torch.allclose()"]}, {"name": "torch.Tensor.torch.Tensor.amax", "path": "generated/torch.tensor.amax", "type": "Tensor", "text": ["See torch.amax()"]}, {"name": "torch.Tensor.torch.Tensor.amin", "path": "generated/torch.tensor.amin", "type": "Tensor", "text": ["See torch.amin()"]}, {"name": "torch.Tensor.torch.Tensor.aminmax", "path": "generated/torch.tensor.aminmax", "type": "Tensor", "text": ["See torch.aminmax()"]}, {"name": "torch.Tensor.torch.Tensor.angle", "path": "generated/torch.tensor.angle", "type": "Tensor", "text": ["See torch.angle()"]}, {"name": "torch.Tensor.torch.Tensor.any", "path": "generated/torch.tensor.any", "type": "Tensor", "text": ["See torch.any()"]}, {"name": "torch.Tensor.torch.Tensor.apply_", "path": "generated/torch.tensor.apply_", "type": "Tensor", "text": ["Applies the function callable to each element in the tensor, replacing each element with the value returned by callable.", "Note", "This function only works with CPU tensors and should not be used in code sections that require high performance."]}, {"name": "torch.Tensor.torch.Tensor.arccos", "path": "generated/torch.tensor.arccos", "type": "Tensor", "text": ["See torch.arccos()"]}, {"name": "torch.Tensor.torch.Tensor.arccos_", "path": "generated/torch.tensor.arccos_", "type": "Tensor", "text": ["In-place version of arccos()"]}, {"name": "torch.Tensor.torch.Tensor.arccosh", "path": "generated/torch.tensor.arccosh", "type": "Tensor", "text": ["acosh() -> Tensor", "See torch.arccosh()"]}, {"name": "torch.Tensor.torch.Tensor.arccosh_", "path": "generated/torch.tensor.arccosh_", "type": "Tensor", "text": ["acosh_() -> Tensor", "In-place version of arccosh()"]}, {"name": "torch.Tensor.torch.Tensor.arcsin", "path": "generated/torch.tensor.arcsin", "type": "Tensor", "text": ["See torch.arcsin()"]}, {"name": "torch.Tensor.torch.Tensor.arcsin_", "path": "generated/torch.tensor.arcsin_", "type": "Tensor", "text": ["In-place version of arcsin()"]}, {"name": "torch.Tensor.torch.Tensor.arcsinh", "path": "generated/torch.tensor.arcsinh", "type": "Tensor", "text": ["See torch.arcsinh()"]}, {"name": "torch.Tensor.torch.Tensor.arcsinh_", "path": "generated/torch.tensor.arcsinh_", "type": "Tensor", "text": ["In-place version of arcsinh()"]}, {"name": "torch.Tensor.torch.Tensor.arctan", "path": "generated/torch.tensor.arctan", "type": "Tensor", "text": ["See torch.arctan()"]}, {"name": "torch.Tensor.torch.Tensor.arctan2", "path": "generated/torch.tensor.arctan2", "type": "Tensor", "text": ["See torch.arctan2()"]}, {"name": "torch.Tensor.torch.Tensor.arctan2_", "path": "generated/torch.tensor.arctan2_", "type": "Tensor", "text": ["atan2_(other) -> Tensor", "In-place version of arctan2()"]}, {"name": "torch.Tensor.torch.Tensor.arctan_", "path": "generated/torch.tensor.arctan_", "type": "Tensor", "text": ["In-place version of arctan()"]}, {"name": "torch.Tensor.torch.Tensor.arctanh", "path": "generated/torch.tensor.arctanh", "type": "Tensor", "text": ["See torch.arctanh()"]}, {"name": "torch.Tensor.torch.Tensor.arctanh_", "path": "generated/torch.tensor.arctanh_", "type": "Tensor", "text": ["In-place version of arctanh()"]}, {"name": "torch.Tensor.torch.Tensor.argmax", "path": "generated/torch.tensor.argmax", "type": "Tensor", "text": ["See torch.argmax()"]}, {"name": "torch.Tensor.torch.Tensor.argmin", "path": "generated/torch.tensor.argmin", "type": "Tensor", "text": ["See torch.argmin()"]}, {"name": "torch.Tensor.torch.Tensor.argsort", "path": "generated/torch.tensor.argsort", "type": "Tensor", "text": ["See torch.argsort()"]}, {"name": "torch.Tensor.torch.Tensor.argwhere", "path": "generated/torch.tensor.argwhere", "type": "Tensor", "text": ["See torch.argwhere()"]}, {"name": "torch.Tensor.torch.Tensor.as_strided", "path": "generated/torch.tensor.as_strided", "type": "Tensor", "text": ["See torch.as_strided()"]}, {"name": "torch.Tensor.torch.Tensor.as_subclass", "path": "generated/torch.tensor.as_subclass", "type": "Tensor", "text": ["Makes a cls instance with the same data pointer as self. Changes in the output mirror changes in self, and the output stays attached to the autograd graph. cls must be a subclass of Tensor."]}, {"name": "torch.Tensor.torch.Tensor.asin", "path": "generated/torch.tensor.asin", "type": "Tensor", "text": ["See torch.asin()"]}, {"name": "torch.Tensor.torch.Tensor.asin_", "path": "generated/torch.tensor.asin_", "type": "Tensor", "text": ["In-place version of asin()"]}, {"name": "torch.Tensor.torch.Tensor.asinh", "path": "generated/torch.tensor.asinh", "type": "Tensor", "text": ["See torch.asinh()"]}, {"name": "torch.Tensor.torch.Tensor.asinh_", "path": "generated/torch.tensor.asinh_", "type": "Tensor", "text": ["In-place version of asinh()"]}, {"name": "torch.Tensor.torch.Tensor.atan", "path": "generated/torch.tensor.atan", "type": "Tensor", "text": ["See torch.atan()"]}, {"name": "torch.Tensor.torch.Tensor.atan2", "path": "generated/torch.tensor.atan2", "type": "Tensor", "text": ["See torch.atan2()"]}, {"name": "torch.Tensor.torch.Tensor.atan2_", "path": "generated/torch.tensor.atan2_", "type": "Tensor", "text": ["In-place version of atan2()"]}, {"name": "torch.Tensor.torch.Tensor.atan_", "path": "generated/torch.tensor.atan_", "type": "Tensor", "text": ["In-place version of atan()"]}, {"name": "torch.Tensor.torch.Tensor.atanh", "path": "generated/torch.tensor.atanh", "type": "Tensor", "text": ["See torch.atanh()"]}, {"name": "torch.Tensor.torch.Tensor.atanh_", "path": "generated/torch.tensor.atanh_", "type": "Tensor", "text": ["In-place version of atanh()"]}, {"name": "torch.Tensor.torch.Tensor.backward", "path": "generated/torch.tensor.backward", "type": "Tensor", "text": ["Computes the gradient of current tensor wrt graph leaves.", "The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying gradient. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. self.", "This function accumulates gradients in the leaves - you might need to zero .grad attributes or set them to None before calling it. See Default gradient layouts for details on the memory layout of accumulated gradients.", "Note", "If you run any forward ops, create gradient, and/or call backward in a user-specified CUDA stream context, see Stream semantics of backward passes.", "Note", "When inputs are provided and a given input is not a leaf, the current implementation will call its grad_fn (though it is not strictly needed to get this gradients). It is an implementation detail on which the user should not rely. See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details."]}, {"name": "torch.Tensor.torch.Tensor.baddbmm", "path": "generated/torch.tensor.baddbmm", "type": "Tensor", "text": ["See torch.baddbmm()"]}, {"name": "torch.Tensor.torch.Tensor.baddbmm_", "path": "generated/torch.tensor.baddbmm_", "type": "Tensor", "text": ["In-place version of baddbmm()"]}, {"name": "torch.Tensor.torch.Tensor.bernoulli", "path": "generated/torch.tensor.bernoulli", "type": "Tensor", "text": ["Returns a result tensor where each result[i]\\texttt{result[i]} is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]}). self must have floating point dtype, and the result will have the same dtype.", "See torch.bernoulli()"]}, {"name": "torch.Tensor.torch.Tensor.bernoulli_", "path": "generated/torch.tensor.bernoulli_", "type": "Tensor", "text": ["Fills each location of self with an independent sample from Bernoulli(p)\\text{Bernoulli}(\\texttt{p}). self can have integral dtype.", "p should either be a scalar or tensor containing probabilities to be used for drawing the binary random number.", "If it is a tensor, the ith\\text{i}^{th} element of self tensor will be set to a value sampled from Bernoulli(p_tensor[i])\\text{Bernoulli}(\\texttt{p\\_tensor[i]}). In this case p must have floating point dtype.", "See also bernoulli() and torch.bernoulli()"]}, {"name": "torch.Tensor.torch.Tensor.bfloat16", "path": "generated/torch.tensor.bfloat16", "type": "Tensor", "text": ["self.bfloat16() is equivalent to self.to(torch.bfloat16). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.torch.Tensor.bincount", "path": "generated/torch.tensor.bincount", "type": "Tensor", "text": ["See torch.bincount()"]}, {"name": "torch.Tensor.torch.Tensor.bitwise_and", "path": "generated/torch.tensor.bitwise_and", "type": "Tensor", "text": ["See torch.bitwise_and()"]}, {"name": "torch.Tensor.torch.Tensor.bitwise_and_", "path": "generated/torch.tensor.bitwise_and_", "type": "Tensor", "text": ["In-place version of bitwise_and()"]}, {"name": "torch.Tensor.torch.Tensor.bitwise_left_shift", "path": "generated/torch.tensor.bitwise_left_shift", "type": "Tensor", "text": ["See torch.bitwise_left_shift()"]}, {"name": "torch.Tensor.torch.Tensor.bitwise_left_shift_", "path": "generated/torch.tensor.bitwise_left_shift_", "type": "Tensor", "text": ["In-place version of bitwise_left_shift()"]}, {"name": "torch.Tensor.torch.Tensor.bitwise_not", "path": "generated/torch.tensor.bitwise_not", "type": "Tensor", "text": ["See torch.bitwise_not()"]}, {"name": "torch.Tensor.torch.Tensor.bitwise_not_", "path": "generated/torch.tensor.bitwise_not_", "type": "Tensor", "text": ["In-place version of bitwise_not()"]}, {"name": "torch.Tensor.torch.Tensor.bitwise_or", "path": "generated/torch.tensor.bitwise_or", "type": "Tensor", "text": ["See torch.bitwise_or()"]}, {"name": "torch.Tensor.torch.Tensor.bitwise_or_", "path": "generated/torch.tensor.bitwise_or_", "type": "Tensor", "text": ["In-place version of bitwise_or()"]}, {"name": "torch.Tensor.torch.Tensor.bitwise_right_shift", "path": "generated/torch.tensor.bitwise_right_shift", "type": "Tensor", "text": ["See torch.bitwise_right_shift()"]}, {"name": "torch.Tensor.torch.Tensor.bitwise_right_shift_", "path": "generated/torch.tensor.bitwise_right_shift_", "type": "Tensor", "text": ["In-place version of bitwise_right_shift()"]}, {"name": "torch.Tensor.torch.Tensor.bitwise_xor", "path": "generated/torch.tensor.bitwise_xor", "type": "Tensor", "text": ["See torch.bitwise_xor()"]}, {"name": "torch.Tensor.torch.Tensor.bitwise_xor_", "path": "generated/torch.tensor.bitwise_xor_", "type": "Tensor", "text": ["In-place version of bitwise_xor()"]}, {"name": "torch.Tensor.torch.Tensor.bmm", "path": "generated/torch.tensor.bmm", "type": "Tensor", "text": ["See torch.bmm()"]}, {"name": "torch.Tensor.torch.Tensor.bool", "path": "generated/torch.tensor.bool", "type": "Tensor", "text": ["self.bool() is equivalent to self.to(torch.bool). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.torch.Tensor.broadcast_to", "path": "generated/torch.tensor.broadcast_to", "type": "Tensor", "text": ["See torch.broadcast_to()."]}, {"name": "torch.Tensor.torch.Tensor.byte", "path": "generated/torch.tensor.byte", "type": "Tensor", "text": ["self.byte() is equivalent to self.to(torch.uint8). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.torch.Tensor.cauchy_", "path": "generated/torch.tensor.cauchy_", "type": "Tensor", "text": ["Fills the tensor with numbers drawn from the Cauchy distribution:"]}, {"name": "torch.Tensor.torch.Tensor.cdouble", "path": "generated/torch.tensor.cdouble", "type": "Tensor", "text": ["self.cdouble() is equivalent to self.to(torch.complex128). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.torch.Tensor.ceil", "path": "generated/torch.tensor.ceil", "type": "Tensor", "text": ["See torch.ceil()"]}, {"name": "torch.Tensor.torch.Tensor.ceil_", "path": "generated/torch.tensor.ceil_", "type": "Tensor", "text": ["In-place version of ceil()"]}, {"name": "torch.Tensor.torch.Tensor.cfloat", "path": "generated/torch.tensor.cfloat", "type": "Tensor", "text": ["self.cfloat() is equivalent to self.to(torch.complex64). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.torch.Tensor.chalf", "path": "generated/torch.tensor.chalf", "type": "Tensor", "text": ["self.chalf() is equivalent to self.to(torch.complex32). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.torch.Tensor.char", "path": "generated/torch.tensor.char", "type": "Tensor", "text": ["self.char() is equivalent to self.to(torch.int8). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.torch.Tensor.cholesky", "path": "generated/torch.tensor.cholesky", "type": "Tensor", "text": ["See torch.cholesky()"]}, {"name": "torch.Tensor.torch.Tensor.cholesky_inverse", "path": "generated/torch.tensor.cholesky_inverse", "type": "Tensor", "text": ["See torch.cholesky_inverse()"]}, {"name": "torch.Tensor.torch.Tensor.cholesky_solve", "path": "generated/torch.tensor.cholesky_solve", "type": "Tensor", "text": ["See torch.cholesky_solve()"]}, {"name": "torch.Tensor.torch.Tensor.chunk", "path": "generated/torch.tensor.chunk", "type": "Tensor", "text": ["See torch.chunk()"]}, {"name": "torch.Tensor.torch.Tensor.clamp", "path": "generated/torch.tensor.clamp", "type": "Tensor", "text": ["See torch.clamp()"]}, {"name": "torch.Tensor.torch.Tensor.clamp_", "path": "generated/torch.tensor.clamp_", "type": "Tensor", "text": ["In-place version of clamp()"]}, {"name": "torch.Tensor.torch.Tensor.clip", "path": "generated/torch.tensor.clip", "type": "Tensor", "text": ["Alias for clamp()."]}, {"name": "torch.Tensor.torch.Tensor.clip_", "path": "generated/torch.tensor.clip_", "type": "Tensor", "text": ["Alias for clamp_()."]}, {"name": "torch.Tensor.torch.Tensor.clone", "path": "generated/torch.tensor.clone", "type": "Tensor", "text": ["See torch.clone()"]}, {"name": "torch.Tensor.torch.Tensor.conj", "path": "generated/torch.tensor.conj", "type": "Tensor", "text": ["See torch.conj()"]}, {"name": "torch.Tensor.torch.Tensor.conj_physical", "path": "generated/torch.tensor.conj_physical", "type": "Tensor", "text": ["See torch.conj_physical()"]}, {"name": "torch.Tensor.torch.Tensor.conj_physical_", "path": "generated/torch.tensor.conj_physical_", "type": "Tensor", "text": ["In-place version of conj_physical()"]}, {"name": "torch.Tensor.torch.Tensor.contiguous", "path": "generated/torch.tensor.contiguous", "type": "Tensor", "text": ["Returns a contiguous in memory tensor containing the same data as self tensor. If self tensor is already in the specified memory format, this function returns the self tensor.", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.contiguous_format."]}, {"name": "torch.Tensor.torch.Tensor.copy_", "path": "generated/torch.tensor.copy_", "type": "Tensor", "text": ["Copies the elements from src into self tensor and returns self.", "The src tensor must be broadcastable with the self tensor. It may be of a different data type or reside on a different device."]}, {"name": "torch.Tensor.torch.Tensor.copysign", "path": "generated/torch.tensor.copysign", "type": "Tensor", "text": ["See torch.copysign()"]}, {"name": "torch.Tensor.torch.Tensor.copysign_", "path": "generated/torch.tensor.copysign_", "type": "Tensor", "text": ["In-place version of copysign()"]}, {"name": "torch.Tensor.torch.Tensor.corrcoef", "path": "generated/torch.tensor.corrcoef", "type": "Tensor", "text": ["See torch.corrcoef()"]}, {"name": "torch.Tensor.torch.Tensor.cos", "path": "generated/torch.tensor.cos", "type": "Tensor", "text": ["See torch.cos()"]}, {"name": "torch.Tensor.torch.Tensor.cos_", "path": "generated/torch.tensor.cos_", "type": "Tensor", "text": ["In-place version of cos()"]}, {"name": "torch.Tensor.torch.Tensor.cosh", "path": "generated/torch.tensor.cosh", "type": "Tensor", "text": ["See torch.cosh()"]}, {"name": "torch.Tensor.torch.Tensor.cosh_", "path": "generated/torch.tensor.cosh_", "type": "Tensor", "text": ["In-place version of cosh()"]}, {"name": "torch.Tensor.torch.Tensor.count_nonzero", "path": "generated/torch.tensor.count_nonzero", "type": "Tensor", "text": ["See torch.count_nonzero()"]}, {"name": "torch.Tensor.torch.Tensor.cov", "path": "generated/torch.tensor.cov", "type": "Tensor", "text": ["See torch.cov()"]}, {"name": "torch.Tensor.torch.Tensor.cpu", "path": "generated/torch.tensor.cpu", "type": "Tensor", "text": ["Returns a copy of this object in CPU memory.", "If this object is already in CPU memory and on the correct device, then no copy is performed and the original object is returned.", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.torch.Tensor.cross", "path": "generated/torch.tensor.cross", "type": "Tensor", "text": ["See torch.cross()"]}, {"name": "torch.Tensor.torch.Tensor.cuda", "path": "generated/torch.tensor.cuda", "type": "Tensor", "text": ["Returns a copy of this object in CUDA memory.", "If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned."]}, {"name": "torch.Tensor.torch.Tensor.cummax", "path": "generated/torch.tensor.cummax", "type": "Tensor", "text": ["See torch.cummax()"]}, {"name": "torch.Tensor.torch.Tensor.cummin", "path": "generated/torch.tensor.cummin", "type": "Tensor", "text": ["See torch.cummin()"]}, {"name": "torch.Tensor.torch.Tensor.cumprod", "path": "generated/torch.tensor.cumprod", "type": "Tensor", "text": ["See torch.cumprod()"]}, {"name": "torch.Tensor.torch.Tensor.cumprod_", "path": "generated/torch.tensor.cumprod_", "type": "Tensor", "text": ["In-place version of cumprod()"]}, {"name": "torch.Tensor.torch.Tensor.cumsum", "path": "generated/torch.tensor.cumsum", "type": "Tensor", "text": ["See torch.cumsum()"]}, {"name": "torch.Tensor.torch.Tensor.cumsum_", "path": "generated/torch.tensor.cumsum_", "type": "Tensor", "text": ["In-place version of cumsum()"]}, {"name": "torch.Tensor.torch.Tensor.data_ptr", "path": "generated/torch.tensor.data_ptr", "type": "Tensor", "text": ["Returns the address of the first element of self tensor."]}, {"name": "torch.Tensor.torch.Tensor.deg2rad", "path": "generated/torch.tensor.deg2rad", "type": "Tensor", "text": ["See torch.deg2rad()"]}, {"name": "torch.Tensor.torch.Tensor.dense_dim", "path": "generated/torch.tensor.dense_dim", "type": "Tensor", "text": ["Return the number of dense dimensions in a sparse tensor self.", "Note", "Returns len(self.shape) if self is not a sparse tensor.", "See also Tensor.sparse_dim() and hybrid tensors."]}, {"name": "torch.Tensor.torch.Tensor.dequantize", "path": "generated/torch.tensor.dequantize", "type": "Tensor", "text": ["Given a quantized Tensor, dequantize it and return the dequantized float Tensor."]}, {"name": "torch.Tensor.torch.Tensor.det", "path": "generated/torch.tensor.det", "type": "Tensor", "text": ["See torch.det()"]}, {"name": "torch.Tensor.torch.Tensor.detach", "path": "generated/torch.tensor.detach", "type": "Tensor", "text": ["Returns a new Tensor, detached from the current graph.", "The result will never require gradient.", "This method also affects forward mode AD gradients and the result will never have forward mode AD gradients.", "Note", "Returned Tensor shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks. IMPORTANT NOTE: Previously, in-place size / stride / storage changes (such as resize_ / resize_as_ / set_ / transpose_) to the returned tensor also update the original tensor. Now, these in-place changes will not update the original tensor anymore, and will instead trigger an error. For sparse tensors: In-place indices / values changes (such as zero_ / copy_ / add_) to the returned tensor will not update the original tensor anymore, and will instead trigger an error."]}, {"name": "torch.Tensor.torch.Tensor.detach_", "path": "generated/torch.tensor.detach_", "type": "Tensor", "text": ["Detaches the Tensor from the graph that created it, making it a leaf. Views cannot be detached in-place.", "This method also affects forward mode AD gradients and the result will never have forward mode AD gradients."]}, {"name": "torch.Tensor.torch.Tensor.device", "path": "generated/torch.tensor.device", "type": "Tensor", "text": ["Is the torch.device where this Tensor is."]}, {"name": "torch.Tensor.torch.Tensor.diag", "path": "generated/torch.tensor.diag", "type": "Tensor", "text": ["See torch.diag()"]}, {"name": "torch.Tensor.torch.Tensor.diag_embed", "path": "generated/torch.tensor.diag_embed", "type": "Tensor", "text": ["See torch.diag_embed()"]}, {"name": "torch.Tensor.torch.Tensor.diagflat", "path": "generated/torch.tensor.diagflat", "type": "Tensor", "text": ["See torch.diagflat()"]}, {"name": "torch.Tensor.torch.Tensor.diagonal", "path": "generated/torch.tensor.diagonal", "type": "Tensor", "text": ["See torch.diagonal()"]}, {"name": "torch.Tensor.torch.Tensor.diagonal_scatter", "path": "generated/torch.tensor.diagonal_scatter", "type": "Tensor", "text": ["See torch.diagonal_scatter()"]}, {"name": "torch.Tensor.torch.Tensor.diff", "path": "generated/torch.tensor.diff", "type": "Tensor", "text": ["See torch.diff()"]}, {"name": "torch.Tensor.torch.Tensor.digamma", "path": "generated/torch.tensor.digamma", "type": "Tensor", "text": ["See torch.digamma()"]}, {"name": "torch.Tensor.torch.Tensor.digamma_", "path": "generated/torch.tensor.digamma_", "type": "Tensor", "text": ["In-place version of digamma()"]}, {"name": "torch.Tensor.torch.Tensor.dim", "path": "generated/torch.tensor.dim", "type": "Tensor", "text": ["Returns the number of dimensions of self tensor."]}, {"name": "torch.Tensor.torch.Tensor.dim_order", "path": "generated/torch.tensor.dim_order", "type": "Tensor", "text": ["Returns a tuple of int describing the dim order or physical layout of self.", "None \u2013 ", "Dim order represents how dimensions are laid out in memory, starting from the outermost to the innermost dimension.", "Warning", "The dim_order tensor API is experimental and subject to change."]}, {"name": "torch.Tensor.torch.Tensor.dist", "path": "generated/torch.tensor.dist", "type": "Tensor", "text": ["See torch.dist()"]}, {"name": "torch.Tensor.torch.Tensor.div", "path": "generated/torch.tensor.div", "type": "Tensor", "text": ["See torch.div()"]}, {"name": "torch.Tensor.torch.Tensor.div_", "path": "generated/torch.tensor.div_", "type": "Tensor", "text": ["In-place version of div()"]}, {"name": "torch.Tensor.torch.Tensor.divide", "path": "generated/torch.tensor.divide", "type": "Tensor", "text": ["See torch.divide()"]}, {"name": "torch.Tensor.torch.Tensor.divide_", "path": "generated/torch.tensor.divide_", "type": "Tensor", "text": ["In-place version of divide()"]}, {"name": "torch.Tensor.torch.Tensor.dot", "path": "generated/torch.tensor.dot", "type": "Tensor", "text": ["See torch.dot()"]}, {"name": "torch.Tensor.torch.Tensor.double", "path": "generated/torch.tensor.double", "type": "Tensor", "text": ["self.double() is equivalent to self.to(torch.float64). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.torch.Tensor.dsplit", "path": "generated/torch.tensor.dsplit", "type": "Tensor", "text": ["See torch.dsplit()"]}, {"name": "torch.Tensor.torch.Tensor.element_size", "path": "generated/torch.tensor.element_size", "type": "Tensor", "text": ["Returns the size in bytes of an individual element.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.eq", "path": "generated/torch.tensor.eq", "type": "Tensor", "text": ["See torch.eq()"]}, {"name": "torch.Tensor.torch.Tensor.eq_", "path": "generated/torch.tensor.eq_", "type": "Tensor", "text": ["In-place version of eq()"]}, {"name": "torch.Tensor.torch.Tensor.equal", "path": "generated/torch.tensor.equal", "type": "Tensor", "text": ["See torch.equal()"]}, {"name": "torch.Tensor.torch.Tensor.erf", "path": "generated/torch.tensor.erf", "type": "Tensor", "text": ["See torch.erf()"]}, {"name": "torch.Tensor.torch.Tensor.erf_", "path": "generated/torch.tensor.erf_", "type": "Tensor", "text": ["In-place version of erf()"]}, {"name": "torch.Tensor.torch.Tensor.erfc", "path": "generated/torch.tensor.erfc", "type": "Tensor", "text": ["See torch.erfc()"]}, {"name": "torch.Tensor.torch.Tensor.erfc_", "path": "generated/torch.tensor.erfc_", "type": "Tensor", "text": ["In-place version of erfc()"]}, {"name": "torch.Tensor.torch.Tensor.erfinv", "path": "generated/torch.tensor.erfinv", "type": "Tensor", "text": ["See torch.erfinv()"]}, {"name": "torch.Tensor.torch.Tensor.erfinv_", "path": "generated/torch.tensor.erfinv_", "type": "Tensor", "text": ["In-place version of erfinv()"]}, {"name": "torch.Tensor.torch.Tensor.exp", "path": "generated/torch.tensor.exp", "type": "Tensor", "text": ["See torch.exp()"]}, {"name": "torch.Tensor.torch.Tensor.exp_", "path": "generated/torch.tensor.exp_", "type": "Tensor", "text": ["In-place version of exp()"]}, {"name": "torch.Tensor.torch.Tensor.expand", "path": "generated/torch.tensor.expand", "type": "Tensor", "text": ["Returns a new view of the self tensor with singleton dimensions expanded to a larger size.", "Passing -1 as the size for a dimension means not changing the size of that dimension.", "Tensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front. For the new dimensions, the size cannot be set to -1.", "Expanding a tensor does not allocate new memory, but only creates a new view on the existing tensor where a dimension of size one is expanded to a larger size by setting the stride to 0. Any dimension of size 1 can be expanded to an arbitrary value without allocating new memory.", "*sizes (torch.Size or int...) \u2013 the desired expanded size", "Warning", "More than one element of an expanded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.expand_as", "path": "generated/torch.tensor.expand_as", "type": "Tensor", "text": ["Expand this tensor to the same size as other. self.expand_as(other) is equivalent to self.expand(other.size()).", "Please see expand() for more information about expand.", "other (torch.Tensor) \u2013 The result tensor has the same size as other."]}, {"name": "torch.Tensor.torch.Tensor.expm1", "path": "generated/torch.tensor.expm1", "type": "Tensor", "text": ["See torch.expm1()"]}, {"name": "torch.Tensor.torch.Tensor.expm1_", "path": "generated/torch.tensor.expm1_", "type": "Tensor", "text": ["In-place version of expm1()"]}, {"name": "torch.Tensor.torch.Tensor.exponential_", "path": "generated/torch.tensor.exponential_", "type": "Tensor", "text": ["Fills self tensor with elements drawn from the exponential distribution:"]}, {"name": "torch.Tensor.torch.Tensor.fill_", "path": "generated/torch.tensor.fill_", "type": "Tensor", "text": ["Fills self tensor with the specified value."]}, {"name": "torch.Tensor.torch.Tensor.fill_diagonal_", "path": "generated/torch.tensor.fill_diagonal_", "type": "Tensor", "text": ["Fill the main diagonal of a tensor that has at least 2-dimensions. When dims>2, all dimensions of input must be of equal length. This function modifies the input tensor in-place, and returns the input tensor.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.fix", "path": "generated/torch.tensor.fix", "type": "Tensor", "text": ["See torch.fix()."]}, {"name": "torch.Tensor.torch.Tensor.fix_", "path": "generated/torch.tensor.fix_", "type": "Tensor", "text": ["In-place version of fix()"]}, {"name": "torch.Tensor.torch.Tensor.flatten", "path": "generated/torch.tensor.flatten", "type": "Tensor", "text": ["See torch.flatten()"]}, {"name": "torch.Tensor.torch.Tensor.flip", "path": "generated/torch.tensor.flip", "type": "Tensor", "text": ["See torch.flip()"]}, {"name": "torch.Tensor.torch.Tensor.fliplr", "path": "generated/torch.tensor.fliplr", "type": "Tensor", "text": ["See torch.fliplr()"]}, {"name": "torch.Tensor.torch.Tensor.flipud", "path": "generated/torch.tensor.flipud", "type": "Tensor", "text": ["See torch.flipud()"]}, {"name": "torch.Tensor.torch.Tensor.float", "path": "generated/torch.tensor.float", "type": "Tensor", "text": ["self.float() is equivalent to self.to(torch.float32). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.torch.Tensor.float_power", "path": "generated/torch.tensor.float_power", "type": "Tensor", "text": ["See torch.float_power()"]}, {"name": "torch.Tensor.torch.Tensor.float_power_", "path": "generated/torch.tensor.float_power_", "type": "Tensor", "text": ["In-place version of float_power()"]}, {"name": "torch.Tensor.torch.Tensor.floor", "path": "generated/torch.tensor.floor", "type": "Tensor", "text": ["See torch.floor()"]}, {"name": "torch.Tensor.torch.Tensor.floor_", "path": "generated/torch.tensor.floor_", "type": "Tensor", "text": ["In-place version of floor()"]}, {"name": "torch.Tensor.torch.Tensor.floor_divide", "path": "generated/torch.tensor.floor_divide", "type": "Tensor", "text": ["See torch.floor_divide()"]}, {"name": "torch.Tensor.torch.Tensor.floor_divide_", "path": "generated/torch.tensor.floor_divide_", "type": "Tensor", "text": ["In-place version of floor_divide()"]}, {"name": "torch.Tensor.torch.Tensor.fmax", "path": "generated/torch.tensor.fmax", "type": "Tensor", "text": ["See torch.fmax()"]}, {"name": "torch.Tensor.torch.Tensor.fmin", "path": "generated/torch.tensor.fmin", "type": "Tensor", "text": ["See torch.fmin()"]}, {"name": "torch.Tensor.torch.Tensor.fmod", "path": "generated/torch.tensor.fmod", "type": "Tensor", "text": ["See torch.fmod()"]}, {"name": "torch.Tensor.torch.Tensor.fmod_", "path": "generated/torch.tensor.fmod_", "type": "Tensor", "text": ["In-place version of fmod()"]}, {"name": "torch.Tensor.torch.Tensor.frac", "path": "generated/torch.tensor.frac", "type": "Tensor", "text": ["See torch.frac()"]}, {"name": "torch.Tensor.torch.Tensor.frac_", "path": "generated/torch.tensor.frac_", "type": "Tensor", "text": ["In-place version of frac()"]}, {"name": "torch.Tensor.torch.Tensor.frexp", "path": "generated/torch.tensor.frexp", "type": "Tensor", "text": ["See torch.frexp()"]}, {"name": "torch.Tensor.torch.Tensor.gather", "path": "generated/torch.tensor.gather", "type": "Tensor", "text": ["See torch.gather()"]}, {"name": "torch.Tensor.torch.Tensor.gcd", "path": "generated/torch.tensor.gcd", "type": "Tensor", "text": ["See torch.gcd()"]}, {"name": "torch.Tensor.torch.Tensor.gcd_", "path": "generated/torch.tensor.gcd_", "type": "Tensor", "text": ["In-place version of gcd()"]}, {"name": "torch.Tensor.torch.Tensor.ge", "path": "generated/torch.tensor.ge", "type": "Tensor", "text": ["See torch.ge()."]}, {"name": "torch.Tensor.torch.Tensor.ge_", "path": "generated/torch.tensor.ge_", "type": "Tensor", "text": ["In-place version of ge()."]}, {"name": "torch.Tensor.torch.Tensor.geometric_", "path": "generated/torch.tensor.geometric_", "type": "Tensor", "text": ["Fills self tensor with elements drawn from the geometric distribution:"]}, {"name": "torch.Tensor.torch.Tensor.geqrf", "path": "generated/torch.tensor.geqrf", "type": "Tensor", "text": ["See torch.geqrf()"]}, {"name": "torch.Tensor.torch.Tensor.ger", "path": "generated/torch.tensor.ger", "type": "Tensor", "text": ["See torch.ger()"]}, {"name": "torch.Tensor.torch.Tensor.get_device", "path": "generated/torch.tensor.get_device", "type": "Tensor", "text": ["For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides. For CPU tensors, this function returns -1.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.grad", "path": "generated/torch.tensor.grad", "type": "Tensor", "text": ["This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. The attribute will then contain the gradients computed and future calls to backward() will accumulate (add) gradients into it."]}, {"name": "torch.Tensor.torch.Tensor.greater", "path": "generated/torch.tensor.greater", "type": "Tensor", "text": ["See torch.greater()."]}, {"name": "torch.Tensor.torch.Tensor.greater_", "path": "generated/torch.tensor.greater_", "type": "Tensor", "text": ["In-place version of greater()."]}, {"name": "torch.Tensor.torch.Tensor.greater_equal", "path": "generated/torch.tensor.greater_equal", "type": "Tensor", "text": ["See torch.greater_equal()."]}, {"name": "torch.Tensor.torch.Tensor.greater_equal_", "path": "generated/torch.tensor.greater_equal_", "type": "Tensor", "text": ["In-place version of greater_equal()."]}, {"name": "torch.Tensor.torch.Tensor.gt", "path": "generated/torch.tensor.gt", "type": "Tensor", "text": ["See torch.gt()."]}, {"name": "torch.Tensor.torch.Tensor.gt_", "path": "generated/torch.tensor.gt_", "type": "Tensor", "text": ["In-place version of gt()."]}, {"name": "torch.Tensor.torch.Tensor.half", "path": "generated/torch.tensor.half", "type": "Tensor", "text": ["self.half() is equivalent to self.to(torch.float16). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.torch.Tensor.hardshrink", "path": "generated/torch.tensor.hardshrink", "type": "Tensor", "text": ["See torch.nn.functional.hardshrink()"]}, {"name": "torch.Tensor.torch.Tensor.heaviside", "path": "generated/torch.tensor.heaviside", "type": "Tensor", "text": ["See torch.heaviside()"]}, {"name": "torch.Tensor.torch.Tensor.histc", "path": "generated/torch.tensor.histc", "type": "Tensor", "text": ["See torch.histc()"]}, {"name": "torch.Tensor.torch.Tensor.histogram", "path": "generated/torch.tensor.histogram", "type": "Tensor", "text": ["See torch.histogram()"]}, {"name": "torch.Tensor.torch.Tensor.hsplit", "path": "generated/torch.tensor.hsplit", "type": "Tensor", "text": ["See torch.hsplit()"]}, {"name": "torch.Tensor.torch.Tensor.hypot", "path": "generated/torch.tensor.hypot", "type": "Tensor", "text": ["See torch.hypot()"]}, {"name": "torch.Tensor.torch.Tensor.hypot_", "path": "generated/torch.tensor.hypot_", "type": "Tensor", "text": ["In-place version of hypot()"]}, {"name": "torch.Tensor.torch.Tensor.i0", "path": "generated/torch.tensor.i0", "type": "Tensor", "text": ["See torch.i0()"]}, {"name": "torch.Tensor.torch.Tensor.i0_", "path": "generated/torch.tensor.i0_", "type": "Tensor", "text": ["In-place version of i0()"]}, {"name": "torch.Tensor.torch.Tensor.igamma", "path": "generated/torch.tensor.igamma", "type": "Tensor", "text": ["See torch.igamma()"]}, {"name": "torch.Tensor.torch.Tensor.igamma_", "path": "generated/torch.tensor.igamma_", "type": "Tensor", "text": ["In-place version of igamma()"]}, {"name": "torch.Tensor.torch.Tensor.igammac", "path": "generated/torch.tensor.igammac", "type": "Tensor", "text": ["See torch.igammac()"]}, {"name": "torch.Tensor.torch.Tensor.igammac_", "path": "generated/torch.tensor.igammac_", "type": "Tensor", "text": ["In-place version of igammac()"]}, {"name": "torch.Tensor.torch.Tensor.imag", "path": "generated/torch.tensor.imag", "type": "Tensor", "text": ["Returns a new tensor containing imaginary values of the self tensor. The returned tensor and self share the same underlying storage.", "Warning", "imag() is only supported for tensors with complex dtypes."]}, {"name": "torch.Tensor.torch.Tensor.index_add", "path": "generated/torch.tensor.index_add", "type": "Tensor", "text": ["Out-of-place version of torch.Tensor.index_add_()."]}, {"name": "torch.Tensor.torch.Tensor.index_add_", "path": "generated/torch.tensor.index_add_", "type": "Tensor", "text": ["Accumulate the elements of alpha times source into the self tensor by adding to the indices in the order given in index. For example, if dim == 0, index[i] == j, and alpha=-1, then the ith row of source is subtracted from the jth row of self.", "The dimth dimension of source must have the same size as the length of index (which must be a vector), and all other dimensions must match self, or an error will be raised.", "For a 3-D tensor the output is given as:", "Note", "This operation may behave nondeterministically when given tensors on a CUDA device. See Reproducibility for more information.", "alpha (Number) \u2013 the scalar multiplier for source", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.index_copy", "path": "generated/torch.tensor.index_copy", "type": "Tensor", "text": ["Out-of-place version of torch.Tensor.index_copy_()."]}, {"name": "torch.Tensor.torch.Tensor.index_copy_", "path": "generated/torch.tensor.index_copy_", "type": "Tensor", "text": ["Copies the elements of tensor into the self tensor by selecting the indices in the order given in index. For example, if dim == 0 and index[i] == j, then the ith row of tensor is copied to the jth row of self.", "The dimth dimension of tensor must have the same size as the length of index (which must be a vector), and all other dimensions must match self, or an error will be raised.", "Note", "If index contains duplicate entries, multiple elements from tensor will be copied to the same index of self. The result is nondeterministic since it depends on which copy occurs last.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.index_fill", "path": "generated/torch.tensor.index_fill", "type": "Tensor", "text": ["Out-of-place version of torch.Tensor.index_fill_()."]}, {"name": "torch.Tensor.torch.Tensor.index_fill_", "path": "generated/torch.tensor.index_fill_", "type": "Tensor", "text": ["Fills the elements of the self tensor with value value by selecting the indices in the order given in index."]}, {"name": "torch.Tensor.torch.Tensor.index_put", "path": "generated/torch.tensor.index_put", "type": "Tensor", "text": ["Out-place version of index_put_()."]}, {"name": "torch.Tensor.torch.Tensor.index_put_", "path": "generated/torch.tensor.index_put_", "type": "Tensor", "text": ["Puts values from the tensor values into the tensor self using the indices specified in indices (which is a tuple of Tensors). The expression tensor.index_put_(indices, values) is equivalent to tensor[indices] = values. Returns self.", "If accumulate is True, the elements in values are added to self. If accumulate is False, the behavior is undefined if indices contain duplicate elements."]}, {"name": "torch.Tensor.torch.Tensor.index_reduce", "path": "generated/torch.tensor.index_reduce", "type": "Tensor", "text": []}, {"name": "torch.Tensor.torch.Tensor.index_reduce_", "path": "generated/torch.tensor.index_reduce_", "type": "Tensor", "text": ["Accumulate the elements of source into the self tensor by accumulating to the indices in the order given in index using the reduction given by the reduce argument. For example, if dim == 0, index[i] == j, reduce == prod and include_self == True then the ith row of source is multiplied by the jth row of self. If include_self=\"True\", the values in the self tensor are included in the reduction, otherwise, rows in the self tensor that are accumulated to are treated as if they were filled with the reduction identites.", "The dimth dimension of source must have the same size as the length of index (which must be a vector), and all other dimensions must match self, or an error will be raised.", "For a 3-D tensor with reduce=\"prod\" and include_self=True the output is given as:", "Note", "This operation may behave nondeterministically when given tensors on a CUDA device. See Reproducibility for more information.", "Note", "This function only supports floating point tensors.", "Warning", "This function is in beta and may change in the near future.", "include_self (bool) \u2013 whether the elements from the self tensor are included in the reduction", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.index_select", "path": "generated/torch.tensor.index_select", "type": "Tensor", "text": ["See torch.index_select()"]}, {"name": "torch.Tensor.torch.Tensor.indices", "path": "generated/torch.tensor.indices", "type": "Tensor", "text": ["Return the indices tensor of a sparse COO tensor.", "Warning", "Throws an error if self is not a sparse COO tensor.", "See also Tensor.values().", "Note", "This method can only be called on a coalesced sparse tensor. See Tensor.coalesce() for details."]}, {"name": "torch.Tensor.torch.Tensor.inner", "path": "generated/torch.tensor.inner", "type": "Tensor", "text": ["See torch.inner()."]}, {"name": "torch.Tensor.torch.Tensor.int", "path": "generated/torch.tensor.int", "type": "Tensor", "text": ["self.int() is equivalent to self.to(torch.int32). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.torch.Tensor.int_repr", "path": "generated/torch.tensor.int_repr", "type": "Tensor", "text": ["Given a quantized Tensor, self.int_repr() returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor."]}, {"name": "torch.Tensor.torch.Tensor.inverse", "path": "generated/torch.tensor.inverse", "type": "Tensor", "text": ["See torch.inverse()"]}, {"name": "torch.Tensor.torch.Tensor.is_complex", "path": "generated/torch.tensor.is_complex", "type": "Tensor", "text": ["Returns True if the data type of self is a complex data type."]}, {"name": "torch.Tensor.torch.Tensor.is_conj", "path": "generated/torch.tensor.is_conj", "type": "Tensor", "text": ["Returns True if the conjugate bit of self is set to true."]}, {"name": "torch.Tensor.torch.Tensor.is_contiguous", "path": "generated/torch.tensor.is_contiguous", "type": "Tensor", "text": ["Returns True if self tensor is contiguous in memory in the order specified by memory format.", "memory_format (torch.memory_format, optional) \u2013 Specifies memory allocation order. Default: torch.contiguous_format."]}, {"name": "torch.Tensor.torch.Tensor.is_cuda", "path": "generated/torch.tensor.is_cuda", "type": "Tensor", "text": ["Is True if the Tensor is stored on the GPU, False otherwise."]}, {"name": "torch.Tensor.torch.Tensor.is_floating_point", "path": "generated/torch.tensor.is_floating_point", "type": "Tensor", "text": ["Returns True if the data type of self is a floating point data type."]}, {"name": "torch.Tensor.torch.Tensor.is_inference", "path": "generated/torch.tensor.is_inference", "type": "Tensor", "text": ["See torch.is_inference()"]}, {"name": "torch.Tensor.torch.Tensor.is_leaf", "path": "generated/torch.tensor.is_leaf", "type": "Tensor", "text": ["All Tensors that have requires_grad which is False will be leaf Tensors by convention.", "For Tensors that have requires_grad which is True, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so grad_fn is None.", "Only leaf Tensors will have their grad populated during a call to backward(). To get grad populated for non-leaf Tensors, you can use retain_grad().", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.is_meta", "path": "generated/torch.tensor.is_meta", "type": "Tensor", "text": ["Is True if the Tensor is a meta tensor, False otherwise. Meta tensors are like normal tensors, but they carry no data."]}, {"name": "torch.Tensor.torch.Tensor.is_pinned", "path": "generated/torch.tensor.is_pinned", "type": "Tensor", "text": ["Returns true if this tensor resides in pinned memory."]}, {"name": "torch.Tensor.torch.Tensor.is_quantized", "path": "generated/torch.tensor.is_quantized", "type": "Tensor", "text": ["Is True if the Tensor is quantized, False otherwise."]}, {"name": "torch.Tensor.torch.Tensor.is_set_to", "path": "generated/torch.tensor.is_set_to", "type": "Tensor", "text": ["Returns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride)."]}, {"name": "torch.Tensor.torch.Tensor.is_shared", "path": "generated/torch.tensor.is_shared", "type": "Tensor", "text": ["Checks if tensor is in shared memory.", "This is always True for CUDA tensors."]}, {"name": "torch.Tensor.torch.Tensor.is_signed", "path": "generated/torch.tensor.is_signed", "type": "Tensor", "text": ["Returns True if the data type of self is a signed data type."]}, {"name": "torch.Tensor.torch.Tensor.is_sparse", "path": "generated/torch.tensor.is_sparse", "type": "Tensor", "text": ["Is True if the Tensor uses sparse COO storage layout, False otherwise."]}, {"name": "torch.Tensor.torch.Tensor.isclose", "path": "generated/torch.tensor.isclose", "type": "Tensor", "text": ["See torch.isclose()"]}, {"name": "torch.Tensor.torch.Tensor.isfinite", "path": "generated/torch.tensor.isfinite", "type": "Tensor", "text": ["See torch.isfinite()"]}, {"name": "torch.Tensor.torch.Tensor.isinf", "path": "generated/torch.tensor.isinf", "type": "Tensor", "text": ["See torch.isinf()"]}, {"name": "torch.Tensor.torch.Tensor.isnan", "path": "generated/torch.tensor.isnan", "type": "Tensor", "text": ["See torch.isnan()"]}, {"name": "torch.Tensor.torch.Tensor.isneginf", "path": "generated/torch.tensor.isneginf", "type": "Tensor", "text": ["See torch.isneginf()"]}, {"name": "torch.Tensor.torch.Tensor.isposinf", "path": "generated/torch.tensor.isposinf", "type": "Tensor", "text": ["See torch.isposinf()"]}, {"name": "torch.Tensor.torch.Tensor.isreal", "path": "generated/torch.tensor.isreal", "type": "Tensor", "text": ["See torch.isreal()"]}, {"name": "torch.Tensor.torch.Tensor.istft", "path": "generated/torch.tensor.istft", "type": "Tensor", "text": ["See torch.istft()"]}, {"name": "torch.Tensor.torch.Tensor.item", "path": "generated/torch.tensor.item", "type": "Tensor", "text": ["Returns the value of this tensor as a standard Python number. This only works for tensors with one element. For other cases, see tolist().", "This operation is not differentiable.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.itemsize", "path": "generated/torch.tensor.itemsize", "type": "Tensor", "text": ["Alias for element_size()"]}, {"name": "torch.Tensor.torch.Tensor.kthvalue", "path": "generated/torch.tensor.kthvalue", "type": "Tensor", "text": ["See torch.kthvalue()"]}, {"name": "torch.Tensor.torch.Tensor.lcm", "path": "generated/torch.tensor.lcm", "type": "Tensor", "text": ["See torch.lcm()"]}, {"name": "torch.Tensor.torch.Tensor.lcm_", "path": "generated/torch.tensor.lcm_", "type": "Tensor", "text": ["In-place version of lcm()"]}, {"name": "torch.Tensor.torch.Tensor.ldexp", "path": "generated/torch.tensor.ldexp", "type": "Tensor", "text": ["See torch.ldexp()"]}, {"name": "torch.Tensor.torch.Tensor.ldexp_", "path": "generated/torch.tensor.ldexp_", "type": "Tensor", "text": ["In-place version of ldexp()"]}, {"name": "torch.Tensor.torch.Tensor.le", "path": "generated/torch.tensor.le", "type": "Tensor", "text": ["See torch.le()."]}, {"name": "torch.Tensor.torch.Tensor.le_", "path": "generated/torch.tensor.le_", "type": "Tensor", "text": ["In-place version of le()."]}, {"name": "torch.Tensor.torch.Tensor.lerp", "path": "generated/torch.tensor.lerp", "type": "Tensor", "text": ["See torch.lerp()"]}, {"name": "torch.Tensor.torch.Tensor.lerp_", "path": "generated/torch.tensor.lerp_", "type": "Tensor", "text": ["In-place version of lerp()"]}, {"name": "torch.Tensor.torch.Tensor.less", "path": "generated/torch.tensor.less", "type": "Tensor", "text": ["lt(other) -> Tensor", "See torch.less()."]}, {"name": "torch.Tensor.torch.Tensor.less_", "path": "generated/torch.tensor.less_", "type": "Tensor", "text": ["In-place version of less()."]}, {"name": "torch.Tensor.torch.Tensor.less_equal", "path": "generated/torch.tensor.less_equal", "type": "Tensor", "text": ["See torch.less_equal()."]}, {"name": "torch.Tensor.torch.Tensor.less_equal_", "path": "generated/torch.tensor.less_equal_", "type": "Tensor", "text": ["In-place version of less_equal()."]}, {"name": "torch.Tensor.torch.Tensor.lgamma", "path": "generated/torch.tensor.lgamma", "type": "Tensor", "text": ["See torch.lgamma()"]}, {"name": "torch.Tensor.torch.Tensor.lgamma_", "path": "generated/torch.tensor.lgamma_", "type": "Tensor", "text": ["In-place version of lgamma()"]}, {"name": "torch.Tensor.torch.Tensor.log", "path": "generated/torch.tensor.log", "type": "Tensor", "text": ["See torch.log()"]}, {"name": "torch.Tensor.torch.Tensor.log10", "path": "generated/torch.tensor.log10", "type": "Tensor", "text": ["See torch.log10()"]}, {"name": "torch.Tensor.torch.Tensor.log10_", "path": "generated/torch.tensor.log10_", "type": "Tensor", "text": ["In-place version of log10()"]}, {"name": "torch.Tensor.torch.Tensor.log1p", "path": "generated/torch.tensor.log1p", "type": "Tensor", "text": ["See torch.log1p()"]}, {"name": "torch.Tensor.torch.Tensor.log1p_", "path": "generated/torch.tensor.log1p_", "type": "Tensor", "text": ["In-place version of log1p()"]}, {"name": "torch.Tensor.torch.Tensor.log2", "path": "generated/torch.tensor.log2", "type": "Tensor", "text": ["See torch.log2()"]}, {"name": "torch.Tensor.torch.Tensor.log2_", "path": "generated/torch.tensor.log2_", "type": "Tensor", "text": ["In-place version of log2()"]}, {"name": "torch.Tensor.torch.Tensor.log_", "path": "generated/torch.tensor.log_", "type": "Tensor", "text": ["In-place version of log()"]}, {"name": "torch.Tensor.torch.Tensor.log_normal_", "path": "generated/torch.tensor.log_normal_", "type": "Tensor", "text": ["Fills self tensor with numbers samples from the log-normal distribution parameterized by the given mean \u03bc\\mu and standard deviation \u03c3\\sigma. Note that mean and std are the mean and standard deviation of the underlying normal distribution, and not of the returned distribution:"]}, {"name": "torch.Tensor.torch.Tensor.logaddexp", "path": "generated/torch.tensor.logaddexp", "type": "Tensor", "text": ["See torch.logaddexp()"]}, {"name": "torch.Tensor.torch.Tensor.logaddexp2", "path": "generated/torch.tensor.logaddexp2", "type": "Tensor", "text": ["See torch.logaddexp2()"]}, {"name": "torch.Tensor.torch.Tensor.logcumsumexp", "path": "generated/torch.tensor.logcumsumexp", "type": "Tensor", "text": ["See torch.logcumsumexp()"]}, {"name": "torch.Tensor.torch.Tensor.logdet", "path": "generated/torch.tensor.logdet", "type": "Tensor", "text": ["See torch.logdet()"]}, {"name": "torch.Tensor.torch.Tensor.logical_and", "path": "generated/torch.tensor.logical_and", "type": "Tensor", "text": ["See torch.logical_and()"]}, {"name": "torch.Tensor.torch.Tensor.logical_and_", "path": "generated/torch.tensor.logical_and_", "type": "Tensor", "text": ["In-place version of logical_and()"]}, {"name": "torch.Tensor.torch.Tensor.logical_not", "path": "generated/torch.tensor.logical_not", "type": "Tensor", "text": ["See torch.logical_not()"]}, {"name": "torch.Tensor.torch.Tensor.logical_not_", "path": "generated/torch.tensor.logical_not_", "type": "Tensor", "text": ["In-place version of logical_not()"]}, {"name": "torch.Tensor.torch.Tensor.logical_or", "path": "generated/torch.tensor.logical_or", "type": "Tensor", "text": ["See torch.logical_or()"]}, {"name": "torch.Tensor.torch.Tensor.logical_or_", "path": "generated/torch.tensor.logical_or_", "type": "Tensor", "text": ["In-place version of logical_or()"]}, {"name": "torch.Tensor.torch.Tensor.logical_xor", "path": "generated/torch.tensor.logical_xor", "type": "Tensor", "text": ["See torch.logical_xor()"]}, {"name": "torch.Tensor.torch.Tensor.logical_xor_", "path": "generated/torch.tensor.logical_xor_", "type": "Tensor", "text": ["In-place version of logical_xor()"]}, {"name": "torch.Tensor.torch.Tensor.logit", "path": "generated/torch.tensor.logit", "type": "Tensor", "text": ["See torch.logit()"]}, {"name": "torch.Tensor.torch.Tensor.logit_", "path": "generated/torch.tensor.logit_", "type": "Tensor", "text": ["In-place version of logit()"]}, {"name": "torch.Tensor.torch.Tensor.logsumexp", "path": "generated/torch.tensor.logsumexp", "type": "Tensor", "text": ["See torch.logsumexp()"]}, {"name": "torch.Tensor.torch.Tensor.long", "path": "generated/torch.tensor.long", "type": "Tensor", "text": ["self.long() is equivalent to self.to(torch.int64). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.torch.Tensor.lt", "path": "generated/torch.tensor.lt", "type": "Tensor", "text": ["See torch.lt()."]}, {"name": "torch.Tensor.torch.Tensor.lt_", "path": "generated/torch.tensor.lt_", "type": "Tensor", "text": ["In-place version of lt()."]}, {"name": "torch.Tensor.torch.Tensor.lu", "path": "generated/torch.tensor.lu", "type": "Tensor", "text": ["See torch.lu()"]}, {"name": "torch.Tensor.torch.Tensor.lu_solve", "path": "generated/torch.tensor.lu_solve", "type": "Tensor", "text": ["See torch.lu_solve()"]}, {"name": "torch.Tensor.torch.Tensor.map_", "path": "generated/torch.tensor.map_", "type": "Tensor", "text": ["Applies callable for each element in self tensor and the given tensor and stores the results in self tensor. self tensor and the given tensor must be broadcastable.", "The callable should have the signature:"]}, {"name": "torch.Tensor.torch.Tensor.masked_fill", "path": "generated/torch.tensor.masked_fill", "type": "Tensor", "text": ["Out-of-place version of torch.Tensor.masked_fill_()"]}, {"name": "torch.Tensor.torch.Tensor.masked_fill_", "path": "generated/torch.tensor.masked_fill_", "type": "Tensor", "text": ["Fills elements of self tensor with value where mask is True. The shape of mask must be broadcastable with the shape of the underlying tensor."]}, {"name": "torch.Tensor.torch.Tensor.masked_scatter", "path": "generated/torch.tensor.masked_scatter", "type": "Tensor", "text": ["Out-of-place version of torch.Tensor.masked_scatter_()", "Note", "The inputs self and mask broadcast."]}, {"name": "torch.Tensor.torch.Tensor.masked_scatter_", "path": "generated/torch.tensor.masked_scatter_", "type": "Tensor", "text": ["Copies elements from source into self tensor at positions where the mask is True. Elements from source are copied into self starting at position 0 of source and continuing in order one-by-one for each occurrence of mask being True. The shape of mask must be broadcastable with the shape of the underlying tensor. The source should have at least as many elements as the number of ones in mask.", "Note", "The mask operates on the self tensor, not on the given source tensor."]}, {"name": "torch.Tensor.torch.Tensor.masked_select", "path": "generated/torch.tensor.masked_select", "type": "Tensor", "text": ["See torch.masked_select()"]}, {"name": "torch.Tensor.torch.Tensor.matmul", "path": "generated/torch.tensor.matmul", "type": "Tensor", "text": ["See torch.matmul()"]}, {"name": "torch.Tensor.torch.Tensor.matrix_exp", "path": "generated/torch.tensor.matrix_exp", "type": "Tensor", "text": ["See torch.matrix_exp()"]}, {"name": "torch.Tensor.torch.Tensor.matrix_power", "path": "generated/torch.tensor.matrix_power", "type": "Tensor", "text": ["Note", "matrix_power() is deprecated, use torch.linalg.matrix_power() instead.", "Alias for torch.linalg.matrix_power()"]}, {"name": "torch.Tensor.torch.Tensor.max", "path": "generated/torch.tensor.max", "type": "Tensor", "text": ["See torch.max()"]}, {"name": "torch.Tensor.torch.Tensor.maximum", "path": "generated/torch.tensor.maximum", "type": "Tensor", "text": ["See torch.maximum()"]}, {"name": "torch.Tensor.torch.Tensor.mean", "path": "generated/torch.tensor.mean", "type": "Tensor", "text": ["See torch.mean()"]}, {"name": "torch.Tensor.torch.Tensor.median", "path": "generated/torch.tensor.median", "type": "Tensor", "text": ["See torch.median()"]}, {"name": "torch.Tensor.torch.Tensor.min", "path": "generated/torch.tensor.min", "type": "Tensor", "text": ["See torch.min()"]}, {"name": "torch.Tensor.torch.Tensor.minimum", "path": "generated/torch.tensor.minimum", "type": "Tensor", "text": ["See torch.minimum()"]}, {"name": "torch.Tensor.torch.Tensor.mm", "path": "generated/torch.tensor.mm", "type": "Tensor", "text": ["See torch.mm()"]}, {"name": "torch.Tensor.torch.Tensor.mode", "path": "generated/torch.tensor.mode", "type": "Tensor", "text": ["See torch.mode()"]}, {"name": "torch.Tensor.torch.Tensor.moveaxis", "path": "generated/torch.tensor.moveaxis", "type": "Tensor", "text": ["See torch.moveaxis()"]}, {"name": "torch.Tensor.torch.Tensor.movedim", "path": "generated/torch.tensor.movedim", "type": "Tensor", "text": ["See torch.movedim()"]}, {"name": "torch.Tensor.torch.Tensor.msort", "path": "generated/torch.tensor.msort", "type": "Tensor", "text": ["See torch.msort()"]}, {"name": "torch.Tensor.torch.Tensor.mul", "path": "generated/torch.tensor.mul", "type": "Tensor", "text": ["See torch.mul()."]}, {"name": "torch.Tensor.torch.Tensor.mul_", "path": "generated/torch.tensor.mul_", "type": "Tensor", "text": ["In-place version of mul()."]}, {"name": "torch.Tensor.torch.Tensor.multinomial", "path": "generated/torch.tensor.multinomial", "type": "Tensor", "text": ["See torch.multinomial()"]}, {"name": "torch.Tensor.torch.Tensor.multiply", "path": "generated/torch.tensor.multiply", "type": "Tensor", "text": ["See torch.multiply()."]}, {"name": "torch.Tensor.torch.Tensor.multiply_", "path": "generated/torch.tensor.multiply_", "type": "Tensor", "text": ["In-place version of multiply()."]}, {"name": "torch.Tensor.torch.Tensor.mv", "path": "generated/torch.tensor.mv", "type": "Tensor", "text": ["See torch.mv()"]}, {"name": "torch.Tensor.torch.Tensor.mvlgamma", "path": "generated/torch.tensor.mvlgamma", "type": "Tensor", "text": ["See torch.mvlgamma()"]}, {"name": "torch.Tensor.torch.Tensor.mvlgamma_", "path": "generated/torch.tensor.mvlgamma_", "type": "Tensor", "text": ["In-place version of mvlgamma()"]}, {"name": "torch.Tensor.torch.Tensor.nan_to_num", "path": "generated/torch.tensor.nan_to_num", "type": "Tensor", "text": ["See torch.nan_to_num()."]}, {"name": "torch.Tensor.torch.Tensor.nan_to_num_", "path": "generated/torch.tensor.nan_to_num_", "type": "Tensor", "text": ["In-place version of nan_to_num()."]}, {"name": "torch.Tensor.torch.Tensor.nanmean", "path": "generated/torch.tensor.nanmean", "type": "Tensor", "text": ["See torch.nanmean()"]}, {"name": "torch.Tensor.torch.Tensor.nanmedian", "path": "generated/torch.tensor.nanmedian", "type": "Tensor", "text": ["See torch.nanmedian()"]}, {"name": "torch.Tensor.torch.Tensor.nanquantile", "path": "generated/torch.tensor.nanquantile", "type": "Tensor", "text": ["See torch.nanquantile()"]}, {"name": "torch.Tensor.torch.Tensor.nansum", "path": "generated/torch.tensor.nansum", "type": "Tensor", "text": ["See torch.nansum()"]}, {"name": "torch.Tensor.torch.Tensor.narrow", "path": "generated/torch.tensor.narrow", "type": "Tensor", "text": ["See torch.narrow()."]}, {"name": "torch.Tensor.torch.Tensor.narrow_copy", "path": "generated/torch.tensor.narrow_copy", "type": "Tensor", "text": ["See torch.narrow_copy()."]}, {"name": "torch.Tensor.torch.Tensor.nbytes", "path": "generated/torch.tensor.nbytes", "type": "Tensor", "text": ["Returns the number of bytes consumed by the \u201cview\u201d of elements of the Tensor if the Tensor does not use sparse storage layout. Defined to be numel() * element_size()"]}, {"name": "torch.Tensor.torch.Tensor.ndim", "path": "generated/torch.tensor.ndim", "type": "Tensor", "text": ["Alias for dim()"]}, {"name": "torch.Tensor.torch.Tensor.ndimension", "path": "generated/torch.tensor.ndimension", "type": "Tensor", "text": ["Alias for dim()"]}, {"name": "torch.Tensor.torch.Tensor.ne", "path": "generated/torch.tensor.ne", "type": "Tensor", "text": ["See torch.ne()."]}, {"name": "torch.Tensor.torch.Tensor.ne_", "path": "generated/torch.tensor.ne_", "type": "Tensor", "text": ["In-place version of ne()."]}, {"name": "torch.Tensor.torch.Tensor.neg", "path": "generated/torch.tensor.neg", "type": "Tensor", "text": ["See torch.neg()"]}, {"name": "torch.Tensor.torch.Tensor.neg_", "path": "generated/torch.tensor.neg_", "type": "Tensor", "text": ["In-place version of neg()"]}, {"name": "torch.Tensor.torch.Tensor.negative", "path": "generated/torch.tensor.negative", "type": "Tensor", "text": ["See torch.negative()"]}, {"name": "torch.Tensor.torch.Tensor.negative_", "path": "generated/torch.tensor.negative_", "type": "Tensor", "text": ["In-place version of negative()"]}, {"name": "torch.Tensor.torch.Tensor.nelement", "path": "generated/torch.tensor.nelement", "type": "Tensor", "text": ["Alias for numel()"]}, {"name": "torch.Tensor.torch.Tensor.new_empty", "path": "generated/torch.tensor.new_empty", "type": "Tensor", "text": ["Returns a Tensor of size size filled with uninitialized data. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.", "size (int...) \u2013 a list, tuple, or torch.Size of integers defining the shape of the output tensor.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.new_full", "path": "generated/torch.tensor.new_full", "type": "Tensor", "text": ["Returns a Tensor of size size filled with fill_value. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.", "fill_value (scalar) \u2013 the number to fill the output tensor with.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.new_ones", "path": "generated/torch.tensor.new_ones", "type": "Tensor", "text": ["Returns a Tensor of size size filled with 1. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.", "size (int...) \u2013 a list, tuple, or torch.Size of integers defining the shape of the output tensor.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.new_tensor", "path": "generated/torch.tensor.new_tensor", "type": "Tensor", "text": ["Returns a new Tensor with data as the tensor data. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.", "Warning", "new_tensor() always copies data. If you have a Tensor data and want to avoid a copy, use torch.Tensor.requires_grad_() or torch.Tensor.detach(). If you have a numpy array and want to avoid a copy, use torch.from_numpy().", "Warning", "When data is a tensor x, new_tensor() reads out \u2018the data\u2019 from whatever it is passed, and constructs a leaf variable. Therefore tensor.new_tensor(x) is equivalent to x.clone().detach() and tensor.new_tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True). The equivalents using clone() and detach() are recommended.", "data (array_like) \u2013 The returned Tensor copies data.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.new_zeros", "path": "generated/torch.tensor.new_zeros", "type": "Tensor", "text": ["Returns a Tensor of size size filled with 0. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.", "size (int...) \u2013 a list, tuple, or torch.Size of integers defining the shape of the output tensor.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.nextafter", "path": "generated/torch.tensor.nextafter", "type": "Tensor", "text": ["See torch.nextafter()"]}, {"name": "torch.Tensor.torch.Tensor.nextafter_", "path": "generated/torch.tensor.nextafter_", "type": "Tensor", "text": ["In-place version of nextafter()"]}, {"name": "torch.Tensor.torch.Tensor.nonzero", "path": "generated/torch.tensor.nonzero", "type": "Tensor", "text": ["See torch.nonzero()"]}, {"name": "torch.Tensor.torch.Tensor.norm", "path": "generated/torch.tensor.norm", "type": "Tensor", "text": ["See torch.norm()"]}, {"name": "torch.Tensor.torch.Tensor.normal_", "path": "generated/torch.tensor.normal_", "type": "Tensor", "text": ["Fills self tensor with elements samples from the normal distribution parameterized by mean and std."]}, {"name": "torch.Tensor.torch.Tensor.not_equal", "path": "generated/torch.tensor.not_equal", "type": "Tensor", "text": ["See torch.not_equal()."]}, {"name": "torch.Tensor.torch.Tensor.not_equal_", "path": "generated/torch.tensor.not_equal_", "type": "Tensor", "text": ["In-place version of not_equal()."]}, {"name": "torch.Tensor.torch.Tensor.numel", "path": "generated/torch.tensor.numel", "type": "Tensor", "text": ["See torch.numel()"]}, {"name": "torch.Tensor.torch.Tensor.numpy", "path": "generated/torch.tensor.numpy", "type": "Tensor", "text": ["Returns the tensor as a NumPy ndarray.", "If force is False (the default), the conversion is performed only if the tensor is on the CPU, does not require grad, does not have its conjugate bit set, and is a dtype and layout that NumPy supports. The returned ndarray and the tensor will share their storage, so changes to the tensor will be reflected in the ndarray and vice versa.", "If force is True this is equivalent to calling t.detach().cpu().resolve_conj().resolve_neg().numpy(). If the tensor isn\u2019t on the CPU or the conjugate or negative bit is set, the tensor won\u2019t share its storage with the returned ndarray. Setting force to True can be a useful shorthand.", "force (bool) \u2013 if True, the ndarray may be a copy of the tensor instead of always sharing memory, defaults to False."]}, {"name": "torch.Tensor.torch.Tensor.orgqr", "path": "generated/torch.tensor.orgqr", "type": "Tensor", "text": ["See torch.orgqr()"]}, {"name": "torch.Tensor.torch.Tensor.ormqr", "path": "generated/torch.tensor.ormqr", "type": "Tensor", "text": ["See torch.ormqr()"]}, {"name": "torch.Tensor.torch.Tensor.outer", "path": "generated/torch.tensor.outer", "type": "Tensor", "text": ["See torch.outer()."]}, {"name": "torch.Tensor.torch.Tensor.permute", "path": "generated/torch.tensor.permute", "type": "Tensor", "text": ["See torch.permute()"]}, {"name": "torch.Tensor.torch.Tensor.pin_memory", "path": "generated/torch.tensor.pin_memory", "type": "Tensor", "text": ["Copies the tensor to pinned memory, if it\u2019s not already pinned."]}, {"name": "torch.Tensor.torch.Tensor.pinverse", "path": "generated/torch.tensor.pinverse", "type": "Tensor", "text": ["See torch.pinverse()"]}, {"name": "torch.Tensor.torch.Tensor.polygamma", "path": "generated/torch.tensor.polygamma", "type": "Tensor", "text": ["See torch.polygamma()"]}, {"name": "torch.Tensor.torch.Tensor.polygamma_", "path": "generated/torch.tensor.polygamma_", "type": "Tensor", "text": ["In-place version of polygamma()"]}, {"name": "torch.Tensor.torch.Tensor.positive", "path": "generated/torch.tensor.positive", "type": "Tensor", "text": ["See torch.positive()"]}, {"name": "torch.Tensor.torch.Tensor.pow", "path": "generated/torch.tensor.pow", "type": "Tensor", "text": ["See torch.pow()"]}, {"name": "torch.Tensor.torch.Tensor.pow_", "path": "generated/torch.tensor.pow_", "type": "Tensor", "text": ["In-place version of pow()"]}, {"name": "torch.Tensor.torch.Tensor.prod", "path": "generated/torch.tensor.prod", "type": "Tensor", "text": ["See torch.prod()"]}, {"name": "torch.Tensor.torch.Tensor.put_", "path": "generated/torch.tensor.put_", "type": "Tensor", "text": ["Copies the elements from source into the positions specified by index. For the purpose of indexing, the self tensor is treated as if it were a 1-D tensor.", "index and source need to have the same number of elements, but not necessarily the same shape.", "If accumulate is True, the elements in source are added to self. If accumulate is False, the behavior is undefined if index contain duplicate elements.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.q_per_channel_axis", "path": "generated/torch.tensor.q_per_channel_axis", "type": "Tensor", "text": ["Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied."]}, {"name": "torch.Tensor.torch.Tensor.q_per_channel_scales", "path": "generated/torch.tensor.q_per_channel_scales", "type": "Tensor", "text": ["Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor."]}, {"name": "torch.Tensor.torch.Tensor.q_per_channel_zero_points", "path": "generated/torch.tensor.q_per_channel_zero_points", "type": "Tensor", "text": ["Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor."]}, {"name": "torch.Tensor.torch.Tensor.q_scale", "path": "generated/torch.tensor.q_scale", "type": "Tensor", "text": ["Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer()."]}, {"name": "torch.Tensor.torch.Tensor.q_zero_point", "path": "generated/torch.tensor.q_zero_point", "type": "Tensor", "text": ["Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer()."]}, {"name": "torch.Tensor.torch.Tensor.qr", "path": "generated/torch.tensor.qr", "type": "Tensor", "text": ["See torch.qr()"]}, {"name": "torch.Tensor.torch.Tensor.qscheme", "path": "generated/torch.tensor.qscheme", "type": "Tensor", "text": ["Returns the quantization scheme of a given QTensor."]}, {"name": "torch.Tensor.torch.Tensor.quantile", "path": "generated/torch.tensor.quantile", "type": "Tensor", "text": ["See torch.quantile()"]}, {"name": "torch.Tensor.torch.Tensor.rad2deg", "path": "generated/torch.tensor.rad2deg", "type": "Tensor", "text": ["See torch.rad2deg()"]}, {"name": "torch.Tensor.torch.Tensor.random_", "path": "generated/torch.tensor.random_", "type": "Tensor", "text": ["Fills self tensor with numbers sampled from the discrete uniform distribution over [from, to - 1]. If not specified, the values are usually only bounded by self tensor\u2019s data type. However, for floating point types, if unspecified, range will be [0, 2^mantissa] to ensure that every value is representable. For example, torch.tensor(1, dtype=torch.double).random_() will be uniform in [0, 2^53]."]}, {"name": "torch.Tensor.torch.Tensor.ravel", "path": "generated/torch.tensor.ravel", "type": "Tensor", "text": ["see torch.ravel()"]}, {"name": "torch.Tensor.torch.Tensor.real", "path": "generated/torch.tensor.real", "type": "Tensor", "text": ["Returns a new tensor containing real values of the self tensor for a complex-valued input tensor. The returned tensor and self share the same underlying storage.", "Returns self if self is a real-valued tensor tensor."]}, {"name": "torch.Tensor.torch.Tensor.reciprocal", "path": "generated/torch.tensor.reciprocal", "type": "Tensor", "text": ["See torch.reciprocal()"]}, {"name": "torch.Tensor.torch.Tensor.reciprocal_", "path": "generated/torch.tensor.reciprocal_", "type": "Tensor", "text": ["In-place version of reciprocal()"]}, {"name": "torch.Tensor.torch.Tensor.record_stream", "path": "generated/torch.tensor.record_stream", "type": "Tensor", "text": ["Ensures that the tensor memory is not reused for another tensor until all current work queued on stream are complete.", "Note", "The caching allocator is aware of only the stream where a tensor was allocated. Due to the awareness, it already correctly manages the life cycle of tensors on only one stream. But if a tensor is used on a stream different from the stream of origin, the allocator might reuse the memory unexpectedly. Calling this method lets the allocator know which streams have used the tensor."]}, {"name": "torch.Tensor.torch.Tensor.register_hook", "path": "generated/torch.tensor.register_hook", "type": "Tensor", "text": ["Registers a backward hook.", "The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature:", "The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of grad.", "This function returns a handle with a method handle.remove() that removes the hook from the module.", "Note", "See Backward Hooks execution for more information on how when this hook is executed, and how its execution is ordered relative to other hooks.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.register_post_accumulate_grad_hook", "path": "generated/torch.tensor.register_post_accumulate_grad_hook", "type": "Tensor", "text": ["Registers a backward hook that runs after grad accumulation.", "The hook will be called after all gradients for a tensor have been accumulated, meaning that the .grad field has been updated on that tensor. The post accumulate grad hook is ONLY applicable for leaf tensors (tensors without a .grad_fn field). Registering this hook on a non-leaf tensor will error!", "The hook should have the following signature:", "Note that, unlike other autograd hooks, this hook operates on the tensor that requires grad and not the grad itself. The hook can in-place modify and access its Tensor argument, including its .grad field.", "This function returns a handle with a method handle.remove() that removes the hook from the module.", "Note", "See Backward Hooks execution for more information on how when this hook is executed, and how its execution is ordered relative to other hooks. Since this hook runs during the backward pass, it will run in no_grad mode (unless create_graph is True). You can use torch.enable_grad() to re-enable autograd within the hook if you need it.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.remainder", "path": "generated/torch.tensor.remainder", "type": "Tensor", "text": ["See torch.remainder()"]}, {"name": "torch.Tensor.torch.Tensor.remainder_", "path": "generated/torch.tensor.remainder_", "type": "Tensor", "text": ["In-place version of remainder()"]}, {"name": "torch.Tensor.torch.Tensor.renorm", "path": "generated/torch.tensor.renorm", "type": "Tensor", "text": ["See torch.renorm()"]}, {"name": "torch.Tensor.torch.Tensor.renorm_", "path": "generated/torch.tensor.renorm_", "type": "Tensor", "text": ["In-place version of renorm()"]}, {"name": "torch.Tensor.torch.Tensor.repeat", "path": "generated/torch.tensor.repeat", "type": "Tensor", "text": ["Repeats this tensor along the specified dimensions.", "Unlike expand(), this function copies the tensor\u2019s data.", "Warning", "repeat() behaves differently from numpy.repeat, but is more similar to numpy.tile. For the operator similar to numpy.repeat, see torch.repeat_interleave().", "sizes (torch.Size or int...) \u2013 The number of times to repeat this tensor along each dimension", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.repeat_interleave", "path": "generated/torch.tensor.repeat_interleave", "type": "Tensor", "text": ["See torch.repeat_interleave()."]}, {"name": "torch.Tensor.torch.Tensor.requires_grad", "path": "generated/torch.tensor.requires_grad", "type": "Tensor", "text": ["Is True if gradients need to be computed for this Tensor, False otherwise.", "Note", "The fact that gradients need to be computed for a Tensor do not mean that the grad attribute will be populated, see is_leaf for more details."]}, {"name": "torch.Tensor.torch.Tensor.requires_grad_", "path": "generated/torch.tensor.requires_grad_", "type": "Tensor", "text": ["Change if autograd should record operations on this tensor: sets this tensor\u2019s requires_grad attribute in-place. Returns this tensor.", "requires_grad_()\u2019s main use case is to tell autograd to begin recording operations on a Tensor tensor. If tensor has requires_grad=False (because it was obtained through a DataLoader, or required preprocessing or initialization), tensor.requires_grad_() makes it so that autograd will begin to record operations on tensor.", "requires_grad (bool) \u2013 If autograd should record operations on this tensor. Default: True.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.reshape", "path": "generated/torch.tensor.reshape", "type": "Tensor", "text": ["Returns a tensor with the same data and number of elements as self but with the specified shape. This method returns a view if shape is compatible with the current shape. See torch.Tensor.view() on when it is possible to return a view.", "See torch.reshape()", "shape (tuple of ints or int...) \u2013 the desired shape"]}, {"name": "torch.Tensor.torch.Tensor.reshape_as", "path": "generated/torch.tensor.reshape_as", "type": "Tensor", "text": ["Returns this tensor as the same shape as other. self.reshape_as(other) is equivalent to self.reshape(other.sizes()). This method returns a view if other.sizes() is compatible with the current shape. See torch.Tensor.view() on when it is possible to return a view.", "Please see reshape() for more information about reshape.", "other (torch.Tensor) \u2013 The result tensor has the same shape as other."]}, {"name": "torch.Tensor.torch.Tensor.resize_", "path": "generated/torch.tensor.resize_", "type": "Tensor", "text": ["Resizes self tensor to the specified size. If the number of elements is larger than the current storage size, then the underlying storage is resized to fit the new number of elements. If the number of elements is smaller, the underlying storage is not changed. Existing elements are preserved but any new memory is uninitialized.", "Warning", "This is a low-level method. The storage is reinterpreted as C-contiguous, ignoring the current strides (unless the target size equals the current size, in which case the tensor is left unchanged). For most purposes, you will instead want to use view(), which checks for contiguity, or reshape(), which copies data if needed. To change the size in-place with custom strides, see set_().", "Note", "If torch.use_deterministic_algorithms() is set to True, new elements are initialized to prevent nondeterministic behavior from using the result as an input to an operation. Floating point and complex values are set to NaN, and integer values are set to the maximum value.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.resize_as_", "path": "generated/torch.tensor.resize_as_", "type": "Tensor", "text": ["Resizes the self tensor to be the same size as the specified tensor. This is equivalent to self.resize_(tensor.size()).", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of Tensor. Default: torch.contiguous_format. Note that memory format of self is going to be unaffected if self.size() matches tensor.size()."]}, {"name": "torch.Tensor.torch.Tensor.resolve_conj", "path": "generated/torch.tensor.resolve_conj", "type": "Tensor", "text": ["See torch.resolve_conj()"]}, {"name": "torch.Tensor.torch.Tensor.resolve_neg", "path": "generated/torch.tensor.resolve_neg", "type": "Tensor", "text": ["See torch.resolve_neg()"]}, {"name": "torch.Tensor.torch.Tensor.retain_grad", "path": "generated/torch.tensor.retain_grad", "type": "Tensor", "text": ["Enables this Tensor to have their grad populated during backward(). This is a no-op for leaf tensors."]}, {"name": "torch.Tensor.torch.Tensor.retains_grad", "path": "generated/torch.tensor.retains_grad", "type": "Tensor", "text": ["Is True if this Tensor is non-leaf and its grad is enabled to be populated during backward(), False otherwise."]}, {"name": "torch.Tensor.torch.Tensor.roll", "path": "generated/torch.tensor.roll", "type": "Tensor", "text": ["See torch.roll()"]}, {"name": "torch.Tensor.torch.Tensor.rot90", "path": "generated/torch.tensor.rot90", "type": "Tensor", "text": ["See torch.rot90()"]}, {"name": "torch.Tensor.torch.Tensor.round", "path": "generated/torch.tensor.round", "type": "Tensor", "text": ["See torch.round()"]}, {"name": "torch.Tensor.torch.Tensor.round_", "path": "generated/torch.tensor.round_", "type": "Tensor", "text": ["In-place version of round()"]}, {"name": "torch.Tensor.torch.Tensor.rsqrt", "path": "generated/torch.tensor.rsqrt", "type": "Tensor", "text": ["See torch.rsqrt()"]}, {"name": "torch.Tensor.torch.Tensor.rsqrt_", "path": "generated/torch.tensor.rsqrt_", "type": "Tensor", "text": ["In-place version of rsqrt()"]}, {"name": "torch.Tensor.torch.Tensor.scatter", "path": "generated/torch.tensor.scatter", "type": "Tensor", "text": ["Out-of-place version of torch.Tensor.scatter_()"]}, {"name": "torch.Tensor.torch.Tensor.scatter_", "path": "generated/torch.tensor.scatter_", "type": "Tensor", "text": ["Writes all values from the tensor src into self at the indices specified in the index tensor. For each value in src, its output index is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim.", "For a 3-D tensor, self is updated as:", "This is the reverse operation of the manner described in gather().", "self, index and src (if it is a Tensor) should all have the same number of dimensions. It is also required that index.size(d) <= src.size(d) for all dimensions d, and that index.size(d) <= self.size(d) for all dimensions d != dim. Note that index and src do not broadcast.", "Moreover, as for gather(), the values of index must be between 0 and self.size(dim) - 1 inclusive.", "Warning", "When indices are not unique, the behavior is non-deterministic (one of the values from src will be picked arbitrarily) and the gradient will be incorrect (it will be propagated to all locations in the source that correspond to the same index)!", "Note", "The backward pass is implemented only for src.shape == index.shape.", "Additionally accepts an optional reduce argument that allows specification of an optional reduction operation, which is applied to all values in the tensor src into self at the indices specified in the index. For each value in src, the reduction operation is applied to an index in self which is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim.", "Given a 3-D tensor and reduction using the multiplication operation, self is updated as:", "Reducing with the addition operation is the same as using scatter_add_().", "Warning", "The reduce argument with Tensor src is deprecated and will be removed in a future PyTorch release. Please use scatter_reduce_() instead for more reduction options.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.scatter_add", "path": "generated/torch.tensor.scatter_add", "type": "Tensor", "text": ["Out-of-place version of torch.Tensor.scatter_add_()"]}, {"name": "torch.Tensor.torch.Tensor.scatter_add_", "path": "generated/torch.tensor.scatter_add_", "type": "Tensor", "text": ["Adds all values from the tensor src into self at the indices specified in the index tensor in a similar fashion as scatter_(). For each value in src, it is added to an index in self which is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim.", "For a 3-D tensor, self is updated as:", "self, index and src should have same number of dimensions. It is also required that index.size(d) <= src.size(d) for all dimensions d, and that index.size(d) <= self.size(d) for all dimensions d != dim. Note that index and src do not broadcast.", "Note", "This operation may behave nondeterministically when given tensors on a CUDA device. See Reproducibility for more information.", "Note", "The backward pass is implemented only for src.shape == index.shape.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.scatter_reduce", "path": "generated/torch.tensor.scatter_reduce", "type": "Tensor", "text": ["Out-of-place version of torch.Tensor.scatter_reduce_()"]}, {"name": "torch.Tensor.torch.Tensor.scatter_reduce_", "path": "generated/torch.tensor.scatter_reduce_", "type": "Tensor", "text": ["Reduces all values from the src tensor to the indices specified in the index tensor in the self tensor using the applied reduction defined via the reduce argument (\"sum\", \"prod\", \"mean\", \"amax\", \"amin\"). For each value in src, it is reduced to an index in self which is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim. If include_self=\"True\", the values in the self tensor are included in the reduction.", "self, index and src should all have the same number of dimensions. It is also required that index.size(d) <= src.size(d) for all dimensions d, and that index.size(d) <= self.size(d) for all dimensions d != dim. Note that index and src do not broadcast.", "For a 3-D tensor with reduce=\"sum\" and include_self=True the output is given as:", "Note", "This operation may behave nondeterministically when given tensors on a CUDA device. See Reproducibility for more information.", "Note", "The backward pass is implemented only for src.shape == index.shape.", "Warning", "This function is in beta and may change in the near future.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.select", "path": "generated/torch.tensor.select", "type": "Tensor", "text": ["See torch.select()"]}, {"name": "torch.Tensor.torch.Tensor.select_scatter", "path": "generated/torch.tensor.select_scatter", "type": "Tensor", "text": ["See torch.select_scatter()"]}, {"name": "torch.Tensor.torch.Tensor.set_", "path": "generated/torch.tensor.set_", "type": "Tensor", "text": ["Sets the underlying storage, size, and strides. If source is a tensor, self tensor will share the same storage and have the same size and strides as source. Changes to elements in one tensor will be reflected in the other.", "If source is a Storage, the method sets the underlying storage, offset, size, and stride."]}, {"name": "torch.Tensor.torch.Tensor.sgn", "path": "generated/torch.tensor.sgn", "type": "Tensor", "text": ["See torch.sgn()"]}, {"name": "torch.Tensor.torch.Tensor.sgn_", "path": "generated/torch.tensor.sgn_", "type": "Tensor", "text": ["In-place version of sgn()"]}, {"name": "torch.Tensor.torch.Tensor.shape", "path": "generated/torch.tensor.shape", "type": "Tensor", "text": ["Returns the size of the self tensor. Alias for size.", "See also Tensor.size().", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.share_memory_", "path": "generated/torch.tensor.share_memory_", "type": "Tensor", "text": ["Moves the underlying storage to shared memory.", "This is a no-op if the underlying storage is already in shared memory and for CUDA tensors. Tensors in shared memory cannot be resized."]}, {"name": "torch.Tensor.torch.Tensor.short", "path": "generated/torch.tensor.short", "type": "Tensor", "text": ["self.short() is equivalent to self.to(torch.int16). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.torch.Tensor.sigmoid", "path": "generated/torch.tensor.sigmoid", "type": "Tensor", "text": ["See torch.sigmoid()"]}, {"name": "torch.Tensor.torch.Tensor.sigmoid_", "path": "generated/torch.tensor.sigmoid_", "type": "Tensor", "text": ["In-place version of sigmoid()"]}, {"name": "torch.Tensor.torch.Tensor.sign", "path": "generated/torch.tensor.sign", "type": "Tensor", "text": ["See torch.sign()"]}, {"name": "torch.Tensor.torch.Tensor.sign_", "path": "generated/torch.tensor.sign_", "type": "Tensor", "text": ["In-place version of sign()"]}, {"name": "torch.Tensor.torch.Tensor.signbit", "path": "generated/torch.tensor.signbit", "type": "Tensor", "text": ["See torch.signbit()"]}, {"name": "torch.Tensor.torch.Tensor.sin", "path": "generated/torch.tensor.sin", "type": "Tensor", "text": ["See torch.sin()"]}, {"name": "torch.Tensor.torch.Tensor.sin_", "path": "generated/torch.tensor.sin_", "type": "Tensor", "text": ["In-place version of sin()"]}, {"name": "torch.Tensor.torch.Tensor.sinc", "path": "generated/torch.tensor.sinc", "type": "Tensor", "text": ["See torch.sinc()"]}, {"name": "torch.Tensor.torch.Tensor.sinc_", "path": "generated/torch.tensor.sinc_", "type": "Tensor", "text": ["In-place version of sinc()"]}, {"name": "torch.Tensor.torch.Tensor.sinh", "path": "generated/torch.tensor.sinh", "type": "Tensor", "text": ["See torch.sinh()"]}, {"name": "torch.Tensor.torch.Tensor.sinh_", "path": "generated/torch.tensor.sinh_", "type": "Tensor", "text": ["In-place version of sinh()"]}, {"name": "torch.Tensor.torch.Tensor.size", "path": "generated/torch.tensor.size", "type": "Tensor", "text": ["Returns the size of the self tensor. If dim is not specified, the returned value is a torch.Size, a subclass of tuple. If dim is specified, returns an int holding the size of that dimension.", "dim (int, optional) \u2013 The dimension for which to retrieve the size.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.slice_scatter", "path": "generated/torch.tensor.slice_scatter", "type": "Tensor", "text": ["See torch.slice_scatter()"]}, {"name": "torch.Tensor.torch.Tensor.slogdet", "path": "generated/torch.tensor.slogdet", "type": "Tensor", "text": ["See torch.slogdet()"]}, {"name": "torch.Tensor.torch.Tensor.smm", "path": "generated/torch.tensor.smm", "type": "Tensor", "text": ["See torch.smm()"]}, {"name": "torch.Tensor.torch.Tensor.softmax", "path": "generated/torch.tensor.softmax", "type": "Tensor", "text": ["Alias for torch.nn.functional.softmax()."]}, {"name": "torch.Tensor.torch.Tensor.sort", "path": "generated/torch.tensor.sort", "type": "Tensor", "text": ["See torch.sort()"]}, {"name": "torch.Tensor.torch.Tensor.sparse_dim", "path": "generated/torch.tensor.sparse_dim", "type": "Tensor", "text": ["Return the number of sparse dimensions in a sparse tensor self.", "Note", "Returns 0 if self is not a sparse tensor.", "See also Tensor.dense_dim() and hybrid tensors."]}, {"name": "torch.Tensor.torch.Tensor.sparse_mask", "path": "generated/torch.tensor.sparse_mask", "type": "Tensor", "text": ["Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. The values of mask sparse tensor are ignored. self and mask tensors must have the same shape.", "Note", "The returned sparse tensor might contain duplicate values if mask is not coalesced. It is therefore advisable to pass mask.coalesce() if such behavior is not desired.", "Note", "The returned sparse tensor has the same indices as the sparse tensor mask, even when the corresponding values in self are zeros.", "mask (Tensor) \u2013 a sparse tensor whose indices are used as a filter", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.split", "path": "generated/torch.tensor.split", "type": "Tensor", "text": ["See torch.split()"]}, {"name": "torch.Tensor.torch.Tensor.sqrt", "path": "generated/torch.tensor.sqrt", "type": "Tensor", "text": ["See torch.sqrt()"]}, {"name": "torch.Tensor.torch.Tensor.sqrt_", "path": "generated/torch.tensor.sqrt_", "type": "Tensor", "text": ["In-place version of sqrt()"]}, {"name": "torch.Tensor.torch.Tensor.square", "path": "generated/torch.tensor.square", "type": "Tensor", "text": ["See torch.square()"]}, {"name": "torch.Tensor.torch.Tensor.square_", "path": "generated/torch.tensor.square_", "type": "Tensor", "text": ["In-place version of square()"]}, {"name": "torch.Tensor.torch.Tensor.squeeze", "path": "generated/torch.tensor.squeeze", "type": "Tensor", "text": ["See torch.squeeze()"]}, {"name": "torch.Tensor.torch.Tensor.squeeze_", "path": "generated/torch.tensor.squeeze_", "type": "Tensor", "text": ["In-place version of squeeze()"]}, {"name": "torch.Tensor.torch.Tensor.sspaddmm", "path": "generated/torch.tensor.sspaddmm", "type": "Tensor", "text": ["See torch.sspaddmm()"]}, {"name": "torch.Tensor.torch.Tensor.std", "path": "generated/torch.tensor.std", "type": "Tensor", "text": ["See torch.std()"]}, {"name": "torch.Tensor.torch.Tensor.stft", "path": "generated/torch.tensor.stft", "type": "Tensor", "text": ["See torch.stft()", "Warning", "This function changed signature at version 0.4.1. Calling with the previous signature may cause error or return incorrect result."]}, {"name": "torch.Tensor.torch.Tensor.storage", "path": "generated/torch.tensor.storage", "type": "Tensor", "text": ["Returns the underlying TypedStorage.", "Warning", "TypedStorage is deprecated. It will be removed in the future, and UntypedStorage will be the only storage class. To access the UntypedStorage directly, use Tensor.untyped_storage()."]}, {"name": "torch.Tensor.torch.Tensor.storage_offset", "path": "generated/torch.tensor.storage_offset", "type": "Tensor", "text": ["Returns self tensor\u2019s offset in the underlying storage in terms of number of storage elements (not bytes).", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.storage_type", "path": "generated/torch.tensor.storage_type", "type": "Tensor", "text": ["Returns the type of the underlying storage."]}, {"name": "torch.Tensor.torch.Tensor.stride", "path": "generated/torch.tensor.stride", "type": "Tensor", "text": ["Returns the stride of self tensor.", "Stride is the jump necessary to go from one element to the next one in the specified dimension dim. A tuple of all strides is returned when no argument is passed in. Otherwise, an integer value is returned as the stride in the particular dimension dim.", "dim (int, optional) \u2013 the desired dimension in which stride is required", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.sub", "path": "generated/torch.tensor.sub", "type": "Tensor", "text": ["See torch.sub()."]}, {"name": "torch.Tensor.torch.Tensor.sub_", "path": "generated/torch.tensor.sub_", "type": "Tensor", "text": ["In-place version of sub()"]}, {"name": "torch.Tensor.torch.Tensor.subtract", "path": "generated/torch.tensor.subtract", "type": "Tensor", "text": ["See torch.subtract()."]}, {"name": "torch.Tensor.torch.Tensor.subtract_", "path": "generated/torch.tensor.subtract_", "type": "Tensor", "text": ["In-place version of subtract()."]}, {"name": "torch.Tensor.torch.Tensor.sum", "path": "generated/torch.tensor.sum", "type": "Tensor", "text": ["See torch.sum()"]}, {"name": "torch.Tensor.torch.Tensor.sum_to_size", "path": "generated/torch.tensor.sum_to_size", "type": "Tensor", "text": ["Sum this tensor to size. size must be broadcastable to this tensor size.", "size (int...) \u2013 a sequence of integers defining the shape of the output tensor."]}, {"name": "torch.Tensor.torch.Tensor.svd", "path": "generated/torch.tensor.svd", "type": "Tensor", "text": ["See torch.svd()"]}, {"name": "torch.Tensor.torch.Tensor.swapaxes", "path": "generated/torch.tensor.swapaxes", "type": "Tensor", "text": ["See torch.swapaxes()"]}, {"name": "torch.Tensor.torch.Tensor.swapdims", "path": "generated/torch.tensor.swapdims", "type": "Tensor", "text": ["See torch.swapdims()"]}, {"name": "torch.Tensor.torch.Tensor.t", "path": "generated/torch.tensor.t", "type": "Tensor", "text": ["See torch.t()"]}, {"name": "torch.Tensor.torch.Tensor.t_", "path": "generated/torch.tensor.t_", "type": "Tensor", "text": ["In-place version of t()"]}, {"name": "torch.Tensor.torch.Tensor.take", "path": "generated/torch.tensor.take", "type": "Tensor", "text": ["See torch.take()"]}, {"name": "torch.Tensor.torch.Tensor.take_along_dim", "path": "generated/torch.tensor.take_along_dim", "type": "Tensor", "text": ["See torch.take_along_dim()"]}, {"name": "torch.Tensor.torch.Tensor.tan", "path": "generated/torch.tensor.tan", "type": "Tensor", "text": ["See torch.tan()"]}, {"name": "torch.Tensor.torch.Tensor.tan_", "path": "generated/torch.tensor.tan_", "type": "Tensor", "text": ["In-place version of tan()"]}, {"name": "torch.Tensor.torch.Tensor.tanh", "path": "generated/torch.tensor.tanh", "type": "Tensor", "text": ["See torch.tanh()"]}, {"name": "torch.Tensor.torch.Tensor.tanh_", "path": "generated/torch.tensor.tanh_", "type": "Tensor", "text": ["In-place version of tanh()"]}, {"name": "torch.Tensor.torch.Tensor.tensor_split", "path": "generated/torch.tensor.tensor_split", "type": "Tensor", "text": ["See torch.tensor_split()"]}, {"name": "torch.Tensor.torch.Tensor.tile", "path": "generated/torch.tensor.tile", "type": "Tensor", "text": ["See torch.tile()"]}, {"name": "torch.Tensor.torch.Tensor.to", "path": "generated/torch.tensor.to", "type": "Tensor", "text": ["Performs Tensor dtype and/or device conversion. A torch.dtype and torch.device are inferred from the arguments of self.to(*args, **kwargs).", "Note", "If the self Tensor already has the correct torch.dtype and torch.device, then self is returned. Otherwise, the returned tensor is a copy of self with the desired torch.dtype and torch.device.", "Here are the ways to call to:", "Returns a Tensor with the specified dtype", "memory_format (torch.memory_format, optional): the desired memory format of returned Tensor. Default: torch.preserve_format.", "Returns a Tensor with the specified device and (optional) dtype. If dtype is None it is inferred to be self.dtype. When non_blocking, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When copy is set, a new Tensor is created even when the Tensor already matches the desired conversion.", "memory_format (torch.memory_format, optional): the desired memory format of returned Tensor. Default: torch.preserve_format.", "Returns a Tensor with same torch.dtype and torch.device as the Tensor other. When non_blocking, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When copy is set, a new Tensor is created even when the Tensor already matches the desired conversion.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.to_dense", "path": "generated/torch.tensor.to_dense", "type": "Tensor", "text": ["Creates a strided copy of self if self is not a strided tensor, otherwise returns self.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.to_mkldnn", "path": "generated/torch.tensor.to_mkldnn", "type": "Tensor", "text": ["Returns a copy of the tensor in torch.mkldnn layout."]}, {"name": "torch.Tensor.torch.Tensor.to_sparse", "path": "generated/torch.tensor.to_sparse", "type": "Tensor", "text": ["Returns a sparse copy of the tensor. PyTorch supports sparse tensors in coordinate format.", "sparseDims (int, optional) \u2013 the number of sparse dimensions to include in the new sparse tensor", "Example:", "Returns a sparse tensor with the specified layout and blocksize. If the self is strided, the number of dense dimensions could be specified, and a hybrid sparse tensor will be created, with dense_dim dense dimensions and self.dim() - 2 - dense_dim batch dimension.", "Note", "If the self layout and blocksize parameters match with the specified layout and blocksize, return self. Otherwise, return a sparse tensor copy of self.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.to_sparse_bsc", "path": "generated/torch.tensor.to_sparse_bsc", "type": "Tensor", "text": ["Convert a tensor to a block sparse column (BSC) storage format of given blocksize. If the self is strided, then the number of dense dimensions could be specified, and a hybrid BSC tensor will be created, with dense_dim dense dimensions and self.dim() - 2 - dense_dim batch dimension.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.to_sparse_bsr", "path": "generated/torch.tensor.to_sparse_bsr", "type": "Tensor", "text": ["Convert a tensor to a block sparse row (BSR) storage format of given blocksize. If the self is strided, then the number of dense dimensions could be specified, and a hybrid BSR tensor will be created, with dense_dim dense dimensions and self.dim() - 2 - dense_dim batch dimension.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.to_sparse_csc", "path": "generated/torch.tensor.to_sparse_csc", "type": "Tensor", "text": ["Convert a tensor to compressed column storage (CSC) format. Except for strided tensors, only works with 2D tensors. If the self is strided, then the number of dense dimensions could be specified, and a hybrid CSC tensor will be created, with dense_dim dense dimensions and self.dim() - 2 - dense_dim batch dimension.", "dense_dim (int, optional) \u2013 Number of dense dimensions of the resulting CSC tensor. This argument should be used only if self is a strided tensor, and must be a value between 0 and dimension of self tensor minus two.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.to_sparse_csr", "path": "generated/torch.tensor.to_sparse_csr", "type": "Tensor", "text": ["Convert a tensor to compressed row storage format (CSR). Except for strided tensors, only works with 2D tensors. If the self is strided, then the number of dense dimensions could be specified, and a hybrid CSR tensor will be created, with dense_dim dense dimensions and self.dim() - 2 - dense_dim batch dimension.", "dense_dim (int, optional) \u2013 Number of dense dimensions of the resulting CSR tensor. This argument should be used only if self is a strided tensor, and must be a value between 0 and dimension of self tensor minus two.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.tolist", "path": "generated/torch.tensor.tolist", "type": "Tensor", "text": ["Returns the tensor as a (nested) list. For scalars, a standard Python number is returned, just like with item(). Tensors are automatically moved to the CPU first if necessary.", "This operation is not differentiable.", "Examples:"]}, {"name": "torch.Tensor.torch.Tensor.topk", "path": "generated/torch.tensor.topk", "type": "Tensor", "text": ["See torch.topk()"]}, {"name": "torch.Tensor.torch.Tensor.trace", "path": "generated/torch.tensor.trace", "type": "Tensor", "text": ["See torch.trace()"]}, {"name": "torch.Tensor.torch.Tensor.transpose", "path": "generated/torch.tensor.transpose", "type": "Tensor", "text": ["See torch.transpose()"]}, {"name": "torch.Tensor.torch.Tensor.transpose_", "path": "generated/torch.tensor.transpose_", "type": "Tensor", "text": ["In-place version of transpose()"]}, {"name": "torch.Tensor.torch.Tensor.triangular_solve", "path": "generated/torch.tensor.triangular_solve", "type": "Tensor", "text": ["See torch.triangular_solve()"]}, {"name": "torch.Tensor.torch.Tensor.tril", "path": "generated/torch.tensor.tril", "type": "Tensor", "text": ["See torch.tril()"]}, {"name": "torch.Tensor.torch.Tensor.tril_", "path": "generated/torch.tensor.tril_", "type": "Tensor", "text": ["In-place version of tril()"]}, {"name": "torch.Tensor.torch.Tensor.triu", "path": "generated/torch.tensor.triu", "type": "Tensor", "text": ["See torch.triu()"]}, {"name": "torch.Tensor.torch.Tensor.triu_", "path": "generated/torch.tensor.triu_", "type": "Tensor", "text": ["In-place version of triu()"]}, {"name": "torch.Tensor.torch.Tensor.true_divide", "path": "generated/torch.tensor.true_divide", "type": "Tensor", "text": ["See torch.true_divide()"]}, {"name": "torch.Tensor.torch.Tensor.true_divide_", "path": "generated/torch.tensor.true_divide_", "type": "Tensor", "text": ["In-place version of true_divide_()"]}, {"name": "torch.Tensor.torch.Tensor.trunc", "path": "generated/torch.tensor.trunc", "type": "Tensor", "text": ["See torch.trunc()"]}, {"name": "torch.Tensor.torch.Tensor.trunc_", "path": "generated/torch.tensor.trunc_", "type": "Tensor", "text": ["In-place version of trunc()"]}, {"name": "torch.Tensor.torch.Tensor.type", "path": "generated/torch.tensor.type", "type": "Tensor", "text": ["Returns the type if dtype is not provided, else casts this object to the specified type.", "If this is already of the correct type, no copy is performed and the original object is returned."]}, {"name": "torch.Tensor.torch.Tensor.type_as", "path": "generated/torch.tensor.type_as", "type": "Tensor", "text": ["Returns this tensor cast to the type of the given tensor.", "This is a no-op if the tensor is already of the correct type. This is equivalent to self.type(tensor.type())", "tensor (Tensor) \u2013 the tensor which has the desired type"]}, {"name": "torch.Tensor.torch.Tensor.unbind", "path": "generated/torch.tensor.unbind", "type": "Tensor", "text": ["See torch.unbind()"]}, {"name": "torch.Tensor.torch.Tensor.unflatten", "path": "generated/torch.tensor.unflatten", "type": "Tensor", "text": ["See torch.unflatten()."]}, {"name": "torch.Tensor.torch.Tensor.unfold", "path": "generated/torch.tensor.unfold", "type": "Tensor", "text": ["Returns a view of the original tensor which contains all slices of size size from self tensor in the dimension dimension.", "Step between two slices is given by step.", "If sizedim is the size of dimension dimension for self, the size of dimension dimension in the returned tensor will be (sizedim - size) / step + 1.", "An additional dimension of size size is appended in the returned tensor.", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.uniform_", "path": "generated/torch.tensor.uniform_", "type": "Tensor", "text": ["Fills self tensor with numbers sampled from the continuous uniform distribution:"]}, {"name": "torch.Tensor.torch.Tensor.unique", "path": "generated/torch.tensor.unique", "type": "Tensor", "text": ["Returns the unique elements of the input tensor.", "See torch.unique()"]}, {"name": "torch.Tensor.torch.Tensor.unique_consecutive", "path": "generated/torch.tensor.unique_consecutive", "type": "Tensor", "text": ["Eliminates all but the first element from every consecutive group of equivalent elements.", "See torch.unique_consecutive()"]}, {"name": "torch.Tensor.torch.Tensor.unsqueeze", "path": "generated/torch.tensor.unsqueeze", "type": "Tensor", "text": ["See torch.unsqueeze()"]}, {"name": "torch.Tensor.torch.Tensor.unsqueeze_", "path": "generated/torch.tensor.unsqueeze_", "type": "Tensor", "text": ["In-place version of unsqueeze()"]}, {"name": "torch.Tensor.torch.Tensor.untyped_storage", "path": "generated/torch.tensor.untyped_storage", "type": "Tensor", "text": ["Returns the underlying UntypedStorage."]}, {"name": "torch.Tensor.torch.Tensor.values", "path": "generated/torch.tensor.values", "type": "Tensor", "text": ["Return the values tensor of a sparse COO tensor.", "Warning", "Throws an error if self is not a sparse COO tensor.", "See also Tensor.indices().", "Note", "This method can only be called on a coalesced sparse tensor. See Tensor.coalesce() for details."]}, {"name": "torch.Tensor.torch.Tensor.var", "path": "generated/torch.tensor.var", "type": "Tensor", "text": ["See torch.var()"]}, {"name": "torch.Tensor.torch.Tensor.vdot", "path": "generated/torch.tensor.vdot", "type": "Tensor", "text": ["See torch.vdot()"]}, {"name": "torch.Tensor.torch.Tensor.view", "path": "generated/torch.tensor.view", "type": "Tensor", "text": ["Returns a new tensor with the same data as the self tensor but of a different shape.", "The returned tensor shares the same data and must have the same number of elements, but may have a different size. For a tensor to be viewed, the new view size must be compatible with its original size and stride, i.e., each new view dimension must either be a subspace of an original dimension, or only span across original dimensions d,d+1,\u2026,d+kd, d+1, \\dots, d+k that satisfy the following contiguity-like condition that \u2200i=d,\u2026,d+k\u22121\\forall i = d, \\dots, d+k-1,", "Otherwise, it will not be possible to view self tensor as shape without copying it (e.g., via contiguous()). When it is unclear whether a view() can be performed, it is advisable to use reshape(), which returns a view if the shapes are compatible, and copies (equivalent to calling contiguous()) otherwise.", "shape (torch.Size or int...) \u2013 the desired size", "Example:", "Returns a new tensor with the same data as the self tensor but of a different dtype.", "If the element size of dtype is different than that of self.dtype, then the size of the last dimension of the output will be scaled proportionally. For instance, if dtype element size is twice that of self.dtype, then each pair of elements in the last dimension of self will be combined, and the size of the last dimension of the output will be half that of self. If dtype element size is half that of self.dtype, then each element in the last dimension of self will be split in two, and the size of the last dimension of the output will be double that of self. For this to be possible, the following conditions must be true:", "Additionally, if the element size of dtype is greater than that of self.dtype, the following conditions must be true as well:", "If any of the above conditions are not met, an error is thrown.", "Warning", "This overload is not supported by TorchScript, and using it in a Torchscript program will cause undefined behavior.", "dtype (torch.dtype) \u2013 the desired dtype", "Example:"]}, {"name": "torch.Tensor.torch.Tensor.view_as", "path": "generated/torch.tensor.view_as", "type": "Tensor", "text": ["View this tensor as the same size as other. self.view_as(other) is equivalent to self.view(other.size()).", "Please see view() for more information about view.", "other (torch.Tensor) \u2013 The result tensor has the same size as other."]}, {"name": "torch.Tensor.torch.Tensor.vsplit", "path": "generated/torch.tensor.vsplit", "type": "Tensor", "text": ["See torch.vsplit()"]}, {"name": "torch.Tensor.torch.Tensor.where", "path": "generated/torch.tensor.where", "type": "Tensor", "text": ["self.where(condition, y) is equivalent to torch.where(condition, self, y). See torch.where()"]}, {"name": "torch.Tensor.torch.Tensor.xlogy", "path": "generated/torch.tensor.xlogy", "type": "Tensor", "text": ["See torch.xlogy()"]}, {"name": "torch.Tensor.torch.Tensor.xlogy_", "path": "generated/torch.tensor.xlogy_", "type": "Tensor", "text": ["In-place version of xlogy()"]}, {"name": "torch.Tensor.torch.Tensor.zero_", "path": "generated/torch.tensor.zero_", "type": "Tensor", "text": ["Fills self tensor with zeros."]}, {"name": "torch.Tensor.trace()", "path": "generated/torch.tensor.trace#torch.Tensor.trace", "type": "Tensor", "text": ["See torch.trace()"]}, {"name": "torch.Tensor.transpose()", "path": "generated/torch.tensor.transpose#torch.Tensor.transpose", "type": "Tensor", "text": ["See torch.transpose()"]}, {"name": "torch.Tensor.transpose_()", "path": "generated/torch.tensor.transpose_#torch.Tensor.transpose_", "type": "Tensor", "text": ["In-place version of transpose()"]}, {"name": "torch.Tensor.triangular_solve()", "path": "generated/torch.tensor.triangular_solve#torch.Tensor.triangular_solve", "type": "Tensor", "text": ["See torch.triangular_solve()"]}, {"name": "torch.Tensor.tril()", "path": "generated/torch.tensor.tril#torch.Tensor.tril", "type": "Tensor", "text": ["See torch.tril()"]}, {"name": "torch.Tensor.tril_()", "path": "generated/torch.tensor.tril_#torch.Tensor.tril_", "type": "Tensor", "text": ["In-place version of tril()"]}, {"name": "torch.Tensor.triu()", "path": "generated/torch.tensor.triu#torch.Tensor.triu", "type": "Tensor", "text": ["See torch.triu()"]}, {"name": "torch.Tensor.triu_()", "path": "generated/torch.tensor.triu_#torch.Tensor.triu_", "type": "Tensor", "text": ["In-place version of triu()"]}, {"name": "torch.Tensor.true_divide()", "path": "generated/torch.tensor.true_divide#torch.Tensor.true_divide", "type": "Tensor", "text": ["See torch.true_divide()"]}, {"name": "torch.Tensor.true_divide_()", "path": "generated/torch.tensor.true_divide_#torch.Tensor.true_divide_", "type": "Tensor", "text": ["In-place version of true_divide_()"]}, {"name": "torch.Tensor.trunc()", "path": "generated/torch.tensor.trunc#torch.Tensor.trunc", "type": "Tensor", "text": ["See torch.trunc()"]}, {"name": "torch.Tensor.trunc_()", "path": "generated/torch.tensor.trunc_#torch.Tensor.trunc_", "type": "Tensor", "text": ["In-place version of trunc()"]}, {"name": "torch.Tensor.type()", "path": "generated/torch.tensor.type#torch.Tensor.type", "type": "Tensor", "text": ["Returns the type if dtype is not provided, else casts this object to the specified type.", "If this is already of the correct type, no copy is performed and the original object is returned."]}, {"name": "torch.Tensor.type_as()", "path": "generated/torch.tensor.type_as#torch.Tensor.type_as", "type": "Tensor", "text": ["Returns this tensor cast to the type of the given tensor.", "This is a no-op if the tensor is already of the correct type. This is equivalent to self.type(tensor.type())", "tensor (Tensor) \u2013 the tensor which has the desired type"]}, {"name": "torch.Tensor.unbind()", "path": "generated/torch.tensor.unbind#torch.Tensor.unbind", "type": "Tensor", "text": ["See torch.unbind()"]}, {"name": "torch.Tensor.unflatten()", "path": "generated/torch.tensor.unflatten#torch.Tensor.unflatten", "type": "Tensor", "text": ["See torch.unflatten()."]}, {"name": "torch.Tensor.unfold()", "path": "generated/torch.tensor.unfold#torch.Tensor.unfold", "type": "Tensor", "text": ["Returns a view of the original tensor which contains all slices of size size from self tensor in the dimension dimension.", "Step between two slices is given by step.", "If sizedim is the size of dimension dimension for self, the size of dimension dimension in the returned tensor will be (sizedim - size) / step + 1.", "An additional dimension of size size is appended in the returned tensor.", "Example:"]}, {"name": "torch.Tensor.uniform_()", "path": "generated/torch.tensor.uniform_#torch.Tensor.uniform_", "type": "Tensor", "text": ["Fills self tensor with numbers sampled from the continuous uniform distribution:"]}, {"name": "torch.Tensor.unique()", "path": "generated/torch.tensor.unique#torch.Tensor.unique", "type": "Tensor", "text": ["Returns the unique elements of the input tensor.", "See torch.unique()"]}, {"name": "torch.Tensor.unique_consecutive()", "path": "generated/torch.tensor.unique_consecutive#torch.Tensor.unique_consecutive", "type": "Tensor", "text": ["Eliminates all but the first element from every consecutive group of equivalent elements.", "See torch.unique_consecutive()"]}, {"name": "torch.Tensor.unsqueeze()", "path": "generated/torch.tensor.unsqueeze#torch.Tensor.unsqueeze", "type": "Tensor", "text": ["See torch.unsqueeze()"]}, {"name": "torch.Tensor.unsqueeze_()", "path": "generated/torch.tensor.unsqueeze_#torch.Tensor.unsqueeze_", "type": "Tensor", "text": ["In-place version of unsqueeze()"]}, {"name": "torch.Tensor.untyped_storage()", "path": "generated/torch.tensor.untyped_storage#torch.Tensor.untyped_storage", "type": "Tensor", "text": ["Returns the underlying UntypedStorage."]}, {"name": "torch.Tensor.values()", "path": "generated/torch.tensor.values#torch.Tensor.values", "type": "Tensor", "text": ["Return the values tensor of a sparse COO tensor.", "Warning", "Throws an error if self is not a sparse COO tensor.", "See also Tensor.indices().", "Note", "This method can only be called on a coalesced sparse tensor. See Tensor.coalesce() for details."]}, {"name": "torch.Tensor.var()", "path": "generated/torch.tensor.var#torch.Tensor.var", "type": "Tensor", "text": ["See torch.var()"]}, {"name": "torch.Tensor.vdot()", "path": "generated/torch.tensor.vdot#torch.Tensor.vdot", "type": "Tensor", "text": ["See torch.vdot()"]}, {"name": "torch.Tensor.view()", "path": "generated/torch.tensor.view#torch.Tensor.view", "type": "Tensor", "text": ["Returns a new tensor with the same data as the self tensor but of a different shape.", "The returned tensor shares the same data and must have the same number of elements, but may have a different size. For a tensor to be viewed, the new view size must be compatible with its original size and stride, i.e., each new view dimension must either be a subspace of an original dimension, or only span across original dimensions d,d+1,\u2026,d+kd, d+1, \\dots, d+k that satisfy the following contiguity-like condition that \u2200i=d,\u2026,d+k\u22121\\forall i = d, \\dots, d+k-1,", "Otherwise, it will not be possible to view self tensor as shape without copying it (e.g., via contiguous()). When it is unclear whether a view() can be performed, it is advisable to use reshape(), which returns a view if the shapes are compatible, and copies (equivalent to calling contiguous()) otherwise.", "shape (torch.Size or int...) \u2013 the desired size", "Example:", "Returns a new tensor with the same data as the self tensor but of a different dtype.", "If the element size of dtype is different than that of self.dtype, then the size of the last dimension of the output will be scaled proportionally. For instance, if dtype element size is twice that of self.dtype, then each pair of elements in the last dimension of self will be combined, and the size of the last dimension of the output will be half that of self. If dtype element size is half that of self.dtype, then each element in the last dimension of self will be split in two, and the size of the last dimension of the output will be double that of self. For this to be possible, the following conditions must be true:", "Additionally, if the element size of dtype is greater than that of self.dtype, the following conditions must be true as well:", "If any of the above conditions are not met, an error is thrown.", "Warning", "This overload is not supported by TorchScript, and using it in a Torchscript program will cause undefined behavior.", "dtype (torch.dtype) \u2013 the desired dtype", "Example:"]}, {"name": "torch.Tensor.view_as()", "path": "generated/torch.tensor.view_as#torch.Tensor.view_as", "type": "Tensor", "text": ["View this tensor as the same size as other. self.view_as(other) is equivalent to self.view(other.size()).", "Please see view() for more information about view.", "other (torch.Tensor) \u2013 The result tensor has the same size as other."]}, {"name": "torch.Tensor.vsplit()", "path": "generated/torch.tensor.vsplit#torch.Tensor.vsplit", "type": "Tensor", "text": ["See torch.vsplit()"]}, {"name": "torch.Tensor.where()", "path": "generated/torch.tensor.where#torch.Tensor.where", "type": "Tensor", "text": ["self.where(condition, y) is equivalent to torch.where(condition, self, y). See torch.where()"]}, {"name": "torch.Tensor.xlogy()", "path": "generated/torch.tensor.xlogy#torch.Tensor.xlogy", "type": "Tensor", "text": ["See torch.xlogy()"]}, {"name": "torch.Tensor.xlogy_()", "path": "generated/torch.tensor.xlogy_#torch.Tensor.xlogy_", "type": "Tensor", "text": ["In-place version of xlogy()"]}, {"name": "torch.Tensor.zero_()", "path": "generated/torch.tensor.zero_#torch.Tensor.zero_", "type": "Tensor", "text": ["Fills self tensor with zeros."]}, {"name": "torch.tensor_split", "path": "generated/torch.tensor_split", "type": "Torch", "text": ["Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections. This function is based on NumPy\u2019s numpy.array_split().", "indices_or_sections (Tensor, int or list or tuple of ints) \u2013 ", "If indices_or_sections is an integer n or a zero dimensional long tensor with value n, input is split into n sections along dimension dim. If input is divisible by n along dimension dim, each section will be of equal size, input.size(dim) / n. If input is not divisible by n, the sizes of the first int(input.size(dim) % n) sections will have size int(input.size(dim) / n) + 1, and the rest will have size int(input.size(dim) / n).", "If indices_or_sections is a list or tuple of ints, or a one-dimensional long tensor, then input is split along dimension dim at each of the indices in the list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0 would result in the tensors input[:2], input[2:3], and input[3:].", "If indices_or_sections is a tensor, it must be a zero-dimensional or one-dimensional long tensor on the CPU.", "Example:"]}, {"name": "torch.tensordot", "path": "generated/torch.tensordot", "type": "Torch", "text": ["Returns a contraction of a and b over multiple dimensions.", "tensordot implements a generalized matrix product.", "When called with a non-negative integer argument dims = dd, and the number of dimensions of a and b is mm and nn, respectively, tensordot() computes", "When called with dims of the list form, the given dimensions will be contracted in place of the last dd of a and the first dd of bb. The sizes in these dimensions must match, but tensordot() will deal with broadcasted dimensions.", "Examples:"]}, {"name": "torch.testing", "path": "testing", "type": "Miscellaneous", "text": ["Asserts that actual and expected are close.", "If actual and expected are strided, non-quantized, real-valued, and finite, they are considered close if", "Non-finite values (-inf and inf) are only considered close if and only if they are equal. NaN\u2019s are only considered equal to each other if equal_nan is True.", "In addition, they are only considered close if they have the same", "If either actual or expected is a meta tensor, only the attribute checks will be performed.", "If actual and expected are sparse (either having COO, CSR, CSC, BSR, or BSC layout), their strided members are checked individually. Indices, namely indices for COO, crow_indices and col_indices for CSR and BSR, or ccol_indices and row_indices for CSC and BSC layouts, respectively, are always checked for equality whereas the values are checked for closeness according to the definition above.", "If actual and expected are quantized, they are considered close if they have the same qscheme() and the result of dequantize() is close according to the definition above.", "actual and expected can be Tensor\u2019s or any tensor-or-scalar-likes from which torch.Tensor\u2019s can be constructed with torch.as_tensor(). Except for Python scalars the input types have to be directly related. In addition, actual and expected can be Sequence\u2019s or Mapping\u2019s in which case they are considered close if their structure matches and all their elements are considered close according to the above definition.", "Note", "Python scalars are an exception to the type relation requirement, because their type(), i.e. int, float, and complex, is equivalent to the dtype of a tensor-like. Thus, Python scalars of different types can be checked, but require check_dtype=False.", "The following table displays the default rtol and atol for different dtype\u2019s. In case of mismatching dtype\u2019s, the maximum of both tolerances is used.", "dtype", "rtol", "atol", "float16", "1e-3", "1e-5", "bfloat16", "1.6e-2", "1e-5", "float32", "1.3e-6", "1e-5", "float64", "1e-7", "1e-7", "complex32", "1e-3", "1e-5", "complex64", "1.3e-6", "1e-5", "complex128", "1e-7", "1e-7", "quint8", "1.3e-6", "1e-5", "quint2x4", "1.3e-6", "1e-5", "quint4x2", "1.3e-6", "1e-5", "qint8", "1.3e-6", "1e-5", "qint32", "1.3e-6", "1e-5", "other", "0.0", "0.0", "Note", "assert_close() is highly configurable with strict default settings. Users are encouraged to partial() it to fit their use case. For example, if an equality check is needed, one might define an assert_equal that uses zero tolerances for every dtype by default:", "Creates a tensor with the given shape, device, and dtype, and filled with values uniformly drawn from [low, high).", "If low or high are specified and are outside the range of the dtype\u2019s representable finite values then they are clamped to the lowest or highest representable finite value, respectively. If None, then the following table describes the default values for low and high, which depend on dtype.", "dtype", "low", "high", "boolean type", "0", "2", "unsigned integral type", "0", "10", "signed integral types", "-9", "10", "floating types", "-9", "9", "complex types", "-9", "9", "high (Optional[Number]) \u2013 ", "Sets the upper limit (exclusive) of the given range. If a number is provided it is clamped to the greatest representable finite value of the given dtype. When None (default) this value is determined based on the dtype (see the table above). Default: None.", "Deprecated since version 2.1: Passing low==high to make_tensor() for floating or complex types is deprecated since 2.1 and will be removed in 2.3. Use torch.full() instead.", "Tensor", "Warning", "torch.testing.assert_allclose() is deprecated since 1.12 and will be removed in a future release. Please use torch.testing.assert_close() instead. You can find detailed upgrade instructions here."]}, {"name": "torch.testing.assert_allclose()", "path": "testing#torch.testing.assert_allclose", "type": "Miscellaneous", "text": ["Warning", "torch.testing.assert_allclose() is deprecated since 1.12 and will be removed in a future release. Please use torch.testing.assert_close() instead. You can find detailed upgrade instructions here."]}, {"name": "torch.testing.assert_close()", "path": "testing#torch.testing.assert_close", "type": "Miscellaneous", "text": ["Asserts that actual and expected are close.", "If actual and expected are strided, non-quantized, real-valued, and finite, they are considered close if", "Non-finite values (-inf and inf) are only considered close if and only if they are equal. NaN\u2019s are only considered equal to each other if equal_nan is True.", "In addition, they are only considered close if they have the same", "If either actual or expected is a meta tensor, only the attribute checks will be performed.", "If actual and expected are sparse (either having COO, CSR, CSC, BSR, or BSC layout), their strided members are checked individually. Indices, namely indices for COO, crow_indices and col_indices for CSR and BSR, or ccol_indices and row_indices for CSC and BSC layouts, respectively, are always checked for equality whereas the values are checked for closeness according to the definition above.", "If actual and expected are quantized, they are considered close if they have the same qscheme() and the result of dequantize() is close according to the definition above.", "actual and expected can be Tensor\u2019s or any tensor-or-scalar-likes from which torch.Tensor\u2019s can be constructed with torch.as_tensor(). Except for Python scalars the input types have to be directly related. In addition, actual and expected can be Sequence\u2019s or Mapping\u2019s in which case they are considered close if their structure matches and all their elements are considered close according to the above definition.", "Note", "Python scalars are an exception to the type relation requirement, because their type(), i.e. int, float, and complex, is equivalent to the dtype of a tensor-like. Thus, Python scalars of different types can be checked, but require check_dtype=False.", "The following table displays the default rtol and atol for different dtype\u2019s. In case of mismatching dtype\u2019s, the maximum of both tolerances is used.", "dtype", "rtol", "atol", "float16", "1e-3", "1e-5", "bfloat16", "1.6e-2", "1e-5", "float32", "1.3e-6", "1e-5", "float64", "1e-7", "1e-7", "complex32", "1e-3", "1e-5", "complex64", "1.3e-6", "1e-5", "complex128", "1e-7", "1e-7", "quint8", "1.3e-6", "1e-5", "quint2x4", "1.3e-6", "1e-5", "quint4x2", "1.3e-6", "1e-5", "qint8", "1.3e-6", "1e-5", "qint32", "1.3e-6", "1e-5", "other", "0.0", "0.0", "Note", "assert_close() is highly configurable with strict default settings. Users are encouraged to partial() it to fit their use case. For example, if an equality check is needed, one might define an assert_equal that uses zero tolerances for every dtype by default:"]}, {"name": "torch.testing.make_tensor()", "path": "testing#torch.testing.make_tensor", "type": "Miscellaneous", "text": ["Creates a tensor with the given shape, device, and dtype, and filled with values uniformly drawn from [low, high).", "If low or high are specified and are outside the range of the dtype\u2019s representable finite values then they are clamped to the lowest or highest representable finite value, respectively. If None, then the following table describes the default values for low and high, which depend on dtype.", "dtype", "low", "high", "boolean type", "0", "2", "unsigned integral type", "0", "10", "signed integral types", "-9", "10", "floating types", "-9", "9", "complex types", "-9", "9", "high (Optional[Number]) \u2013 ", "Sets the upper limit (exclusive) of the given range. If a number is provided it is clamped to the greatest representable finite value of the given dtype. When None (default) this value is determined based on the dtype (see the table above). Default: None.", "Deprecated since version 2.1: Passing low==high to make_tensor() for floating or complex types is deprecated since 2.1 and will be removed in 2.3. Use torch.full() instead.", "Tensor"]}, {"name": "torch.tile", "path": "generated/torch.tile", "type": "Torch", "text": ["Constructs a tensor by repeating the elements of input. The dims argument specifies the number of repetitions in each dimension.", "If dims specifies fewer dimensions than input has, then ones are prepended to dims until all dimensions are specified. For example, if input has shape (8, 6, 4, 2) and dims is (2, 2), then dims is treated as (1, 1, 2, 2).", "Analogously, if input has fewer dimensions than dims specifies, then input is treated as if it were unsqueezed at dimension zero until it has as many dimensions as dims specifies. For example, if input has shape (4, 2) and dims is (3, 3, 2, 2), then input is treated as if it had the shape (1, 1, 4, 2).", "Note", "This function is similar to NumPy\u2019s tile function.", "Example:"]}, {"name": "torch.topk", "path": "generated/torch.topk", "type": "Torch", "text": ["Returns the k largest elements of the given input tensor along a given dimension.", "If dim is not given, the last dimension of the input is chosen.", "If largest is False then the k smallest elements are returned.", "A namedtuple of (values, indices) is returned with the values and indices of the largest k elements of each row of the input tensor in the given dimension dim.", "The boolean option sorted if True, will make sure that the returned k elements are themselves sorted", "out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers", "Example:"]}, {"name": "torch.torch.default_generator", "path": "torch#torch.torch.default_generator", "type": "Torch", "text": []}, {"name": "torch.torch.finfo", "path": "type_info#torch.torch.finfo", "type": "Miscellaneous", "text": []}, {"name": "torch.torch.iinfo", "path": "type_info#torch.torch.iinfo", "type": "Miscellaneous", "text": []}, {"name": "torch.trace", "path": "generated/torch.trace", "type": "Torch", "text": ["Returns the sum of the elements of the diagonal of the input 2-D matrix.", "Example:"]}, {"name": "torch.transpose", "path": "generated/torch.transpose", "type": "Torch", "text": ["Returns a tensor that is a transposed version of input. The given dimensions dim0 and dim1 are swapped.", "If input is a strided tensor then the resulting out tensor shares its underlying storage with the input tensor, so changing the content of one would change the content of the other.", "If input is a sparse tensor then the resulting out tensor does not share the underlying storage with the input tensor.", "If input is a sparse tensor with compressed layout (SparseCSR, SparseBSR, SparseCSC or SparseBSC) the arguments dim0 and dim1 must be both batch dimensions, or must both be sparse dimensions. The batch dimensions of a sparse tensor are the dimensions preceding the sparse dimensions.", "Note", "Transpositions which interchange the sparse dimensions of a SparseCSR or SparseCSC layout tensor will result in the layout changing between the two options. Transposition of the sparse dimensions of a ` SparseBSR` or SparseBSC layout tensor will likewise generate a result with the opposite layout.", "Example:", "See also torch.t()."]}, {"name": "torch.trapezoid", "path": "generated/torch.trapezoid", "type": "Torch", "text": ["Computes the trapezoidal rule along dim. By default the spacing between elements is assumed to be 1, but dx can be used to specify a different constant spacing, and x can be used to specify arbitrary spacing along dim.", "Assuming y is a one-dimensional tensor with elements y0,y1,...,yn{y_0, y_1, ..., y_n}, the default computation is", "When dx is specified the computation becomes", "effectively multiplying the result by dx. When x is specified, assuming x is also a one-dimensional tensor with elements x0,x1,...,xn{x_0, x_1, ..., x_n}, the computation becomes", "When x and y have the same size, the computation is as described above and no broadcasting is needed. The broadcasting behavior of this function is as follows when their sizes are different. For both x and y, the function computes the difference between consecutive elements along dimension dim. This effectively creates two tensors, x_diff and y_diff, that have the same shape as the original tensors except their lengths along the dimension dim is reduced by 1. After that, those two tensors are broadcast together to compute final output as part of the trapezoidal rule. See the examples below for details.", "Note", "The trapezoidal rule is a technique for approximating the definite integral of a function by averaging its left and right Riemann sums. The approximation becomes more accurate as the resolution of the partition increases.", "Examples:"]}, {"name": "torch.trapz", "path": "generated/torch.trapz", "type": "Torch", "text": ["Alias for torch.trapezoid()."]}, {"name": "torch.triangular_solve", "path": "generated/torch.triangular_solve", "type": "Torch", "text": ["Solves a system of equations with a square upper or lower triangular invertible matrix AA and multiple right-hand sides bb.", "In symbols, it solves AX=bAX = b and assumes AA is square upper-triangular (or lower-triangular if upper= False) and does not have zeros on the diagonal.", "torch.triangular_solve(b, A) can take in 2D inputs b, A or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs X", "If the diagonal of A contains zeros or elements that are very close to zero and unitriangular= False (default) or if the input matrix is badly conditioned, the result may contain NaN s.", "Supports input of float, double, cfloat and cdouble data types.", "Warning", "torch.triangular_solve() is deprecated in favor of torch.linalg.solve_triangular() and will be removed in a future PyTorch release. torch.linalg.solve_triangular() has its arguments reversed and does not return a copy of one of the inputs.", "X = torch.triangular_solve(B, A).solution should be replaced with", "out ((Tensor, Tensor), optional) \u2013 tuple of two tensors to write the output to. Ignored if None. Default: None.", "A namedtuple (solution, cloned_coefficient) where cloned_coefficient is a clone of AA and solution is the solution XX to AX=bAX = b (or whatever variant of the system of equations, depending on the keyword arguments.)", "Examples:"]}, {"name": "torch.tril", "path": "generated/torch.tril", "type": "Torch", "text": ["Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices input, the other elements of the result tensor out are set to 0.", "The lower triangular part of the matrix is defined as the elements on and below the diagonal.", "The argument diagonal controls which diagonal to consider. If diagonal = 0, all elements on and below the main diagonal are retained. A positive value includes just as many diagonals above the main diagonal, and similarly a negative value excludes just as many diagonals below the main diagonal. The main diagonal are the set of indices {(i,i)}\\lbrace (i, i) \\rbrace for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1] where d1,d2d_{1}, d_{2} are the dimensions of the matrix.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.tril_indices", "path": "generated/torch.tril_indices", "type": "Torch", "text": ["Returns the indices of the lower triangular part of a row-by- col matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates. Indices are ordered based on rows and then columns.", "The lower triangular part of the matrix is defined as the elements on and below the diagonal.", "The argument offset controls which diagonal to consider. If offset = 0, all elements on and below the main diagonal are retained. A positive value includes just as many diagonals above the main diagonal, and similarly a negative value excludes just as many diagonals below the main diagonal. The main diagonal are the set of indices {(i,i)}\\lbrace (i, i) \\rbrace for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1] where d1,d2d_{1}, d_{2} are the dimensions of the matrix.", "Note", "When running on CUDA, row * col must be less than 2592^{59} to prevent overflow during calculation.", "Example:"]}, {"name": "torch.triu", "path": "generated/torch.triu", "type": "Torch", "text": ["Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices input, the other elements of the result tensor out are set to 0.", "The upper triangular part of the matrix is defined as the elements on and above the diagonal.", "The argument diagonal controls which diagonal to consider. If diagonal = 0, all elements on and above the main diagonal are retained. A positive value excludes just as many diagonals above the main diagonal, and similarly a negative value includes just as many diagonals below the main diagonal. The main diagonal are the set of indices {(i,i)}\\lbrace (i, i) \\rbrace for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1] where d1,d2d_{1}, d_{2} are the dimensions of the matrix.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.triu_indices", "path": "generated/torch.triu_indices", "type": "Torch", "text": ["Returns the indices of the upper triangular part of a row by col matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates. Indices are ordered based on rows and then columns.", "The upper triangular part of the matrix is defined as the elements on and above the diagonal.", "The argument offset controls which diagonal to consider. If offset = 0, all elements on and above the main diagonal are retained. A positive value excludes just as many diagonals above the main diagonal, and similarly a negative value includes just as many diagonals below the main diagonal. The main diagonal are the set of indices {(i,i)}\\lbrace (i, i) \\rbrace for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1] where d1,d2d_{1}, d_{2} are the dimensions of the matrix.", "Note", "When running on CUDA, row * col must be less than 2592^{59} to prevent overflow during calculation.", "Example:"]}, {"name": "torch.true_divide", "path": "generated/torch.true_divide", "type": "Torch", "text": ["Alias for torch.div() with rounding_mode=None."]}, {"name": "torch.trunc", "path": "generated/torch.trunc", "type": "Torch", "text": ["Returns a new tensor with the truncated integer values of the elements of input.", "For integer inputs, follows the array-api convention of returning a copy of the input tensor.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.TypedStorage", "path": "storage#torch.TypedStorage", "type": "Storage", "text": ["Casts this storage to bfloat16 type", "Casts this storage to bool type", "Casts this storage to byte type", "Casts this storage to char type", "Returns a copy of this storage", "Casts this storage to complex double type", "Casts this storage to complex float type", "Returns a CPU copy of this storage if it\u2019s not already on the CPU", "Returns a copy of this object in CUDA memory.", "If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.", "T", "Casts this storage to double type", "Casts this storage to float type", "If shared is True, then memory is shared between all processes. All changes are written to the file. If shared is False, then the changes on the storage do not affect the file.", "size is the number of elements in the storage. If shared is False, then the file must contain at least size * sizeof(Type) bytes (Type is the type of storage). If shared is True the file will be created if needed.", "int", "Casts this storage to half type", "Returns a copy of this object in HPU memory.", "If this object is already in HPU memory and on the correct device, then no copy is performed and the original object is returned.", "T", "Casts this storage to int type", "Determine whether the CPU TypedStorage is already pinned on device.", "device (str or torch.device) \u2013 The device to pin memory on. Default: 'cuda'", "A boolean variable.", "Casts this storage to long type", "Copies the CPU TypedStorage to pinned memory, if it\u2019s not already pinned.", "device (str or torch.device) \u2013 The device to pin memory on. Default: 'cuda'.", "A pinned CPU storage.", "Moves the storage to shared memory.", "This is a no-op for storages already in shared memory and for CUDA storages, which do not need to be moved for sharing across processes. Storages in shared memory cannot be resized.", "Returns: self", "Casts this storage to short type", "Returns a list containing the elements of this storage", "Returns the type if dtype is not provided, else casts this object to the specified type.", "If this is already of the correct type, no copy is performed and the original object is returned.", "Union[T, str]", "Returns the internal torch.UntypedStorage"]}, {"name": "torch.TypedStorage.bfloat16()", "path": "storage#torch.TypedStorage.bfloat16", "type": "Storage", "text": ["Casts this storage to bfloat16 type"]}, {"name": "torch.TypedStorage.bool()", "path": "storage#torch.TypedStorage.bool", "type": "Storage", "text": ["Casts this storage to bool type"]}, {"name": "torch.TypedStorage.byte()", "path": "storage#torch.TypedStorage.byte", "type": "Storage", "text": ["Casts this storage to byte type"]}, {"name": "torch.TypedStorage.char()", "path": "storage#torch.TypedStorage.char", "type": "Storage", "text": ["Casts this storage to char type"]}, {"name": "torch.TypedStorage.clone()", "path": "storage#torch.TypedStorage.clone", "type": "Storage", "text": ["Returns a copy of this storage"]}, {"name": "torch.TypedStorage.complex_double()", "path": "storage#torch.TypedStorage.complex_double", "type": "Storage", "text": ["Casts this storage to complex double type"]}, {"name": "torch.TypedStorage.complex_float()", "path": "storage#torch.TypedStorage.complex_float", "type": "Storage", "text": ["Casts this storage to complex float type"]}, {"name": "torch.TypedStorage.copy_()", "path": "storage#torch.TypedStorage.copy_", "type": "Storage", "text": []}, {"name": "torch.TypedStorage.cpu()", "path": "storage#torch.TypedStorage.cpu", "type": "Storage", "text": ["Returns a CPU copy of this storage if it\u2019s not already on the CPU"]}, {"name": "torch.TypedStorage.cuda()", "path": "storage#torch.TypedStorage.cuda", "type": "Storage", "text": ["Returns a copy of this object in CUDA memory.", "If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.", "T"]}, {"name": "torch.TypedStorage.data_ptr()", "path": "storage#torch.TypedStorage.data_ptr", "type": "Storage", "text": []}, {"name": "torch.TypedStorage.device", "path": "storage#torch.TypedStorage.device", "type": "Storage", "text": []}, {"name": "torch.TypedStorage.double()", "path": "storage#torch.TypedStorage.double", "type": "Storage", "text": ["Casts this storage to double type"]}, {"name": "torch.TypedStorage.dtype", "path": "storage#torch.TypedStorage.dtype", "type": "Storage", "text": []}, {"name": "torch.TypedStorage.element_size()", "path": "storage#torch.TypedStorage.element_size", "type": "Storage", "text": []}, {"name": "torch.TypedStorage.fill_()", "path": "storage#torch.TypedStorage.fill_", "type": "Storage", "text": []}, {"name": "torch.TypedStorage.float()", "path": "storage#torch.TypedStorage.float", "type": "Storage", "text": ["Casts this storage to float type"]}, {"name": "torch.TypedStorage.from_buffer()", "path": "storage#torch.TypedStorage.from_buffer", "type": "Storage", "text": []}, {"name": "torch.TypedStorage.from_file()", "path": "storage#torch.TypedStorage.from_file", "type": "Storage", "text": ["If shared is True, then memory is shared between all processes. All changes are written to the file. If shared is False, then the changes on the storage do not affect the file.", "size is the number of elements in the storage. If shared is False, then the file must contain at least size * sizeof(Type) bytes (Type is the type of storage). If shared is True the file will be created if needed."]}, {"name": "torch.TypedStorage.get_device()", "path": "storage#torch.TypedStorage.get_device", "type": "Storage", "text": ["int"]}, {"name": "torch.TypedStorage.half()", "path": "storage#torch.TypedStorage.half", "type": "Storage", "text": ["Casts this storage to half type"]}, {"name": "torch.TypedStorage.hpu()", "path": "storage#torch.TypedStorage.hpu", "type": "Storage", "text": ["Returns a copy of this object in HPU memory.", "If this object is already in HPU memory and on the correct device, then no copy is performed and the original object is returned.", "T"]}, {"name": "torch.TypedStorage.int()", "path": "storage#torch.TypedStorage.int", "type": "Storage", "text": ["Casts this storage to int type"]}, {"name": "torch.TypedStorage.is_cuda", "path": "storage#torch.TypedStorage.is_cuda", "type": "Storage", "text": []}, {"name": "torch.TypedStorage.is_hpu", "path": "storage#torch.TypedStorage.is_hpu", "type": "Storage", "text": []}, {"name": "torch.TypedStorage.is_pinned()", "path": "storage#torch.TypedStorage.is_pinned", "type": "Storage", "text": ["Determine whether the CPU TypedStorage is already pinned on device.", "device (str or torch.device) \u2013 The device to pin memory on. Default: 'cuda'", "A boolean variable."]}, {"name": "torch.TypedStorage.is_shared()", "path": "storage#torch.TypedStorage.is_shared", "type": "Storage", "text": []}, {"name": "torch.TypedStorage.is_sparse", "path": "storage#torch.TypedStorage.is_sparse", "type": "Storage", "text": []}, {"name": "torch.TypedStorage.long()", "path": "storage#torch.TypedStorage.long", "type": "Storage", "text": ["Casts this storage to long type"]}, {"name": "torch.TypedStorage.nbytes()", "path": "storage#torch.TypedStorage.nbytes", "type": "Storage", "text": []}, {"name": "torch.TypedStorage.pickle_storage_type()", "path": "storage#torch.TypedStorage.pickle_storage_type", "type": "Storage", "text": []}, {"name": "torch.TypedStorage.pin_memory()", "path": "storage#torch.TypedStorage.pin_memory", "type": "Storage", "text": ["Copies the CPU TypedStorage to pinned memory, if it\u2019s not already pinned.", "device (str or torch.device) \u2013 The device to pin memory on. Default: 'cuda'.", "A pinned CPU storage."]}, {"name": "torch.TypedStorage.resize_()", "path": "storage#torch.TypedStorage.resize_", "type": "Storage", "text": []}, {"name": "torch.TypedStorage.share_memory_()", "path": "storage#torch.TypedStorage.share_memory_", "type": "Storage", "text": ["Moves the storage to shared memory.", "This is a no-op for storages already in shared memory and for CUDA storages, which do not need to be moved for sharing across processes. Storages in shared memory cannot be resized.", "Returns: self"]}, {"name": "torch.TypedStorage.short()", "path": "storage#torch.TypedStorage.short", "type": "Storage", "text": ["Casts this storage to short type"]}, {"name": "torch.TypedStorage.size()", "path": "storage#torch.TypedStorage.size", "type": "Storage", "text": []}, {"name": "torch.TypedStorage.tolist()", "path": "storage#torch.TypedStorage.tolist", "type": "Storage", "text": ["Returns a list containing the elements of this storage"]}, {"name": "torch.TypedStorage.type()", "path": "storage#torch.TypedStorage.type", "type": "Storage", "text": ["Returns the type if dtype is not provided, else casts this object to the specified type.", "If this is already of the correct type, no copy is performed and the original object is returned.", "Union[T, str]"]}, {"name": "torch.TypedStorage.untyped()", "path": "storage#torch.TypedStorage.untyped", "type": "Storage", "text": ["Returns the internal torch.UntypedStorage"]}, {"name": "torch.unbind", "path": "generated/torch.unbind", "type": "Torch", "text": ["Removes a tensor dimension.", "Returns a tuple of all slices along a given dimension, already without it.", "Example:"]}, {"name": "torch.unflatten", "path": "generated/torch.unflatten", "type": "Torch", "text": ["Expands a dimension of the input tensor over multiple dimensions.", "See also", "torch.flatten() the inverse of this function. It coalesces several dimensions into one.", "A View of input with the specified dimension unflattened."]}, {"name": "torch.unique", "path": "generated/torch.unique", "type": "Torch", "text": ["Returns the unique elements of the input tensor.", "Note", "This function is different from torch.unique_consecutive() in the sense that this function also eliminates non-consecutive duplicate values.", "Note", "Currently in the CUDA implementation and the CPU implementation when dim is specified, torch.unique always sort the tensor at the beginning regardless of the sort argument. Sorting could be slow, so if your input tensor is already sorted, it is recommended to use torch.unique_consecutive() which avoids the sorting.", "A tensor or a tuple of tensors containing", "(Tensor, Tensor (optional), Tensor (optional))", "Example:"]}, {"name": "torch.unique_consecutive", "path": "generated/torch.unique_consecutive", "type": "Torch", "text": ["Eliminates all but the first element from every consecutive group of equivalent elements.", "Note", "This function is different from torch.unique() in the sense that this function only eliminates consecutive duplicate values. This semantics is similar to std::unique in C++.", "A tensor or a tuple of tensors containing", "(Tensor, Tensor (optional), Tensor (optional))", "Example:"]}, {"name": "torch.unsqueeze", "path": "generated/torch.unsqueeze", "type": "Torch", "text": ["Returns a new tensor with a dimension of size one inserted at the specified position.", "The returned tensor shares the same underlying data with this tensor.", "A dim value within the range [-input.dim() - 1, input.dim() + 1) can be used. Negative dim will correspond to unsqueeze() applied at dim = dim + input.dim() + 1.", "Example:"]}, {"name": "torch.UntypedStorage", "path": "storage#torch.UntypedStorage", "type": "Storage", "text": ["Casts this storage to bfloat16 type", "Casts this storage to bool type", "Casts this storage to byte type", "Swaps bytes in underlying data", "Casts this storage to char type", "Returns a copy of this storage", "Casts this storage to complex double type", "Casts this storage to complex float type", "Returns a CPU copy of this storage if it\u2019s not already on the CPU", "Returns a copy of this object in CUDA memory.", "If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.", "Casts this storage to double type", "Casts this storage to float type", "If shared is True, then memory is shared between all processes. All changes are written to the file. If shared is False, then the changes on the storage do not affect the file.", "size is the number of elements in the storage. If shared is False, then the file must contain at least size * sizeof(Type) bytes (Type is the type of storage). If shared is True the file will be created if needed.", "int", "Casts this storage to half type", "Returns a copy of this object in HPU memory.", "If this object is already in HPU memory and on the correct device, then no copy is performed and the original object is returned.", "Casts this storage to int type", "Determine whether the CPU storage is already pinned on device.", "device (str or torch.device) \u2013 The device to pin memory on. Default: 'cuda'.", "A boolean variable.", "Casts this storage to long type", "Returns a MPS copy of this storage if it\u2019s not already on the MPS", "Copies the CPU storage to pinned memory, if it\u2019s not already pinned.", "device (str or torch.device) \u2013 The device to pin memory on. Default: 'cuda'.", "A pinned CPU storage.", "Casts this storage to short type", "int", "Returns a list containing the elements of this storage", "Returns the type if dtype is not provided, else casts this object to the specified type.", "If this is already of the correct type, no copy is performed and the original object is returned."]}, {"name": "torch.UntypedStorage.bfloat16()", "path": "storage#torch.UntypedStorage.bfloat16", "type": "Storage", "text": ["Casts this storage to bfloat16 type"]}, {"name": "torch.UntypedStorage.bool()", "path": "storage#torch.UntypedStorage.bool", "type": "Storage", "text": ["Casts this storage to bool type"]}, {"name": "torch.UntypedStorage.byte()", "path": "storage#torch.UntypedStorage.byte", "type": "Storage", "text": ["Casts this storage to byte type"]}, {"name": "torch.UntypedStorage.byteswap()", "path": "storage#torch.UntypedStorage.byteswap", "type": "Storage", "text": ["Swaps bytes in underlying data"]}, {"name": "torch.UntypedStorage.char()", "path": "storage#torch.UntypedStorage.char", "type": "Storage", "text": ["Casts this storage to char type"]}, {"name": "torch.UntypedStorage.clone()", "path": "storage#torch.UntypedStorage.clone", "type": "Storage", "text": ["Returns a copy of this storage"]}, {"name": "torch.UntypedStorage.complex_double()", "path": "storage#torch.UntypedStorage.complex_double", "type": "Storage", "text": ["Casts this storage to complex double type"]}, {"name": "torch.UntypedStorage.complex_float()", "path": "storage#torch.UntypedStorage.complex_float", "type": "Storage", "text": ["Casts this storage to complex float type"]}, {"name": "torch.UntypedStorage.copy_()", "path": "storage#torch.UntypedStorage.copy_", "type": "Storage", "text": []}, {"name": "torch.UntypedStorage.cpu()", "path": "storage#torch.UntypedStorage.cpu", "type": "Storage", "text": ["Returns a CPU copy of this storage if it\u2019s not already on the CPU"]}, {"name": "torch.UntypedStorage.cuda()", "path": "storage#torch.UntypedStorage.cuda", "type": "Storage", "text": ["Returns a copy of this object in CUDA memory.", "If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned."]}, {"name": "torch.UntypedStorage.data_ptr()", "path": "storage#torch.UntypedStorage.data_ptr", "type": "Storage", "text": []}, {"name": "torch.UntypedStorage.device", "path": "storage#torch.UntypedStorage.device", "type": "Storage", "text": []}, {"name": "torch.UntypedStorage.double()", "path": "storage#torch.UntypedStorage.double", "type": "Storage", "text": ["Casts this storage to double type"]}, {"name": "torch.UntypedStorage.element_size()", "path": "storage#torch.UntypedStorage.element_size", "type": "Storage", "text": []}, {"name": "torch.UntypedStorage.fill_()", "path": "storage#torch.UntypedStorage.fill_", "type": "Storage", "text": []}, {"name": "torch.UntypedStorage.float()", "path": "storage#torch.UntypedStorage.float", "type": "Storage", "text": ["Casts this storage to float type"]}, {"name": "torch.UntypedStorage.from_buffer()", "path": "storage#torch.UntypedStorage.from_buffer", "type": "Storage", "text": []}, {"name": "torch.UntypedStorage.from_file()", "path": "storage#torch.UntypedStorage.from_file", "type": "Storage", "text": ["If shared is True, then memory is shared between all processes. All changes are written to the file. If shared is False, then the changes on the storage do not affect the file.", "size is the number of elements in the storage. If shared is False, then the file must contain at least size * sizeof(Type) bytes (Type is the type of storage). If shared is True the file will be created if needed."]}, {"name": "torch.UntypedStorage.get_device()", "path": "storage#torch.UntypedStorage.get_device", "type": "Storage", "text": ["int"]}, {"name": "torch.UntypedStorage.half()", "path": "storage#torch.UntypedStorage.half", "type": "Storage", "text": ["Casts this storage to half type"]}, {"name": "torch.UntypedStorage.hpu()", "path": "storage#torch.UntypedStorage.hpu", "type": "Storage", "text": ["Returns a copy of this object in HPU memory.", "If this object is already in HPU memory and on the correct device, then no copy is performed and the original object is returned."]}, {"name": "torch.UntypedStorage.int()", "path": "storage#torch.UntypedStorage.int", "type": "Storage", "text": ["Casts this storage to int type"]}, {"name": "torch.UntypedStorage.is_cuda", "path": "storage#torch.UntypedStorage.is_cuda", "type": "Storage", "text": []}, {"name": "torch.UntypedStorage.is_hpu", "path": "storage#torch.UntypedStorage.is_hpu", "type": "Storage", "text": []}, {"name": "torch.UntypedStorage.is_pinned()", "path": "storage#torch.UntypedStorage.is_pinned", "type": "Storage", "text": ["Determine whether the CPU storage is already pinned on device.", "device (str or torch.device) \u2013 The device to pin memory on. Default: 'cuda'.", "A boolean variable."]}, {"name": "torch.UntypedStorage.is_shared()", "path": "storage#torch.UntypedStorage.is_shared", "type": "Storage", "text": []}, {"name": "torch.UntypedStorage.is_sparse", "path": "storage#torch.UntypedStorage.is_sparse", "type": "Storage", "text": []}, {"name": "torch.UntypedStorage.is_sparse_csr", "path": "storage#torch.UntypedStorage.is_sparse_csr", "type": "Storage", "text": []}, {"name": "torch.UntypedStorage.long()", "path": "storage#torch.UntypedStorage.long", "type": "Storage", "text": ["Casts this storage to long type"]}, {"name": "torch.UntypedStorage.mps()", "path": "storage#torch.UntypedStorage.mps", "type": "Storage", "text": ["Returns a MPS copy of this storage if it\u2019s not already on the MPS"]}, {"name": "torch.UntypedStorage.nbytes()", "path": "storage#torch.UntypedStorage.nbytes", "type": "Storage", "text": []}, {"name": "torch.UntypedStorage.new()", "path": "storage#torch.UntypedStorage.new", "type": "Storage", "text": []}, {"name": "torch.UntypedStorage.pin_memory()", "path": "storage#torch.UntypedStorage.pin_memory", "type": "Storage", "text": ["Copies the CPU storage to pinned memory, if it\u2019s not already pinned.", "device (str or torch.device) \u2013 The device to pin memory on. Default: 'cuda'.", "A pinned CPU storage."]}, {"name": "torch.UntypedStorage.resize_()", "path": "storage#torch.UntypedStorage.resize_", "type": "Storage", "text": []}, {"name": "torch.UntypedStorage.share_memory_()", "path": "storage#torch.UntypedStorage.share_memory_", "type": "Storage", "text": []}, {"name": "torch.UntypedStorage.short()", "path": "storage#torch.UntypedStorage.short", "type": "Storage", "text": ["Casts this storage to short type"]}, {"name": "torch.UntypedStorage.size()", "path": "storage#torch.UntypedStorage.size", "type": "Storage", "text": ["int"]}, {"name": "torch.UntypedStorage.tolist()", "path": "storage#torch.UntypedStorage.tolist", "type": "Storage", "text": ["Returns a list containing the elements of this storage"]}, {"name": "torch.UntypedStorage.type()", "path": "storage#torch.UntypedStorage.type", "type": "Storage", "text": ["Returns the type if dtype is not provided, else casts this object to the specified type.", "If this is already of the correct type, no copy is performed and the original object is returned."]}, {"name": "torch.UntypedStorage.untyped()", "path": "storage#torch.UntypedStorage.untyped", "type": "Storage", "text": []}, {"name": "torch.use_deterministic_algorithms", "path": "generated/torch.use_deterministic_algorithms", "type": "Torch", "text": ["Sets whether PyTorch operations must use \u201cdeterministic\u201d algorithms. That is, algorithms which, given the same input, and when run on the same software and hardware, always produce the same output. When enabled, operations will use deterministic algorithms when available, and if only nondeterministic algorithms are available they will throw a RuntimeError when called.", "Note", "This setting alone is not always enough to make an application reproducible. Refer to Reproducibility for more information.", "Note", "torch.set_deterministic_debug_mode() offers an alternative interface for this feature.", "The following normally-nondeterministic operations will act deterministically when mode=True:", "The following normally-nondeterministic operations will throw a RuntimeError when mode=True:", "torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor and one of the following modes is used:", "A handful of CUDA operations are nondeterministic if the CUDA version is 10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more details: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility If one of these environment variable configurations is not set, a RuntimeError will be raised from these operations when called with CUDA tensors:", "Note that deterministic operations tend to have worse performance than nondeterministic operations.", "Note", "This flag does not detect or prevent nondeterministic behavior caused by calling an inplace operation on a tensor with an internal memory overlap or by giving such a tensor as the out argument for an operation. In these cases, multiple writes of different data may target a single memory location, and the order of writes is not guaranteed.", "mode (bool) \u2013 If True, makes potentially nondeterministic operations switch to a deterministic algorithm or throw a runtime error. If False, allows nondeterministic operations.", "warn_only (bool, optional) \u2013 If True, operations that do not have a deterministic implementation will throw a warning instead of an error. Default: False", "Example:"]}, {"name": "torch.utils", "path": "utils", "type": "Miscellaneous", "text": ["This API should be use to rename the privateuse1 backend device to make it more convenient to use as a device name within PyTorch APIs.", "Automatically generate attributes and methods for the custom backend after rename privateuse1 backend.", "Returns a string containing the C++ stack trace of the current thread.", "Set the module attribute on a python object for a given object for nicer printing"]}, {"name": "torch.utils.benchmark.CallgrindStats", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats", "type": "Benchmark Utils", "text": ["Top level container for Callgrind results collected by Timer.", "Manipulation is generally done using the FunctionCounts class, which is obtained by calling CallgrindStats.stats(\u2026). Several convenience methods are provided as well; the most significant is CallgrindStats.as_standardized().", "Strip library names and some prefixes from function strings.", "When comparing two different sets of instruction counts, on stumbling block can be path prefixes. Callgrind includes the full filepath when reporting a function (as it should). However, this can cause issues when diffing profiles. If a key component such as Python or PyTorch was built in separate locations in the two profiles, which can result in something resembling:", "Stripping prefixes can ameliorate this issue by regularizing the strings and causing better cancellation of equivalent call sites when diffing.", "CallgrindStats", "Returns the total number of instructions executed.", "See FunctionCounts.denoise() for an explanation of the denoise arg.", "int", "Diff two sets of counts.", "One common reason to collect instruction counts is to determine the the effect that a particular change will have on the number of instructions needed to perform some unit of work. If a change increases that number, the next logical question is \u201cwhy\u201d. This generally involves looking at what part if the code increased in instruction count. This function automates that process so that one can easily diff counts on both an inclusive and exclusive basis.", "FunctionCounts", "Returns detailed function counts.", "Conceptually, the FunctionCounts returned can be thought of as a tuple of (count, path_and_function_name) tuples.", "inclusive matches the semantics of callgrind. If True, the counts include instructions executed by children. inclusive=True is useful for identifying hot spots in code; inclusive=False is useful for reducing noise when diffing counts from two different runs. (See CallgrindStats.delta(\u2026) for more details)", "FunctionCounts"]}, {"name": "torch.utils.benchmark.CallgrindStats.as_standardized()", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats.as_standardized", "type": "Benchmark Utils", "text": ["Strip library names and some prefixes from function strings.", "When comparing two different sets of instruction counts, on stumbling block can be path prefixes. Callgrind includes the full filepath when reporting a function (as it should). However, this can cause issues when diffing profiles. If a key component such as Python or PyTorch was built in separate locations in the two profiles, which can result in something resembling:", "Stripping prefixes can ameliorate this issue by regularizing the strings and causing better cancellation of equivalent call sites when diffing.", "CallgrindStats"]}, {"name": "torch.utils.benchmark.CallgrindStats.counts()", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats.counts", "type": "Benchmark Utils", "text": ["Returns the total number of instructions executed.", "See FunctionCounts.denoise() for an explanation of the denoise arg.", "int"]}, {"name": "torch.utils.benchmark.CallgrindStats.delta()", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats.delta", "type": "Benchmark Utils", "text": ["Diff two sets of counts.", "One common reason to collect instruction counts is to determine the the effect that a particular change will have on the number of instructions needed to perform some unit of work. If a change increases that number, the next logical question is \u201cwhy\u201d. This generally involves looking at what part if the code increased in instruction count. This function automates that process so that one can easily diff counts on both an inclusive and exclusive basis.", "FunctionCounts"]}, {"name": "torch.utils.benchmark.CallgrindStats.stats()", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats.stats", "type": "Benchmark Utils", "text": ["Returns detailed function counts.", "Conceptually, the FunctionCounts returned can be thought of as a tuple of (count, path_and_function_name) tuples.", "inclusive matches the semantics of callgrind. If True, the counts include instructions executed by children. inclusive=True is useful for identifying hot spots in code; inclusive=False is useful for reducing noise when diffing counts from two different runs. (See CallgrindStats.delta(\u2026) for more details)", "FunctionCounts"]}, {"name": "torch.utils.benchmark.FunctionCounts", "path": "benchmark_utils#torch.utils.benchmark.FunctionCounts", "type": "Benchmark Utils", "text": ["Container for manipulating Callgrind results.", "Remove known noisy instructions.", "Several instructions in the CPython interpreter are rather noisy. These instructions involve unicode to dictionary lookups which Python uses to map variable names. FunctionCounts is generally a content agnostic container, however this is sufficiently important for obtaining reliable results to warrant an exception.", "FunctionCounts", "Keep only the elements where filter_fn applied to function name returns True.", "FunctionCounts", "Apply map_fn to all of the function names.", "This can be used to regularize function names (e.g. stripping irrelevant parts of the file path), coalesce entries by mapping multiple functions to the same name (in which case the counts are added together), etc.", "FunctionCounts"]}, {"name": "torch.utils.benchmark.FunctionCounts.denoise()", "path": "benchmark_utils#torch.utils.benchmark.FunctionCounts.denoise", "type": "Benchmark Utils", "text": ["Remove known noisy instructions.", "Several instructions in the CPython interpreter are rather noisy. These instructions involve unicode to dictionary lookups which Python uses to map variable names. FunctionCounts is generally a content agnostic container, however this is sufficiently important for obtaining reliable results to warrant an exception.", "FunctionCounts"]}, {"name": "torch.utils.benchmark.FunctionCounts.filter()", "path": "benchmark_utils#torch.utils.benchmark.FunctionCounts.filter", "type": "Benchmark Utils", "text": ["Keep only the elements where filter_fn applied to function name returns True.", "FunctionCounts"]}, {"name": "torch.utils.benchmark.FunctionCounts.transform()", "path": "benchmark_utils#torch.utils.benchmark.FunctionCounts.transform", "type": "Benchmark Utils", "text": ["Apply map_fn to all of the function names.", "This can be used to regularize function names (e.g. stripping irrelevant parts of the file path), coalesce entries by mapping multiple functions to the same name (in which case the counts are added together), etc.", "FunctionCounts"]}, {"name": "torch.utils.benchmark.Measurement", "path": "benchmark_utils#torch.utils.benchmark.Measurement", "type": "Benchmark Utils", "text": ["The result of a Timer measurement.", "This class stores one or more measurements of a given statement. It is serializable and provides several convenience methods (including a detailed __repr__) for downstream consumers.", "Convenience method for merging replicates.", "Merge will extrapolate times to number_per_run=1 and will not transfer any metadata. (Since it might differ between replicates)", "List[Measurement]", "Approximate significant figure estimate.", "This property is intended to give a convenient way to estimate the precision of a measurement. It only uses the interquartile region to estimate statistics to try to mitigate skew from the tails, and uses a static z value of 1.645 since it is not expected to be used for small values of n, so z can approximate t.", "The significant figure estimation used in conjunction with the trim_sigfig method to provide a more human interpretable data summary. __repr__ does not use this method; it simply displays raw values. Significant figure estimation is intended for Compare."]}, {"name": "torch.utils.benchmark.Measurement.merge()", "path": "benchmark_utils#torch.utils.benchmark.Measurement.merge", "type": "Benchmark Utils", "text": ["Convenience method for merging replicates.", "Merge will extrapolate times to number_per_run=1 and will not transfer any metadata. (Since it might differ between replicates)", "List[Measurement]"]}, {"name": "torch.utils.benchmark.Measurement.significant_figures", "path": "benchmark_utils#torch.utils.benchmark.Measurement.significant_figures", "type": "Benchmark Utils", "text": ["Approximate significant figure estimate.", "This property is intended to give a convenient way to estimate the precision of a measurement. It only uses the interquartile region to estimate statistics to try to mitigate skew from the tails, and uses a static z value of 1.645 since it is not expected to be used for small values of n, so z can approximate t.", "The significant figure estimation used in conjunction with the trim_sigfig method to provide a more human interpretable data summary. __repr__ does not use this method; it simply displays raw values. Significant figure estimation is intended for Compare."]}, {"name": "torch.utils.benchmark.Timer", "path": "benchmark_utils#torch.utils.benchmark.Timer", "type": "Benchmark Utils", "text": ["Helper class for measuring execution time of PyTorch statements.", "For a full tutorial on how to use this class, see: https://pytorch.org/tutorials/recipes/recipes/benchmark.html", "The PyTorch Timer is based on timeit.Timer (and in fact uses timeit.Timer internally), but with several key differences:", "Timer will perform warmups (important as some elements of PyTorch are lazily initialized), set threadpool size so that comparisons are apples-to-apples, and synchronize asynchronous CUDA functions when necessary.", "When measuring code, and particularly complex kernels / models, run-to-run variation is a significant confounding factor. It is expected that all measurements should include replicates to quantify noise and allow median computation, which is more robust than mean. To that effect, this class deviates from the timeit API by conceptually merging timeit.Timer.repeat and timeit.Timer.autorange. (Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not desired.", "When defining a Timer, one can optionally specify label, sub_label, description, and env. (Defined later) These fields are included in the representation of result object and by the Compare class to group and display results for comparison.", "In addition to wall times, Timer can run a statement under Callgrind and report instructions executed.", "Directly analogous to timeit.Timer constructor arguments:", "stmt, setup, timer, globals", "PyTorch Timer specific constructor arguments:", "label, sub_label, description, env, num_threads", "sub_label (Optional[str]) \u2013 ", "Provide supplemental information to disambiguate measurements with identical stmt or label. For instance, in our example above sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy to differentiate: \u201cReLU(x + 1): (float)\u201d", "\u201dReLU(x + 1): (int)\u201d when printing Measurements or summarizing using Compare.", "description (Optional[str]) \u2013 ", "String to distinguish measurements with identical label and sub_label. The principal use of description is to signal to Compare the columns of data. For instance one might set it based on the input size to create a table of the form:", "using Compare. It is also included when printing a Measurement.", "Measure many replicates while keeping timer overhead to a minimum.", "At a high level, blocked_autorange executes the following pseudo-code:", "Note the variable block_size in the inner loop. The choice of block size is important to measurement quality, and must balance two competing objectives:", "blocked_autorange sets block_size by running a warmup period, increasing block size until timer overhead is less than 0.1% of the overall computation. This value is then used for the main measurement loop.", "A Measurement object that contains measured runtimes and repetition counts, and can be used to compute statistics. (mean, median, etc.)", "Measurement", "Collect instruction counts using Callgrind.", "Unlike wall times, instruction counts are deterministic (modulo non-determinism in the program itself and small amounts of jitter from the Python interpreter.) This makes them ideal for detailed performance analysis. This method runs stmt in a separate process so that Valgrind can instrument the program. Performance is severely degraded due to the instrumentation, however this is ameliorated by the fact that a small number of iterations is generally sufficient to obtain good measurements.", "In order to to use this method valgrind, callgrind_control, and callgrind_annotate must be installed.", "Because there is a process boundary between the caller (this process) and the stmt execution, globals cannot contain arbitrary in-memory data structures. (Unlike timing methods) Instead, globals are restricted to builtins, nn.Modules\u2019s, and TorchScripted functions/modules to reduce the surprise factor from serialization and subsequent deserialization. The GlobalsBridge class provides more detail on this subject. Take particular care with nn.Modules: they rely on pickle and you may need to add an import to setup for them to transfer properly.", "By default, a profile for an empty statement will be collected and cached to indicate how many instructions are from the Python loop which drives stmt.", "A CallgrindStats object which provides instruction counts and some basic facilities for analyzing and manipulating results.", "Mirrors the semantics of timeit.Timer.timeit().", "Execute the main statement (stmt) number times. https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit", "Measurement"]}, {"name": "torch.utils.benchmark.Timer.blocked_autorange()", "path": "benchmark_utils#torch.utils.benchmark.Timer.blocked_autorange", "type": "Benchmark Utils", "text": ["Measure many replicates while keeping timer overhead to a minimum.", "At a high level, blocked_autorange executes the following pseudo-code:", "Note the variable block_size in the inner loop. The choice of block size is important to measurement quality, and must balance two competing objectives:", "blocked_autorange sets block_size by running a warmup period, increasing block size until timer overhead is less than 0.1% of the overall computation. This value is then used for the main measurement loop.", "A Measurement object that contains measured runtimes and repetition counts, and can be used to compute statistics. (mean, median, etc.)", "Measurement"]}, {"name": "torch.utils.benchmark.Timer.collect_callgrind()", "path": "benchmark_utils#torch.utils.benchmark.Timer.collect_callgrind", "type": "Benchmark Utils", "text": ["Collect instruction counts using Callgrind.", "Unlike wall times, instruction counts are deterministic (modulo non-determinism in the program itself and small amounts of jitter from the Python interpreter.) This makes them ideal for detailed performance analysis. This method runs stmt in a separate process so that Valgrind can instrument the program. Performance is severely degraded due to the instrumentation, however this is ameliorated by the fact that a small number of iterations is generally sufficient to obtain good measurements.", "In order to to use this method valgrind, callgrind_control, and callgrind_annotate must be installed.", "Because there is a process boundary between the caller (this process) and the stmt execution, globals cannot contain arbitrary in-memory data structures. (Unlike timing methods) Instead, globals are restricted to builtins, nn.Modules\u2019s, and TorchScripted functions/modules to reduce the surprise factor from serialization and subsequent deserialization. The GlobalsBridge class provides more detail on this subject. Take particular care with nn.Modules: they rely on pickle and you may need to add an import to setup for them to transfer properly.", "By default, a profile for an empty statement will be collected and cached to indicate how many instructions are from the Python loop which drives stmt.", "A CallgrindStats object which provides instruction counts and some basic facilities for analyzing and manipulating results."]}, {"name": "torch.utils.benchmark.Timer.timeit()", "path": "benchmark_utils#torch.utils.benchmark.Timer.timeit", "type": "Benchmark Utils", "text": ["Mirrors the semantics of timeit.Timer.timeit().", "Execute the main statement (stmt) number times. https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit", "Measurement"]}, {"name": "torch.utils.bottleneck", "path": "bottleneck", "type": "Miscellaneous", "text": ["torch.utils.bottleneck is a tool that can be used as an initial step for debugging bottlenecks in your program. It summarizes runs of your script with the Python profiler and PyTorch\u2019s autograd profiler.", "Run it on the command line with", "where [args] are any number of arguments to script.py, or run python -m torch.utils.bottleneck -h for more usage instructions.", "Warning", "Because your script will be profiled, please ensure that it exits in a finite amount of time.", "Warning", "Due to the asynchronous nature of CUDA kernels, when running against CUDA code, the cProfile output and CPU-mode autograd profilers may not show correct timings: the reported CPU time reports the amount of time used to launch the kernels but does not include the time the kernel spent executing on a GPU unless the operation does a synchronize. Ops that do synchronize appear to be extremely expensive under regular CPU-mode profilers. In these case where timings are incorrect, the CUDA-mode autograd profiler may be helpful.", "Note", "To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to look at, you should first check if your script is CPU-bound (\u201cCPU total time is much greater than CUDA total time\u201d). If it is CPU-bound, looking at the results of the CPU-mode autograd profiler will help. If on the other hand your script spends most of its time executing on the GPU, then it makes sense to start looking for responsible CUDA operators in the output of the CUDA-mode autograd profiler.", "Of course the reality is much more complicated and your script might not be in one of those two extremes depending on the part of the model you\u2019re evaluating. If the profiler outputs don\u2019t help, you could try looking at the result of torch.autograd.profiler.emit_nvtx() with nvprof. However, please take into account that the NVTX overhead is very high and often gives a heavily skewed timeline. Similarly, Intel\u00ae VTune\u2122 Profiler helps to analyze performance on Intel platforms further with torch.autograd.profiler.emit_itt().", "Warning", "If you are profiling CUDA code, the first profiler that bottleneck runs (cProfile) will include the CUDA startup time (CUDA buffer allocation cost) in its time reporting. This should not matter if your bottlenecks result in code much slower than the CUDA startup time.", "For more complicated uses of the profilers (like in a multi-GPU case), please see https://docs.python.org/3/library/profile.html or torch.autograd.profiler.profile() for more information."]}, {"name": "torch.utils.checkpoint", "path": "checkpoint", "type": "Miscellaneous", "text": ["Note", "Checkpointing is implemented by rerunning a forward-pass segment for each checkpointed segment during backward. This can cause persistent states like the RNG state to be advanced than they would without checkpointing. By default, checkpointing includes logic to juggle the RNG state such that checkpointed passes making use of RNG (through dropout for example) have deterministic output as compared to non-checkpointed passes. The logic to stash and restore RNG states can incur a moderate performance hit depending on the runtime of checkpointed operations. If deterministic output compared to non-checkpointed passes is not required, supply preserve_rng_state=False to checkpoint or checkpoint_sequential to omit stashing and restoring the RNG state during each checkpoint.", "The stashing logic saves and restores the RNG state for CPU and another device type (infer the device type from Tensor arguments excluding CPU tensors by _infer_device_type) to the run_fn. If there are multiple device, device state will only be saved for devices of a single device type, and the remaining devices will be ignored. Consequently, if any checkpointed functions involve randomness, this may result in incorrect gradients. (Note that if CUDA devices are among the devices detected, it will be prioritized; otherwise, the first device encountered will be selected.) If there are no CPU-tensors, the default device type state (default value is cuda, and it could be set to other device by DefaultDeviceType) will be saved and restored. However, the logic has no way to anticipate if the user will move Tensors to a new device within the run_fn itself. Therefore, if you move Tensors to a new device (\u201cnew\u201d meaning not belonging to the set of [current device + devices of Tensor arguments]) within run_fn, deterministic output compared to non-checkpointed passes is never guaranteed.", "Checkpoint a model or part of the model", "Activation checkpointing is a technique that trades compute for memory. Instead of keeping tensors needed for backward alive until they are used in gradient computation during backward, forward computation in checkpointed regions omits saving tensors for backward and recomputes them during the backward pass. Activation checkpointing can be applied to any part of a model.", "There are currently two checkpointing implementations available, determined by the use_reentrant parameter. It is recommended that you use use_reentrant=False. Please refer the note below for a discussion of their differences.", "Warning", "If the function invocation during the backward pass differs from the forward pass, e.g., due to a global variable, the checkpointed checkpointed version may not be equivalent, potentially causing an error being raised or leading to silently incorrect gradients.", "Warning", "If you are using the use_reentrant=True variant (this is currently the default), please refer to the note below for important considerations and potential limitations.", "Note", "The reentrant variant of checkpoint (use_reentrant=True) and the non-reentrant variant of checkpoint (use_reentrant=False) differ in the following ways:", "Output of running function on *args", "A helper function for checkpointing sequential models.", "Sequential models execute a list of modules/functions in order (sequentially). Therefore, we can divide such a model in various segments and checkpoint each segment. All segments except the last will not store the intermediate activations. The inputs of each checkpointed segment will be saved for re-running the segment in the backward pass.", "Warning", "If you are using the use_reentrant=True` variant (this is the\ndefault), please see :func:`~torch.utils.checkpoint.checkpoint` for\nthe important considerations and limitations of this variant. It is\nrecommended that you use ``use_reentrant=False.", "Output of running functions sequentially on *inputs"]}, {"name": "torch.utils.checkpoint.checkpoint()", "path": "checkpoint#torch.utils.checkpoint.checkpoint", "type": "Miscellaneous", "text": ["Checkpoint a model or part of the model", "Activation checkpointing is a technique that trades compute for memory. Instead of keeping tensors needed for backward alive until they are used in gradient computation during backward, forward computation in checkpointed regions omits saving tensors for backward and recomputes them during the backward pass. Activation checkpointing can be applied to any part of a model.", "There are currently two checkpointing implementations available, determined by the use_reentrant parameter. It is recommended that you use use_reentrant=False. Please refer the note below for a discussion of their differences.", "Warning", "If the function invocation during the backward pass differs from the forward pass, e.g., due to a global variable, the checkpointed checkpointed version may not be equivalent, potentially causing an error being raised or leading to silently incorrect gradients.", "Warning", "If you are using the use_reentrant=True variant (this is currently the default), please refer to the note below for important considerations and potential limitations.", "Note", "The reentrant variant of checkpoint (use_reentrant=True) and the non-reentrant variant of checkpoint (use_reentrant=False) differ in the following ways:", "Output of running function on *args"]}, {"name": "torch.utils.checkpoint.checkpoint_sequential()", "path": "checkpoint#torch.utils.checkpoint.checkpoint_sequential", "type": "Miscellaneous", "text": ["A helper function for checkpointing sequential models.", "Sequential models execute a list of modules/functions in order (sequentially). Therefore, we can divide such a model in various segments and checkpoint each segment. All segments except the last will not store the intermediate activations. The inputs of each checkpointed segment will be saved for re-running the segment in the backward pass.", "Warning", "If you are using the use_reentrant=True` variant (this is the\ndefault), please see :func:`~torch.utils.checkpoint.checkpoint` for\nthe important considerations and limitations of this variant. It is\nrecommended that you use ``use_reentrant=False.", "Output of running functions sequentially on *inputs"]}, {"name": "torch.utils.cpp_extension", "path": "cpp_extension", "type": "Miscellaneous", "text": ["Creates a setuptools.Extension for C++.", "Convenience method that creates a setuptools.Extension with the bare minimum (but often sufficient) arguments to build a C++ extension.", "All arguments are forwarded to the setuptools.Extension constructor.", "Creates a setuptools.Extension for CUDA/C++.", "Convenience method that creates a setuptools.Extension with the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension. This includes the CUDA include path, library path and runtime library.", "All arguments are forwarded to the setuptools.Extension constructor.", "Compute capabilities:", "By default the extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus PTX. If down the road a new card is installed the extension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s newer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch will make nvcc fall back to building kernels with the newest version of PTX your nvcc does support (see below for details on PTX).", "You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which CCs you want the extension to support:", "TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py TORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py", "The +PTX option causes extension kernel binaries to include PTX instructions for the specified CC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >= the specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with CC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to provide forward compat by runtime-compiling for newer CCs can modestly reduce performance on those newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better off specifying them individually. For example, if you want your extension to run on 8.0 and 8.6, \u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but \u201c8.0 8.6\u201d would be better.", "Note that while it\u2019s possible to include all supported archs, the more archs get included the slower the building process will be, as it will build a separate kernel image for each arch.", "Note that CUDA-11.5 nvcc will hit internal compiler error while parsing torch/extension.h on Windows. To workaround the issue, move python binding logic to pure C++ file.", "#include <ATen/ATen.h> at::Tensor SigmoidAlphaBlendForwardCuda(\u2026.)", "#include <torch/extension.h> torch::Tensor SigmoidAlphaBlendForwardCuda(\u2026)", "Currently open issue for nvcc bug: https://github.com/pytorch/pytorch/issues/69460 Complete workaround code example: https://github.com/facebookresearch/pytorch3d/commit/cb170ac024a949f1f9614ffe6af1c38d972f7d48", "Relocatable device code linking:", "If you want to reference device symbols across compilation units (across object files), the object files need to be built with relocatable device code (-rdc=true or -dc). An exception to this rule is \u201cdynamic parallelism\u201d (nested kernel launches) which is not used a lot anymore. Relocatable device code is less optimized so it needs to be used only on object files that need it. Using -dlto (Device Link Time Optimization) at the device code compilation step and dlink step help reduce the protentional perf degradation of -rdc. Note that it needs to be used at both steps to be useful.", "If you have rdc objects you need to have an extra -dlink (device linking) step before the CPU symbol linking step. There is also a case where -dlink is used without -rdc: when an extension is linked against a static lib containing rdc-compiled objects like the [NVSHMEM library](https://developer.nvidia.com/nvshmem).", "Note: Ninja is required to build a CUDA Extension with RDC linking.", "A custom setuptools build extension .", "This setuptools.build_ext subclass takes care of passing the minimum required compiler flags (e.g. -std=c++17) as well as mixed C++/CUDA compilation (and support for CUDA files in general).", "When using BuildExtension, it is allowed to supply a dictionary for extra_compile_args (rather than the usual list) that maps from languages (cxx or nvcc) to a list of additional compiler flags to supply to the compiler. This makes it possible to supply different flags to the C++ and CUDA compiler during mixed compilation.", "use_ninja (bool): If use_ninja is True (default), then we attempt to build using the Ninja backend. Ninja greatly speeds up compilation compared to the standard setuptools.build_ext. Fallbacks to the standard distutils backend if Ninja is not available.", "Note", "By default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the MAX_JOBS environment variable to a non-negative number.", "Loads a PyTorch C++ extension just-in-time (JIT).", "To load an extension, a Ninja build file is emitted, which is used to compile the given sources into a dynamic library. This library is subsequently loaded into the current Python process as a module and returned from this function, ready for use.", "By default, the directory to which the build file is emitted and the resulting library compiled to is <tmp>/torch_extensions/<name>, where <tmp> is the temporary folder on the current platform and <name> the name of the extension. This location can be overridden in two ways. First, if the TORCH_EXTENSIONS_DIR environment variable is set, it replaces <tmp>/torch_extensions and all extensions will be compiled into subfolders of this directory. Second, if the build_directory argument to this function is supplied, it overrides the entire path, i.e. the library will be compiled into that folder directly.", "To compile the sources, the default system compiler (c++) is used, which can be overridden by setting the CXX environment variable. To pass additional arguments to the compilation process, extra_cflags or extra_ldflags can be provided. For example, to compile your extension with optimizations, pass extra_cflags=['-O3']. You can also use extra_cflags to pass further include directories.", "CUDA support with mixed compilation is provided. Simply pass CUDA source files (.cu or .cuh) along with other sources. Such files will be detected and compiled with nvcc rather than the C++ compiler. This includes passing the CUDA lib64 directory as a library directory, and linking cudart. You can pass additional flags to nvcc via extra_cuda_cflags, just like with extra_cflags for C++. Various heuristics for finding the CUDA install directory are used, which usually work fine. If not, setting the CUDA_HOME environment variable is the safest option.", "Returns the loaded PyTorch extension as a Python module.", "Returns nothing. (The shared library is loaded into the process as a side effect.)", "Return the path to the executable. (On Windows, TORCH_LIB_PATH is added to the PATH environment variable as a side effect.)", "If is_python_module is True", "Loads a PyTorch C++ extension just-in-time (JIT) from string sources.", "This function behaves exactly like load(), but takes its sources as strings rather than filenames. These strings are stored to files in the build directory, after which the behavior of load_inline() is identical to load().", "See the tests for good examples of using this function.", "Sources may omit two required parts of a typical non-inline C++ extension: the necessary header includes, as well as the (pybind11) binding code. More precisely, strings passed to cpp_sources are first concatenated into a single .cpp file. This file is then prepended with #include\n<torch/extension.h>.", "Furthermore, if the functions argument is supplied, bindings will be automatically generated for each function specified. functions can either be a list of function names, or a dictionary mapping from function names to docstrings. If a list is given, the name of each function is used as its docstring.", "The sources in cuda_sources are concatenated into a separate .cu file and prepended with torch/types.h, cuda.h and cuda_runtime.h includes. The .cpp and .cu files are compiled separately, but ultimately linked into a single library. Note that no bindings are generated for functions in cuda_sources per se. To bind to a CUDA kernel, you must create a C++ function that calls it, and either declare or define this C++ function in one of the cpp_sources (and include its name in functions).", "See load() for a description of arguments omitted below.", "Note", "By default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the MAX_JOBS environment variable to a non-negative number.", "Get the include paths required to build a C++ or CUDA extension.", "cuda (bool) \u2013 If True, includes CUDA-specific include paths.", "A list of include path strings.", "List[str]", "Determine if the given compiler is ABI-compatible with PyTorch alongside its version.", "compiler (str) \u2013 The compiler executable name to check (e.g. g++). Must be executable in a shell process.", "A tuple that contains a boolean that defines if the compiler is (likely) ABI-incompatible with PyTorch, followed by a TorchVersion string that contains the compiler version separated by dots.", "Tuple[bool, TorchVersion]", "Raises RuntimeError if ninja build system is not available on the system, does nothing otherwise.", "Returns True if the ninja build system is available on the system, False otherwise."]}, {"name": "torch.utils.cpp_extension.BuildExtension()", "path": "cpp_extension#torch.utils.cpp_extension.BuildExtension", "type": "Miscellaneous", "text": ["A custom setuptools build extension .", "This setuptools.build_ext subclass takes care of passing the minimum required compiler flags (e.g. -std=c++17) as well as mixed C++/CUDA compilation (and support for CUDA files in general).", "When using BuildExtension, it is allowed to supply a dictionary for extra_compile_args (rather than the usual list) that maps from languages (cxx or nvcc) to a list of additional compiler flags to supply to the compiler. This makes it possible to supply different flags to the C++ and CUDA compiler during mixed compilation.", "use_ninja (bool): If use_ninja is True (default), then we attempt to build using the Ninja backend. Ninja greatly speeds up compilation compared to the standard setuptools.build_ext. Fallbacks to the standard distutils backend if Ninja is not available.", "Note", "By default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the MAX_JOBS environment variable to a non-negative number."]}, {"name": "torch.utils.cpp_extension.CppExtension()", "path": "cpp_extension#torch.utils.cpp_extension.CppExtension", "type": "Miscellaneous", "text": ["Creates a setuptools.Extension for C++.", "Convenience method that creates a setuptools.Extension with the bare minimum (but often sufficient) arguments to build a C++ extension.", "All arguments are forwarded to the setuptools.Extension constructor."]}, {"name": "torch.utils.cpp_extension.CUDAExtension()", "path": "cpp_extension#torch.utils.cpp_extension.CUDAExtension", "type": "Miscellaneous", "text": ["Creates a setuptools.Extension for CUDA/C++.", "Convenience method that creates a setuptools.Extension with the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension. This includes the CUDA include path, library path and runtime library.", "All arguments are forwarded to the setuptools.Extension constructor.", "Compute capabilities:", "By default the extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus PTX. If down the road a new card is installed the extension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s newer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch will make nvcc fall back to building kernels with the newest version of PTX your nvcc does support (see below for details on PTX).", "You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which CCs you want the extension to support:", "TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py TORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py", "The +PTX option causes extension kernel binaries to include PTX instructions for the specified CC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >= the specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with CC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to provide forward compat by runtime-compiling for newer CCs can modestly reduce performance on those newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better off specifying them individually. For example, if you want your extension to run on 8.0 and 8.6, \u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but \u201c8.0 8.6\u201d would be better.", "Note that while it\u2019s possible to include all supported archs, the more archs get included the slower the building process will be, as it will build a separate kernel image for each arch.", "Note that CUDA-11.5 nvcc will hit internal compiler error while parsing torch/extension.h on Windows. To workaround the issue, move python binding logic to pure C++ file.", "#include <ATen/ATen.h> at::Tensor SigmoidAlphaBlendForwardCuda(\u2026.)", "#include <torch/extension.h> torch::Tensor SigmoidAlphaBlendForwardCuda(\u2026)", "Currently open issue for nvcc bug: https://github.com/pytorch/pytorch/issues/69460 Complete workaround code example: https://github.com/facebookresearch/pytorch3d/commit/cb170ac024a949f1f9614ffe6af1c38d972f7d48", "Relocatable device code linking:", "If you want to reference device symbols across compilation units (across object files), the object files need to be built with relocatable device code (-rdc=true or -dc). An exception to this rule is \u201cdynamic parallelism\u201d (nested kernel launches) which is not used a lot anymore. Relocatable device code is less optimized so it needs to be used only on object files that need it. Using -dlto (Device Link Time Optimization) at the device code compilation step and dlink step help reduce the protentional perf degradation of -rdc. Note that it needs to be used at both steps to be useful.", "If you have rdc objects you need to have an extra -dlink (device linking) step before the CPU symbol linking step. There is also a case where -dlink is used without -rdc: when an extension is linked against a static lib containing rdc-compiled objects like the [NVSHMEM library](https://developer.nvidia.com/nvshmem).", "Note: Ninja is required to build a CUDA Extension with RDC linking."]}, {"name": "torch.utils.cpp_extension.get_compiler_abi_compatibility_and_version()", "path": "cpp_extension#torch.utils.cpp_extension.get_compiler_abi_compatibility_and_version", "type": "Miscellaneous", "text": ["Determine if the given compiler is ABI-compatible with PyTorch alongside its version.", "compiler (str) \u2013 The compiler executable name to check (e.g. g++). Must be executable in a shell process.", "A tuple that contains a boolean that defines if the compiler is (likely) ABI-incompatible with PyTorch, followed by a TorchVersion string that contains the compiler version separated by dots.", "Tuple[bool, TorchVersion]"]}, {"name": "torch.utils.cpp_extension.include_paths()", "path": "cpp_extension#torch.utils.cpp_extension.include_paths", "type": "Miscellaneous", "text": ["Get the include paths required to build a C++ or CUDA extension.", "cuda (bool) \u2013 If True, includes CUDA-specific include paths.", "A list of include path strings.", "List[str]"]}, {"name": "torch.utils.cpp_extension.is_ninja_available()", "path": "cpp_extension#torch.utils.cpp_extension.is_ninja_available", "type": "Miscellaneous", "text": ["Returns True if the ninja build system is available on the system, False otherwise."]}, {"name": "torch.utils.cpp_extension.load()", "path": "cpp_extension#torch.utils.cpp_extension.load", "type": "Miscellaneous", "text": ["Loads a PyTorch C++ extension just-in-time (JIT).", "To load an extension, a Ninja build file is emitted, which is used to compile the given sources into a dynamic library. This library is subsequently loaded into the current Python process as a module and returned from this function, ready for use.", "By default, the directory to which the build file is emitted and the resulting library compiled to is <tmp>/torch_extensions/<name>, where <tmp> is the temporary folder on the current platform and <name> the name of the extension. This location can be overridden in two ways. First, if the TORCH_EXTENSIONS_DIR environment variable is set, it replaces <tmp>/torch_extensions and all extensions will be compiled into subfolders of this directory. Second, if the build_directory argument to this function is supplied, it overrides the entire path, i.e. the library will be compiled into that folder directly.", "To compile the sources, the default system compiler (c++) is used, which can be overridden by setting the CXX environment variable. To pass additional arguments to the compilation process, extra_cflags or extra_ldflags can be provided. For example, to compile your extension with optimizations, pass extra_cflags=['-O3']. You can also use extra_cflags to pass further include directories.", "CUDA support with mixed compilation is provided. Simply pass CUDA source files (.cu or .cuh) along with other sources. Such files will be detected and compiled with nvcc rather than the C++ compiler. This includes passing the CUDA lib64 directory as a library directory, and linking cudart. You can pass additional flags to nvcc via extra_cuda_cflags, just like with extra_cflags for C++. Various heuristics for finding the CUDA install directory are used, which usually work fine. If not, setting the CUDA_HOME environment variable is the safest option.", "Returns the loaded PyTorch extension as a Python module.", "Returns nothing. (The shared library is loaded into the process as a side effect.)", "Return the path to the executable. (On Windows, TORCH_LIB_PATH is added to the PATH environment variable as a side effect.)", "If is_python_module is True"]}, {"name": "torch.utils.cpp_extension.load_inline()", "path": "cpp_extension#torch.utils.cpp_extension.load_inline", "type": "Miscellaneous", "text": ["Loads a PyTorch C++ extension just-in-time (JIT) from string sources.", "This function behaves exactly like load(), but takes its sources as strings rather than filenames. These strings are stored to files in the build directory, after which the behavior of load_inline() is identical to load().", "See the tests for good examples of using this function.", "Sources may omit two required parts of a typical non-inline C++ extension: the necessary header includes, as well as the (pybind11) binding code. More precisely, strings passed to cpp_sources are first concatenated into a single .cpp file. This file is then prepended with #include\n<torch/extension.h>.", "Furthermore, if the functions argument is supplied, bindings will be automatically generated for each function specified. functions can either be a list of function names, or a dictionary mapping from function names to docstrings. If a list is given, the name of each function is used as its docstring.", "The sources in cuda_sources are concatenated into a separate .cu file and prepended with torch/types.h, cuda.h and cuda_runtime.h includes. The .cpp and .cu files are compiled separately, but ultimately linked into a single library. Note that no bindings are generated for functions in cuda_sources per se. To bind to a CUDA kernel, you must create a C++ function that calls it, and either declare or define this C++ function in one of the cpp_sources (and include its name in functions).", "See load() for a description of arguments omitted below.", "Note", "By default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the MAX_JOBS environment variable to a non-negative number."]}, {"name": "torch.utils.cpp_extension.verify_ninja_availability()", "path": "cpp_extension#torch.utils.cpp_extension.verify_ninja_availability", "type": "Miscellaneous", "text": ["Raises RuntimeError if ninja build system is not available on the system, does nothing otherwise."]}, {"name": "torch.utils.data", "path": "data", "type": "Datasets and Data Loaders", "text": ["At the heart of PyTorch data loading utility is the torch.utils.data.DataLoader class. It represents a Python iterable over a dataset, with support for", "These options are configured by the constructor arguments of a DataLoader, which has signature:", "The sections below describe in details the effects and usages of these options.", "The most important argument of DataLoader constructor is dataset, which indicates a dataset object to load data from. PyTorch supports two different types of datasets:", "A map-style dataset is one that implements the __getitem__() and __len__() protocols, and represents a map from (possibly non-integral) indices/keys to data samples.", "For example, such a dataset, when accessed with dataset[idx], could read the idx-th image and its corresponding label from a folder on the disk.", "See Dataset for more details.", "An iterable-style dataset is an instance of a subclass of IterableDataset that implements the __iter__() protocol, and represents an iterable over data samples. This type of datasets is particularly suitable for cases where random reads are expensive or even improbable, and where the batch size depends on the fetched data.", "For example, such a dataset, when called iter(dataset), could return a stream of data reading from a database, a remote server, or even logs generated in real time.", "See IterableDataset for more details.", "Note", "When using a IterableDataset with multi-process data loading. The same dataset object is replicated on each worker process, and thus the replicas must be configured differently to avoid duplicated data. See IterableDataset documentations for how to achieve this.", "For iterable-style datasets, data loading order is entirely controlled by the user-defined iterable. This allows easier implementations of chunk-reading and dynamic batch size (e.g., by yielding a batched sample at each time).", "The rest of this section concerns the case with map-style datasets. torch.utils.data.Sampler classes are used to specify the sequence of indices/keys used in data loading. They represent iterable objects over the indices to datasets. E.g., in the common case with stochastic gradient decent (SGD), a Sampler could randomly permute a list of indices and yield each one at a time, or yield a small number of them for mini-batch SGD.", "A sequential or shuffled sampler will be automatically constructed based on the shuffle argument to a DataLoader. Alternatively, users may use the sampler argument to specify a custom Sampler object that at each time yields the next index/key to fetch.", "A custom Sampler that yields a list of batch indices at a time can be passed as the batch_sampler argument. Automatic batching can also be enabled via batch_size and drop_last arguments. See the next section for more details on this.", "Note", "Neither sampler nor batch_sampler is compatible with iterable-style datasets, since such datasets have no notion of a key or an index.", "DataLoader supports automatically collating individual fetched data samples into batches via arguments batch_size, drop_last, batch_sampler, and collate_fn (which has a default function).", "This is the most common case, and corresponds to fetching a minibatch of data and collating them into batched samples, i.e., containing Tensors with one dimension being the batch dimension (usually the first).", "When batch_size (default 1) is not None, the data loader yields batched samples instead of individual samples. batch_size and drop_last arguments are used to specify how the data loader obtains batches of dataset keys. For map-style datasets, users can alternatively specify batch_sampler, which yields a list of keys at a time.", "Note", "The batch_size and drop_last arguments essentially are used to construct a batch_sampler from sampler. For map-style datasets, the sampler is either provided by user or constructed based on the shuffle argument. For iterable-style datasets, the sampler is a dummy infinite one. See this section on more details on samplers.", "Note", "When fetching from iterable-style datasets with multi-processing, the drop_last argument drops the last non-full batch of each worker\u2019s dataset replica.", "After fetching a list of samples using the indices from sampler, the function passed as the collate_fn argument is used to collate lists of samples into batches.", "In this case, loading from a map-style dataset is roughly equivalent with:", "and loading from an iterable-style dataset is roughly equivalent with:", "A custom collate_fn can be used to customize collation, e.g., padding sequential data to max length of a batch. See this section on more about collate_fn.", "In certain cases, users may want to handle batching manually in dataset code, or simply load individual samples. For example, it could be cheaper to directly load batched data (e.g., bulk reads from a database or reading continuous chunks of memory), or the batch size is data dependent, or the program is designed to work on individual samples. Under these scenarios, it\u2019s likely better to not use automatic batching (where collate_fn is used to collate the samples), but let the data loader directly return each member of the dataset object.", "When both batch_size and batch_sampler are None (default value for batch_sampler is already None), automatic batching is disabled. Each sample obtained from the dataset is processed with the function passed as the collate_fn argument.", "When automatic batching is disabled, the default collate_fn simply converts NumPy arrays into PyTorch Tensors, and keeps everything else untouched.", "In this case, loading from a map-style dataset is roughly equivalent with:", "and loading from an iterable-style dataset is roughly equivalent with:", "See this section on more about collate_fn.", "The use of collate_fn is slightly different when automatic batching is enabled or disabled.", "When automatic batching is disabled, collate_fn is called with each individual data sample, and the output is yielded from the data loader iterator. In this case, the default collate_fn simply converts NumPy arrays in PyTorch tensors.", "When automatic batching is enabled, collate_fn is called with a list of data samples at each time. It is expected to collate the input samples into a batch for yielding from the data loader iterator. The rest of this section describes the behavior of the default collate_fn (default_collate()).", "For instance, if each data sample consists of a 3-channel image and an integral class label, i.e., each element of the dataset returns a tuple (image, class_index), the default collate_fn collates a list of such tuples into a single tuple of a batched image tensor and a batched class label Tensor. In particular, the default collate_fn has the following properties:", "Users may use customized collate_fn to achieve custom batching, e.g., collating along a dimension other than the first, padding sequences of various lengths, or adding support for custom data types.", "If you run into a situation where the outputs of DataLoader have dimensions or type that is different from your expectation, you may want to check your collate_fn.", "A DataLoader uses single-process data loading by default.", "Within a Python process, the Global Interpreter Lock (GIL) prevents true fully parallelizing Python code across threads. To avoid blocking computation code with data loading, PyTorch provides an easy switch to perform multi-process data loading by simply setting the argument num_workers to a positive integer.", "In this mode, data fetching is done in the same process a DataLoader is initialized. Therefore, data loading may block computing. However, this mode may be preferred when resource(s) used for sharing data among processes (e.g., shared memory, file descriptors) is limited, or when the entire dataset is small and can be loaded entirely in memory. Additionally, single-process loading often shows more readable error traces and thus is useful for debugging.", "Setting the argument num_workers as a positive integer will turn on multi-process data loading with the specified number of loader worker processes.", "Warning", "After several iterations, the loader worker processes will consume the same amount of CPU memory as the parent process for all Python objects in the parent process which are accessed from the worker processes. This can be problematic if the Dataset contains a lot of data (e.g., you are loading a very large list of filenames at Dataset construction time) and/or you are using a lot of workers (overall memory usage is number of workers * size of parent process). The simplest workaround is to replace Python objects with non-refcounted representations such as Pandas, Numpy or PyArrow objects. Check out issue #13246 for more details on why this occurs and example code for how to workaround these problems.", "In this mode, each time an iterator of a DataLoader is created (e.g., when you call enumerate(dataloader)), num_workers worker processes are created. At this point, the dataset, collate_fn, and worker_init_fn are passed to each worker, where they are used to initialize, and fetch data. This means that dataset access together with its internal IO, transforms (including collate_fn) runs in the worker process.", "torch.utils.data.get_worker_info() returns various useful information in a worker process (including the worker id, dataset replica, initial seed, etc.), and returns None in main process. Users may use this function in dataset code and/or worker_init_fn to individually configure each dataset replica, and to determine whether the code is running in a worker process. For example, this can be particularly helpful in sharding the dataset.", "For map-style datasets, the main process generates the indices using sampler and sends them to the workers. So any shuffle randomization is done in the main process which guides loading by assigning indices to load.", "For iterable-style datasets, since each worker process gets a replica of the dataset object, naive multi-process loading will often result in duplicated data. Using torch.utils.data.get_worker_info() and/or worker_init_fn, users may configure each replica independently. (See IterableDataset documentations for how to achieve this. ) For similar reasons, in multi-process loading, the drop_last argument drops the last non-full batch of each worker\u2019s iterable-style dataset replica.", "Workers are shut down once the end of the iteration is reached, or when the iterator becomes garbage collected.", "Warning", "It is generally not recommended to return CUDA tensors in multi-process loading because of many subtleties in using CUDA and sharing CUDA tensors in multiprocessing (see CUDA in multiprocessing). Instead, we recommend using automatic memory pinning (i.e., setting pin_memory=True), which enables fast data transfer to CUDA-enabled GPUs.", "Since workers rely on Python multiprocessing, worker launch behavior is different on Windows compared to Unix.", "This separate serialization means that you should take two steps to ensure you are compatible with Windows while using multi-process data loading:", "By default, each worker will have its PyTorch seed set to base_seed + worker_id, where base_seed is a long generated by main process using its RNG (thereby, consuming a RNG state mandatorily) or a specified generator. However, seeds for other libraries may be duplicated upon initializing workers, causing each worker to return identical random numbers. (See this section in FAQ.).", "In worker_init_fn, you may access the PyTorch seed set for each worker with either torch.utils.data.get_worker_info().seed or torch.initial_seed(), and use it to seed other libraries before data loading.", "Host to GPU copies are much faster when they originate from pinned (page-locked) memory. See Use pinned memory buffers for more details on when and how to use pinned memory generally.", "For data loading, passing pin_memory=True to a DataLoader will automatically put the fetched data Tensors in pinned memory, and thus enables faster data transfer to CUDA-enabled GPUs.", "The default memory pinning logic only recognizes Tensors and maps and iterables containing Tensors. By default, if the pinning logic sees a batch that is a custom type (which will occur if you have a collate_fn that returns a custom batch type), or if each element of your batch is a custom type, the pinning logic will not recognize them, and it will return that batch (or those elements) without pinning the memory. To enable memory pinning for custom batch or data type(s), define a pin_memory() method on your custom type(s).", "See the example below.", "Example:", "Data loader. Combines a dataset and a sampler, and provides an iterable over the given dataset.", "The DataLoader supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning.", "See torch.utils.data documentation page for more details.", "Warning", "If the spawn start method is used, worker_init_fn cannot be an unpicklable object, e.g., a lambda function. See Multiprocessing best practices on more details related to multiprocessing in PyTorch.", "Warning", "len(dataloader) heuristic is based on the length of the sampler used. When dataset is an IterableDataset, it instead returns an estimate based on len(dataset) / batch_size, with proper rounding depending on drop_last, regardless of multi-process loading configurations. This represents the best guess PyTorch can make because PyTorch trusts user dataset code in correctly handling multi-process loading to avoid duplicate data.", "However, if sharding results in multiple workers having incomplete last batches, this estimate can still be inaccurate, because (1) an otherwise complete batch can be broken into multiple ones and (2) more than one batch worth of samples can be dropped when drop_last is set. Unfortunately, PyTorch can not detect such cases in general.", "See Dataset Types for more details on these two types of datasets and how IterableDataset interacts with Multi-process data loading.", "Warning", "See Reproducibility, and My data loader workers return identical random numbers, and Randomness in multi-process data loading notes for random seed related questions.", "An abstract class representing a Dataset.", "All datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite __getitem__(), supporting fetching a data sample for a given key. Subclasses could also optionally overwrite __len__(), which is expected to return the size of the dataset by many Sampler implementations and the default options of DataLoader. Subclasses could also optionally implement __getitems__(), for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.", "Note", "DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.", "An iterable Dataset.", "All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream.", "All subclasses should overwrite __iter__(), which would return an iterator of samples in this dataset.", "When a subclass is used with DataLoader, each item in the dataset will be yielded from the DataLoader iterator. When num_workers > 0, each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. get_worker_info(), when called in a worker process, returns information about the worker. It can be used in either the dataset\u2019s __iter__() method or the DataLoader \u2018s worker_init_fn option to modify each copy\u2019s behavior.", "Example 1: splitting workload across all workers in __iter__():", "Example 2: splitting workload across all workers using worker_init_fn:", "Dataset wrapping tensors.", "Each sample will be retrieved by indexing tensors along the first dimension.", "*tensors (Tensor) \u2013 tensors that have the same size of the first dimension.", "Dataset as a stacking of multiple datasets.", "This class is useful to assemble different parts of complex input data, given as datasets.", "Dataset as a concatenation of multiple datasets.", "This class is useful to assemble different existing datasets.", "datasets (sequence) \u2013 List of datasets to be concatenated", "Dataset for chaining multiple IterableDataset s.", "This class is useful to assemble different existing dataset streams. The chaining operation is done on-the-fly, so concatenating large-scale datasets with this class will be efficient.", "datasets (iterable of IterableDataset) \u2013 datasets to be chained together", "Subset of a dataset at specified indices.", "General collate function that handles collection type of element within each batch and opens function registry to deal with specific element types. default_collate_fn_map provides default collate functions for tensors, numpy arrays, numbers and strings.", "Note", "Each collate function requires a positional argument for batch and a keyword argument for the dictionary of collate functions as collate_fn_map.", "Function that takes in a batch of data and puts the elements within the batch into a tensor with an additional outer dimension - batch size. The exact output type can be a torch.Tensor, a Sequence of torch.Tensor, a Collection of torch.Tensor, or left unchanged, depending on the input type. This is used as the default function for collation when batch_size or batch_sampler is defined in DataLoader.", "Here is the general input type (based on the type of the element within the batch) to output type mapping:", "batch \u2013 a single batch to be collated", "Function that converts each NumPy array element into a torch.Tensor. If the input is a Sequence, Collection, or Mapping, it tries to convert each element inside to a torch.Tensor. If the input is not an NumPy array, it is left unchanged. This is used as the default function for collation when both batch_sampler and batch_size are NOT defined in DataLoader.", "The general input type to output type mapping is similar to that of default_collate(). See the description there for more details.", "data \u2013 a single data point to be converted", "Returns the information about the current DataLoader iterator worker process.", "When called in a worker, this returns an object guaranteed to have the following attributes:", "When called in the main process, this returns None.", "Note", "When used in a worker_init_fn passed over to DataLoader, this method can be useful to set up each worker process differently, for instance, using worker_id to configure the dataset object to only read a specific fraction of a sharded dataset, or use seed to seed other libraries used in dataset code.", "Optional[WorkerInfo]", "Randomly split a dataset into non-overlapping new datasets of given lengths.", "If a list of fractions that sum up to 1 is given, the lengths will be computed automatically as floor(frac * len(dataset)) for each fraction provided.", "After computing the lengths, if there are any remainders, 1 count will be distributed in round-robin fashion to the lengths until there are no remainders left.", "Optionally fix the generator for reproducible results, e.g.:", "List[Subset[T]]", "Base class for all Samplers.", "Every Sampler subclass has to provide an __iter__() method, providing a way to iterate over indices or lists of indices (batches) of dataset elements, and a __len__() method that returns the length of the returned iterators.", "data_source (Dataset) \u2013 This argument is not used and will be removed in 2.2.0. You may still have custom implementation that utilizes it.", "Note", "The __len__() method isn\u2019t strictly required by DataLoader, but is expected in any calculation involving the length of a DataLoader.", "Samples elements sequentially, always in the same order.", "data_source (Dataset) \u2013 dataset to sample from", "Samples elements randomly. If without replacement, then sample from a shuffled dataset. If with replacement, then user can specify num_samples to draw.", "Samples elements randomly from a given list of indices, without replacement.", "Samples elements from [0,..,len(weights)-1] with given probabilities (weights).", "Wraps another sampler to yield a mini-batch of indices.", "Sampler that restricts data loading to a subset of the dataset.", "It is especially useful in conjunction with torch.nn.parallel.DistributedDataParallel. In such a case, each process can pass a DistributedSampler instance as a DataLoader sampler, and load a subset of the original dataset that is exclusive to it.", "Note", "Dataset is assumed to be of constant size and that any instance of it always returns the same elements in the same order.", "Warning", "In distributed mode, calling the set_epoch() method at the beginning of each epoch before creating the DataLoader iterator is necessary to make shuffling work properly across multiple epochs. Otherwise, the same ordering will be always used.", "Example:"]}, {"name": "torch.utils.data._utils.collate.collate()", "path": "data#torch.utils.data._utils.collate.collate", "type": "Datasets and Data Loaders", "text": ["General collate function that handles collection type of element within each batch and opens function registry to deal with specific element types. default_collate_fn_map provides default collate functions for tensors, numpy arrays, numbers and strings.", "Note", "Each collate function requires a positional argument for batch and a keyword argument for the dictionary of collate functions as collate_fn_map."]}, {"name": "torch.utils.data.BatchSampler", "path": "data#torch.utils.data.BatchSampler", "type": "Datasets and Data Loaders", "text": ["Wraps another sampler to yield a mini-batch of indices."]}, {"name": "torch.utils.data.ChainDataset", "path": "data#torch.utils.data.ChainDataset", "type": "Datasets and Data Loaders", "text": ["Dataset for chaining multiple IterableDataset s.", "This class is useful to assemble different existing dataset streams. The chaining operation is done on-the-fly, so concatenating large-scale datasets with this class will be efficient.", "datasets (iterable of IterableDataset) \u2013 datasets to be chained together"]}, {"name": "torch.utils.data.ConcatDataset", "path": "data#torch.utils.data.ConcatDataset", "type": "Datasets and Data Loaders", "text": ["Dataset as a concatenation of multiple datasets.", "This class is useful to assemble different existing datasets.", "datasets (sequence) \u2013 List of datasets to be concatenated"]}, {"name": "torch.utils.data.DataLoader", "path": "data#torch.utils.data.DataLoader", "type": "Datasets and Data Loaders", "text": ["Data loader. Combines a dataset and a sampler, and provides an iterable over the given dataset.", "The DataLoader supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning.", "See torch.utils.data documentation page for more details.", "Warning", "If the spawn start method is used, worker_init_fn cannot be an unpicklable object, e.g., a lambda function. See Multiprocessing best practices on more details related to multiprocessing in PyTorch.", "Warning", "len(dataloader) heuristic is based on the length of the sampler used. When dataset is an IterableDataset, it instead returns an estimate based on len(dataset) / batch_size, with proper rounding depending on drop_last, regardless of multi-process loading configurations. This represents the best guess PyTorch can make because PyTorch trusts user dataset code in correctly handling multi-process loading to avoid duplicate data.", "However, if sharding results in multiple workers having incomplete last batches, this estimate can still be inaccurate, because (1) an otherwise complete batch can be broken into multiple ones and (2) more than one batch worth of samples can be dropped when drop_last is set. Unfortunately, PyTorch can not detect such cases in general.", "See Dataset Types for more details on these two types of datasets and how IterableDataset interacts with Multi-process data loading.", "Warning", "See Reproducibility, and My data loader workers return identical random numbers, and Randomness in multi-process data loading notes for random seed related questions."]}, {"name": "torch.utils.data.Dataset", "path": "data#torch.utils.data.Dataset", "type": "Datasets and Data Loaders", "text": ["An abstract class representing a Dataset.", "All datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite __getitem__(), supporting fetching a data sample for a given key. Subclasses could also optionally overwrite __len__(), which is expected to return the size of the dataset by many Sampler implementations and the default options of DataLoader. Subclasses could also optionally implement __getitems__(), for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.", "Note", "DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided."]}, {"name": "torch.utils.data.default_collate()", "path": "data#torch.utils.data.default_collate", "type": "Datasets and Data Loaders", "text": ["Function that takes in a batch of data and puts the elements within the batch into a tensor with an additional outer dimension - batch size. The exact output type can be a torch.Tensor, a Sequence of torch.Tensor, a Collection of torch.Tensor, or left unchanged, depending on the input type. This is used as the default function for collation when batch_size or batch_sampler is defined in DataLoader.", "Here is the general input type (based on the type of the element within the batch) to output type mapping:", "batch \u2013 a single batch to be collated"]}, {"name": "torch.utils.data.default_convert()", "path": "data#torch.utils.data.default_convert", "type": "Datasets and Data Loaders", "text": ["Function that converts each NumPy array element into a torch.Tensor. If the input is a Sequence, Collection, or Mapping, it tries to convert each element inside to a torch.Tensor. If the input is not an NumPy array, it is left unchanged. This is used as the default function for collation when both batch_sampler and batch_size are NOT defined in DataLoader.", "The general input type to output type mapping is similar to that of default_collate(). See the description there for more details.", "data \u2013 a single data point to be converted"]}, {"name": "torch.utils.data.distributed.DistributedSampler", "path": "data#torch.utils.data.distributed.DistributedSampler", "type": "Datasets and Data Loaders", "text": ["Sampler that restricts data loading to a subset of the dataset.", "It is especially useful in conjunction with torch.nn.parallel.DistributedDataParallel. In such a case, each process can pass a DistributedSampler instance as a DataLoader sampler, and load a subset of the original dataset that is exclusive to it.", "Note", "Dataset is assumed to be of constant size and that any instance of it always returns the same elements in the same order.", "Warning", "In distributed mode, calling the set_epoch() method at the beginning of each epoch before creating the DataLoader iterator is necessary to make shuffling work properly across multiple epochs. Otherwise, the same ordering will be always used.", "Example:"]}, {"name": "torch.utils.data.get_worker_info()", "path": "data#torch.utils.data.get_worker_info", "type": "Datasets and Data Loaders", "text": ["Returns the information about the current DataLoader iterator worker process.", "When called in a worker, this returns an object guaranteed to have the following attributes:", "When called in the main process, this returns None.", "Note", "When used in a worker_init_fn passed over to DataLoader, this method can be useful to set up each worker process differently, for instance, using worker_id to configure the dataset object to only read a specific fraction of a sharded dataset, or use seed to seed other libraries used in dataset code.", "Optional[WorkerInfo]"]}, {"name": "torch.utils.data.IterableDataset", "path": "data#torch.utils.data.IterableDataset", "type": "Datasets and Data Loaders", "text": ["An iterable Dataset.", "All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream.", "All subclasses should overwrite __iter__(), which would return an iterator of samples in this dataset.", "When a subclass is used with DataLoader, each item in the dataset will be yielded from the DataLoader iterator. When num_workers > 0, each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. get_worker_info(), when called in a worker process, returns information about the worker. It can be used in either the dataset\u2019s __iter__() method or the DataLoader \u2018s worker_init_fn option to modify each copy\u2019s behavior.", "Example 1: splitting workload across all workers in __iter__():", "Example 2: splitting workload across all workers using worker_init_fn:"]}, {"name": "torch.utils.data.random_split()", "path": "data#torch.utils.data.random_split", "type": "Datasets and Data Loaders", "text": ["Randomly split a dataset into non-overlapping new datasets of given lengths.", "If a list of fractions that sum up to 1 is given, the lengths will be computed automatically as floor(frac * len(dataset)) for each fraction provided.", "After computing the lengths, if there are any remainders, 1 count will be distributed in round-robin fashion to the lengths until there are no remainders left.", "Optionally fix the generator for reproducible results, e.g.:", "List[Subset[T]]"]}, {"name": "torch.utils.data.RandomSampler", "path": "data#torch.utils.data.RandomSampler", "type": "Datasets and Data Loaders", "text": ["Samples elements randomly. If without replacement, then sample from a shuffled dataset. If with replacement, then user can specify num_samples to draw."]}, {"name": "torch.utils.data.Sampler", "path": "data#torch.utils.data.Sampler", "type": "Datasets and Data Loaders", "text": ["Base class for all Samplers.", "Every Sampler subclass has to provide an __iter__() method, providing a way to iterate over indices or lists of indices (batches) of dataset elements, and a __len__() method that returns the length of the returned iterators.", "data_source (Dataset) \u2013 This argument is not used and will be removed in 2.2.0. You may still have custom implementation that utilizes it.", "Note", "The __len__() method isn\u2019t strictly required by DataLoader, but is expected in any calculation involving the length of a DataLoader."]}, {"name": "torch.utils.data.SequentialSampler", "path": "data#torch.utils.data.SequentialSampler", "type": "Datasets and Data Loaders", "text": ["Samples elements sequentially, always in the same order.", "data_source (Dataset) \u2013 dataset to sample from"]}, {"name": "torch.utils.data.StackDataset", "path": "data#torch.utils.data.StackDataset", "type": "Datasets and Data Loaders", "text": ["Dataset as a stacking of multiple datasets.", "This class is useful to assemble different parts of complex input data, given as datasets."]}, {"name": "torch.utils.data.Subset", "path": "data#torch.utils.data.Subset", "type": "Datasets and Data Loaders", "text": ["Subset of a dataset at specified indices."]}, {"name": "torch.utils.data.SubsetRandomSampler", "path": "data#torch.utils.data.SubsetRandomSampler", "type": "Datasets and Data Loaders", "text": ["Samples elements randomly from a given list of indices, without replacement."]}, {"name": "torch.utils.data.TensorDataset", "path": "data#torch.utils.data.TensorDataset", "type": "Datasets and Data Loaders", "text": ["Dataset wrapping tensors.", "Each sample will be retrieved by indexing tensors along the first dimension.", "*tensors (Tensor) \u2013 tensors that have the same size of the first dimension."]}, {"name": "torch.utils.data.WeightedRandomSampler", "path": "data#torch.utils.data.WeightedRandomSampler", "type": "Datasets and Data Loaders", "text": ["Samples elements from [0,..,len(weights)-1] with given probabilities (weights)."]}, {"name": "torch.utils.dlpack", "path": "dlpack", "type": "Miscellaneous", "text": ["Converts a tensor from an external library into a torch.Tensor.", "The returned PyTorch tensor will share the memory with the input tensor (which may have come from another library). Note that in-place operations will therefore also affect the data of the input tensor. This may lead to unexpected issues (e.g., other libraries may have read-only flags or immutable data structures), so the user should only do this if they know for sure that this is fine.", "ext_tensor (object with __dlpack__ attribute, or a DLPack capsule) \u2013 ", "The tensor or DLPack capsule to convert.", "If ext_tensor is a tensor (or ndarray) object, it must support the __dlpack__ protocol (i.e., have a ext_tensor.__dlpack__ method). Otherwise ext_tensor may be a DLPack capsule, which is an opaque PyCapsule instance, typically produced by a to_dlpack function or method.", "Tensor", "Examples:", "Returns an opaque object (a \u201cDLPack capsule\u201d) representing the tensor.", "Note", "to_dlpack is a legacy DLPack interface. The capsule it returns cannot be used for anything in Python other than use it as input to from_dlpack. The more idiomatic use of DLPack is to call from_dlpack directly on the tensor object - this works when that object has a __dlpack__ method, which PyTorch and most other libraries indeed have now.", "Warning", "Only call from_dlpack once per capsule produced with to_dlpack. Behavior when a capsule is consumed multiple times is undefined.", "tensor \u2013 a tensor to be exported", "The DLPack capsule shares the tensor\u2019s memory."]}, {"name": "torch.utils.dlpack.from_dlpack()", "path": "dlpack#torch.utils.dlpack.from_dlpack", "type": "Miscellaneous", "text": ["Converts a tensor from an external library into a torch.Tensor.", "The returned PyTorch tensor will share the memory with the input tensor (which may have come from another library). Note that in-place operations will therefore also affect the data of the input tensor. This may lead to unexpected issues (e.g., other libraries may have read-only flags or immutable data structures), so the user should only do this if they know for sure that this is fine.", "ext_tensor (object with __dlpack__ attribute, or a DLPack capsule) \u2013 ", "The tensor or DLPack capsule to convert.", "If ext_tensor is a tensor (or ndarray) object, it must support the __dlpack__ protocol (i.e., have a ext_tensor.__dlpack__ method). Otherwise ext_tensor may be a DLPack capsule, which is an opaque PyCapsule instance, typically produced by a to_dlpack function or method.", "Tensor", "Examples:"]}, {"name": "torch.utils.dlpack.to_dlpack()", "path": "dlpack#torch.utils.dlpack.to_dlpack", "type": "Miscellaneous", "text": ["Returns an opaque object (a \u201cDLPack capsule\u201d) representing the tensor.", "Note", "to_dlpack is a legacy DLPack interface. The capsule it returns cannot be used for anything in Python other than use it as input to from_dlpack. The more idiomatic use of DLPack is to call from_dlpack directly on the tensor object - this works when that object has a __dlpack__ method, which PyTorch and most other libraries indeed have now.", "Warning", "Only call from_dlpack once per capsule produced with to_dlpack. Behavior when a capsule is consumed multiple times is undefined.", "tensor \u2013 a tensor to be exported", "The DLPack capsule shares the tensor\u2019s memory."]}, {"name": "torch.utils.generate_methods_for_privateuse1_backend()", "path": "generated/torch.utils.generate_methods_for_privateuse1_backend#torch.utils.generate_methods_for_privateuse1_backend", "type": "Miscellaneous", "text": ["Automatically generate attributes and methods for the custom backend after rename privateuse1 backend. In the default scenario, storage-related methods will not be generated automatically.", "When you implement kernels for various torch operations, and register them to the PrivateUse1 dispatch key. And call the function torch.rename_privateuse1_backend(\u201cfoo\u201d) to rename your backend name. At this point, you can easily register specific methods and attributes by calling this function. Just like torch.Tensor.foo(), torch.Tensor.is_foo, torch.Storage.foo(), torch.Storage.is_foo.", "Note: We recommend you use generic functions (check devices are equal or to(device=)). We provide these methods for convenience only and they will be \u201cmonkey patched\u201d onto the objects and so will not be properly typed. For Storage methods generate, if you need to support sparse data storage, you need to extend the implementation yourself.", "Example:"]}, {"name": "torch.utils.get_cpp_backtrace()", "path": "generated/torch.utils.get_cpp_backtrace#torch.utils.get_cpp_backtrace", "type": "Miscellaneous", "text": ["Returns a string containing the C++ stack trace of the current thread. :param frames_to_skip: the number of frames to skip from the top of the stack :type frames_to_skip: int :param maximum_number_of_frames: the maximum number of frames to return :type maximum_number_of_frames: int", "str"]}, {"name": "torch.utils.mobile_optimizer", "path": "mobile_optimizer", "type": "Miscellaneous", "text": ["Warning", "This API is in beta and may change in the near future.", "Torch mobile supports torch.utils.mobile_optimizer.optimize_for_mobile utility to run a list of optimization pass with modules in eval mode. The method takes the following parameters: a torch.jit.ScriptModule object, a blocklisting optimization set, a preserved method list, and a backend.", "optimize_for_mobile will also invoke freeze_module pass which only preserves forward method. If you have other method to that needed to be preserved, add them into the preserved method list and pass into the method.", "A new optimized torch script module", "RecursiveScriptModule"]}, {"name": "torch.utils.mobile_optimizer.optimize_for_mobile()", "path": "mobile_optimizer#torch.utils.mobile_optimizer.optimize_for_mobile", "type": "Miscellaneous", "text": ["A new optimized torch script module", "RecursiveScriptModule"]}, {"name": "torch.utils.model_zoo", "path": "model_zoo", "type": "Miscellaneous", "text": ["Moved to torch.hub.", "Loads the Torch serialized object at the given URL.", "If downloaded file is a zip file, it will be automatically decompressed.", "If the object is already present in model_dir, it\u2019s deserialized and returned. The default value of model_dir is <hub_dir>/checkpoints where hub_dir is the directory returned by get_dir().", "Dict[str, Any]"]}, {"name": "torch.utils.model_zoo.load_url()", "path": "model_zoo#torch.utils.model_zoo.load_url", "type": "Miscellaneous", "text": ["Loads the Torch serialized object at the given URL.", "If downloaded file is a zip file, it will be automatically decompressed.", "If the object is already present in model_dir, it\u2019s deserialized and returned. The default value of model_dir is <hub_dir>/checkpoints where hub_dir is the directory returned by get_dir().", "Dict[str, Any]"]}, {"name": "torch.utils.rename_privateuse1_backend()", "path": "generated/torch.utils.rename_privateuse1_backend#torch.utils.rename_privateuse1_backend", "type": "Miscellaneous", "text": ["This API should be use to rename the privateuse1 backend device to make it more convenient to use as a device name within PyTorch APIs.", "The steps are:", "You can now use \u201cfoo\u201d as an ordinary device string in python.", "Note: this API can only be called once per process. Attempting to change the external backend after it\u2019s already been set will result in an error.", "Note(AMP): If you want to support AMP on your device, you can register a custom backend module. The backend must register a custom backend module with torch._register_device_module(\"foo\", BackendModule). BackendModule needs to have the following API\u2019s:", "Note(random): If you want to support to set seed for your device, BackendModule needs to have the following API\u2019s:", "And there are some common funcs:", "For more details, see https://pytorch.org/tutorials/advanced/extend_dispatcher.html#get-a-dispatch-key-for-your-backend For an existing example, see https://github.com/bdhirsh/pytorch_open_registration_example", "Example:"]}, {"name": "torch.utils.set_module()", "path": "generated/torch.utils.set_module#torch.utils.set_module", "type": "Miscellaneous", "text": ["Set the module attribute on a python object for a given object for nicer printing"]}, {"name": "torch.utils.tensorboard", "path": "tensorboard", "type": "Tensorboard", "text": ["Before going further, more details on TensorBoard can be found at https://www.tensorflow.org/tensorboard/", "Once you\u2019ve installed TensorBoard, these utilities let you log PyTorch models and metrics into a directory for visualization within the TensorBoard UI. Scalars, images, histograms, graphs, and embedding visualizations are all supported for PyTorch models and tensors as well as Caffe2 nets and blobs.", "The SummaryWriter class is your main entry to log data for consumption and visualization by TensorBoard. For example:", "This can then be visualized with TensorBoard, which should be installable and runnable with:", "Lots of information can be logged for one experiment. To avoid cluttering the UI and have better result clustering, we can group plots by naming them hierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped together, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately in the TensorBoard interface.", "Expected result:", "Writes entries directly to event files in the log_dir to be consumed by TensorBoard.", "The SummaryWriter class provides a high-level API to create an event file in a given directory and add summaries and events to it. The class updates the file contents asynchronously. This allows a training program to call methods to add data to the file directly from the training loop, without slowing down training.", "Creates a SummaryWriter that will write out events and summaries to the event file.", "Examples:", "Add scalar data to summary.", "Examples:", "Expected result:", "Adds many scalar data to summary.", "Examples:", "Expected result:", "Add histogram to summary.", "Examples:", "Expected result:", "Add image data to summary.", "Note that this requires the pillow package.", "img_tensor: Default is (3,H,W)(3, H, W). You can use torchvision.utils.make_grid() to convert a batch of tensor into 3xHxW format or call add_images and let us do the job. Tensor with (1,H,W)(1, H, W), (H,W)(H, W), (H,W,3)(H, W, 3) is also suitable as long as corresponding dataformats argument is passed, e.g. CHW, HWC, HW.", "Examples:", "Expected result:", "Add batched image data to summary.", "Note that this requires the pillow package.", "img_tensor: Default is (N,3,H,W)(N, 3, H, W). If dataformats is specified, other shape will be accepted. e.g. NCHW or NHWC.", "Examples:", "Expected result:", "Render matplotlib figure into an image and add it to summary.", "Note that this requires the matplotlib package.", "Add video data to summary.", "Note that this requires the moviepy package.", "vid_tensor: (N,T,C,H,W)(N, T, C, H, W). The values should lie in [0, 255] for type uint8 or [0, 1] for type float.", "Add audio data to summary.", "snd_tensor: (1,L)(1, L). The values should lie between [-1, 1].", "Add text data to summary.", "Examples:", "Add graph data to summary.", "Add embedding projector data to summary.", "mat: (N,D)(N, D), where N is number of data and D is feature dimension", "label_img: (N,C,H,W)(N, C, H, W)", "Examples:", "Adds precision recall curve. Plotting a precision-recall curve lets you understand your model\u2019s performance under different threshold settings. With this function, you provide the ground truth labeling (T/F) and prediction confidence (usually the output of your model) for each target. The TensorBoard UI will let you choose the threshold interactively.", "Examples:", "Create special chart by collecting charts tags in \u2018scalars\u2019. Note that this function can only be called once for each SummaryWriter() object. Because it only provides metadata to tensorboard, the function can be called before or after the training loop.", "layout (dict) \u2013 {categoryName: charts}, where charts is also a dictionary {chartName: ListOfProperties}. The first element in ListOfProperties is the chart\u2019s type (one of Multiline or Margin) and the second element should be a list containing the tags you have used in add_scalar function, which will be collected into the new chart.", "Examples:", "Add meshes or 3D point clouds to TensorBoard. The visualization is based on Three.js, so it allows users to interact with the rendered object. Besides the basic definitions such as vertices, faces, users can further provide camera parameter, lighting condition, etc. Please check https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene for advanced usage.", "vertices: (B,N,3)(B, N, 3). (batch, number_of_vertices, channels)", "colors: (B,N,3)(B, N, 3). The values should lie in [0, 255] for type uint8 or [0, 1] for type float.", "faces: (B,N,3)(B, N, 3). The values should lie in [0, number_of_vertices] for type uint8.", "Examples:", "Add a set of hyperparameters to be compared in TensorBoard.", "Examples:", "Expected result:", "Flushes the event file to disk. Call this method to make sure that all pending events have been written to disk."]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter", "type": "Tensorboard", "text": ["Writes entries directly to event files in the log_dir to be consumed by TensorBoard.", "The SummaryWriter class provides a high-level API to create an event file in a given directory and add summaries and events to it. The class updates the file contents asynchronously. This allows a training program to call methods to add data to the file directly from the training loop, without slowing down training.", "Creates a SummaryWriter that will write out events and summaries to the event file.", "Examples:", "Add scalar data to summary.", "Examples:", "Expected result:", "Adds many scalar data to summary.", "Examples:", "Expected result:", "Add histogram to summary.", "Examples:", "Expected result:", "Add image data to summary.", "Note that this requires the pillow package.", "img_tensor: Default is (3,H,W)(3, H, W). You can use torchvision.utils.make_grid() to convert a batch of tensor into 3xHxW format or call add_images and let us do the job. Tensor with (1,H,W)(1, H, W), (H,W)(H, W), (H,W,3)(H, W, 3) is also suitable as long as corresponding dataformats argument is passed, e.g. CHW, HWC, HW.", "Examples:", "Expected result:", "Add batched image data to summary.", "Note that this requires the pillow package.", "img_tensor: Default is (N,3,H,W)(N, 3, H, W). If dataformats is specified, other shape will be accepted. e.g. NCHW or NHWC.", "Examples:", "Expected result:", "Render matplotlib figure into an image and add it to summary.", "Note that this requires the matplotlib package.", "Add video data to summary.", "Note that this requires the moviepy package.", "vid_tensor: (N,T,C,H,W)(N, T, C, H, W). The values should lie in [0, 255] for type uint8 or [0, 1] for type float.", "Add audio data to summary.", "snd_tensor: (1,L)(1, L). The values should lie between [-1, 1].", "Add text data to summary.", "Examples:", "Add graph data to summary.", "Add embedding projector data to summary.", "mat: (N,D)(N, D), where N is number of data and D is feature dimension", "label_img: (N,C,H,W)(N, C, H, W)", "Examples:", "Adds precision recall curve. Plotting a precision-recall curve lets you understand your model\u2019s performance under different threshold settings. With this function, you provide the ground truth labeling (T/F) and prediction confidence (usually the output of your model) for each target. The TensorBoard UI will let you choose the threshold interactively.", "Examples:", "Create special chart by collecting charts tags in \u2018scalars\u2019. Note that this function can only be called once for each SummaryWriter() object. Because it only provides metadata to tensorboard, the function can be called before or after the training loop.", "layout (dict) \u2013 {categoryName: charts}, where charts is also a dictionary {chartName: ListOfProperties}. The first element in ListOfProperties is the chart\u2019s type (one of Multiline or Margin) and the second element should be a list containing the tags you have used in add_scalar function, which will be collected into the new chart.", "Examples:", "Add meshes or 3D point clouds to TensorBoard. The visualization is based on Three.js, so it allows users to interact with the rendered object. Besides the basic definitions such as vertices, faces, users can further provide camera parameter, lighting condition, etc. Please check https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene for advanced usage.", "vertices: (B,N,3)(B, N, 3). (batch, number_of_vertices, channels)", "colors: (B,N,3)(B, N, 3). The values should lie in [0, 255] for type uint8 or [0, 1] for type float.", "faces: (B,N,3)(B, N, 3). The values should lie in [0, number_of_vertices] for type uint8.", "Examples:", "Add a set of hyperparameters to be compared in TensorBoard.", "Examples:", "Expected result:", "Flushes the event file to disk. Call this method to make sure that all pending events have been written to disk."]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.__init__()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.__init__", "type": "Tensorboard", "text": ["Creates a SummaryWriter that will write out events and summaries to the event file.", "Examples:"]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_audio()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_audio", "type": "Tensorboard", "text": ["Add audio data to summary.", "snd_tensor: (1,L)(1, L). The values should lie between [-1, 1]."]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars", "type": "Tensorboard", "text": ["Create special chart by collecting charts tags in \u2018scalars\u2019. Note that this function can only be called once for each SummaryWriter() object. Because it only provides metadata to tensorboard, the function can be called before or after the training loop.", "layout (dict) \u2013 {categoryName: charts}, where charts is also a dictionary {chartName: ListOfProperties}. The first element in ListOfProperties is the chart\u2019s type (one of Multiline or Margin) and the second element should be a list containing the tags you have used in add_scalar function, which will be collected into the new chart.", "Examples:"]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_embedding()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_embedding", "type": "Tensorboard", "text": ["Add embedding projector data to summary.", "mat: (N,D)(N, D), where N is number of data and D is feature dimension", "label_img: (N,C,H,W)(N, C, H, W)", "Examples:"]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_figure()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_figure", "type": "Tensorboard", "text": ["Render matplotlib figure into an image and add it to summary.", "Note that this requires the matplotlib package."]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_graph()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_graph", "type": "Tensorboard", "text": ["Add graph data to summary."]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_histogram()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_histogram", "type": "Tensorboard", "text": ["Add histogram to summary.", "Examples:", "Expected result:"]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_hparams()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_hparams", "type": "Tensorboard", "text": ["Add a set of hyperparameters to be compared in TensorBoard.", "Examples:", "Expected result:"]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_image()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_image", "type": "Tensorboard", "text": ["Add image data to summary.", "Note that this requires the pillow package.", "img_tensor: Default is (3,H,W)(3, H, W). You can use torchvision.utils.make_grid() to convert a batch of tensor into 3xHxW format or call add_images and let us do the job. Tensor with (1,H,W)(1, H, W), (H,W)(H, W), (H,W,3)(H, W, 3) is also suitable as long as corresponding dataformats argument is passed, e.g. CHW, HWC, HW.", "Examples:", "Expected result:"]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_images()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_images", "type": "Tensorboard", "text": ["Add batched image data to summary.", "Note that this requires the pillow package.", "img_tensor: Default is (N,3,H,W)(N, 3, H, W). If dataformats is specified, other shape will be accepted. e.g. NCHW or NHWC.", "Examples:", "Expected result:"]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_mesh()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_mesh", "type": "Tensorboard", "text": ["Add meshes or 3D point clouds to TensorBoard. The visualization is based on Three.js, so it allows users to interact with the rendered object. Besides the basic definitions such as vertices, faces, users can further provide camera parameter, lighting condition, etc. Please check https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene for advanced usage.", "vertices: (B,N,3)(B, N, 3). (batch, number_of_vertices, channels)", "colors: (B,N,3)(B, N, 3). The values should lie in [0, 255] for type uint8 or [0, 1] for type float.", "faces: (B,N,3)(B, N, 3). The values should lie in [0, number_of_vertices] for type uint8.", "Examples:"]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve", "type": "Tensorboard", "text": ["Adds precision recall curve. Plotting a precision-recall curve lets you understand your model\u2019s performance under different threshold settings. With this function, you provide the ground truth labeling (T/F) and prediction confidence (usually the output of your model) for each target. The TensorBoard UI will let you choose the threshold interactively.", "Examples:"]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_scalar()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_scalar", "type": "Tensorboard", "text": ["Add scalar data to summary.", "Examples:", "Expected result:"]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_scalars()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_scalars", "type": "Tensorboard", "text": ["Adds many scalar data to summary.", "Examples:", "Expected result:"]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_text()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_text", "type": "Tensorboard", "text": ["Add text data to summary.", "Examples:"]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_video()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_video", "type": "Tensorboard", "text": ["Add video data to summary.", "Note that this requires the moviepy package.", "vid_tensor: (N,T,C,H,W)(N, T, C, H, W). The values should lie in [0, 255] for type uint8 or [0, 1] for type float."]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.close()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.close", "type": "Tensorboard", "text": []}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.flush()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.flush", "type": "Tensorboard", "text": ["Flushes the event file to disk. Call this method to make sure that all pending events have been written to disk."]}, {"name": "torch.utils.torch.utils.generate_methods_for_privateuse1_backend", "path": "generated/torch.utils.generate_methods_for_privateuse1_backend", "type": "Miscellaneous", "text": ["Automatically generate attributes and methods for the custom backend after rename privateuse1 backend. In the default scenario, storage-related methods will not be generated automatically.", "When you implement kernels for various torch operations, and register them to the PrivateUse1 dispatch key. And call the function torch.rename_privateuse1_backend(\u201cfoo\u201d) to rename your backend name. At this point, you can easily register specific methods and attributes by calling this function. Just like torch.Tensor.foo(), torch.Tensor.is_foo, torch.Storage.foo(), torch.Storage.is_foo.", "Note: We recommend you use generic functions (check devices are equal or to(device=)). We provide these methods for convenience only and they will be \u201cmonkey patched\u201d onto the objects and so will not be properly typed. For Storage methods generate, if you need to support sparse data storage, you need to extend the implementation yourself.", "Example:"]}, {"name": "torch.utils.torch.utils.get_cpp_backtrace", "path": "generated/torch.utils.get_cpp_backtrace", "type": "Miscellaneous", "text": ["Returns a string containing the C++ stack trace of the current thread. :param frames_to_skip: the number of frames to skip from the top of the stack :type frames_to_skip: int :param maximum_number_of_frames: the maximum number of frames to return :type maximum_number_of_frames: int", "str"]}, {"name": "torch.utils.torch.utils.rename_privateuse1_backend", "path": "generated/torch.utils.rename_privateuse1_backend", "type": "Miscellaneous", "text": ["This API should be use to rename the privateuse1 backend device to make it more convenient to use as a device name within PyTorch APIs.", "The steps are:", "You can now use \u201cfoo\u201d as an ordinary device string in python.", "Note: this API can only be called once per process. Attempting to change the external backend after it\u2019s already been set will result in an error.", "Note(AMP): If you want to support AMP on your device, you can register a custom backend module. The backend must register a custom backend module with torch._register_device_module(\"foo\", BackendModule). BackendModule needs to have the following API\u2019s:", "Note(random): If you want to support to set seed for your device, BackendModule needs to have the following API\u2019s:", "And there are some common funcs:", "For more details, see https://pytorch.org/tutorials/advanced/extend_dispatcher.html#get-a-dispatch-key-for-your-backend For an existing example, see https://github.com/bdhirsh/pytorch_open_registration_example", "Example:"]}, {"name": "torch.utils.torch.utils.set_module", "path": "generated/torch.utils.set_module", "type": "Miscellaneous", "text": ["Set the module attribute on a python object for a given object for nicer printing"]}, {"name": "torch.vander", "path": "generated/torch.vander", "type": "Torch", "text": ["Generates a Vandermonde matrix.", "The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0. If increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}. Such a matrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde.", "Vandermonde matrix. If increasing is False, the first column is x(N\u22121)x^{(N-1)}, the second x(N\u22122)x^{(N-2)} and so forth. If increasing is True, the columns are x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}.", "Tensor", "Example:"]}, {"name": "torch.var", "path": "generated/torch.var", "type": "Torch", "text": ["Calculates the variance over the dimensions specified by dim. dim can be a single dimension, list of dimensions, or None to reduce over all dimensions.", "The variance (\u03c32\\sigma^2) is calculated as", "where xx is the sample set of elements, x\u02c9\\bar{x} is the sample mean, NN is the number of samples and \u03b4N\\delta N is the correction.", "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).", "correction (int) \u2013 ", "difference between the sample size and sample degrees of freedom. Defaults to Bessel\u2019s correction, correction=1.", "Changed in version 2.0: Previously this argument was called unbiased and was a boolean with True corresponding to correction=1 and False being correction=0."]}, {"name": "torch.var_mean", "path": "generated/torch.var_mean", "type": "Torch", "text": ["Calculates the variance and mean over the dimensions specified by dim. dim can be a single dimension, list of dimensions, or None to reduce over all dimensions.", "The variance (\u03c32\\sigma^2) is calculated as", "where xx is the sample set of elements, x\u02c9\\bar{x} is the sample mean, NN is the number of samples and \u03b4N\\delta N is the correction.", "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).", "correction (int) \u2013 ", "difference between the sample size and sample degrees of freedom. Defaults to Bessel\u2019s correction, correction=1.", "Changed in version 2.0: Previously this argument was called unbiased and was a boolean with True corresponding to correction=1 and False being correction=0.", "A tuple (var, mean) containing the variance and mean."]}, {"name": "torch.vdot", "path": "generated/torch.vdot", "type": "Torch", "text": ["Computes the dot product of two 1D vectors along a dimension.", "In symbols, this function computes", "where xi\u203e\\overline{x_i} denotes the conjugate for complex vectors, and it is the identity for real vectors.", "Note", "Unlike NumPy\u2019s vdot, torch.vdot intentionally only supports computing the dot product of two 1D tensors with the same number of elements.", "See also", "torch.linalg.vecdot() computes the dot product of two batches of vectors along a dimension.", "Keyword args:", "Note", "out (Tensor, optional): the output tensor.", "Example:"]}, {"name": "torch.view_as_complex", "path": "generated/torch.view_as_complex", "type": "Torch", "text": ["Returns a view of input as a complex tensor. For an input complex tensor of size m1,m2,\u2026,mi,2m1, m2, \\dots, mi, 2, this function returns a new complex tensor of size m1,m2,\u2026,mim1, m2, \\dots, mi where the last dimension of the input tensor is expected to represent the real and imaginary components of complex numbers.", "Warning", "view_as_complex() is only supported for tensors with torch.dtype torch.float64 and torch.float32. The input is expected to have the last dimension of size 2. In addition, the tensor must have a stride of 1 for its last dimension. The strides of all other dimensions must be even numbers.", "input (Tensor) \u2013 the input tensor.", "Example:"]}, {"name": "torch.view_as_real", "path": "generated/torch.view_as_real", "type": "Torch", "text": ["Returns a view of input as a real tensor. For an input complex tensor of size m1,m2,\u2026,mim1, m2, \\dots, mi, this function returns a new real tensor of size m1,m2,\u2026,mi,2m1, m2, \\dots, mi, 2, where the last dimension of size 2 represents the real and imaginary components of complex numbers.", "Warning", "view_as_real() is only supported for tensors with complex dtypes.", "input (Tensor) \u2013 the input tensor.", "Example:"]}, {"name": "torch.vmap", "path": "generated/torch.vmap", "type": "Torch", "text": ["vmap is the vectorizing map; vmap(func) returns a new function that maps func over some dimension of the inputs. Semantically, vmap pushes the map into PyTorch operations called by func, effectively vectorizing those operations.", "vmap is useful for handling batch dimensions: one can write a function func that runs on examples and then lift it to a function that can take batches of examples with vmap(func). vmap can also be used to compute batched gradients when composed with autograd.", "Note", "torch.vmap() is aliased to torch.func.vmap() for convenience. Use whichever one you\u2019d like.", "Returns a new \u201cbatched\u201d function. It takes the same inputs as func, except each input has an extra dimension at the index specified by in_dims. It takes returns the same outputs as func, except each output has an extra dimension at the index specified by out_dims.", "Callable", "One example of using vmap() is to compute batched dot products. PyTorch doesn\u2019t provide a batched torch.dot API; instead of unsuccessfully rummaging through docs, use vmap() to construct a new function.", "vmap() can be helpful in hiding batch dimensions, leading to a simpler model authoring experience.", "vmap() can also help vectorize computations that were previously difficult or impossible to batch. One example is higher-order gradient computation. The PyTorch autograd engine computes vjps (vector-Jacobian products). Computing a full Jacobian matrix for some function f: R^N -> R^N usually requires N calls to autograd.grad, one per Jacobian row. Using vmap(), we can vectorize the whole computation, computing the Jacobian in a single call to autograd.grad.", "vmap() can also be nested, producing an output with multiple batched dimensions", "If the inputs are not batched along the first dimension, in_dims specifies the dimension that each inputs are batched along as", "If there are multiple inputs each of which is batched along different dimensions, in_dims must be a tuple with the batch dimension for each input as", "If the input is a Python struct, in_dims must be a tuple containing a struct matching the shape of the input:", "By default, the output is batched along the first dimension. However, it can be batched along any dimension by using out_dims", "For any function that uses kwargs, the returned function will not batch the kwargs but will accept kwargs", "Note", "vmap does not provide general autobatching or handle variable-length sequences out of the box."]}, {"name": "torch.vsplit", "path": "generated/torch.vsplit", "type": "Torch", "text": ["Splits input, a tensor with two or more dimensions, into multiple tensors vertically according to indices_or_sections. Each split is a view of input.", "This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=0) (the split dimension is 0), except that if indices_or_sections is an integer it must evenly divide the split dimension or a runtime error will be thrown.", "This function is based on NumPy\u2019s numpy.vsplit()."]}, {"name": "torch.vstack", "path": "generated/torch.vstack", "type": "Torch", "text": ["Stack tensors in sequence vertically (row wise).", "This is equivalent to concatenation along the first axis after all 1-D tensors have been reshaped by torch.atleast_2d().", "tensors (sequence of Tensors) \u2013 sequence of tensors to concatenate", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.where", "path": "generated/torch.where", "type": "Torch", "text": ["Return a tensor of elements selected from either input or other, depending on condition.", "The operation is defined as:", "Note", "The tensors condition, input, other must be broadcastable.", "out (Tensor, optional) \u2013 the output tensor.", "A tensor of shape equal to the broadcasted shape of condition, input, other", "Tensor", "Example:", "torch.where(condition) is identical to torch.nonzero(condition, as_tuple=True).", "Note", "See also torch.nonzero()."]}, {"name": "torch.xlogy", "path": "generated/torch.xlogy", "type": "Torch", "text": ["Alias for torch.special.xlogy()."]}, {"name": "torch.zeros", "path": "generated/torch.zeros", "type": "Torch", "text": ["Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.", "size (int...) \u2013 a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.", "Example:"]}, {"name": "torch.zeros_like", "path": "generated/torch.zeros_like", "type": "Torch", "text": ["Returns a tensor filled with the scalar value 0, with the same size as input. torch.zeros_like(input) is equivalent to torch.zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).", "Warning", "As of 0.4, this function does not support an out keyword. As an alternative, the old torch.zeros_like(input, out=output) is equivalent to torch.zeros(input.size(), out=output).", "input (Tensor) \u2013 the size of input will determine size of the output tensor.", "Example:"]}]