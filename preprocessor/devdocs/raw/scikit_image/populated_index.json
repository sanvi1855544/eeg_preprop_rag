[{"name": "A crash course on NumPy for images", "path": "user_guide/numpy_images", "type": "Guide", "text": "\nImages in `scikit-image` are represented by NumPy ndarrays. Hence, many common\noperations can be achieved using standard NumPy methods for manipulating\narrays:\n\nRetrieving the geometry of the image and the number of pixels:\n\nRetrieving statistical information about image intensity values:\n\nNumPy arrays representing images can be of different integer or float\nnumerical types. See Image data types and what they mean for more information\nabout these types and how `scikit-image` treats them.\n\nNumPy indexing can be used both for looking at the pixel values and to modify\nthem:\n\nBe careful! In NumPy indexing, the first dimension (`camera.shape[0]`)\ncorresponds to rows, while the second (`camera.shape[1]`) corresponds to\ncolumns, with the origin (`camera[0, 0]`) at the top-left corner. This matches\nmatrix/linear algebra notation, but is in contrast to Cartesian (x, y)\ncoordinates. See Coordinate conventions below for more details.\n\nBeyond individual pixels, it is possible to access/modify values of whole sets\nof pixels using the different indexing capabilities of NumPy.\n\nSlicing:\n\nMasking (indexing with masks of booleans):\n\nFancy indexing (indexing with sets of indices):\n\nMasks are very useful when you need to select a set of pixels on which to\nperform the manipulations. The mask can be any boolean array of the same shape\nas the image (or a shape broadcastable to the image shape). This can be used\nto define a region of interest, for example, a disk:\n\nBoolean operations from NumPy can be used to define even more complex masks:\n\nAll of the above remains true for color images. A color image is a NumPy array\nwith an additional trailing dimension for the channels:\n\nThis shows that `cat` is a 300-by-451 pixel image with three channels (red,\ngreen, and blue). As before, we can get and set the pixel values:\n\nWe can also use 2D boolean masks for 2D multichannel images, as we did with\nthe grayscale image above:\n\nUsing a 2D mask on a 2D color image\n\n(Source code, png, pdf)\n\nBecause `scikit-image` represents images using NumPy arrays, the coordinate\nconventions must match. Two-dimensional (2D) grayscale images (such as\n`camera` above) are indexed by rows and columns (abbreviated to either `(row,\ncol)` or `(r, c)`), with the lowest element `(0, 0)` at the top-left corner.\nIn various parts of the library, you will also see `rr` and `cc` refer to\nlists of row and column coordinates. We distinguish this convention from `(x,\ny)`, which commonly denote standard Cartesian coordinates, where `x` is the\nhorizontal coordinate, `y` \\- the vertical one, and the origin is at the\nbottom left (Matplotlib axes, for example, use this convention).\n\nIn the case of multichannel images, the last dimension is used for color\nchannels and is denoted by `channel` or `ch`.\n\nFinally, for volumetric (3D) images, such as videos, magnetic resonance\nimaging (MRI) scans, confocal microscopy, etc. we refer to the leading\ndimension as `plane`, abbreviated as `pln` or `p`.\n\nThese conventions are summarized below:\n\nImage type\n\nCoordinates\n\n2D grayscale\n\n(row, col)\n\n2D multichannel (eg. RGB)\n\n(row, col, ch)\n\n3D grayscale\n\n(pln, row, col)\n\n3D multichannel\n\n(pln, row, col, ch)\n\nMany functions in `scikit-image` can operate on 3D images directly:\n\nIn many cases, however, the third spatial dimension has lower resolution than\nthe other two. Some `scikit-image` functions provide a `spacing` keyword\nargument to help handle this kind of data:\n\nOther times, the processing must be done plane-wise. When planes are stacked\nalong the leading dimension (in agreement with our convention), the following\nsyntax can be used:\n\nAlthough the labeling of the axes might seem arbitrary, it can have a\nsignificant effect on the speed of operations. This is because modern\nprocessors never retrieve just one item from memory, but rather a whole chunk\nof adjacent items (an operation called prefetching). Therefore, processing of\nelements that are next to each other in memory is faster than processing them\nwhen they are scattered, even if the number of operations is the same:\n\nWhen the last/rightmost dimension becomes even larger the speedup is even more\ndramatic. It is worth thinking about data locality when developing algorithms.\nIn particular, `scikit-image` uses C-contiguous arrays by default. When using\nnested loops, the last/rightmost dimension of the array should be in the\ninnermost loop of the computation. In the example above, the `*=` numpy\noperator iterates over all remaining dimensions.\n\nAlthough `scikit-image` does not currently provide functions to work\nspecifically with time-varying 3D data, its compatibility with NumPy arrays\nallows us to work quite naturally with a 5D array of the shape (t, pln, row,\ncol, ch):\n\nWe can then supplement the above table as follows:\n\nImage type\n\ncoordinates\n\n2D color video\n\n(t, row, col, ch)\n\n3D multichannel video\n\n(t, pln, row, col, ch)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color", "path": "api/skimage.color", "type": "color", "text": "\n`skimage.color.combine_stains`(stains, conv_matrix)\n\nStain to RGB color space conversion.\n\n`skimage.color.convert_colorspace`(arr, \u2026)\n\nConvert an image array to a new color space.\n\n`skimage.color.deltaE_cie76`(lab1, lab2)\n\nEuclidean distance between two points in Lab color space\n\n`skimage.color.deltaE_ciede2000`(lab1, lab2[, \u2026])\n\nColor difference as given by the CIEDE 2000 standard.\n\n`skimage.color.deltaE_ciede94`(lab1, lab2[, \u2026])\n\nColor difference according to CIEDE 94 standard\n\n`skimage.color.deltaE_cmc`(lab1, lab2[, kL, kC])\n\nColor difference from the CMC l:c standard.\n\n`skimage.color.gray2rgb`(image[, alpha])\n\nCreate an RGB representation of a gray-level image.\n\n`skimage.color.gray2rgba`(image[, alpha])\n\nCreate a RGBA representation of a gray-level image.\n\n`skimage.color.grey2rgb`(image[, alpha])\n\nCreate an RGB representation of a gray-level image.\n\n`skimage.color.hed2rgb`(hed)\n\nHaematoxylin-Eosin-DAB (HED) to RGB color space conversion.\n\n`skimage.color.hsv2rgb`(hsv)\n\nHSV to RGB color space conversion.\n\n`skimage.color.lab2lch`(lab)\n\nCIE-LAB to CIE-LCH color space conversion.\n\n`skimage.color.lab2rgb`(lab[, illuminant, \u2026])\n\nLab to RGB color space conversion.\n\n`skimage.color.lab2xyz`(lab[, illuminant, \u2026])\n\nCIE-LAB to XYZcolor space conversion.\n\n`skimage.color.label2rgb`(label[, image, \u2026])\n\nReturn an RGB image where color-coded labels are painted over the image.\n\n`skimage.color.lch2lab`(lch)\n\nCIE-LCH to CIE-LAB color space conversion.\n\n`skimage.color.rgb2gray`(rgb)\n\nCompute luminance of an RGB image.\n\n`skimage.color.rgb2grey`(rgb)\n\nCompute luminance of an RGB image.\n\n`skimage.color.rgb2hed`(rgb)\n\nRGB to Haematoxylin-Eosin-DAB (HED) color space conversion.\n\n`skimage.color.rgb2hsv`(rgb)\n\nRGB to HSV color space conversion.\n\n`skimage.color.rgb2lab`(rgb[, illuminant, \u2026])\n\nConversion from the sRGB color space (IEC 61966-2-1:1999) to the CIE Lab\ncolorspace under the given illuminant and observer.\n\n`skimage.color.rgb2rgbcie`(rgb)\n\nRGB to RGB CIE color space conversion.\n\n`skimage.color.rgb2xyz`(rgb)\n\nRGB to XYZ color space conversion.\n\n`skimage.color.rgb2ycbcr`(rgb)\n\nRGB to YCbCr color space conversion.\n\n`skimage.color.rgb2ydbdr`(rgb)\n\nRGB to YDbDr color space conversion.\n\n`skimage.color.rgb2yiq`(rgb)\n\nRGB to YIQ color space conversion.\n\n`skimage.color.rgb2ypbpr`(rgb)\n\nRGB to YPbPr color space conversion.\n\n`skimage.color.rgb2yuv`(rgb)\n\nRGB to YUV color space conversion.\n\n`skimage.color.rgba2rgb`(rgba[, background])\n\nRGBA to RGB conversion using alpha blending [1].\n\n`skimage.color.rgbcie2rgb`(rgbcie)\n\nRGB CIE to RGB color space conversion.\n\n`skimage.color.separate_stains`(rgb, conv_matrix)\n\nRGB to stain color space conversion.\n\n`skimage.color.xyz2lab`(xyz[, illuminant, \u2026])\n\nXYZ to CIE-LAB color space conversion.\n\n`skimage.color.xyz2rgb`(xyz)\n\nXYZ to RGB color space conversion.\n\n`skimage.color.ycbcr2rgb`(ycbcr)\n\nYCbCr to RGB color space conversion.\n\n`skimage.color.ydbdr2rgb`(ydbdr)\n\nYDbDr to RGB color space conversion.\n\n`skimage.color.yiq2rgb`(yiq)\n\nYIQ to RGB color space conversion.\n\n`skimage.color.ypbpr2rgb`(ypbpr)\n\nYPbPr to RGB color space conversion.\n\n`skimage.color.yuv2rgb`(yuv)\n\nYUV to RGB color space conversion.\n\nStain to RGB color space conversion.\n\nThe image in stain color space. Final dimension denotes channels.\n\nThe stain separation matrix as described by G. Landini [1].\n\nThe image in RGB format. Same dimensions as input.\n\nIf `stains` is not at least 2-D with shape (\u2026, 3).\n\nStain combination matrices available in the `color` module and their\nrespective colorspace:\n\nhttps://web.archive.org/web/20160624145052/http://www.mecourse.com/landinig/software/cdeconv/cdeconv.html\n\nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by\ncolor deconvolution,\u201d Anal. Quant. Cytol. Histol., vol. 23, no. 4, pp.\n291\u2013299, Aug. 2001.\n\nConvert an image array to a new color space.\n\n\u2018RGB\u2019, \u2018HSV\u2019, \u2018RGB CIE\u2019, \u2018XYZ\u2019, \u2018YUV\u2019, \u2018YIQ\u2019, \u2018YPbPr\u2019, \u2018YCbCr\u2019, \u2018YDbDr\u2019\n\nThe image to convert. Final dimension denotes channels.\n\nThe color space to convert from. Can be specified in lower case.\n\nThe color space to convert to. Can be specified in lower case.\n\nThe converted image. Same dimensions as input.\n\nIf fromspace is not a valid color space\n\nIf tospace is not a valid color space\n\nConversion is performed through the \u201ccentral\u201d RGB color space, i.e. conversion\nfrom XYZ to HSV is implemented as `XYZ -> RGB -> HSV` instead of directly.\n\nEuclidean distance between two points in Lab color space\n\nreference color (Lab colorspace)\n\ncomparison color (Lab colorspace)\n\ndistance between colors `lab1` and `lab2`\n\nhttps://en.wikipedia.org/wiki/Color_difference\n\nA. R. Robertson, \u201cThe CIE 1976 color-difference formulae,\u201d Color Res. Appl. 2,\n7-11 (1977).\n\nColor difference as given by the CIEDE 2000 standard.\n\nCIEDE 2000 is a major revision of CIDE94. The perceptual calibration is\nlargely based on experience with automotive paint on smooth surfaces.\n\nreference color (Lab colorspace)\n\ncomparison color (Lab colorspace)\n\nlightness scale factor, 1 for \u201cacceptably close\u201d; 2 for \u201cimperceptible\u201d see\ndeltaE_cmc\n\nchroma scale factor, usually 1\n\nhue scale factor, usually 1\n\nThe distance between `lab1` and `lab2`\n\nCIEDE 2000 assumes parametric weighting factors for the lightness, chroma, and\nhue (`kL`, `kC`, `kH` respectively). These default to 1.\n\nhttps://en.wikipedia.org/wiki/Color_difference\n\nhttp://www.ece.rochester.edu/~gsharma/ciede2000/ciede2000noteCRNA.pdf\nDOI:10.1364/AO.33.008069\n\nM. Melgosa, J. Quesada, and E. Hita, \u201cUniformity of some recent color metrics\ntested with an accurate color-difference tolerance dataset,\u201d Appl. Opt. 33,\n8069-8077 (1994).\n\nColor difference according to CIEDE 94 standard\n\nAccommodates perceptual non-uniformities through the use of application\nspecific scale factors (`kH`, `kC`, `kL`, `k1`, and `k2`).\n\nreference color (Lab colorspace)\n\ncomparison color (Lab colorspace)\n\nHue scale\n\nChroma scale\n\nLightness scale\n\nfirst scale parameter\n\nsecond scale parameter\n\ncolor difference between `lab1` and `lab2`\n\ndeltaE_ciede94 is not symmetric with respect to lab1 and lab2. CIEDE94 defines\nthe scales for the lightness, hue, and chroma in terms of the first color.\nConsequently, the first color should be regarded as the \u201creference\u201d color.\n\n`kL`, `k1`, `k2` depend on the application and default to the values suggested\nfor graphic arts\n\nParameter\n\nGraphic Arts\n\nTextiles\n\n`kL`\n\n1.000\n\n2.000\n\n`k1`\n\n0.045\n\n0.048\n\n`k2`\n\n0.015\n\n0.014\n\nhttps://en.wikipedia.org/wiki/Color_difference\n\nhttp://www.brucelindbloom.com/index.html?Eqn_DeltaE_CIE94.html\n\nColor difference from the CMC l:c standard.\n\nThis color difference was developed by the Colour Measurement Committee (CMC)\nof the Society of Dyers and Colourists (United Kingdom). It is intended for\nuse in the textile industry.\n\nThe scale factors `kL`, `kC` set the weight given to differences in lightness\nand chroma relative to differences in hue. The usual values are `kL=2`, `kC=1`\nfor \u201cacceptability\u201d and `kL=1`, `kC=1` for \u201cimperceptibility\u201d. Colors with `dE\n> 1` are \u201cdifferent\u201d for the given scale factors.\n\nreference color (Lab colorspace)\n\ncomparison color (Lab colorspace)\n\ndistance between colors `lab1` and `lab2`\n\ndeltaE_cmc the defines the scales for the lightness, hue, and chroma in terms\nof the first color. Consequently `deltaE_cmc(lab1, lab2) != deltaE_cmc(lab2,\nlab1)`\n\nhttps://en.wikipedia.org/wiki/Color_difference\n\nhttp://www.brucelindbloom.com/index.html?Eqn_DeltaE_CIE94.html\n\nF. J. J. Clarke, R. McDonald, and B. Rigg, \u201cModification to the JPC79 colour-\ndifference formula,\u201d J. Soc. Dyers Colour. 100, 128-132 (1984).\n\nCreate an RGB representation of a gray-level image.\n\nInput image.\n\nEnsure that the output image has an alpha layer. If None, alpha layers are\npassed through but not created.\n\nRGB image. A new dimension of length 3 is added to input image.\n\nIf the input is a 1-dimensional image of shape `(M, )`, the output will be\nshape `(M, 3)`.\n\nTinting gray-scale images\n\nCreate a RGBA representation of a gray-level image.\n\nInput image.\n\nAlpha channel of the output image. It may be a scalar or an array that can be\nbroadcast to `image`. If not specified it is set to the maximum limit\ncorresponding to the `image` dtype.\n\nRGBA image. A new dimension of length 4 is added to input image shape.\n\nCreate an RGB representation of a gray-level image.\n\nInput image.\n\nEnsure that the output image has an alpha layer. If None, alpha layers are\npassed through but not created.\n\nRGB image. A new dimension of length 3 is added to input image.\n\nIf the input is a 1-dimensional image of shape `(M, )`, the output will be\nshape `(M, 3)`.\n\nHaematoxylin-Eosin-DAB (HED) to RGB color space conversion.\n\nThe image in the HED color space. Final dimension denotes channels.\n\nThe image in RGB. Same dimensions as input.\n\nIf `hed` is not at least 2-D with shape (\u2026, 3).\n\nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by\ncolor deconvolution.,\u201d Analytical and quantitative cytology and histology /\nthe International Academy of Cytology [and] American Society of Cytology, vol.\n23, no. 4, pp. 291-9, Aug. 2001.\n\nHSV to RGB color space conversion.\n\nThe image in HSV format. Final dimension denotes channels.\n\nThe image in RGB format. Same dimensions as input.\n\nIf `hsv` is not at least 2-D with shape (\u2026, 3).\n\nConversion between RGB and HSV color spaces results in some loss of precision,\ndue to integer arithmetic and rounding [1].\n\nhttps://en.wikipedia.org/wiki/HSL_and_HSV\n\nTinting gray-scale images\n\nFlood Fill\n\nCIE-LAB to CIE-LCH color space conversion.\n\nLCH is the cylindrical representation of the LAB (Cartesian) colorspace\n\nThe N-D image in CIE-LAB format. The last (`N+1`-th) dimension must have at\nleast 3 elements, corresponding to the `L`, `a`, and `b` color channels.\nSubsequent elements are copied.\n\nThe image in LCH format, in a N-D array with same shape as input `lab`.\n\nIf `lch` does not have at least 3 color channels (i.e. l, a, b).\n\nThe Hue is expressed as an angle between `(0, 2*pi)`\n\nLab to RGB color space conversion.\n\nThe image in Lab format. Final dimension denotes channels.\n\nThe name of the illuminant (the function is NOT case sensitive).\n\nThe aperture angle of the observer.\n\nThe image in RGB format. Same dimensions as input.\n\nIf `lab` is not at least 2-D with shape (\u2026, 3).\n\nThis function uses lab2xyz and xyz2rgb. By default Observer= 2A, Illuminant=\nD65. CIE XYZ tristimulus values x_ref=95.047, y_ref=100., z_ref=108.883. See\nfunction `get_xyz_coords` for a list of supported illuminants.\n\nhttps://en.wikipedia.org/wiki/Standard_illuminant\n\nCIE-LAB to XYZcolor space conversion.\n\nThe image in Lab format. Final dimension denotes channels.\n\nThe name of the illuminant (the function is NOT case sensitive).\n\nThe aperture angle of the observer.\n\nThe image in XYZ format. Same dimensions as input.\n\nIf `lab` is not at least 2-D with shape (\u2026, 3).\n\nIf either the illuminant or the observer angle are not supported or unknown.\n\nIf any of the pixels are invalid (Z < 0).\n\nBy default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref =\n95.047, y_ref = 100., z_ref = 108.883. See function \u2018get_xyz_coords\u2019 for a\nlist of supported illuminants.\n\nhttp://www.easyrgb.com/index.php?X=MATH&H=07\n\nhttps://en.wikipedia.org/wiki/Lab_color_space\n\nReturn an RGB image where color-coded labels are painted over the image.\n\nInteger array of labels with the same shape as `image`.\n\nImage used as underlay for labels. If the input is an RGB image, it\u2019s\nconverted to grayscale before coloring.\n\nList of colors. If the number of labels exceeds the number of colors, then the\ncolors are cycled.\n\nOpacity of colorized labels. Ignored if image is `None`.\n\nLabel that\u2019s treated as the background. If `bg_label` is specified, `bg_color`\nis `None`, and `kind` is `overlay`, background is not painted by any colors.\n\nBackground color. Must be a name in `color_dict` or RGB float values between\n[0, 1].\n\nOpacity of the image.\n\nThe kind of color image desired. \u2018overlay\u2019 cycles over defined colors and\noverlays the colored labels over the original image. \u2018avg\u2019 replaces each\nlabeled segment with its average color, for a stained-class or pastel painting\nappearance.\n\nThe result of blending a cycling colormap (`colors`) for each distinct value\nin `label` with the image, at a certain alpha value.\n\nSegment human cells (in mitosis)\n\nCIE-LCH to CIE-LAB color space conversion.\n\nLCH is the cylindrical representation of the LAB (Cartesian) colorspace\n\nThe N-D image in CIE-LCH format. The last (`N+1`-th) dimension must have at\nleast 3 elements, corresponding to the `L`, `a`, and `b` color channels.\nSubsequent elements are copied.\n\nThe image in LAB format, with same shape as input `lch`.\n\nIf `lch` does not have at least 3 color channels (i.e. l, c, h).\n\nCompute luminance of an RGB image.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe luminance image - an array which is the same size as the input array, but\nwith the channel dimension removed.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nThe weights used in this conversion are calibrated for contemporary CRT\nphosphors:\n\nIf there is an alpha channel present, it is ignored.\n\nhttp://poynton.ca/PDFs/ColorFAQ.pdf\n\nRegistration using optical flow\n\nPhase Unwrapping\n\nCompute luminance of an RGB image.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe luminance image - an array which is the same size as the input array, but\nwith the channel dimension removed.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nThe weights used in this conversion are calibrated for contemporary CRT\nphosphors:\n\nIf there is an alpha channel present, it is ignored.\n\nhttp://poynton.ca/PDFs/ColorFAQ.pdf\n\nRGB to Haematoxylin-Eosin-DAB (HED) color space conversion.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe image in HED format. Same dimensions as input.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by\ncolor deconvolution.,\u201d Analytical and quantitative cytology and histology /\nthe International Academy of Cytology [and] American Society of Cytology, vol.\n23, no. 4, pp. 291-9, Aug. 2001.\n\nRGB to HSV color space conversion.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe image in HSV format. Same dimensions as input.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nConversion between RGB and HSV color spaces results in some loss of precision,\ndue to integer arithmetic and rounding [1].\n\nhttps://en.wikipedia.org/wiki/HSL_and_HSV\n\nTinting gray-scale images\n\nFlood Fill\n\nConversion from the sRGB color space (IEC 61966-2-1:1999) to the CIE Lab\ncolorspace under the given illuminant and observer.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe name of the illuminant (the function is NOT case sensitive).\n\nThe aperture angle of the observer.\n\nThe image in Lab format. Same dimensions as input.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nRGB is a device-dependent color space so, if you use this function, be sure\nthat the image you are analyzing has been mapped to the sRGB color space.\n\nThis function uses rgb2xyz and xyz2lab. By default Observer= 2A, Illuminant=\nD65. CIE XYZ tristimulus values x_ref=95.047, y_ref=100., z_ref=108.883. See\nfunction `get_xyz_coords` for a list of supported illuminants.\n\nhttps://en.wikipedia.org/wiki/Standard_illuminant\n\nRGB to RGB CIE color space conversion.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe image in RGB CIE format. Same dimensions as input.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nhttps://en.wikipedia.org/wiki/CIE_1931_color_space\n\nRGB to XYZ color space conversion.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe image in XYZ format. Same dimensions as input.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nThe CIE XYZ color space is derived from the CIE RGB color space. Note however\nthat this function converts from sRGB.\n\nhttps://en.wikipedia.org/wiki/CIE_1931_color_space\n\nRGB to YCbCr color space conversion.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe image in YCbCr format. Same dimensions as input.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nY is between 16 and 235. This is the color space commonly used by video\ncodecs; it is sometimes incorrectly called \u201cYUV\u201d.\n\nhttps://en.wikipedia.org/wiki/YCbCr\n\nRGB to YDbDr color space conversion.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe image in YDbDr format. Same dimensions as input.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nThis is the color space commonly used by video codecs. It is also the\nreversible color transform in JPEG2000.\n\nhttps://en.wikipedia.org/wiki/YDbDr\n\nRGB to YIQ color space conversion.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe image in YIQ format. Same dimensions as input.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nRGB to YPbPr color space conversion.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe image in YPbPr format. Same dimensions as input.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nhttps://en.wikipedia.org/wiki/YPbPr\n\nRGB to YUV color space conversion.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe image in YUV format. Same dimensions as input.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nY is between 0 and 1. Use YCbCr instead of YUV for the color space commonly\nused by video codecs, where Y ranges from 16 to 235.\n\nhttps://en.wikipedia.org/wiki/YUV\n\nRGBA to RGB conversion using alpha blending [1].\n\nThe image in RGBA format. Final dimension denotes channels.\n\nThe color of the background to blend the image with (3 floats between 0 to 1 -\nthe RGB value of the background).\n\nThe image in RGB format. Same dimensions as input.\n\nIf `rgba` is not at least 2-D with shape (\u2026, 4).\n\nhttps://en.wikipedia.org/wiki/Alpha_compositing#Alpha_blending\n\nRGB CIE to RGB color space conversion.\n\nThe image in RGB CIE format. Final dimension denotes channels.\n\nThe image in RGB format. Same dimensions as input.\n\nIf `rgbcie` is not at least 2-D with shape (\u2026, 3).\n\nhttps://en.wikipedia.org/wiki/CIE_1931_color_space\n\nRGB to stain color space conversion.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe stain separation matrix as described by G. Landini [1].\n\nThe image in stain color space. Same dimensions as input.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nStain separation matrices available in the `color` module and their respective\ncolorspace:\n\nThis implementation borrows some ideas from DIPlib [2], e.g. the compensation\nusing a small value to avoid log artifacts when calculating the Beer-Lambert\nlaw.\n\nhttps://web.archive.org/web/20160624145052/http://www.mecourse.com/landinig/software/cdeconv/cdeconv.html\n\nhttps://github.com/DIPlib/diplib/\n\nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by\ncolor deconvolution,\u201d Anal. Quant. Cytol. Histol., vol. 23, no. 4, pp.\n291\u2013299, Aug. 2001.\n\nXYZ to CIE-LAB color space conversion.\n\nThe image in XYZ format. Final dimension denotes channels.\n\nThe name of the illuminant (the function is NOT case sensitive).\n\nThe aperture angle of the observer.\n\nThe image in CIE-LAB format. Same dimensions as input.\n\nIf `xyz` is not at least 2-D with shape (\u2026, 3).\n\nIf either the illuminant or the observer angle is unsupported or unknown.\n\nBy default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values\nx_ref=95.047, y_ref=100., z_ref=108.883. See function `get_xyz_coords` for a\nlist of supported illuminants.\n\nhttp://www.easyrgb.com/index.php?X=MATH&H=07\n\nhttps://en.wikipedia.org/wiki/Lab_color_space\n\nXYZ to RGB color space conversion.\n\nThe image in XYZ format. Final dimension denotes channels.\n\nThe image in RGB format. Same dimensions as input.\n\nIf `xyz` is not at least 2-D with shape (\u2026, 3).\n\nThe CIE XYZ color space is derived from the CIE RGB color space. Note however\nthat this function converts to sRGB.\n\nhttps://en.wikipedia.org/wiki/CIE_1931_color_space\n\nYCbCr to RGB color space conversion.\n\nThe image in YCbCr format. Final dimension denotes channels.\n\nThe image in RGB format. Same dimensions as input.\n\nIf `ycbcr` is not at least 2-D with shape (\u2026, 3).\n\nY is between 16 and 235. This is the color space commonly used by video\ncodecs; it is sometimes incorrectly called \u201cYUV\u201d.\n\nhttps://en.wikipedia.org/wiki/YCbCr\n\nYDbDr to RGB color space conversion.\n\nThe image in YDbDr format. Final dimension denotes channels.\n\nThe image in RGB format. Same dimensions as input.\n\nIf `ydbdr` is not at least 2-D with shape (\u2026, 3).\n\nThis is the color space commonly used by video codecs, also called the\nreversible color transform in JPEG2000.\n\nhttps://en.wikipedia.org/wiki/YDbDr\n\nYIQ to RGB color space conversion.\n\nThe image in YIQ format. Final dimension denotes channels.\n\nThe image in RGB format. Same dimensions as input.\n\nIf `yiq` is not at least 2-D with shape (\u2026, 3).\n\nYPbPr to RGB color space conversion.\n\nThe image in YPbPr format. Final dimension denotes channels.\n\nThe image in RGB format. Same dimensions as input.\n\nIf `ypbpr` is not at least 2-D with shape (\u2026, 3).\n\nhttps://en.wikipedia.org/wiki/YPbPr\n\nYUV to RGB color space conversion.\n\nThe image in YUV format. Final dimension denotes channels.\n\nThe image in RGB format. Same dimensions as input.\n\nIf `yuv` is not at least 2-D with shape (\u2026, 3).\n\nhttps://en.wikipedia.org/wiki/YUV\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.combine_stains()", "path": "api/skimage.color#skimage.color.combine_stains", "type": "color", "text": "\nStain to RGB color space conversion.\n\nThe image in stain color space. Final dimension denotes channels.\n\nThe stain separation matrix as described by G. Landini [1].\n\nThe image in RGB format. Same dimensions as input.\n\nIf `stains` is not at least 2-D with shape (\u2026, 3).\n\nStain combination matrices available in the `color` module and their\nrespective colorspace:\n\nhttps://web.archive.org/web/20160624145052/http://www.mecourse.com/landinig/software/cdeconv/cdeconv.html\n\nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by\ncolor deconvolution,\u201d Anal. Quant. Cytol. Histol., vol. 23, no. 4, pp.\n291\u2013299, Aug. 2001.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.convert_colorspace()", "path": "api/skimage.color#skimage.color.convert_colorspace", "type": "color", "text": "\nConvert an image array to a new color space.\n\n\u2018RGB\u2019, \u2018HSV\u2019, \u2018RGB CIE\u2019, \u2018XYZ\u2019, \u2018YUV\u2019, \u2018YIQ\u2019, \u2018YPbPr\u2019, \u2018YCbCr\u2019, \u2018YDbDr\u2019\n\nThe image to convert. Final dimension denotes channels.\n\nThe color space to convert from. Can be specified in lower case.\n\nThe color space to convert to. Can be specified in lower case.\n\nThe converted image. Same dimensions as input.\n\nIf fromspace is not a valid color space\n\nIf tospace is not a valid color space\n\nConversion is performed through the \u201ccentral\u201d RGB color space, i.e. conversion\nfrom XYZ to HSV is implemented as `XYZ -> RGB -> HSV` instead of directly.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.deltaE_cie76()", "path": "api/skimage.color#skimage.color.deltaE_cie76", "type": "color", "text": "\nEuclidean distance between two points in Lab color space\n\nreference color (Lab colorspace)\n\ncomparison color (Lab colorspace)\n\ndistance between colors `lab1` and `lab2`\n\nhttps://en.wikipedia.org/wiki/Color_difference\n\nA. R. Robertson, \u201cThe CIE 1976 color-difference formulae,\u201d Color Res. Appl. 2,\n7-11 (1977).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.deltaE_ciede2000()", "path": "api/skimage.color#skimage.color.deltaE_ciede2000", "type": "color", "text": "\nColor difference as given by the CIEDE 2000 standard.\n\nCIEDE 2000 is a major revision of CIDE94. The perceptual calibration is\nlargely based on experience with automotive paint on smooth surfaces.\n\nreference color (Lab colorspace)\n\ncomparison color (Lab colorspace)\n\nlightness scale factor, 1 for \u201cacceptably close\u201d; 2 for \u201cimperceptible\u201d see\ndeltaE_cmc\n\nchroma scale factor, usually 1\n\nhue scale factor, usually 1\n\nThe distance between `lab1` and `lab2`\n\nCIEDE 2000 assumes parametric weighting factors for the lightness, chroma, and\nhue (`kL`, `kC`, `kH` respectively). These default to 1.\n\nhttps://en.wikipedia.org/wiki/Color_difference\n\nhttp://www.ece.rochester.edu/~gsharma/ciede2000/ciede2000noteCRNA.pdf\nDOI:10.1364/AO.33.008069\n\nM. Melgosa, J. Quesada, and E. Hita, \u201cUniformity of some recent color metrics\ntested with an accurate color-difference tolerance dataset,\u201d Appl. Opt. 33,\n8069-8077 (1994).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.deltaE_ciede94()", "path": "api/skimage.color#skimage.color.deltaE_ciede94", "type": "color", "text": "\nColor difference according to CIEDE 94 standard\n\nAccommodates perceptual non-uniformities through the use of application\nspecific scale factors (`kH`, `kC`, `kL`, `k1`, and `k2`).\n\nreference color (Lab colorspace)\n\ncomparison color (Lab colorspace)\n\nHue scale\n\nChroma scale\n\nLightness scale\n\nfirst scale parameter\n\nsecond scale parameter\n\ncolor difference between `lab1` and `lab2`\n\ndeltaE_ciede94 is not symmetric with respect to lab1 and lab2. CIEDE94 defines\nthe scales for the lightness, hue, and chroma in terms of the first color.\nConsequently, the first color should be regarded as the \u201creference\u201d color.\n\n`kL`, `k1`, `k2` depend on the application and default to the values suggested\nfor graphic arts\n\nParameter\n\nGraphic Arts\n\nTextiles\n\n`kL`\n\n1.000\n\n2.000\n\n`k1`\n\n0.045\n\n0.048\n\n`k2`\n\n0.015\n\n0.014\n\nhttps://en.wikipedia.org/wiki/Color_difference\n\nhttp://www.brucelindbloom.com/index.html?Eqn_DeltaE_CIE94.html\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.deltaE_cmc()", "path": "api/skimage.color#skimage.color.deltaE_cmc", "type": "color", "text": "\nColor difference from the CMC l:c standard.\n\nThis color difference was developed by the Colour Measurement Committee (CMC)\nof the Society of Dyers and Colourists (United Kingdom). It is intended for\nuse in the textile industry.\n\nThe scale factors `kL`, `kC` set the weight given to differences in lightness\nand chroma relative to differences in hue. The usual values are `kL=2`, `kC=1`\nfor \u201cacceptability\u201d and `kL=1`, `kC=1` for \u201cimperceptibility\u201d. Colors with `dE\n> 1` are \u201cdifferent\u201d for the given scale factors.\n\nreference color (Lab colorspace)\n\ncomparison color (Lab colorspace)\n\ndistance between colors `lab1` and `lab2`\n\ndeltaE_cmc the defines the scales for the lightness, hue, and chroma in terms\nof the first color. Consequently `deltaE_cmc(lab1, lab2) != deltaE_cmc(lab2,\nlab1)`\n\nhttps://en.wikipedia.org/wiki/Color_difference\n\nhttp://www.brucelindbloom.com/index.html?Eqn_DeltaE_CIE94.html\n\nF. J. J. Clarke, R. McDonald, and B. Rigg, \u201cModification to the JPC79 colour-\ndifference formula,\u201d J. Soc. Dyers Colour. 100, 128-132 (1984).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.gray2rgb()", "path": "api/skimage.color#skimage.color.gray2rgb", "type": "color", "text": "\nCreate an RGB representation of a gray-level image.\n\nInput image.\n\nEnsure that the output image has an alpha layer. If None, alpha layers are\npassed through but not created.\n\nRGB image. A new dimension of length 3 is added to input image.\n\nIf the input is a 1-dimensional image of shape `(M, )`, the output will be\nshape `(M, 3)`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.gray2rgba()", "path": "api/skimage.color#skimage.color.gray2rgba", "type": "color", "text": "\nCreate a RGBA representation of a gray-level image.\n\nInput image.\n\nAlpha channel of the output image. It may be a scalar or an array that can be\nbroadcast to `image`. If not specified it is set to the maximum limit\ncorresponding to the `image` dtype.\n\nRGBA image. A new dimension of length 4 is added to input image shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.grey2rgb()", "path": "api/skimage.color#skimage.color.grey2rgb", "type": "color", "text": "\nCreate an RGB representation of a gray-level image.\n\nInput image.\n\nEnsure that the output image has an alpha layer. If None, alpha layers are\npassed through but not created.\n\nRGB image. A new dimension of length 3 is added to input image.\n\nIf the input is a 1-dimensional image of shape `(M, )`, the output will be\nshape `(M, 3)`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.hed2rgb()", "path": "api/skimage.color#skimage.color.hed2rgb", "type": "color", "text": "\nHaematoxylin-Eosin-DAB (HED) to RGB color space conversion.\n\nThe image in the HED color space. Final dimension denotes channels.\n\nThe image in RGB. Same dimensions as input.\n\nIf `hed` is not at least 2-D with shape (\u2026, 3).\n\nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by\ncolor deconvolution.,\u201d Analytical and quantitative cytology and histology /\nthe International Academy of Cytology [and] American Society of Cytology, vol.\n23, no. 4, pp. 291-9, Aug. 2001.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.hsv2rgb()", "path": "api/skimage.color#skimage.color.hsv2rgb", "type": "color", "text": "\nHSV to RGB color space conversion.\n\nThe image in HSV format. Final dimension denotes channels.\n\nThe image in RGB format. Same dimensions as input.\n\nIf `hsv` is not at least 2-D with shape (\u2026, 3).\n\nConversion between RGB and HSV color spaces results in some loss of precision,\ndue to integer arithmetic and rounding [1].\n\nhttps://en.wikipedia.org/wiki/HSL_and_HSV\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.lab2lch()", "path": "api/skimage.color#skimage.color.lab2lch", "type": "color", "text": "\nCIE-LAB to CIE-LCH color space conversion.\n\nLCH is the cylindrical representation of the LAB (Cartesian) colorspace\n\nThe N-D image in CIE-LAB format. The last (`N+1`-th) dimension must have at\nleast 3 elements, corresponding to the `L`, `a`, and `b` color channels.\nSubsequent elements are copied.\n\nThe image in LCH format, in a N-D array with same shape as input `lab`.\n\nIf `lch` does not have at least 3 color channels (i.e. l, a, b).\n\nThe Hue is expressed as an angle between `(0, 2*pi)`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.lab2rgb()", "path": "api/skimage.color#skimage.color.lab2rgb", "type": "color", "text": "\nLab to RGB color space conversion.\n\nThe image in Lab format. Final dimension denotes channels.\n\nThe name of the illuminant (the function is NOT case sensitive).\n\nThe aperture angle of the observer.\n\nThe image in RGB format. Same dimensions as input.\n\nIf `lab` is not at least 2-D with shape (\u2026, 3).\n\nThis function uses lab2xyz and xyz2rgb. By default Observer= 2A, Illuminant=\nD65. CIE XYZ tristimulus values x_ref=95.047, y_ref=100., z_ref=108.883. See\nfunction `get_xyz_coords` for a list of supported illuminants.\n\nhttps://en.wikipedia.org/wiki/Standard_illuminant\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.lab2xyz()", "path": "api/skimage.color#skimage.color.lab2xyz", "type": "color", "text": "\nCIE-LAB to XYZcolor space conversion.\n\nThe image in Lab format. Final dimension denotes channels.\n\nThe name of the illuminant (the function is NOT case sensitive).\n\nThe aperture angle of the observer.\n\nThe image in XYZ format. Same dimensions as input.\n\nIf `lab` is not at least 2-D with shape (\u2026, 3).\n\nIf either the illuminant or the observer angle are not supported or unknown.\n\nIf any of the pixels are invalid (Z < 0).\n\nBy default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref =\n95.047, y_ref = 100., z_ref = 108.883. See function \u2018get_xyz_coords\u2019 for a\nlist of supported illuminants.\n\nhttp://www.easyrgb.com/index.php?X=MATH&H=07\n\nhttps://en.wikipedia.org/wiki/Lab_color_space\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.label2rgb()", "path": "api/skimage.color#skimage.color.label2rgb", "type": "color", "text": "\nReturn an RGB image where color-coded labels are painted over the image.\n\nInteger array of labels with the same shape as `image`.\n\nImage used as underlay for labels. If the input is an RGB image, it\u2019s\nconverted to grayscale before coloring.\n\nList of colors. If the number of labels exceeds the number of colors, then the\ncolors are cycled.\n\nOpacity of colorized labels. Ignored if image is `None`.\n\nLabel that\u2019s treated as the background. If `bg_label` is specified, `bg_color`\nis `None`, and `kind` is `overlay`, background is not painted by any colors.\n\nBackground color. Must be a name in `color_dict` or RGB float values between\n[0, 1].\n\nOpacity of the image.\n\nThe kind of color image desired. \u2018overlay\u2019 cycles over defined colors and\noverlays the colored labels over the original image. \u2018avg\u2019 replaces each\nlabeled segment with its average color, for a stained-class or pastel painting\nappearance.\n\nThe result of blending a cycling colormap (`colors`) for each distinct value\nin `label` with the image, at a certain alpha value.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.lch2lab()", "path": "api/skimage.color#skimage.color.lch2lab", "type": "color", "text": "\nCIE-LCH to CIE-LAB color space conversion.\n\nLCH is the cylindrical representation of the LAB (Cartesian) colorspace\n\nThe N-D image in CIE-LCH format. The last (`N+1`-th) dimension must have at\nleast 3 elements, corresponding to the `L`, `a`, and `b` color channels.\nSubsequent elements are copied.\n\nThe image in LAB format, with same shape as input `lch`.\n\nIf `lch` does not have at least 3 color channels (i.e. l, c, h).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.rgb2gray()", "path": "api/skimage.color#skimage.color.rgb2gray", "type": "color", "text": "\nCompute luminance of an RGB image.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe luminance image - an array which is the same size as the input array, but\nwith the channel dimension removed.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nThe weights used in this conversion are calibrated for contemporary CRT\nphosphors:\n\nIf there is an alpha channel present, it is ignored.\n\nhttp://poynton.ca/PDFs/ColorFAQ.pdf\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.rgb2grey()", "path": "api/skimage.color#skimage.color.rgb2grey", "type": "color", "text": "\nCompute luminance of an RGB image.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe luminance image - an array which is the same size as the input array, but\nwith the channel dimension removed.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nThe weights used in this conversion are calibrated for contemporary CRT\nphosphors:\n\nIf there is an alpha channel present, it is ignored.\n\nhttp://poynton.ca/PDFs/ColorFAQ.pdf\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.rgb2hed()", "path": "api/skimage.color#skimage.color.rgb2hed", "type": "color", "text": "\nRGB to Haematoxylin-Eosin-DAB (HED) color space conversion.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe image in HED format. Same dimensions as input.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by\ncolor deconvolution.,\u201d Analytical and quantitative cytology and histology /\nthe International Academy of Cytology [and] American Society of Cytology, vol.\n23, no. 4, pp. 291-9, Aug. 2001.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.rgb2hsv()", "path": "api/skimage.color#skimage.color.rgb2hsv", "type": "color", "text": "\nRGB to HSV color space conversion.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe image in HSV format. Same dimensions as input.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nConversion between RGB and HSV color spaces results in some loss of precision,\ndue to integer arithmetic and rounding [1].\n\nhttps://en.wikipedia.org/wiki/HSL_and_HSV\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.rgb2lab()", "path": "api/skimage.color#skimage.color.rgb2lab", "type": "color", "text": "\nConversion from the sRGB color space (IEC 61966-2-1:1999) to the CIE Lab\ncolorspace under the given illuminant and observer.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe name of the illuminant (the function is NOT case sensitive).\n\nThe aperture angle of the observer.\n\nThe image in Lab format. Same dimensions as input.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nRGB is a device-dependent color space so, if you use this function, be sure\nthat the image you are analyzing has been mapped to the sRGB color space.\n\nThis function uses rgb2xyz and xyz2lab. By default Observer= 2A, Illuminant=\nD65. CIE XYZ tristimulus values x_ref=95.047, y_ref=100., z_ref=108.883. See\nfunction `get_xyz_coords` for a list of supported illuminants.\n\nhttps://en.wikipedia.org/wiki/Standard_illuminant\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.rgb2rgbcie()", "path": "api/skimage.color#skimage.color.rgb2rgbcie", "type": "color", "text": "\nRGB to RGB CIE color space conversion.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe image in RGB CIE format. Same dimensions as input.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nhttps://en.wikipedia.org/wiki/CIE_1931_color_space\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.rgb2xyz()", "path": "api/skimage.color#skimage.color.rgb2xyz", "type": "color", "text": "\nRGB to XYZ color space conversion.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe image in XYZ format. Same dimensions as input.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nThe CIE XYZ color space is derived from the CIE RGB color space. Note however\nthat this function converts from sRGB.\n\nhttps://en.wikipedia.org/wiki/CIE_1931_color_space\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.rgb2ycbcr()", "path": "api/skimage.color#skimage.color.rgb2ycbcr", "type": "color", "text": "\nRGB to YCbCr color space conversion.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe image in YCbCr format. Same dimensions as input.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nY is between 16 and 235. This is the color space commonly used by video\ncodecs; it is sometimes incorrectly called \u201cYUV\u201d.\n\nhttps://en.wikipedia.org/wiki/YCbCr\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.rgb2ydbdr()", "path": "api/skimage.color#skimage.color.rgb2ydbdr", "type": "color", "text": "\nRGB to YDbDr color space conversion.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe image in YDbDr format. Same dimensions as input.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nThis is the color space commonly used by video codecs. It is also the\nreversible color transform in JPEG2000.\n\nhttps://en.wikipedia.org/wiki/YDbDr\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.rgb2yiq()", "path": "api/skimage.color#skimage.color.rgb2yiq", "type": "color", "text": "\nRGB to YIQ color space conversion.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe image in YIQ format. Same dimensions as input.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.rgb2ypbpr()", "path": "api/skimage.color#skimage.color.rgb2ypbpr", "type": "color", "text": "\nRGB to YPbPr color space conversion.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe image in YPbPr format. Same dimensions as input.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nhttps://en.wikipedia.org/wiki/YPbPr\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.rgb2yuv()", "path": "api/skimage.color#skimage.color.rgb2yuv", "type": "color", "text": "\nRGB to YUV color space conversion.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe image in YUV format. Same dimensions as input.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nY is between 0 and 1. Use YCbCr instead of YUV for the color space commonly\nused by video codecs, where Y ranges from 16 to 235.\n\nhttps://en.wikipedia.org/wiki/YUV\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.rgba2rgb()", "path": "api/skimage.color#skimage.color.rgba2rgb", "type": "color", "text": "\nRGBA to RGB conversion using alpha blending [1].\n\nThe image in RGBA format. Final dimension denotes channels.\n\nThe color of the background to blend the image with (3 floats between 0 to 1 -\nthe RGB value of the background).\n\nThe image in RGB format. Same dimensions as input.\n\nIf `rgba` is not at least 2-D with shape (\u2026, 4).\n\nhttps://en.wikipedia.org/wiki/Alpha_compositing#Alpha_blending\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.rgbcie2rgb()", "path": "api/skimage.color#skimage.color.rgbcie2rgb", "type": "color", "text": "\nRGB CIE to RGB color space conversion.\n\nThe image in RGB CIE format. Final dimension denotes channels.\n\nThe image in RGB format. Same dimensions as input.\n\nIf `rgbcie` is not at least 2-D with shape (\u2026, 3).\n\nhttps://en.wikipedia.org/wiki/CIE_1931_color_space\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.separate_stains()", "path": "api/skimage.color#skimage.color.separate_stains", "type": "color", "text": "\nRGB to stain color space conversion.\n\nThe image in RGB format. Final dimension denotes channels.\n\nThe stain separation matrix as described by G. Landini [1].\n\nThe image in stain color space. Same dimensions as input.\n\nIf `rgb` is not at least 2-D with shape (\u2026, 3).\n\nStain separation matrices available in the `color` module and their respective\ncolorspace:\n\nThis implementation borrows some ideas from DIPlib [2], e.g. the compensation\nusing a small value to avoid log artifacts when calculating the Beer-Lambert\nlaw.\n\nhttps://web.archive.org/web/20160624145052/http://www.mecourse.com/landinig/software/cdeconv/cdeconv.html\n\nhttps://github.com/DIPlib/diplib/\n\nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by\ncolor deconvolution,\u201d Anal. Quant. Cytol. Histol., vol. 23, no. 4, pp.\n291\u2013299, Aug. 2001.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.xyz2lab()", "path": "api/skimage.color#skimage.color.xyz2lab", "type": "color", "text": "\nXYZ to CIE-LAB color space conversion.\n\nThe image in XYZ format. Final dimension denotes channels.\n\nThe name of the illuminant (the function is NOT case sensitive).\n\nThe aperture angle of the observer.\n\nThe image in CIE-LAB format. Same dimensions as input.\n\nIf `xyz` is not at least 2-D with shape (\u2026, 3).\n\nIf either the illuminant or the observer angle is unsupported or unknown.\n\nBy default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values\nx_ref=95.047, y_ref=100., z_ref=108.883. See function `get_xyz_coords` for a\nlist of supported illuminants.\n\nhttp://www.easyrgb.com/index.php?X=MATH&H=07\n\nhttps://en.wikipedia.org/wiki/Lab_color_space\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.xyz2rgb()", "path": "api/skimage.color#skimage.color.xyz2rgb", "type": "color", "text": "\nXYZ to RGB color space conversion.\n\nThe image in XYZ format. Final dimension denotes channels.\n\nThe image in RGB format. Same dimensions as input.\n\nIf `xyz` is not at least 2-D with shape (\u2026, 3).\n\nThe CIE XYZ color space is derived from the CIE RGB color space. Note however\nthat this function converts to sRGB.\n\nhttps://en.wikipedia.org/wiki/CIE_1931_color_space\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.ycbcr2rgb()", "path": "api/skimage.color#skimage.color.ycbcr2rgb", "type": "color", "text": "\nYCbCr to RGB color space conversion.\n\nThe image in YCbCr format. Final dimension denotes channels.\n\nThe image in RGB format. Same dimensions as input.\n\nIf `ycbcr` is not at least 2-D with shape (\u2026, 3).\n\nY is between 16 and 235. This is the color space commonly used by video\ncodecs; it is sometimes incorrectly called \u201cYUV\u201d.\n\nhttps://en.wikipedia.org/wiki/YCbCr\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.ydbdr2rgb()", "path": "api/skimage.color#skimage.color.ydbdr2rgb", "type": "color", "text": "\nYDbDr to RGB color space conversion.\n\nThe image in YDbDr format. Final dimension denotes channels.\n\nThe image in RGB format. Same dimensions as input.\n\nIf `ydbdr` is not at least 2-D with shape (\u2026, 3).\n\nThis is the color space commonly used by video codecs, also called the\nreversible color transform in JPEG2000.\n\nhttps://en.wikipedia.org/wiki/YDbDr\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.yiq2rgb()", "path": "api/skimage.color#skimage.color.yiq2rgb", "type": "color", "text": "\nYIQ to RGB color space conversion.\n\nThe image in YIQ format. Final dimension denotes channels.\n\nThe image in RGB format. Same dimensions as input.\n\nIf `yiq` is not at least 2-D with shape (\u2026, 3).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.ypbpr2rgb()", "path": "api/skimage.color#skimage.color.ypbpr2rgb", "type": "color", "text": "\nYPbPr to RGB color space conversion.\n\nThe image in YPbPr format. Final dimension denotes channels.\n\nThe image in RGB format. Same dimensions as input.\n\nIf `ypbpr` is not at least 2-D with shape (\u2026, 3).\n\nhttps://en.wikipedia.org/wiki/YPbPr\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "color.yuv2rgb()", "path": "api/skimage.color#skimage.color.yuv2rgb", "type": "color", "text": "\nYUV to RGB color space conversion.\n\nThe image in YUV format. Final dimension denotes channels.\n\nThe image in RGB format. Same dimensions as input.\n\nIf `yuv` is not at least 2-D with shape (\u2026, 3).\n\nhttps://en.wikipedia.org/wiki/YUV\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data", "path": "api/skimage.data", "type": "data", "text": "\nStandard test images.\n\nFor more images, see\n\n`skimage.data.astronaut`()\n\nColor image of the astronaut Eileen Collins.\n\n`skimage.data.binary_blobs`([length, \u2026])\n\nGenerate synthetic binary image with several rounded blob-like objects.\n\n`skimage.data.brain`()\n\nSubset of data from the University of North Carolina Volume Rendering Test\nData Set.\n\n`skimage.data.brick`()\n\nBrick wall.\n\n`skimage.data.camera`()\n\nGray-level \u201ccamera\u201d image.\n\n`skimage.data.cat`()\n\nChelsea the cat.\n\n`skimage.data.cell`()\n\nCell floating in saline.\n\n`skimage.data.cells3d`()\n\n3D fluorescence microscopy image of cells.\n\n`skimage.data.checkerboard`()\n\nCheckerboard image.\n\n`skimage.data.chelsea`()\n\nChelsea the cat.\n\n`skimage.data.clock`()\n\nMotion blurred clock.\n\n`skimage.data.coffee`()\n\nCoffee cup.\n\n`skimage.data.coins`()\n\nGreek coins from Pompeii.\n\n`skimage.data.colorwheel`()\n\nColor Wheel.\n\n`skimage.data.download_all`([directory])\n\nDownload all datasets for use with scikit-image offline.\n\n`skimage.data.eagle`()\n\nA golden eagle.\n\n`skimage.data.grass`()\n\nGrass.\n\n`skimage.data.gravel`()\n\nGravel\n\n`skimage.data.horse`()\n\nBlack and white silhouette of a horse.\n\n`skimage.data.hubble_deep_field`()\n\nHubble eXtreme Deep Field.\n\n`skimage.data.human_mitosis`()\n\nImage of human cells undergoing mitosis.\n\n`skimage.data.immunohistochemistry`()\n\nImmunohistochemical (IHC) staining with hematoxylin counterstaining.\n\n`skimage.data.kidney`()\n\nMouse kidney tissue.\n\n`skimage.data.lbp_frontal_face_cascade_filename`()\n\nReturn the path to the XML file containing the weak classifier cascade.\n\n`skimage.data.lfw_subset`()\n\nSubset of data from the LFW dataset.\n\n`skimage.data.lily`()\n\nLily of the valley plant stem.\n\n`skimage.data.logo`()\n\nScikit-image logo, a RGBA image.\n\n`skimage.data.microaneurysms`()\n\nGray-level \u201cmicroaneurysms\u201d image.\n\n`skimage.data.moon`()\n\nSurface of the moon.\n\n`skimage.data.page`()\n\nScanned page.\n\n`skimage.data.retina`()\n\nHuman retina.\n\n`skimage.data.rocket`()\n\nLaunch photo of DSCOVR on Falcon 9 by SpaceX.\n\n`skimage.data.shepp_logan_phantom`()\n\nShepp Logan Phantom.\n\n`skimage.data.skin`()\n\nMicroscopy image of dermis and epidermis (skin layers).\n\n`skimage.data.stereo_motorcycle`()\n\nRectified stereo image pair with ground-truth disparities.\n\n`skimage.data.text`()\n\nGray-level \u201ctext\u201d image used for corner detection.\n\nColor image of the astronaut Eileen Collins.\n\nPhotograph of Eileen Collins, an American astronaut. She was selected as an\nastronaut in 1992 and first piloted the space shuttle STS-63 in 1995. She\nretired in 2006 after spending a total of 38 days, 8 hours and 10 minutes in\nouter space.\n\nThis image was downloaded from the NASA Great Images database\n<https://flic.kr/p/r9qvLn>`__.\n\nNo known copyright restrictions, released into the public domain.\n\nAstronaut image.\n\nFlood Fill\n\nGenerate synthetic binary image with several rounded blob-like objects.\n\nLinear size of output image.\n\nTypical linear size of blob, as a fraction of `length`, should be smaller than\n1.\n\nNumber of dimensions of output image.\n\nFraction of image pixels covered by the blobs (where the output is 1). Should\nbe in [0, 1].\n\nSeed to initialize the random number generator. If `None`, a random seed from\nthe operating system is used.\n\nOutput binary image\n\nSubset of data from the University of North Carolina Volume Rendering Test\nData Set.\n\nThe full dataset is available at [1].\n\nThe 3D volume consists of 10 layers from the larger volume.\n\nhttps://graphics.stanford.edu/data/voldata/\n\nLocal Histogram Equalization\n\nRank filters\n\nBrick wall.\n\nA small section of a brick wall.\n\nThe original image was downloaded from CC0Textures and licensed under the\nCreative Commons CC0 License.\n\nA perspective transform was then applied to the image, prior to rotating it by\n90 degrees, cropping and scaling it to obtain the final image.\n\nGray-level \u201ccamera\u201d image.\n\nCan be used for segmentation and denoising examples.\n\nCamera image.\n\nNo copyright restrictions. CC0 by the photographer (Lav Varshney).\n\nChanged in version 0.18: This image was replaced due to copyright\nrestrictions. For more information, please see [1].\n\nhttps://github.com/scikit-image/scikit-image/issues/3927\n\nTinting gray-scale images\n\nMasked Normalized Cross-Correlation\n\nEntropy\n\nGLCM Texture Features\n\nMulti-Otsu Thresholding\n\nFlood Fill\n\nRank filters\n\nChelsea the cat.\n\nAn example with texture, prominent edges in horizontal and diagonal\ndirections, as well as features of differing scales.\n\nChelsea image.\n\nNo copyright restrictions. CC0 by the photographer (Stefan van der Walt).\n\nCell floating in saline.\n\nThis is a quantitative phase image retrieved from a digital hologram using the\nPython library `qpformat`. The image shows a cell with high phase value, above\nthe background phase.\n\nBecause of a banding pattern artifact in the background, this image is a good\ntest of thresholding algorithms. The pixel spacing is 0.107 \u00b5m.\n\nThese data were part of a comparison between several refractive index\nretrieval techniques for spherical objects as part of [1].\n\nThis image is CC0, dedicated to the public domain. You may copy, modify, or\ndistribute it without asking permission.\n\nImage of a cell.\n\nPaul M\u00fcller, Mirjam Sch\u00fcrmann, Salvatore Girardo, Gheorghe Cojoc, and Jochen\nGuck. \u201cAccurate evaluation of size and refractive index for spherical objects\nin quantitative phase imaging.\u201d Optics Express 26(8): 10729-10743 (2018).\nDOI:10.1364/OE.26.010729\n\n3D fluorescence microscopy image of cells.\n\nThe returned data is a 3D multichannel array with dimensions provided in `(z,\nc, y, x)` order. Each voxel has a size of `(0.29 0.26 0.26)` micrometer.\nChannel 0 contains cell membranes, channel 1 contains nuclei.\n\nThe volumetric images of cells taken with an optical microscope.\n\nThe data for this was provided by the Allen Institute for Cell Science.\n\nIt has been downsampled by a factor of 4 in the row and column dimensions to\nreduce computational time.\n\nThe microscope reports the following voxel spacing in microns:\n\n3D adaptive histogram equalization\n\nUse rolling-ball algorithm for estimating background intensity\n\nExplore 3D images (of cells)\n\nCheckerboard image.\n\nCheckerboards are often used in image calibration, since the corner-points are\neasy to locate. Because of the many parallel edges, they also visualise\ndistortions particularly well.\n\nCheckerboard image.\n\nFlood Fill\n\nChelsea the cat.\n\nAn example with texture, prominent edges in horizontal and diagonal\ndirections, as well as features of differing scales.\n\nChelsea image.\n\nNo copyright restrictions. CC0 by the photographer (Stefan van der Walt).\n\nPhase Unwrapping\n\nFlood Fill\n\nMotion blurred clock.\n\nThis photograph of a wall clock was taken while moving the camera in an\naproximately horizontal direction. It may be used to illustrate inverse\nfilters and deconvolution.\n\nReleased into the public domain by the photographer (Stefan van der Walt).\n\nClock image.\n\nCoffee cup.\n\nThis photograph is courtesy of Pikolo Espresso Bar. It contains several\nelliptical shapes as well as varying texture (smooth porcelain to course wood\ngrain).\n\nCoffee image.\n\nNo copyright restrictions. CC0 by the photographer (Rachel Michetti).\n\nGreek coins from Pompeii.\n\nThis image shows several coins outlined against a gray background. It is\nespecially useful in, e.g. segmentation tests, where individual objects need\nto be identified against a background. The background shares enough grey\nlevels with the coins that a simple segmentation is not sufficient.\n\nCoins image.\n\nThis image was downloaded from the Brooklyn Museum Collection.\n\nNo known copyright restrictions.\n\nFinding local maxima\n\nMeasure region properties\n\nUse rolling-ball algorithm for estimating background intensity\n\nColor Wheel.\n\nA colorwheel.\n\nDownload all datasets for use with scikit-image offline.\n\nScikit-image datasets are no longer shipped with the library by default. This\nallows us to use higher quality datasets, while keeping the library download\nsize small.\n\nThis function requires the installation of an optional dependency, pooch, to\ndownload the full dataset. Follow installation instruction found at\n\nhttps://scikit-image.org/docs/stable/install.html\n\nCall this function to download all sample images making them available offline\non your machine.\n\nThe directory where the dataset should be stored.\n\nIf pooch is not install, this error will be raised.\n\nscikit-image will only search for images stored in the default directory. Only\nspecify the directory if you wish to download the images to your own folder\nfor a particular reason. You can access the location of the default data\ndirectory by inspecting the variable `skimage.data.data_dir`.\n\nA golden eagle.\n\nSuitable for examples on segmentation, Hough transforms, and corner detection.\n\nEagle image.\n\nNo copyright restrictions. CC0 by the photographer (Dayane Machado).\n\nMarkers for watershed transform\n\nGrass.\n\nSome grass.\n\nThe original image was downloaded from DeviantArt and licensed underthe\nCreative Commons CC0 License.\n\nThe downloaded image was cropped to include a region of `(512, 512)` pixels\naround the top left corner, converted to grayscale, then to uint8 prior to\nsaving the result in PNG format.\n\nGravel\n\nGrayscale gravel sample.\n\nThe original image was downloaded from CC0Textures and licensed under the\nCreative Commons CC0 License.\n\nThe downloaded image was then rescaled to `(1024, 1024)`, then the top left\n`(512, 512)` pixel region was cropped prior to converting the image to\ngrayscale and uint8 data type. The result was saved using the PNG format.\n\nBlack and white silhouette of a horse.\n\nThis image was downloaded from `openclipart`\n\nNo copyright restrictions. CC0 given by owner (Andreas Preuss (marauder)).\n\nHorse image.\n\nHubble eXtreme Deep Field.\n\nThis photograph contains the Hubble Telescope\u2019s farthest ever view of the\nuniverse. It can be useful as an example for multi-scale detection.\n\nHubble deep field image.\n\nThis image was downloaded from HubbleSite.\n\nThe image was captured by NASA and may be freely used in the public domain.\n\nImage of human cells undergoing mitosis.\n\nData of human cells undergoing mitosis taken during the preperation of the\nmanuscript in [1].\n\nCopyright David Root. Licensed under CC-0 [2].\n\nMoffat J, Grueneberg DA, Yang X, Kim SY, Kloepfer AM, Hinkle G, Piqani B,\nEisenhaure TM, Luo B, Grenier JK, Carpenter AE, Foo SY, Stewart SA, Stockwell\nBR, Hacohen N, Hahn WC, Lander ES, Sabatini DM, Root DE (2006) A lentiviral\nRNAi library for human and mouse genes applied to an arrayed viral high-\ncontent screen. Cell, 124(6):1283-98 / :DOI: `10.1016/j.cell.2006.01.040` PMID\n16564017\n\nGitHub licensing discussion https://github.com/CellProfiler/examples/issues/41\n\nSegment human cells (in mitosis)\n\nImmunohistochemical (IHC) staining with hematoxylin counterstaining.\n\nThis picture shows colonic glands where the IHC expression of FHL2 protein is\nrevealed with DAB. Hematoxylin counterstaining is applied to enhance the\nnegative parts of the tissue.\n\nThis image was acquired at the Center for Microscopy And Molecular Imaging\n(CMMI).\n\nNo known copyright restrictions.\n\nImmunohistochemistry image.\n\nMouse kidney tissue.\n\nThis biological tissue on a pre-prepared slide was imaged with confocal\nfluorescence microscopy (Nikon C1 inverted microscope). Image shape is (16,\n512, 512, 3). That is 512x512 pixels in X-Y, 16 image slices in Z, and 3 color\nchannels (emission wavelengths 450nm, 515nm, and 605nm, respectively). Real-\nspace voxel size is 1.24 microns in X-Y, and 1.25 microns in Z. Data type is\nunsigned 16-bit integers.\n\nKidney 3D multichannel image.\n\nThis image was acquired by Genevieve Buckley at Monasoh Micro Imaging in 2018.\nLicense: CC0\n\nReturn the path to the XML file containing the weak classifier cascade.\n\nThese classifiers were trained using LBP features. The file is part of the\nOpenCV repository [1].\n\nOpenCV lbpcascade trained files\nhttps://github.com/opencv/opencv/tree/master/data/lbpcascades\n\nSubset of data from the LFW dataset.\n\nThis database is a subset of the LFW database containing:\n\nThe full dataset is available at [2].\n\n100 first images are faces and subsequent 100 are non-faces.\n\nThe faces were randomly selected from the LFW dataset and the non-faces were\nextracted from the background of the same dataset. The cropped ROIs have been\nresized to a 25 x 25 pixels.\n\nHuang, G., Mattar, M., Lee, H., & Learned-Miller, E. G. (2012). Learning to\nalign from scratch. In Advances in Neural Information Processing Systems (pp.\n764-772).\n\nhttp://vis-www.cs.umass.edu/lfw/\n\nSpecific images\n\nLily of the valley plant stem.\n\nThis plant stem on a pre-prepared slide was imaged with confocal fluorescence\nmicroscopy (Nikon C1 inverted microscope). Image shape is (922, 922, 4). That\nis 922x922 pixels in X-Y, with 4 color channels. Real-space voxel size is 1.24\nmicrons in X-Y. Data type is unsigned 16-bit integers.\n\nLily 2D multichannel image.\n\nThis image was acquired by Genevieve Buckley at Monasoh Micro Imaging in 2018.\nLicense: CC0\n\nScikit-image logo, a RGBA image.\n\nLogo image.\n\nGray-level \u201cmicroaneurysms\u201d image.\n\nDetail from an image of the retina (green channel). The image is a crop of\nimage 07_dr.JPG from the High-Resolution Fundus (HRF) Image Database:\nhttps://www5.cs.fau.de/research/data/fundus-images/\n\nRetina image with lesions.\n\nNo copyright restrictions. CC0 given by owner (Andreas Maier).\n\nBudai, A., Bock, R, Maier, A., Hornegger, J., Michelson, G. (2013). Robust\nVessel Segmentation in Fundus Images. International Journal of Biomedical\nImaging, vol. 2013, 2013. DOI:10.1155/2013/154860\n\nSurface of the moon.\n\nThis low-contrast image of the surface of the moon is useful for illustrating\nhistogram equalization and contrast stretching.\n\nMoon image.\n\nLocal Histogram Equalization\n\nScanned page.\n\nThis image of printed text is useful for demonstrations requiring uneven\nbackground illumination.\n\nPage image.\n\nUse rolling-ball algorithm for estimating background intensity\n\nRank filters\n\nHuman retina.\n\nThis image of a retina is useful for demonstrations requiring circular images.\n\nRetina image in RGB.\n\nThis image was downloaded from `wikimedia`. This file is made available under\nthe Creative Commons CC0 1.0 Universal Public Domain Dedication.\n\nH\u00e4ggstr\u00f6m, Mikael (2014). \u201cMedical gallery of Mikael H\u00e4ggstr\u00f6m 2014\u201d.\nWikiJournal of Medicine 1 (2). DOI:10.15347/wjm/2014.008. ISSN 2002-4436.\nPublic Domain\n\nLaunch photo of DSCOVR on Falcon 9 by SpaceX.\n\nThis is the launch photo of Falcon 9 carrying DSCOVR lifted off from SpaceX\u2019s\nLaunch Complex 40 at Cape Canaveral Air Force Station, FL.\n\nRocket image.\n\nThis image was downloaded from SpaceX Photos.\n\nThe image was captured by SpaceX and released in the public domain.\n\nShepp Logan Phantom.\n\nImage of the Shepp-Logan phantom in grayscale.\n\nL. A. Shepp and B. F. Logan, \u201cThe Fourier reconstruction of a head section,\u201d\nin IEEE Transactions on Nuclear Science, vol. 21, no. 3, pp. 21-43, June 1974.\nDOI:10.1109/TNS.1974.6499235\n\nMicroscopy image of dermis and epidermis (skin layers).\n\nHematoxylin and eosin stained slide at 10x of normal epidermis and dermis with\na benign intradermal nevus.\n\nThis image requires an Internet connection the first time it is called, and to\nhave the `pooch` package installed, in order to fetch the image file from the\nscikit-image datasets repository.\n\nThe source of this image is\nhttps://en.wikipedia.org/wiki/File:Normal_Epidermis_and_Dermis_with_Intradermal_Nevus_10x.JPG\n\nThe image was released in the public domain by its author Kilbad.\n\nTrainable segmentation using local features and random forests\n\nRectified stereo image pair with ground-truth disparities.\n\nThe two images are rectified such that every pixel in the left image has its\ncorresponding pixel on the same scanline in the right image. That means that\nboth images are warped such that they have the same orientation but a\nhorizontal spatial offset (baseline). The ground-truth pixel offset in column\ndirection is specified by the included disparity map.\n\nThe two images are part of the Middlebury 2014 stereo benchmark. The dataset\nwas created by Nera Nesic, Porter Westling, Xi Wang, York Kitajima, Greg\nKrathwohl, and Daniel Scharstein at Middlebury College. A detailed description\nof the acquisition process can be found in [1].\n\nThe images included here are down-sampled versions of the default exposure\nimages in the benchmark. The images are down-sampled by a factor of 4 using\nthe function `skimage.transform.downscale_local_mean`. The calibration data in\nthe following and the included ground-truth disparity map are valid for the\ndown-sampled images:\n\nLeft stereo image.\n\nRight stereo image.\n\nGround-truth disparity map, where each value describes the offset in column\ndirection between corresponding pixels in the left and the right stereo\nimages. E.g. the corresponding pixel of `img_left[10, 10 + disp[10, 10]]` is\n`img_right[10, 10]`. NaNs denote pixels in the left image that do not have\nground-truth.\n\nThe original resolution images, images with different exposure and lighting,\nand ground-truth depth maps can be found at the Middlebury website [2].\n\nD. Scharstein, H. Hirschmueller, Y. Kitajima, G. Krathwohl, N. Nesic, X. Wang,\nand P. Westling. High-resolution stereo datasets with subpixel-accurate ground\ntruth. In German Conference on Pattern Recognition (GCPR 2014), Muenster,\nGermany, September 2014.\n\nhttp://vision.middlebury.edu/stereo/data/scenes2014/\n\nSpecific images\n\nRegistration using optical flow\n\nGray-level \u201ctext\u201d image used for corner detection.\n\nText image.\n\nThis image was downloaded from Wikipedia\n<https://en.wikipedia.org/wiki/File:Corner.png>`__.\n\nNo known copyright restrictions, released into the public domain.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "Data visualization", "path": "user_guide/visualization", "type": "Guide", "text": "\nData visualization takes an important place in image processing. Data can be a\nsimple unique 2D image or a more complex with multidimensional aspects: 3D in\nspace, timeslapse, multiple channels.\n\nTherefore, the visualization strategy will depend on the data complexity and a\nrange of tools external to scikit-image can be used for this purpose.\nHistorically, scikit-image provided viewer tools but powerful packages are now\navailable and must be preferred.\n\nMatplotlib is a library able to generate static plots, which includes image\nvisualization.\n\nPlotly is a plotting library relying on web technologies with interaction\ncapabilities.\n\nMayavi can be used to visualize 3D images.\n\nNapari is a multi-dimensional image viewer. It\u2019s designed for browsing,\nannotating, and analyzing large multi-dimensional images.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.astronaut()", "path": "api/skimage.data#skimage.data.astronaut", "type": "data", "text": "\nColor image of the astronaut Eileen Collins.\n\nPhotograph of Eileen Collins, an American astronaut. She was selected as an\nastronaut in 1992 and first piloted the space shuttle STS-63 in 1995. She\nretired in 2006 after spending a total of 38 days, 8 hours and 10 minutes in\nouter space.\n\nThis image was downloaded from the NASA Great Images database\n<https://flic.kr/p/r9qvLn>`__.\n\nNo known copyright restrictions, released into the public domain.\n\nAstronaut image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.binary_blobs()", "path": "api/skimage.data#skimage.data.binary_blobs", "type": "data", "text": "\nGenerate synthetic binary image with several rounded blob-like objects.\n\nLinear size of output image.\n\nTypical linear size of blob, as a fraction of `length`, should be smaller than\n1.\n\nNumber of dimensions of output image.\n\nFraction of image pixels covered by the blobs (where the output is 1). Should\nbe in [0, 1].\n\nSeed to initialize the random number generator. If `None`, a random seed from\nthe operating system is used.\n\nOutput binary image\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.brain()", "path": "api/skimage.data#skimage.data.brain", "type": "data", "text": "\nSubset of data from the University of North Carolina Volume Rendering Test\nData Set.\n\nThe full dataset is available at [1].\n\nThe 3D volume consists of 10 layers from the larger volume.\n\nhttps://graphics.stanford.edu/data/voldata/\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.brick()", "path": "api/skimage.data#skimage.data.brick", "type": "data", "text": "\nBrick wall.\n\nA small section of a brick wall.\n\nThe original image was downloaded from CC0Textures and licensed under the\nCreative Commons CC0 License.\n\nA perspective transform was then applied to the image, prior to rotating it by\n90 degrees, cropping and scaling it to obtain the final image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.camera()", "path": "api/skimage.data#skimage.data.camera", "type": "data", "text": "\nGray-level \u201ccamera\u201d image.\n\nCan be used for segmentation and denoising examples.\n\nCamera image.\n\nNo copyright restrictions. CC0 by the photographer (Lav Varshney).\n\nChanged in version 0.18: This image was replaced due to copyright\nrestrictions. For more information, please see [1].\n\nhttps://github.com/scikit-image/scikit-image/issues/3927\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.cat()", "path": "api/skimage.data#skimage.data.cat", "type": "data", "text": "\nChelsea the cat.\n\nAn example with texture, prominent edges in horizontal and diagonal\ndirections, as well as features of differing scales.\n\nChelsea image.\n\nNo copyright restrictions. CC0 by the photographer (Stefan van der Walt).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.cell()", "path": "api/skimage.data#skimage.data.cell", "type": "data", "text": "\nCell floating in saline.\n\nThis is a quantitative phase image retrieved from a digital hologram using the\nPython library `qpformat`. The image shows a cell with high phase value, above\nthe background phase.\n\nBecause of a banding pattern artifact in the background, this image is a good\ntest of thresholding algorithms. The pixel spacing is 0.107 \u00b5m.\n\nThese data were part of a comparison between several refractive index\nretrieval techniques for spherical objects as part of [1].\n\nThis image is CC0, dedicated to the public domain. You may copy, modify, or\ndistribute it without asking permission.\n\nImage of a cell.\n\nPaul M\u00fcller, Mirjam Sch\u00fcrmann, Salvatore Girardo, Gheorghe Cojoc, and Jochen\nGuck. \u201cAccurate evaluation of size and refractive index for spherical objects\nin quantitative phase imaging.\u201d Optics Express 26(8): 10729-10743 (2018).\nDOI:10.1364/OE.26.010729\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.cells3d()", "path": "api/skimage.data#skimage.data.cells3d", "type": "data", "text": "\n3D fluorescence microscopy image of cells.\n\nThe returned data is a 3D multichannel array with dimensions provided in `(z,\nc, y, x)` order. Each voxel has a size of `(0.29 0.26 0.26)` micrometer.\nChannel 0 contains cell membranes, channel 1 contains nuclei.\n\nThe volumetric images of cells taken with an optical microscope.\n\nThe data for this was provided by the Allen Institute for Cell Science.\n\nIt has been downsampled by a factor of 4 in the row and column dimensions to\nreduce computational time.\n\nThe microscope reports the following voxel spacing in microns:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.checkerboard()", "path": "api/skimage.data#skimage.data.checkerboard", "type": "data", "text": "\nCheckerboard image.\n\nCheckerboards are often used in image calibration, since the corner-points are\neasy to locate. Because of the many parallel edges, they also visualise\ndistortions particularly well.\n\nCheckerboard image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.chelsea()", "path": "api/skimage.data#skimage.data.chelsea", "type": "data", "text": "\nChelsea the cat.\n\nAn example with texture, prominent edges in horizontal and diagonal\ndirections, as well as features of differing scales.\n\nChelsea image.\n\nNo copyright restrictions. CC0 by the photographer (Stefan van der Walt).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.clock()", "path": "api/skimage.data#skimage.data.clock", "type": "data", "text": "\nMotion blurred clock.\n\nThis photograph of a wall clock was taken while moving the camera in an\naproximately horizontal direction. It may be used to illustrate inverse\nfilters and deconvolution.\n\nReleased into the public domain by the photographer (Stefan van der Walt).\n\nClock image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.coffee()", "path": "api/skimage.data#skimage.data.coffee", "type": "data", "text": "\nCoffee cup.\n\nThis photograph is courtesy of Pikolo Espresso Bar. It contains several\nelliptical shapes as well as varying texture (smooth porcelain to course wood\ngrain).\n\nCoffee image.\n\nNo copyright restrictions. CC0 by the photographer (Rachel Michetti).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.coins()", "path": "api/skimage.data#skimage.data.coins", "type": "data", "text": "\nGreek coins from Pompeii.\n\nThis image shows several coins outlined against a gray background. It is\nespecially useful in, e.g. segmentation tests, where individual objects need\nto be identified against a background. The background shares enough grey\nlevels with the coins that a simple segmentation is not sufficient.\n\nCoins image.\n\nThis image was downloaded from the Brooklyn Museum Collection.\n\nNo known copyright restrictions.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.colorwheel()", "path": "api/skimage.data#skimage.data.colorwheel", "type": "data", "text": "\nColor Wheel.\n\nA colorwheel.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.download_all()", "path": "api/skimage.data#skimage.data.download_all", "type": "data", "text": "\nDownload all datasets for use with scikit-image offline.\n\nScikit-image datasets are no longer shipped with the library by default. This\nallows us to use higher quality datasets, while keeping the library download\nsize small.\n\nThis function requires the installation of an optional dependency, pooch, to\ndownload the full dataset. Follow installation instruction found at\n\nhttps://scikit-image.org/docs/stable/install.html\n\nCall this function to download all sample images making them available offline\non your machine.\n\nThe directory where the dataset should be stored.\n\nIf pooch is not install, this error will be raised.\n\nscikit-image will only search for images stored in the default directory. Only\nspecify the directory if you wish to download the images to your own folder\nfor a particular reason. You can access the location of the default data\ndirectory by inspecting the variable `skimage.data.data_dir`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.eagle()", "path": "api/skimage.data#skimage.data.eagle", "type": "data", "text": "\nA golden eagle.\n\nSuitable for examples on segmentation, Hough transforms, and corner detection.\n\nEagle image.\n\nNo copyright restrictions. CC0 by the photographer (Dayane Machado).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.grass()", "path": "api/skimage.data#skimage.data.grass", "type": "data", "text": "\nGrass.\n\nSome grass.\n\nThe original image was downloaded from DeviantArt and licensed underthe\nCreative Commons CC0 License.\n\nThe downloaded image was cropped to include a region of `(512, 512)` pixels\naround the top left corner, converted to grayscale, then to uint8 prior to\nsaving the result in PNG format.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.gravel()", "path": "api/skimage.data#skimage.data.gravel", "type": "data", "text": "\nGravel\n\nGrayscale gravel sample.\n\nThe original image was downloaded from CC0Textures and licensed under the\nCreative Commons CC0 License.\n\nThe downloaded image was then rescaled to `(1024, 1024)`, then the top left\n`(512, 512)` pixel region was cropped prior to converting the image to\ngrayscale and uint8 data type. The result was saved using the PNG format.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.horse()", "path": "api/skimage.data#skimage.data.horse", "type": "data", "text": "\nBlack and white silhouette of a horse.\n\nThis image was downloaded from `openclipart`\n\nNo copyright restrictions. CC0 given by owner (Andreas Preuss (marauder)).\n\nHorse image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.hubble_deep_field()", "path": "api/skimage.data#skimage.data.hubble_deep_field", "type": "data", "text": "\nHubble eXtreme Deep Field.\n\nThis photograph contains the Hubble Telescope\u2019s farthest ever view of the\nuniverse. It can be useful as an example for multi-scale detection.\n\nHubble deep field image.\n\nThis image was downloaded from HubbleSite.\n\nThe image was captured by NASA and may be freely used in the public domain.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.human_mitosis()", "path": "api/skimage.data#skimage.data.human_mitosis", "type": "data", "text": "\nImage of human cells undergoing mitosis.\n\nData of human cells undergoing mitosis taken during the preperation of the\nmanuscript in [1].\n\nCopyright David Root. Licensed under CC-0 [2].\n\nMoffat J, Grueneberg DA, Yang X, Kim SY, Kloepfer AM, Hinkle G, Piqani B,\nEisenhaure TM, Luo B, Grenier JK, Carpenter AE, Foo SY, Stewart SA, Stockwell\nBR, Hacohen N, Hahn WC, Lander ES, Sabatini DM, Root DE (2006) A lentiviral\nRNAi library for human and mouse genes applied to an arrayed viral high-\ncontent screen. Cell, 124(6):1283-98 / :DOI: `10.1016/j.cell.2006.01.040` PMID\n16564017\n\nGitHub licensing discussion https://github.com/CellProfiler/examples/issues/41\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.immunohistochemistry()", "path": "api/skimage.data#skimage.data.immunohistochemistry", "type": "data", "text": "\nImmunohistochemical (IHC) staining with hematoxylin counterstaining.\n\nThis picture shows colonic glands where the IHC expression of FHL2 protein is\nrevealed with DAB. Hematoxylin counterstaining is applied to enhance the\nnegative parts of the tissue.\n\nThis image was acquired at the Center for Microscopy And Molecular Imaging\n(CMMI).\n\nNo known copyright restrictions.\n\nImmunohistochemistry image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.kidney()", "path": "api/skimage.data#skimage.data.kidney", "type": "data", "text": "\nMouse kidney tissue.\n\nThis biological tissue on a pre-prepared slide was imaged with confocal\nfluorescence microscopy (Nikon C1 inverted microscope). Image shape is (16,\n512, 512, 3). That is 512x512 pixels in X-Y, 16 image slices in Z, and 3 color\nchannels (emission wavelengths 450nm, 515nm, and 605nm, respectively). Real-\nspace voxel size is 1.24 microns in X-Y, and 1.25 microns in Z. Data type is\nunsigned 16-bit integers.\n\nKidney 3D multichannel image.\n\nThis image was acquired by Genevieve Buckley at Monasoh Micro Imaging in 2018.\nLicense: CC0\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.lbp_frontal_face_cascade_filename()", "path": "api/skimage.data#skimage.data.lbp_frontal_face_cascade_filename", "type": "data", "text": "\nReturn the path to the XML file containing the weak classifier cascade.\n\nThese classifiers were trained using LBP features. The file is part of the\nOpenCV repository [1].\n\nOpenCV lbpcascade trained files\nhttps://github.com/opencv/opencv/tree/master/data/lbpcascades\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.lfw_subset()", "path": "api/skimage.data#skimage.data.lfw_subset", "type": "data", "text": "\nSubset of data from the LFW dataset.\n\nThis database is a subset of the LFW database containing:\n\nThe full dataset is available at [2].\n\n100 first images are faces and subsequent 100 are non-faces.\n\nThe faces were randomly selected from the LFW dataset and the non-faces were\nextracted from the background of the same dataset. The cropped ROIs have been\nresized to a 25 x 25 pixels.\n\nHuang, G., Mattar, M., Lee, H., & Learned-Miller, E. G. (2012). Learning to\nalign from scratch. In Advances in Neural Information Processing Systems (pp.\n764-772).\n\nhttp://vis-www.cs.umass.edu/lfw/\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.lily()", "path": "api/skimage.data#skimage.data.lily", "type": "data", "text": "\nLily of the valley plant stem.\n\nThis plant stem on a pre-prepared slide was imaged with confocal fluorescence\nmicroscopy (Nikon C1 inverted microscope). Image shape is (922, 922, 4). That\nis 922x922 pixels in X-Y, with 4 color channels. Real-space voxel size is 1.24\nmicrons in X-Y. Data type is unsigned 16-bit integers.\n\nLily 2D multichannel image.\n\nThis image was acquired by Genevieve Buckley at Monasoh Micro Imaging in 2018.\nLicense: CC0\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.logo()", "path": "api/skimage.data#skimage.data.logo", "type": "data", "text": "\nScikit-image logo, a RGBA image.\n\nLogo image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.microaneurysms()", "path": "api/skimage.data#skimage.data.microaneurysms", "type": "data", "text": "\nGray-level \u201cmicroaneurysms\u201d image.\n\nDetail from an image of the retina (green channel). The image is a crop of\nimage 07_dr.JPG from the High-Resolution Fundus (HRF) Image Database:\nhttps://www5.cs.fau.de/research/data/fundus-images/\n\nRetina image with lesions.\n\nNo copyright restrictions. CC0 given by owner (Andreas Maier).\n\nBudai, A., Bock, R, Maier, A., Hornegger, J., Michelson, G. (2013). Robust\nVessel Segmentation in Fundus Images. International Journal of Biomedical\nImaging, vol. 2013, 2013. DOI:10.1155/2013/154860\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.moon()", "path": "api/skimage.data#skimage.data.moon", "type": "data", "text": "\nSurface of the moon.\n\nThis low-contrast image of the surface of the moon is useful for illustrating\nhistogram equalization and contrast stretching.\n\nMoon image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.page()", "path": "api/skimage.data#skimage.data.page", "type": "data", "text": "\nScanned page.\n\nThis image of printed text is useful for demonstrations requiring uneven\nbackground illumination.\n\nPage image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.retina()", "path": "api/skimage.data#skimage.data.retina", "type": "data", "text": "\nHuman retina.\n\nThis image of a retina is useful for demonstrations requiring circular images.\n\nRetina image in RGB.\n\nThis image was downloaded from `wikimedia`. This file is made available under\nthe Creative Commons CC0 1.0 Universal Public Domain Dedication.\n\nH\u00e4ggstr\u00f6m, Mikael (2014). \u201cMedical gallery of Mikael H\u00e4ggstr\u00f6m 2014\u201d.\nWikiJournal of Medicine 1 (2). DOI:10.15347/wjm/2014.008. ISSN 2002-4436.\nPublic Domain\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.rocket()", "path": "api/skimage.data#skimage.data.rocket", "type": "data", "text": "\nLaunch photo of DSCOVR on Falcon 9 by SpaceX.\n\nThis is the launch photo of Falcon 9 carrying DSCOVR lifted off from SpaceX\u2019s\nLaunch Complex 40 at Cape Canaveral Air Force Station, FL.\n\nRocket image.\n\nThis image was downloaded from SpaceX Photos.\n\nThe image was captured by SpaceX and released in the public domain.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.shepp_logan_phantom()", "path": "api/skimage.data#skimage.data.shepp_logan_phantom", "type": "data", "text": "\nShepp Logan Phantom.\n\nImage of the Shepp-Logan phantom in grayscale.\n\nL. A. Shepp and B. F. Logan, \u201cThe Fourier reconstruction of a head section,\u201d\nin IEEE Transactions on Nuclear Science, vol. 21, no. 3, pp. 21-43, June 1974.\nDOI:10.1109/TNS.1974.6499235\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.skin()", "path": "api/skimage.data#skimage.data.skin", "type": "data", "text": "\nMicroscopy image of dermis and epidermis (skin layers).\n\nHematoxylin and eosin stained slide at 10x of normal epidermis and dermis with\na benign intradermal nevus.\n\nThis image requires an Internet connection the first time it is called, and to\nhave the `pooch` package installed, in order to fetch the image file from the\nscikit-image datasets repository.\n\nThe source of this image is\nhttps://en.wikipedia.org/wiki/File:Normal_Epidermis_and_Dermis_with_Intradermal_Nevus_10x.JPG\n\nThe image was released in the public domain by its author Kilbad.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.stereo_motorcycle()", "path": "api/skimage.data#skimage.data.stereo_motorcycle", "type": "data", "text": "\nRectified stereo image pair with ground-truth disparities.\n\nThe two images are rectified such that every pixel in the left image has its\ncorresponding pixel on the same scanline in the right image. That means that\nboth images are warped such that they have the same orientation but a\nhorizontal spatial offset (baseline). The ground-truth pixel offset in column\ndirection is specified by the included disparity map.\n\nThe two images are part of the Middlebury 2014 stereo benchmark. The dataset\nwas created by Nera Nesic, Porter Westling, Xi Wang, York Kitajima, Greg\nKrathwohl, and Daniel Scharstein at Middlebury College. A detailed description\nof the acquisition process can be found in [1].\n\nThe images included here are down-sampled versions of the default exposure\nimages in the benchmark. The images are down-sampled by a factor of 4 using\nthe function `skimage.transform.downscale_local_mean`. The calibration data in\nthe following and the included ground-truth disparity map are valid for the\ndown-sampled images:\n\nLeft stereo image.\n\nRight stereo image.\n\nGround-truth disparity map, where each value describes the offset in column\ndirection between corresponding pixels in the left and the right stereo\nimages. E.g. the corresponding pixel of `img_left[10, 10 + disp[10, 10]]` is\n`img_right[10, 10]`. NaNs denote pixels in the left image that do not have\nground-truth.\n\nThe original resolution images, images with different exposure and lighting,\nand ground-truth depth maps can be found at the Middlebury website [2].\n\nD. Scharstein, H. Hirschmueller, Y. Kitajima, G. Krathwohl, N. Nesic, X. Wang,\nand P. Westling. High-resolution stereo datasets with subpixel-accurate ground\ntruth. In German Conference on Pattern Recognition (GCPR 2014), Muenster,\nGermany, September 2014.\n\nhttp://vision.middlebury.edu/stereo/data/scenes2014/\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "data.text()", "path": "api/skimage.data#skimage.data.text", "type": "data", "text": "\nGray-level \u201ctext\u201d image used for corner detection.\n\nText image.\n\nThis image was downloaded from Wikipedia\n<https://en.wikipedia.org/wiki/File:Corner.png>`__.\n\nNo known copyright restrictions, released into the public domain.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "draw", "path": "api/skimage.draw", "type": "draw", "text": "\n`skimage.draw.bezier_curve`(r0, c0, r1, c1, \u2026)\n\nGenerate Bezier curve coordinates.\n\n`skimage.draw.circle`(r, c, radius[, shape])\n\nGenerate coordinates of pixels within circle.\n\n`skimage.draw.circle_perimeter`(r, c, radius)\n\nGenerate circle perimeter coordinates.\n\n`skimage.draw.circle_perimeter_aa`(r, c, radius)\n\nGenerate anti-aliased circle perimeter coordinates.\n\n`skimage.draw.disk`(center, radius, *[, shape])\n\nGenerate coordinates of pixels within circle.\n\n`skimage.draw.ellipse`(r, c, r_radius, c_radius)\n\nGenerate coordinates of pixels within ellipse.\n\n`skimage.draw.ellipse_perimeter`(r, c, \u2026[, \u2026])\n\nGenerate ellipse perimeter coordinates.\n\n`skimage.draw.ellipsoid`(a, b, c[, spacing, \u2026])\n\nGenerates ellipsoid with semimajor axes aligned with grid dimensions on grid\nwith specified `spacing`.\n\n`skimage.draw.ellipsoid_stats`(a, b, c)\n\nCalculates analytical surface area and volume for ellipsoid with semimajor\naxes aligned with grid dimensions of specified `spacing`.\n\n`skimage.draw.line`(r0, c0, r1, c1)\n\nGenerate line pixel coordinates.\n\n`skimage.draw.line_aa`(r0, c0, r1, c1)\n\nGenerate anti-aliased line pixel coordinates.\n\n`skimage.draw.line_nd`(start, stop, *[, \u2026])\n\nDraw a single-pixel thick line in n dimensions.\n\n`skimage.draw.polygon`(r, c[, shape])\n\nGenerate coordinates of pixels within polygon.\n\n`skimage.draw.polygon2mask`(image_shape, polygon)\n\nCompute a mask from polygon.\n\n`skimage.draw.polygon_perimeter`(r, c[, \u2026])\n\nGenerate polygon perimeter coordinates.\n\n`skimage.draw.random_shapes`(image_shape, \u2026)\n\nGenerate an image with random shapes, labeled with bounding boxes.\n\n`skimage.draw.rectangle`(start[, end, extent, \u2026])\n\nGenerate coordinates of pixels within a rectangle.\n\n`skimage.draw.rectangle_perimeter`(start[, \u2026])\n\nGenerate coordinates of pixels that are exactly around a rectangle.\n\n`skimage.draw.set_color`(image, coords, color)\n\nSet pixel color in the image at the given coordinates.\n\nGenerate Bezier curve coordinates.\n\nCoordinates of the first control point.\n\nCoordinates of the middle control point.\n\nCoordinates of the last control point.\n\nMiddle control point weight, it describes the line tension.\n\nImage shape which is used to determine the maximum extent of output pixel\ncoordinates. This is useful for curves that exceed the image size. If None,\nthe full extent of the curve is used.\n\nIndices of pixels that belong to the Bezier curve. May be used to directly\nindex into an array, e.g. `img[rr, cc] = 1`.\n\nThe algorithm is the rational quadratic algorithm presented in reference [1].\n\nA Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012\nhttp://members.chello.at/easyfilter/Bresenham.pdf\n\nGenerate coordinates of pixels within circle.\n\nCenter coordinate of disk.\n\nRadius of disk.\n\nImage shape which is used to determine the maximum extent of output pixel\ncoordinates. This is useful for disks that exceed the image size. If None, the\nfull extent of the disk is used. Must be at least length 2. Only the first two\nvalues are used to determine the extent of the input image.\n\nPixel coordinates of disk. May be used to directly index into an array, e.g.\n`img[rr, cc] = 1`.\n\nNew in version 0.17: This function is deprecated and will be removed in\nscikit-image 0.19. Please use the function named `disk` instead.\n\nGenerate circle perimeter coordinates.\n\nCentre coordinate of circle.\n\nRadius of circle.\n\nbresenham : Bresenham method (default) andres : Andres method\n\nImage shape which is used to determine the maximum extent of output pixel\ncoordinates. This is useful for circles that exceed the image size. If None,\nthe full extent of the circle is used. Must be at least length 2. Only the\nfirst two values are used to determine the extent of the input image.\n\nBresenham and Andres\u2019 method: Indices of pixels that belong to the circle\nperimeter. May be used to directly index into an array, e.g. `img[rr, cc] =\n1`.\n\nAndres method presents the advantage that concentric circles create a disc\nwhereas Bresenham can make holes. There is also less distortions when Andres\ncircles are rotated. Bresenham method is also known as midpoint circle\nalgorithm. Anti-aliased circle generator is available with\n`circle_perimeter_aa`.\n\nJ.E. Bresenham, \u201cAlgorithm for computer control of a digital plotter\u201d, IBM\nSystems journal, 4 (1965) 25-30.\n\nE. Andres, \u201cDiscrete circles, rings and spheres\u201d, Computers & Graphics, 18\n(1994) 695-706.\n\nGenerate anti-aliased circle perimeter coordinates.\n\nCentre coordinate of circle.\n\nRadius of circle.\n\nImage shape which is used to determine the maximum extent of output pixel\ncoordinates. This is useful for circles that exceed the image size. If None,\nthe full extent of the circle is used. Must be at least length 2. Only the\nfirst two values are used to determine the extent of the input image.\n\nIndices of pixels (`rr`, `cc`) and intensity values (`val`). `img[rr, cc] =\nval`.\n\nWu\u2019s method draws anti-aliased circle. This implementation doesn\u2019t use lookup\ntable optimization.\n\nUse the function `draw.set_color` to apply `circle_perimeter_aa` results to\ncolor images.\n\nX. Wu, \u201cAn efficient antialiasing technique\u201d, In ACM SIGGRAPH Computer\nGraphics, 25 (1991) 143-152.\n\nGenerate coordinates of pixels within circle.\n\nCenter coordinate of disk.\n\nRadius of disk.\n\nImage shape which is used to determine the maximum extent of output pixel\ncoordinates. This is useful for disks that exceed the image size. If None, the\nfull extent of the disk is used. Must be at least length 2. Only the first two\nvalues are used to determine the extent of the input image.\n\nPixel coordinates of disk. May be used to directly index into an array, e.g.\n`img[rr, cc] = 1`.\n\nGenerate coordinates of pixels within ellipse.\n\nCentre coordinate of ellipse.\n\nMinor and major semi-axes. `(r/r_radius)**2 + (c/c_radius)**2 = 1`.\n\nImage shape which is used to determine the maximum extent of output pixel\ncoordinates. This is useful for ellipses which exceed the image size. By\ndefault the full extent of the ellipse are used. Must be at least length 2.\nOnly the first two values are used to determine the extent.\n\nSet the ellipse rotation (rotation) in range (-PI, PI) in contra clock wise\ndirection, so PI/2 degree means swap ellipse axis\n\nPixel coordinates of ellipse. May be used to directly index into an array,\ne.g. `img[rr, cc] = 1`.\n\nThe ellipse equation:\n\nNote that the positions of `ellipse` without specified `shape` can have also,\nnegative values, as this is correct on the plane. On the other hand using\nthese ellipse positions for an image afterwards may lead to appearing on the\nother side of image, because `image[-1, -1] = image[end-1, end-1]`\n\nMasked Normalized Cross-Correlation\n\nMeasure region properties\n\nGenerate ellipse perimeter coordinates.\n\nCentre coordinate of ellipse.\n\nMinor and major semi-axes. `(r/r_radius)**2 + (c/c_radius)**2 = 1`.\n\nMajor axis orientation in clockwise direction as radians.\n\nImage shape which is used to determine the maximum extent of output pixel\ncoordinates. This is useful for ellipses that exceed the image size. If None,\nthe full extent of the ellipse is used. Must be at least length 2. Only the\nfirst two values are used to determine the extent of the input image.\n\nIndices of pixels that belong to the ellipse perimeter. May be used to\ndirectly index into an array, e.g. `img[rr, cc] = 1`.\n\nA Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012\nhttp://members.chello.at/easyfilter/Bresenham.pdf\n\nNote that the positions of `ellipse` without specified `shape` can have also,\nnegative values, as this is correct on the plane. On the other hand using\nthese ellipse positions for an image afterwards may lead to appearing on the\nother side of image, because `image[-1, -1] = image[end-1, end-1]`\n\nGenerates ellipsoid with semimajor axes aligned with grid dimensions on grid\nwith specified `spacing`.\n\nLength of semimajor axis aligned with x-axis.\n\nLength of semimajor axis aligned with y-axis.\n\nLength of semimajor axis aligned with z-axis.\n\nSpacing in (x, y, z) spatial dimensions.\n\nIf True, returns the level set for this ellipsoid (signed level set about\nzero, with positive denoting interior) as np.float64. False returns a\nbinarized version of said level set.\n\nEllipsoid centered in a correctly sized array for given `spacing`. Boolean\ndtype unless `levelset=True`, in which case a float array is returned with the\nlevel set above 0.0 representing the ellipsoid.\n\nCalculates analytical surface area and volume for ellipsoid with semimajor\naxes aligned with grid dimensions of specified `spacing`.\n\nLength of semimajor axis aligned with x-axis.\n\nLength of semimajor axis aligned with y-axis.\n\nLength of semimajor axis aligned with z-axis.\n\nCalculated volume of ellipsoid.\n\nCalculated surface area of ellipsoid.\n\nGenerate line pixel coordinates.\n\nStarting position (row, column).\n\nEnd position (row, column).\n\nIndices of pixels that belong to the line. May be used to directly index into\nan array, e.g. `img[rr, cc] = 1`.\n\nAnti-aliased line generator is available with `line_aa`.\n\nGenerate anti-aliased line pixel coordinates.\n\nStarting position (row, column).\n\nEnd position (row, column).\n\nIndices of pixels (`rr`, `cc`) and intensity values (`val`). `img[rr, cc] =\nval`.\n\nA Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012\nhttp://members.chello.at/easyfilter/Bresenham.pdf\n\nDraw a single-pixel thick line in n dimensions.\n\nThe line produced will be ndim-connected. That is, two subsequent pixels in\nthe line will be either direct or diagonal neighbours in n dimensions.\n\nThe start coordinates of the line.\n\nThe end coordinates of the line.\n\nWhether to include the endpoint in the returned line. Defaults to False, which\nallows for easy drawing of multi-point paths.\n\nWhether to round the coordinates to integer. If True (default), the returned\ncoordinates can be used to directly index into an array. `False` could be used\nfor e.g. vector drawing.\n\nThe coordinates of points on the line.\n\nGenerate coordinates of pixels within polygon.\n\nRow coordinates of vertices of polygon.\n\nColumn coordinates of vertices of polygon.\n\nImage shape which is used to determine the maximum extent of output pixel\ncoordinates. This is useful for polygons that exceed the image size. If None,\nthe full extent of the polygon is used. Must be at least length 2. Only the\nfirst two values are used to determine the extent of the input image.\n\nPixel coordinates of polygon. May be used to directly index into an array,\ne.g. `img[rr, cc] = 1`.\n\nCompute a mask from polygon.\n\nThe shape of the mask.\n\nThe polygon coordinates of shape (N, 2) where N is the number of points.\n\nThe mask that corresponds to the input polygon.\n\nThis function does not do any border checking, so that all the vertices need\nto be within the given shape.\n\nGenerate polygon perimeter coordinates.\n\nRow coordinates of vertices of polygon.\n\nColumn coordinates of vertices of polygon.\n\nImage shape which is used to determine maximum extents of output pixel\ncoordinates. This is useful for polygons that exceed the image size. If None,\nthe full extents of the polygon is used. Must be at least length 2. Only the\nfirst two values are used to determine the extent of the input image.\n\nWhether to clip the polygon to the provided shape. If this is set to True, the\ndrawn figure will always be a closed polygon with all edges visible.\n\nPixel coordinates of polygon. May be used to directly index into an array,\ne.g. `img[rr, cc] = 1`.\n\nGenerate an image with random shapes, labeled with bounding boxes.\n\nThe image is populated with random shapes with random sizes, random locations,\nand random colors, with or without overlap.\n\nShapes have random (row, col) starting coordinates and random sizes bounded by\n`min_size` and `max_size`. It can occur that a randomly generated shape will\nnot fit the image at all. In that case, the algorithm will try again with new\nstarting coordinates a certain number of times. However, it also means that\nsome shapes may be skipped altogether. In that case, this function will\ngenerate fewer shapes than requested.\n\nThe number of rows and columns of the image to generate.\n\nThe maximum number of shapes to (attempt to) fit into the shape.\n\nThe minimum number of shapes to (attempt to) fit into the shape.\n\nThe minimum dimension of each shape to fit into the image.\n\nThe maximum dimension of each shape to fit into the image.\n\nIf True, the generated image has `num_channels` color channels, otherwise\ngenerates grayscale image.\n\nNumber of channels in the generated image. If 1, generate monochrome images,\nelse color images with multiple channels. Ignored if `multichannel` is set to\nFalse.\n\nThe name of the shape to generate or `None` to pick random ones.\n\nThe range of values to sample pixel values from. For grayscale images the\nformat is (min, max). For multichannel - ((min, max),) if the ranges are equal\nacross the channels, and ((min_0, max_0), \u2026 (min_N, max_N)) if they differ. As\nthe function supports generation of uint8 arrays only, the maximum range is\n(0, 255). If None, set to (0, 254) for each channel reserving color of\nintensity = 255 for background.\n\nIf `True`, allow shapes to overlap.\n\nHow often to attempt to fit a shape into the image before skipping it.\n\nSeed to initialize the random number generator. If `None`, a random seed from\nthe operating system is used.\n\nAn image with the fitted shapes.\n\nA list of labels, one per shape in the image. Each label is a (category, ((r0,\nr1), (c0, c1))) tuple specifying the category and bounding box coordinates of\nthe shape.\n\nGenerate coordinates of pixels within a rectangle.\n\nOrigin point of the rectangle, e.g., `([plane,] row, column)`.\n\nEnd point of the rectangle `([plane,] row, column)`. For a 2D matrix, the\nslice defined by the rectangle is `[start:(end+1)]`. Either `end` or `extent`\nmust be specified.\n\nThe extent (size) of the drawn rectangle. E.g., `([num_planes,] num_rows,\nnum_cols)`. Either `end` or `extent` must be specified. A negative extent is\nvalid, and will result in a rectangle going along the opposite direction. If\nextent is negative, the `start` point is not included.\n\nImage shape used to determine the maximum bounds of the output coordinates.\nThis is useful for clipping rectangles that exceed the image size. By default,\nno clipping is done.\n\nThe coordinates of all pixels in the rectangle.\n\nThis function can be applied to N-dimensional images, by passing `start` and\n`end` or `extent` as tuples of length N.\n\nGenerate coordinates of pixels that are exactly around a rectangle.\n\nOrigin point of the inner rectangle, e.g., `(row, column)`.\n\nEnd point of the inner rectangle `(row, column)`. For a 2D matrix, the slice\ndefined by inner the rectangle is `[start:(end+1)]`. Either `end` or `extent`\nmust be specified.\n\nThe extent (size) of the inner rectangle. E.g., `(num_rows, num_cols)`. Either\n`end` or `extent` must be specified. Negative extents are permitted. See\n`rectangle` to better understand how they behave.\n\nImage shape used to determine the maximum bounds of the output coordinates.\nThis is useful for clipping perimeters that exceed the image size. By default,\nno clipping is done. Must be at least length 2. Only the first two values are\nused to determine the extent of the input image.\n\nWhether to clip the perimeter to the provided shape. If this is set to True,\nthe drawn figure will always be a closed polygon with all edges visible.\n\nThe coordinates of all pixels in the rectangle.\n\nSet pixel color in the image at the given coordinates.\n\nNote that this function modifies the color of the image in-place. Coordinates\nthat exceed the shape of the image will be ignored.\n\nImage\n\nRow and column coordinates of pixels to be colored.\n\nColor to be assigned to coordinates in the image.\n\nAlpha values used to blend color with image. 0 is transparent, 1 is opaque.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "draw.bezier_curve()", "path": "api/skimage.draw#skimage.draw.bezier_curve", "type": "draw", "text": "\nGenerate Bezier curve coordinates.\n\nCoordinates of the first control point.\n\nCoordinates of the middle control point.\n\nCoordinates of the last control point.\n\nMiddle control point weight, it describes the line tension.\n\nImage shape which is used to determine the maximum extent of output pixel\ncoordinates. This is useful for curves that exceed the image size. If None,\nthe full extent of the curve is used.\n\nIndices of pixels that belong to the Bezier curve. May be used to directly\nindex into an array, e.g. `img[rr, cc] = 1`.\n\nThe algorithm is the rational quadratic algorithm presented in reference [1].\n\nA Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012\nhttp://members.chello.at/easyfilter/Bresenham.pdf\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "draw.circle()", "path": "api/skimage.draw#skimage.draw.circle", "type": "draw", "text": "\nGenerate coordinates of pixels within circle.\n\nCenter coordinate of disk.\n\nRadius of disk.\n\nImage shape which is used to determine the maximum extent of output pixel\ncoordinates. This is useful for disks that exceed the image size. If None, the\nfull extent of the disk is used. Must be at least length 2. Only the first two\nvalues are used to determine the extent of the input image.\n\nPixel coordinates of disk. May be used to directly index into an array, e.g.\n`img[rr, cc] = 1`.\n\nNew in version 0.17: This function is deprecated and will be removed in\nscikit-image 0.19. Please use the function named `disk` instead.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "draw.circle_perimeter()", "path": "api/skimage.draw#skimage.draw.circle_perimeter", "type": "draw", "text": "\nGenerate circle perimeter coordinates.\n\nCentre coordinate of circle.\n\nRadius of circle.\n\nbresenham : Bresenham method (default) andres : Andres method\n\nImage shape which is used to determine the maximum extent of output pixel\ncoordinates. This is useful for circles that exceed the image size. If None,\nthe full extent of the circle is used. Must be at least length 2. Only the\nfirst two values are used to determine the extent of the input image.\n\nBresenham and Andres\u2019 method: Indices of pixels that belong to the circle\nperimeter. May be used to directly index into an array, e.g. `img[rr, cc] =\n1`.\n\nAndres method presents the advantage that concentric circles create a disc\nwhereas Bresenham can make holes. There is also less distortions when Andres\ncircles are rotated. Bresenham method is also known as midpoint circle\nalgorithm. Anti-aliased circle generator is available with\n`circle_perimeter_aa`.\n\nJ.E. Bresenham, \u201cAlgorithm for computer control of a digital plotter\u201d, IBM\nSystems journal, 4 (1965) 25-30.\n\nE. Andres, \u201cDiscrete circles, rings and spheres\u201d, Computers & Graphics, 18\n(1994) 695-706.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "draw.circle_perimeter_aa()", "path": "api/skimage.draw#skimage.draw.circle_perimeter_aa", "type": "draw", "text": "\nGenerate anti-aliased circle perimeter coordinates.\n\nCentre coordinate of circle.\n\nRadius of circle.\n\nImage shape which is used to determine the maximum extent of output pixel\ncoordinates. This is useful for circles that exceed the image size. If None,\nthe full extent of the circle is used. Must be at least length 2. Only the\nfirst two values are used to determine the extent of the input image.\n\nIndices of pixels (`rr`, `cc`) and intensity values (`val`). `img[rr, cc] =\nval`.\n\nWu\u2019s method draws anti-aliased circle. This implementation doesn\u2019t use lookup\ntable optimization.\n\nUse the function `draw.set_color` to apply `circle_perimeter_aa` results to\ncolor images.\n\nX. Wu, \u201cAn efficient antialiasing technique\u201d, In ACM SIGGRAPH Computer\nGraphics, 25 (1991) 143-152.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "draw.disk()", "path": "api/skimage.draw#skimage.draw.disk", "type": "draw", "text": "\nGenerate coordinates of pixels within circle.\n\nCenter coordinate of disk.\n\nRadius of disk.\n\nImage shape which is used to determine the maximum extent of output pixel\ncoordinates. This is useful for disks that exceed the image size. If None, the\nfull extent of the disk is used. Must be at least length 2. Only the first two\nvalues are used to determine the extent of the input image.\n\nPixel coordinates of disk. May be used to directly index into an array, e.g.\n`img[rr, cc] = 1`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "draw.ellipse()", "path": "api/skimage.draw#skimage.draw.ellipse", "type": "draw", "text": "\nGenerate coordinates of pixels within ellipse.\n\nCentre coordinate of ellipse.\n\nMinor and major semi-axes. `(r/r_radius)**2 + (c/c_radius)**2 = 1`.\n\nImage shape which is used to determine the maximum extent of output pixel\ncoordinates. This is useful for ellipses which exceed the image size. By\ndefault the full extent of the ellipse are used. Must be at least length 2.\nOnly the first two values are used to determine the extent.\n\nSet the ellipse rotation (rotation) in range (-PI, PI) in contra clock wise\ndirection, so PI/2 degree means swap ellipse axis\n\nPixel coordinates of ellipse. May be used to directly index into an array,\ne.g. `img[rr, cc] = 1`.\n\nThe ellipse equation:\n\nNote that the positions of `ellipse` without specified `shape` can have also,\nnegative values, as this is correct on the plane. On the other hand using\nthese ellipse positions for an image afterwards may lead to appearing on the\nother side of image, because `image[-1, -1] = image[end-1, end-1]`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "draw.ellipse_perimeter()", "path": "api/skimage.draw#skimage.draw.ellipse_perimeter", "type": "draw", "text": "\nGenerate ellipse perimeter coordinates.\n\nCentre coordinate of ellipse.\n\nMinor and major semi-axes. `(r/r_radius)**2 + (c/c_radius)**2 = 1`.\n\nMajor axis orientation in clockwise direction as radians.\n\nImage shape which is used to determine the maximum extent of output pixel\ncoordinates. This is useful for ellipses that exceed the image size. If None,\nthe full extent of the ellipse is used. Must be at least length 2. Only the\nfirst two values are used to determine the extent of the input image.\n\nIndices of pixels that belong to the ellipse perimeter. May be used to\ndirectly index into an array, e.g. `img[rr, cc] = 1`.\n\nA Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012\nhttp://members.chello.at/easyfilter/Bresenham.pdf\n\nNote that the positions of `ellipse` without specified `shape` can have also,\nnegative values, as this is correct on the plane. On the other hand using\nthese ellipse positions for an image afterwards may lead to appearing on the\nother side of image, because `image[-1, -1] = image[end-1, end-1]`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "draw.ellipsoid()", "path": "api/skimage.draw#skimage.draw.ellipsoid", "type": "draw", "text": "\nGenerates ellipsoid with semimajor axes aligned with grid dimensions on grid\nwith specified `spacing`.\n\nLength of semimajor axis aligned with x-axis.\n\nLength of semimajor axis aligned with y-axis.\n\nLength of semimajor axis aligned with z-axis.\n\nSpacing in (x, y, z) spatial dimensions.\n\nIf True, returns the level set for this ellipsoid (signed level set about\nzero, with positive denoting interior) as np.float64. False returns a\nbinarized version of said level set.\n\nEllipsoid centered in a correctly sized array for given `spacing`. Boolean\ndtype unless `levelset=True`, in which case a float array is returned with the\nlevel set above 0.0 representing the ellipsoid.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "draw.ellipsoid_stats()", "path": "api/skimage.draw#skimage.draw.ellipsoid_stats", "type": "draw", "text": "\nCalculates analytical surface area and volume for ellipsoid with semimajor\naxes aligned with grid dimensions of specified `spacing`.\n\nLength of semimajor axis aligned with x-axis.\n\nLength of semimajor axis aligned with y-axis.\n\nLength of semimajor axis aligned with z-axis.\n\nCalculated volume of ellipsoid.\n\nCalculated surface area of ellipsoid.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "draw.line()", "path": "api/skimage.draw#skimage.draw.line", "type": "draw", "text": "\nGenerate line pixel coordinates.\n\nStarting position (row, column).\n\nEnd position (row, column).\n\nIndices of pixels that belong to the line. May be used to directly index into\nan array, e.g. `img[rr, cc] = 1`.\n\nAnti-aliased line generator is available with `line_aa`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "draw.line_aa()", "path": "api/skimage.draw#skimage.draw.line_aa", "type": "draw", "text": "\nGenerate anti-aliased line pixel coordinates.\n\nStarting position (row, column).\n\nEnd position (row, column).\n\nIndices of pixels (`rr`, `cc`) and intensity values (`val`). `img[rr, cc] =\nval`.\n\nA Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012\nhttp://members.chello.at/easyfilter/Bresenham.pdf\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "draw.line_nd()", "path": "api/skimage.draw#skimage.draw.line_nd", "type": "draw", "text": "\nDraw a single-pixel thick line in n dimensions.\n\nThe line produced will be ndim-connected. That is, two subsequent pixels in\nthe line will be either direct or diagonal neighbours in n dimensions.\n\nThe start coordinates of the line.\n\nThe end coordinates of the line.\n\nWhether to include the endpoint in the returned line. Defaults to False, which\nallows for easy drawing of multi-point paths.\n\nWhether to round the coordinates to integer. If True (default), the returned\ncoordinates can be used to directly index into an array. `False` could be used\nfor e.g. vector drawing.\n\nThe coordinates of points on the line.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "draw.polygon()", "path": "api/skimage.draw#skimage.draw.polygon", "type": "draw", "text": "\nGenerate coordinates of pixels within polygon.\n\nRow coordinates of vertices of polygon.\n\nColumn coordinates of vertices of polygon.\n\nImage shape which is used to determine the maximum extent of output pixel\ncoordinates. This is useful for polygons that exceed the image size. If None,\nthe full extent of the polygon is used. Must be at least length 2. Only the\nfirst two values are used to determine the extent of the input image.\n\nPixel coordinates of polygon. May be used to directly index into an array,\ne.g. `img[rr, cc] = 1`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "draw.polygon2mask()", "path": "api/skimage.draw#skimage.draw.polygon2mask", "type": "draw", "text": "\nCompute a mask from polygon.\n\nThe shape of the mask.\n\nThe polygon coordinates of shape (N, 2) where N is the number of points.\n\nThe mask that corresponds to the input polygon.\n\nThis function does not do any border checking, so that all the vertices need\nto be within the given shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "draw.polygon_perimeter()", "path": "api/skimage.draw#skimage.draw.polygon_perimeter", "type": "draw", "text": "\nGenerate polygon perimeter coordinates.\n\nRow coordinates of vertices of polygon.\n\nColumn coordinates of vertices of polygon.\n\nImage shape which is used to determine maximum extents of output pixel\ncoordinates. This is useful for polygons that exceed the image size. If None,\nthe full extents of the polygon is used. Must be at least length 2. Only the\nfirst two values are used to determine the extent of the input image.\n\nWhether to clip the polygon to the provided shape. If this is set to True, the\ndrawn figure will always be a closed polygon with all edges visible.\n\nPixel coordinates of polygon. May be used to directly index into an array,\ne.g. `img[rr, cc] = 1`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "draw.random_shapes()", "path": "api/skimage.draw#skimage.draw.random_shapes", "type": "draw", "text": "\nGenerate an image with random shapes, labeled with bounding boxes.\n\nThe image is populated with random shapes with random sizes, random locations,\nand random colors, with or without overlap.\n\nShapes have random (row, col) starting coordinates and random sizes bounded by\n`min_size` and `max_size`. It can occur that a randomly generated shape will\nnot fit the image at all. In that case, the algorithm will try again with new\nstarting coordinates a certain number of times. However, it also means that\nsome shapes may be skipped altogether. In that case, this function will\ngenerate fewer shapes than requested.\n\nThe number of rows and columns of the image to generate.\n\nThe maximum number of shapes to (attempt to) fit into the shape.\n\nThe minimum number of shapes to (attempt to) fit into the shape.\n\nThe minimum dimension of each shape to fit into the image.\n\nThe maximum dimension of each shape to fit into the image.\n\nIf True, the generated image has `num_channels` color channels, otherwise\ngenerates grayscale image.\n\nNumber of channels in the generated image. If 1, generate monochrome images,\nelse color images with multiple channels. Ignored if `multichannel` is set to\nFalse.\n\nThe name of the shape to generate or `None` to pick random ones.\n\nThe range of values to sample pixel values from. For grayscale images the\nformat is (min, max). For multichannel - ((min, max),) if the ranges are equal\nacross the channels, and ((min_0, max_0), \u2026 (min_N, max_N)) if they differ. As\nthe function supports generation of uint8 arrays only, the maximum range is\n(0, 255). If None, set to (0, 254) for each channel reserving color of\nintensity = 255 for background.\n\nIf `True`, allow shapes to overlap.\n\nHow often to attempt to fit a shape into the image before skipping it.\n\nSeed to initialize the random number generator. If `None`, a random seed from\nthe operating system is used.\n\nAn image with the fitted shapes.\n\nA list of labels, one per shape in the image. Each label is a (category, ((r0,\nr1), (c0, c1))) tuple specifying the category and bounding box coordinates of\nthe shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "draw.rectangle()", "path": "api/skimage.draw#skimage.draw.rectangle", "type": "draw", "text": "\nGenerate coordinates of pixels within a rectangle.\n\nOrigin point of the rectangle, e.g., `([plane,] row, column)`.\n\nEnd point of the rectangle `([plane,] row, column)`. For a 2D matrix, the\nslice defined by the rectangle is `[start:(end+1)]`. Either `end` or `extent`\nmust be specified.\n\nThe extent (size) of the drawn rectangle. E.g., `([num_planes,] num_rows,\nnum_cols)`. Either `end` or `extent` must be specified. A negative extent is\nvalid, and will result in a rectangle going along the opposite direction. If\nextent is negative, the `start` point is not included.\n\nImage shape used to determine the maximum bounds of the output coordinates.\nThis is useful for clipping rectangles that exceed the image size. By default,\nno clipping is done.\n\nThe coordinates of all pixels in the rectangle.\n\nThis function can be applied to N-dimensional images, by passing `start` and\n`end` or `extent` as tuples of length N.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "draw.rectangle_perimeter()", "path": "api/skimage.draw#skimage.draw.rectangle_perimeter", "type": "draw", "text": "\nGenerate coordinates of pixels that are exactly around a rectangle.\n\nOrigin point of the inner rectangle, e.g., `(row, column)`.\n\nEnd point of the inner rectangle `(row, column)`. For a 2D matrix, the slice\ndefined by inner the rectangle is `[start:(end+1)]`. Either `end` or `extent`\nmust be specified.\n\nThe extent (size) of the inner rectangle. E.g., `(num_rows, num_cols)`. Either\n`end` or `extent` must be specified. Negative extents are permitted. See\n`rectangle` to better understand how they behave.\n\nImage shape used to determine the maximum bounds of the output coordinates.\nThis is useful for clipping perimeters that exceed the image size. By default,\nno clipping is done. Must be at least length 2. Only the first two values are\nused to determine the extent of the input image.\n\nWhether to clip the perimeter to the provided shape. If this is set to True,\nthe drawn figure will always be a closed polygon with all edges visible.\n\nThe coordinates of all pixels in the rectangle.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "draw.set_color()", "path": "api/skimage.draw#skimage.draw.set_color", "type": "draw", "text": "\nSet pixel color in the image at the given coordinates.\n\nNote that this function modifies the color of the image in-place. Coordinates\nthat exceed the shape of the image will be ignored.\n\nImage\n\nRow and column coordinates of pixels to be colored.\n\nColor to be assigned to coordinates in the image.\n\nAlpha values used to blend color with image. 0 is transparent, 1 is opaque.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "dtype_limits()", "path": "api/skimage#skimage.dtype_limits", "type": "skimage", "text": "\nReturn intensity limits, i.e. (min, max) tuple, of the image\u2019s dtype.\n\nInput image.\n\nIf True, clip the negative range (i.e. return 0 for min intensity) even if the\nimage dtype allows negative values.\n\nLower and upper intensity limits.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "ensure_python_version()", "path": "api/skimage#skimage.ensure_python_version", "type": "skimage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "exposure", "path": "api/skimage.exposure", "type": "exposure", "text": "\n`skimage.exposure.adjust_gamma`(image[, \u2026])\n\nPerforms Gamma Correction on the input image.\n\n`skimage.exposure.adjust_log`(image[, gain, inv])\n\nPerforms Logarithmic correction on the input image.\n\n`skimage.exposure.adjust_sigmoid`(image[, \u2026])\n\nPerforms Sigmoid Correction on the input image.\n\n`skimage.exposure.cumulative_distribution`(image)\n\nReturn cumulative distribution function (cdf) for the given image.\n\n`skimage.exposure.equalize_adapthist`(image[, \u2026])\n\nContrast Limited Adaptive Histogram Equalization (CLAHE).\n\n`skimage.exposure.equalize_hist`(image[, \u2026])\n\nReturn image after histogram equalization.\n\n`skimage.exposure.histogram`(image[, nbins, \u2026])\n\nReturn histogram of image.\n\n`skimage.exposure.is_low_contrast`(image[, \u2026])\n\nDetermine if an image is low contrast.\n\n`skimage.exposure.match_histograms`(image, \u2026)\n\nAdjust an image so that its cumulative histogram matches that of another.\n\n`skimage.exposure.rescale_intensity`(image[, \u2026])\n\nReturn image after stretching or shrinking its intensity levels.\n\nPerforms Gamma Correction on the input image.\n\nAlso known as Power Law Transform. This function transforms the input image\npixelwise according to the equation `O = I**gamma` after scaling each pixel to\nthe range 0 to 1.\n\nInput image.\n\nNon negative real number. Default value is 1.\n\nThe constant multiplier. Default value is 1.\n\nGamma corrected output image.\n\nSee also\n\nFor gamma greater than 1, the histogram will shift towards left and the output\nimage will be darker than the input image.\n\nFor gamma less than 1, the histogram will shift towards right and the output\nimage will be brighter than the input image.\n\nhttps://en.wikipedia.org/wiki/Gamma_correction\n\nExplore 3D images (of cells)\n\nPerforms Logarithmic correction on the input image.\n\nThis function transforms the input image pixelwise according to the equation\n`O = gain*log(1 + I)` after scaling each pixel to the range 0 to 1. For\ninverse logarithmic correction, the equation is `O = gain*(2**I - 1)`.\n\nInput image.\n\nThe constant multiplier. Default value is 1.\n\nIf True, it performs inverse logarithmic correction, else correction will be\nlogarithmic. Defaults to False.\n\nLogarithm corrected output image.\n\nSee also\n\nhttp://www.ece.ucsb.edu/Faculty/Manjunath/courses/ece178W03/EnhancePart1.pdf\n\nPerforms Sigmoid Correction on the input image.\n\nAlso known as Contrast Adjustment. This function transforms the input image\npixelwise according to the equation `O = 1/(1 + exp*(gain*(cutoff - I)))`\nafter scaling each pixel to the range 0 to 1.\n\nInput image.\n\nCutoff of the sigmoid function that shifts the characteristic curve in\nhorizontal direction. Default value is 0.5.\n\nThe constant multiplier in exponential\u2019s power of sigmoid function. Default\nvalue is 10.\n\nIf True, returns the negative sigmoid correction. Defaults to False.\n\nSigmoid corrected output image.\n\nSee also\n\nGustav J. Braun, \u201cImage Lightness Rescaling Using Sigmoidal Contrast\nEnhancement Functions\u201d, http://www.cis.rit.edu/fairchild/PDFs/PAP07.pdf\n\nReturn cumulative distribution function (cdf) for the given image.\n\nImage array.\n\nNumber of bins for image histogram.\n\nValues of cumulative distribution function.\n\nCenters of bins.\n\nSee also\n\nhttps://en.wikipedia.org/wiki/Cumulative_distribution_function\n\nLocal Histogram Equalization\n\nExplore 3D images (of cells)\n\nContrast Limited Adaptive Histogram Equalization (CLAHE).\n\nAn algorithm for local contrast enhancement, that uses histograms computed\nover different tile regions of the image. Local details can therefore be\nenhanced even in regions that are darker or lighter than most of the image.\n\nInput image.\n\nDefines the shape of contextual regions used in the algorithm. If iterable is\npassed, it must have the same number of elements as `image.ndim` (without\ncolor channel). If integer, it is broadcasted to each `image` dimension. By\ndefault, `kernel_size` is 1/8 of `image` height by 1/8 of its width.\n\nClipping limit, normalized between 0 and 1 (higher values give more contrast).\n\nNumber of gray bins for histogram (\u201cdata range\u201d).\n\nEqualized image with float64 dtype.\n\nSee also\n\nChanged in version 0.17: The values returned by this function are slightly\nshifted upwards because of an internal change in rounding behavior.\n\nhttp://tog.acm.org/resources/GraphicsGems/\n\nhttps://en.wikipedia.org/wiki/CLAHE#CLAHE\n\n3D adaptive histogram equalization\n\nReturn image after histogram equalization.\n\nImage array.\n\nNumber of bins for image histogram. Note: this argument is ignored for integer\nimages, for which each integer is its own bin.\n\nArray of same shape as `image`. Only points at which mask == True are used for\nthe equalization, which is applied to the whole image.\n\nImage array after histogram equalization.\n\nThis function is adapted from [1] with the author\u2019s permission.\n\nhttp://www.janeriksolem.net/histogram-equalization-with-python-and.html\n\nhttps://en.wikipedia.org/wiki/Histogram_equalization\n\nLocal Histogram Equalization\n\n3D adaptive histogram equalization\n\nExplore 3D images (of cells)\n\nRank filters\n\nReturn histogram of image.\n\nUnlike `numpy.histogram`, this function returns the centers of bins and does\nnot rebin integer arrays. For integer arrays, each integer value has its own\nbin, which improves speed and intensity-resolution.\n\nThe histogram is computed on the flattened image: for color images, the\nfunction should be used separately on each channel to obtain a histogram for\neach color channel.\n\nInput image.\n\nNumber of bins used to calculate histogram. This value is ignored for integer\narrays.\n\n\u2018image\u2019 (default) determines the range from the input image. \u2018dtype\u2019\ndetermines the range from the expected range of the images of that data type.\n\nIf True, normalize the histogram by the sum of its values.\n\nThe values of the histogram.\n\nThe values at the center of the bins.\n\nSee also\n\nRank filters\n\nDetermine if an image is low contrast.\n\nThe image under test.\n\nThe low contrast fraction threshold. An image is considered low- contrast when\nits range of brightness spans less than this fraction of its data type\u2019s full\nrange. [1]\n\nDisregard values below this percentile when computing image contrast.\n\nDisregard values above this percentile when computing image contrast.\n\nThe contrast determination method. Right now the only available option is\n\u201clinear\u201d.\n\nTrue when the image is determined to be low contrast.\n\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nAdjust an image so that its cumulative histogram matches that of another.\n\nThe adjustment is applied separately for each channel.\n\nInput image. Can be gray-scale or in color.\n\nImage to match histogram of. Must have the same number of channels as image.\n\nApply the matching separately for each channel.\n\nTransformed input image.\n\nThrown when the number of channels in the input image and the reference\ndiffer.\n\nhttp://paulbourke.net/miscellaneous/equalisation/\n\nReturn image after stretching or shrinking its intensity levels.\n\nThe desired intensity range of the input and output, `in_range` and\n`out_range` respectively, are used to stretch or shrink the intensity range of\nthe input image. See examples below.\n\nImage array.\n\nMin and max intensity values of input and output image. The possible values\nfor this parameter are enumerated below.\n\nUse image min/max as the intensity range.\n\nUse min/max of the image\u2019s dtype as the intensity range.\n\nUse intensity range based on desired `dtype`. Must be valid key in\n`DTYPE_RANGE`.\n\nUse `range_values` as explicit min/max intensities.\n\nImage array after rescaling its intensity. This image is the same dtype as the\ninput image.\n\nSee also\n\nChanged in version 0.17: The dtype of the output array has changed to match\nthe output dtype, or float if the output range is specified by a pair of\nfloats.\n\nBy default, the min/max intensities of the input image are stretched to the\nlimits allowed by the image\u2019s dtype, since `in_range` defaults to \u2018image\u2019 and\n`out_range` defaults to \u2018dtype\u2019:\n\nIt\u2019s easy to accidentally convert an image dtype from uint8 to float:\n\nUse `rescale_intensity` to rescale to the proper range for float dtypes:\n\nTo maintain the low contrast of the original, use the `in_range` parameter:\n\nIf the min/max value of `in_range` is more/less than the min/max image\nintensity, then the intensity levels are clipped:\n\nIf you have an image with signed integers but want to rescale the image to\njust the positive range, use the `out_range` parameter. In that case, the\noutput dtype will be float:\n\nTo get the desired range with a specific dtype, use `.astype()`:\n\nIf the input image is constant, the output will be clipped directly to the\noutput range: >>> image = np.array([130, 130, 130], dtype=np.int32) >>>\nrescale_intensity(image, out_range=(0, 127)).astype(np.int32) array([127, 127,\n127], dtype=int32)\n\nPhase Unwrapping\n\nExplore 3D images (of cells)\n\nRank filters\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "exposure.adjust_gamma()", "path": "api/skimage.exposure#skimage.exposure.adjust_gamma", "type": "exposure", "text": "\nPerforms Gamma Correction on the input image.\n\nAlso known as Power Law Transform. This function transforms the input image\npixelwise according to the equation `O = I**gamma` after scaling each pixel to\nthe range 0 to 1.\n\nInput image.\n\nNon negative real number. Default value is 1.\n\nThe constant multiplier. Default value is 1.\n\nGamma corrected output image.\n\nSee also\n\nFor gamma greater than 1, the histogram will shift towards left and the output\nimage will be darker than the input image.\n\nFor gamma less than 1, the histogram will shift towards right and the output\nimage will be brighter than the input image.\n\nhttps://en.wikipedia.org/wiki/Gamma_correction\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "exposure.adjust_log()", "path": "api/skimage.exposure#skimage.exposure.adjust_log", "type": "exposure", "text": "\nPerforms Logarithmic correction on the input image.\n\nThis function transforms the input image pixelwise according to the equation\n`O = gain*log(1 + I)` after scaling each pixel to the range 0 to 1. For\ninverse logarithmic correction, the equation is `O = gain*(2**I - 1)`.\n\nInput image.\n\nThe constant multiplier. Default value is 1.\n\nIf True, it performs inverse logarithmic correction, else correction will be\nlogarithmic. Defaults to False.\n\nLogarithm corrected output image.\n\nSee also\n\nhttp://www.ece.ucsb.edu/Faculty/Manjunath/courses/ece178W03/EnhancePart1.pdf\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "exposure.adjust_sigmoid()", "path": "api/skimage.exposure#skimage.exposure.adjust_sigmoid", "type": "exposure", "text": "\nPerforms Sigmoid Correction on the input image.\n\nAlso known as Contrast Adjustment. This function transforms the input image\npixelwise according to the equation `O = 1/(1 + exp*(gain*(cutoff - I)))`\nafter scaling each pixel to the range 0 to 1.\n\nInput image.\n\nCutoff of the sigmoid function that shifts the characteristic curve in\nhorizontal direction. Default value is 0.5.\n\nThe constant multiplier in exponential\u2019s power of sigmoid function. Default\nvalue is 10.\n\nIf True, returns the negative sigmoid correction. Defaults to False.\n\nSigmoid corrected output image.\n\nSee also\n\nGustav J. Braun, \u201cImage Lightness Rescaling Using Sigmoidal Contrast\nEnhancement Functions\u201d, http://www.cis.rit.edu/fairchild/PDFs/PAP07.pdf\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "exposure.cumulative_distribution()", "path": "api/skimage.exposure#skimage.exposure.cumulative_distribution", "type": "exposure", "text": "\nReturn cumulative distribution function (cdf) for the given image.\n\nImage array.\n\nNumber of bins for image histogram.\n\nValues of cumulative distribution function.\n\nCenters of bins.\n\nSee also\n\nhttps://en.wikipedia.org/wiki/Cumulative_distribution_function\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "exposure.equalize_adapthist()", "path": "api/skimage.exposure#skimage.exposure.equalize_adapthist", "type": "exposure", "text": "\nContrast Limited Adaptive Histogram Equalization (CLAHE).\n\nAn algorithm for local contrast enhancement, that uses histograms computed\nover different tile regions of the image. Local details can therefore be\nenhanced even in regions that are darker or lighter than most of the image.\n\nInput image.\n\nDefines the shape of contextual regions used in the algorithm. If iterable is\npassed, it must have the same number of elements as `image.ndim` (without\ncolor channel). If integer, it is broadcasted to each `image` dimension. By\ndefault, `kernel_size` is 1/8 of `image` height by 1/8 of its width.\n\nClipping limit, normalized between 0 and 1 (higher values give more contrast).\n\nNumber of gray bins for histogram (\u201cdata range\u201d).\n\nEqualized image with float64 dtype.\n\nSee also\n\nChanged in version 0.17: The values returned by this function are slightly\nshifted upwards because of an internal change in rounding behavior.\n\nhttp://tog.acm.org/resources/GraphicsGems/\n\nhttps://en.wikipedia.org/wiki/CLAHE#CLAHE\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "exposure.equalize_hist()", "path": "api/skimage.exposure#skimage.exposure.equalize_hist", "type": "exposure", "text": "\nReturn image after histogram equalization.\n\nImage array.\n\nNumber of bins for image histogram. Note: this argument is ignored for integer\nimages, for which each integer is its own bin.\n\nArray of same shape as `image`. Only points at which mask == True are used for\nthe equalization, which is applied to the whole image.\n\nImage array after histogram equalization.\n\nThis function is adapted from [1] with the author\u2019s permission.\n\nhttp://www.janeriksolem.net/histogram-equalization-with-python-and.html\n\nhttps://en.wikipedia.org/wiki/Histogram_equalization\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "exposure.histogram()", "path": "api/skimage.exposure#skimage.exposure.histogram", "type": "exposure", "text": "\nReturn histogram of image.\n\nUnlike `numpy.histogram`, this function returns the centers of bins and does\nnot rebin integer arrays. For integer arrays, each integer value has its own\nbin, which improves speed and intensity-resolution.\n\nThe histogram is computed on the flattened image: for color images, the\nfunction should be used separately on each channel to obtain a histogram for\neach color channel.\n\nInput image.\n\nNumber of bins used to calculate histogram. This value is ignored for integer\narrays.\n\n\u2018image\u2019 (default) determines the range from the input image. \u2018dtype\u2019\ndetermines the range from the expected range of the images of that data type.\n\nIf True, normalize the histogram by the sum of its values.\n\nThe values of the histogram.\n\nThe values at the center of the bins.\n\nSee also\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "exposure.is_low_contrast()", "path": "api/skimage.exposure#skimage.exposure.is_low_contrast", "type": "exposure", "text": "\nDetermine if an image is low contrast.\n\nThe image under test.\n\nThe low contrast fraction threshold. An image is considered low- contrast when\nits range of brightness spans less than this fraction of its data type\u2019s full\nrange. [1]\n\nDisregard values below this percentile when computing image contrast.\n\nDisregard values above this percentile when computing image contrast.\n\nThe contrast determination method. Right now the only available option is\n\u201clinear\u201d.\n\nTrue when the image is determined to be low contrast.\n\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "exposure.match_histograms()", "path": "api/skimage.exposure#skimage.exposure.match_histograms", "type": "exposure", "text": "\nAdjust an image so that its cumulative histogram matches that of another.\n\nThe adjustment is applied separately for each channel.\n\nInput image. Can be gray-scale or in color.\n\nImage to match histogram of. Must have the same number of channels as image.\n\nApply the matching separately for each channel.\n\nTransformed input image.\n\nThrown when the number of channels in the input image and the reference\ndiffer.\n\nhttp://paulbourke.net/miscellaneous/equalisation/\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "exposure.rescale_intensity()", "path": "api/skimage.exposure#skimage.exposure.rescale_intensity", "type": "exposure", "text": "\nReturn image after stretching or shrinking its intensity levels.\n\nThe desired intensity range of the input and output, `in_range` and\n`out_range` respectively, are used to stretch or shrink the intensity range of\nthe input image. See examples below.\n\nImage array.\n\nMin and max intensity values of input and output image. The possible values\nfor this parameter are enumerated below.\n\nUse image min/max as the intensity range.\n\nUse min/max of the image\u2019s dtype as the intensity range.\n\nUse intensity range based on desired `dtype`. Must be valid key in\n`DTYPE_RANGE`.\n\nUse `range_values` as explicit min/max intensities.\n\nImage array after rescaling its intensity. This image is the same dtype as the\ninput image.\n\nSee also\n\nChanged in version 0.17: The dtype of the output array has changed to match\nthe output dtype, or float if the output range is specified by a pair of\nfloats.\n\nBy default, the min/max intensities of the input image are stretched to the\nlimits allowed by the image\u2019s dtype, since `in_range` defaults to \u2018image\u2019 and\n`out_range` defaults to \u2018dtype\u2019:\n\nIt\u2019s easy to accidentally convert an image dtype from uint8 to float:\n\nUse `rescale_intensity` to rescale to the proper range for float dtypes:\n\nTo maintain the low contrast of the original, use the `in_range` parameter:\n\nIf the min/max value of `in_range` is more/less than the min/max image\nintensity, then the intensity levels are clipped:\n\nIf you have an image with signed integers but want to rescale the image to\njust the positive range, use the `out_range` parameter. In that case, the\noutput dtype will be float:\n\nTo get the desired range with a specific dtype, use `.astype()`:\n\nIf the input image is constant, the output will be clipped directly to the\noutput range: >>> image = np.array([130, 130, 130], dtype=np.int32) >>>\nrescale_intensity(image, out_range=(0, 127)).astype(np.int32) array([127, 127,\n127], dtype=int32)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature", "path": "api/skimage.feature", "type": "feature", "text": "\n`skimage.feature.blob_dog`(image[, min_sigma, \u2026])\n\nFinds blobs in the given grayscale image.\n\n`skimage.feature.blob_doh`(image[, min_sigma, \u2026])\n\nFinds blobs in the given grayscale image.\n\n`skimage.feature.blob_log`(image[, min_sigma, \u2026])\n\nFinds blobs in the given grayscale image.\n\n`skimage.feature.canny`(image[, sigma, \u2026])\n\nEdge filter an image using the Canny algorithm.\n\n`skimage.feature.corner_fast`(image[, n, \u2026])\n\nExtract FAST corners for a given image.\n\n`skimage.feature.corner_foerstner`(image[, sigma])\n\nCompute Foerstner corner measure response image.\n\n`skimage.feature.corner_harris`(image[, \u2026])\n\nCompute Harris corner measure response image.\n\n`skimage.feature.corner_kitchen_rosenfeld`(image)\n\nCompute Kitchen and Rosenfeld corner measure response image.\n\n`skimage.feature.corner_moravec`(image[, \u2026])\n\nCompute Moravec corner measure response image.\n\n`skimage.feature.corner_orientations`(image, \u2026)\n\nCompute the orientation of corners.\n\n`skimage.feature.corner_peaks`(image[, \u2026])\n\nFind peaks in corner measure response image.\n\n`skimage.feature.corner_shi_tomasi`(image[, sigma])\n\nCompute Shi-Tomasi (Kanade-Tomasi) corner measure response image.\n\n`skimage.feature.corner_subpix`(image, corners)\n\nDetermine subpixel position of corners.\n\n`skimage.feature.daisy`(image[, step, radius, \u2026])\n\nExtract DAISY feature descriptors densely for the given image.\n\n`skimage.feature.draw_haar_like_feature`(\u2026)\n\nVisualization of Haar-like features.\n\n`skimage.feature.draw_multiblock_lbp`(image, \u2026)\n\nMulti-block local binary pattern visualization.\n\n`skimage.feature.greycomatrix`(image, \u2026[, \u2026])\n\nCalculate the grey-level co-occurrence matrix.\n\n`skimage.feature.greycoprops`(P[, prop])\n\nCalculate texture properties of a GLCM.\n\n`skimage.feature.haar_like_feature`(int_image, \u2026)\n\nCompute the Haar-like features for a region of interest (ROI) of an integral\nimage.\n\n`skimage.feature.haar_like_feature_coord`(\u2026)\n\nCompute the coordinates of Haar-like features.\n\n`skimage.feature.hessian_matrix`(image[, \u2026])\n\nCompute Hessian matrix.\n\n`skimage.feature.hessian_matrix_det`(image[, \u2026])\n\nCompute the approximate Hessian Determinant over an image.\n\n`skimage.feature.hessian_matrix_eigvals`(H_elems)\n\nCompute eigenvalues of Hessian matrix.\n\n`skimage.feature.hog`(image[, orientations, \u2026])\n\nExtract Histogram of Oriented Gradients (HOG) for a given image.\n\n`skimage.feature.local_binary_pattern`(image, P, R)\n\nGray scale and rotation invariant LBP (Local Binary Patterns).\n\n`skimage.feature.masked_register_translation`(\u2026)\n\nDeprecated function.\n\n`skimage.feature.match_descriptors`(\u2026[, \u2026])\n\nBrute-force matching of descriptors.\n\n`skimage.feature.match_template`(image, template)\n\nMatch a template to a 2-D or 3-D image using normalized correlation.\n\n`skimage.feature.multiblock_lbp`(int_image, r, \u2026)\n\nMulti-block local binary pattern (MB-LBP).\n\n`skimage.feature.multiscale_basic_features`(image)\n\nLocal features for a single- or multi-channel nd image.\n\n`skimage.feature.peak_local_max`(image[, \u2026])\n\nFind peaks in an image as coordinate list or boolean mask.\n\n`skimage.feature.plot_matches`(ax, image1, \u2026)\n\nPlot matched features.\n\n`skimage.feature.register_translation`(\u2026[, \u2026])\n\nDeprecated function.\n\n`skimage.feature.shape_index`(image[, sigma, \u2026])\n\nCompute the shape index.\n\n`skimage.feature.structure_tensor`(image[, \u2026])\n\nCompute structure tensor using sum of squared differences.\n\n`skimage.feature.structure_tensor_eigenvalues`(A_elems)\n\nCompute eigenvalues of structure tensor.\n\n`skimage.feature.structure_tensor_eigvals`(\u2026)\n\nCompute eigenvalues of structure tensor.\n\n`skimage.feature.BRIEF`([descriptor_size, \u2026])\n\nBRIEF binary descriptor extractor.\n\n`skimage.feature.CENSURE`([min_scale, \u2026])\n\nCENSURE keypoint detector.\n\n`skimage.feature.Cascade`\n\nClass for cascade of classifiers that is used for object detection.\n\n`skimage.feature.ORB`([downscale, n_scales, \u2026])\n\nOriented FAST and rotated BRIEF feature detector and binary descriptor\nextractor.\n\nFinds blobs in the given grayscale image.\n\nBlobs are found using the Difference of Gaussian (DoG) method [1]. For each\nblob found, the method returns its coordinates and the standard deviation of\nthe Gaussian kernel that detected the blob.\n\nInput grayscale image, blobs are assumed to be light on dark background (white\non black).\n\nThe minimum standard deviation for Gaussian kernel. Keep this low to detect\nsmaller blobs. The standard deviations of the Gaussian filter are given for\neach axis as a sequence, or as a single number, in which case it is equal for\nall axes.\n\nThe maximum standard deviation for Gaussian kernel. Keep this high to detect\nlarger blobs. The standard deviations of the Gaussian filter are given for\neach axis as a sequence, or as a single number, in which case it is equal for\nall axes.\n\nThe ratio between the standard deviation of Gaussian Kernels used for\ncomputing the Difference of Gaussians\n\nThe absolute lower bound for scale space maxima. Local maxima smaller than\nthresh are ignored. Reduce this to detect blobs with less intensities.\n\nA value between 0 and 1. If the area of two blobs overlaps by a fraction\ngreater than `threshold`, the smaller blob is eliminated.\n\nIf tuple of ints, the length of the tuple must match the input array\u2019s\ndimensionality. Each element of the tuple will exclude peaks from within\n`exclude_border`-pixels of the border of the image along that dimension. If\nnonzero int, `exclude_border` excludes peaks from within\n`exclude_border`-pixels of the border of the image. If zero or False, peaks\nare identified regardless of their distance from the border.\n\nA 2d array with each row representing 2 coordinate values for a 2D image, and\n3 coordinate values for a 3D image, plus the sigma(s) used. When a single\nsigma is passed, outputs are: `(r, c, sigma)` or `(p, r, c, sigma)` where `(r,\nc)` or `(p, r, c)` are coordinates of the blob and `sigma` is the standard\ndeviation of the Gaussian kernel which detected the blob. When an anisotropic\ngaussian is used (sigmas per dimension), the detected sigma is returned for\neach dimension.\n\nSee also\n\nThe radius of each blob is approximately \\\\(\\sqrt{2}\\sigma\\\\) for a 2-D image\nand \\\\(\\sqrt{3}\\sigma\\\\) for a 3-D image.\n\nhttps://en.wikipedia.org/wiki/Blob_detection#The_difference_of_Gaussians_approach\n\nFinds blobs in the given grayscale image.\n\nBlobs are found using the Determinant of Hessian method [1]. For each blob\nfound, the method returns its coordinates and the standard deviation of the\nGaussian Kernel used for the Hessian matrix whose determinant detected the\nblob. Determinant of Hessians is approximated using [2].\n\nInput grayscale image.Blobs can either be light on dark or vice versa.\n\nThe minimum standard deviation for Gaussian Kernel used to compute Hessian\nmatrix. Keep this low to detect smaller blobs.\n\nThe maximum standard deviation for Gaussian Kernel used to compute Hessian\nmatrix. Keep this high to detect larger blobs.\n\nThe number of intermediate values of standard deviations to consider between\n`min_sigma` and `max_sigma`.\n\nThe absolute lower bound for scale space maxima. Local maxima smaller than\nthresh are ignored. Reduce this to detect less prominent blobs.\n\nA value between 0 and 1. If the area of two blobs overlaps by a fraction\ngreater than `threshold`, the smaller blob is eliminated.\n\nIf set intermediate values of standard deviations are interpolated using a\nlogarithmic scale to the base `10`. If not, linear interpolation is used.\n\nA 2d array with each row representing 3 values, `(y,x,sigma)` where `(y,x)`\nare coordinates of the blob and `sigma` is the standard deviation of the\nGaussian kernel of the Hessian Matrix whose determinant detected the blob.\n\nThe radius of each blob is approximately `sigma`. Computation of Determinant\nof Hessians is independent of the standard deviation. Therefore detecting\nlarger blobs won\u2019t take more time. In methods line `blob_dog()` and\n`blob_log()` the computation of Gaussians for larger `sigma` takes more time.\nThe downside is that this method can\u2019t be used for detecting blobs of radius\nless than `3px` due to the box filters used in the approximation of Hessian\nDeterminant.\n\nhttps://en.wikipedia.org/wiki/Blob_detection#The_determinant_of_the_Hessian\n\nHerbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool, \u201cSURF: Speeded Up\nRobust Features\u201d\nftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf\n\nFinds blobs in the given grayscale image.\n\nBlobs are found using the Laplacian of Gaussian (LoG) method [1]. For each\nblob found, the method returns its coordinates and the standard deviation of\nthe Gaussian kernel that detected the blob.\n\nInput grayscale image, blobs are assumed to be light on dark background (white\non black).\n\nthe minimum standard deviation for Gaussian kernel. Keep this low to detect\nsmaller blobs. The standard deviations of the Gaussian filter are given for\neach axis as a sequence, or as a single number, in which case it is equal for\nall axes.\n\nThe maximum standard deviation for Gaussian kernel. Keep this high to detect\nlarger blobs. The standard deviations of the Gaussian filter are given for\neach axis as a sequence, or as a single number, in which case it is equal for\nall axes.\n\nThe number of intermediate values of standard deviations to consider between\n`min_sigma` and `max_sigma`.\n\nThe absolute lower bound for scale space maxima. Local maxima smaller than\nthresh are ignored. Reduce this to detect blobs with less intensities.\n\nA value between 0 and 1. If the area of two blobs overlaps by a fraction\ngreater than `threshold`, the smaller blob is eliminated.\n\nIf set intermediate values of standard deviations are interpolated using a\nlogarithmic scale to the base `10`. If not, linear interpolation is used.\n\nIf tuple of ints, the length of the tuple must match the input array\u2019s\ndimensionality. Each element of the tuple will exclude peaks from within\n`exclude_border`-pixels of the border of the image along that dimension. If\nnonzero int, `exclude_border` excludes peaks from within\n`exclude_border`-pixels of the border of the image. If zero or False, peaks\nare identified regardless of their distance from the border.\n\nA 2d array with each row representing 2 coordinate values for a 2D image, and\n3 coordinate values for a 3D image, plus the sigma(s) used. When a single\nsigma is passed, outputs are: `(r, c, sigma)` or `(p, r, c, sigma)` where `(r,\nc)` or `(p, r, c)` are coordinates of the blob and `sigma` is the standard\ndeviation of the Gaussian kernel which detected the blob. When an anisotropic\ngaussian is used (sigmas per dimension), the detected sigma is returned for\neach dimension.\n\nThe radius of each blob is approximately \\\\(\\sqrt{2}\\sigma\\\\) for a 2-D image\nand \\\\(\\sqrt{3}\\sigma\\\\) for a 3-D image.\n\nhttps://en.wikipedia.org/wiki/Blob_detection#The_Laplacian_of_Gaussian\n\nEdge filter an image using the Canny algorithm.\n\nGrayscale input image to detect edges on; can be of any dtype.\n\nStandard deviation of the Gaussian filter.\n\nLower bound for hysteresis thresholding (linking edges). If None,\nlow_threshold is set to 10% of dtype\u2019s max.\n\nUpper bound for hysteresis thresholding (linking edges). If None,\nhigh_threshold is set to 20% of dtype\u2019s max.\n\nMask to limit the application of Canny to a certain area.\n\nIf True then treat low_threshold and high_threshold as quantiles of the edge\nmagnitude image, rather than absolute edge magnitude values. If True then the\nthresholds must be in the range [0, 1].\n\nThe binary edge map.\n\nSee also\n\nThe steps of the algorithm are as follows:\n\nCanny, J., A Computational Approach To Edge Detection, IEEE Trans. Pattern\nAnalysis and Machine Intelligence, 8:679-714, 1986\nDOI:10.1109/TPAMI.1986.4767851\n\nWilliam Green\u2019s Canny tutorial\nhttps://en.wikipedia.org/wiki/Canny_edge_detector\n\nExtract FAST corners for a given image.\n\nInput image.\n\nMinimum number of consecutive pixels out of 16 pixels on the circle that\nshould all be either brighter or darker w.r.t testpixel. A point c on the\ncircle is darker w.r.t test pixel p if `Ic < Ip - threshold` and brighter if\n`Ic > Ip + threshold`. Also stands for the n in `FAST-n` corner detector.\n\nThreshold used in deciding whether the pixels on the circle are brighter,\ndarker or similar w.r.t. the test pixel. Decrease the threshold when more\ncorners are desired and vice-versa.\n\nFAST corner response image.\n\nRosten, E., & Drummond, T. (2006, May). Machine learning for high-speed corner\ndetection. In European conference on computer vision (pp. 430-443). Springer,\nBerlin, Heidelberg. DOI:10.1007/11744023_34\nhttp://www.edwardrosten.com/work/rosten_2006_machine.pdf\n\nWikipedia, \u201cFeatures from accelerated segment test\u201d,\nhttps://en.wikipedia.org/wiki/Features_from_accelerated_segment_test\n\nCompute Foerstner corner measure response image.\n\nThis corner detector uses information from the auto-correlation matrix A:\n\nWhere imx and imy are first derivatives, averaged with a gaussian filter. The\ncorner measure is then defined as:\n\nInput image.\n\nStandard deviation used for the Gaussian kernel, which is used as weighting\nfunction for the auto-correlation matrix.\n\nError ellipse sizes.\n\nRoundness of error ellipse.\n\nF\u00f6rstner, W., & G\u00fclch, E. (1987, June). A fast operator for detection and\nprecise location of distinct points, corners and centres of circular features.\nIn Proc. ISPRS intercommission conference on fast processing of\nphotogrammetric data (pp. 281-305).\nhttps://cseweb.ucsd.edu/classes/sp02/cse252/foerstner/foerstner.pdf\n\nhttps://en.wikipedia.org/wiki/Corner_detection\n\nCompute Harris corner measure response image.\n\nThis corner detector uses information from the auto-correlation matrix A:\n\nWhere imx and imy are first derivatives, averaged with a gaussian filter. The\ncorner measure is then defined as:\n\nor:\n\nInput image.\n\nMethod to compute the response image from the auto-correlation matrix.\n\nSensitivity factor to separate corners from edges, typically in range `[0,\n0.2]`. Small values of k result in detection of sharp corners.\n\nNormalisation factor (Noble\u2019s corner measure).\n\nStandard deviation used for the Gaussian kernel, which is used as weighting\nfunction for the auto-correlation matrix.\n\nHarris response image.\n\nhttps://en.wikipedia.org/wiki/Corner_detection\n\nCompute Kitchen and Rosenfeld corner measure response image.\n\nThe corner measure is calculated as follows:\n\nWhere imx and imy are the first and imxx, imxy, imyy the second derivatives.\n\nInput image.\n\nHow to handle values outside the image borders.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nKitchen and Rosenfeld response image.\n\nKitchen, L., & Rosenfeld, A. (1982). Gray-level corner detection. Pattern\nrecognition letters, 1(2), 95-102. DOI:10.1016/0167-8655(82)90020-4\n\nCompute Moravec corner measure response image.\n\nThis is one of the simplest corner detectors and is comparatively fast but has\nseveral limitations (e.g. not rotation invariant).\n\nInput image.\n\nWindow size.\n\nMoravec response image.\n\nhttps://en.wikipedia.org/wiki/Corner_detection\n\nCompute the orientation of corners.\n\nThe orientation of corners is computed using the first order central moment\ni.e. the center of mass approach. The corner orientation is the angle of the\nvector from the corner coordinate to the intensity centroid in the local\nneighborhood around the corner calculated using first order central moment.\n\nInput grayscale image.\n\nCorner coordinates as `(row, col)`.\n\nMask defining the local neighborhood of the corner used for the calculation of\nthe central moment.\n\nOrientations of corners in the range [-pi, pi].\n\nEthan Rublee, Vincent Rabaud, Kurt Konolige and Gary Bradski \u201cORB : An\nefficient alternative to SIFT and SURF\u201d\nhttp://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf\n\nPaul L. Rosin, \u201cMeasuring Corner Properties\u201d\nhttp://users.cs.cf.ac.uk/Paul.Rosin/corner2.pdf\n\nFind peaks in corner measure response image.\n\nThis differs from `skimage.feature.peak_local_max` in that it suppresses\nmultiple connected peaks with the same accumulator value.\n\nInput image.\n\nThe minimal allowed distance separating peaks.\n\nSee `skimage.feature.peak_local_max()`.\n\nWhich Minkowski p-norm to use. Should be in the range [1, inf]. A finite large\np may cause a ValueError if overflow can occur. `inf` corresponds to the\nChebyshev distance and 2 to the Euclidean distance.\n\nSee also\n\nChanged in version 0.18: The default value of `threshold_rel` has changed to\nNone, which corresponds to letting `skimage.feature.peak_local_max` decide on\nthe default. This is equivalent to `threshold_rel=0`.\n\nThe `num_peaks` limit is applied before suppression of connected peaks. To\nlimit the number of peaks after suppression, set `num_peaks=np.inf` and post-\nprocess the output of this function.\n\nCompute Shi-Tomasi (Kanade-Tomasi) corner measure response image.\n\nThis corner detector uses information from the auto-correlation matrix A:\n\nWhere imx and imy are first derivatives, averaged with a gaussian filter. The\ncorner measure is then defined as the smaller eigenvalue of A:\n\nInput image.\n\nStandard deviation used for the Gaussian kernel, which is used as weighting\nfunction for the auto-correlation matrix.\n\nShi-Tomasi response image.\n\nhttps://en.wikipedia.org/wiki/Corner_detection\n\nDetermine subpixel position of corners.\n\nA statistical test decides whether the corner is defined as the intersection\nof two edges or a single peak. Depending on the classification result, the\nsubpixel corner location is determined based on the local covariance of the\ngrey-values. If the significance level for either statistical test is not\nsufficient, the corner cannot be classified, and the output subpixel position\nis set to NaN.\n\nInput image.\n\nCorner coordinates `(row, col)`.\n\nSearch window size for subpixel estimation.\n\nSignificance level for corner classification.\n\nSubpixel corner positions. NaN for \u201cnot classified\u201d corners.\n\nF\u00f6rstner, W., & G\u00fclch, E. (1987, June). A fast operator for detection and\nprecise location of distinct points, corners and centres of circular features.\nIn Proc. ISPRS intercommission conference on fast processing of\nphotogrammetric data (pp. 281-305).\nhttps://cseweb.ucsd.edu/classes/sp02/cse252/foerstner/foerstner.pdf\n\nhttps://en.wikipedia.org/wiki/Corner_detection\n\nExtract DAISY feature descriptors densely for the given image.\n\nDAISY is a feature descriptor similar to SIFT formulated in a way that allows\nfor fast dense extraction. Typically, this is practical for bag-of-features\nimage representations.\n\nThe implementation follows Tola et al. [1] but deviate on the following\npoints:\n\nInput image (grayscale).\n\nDistance between descriptor sampling points.\n\nRadius (in pixels) of the outermost ring.\n\nNumber of rings.\n\nNumber of histograms sampled per ring.\n\nNumber of orientations (bins) per histogram.\n\nHow to normalize the descriptors\n\nStandard deviation of spatial Gaussian smoothing for the center histogram and\nfor each ring of histograms. The array of sigmas should be sorted from the\ncenter and out. I.e. the first sigma value defines the spatial smoothing of\nthe center histogram and the last sigma value defines the spatial smoothing of\nthe outermost ring. Specifying sigmas overrides the following parameter.\n\n`rings = len(sigmas) - 1`\n\nRadius (in pixels) for each ring. Specifying ring_radii overrides the\nfollowing two parameters.\n\n`rings = len(ring_radii)` `radius = ring_radii[-1]`\n\nIf both sigmas and ring_radii are given, they must satisfy the following\npredicate since no radius is needed for the center histogram.\n\n`len(ring_radii) == len(sigmas) + 1`\n\nGenerate a visualization of the DAISY descriptors\n\nGrid of DAISY descriptors for the given image as an array dimensionality (P,\nQ, R) where\n\n`P = ceil((M - radius*2) / step)` `Q = ceil((N - radius*2) / step)` `R =\n(rings * histograms + 1) * orientations`\n\nVisualization of the DAISY descriptors.\n\nTola et al. \u201cDaisy: An efficient dense descriptor applied to wide- baseline\nstereo.\u201d Pattern Analysis and Machine Intelligence, IEEE Transactions on 32.5\n(2010): 815-830.\n\nhttp://cvlab.epfl.ch/software/daisy\n\nVisualization of Haar-like features.\n\nThe region of an integral image for which the features need to be computed.\n\nRow-coordinate of top left corner of the detection window.\n\nColumn-coordinate of top left corner of the detection window.\n\nWidth of the detection window.\n\nHeight of the detection window.\n\nThe array of coordinates to be extracted. This is useful when you want to\nrecompute only a subset of features. In this case `feature_type` needs to be\nan array containing the type of each feature, as returned by\n`haar_like_feature_coord()`. By default, all coordinates are computed.\n\nFloats specifying the color for the positive block. Corresponding values\ndefine (R, G, B) values. Default value is red (1, 0, 0).\n\nFloats specifying the color for the negative block Corresponding values define\n(R, G, B) values. Default value is blue (0, 1, 0).\n\nValue in the range [0, 1] that specifies opacity of visualization. 1 - fully\ntransparent, 0 - opaque.\n\nThe maximum number of features to be returned. By default, all features are\nreturned.\n\nIf int, random_state is the seed used by the random number generator; If\nRandomState instance, random_state is the random number generator; If None,\nthe random number generator is the RandomState instance used by `np.random`.\nThe random state is used when generating a set of features smaller than the\ntotal number of available features.\n\nAn image in which the different features will be added.\n\nMulti-block local binary pattern visualization.\n\nBlocks with higher sums are colored with alpha-blended white rectangles,\nwhereas blocks with lower sums are colored alpha-blended cyan. Colors and the\n`alpha` parameter can be changed.\n\nImage on which to visualize the pattern.\n\nRow-coordinate of top left corner of a rectangle containing feature.\n\nColumn-coordinate of top left corner of a rectangle containing feature.\n\nWidth of one of 9 equal rectangles that will be used to compute a feature.\n\nHeight of one of 9 equal rectangles that will be used to compute a feature.\n\nThe descriptor of feature to visualize. If not provided, the descriptor with 0\nvalue will be used.\n\nFloats specifying the color for the block that has greater intensity value.\nThey should be in the range [0, 1]. Corresponding values define (R, G, B)\nvalues. Default value is white (1, 1, 1).\n\nFloats specifying the color for the block that has greater intensity value.\nThey should be in the range [0, 1]. Corresponding values define (R, G, B)\nvalues. Default value is cyan (0, 0.69, 0.96).\n\nValue in the range [0, 1] that specifies opacity of visualization. 1 - fully\ntransparent, 0 - opaque.\n\nImage with MB-LBP visualization.\n\nFace Detection Based on Multi-Block LBP Representation. Lun Zhang, Rufeng Chu,\nShiming Xiang, Shengcai Liao, Stan Z. Li\nhttp://www.cbsr.ia.ac.cn/users/scliao/papers/Zhang-ICB07-MBLBP.pdf\n\nCalculate the grey-level co-occurrence matrix.\n\nA grey level co-occurrence matrix is a histogram of co-occurring greyscale\nvalues at a given offset over an image.\n\nInteger typed input image. Only positive valued images are supported. If type\nis other than uint8, the argument `levels` needs to be set.\n\nList of pixel pair distance offsets.\n\nList of pixel pair angles in radians.\n\nThe input image should contain integers in [0, `levels`-1], where levels\nindicate the number of grey-levels counted (typically 256 for an 8-bit image).\nThis argument is required for 16-bit images or higher and is typically the\nmaximum of the image. As the output matrix is at least `levels` x `levels`, it\nmight be preferable to use binning of the input image rather than large values\nfor `levels`.\n\nIf True, the output matrix `P[:, :, d, theta]` is symmetric. This is\naccomplished by ignoring the order of value pairs, so both (i, j) and (j, i)\nare accumulated when (i, j) is encountered for a given offset. The default is\nFalse.\n\nIf True, normalize each matrix `P[:, :, d, theta]` by dividing by the total\nnumber of accumulated co-occurrences for the given offset. The elements of the\nresulting matrix sum to 1. The default is False.\n\nThe grey-level co-occurrence histogram. The value `P[i,j,d,theta]` is the\nnumber of times that grey-level `j` occurs at a distance `d` and at an angle\n`theta` from grey-level `i`. If `normed` is `False`, the output is of type\nuint32, otherwise it is float64. The dimensions are: levels x levels x number\nof distances x number of angles.\n\nThe GLCM Tutorial Home Page, http://www.fp.ucalgary.ca/mhallbey/tutorial.htm\n\nHaralick, RM.; Shanmugam, K., \u201cTextural features for image classification\u201d\nIEEE Transactions on systems, man, and cybernetics 6 (1973): 610-621.\nDOI:10.1109/TSMC.1973.4309314\n\nPattern Recognition Engineering, Morton Nadler & Eric P. Smith\n\nWikipedia, https://en.wikipedia.org/wiki/Co-occurrence_matrix\n\nCompute 2 GLCMs: One for a 1-pixel offset to the right, and one for a 1-pixel\noffset upwards.\n\nGLCM Texture Features\n\nCalculate texture properties of a GLCM.\n\nCompute a feature of a grey level co-occurrence matrix to serve as a compact\nsummary of the matrix. The properties are computed as follows:\n\nEach GLCM is normalized to have a sum of 1 before the computation of texture\nproperties.\n\nInput array. `P` is the grey-level co-occurrence histogram for which to\ncompute the specified property. The value `P[i,j,d,theta]` is the number of\ntimes that grey-level j occurs at a distance d and at an angle theta from\ngrey-level i.\n\nThe property of the GLCM to compute. The default is \u2018contrast\u2019.\n\n2-dimensional array. `results[d, a]` is the property \u2018prop\u2019 for the d\u2019th\ndistance and the a\u2019th angle.\n\nThe GLCM Tutorial Home Page, http://www.fp.ucalgary.ca/mhallbey/tutorial.htm\n\nCompute the contrast for GLCMs with distances [1, 2] and angles [0 degrees, 90\ndegrees]\n\nGLCM Texture Features\n\nCompute the Haar-like features for a region of interest (ROI) of an integral\nimage.\n\nHaar-like features have been successfully used for image classification and\nobject detection [1]. It has been used for real-time face detection algorithm\nproposed in [2].\n\nIntegral image for which the features need to be computed.\n\nRow-coordinate of top left corner of the detection window.\n\nColumn-coordinate of top left corner of the detection window.\n\nWidth of the detection window.\n\nHeight of the detection window.\n\nThe type of feature to consider:\n\nBy default all features are extracted.\n\nIf using with `feature_coord`, it should correspond to the feature type of\neach associated coordinate feature.\n\nThe array of coordinates to be extracted. This is useful when you want to\nrecompute only a subset of features. In this case `feature_type` needs to be\nan array containing the type of each feature, as returned by\n`haar_like_feature_coord()`. By default, all coordinates are computed.\n\nResulting Haar-like features. Each value is equal to the subtraction of sums\nof the positive and negative rectangles. The data type depends of the data\ntype of `int_image`: `int` when the data type of `int_image` is `uint` or\n`int` and `float` when the data type of `int_image` is `float`.\n\nWhen extracting those features in parallel, be aware that the choice of the\nbackend (i.e. multiprocessing vs threading) will have an impact on the\nperformance. The rule of thumb is as follows: use multiprocessing when\nextracting features for all possible ROI in an image; use threading when\nextracting the feature at specific location for a limited number of ROIs.\nRefer to the example Face classification using Haar-like feature descriptor\nfor more insights.\n\nhttps://en.wikipedia.org/wiki/Haar-like_feature\n\nOren, M., Papageorgiou, C., Sinha, P., Osuna, E., & Poggio, T. (1997, June).\nPedestrian detection using wavelet templates. In Computer Vision and Pattern\nRecognition, 1997. Proceedings., 1997 IEEE Computer Society Conference on (pp.\n193-199). IEEE. http://tinyurl.com/y6ulxfta DOI:10.1109/CVPR.1997.609319\n\nViola, Paul, and Michael J. Jones. \u201cRobust real-time face detection.\u201d\nInternational journal of computer vision 57.2 (2004): 137-154.\nhttps://www.merl.com/publications/docs/TR2004-043.pdf\nDOI:10.1109/CVPR.2001.990517\n\nYou can compute the feature for some pre-computed coordinates.\n\nCompute the coordinates of Haar-like features.\n\nWidth of the detection window.\n\nHeight of the detection window.\n\nThe type of feature to consider:\n\nBy default all features are extracted.\n\nCoordinates of the rectangles for each feature.\n\nThe corresponding type for each feature.\n\nCompute Hessian matrix.\n\nThe Hessian matrix is defined as:\n\nwhich is computed by convolving the image with the second derivatives of the\nGaussian kernel in the respective r- and c-directions.\n\nInput image.\n\nStandard deviation used for the Gaussian kernel, which is used as weighting\nfunction for the auto-correlation matrix.\n\nHow to handle values outside the image borders.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nThis parameter allows for the use of reverse or forward order of the image\naxes in gradient computation. \u2018rc\u2019 indicates the use of the first axis\ninitially (Hrr, Hrc, Hcc), whilst \u2018xy\u2019 indicates the usage of the last axis\ninitially (Hxx, Hxy, Hyy)\n\nElement of the Hessian matrix for each pixel in the input image.\n\nElement of the Hessian matrix for each pixel in the input image.\n\nElement of the Hessian matrix for each pixel in the input image.\n\nCompute the approximate Hessian Determinant over an image.\n\nThe 2D approximate method uses box filters over integral images to compute the\napproximate Hessian Determinant, as described in [1].\n\nThe image over which to compute Hessian Determinant.\n\nStandard deviation used for the Gaussian kernel, used for the Hessian matrix.\n\nIf `True` and the image is 2D, use a much faster approximate computation. This\nargument has no effect on 3D and higher images.\n\nThe array of the Determinant of Hessians.\n\nFor 2D images when `approximate=True`, the running time of this method only\ndepends on size of the image. It is independent of `sigma` as one would\nexpect. The downside is that the result for `sigma` less than `3` is not\naccurate, i.e., not similar to the result obtained if someone computed the\nHessian and took its determinant.\n\nHerbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool, \u201cSURF: Speeded Up\nRobust Features\u201d\nftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf\n\nCompute eigenvalues of Hessian matrix.\n\nThe upper-diagonal elements of the Hessian matrix, as returned by\n`hessian_matrix`.\n\nThe eigenvalues of the Hessian matrix, in decreasing order. The eigenvalues\nare the leading dimension. That is, `eigs[i, j, k]` contains the ith-largest\neigenvalue at position (j, k).\n\nExtract Histogram of Oriented Gradients (HOG) for a given image.\n\nCompute a Histogram of Oriented Gradients (HOG) by\n\nInput image.\n\nNumber of orientation bins.\n\nSize (in pixels) of a cell.\n\nNumber of cells in each block.\n\nBlock normalization method:\n\nNormalization using L1-norm.\n\nNormalization using L1-norm, followed by square root.\n\nNormalization using L2-norm.\n\nNormalization using L2-norm, followed by limiting the maximum values to 0.2\n(`Hys` stands for `hysteresis`) and renormalization using L2-norm. (default)\nFor details, see [3], [4].\n\nAlso return an image of the HOG. For each cell and orientation bin, the image\ncontains a line segment that is centered at the cell center, is perpendicular\nto the midpoint of the range of angles spanned by the orientation bin, and has\nintensity proportional to the corresponding histogram value.\n\nApply power law compression to normalize the image before processing. DO NOT\nuse this if the image contains negative values. Also see `notes` section\nbelow.\n\nReturn the data as a feature vector by calling .ravel() on the result just\nbefore returning.\n\nIf True, the last `image` dimension is considered as a color channel,\notherwise as spatial.\n\nHOG descriptor for the image. If `feature_vector` is True, a 1D (flattened)\narray is returned.\n\nA visualisation of the HOG image. Only provided if `visualize` is True.\n\nThe presented code implements the HOG extraction method from [2] with the\nfollowing changes: (I) blocks of (3, 3) cells are used ((2, 2) in the paper);\n(II) no smoothing within cells (Gaussian spatial window with sigma=8pix in the\npaper); (III) L1 block normalization is used (L2-Hys in the paper).\n\nPower law compression, also known as Gamma correction, is used to reduce the\neffects of shadowing and illumination variations. The compression makes the\ndark regions lighter. When the kwarg `transform_sqrt` is set to `True`, the\nfunction computes the square root of each color channel and then applies the\nhog algorithm to the image.\n\nhttps://en.wikipedia.org/wiki/Histogram_of_oriented_gradients\n\nDalal, N and Triggs, B, Histograms of Oriented Gradients for Human Detection,\nIEEE Computer Society Conference on Computer Vision and Pattern Recognition\n2005 San Diego, CA, USA, https://lear.inrialpes.fr/people/triggs/pubs/Dalal-\ncvpr05.pdf, DOI:10.1109/CVPR.2005.177\n\nLowe, D.G., Distinctive image features from scale-invatiant keypoints,\nInternational Journal of Computer Vision (2004) 60: 91,\nhttp://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf,\nDOI:10.1023/B:VISI.0000029664.99615.94\n\nDalal, N, Finding People in Images and Videos, Human-Computer Interaction\n[cs.HC], Institut National Polytechnique de Grenoble - INPG, 2006,\nhttps://tel.archives-ouvertes.fr/tel-00390303/file/NavneetDalalThesis.pdf\n\nGray scale and rotation invariant LBP (Local Binary Patterns).\n\nLBP is an invariant descriptor that can be used for texture classification.\n\nGraylevel image.\n\nNumber of circularly symmetric neighbour set points (quantization of the\nangular space).\n\nRadius of circle (spatial resolution of the operator).\n\nMethod to determine the pattern.\n\nrotation invariant.\n\nrotation invariant.\n\nfiner quantization of the angular space which is gray scale and rotation\ninvariant.\n\nwhich is only gray scale invariant [2].\n\nimage texture which is rotation but not gray scale invariant.\n\nLBP image.\n\nMultiresolution Gray-Scale and Rotation Invariant Texture Classification with\nLocal Binary Patterns. Timo Ojala, Matti Pietikainen, Topi Maenpaa.\nhttp://www.ee.oulu.fi/research/mvmp/mvg/files/pdf/pdf_94.pdf, 2002.\n\nFace recognition with local binary patterns. Timo Ahonen, Abdenour Hadid,\nMatti Pietikainen,\nhttp://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.214.6851, 2004.\n\nDeprecated function. Use `skimage.registration.phase_cross_correlation`\ninstead.\n\nBrute-force matching of descriptors.\n\nFor each descriptor in the first set this matcher finds the closest descriptor\nin the second set (and vice-versa in the case of enabled cross-checking).\n\nDescriptors of size P about M keypoints in the first image.\n\nDescriptors of size P about N keypoints in the second image.\n\nThe metric to compute the distance between two descriptors. See\n`scipy.spatial.distance.cdist` for all possible types. The hamming distance\nshould be used for binary descriptors. By default the L2-norm is used for all\ndescriptors of dtype float or double and the Hamming distance is used for\nbinary descriptors automatically.\n\nThe p-norm to apply for `metric='minkowski'`.\n\nMaximum allowed distance between descriptors of two keypoints in separate\nimages to be regarded as a match.\n\nIf True, the matched keypoints are returned after cross checking i.e. a\nmatched pair (keypoint1, keypoint2) is returned if keypoint2 is the best match\nfor keypoint1 in second image and keypoint1 is the best match for keypoint2 in\nfirst image.\n\nMaximum ratio of distances between first and second closest descriptor in the\nsecond set of descriptors. This threshold is useful to filter ambiguous\nmatches between the two descriptor sets. The choice of this value depends on\nthe statistics of the chosen descriptor, e.g., for SIFT descriptors a value of\n0.8 is usually chosen, see D.G. Lowe, \u201cDistinctive Image Features from Scale-\nInvariant Keypoints\u201d, International Journal of Computer Vision, 2004.\n\nIndices of corresponding matches in first and second set of descriptors, where\n`matches[:, 0]` denote the indices in the first and `matches[:, 1]` the\nindices in the second set of descriptors.\n\nMatch a template to a 2-D or 3-D image using normalized correlation.\n\nThe output is an array with values between -1.0 and 1.0. The value at a given\nposition corresponds to the correlation coefficient between the image and the\ntemplate.\n\nFor `pad_input=True` matches correspond to the center and otherwise to the\ntop-left corner of the template. To find the best match you must search for\npeaks in the response (output) image.\n\n2-D or 3-D input image.\n\nTemplate to locate. It must be `(m <= M, n <= N[, d <= D])`.\n\nIf True, pad `image` so that output is the same size as the image, and output\nvalues correspond to the template center. Otherwise, the output is an array\nwith shape `(M - m + 1, N - n + 1)` for an `(M, N)` image and an `(m, n)`\ntemplate, and matches correspond to origin (top-left corner) of the template.\n\nPadding mode.\n\nConstant values used in conjunction with `mode='constant'`.\n\nResponse image with correlation coefficients.\n\nDetails on the cross-correlation are presented in [1]. This implementation\nuses FFT convolutions of the image and the template. Reference [2] presents\nsimilar derivations but the approximation presented in this reference is not\nused in our implementation.\n\nJ. P. Lewis, \u201cFast Normalized Cross-Correlation\u201d, Industrial Light and Magic.\n\nBriechle and Hanebeck, \u201cTemplate Matching using Fast Normalized Cross\nCorrelation\u201d, Proceedings of the SPIE (2001). DOI:10.1117/12.421129\n\nMulti-block local binary pattern (MB-LBP).\n\nThe features are calculated similarly to local binary patterns (LBPs), (See\n`local_binary_pattern()`) except that summed blocks are used instead of\nindividual pixel values.\n\nMB-LBP is an extension of LBP that can be computed on multiple scales in\nconstant time using the integral image. Nine equally-sized rectangles are used\nto compute a feature. For each rectangle, the sum of the pixel intensities is\ncomputed. Comparisons of these sums to that of the central rectangle determine\nthe feature, similarly to LBP.\n\nIntegral image.\n\nRow-coordinate of top left corner of a rectangle containing feature.\n\nColumn-coordinate of top left corner of a rectangle containing feature.\n\nWidth of one of the 9 equal rectangles that will be used to compute a feature.\n\nHeight of one of the 9 equal rectangles that will be used to compute a\nfeature.\n\n8-bit MB-LBP feature descriptor.\n\nFace Detection Based on Multi-Block LBP Representation. Lun Zhang, Rufeng Chu,\nShiming Xiang, Shengcai Liao, Stan Z. Li\nhttp://www.cbsr.ia.ac.cn/users/scliao/papers/Zhang-ICB07-MBLBP.pdf\n\nLocal features for a single- or multi-channel nd image.\n\nIntensity, gradient intensity and local structure are computed at different\nscales thanks to Gaussian blurring.\n\nInput image, which can be grayscale or multichannel.\n\nTrue if the last dimension corresponds to color channels.\n\nIf True, pixel intensities averaged over the different scales are added to the\nfeature set.\n\nIf True, intensities of local gradients averaged over the different scales are\nadded to the feature set.\n\nIf True, eigenvalues of the Hessian matrix after Gaussian blurring at\ndifferent scales are added to the feature set.\n\nSmallest value of the Gaussian kernel used to average local neighbourhoods\nbefore extracting features.\n\nLargest value of the Gaussian kernel used to average local neighbourhoods\nbefore extracting features.\n\nNumber of values of the Gaussian kernel between sigma_min and sigma_max. If\nNone, sigma_min multiplied by powers of 2 are used.\n\nThe number of parallel threads to use. If set to `None`, the full set of\navailable cores are used.\n\nArray of shape `image.shape + (n_features,)`\n\nTrainable segmentation using local features and random forests\n\nFind peaks in an image as coordinate list or boolean mask.\n\nPeaks are the local maxima in a region of `2 * min_distance + 1` (i.e. peaks\nare separated by at least `min_distance`).\n\nIf both `threshold_abs` and `threshold_rel` are provided, the maximum of the\ntwo is chosen as the minimum intensity threshold of peaks.\n\nChanged in version 0.18: Prior to version 0.18, peaks of the same height\nwithin a radius of `min_distance` were all returned, but this could cause\nunexpected behaviour. From 0.18 onwards, an arbitrary peak within the region\nis returned. See issue gh-2592.\n\nInput image.\n\nThe minimal allowed distance separating peaks. To find the maximum number of\npeaks, use `min_distance=1`.\n\nMinimum intensity of peaks. By default, the absolute threshold is the minimum\nintensity of the image.\n\nMinimum intensity of peaks, calculated as `max(image) * threshold_rel`.\n\nIf positive integer, `exclude_border` excludes peaks from within\n`exclude_border`-pixels of the border of the image. If tuple of non-negative\nints, the length of the tuple must match the input array\u2019s dimensionality.\nEach element of the tuple will exclude peaks from within\n`exclude_border`-pixels of the border of the image along that dimension. If\nTrue, takes the `min_distance` parameter as value. If zero or False, peaks are\nidentified regardless of their distance from the border.\n\nIf True, the output will be an array representing peak coordinates. The\ncoordinates are sorted according to peaks values (Larger first). If False, the\noutput will be a boolean array shaped as `image.shape` with peaks present at\nTrue elements. `indices` is deprecated and will be removed in version 0.20.\nDefault behavior will be to always return peak coordinates. You can obtain a\nmask as shown in the example below.\n\nMaximum number of peaks. When the number of peaks exceeds `num_peaks`, return\n`num_peaks` peaks based on highest peak intensity.\n\nIf provided, `footprint == 1` represents the local region within which to\nsearch for peaks at every point in `image`.\n\nIf provided, each unique region `labels == value` represents a unique region\nto search for peaks. Zero is reserved for background.\n\nMaximum number of peaks for each label.\n\nWhich Minkowski p-norm to use. Should be in the range [1, inf]. A finite large\np may cause a ValueError if overflow can occur. `inf` corresponds to the\nChebyshev distance and 2 to the Euclidean distance.\n\nSee also\n\nThe peak local maximum function returns the coordinates of local peaks\n(maxima) in an image. Internally, a maximum filter is used for finding local\nmaxima. This operation dilates the original image. After comparison of the\ndilated and original image, this function returns the coordinates or a mask of\nthe peaks where the dilated image equals the original image.\n\nFinding local maxima\n\nWatershed segmentation\n\nSegment human cells (in mitosis)\n\nPlot matched features.\n\nMatches and image are drawn in this ax.\n\nFirst grayscale or color image.\n\nSecond grayscale or color image.\n\nFirst keypoint coordinates as `(row, col)`.\n\nSecond keypoint coordinates as `(row, col)`.\n\nIndices of corresponding matches in first and second set of descriptors, where\n`matches[:, 0]` denote the indices in the first and `matches[:, 1]` the\nindices in the second set of descriptors.\n\nColor for keypoint locations.\n\nColor for lines which connect keypoint matches. By default the color is chosen\nrandomly.\n\nWhether to only plot matches and not plot the keypoint locations.\n\nWhether to show images side by side, `'horizontal'`, or one above the other,\n`'vertical'`.\n\nDeprecated function. Use `skimage.registration.phase_cross_correlation`\ninstead.\n\nCompute the shape index.\n\nThe shape index, as defined by Koenderink & van Doorn [1], is a single valued\nmeasure of local curvature, assuming the image as a 3D plane with intensities\nrepresenting heights.\n\nIt is derived from the eigen values of the Hessian, and its value ranges from\n-1 to 1 (and is undefined (=NaN) in flat regions), with following ranges\nrepresenting following shapes:\n\nInterval (s in \u2026)\n\nShape\n\n[ -1, -7/8)\n\nSpherical cup\n\n[-7/8, -5/8)\n\nThrough\n\n[-5/8, -3/8)\n\nRut\n\n[-3/8, -1/8)\n\nSaddle rut\n\n[-1/8, +1/8)\n\nSaddle\n\n[+1/8, +3/8)\n\nSaddle ridge\n\n[+3/8, +5/8)\n\nRidge\n\n[+5/8, +7/8)\n\nDome\n\n[+7/8, +1]\n\nSpherical cap\n\nInput image.\n\nStandard deviation used for the Gaussian kernel, which is used for smoothing\nthe input data before Hessian eigen value calculation.\n\nHow to handle values outside the image borders\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nShape index\n\nKoenderink, J. J. & van Doorn, A. J., \u201cSurface shape and curvature scales\u201d,\nImage and Vision Computing, 1992, 10, 557-564.\nDOI:10.1016/0262-8856(92)90076-F\n\nCompute structure tensor using sum of squared differences.\n\nThe (2-dimensional) structure tensor A is defined as:\n\nwhich is approximated by the weighted sum of squared differences in a local\nwindow around each pixel in the image. This formula can be extended to a\nlarger number of dimensions (see [1]).\n\nInput image.\n\nStandard deviation used for the Gaussian kernel, which is used as a weighting\nfunction for the local summation of squared differences.\n\nHow to handle values outside the image borders.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nNOTE: Only applies in 2D. Higher dimensions must always use \u2018rc\u2019 order. This\nparameter allows for the use of reverse or forward order of the image axes in\ngradient computation. \u2018rc\u2019 indicates the use of the first axis initially (Arr,\nArc, Acc), whilst \u2018xy\u2019 indicates the usage of the last axis initially (Axx,\nAxy, Ayy).\n\nUpper-diagonal elements of the structure tensor for each pixel in the input\nimage.\n\nSee also\n\nhttps://en.wikipedia.org/wiki/Structure_tensor\n\nCompute eigenvalues of structure tensor.\n\nThe upper-diagonal elements of the structure tensor, as returned by\n`structure_tensor`.\n\nThe eigenvalues of the structure tensor, in decreasing order. The eigenvalues\nare the leading dimension. That is, the coordinate [i, j, k] corresponds to\nthe ith-largest eigenvalue at position (j, k).\n\nSee also\n\nCompute eigenvalues of structure tensor.\n\nElement of the structure tensor for each pixel in the input image.\n\nElement of the structure tensor for each pixel in the input image.\n\nElement of the structure tensor for each pixel in the input image.\n\nLarger eigen value for each input matrix.\n\nSmaller eigen value for each input matrix.\n\nBases: `skimage.feature.util.DescriptorExtractor`\n\nBRIEF binary descriptor extractor.\n\nBRIEF (Binary Robust Independent Elementary Features) is an efficient feature\npoint descriptor. It is highly discriminative even when using relatively few\nbits and is computed using simple intensity difference tests.\n\nFor each keypoint, intensity comparisons are carried out for a specifically\ndistributed number N of pixel-pairs resulting in a binary descriptor of length\nN. For binary descriptors the Hamming distance can be used for feature\nmatching, which leads to lower computational cost in comparison to the L2\nnorm.\n\nSize of BRIEF descriptor for each keypoint. Sizes 128, 256 and 512 recommended\nby the authors. Default is 256.\n\nLength of the two dimensional square patch sampling region around the\nkeypoints. Default is 49.\n\nProbability distribution for sampling location of decision pixel-pairs around\nkeypoints.\n\nSeed for the random sampling of the decision pixel-pairs. From a square window\nwith length `patch_size`, pixel pairs are sampled using the `mode` parameter\nto build the descriptors using intensity comparison. The value of\n`sample_seed` must be the same for the images to be matched while building the\ndescriptors.\n\nStandard deviation of the Gaussian low-pass filter applied to the image to\nalleviate noise sensitivity, which is strongly recommended to obtain\ndiscriminative and good descriptors.\n\n2D ndarray of binary descriptors of size `descriptor_size` for Q keypoints\nafter filtering out border keypoints with value at an index `(i, j)` either\nbeing `True` or `False` representing the outcome of the intensity comparison\nfor i-th keypoint on j-th decision pixel-pair. It is `Q == np.sum(mask)`.\n\nMask indicating whether a keypoint has been filtered out (`False`) or is\ndescribed in the `descriptors` array (`True`).\n\nInitialize self. See help(type(self)) for accurate signature.\n\nExtract BRIEF binary descriptors for given keypoints in image.\n\nInput image.\n\nKeypoint coordinates as `(row, col)`.\n\nBases: `skimage.feature.util.FeatureDetector`\n\nCENSURE keypoint detector.\n\nMinimum scale to extract keypoints from.\n\nMaximum scale to extract keypoints from. The keypoints will be extracted from\nall the scales except the first and the last i.e. from the scales in the range\n[min_scale + 1, max_scale - 1]. The filter sizes for different scales is such\nthat the two adjacent scales comprise of an octave.\n\nType of bi-level filter used to get the scales of the input image. Possible\nvalues are \u2018DoB\u2019, \u2018Octagon\u2019 and \u2018STAR\u2019. The three modes represent the shape of\nthe bi-level filters i.e. box(square), octagon and star respectively. For\ninstance, a bi-level octagon filter consists of a smaller inner octagon and a\nlarger outer octagon with the filter weights being uniformly negative in both\nthe inner octagon while uniformly positive in the difference region. Use STAR\nand Octagon for better features and DoB for better performance.\n\nThreshold value used to suppress maximas and minimas with a weak magnitude\nresponse obtained after Non-Maximal Suppression.\n\nThreshold for rejecting interest points which have ratio of principal\ncurvatures greater than this value.\n\nMotilal Agrawal, Kurt Konolige and Morten Rufus Blas \u201cCENSURE: Center Surround\nExtremas for Realtime Feature Detection and Matching\u201d,\nhttps://link.springer.com/chapter/10.1007/978-3-540-88693-8_8\nDOI:10.1007/978-3-540-88693-8_8\n\nAdam Schmidt, Marek Kraft, Michal Fularz and Zuzanna Domagala \u201cComparative\nAssessment of Point Feature Detectors and Descriptors in the Context of Robot\nNavigation\u201d\nhttp://yadda.icm.edu.pl/yadda/element/bwmeta1.element.baztech-268aaf28-0faf-4872-a4df-7e2e61cb364c/c/Schmidt_comparative.pdf\nDOI:10.1.1.465.1117\n\nKeypoint coordinates as `(row, col)`.\n\nCorresponding scales.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nDetect CENSURE keypoints along with the corresponding scale.\n\nInput image.\n\nBases: `object`\n\nClass for cascade of classifiers that is used for object detection.\n\nThe main idea behind cascade of classifiers is to create classifiers of medium\naccuracy and ensemble them into one strong classifier instead of just creating\na strong one. The second advantage of cascade classifier is that easy examples\ncan be classified only by evaluating some of the classifiers in the cascade,\nmaking the process much faster than the process of evaluating a one strong\nclassifier.\n\nAccuracy parameter. Increasing it, makes the classifier detect less false\npositives but at the same time the false negative score increases.\n\nAmount of stages in a cascade. Each cascade consists of stumps i.e. trained\nfeatures.\n\nThe overall amount of stumps in all the stages of cascade.\n\nThe overall amount of different features used by cascade. Two stumps can use\nthe same features but has different trained values.\n\nThe width of a detection window that is used. Objects smaller than this window\ncan\u2019t be detected.\n\nThe height of a detection window.\n\nA link to the c array that stores stages information using Stage struct.\n\nLink to the c array that stores MBLBP features using MBLBP struct.\n\nThe ling to the array with look-up tables that are used by trained MBLBP\nfeatures (MBLBPStumps) to evaluate a particular region.\n\nInitialize cascade classifier.\n\nA file in a OpenCv format from which all the cascade classifier\u2019s parameters\nare loaded.\n\nAccuracy parameter. Increasing it, makes the classifier detect less false\npositives but at the same time the false negative score increases.\n\nSearch for the object on multiple scales of input image.\n\nThe function takes the input image, the scale factor by which the searching\nwindow is multiplied on each step, minimum window size and maximum window size\nthat specify the interval for the search windows that are applied to the input\nimage to detect objects.\n\nNdarray that represents the input image.\n\nThe scale by which searching window is multiplied on each step.\n\nThe ratio by which the search step in multiplied on each scale of the image. 1\nrepresents the exaustive search and usually is slow. By setting this parameter\nto higher values the results will be worse but the computation will be much\nfaster. Usually, values in the interval [1, 1.5] give good results.\n\nMinimum size of the search window.\n\nMaximum size of the search window.\n\nMinimum amount of intersecting detections in order for detection to be\napproved by the function.\n\nThe minimum value of value of ratio (intersection area) / (small rectangle\nratio) in order to merge two detections into one.\n\nDict have form {\u2018r\u2019: int, \u2018c\u2019: int, \u2018width\u2019: int, \u2018height\u2019: int}, where \u2018r\u2019\nrepresents row position of top left corner of detected window, \u2018c\u2019 - col\nposition, \u2018width\u2019 - width of detected window, \u2018height\u2019 - height of detected\nwindow.\n\nBases: `skimage.feature.util.FeatureDetector`,\n`skimage.feature.util.DescriptorExtractor`\n\nOriented FAST and rotated BRIEF feature detector and binary descriptor\nextractor.\n\nNumber of keypoints to be returned. The function will return the best\n`n_keypoints` according to the Harris corner response if more than\n`n_keypoints` are detected. If not, then all the detected keypoints are\nreturned.\n\nThe `n` parameter in `skimage.feature.corner_fast`. Minimum number of\nconsecutive pixels out of 16 pixels on the circle that should all be either\nbrighter or darker w.r.t test-pixel. A point c on the circle is darker w.r.t\ntest pixel p if `Ic < Ip - threshold` and brighter if `Ic > Ip + threshold`.\nAlso stands for the n in `FAST-n` corner detector.\n\nThe `threshold` parameter in `feature.corner_fast`. Threshold used to decide\nwhether the pixels on the circle are brighter, darker or similar w.r.t. the\ntest pixel. Decrease the threshold when more corners are desired and vice-\nversa.\n\nThe `k` parameter in `skimage.feature.corner_harris`. Sensitivity factor to\nseparate corners from edges, typically in range `[0, 0.2]`. Small values of\n`k` result in detection of sharp corners.\n\nDownscale factor for the image pyramid. Default value 1.2 is chosen so that\nthere are more dense scales which enable robust scale invariance for a\nsubsequent feature description.\n\nMaximum number of scales from the bottom of the image pyramid to extract the\nfeatures from.\n\nEthan Rublee, Vincent Rabaud, Kurt Konolige and Gary Bradski \u201cORB: An\nefficient alternative to SIFT and SURF\u201d\nhttp://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf\n\nKeypoint coordinates as `(row, col)`.\n\nCorresponding scales.\n\nCorresponding orientations in radians.\n\nCorresponding Harris corner responses.\n\n2D array of binary descriptors of size `descriptor_size` for Q keypoints after\nfiltering out border keypoints with value at an index `(i, j)` either being\n`True` or `False` representing the outcome of the intensity comparison for\ni-th keypoint on j-th decision pixel-pair. It is `Q == np.sum(mask)`.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nDetect oriented FAST keypoints along with the corresponding scale.\n\nInput image.\n\nDetect oriented FAST keypoints and extract rBRIEF descriptors.\n\nNote that this is faster than first calling `detect` and then `extract`.\n\nInput image.\n\nExtract rBRIEF binary descriptors for given keypoints in image.\n\nNote that the keypoints must be extracted using the same `downscale` and\n`n_scales` parameters. Additionally, if you want to extract both keypoints and\ndescriptors you should use the faster `detect_and_extract`.\n\nInput image.\n\nKeypoint coordinates as `(row, col)`.\n\nCorresponding scales.\n\nCorresponding orientations in radians.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.blob_dog()", "path": "api/skimage.feature#skimage.feature.blob_dog", "type": "feature", "text": "\nFinds blobs in the given grayscale image.\n\nBlobs are found using the Difference of Gaussian (DoG) method [1]. For each\nblob found, the method returns its coordinates and the standard deviation of\nthe Gaussian kernel that detected the blob.\n\nInput grayscale image, blobs are assumed to be light on dark background (white\non black).\n\nThe minimum standard deviation for Gaussian kernel. Keep this low to detect\nsmaller blobs. The standard deviations of the Gaussian filter are given for\neach axis as a sequence, or as a single number, in which case it is equal for\nall axes.\n\nThe maximum standard deviation for Gaussian kernel. Keep this high to detect\nlarger blobs. The standard deviations of the Gaussian filter are given for\neach axis as a sequence, or as a single number, in which case it is equal for\nall axes.\n\nThe ratio between the standard deviation of Gaussian Kernels used for\ncomputing the Difference of Gaussians\n\nThe absolute lower bound for scale space maxima. Local maxima smaller than\nthresh are ignored. Reduce this to detect blobs with less intensities.\n\nA value between 0 and 1. If the area of two blobs overlaps by a fraction\ngreater than `threshold`, the smaller blob is eliminated.\n\nIf tuple of ints, the length of the tuple must match the input array\u2019s\ndimensionality. Each element of the tuple will exclude peaks from within\n`exclude_border`-pixels of the border of the image along that dimension. If\nnonzero int, `exclude_border` excludes peaks from within\n`exclude_border`-pixels of the border of the image. If zero or False, peaks\nare identified regardless of their distance from the border.\n\nA 2d array with each row representing 2 coordinate values for a 2D image, and\n3 coordinate values for a 3D image, plus the sigma(s) used. When a single\nsigma is passed, outputs are: `(r, c, sigma)` or `(p, r, c, sigma)` where `(r,\nc)` or `(p, r, c)` are coordinates of the blob and `sigma` is the standard\ndeviation of the Gaussian kernel which detected the blob. When an anisotropic\ngaussian is used (sigmas per dimension), the detected sigma is returned for\neach dimension.\n\nSee also\n\nThe radius of each blob is approximately \\\\(\\sqrt{2}\\sigma\\\\) for a 2-D image\nand \\\\(\\sqrt{3}\\sigma\\\\) for a 3-D image.\n\nhttps://en.wikipedia.org/wiki/Blob_detection#The_difference_of_Gaussians_approach\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.blob_doh()", "path": "api/skimage.feature#skimage.feature.blob_doh", "type": "feature", "text": "\nFinds blobs in the given grayscale image.\n\nBlobs are found using the Determinant of Hessian method [1]. For each blob\nfound, the method returns its coordinates and the standard deviation of the\nGaussian Kernel used for the Hessian matrix whose determinant detected the\nblob. Determinant of Hessians is approximated using [2].\n\nInput grayscale image.Blobs can either be light on dark or vice versa.\n\nThe minimum standard deviation for Gaussian Kernel used to compute Hessian\nmatrix. Keep this low to detect smaller blobs.\n\nThe maximum standard deviation for Gaussian Kernel used to compute Hessian\nmatrix. Keep this high to detect larger blobs.\n\nThe number of intermediate values of standard deviations to consider between\n`min_sigma` and `max_sigma`.\n\nThe absolute lower bound for scale space maxima. Local maxima smaller than\nthresh are ignored. Reduce this to detect less prominent blobs.\n\nA value between 0 and 1. If the area of two blobs overlaps by a fraction\ngreater than `threshold`, the smaller blob is eliminated.\n\nIf set intermediate values of standard deviations are interpolated using a\nlogarithmic scale to the base `10`. If not, linear interpolation is used.\n\nA 2d array with each row representing 3 values, `(y,x,sigma)` where `(y,x)`\nare coordinates of the blob and `sigma` is the standard deviation of the\nGaussian kernel of the Hessian Matrix whose determinant detected the blob.\n\nThe radius of each blob is approximately `sigma`. Computation of Determinant\nof Hessians is independent of the standard deviation. Therefore detecting\nlarger blobs won\u2019t take more time. In methods line `blob_dog()` and\n`blob_log()` the computation of Gaussians for larger `sigma` takes more time.\nThe downside is that this method can\u2019t be used for detecting blobs of radius\nless than `3px` due to the box filters used in the approximation of Hessian\nDeterminant.\n\nhttps://en.wikipedia.org/wiki/Blob_detection#The_determinant_of_the_Hessian\n\nHerbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool, \u201cSURF: Speeded Up\nRobust Features\u201d\nftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.blob_log()", "path": "api/skimage.feature#skimage.feature.blob_log", "type": "feature", "text": "\nFinds blobs in the given grayscale image.\n\nBlobs are found using the Laplacian of Gaussian (LoG) method [1]. For each\nblob found, the method returns its coordinates and the standard deviation of\nthe Gaussian kernel that detected the blob.\n\nInput grayscale image, blobs are assumed to be light on dark background (white\non black).\n\nthe minimum standard deviation for Gaussian kernel. Keep this low to detect\nsmaller blobs. The standard deviations of the Gaussian filter are given for\neach axis as a sequence, or as a single number, in which case it is equal for\nall axes.\n\nThe maximum standard deviation for Gaussian kernel. Keep this high to detect\nlarger blobs. The standard deviations of the Gaussian filter are given for\neach axis as a sequence, or as a single number, in which case it is equal for\nall axes.\n\nThe number of intermediate values of standard deviations to consider between\n`min_sigma` and `max_sigma`.\n\nThe absolute lower bound for scale space maxima. Local maxima smaller than\nthresh are ignored. Reduce this to detect blobs with less intensities.\n\nA value between 0 and 1. If the area of two blobs overlaps by a fraction\ngreater than `threshold`, the smaller blob is eliminated.\n\nIf set intermediate values of standard deviations are interpolated using a\nlogarithmic scale to the base `10`. If not, linear interpolation is used.\n\nIf tuple of ints, the length of the tuple must match the input array\u2019s\ndimensionality. Each element of the tuple will exclude peaks from within\n`exclude_border`-pixels of the border of the image along that dimension. If\nnonzero int, `exclude_border` excludes peaks from within\n`exclude_border`-pixels of the border of the image. If zero or False, peaks\nare identified regardless of their distance from the border.\n\nA 2d array with each row representing 2 coordinate values for a 2D image, and\n3 coordinate values for a 3D image, plus the sigma(s) used. When a single\nsigma is passed, outputs are: `(r, c, sigma)` or `(p, r, c, sigma)` where `(r,\nc)` or `(p, r, c)` are coordinates of the blob and `sigma` is the standard\ndeviation of the Gaussian kernel which detected the blob. When an anisotropic\ngaussian is used (sigmas per dimension), the detected sigma is returned for\neach dimension.\n\nThe radius of each blob is approximately \\\\(\\sqrt{2}\\sigma\\\\) for a 2-D image\nand \\\\(\\sqrt{3}\\sigma\\\\) for a 3-D image.\n\nhttps://en.wikipedia.org/wiki/Blob_detection#The_Laplacian_of_Gaussian\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.BRIEF", "path": "api/skimage.feature#skimage.feature.BRIEF", "type": "feature", "text": "\nBases: `skimage.feature.util.DescriptorExtractor`\n\nBRIEF binary descriptor extractor.\n\nBRIEF (Binary Robust Independent Elementary Features) is an efficient feature\npoint descriptor. It is highly discriminative even when using relatively few\nbits and is computed using simple intensity difference tests.\n\nFor each keypoint, intensity comparisons are carried out for a specifically\ndistributed number N of pixel-pairs resulting in a binary descriptor of length\nN. For binary descriptors the Hamming distance can be used for feature\nmatching, which leads to lower computational cost in comparison to the L2\nnorm.\n\nSize of BRIEF descriptor for each keypoint. Sizes 128, 256 and 512 recommended\nby the authors. Default is 256.\n\nLength of the two dimensional square patch sampling region around the\nkeypoints. Default is 49.\n\nProbability distribution for sampling location of decision pixel-pairs around\nkeypoints.\n\nSeed for the random sampling of the decision pixel-pairs. From a square window\nwith length `patch_size`, pixel pairs are sampled using the `mode` parameter\nto build the descriptors using intensity comparison. The value of\n`sample_seed` must be the same for the images to be matched while building the\ndescriptors.\n\nStandard deviation of the Gaussian low-pass filter applied to the image to\nalleviate noise sensitivity, which is strongly recommended to obtain\ndiscriminative and good descriptors.\n\n2D ndarray of binary descriptors of size `descriptor_size` for Q keypoints\nafter filtering out border keypoints with value at an index `(i, j)` either\nbeing `True` or `False` representing the outcome of the intensity comparison\nfor i-th keypoint on j-th decision pixel-pair. It is `Q == np.sum(mask)`.\n\nMask indicating whether a keypoint has been filtered out (`False`) or is\ndescribed in the `descriptors` array (`True`).\n\nInitialize self. See help(type(self)) for accurate signature.\n\nExtract BRIEF binary descriptors for given keypoints in image.\n\nInput image.\n\nKeypoint coordinates as `(row, col)`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.BRIEF.extract()", "path": "api/skimage.feature#skimage.feature.BRIEF.extract", "type": "feature", "text": "\nExtract BRIEF binary descriptors for given keypoints in image.\n\nInput image.\n\nKeypoint coordinates as `(row, col)`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.BRIEF.__init__()", "path": "api/skimage.feature#skimage.feature.BRIEF.__init__", "type": "feature", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.canny()", "path": "api/skimage.feature#skimage.feature.canny", "type": "feature", "text": "\nEdge filter an image using the Canny algorithm.\n\nGrayscale input image to detect edges on; can be of any dtype.\n\nStandard deviation of the Gaussian filter.\n\nLower bound for hysteresis thresholding (linking edges). If None,\nlow_threshold is set to 10% of dtype\u2019s max.\n\nUpper bound for hysteresis thresholding (linking edges). If None,\nhigh_threshold is set to 20% of dtype\u2019s max.\n\nMask to limit the application of Canny to a certain area.\n\nIf True then treat low_threshold and high_threshold as quantiles of the edge\nmagnitude image, rather than absolute edge magnitude values. If True then the\nthresholds must be in the range [0, 1].\n\nThe binary edge map.\n\nSee also\n\nThe steps of the algorithm are as follows:\n\nCanny, J., A Computational Approach To Edge Detection, IEEE Trans. Pattern\nAnalysis and Machine Intelligence, 8:679-714, 1986\nDOI:10.1109/TPAMI.1986.4767851\n\nWilliam Green\u2019s Canny tutorial\nhttps://en.wikipedia.org/wiki/Canny_edge_detector\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.Cascade", "path": "api/skimage.feature#skimage.feature.Cascade", "type": "feature", "text": "\nBases: `object`\n\nClass for cascade of classifiers that is used for object detection.\n\nThe main idea behind cascade of classifiers is to create classifiers of medium\naccuracy and ensemble them into one strong classifier instead of just creating\na strong one. The second advantage of cascade classifier is that easy examples\ncan be classified only by evaluating some of the classifiers in the cascade,\nmaking the process much faster than the process of evaluating a one strong\nclassifier.\n\nAccuracy parameter. Increasing it, makes the classifier detect less false\npositives but at the same time the false negative score increases.\n\nAmount of stages in a cascade. Each cascade consists of stumps i.e. trained\nfeatures.\n\nThe overall amount of stumps in all the stages of cascade.\n\nThe overall amount of different features used by cascade. Two stumps can use\nthe same features but has different trained values.\n\nThe width of a detection window that is used. Objects smaller than this window\ncan\u2019t be detected.\n\nThe height of a detection window.\n\nA link to the c array that stores stages information using Stage struct.\n\nLink to the c array that stores MBLBP features using MBLBP struct.\n\nThe ling to the array with look-up tables that are used by trained MBLBP\nfeatures (MBLBPStumps) to evaluate a particular region.\n\nInitialize cascade classifier.\n\nA file in a OpenCv format from which all the cascade classifier\u2019s parameters\nare loaded.\n\nAccuracy parameter. Increasing it, makes the classifier detect less false\npositives but at the same time the false negative score increases.\n\nSearch for the object on multiple scales of input image.\n\nThe function takes the input image, the scale factor by which the searching\nwindow is multiplied on each step, minimum window size and maximum window size\nthat specify the interval for the search windows that are applied to the input\nimage to detect objects.\n\nNdarray that represents the input image.\n\nThe scale by which searching window is multiplied on each step.\n\nThe ratio by which the search step in multiplied on each scale of the image. 1\nrepresents the exaustive search and usually is slow. By setting this parameter\nto higher values the results will be worse but the computation will be much\nfaster. Usually, values in the interval [1, 1.5] give good results.\n\nMinimum size of the search window.\n\nMaximum size of the search window.\n\nMinimum amount of intersecting detections in order for detection to be\napproved by the function.\n\nThe minimum value of value of ratio (intersection area) / (small rectangle\nratio) in order to merge two detections into one.\n\nDict have form {\u2018r\u2019: int, \u2018c\u2019: int, \u2018width\u2019: int, \u2018height\u2019: int}, where \u2018r\u2019\nrepresents row position of top left corner of detected window, \u2018c\u2019 - col\nposition, \u2018width\u2019 - width of detected window, \u2018height\u2019 - height of detected\nwindow.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.Cascade.detect_multi_scale()", "path": "api/skimage.feature#skimage.feature.Cascade.detect_multi_scale", "type": "feature", "text": "\nSearch for the object on multiple scales of input image.\n\nThe function takes the input image, the scale factor by which the searching\nwindow is multiplied on each step, minimum window size and maximum window size\nthat specify the interval for the search windows that are applied to the input\nimage to detect objects.\n\nNdarray that represents the input image.\n\nThe scale by which searching window is multiplied on each step.\n\nThe ratio by which the search step in multiplied on each scale of the image. 1\nrepresents the exaustive search and usually is slow. By setting this parameter\nto higher values the results will be worse but the computation will be much\nfaster. Usually, values in the interval [1, 1.5] give good results.\n\nMinimum size of the search window.\n\nMaximum size of the search window.\n\nMinimum amount of intersecting detections in order for detection to be\napproved by the function.\n\nThe minimum value of value of ratio (intersection area) / (small rectangle\nratio) in order to merge two detections into one.\n\nDict have form {\u2018r\u2019: int, \u2018c\u2019: int, \u2018width\u2019: int, \u2018height\u2019: int}, where \u2018r\u2019\nrepresents row position of top left corner of detected window, \u2018c\u2019 - col\nposition, \u2018width\u2019 - width of detected window, \u2018height\u2019 - height of detected\nwindow.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.Cascade.eps", "path": "api/skimage.feature#skimage.feature.Cascade.eps", "type": "feature", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.Cascade.features_number", "path": "api/skimage.feature#skimage.feature.Cascade.features_number", "type": "feature", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.Cascade.stages_number", "path": "api/skimage.feature#skimage.feature.Cascade.stages_number", "type": "feature", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.Cascade.stumps_number", "path": "api/skimage.feature#skimage.feature.Cascade.stumps_number", "type": "feature", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.Cascade.window_height", "path": "api/skimage.feature#skimage.feature.Cascade.window_height", "type": "feature", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.Cascade.window_width", "path": "api/skimage.feature#skimage.feature.Cascade.window_width", "type": "feature", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.Cascade.__init__()", "path": "api/skimage.feature#skimage.feature.Cascade.__init__", "type": "feature", "text": "\nInitialize cascade classifier.\n\nA file in a OpenCv format from which all the cascade classifier\u2019s parameters\nare loaded.\n\nAccuracy parameter. Increasing it, makes the classifier detect less false\npositives but at the same time the false negative score increases.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.CENSURE", "path": "api/skimage.feature#skimage.feature.CENSURE", "type": "feature", "text": "\nBases: `skimage.feature.util.FeatureDetector`\n\nCENSURE keypoint detector.\n\nMinimum scale to extract keypoints from.\n\nMaximum scale to extract keypoints from. The keypoints will be extracted from\nall the scales except the first and the last i.e. from the scales in the range\n[min_scale + 1, max_scale - 1]. The filter sizes for different scales is such\nthat the two adjacent scales comprise of an octave.\n\nType of bi-level filter used to get the scales of the input image. Possible\nvalues are \u2018DoB\u2019, \u2018Octagon\u2019 and \u2018STAR\u2019. The three modes represent the shape of\nthe bi-level filters i.e. box(square), octagon and star respectively. For\ninstance, a bi-level octagon filter consists of a smaller inner octagon and a\nlarger outer octagon with the filter weights being uniformly negative in both\nthe inner octagon while uniformly positive in the difference region. Use STAR\nand Octagon for better features and DoB for better performance.\n\nThreshold value used to suppress maximas and minimas with a weak magnitude\nresponse obtained after Non-Maximal Suppression.\n\nThreshold for rejecting interest points which have ratio of principal\ncurvatures greater than this value.\n\nMotilal Agrawal, Kurt Konolige and Morten Rufus Blas \u201cCENSURE: Center Surround\nExtremas for Realtime Feature Detection and Matching\u201d,\nhttps://link.springer.com/chapter/10.1007/978-3-540-88693-8_8\nDOI:10.1007/978-3-540-88693-8_8\n\nAdam Schmidt, Marek Kraft, Michal Fularz and Zuzanna Domagala \u201cComparative\nAssessment of Point Feature Detectors and Descriptors in the Context of Robot\nNavigation\u201d\nhttp://yadda.icm.edu.pl/yadda/element/bwmeta1.element.baztech-268aaf28-0faf-4872-a4df-7e2e61cb364c/c/Schmidt_comparative.pdf\nDOI:10.1.1.465.1117\n\nKeypoint coordinates as `(row, col)`.\n\nCorresponding scales.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nDetect CENSURE keypoints along with the corresponding scale.\n\nInput image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.CENSURE.detect()", "path": "api/skimage.feature#skimage.feature.CENSURE.detect", "type": "feature", "text": "\nDetect CENSURE keypoints along with the corresponding scale.\n\nInput image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.CENSURE.__init__()", "path": "api/skimage.feature#skimage.feature.CENSURE.__init__", "type": "feature", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.corner_fast()", "path": "api/skimage.feature#skimage.feature.corner_fast", "type": "feature", "text": "\nExtract FAST corners for a given image.\n\nInput image.\n\nMinimum number of consecutive pixels out of 16 pixels on the circle that\nshould all be either brighter or darker w.r.t testpixel. A point c on the\ncircle is darker w.r.t test pixel p if `Ic < Ip - threshold` and brighter if\n`Ic > Ip + threshold`. Also stands for the n in `FAST-n` corner detector.\n\nThreshold used in deciding whether the pixels on the circle are brighter,\ndarker or similar w.r.t. the test pixel. Decrease the threshold when more\ncorners are desired and vice-versa.\n\nFAST corner response image.\n\nRosten, E., & Drummond, T. (2006, May). Machine learning for high-speed corner\ndetection. In European conference on computer vision (pp. 430-443). Springer,\nBerlin, Heidelberg. DOI:10.1007/11744023_34\nhttp://www.edwardrosten.com/work/rosten_2006_machine.pdf\n\nWikipedia, \u201cFeatures from accelerated segment test\u201d,\nhttps://en.wikipedia.org/wiki/Features_from_accelerated_segment_test\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.corner_foerstner()", "path": "api/skimage.feature#skimage.feature.corner_foerstner", "type": "feature", "text": "\nCompute Foerstner corner measure response image.\n\nThis corner detector uses information from the auto-correlation matrix A:\n\nWhere imx and imy are first derivatives, averaged with a gaussian filter. The\ncorner measure is then defined as:\n\nInput image.\n\nStandard deviation used for the Gaussian kernel, which is used as weighting\nfunction for the auto-correlation matrix.\n\nError ellipse sizes.\n\nRoundness of error ellipse.\n\nF\u00f6rstner, W., & G\u00fclch, E. (1987, June). A fast operator for detection and\nprecise location of distinct points, corners and centres of circular features.\nIn Proc. ISPRS intercommission conference on fast processing of\nphotogrammetric data (pp. 281-305).\nhttps://cseweb.ucsd.edu/classes/sp02/cse252/foerstner/foerstner.pdf\n\nhttps://en.wikipedia.org/wiki/Corner_detection\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.corner_harris()", "path": "api/skimage.feature#skimage.feature.corner_harris", "type": "feature", "text": "\nCompute Harris corner measure response image.\n\nThis corner detector uses information from the auto-correlation matrix A:\n\nWhere imx and imy are first derivatives, averaged with a gaussian filter. The\ncorner measure is then defined as:\n\nor:\n\nInput image.\n\nMethod to compute the response image from the auto-correlation matrix.\n\nSensitivity factor to separate corners from edges, typically in range `[0,\n0.2]`. Small values of k result in detection of sharp corners.\n\nNormalisation factor (Noble\u2019s corner measure).\n\nStandard deviation used for the Gaussian kernel, which is used as weighting\nfunction for the auto-correlation matrix.\n\nHarris response image.\n\nhttps://en.wikipedia.org/wiki/Corner_detection\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.corner_kitchen_rosenfeld()", "path": "api/skimage.feature#skimage.feature.corner_kitchen_rosenfeld", "type": "feature", "text": "\nCompute Kitchen and Rosenfeld corner measure response image.\n\nThe corner measure is calculated as follows:\n\nWhere imx and imy are the first and imxx, imxy, imyy the second derivatives.\n\nInput image.\n\nHow to handle values outside the image borders.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nKitchen and Rosenfeld response image.\n\nKitchen, L., & Rosenfeld, A. (1982). Gray-level corner detection. Pattern\nrecognition letters, 1(2), 95-102. DOI:10.1016/0167-8655(82)90020-4\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.corner_moravec()", "path": "api/skimage.feature#skimage.feature.corner_moravec", "type": "feature", "text": "\nCompute Moravec corner measure response image.\n\nThis is one of the simplest corner detectors and is comparatively fast but has\nseveral limitations (e.g. not rotation invariant).\n\nInput image.\n\nWindow size.\n\nMoravec response image.\n\nhttps://en.wikipedia.org/wiki/Corner_detection\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.corner_orientations()", "path": "api/skimage.feature#skimage.feature.corner_orientations", "type": "feature", "text": "\nCompute the orientation of corners.\n\nThe orientation of corners is computed using the first order central moment\ni.e. the center of mass approach. The corner orientation is the angle of the\nvector from the corner coordinate to the intensity centroid in the local\nneighborhood around the corner calculated using first order central moment.\n\nInput grayscale image.\n\nCorner coordinates as `(row, col)`.\n\nMask defining the local neighborhood of the corner used for the calculation of\nthe central moment.\n\nOrientations of corners in the range [-pi, pi].\n\nEthan Rublee, Vincent Rabaud, Kurt Konolige and Gary Bradski \u201cORB : An\nefficient alternative to SIFT and SURF\u201d\nhttp://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf\n\nPaul L. Rosin, \u201cMeasuring Corner Properties\u201d\nhttp://users.cs.cf.ac.uk/Paul.Rosin/corner2.pdf\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.corner_peaks()", "path": "api/skimage.feature#skimage.feature.corner_peaks", "type": "feature", "text": "\nFind peaks in corner measure response image.\n\nThis differs from `skimage.feature.peak_local_max` in that it suppresses\nmultiple connected peaks with the same accumulator value.\n\nInput image.\n\nThe minimal allowed distance separating peaks.\n\nSee `skimage.feature.peak_local_max()`.\n\nWhich Minkowski p-norm to use. Should be in the range [1, inf]. A finite large\np may cause a ValueError if overflow can occur. `inf` corresponds to the\nChebyshev distance and 2 to the Euclidean distance.\n\nSee also\n\nChanged in version 0.18: The default value of `threshold_rel` has changed to\nNone, which corresponds to letting `skimage.feature.peak_local_max` decide on\nthe default. This is equivalent to `threshold_rel=0`.\n\nThe `num_peaks` limit is applied before suppression of connected peaks. To\nlimit the number of peaks after suppression, set `num_peaks=np.inf` and post-\nprocess the output of this function.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.corner_shi_tomasi()", "path": "api/skimage.feature#skimage.feature.corner_shi_tomasi", "type": "feature", "text": "\nCompute Shi-Tomasi (Kanade-Tomasi) corner measure response image.\n\nThis corner detector uses information from the auto-correlation matrix A:\n\nWhere imx and imy are first derivatives, averaged with a gaussian filter. The\ncorner measure is then defined as the smaller eigenvalue of A:\n\nInput image.\n\nStandard deviation used for the Gaussian kernel, which is used as weighting\nfunction for the auto-correlation matrix.\n\nShi-Tomasi response image.\n\nhttps://en.wikipedia.org/wiki/Corner_detection\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.corner_subpix()", "path": "api/skimage.feature#skimage.feature.corner_subpix", "type": "feature", "text": "\nDetermine subpixel position of corners.\n\nA statistical test decides whether the corner is defined as the intersection\nof two edges or a single peak. Depending on the classification result, the\nsubpixel corner location is determined based on the local covariance of the\ngrey-values. If the significance level for either statistical test is not\nsufficient, the corner cannot be classified, and the output subpixel position\nis set to NaN.\n\nInput image.\n\nCorner coordinates `(row, col)`.\n\nSearch window size for subpixel estimation.\n\nSignificance level for corner classification.\n\nSubpixel corner positions. NaN for \u201cnot classified\u201d corners.\n\nF\u00f6rstner, W., & G\u00fclch, E. (1987, June). A fast operator for detection and\nprecise location of distinct points, corners and centres of circular features.\nIn Proc. ISPRS intercommission conference on fast processing of\nphotogrammetric data (pp. 281-305).\nhttps://cseweb.ucsd.edu/classes/sp02/cse252/foerstner/foerstner.pdf\n\nhttps://en.wikipedia.org/wiki/Corner_detection\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.daisy()", "path": "api/skimage.feature#skimage.feature.daisy", "type": "feature", "text": "\nExtract DAISY feature descriptors densely for the given image.\n\nDAISY is a feature descriptor similar to SIFT formulated in a way that allows\nfor fast dense extraction. Typically, this is practical for bag-of-features\nimage representations.\n\nThe implementation follows Tola et al. [1] but deviate on the following\npoints:\n\nInput image (grayscale).\n\nDistance between descriptor sampling points.\n\nRadius (in pixels) of the outermost ring.\n\nNumber of rings.\n\nNumber of histograms sampled per ring.\n\nNumber of orientations (bins) per histogram.\n\nHow to normalize the descriptors\n\nStandard deviation of spatial Gaussian smoothing for the center histogram and\nfor each ring of histograms. The array of sigmas should be sorted from the\ncenter and out. I.e. the first sigma value defines the spatial smoothing of\nthe center histogram and the last sigma value defines the spatial smoothing of\nthe outermost ring. Specifying sigmas overrides the following parameter.\n\n`rings = len(sigmas) - 1`\n\nRadius (in pixels) for each ring. Specifying ring_radii overrides the\nfollowing two parameters.\n\n`rings = len(ring_radii)` `radius = ring_radii[-1]`\n\nIf both sigmas and ring_radii are given, they must satisfy the following\npredicate since no radius is needed for the center histogram.\n\n`len(ring_radii) == len(sigmas) + 1`\n\nGenerate a visualization of the DAISY descriptors\n\nGrid of DAISY descriptors for the given image as an array dimensionality (P,\nQ, R) where\n\n`P = ceil((M - radius*2) / step)` `Q = ceil((N - radius*2) / step)` `R =\n(rings * histograms + 1) * orientations`\n\nVisualization of the DAISY descriptors.\n\nTola et al. \u201cDaisy: An efficient dense descriptor applied to wide- baseline\nstereo.\u201d Pattern Analysis and Machine Intelligence, IEEE Transactions on 32.5\n(2010): 815-830.\n\nhttp://cvlab.epfl.ch/software/daisy\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.draw_haar_like_feature()", "path": "api/skimage.feature#skimage.feature.draw_haar_like_feature", "type": "feature", "text": "\nVisualization of Haar-like features.\n\nThe region of an integral image for which the features need to be computed.\n\nRow-coordinate of top left corner of the detection window.\n\nColumn-coordinate of top left corner of the detection window.\n\nWidth of the detection window.\n\nHeight of the detection window.\n\nThe array of coordinates to be extracted. This is useful when you want to\nrecompute only a subset of features. In this case `feature_type` needs to be\nan array containing the type of each feature, as returned by\n`haar_like_feature_coord()`. By default, all coordinates are computed.\n\nFloats specifying the color for the positive block. Corresponding values\ndefine (R, G, B) values. Default value is red (1, 0, 0).\n\nFloats specifying the color for the negative block Corresponding values define\n(R, G, B) values. Default value is blue (0, 1, 0).\n\nValue in the range [0, 1] that specifies opacity of visualization. 1 - fully\ntransparent, 0 - opaque.\n\nThe maximum number of features to be returned. By default, all features are\nreturned.\n\nIf int, random_state is the seed used by the random number generator; If\nRandomState instance, random_state is the random number generator; If None,\nthe random number generator is the RandomState instance used by `np.random`.\nThe random state is used when generating a set of features smaller than the\ntotal number of available features.\n\nAn image in which the different features will be added.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.draw_multiblock_lbp()", "path": "api/skimage.feature#skimage.feature.draw_multiblock_lbp", "type": "feature", "text": "\nMulti-block local binary pattern visualization.\n\nBlocks with higher sums are colored with alpha-blended white rectangles,\nwhereas blocks with lower sums are colored alpha-blended cyan. Colors and the\n`alpha` parameter can be changed.\n\nImage on which to visualize the pattern.\n\nRow-coordinate of top left corner of a rectangle containing feature.\n\nColumn-coordinate of top left corner of a rectangle containing feature.\n\nWidth of one of 9 equal rectangles that will be used to compute a feature.\n\nHeight of one of 9 equal rectangles that will be used to compute a feature.\n\nThe descriptor of feature to visualize. If not provided, the descriptor with 0\nvalue will be used.\n\nFloats specifying the color for the block that has greater intensity value.\nThey should be in the range [0, 1]. Corresponding values define (R, G, B)\nvalues. Default value is white (1, 1, 1).\n\nFloats specifying the color for the block that has greater intensity value.\nThey should be in the range [0, 1]. Corresponding values define (R, G, B)\nvalues. Default value is cyan (0, 0.69, 0.96).\n\nValue in the range [0, 1] that specifies opacity of visualization. 1 - fully\ntransparent, 0 - opaque.\n\nImage with MB-LBP visualization.\n\nFace Detection Based on Multi-Block LBP Representation. Lun Zhang, Rufeng Chu,\nShiming Xiang, Shengcai Liao, Stan Z. Li\nhttp://www.cbsr.ia.ac.cn/users/scliao/papers/Zhang-ICB07-MBLBP.pdf\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.greycomatrix()", "path": "api/skimage.feature#skimage.feature.greycomatrix", "type": "feature", "text": "\nCalculate the grey-level co-occurrence matrix.\n\nA grey level co-occurrence matrix is a histogram of co-occurring greyscale\nvalues at a given offset over an image.\n\nInteger typed input image. Only positive valued images are supported. If type\nis other than uint8, the argument `levels` needs to be set.\n\nList of pixel pair distance offsets.\n\nList of pixel pair angles in radians.\n\nThe input image should contain integers in [0, `levels`-1], where levels\nindicate the number of grey-levels counted (typically 256 for an 8-bit image).\nThis argument is required for 16-bit images or higher and is typically the\nmaximum of the image. As the output matrix is at least `levels` x `levels`, it\nmight be preferable to use binning of the input image rather than large values\nfor `levels`.\n\nIf True, the output matrix `P[:, :, d, theta]` is symmetric. This is\naccomplished by ignoring the order of value pairs, so both (i, j) and (j, i)\nare accumulated when (i, j) is encountered for a given offset. The default is\nFalse.\n\nIf True, normalize each matrix `P[:, :, d, theta]` by dividing by the total\nnumber of accumulated co-occurrences for the given offset. The elements of the\nresulting matrix sum to 1. The default is False.\n\nThe grey-level co-occurrence histogram. The value `P[i,j,d,theta]` is the\nnumber of times that grey-level `j` occurs at a distance `d` and at an angle\n`theta` from grey-level `i`. If `normed` is `False`, the output is of type\nuint32, otherwise it is float64. The dimensions are: levels x levels x number\nof distances x number of angles.\n\nThe GLCM Tutorial Home Page, http://www.fp.ucalgary.ca/mhallbey/tutorial.htm\n\nHaralick, RM.; Shanmugam, K., \u201cTextural features for image classification\u201d\nIEEE Transactions on systems, man, and cybernetics 6 (1973): 610-621.\nDOI:10.1109/TSMC.1973.4309314\n\nPattern Recognition Engineering, Morton Nadler & Eric P. Smith\n\nWikipedia, https://en.wikipedia.org/wiki/Co-occurrence_matrix\n\nCompute 2 GLCMs: One for a 1-pixel offset to the right, and one for a 1-pixel\noffset upwards.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.greycoprops()", "path": "api/skimage.feature#skimage.feature.greycoprops", "type": "feature", "text": "\nCalculate texture properties of a GLCM.\n\nCompute a feature of a grey level co-occurrence matrix to serve as a compact\nsummary of the matrix. The properties are computed as follows:\n\nEach GLCM is normalized to have a sum of 1 before the computation of texture\nproperties.\n\nInput array. `P` is the grey-level co-occurrence histogram for which to\ncompute the specified property. The value `P[i,j,d,theta]` is the number of\ntimes that grey-level j occurs at a distance d and at an angle theta from\ngrey-level i.\n\nThe property of the GLCM to compute. The default is \u2018contrast\u2019.\n\n2-dimensional array. `results[d, a]` is the property \u2018prop\u2019 for the d\u2019th\ndistance and the a\u2019th angle.\n\nThe GLCM Tutorial Home Page, http://www.fp.ucalgary.ca/mhallbey/tutorial.htm\n\nCompute the contrast for GLCMs with distances [1, 2] and angles [0 degrees, 90\ndegrees]\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.haar_like_feature()", "path": "api/skimage.feature#skimage.feature.haar_like_feature", "type": "feature", "text": "\nCompute the Haar-like features for a region of interest (ROI) of an integral\nimage.\n\nHaar-like features have been successfully used for image classification and\nobject detection [1]. It has been used for real-time face detection algorithm\nproposed in [2].\n\nIntegral image for which the features need to be computed.\n\nRow-coordinate of top left corner of the detection window.\n\nColumn-coordinate of top left corner of the detection window.\n\nWidth of the detection window.\n\nHeight of the detection window.\n\nThe type of feature to consider:\n\nBy default all features are extracted.\n\nIf using with `feature_coord`, it should correspond to the feature type of\neach associated coordinate feature.\n\nThe array of coordinates to be extracted. This is useful when you want to\nrecompute only a subset of features. In this case `feature_type` needs to be\nan array containing the type of each feature, as returned by\n`haar_like_feature_coord()`. By default, all coordinates are computed.\n\nResulting Haar-like features. Each value is equal to the subtraction of sums\nof the positive and negative rectangles. The data type depends of the data\ntype of `int_image`: `int` when the data type of `int_image` is `uint` or\n`int` and `float` when the data type of `int_image` is `float`.\n\nWhen extracting those features in parallel, be aware that the choice of the\nbackend (i.e. multiprocessing vs threading) will have an impact on the\nperformance. The rule of thumb is as follows: use multiprocessing when\nextracting features for all possible ROI in an image; use threading when\nextracting the feature at specific location for a limited number of ROIs.\nRefer to the example Face classification using Haar-like feature descriptor\nfor more insights.\n\nhttps://en.wikipedia.org/wiki/Haar-like_feature\n\nOren, M., Papageorgiou, C., Sinha, P., Osuna, E., & Poggio, T. (1997, June).\nPedestrian detection using wavelet templates. In Computer Vision and Pattern\nRecognition, 1997. Proceedings., 1997 IEEE Computer Society Conference on (pp.\n193-199). IEEE. http://tinyurl.com/y6ulxfta DOI:10.1109/CVPR.1997.609319\n\nViola, Paul, and Michael J. Jones. \u201cRobust real-time face detection.\u201d\nInternational journal of computer vision 57.2 (2004): 137-154.\nhttps://www.merl.com/publications/docs/TR2004-043.pdf\nDOI:10.1109/CVPR.2001.990517\n\nYou can compute the feature for some pre-computed coordinates.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.haar_like_feature_coord()", "path": "api/skimage.feature#skimage.feature.haar_like_feature_coord", "type": "feature", "text": "\nCompute the coordinates of Haar-like features.\n\nWidth of the detection window.\n\nHeight of the detection window.\n\nThe type of feature to consider:\n\nBy default all features are extracted.\n\nCoordinates of the rectangles for each feature.\n\nThe corresponding type for each feature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.hessian_matrix()", "path": "api/skimage.feature#skimage.feature.hessian_matrix", "type": "feature", "text": "\nCompute Hessian matrix.\n\nThe Hessian matrix is defined as:\n\nwhich is computed by convolving the image with the second derivatives of the\nGaussian kernel in the respective r- and c-directions.\n\nInput image.\n\nStandard deviation used for the Gaussian kernel, which is used as weighting\nfunction for the auto-correlation matrix.\n\nHow to handle values outside the image borders.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nThis parameter allows for the use of reverse or forward order of the image\naxes in gradient computation. \u2018rc\u2019 indicates the use of the first axis\ninitially (Hrr, Hrc, Hcc), whilst \u2018xy\u2019 indicates the usage of the last axis\ninitially (Hxx, Hxy, Hyy)\n\nElement of the Hessian matrix for each pixel in the input image.\n\nElement of the Hessian matrix for each pixel in the input image.\n\nElement of the Hessian matrix for each pixel in the input image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.hessian_matrix_det()", "path": "api/skimage.feature#skimage.feature.hessian_matrix_det", "type": "feature", "text": "\nCompute the approximate Hessian Determinant over an image.\n\nThe 2D approximate method uses box filters over integral images to compute the\napproximate Hessian Determinant, as described in [1].\n\nThe image over which to compute Hessian Determinant.\n\nStandard deviation used for the Gaussian kernel, used for the Hessian matrix.\n\nIf `True` and the image is 2D, use a much faster approximate computation. This\nargument has no effect on 3D and higher images.\n\nThe array of the Determinant of Hessians.\n\nFor 2D images when `approximate=True`, the running time of this method only\ndepends on size of the image. It is independent of `sigma` as one would\nexpect. The downside is that the result for `sigma` less than `3` is not\naccurate, i.e., not similar to the result obtained if someone computed the\nHessian and took its determinant.\n\nHerbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool, \u201cSURF: Speeded Up\nRobust Features\u201d\nftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.hessian_matrix_eigvals()", "path": "api/skimage.feature#skimage.feature.hessian_matrix_eigvals", "type": "feature", "text": "\nCompute eigenvalues of Hessian matrix.\n\nThe upper-diagonal elements of the Hessian matrix, as returned by\n`hessian_matrix`.\n\nThe eigenvalues of the Hessian matrix, in decreasing order. The eigenvalues\nare the leading dimension. That is, `eigs[i, j, k]` contains the ith-largest\neigenvalue at position (j, k).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.hog()", "path": "api/skimage.feature#skimage.feature.hog", "type": "feature", "text": "\nExtract Histogram of Oriented Gradients (HOG) for a given image.\n\nCompute a Histogram of Oriented Gradients (HOG) by\n\nInput image.\n\nNumber of orientation bins.\n\nSize (in pixels) of a cell.\n\nNumber of cells in each block.\n\nBlock normalization method:\n\nNormalization using L1-norm.\n\nNormalization using L1-norm, followed by square root.\n\nNormalization using L2-norm.\n\nNormalization using L2-norm, followed by limiting the maximum values to 0.2\n(`Hys` stands for `hysteresis`) and renormalization using L2-norm. (default)\nFor details, see [3], [4].\n\nAlso return an image of the HOG. For each cell and orientation bin, the image\ncontains a line segment that is centered at the cell center, is perpendicular\nto the midpoint of the range of angles spanned by the orientation bin, and has\nintensity proportional to the corresponding histogram value.\n\nApply power law compression to normalize the image before processing. DO NOT\nuse this if the image contains negative values. Also see `notes` section\nbelow.\n\nReturn the data as a feature vector by calling .ravel() on the result just\nbefore returning.\n\nIf True, the last `image` dimension is considered as a color channel,\notherwise as spatial.\n\nHOG descriptor for the image. If `feature_vector` is True, a 1D (flattened)\narray is returned.\n\nA visualisation of the HOG image. Only provided if `visualize` is True.\n\nThe presented code implements the HOG extraction method from [2] with the\nfollowing changes: (I) blocks of (3, 3) cells are used ((2, 2) in the paper);\n(II) no smoothing within cells (Gaussian spatial window with sigma=8pix in the\npaper); (III) L1 block normalization is used (L2-Hys in the paper).\n\nPower law compression, also known as Gamma correction, is used to reduce the\neffects of shadowing and illumination variations. The compression makes the\ndark regions lighter. When the kwarg `transform_sqrt` is set to `True`, the\nfunction computes the square root of each color channel and then applies the\nhog algorithm to the image.\n\nhttps://en.wikipedia.org/wiki/Histogram_of_oriented_gradients\n\nDalal, N and Triggs, B, Histograms of Oriented Gradients for Human Detection,\nIEEE Computer Society Conference on Computer Vision and Pattern Recognition\n2005 San Diego, CA, USA, https://lear.inrialpes.fr/people/triggs/pubs/Dalal-\ncvpr05.pdf, DOI:10.1109/CVPR.2005.177\n\nLowe, D.G., Distinctive image features from scale-invatiant keypoints,\nInternational Journal of Computer Vision (2004) 60: 91,\nhttp://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf,\nDOI:10.1023/B:VISI.0000029664.99615.94\n\nDalal, N, Finding People in Images and Videos, Human-Computer Interaction\n[cs.HC], Institut National Polytechnique de Grenoble - INPG, 2006,\nhttps://tel.archives-ouvertes.fr/tel-00390303/file/NavneetDalalThesis.pdf\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.local_binary_pattern()", "path": "api/skimage.feature#skimage.feature.local_binary_pattern", "type": "feature", "text": "\nGray scale and rotation invariant LBP (Local Binary Patterns).\n\nLBP is an invariant descriptor that can be used for texture classification.\n\nGraylevel image.\n\nNumber of circularly symmetric neighbour set points (quantization of the\nangular space).\n\nRadius of circle (spatial resolution of the operator).\n\nMethod to determine the pattern.\n\nrotation invariant.\n\nrotation invariant.\n\nfiner quantization of the angular space which is gray scale and rotation\ninvariant.\n\nwhich is only gray scale invariant [2].\n\nimage texture which is rotation but not gray scale invariant.\n\nLBP image.\n\nMultiresolution Gray-Scale and Rotation Invariant Texture Classification with\nLocal Binary Patterns. Timo Ojala, Matti Pietikainen, Topi Maenpaa.\nhttp://www.ee.oulu.fi/research/mvmp/mvg/files/pdf/pdf_94.pdf, 2002.\n\nFace recognition with local binary patterns. Timo Ahonen, Abdenour Hadid,\nMatti Pietikainen,\nhttp://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.214.6851, 2004.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.masked_register_translation()", "path": "api/skimage.feature#skimage.feature.masked_register_translation", "type": "feature", "text": "\nDeprecated function. Use `skimage.registration.phase_cross_correlation`\ninstead.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.match_descriptors()", "path": "api/skimage.feature#skimage.feature.match_descriptors", "type": "feature", "text": "\nBrute-force matching of descriptors.\n\nFor each descriptor in the first set this matcher finds the closest descriptor\nin the second set (and vice-versa in the case of enabled cross-checking).\n\nDescriptors of size P about M keypoints in the first image.\n\nDescriptors of size P about N keypoints in the second image.\n\nThe metric to compute the distance between two descriptors. See\n`scipy.spatial.distance.cdist` for all possible types. The hamming distance\nshould be used for binary descriptors. By default the L2-norm is used for all\ndescriptors of dtype float or double and the Hamming distance is used for\nbinary descriptors automatically.\n\nThe p-norm to apply for `metric='minkowski'`.\n\nMaximum allowed distance between descriptors of two keypoints in separate\nimages to be regarded as a match.\n\nIf True, the matched keypoints are returned after cross checking i.e. a\nmatched pair (keypoint1, keypoint2) is returned if keypoint2 is the best match\nfor keypoint1 in second image and keypoint1 is the best match for keypoint2 in\nfirst image.\n\nMaximum ratio of distances between first and second closest descriptor in the\nsecond set of descriptors. This threshold is useful to filter ambiguous\nmatches between the two descriptor sets. The choice of this value depends on\nthe statistics of the chosen descriptor, e.g., for SIFT descriptors a value of\n0.8 is usually chosen, see D.G. Lowe, \u201cDistinctive Image Features from Scale-\nInvariant Keypoints\u201d, International Journal of Computer Vision, 2004.\n\nIndices of corresponding matches in first and second set of descriptors, where\n`matches[:, 0]` denote the indices in the first and `matches[:, 1]` the\nindices in the second set of descriptors.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.match_template()", "path": "api/skimage.feature#skimage.feature.match_template", "type": "feature", "text": "\nMatch a template to a 2-D or 3-D image using normalized correlation.\n\nThe output is an array with values between -1.0 and 1.0. The value at a given\nposition corresponds to the correlation coefficient between the image and the\ntemplate.\n\nFor `pad_input=True` matches correspond to the center and otherwise to the\ntop-left corner of the template. To find the best match you must search for\npeaks in the response (output) image.\n\n2-D or 3-D input image.\n\nTemplate to locate. It must be `(m <= M, n <= N[, d <= D])`.\n\nIf True, pad `image` so that output is the same size as the image, and output\nvalues correspond to the template center. Otherwise, the output is an array\nwith shape `(M - m + 1, N - n + 1)` for an `(M, N)` image and an `(m, n)`\ntemplate, and matches correspond to origin (top-left corner) of the template.\n\nPadding mode.\n\nConstant values used in conjunction with `mode='constant'`.\n\nResponse image with correlation coefficients.\n\nDetails on the cross-correlation are presented in [1]. This implementation\nuses FFT convolutions of the image and the template. Reference [2] presents\nsimilar derivations but the approximation presented in this reference is not\nused in our implementation.\n\nJ. P. Lewis, \u201cFast Normalized Cross-Correlation\u201d, Industrial Light and Magic.\n\nBriechle and Hanebeck, \u201cTemplate Matching using Fast Normalized Cross\nCorrelation\u201d, Proceedings of the SPIE (2001). DOI:10.1117/12.421129\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.multiblock_lbp()", "path": "api/skimage.feature#skimage.feature.multiblock_lbp", "type": "feature", "text": "\nMulti-block local binary pattern (MB-LBP).\n\nThe features are calculated similarly to local binary patterns (LBPs), (See\n`local_binary_pattern()`) except that summed blocks are used instead of\nindividual pixel values.\n\nMB-LBP is an extension of LBP that can be computed on multiple scales in\nconstant time using the integral image. Nine equally-sized rectangles are used\nto compute a feature. For each rectangle, the sum of the pixel intensities is\ncomputed. Comparisons of these sums to that of the central rectangle determine\nthe feature, similarly to LBP.\n\nIntegral image.\n\nRow-coordinate of top left corner of a rectangle containing feature.\n\nColumn-coordinate of top left corner of a rectangle containing feature.\n\nWidth of one of the 9 equal rectangles that will be used to compute a feature.\n\nHeight of one of the 9 equal rectangles that will be used to compute a\nfeature.\n\n8-bit MB-LBP feature descriptor.\n\nFace Detection Based on Multi-Block LBP Representation. Lun Zhang, Rufeng Chu,\nShiming Xiang, Shengcai Liao, Stan Z. Li\nhttp://www.cbsr.ia.ac.cn/users/scliao/papers/Zhang-ICB07-MBLBP.pdf\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.multiscale_basic_features()", "path": "api/skimage.feature#skimage.feature.multiscale_basic_features", "type": "feature", "text": "\nLocal features for a single- or multi-channel nd image.\n\nIntensity, gradient intensity and local structure are computed at different\nscales thanks to Gaussian blurring.\n\nInput image, which can be grayscale or multichannel.\n\nTrue if the last dimension corresponds to color channels.\n\nIf True, pixel intensities averaged over the different scales are added to the\nfeature set.\n\nIf True, intensities of local gradients averaged over the different scales are\nadded to the feature set.\n\nIf True, eigenvalues of the Hessian matrix after Gaussian blurring at\ndifferent scales are added to the feature set.\n\nSmallest value of the Gaussian kernel used to average local neighbourhoods\nbefore extracting features.\n\nLargest value of the Gaussian kernel used to average local neighbourhoods\nbefore extracting features.\n\nNumber of values of the Gaussian kernel between sigma_min and sigma_max. If\nNone, sigma_min multiplied by powers of 2 are used.\n\nThe number of parallel threads to use. If set to `None`, the full set of\navailable cores are used.\n\nArray of shape `image.shape + (n_features,)`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.ORB", "path": "api/skimage.feature#skimage.feature.ORB", "type": "feature", "text": "\nBases: `skimage.feature.util.FeatureDetector`,\n`skimage.feature.util.DescriptorExtractor`\n\nOriented FAST and rotated BRIEF feature detector and binary descriptor\nextractor.\n\nNumber of keypoints to be returned. The function will return the best\n`n_keypoints` according to the Harris corner response if more than\n`n_keypoints` are detected. If not, then all the detected keypoints are\nreturned.\n\nThe `n` parameter in `skimage.feature.corner_fast`. Minimum number of\nconsecutive pixels out of 16 pixels on the circle that should all be either\nbrighter or darker w.r.t test-pixel. A point c on the circle is darker w.r.t\ntest pixel p if `Ic < Ip - threshold` and brighter if `Ic > Ip + threshold`.\nAlso stands for the n in `FAST-n` corner detector.\n\nThe `threshold` parameter in `feature.corner_fast`. Threshold used to decide\nwhether the pixels on the circle are brighter, darker or similar w.r.t. the\ntest pixel. Decrease the threshold when more corners are desired and vice-\nversa.\n\nThe `k` parameter in `skimage.feature.corner_harris`. Sensitivity factor to\nseparate corners from edges, typically in range `[0, 0.2]`. Small values of\n`k` result in detection of sharp corners.\n\nDownscale factor for the image pyramid. Default value 1.2 is chosen so that\nthere are more dense scales which enable robust scale invariance for a\nsubsequent feature description.\n\nMaximum number of scales from the bottom of the image pyramid to extract the\nfeatures from.\n\nEthan Rublee, Vincent Rabaud, Kurt Konolige and Gary Bradski \u201cORB: An\nefficient alternative to SIFT and SURF\u201d\nhttp://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf\n\nKeypoint coordinates as `(row, col)`.\n\nCorresponding scales.\n\nCorresponding orientations in radians.\n\nCorresponding Harris corner responses.\n\n2D array of binary descriptors of size `descriptor_size` for Q keypoints after\nfiltering out border keypoints with value at an index `(i, j)` either being\n`True` or `False` representing the outcome of the intensity comparison for\ni-th keypoint on j-th decision pixel-pair. It is `Q == np.sum(mask)`.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nDetect oriented FAST keypoints along with the corresponding scale.\n\nInput image.\n\nDetect oriented FAST keypoints and extract rBRIEF descriptors.\n\nNote that this is faster than first calling `detect` and then `extract`.\n\nInput image.\n\nExtract rBRIEF binary descriptors for given keypoints in image.\n\nNote that the keypoints must be extracted using the same `downscale` and\n`n_scales` parameters. Additionally, if you want to extract both keypoints and\ndescriptors you should use the faster `detect_and_extract`.\n\nInput image.\n\nKeypoint coordinates as `(row, col)`.\n\nCorresponding scales.\n\nCorresponding orientations in radians.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.ORB.detect()", "path": "api/skimage.feature#skimage.feature.ORB.detect", "type": "feature", "text": "\nDetect oriented FAST keypoints along with the corresponding scale.\n\nInput image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.ORB.detect_and_extract()", "path": "api/skimage.feature#skimage.feature.ORB.detect_and_extract", "type": "feature", "text": "\nDetect oriented FAST keypoints and extract rBRIEF descriptors.\n\nNote that this is faster than first calling `detect` and then `extract`.\n\nInput image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.ORB.extract()", "path": "api/skimage.feature#skimage.feature.ORB.extract", "type": "feature", "text": "\nExtract rBRIEF binary descriptors for given keypoints in image.\n\nNote that the keypoints must be extracted using the same `downscale` and\n`n_scales` parameters. Additionally, if you want to extract both keypoints and\ndescriptors you should use the faster `detect_and_extract`.\n\nInput image.\n\nKeypoint coordinates as `(row, col)`.\n\nCorresponding scales.\n\nCorresponding orientations in radians.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.ORB.__init__()", "path": "api/skimage.feature#skimage.feature.ORB.__init__", "type": "feature", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.peak_local_max()", "path": "api/skimage.feature#skimage.feature.peak_local_max", "type": "feature", "text": "\nFind peaks in an image as coordinate list or boolean mask.\n\nPeaks are the local maxima in a region of `2 * min_distance + 1` (i.e. peaks\nare separated by at least `min_distance`).\n\nIf both `threshold_abs` and `threshold_rel` are provided, the maximum of the\ntwo is chosen as the minimum intensity threshold of peaks.\n\nChanged in version 0.18: Prior to version 0.18, peaks of the same height\nwithin a radius of `min_distance` were all returned, but this could cause\nunexpected behaviour. From 0.18 onwards, an arbitrary peak within the region\nis returned. See issue gh-2592.\n\nInput image.\n\nThe minimal allowed distance separating peaks. To find the maximum number of\npeaks, use `min_distance=1`.\n\nMinimum intensity of peaks. By default, the absolute threshold is the minimum\nintensity of the image.\n\nMinimum intensity of peaks, calculated as `max(image) * threshold_rel`.\n\nIf positive integer, `exclude_border` excludes peaks from within\n`exclude_border`-pixels of the border of the image. If tuple of non-negative\nints, the length of the tuple must match the input array\u2019s dimensionality.\nEach element of the tuple will exclude peaks from within\n`exclude_border`-pixels of the border of the image along that dimension. If\nTrue, takes the `min_distance` parameter as value. If zero or False, peaks are\nidentified regardless of their distance from the border.\n\nIf True, the output will be an array representing peak coordinates. The\ncoordinates are sorted according to peaks values (Larger first). If False, the\noutput will be a boolean array shaped as `image.shape` with peaks present at\nTrue elements. `indices` is deprecated and will be removed in version 0.20.\nDefault behavior will be to always return peak coordinates. You can obtain a\nmask as shown in the example below.\n\nMaximum number of peaks. When the number of peaks exceeds `num_peaks`, return\n`num_peaks` peaks based on highest peak intensity.\n\nIf provided, `footprint == 1` represents the local region within which to\nsearch for peaks at every point in `image`.\n\nIf provided, each unique region `labels == value` represents a unique region\nto search for peaks. Zero is reserved for background.\n\nMaximum number of peaks for each label.\n\nWhich Minkowski p-norm to use. Should be in the range [1, inf]. A finite large\np may cause a ValueError if overflow can occur. `inf` corresponds to the\nChebyshev distance and 2 to the Euclidean distance.\n\nSee also\n\nThe peak local maximum function returns the coordinates of local peaks\n(maxima) in an image. Internally, a maximum filter is used for finding local\nmaxima. This operation dilates the original image. After comparison of the\ndilated and original image, this function returns the coordinates or a mask of\nthe peaks where the dilated image equals the original image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.plot_matches()", "path": "api/skimage.feature#skimage.feature.plot_matches", "type": "feature", "text": "\nPlot matched features.\n\nMatches and image are drawn in this ax.\n\nFirst grayscale or color image.\n\nSecond grayscale or color image.\n\nFirst keypoint coordinates as `(row, col)`.\n\nSecond keypoint coordinates as `(row, col)`.\n\nIndices of corresponding matches in first and second set of descriptors, where\n`matches[:, 0]` denote the indices in the first and `matches[:, 1]` the\nindices in the second set of descriptors.\n\nColor for keypoint locations.\n\nColor for lines which connect keypoint matches. By default the color is chosen\nrandomly.\n\nWhether to only plot matches and not plot the keypoint locations.\n\nWhether to show images side by side, `'horizontal'`, or one above the other,\n`'vertical'`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.register_translation()", "path": "api/skimage.feature#skimage.feature.register_translation", "type": "feature", "text": "\nDeprecated function. Use `skimage.registration.phase_cross_correlation`\ninstead.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.shape_index()", "path": "api/skimage.feature#skimage.feature.shape_index", "type": "feature", "text": "\nCompute the shape index.\n\nThe shape index, as defined by Koenderink & van Doorn [1], is a single valued\nmeasure of local curvature, assuming the image as a 3D plane with intensities\nrepresenting heights.\n\nIt is derived from the eigen values of the Hessian, and its value ranges from\n-1 to 1 (and is undefined (=NaN) in flat regions), with following ranges\nrepresenting following shapes:\n\nInterval (s in \u2026)\n\nShape\n\n[ -1, -7/8)\n\nSpherical cup\n\n[-7/8, -5/8)\n\nThrough\n\n[-5/8, -3/8)\n\nRut\n\n[-3/8, -1/8)\n\nSaddle rut\n\n[-1/8, +1/8)\n\nSaddle\n\n[+1/8, +3/8)\n\nSaddle ridge\n\n[+3/8, +5/8)\n\nRidge\n\n[+5/8, +7/8)\n\nDome\n\n[+7/8, +1]\n\nSpherical cap\n\nInput image.\n\nStandard deviation used for the Gaussian kernel, which is used for smoothing\nthe input data before Hessian eigen value calculation.\n\nHow to handle values outside the image borders\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nShape index\n\nKoenderink, J. J. & van Doorn, A. J., \u201cSurface shape and curvature scales\u201d,\nImage and Vision Computing, 1992, 10, 557-564.\nDOI:10.1016/0262-8856(92)90076-F\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.structure_tensor()", "path": "api/skimage.feature#skimage.feature.structure_tensor", "type": "feature", "text": "\nCompute structure tensor using sum of squared differences.\n\nThe (2-dimensional) structure tensor A is defined as:\n\nwhich is approximated by the weighted sum of squared differences in a local\nwindow around each pixel in the image. This formula can be extended to a\nlarger number of dimensions (see [1]).\n\nInput image.\n\nStandard deviation used for the Gaussian kernel, which is used as a weighting\nfunction for the local summation of squared differences.\n\nHow to handle values outside the image borders.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nNOTE: Only applies in 2D. Higher dimensions must always use \u2018rc\u2019 order. This\nparameter allows for the use of reverse or forward order of the image axes in\ngradient computation. \u2018rc\u2019 indicates the use of the first axis initially (Arr,\nArc, Acc), whilst \u2018xy\u2019 indicates the usage of the last axis initially (Axx,\nAxy, Ayy).\n\nUpper-diagonal elements of the structure tensor for each pixel in the input\nimage.\n\nSee also\n\nhttps://en.wikipedia.org/wiki/Structure_tensor\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.structure_tensor_eigenvalues()", "path": "api/skimage.feature#skimage.feature.structure_tensor_eigenvalues", "type": "feature", "text": "\nCompute eigenvalues of structure tensor.\n\nThe upper-diagonal elements of the structure tensor, as returned by\n`structure_tensor`.\n\nThe eigenvalues of the structure tensor, in decreasing order. The eigenvalues\nare the leading dimension. That is, the coordinate [i, j, k] corresponds to\nthe ith-largest eigenvalue at position (j, k).\n\nSee also\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "feature.structure_tensor_eigvals()", "path": "api/skimage.feature#skimage.feature.structure_tensor_eigvals", "type": "feature", "text": "\nCompute eigenvalues of structure tensor.\n\nElement of the structure tensor for each pixel in the input image.\n\nElement of the structure tensor for each pixel in the input image.\n\nElement of the structure tensor for each pixel in the input image.\n\nLarger eigen value for each input matrix.\n\nSmaller eigen value for each input matrix.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters", "path": "api/skimage.filters", "type": "filters", "text": "\n`skimage.filters.apply_hysteresis_threshold`(\u2026)\n\nApply hysteresis thresholding to `image`.\n\n`skimage.filters.correlate_sparse`(image, kernel)\n\nCompute valid cross-correlation of `padded_array` and `kernel`.\n\n`skimage.filters.difference_of_gaussians`(\u2026)\n\nFind features between `low_sigma` and `high_sigma` in size.\n\n`skimage.filters.farid`(image, *[, mask])\n\nFind the edge magnitude using the Farid transform.\n\n`skimage.filters.farid_h`(image, *[, mask])\n\nFind the horizontal edges of an image using the Farid transform.\n\n`skimage.filters.farid_v`(image, *[, mask])\n\nFind the vertical edges of an image using the Farid transform.\n\n`skimage.filters.frangi`(image[, sigmas, \u2026])\n\nFilter an image with the Frangi vesselness filter.\n\n`skimage.filters.gabor`(image, frequency[, \u2026])\n\nReturn real and imaginary responses to Gabor filter.\n\n`skimage.filters.gabor_kernel`(frequency[, \u2026])\n\nReturn complex 2D Gabor filter kernel.\n\n`skimage.filters.gaussian`(image[, sigma, \u2026])\n\nMulti-dimensional Gaussian filter.\n\n`skimage.filters.hessian`(image[, sigmas, \u2026])\n\nFilter an image with the Hybrid Hessian filter.\n\n`skimage.filters.inverse`(data[, \u2026])\n\nApply the filter in reverse to the given data.\n\n`skimage.filters.laplace`(image[, ksize, mask])\n\nFind the edges of an image using the Laplace operator.\n\n`skimage.filters.median`(image[, selem, out, \u2026])\n\nReturn local median of an image.\n\n`skimage.filters.meijering`(image[, sigmas, \u2026])\n\nFilter an image with the Meijering neuriteness filter.\n\n`skimage.filters.prewitt`(image[, mask, axis, \u2026])\n\nFind the edge magnitude using the Prewitt transform.\n\n`skimage.filters.prewitt_h`(image[, mask])\n\nFind the horizontal edges of an image using the Prewitt transform.\n\n`skimage.filters.prewitt_v`(image[, mask])\n\nFind the vertical edges of an image using the Prewitt transform.\n\n`skimage.filters.rank_order`(image)\n\nReturn an image of the same shape where each pixel is the index of the pixel\nvalue in the ascending order of the unique values of `image`, aka the rank-\norder value.\n\n`skimage.filters.roberts`(image[, mask])\n\nFind the edge magnitude using Roberts\u2019 cross operator.\n\n`skimage.filters.roberts_neg_diag`(image[, mask])\n\nFind the cross edges of an image using the Roberts\u2019 Cross operator.\n\n`skimage.filters.roberts_pos_diag`(image[, mask])\n\nFind the cross edges of an image using Roberts\u2019 cross operator.\n\n`skimage.filters.sato`(image[, sigmas, \u2026])\n\nFilter an image with the Sato tubeness filter.\n\n`skimage.filters.scharr`(image[, mask, axis, \u2026])\n\nFind the edge magnitude using the Scharr transform.\n\n`skimage.filters.scharr_h`(image[, mask])\n\nFind the horizontal edges of an image using the Scharr transform.\n\n`skimage.filters.scharr_v`(image[, mask])\n\nFind the vertical edges of an image using the Scharr transform.\n\n`skimage.filters.sobel`(image[, mask, axis, \u2026])\n\nFind edges in an image using the Sobel filter.\n\n`skimage.filters.sobel_h`(image[, mask])\n\nFind the horizontal edges of an image using the Sobel transform.\n\n`skimage.filters.sobel_v`(image[, mask])\n\nFind the vertical edges of an image using the Sobel transform.\n\n`skimage.filters.threshold_isodata`([image, \u2026])\n\nReturn threshold value(s) based on ISODATA method.\n\n`skimage.filters.threshold_li`(image, *[, \u2026])\n\nCompute threshold value by Li\u2019s iterative Minimum Cross Entropy method.\n\n`skimage.filters.threshold_local`(image, \u2026)\n\nCompute a threshold mask image based on local pixel neighborhood.\n\n`skimage.filters.threshold_mean`(image)\n\nReturn threshold value based on the mean of grayscale values.\n\n`skimage.filters.threshold_minimum`([image, \u2026])\n\nReturn threshold value based on minimum method.\n\n`skimage.filters.threshold_multiotsu`(image[, \u2026])\n\nGenerate `classes`-1 threshold values to divide gray levels in `image`.\n\n`skimage.filters.threshold_niblack`(image[, \u2026])\n\nApplies Niblack local threshold to an array.\n\n`skimage.filters.threshold_otsu`([image, \u2026])\n\nReturn threshold value based on Otsu\u2019s method.\n\n`skimage.filters.threshold_sauvola`(image[, \u2026])\n\nApplies Sauvola local threshold to an array.\n\n`skimage.filters.threshold_triangle`(image[, \u2026])\n\nReturn threshold value based on the triangle algorithm.\n\n`skimage.filters.threshold_yen`([image, \u2026])\n\nReturn threshold value based on Yen\u2019s method.\n\n`skimage.filters.try_all_threshold`(image[, \u2026])\n\nReturns a figure comparing the outputs of different thresholding methods.\n\n`skimage.filters.unsharp_mask`(image[, \u2026])\n\nUnsharp masking filter.\n\n`skimage.filters.wiener`(data[, \u2026])\n\nMinimum Mean Square Error (Wiener) inverse filter.\n\n`skimage.filters.window`(window_type, shape[, \u2026])\n\nReturn an n-dimensional window of a given size and dimensionality.\n\n`skimage.filters.LPIFilter2D`(\u2026)\n\nLinear Position-Invariant Filter (2-dimensional)\n\n`skimage.filters.rank`\n\nApply hysteresis thresholding to `image`.\n\nThis algorithm finds regions where `image` is greater than `high` OR `image`\nis greater than `low` and that region is connected to a region greater than\n`high`.\n\nGrayscale input image.\n\nLower threshold.\n\nHigher threshold.\n\nArray in which `True` indicates the locations where `image` was above the\nhysteresis threshold.\n\nJ. Canny. A computational approach to edge detection. IEEE Transactions on\nPattern Analysis and Machine Intelligence. 1986; vol. 8, pp.679-698.\nDOI:10.1109/TPAMI.1986.4767851\n\nCompute valid cross-correlation of `padded_array` and `kernel`.\n\nThis function is fast when `kernel` is large with many zeros.\n\nSee `scipy.ndimage.correlate` for a description of cross-correlation.\n\nThe input array. If mode is \u2018valid\u2019, this array should already be padded, as a\nmargin of the same shape as kernel will be stripped off.\n\nThe kernel to be correlated. Must have the same number of dimensions as\n`padded_array`. For high performance, it should be sparse (few nonzero\nentries).\n\nSee `scipy.ndimage.correlate` for valid modes. Additionally, mode \u2018valid\u2019 is\naccepted, in which case no padding is applied and the result is the result for\nthe smaller image for which the kernel is entirely inside the original data.\n\nThe result of cross-correlating `image` with `kernel`. If mode \u2018valid\u2019 is\nused, the resulting shape is (M-Q+1, N-R+1,[ \u2026,] P-S+1).\n\nFind features between `low_sigma` and `high_sigma` in size.\n\nThis function uses the Difference of Gaussians method for applying band-pass\nfilters to multi-dimensional arrays. The input array is blurred with two\nGaussian kernels of differing sigmas to produce two intermediate, filtered\nimages. The more-blurred image is then subtracted from the less-blurred image.\nThe final output image will therefore have had high-frequency components\nattenuated by the smaller-sigma Gaussian, and low frequency components will\nhave been removed due to their presence in the more-blurred intermediate.\n\nInput array to filter.\n\nStandard deviation(s) for the Gaussian kernel with the smaller sigmas across\nall axes. The standard deviations are given for each axis as a sequence, or as\na single number, in which case the single number is used as the standard\ndeviation value for all axes.\n\nStandard deviation(s) for the Gaussian kernel with the larger sigmas across\nall axes. The standard deviations are given for each axis as a sequence, or as\na single number, in which case the single number is used as the standard\ndeviation value for all axes. If None is given (default), sigmas for all axes\nare calculated as 1.6 * low_sigma.\n\nThe `mode` parameter determines how the array borders are handled, where\n`cval` is the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.\n\nValue to fill past edges of input if `mode` is \u2018constant\u2019. Default is 0.0\n\nWhether the last axis of the image is to be interpreted as multiple channels.\nIf True, each channel is filtered separately (channels are not mixed\ntogether).\n\nTruncate the filter at this many standard deviations.\n\nthe filtered array.\n\nSee also\n\nThis function will subtract an array filtered with a Gaussian kernel with\nsigmas given by `high_sigma` from an array filtered with a Gaussian kernel\nwith sigmas provided by `low_sigma`. The values for `high_sigma` must always\nbe greater than or equal to the corresponding values in `low_sigma`, or a\n`ValueError` will be raised.\n\nWhen `high_sigma` is none, the values for `high_sigma` will be calculated as\n1.6x the corresponding values in `low_sigma`. This ratio was originally\nproposed by Marr and Hildreth (1980) [1] and is commonly used when\napproximating the inverted Laplacian of Gaussian, which is used in edge and\nblob detection.\n\nInput image is converted according to the conventions of `img_as_float`.\n\nExcept for sigma values, all parameters are used for both filters.\n\nMarr, D. and Hildreth, E. Theory of Edge Detection. Proc. R. Soc. Lond. Series\nB 207, 187-217 (1980). https://doi.org/10.1098/rspb.1980.0020\n\nApply a simple Difference of Gaussians filter to a color image:\n\nApply a Laplacian of Gaussian filter as approximated by the Difference of\nGaussians filter:\n\nApply a Difference of Gaussians filter to a grayscale image using different\nsigma values for each axis:\n\nFind the edge magnitude using the Farid transform.\n\nImage to process.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Farid edge map.\n\nSee also\n\nTake the square root of the sum of the squares of the horizontal and vertical\nderivatives to get a magnitude that is somewhat insensitive to direction.\nSimilar to the Scharr operator, this operator is designed with a rotation\ninvariance constraint.\n\nFarid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional\nsignals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004.\nDOI:10.1109/TIP.2004.823819\n\nWikipedia, \u201cFarid and Simoncelli Derivatives.\u201d Available at:\n<https://en.wikipedia.org/wiki/Image_derivatives#Farid_and_Simoncelli_Derivatives>\n\nFind the horizontal edges of an image using the Farid transform.\n\nImage to process.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Farid edge map.\n\nThe kernel was constructed using the 5-tap weights from [1].\n\nFarid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional\nsignals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004.\nDOI:10.1109/TIP.2004.823819\n\nFarid, H. and Simoncelli, E. P. \u201cOptimally rotation-equivariant directional\nderivative kernels\u201d, In: 7th International Conference on Computer Analysis of\nImages and Patterns, Kiel, Germany. Sep, 1997.\n\nFind the vertical edges of an image using the Farid transform.\n\nImage to process.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Farid edge map.\n\nThe kernel was constructed using the 5-tap weights from [1].\n\nFarid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional\nsignals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004.\nDOI:10.1109/TIP.2004.823819\n\nFilter an image with the Frangi vesselness filter.\n\nThis filter can be used to detect continuous ridges, e.g. vessels, wrinkles,\nrivers. It can be used to calculate the fraction of the whole image containing\nsuch objects.\n\nDefined only for 2-D and 3-D images. Calculates the eigenvectors of the\nHessian to compute the similarity of an image region to vessels, according to\nthe method described in [1].\n\nArray with input image data.\n\nSigmas used as scales of filter, i.e., np.arange(scale_range[0],\nscale_range[1], scale_step)\n\nThe range of sigmas used.\n\nStep size between sigmas.\n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation\nfrom a plate-like structure.\n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation\nfrom a blob-like structure.\n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to areas of\nhigh variance/texture/structure.\n\nWhen True (the default), the filter detects black ridges; when False, it\ndetects white ridges.\n\nHow to handle values outside the image borders.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nFiltered image (maximum of pixels across all scales).\n\nSee also\n\nWritten by Marc Schrijver, November 2001 Re-Written by D. J. Kroon, University\nof Twente, May 2009, [2] Adoption of 3D version from D. G. Ellis, Januar\n20017, [3]\n\nFrangi, A. F., Niessen, W. J., Vincken, K. L., & Viergever, M. A. (1998,).\nMultiscale vessel enhancement filtering. In International Conference on\nMedical Image Computing and Computer-Assisted Intervention (pp. 130-137).\nSpringer Berlin Heidelberg. DOI:10.1007/BFb0056195\n\nKroon, D. J.: Hessian based Frangi vesselness filter.\n\nEllis, D. G.: https://github.com/ellisdg/frangi3d/tree/master/frangi\n\nReturn real and imaginary responses to Gabor filter.\n\nThe real and imaginary parts of the Gabor filter kernel are applied to the\nimage and the response is returned as a pair of arrays.\n\nGabor filter is a linear filter with a Gaussian kernel which is modulated by a\nsinusoidal plane wave. Frequency and orientation representations of the Gabor\nfilter are similar to those of the human visual system. Gabor filter banks are\ncommonly used in computer vision and image processing. They are especially\nsuitable for edge detection and texture classification.\n\nInput image.\n\nSpatial frequency of the harmonic function. Specified in pixels.\n\nOrientation in radians. If 0, the harmonic is in the x-direction.\n\nThe bandwidth captured by the filter. For fixed bandwidth, `sigma_x` and\n`sigma_y` will decrease with increasing frequency. This value is ignored if\n`sigma_x` and `sigma_y` are set by the user.\n\nStandard deviation in x- and y-directions. These directions apply to the\nkernel before rotation. If `theta = pi/2`, then the kernel is rotated 90\ndegrees so that `sigma_x` controls the vertical direction.\n\nThe linear size of the kernel is n_stds (3 by default) standard deviations.\n\nPhase offset of harmonic function in radians.\n\nMode used to convolve image with a kernel, passed to `ndi.convolve`\n\nValue to fill past edges of input if `mode` of convolution is \u2018constant\u2019. The\nparameter is passed to `ndi.convolve`.\n\nFiltered images using the real and imaginary parts of the Gabor filter kernel.\nImages are of the same dimensions as the input one.\n\nhttps://en.wikipedia.org/wiki/Gabor_filter\n\nhttps://web.archive.org/web/20180127125930/http://mplab.ucsd.edu/tutorials/gabor.pdf\n\nReturn complex 2D Gabor filter kernel.\n\nGabor kernel is a Gaussian kernel modulated by a complex harmonic function.\nHarmonic function consists of an imaginary sine function and a real cosine\nfunction. Spatial frequency is inversely proportional to the wavelength of the\nharmonic and to the standard deviation of a Gaussian kernel. The bandwidth is\nalso inversely proportional to the standard deviation.\n\nSpatial frequency of the harmonic function. Specified in pixels.\n\nOrientation in radians. If 0, the harmonic is in the x-direction.\n\nThe bandwidth captured by the filter. For fixed bandwidth, `sigma_x` and\n`sigma_y` will decrease with increasing frequency. This value is ignored if\n`sigma_x` and `sigma_y` are set by the user.\n\nStandard deviation in x- and y-directions. These directions apply to the\nkernel before rotation. If `theta = pi/2`, then the kernel is rotated 90\ndegrees so that `sigma_x` controls the vertical direction.\n\nThe linear size of the kernel is n_stds (3 by default) standard deviations\n\nPhase offset of harmonic function in radians.\n\nComplex filter kernel.\n\nhttps://en.wikipedia.org/wiki/Gabor_filter\n\nhttps://web.archive.org/web/20180127125930/http://mplab.ucsd.edu/tutorials/gabor.pdf\n\nMulti-dimensional Gaussian filter.\n\nInput image (grayscale or color) to filter.\n\nStandard deviation for Gaussian kernel. The standard deviations of the\nGaussian filter are given for each axis as a sequence, or as a single number,\nin which case it is equal for all axes.\n\nThe `output` parameter passes an array in which to store the filter output.\n\nThe `mode` parameter determines how the array borders are handled, where\n`cval` is the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.\n\nValue to fill past edges of input if `mode` is \u2018constant\u2019. Default is 0.0\n\nWhether the last axis of the image is to be interpreted as multiple channels.\nIf True, each channel is filtered separately (channels are not mixed\ntogether). Only 3 channels are supported. If `None`, the function will attempt\nto guess this, and raise a warning if ambiguous, when the array has shape (M,\nN, 3).\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nTruncate the filter at this many standard deviations.\n\nthe filtered array\n\nThis function is a wrapper around `scipy.ndi.gaussian_filter()`.\n\nInteger arrays are converted to float.\n\nThe `output` should be floating point data type since gaussian converts to\nfloat provided `image`. If `output` is not provided, another array will be\nallocated and returned as the result.\n\nThe multi-dimensional filter is implemented as a sequence of one-dimensional\nconvolution filters. The intermediate arrays are stored in the same data type\nas the output. Therefore, for output types with a limited precision, the\nresults may be imprecise because intermediate results may be stored with\ninsufficient precision.\n\nFilter an image with the Hybrid Hessian filter.\n\nThis filter can be used to detect continuous edges, e.g. vessels, wrinkles,\nrivers. It can be used to calculate the fraction of the whole image containing\nsuch objects.\n\nDefined only for 2-D and 3-D images. Almost equal to Frangi filter, but uses\nalternative method of smoothing. Refer to [1] to find the differences between\nFrangi and Hessian filters.\n\nArray with input image data.\n\nSigmas used as scales of filter, i.e., np.arange(scale_range[0],\nscale_range[1], scale_step)\n\nThe range of sigmas used.\n\nStep size between sigmas.\n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation\nfrom a blob-like structure.\n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to areas of\nhigh variance/texture/structure.\n\nWhen True (the default), the filter detects black ridges; when False, it\ndetects white ridges.\n\nHow to handle values outside the image borders.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nFiltered image (maximum of pixels across all scales).\n\nSee also\n\nWritten by Marc Schrijver (November 2001) Re-Written by D. J. Kroon University\nof Twente (May 2009) [2]\n\nNg, C. C., Yap, M. H., Costen, N., & Li, B. (2014,). Automatic wrinkle\ndetection using hybrid Hessian filter. In Asian Conference on Computer Vision\n(pp. 609-622). Springer International Publishing.\nDOI:10.1007/978-3-319-16811-1_40\n\nKroon, D. J.: Hessian based Frangi vesselness filter.\n\nApply the filter in reverse to the given data.\n\nInput data.\n\nImpulse response of the filter. See LPIFilter2D.__init__.\n\nAdditional keyword parameters to the impulse_response function.\n\nLimit the filter gain. Often, the filter contains zeros, which would cause the\ninverse filter to have infinite gain. High gain causes amplification of\nartefacts, so a conservative limit is recommended.\n\nIf you need to apply the same filter multiple times over different images,\nconstruct the LPIFilter2D and specify it here.\n\nFind the edges of an image using the Laplace operator.\n\nImage to process.\n\nDefine the size of the discrete Laplacian operator such that it will have a\nsize of (ksize,) * image.ndim.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Laplace edge map.\n\nThe Laplacian operator is generated using the function\nskimage.restoration.uft.laplacian().\n\nReturn local median of an image.\n\nInput image.\n\nIf `behavior=='rank'`, `selem` is a 2-D array of 1\u2019s and 0\u2019s. If\n`behavior=='ndimage'`, `selem` is a N-D array of 1\u2019s and 0\u2019s with the same\nnumber of dimension than `image`. If None, `selem` will be a N-D array with 3\nelements for each dimension (e.g., vector, square, cube, etc.)\n\nIf None, a new array is allocated.\n\nThe mode parameter determines how the array borders are handled, where `cval`\nis the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.\n\nNew in version 0.15: `mode` is used when `behavior='ndimage'`.\n\nValue to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0\n\nNew in version 0.15: `cval` was added in 0.15 is used when\n`behavior='ndimage'`.\n\nEither to use the old behavior (i.e., < 0.15) or the new behavior. The old\nbehavior will call the `skimage.filters.rank.median()`. The new behavior will\ncall the `scipy.ndimage.median_filter()`. Default is \u2018ndimage\u2019.\n\nNew in version 0.15: `behavior` is introduced in 0.15\n\nChanged in version 0.16: Default `behavior` has been changed from \u2018rank\u2019 to\n\u2018ndimage\u2019\n\nOutput image.\n\nSee also\n\nRank-based implementation of the median filtering offering more flexibility\nwith additional parameters but dedicated for unsigned integer images.\n\nFilter an image with the Meijering neuriteness filter.\n\nThis filter can be used to detect continuous ridges, e.g. neurites, wrinkles,\nrivers. It can be used to calculate the fraction of the whole image containing\nsuch objects.\n\nCalculates the eigenvectors of the Hessian to compute the similarity of an\nimage region to neurites, according to the method described in [1].\n\nArray with input image data.\n\nSigmas used as scales of filter\n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation\nfrom a plate-like structure.\n\nWhen True (the default), the filter detects black ridges; when False, it\ndetects white ridges.\n\nHow to handle values outside the image borders.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nFiltered image (maximum of pixels across all scales).\n\nSee also\n\nMeijering, E., Jacob, M., Sarria, J. C., Steiner, P., Hirling, H., Unser, M.\n(2004). Design and validation of a tool for neurite tracing and analysis in\nfluorescence microscopy images. Cytometry Part A, 58(2), 167-176.\nDOI:10.1002/cyto.a.20022\n\nFind the edge magnitude using the Prewitt transform.\n\nThe input image.\n\nClip the output image to this mask. (Values where mask=0 will be set to 0.)\n\nCompute the edge filter along this axis. If not provided, the edge magnitude\nis computed. This is defined as:\n\nThe magnitude is also computed if axis is a sequence.\n\nThe boundary mode for the convolution. See `scipy.ndimage.convolve` for a\ndescription of the modes. This can be either a single boundary mode or one\nboundary mode per axis.\n\nWhen `mode` is `'constant'`, this is the constant used in values outside the\nboundary of the image data.\n\nThe Prewitt edge map.\n\nSee also\n\nThe edge magnitude depends slightly on edge directions, since the\napproximation of the gradient operator by the Prewitt operator is not\ncompletely rotation invariant. For a better rotation invariance, the Scharr\noperator should be used. The Sobel operator has a better rotation invariance\nthan the Prewitt operator, but a worse rotation invariance than the Scharr\noperator.\n\nFind the horizontal edges of an image using the Prewitt transform.\n\nImage to process.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Prewitt edge map.\n\nWe use the following kernel:\n\nFind the vertical edges of an image using the Prewitt transform.\n\nImage to process.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Prewitt edge map.\n\nWe use the following kernel:\n\nReturn an image of the same shape where each pixel is the index of the pixel\nvalue in the ascending order of the unique values of `image`, aka the rank-\norder value.\n\nNew array where each pixel has the rank-order value of the corresponding pixel\nin `image`. Pixel values are between 0 and n - 1, where n is the number of\ndistinct unique values in `image`.\n\nUnique original values of `image`\n\nFind the edge magnitude using Roberts\u2019 cross operator.\n\nImage to process.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Roberts\u2019 Cross edge map.\n\nSee also\n\nFind the cross edges of an image using the Roberts\u2019 Cross operator.\n\nThe kernel is applied to the input image to produce separate measurements of\nthe gradient component one orientation.\n\nImage to process.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Robert\u2019s edge map.\n\nWe use the following kernel:\n\nFind the cross edges of an image using Roberts\u2019 cross operator.\n\nThe kernel is applied to the input image to produce separate measurements of\nthe gradient component one orientation.\n\nImage to process.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Robert\u2019s edge map.\n\nWe use the following kernel:\n\nFilter an image with the Sato tubeness filter.\n\nThis filter can be used to detect continuous ridges, e.g. tubes, wrinkles,\nrivers. It can be used to calculate the fraction of the whole image containing\nsuch objects.\n\nDefined only for 2-D and 3-D images. Calculates the eigenvectors of the\nHessian to compute the similarity of an image region to tubes, according to\nthe method described in [1].\n\nArray with input image data.\n\nSigmas used as scales of filter.\n\nWhen True (the default), the filter detects black ridges; when False, it\ndetects white ridges.\n\nHow to handle values outside the image borders.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nFiltered image (maximum of pixels across all scales).\n\nSee also\n\nSato, Y., Nakajima, S., Shiraga, N., Atsumi, H., Yoshida, S., Koller, T., \u2026,\nKikinis, R. (1998). Three-dimensional multi-scale line filter for segmentation\nand visualization of curvilinear structures in medical images. Medical image\nanalysis, 2(2), 143-168. DOI:10.1016/S1361-8415(98)80009-1\n\nFind the edge magnitude using the Scharr transform.\n\nThe input image.\n\nClip the output image to this mask. (Values where mask=0 will be set to 0.)\n\nCompute the edge filter along this axis. If not provided, the edge magnitude\nis computed. This is defined as:\n\nThe magnitude is also computed if axis is a sequence.\n\nThe boundary mode for the convolution. See `scipy.ndimage.convolve` for a\ndescription of the modes. This can be either a single boundary mode or one\nboundary mode per axis.\n\nWhen `mode` is `'constant'`, this is the constant used in values outside the\nboundary of the image data.\n\nThe Scharr edge map.\n\nSee also\n\nThe Scharr operator has a better rotation invariance than other edge filters\nsuch as the Sobel or the Prewitt operators.\n\nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of\nKernel Based Image Derivatives.\n\nhttps://en.wikipedia.org/wiki/Sobel_operator#Alternative_operators\n\nFind the horizontal edges of an image using the Scharr transform.\n\nImage to process.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Scharr edge map.\n\nWe use the following kernel:\n\nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of\nKernel Based Image Derivatives.\n\nFind the vertical edges of an image using the Scharr transform.\n\nImage to process\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Scharr edge map.\n\nWe use the following kernel:\n\nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of\nKernel Based Image Derivatives.\n\nFind edges in an image using the Sobel filter.\n\nThe input image.\n\nClip the output image to this mask. (Values where mask=0 will be set to 0.)\n\nCompute the edge filter along this axis. If not provided, the edge magnitude\nis computed. This is defined as:\n\nThe magnitude is also computed if axis is a sequence.\n\nThe boundary mode for the convolution. See `scipy.ndimage.convolve` for a\ndescription of the modes. This can be either a single boundary mode or one\nboundary mode per axis.\n\nWhen `mode` is `'constant'`, this is the constant used in values outside the\nboundary of the image data.\n\nThe Sobel edge map.\n\nSee also\n\nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of\nKernel Based Image Derivatives.\n\nhttps://en.wikipedia.org/wiki/Sobel_operator\n\nFlood Fill\n\nFind the horizontal edges of an image using the Sobel transform.\n\nImage to process.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Sobel edge map.\n\nWe use the following kernel:\n\nFind the vertical edges of an image using the Sobel transform.\n\nImage to process.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Sobel edge map.\n\nWe use the following kernel:\n\nReturn threshold value(s) based on ISODATA method.\n\nHistogram-based threshold, known as Ridler-Calvard method or inter-means.\nThreshold values returned satisfy the following equality:\n\nThat is, returned thresholds are intensities that separate the image into two\ngroups of pixels, where the threshold intensity is midway between the mean\nintensities of these groups.\n\nFor integer images, the above equality holds to within one; for floating-\npoint images, the equality holds to within the histogram bin-width.\n\nEither image or hist must be provided. In case hist is given, the actual\nhistogram of the image is ignored.\n\nInput image.\n\nNumber of bins used to calculate histogram. This value is ignored for integer\narrays.\n\nIf False (default), return only the lowest threshold that satisfies the above\nequality. If True, return all valid thresholds.\n\nHistogram to determine the threshold from and a corresponding array of bin\ncenter intensities. Alternatively, only the histogram can be passed.\n\nThreshold value(s).\n\nRidler, TW & Calvard, S (1978), \u201cPicture thresholding using an iterative\nselection method\u201d IEEE Transactions on Systems, Man and Cybernetics 8:\n630-632, DOI:10.1109/TSMC.1978.4310039\n\nSezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and\nQuantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1):\n146-165,\nhttp://www.busim.ee.boun.edu.tr/~sankur/SankurFolder/Threshold_survey.pdf\nDOI:10.1117/1.1631315\n\nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold\n\nCompute threshold value by Li\u2019s iterative Minimum Cross Entropy method.\n\nInput image.\n\nFinish the computation when the change in the threshold in an iteration is\nless than this value. By default, this is half the smallest difference between\nintensity values in `image`.\n\nLi\u2019s iterative method uses gradient descent to find the optimal threshold. If\nthe image intensity histogram contains more than two modes (peaks), the\ngradient descent could get stuck in a local optimum. An initial guess for the\niteration can help the algorithm find the globally-optimal threshold. A float\nvalue defines a specific start point, while a callable should take in an array\nof image intensities and return a float value. Example valid callables include\n`numpy.mean` (default), `lambda arr: numpy.quantile(arr, 0.95)`, or even\n`skimage.filters.threshold_otsu()`.\n\nA function that will be called on the threshold at every iteration of the\nalgorithm.\n\nUpper threshold value. All pixels with an intensity higher than this value are\nassumed to be foreground.\n\nLi C.H. and Lee C.K. (1993) \u201cMinimum Cross Entropy Thresholding\u201d Pattern\nRecognition, 26(4): 617-625 DOI:10.1016/0031-3203(93)90115-D\n\nLi C.H. and Tam P.K.S. (1998) \u201cAn Iterative Algorithm for Minimum Cross\nEntropy Thresholding\u201d Pattern Recognition Letters, 18(8): 771-776\nDOI:10.1016/S0167-8655(98)00057-9\n\nSezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and\nQuantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1):\n146-165 DOI:10.1117/1.1631315\n\nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold\n\nCompute a threshold mask image based on local pixel neighborhood.\n\nAlso known as adaptive or dynamic thresholding. The threshold value is the\nweighted mean for the local neighborhood of a pixel subtracted by a constant.\nAlternatively the threshold can be determined dynamically by a given function,\nusing the \u2018generic\u2019 method.\n\nInput image.\n\nOdd size of pixel neighborhood which is used to calculate the threshold value\n(e.g. 3, 5, 7, \u2026, 21, \u2026).\n\nMethod used to determine adaptive threshold for local neighbourhood in\nweighted mean image.\n\nBy default the \u2018gaussian\u2019 method is used.\n\nConstant subtracted from weighted mean of neighborhood to calculate the local\nthreshold value. Default offset is 0.\n\nThe mode parameter determines how the array borders are handled, where cval is\nthe value when mode is equal to \u2018constant\u2019. Default is \u2018reflect\u2019.\n\nEither specify sigma for \u2018gaussian\u2019 method or function object for \u2018generic\u2019\nmethod. This functions takes the flat array of local neighbourhood as a single\nargument and returns the calculated threshold for the centre pixel.\n\nValue to fill past edges of input if mode is \u2018constant\u2019.\n\nThreshold image. All pixels in the input image higher than the corresponding\npixel in the threshold image are considered foreground.\n\nhttps://docs.opencv.org/modules/imgproc/doc/miscellaneous_transformations.html?highlight=threshold#adaptivethreshold\n\nReturn threshold value based on the mean of grayscale values.\n\nGrayscale input image.\n\nUpper threshold value. All pixels with an intensity higher than this value are\nassumed to be foreground.\n\nC. A. Glasbey, \u201cAn analysis of histogram-based thresholding algorithms,\u201d\nCVGIP: Graphical Models and Image Processing, vol. 55, pp. 532-537, 1993.\nDOI:10.1006/cgip.1993.1040\n\nReturn threshold value based on minimum method.\n\nThe histogram of the input `image` is computed if not provided and smoothed\nuntil there are only two maxima. Then the minimum in between is the threshold\nvalue.\n\nEither image or hist must be provided. In case hist is given, the actual\nhistogram of the image is ignored.\n\nInput image.\n\nNumber of bins used to calculate histogram. This value is ignored for integer\narrays.\n\nMaximum number of iterations to smooth the histogram.\n\nHistogram to determine the threshold from and a corresponding array of bin\ncenter intensities. Alternatively, only the histogram can be passed.\n\nUpper threshold value. All pixels with an intensity higher than this value are\nassumed to be foreground.\n\nIf unable to find two local maxima in the histogram or if the smoothing takes\nmore than 1e4 iterations.\n\nC. A. Glasbey, \u201cAn analysis of histogram-based thresholding algorithms,\u201d\nCVGIP: Graphical Models and Image Processing, vol. 55, pp. 532-537, 1993.\n\nPrewitt, JMS & Mendelsohn, ML (1966), \u201cThe analysis of cell images\u201d, Annals of\nthe New York Academy of Sciences 128: 1035-1053\nDOI:10.1111/j.1749-6632.1965.tb11715.x\n\nGenerate `classes`-1 threshold values to divide gray levels in `image`.\n\nThe threshold values are chosen to maximize the total sum of pairwise\nvariances between the thresholded graylevel classes. See Notes and [1] for\nmore details.\n\nGrayscale input image.\n\nNumber of classes to be thresholded, i.e. the number of resulting regions.\n\nNumber of bins used to calculate the histogram. This value is ignored for\ninteger arrays.\n\nArray containing the threshold values for the desired classes.\n\nIf `image` contains less grayscale value then the desired number of classes.\n\nThis implementation relies on a Cython function whose complexity is\n\\\\(O\\left(\\frac{Ch^{C-1}}{(C-1)!}\\right)\\\\), where \\\\(h\\\\) is the number of\nhistogram bins and \\\\(C\\\\) is the number of classes desired.\n\nThe input image must be grayscale.\n\nLiao, P-S., Chen, T-S. and Chung, P-C., \u201cA fast algorithm for multilevel\nthresholding\u201d, Journal of Information Science and Engineering 17 (5): 713-727,\n2001. Available at: <https://ftp.iis.sinica.edu.tw/JISE/2001/200109_01.pdf>\nDOI:10.6688/JISE.2001.17.5.1\n\nTosa, Y., \u201cMulti-Otsu Threshold\u201d, a java plugin for ImageJ. Available at:\n<http://imagej.net/plugins/download/Multi_OtsuThreshold.java>\n\nMulti-Otsu Thresholding\n\nSegment human cells (in mitosis)\n\nApplies Niblack local threshold to an array.\n\nA threshold T is calculated for every pixel in the image using the following\nformula:\n\nwhere m(x,y) and s(x,y) are the mean and standard deviation of pixel (x,y)\nneighborhood defined by a rectangular window with size w times w centered\naround the pixel. k is a configurable parameter that weights the effect of\nstandard deviation.\n\nInput image.\n\nWindow size specified as a single odd integer (3, 5, 7, \u2026), or an iterable of\nlength `image.ndim` containing only odd integers (e.g. `(1, 5, 5)`).\n\nValue of parameter k in threshold formula.\n\nThreshold mask. All pixels with an intensity higher than this value are\nassumed to be foreground.\n\nThis algorithm is originally designed for text recognition.\n\nThe Bradley threshold is a particular case of the Niblack one, being\nequivalent to\n\nfor some value `q`. By default, Bradley and Roth use `q=1`.\n\nW. Niblack, An introduction to Digital Image Processing, Prentice-Hall, 1986.\n\nD. Bradley and G. Roth, \u201cAdaptive thresholding using Integral Image\u201d, Journal\nof Graphics Tools 12(2), pp. 13-21, 2007. DOI:10.1080/2151237X.2007.10129236\n\nReturn threshold value based on Otsu\u2019s method.\n\nEither image or hist must be provided. If hist is provided, the actual\nhistogram of the image is ignored.\n\nGrayscale input image.\n\nNumber of bins used to calculate histogram. This value is ignored for integer\narrays.\n\nHistogram from which to determine the threshold, and optionally a\ncorresponding array of bin center intensities. An alternative use of this\nfunction is to pass it only hist.\n\nUpper threshold value. All pixels with an intensity higher than this value are\nassumed to be foreground.\n\nThe input image must be grayscale.\n\nWikipedia, https://en.wikipedia.org/wiki/Otsu\u2019s_Method\n\nMeasure region properties\n\nRank filters\n\nApplies Sauvola local threshold to an array. Sauvola is a modification of\nNiblack technique.\n\nIn the original method a threshold T is calculated for every pixel in the\nimage using the following formula:\n\nwhere m(x,y) and s(x,y) are the mean and standard deviation of pixel (x,y)\nneighborhood defined by a rectangular window with size w times w centered\naround the pixel. k is a configurable parameter that weights the effect of\nstandard deviation. R is the maximum standard deviation of a greyscale image.\n\nInput image.\n\nWindow size specified as a single odd integer (3, 5, 7, \u2026), or an iterable of\nlength `image.ndim` containing only odd integers (e.g. `(1, 5, 5)`).\n\nValue of the positive parameter k.\n\nValue of R, the dynamic range of standard deviation. If None, set to the half\nof the image dtype range.\n\nThreshold mask. All pixels with an intensity higher than this value are\nassumed to be foreground.\n\nThis algorithm is originally designed for text recognition.\n\nJ. Sauvola and M. Pietikainen, \u201cAdaptive document image binarization,\u201d Pattern\nRecognition 33(2), pp. 225-236, 2000. DOI:10.1016/S0031-3203(99)00055-2\n\nReturn threshold value based on the triangle algorithm.\n\nGrayscale input image.\n\nNumber of bins used to calculate histogram. This value is ignored for integer\narrays.\n\nUpper threshold value. All pixels with an intensity higher than this value are\nassumed to be foreground.\n\nZack, G. W., Rogers, W. E. and Latt, S. A., 1977, Automatic Measurement of\nSister Chromatid Exchange Frequency, Journal of Histochemistry and\nCytochemistry 25 (7), pp. 741-753 DOI:10.1177/25.7.70454\n\nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold\n\nReturn threshold value based on Yen\u2019s method. Either image or hist must be\nprovided. In case hist is given, the actual histogram of the image is ignored.\n\nInput image.\n\nNumber of bins used to calculate histogram. This value is ignored for integer\narrays.\n\nHistogram from which to determine the threshold, and optionally a\ncorresponding array of bin center intensities. An alternative use of this\nfunction is to pass it only hist.\n\nUpper threshold value. All pixels with an intensity higher than this value are\nassumed to be foreground.\n\nYen J.C., Chang F.J., and Chang S. (1995) \u201cA New Criterion for Automatic\nMultilevel Thresholding\u201d IEEE Trans. on Image Processing, 4(3): 370-378.\nDOI:10.1109/83.366472\n\nSezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and\nQuantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1):\n146-165, DOI:10.1117/1.1631315\nhttp://www.busim.ee.boun.edu.tr/~sankur/SankurFolder/Threshold_survey.pdf\n\nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold\n\nReturns a figure comparing the outputs of different thresholding methods.\n\nInput image.\n\nFigure size (in inches).\n\nPrint function name for each method.\n\nMatplotlib figure and axes.\n\nThe following algorithms are used:\n\nUnsharp masking filter.\n\nThe sharp details are identified as the difference between the original image\nand its blurred version. These details are then scaled, and added back to the\noriginal image.\n\nInput image.\n\nIf a scalar is given, then its value is used for all dimensions. If sequence\nis given, then there must be exactly one radius for each dimension except the\nlast dimension for multichannel images. Note that 0 radius means no blurring,\nand negative values are not allowed.\n\nThe details will be amplified with this factor. The factor could be 0 or\nnegative. Typically, it is a small positive number, e.g. 1.0.\n\nIf True, the last `image` dimension is considered as a color channel,\notherwise as spatial. Color channels are processed individually.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nImage with unsharp mask applied.\n\nUnsharp masking is an image sharpening technique. It is a linear image\noperation, and numerically stable, unlike deconvolution which is an ill-posed\nproblem. Because of this stability, it is often preferred over deconvolution.\n\nThe main idea is as follows: sharp details are identified as the difference\nbetween the original image and its blurred version. These details are added\nback to the original image after a scaling step:\n\nenhanced image = original + amount * (original - blurred)\n\nWhen applying this filter to several color layers independently, color\nbleeding may occur. More visually pleasing result can be achieved by\nprocessing only the brightness/lightness/intensity channel in a suitable color\nspace such as HSV, HSL, YUV, or YCbCr.\n\nUnsharp masking is described in most introductory digital image processing\nbooks. This implementation is based on [1].\n\nMaria Petrou, Costas Petrou \u201cImage Processing: The Fundamentals\u201d, (2010), ed\nii., page 357, ISBN 13: 9781119994398 DOI:10.1002/9781119994398\n\nWikipedia. Unsharp masking https://en.wikipedia.org/wiki/Unsharp_masking\n\nMinimum Mean Square Error (Wiener) inverse filter.\n\nInput data.\n\nRatio between power spectrum of noise and undegraded image.\n\nImpulse response of the filter. See LPIFilter2D.__init__.\n\nAdditional keyword parameters to the impulse_response function.\n\nIf you need to apply the same filter multiple times over different images,\nconstruct the LPIFilter2D and specify it here.\n\nReturn an n-dimensional window of a given size and dimensionality.\n\nThe type of window to be created. Any window type supported by\n`scipy.signal.get_window` is allowed here. See notes below for a current list,\nor the SciPy documentation for the version of SciPy on your machine.\n\nThe shape of the window along each axis. If an integer is provided, a 1D\nwindow is generated.\n\nKeyword arguments passed to `skimage.transform.warp` (e.g.,\n`warp_kwargs={'order':3}` to change interpolation method).\n\nA window of the specified `shape`. `dtype` is `np.double`.\n\nThis function is based on `scipy.signal.get_window` and thus can access all of\nthe window types available to that function (e.g., `\"hann\"`, `\"boxcar\"`). Note\nthat certain window types require parameters that have to be supplied with the\nwindow name as a tuple (e.g., `(\"tukey\", 0.8)`). If only a float is supplied,\nit is interpreted as the beta parameter of the Kaiser window.\n\nSee\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.windows.get_window.html\nfor more details.\n\nNote that this function generates a double precision array of the specified\n`shape` and can thus generate very large arrays that consume a large amount of\navailable memory.\n\nThe approach taken here to create nD windows is to first calculate the\nEuclidean distance from the center of the intended nD window to each position\nin the array. That distance is used to sample, with interpolation, from a 1D\nwindow returned from `scipy.signal.get_window`. The method of interpolation\ncan be changed with the `order` keyword argument passed to\n`skimage.transform.warp`.\n\nSome coordinates in the output window will be outside of the original signal;\nthese will be filled in with zeros.\n\nWindow types: - boxcar - triang - blackman - hamming - hann - bartlett -\nflattop - parzen - bohman - blackmanharris - nuttall - barthann - kaiser\n(needs beta) - gaussian (needs standard deviation) - general_gaussian (needs\npower, width) - slepian (needs width) - dpss (needs normalized half-bandwidth)\n- chebwin (needs attenuation) - exponential (needs decay scale) - tukey (needs\ntaper fraction)\n\nTwo-dimensional window design, Wikipedia,\nhttps://en.wikipedia.org/wiki/Two_dimensional_window_design\n\nReturn a Hann window with shape (512, 512):\n\nReturn a Kaiser window with beta parameter of 16 and shape (256, 256, 35):\n\nReturn a Tukey window with an alpha parameter of 0.8 and shape (100, 300):\n\nBases: `object`\n\nLinear Position-Invariant Filter (2-dimensional)\n\nFunction that yields the impulse response. `r` and `c` are 1-dimensional\nvectors that represent row and column positions, in other words coordinates\nare (r[0],c[0]),(r[0],c[1]) etc. `**filter_params` are passed through.\n\nIn other words, `impulse_response` would be called like this:\n\nGaussian filter: Use a 1-D gaussian in each direction without normalization\ncoefficients.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.apply_hysteresis_threshold()", "path": "api/skimage.filters#skimage.filters.apply_hysteresis_threshold", "type": "filters", "text": "\nApply hysteresis thresholding to `image`.\n\nThis algorithm finds regions where `image` is greater than `high` OR `image`\nis greater than `low` and that region is connected to a region greater than\n`high`.\n\nGrayscale input image.\n\nLower threshold.\n\nHigher threshold.\n\nArray in which `True` indicates the locations where `image` was above the\nhysteresis threshold.\n\nJ. Canny. A computational approach to edge detection. IEEE Transactions on\nPattern Analysis and Machine Intelligence. 1986; vol. 8, pp.679-698.\nDOI:10.1109/TPAMI.1986.4767851\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.correlate_sparse()", "path": "api/skimage.filters#skimage.filters.correlate_sparse", "type": "filters", "text": "\nCompute valid cross-correlation of `padded_array` and `kernel`.\n\nThis function is fast when `kernel` is large with many zeros.\n\nSee `scipy.ndimage.correlate` for a description of cross-correlation.\n\nThe input array. If mode is \u2018valid\u2019, this array should already be padded, as a\nmargin of the same shape as kernel will be stripped off.\n\nThe kernel to be correlated. Must have the same number of dimensions as\n`padded_array`. For high performance, it should be sparse (few nonzero\nentries).\n\nSee `scipy.ndimage.correlate` for valid modes. Additionally, mode \u2018valid\u2019 is\naccepted, in which case no padding is applied and the result is the result for\nthe smaller image for which the kernel is entirely inside the original data.\n\nThe result of cross-correlating `image` with `kernel`. If mode \u2018valid\u2019 is\nused, the resulting shape is (M-Q+1, N-R+1,[ \u2026,] P-S+1).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.difference_of_gaussians()", "path": "api/skimage.filters#skimage.filters.difference_of_gaussians", "type": "filters", "text": "\nFind features between `low_sigma` and `high_sigma` in size.\n\nThis function uses the Difference of Gaussians method for applying band-pass\nfilters to multi-dimensional arrays. The input array is blurred with two\nGaussian kernels of differing sigmas to produce two intermediate, filtered\nimages. The more-blurred image is then subtracted from the less-blurred image.\nThe final output image will therefore have had high-frequency components\nattenuated by the smaller-sigma Gaussian, and low frequency components will\nhave been removed due to their presence in the more-blurred intermediate.\n\nInput array to filter.\n\nStandard deviation(s) for the Gaussian kernel with the smaller sigmas across\nall axes. The standard deviations are given for each axis as a sequence, or as\na single number, in which case the single number is used as the standard\ndeviation value for all axes.\n\nStandard deviation(s) for the Gaussian kernel with the larger sigmas across\nall axes. The standard deviations are given for each axis as a sequence, or as\na single number, in which case the single number is used as the standard\ndeviation value for all axes. If None is given (default), sigmas for all axes\nare calculated as 1.6 * low_sigma.\n\nThe `mode` parameter determines how the array borders are handled, where\n`cval` is the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.\n\nValue to fill past edges of input if `mode` is \u2018constant\u2019. Default is 0.0\n\nWhether the last axis of the image is to be interpreted as multiple channels.\nIf True, each channel is filtered separately (channels are not mixed\ntogether).\n\nTruncate the filter at this many standard deviations.\n\nthe filtered array.\n\nSee also\n\nThis function will subtract an array filtered with a Gaussian kernel with\nsigmas given by `high_sigma` from an array filtered with a Gaussian kernel\nwith sigmas provided by `low_sigma`. The values for `high_sigma` must always\nbe greater than or equal to the corresponding values in `low_sigma`, or a\n`ValueError` will be raised.\n\nWhen `high_sigma` is none, the values for `high_sigma` will be calculated as\n1.6x the corresponding values in `low_sigma`. This ratio was originally\nproposed by Marr and Hildreth (1980) [1] and is commonly used when\napproximating the inverted Laplacian of Gaussian, which is used in edge and\nblob detection.\n\nInput image is converted according to the conventions of `img_as_float`.\n\nExcept for sigma values, all parameters are used for both filters.\n\nMarr, D. and Hildreth, E. Theory of Edge Detection. Proc. R. Soc. Lond. Series\nB 207, 187-217 (1980). https://doi.org/10.1098/rspb.1980.0020\n\nApply a simple Difference of Gaussians filter to a color image:\n\nApply a Laplacian of Gaussian filter as approximated by the Difference of\nGaussians filter:\n\nApply a Difference of Gaussians filter to a grayscale image using different\nsigma values for each axis:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.farid()", "path": "api/skimage.filters#skimage.filters.farid", "type": "filters", "text": "\nFind the edge magnitude using the Farid transform.\n\nImage to process.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Farid edge map.\n\nSee also\n\nTake the square root of the sum of the squares of the horizontal and vertical\nderivatives to get a magnitude that is somewhat insensitive to direction.\nSimilar to the Scharr operator, this operator is designed with a rotation\ninvariance constraint.\n\nFarid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional\nsignals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004.\nDOI:10.1109/TIP.2004.823819\n\nWikipedia, \u201cFarid and Simoncelli Derivatives.\u201d Available at:\n<https://en.wikipedia.org/wiki/Image_derivatives#Farid_and_Simoncelli_Derivatives>\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.farid_h()", "path": "api/skimage.filters#skimage.filters.farid_h", "type": "filters", "text": "\nFind the horizontal edges of an image using the Farid transform.\n\nImage to process.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Farid edge map.\n\nThe kernel was constructed using the 5-tap weights from [1].\n\nFarid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional\nsignals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004.\nDOI:10.1109/TIP.2004.823819\n\nFarid, H. and Simoncelli, E. P. \u201cOptimally rotation-equivariant directional\nderivative kernels\u201d, In: 7th International Conference on Computer Analysis of\nImages and Patterns, Kiel, Germany. Sep, 1997.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.farid_v()", "path": "api/skimage.filters#skimage.filters.farid_v", "type": "filters", "text": "\nFind the vertical edges of an image using the Farid transform.\n\nImage to process.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Farid edge map.\n\nThe kernel was constructed using the 5-tap weights from [1].\n\nFarid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional\nsignals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004.\nDOI:10.1109/TIP.2004.823819\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.frangi()", "path": "api/skimage.filters#skimage.filters.frangi", "type": "filters", "text": "\nFilter an image with the Frangi vesselness filter.\n\nThis filter can be used to detect continuous ridges, e.g. vessels, wrinkles,\nrivers. It can be used to calculate the fraction of the whole image containing\nsuch objects.\n\nDefined only for 2-D and 3-D images. Calculates the eigenvectors of the\nHessian to compute the similarity of an image region to vessels, according to\nthe method described in [1].\n\nArray with input image data.\n\nSigmas used as scales of filter, i.e., np.arange(scale_range[0],\nscale_range[1], scale_step)\n\nThe range of sigmas used.\n\nStep size between sigmas.\n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation\nfrom a plate-like structure.\n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation\nfrom a blob-like structure.\n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to areas of\nhigh variance/texture/structure.\n\nWhen True (the default), the filter detects black ridges; when False, it\ndetects white ridges.\n\nHow to handle values outside the image borders.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nFiltered image (maximum of pixels across all scales).\n\nSee also\n\nWritten by Marc Schrijver, November 2001 Re-Written by D. J. Kroon, University\nof Twente, May 2009, [2] Adoption of 3D version from D. G. Ellis, Januar\n20017, [3]\n\nFrangi, A. F., Niessen, W. J., Vincken, K. L., & Viergever, M. A. (1998,).\nMultiscale vessel enhancement filtering. In International Conference on\nMedical Image Computing and Computer-Assisted Intervention (pp. 130-137).\nSpringer Berlin Heidelberg. DOI:10.1007/BFb0056195\n\nKroon, D. J.: Hessian based Frangi vesselness filter.\n\nEllis, D. G.: https://github.com/ellisdg/frangi3d/tree/master/frangi\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.gabor()", "path": "api/skimage.filters#skimage.filters.gabor", "type": "filters", "text": "\nReturn real and imaginary responses to Gabor filter.\n\nThe real and imaginary parts of the Gabor filter kernel are applied to the\nimage and the response is returned as a pair of arrays.\n\nGabor filter is a linear filter with a Gaussian kernel which is modulated by a\nsinusoidal plane wave. Frequency and orientation representations of the Gabor\nfilter are similar to those of the human visual system. Gabor filter banks are\ncommonly used in computer vision and image processing. They are especially\nsuitable for edge detection and texture classification.\n\nInput image.\n\nSpatial frequency of the harmonic function. Specified in pixels.\n\nOrientation in radians. If 0, the harmonic is in the x-direction.\n\nThe bandwidth captured by the filter. For fixed bandwidth, `sigma_x` and\n`sigma_y` will decrease with increasing frequency. This value is ignored if\n`sigma_x` and `sigma_y` are set by the user.\n\nStandard deviation in x- and y-directions. These directions apply to the\nkernel before rotation. If `theta = pi/2`, then the kernel is rotated 90\ndegrees so that `sigma_x` controls the vertical direction.\n\nThe linear size of the kernel is n_stds (3 by default) standard deviations.\n\nPhase offset of harmonic function in radians.\n\nMode used to convolve image with a kernel, passed to `ndi.convolve`\n\nValue to fill past edges of input if `mode` of convolution is \u2018constant\u2019. The\nparameter is passed to `ndi.convolve`.\n\nFiltered images using the real and imaginary parts of the Gabor filter kernel.\nImages are of the same dimensions as the input one.\n\nhttps://en.wikipedia.org/wiki/Gabor_filter\n\nhttps://web.archive.org/web/20180127125930/http://mplab.ucsd.edu/tutorials/gabor.pdf\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.gabor_kernel()", "path": "api/skimage.filters#skimage.filters.gabor_kernel", "type": "filters", "text": "\nReturn complex 2D Gabor filter kernel.\n\nGabor kernel is a Gaussian kernel modulated by a complex harmonic function.\nHarmonic function consists of an imaginary sine function and a real cosine\nfunction. Spatial frequency is inversely proportional to the wavelength of the\nharmonic and to the standard deviation of a Gaussian kernel. The bandwidth is\nalso inversely proportional to the standard deviation.\n\nSpatial frequency of the harmonic function. Specified in pixels.\n\nOrientation in radians. If 0, the harmonic is in the x-direction.\n\nThe bandwidth captured by the filter. For fixed bandwidth, `sigma_x` and\n`sigma_y` will decrease with increasing frequency. This value is ignored if\n`sigma_x` and `sigma_y` are set by the user.\n\nStandard deviation in x- and y-directions. These directions apply to the\nkernel before rotation. If `theta = pi/2`, then the kernel is rotated 90\ndegrees so that `sigma_x` controls the vertical direction.\n\nThe linear size of the kernel is n_stds (3 by default) standard deviations\n\nPhase offset of harmonic function in radians.\n\nComplex filter kernel.\n\nhttps://en.wikipedia.org/wiki/Gabor_filter\n\nhttps://web.archive.org/web/20180127125930/http://mplab.ucsd.edu/tutorials/gabor.pdf\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.gaussian()", "path": "api/skimage.filters#skimage.filters.gaussian", "type": "filters", "text": "\nMulti-dimensional Gaussian filter.\n\nInput image (grayscale or color) to filter.\n\nStandard deviation for Gaussian kernel. The standard deviations of the\nGaussian filter are given for each axis as a sequence, or as a single number,\nin which case it is equal for all axes.\n\nThe `output` parameter passes an array in which to store the filter output.\n\nThe `mode` parameter determines how the array borders are handled, where\n`cval` is the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.\n\nValue to fill past edges of input if `mode` is \u2018constant\u2019. Default is 0.0\n\nWhether the last axis of the image is to be interpreted as multiple channels.\nIf True, each channel is filtered separately (channels are not mixed\ntogether). Only 3 channels are supported. If `None`, the function will attempt\nto guess this, and raise a warning if ambiguous, when the array has shape (M,\nN, 3).\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nTruncate the filter at this many standard deviations.\n\nthe filtered array\n\nThis function is a wrapper around `scipy.ndi.gaussian_filter()`.\n\nInteger arrays are converted to float.\n\nThe `output` should be floating point data type since gaussian converts to\nfloat provided `image`. If `output` is not provided, another array will be\nallocated and returned as the result.\n\nThe multi-dimensional filter is implemented as a sequence of one-dimensional\nconvolution filters. The intermediate arrays are stored in the same data type\nas the output. Therefore, for output types with a limited precision, the\nresults may be imprecise because intermediate results may be stored with\ninsufficient precision.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.hessian()", "path": "api/skimage.filters#skimage.filters.hessian", "type": "filters", "text": "\nFilter an image with the Hybrid Hessian filter.\n\nThis filter can be used to detect continuous edges, e.g. vessels, wrinkles,\nrivers. It can be used to calculate the fraction of the whole image containing\nsuch objects.\n\nDefined only for 2-D and 3-D images. Almost equal to Frangi filter, but uses\nalternative method of smoothing. Refer to [1] to find the differences between\nFrangi and Hessian filters.\n\nArray with input image data.\n\nSigmas used as scales of filter, i.e., np.arange(scale_range[0],\nscale_range[1], scale_step)\n\nThe range of sigmas used.\n\nStep size between sigmas.\n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation\nfrom a blob-like structure.\n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to areas of\nhigh variance/texture/structure.\n\nWhen True (the default), the filter detects black ridges; when False, it\ndetects white ridges.\n\nHow to handle values outside the image borders.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nFiltered image (maximum of pixels across all scales).\n\nSee also\n\nWritten by Marc Schrijver (November 2001) Re-Written by D. J. Kroon University\nof Twente (May 2009) [2]\n\nNg, C. C., Yap, M. H., Costen, N., & Li, B. (2014,). Automatic wrinkle\ndetection using hybrid Hessian filter. In Asian Conference on Computer Vision\n(pp. 609-622). Springer International Publishing.\nDOI:10.1007/978-3-319-16811-1_40\n\nKroon, D. J.: Hessian based Frangi vesselness filter.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.inverse()", "path": "api/skimage.filters#skimage.filters.inverse", "type": "filters", "text": "\nApply the filter in reverse to the given data.\n\nInput data.\n\nImpulse response of the filter. See LPIFilter2D.__init__.\n\nAdditional keyword parameters to the impulse_response function.\n\nLimit the filter gain. Often, the filter contains zeros, which would cause the\ninverse filter to have infinite gain. High gain causes amplification of\nartefacts, so a conservative limit is recommended.\n\nIf you need to apply the same filter multiple times over different images,\nconstruct the LPIFilter2D and specify it here.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.laplace()", "path": "api/skimage.filters#skimage.filters.laplace", "type": "filters", "text": "\nFind the edges of an image using the Laplace operator.\n\nImage to process.\n\nDefine the size of the discrete Laplacian operator such that it will have a\nsize of (ksize,) * image.ndim.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Laplace edge map.\n\nThe Laplacian operator is generated using the function\nskimage.restoration.uft.laplacian().\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.LPIFilter2D", "path": "api/skimage.filters#skimage.filters.LPIFilter2D", "type": "filters", "text": "\nBases: `object`\n\nLinear Position-Invariant Filter (2-dimensional)\n\nFunction that yields the impulse response. `r` and `c` are 1-dimensional\nvectors that represent row and column positions, in other words coordinates\nare (r[0],c[0]),(r[0],c[1]) etc. `**filter_params` are passed through.\n\nIn other words, `impulse_response` would be called like this:\n\nGaussian filter: Use a 1-D gaussian in each direction without normalization\ncoefficients.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.LPIFilter2D.__init__()", "path": "api/skimage.filters#skimage.filters.LPIFilter2D.__init__", "type": "filters", "text": "\nFunction that yields the impulse response. `r` and `c` are 1-dimensional\nvectors that represent row and column positions, in other words coordinates\nare (r[0],c[0]),(r[0],c[1]) etc. `**filter_params` are passed through.\n\nIn other words, `impulse_response` would be called like this:\n\nGaussian filter: Use a 1-D gaussian in each direction without normalization\ncoefficients.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.median()", "path": "api/skimage.filters#skimage.filters.median", "type": "filters", "text": "\nReturn local median of an image.\n\nInput image.\n\nIf `behavior=='rank'`, `selem` is a 2-D array of 1\u2019s and 0\u2019s. If\n`behavior=='ndimage'`, `selem` is a N-D array of 1\u2019s and 0\u2019s with the same\nnumber of dimension than `image`. If None, `selem` will be a N-D array with 3\nelements for each dimension (e.g., vector, square, cube, etc.)\n\nIf None, a new array is allocated.\n\nThe mode parameter determines how the array borders are handled, where `cval`\nis the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.\n\nNew in version 0.15: `mode` is used when `behavior='ndimage'`.\n\nValue to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0\n\nNew in version 0.15: `cval` was added in 0.15 is used when\n`behavior='ndimage'`.\n\nEither to use the old behavior (i.e., < 0.15) or the new behavior. The old\nbehavior will call the `skimage.filters.rank.median()`. The new behavior will\ncall the `scipy.ndimage.median_filter()`. Default is \u2018ndimage\u2019.\n\nNew in version 0.15: `behavior` is introduced in 0.15\n\nChanged in version 0.16: Default `behavior` has been changed from \u2018rank\u2019 to\n\u2018ndimage\u2019\n\nOutput image.\n\nSee also\n\nRank-based implementation of the median filtering offering more flexibility\nwith additional parameters but dedicated for unsigned integer images.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.meijering()", "path": "api/skimage.filters#skimage.filters.meijering", "type": "filters", "text": "\nFilter an image with the Meijering neuriteness filter.\n\nThis filter can be used to detect continuous ridges, e.g. neurites, wrinkles,\nrivers. It can be used to calculate the fraction of the whole image containing\nsuch objects.\n\nCalculates the eigenvectors of the Hessian to compute the similarity of an\nimage region to neurites, according to the method described in [1].\n\nArray with input image data.\n\nSigmas used as scales of filter\n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation\nfrom a plate-like structure.\n\nWhen True (the default), the filter detects black ridges; when False, it\ndetects white ridges.\n\nHow to handle values outside the image borders.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nFiltered image (maximum of pixels across all scales).\n\nSee also\n\nMeijering, E., Jacob, M., Sarria, J. C., Steiner, P., Hirling, H., Unser, M.\n(2004). Design and validation of a tool for neurite tracing and analysis in\nfluorescence microscopy images. Cytometry Part A, 58(2), 167-176.\nDOI:10.1002/cyto.a.20022\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.prewitt()", "path": "api/skimage.filters#skimage.filters.prewitt", "type": "filters", "text": "\nFind the edge magnitude using the Prewitt transform.\n\nThe input image.\n\nClip the output image to this mask. (Values where mask=0 will be set to 0.)\n\nCompute the edge filter along this axis. If not provided, the edge magnitude\nis computed. This is defined as:\n\nThe magnitude is also computed if axis is a sequence.\n\nThe boundary mode for the convolution. See `scipy.ndimage.convolve` for a\ndescription of the modes. This can be either a single boundary mode or one\nboundary mode per axis.\n\nWhen `mode` is `'constant'`, this is the constant used in values outside the\nboundary of the image data.\n\nThe Prewitt edge map.\n\nSee also\n\nThe edge magnitude depends slightly on edge directions, since the\napproximation of the gradient operator by the Prewitt operator is not\ncompletely rotation invariant. For a better rotation invariance, the Scharr\noperator should be used. The Sobel operator has a better rotation invariance\nthan the Prewitt operator, but a worse rotation invariance than the Scharr\noperator.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.prewitt_h()", "path": "api/skimage.filters#skimage.filters.prewitt_h", "type": "filters", "text": "\nFind the horizontal edges of an image using the Prewitt transform.\n\nImage to process.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Prewitt edge map.\n\nWe use the following kernel:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.prewitt_v()", "path": "api/skimage.filters#skimage.filters.prewitt_v", "type": "filters", "text": "\nFind the vertical edges of an image using the Prewitt transform.\n\nImage to process.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Prewitt edge map.\n\nWe use the following kernel:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank", "path": "api/skimage.filters.rank", "type": "filters", "text": "\n`skimage.filters.rank.autolevel`(image, selem)\n\nAuto-level image using local histogram.\n\n`skimage.filters.rank.autolevel_percentile`(\u2026)\n\nReturn greyscale local autolevel of an image.\n\n`skimage.filters.rank.bottomhat`(image, selem)\n\nLocal bottom-hat of an image.\n\n`skimage.filters.rank.enhance_contrast`(image, \u2026)\n\nEnhance contrast of an image.\n\n`skimage.filters.rank.enhance_contrast_percentile`(\u2026)\n\nEnhance contrast of an image.\n\n`skimage.filters.rank.entropy`(image, selem[, \u2026])\n\nLocal entropy.\n\n`skimage.filters.rank.equalize`(image, selem)\n\nEqualize image using local histogram.\n\n`skimage.filters.rank.geometric_mean`(image, selem)\n\nReturn local geometric mean of an image.\n\n`skimage.filters.rank.gradient`(image, selem)\n\nReturn local gradient of an image (i.e.\n\n`skimage.filters.rank.gradient_percentile`(\u2026)\n\nReturn local gradient of an image (i.e.\n\n`skimage.filters.rank.majority`(image, selem, *)\n\nMajority filter assign to each pixel the most occuring value within its\nneighborhood.\n\n`skimage.filters.rank.maximum`(image, selem[, \u2026])\n\nReturn local maximum of an image.\n\n`skimage.filters.rank.mean`(image, selem[, \u2026])\n\nReturn local mean of an image.\n\n`skimage.filters.rank.mean_bilateral`(image, selem)\n\nApply a flat kernel bilateral filter.\n\n`skimage.filters.rank.mean_percentile`(image, \u2026)\n\nReturn local mean of an image.\n\n`skimage.filters.rank.median`(image[, selem, \u2026])\n\nReturn local median of an image.\n\n`skimage.filters.rank.minimum`(image, selem[, \u2026])\n\nReturn local minimum of an image.\n\n`skimage.filters.rank.modal`(image, selem[, \u2026])\n\nReturn local mode of an image.\n\n`skimage.filters.rank.noise_filter`(image, selem)\n\nNoise feature.\n\n`skimage.filters.rank.otsu`(image, selem[, \u2026])\n\nLocal Otsu\u2019s threshold value for each pixel.\n\n`skimage.filters.rank.percentile`(image, selem)\n\nReturn local percentile of an image.\n\n`skimage.filters.rank.pop`(image, selem[, \u2026])\n\nReturn the local number (population) of pixels.\n\n`skimage.filters.rank.pop_bilateral`(image, selem)\n\nReturn the local number (population) of pixels.\n\n`skimage.filters.rank.pop_percentile`(image, selem)\n\nReturn the local number (population) of pixels.\n\n`skimage.filters.rank.subtract_mean`(image, selem)\n\nReturn image subtracted from its local mean.\n\n`skimage.filters.rank.subtract_mean_percentile`(\u2026)\n\nReturn image subtracted from its local mean.\n\n`skimage.filters.rank.sum`(image, selem[, \u2026])\n\nReturn the local sum of pixels.\n\n`skimage.filters.rank.sum_bilateral`(image, selem)\n\nApply a flat kernel bilateral filter.\n\n`skimage.filters.rank.sum_percentile`(image, selem)\n\nReturn the local sum of pixels.\n\n`skimage.filters.rank.threshold`(image, selem)\n\nLocal threshold of an image.\n\n`skimage.filters.rank.threshold_percentile`(\u2026)\n\nLocal threshold of an image.\n\n`skimage.filters.rank.tophat`(image, selem[, \u2026])\n\nLocal top-hat of an image.\n\n`skimage.filters.rank.windowed_histogram`(\u2026)\n\nNormalized sliding window histogram\n\nAuto-level image using local histogram.\n\nThis filter locally stretches the histogram of gray values to cover the entire\nrange of values from \u201cwhite\u201d to \u201cblack\u201d.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nRank filters\n\nReturn greyscale local autolevel of an image.\n\nThis filter locally stretches the histogram of greyvalues to cover the entire\nrange of values from \u201cwhite\u201d to \u201cblack\u201d.\n\nOnly greyvalues between percentiles [p0, p1] are considered in the filter.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nDefine the [p0, p1] percentile interval to be considered for computing the\nvalue.\n\nOutput image.\n\nRank filters\n\nLocal bottom-hat of an image.\n\nThis filter computes the morphological closing of the image and then subtracts\nthe result from the original image.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nNew in version 0.17.\n\nThis function is deprecated and will be removed in scikit-image 0.19. This\nfilter was misnamed and we believe that the usefulness is narrow.\n\nEnhance contrast of an image.\n\nThis replaces each pixel by the local maximum if the pixel gray value is\ncloser to the local maximum than the local minimum. Otherwise it is replaced\nby the local minimum.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image\n\nRank filters\n\nEnhance contrast of an image.\n\nThis replaces each pixel by the local maximum if the pixel greyvalue is closer\nto the local maximum than the local minimum. Otherwise it is replaced by the\nlocal minimum.\n\nOnly greyvalues between percentiles [p0, p1] are considered in the filter.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nDefine the [p0, p1] percentile interval to be considered for computing the\nvalue.\n\nOutput image.\n\nRank filters\n\nLocal entropy.\n\nThe entropy is computed using base 2 logarithm i.e. the filter returns the\nminimum number of bits needed to encode the local gray level distribution.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nhttps://en.wikipedia.org/wiki/Entropy_(information_theory)\n\nTinting gray-scale images\n\nEntropy\n\nRank filters\n\nEqualize image using local histogram.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nLocal Histogram Equalization\n\nRank filters\n\nReturn local geometric mean of an image.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nGonzalez, R. C. and Wood, R. E. \u201cDigital Image Processing (3rd Edition).\u201d\nPrentice-Hall Inc, 2006.\n\nReturn local gradient of an image (i.e. local maximum - local minimum).\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nMarkers for watershed transform\n\nRank filters\n\nReturn local gradient of an image (i.e. local maximum - local minimum).\n\nOnly greyvalues between percentiles [p0, p1] are considered in the filter.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nDefine the [p0, p1] percentile interval to be considered for computing the\nvalue.\n\nOutput image.\n\nMajority filter assign to each pixel the most occuring value within its\nneighborhood.\n\nImage array (uint8, uint16 array).\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array will be allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nReturn local maximum of an image.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nSee also\n\nThe lower algorithm complexity makes `skimage.filters.rank.maximum` more\nefficient for larger images and structuring elements.\n\nRank filters\n\nReturn local mean of an image.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nSegment human cells (in mitosis)\n\nRank filters\n\nApply a flat kernel bilateral filter.\n\nThis is an edge-preserving and noise reducing denoising filter. It averages\npixels based on their spatial closeness and radiometric similarity.\n\nSpatial closeness is measured by considering only the local pixel neighborhood\ngiven by a structuring element.\n\nRadiometric similarity is defined by the greylevel interval [g-s0, g+s1] where\ng is the current pixel greylevel.\n\nOnly pixels belonging to the structuring element and having a greylevel inside\nthis interval are averaged.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nDefine the [s0, s1] interval around the greyvalue of the center pixel to be\nconsidered for computing the value.\n\nOutput image.\n\nSee also\n\nRank filters\n\nReturn local mean of an image.\n\nOnly greyvalues between percentiles [p0, p1] are considered in the filter.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nDefine the [p0, p1] percentile interval to be considered for computing the\nvalue.\n\nOutput image.\n\nReturn local median of an image.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s. If None, a full\nsquare of size 3 is used.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nSee also\n\nImplementation of a median filtering which handles images with floating\nprecision.\n\nMarkers for watershed transform\n\nRank filters\n\nReturn local minimum of an image.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nSee also\n\nThe lower algorithm complexity makes `skimage.filters.rank.minimum` more\nefficient for larger images and structuring elements.\n\nRank filters\n\nReturn local mode of an image.\n\nThe mode is the value that appears most often in the local histogram.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nNoise feature.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nN. Hashimoto et al. Referenceless image quality evaluation for whole slide\nimaging. J Pathol Inform 2012;3:9.\n\nLocal Otsu\u2019s threshold value for each pixel.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nhttps://en.wikipedia.org/wiki/Otsu\u2019s_method\n\nRank filters\n\nReturn local percentile of an image.\n\nReturns the value of the p0 lower percentile of the local greyvalue\ndistribution.\n\nOnly greyvalues between percentiles [p0, p1] are considered in the filter.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nSet the percentile value.\n\nOutput image.\n\nReturn the local number (population) of pixels.\n\nThe number of pixels is defined as the number of pixels which are included in\nthe structuring element and the mask.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nReturn the local number (population) of pixels.\n\nThe number of pixels is defined as the number of pixels which are included in\nthe structuring element and the mask. Additionally pixels must have a\ngreylevel inside the interval [g-s0, g+s1] where g is the greyvalue of the\ncenter pixel.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nDefine the [s0, s1] interval around the greyvalue of the center pixel to be\nconsidered for computing the value.\n\nOutput image.\n\nReturn the local number (population) of pixels.\n\nThe number of pixels is defined as the number of pixels which are included in\nthe structuring element and the mask.\n\nOnly greyvalues between percentiles [p0, p1] are considered in the filter.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nDefine the [p0, p1] percentile interval to be considered for computing the\nvalue.\n\nOutput image.\n\nReturn image subtracted from its local mean.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nSubtracting the mean value may introduce underflow. To compensate this\npotential underflow, the obtained difference is downscaled by a factor of 2\nand shifted by `n_bins / 2 - 1`, the median value of the local histogram\n(`n_bins = max(3, image.max()) +1` for 16-bits images and 256 otherwise).\n\nReturn image subtracted from its local mean.\n\nOnly greyvalues between percentiles [p0, p1] are considered in the filter.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nDefine the [p0, p1] percentile interval to be considered for computing the\nvalue.\n\nOutput image.\n\nReturn the local sum of pixels.\n\nNote that the sum may overflow depending on the data type of the input array.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nApply a flat kernel bilateral filter.\n\nThis is an edge-preserving and noise reducing denoising filter. It averages\npixels based on their spatial closeness and radiometric similarity.\n\nSpatial closeness is measured by considering only the local pixel neighborhood\ngiven by a structuring element (selem).\n\nRadiometric similarity is defined by the greylevel interval [g-s0, g+s1] where\ng is the current pixel greylevel.\n\nOnly pixels belonging to the structuring element AND having a greylevel inside\nthis interval are summed.\n\nNote that the sum may overflow depending on the data type of the input array.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nDefine the [s0, s1] interval around the greyvalue of the center pixel to be\nconsidered for computing the value.\n\nOutput image.\n\nSee also\n\nReturn the local sum of pixels.\n\nOnly greyvalues between percentiles [p0, p1] are considered in the filter.\n\nNote that the sum may overflow depending on the data type of the input array.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nDefine the [p0, p1] percentile interval to be considered for computing the\nvalue.\n\nOutput image.\n\nLocal threshold of an image.\n\nThe resulting binary mask is True if the gray value of the center pixel is\ngreater than the local mean.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nLocal threshold of an image.\n\nThe resulting binary mask is True if the greyvalue of the center pixel is\ngreater than the local mean.\n\nOnly greyvalues between percentiles [p0, p1] are considered in the filter.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nSet the percentile value.\n\nOutput image.\n\nLocal top-hat of an image.\n\nThis filter computes the morphological opening of the image and then subtracts\nthe result from the original image.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nNew in version 0.17.\n\nThis function is deprecated and will be removed in scikit-image 0.19. This\nfilter was misnamed and we believe that the usefulness is narrow.\n\nNormalized sliding window histogram\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nThe number of histogram bins. Will default to `image.max() + 1` if None is\npassed.\n\nArray of dimensions (H,W,N), where (H,W) are the dimensions of the input image\nand N is n_bins or `image.max() + 1` if no value is provided as a parameter.\nEffectively, each pixel is a N-D feature vector that is the histogram. The sum\nof the elements in the feature vector will be 1, unless no pixels in the\nwindow were covered by both selem and mask, in which case all elements will be\n0.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.autolevel()", "path": "api/skimage.filters.rank#skimage.filters.rank.autolevel", "type": "filters", "text": "\nAuto-level image using local histogram.\n\nThis filter locally stretches the histogram of gray values to cover the entire\nrange of values from \u201cwhite\u201d to \u201cblack\u201d.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.autolevel_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.autolevel_percentile", "type": "filters", "text": "\nReturn greyscale local autolevel of an image.\n\nThis filter locally stretches the histogram of greyvalues to cover the entire\nrange of values from \u201cwhite\u201d to \u201cblack\u201d.\n\nOnly greyvalues between percentiles [p0, p1] are considered in the filter.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nDefine the [p0, p1] percentile interval to be considered for computing the\nvalue.\n\nOutput image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.bottomhat()", "path": "api/skimage.filters.rank#skimage.filters.rank.bottomhat", "type": "filters", "text": "\nLocal bottom-hat of an image.\n\nThis filter computes the morphological closing of the image and then subtracts\nthe result from the original image.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nNew in version 0.17.\n\nThis function is deprecated and will be removed in scikit-image 0.19. This\nfilter was misnamed and we believe that the usefulness is narrow.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.enhance_contrast()", "path": "api/skimage.filters.rank#skimage.filters.rank.enhance_contrast", "type": "filters", "text": "\nEnhance contrast of an image.\n\nThis replaces each pixel by the local maximum if the pixel gray value is\ncloser to the local maximum than the local minimum. Otherwise it is replaced\nby the local minimum.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.enhance_contrast_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.enhance_contrast_percentile", "type": "filters", "text": "\nEnhance contrast of an image.\n\nThis replaces each pixel by the local maximum if the pixel greyvalue is closer\nto the local maximum than the local minimum. Otherwise it is replaced by the\nlocal minimum.\n\nOnly greyvalues between percentiles [p0, p1] are considered in the filter.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nDefine the [p0, p1] percentile interval to be considered for computing the\nvalue.\n\nOutput image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.entropy()", "path": "api/skimage.filters.rank#skimage.filters.rank.entropy", "type": "filters", "text": "\nLocal entropy.\n\nThe entropy is computed using base 2 logarithm i.e. the filter returns the\nminimum number of bits needed to encode the local gray level distribution.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nhttps://en.wikipedia.org/wiki/Entropy_(information_theory)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.equalize()", "path": "api/skimage.filters.rank#skimage.filters.rank.equalize", "type": "filters", "text": "\nEqualize image using local histogram.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.geometric_mean()", "path": "api/skimage.filters.rank#skimage.filters.rank.geometric_mean", "type": "filters", "text": "\nReturn local geometric mean of an image.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nGonzalez, R. C. and Wood, R. E. \u201cDigital Image Processing (3rd Edition).\u201d\nPrentice-Hall Inc, 2006.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.gradient()", "path": "api/skimage.filters.rank#skimage.filters.rank.gradient", "type": "filters", "text": "\nReturn local gradient of an image (i.e. local maximum - local minimum).\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.gradient_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.gradient_percentile", "type": "filters", "text": "\nReturn local gradient of an image (i.e. local maximum - local minimum).\n\nOnly greyvalues between percentiles [p0, p1] are considered in the filter.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nDefine the [p0, p1] percentile interval to be considered for computing the\nvalue.\n\nOutput image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.majority()", "path": "api/skimage.filters.rank#skimage.filters.rank.majority", "type": "filters", "text": "\nMajority filter assign to each pixel the most occuring value within its\nneighborhood.\n\nImage array (uint8, uint16 array).\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array will be allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.maximum()", "path": "api/skimage.filters.rank#skimage.filters.rank.maximum", "type": "filters", "text": "\nReturn local maximum of an image.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nSee also\n\nThe lower algorithm complexity makes `skimage.filters.rank.maximum` more\nefficient for larger images and structuring elements.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.mean()", "path": "api/skimage.filters.rank#skimage.filters.rank.mean", "type": "filters", "text": "\nReturn local mean of an image.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.mean_bilateral()", "path": "api/skimage.filters.rank#skimage.filters.rank.mean_bilateral", "type": "filters", "text": "\nApply a flat kernel bilateral filter.\n\nThis is an edge-preserving and noise reducing denoising filter. It averages\npixels based on their spatial closeness and radiometric similarity.\n\nSpatial closeness is measured by considering only the local pixel neighborhood\ngiven by a structuring element.\n\nRadiometric similarity is defined by the greylevel interval [g-s0, g+s1] where\ng is the current pixel greylevel.\n\nOnly pixels belonging to the structuring element and having a greylevel inside\nthis interval are averaged.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nDefine the [s0, s1] interval around the greyvalue of the center pixel to be\nconsidered for computing the value.\n\nOutput image.\n\nSee also\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.mean_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.mean_percentile", "type": "filters", "text": "\nReturn local mean of an image.\n\nOnly greyvalues between percentiles [p0, p1] are considered in the filter.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nDefine the [p0, p1] percentile interval to be considered for computing the\nvalue.\n\nOutput image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.median()", "path": "api/skimage.filters.rank#skimage.filters.rank.median", "type": "filters", "text": "\nReturn local median of an image.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s. If None, a full\nsquare of size 3 is used.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nSee also\n\nImplementation of a median filtering which handles images with floating\nprecision.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.minimum()", "path": "api/skimage.filters.rank#skimage.filters.rank.minimum", "type": "filters", "text": "\nReturn local minimum of an image.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nSee also\n\nThe lower algorithm complexity makes `skimage.filters.rank.minimum` more\nefficient for larger images and structuring elements.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.modal()", "path": "api/skimage.filters.rank#skimage.filters.rank.modal", "type": "filters", "text": "\nReturn local mode of an image.\n\nThe mode is the value that appears most often in the local histogram.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.noise_filter()", "path": "api/skimage.filters.rank#skimage.filters.rank.noise_filter", "type": "filters", "text": "\nNoise feature.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nN. Hashimoto et al. Referenceless image quality evaluation for whole slide\nimaging. J Pathol Inform 2012;3:9.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.otsu()", "path": "api/skimage.filters.rank#skimage.filters.rank.otsu", "type": "filters", "text": "\nLocal Otsu\u2019s threshold value for each pixel.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nhttps://en.wikipedia.org/wiki/Otsu\u2019s_method\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.percentile", "type": "filters", "text": "\nReturn local percentile of an image.\n\nReturns the value of the p0 lower percentile of the local greyvalue\ndistribution.\n\nOnly greyvalues between percentiles [p0, p1] are considered in the filter.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nSet the percentile value.\n\nOutput image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.pop()", "path": "api/skimage.filters.rank#skimage.filters.rank.pop", "type": "filters", "text": "\nReturn the local number (population) of pixels.\n\nThe number of pixels is defined as the number of pixels which are included in\nthe structuring element and the mask.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.pop_bilateral()", "path": "api/skimage.filters.rank#skimage.filters.rank.pop_bilateral", "type": "filters", "text": "\nReturn the local number (population) of pixels.\n\nThe number of pixels is defined as the number of pixels which are included in\nthe structuring element and the mask. Additionally pixels must have a\ngreylevel inside the interval [g-s0, g+s1] where g is the greyvalue of the\ncenter pixel.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nDefine the [s0, s1] interval around the greyvalue of the center pixel to be\nconsidered for computing the value.\n\nOutput image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.pop_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.pop_percentile", "type": "filters", "text": "\nReturn the local number (population) of pixels.\n\nThe number of pixels is defined as the number of pixels which are included in\nthe structuring element and the mask.\n\nOnly greyvalues between percentiles [p0, p1] are considered in the filter.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nDefine the [p0, p1] percentile interval to be considered for computing the\nvalue.\n\nOutput image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.subtract_mean()", "path": "api/skimage.filters.rank#skimage.filters.rank.subtract_mean", "type": "filters", "text": "\nReturn image subtracted from its local mean.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nSubtracting the mean value may introduce underflow. To compensate this\npotential underflow, the obtained difference is downscaled by a factor of 2\nand shifted by `n_bins / 2 - 1`, the median value of the local histogram\n(`n_bins = max(3, image.max()) +1` for 16-bits images and 256 otherwise).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.subtract_mean_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.subtract_mean_percentile", "type": "filters", "text": "\nReturn image subtracted from its local mean.\n\nOnly greyvalues between percentiles [p0, p1] are considered in the filter.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nDefine the [p0, p1] percentile interval to be considered for computing the\nvalue.\n\nOutput image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.sum()", "path": "api/skimage.filters.rank#skimage.filters.rank.sum", "type": "filters", "text": "\nReturn the local sum of pixels.\n\nNote that the sum may overflow depending on the data type of the input array.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.sum_bilateral()", "path": "api/skimage.filters.rank#skimage.filters.rank.sum_bilateral", "type": "filters", "text": "\nApply a flat kernel bilateral filter.\n\nThis is an edge-preserving and noise reducing denoising filter. It averages\npixels based on their spatial closeness and radiometric similarity.\n\nSpatial closeness is measured by considering only the local pixel neighborhood\ngiven by a structuring element (selem).\n\nRadiometric similarity is defined by the greylevel interval [g-s0, g+s1] where\ng is the current pixel greylevel.\n\nOnly pixels belonging to the structuring element AND having a greylevel inside\nthis interval are summed.\n\nNote that the sum may overflow depending on the data type of the input array.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nDefine the [s0, s1] interval around the greyvalue of the center pixel to be\nconsidered for computing the value.\n\nOutput image.\n\nSee also\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.sum_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.sum_percentile", "type": "filters", "text": "\nReturn the local sum of pixels.\n\nOnly greyvalues between percentiles [p0, p1] are considered in the filter.\n\nNote that the sum may overflow depending on the data type of the input array.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nDefine the [p0, p1] percentile interval to be considered for computing the\nvalue.\n\nOutput image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.threshold()", "path": "api/skimage.filters.rank#skimage.filters.rank.threshold", "type": "filters", "text": "\nLocal threshold of an image.\n\nThe resulting binary mask is True if the gray value of the center pixel is\ngreater than the local mean.\n\nInput image.\n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.threshold_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.threshold_percentile", "type": "filters", "text": "\nLocal threshold of an image.\n\nThe resulting binary mask is True if the greyvalue of the center pixel is\ngreater than the local mean.\n\nOnly greyvalues between percentiles [p0, p1] are considered in the filter.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nSet the percentile value.\n\nOutput image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.tophat()", "path": "api/skimage.filters.rank#skimage.filters.rank.tophat", "type": "filters", "text": "\nLocal top-hat of an image.\n\nThis filter computes the morphological opening of the image and then subtracts\nthe result from the original image.\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nOutput image.\n\nNew in version 0.17.\n\nThis function is deprecated and will be removed in scikit-image 0.19. This\nfilter was misnamed and we believe that the usefulness is narrow.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank.windowed_histogram()", "path": "api/skimage.filters.rank#skimage.filters.rank.windowed_histogram", "type": "filters", "text": "\nNormalized sliding window histogram\n\nInput image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.\n\nIf None, a new array is allocated.\n\nMask array that defines (>0) area of the image included in the local\nneighborhood. If None, the complete image is used (default).\n\nOffset added to the structuring element center point. Shift is bounded to the\nstructuring element sizes (center must be inside the given structuring\nelement).\n\nThe number of histogram bins. Will default to `image.max() + 1` if None is\npassed.\n\nArray of dimensions (H,W,N), where (H,W) are the dimensions of the input image\nand N is n_bins or `image.max() + 1` if no value is provided as a parameter.\nEffectively, each pixel is a N-D feature vector that is the histogram. The sum\nof the elements in the feature vector will be 1, unless no pixels in the\nwindow were covered by both selem and mask, in which case all elements will be\n0.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.rank_order()", "path": "api/skimage.filters#skimage.filters.rank_order", "type": "filters", "text": "\nReturn an image of the same shape where each pixel is the index of the pixel\nvalue in the ascending order of the unique values of `image`, aka the rank-\norder value.\n\nNew array where each pixel has the rank-order value of the corresponding pixel\nin `image`. Pixel values are between 0 and n - 1, where n is the number of\ndistinct unique values in `image`.\n\nUnique original values of `image`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.roberts()", "path": "api/skimage.filters#skimage.filters.roberts", "type": "filters", "text": "\nFind the edge magnitude using Roberts\u2019 cross operator.\n\nImage to process.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Roberts\u2019 Cross edge map.\n\nSee also\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.roberts_neg_diag()", "path": "api/skimage.filters#skimage.filters.roberts_neg_diag", "type": "filters", "text": "\nFind the cross edges of an image using the Roberts\u2019 Cross operator.\n\nThe kernel is applied to the input image to produce separate measurements of\nthe gradient component one orientation.\n\nImage to process.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Robert\u2019s edge map.\n\nWe use the following kernel:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.roberts_pos_diag()", "path": "api/skimage.filters#skimage.filters.roberts_pos_diag", "type": "filters", "text": "\nFind the cross edges of an image using Roberts\u2019 cross operator.\n\nThe kernel is applied to the input image to produce separate measurements of\nthe gradient component one orientation.\n\nImage to process.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Robert\u2019s edge map.\n\nWe use the following kernel:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.sato()", "path": "api/skimage.filters#skimage.filters.sato", "type": "filters", "text": "\nFilter an image with the Sato tubeness filter.\n\nThis filter can be used to detect continuous ridges, e.g. tubes, wrinkles,\nrivers. It can be used to calculate the fraction of the whole image containing\nsuch objects.\n\nDefined only for 2-D and 3-D images. Calculates the eigenvectors of the\nHessian to compute the similarity of an image region to tubes, according to\nthe method described in [1].\n\nArray with input image data.\n\nSigmas used as scales of filter.\n\nWhen True (the default), the filter detects black ridges; when False, it\ndetects white ridges.\n\nHow to handle values outside the image borders.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nFiltered image (maximum of pixels across all scales).\n\nSee also\n\nSato, Y., Nakajima, S., Shiraga, N., Atsumi, H., Yoshida, S., Koller, T., \u2026,\nKikinis, R. (1998). Three-dimensional multi-scale line filter for segmentation\nand visualization of curvilinear structures in medical images. Medical image\nanalysis, 2(2), 143-168. DOI:10.1016/S1361-8415(98)80009-1\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.scharr()", "path": "api/skimage.filters#skimage.filters.scharr", "type": "filters", "text": "\nFind the edge magnitude using the Scharr transform.\n\nThe input image.\n\nClip the output image to this mask. (Values where mask=0 will be set to 0.)\n\nCompute the edge filter along this axis. If not provided, the edge magnitude\nis computed. This is defined as:\n\nThe magnitude is also computed if axis is a sequence.\n\nThe boundary mode for the convolution. See `scipy.ndimage.convolve` for a\ndescription of the modes. This can be either a single boundary mode or one\nboundary mode per axis.\n\nWhen `mode` is `'constant'`, this is the constant used in values outside the\nboundary of the image data.\n\nThe Scharr edge map.\n\nSee also\n\nThe Scharr operator has a better rotation invariance than other edge filters\nsuch as the Sobel or the Prewitt operators.\n\nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of\nKernel Based Image Derivatives.\n\nhttps://en.wikipedia.org/wiki/Sobel_operator#Alternative_operators\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.scharr_h()", "path": "api/skimage.filters#skimage.filters.scharr_h", "type": "filters", "text": "\nFind the horizontal edges of an image using the Scharr transform.\n\nImage to process.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Scharr edge map.\n\nWe use the following kernel:\n\nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of\nKernel Based Image Derivatives.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.scharr_v()", "path": "api/skimage.filters#skimage.filters.scharr_v", "type": "filters", "text": "\nFind the vertical edges of an image using the Scharr transform.\n\nImage to process\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Scharr edge map.\n\nWe use the following kernel:\n\nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of\nKernel Based Image Derivatives.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.sobel()", "path": "api/skimage.filters#skimage.filters.sobel", "type": "filters", "text": "\nFind edges in an image using the Sobel filter.\n\nThe input image.\n\nClip the output image to this mask. (Values where mask=0 will be set to 0.)\n\nCompute the edge filter along this axis. If not provided, the edge magnitude\nis computed. This is defined as:\n\nThe magnitude is also computed if axis is a sequence.\n\nThe boundary mode for the convolution. See `scipy.ndimage.convolve` for a\ndescription of the modes. This can be either a single boundary mode or one\nboundary mode per axis.\n\nWhen `mode` is `'constant'`, this is the constant used in values outside the\nboundary of the image data.\n\nThe Sobel edge map.\n\nSee also\n\nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of\nKernel Based Image Derivatives.\n\nhttps://en.wikipedia.org/wiki/Sobel_operator\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.sobel_h()", "path": "api/skimage.filters#skimage.filters.sobel_h", "type": "filters", "text": "\nFind the horizontal edges of an image using the Sobel transform.\n\nImage to process.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Sobel edge map.\n\nWe use the following kernel:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.sobel_v()", "path": "api/skimage.filters#skimage.filters.sobel_v", "type": "filters", "text": "\nFind the vertical edges of an image using the Sobel transform.\n\nImage to process.\n\nAn optional mask to limit the application to a certain area. Note that pixels\nsurrounding masked regions are also masked to prevent masked regions from\naffecting the result.\n\nThe Sobel edge map.\n\nWe use the following kernel:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.threshold_isodata()", "path": "api/skimage.filters#skimage.filters.threshold_isodata", "type": "filters", "text": "\nReturn threshold value(s) based on ISODATA method.\n\nHistogram-based threshold, known as Ridler-Calvard method or inter-means.\nThreshold values returned satisfy the following equality:\n\nThat is, returned thresholds are intensities that separate the image into two\ngroups of pixels, where the threshold intensity is midway between the mean\nintensities of these groups.\n\nFor integer images, the above equality holds to within one; for floating-\npoint images, the equality holds to within the histogram bin-width.\n\nEither image or hist must be provided. In case hist is given, the actual\nhistogram of the image is ignored.\n\nInput image.\n\nNumber of bins used to calculate histogram. This value is ignored for integer\narrays.\n\nIf False (default), return only the lowest threshold that satisfies the above\nequality. If True, return all valid thresholds.\n\nHistogram to determine the threshold from and a corresponding array of bin\ncenter intensities. Alternatively, only the histogram can be passed.\n\nThreshold value(s).\n\nRidler, TW & Calvard, S (1978), \u201cPicture thresholding using an iterative\nselection method\u201d IEEE Transactions on Systems, Man and Cybernetics 8:\n630-632, DOI:10.1109/TSMC.1978.4310039\n\nSezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and\nQuantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1):\n146-165,\nhttp://www.busim.ee.boun.edu.tr/~sankur/SankurFolder/Threshold_survey.pdf\nDOI:10.1117/1.1631315\n\nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.threshold_li()", "path": "api/skimage.filters#skimage.filters.threshold_li", "type": "filters", "text": "\nCompute threshold value by Li\u2019s iterative Minimum Cross Entropy method.\n\nInput image.\n\nFinish the computation when the change in the threshold in an iteration is\nless than this value. By default, this is half the smallest difference between\nintensity values in `image`.\n\nLi\u2019s iterative method uses gradient descent to find the optimal threshold. If\nthe image intensity histogram contains more than two modes (peaks), the\ngradient descent could get stuck in a local optimum. An initial guess for the\niteration can help the algorithm find the globally-optimal threshold. A float\nvalue defines a specific start point, while a callable should take in an array\nof image intensities and return a float value. Example valid callables include\n`numpy.mean` (default), `lambda arr: numpy.quantile(arr, 0.95)`, or even\n`skimage.filters.threshold_otsu()`.\n\nA function that will be called on the threshold at every iteration of the\nalgorithm.\n\nUpper threshold value. All pixels with an intensity higher than this value are\nassumed to be foreground.\n\nLi C.H. and Lee C.K. (1993) \u201cMinimum Cross Entropy Thresholding\u201d Pattern\nRecognition, 26(4): 617-625 DOI:10.1016/0031-3203(93)90115-D\n\nLi C.H. and Tam P.K.S. (1998) \u201cAn Iterative Algorithm for Minimum Cross\nEntropy Thresholding\u201d Pattern Recognition Letters, 18(8): 771-776\nDOI:10.1016/S0167-8655(98)00057-9\n\nSezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and\nQuantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1):\n146-165 DOI:10.1117/1.1631315\n\nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.threshold_local()", "path": "api/skimage.filters#skimage.filters.threshold_local", "type": "filters", "text": "\nCompute a threshold mask image based on local pixel neighborhood.\n\nAlso known as adaptive or dynamic thresholding. The threshold value is the\nweighted mean for the local neighborhood of a pixel subtracted by a constant.\nAlternatively the threshold can be determined dynamically by a given function,\nusing the \u2018generic\u2019 method.\n\nInput image.\n\nOdd size of pixel neighborhood which is used to calculate the threshold value\n(e.g. 3, 5, 7, \u2026, 21, \u2026).\n\nMethod used to determine adaptive threshold for local neighbourhood in\nweighted mean image.\n\nBy default the \u2018gaussian\u2019 method is used.\n\nConstant subtracted from weighted mean of neighborhood to calculate the local\nthreshold value. Default offset is 0.\n\nThe mode parameter determines how the array borders are handled, where cval is\nthe value when mode is equal to \u2018constant\u2019. Default is \u2018reflect\u2019.\n\nEither specify sigma for \u2018gaussian\u2019 method or function object for \u2018generic\u2019\nmethod. This functions takes the flat array of local neighbourhood as a single\nargument and returns the calculated threshold for the centre pixel.\n\nValue to fill past edges of input if mode is \u2018constant\u2019.\n\nThreshold image. All pixels in the input image higher than the corresponding\npixel in the threshold image are considered foreground.\n\nhttps://docs.opencv.org/modules/imgproc/doc/miscellaneous_transformations.html?highlight=threshold#adaptivethreshold\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.threshold_mean()", "path": "api/skimage.filters#skimage.filters.threshold_mean", "type": "filters", "text": "\nReturn threshold value based on the mean of grayscale values.\n\nGrayscale input image.\n\nUpper threshold value. All pixels with an intensity higher than this value are\nassumed to be foreground.\n\nC. A. Glasbey, \u201cAn analysis of histogram-based thresholding algorithms,\u201d\nCVGIP: Graphical Models and Image Processing, vol. 55, pp. 532-537, 1993.\nDOI:10.1006/cgip.1993.1040\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.threshold_minimum()", "path": "api/skimage.filters#skimage.filters.threshold_minimum", "type": "filters", "text": "\nReturn threshold value based on minimum method.\n\nThe histogram of the input `image` is computed if not provided and smoothed\nuntil there are only two maxima. Then the minimum in between is the threshold\nvalue.\n\nEither image or hist must be provided. In case hist is given, the actual\nhistogram of the image is ignored.\n\nInput image.\n\nNumber of bins used to calculate histogram. This value is ignored for integer\narrays.\n\nMaximum number of iterations to smooth the histogram.\n\nHistogram to determine the threshold from and a corresponding array of bin\ncenter intensities. Alternatively, only the histogram can be passed.\n\nUpper threshold value. All pixels with an intensity higher than this value are\nassumed to be foreground.\n\nIf unable to find two local maxima in the histogram or if the smoothing takes\nmore than 1e4 iterations.\n\nC. A. Glasbey, \u201cAn analysis of histogram-based thresholding algorithms,\u201d\nCVGIP: Graphical Models and Image Processing, vol. 55, pp. 532-537, 1993.\n\nPrewitt, JMS & Mendelsohn, ML (1966), \u201cThe analysis of cell images\u201d, Annals of\nthe New York Academy of Sciences 128: 1035-1053\nDOI:10.1111/j.1749-6632.1965.tb11715.x\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.threshold_multiotsu()", "path": "api/skimage.filters#skimage.filters.threshold_multiotsu", "type": "filters", "text": "\nGenerate `classes`-1 threshold values to divide gray levels in `image`.\n\nThe threshold values are chosen to maximize the total sum of pairwise\nvariances between the thresholded graylevel classes. See Notes and [1] for\nmore details.\n\nGrayscale input image.\n\nNumber of classes to be thresholded, i.e. the number of resulting regions.\n\nNumber of bins used to calculate the histogram. This value is ignored for\ninteger arrays.\n\nArray containing the threshold values for the desired classes.\n\nIf `image` contains less grayscale value then the desired number of classes.\n\nThis implementation relies on a Cython function whose complexity is\n\\\\(O\\left(\\frac{Ch^{C-1}}{(C-1)!}\\right)\\\\), where \\\\(h\\\\) is the number of\nhistogram bins and \\\\(C\\\\) is the number of classes desired.\n\nThe input image must be grayscale.\n\nLiao, P-S., Chen, T-S. and Chung, P-C., \u201cA fast algorithm for multilevel\nthresholding\u201d, Journal of Information Science and Engineering 17 (5): 713-727,\n2001. Available at: <https://ftp.iis.sinica.edu.tw/JISE/2001/200109_01.pdf>\nDOI:10.6688/JISE.2001.17.5.1\n\nTosa, Y., \u201cMulti-Otsu Threshold\u201d, a java plugin for ImageJ. Available at:\n<http://imagej.net/plugins/download/Multi_OtsuThreshold.java>\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.threshold_niblack()", "path": "api/skimage.filters#skimage.filters.threshold_niblack", "type": "filters", "text": "\nApplies Niblack local threshold to an array.\n\nA threshold T is calculated for every pixel in the image using the following\nformula:\n\nwhere m(x,y) and s(x,y) are the mean and standard deviation of pixel (x,y)\nneighborhood defined by a rectangular window with size w times w centered\naround the pixel. k is a configurable parameter that weights the effect of\nstandard deviation.\n\nInput image.\n\nWindow size specified as a single odd integer (3, 5, 7, \u2026), or an iterable of\nlength `image.ndim` containing only odd integers (e.g. `(1, 5, 5)`).\n\nValue of parameter k in threshold formula.\n\nThreshold mask. All pixels with an intensity higher than this value are\nassumed to be foreground.\n\nThis algorithm is originally designed for text recognition.\n\nThe Bradley threshold is a particular case of the Niblack one, being\nequivalent to\n\nfor some value `q`. By default, Bradley and Roth use `q=1`.\n\nW. Niblack, An introduction to Digital Image Processing, Prentice-Hall, 1986.\n\nD. Bradley and G. Roth, \u201cAdaptive thresholding using Integral Image\u201d, Journal\nof Graphics Tools 12(2), pp. 13-21, 2007. DOI:10.1080/2151237X.2007.10129236\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.threshold_otsu()", "path": "api/skimage.filters#skimage.filters.threshold_otsu", "type": "filters", "text": "\nReturn threshold value based on Otsu\u2019s method.\n\nEither image or hist must be provided. If hist is provided, the actual\nhistogram of the image is ignored.\n\nGrayscale input image.\n\nNumber of bins used to calculate histogram. This value is ignored for integer\narrays.\n\nHistogram from which to determine the threshold, and optionally a\ncorresponding array of bin center intensities. An alternative use of this\nfunction is to pass it only hist.\n\nUpper threshold value. All pixels with an intensity higher than this value are\nassumed to be foreground.\n\nThe input image must be grayscale.\n\nWikipedia, https://en.wikipedia.org/wiki/Otsu\u2019s_Method\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.threshold_sauvola()", "path": "api/skimage.filters#skimage.filters.threshold_sauvola", "type": "filters", "text": "\nApplies Sauvola local threshold to an array. Sauvola is a modification of\nNiblack technique.\n\nIn the original method a threshold T is calculated for every pixel in the\nimage using the following formula:\n\nwhere m(x,y) and s(x,y) are the mean and standard deviation of pixel (x,y)\nneighborhood defined by a rectangular window with size w times w centered\naround the pixel. k is a configurable parameter that weights the effect of\nstandard deviation. R is the maximum standard deviation of a greyscale image.\n\nInput image.\n\nWindow size specified as a single odd integer (3, 5, 7, \u2026), or an iterable of\nlength `image.ndim` containing only odd integers (e.g. `(1, 5, 5)`).\n\nValue of the positive parameter k.\n\nValue of R, the dynamic range of standard deviation. If None, set to the half\nof the image dtype range.\n\nThreshold mask. All pixels with an intensity higher than this value are\nassumed to be foreground.\n\nThis algorithm is originally designed for text recognition.\n\nJ. Sauvola and M. Pietikainen, \u201cAdaptive document image binarization,\u201d Pattern\nRecognition 33(2), pp. 225-236, 2000. DOI:10.1016/S0031-3203(99)00055-2\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.threshold_triangle()", "path": "api/skimage.filters#skimage.filters.threshold_triangle", "type": "filters", "text": "\nReturn threshold value based on the triangle algorithm.\n\nGrayscale input image.\n\nNumber of bins used to calculate histogram. This value is ignored for integer\narrays.\n\nUpper threshold value. All pixels with an intensity higher than this value are\nassumed to be foreground.\n\nZack, G. W., Rogers, W. E. and Latt, S. A., 1977, Automatic Measurement of\nSister Chromatid Exchange Frequency, Journal of Histochemistry and\nCytochemistry 25 (7), pp. 741-753 DOI:10.1177/25.7.70454\n\nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.threshold_yen()", "path": "api/skimage.filters#skimage.filters.threshold_yen", "type": "filters", "text": "\nReturn threshold value based on Yen\u2019s method. Either image or hist must be\nprovided. In case hist is given, the actual histogram of the image is ignored.\n\nInput image.\n\nNumber of bins used to calculate histogram. This value is ignored for integer\narrays.\n\nHistogram from which to determine the threshold, and optionally a\ncorresponding array of bin center intensities. An alternative use of this\nfunction is to pass it only hist.\n\nUpper threshold value. All pixels with an intensity higher than this value are\nassumed to be foreground.\n\nYen J.C., Chang F.J., and Chang S. (1995) \u201cA New Criterion for Automatic\nMultilevel Thresholding\u201d IEEE Trans. on Image Processing, 4(3): 370-378.\nDOI:10.1109/83.366472\n\nSezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and\nQuantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1):\n146-165, DOI:10.1117/1.1631315\nhttp://www.busim.ee.boun.edu.tr/~sankur/SankurFolder/Threshold_survey.pdf\n\nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.try_all_threshold()", "path": "api/skimage.filters#skimage.filters.try_all_threshold", "type": "filters", "text": "\nReturns a figure comparing the outputs of different thresholding methods.\n\nInput image.\n\nFigure size (in inches).\n\nPrint function name for each method.\n\nMatplotlib figure and axes.\n\nThe following algorithms are used:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.unsharp_mask()", "path": "api/skimage.filters#skimage.filters.unsharp_mask", "type": "filters", "text": "\nUnsharp masking filter.\n\nThe sharp details are identified as the difference between the original image\nand its blurred version. These details are then scaled, and added back to the\noriginal image.\n\nInput image.\n\nIf a scalar is given, then its value is used for all dimensions. If sequence\nis given, then there must be exactly one radius for each dimension except the\nlast dimension for multichannel images. Note that 0 radius means no blurring,\nand negative values are not allowed.\n\nThe details will be amplified with this factor. The factor could be 0 or\nnegative. Typically, it is a small positive number, e.g. 1.0.\n\nIf True, the last `image` dimension is considered as a color channel,\notherwise as spatial. Color channels are processed individually.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nImage with unsharp mask applied.\n\nUnsharp masking is an image sharpening technique. It is a linear image\noperation, and numerically stable, unlike deconvolution which is an ill-posed\nproblem. Because of this stability, it is often preferred over deconvolution.\n\nThe main idea is as follows: sharp details are identified as the difference\nbetween the original image and its blurred version. These details are added\nback to the original image after a scaling step:\n\nenhanced image = original + amount * (original - blurred)\n\nWhen applying this filter to several color layers independently, color\nbleeding may occur. More visually pleasing result can be achieved by\nprocessing only the brightness/lightness/intensity channel in a suitable color\nspace such as HSV, HSL, YUV, or YCbCr.\n\nUnsharp masking is described in most introductory digital image processing\nbooks. This implementation is based on [1].\n\nMaria Petrou, Costas Petrou \u201cImage Processing: The Fundamentals\u201d, (2010), ed\nii., page 357, ISBN 13: 9781119994398 DOI:10.1002/9781119994398\n\nWikipedia. Unsharp masking https://en.wikipedia.org/wiki/Unsharp_masking\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.wiener()", "path": "api/skimage.filters#skimage.filters.wiener", "type": "filters", "text": "\nMinimum Mean Square Error (Wiener) inverse filter.\n\nInput data.\n\nRatio between power spectrum of noise and undegraded image.\n\nImpulse response of the filter. See LPIFilter2D.__init__.\n\nAdditional keyword parameters to the impulse_response function.\n\nIf you need to apply the same filter multiple times over different images,\nconstruct the LPIFilter2D and specify it here.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "filters.window()", "path": "api/skimage.filters#skimage.filters.window", "type": "filters", "text": "\nReturn an n-dimensional window of a given size and dimensionality.\n\nThe type of window to be created. Any window type supported by\n`scipy.signal.get_window` is allowed here. See notes below for a current list,\nor the SciPy documentation for the version of SciPy on your machine.\n\nThe shape of the window along each axis. If an integer is provided, a 1D\nwindow is generated.\n\nKeyword arguments passed to `skimage.transform.warp` (e.g.,\n`warp_kwargs={'order':3}` to change interpolation method).\n\nA window of the specified `shape`. `dtype` is `np.double`.\n\nThis function is based on `scipy.signal.get_window` and thus can access all of\nthe window types available to that function (e.g., `\"hann\"`, `\"boxcar\"`). Note\nthat certain window types require parameters that have to be supplied with the\nwindow name as a tuple (e.g., `(\"tukey\", 0.8)`). If only a float is supplied,\nit is interpreted as the beta parameter of the Kaiser window.\n\nSee\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.windows.get_window.html\nfor more details.\n\nNote that this function generates a double precision array of the specified\n`shape` and can thus generate very large arrays that consume a large amount of\navailable memory.\n\nThe approach taken here to create nD windows is to first calculate the\nEuclidean distance from the center of the intended nD window to each position\nin the array. That distance is used to sample, with interpolation, from a 1D\nwindow returned from `scipy.signal.get_window`. The method of interpolation\ncan be changed with the `order` keyword argument passed to\n`skimage.transform.warp`.\n\nSome coordinates in the output window will be outside of the original signal;\nthese will be filled in with zeros.\n\nWindow types: - boxcar - triang - blackman - hamming - hann - bartlett -\nflattop - parzen - bohman - blackmanharris - nuttall - barthann - kaiser\n(needs beta) - gaussian (needs standard deviation) - general_gaussian (needs\npower, width) - slepian (needs width) - dpss (needs normalized half-bandwidth)\n- chebwin (needs attenuation) - exponential (needs decay scale) - tukey (needs\ntaper fraction)\n\nTwo-dimensional window design, Wikipedia,\nhttps://en.wikipedia.org/wiki/Two_dimensional_window_design\n\nReturn a Hann window with shape (512, 512):\n\nReturn a Kaiser window with beta parameter of 16 and shape (256, 256, 35):\n\nReturn a Tukey window with an alpha parameter of 0.8 and shape (100, 300):\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future", "path": "api/skimage.future", "type": "future", "text": "\nFunctionality with an experimental API. Although you can count on the\nfunctions in this package being around in the future, the API may change with\nany version update and will not follow the skimage two-version deprecation\npath. Therefore, use the functions herein with care, and do not use them in\nproduction code that will depend on updated skimage versions.\n\n`skimage.future.fit_segmenter`(labels, \u2026)\n\nSegmentation using labeled parts of the image and a classifier.\n\n`skimage.future.manual_lasso_segmentation`(image)\n\nReturn a label image based on freeform selections made with the mouse.\n\n`skimage.future.manual_polygon_segmentation`(image)\n\nReturn a label image based on polygon selections made with the mouse.\n\n`skimage.future.predict_segmenter`(features, clf)\n\nSegmentation of images using a pretrained classifier.\n\n`skimage.future.TrainableSegmenter`([clf, \u2026])\n\nEstimator for classifying pixels.\n\n`skimage.future.graph`\n\nSegmentation using labeled parts of the image and a classifier.\n\nImage of labels. Labels >= 1 correspond to the training set and label 0 to\nunlabeled pixels to be segmented.\n\nArray of features, with the first dimension corresponding to the number of\nfeatures, and the other dimensions correspond to `labels.shape`.\n\nclassifier object, exposing a `fit` and a `predict` method as in scikit-\nlearn\u2019s API, for example an instance of `RandomForestClassifier` or\n`LogisticRegression` classifier.\n\nclassifier trained on `labels`\n\nTrainable segmentation using local features and random forests\n\nReturn a label image based on freeform selections made with the mouse.\n\nGrayscale or RGB image.\n\nTransparency value for polygons drawn over the image.\n\nIf True, an array containing each separate polygon drawn is returned. (The\npolygons may overlap.) If False (default), latter polygons \u201coverwrite\u201d earlier\nones where they overlap.\n\nThe segmented regions. If mode is `\u2018separate\u2019`, the leading dimension of the\narray corresponds to the number of regions that the user drew.\n\nPress and hold the left mouse button to draw around each object.\n\nReturn a label image based on polygon selections made with the mouse.\n\nGrayscale or RGB image.\n\nTransparency value for polygons drawn over the image.\n\nIf True, an array containing each separate polygon drawn is returned. (The\npolygons may overlap.) If False (default), latter polygons \u201coverwrite\u201d earlier\nones where they overlap.\n\nThe segmented regions. If mode is `\u2018separate\u2019`, the leading dimension of the\narray corresponds to the number of regions that the user drew.\n\nUse left click to select the vertices of the polygon and right click to\nconfirm the selection once all vertices are selected.\n\nSegmentation of images using a pretrained classifier.\n\nArray of features, with the last dimension corresponding to the number of\nfeatures, and the other dimensions are compatible with the shape of the image\nto segment, or a flattened image.\n\ntrained classifier object, exposing a `predict` method as in scikit-learn\u2019s\nAPI, for example an instance of `RandomForestClassifier` or\n`LogisticRegression` classifier. The classifier must be already trained, for\nexample with `skimage.segmentation.fit_segmenter()`.\n\nLabeled array, built from the prediction of the classifier.\n\nTrainable segmentation using local features and random forests\n\nBases: `object`\n\nEstimator for classifying pixels.\n\nclassifier object, exposing a `fit` and a `predict` method as in scikit-\nlearn\u2019s API, for example an instance of `RandomForestClassifier` or\n`LogisticRegression` classifier.\n\nfunction computing features on all pixels of the image, to be passed to the\nclassifier. The output should be of shape `(m_features, *labels.shape)`. If\nNone, `skimage.segmentation.multiscale_basic_features()` is used.\n\n`fit`(image, labels)\n\nTrain classifier using partially labeled (annotated) image.\n\n`predict`(image)\n\nSegment new image using trained internal classifier.\n\ncompute_features\n\nInitialize self. See help(type(self)) for accurate signature.\n\nTrain classifier using partially labeled (annotated) image.\n\nInput image, which can be grayscale or multichannel, and must have a number of\ndimensions compatible with `self.features_func`.\n\nLabeled array of shape compatible with `image` (same shape for a single-\nchannel image). Labels >= 1 correspond to the training set and label 0 to\nunlabeled pixels to be segmented.\n\nSegment new image using trained internal classifier.\n\nInput image, which can be grayscale or multichannel, and must have a number of\ndimensions compatible with `self.features_func`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.fit_segmenter()", "path": "api/skimage.future#skimage.future.fit_segmenter", "type": "future", "text": "\nSegmentation using labeled parts of the image and a classifier.\n\nImage of labels. Labels >= 1 correspond to the training set and label 0 to\nunlabeled pixels to be segmented.\n\nArray of features, with the first dimension corresponding to the number of\nfeatures, and the other dimensions correspond to `labels.shape`.\n\nclassifier object, exposing a `fit` and a `predict` method as in scikit-\nlearn\u2019s API, for example an instance of `RandomForestClassifier` or\n`LogisticRegression` classifier.\n\nclassifier trained on `labels`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.graph", "path": "api/skimage.future.graph", "type": "future", "text": "\n`skimage.future.graph.cut_normalized`(labels, rag)\n\nPerform Normalized Graph cut on the Region Adjacency Graph.\n\n`skimage.future.graph.cut_threshold`(labels, \u2026)\n\nCombine regions separated by weight less than threshold.\n\n`skimage.future.graph.merge_hierarchical`(\u2026)\n\nPerform hierarchical merging of a RAG.\n\n`skimage.future.graph.ncut`(labels, rag[, \u2026])\n\nPerform Normalized Graph cut on the Region Adjacency Graph.\n\n`skimage.future.graph.rag_boundary`(labels, \u2026)\n\nComouter RAG based on region boundaries\n\n`skimage.future.graph.rag_mean_color`(image, \u2026)\n\nCompute the Region Adjacency Graph using mean colors.\n\n`skimage.future.graph.show_rag`(labels, rag, image)\n\nShow a Region Adjacency Graph on an image.\n\n`skimage.future.graph.RAG`([label_image, \u2026])\n\nThe Region Adjacency Graph (RAG) of an image, subclasses networx.Graph\n\nPerform Normalized Graph cut on the Region Adjacency Graph.\n\nGiven an image\u2019s labels and its similarity RAG, recursively perform a 2-way\nnormalized cut on it. All nodes belonging to a subgraph that cannot be cut\nfurther are assigned a unique label in the output.\n\nThe array of labels.\n\nThe region adjacency graph.\n\nThe threshold. A subgraph won\u2019t be further subdivided if the value of the\nN-cut exceeds `thresh`.\n\nThe number or N-cuts to perform before determining the optimal one.\n\nIf set, modifies `rag` in place. For each node `n` the function will set a new\nattribute `rag.nodes[n]['ncut label']`.\n\nThe maximum possible value of an edge in the RAG. This corresponds to an edge\nbetween identical regions. This is used to put self edges in the RAG.\n\nIf int, random_state is the seed used by the random number generator; If\nRandomState instance, random_state is the random number generator; If None,\nthe random number generator is the RandomState instance used by `np.random`.\nThe random state is used for the starting point of\n`scipy.sparse.linalg.eigsh`.\n\nThe new labeled array.\n\nShi, J.; Malik, J., \u201cNormalized cuts and image segmentation\u201d, Pattern Analysis\nand Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 888-905,\nAugust 2000.\n\nCombine regions separated by weight less than threshold.\n\nGiven an image\u2019s labels and its RAG, output new labels by combining regions\nwhose nodes are separated by a weight less than the given threshold.\n\nThe array of labels.\n\nThe region adjacency graph.\n\nThe threshold. Regions connected by edges with smaller weights are combined.\n\nIf set, modifies `rag` in place. The function will remove the edges with\nweights less that `thresh`. If set to `False` the function makes a copy of\n`rag` before proceeding.\n\nThe new labelled array.\n\nAlain Tremeau and Philippe Colantoni \u201cRegions Adjacency Graph Applied To Color\nImage Segmentation\u201d DOI:10.1109/83.841950\n\nPerform hierarchical merging of a RAG.\n\nGreedily merges the most similar pair of nodes until no edges lower than\n`thresh` remain.\n\nThe array of labels.\n\nThe Region Adjacency Graph.\n\nRegions connected by an edge with weight smaller than `thresh` are merged.\n\nIf set, the RAG copied before modifying.\n\nIf set, the nodes are merged in place. Otherwise, a new node is created for\neach merge..\n\nThis function is called before merging two nodes. For the RAG `graph` while\nmerging `src` and `dst`, it is called as follows `merge_func(graph, src,\ndst)`.\n\nThe function to compute the new weights of the nodes adjacent to the merged\nnode. This is directly supplied as the argument `weight_func` to\n`merge_nodes`.\n\nThe new labeled array.\n\nPerform Normalized Graph cut on the Region Adjacency Graph.\n\nGiven an image\u2019s labels and its similarity RAG, recursively perform a 2-way\nnormalized cut on it. All nodes belonging to a subgraph that cannot be cut\nfurther are assigned a unique label in the output.\n\nThe array of labels.\n\nThe region adjacency graph.\n\nThe threshold. A subgraph won\u2019t be further subdivided if the value of the\nN-cut exceeds `thresh`.\n\nThe number or N-cuts to perform before determining the optimal one.\n\nIf set, modifies `rag` in place. For each node `n` the function will set a new\nattribute `rag.nodes[n]['ncut label']`.\n\nThe maximum possible value of an edge in the RAG. This corresponds to an edge\nbetween identical regions. This is used to put self edges in the RAG.\n\nIf int, random_state is the seed used by the random number generator; If\nRandomState instance, random_state is the random number generator; If None,\nthe random number generator is the RandomState instance used by `np.random`.\nThe random state is used for the starting point of\n`scipy.sparse.linalg.eigsh`.\n\nThe new labeled array.\n\nShi, J.; Malik, J., \u201cNormalized cuts and image segmentation\u201d, Pattern Analysis\nand Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 888-905,\nAugust 2000.\n\nComouter RAG based on region boundaries\n\nGiven an image\u2019s initial segmentation and its edge map this method constructs\nthe corresponding Region Adjacency Graph (RAG). Each node in the RAG\nrepresents a set of pixels within the image with the same label in `labels`.\nThe weight between two adjacent regions is the average value in `edge_map`\nalong their boundary.\n\nThe labelled image.\n\nThis should have the same shape as that of `labels`. For all pixels along the\nboundary between 2 adjacent regions, the average value of the corresponding\npixels in `edge_map` is the edge weight between them.\n\nPixels with a squared distance less than `connectivity` from each other are\nconsidered adjacent. It can range from 1 to `labels.ndim`. Its behavior is the\nsame as `connectivity` parameter in\n`scipy.ndimage.filters.generate_binary_structure`.\n\nCompute the Region Adjacency Graph using mean colors.\n\nGiven an image and its initial segmentation, this method constructs the\ncorresponding Region Adjacency Graph (RAG). Each node in the RAG represents a\nset of pixels within `image` with the same label in `labels`. The weight\nbetween two adjacent regions represents how similar or dissimilar two regions\nare depending on the `mode` parameter.\n\nInput image.\n\nThe labelled image. This should have one dimension less than `image`. If\n`image` has dimensions `(M, N, 3)` `labels` should have dimensions `(M, N)`.\n\nPixels with a squared distance less than `connectivity` from each other are\nconsidered adjacent. It can range from 1 to `labels.ndim`. Its behavior is the\nsame as `connectivity` parameter in `scipy.ndimage.generate_binary_structure`.\n\nThe strategy to assign edge weights.\n\n\u2018distance\u2019 : The weight between two adjacent regions is the \\\\(|c_1 - c_2|\\\\),\nwhere \\\\(c_1\\\\) and \\\\(c_2\\\\) are the mean colors of the two regions. It\nrepresents the Euclidean distance in their average color.\n\n\u2018similarity\u2019 : The weight between two adjacent is \\\\(e^{-d^2/sigma}\\\\) where\n\\\\(d=|c_1 - c_2|\\\\), where \\\\(c_1\\\\) and \\\\(c_2\\\\) are the mean colors of the\ntwo regions. It represents how similar two regions are.\n\nUsed for computation when `mode` is \u201csimilarity\u201d. It governs how close to each\nother two colors should be, for their corresponding edge weight to be\nsignificant. A very large value of `sigma` could make any two colors behave as\nthough they were similar.\n\nThe region adjacency graph.\n\nAlain Tremeau and Philippe Colantoni \u201cRegions Adjacency Graph Applied To Color\nImage Segmentation\u201d DOI:10.1109/83.841950\n\nShow a Region Adjacency Graph on an image.\n\nGiven a labelled image and its corresponding RAG, show the nodes and edges of\nthe RAG on the image with the specified colors. Edges are displayed between\nthe centroid of the 2 adjacent regions in the image.\n\nThe labelled image.\n\nThe Region Adjacency Graph.\n\nInput image. If `colormap` is `None`, the image should be in RGB format.\n\nColor with which the borders between regions are drawn.\n\nThe thickness with which the RAG edges are drawn.\n\nAny matplotlib colormap with which the edges are drawn.\n\nAny matplotlib colormap with which the image is draw. If set to `None` the\nimage is drawn as it is.\n\nIf set, the RAG is modified in place. For each node `n` the function will set\na new attribute `rag.nodes[n]['centroid']`.\n\nThe axes to draw on. If not specified, new axes are created and drawn on.\n\nA colection of lines that represent the edges of the graph. It can be passed\nto the `matplotlib.figure.Figure.colorbar()` function.\n\nBases: `networkx.classes.graph.Graph`\n\nThe Region Adjacency Graph (RAG) of an image, subclasses networx.Graph\n\nAn initial segmentation, with each region labeled as a different integer.\nEvery unique value in `label_image` will correspond to a node in the graph.\n\nThe connectivity between pixels in `label_image`. For a 2D image, a\nconnectivity of 1 corresponds to immediate neighbors up, down, left, and\nright, while a connectivity of 2 also includes diagonal neighbors. See\n`scipy.ndimage.generate_binary_structure`.\n\nInitial or additional edges to pass to the NetworkX Graph constructor. See\n`networkx.Graph`. Valid edge specifications include edge list (list of\ntuples), NumPy arrays, and SciPy sparse matrices.\n\nAdditional attributes to add to the graph.\n\nInitialize a graph with edges, name, or graph attributes.\n\nData to initialize graph. If None (default) an empty graph is created. The\ndata can be an edge list, or any NetworkX graph object. If the corresponding\noptional Python packages are installed the data can also be a NumPy matrix or\n2d ndarray, a SciPy sparse matrix, or a PyGraphviz graph.\n\nAttributes to add to graph as key=value pairs.\n\nSee also\n\nArbitrary graph attribute pairs (key=value) may be assigned\n\nAdd an edge between `u` and `v` while updating max node id.\n\nSee also\n\n`networkx.Graph.add_edge()`.\n\nAdd node `n` while updating the maximum node id.\n\nSee also\n\n`networkx.Graph.add_node()`.\n\nCopy the graph with its max node id.\n\nSee also\n\n`networkx.Graph.copy()`.\n\nReturn a fresh copy graph with the same data structure.\n\nA fresh copy has no nodes, edges or graph attributes. It is the same data\nstructure as the current graph. This method is typically used to create an\nempty version of the graph.\n\nThis is required when subclassing Graph with networkx v2 and does not cause\nproblems for v1. Here is more detail from the network migrating from 1.x to\n2.x document:\n\nMerge node `src` and `dst`.\n\nThe new combined node is adjacent to all the neighbors of `src` and `dst`.\n`weight_func` is called to decide the weight of edges incident on the new\nnode.\n\nNodes to be merged.\n\nFunction to decide the attributes of edges incident on the new node. For each\nneighbor `n` for `src and `dst`, `weight_func` will be called as follows:\n`weight_func(src, dst, n, *extra_arguments, **extra_keywords)`. `src`, `dst`\nand `n` are IDs of vertices in the RAG object which is in turn a subclass of\n`networkx.Graph`. It is expected to return a dict of attributes of the\nresulting edge.\n\nIf set to `True`, the merged node has the id `dst`, else merged node has a new\nid which is returned.\n\nThe sequence of extra positional arguments passed to `weight_func`.\n\nThe dict of keyword arguments passed to the `weight_func`.\n\nThe id of the new node.\n\nIf `in_place` is `False` the resulting node has a new id, rather than `dst`.\n\nReturns the `id` for the new node to be inserted.\n\nThe current implementation returns one more than the maximum `id`.\n\nThe `id` of the new node to be inserted.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.graph.cut_normalized()", "path": "api/skimage.future.graph#skimage.future.graph.cut_normalized", "type": "future", "text": "\nPerform Normalized Graph cut on the Region Adjacency Graph.\n\nGiven an image\u2019s labels and its similarity RAG, recursively perform a 2-way\nnormalized cut on it. All nodes belonging to a subgraph that cannot be cut\nfurther are assigned a unique label in the output.\n\nThe array of labels.\n\nThe region adjacency graph.\n\nThe threshold. A subgraph won\u2019t be further subdivided if the value of the\nN-cut exceeds `thresh`.\n\nThe number or N-cuts to perform before determining the optimal one.\n\nIf set, modifies `rag` in place. For each node `n` the function will set a new\nattribute `rag.nodes[n]['ncut label']`.\n\nThe maximum possible value of an edge in the RAG. This corresponds to an edge\nbetween identical regions. This is used to put self edges in the RAG.\n\nIf int, random_state is the seed used by the random number generator; If\nRandomState instance, random_state is the random number generator; If None,\nthe random number generator is the RandomState instance used by `np.random`.\nThe random state is used for the starting point of\n`scipy.sparse.linalg.eigsh`.\n\nThe new labeled array.\n\nShi, J.; Malik, J., \u201cNormalized cuts and image segmentation\u201d, Pattern Analysis\nand Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 888-905,\nAugust 2000.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.graph.cut_threshold()", "path": "api/skimage.future.graph#skimage.future.graph.cut_threshold", "type": "future", "text": "\nCombine regions separated by weight less than threshold.\n\nGiven an image\u2019s labels and its RAG, output new labels by combining regions\nwhose nodes are separated by a weight less than the given threshold.\n\nThe array of labels.\n\nThe region adjacency graph.\n\nThe threshold. Regions connected by edges with smaller weights are combined.\n\nIf set, modifies `rag` in place. The function will remove the edges with\nweights less that `thresh`. If set to `False` the function makes a copy of\n`rag` before proceeding.\n\nThe new labelled array.\n\nAlain Tremeau and Philippe Colantoni \u201cRegions Adjacency Graph Applied To Color\nImage Segmentation\u201d DOI:10.1109/83.841950\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.graph.merge_hierarchical()", "path": "api/skimage.future.graph#skimage.future.graph.merge_hierarchical", "type": "future", "text": "\nPerform hierarchical merging of a RAG.\n\nGreedily merges the most similar pair of nodes until no edges lower than\n`thresh` remain.\n\nThe array of labels.\n\nThe Region Adjacency Graph.\n\nRegions connected by an edge with weight smaller than `thresh` are merged.\n\nIf set, the RAG copied before modifying.\n\nIf set, the nodes are merged in place. Otherwise, a new node is created for\neach merge..\n\nThis function is called before merging two nodes. For the RAG `graph` while\nmerging `src` and `dst`, it is called as follows `merge_func(graph, src,\ndst)`.\n\nThe function to compute the new weights of the nodes adjacent to the merged\nnode. This is directly supplied as the argument `weight_func` to\n`merge_nodes`.\n\nThe new labeled array.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.graph.ncut()", "path": "api/skimage.future.graph#skimage.future.graph.ncut", "type": "future", "text": "\nPerform Normalized Graph cut on the Region Adjacency Graph.\n\nGiven an image\u2019s labels and its similarity RAG, recursively perform a 2-way\nnormalized cut on it. All nodes belonging to a subgraph that cannot be cut\nfurther are assigned a unique label in the output.\n\nThe array of labels.\n\nThe region adjacency graph.\n\nThe threshold. A subgraph won\u2019t be further subdivided if the value of the\nN-cut exceeds `thresh`.\n\nThe number or N-cuts to perform before determining the optimal one.\n\nIf set, modifies `rag` in place. For each node `n` the function will set a new\nattribute `rag.nodes[n]['ncut label']`.\n\nThe maximum possible value of an edge in the RAG. This corresponds to an edge\nbetween identical regions. This is used to put self edges in the RAG.\n\nIf int, random_state is the seed used by the random number generator; If\nRandomState instance, random_state is the random number generator; If None,\nthe random number generator is the RandomState instance used by `np.random`.\nThe random state is used for the starting point of\n`scipy.sparse.linalg.eigsh`.\n\nThe new labeled array.\n\nShi, J.; Malik, J., \u201cNormalized cuts and image segmentation\u201d, Pattern Analysis\nand Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 888-905,\nAugust 2000.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.graph.RAG", "path": "api/skimage.future.graph#skimage.future.graph.RAG", "type": "future", "text": "\nBases: `networkx.classes.graph.Graph`\n\nThe Region Adjacency Graph (RAG) of an image, subclasses networx.Graph\n\nAn initial segmentation, with each region labeled as a different integer.\nEvery unique value in `label_image` will correspond to a node in the graph.\n\nThe connectivity between pixels in `label_image`. For a 2D image, a\nconnectivity of 1 corresponds to immediate neighbors up, down, left, and\nright, while a connectivity of 2 also includes diagonal neighbors. See\n`scipy.ndimage.generate_binary_structure`.\n\nInitial or additional edges to pass to the NetworkX Graph constructor. See\n`networkx.Graph`. Valid edge specifications include edge list (list of\ntuples), NumPy arrays, and SciPy sparse matrices.\n\nAdditional attributes to add to the graph.\n\nInitialize a graph with edges, name, or graph attributes.\n\nData to initialize graph. If None (default) an empty graph is created. The\ndata can be an edge list, or any NetworkX graph object. If the corresponding\noptional Python packages are installed the data can also be a NumPy matrix or\n2d ndarray, a SciPy sparse matrix, or a PyGraphviz graph.\n\nAttributes to add to graph as key=value pairs.\n\nSee also\n\nArbitrary graph attribute pairs (key=value) may be assigned\n\nAdd an edge between `u` and `v` while updating max node id.\n\nSee also\n\n`networkx.Graph.add_edge()`.\n\nAdd node `n` while updating the maximum node id.\n\nSee also\n\n`networkx.Graph.add_node()`.\n\nCopy the graph with its max node id.\n\nSee also\n\n`networkx.Graph.copy()`.\n\nReturn a fresh copy graph with the same data structure.\n\nA fresh copy has no nodes, edges or graph attributes. It is the same data\nstructure as the current graph. This method is typically used to create an\nempty version of the graph.\n\nThis is required when subclassing Graph with networkx v2 and does not cause\nproblems for v1. Here is more detail from the network migrating from 1.x to\n2.x document:\n\nMerge node `src` and `dst`.\n\nThe new combined node is adjacent to all the neighbors of `src` and `dst`.\n`weight_func` is called to decide the weight of edges incident on the new\nnode.\n\nNodes to be merged.\n\nFunction to decide the attributes of edges incident on the new node. For each\nneighbor `n` for `src and `dst`, `weight_func` will be called as follows:\n`weight_func(src, dst, n, *extra_arguments, **extra_keywords)`. `src`, `dst`\nand `n` are IDs of vertices in the RAG object which is in turn a subclass of\n`networkx.Graph`. It is expected to return a dict of attributes of the\nresulting edge.\n\nIf set to `True`, the merged node has the id `dst`, else merged node has a new\nid which is returned.\n\nThe sequence of extra positional arguments passed to `weight_func`.\n\nThe dict of keyword arguments passed to the `weight_func`.\n\nThe id of the new node.\n\nIf `in_place` is `False` the resulting node has a new id, rather than `dst`.\n\nReturns the `id` for the new node to be inserted.\n\nThe current implementation returns one more than the maximum `id`.\n\nThe `id` of the new node to be inserted.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.graph.RAG.add_edge()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.add_edge", "type": "future", "text": "\nAdd an edge between `u` and `v` while updating max node id.\n\nSee also\n\n`networkx.Graph.add_edge()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.graph.RAG.add_node()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.add_node", "type": "future", "text": "\nAdd node `n` while updating the maximum node id.\n\nSee also\n\n`networkx.Graph.add_node()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.graph.RAG.copy()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.copy", "type": "future", "text": "\nCopy the graph with its max node id.\n\nSee also\n\n`networkx.Graph.copy()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.graph.RAG.fresh_copy()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.fresh_copy", "type": "future", "text": "\nReturn a fresh copy graph with the same data structure.\n\nA fresh copy has no nodes, edges or graph attributes. It is the same data\nstructure as the current graph. This method is typically used to create an\nempty version of the graph.\n\nThis is required when subclassing Graph with networkx v2 and does not cause\nproblems for v1. Here is more detail from the network migrating from 1.x to\n2.x document:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.graph.RAG.merge_nodes()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.merge_nodes", "type": "future", "text": "\nMerge node `src` and `dst`.\n\nThe new combined node is adjacent to all the neighbors of `src` and `dst`.\n`weight_func` is called to decide the weight of edges incident on the new\nnode.\n\nNodes to be merged.\n\nFunction to decide the attributes of edges incident on the new node. For each\nneighbor `n` for `src and `dst`, `weight_func` will be called as follows:\n`weight_func(src, dst, n, *extra_arguments, **extra_keywords)`. `src`, `dst`\nand `n` are IDs of vertices in the RAG object which is in turn a subclass of\n`networkx.Graph`. It is expected to return a dict of attributes of the\nresulting edge.\n\nIf set to `True`, the merged node has the id `dst`, else merged node has a new\nid which is returned.\n\nThe sequence of extra positional arguments passed to `weight_func`.\n\nThe dict of keyword arguments passed to the `weight_func`.\n\nThe id of the new node.\n\nIf `in_place` is `False` the resulting node has a new id, rather than `dst`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.graph.RAG.next_id()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.next_id", "type": "future", "text": "\nReturns the `id` for the new node to be inserted.\n\nThe current implementation returns one more than the maximum `id`.\n\nThe `id` of the new node to be inserted.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.graph.RAG.__init__()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.__init__", "type": "future", "text": "\nInitialize a graph with edges, name, or graph attributes.\n\nData to initialize graph. If None (default) an empty graph is created. The\ndata can be an edge list, or any NetworkX graph object. If the corresponding\noptional Python packages are installed the data can also be a NumPy matrix or\n2d ndarray, a SciPy sparse matrix, or a PyGraphviz graph.\n\nAttributes to add to graph as key=value pairs.\n\nSee also\n\nArbitrary graph attribute pairs (key=value) may be assigned\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.graph.rag_boundary()", "path": "api/skimage.future.graph#skimage.future.graph.rag_boundary", "type": "future", "text": "\nComouter RAG based on region boundaries\n\nGiven an image\u2019s initial segmentation and its edge map this method constructs\nthe corresponding Region Adjacency Graph (RAG). Each node in the RAG\nrepresents a set of pixels within the image with the same label in `labels`.\nThe weight between two adjacent regions is the average value in `edge_map`\nalong their boundary.\n\nThe labelled image.\n\nThis should have the same shape as that of `labels`. For all pixels along the\nboundary between 2 adjacent regions, the average value of the corresponding\npixels in `edge_map` is the edge weight between them.\n\nPixels with a squared distance less than `connectivity` from each other are\nconsidered adjacent. It can range from 1 to `labels.ndim`. Its behavior is the\nsame as `connectivity` parameter in\n`scipy.ndimage.filters.generate_binary_structure`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.graph.rag_mean_color()", "path": "api/skimage.future.graph#skimage.future.graph.rag_mean_color", "type": "future", "text": "\nCompute the Region Adjacency Graph using mean colors.\n\nGiven an image and its initial segmentation, this method constructs the\ncorresponding Region Adjacency Graph (RAG). Each node in the RAG represents a\nset of pixels within `image` with the same label in `labels`. The weight\nbetween two adjacent regions represents how similar or dissimilar two regions\nare depending on the `mode` parameter.\n\nInput image.\n\nThe labelled image. This should have one dimension less than `image`. If\n`image` has dimensions `(M, N, 3)` `labels` should have dimensions `(M, N)`.\n\nPixels with a squared distance less than `connectivity` from each other are\nconsidered adjacent. It can range from 1 to `labels.ndim`. Its behavior is the\nsame as `connectivity` parameter in `scipy.ndimage.generate_binary_structure`.\n\nThe strategy to assign edge weights.\n\n\u2018distance\u2019 : The weight between two adjacent regions is the \\\\(|c_1 - c_2|\\\\),\nwhere \\\\(c_1\\\\) and \\\\(c_2\\\\) are the mean colors of the two regions. It\nrepresents the Euclidean distance in their average color.\n\n\u2018similarity\u2019 : The weight between two adjacent is \\\\(e^{-d^2/sigma}\\\\) where\n\\\\(d=|c_1 - c_2|\\\\), where \\\\(c_1\\\\) and \\\\(c_2\\\\) are the mean colors of the\ntwo regions. It represents how similar two regions are.\n\nUsed for computation when `mode` is \u201csimilarity\u201d. It governs how close to each\nother two colors should be, for their corresponding edge weight to be\nsignificant. A very large value of `sigma` could make any two colors behave as\nthough they were similar.\n\nThe region adjacency graph.\n\nAlain Tremeau and Philippe Colantoni \u201cRegions Adjacency Graph Applied To Color\nImage Segmentation\u201d DOI:10.1109/83.841950\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.graph.show_rag()", "path": "api/skimage.future.graph#skimage.future.graph.show_rag", "type": "future", "text": "\nShow a Region Adjacency Graph on an image.\n\nGiven a labelled image and its corresponding RAG, show the nodes and edges of\nthe RAG on the image with the specified colors. Edges are displayed between\nthe centroid of the 2 adjacent regions in the image.\n\nThe labelled image.\n\nThe Region Adjacency Graph.\n\nInput image. If `colormap` is `None`, the image should be in RGB format.\n\nColor with which the borders between regions are drawn.\n\nThe thickness with which the RAG edges are drawn.\n\nAny matplotlib colormap with which the edges are drawn.\n\nAny matplotlib colormap with which the image is draw. If set to `None` the\nimage is drawn as it is.\n\nIf set, the RAG is modified in place. For each node `n` the function will set\na new attribute `rag.nodes[n]['centroid']`.\n\nThe axes to draw on. If not specified, new axes are created and drawn on.\n\nA colection of lines that represent the edges of the graph. It can be passed\nto the `matplotlib.figure.Figure.colorbar()` function.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.manual_lasso_segmentation()", "path": "api/skimage.future#skimage.future.manual_lasso_segmentation", "type": "future", "text": "\nReturn a label image based on freeform selections made with the mouse.\n\nGrayscale or RGB image.\n\nTransparency value for polygons drawn over the image.\n\nIf True, an array containing each separate polygon drawn is returned. (The\npolygons may overlap.) If False (default), latter polygons \u201coverwrite\u201d earlier\nones where they overlap.\n\nThe segmented regions. If mode is `\u2018separate\u2019`, the leading dimension of the\narray corresponds to the number of regions that the user drew.\n\nPress and hold the left mouse button to draw around each object.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.manual_polygon_segmentation()", "path": "api/skimage.future#skimage.future.manual_polygon_segmentation", "type": "future", "text": "\nReturn a label image based on polygon selections made with the mouse.\n\nGrayscale or RGB image.\n\nTransparency value for polygons drawn over the image.\n\nIf True, an array containing each separate polygon drawn is returned. (The\npolygons may overlap.) If False (default), latter polygons \u201coverwrite\u201d earlier\nones where they overlap.\n\nThe segmented regions. If mode is `\u2018separate\u2019`, the leading dimension of the\narray corresponds to the number of regions that the user drew.\n\nUse left click to select the vertices of the polygon and right click to\nconfirm the selection once all vertices are selected.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.predict_segmenter()", "path": "api/skimage.future#skimage.future.predict_segmenter", "type": "future", "text": "\nSegmentation of images using a pretrained classifier.\n\nArray of features, with the last dimension corresponding to the number of\nfeatures, and the other dimensions are compatible with the shape of the image\nto segment, or a flattened image.\n\ntrained classifier object, exposing a `predict` method as in scikit-learn\u2019s\nAPI, for example an instance of `RandomForestClassifier` or\n`LogisticRegression` classifier. The classifier must be already trained, for\nexample with `skimage.segmentation.fit_segmenter()`.\n\nLabeled array, built from the prediction of the classifier.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.TrainableSegmenter", "path": "api/skimage.future#skimage.future.TrainableSegmenter", "type": "future", "text": "\nBases: `object`\n\nEstimator for classifying pixels.\n\nclassifier object, exposing a `fit` and a `predict` method as in scikit-\nlearn\u2019s API, for example an instance of `RandomForestClassifier` or\n`LogisticRegression` classifier.\n\nfunction computing features on all pixels of the image, to be passed to the\nclassifier. The output should be of shape `(m_features, *labels.shape)`. If\nNone, `skimage.segmentation.multiscale_basic_features()` is used.\n\n`fit`(image, labels)\n\nTrain classifier using partially labeled (annotated) image.\n\n`predict`(image)\n\nSegment new image using trained internal classifier.\n\ncompute_features\n\nInitialize self. See help(type(self)) for accurate signature.\n\nTrain classifier using partially labeled (annotated) image.\n\nInput image, which can be grayscale or multichannel, and must have a number of\ndimensions compatible with `self.features_func`.\n\nLabeled array of shape compatible with `image` (same shape for a single-\nchannel image). Labels >= 1 correspond to the training set and label 0 to\nunlabeled pixels to be segmented.\n\nSegment new image using trained internal classifier.\n\nInput image, which can be grayscale or multichannel, and must have a number of\ndimensions compatible with `self.features_func`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.TrainableSegmenter.compute_features()", "path": "api/skimage.future#skimage.future.TrainableSegmenter.compute_features", "type": "future", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.TrainableSegmenter.fit()", "path": "api/skimage.future#skimage.future.TrainableSegmenter.fit", "type": "future", "text": "\nTrain classifier using partially labeled (annotated) image.\n\nInput image, which can be grayscale or multichannel, and must have a number of\ndimensions compatible with `self.features_func`.\n\nLabeled array of shape compatible with `image` (same shape for a single-\nchannel image). Labels >= 1 correspond to the training set and label 0 to\nunlabeled pixels to be segmented.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.TrainableSegmenter.predict()", "path": "api/skimage.future#skimage.future.TrainableSegmenter.predict", "type": "future", "text": "\nSegment new image using trained internal classifier.\n\nInput image, which can be grayscale or multichannel, and must have a number of\ndimensions compatible with `self.features_func`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "future.TrainableSegmenter.__init__()", "path": "api/skimage.future#skimage.future.TrainableSegmenter.__init__", "type": "future", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "Geometrical transformations of images", "path": "user_guide/geometrical_transform", "type": "Guide", "text": "\nImages being NumPy arrays (as described in the A crash course on NumPy for\nimages section), cropping an image can be done with simple slicing operations.\nBelow we crop a 100x100 square corresponding to the top-left corner of the\nastronaut image. Note that this operation is done for all color channels (the\ncolor dimension is the last, third dimension):\n\nIn order to change the shape of the image, `skimage.color` provides several\nfunctions described in Rescale, resize, and downscale .\n\nHomographies are transformations of a Euclidean space that preserve the\nalignment of points. Specific cases of homographies correspond to the\nconservation of more properties, such as parallelism (affine transformation),\nshape (similar transformation) or distances (Euclidean transformation). The\ndifferent types of homographies available in scikit-image are presented in\nTypes of homographies.\n\nProjective transformations can either be created using the explicit parameters\n(e.g. scale, shear, rotation and translation):\n\nor the full transformation matrix:\n\nThe transformation matrix of a transform is available as its `tform.params`\nattribute. Transformations can be composed by multiplying matrices with the\n`@` matrix multiplication operator.\n\nTransformation matrices use Homogeneous coordinates, which are the extension\nof Cartesian coordinates used in Euclidean geometry to the more general\nprojective geometry. In particular, points at infinity can be represented with\nfinite coordinates.\n\nTransformations can be applied to images using `skimage.transform.warp()`:\n\nThe different transformations in `skimage.transform` have a `estimate` method\nin order to estimate the parameters of the transformation from two sets of\npoints (the source and the destination), as explained in the Using geometric\ntransformations tutorial:\n\nThe `estimate` method uses least-squares optimization to minimize the distance\nbetween source and optimization. Source and destination points can be\ndetermined manually, or using the different methods for feature detection\navailable in `skimage.feature`, such as\n\nand matching points using `skimage.feature.match_descriptors()` before\nestimating transformation parameters. However, spurious matches are often\nmade, and it is advisable to use the RANSAC algorithm (instead of simple\nleast-squares optimization) to improve the robustness to outliers, as\nexplained in Robust matching using RANSAC.\n\nExamples showing applications of transformation estimation are\n\nThe `estimate` method is point-based, that is, it uses only a set of points\nfrom the source and destination images. For estimating translations (shifts),\nit is also possible to use a full-field method using all pixels, based on\nFourier-space cross-correlation. This method is implemented by\n`skimage.registration.register_translation()` and explained in the Image\nRegistration tutorial.\n\nThe Using Polar and Log-Polar Transformations for Registration tutorial\nexplains a variant of this full-field method for estimating a rotation, by\nusing first a log-polar transformation.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "Getting help on using skimage", "path": "user_guide/getting_help", "type": "Guide", "text": "\nBesides the user guide, there exist other opportunities to get help on using\n`skimage`.\n\nThe General examples gallery provides graphical examples of typical image\nprocessing tasks. By a quick glance at the different thumbnails, the user may\nfind an example close to a typical use case of interest. Each graphical\nexample page displays an introductory paragraph, a figure, and the source code\nthat generated the figure. Downloading the Python source code enables one to\nmodify quickly the example into a case closer to one\u2019s image processing\napplications.\n\nUsers are warmly encouraged to report on their use of `skimage` on the\nMailing-list, in order to propose more examples in the future. Contributing\nexamples to the gallery can be done on github (see How to contribute to\nscikit-image).\n\nThe `quick search` field located in the navigation bar of the html\ndocumentation can be used to search for specific keywords (segmentation,\nrescaling, denoising, etc.).\n\nNumPy provides a `lookfor` function to search API functions. By default\n`lookfor` will search the NumPy API. NumPy lookfor example:\n``np.lookfor('eigenvector') ``\n\nBut it can be used to search in modules, by passing in the module name as a\nstring:\n\n`` np.lookfor('boundaries', 'skimage') ``\n\nor the module itself. `` > import skimage > np.lookfor('boundaries', skimage)\n``\n\nDocstrings of `skimage` functions are formatted using Numpy\u2019s documentation\nstandard, starting with a `Parameters` section for the arguments and a\n`Returns` section for the objects returned by the function. Also, most\nfunctions include one or more examples.\n\nThe scikit-image mailing-list is scikit-image@python.org (users should join\nbefore posting). This mailing-list is shared by users and developers, and it\nis the right place to ask any question about `skimage`, or in general, image\nprocessing using Python. Posting snippets of code with minimal examples\nensures to get more relevant and focused answers.\n\nWe would love to hear from how you use `skimage` for your work on the mailing-\nlist!\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "Getting started", "path": "user_guide/getting_started", "type": "Guide", "text": "\n`scikit-image` is an image processing Python package that works with `numpy`\narrays. The package is imported as `skimage`:\n\nMost functions of `skimage` are found within submodules:\n\nA list of submodules and functions is found on the API reference webpage.\n\nWithin scikit-image, images are represented as NumPy arrays, for example 2-D\narrays for grayscale 2-D images\n\nThe `skimage.data` submodule provides a set of functions returning example\nimages, that can be used to get started quickly on using scikit-image\u2019s\nfunctions:\n\nOf course, it is also possible to load your own images as NumPy arrays from\nimage files, using `skimage.io.imread()`:\n\nUse natsort to load multiple images\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "graph", "path": "api/skimage.graph", "type": "graph", "text": "\n`skimage.graph.route_through_array`(array, \u2026)\n\nSimple example of how to use the MCP and MCP_Geometric classes.\n\n`skimage.graph.shortest_path`(arr[, reach, \u2026])\n\nFind the shortest path through an n-d array from one side to another.\n\n`skimage.graph.MCP`(costs[, offsets, \u2026])\n\nA class for finding the minimum cost path through a given n-d costs array.\n\n`skimage.graph.MCP_Connect`(costs[, offsets, \u2026])\n\nConnect source points using the distance-weighted minimum cost function.\n\n`skimage.graph.MCP_Flexible`(costs[, offsets, \u2026])\n\nFind minimum cost paths through an N-d costs array.\n\n`skimage.graph.MCP_Geometric`(costs[, \u2026])\n\nFind distance-weighted minimum cost paths through an n-d costs array.\n\nSimple example of how to use the MCP and MCP_Geometric classes.\n\nSee the MCP and MCP_Geometric class documentation for explanation of the path-\nfinding algorithm.\n\nArray of costs.\n\nn-d index into `array` defining the starting point\n\nn-d index into `array` defining the end point\n\nIf True, diagonal moves are permitted, if False, only axial moves.\n\nIf True, the MCP_Geometric class is used to calculate costs, if False, the MCP\nbase class is used. See the class documentation for an explanation of the\ndifferences between MCP and MCP_Geometric.\n\nList of n-d index tuples defining the path from `start` to `end`.\n\nCost of the path. If `geometric` is False, the cost of the path is the sum of\nthe values of `array` along the path. If `geometric` is True, a finer\ncomputation is made (see the documentation of the MCP_Geometric class).\n\nSee also\n\nFind the shortest path through an n-d array from one side to another.\n\nBy default (`reach = 1`), the shortest path can only move one row up or down\nfor every step it moves forward (i.e., the path gradient is limited to 1).\n`reach` defines the number of elements that can be skipped along each non-axis\ndimension at each step.\n\nThe axis along which the path must always move forward (default -1)\n\nSee return value `p` for explanation.\n\nFor each step along `axis`, the coordinate of the shortest path. If\n`output_indexlist` is True, then the path is returned as a list of n-d tuples\nthat index into `arr`. If False, then the path is returned as an array listing\nthe coordinates of the path along the non-axis dimensions for each step along\nthe axis dimension. That is, `p.shape == (arr.shape[axis], arr.ndim-1)` except\nthat p is squeezed before returning so if `arr.ndim == 2`, then `p.shape ==\n(arr.shape[axis],)`\n\nCost of path. This is the absolute sum of all the differences along the path.\n\nBases: `object`\n\nA class for finding the minimum cost path through a given n-d costs array.\n\nGiven an n-d costs array, this class can be used to find the minimum-cost path\nthrough that array from any set of points to any other set of points. Basic\nusage is to initialize the class and call find_costs() with a one or more\nstarting indices (and an optional list of end indices). After that, call\ntraceback() one or more times to find the path from any given end-position to\nthe closest starting index. New paths through the same costs array can be\nfound by calling find_costs() repeatedly.\n\nThe cost of a path is calculated simply as the sum of the values of the\n`costs` array at each point on the path. The class MCP_Geometric, on the other\nhand, accounts for the fact that diagonal vs. axial moves are of different\nlengths, and weights the path cost accordingly.\n\nArray elements with infinite or negative costs will simply be ignored, as will\npaths whose cumulative cost overflows to infinite.\n\nA list of offset tuples: each offset specifies a valid move from a given n-d\nposition. If not provided, offsets corresponding to a singly- or fully-\nconnected n-d neighborhood will be constructed with make_offsets(), using the\n`fully_connected` parameter value.\n\nIf no `offsets` are provided, this determines the connectivity of the\ngenerated neighborhood. If true, the path may go along diagonals between\nelements of the `costs` array; otherwise only axial moves are permitted.\n\nFor each dimension, specifies the distance between two cells/voxels. If not\ngiven or None, the distance is assumed unit.\n\nEquivalent to the `offsets` provided to the constructor, or if none were so\nprovided, the offsets created for the requested n-d neighborhood. These are\nuseful for interpreting the `traceback` array returned by the find_costs()\nmethod.\n\nSee class documentation.\n\nFind the minimum-cost path from the given starting points.\n\nThis method finds the minimum-cost path to the specified ending indices from\nany one of the specified starting indices. If no end positions are given, then\nthe minimum-cost path to every position in the costs array will be found.\n\nA list of n-d starting indices (where n is the dimension of the `costs`\narray). The minimum cost path to the closest/cheapest starting point will be\nfound.\n\nA list of n-d ending indices.\n\nIf \u2018True\u2019 (default), the minimum-cost-path to every specified end-position\nwill be found; otherwise the algorithm will stop when a a path is found to any\nend-position. (If no `ends` were specified, then this parameter has no\neffect.)\n\nSame shape as the `costs` array; this array records the minimum cost path from\nthe nearest/cheapest starting index to each index considered. (If `ends` were\nspecified, not all elements in the array will necessarily be considered:\npositions not evaluated will have a cumulative cost of inf. If `find_all_ends`\nis \u2018False\u2019, only one of the specified end-positions will have a finite\ncumulative cost.)\n\nSame shape as the `costs` array; this array contains the offset to any given\nindex from its predecessor index. The offset indices index into the `offsets`\nattribute, which is a array of n-d offsets. In the 2-d case, if\noffsets[traceback[x, y]] is (-1, -1), that means that the predecessor of [x,\ny] in the minimum cost path to some start position is [x+1, y+1]. Note that if\nthe offset_index is -1, then the given index was not considered.\n\nint goal_reached(int index, float cumcost) This method is called each\niteration after popping an index from the heap, before examining the\nneighbours.\n\nThis method can be overloaded to modify the behavior of the MCP algorithm. An\nexample might be to stop the algorithm when a certain cumulative cost is\nreached, or when the front is a certain distance away from the seed point.\n\nThis method should return 1 if the algorithm should not check the current\npoint\u2019s neighbours and 2 if the algorithm is now done.\n\nTrace a minimum cost path through the pre-calculated traceback array.\n\nThis convenience function reconstructs the the minimum cost path to a given\nend position from one of the starting indices provided to find_costs(), which\nmust have been called previously. This function can be called as many times as\ndesired after find_costs() has been run.\n\nAn n-d index into the `costs` array.\n\nA list of indices into the `costs` array, starting with one of the start\npositions passed to find_costs(), and ending with the given `end` index. These\nindices specify the minimum-cost path from any given start index to the `end`\nindex. (The total cost of that path can be read out from the\n`cumulative_costs` array returned by find_costs().)\n\nBases: `skimage.graph._mcp.MCP`\n\nConnect source points using the distance-weighted minimum cost function.\n\nA front is grown from each seed point simultaneously, while the origin of the\nfront is tracked as well. When two fronts meet, create_connection() is called.\nThis method must be overloaded to deal with the found edges in a way that is\nappropriate for the application.\n\nInitialize self. See help(type(self)) for accurate signature.\n\ncreate_connection id1, id2, pos1, pos2, cost1, cost2)\n\nOverload this method to keep track of the connections that are found during\nMCP processing. Note that a connection with the same ids can be found multiple\ntimes (but with different positions and costs).\n\nAt the time that this method is called, both points are \u201cfrozen\u201d and will not\nbe visited again by the MCP algorithm.\n\nThe seed point id where the first neighbor originated from.\n\nThe seed point id where the second neighbor originated from.\n\nThe index of of the first neighbour in the connection.\n\nThe index of of the second neighbour in the connection.\n\nThe cumulative cost at `pos1`.\n\nThe cumulative costs at `pos2`.\n\nBases: `skimage.graph._mcp.MCP`\n\nFind minimum cost paths through an N-d costs array.\n\nSee the documentation for MCP for full details. This class differs from MCP in\nthat several methods can be overloaded (from pure Python) to modify the\nbehavior of the algorithm and/or create custom algorithms based on MCP. Note\nthat goal_reached can also be overloaded in the MCP class.\n\nSee class documentation.\n\nThis method is called once for every pair of neighboring nodes, as soon as\nboth nodes are frozen.\n\nThis method can be overloaded to obtain information about neightboring nodes,\nand/or to modify the behavior of the MCP algorithm. One example is the\nMCP_Connect class, which checks for meeting fronts using this hook.\n\nThis method calculates the travel cost for going from the current node to the\nnext. The default implementation returns new_cost. Overload this method to\nadapt the behaviour of the algorithm.\n\nThis method is called when a node is updated, right after new_index is pushed\nonto the heap and the traceback map is updated.\n\nThis method can be overloaded to keep track of other arrays that are used by a\nspecific implementation of the algorithm. For instance the MCP_Connect class\nuses it to update an id map.\n\nBases: `skimage.graph._mcp.MCP`\n\nFind distance-weighted minimum cost paths through an n-d costs array.\n\nSee the documentation for MCP for full details. This class differs from MCP in\nthat the cost of a path is not simply the sum of the costs along that path.\n\nThis class instead assumes that the costs array contains at each position the\n\u201ccost\u201d of a unit distance of travel through that position. For example, a move\n(in 2-d) from (1, 1) to (1, 2) is assumed to originate in the center of the\npixel (1, 1) and terminate in the center of (1, 2). The entire move is of\ndistance 1, half through (1, 1) and half through (1, 2); thus the cost of that\nmove is `(1/2)*costs[1,1] + (1/2)*costs[1,2]`.\n\nOn the other hand, a move from (1, 1) to (2, 2) is along the diagonal and is\nsqrt(2) in length. Half of this move is within the pixel (1, 1) and the other\nhalf in (2, 2), so the cost of this move is calculated as\n`(sqrt(2)/2)*costs[1,1] + (sqrt(2)/2)*costs[2,2]`.\n\nThese calculations don\u2019t make a lot of sense with offsets of magnitude greater\nthan 1. Use the `sampling` argument in order to deal with anisotropic data.\n\nSee class documentation.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "graph.MCP", "path": "api/skimage.graph#skimage.graph.MCP", "type": "graph", "text": "\nBases: `object`\n\nA class for finding the minimum cost path through a given n-d costs array.\n\nGiven an n-d costs array, this class can be used to find the minimum-cost path\nthrough that array from any set of points to any other set of points. Basic\nusage is to initialize the class and call find_costs() with a one or more\nstarting indices (and an optional list of end indices). After that, call\ntraceback() one or more times to find the path from any given end-position to\nthe closest starting index. New paths through the same costs array can be\nfound by calling find_costs() repeatedly.\n\nThe cost of a path is calculated simply as the sum of the values of the\n`costs` array at each point on the path. The class MCP_Geometric, on the other\nhand, accounts for the fact that diagonal vs. axial moves are of different\nlengths, and weights the path cost accordingly.\n\nArray elements with infinite or negative costs will simply be ignored, as will\npaths whose cumulative cost overflows to infinite.\n\nA list of offset tuples: each offset specifies a valid move from a given n-d\nposition. If not provided, offsets corresponding to a singly- or fully-\nconnected n-d neighborhood will be constructed with make_offsets(), using the\n`fully_connected` parameter value.\n\nIf no `offsets` are provided, this determines the connectivity of the\ngenerated neighborhood. If true, the path may go along diagonals between\nelements of the `costs` array; otherwise only axial moves are permitted.\n\nFor each dimension, specifies the distance between two cells/voxels. If not\ngiven or None, the distance is assumed unit.\n\nEquivalent to the `offsets` provided to the constructor, or if none were so\nprovided, the offsets created for the requested n-d neighborhood. These are\nuseful for interpreting the `traceback` array returned by the find_costs()\nmethod.\n\nSee class documentation.\n\nFind the minimum-cost path from the given starting points.\n\nThis method finds the minimum-cost path to the specified ending indices from\nany one of the specified starting indices. If no end positions are given, then\nthe minimum-cost path to every position in the costs array will be found.\n\nA list of n-d starting indices (where n is the dimension of the `costs`\narray). The minimum cost path to the closest/cheapest starting point will be\nfound.\n\nA list of n-d ending indices.\n\nIf \u2018True\u2019 (default), the minimum-cost-path to every specified end-position\nwill be found; otherwise the algorithm will stop when a a path is found to any\nend-position. (If no `ends` were specified, then this parameter has no\neffect.)\n\nSame shape as the `costs` array; this array records the minimum cost path from\nthe nearest/cheapest starting index to each index considered. (If `ends` were\nspecified, not all elements in the array will necessarily be considered:\npositions not evaluated will have a cumulative cost of inf. If `find_all_ends`\nis \u2018False\u2019, only one of the specified end-positions will have a finite\ncumulative cost.)\n\nSame shape as the `costs` array; this array contains the offset to any given\nindex from its predecessor index. The offset indices index into the `offsets`\nattribute, which is a array of n-d offsets. In the 2-d case, if\noffsets[traceback[x, y]] is (-1, -1), that means that the predecessor of [x,\ny] in the minimum cost path to some start position is [x+1, y+1]. Note that if\nthe offset_index is -1, then the given index was not considered.\n\nint goal_reached(int index, float cumcost) This method is called each\niteration after popping an index from the heap, before examining the\nneighbours.\n\nThis method can be overloaded to modify the behavior of the MCP algorithm. An\nexample might be to stop the algorithm when a certain cumulative cost is\nreached, or when the front is a certain distance away from the seed point.\n\nThis method should return 1 if the algorithm should not check the current\npoint\u2019s neighbours and 2 if the algorithm is now done.\n\nTrace a minimum cost path through the pre-calculated traceback array.\n\nThis convenience function reconstructs the the minimum cost path to a given\nend position from one of the starting indices provided to find_costs(), which\nmust have been called previously. This function can be called as many times as\ndesired after find_costs() has been run.\n\nAn n-d index into the `costs` array.\n\nA list of indices into the `costs` array, starting with one of the start\npositions passed to find_costs(), and ending with the given `end` index. These\nindices specify the minimum-cost path from any given start index to the `end`\nindex. (The total cost of that path can be read out from the\n`cumulative_costs` array returned by find_costs().)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "graph.MCP.find_costs()", "path": "api/skimage.graph#skimage.graph.MCP.find_costs", "type": "graph", "text": "\nFind the minimum-cost path from the given starting points.\n\nThis method finds the minimum-cost path to the specified ending indices from\nany one of the specified starting indices. If no end positions are given, then\nthe minimum-cost path to every position in the costs array will be found.\n\nA list of n-d starting indices (where n is the dimension of the `costs`\narray). The minimum cost path to the closest/cheapest starting point will be\nfound.\n\nA list of n-d ending indices.\n\nIf \u2018True\u2019 (default), the minimum-cost-path to every specified end-position\nwill be found; otherwise the algorithm will stop when a a path is found to any\nend-position. (If no `ends` were specified, then this parameter has no\neffect.)\n\nSame shape as the `costs` array; this array records the minimum cost path from\nthe nearest/cheapest starting index to each index considered. (If `ends` were\nspecified, not all elements in the array will necessarily be considered:\npositions not evaluated will have a cumulative cost of inf. If `find_all_ends`\nis \u2018False\u2019, only one of the specified end-positions will have a finite\ncumulative cost.)\n\nSame shape as the `costs` array; this array contains the offset to any given\nindex from its predecessor index. The offset indices index into the `offsets`\nattribute, which is a array of n-d offsets. In the 2-d case, if\noffsets[traceback[x, y]] is (-1, -1), that means that the predecessor of [x,\ny] in the minimum cost path to some start position is [x+1, y+1]. Note that if\nthe offset_index is -1, then the given index was not considered.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "graph.MCP.goal_reached()", "path": "api/skimage.graph#skimage.graph.MCP.goal_reached", "type": "graph", "text": "\nint goal_reached(int index, float cumcost) This method is called each\niteration after popping an index from the heap, before examining the\nneighbours.\n\nThis method can be overloaded to modify the behavior of the MCP algorithm. An\nexample might be to stop the algorithm when a certain cumulative cost is\nreached, or when the front is a certain distance away from the seed point.\n\nThis method should return 1 if the algorithm should not check the current\npoint\u2019s neighbours and 2 if the algorithm is now done.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "graph.MCP.traceback()", "path": "api/skimage.graph#skimage.graph.MCP.traceback", "type": "graph", "text": "\nTrace a minimum cost path through the pre-calculated traceback array.\n\nThis convenience function reconstructs the the minimum cost path to a given\nend position from one of the starting indices provided to find_costs(), which\nmust have been called previously. This function can be called as many times as\ndesired after find_costs() has been run.\n\nAn n-d index into the `costs` array.\n\nA list of indices into the `costs` array, starting with one of the start\npositions passed to find_costs(), and ending with the given `end` index. These\nindices specify the minimum-cost path from any given start index to the `end`\nindex. (The total cost of that path can be read out from the\n`cumulative_costs` array returned by find_costs().)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "graph.MCP.__init__()", "path": "api/skimage.graph#skimage.graph.MCP.__init__", "type": "graph", "text": "\nSee class documentation.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "graph.MCP_Connect", "path": "api/skimage.graph#skimage.graph.MCP_Connect", "type": "graph", "text": "\nBases: `skimage.graph._mcp.MCP`\n\nConnect source points using the distance-weighted minimum cost function.\n\nA front is grown from each seed point simultaneously, while the origin of the\nfront is tracked as well. When two fronts meet, create_connection() is called.\nThis method must be overloaded to deal with the found edges in a way that is\nappropriate for the application.\n\nInitialize self. See help(type(self)) for accurate signature.\n\ncreate_connection id1, id2, pos1, pos2, cost1, cost2)\n\nOverload this method to keep track of the connections that are found during\nMCP processing. Note that a connection with the same ids can be found multiple\ntimes (but with different positions and costs).\n\nAt the time that this method is called, both points are \u201cfrozen\u201d and will not\nbe visited again by the MCP algorithm.\n\nThe seed point id where the first neighbor originated from.\n\nThe seed point id where the second neighbor originated from.\n\nThe index of of the first neighbour in the connection.\n\nThe index of of the second neighbour in the connection.\n\nThe cumulative cost at `pos1`.\n\nThe cumulative costs at `pos2`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "graph.MCP_Connect.create_connection()", "path": "api/skimage.graph#skimage.graph.MCP_Connect.create_connection", "type": "graph", "text": "\ncreate_connection id1, id2, pos1, pos2, cost1, cost2)\n\nOverload this method to keep track of the connections that are found during\nMCP processing. Note that a connection with the same ids can be found multiple\ntimes (but with different positions and costs).\n\nAt the time that this method is called, both points are \u201cfrozen\u201d and will not\nbe visited again by the MCP algorithm.\n\nThe seed point id where the first neighbor originated from.\n\nThe seed point id where the second neighbor originated from.\n\nThe index of of the first neighbour in the connection.\n\nThe index of of the second neighbour in the connection.\n\nThe cumulative cost at `pos1`.\n\nThe cumulative costs at `pos2`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "graph.MCP_Connect.__init__()", "path": "api/skimage.graph#skimage.graph.MCP_Connect.__init__", "type": "graph", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "graph.MCP_Flexible", "path": "api/skimage.graph#skimage.graph.MCP_Flexible", "type": "graph", "text": "\nBases: `skimage.graph._mcp.MCP`\n\nFind minimum cost paths through an N-d costs array.\n\nSee the documentation for MCP for full details. This class differs from MCP in\nthat several methods can be overloaded (from pure Python) to modify the\nbehavior of the algorithm and/or create custom algorithms based on MCP. Note\nthat goal_reached can also be overloaded in the MCP class.\n\nSee class documentation.\n\nThis method is called once for every pair of neighboring nodes, as soon as\nboth nodes are frozen.\n\nThis method can be overloaded to obtain information about neightboring nodes,\nand/or to modify the behavior of the MCP algorithm. One example is the\nMCP_Connect class, which checks for meeting fronts using this hook.\n\nThis method calculates the travel cost for going from the current node to the\nnext. The default implementation returns new_cost. Overload this method to\nadapt the behaviour of the algorithm.\n\nThis method is called when a node is updated, right after new_index is pushed\nonto the heap and the traceback map is updated.\n\nThis method can be overloaded to keep track of other arrays that are used by a\nspecific implementation of the algorithm. For instance the MCP_Connect class\nuses it to update an id map.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "graph.MCP_Flexible.examine_neighbor()", "path": "api/skimage.graph#skimage.graph.MCP_Flexible.examine_neighbor", "type": "graph", "text": "\nThis method is called once for every pair of neighboring nodes, as soon as\nboth nodes are frozen.\n\nThis method can be overloaded to obtain information about neightboring nodes,\nand/or to modify the behavior of the MCP algorithm. One example is the\nMCP_Connect class, which checks for meeting fronts using this hook.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "graph.MCP_Flexible.travel_cost()", "path": "api/skimage.graph#skimage.graph.MCP_Flexible.travel_cost", "type": "graph", "text": "\nThis method calculates the travel cost for going from the current node to the\nnext. The default implementation returns new_cost. Overload this method to\nadapt the behaviour of the algorithm.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "graph.MCP_Flexible.update_node()", "path": "api/skimage.graph#skimage.graph.MCP_Flexible.update_node", "type": "graph", "text": "\nThis method is called when a node is updated, right after new_index is pushed\nonto the heap and the traceback map is updated.\n\nThis method can be overloaded to keep track of other arrays that are used by a\nspecific implementation of the algorithm. For instance the MCP_Connect class\nuses it to update an id map.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "graph.MCP_Flexible.__init__()", "path": "api/skimage.graph#skimage.graph.MCP_Flexible.__init__", "type": "graph", "text": "\nSee class documentation.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "graph.MCP_Geometric", "path": "api/skimage.graph#skimage.graph.MCP_Geometric", "type": "graph", "text": "\nBases: `skimage.graph._mcp.MCP`\n\nFind distance-weighted minimum cost paths through an n-d costs array.\n\nSee the documentation for MCP for full details. This class differs from MCP in\nthat the cost of a path is not simply the sum of the costs along that path.\n\nThis class instead assumes that the costs array contains at each position the\n\u201ccost\u201d of a unit distance of travel through that position. For example, a move\n(in 2-d) from (1, 1) to (1, 2) is assumed to originate in the center of the\npixel (1, 1) and terminate in the center of (1, 2). The entire move is of\ndistance 1, half through (1, 1) and half through (1, 2); thus the cost of that\nmove is `(1/2)*costs[1,1] + (1/2)*costs[1,2]`.\n\nOn the other hand, a move from (1, 1) to (2, 2) is along the diagonal and is\nsqrt(2) in length. Half of this move is within the pixel (1, 1) and the other\nhalf in (2, 2), so the cost of this move is calculated as\n`(sqrt(2)/2)*costs[1,1] + (sqrt(2)/2)*costs[2,2]`.\n\nThese calculations don\u2019t make a lot of sense with offsets of magnitude greater\nthan 1. Use the `sampling` argument in order to deal with anisotropic data.\n\nSee class documentation.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "graph.MCP_Geometric.__init__()", "path": "api/skimage.graph#skimage.graph.MCP_Geometric.__init__", "type": "graph", "text": "\nSee class documentation.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "graph.route_through_array()", "path": "api/skimage.graph#skimage.graph.route_through_array", "type": "graph", "text": "\nSimple example of how to use the MCP and MCP_Geometric classes.\n\nSee the MCP and MCP_Geometric class documentation for explanation of the path-\nfinding algorithm.\n\nArray of costs.\n\nn-d index into `array` defining the starting point\n\nn-d index into `array` defining the end point\n\nIf True, diagonal moves are permitted, if False, only axial moves.\n\nIf True, the MCP_Geometric class is used to calculate costs, if False, the MCP\nbase class is used. See the class documentation for an explanation of the\ndifferences between MCP and MCP_Geometric.\n\nList of n-d index tuples defining the path from `start` to `end`.\n\nCost of the path. If `geometric` is False, the cost of the path is the sum of\nthe values of `array` along the path. If `geometric` is True, a finer\ncomputation is made (see the documentation of the MCP_Geometric class).\n\nSee also\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "graph.shortest_path()", "path": "api/skimage.graph#skimage.graph.shortest_path", "type": "graph", "text": "\nFind the shortest path through an n-d array from one side to another.\n\nBy default (`reach = 1`), the shortest path can only move one row up or down\nfor every step it moves forward (i.e., the path gradient is limited to 1).\n`reach` defines the number of elements that can be skipped along each non-axis\ndimension at each step.\n\nThe axis along which the path must always move forward (default -1)\n\nSee return value `p` for explanation.\n\nFor each step along `axis`, the coordinate of the shortest path. If\n`output_indexlist` is True, then the path is returned as a list of n-d tuples\nthat index into `arr`. If False, then the path is returned as an array listing\nthe coordinates of the path along the non-axis dimensions for each step along\nthe axis dimension. That is, `p.shape == (arr.shape[axis], arr.ndim-1)` except\nthat p is squeezed before returning so if `arr.ndim == 2`, then `p.shape ==\n(arr.shape[axis],)`\n\nCost of path. This is the absolute sum of all the differences along the path.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "Handling Video Files", "path": "user_guide/video", "type": "Guide", "text": "\nSometimes it is necessary to read a sequence of images from a standard video\nfile, such as .avi and .mov files.\n\nIn a scientific context, it is usually better to avoid these formats in favor\nof a simple directory of images or a multi-dimensional TIF. Video formats are\nmore difficult to read piecemeal, typically do not support random frame access\nor research-minded meta data, and use lossy compression if not carefully\nconfigured. But video files are in widespread use, and they are easy to share,\nso it is convenient to be equipped to read and write them when necessary.\n\nTools for reading video files vary in their ease of installation and use,\ntheir disk and memory usage, and their cross-platform compatibility. This is a\npractical guide.\n\nFor a one-off solution, the simplest, surest route is to convert the video to\na collection of sequentially-numbered image files, often called an image\nsequence. Then the images files can be read into an `ImageCollection` by\n`skimage.io.imread_collection`. Converting the video to frames can be done\neasily in ImageJ, a cross-platform, GUI-based program from the bio-imaging\ncommunity, or FFmpeg, a powerful command-line utility for manipulating video\nfiles.\n\nIn FFmpeg, the following command generates an image file from each frame in a\nvideo. The files are numbered with five digits, padded on the left with zeros.\n\nMore information is available in an FFmpeg tutorial on image sequences.\n\nGenerating an image sequence has disadvantages: they can be large and\nunwieldy, and generating them can take some time. It is generally preferable\nto work directly with the original video file. For a more direct solution, we\nneed to execute FFmpeg or LibAV from Python to read frames from the video.\nFFmpeg and LibAV are two large open-source projects that decode video from the\nsprawling variety of formats used in the wild. There are several ways to use\nthem from Python. Each, unfortunately, has some disadvantages.\n\nPyAV uses FFmpeg\u2019s (or LibAV\u2019s) libraries to read image data directly from the\nvideo file. It invokes them using Cython bindings, so it is very fast.\n\nPyAV\u2019s API reflects the way frames are stored in a video file.\n\nThe `Video` class in PIMS invokes PyAV and adds additional functionality to\nsolve a common problem in scientific applications, accessing a video by frame\nnumber. Video file formats are designed to be searched in an approximate way,\nby time, and they do not support an efficient means of seeking a specific\nframe number. PIMS adds this missing functionality by decoding (but not\nreading) the entire video at and producing an internal table of contents that\nsupports indexing by frame.\n\nMoviepy invokes FFmpeg through a subprocess, pipes the decoded video from\nFFmpeg into RAM, and reads it out. This approach is straightforward, but it\ncan be brittle, and it\u2019s not workable for large videos that exceed available\nRAM. It works on all platforms if FFmpeg is installed.\n\nSince it does not link to FFmpeg\u2019s underlying libraries, it is easier to\ninstall but about half as fast.\n\nImageio takes the same approach as MoviePy. It supports a wide range of other\nimage file formats as well.\n\nFinally, another solution is the VideoReader class in OpenCV, which has\nbindings to FFmpeg. If you need OpenCV for other reasons, then this may be the\nbest approach.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "How to parallelize loops", "path": "user_guide/tutorial_parallelization", "type": "Guide", "text": "\nIn image processing, we frequently apply the same algorithm on a large batch\nof images. In this paragraph, we propose to use joblib to parallelize loops.\nHere is an example of such repetitive tasks:\n\nTo call the function `task` on each element of the list `pics`, it is usual to\nwrite a for loop. To measure the execution time of this loop, you can use\nipython and measure the execution time with `%timeit`.\n\nAnother equivalent way to code this loop is to use a comprehension list which\nhas the same efficiency.\n\n`joblib` is a library providing an easy way to parallelize for loops once we\nhave a comprehension list. The number of jobs can be specified.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "I/O Plugin Infrastructure", "path": "user_guide/plugins", "type": "Guide", "text": "\nA plugin consists of two files, the source and the descriptor `.ini`. Let\u2019s\nsay we\u2019d like to provide a plugin for `imshow` using `matplotlib`. We\u2019ll call\nour plugin `mpl`:\n\nThe name of the `.py` and `.ini` files must correspond. Inside the `.ini`\nfile, we give the plugin meta-data:\n\nThe \u201cprovides\u201d-line lists all the functions provided by the plugin. Since our\nplugin provides `imshow`, we have to define it inside `mpl.py`:\n\nNote that, by default, `imshow` is non-blocking, so a special function\n`_app_show` must be provided to block the GUI. We can modify our plugin to\nprovide it as follows:\n\nAny plugin in the `_plugins` directory is automatically examined by\n`skimage.io` upon import. You may list all the plugins on your system:\n\nor only those already loaded:\n\nA plugin is loaded using the `use_plugin` command:\n\nor\n\nNote that, if more than one plugin provides certain functionality, the last\nplugin loaded is used.\n\nTo query a plugin\u2019s capabilities, use `plugin_info`:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "Image adjustment: transforming image content", "path": "user_guide/transforming_image_data", "type": "Guide", "text": "\nMost functions for manipulating color channels are found in the submodule\n`skimage.color`.\n\nColor images can be represented using different color spaces. One of the most\ncommon color spaces is the RGB space, where an image has red, green and blue\nchannels. However, other color models are widely used, such as the HSV color\nmodel, where hue, saturation and value are independent channels, or the CMYK\nmodel used for printing.\n\n`skimage.color` provides utility functions to convert images to and from\ndifferent color spaces. Integer-type arrays can be transformed to floating-\npoint type by the conversion operation:\n\nConverting an RGBA image to an RGB image by alpha blending it with a\nbackground is realized with `rgba2rgb()`\n\nConverting an RGB image to a grayscale image is realized with `rgb2gray()`\n\n`rgb2gray()` uses a non-uniform weighting of color channels, because of the\ndifferent sensitivity of the human eye to different colors. Therefore, such a\nweighting ensures luminance preservation from RGB to grayscale:\n\nConverting a grayscale image to RGB with `gray2rgb()` simply duplicates the\ngray values over the three color channels.\n\nAn inverted image is also called complementary image. For binary images, True\nvalues become False and conversely. For grayscale images, pixel values are\nreplaced by the difference of the maximum value of the data type and the\nactual value. For RGB images, the same operation is done for each channel.\nThis operation can be achieved with `skimage.util.invert()`:\n\n`label2rgb()` can be used to superimpose colors on a grayscale image using an\narray of labels to encode the regions to be represented with the same color.\n\nExamples:\n\nImage pixels can take values determined by the `dtype` of the image (see Image\ndata types and what they mean), such as 0 to 255 for `uint8` images or `[0,\n1]` for floating-point images. However, most images either have a narrower\nrange of values (because of poor contrast), or have most pixel values\nconcentrated in a subrange of the accessible values. `skimage.exposure`\nprovides functions that spread the intensity values over a larger range.\n\nA first class of methods compute a nonlinear function of the intensity, that\nis independent of the pixel values of a specific image. Such methods are often\nused for correcting a known non-linearity of sensors, or receptors such as the\nhuman eye. A well-known example is Gamma correction, implemented in\n`adjust_gamma()`.\n\nOther methods re-distribute pixel values according to the histogram of the\nimage. The histogram of pixel values is computed with\n`skimage.exposure.histogram()`:\n\n`histogram()` returns the number of pixels for each value bin, and the centers\nof the bins. The behavior of `histogram()` is therefore slightly different\nfrom the one of `numpy.histogram()`, which returns the boundaries of the bins.\n\nThe simplest contrast enhancement `rescale_intensity()` consists in stretching\npixel values to the whole allowed range, using a linear transformation:\n\nEven if an image uses the whole value range, sometimes there is very little\nweight at the ends of the value range. In such a case, clipping pixel values\nusing percentiles of the image improves the contrast (at the expense of some\nloss of information, because some pixels are saturated by this operation):\n\nThe function `equalize_hist()` maps the cumulative distribution function (cdf)\nof pixel values onto a linear cdf, ensuring that all parts of the value range\nare equally represented in the image. As a result, details are enhanced in\nlarge regions with poor contrast. As a further refinement, histogram\nequalization can be performed in subregions of the image with\n`equalize_adapthist()`, in order to correct for exposure gradients across the\nimage. See the example Histogram Equalization.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "Image data types and what they mean", "path": "user_guide/data_types", "type": "Guide", "text": "\nIn `skimage`, images are simply numpy arrays, which support a variety of data\ntypes 1, i.e. \u201cdtypes\u201d. To avoid distorting image intensities (see Rescaling\nintensity values), we assume that images use the following dtype ranges:\n\nData type\n\nRange\n\nuint8\n\n0 to 255\n\nuint16\n\n0 to 65535\n\nuint32\n\n0 to 232 \\- 1\n\nfloat\n\n-1 to 1 or 0 to 1\nint8\n\n-128 to 127\nint16\n\n-32768 to 32767\nint32\n\n-231 to 231 \\- 1\nNote that float images should be restricted to the range -1 to 1 even though\nthe data type itself can exceed this range; all integer dtypes, on the other\nhand, have pixel intensities that can span the entire data type range. With a\nfew exceptions, 64-bit (u)int images are not supported.\n\nFunctions in `skimage` are designed so that they accept any of these dtypes,\nbut, for efficiency, may return an image of a different dtype (see Output\ntypes). If you need a particular dtype, `skimage` provides utility functions\nthat convert dtypes and properly rescale image intensities (see Input types).\nYou should never use `astype` on an image, because it violates these\nassumptions about the dtype range:\n\nAlthough we aim to preserve the data range and type of input images, functions\nmay support only a subset of these data-types. In such a case, the input will\nbe converted to the required type (if possible), and a warning message printed\nto the log if a memory copy is needed. Type requirements should be noted in\nthe docstrings.\n\nThe following utility functions in the main package are available to\ndevelopers and users:\n\nFunction name\n\nDescription\n\nimg_as_float\n\nConvert to 64-bit floating point.\n\nimg_as_ubyte\n\nConvert to 8-bit uint.\n\nimg_as_uint\n\nConvert to 16-bit uint.\n\nimg_as_int\n\nConvert to 16-bit int.\n\nThese functions convert images to the desired dtype and properly rescale their\nvalues:\n\nBe careful! These conversions can result in a loss of precision, since 8 bits\ncannot hold the same amount of information as 64 bits:\n\nAdditionally, some functions take a `preserve_range` argument where a range\nconversion is convenient but not necessary. For example, interpolation in\n`transform.warp` requires an image of type float, which should have a range in\n[0, 1]. So, by default, input images will be rescaled to this range. However,\nin some cases, the image values represent physical measurements, such as\ntemperature or rainfall values, that the user does not want rescaled. With\n`preserve_range=True`, the original range of the data will be preserved, even\nthough the output is a float image. Users must then ensure this non-standard\nimage is properly processed by downstream functions, which may expect an image\nin [0, 1].\n\nThe output type of a function is determined by the function author and is\ndocumented for the benefit of the user. While this requires the user to\nexplicitly convert the output to whichever format is needed, it ensures that\nno unnecessary data copies take place.\n\nA user that requires a specific type of output (e.g., for display purposes),\nmay write:\n\nIt is possible that you may need to use an image created using `skimage` with\nOpenCV or vice versa. OpenCV image data can be accessed (without copying) in\nNumPy (and, thus, in scikit-image). OpenCV uses BGR (instead of scikit-image\u2019s\nRGB) for color images, and its dtype is uint8 by default (See Image data types\nand what they mean). BGR stands for Blue Green Red.\n\nThe color images in `skimage` and OpenCV have 3 dimensions: width, height and\ncolor. RGB and BGR use the same color space, except the order of colors is\nreversed.\n\nNote that in `scikit-image` we usually refer to `rows` and `columns` instead\nof width and height (see Coordinate conventions).\n\nThe following instruction effectively reverses the order of the colors,\nleaving the rows and columns unaffected.\n\nIf cv_image is an array of unsigned bytes, `skimage` will understand it by\ndefault. If you prefer working with floating point images, `img_as_float()`\ncan be used to convert the image:\n\nThe reverse can be achieved with `img_as_ubyte()`:\n\nThis dtype behavior allows you to string together any `skimage` function\nwithout worrying about the image dtype. On the other hand, if you want to use\na custom function that requires a particular dtype, you should call one of the\ndtype conversion functions (here, `func1` and `func2` are `skimage`\nfunctions):\n\nBetter yet, you can convert the image internally and use a simplified\nprocessing pipeline:\n\nWhen possible, functions should avoid blindly stretching image intensities\n(e.g. rescaling a float image so that the min and max intensities are 0 and\n1), since this can heavily distort an image. For example, if you\u2019re looking\nfor bright markers in dark images, there may be an image where no markers are\npresent; stretching its input intensity to span the full range would make\nbackground noise look like markers.\n\nSometimes, however, you have images that should span the entire intensity\nrange but do not. For example, some cameras store images with 10-, 12-, or\n14-bit depth per pixel. If these images are stored in an array with dtype\nuint16, then the image won\u2019t extend over the full intensity range, and thus,\nwould appear dimmer than it should. To correct for this, you can use the\n`rescale_intensity` function to rescale the image so that it uses the full\ndtype range:\n\nHere, the `in_range` argument is set to the maximum range for a 10-bit image.\nBy default, `rescale_intensity` stretches the values of `in_range` to match\nthe range of the dtype. `rescale_intensity` also accepts strings as inputs to\n`in_range` and `out_range`, so the example above could also be written as:\n\nPeople very often represent images in signed dtypes, even though they only\nmanipulate the positive values of the image (e.g., using only 0-127 in an int8\nimage). For this reason, conversion functions only spread the positive values\nof a signed dtype over the entire range of an unsigned dtype. In other words,\nnegative values are clipped to 0 when converting from signed to unsigned\ndtypes. (Negative values are preserved when converting between signed dtypes.)\nTo prevent this clipping behavior, you should rescale your image beforehand:\n\nThis behavior is symmetric: The values in an unsigned dtype are spread over\njust the positive range of a signed dtype.\n\nhttps://docs.scipy.org/doc/numpy/user/basics.types.html\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "Image Segmentation", "path": "user_guide/tutorial_segmentation", "type": "Guide", "text": "\nImage segmentation is the task of labeling the pixels of objects of interest\nin an image.\n\nIn this tutorial, we will see how to segment objects from a background. We use\nthe `coins` image from `skimage.data`. This image shows several coins outlined\nagainst a darker background. The segmentation of the coins cannot be done\ndirectly from the histogram of grey values, because the background shares\nenough grey levels with the coins that a thresholding segmentation is not\nsufficient.\n\nSimply thresholding the image leads either to missing significant parts of the\ncoins, or to merging parts of the background with the coins. This is due to\nthe inhomogeneous lighting of the image.\n\nA first idea is to take advantage of the local contrast, that is, to use the\ngradients rather than the grey values.\n\nLet us first try to detect edges that enclose the coins. For edge detection,\nwe use the Canny detector of `skimage.feature.canny`\n\nAs the background is very smooth, almost all edges are found at the boundary\nof the coins, or inside the coins.\n\nNow that we have contours that delineate the outer boundary of the coins, we\nfill the inner part of the coins using the `ndi.binary_fill_holes` function,\nwhich uses mathematical morphology to fill the holes.\n\nMost coins are well segmented out of the background. Small objects from the\nbackground can be easily removed using the `ndi.label` function to remove\nobjects smaller than a small threshold.\n\nHowever, the segmentation is not very satisfying, since one of the coins has\nnot been segmented correctly at all. The reason is that the contour that we\ngot from the Canny detector was not completely closed, therefore the filling\nfunction did not fill the inner part of the coin.\n\nTherefore, this segmentation method is not very robust: if we miss a single\npixel of the contour of the object, we will not be able to fill it. Of course,\nwe could try to dilate the contours in order to close them. However, it is\npreferable to try a more robust method.\n\nLet us first determine markers of the coins and the background. These markers\nare pixels that we can label unambiguously as either object or background.\nHere, the markers are found at the two extreme parts of the histogram of grey\nvalues:\n\nWe will use these markers in a watershed segmentation. The name watershed\ncomes from an analogy with hydrology. The watershed transform floods an image\nof elevation starting from markers, in order to determine the catchment basins\nof these markers. Watershed lines separate these catchment basins, and\ncorrespond to the desired segmentation.\n\nThe choice of the elevation map is critical for good segmentation. Here, the\namplitude of the gradient provides a good elevation map. We use the Sobel\noperator for computing the amplitude of the gradient:\n\nFrom the 3-D surface plot shown below, we see that high barriers effectively\nseparate the coins from the background.\n\nand here is the corresponding 2-D plot:\n\nThe next step is to find markers of the background and the coins based on the\nextreme parts of the histogram of grey values:\n\nLet us now compute the watershed transform:\n\nWith this method, the result is satisfying for all coins. Even if the markers\nfor the background were not well distributed, the barriers in the elevation\nmap were high enough for these markers to flood the entire background.\n\nWe remove a few small holes with mathematical morphology:\n\nWe can now label all the coins one by one using `ndi.label`:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "Image Viewer", "path": "user_guide/viewer", "type": "Guide", "text": "\nWarning\n\nThe scikit-image viewer is deprecated since 0.18 and will be removed in 0.20.\nPlease, refer to the visualization software page for alternatives.\n\n`skimage.viewer` provides a matplotlib-based canvas for displaying images and\na Qt-based GUI-toolkit, with the goal of making it easy to create interactive\nimage editors. You can simply use it to display an image:\n\nOf course, you could just as easily use `imshow` from matplotlib (or\nalternatively, `skimage.io.imshow` which adds support for multiple io-plugins)\nto display images. The advantage of `ImageViewer` is that you can easily add\nplugins for manipulating images. Currently, only a few plugins are\nimplemented, but it is easy to write your own. Before going into the details,\nlet\u2019s see an example of how a pre-defined plugin is added to the viewer:\n\nThe viewer\u2019s `show()` method returns a list of tuples, one for each attached\nplugin. Each tuple contains two elements: an overlay of the same shape as the\ninput image, and a data field (which may be `None`). A plugin class documents\nits return value in its `output` method.\n\nIn this example, only one plugin is attached, so the list returned by `show`\nwill have length 1. We extract the single tuple and bind its `overlay` and\n`data` elements to individual variables. Here, `overlay` contains an image of\nthe line drawn on the viewer, and `data` contains the 1-dimensional intensity\nprofile along that line.\n\nAt the moment, there are not many plugins pre-defined, but there is a really\nsimple interface for creating your own plugin. First, let us create a plugin\nto call the total-variation denoising function, `denoise_tv_bregman`:\n\nNote\n\nThe `Plugin` assumes the first argument given to the image filter is the image\nfrom the image viewer. In the future, this should be changed so you can pass\nthe image to a different argument of the filter function.\n\nTo actually interact with the filter, you have to add widgets that adjust the\nparameters of the function. Typically, that means adding a slider widget and\nconnecting it to the filter parameter and the minimum and maximum values of\nthe slider:\n\nHere, we connect a slider widget to the filter\u2019s \u2018weight\u2019 argument. We also\nadded some buttons for saving the image to file or to the `scikit-image` image\nstack (see `skimage.io.push` and `skimage.io.pop`).\n\nAll that\u2019s left is to create an image viewer and add the plugin to that\nviewer.\n\nHere, we access only the overlay returned by the plugin, which contains the\nfiltered image for the last used setting of `weight`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "img_as_bool()", "path": "api/skimage#skimage.img_as_bool", "type": "skimage", "text": "\nConvert an image to boolean format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nThe upper half of the input dtype\u2019s positive range is True, and the lower half\nis False. All negative values (if present) are False.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "img_as_float()", "path": "api/skimage#skimage.img_as_float", "type": "skimage", "text": "\nConvert an image to floating point format.\n\nThis function is similar to `img_as_float64`, but will not convert lower-\nprecision floating point arrays to `float64`.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nThe range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when\nconverting from unsigned or signed datatypes, respectively. If the input image\nhas a float type, intensity values are not modified and can be outside the\nranges [0.0, 1.0] or [-1.0, 1.0].\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "img_as_float32()", "path": "api/skimage#skimage.img_as_float32", "type": "skimage", "text": "\nConvert an image to single-precision (32-bit) floating point format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nThe range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when\nconverting from unsigned or signed datatypes, respectively. If the input image\nhas a float type, intensity values are not modified and can be outside the\nranges [0.0, 1.0] or [-1.0, 1.0].\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "img_as_float64()", "path": "api/skimage#skimage.img_as_float64", "type": "skimage", "text": "\nConvert an image to double-precision (64-bit) floating point format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nThe range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when\nconverting from unsigned or signed datatypes, respectively. If the input image\nhas a float type, intensity values are not modified and can be outside the\nranges [0.0, 1.0] or [-1.0, 1.0].\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "img_as_int()", "path": "api/skimage#skimage.img_as_int", "type": "skimage", "text": "\nConvert an image to 16-bit signed integer format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nThe values are scaled between -32768 and 32767. If the input data-type is\npositive-only (e.g., uint8), then the output image will still only have\npositive values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "img_as_ubyte()", "path": "api/skimage#skimage.img_as_ubyte", "type": "skimage", "text": "\nConvert an image to 8-bit unsigned integer format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nNegative input values will be clipped. Positive values are scaled between 0\nand 255.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "img_as_uint()", "path": "api/skimage#skimage.img_as_uint", "type": "skimage", "text": "\nConvert an image to 16-bit unsigned integer format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nNegative input values will be clipped. Positive values are scaled between 0\nand 65535.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io", "path": "api/skimage.io", "type": "io", "text": "\nUtilities to read and write images in various formats.\n\nThe following plug-ins are available:\n\nPlugin\n\nDescription\n\nqt\n\nFast image display using the Qt library. Deprecated since 0.18. Will be\nremoved in 0.20.\n\nimread\n\nImage reading and writing via imread\n\ngdal\n\nImage reading via the GDAL Library (www.gdal.org)\n\nsimpleitk\n\nImage reading and writing via SimpleITK\n\ngtk\n\nFast image display using the GTK library\n\npil\n\nImage reading via the Python Imaging Library\n\nfits\n\nFITS image reading via PyFITS\n\nmatplotlib\n\nDisplay or save images using Matplotlib\n\ntifffile\n\nLoad and save TIFF and TIFF-based images using tifffile.py\n\nimageio\n\nImage reading via the ImageIO Library\n\n`skimage.io.call_plugin`(kind, *args, **kwargs)\n\nFind the appropriate plugin of \u2018kind\u2019 and execute it.\n\n`skimage.io.concatenate_images`(ic)\n\nConcatenate all images in the image collection into an array.\n\n`skimage.io.find_available_plugins`([loaded])\n\nList available plugins.\n\n`skimage.io.imread`(fname[, as_gray, plugin])\n\nLoad an image from file.\n\n`skimage.io.imread_collection`(load_pattern[, \u2026])\n\nLoad a collection of images.\n\n`skimage.io.imread_collection_wrapper`(imread)\n\n`skimage.io.imsave`(fname, arr[, plugin, \u2026])\n\nSave an image to file.\n\n`skimage.io.imshow`(arr[, plugin])\n\nDisplay an image.\n\n`skimage.io.imshow_collection`(ic[, plugin])\n\nDisplay a collection of images.\n\n`skimage.io.load_sift`(f)\n\nRead SIFT or SURF features from externally generated file.\n\n`skimage.io.load_surf`(f)\n\nRead SIFT or SURF features from externally generated file.\n\n`skimage.io.plugin_info`(plugin)\n\nReturn plugin meta-data.\n\n`skimage.io.plugin_order`()\n\nReturn the currently preferred plugin order.\n\n`skimage.io.pop`()\n\nPop an image from the shared image stack.\n\n`skimage.io.push`(img)\n\nPush an image onto the shared image stack.\n\n`skimage.io.reset_plugins`()\n\n`skimage.io.show`()\n\nDisplay pending images.\n\n`skimage.io.use_plugin`(name[, kind])\n\nSet the default plugin for a specified operation.\n\n`skimage.io.ImageCollection`(load_pattern[, \u2026])\n\nLoad and manage a collection of image files.\n\n`skimage.io.MultiImage`(filename[, \u2026])\n\nA class containing all frames from multi-frame images.\n\n`skimage.io.collection`\n\nData structures to hold collections of images, with optional caching.\n\n`skimage.io.manage_plugins`\n\nHandle image reading, writing and plotting plugins.\n\n`skimage.io.sift`\n\n`skimage.io.util`\n\nFind the appropriate plugin of \u2018kind\u2019 and execute it.\n\nFunction to look up.\n\nPlugin to load. Defaults to None, in which case the first matching plugin is\nused.\n\nPassed to the plugin function.\n\nConcatenate all images in the image collection into an array.\n\nThe images to be concatenated.\n\nAn array having one more dimension than the images in `ic`.\n\nIf images in `ic` don\u2019t have identical shapes.\n\nSee also\n\n`concatenate_images` receives any iterable object containing images, including\nImageCollection and MultiImage, and returns a NumPy array.\n\nList available plugins.\n\nIf True, show only those plugins currently loaded. By default, all plugins are\nshown.\n\nDictionary with plugin names as keys and exposed functions as values.\n\nLoad an image from file.\n\nImage file name, e.g. `test.jpg` or URL.\n\nIf True, convert color images to gray-scale (64-bit floats). Images that are\nalready in gray-scale format are not converted.\n\nName of plugin to use. By default, the different plugins are tried (starting\nwith imageio) until a suitable candidate is found. If not given and fname is a\ntiff file, the tifffile plugin will be used.\n\nThe different color bands/channels are stored in the third dimension, such\nthat a gray-image is MxN, an RGB-image MxNx3 and an RGBA-image MxNx4.\n\nPassed to the given plugin.\n\nLoad a collection of images.\n\nList of objects to load. These are usually filenames, but may vary depending\non the currently active plugin. See the docstring for `ImageCollection` for\nthe default behaviour of this parameter.\n\nIf True, never keep more than one in memory at a specific time. Otherwise,\nimages will be cached once they are loaded.\n\nCollection of images.\n\nPassed to the given plugin.\n\nSave an image to file.\n\nTarget filename.\n\nImage data.\n\nName of plugin to use. By default, the different plugins are tried (starting\nwith imageio) until a suitable candidate is found. If not given and fname is a\ntiff file, the tifffile plugin will be used.\n\nCheck for low contrast and print warning (default: True).\n\nPassed to the given plugin.\n\nWhen saving a JPEG, the compression ratio may be controlled using the\n`quality` keyword argument which is an integer with values in [1, 100] where 1\nis worst quality and smallest file size, and 100 is best quality and largest\nfile size (default 75). This is only available when using the PIL and imageio\nplugins.\n\nDisplay an image.\n\nImage data or name of image file.\n\nName of plugin to use. By default, the different plugins are tried (starting\nwith imageio) until a suitable candidate is found.\n\nPassed to the given plugin.\n\nExplore 3D images (of cells)\n\nDisplay a collection of images.\n\nCollection to display.\n\nName of plugin to use. By default, the different plugins are tried until a\nsuitable candidate is found.\n\nPassed to the given plugin.\n\nRead SIFT or SURF features from externally generated file.\n\nThis routine reads SIFT or SURF files generated by binary utilities from\nhttp://people.cs.ubc.ca/~lowe/keypoints/ and\nhttp://www.vision.ee.ethz.ch/~surf/.\n\nThis routine does not generate SIFT/SURF features from an image. These\nalgorithms are patent encumbered. Please use `skimage.feature.CENSURE`\ninstead.\n\nInput file generated by the feature detectors from\nhttp://people.cs.ubc.ca/~lowe/keypoints/ or\nhttp://www.vision.ee.ethz.ch/~surf/ .\n\nKind of descriptor used to generate `filelike`.\n\nrow position of feature\n\ncolumn position of feature\n\nfeature scale\n\nfeature orientation\n\nfeature values\n\nRead SIFT or SURF features from externally generated file.\n\nThis routine reads SIFT or SURF files generated by binary utilities from\nhttp://people.cs.ubc.ca/~lowe/keypoints/ and\nhttp://www.vision.ee.ethz.ch/~surf/.\n\nThis routine does not generate SIFT/SURF features from an image. These\nalgorithms are patent encumbered. Please use `skimage.feature.CENSURE`\ninstead.\n\nInput file generated by the feature detectors from\nhttp://people.cs.ubc.ca/~lowe/keypoints/ or\nhttp://www.vision.ee.ethz.ch/~surf/ .\n\nKind of descriptor used to generate `filelike`.\n\nrow position of feature\n\ncolumn position of feature\n\nfeature scale\n\nfeature orientation\n\nfeature values\n\nReturn plugin meta-data.\n\nName of plugin.\n\nMeta data as specified in plugin `.ini`.\n\nReturn the currently preferred plugin order.\n\nDictionary of preferred plugin order, with function name as key and plugins\n(in order of preference) as value.\n\nPop an image from the shared image stack.\n\nImage popped from the stack.\n\nPush an image onto the shared image stack.\n\nImage to push.\n\nDisplay pending images.\n\nLaunch the event loop of the current gui plugin, and display all pending\nimages, queued via `imshow`. This is required when using `imshow` from non-\ninteractive scripts.\n\nA call to `show` will block execution of code until all windows have been\nclosed.\n\nSet the default plugin for a specified operation. The plugin will be loaded if\nit hasn\u2019t been already.\n\nName of plugin.\n\nSet the plugin for this function. By default, the plugin is set for all\nfunctions.\n\nSee also\n\nList of available plugins\n\nTo use Matplotlib as the default image reader, you would write:\n\nTo see a list of available plugins run `io.available_plugins`. Note that this\nlists plugins that are defined, but the full list may not be usable if your\nsystem does not have the required libraries installed.\n\nBases: `object`\n\nLoad and manage a collection of image files.\n\nPattern string or list of strings to load. The filename path can be absolute\nor relative.\n\nIf True, `ImageCollection` does not keep more than one in memory at a specific\ntime. Otherwise, images will be cached once they are loaded.\n\n`imread` by default. See notes below.\n\nNote that files are always returned in alphanumerical order. Also note that\nslicing returns a new ImageCollection, not a view into the data.\n\nImageCollection can be modified to load images from an arbitrary source by\nspecifying a combination of `load_pattern` and `load_func`. For an\nImageCollection `ic`, `ic[5]` uses `load_func(load_pattern[5])` to load the\nimage.\n\nImagine, for example, an ImageCollection that loads every third frame from a\nvideo file:\n\nAnother use of `load_func` would be to convert all images to `uint8`:\n\nIf a pattern string is given for `load_pattern`, this attribute stores the\nexpanded file list. Otherwise, this is equal to `load_pattern`.\n\nLoad and manage a collection of images.\n\nConcatenate all images in the collection into an array.\n\nAn array having one more dimension than the images in `self`.\n\nIf images in the `ImageCollection` don\u2019t have identical shapes.\n\nSee also\n\nClear the image cache.\n\nClear the cache for this image only. By default, the entire cache is erased.\n\nBases: `skimage.io.collection.ImageCollection`\n\nA class containing all frames from multi-frame images.\n\nPattern glob or filenames to load. The path can be absolute or relative.\n\nWhether to conserve memory by only caching a single frame. Default is True.\n\n`imread` by default. See notes below.\n\nIf `conserve_memory=True` the memory footprint can be reduced, however the\nperformance can be affected because frames have to be read from file more\noften.\n\nThe last accessed frame is cached, all other frames will have to be read from\nfile.\n\nThe current implementation makes use of `tifffile` for Tiff files and PIL\notherwise.\n\nLoad a multi-img.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.call_plugin()", "path": "api/skimage.io#skimage.io.call_plugin", "type": "io", "text": "\nFind the appropriate plugin of \u2018kind\u2019 and execute it.\n\nFunction to look up.\n\nPlugin to load. Defaults to None, in which case the first matching plugin is\nused.\n\nPassed to the plugin function.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.concatenate_images()", "path": "api/skimage.io#skimage.io.concatenate_images", "type": "io", "text": "\nConcatenate all images in the image collection into an array.\n\nThe images to be concatenated.\n\nAn array having one more dimension than the images in `ic`.\n\nIf images in `ic` don\u2019t have identical shapes.\n\nSee also\n\n`concatenate_images` receives any iterable object containing images, including\nImageCollection and MultiImage, and returns a NumPy array.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.find_available_plugins()", "path": "api/skimage.io#skimage.io.find_available_plugins", "type": "io", "text": "\nList available plugins.\n\nIf True, show only those plugins currently loaded. By default, all plugins are\nshown.\n\nDictionary with plugin names as keys and exposed functions as values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.ImageCollection", "path": "api/skimage.io#skimage.io.ImageCollection", "type": "io", "text": "\nBases: `object`\n\nLoad and manage a collection of image files.\n\nPattern string or list of strings to load. The filename path can be absolute\nor relative.\n\nIf True, `ImageCollection` does not keep more than one in memory at a specific\ntime. Otherwise, images will be cached once they are loaded.\n\n`imread` by default. See notes below.\n\nNote that files are always returned in alphanumerical order. Also note that\nslicing returns a new ImageCollection, not a view into the data.\n\nImageCollection can be modified to load images from an arbitrary source by\nspecifying a combination of `load_pattern` and `load_func`. For an\nImageCollection `ic`, `ic[5]` uses `load_func(load_pattern[5])` to load the\nimage.\n\nImagine, for example, an ImageCollection that loads every third frame from a\nvideo file:\n\nAnother use of `load_func` would be to convert all images to `uint8`:\n\nIf a pattern string is given for `load_pattern`, this attribute stores the\nexpanded file list. Otherwise, this is equal to `load_pattern`.\n\nLoad and manage a collection of images.\n\nConcatenate all images in the collection into an array.\n\nAn array having one more dimension than the images in `self`.\n\nIf images in the `ImageCollection` don\u2019t have identical shapes.\n\nSee also\n\nClear the image cache.\n\nClear the cache for this image only. By default, the entire cache is erased.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.ImageCollection.concatenate()", "path": "api/skimage.io#skimage.io.ImageCollection.concatenate", "type": "io", "text": "\nConcatenate all images in the collection into an array.\n\nAn array having one more dimension than the images in `self`.\n\nIf images in the `ImageCollection` don\u2019t have identical shapes.\n\nSee also\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.ImageCollection.conserve_memory()", "path": "api/skimage.io#skimage.io.ImageCollection.conserve_memory", "type": "io", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.ImageCollection.files()", "path": "api/skimage.io#skimage.io.ImageCollection.files", "type": "io", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.ImageCollection.reload()", "path": "api/skimage.io#skimage.io.ImageCollection.reload", "type": "io", "text": "\nClear the image cache.\n\nClear the cache for this image only. By default, the entire cache is erased.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.ImageCollection.__init__()", "path": "api/skimage.io#skimage.io.ImageCollection.__init__", "type": "io", "text": "\nLoad and manage a collection of images.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.imread()", "path": "api/skimage.io#skimage.io.imread", "type": "io", "text": "\nLoad an image from file.\n\nImage file name, e.g. `test.jpg` or URL.\n\nIf True, convert color images to gray-scale (64-bit floats). Images that are\nalready in gray-scale format are not converted.\n\nName of plugin to use. By default, the different plugins are tried (starting\nwith imageio) until a suitable candidate is found. If not given and fname is a\ntiff file, the tifffile plugin will be used.\n\nThe different color bands/channels are stored in the third dimension, such\nthat a gray-image is MxN, an RGB-image MxNx3 and an RGBA-image MxNx4.\n\nPassed to the given plugin.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.imread_collection()", "path": "api/skimage.io#skimage.io.imread_collection", "type": "io", "text": "\nLoad a collection of images.\n\nList of objects to load. These are usually filenames, but may vary depending\non the currently active plugin. See the docstring for `ImageCollection` for\nthe default behaviour of this parameter.\n\nIf True, never keep more than one in memory at a specific time. Otherwise,\nimages will be cached once they are loaded.\n\nCollection of images.\n\nPassed to the given plugin.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.imread_collection_wrapper()", "path": "api/skimage.io#skimage.io.imread_collection_wrapper", "type": "io", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.imsave()", "path": "api/skimage.io#skimage.io.imsave", "type": "io", "text": "\nSave an image to file.\n\nTarget filename.\n\nImage data.\n\nName of plugin to use. By default, the different plugins are tried (starting\nwith imageio) until a suitable candidate is found. If not given and fname is a\ntiff file, the tifffile plugin will be used.\n\nCheck for low contrast and print warning (default: True).\n\nPassed to the given plugin.\n\nWhen saving a JPEG, the compression ratio may be controlled using the\n`quality` keyword argument which is an integer with values in [1, 100] where 1\nis worst quality and smallest file size, and 100 is best quality and largest\nfile size (default 75). This is only available when using the PIL and imageio\nplugins.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.imshow()", "path": "api/skimage.io#skimage.io.imshow", "type": "io", "text": "\nDisplay an image.\n\nImage data or name of image file.\n\nName of plugin to use. By default, the different plugins are tried (starting\nwith imageio) until a suitable candidate is found.\n\nPassed to the given plugin.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.imshow_collection()", "path": "api/skimage.io#skimage.io.imshow_collection", "type": "io", "text": "\nDisplay a collection of images.\n\nCollection to display.\n\nName of plugin to use. By default, the different plugins are tried until a\nsuitable candidate is found.\n\nPassed to the given plugin.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.load_sift()", "path": "api/skimage.io#skimage.io.load_sift", "type": "io", "text": "\nRead SIFT or SURF features from externally generated file.\n\nThis routine reads SIFT or SURF files generated by binary utilities from\nhttp://people.cs.ubc.ca/~lowe/keypoints/ and\nhttp://www.vision.ee.ethz.ch/~surf/.\n\nThis routine does not generate SIFT/SURF features from an image. These\nalgorithms are patent encumbered. Please use `skimage.feature.CENSURE`\ninstead.\n\nInput file generated by the feature detectors from\nhttp://people.cs.ubc.ca/~lowe/keypoints/ or\nhttp://www.vision.ee.ethz.ch/~surf/ .\n\nKind of descriptor used to generate `filelike`.\n\nrow position of feature\n\ncolumn position of feature\n\nfeature scale\n\nfeature orientation\n\nfeature values\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.load_surf()", "path": "api/skimage.io#skimage.io.load_surf", "type": "io", "text": "\nRead SIFT or SURF features from externally generated file.\n\nThis routine reads SIFT or SURF files generated by binary utilities from\nhttp://people.cs.ubc.ca/~lowe/keypoints/ and\nhttp://www.vision.ee.ethz.ch/~surf/.\n\nThis routine does not generate SIFT/SURF features from an image. These\nalgorithms are patent encumbered. Please use `skimage.feature.CENSURE`\ninstead.\n\nInput file generated by the feature detectors from\nhttp://people.cs.ubc.ca/~lowe/keypoints/ or\nhttp://www.vision.ee.ethz.ch/~surf/ .\n\nKind of descriptor used to generate `filelike`.\n\nrow position of feature\n\ncolumn position of feature\n\nfeature scale\n\nfeature orientation\n\nfeature values\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.MultiImage", "path": "api/skimage.io#skimage.io.MultiImage", "type": "io", "text": "\nBases: `skimage.io.collection.ImageCollection`\n\nA class containing all frames from multi-frame images.\n\nPattern glob or filenames to load. The path can be absolute or relative.\n\nWhether to conserve memory by only caching a single frame. Default is True.\n\n`imread` by default. See notes below.\n\nIf `conserve_memory=True` the memory footprint can be reduced, however the\nperformance can be affected because frames have to be read from file more\noften.\n\nThe last accessed frame is cached, all other frames will have to be read from\nfile.\n\nThe current implementation makes use of `tifffile` for Tiff files and PIL\notherwise.\n\nLoad a multi-img.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.MultiImage.filename()", "path": "api/skimage.io#skimage.io.MultiImage.filename", "type": "io", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.MultiImage.__init__()", "path": "api/skimage.io#skimage.io.MultiImage.__init__", "type": "io", "text": "\nLoad a multi-img.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.plugin_info()", "path": "api/skimage.io#skimage.io.plugin_info", "type": "io", "text": "\nReturn plugin meta-data.\n\nName of plugin.\n\nMeta data as specified in plugin `.ini`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.plugin_order()", "path": "api/skimage.io#skimage.io.plugin_order", "type": "io", "text": "\nReturn the currently preferred plugin order.\n\nDictionary of preferred plugin order, with function name as key and plugins\n(in order of preference) as value.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.pop()", "path": "api/skimage.io#skimage.io.pop", "type": "io", "text": "\nPop an image from the shared image stack.\n\nImage popped from the stack.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.push()", "path": "api/skimage.io#skimage.io.push", "type": "io", "text": "\nPush an image onto the shared image stack.\n\nImage to push.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.reset_plugins()", "path": "api/skimage.io#skimage.io.reset_plugins", "type": "io", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.show()", "path": "api/skimage.io#skimage.io.show", "type": "io", "text": "\nDisplay pending images.\n\nLaunch the event loop of the current gui plugin, and display all pending\nimages, queued via `imshow`. This is required when using `imshow` from non-\ninteractive scripts.\n\nA call to `show` will block execution of code until all windows have been\nclosed.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "io.use_plugin()", "path": "api/skimage.io#skimage.io.use_plugin", "type": "io", "text": "\nSet the default plugin for a specified operation. The plugin will be loaded if\nit hasn\u2019t been already.\n\nName of plugin.\n\nSet the plugin for this function. By default, the plugin is set for all\nfunctions.\n\nSee also\n\nList of available plugins\n\nTo use Matplotlib as the default image reader, you would write:\n\nTo see a list of available plugins run `io.available_plugins`. Note that this\nlists plugins that are defined, but the full list may not be usable if your\nsystem does not have the required libraries installed.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "lookfor()", "path": "api/skimage#skimage.lookfor", "type": "skimage", "text": "\nDo a keyword search on scikit-image docstrings.\n\nWords to look for.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure", "path": "api/skimage.measure", "type": "measure", "text": "\n`skimage.measure.approximate_polygon`(coords, \u2026)\n\nApproximate a polygonal chain with the specified tolerance.\n\n`skimage.measure.block_reduce`(image, block_size)\n\nDownsample image by applying function `func` to local blocks.\n\n`skimage.measure.euler_number`(image[, \u2026])\n\nCalculate the Euler characteristic in binary image.\n\n`skimage.measure.find_contours`(image[, \u2026])\n\nFind iso-valued contours in a 2D array for a given level value.\n\n`skimage.measure.grid_points_in_poly`(shape, verts)\n\nTest whether points on a specified grid are inside a polygon.\n\n`skimage.measure.inertia_tensor`(image[, mu])\n\nCompute the inertia tensor of the input image.\n\n`skimage.measure.inertia_tensor_eigvals`(image)\n\nCompute the eigenvalues of the inertia tensor of the image.\n\n`skimage.measure.label`(input[, background, \u2026])\n\nLabel connected regions of an integer array.\n\n`skimage.measure.marching_cubes`(volume[, \u2026])\n\nMarching cubes algorithm to find surfaces in 3d volumetric data.\n\n`skimage.measure.marching_cubes_classic`(volume)\n\nClassic marching cubes algorithm to find surfaces in 3d volumetric data.\n\n`skimage.measure.marching_cubes_lewiner`(volume)\n\nLewiner marching cubes algorithm to find surfaces in 3d volumetric data.\n\n`skimage.measure.mesh_surface_area`(verts, faces)\n\nCompute surface area, given vertices & triangular faces\n\n`skimage.measure.moments`(image[, order])\n\nCalculate all raw image moments up to a certain order.\n\n`skimage.measure.moments_central`(image[, \u2026])\n\nCalculate all central image moments up to a certain order.\n\n`skimage.measure.moments_coords`(coords[, order])\n\nCalculate all raw image moments up to a certain order.\n\n`skimage.measure.moments_coords_central`(coords)\n\nCalculate all central image moments up to a certain order.\n\n`skimage.measure.moments_hu`(nu)\n\nCalculate Hu\u2019s set of image moments (2D-only).\n\n`skimage.measure.moments_normalized`(mu[, order])\n\nCalculate all normalized central image moments up to a certain order.\n\n`skimage.measure.perimeter`(image[, neighbourhood])\n\nCalculate total perimeter of all objects in binary image.\n\n`skimage.measure.perimeter_crofton`(image[, \u2026])\n\nCalculate total Crofton perimeter of all objects in binary image.\n\n`skimage.measure.points_in_poly`(points, verts)\n\nTest whether points lie inside a polygon.\n\n`skimage.measure.profile_line`(image, src, dst)\n\nReturn the intensity profile of an image measured along a scan line.\n\n`skimage.measure.ransac`(data, model_class, \u2026)\n\nFit a model to data with the RANSAC (random sample consensus) algorithm.\n\n`skimage.measure.regionprops`(label_image[, \u2026])\n\nMeasure properties of labeled image regions.\n\n`skimage.measure.regionprops_table`(label_image)\n\nCompute image properties and return them as a pandas-compatible table.\n\n`skimage.measure.shannon_entropy`(image[, base])\n\nCalculate the Shannon entropy of an image.\n\n`skimage.measure.subdivide_polygon`(coords[, \u2026])\n\nSubdivision of polygonal curves using B-Splines.\n\n`skimage.measure.CircleModel`()\n\nTotal least squares estimator for 2D circles.\n\n`skimage.measure.EllipseModel`()\n\nTotal least squares estimator for 2D ellipses.\n\n`skimage.measure.LineModelND`()\n\nTotal least squares estimator for N-dimensional lines.\n\nApproximate a polygonal chain with the specified tolerance.\n\nIt is based on the Douglas-Peucker algorithm.\n\nNote that the approximated polygon is always within the convex hull of the\noriginal polygon.\n\nCoordinate array.\n\nMaximum distance from original points of polygon to approximated polygonal\nchain. If tolerance is 0, the original coordinate array is returned.\n\nApproximated polygonal chain where M <= N.\n\nhttps://en.wikipedia.org/wiki/Ramer-Douglas-Peucker_algorithm\n\nDownsample image by applying function `func` to local blocks.\n\nThis function is useful for max and mean pooling, for example.\n\nN-dimensional input image.\n\nArray containing down-sampling integer factor along each axis.\n\nFunction object which is used to calculate the return value for each local\nblock. This function must implement an `axis` parameter. Primary functions are\n`numpy.sum`, `numpy.min`, `numpy.max`, `numpy.mean` and `numpy.median`. See\nalso `func_kwargs`.\n\nConstant padding value if image is not perfectly divisible by the block size.\n\nKeyword arguments passed to `func`. Notably useful for passing dtype argument\nto `np.mean`. Takes dictionary of inputs, e.g.: `func_kwargs={'dtype':\nnp.float16})`.\n\nDown-sampled image with same number of dimensions as input image.\n\nCalculate the Euler characteristic in binary image.\n\nFor 2D objects, the Euler number is the number of objects minus the number of\nholes. For 3D objects, the Euler number is obtained as the number of objects\nplus the number of holes, minus the number of tunnels, or loops.\n\n2D or 3D images. If image is not binary, all values strictly greater than zero\nare considered as the object.\n\nMaximum number of orthogonal hops to consider a pixel/voxel as a neighbor.\nAccepted values are ranging from 1 to input.ndim. If `None`, a full\nconnectivity of `input.ndim` is used. 4 or 8 neighborhoods are defined for 2D\nimages (connectivity 1 and 2, respectively). 6 or 26 neighborhoods are defined\nfor 3D images, (connectivity 1 and 3, respectively). Connectivity 2 is not\ndefined.\n\nEuler characteristic of the set of all objects in the image.\n\nThe Euler characteristic is an integer number that describes the topology of\nthe set of all objects in the input image. If object is 4-connected, then\nbackground is 8-connected, and conversely.\n\nThe computation of the Euler characteristic is based on an integral geometry\nformula in discretized space. In practice, a neighbourhood configuration is\nconstructed, and a LUT is applied for each configuration. The coefficients\nused are the ones of Ohser et al.\n\nIt can be useful to compute the Euler characteristic for several\nconnectivities. A large relative difference between results for different\nconnectivities suggests that the image resolution (with respect to the size of\nobjects and holes) is too low.\n\nS. Rivollier. Analyse d\u2019image geometrique et morphometrique par diagrammes de\nforme et voisinages adaptatifs generaux. PhD thesis, 2010. Ecole Nationale\nSuperieure des Mines de Saint-Etienne. https://tel.archives-\nouvertes.fr/tel-00560838\n\nOhser J., Nagel W., Schladitz K. (2002) The Euler Number of Discretized Sets -\nOn the Choice of Adjacency in Homogeneous Lattices. In: Mecke K., Stoyan D.\n(eds) Morphology of Condensed Matter. Lecture Notes in Physics, vol 600.\nSpringer, Berlin, Heidelberg.\n\nEuler number\n\nFind iso-valued contours in a 2D array for a given level value.\n\nUses the \u201cmarching squares\u201d method to compute a the iso-valued contours of the\ninput 2D array for a particular level value. Array values are linearly\ninterpolated to provide better precision for the output contours.\n\nInput image in which to find contours.\n\nValue along which to find contours in the array. By default, the level is set\nto (max(image) + min(image)) / 2\n\nChanged in version 0.18: This parameter is now optional.\n\nIndicates whether array elements below the given level value are to be\nconsidered fully-connected (and hence elements above the value will only be\nface connected), or vice-versa. (See notes below for details.)\n\nIndicates whether the output contours will produce positively-oriented\npolygons around islands of low- or high-valued elements. If \u2018low\u2019 then\ncontours will wind counter- clockwise around elements below the iso-value.\nAlternately, this means that low-valued elements are always on the left of the\ncontour. (See below for details.)\n\nA boolean mask, True where we want to draw contours. Note that NaN values are\nalways excluded from the considered region (`mask` is set to `False` wherever\n`array` is `NaN`).\n\nEach contour is an ndarray of shape `(n, 2)`, consisting of n `(row, column)`\ncoordinates along the contour.\n\nSee also\n\nThe marching squares algorithm is a special case of the marching cubes\nalgorithm [1]. A simple explanation is available here:\n\nhttp://users.polytech.unice.fr/~lingrand/MarchingCubes/algo.html\n\nThere is a single ambiguous case in the marching squares algorithm: when a\ngiven `2 x 2`-element square has two high-valued and two low-valued elements,\neach pair diagonally adjacent. (Where high- and low-valued is with respect to\nthe contour value sought.) In this case, either the high-valued elements can\nbe \u2018connected together\u2019 via a thin isthmus that separates the low-valued\nelements, or vice-versa. When elements are connected together across a\ndiagonal, they are considered \u2018fully connected\u2019 (also known as \u2018face+vertex-\nconnected\u2019 or \u20188-connected\u2019). Only high-valued or low-valued elements can be\nfully-connected, the other set will be considered as \u2018face-connected\u2019 or\n\u20184-connected\u2019. By default, low-valued elements are considered fully-connected;\nthis can be altered with the \u2018fully_connected\u2019 parameter.\n\nOutput contours are not guaranteed to be closed: contours which intersect the\narray edge or a masked-off region (either where mask is False or where array\nis NaN) will be left open. All other contours will be closed. (The closed-ness\nof a contours can be tested by checking whether the beginning point is the\nsame as the end point.)\n\nContours are oriented. By default, array values lower than the contour value\nare to the left of the contour and values greater than the contour value are\nto the right. This means that contours will wind counter-clockwise (i.e. in\n\u2018positive orientation\u2019) around islands of low-valued pixels. This behavior can\nbe altered with the \u2018positive_orientation\u2019 parameter.\n\nThe order of the contours in the output list is determined by the position of\nthe smallest `x,y` (in lexicographical order) coordinate in the contour. This\nis a side-effect of how the input array is traversed, but can be relied upon.\n\nWarning\n\nArray coordinates/values are assumed to refer to the center of the array\nelement. Take a simple example input: `[0, 1]`. The interpolated position of\n0.5 in this array is midway between the 0-element (at `x=0`) and the 1-element\n(at `x=1`), and thus would fall at `x=0.5`.\n\nThis means that to find reasonable contours, it is best to find contours\nmidway between the expected \u201clight\u201d and \u201cdark\u201d values. In particular, given a\nbinarized array, do not choose to find contours at the low or high value of\nthe array. This will often yield degenerate contours, especially around\nstructures that are a single array element wide. Instead choose a middle\nvalue, as above.\n\nLorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D\nSurface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings)\n21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422\n\nContour finding\n\nMeasure region properties\n\nTest whether points on a specified grid are inside a polygon.\n\nFor each `(r, c)` coordinate on a grid, i.e. `(0, 0)`, `(0, 1)` etc., test\nwhether that point lies inside a polygon.\n\nShape of the grid.\n\nSpecify the V vertices of the polygon, sorted either clockwise or anti-\nclockwise. The first point may (but does not need to be) duplicated.\n\nTrue where the grid falls inside the polygon.\n\nSee also\n\nCompute the inertia tensor of the input image.\n\nThe input image.\n\nThe pre-computed central moments of `image`. The inertia tensor computation\nrequires the central moments of the image. If an application requires both the\ncentral moments and the inertia tensor (for example,\n`skimage.measure.regionprops`), then it is more efficient to pre-compute them\nand pass them to the inertia tensor call.\n\nThe inertia tensor of the input image. \\\\(T_{i, j}\\\\) contains the covariance\nof image intensity along axes \\\\(i\\\\) and \\\\(j\\\\).\n\nhttps://en.wikipedia.org/wiki/Moment_of_inertia#Inertia_tensor\n\nBernd J\u00e4hne. Spatio-Temporal Image Processing: Theory and Scientific\nApplications. (Chapter 8: Tensor Methods) Springer, 1993.\n\nCompute the eigenvalues of the inertia tensor of the image.\n\nThe inertia tensor measures covariance of the image intensity along the image\naxes. (See `inertia_tensor`.) The relative magnitude of the eigenvalues of the\ntensor is thus a measure of the elongation of a (bright) object in the image.\n\nThe input image.\n\nThe pre-computed central moments of `image`.\n\nThe pre-computed inertia tensor. If `T` is given, `mu` and `image` are\nignored.\n\nThe eigenvalues of the inertia tensor of `image`, in descending order.\n\nComputing the eigenvalues requires the inertia tensor of the input image. This\nis much faster if the central moments (`mu`) are provided, or, alternatively,\none can provide the inertia tensor (`T`) directly.\n\nLabel connected regions of an integer array.\n\nTwo pixels are connected when they are neighbors and have the same value. In\n2D, they can be neighbors either in a 1- or 2-connected sense. The value\nrefers to the maximum number of orthogonal hops to consider a pixel/voxel a\nneighbor:\n\nImage to label.\n\nConsider all pixels with this value as background pixels, and label them as 0.\nBy default, 0-valued pixels are considered as background pixels.\n\nWhether to return the number of assigned labels.\n\nMaximum number of orthogonal hops to consider a pixel/voxel as a neighbor.\nAccepted values are ranging from 1 to input.ndim. If `None`, a full\nconnectivity of `input.ndim` is used.\n\nLabeled array, where all connected regions are assigned the same integer\nvalue.\n\nNumber of labels, which equals the maximum label index and is only returned if\nreturn_num is `True`.\n\nSee also\n\nChristophe Fiorio and Jens Gustedt, \u201cTwo linear time Union-Find strategies for\nimage processing\u201d, Theoretical Computer Science 154 (1996), pp. 165-181.\n\nKensheng Wu, Ekow Otoo and Arie Shoshani, \u201cOptimizing connected component\nlabeling algorithms\u201d, Paper LBNL-56864, 2005, Lawrence Berkeley National\nLaboratory (University of California),\nhttp://repositories.cdlib.org/lbnl/LBNL-56864\n\nMeasure region properties\n\nEuler number\n\nSegment human cells (in mitosis)\n\nMarching cubes algorithm to find surfaces in 3d volumetric data.\n\nIn contrast with Lorensen et al. approach [2], Lewiner et al. algorithm is\nfaster, resolves ambiguities, and guarantees topologically correct results.\nTherefore, this algorithm generally a better choice.\n\nInput data volume to find isosurfaces. Will internally be converted to float32\nif necessary.\n\nContour value to search for isosurfaces in `volume`. If not given or None, the\naverage of the min and max of vol is used.\n\nVoxel spacing in spatial dimensions corresponding to numpy array indexing\ndimensions (M, N, P) as in `volume`.\n\nControls if the mesh was generated from an isosurface with gradient descent\ntoward objects of interest (the default), or the opposite, considering the\nleft-hand rule. The two options are: * descent : Object was greater than\nexterior * ascent : Exterior was greater than object\n\nStep size in voxels. Default 1. Larger steps yield faster but coarser results.\nThe result will always be topologically correct though.\n\nWhether to allow degenerate (i.e. zero-area) triangles in the end-result.\nDefault True. If False, degenerate triangles are removed, at the cost of\nmaking the algorithm slower.\n\nOne of \u2018lewiner\u2019, \u2018lorensen\u2019 or \u2018_lorensen\u2019. Specify witch of Lewiner et al.\nor Lorensen et al. method will be used. The \u2018_lorensen\u2019 flag correspond to an\nold implementation that will be deprecated in version 0.19.\n\nBoolean array. The marching cube algorithm will be computed only on True\nelements. This will save computational time when interfaces are located within\ncertain region of the volume M, N, P-e.g. the top half of the cube-and also\nallow to compute finite surfaces-i.e. open surfaces that do not end at the\nborder of the cube.\n\nSpatial coordinates for V unique mesh vertices. Coordinate order matches input\n`volume` (M, N, P). If `allow_degenerate` is set to True, then the presence of\ndegenerate triangles in the mesh can make this array have duplicate vertices.\n\nDefine triangular faces via referencing vertex indices from `verts`. This\nalgorithm specifically outputs triangles, so each face has exactly three\nindices.\n\nThe normal direction at each vertex, as calculated from the data.\n\nGives a measure for the maximum value of the data in the local region near\neach vertex. This can be used by visualization tools to apply a colormap to\nthe mesh.\n\nSee also\n\nThe algorithm [1] is an improved version of Chernyaev\u2019s Marching Cubes 33\nalgorithm. It is an efficient algorithm that relies on heavy use of lookup\ntables to handle the many different cases, keeping the algorithm relatively\neasy. This implementation is written in Cython, ported from Lewiner\u2019s C++\nimplementation.\n\nTo quantify the area of an isosurface generated by this algorithm, pass verts\nand faces to `skimage.measure.mesh_surface_area`.\n\nRegarding visualization of algorithm output, to contour a volume named\n`myvolume` about the level 0.0, using the `mayavi` package:\n\nSimilarly using the `visvis` package:\n\nTo reduce the number of triangles in the mesh for better performance, see this\nexample using the `mayavi` package.\n\nThomas Lewiner, Helio Lopes, Antonio Wilson Vieira and Geovan Tavares.\nEfficient implementation of Marching Cubes\u2019 cases with topological guarantees.\nJournal of Graphics Tools 8(2) pp. 1-15 (december 2003).\nDOI:10.1080/10867651.2003.10487582\n\nLorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D\nSurface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings)\n21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422\n\nClassic marching cubes algorithm to find surfaces in 3d volumetric data.\n\nNote that the `marching_cubes()` algorithm is recommended over this algorithm,\nbecause it\u2019s faster and produces better results.\n\nInput data volume to find isosurfaces. Will be cast to `np.float64`.\n\nContour value to search for isosurfaces in `volume`. If not given or None, the\naverage of the min and max of vol is used.\n\nVoxel spacing in spatial dimensions corresponding to numpy array indexing\ndimensions (M, N, P) as in `volume`.\n\nControls if the mesh was generated from an isosurface with gradient descent\ntoward objects of interest (the default), or the opposite. The two options\nare: * descent : Object was greater than exterior * ascent : Exterior was\ngreater than object\n\nSpatial coordinates for V unique mesh vertices. Coordinate order matches input\n`volume` (M, N, P). If `allow_degenerate` is set to True, then the presence of\ndegenerate triangles in the mesh can make this array have duplicate vertices.\n\nDefine triangular faces via referencing vertex indices from `verts`. This\nalgorithm specifically outputs triangles, so each face has exactly three\nindices.\n\nSee also\n\nThe marching cubes algorithm is implemented as described in [1]. A simple\nexplanation is available here:\n\nThere are several known ambiguous cases in the marching cubes algorithm. Using\npoint labeling as in [1], Figure 4, as shown:\n\nMost notably, if v4, v8, v2, and v6 are all >= `level` (or any generalization\nof this case) two parallel planes are generated by this algorithm, separating\nv4 and v8 from v2 and v6. An equally valid interpretation would be a single\nconnected thin surface enclosing all four points. This is the best known\nambiguity, though there are others.\n\nThis algorithm does not attempt to resolve such ambiguities; it is a naive\nimplementation of marching cubes as in [1], but may be a good beginning for\nwork with more recent techniques (Dual Marching Cubes, Extended Marching\nCubes, Cubic Marching Squares, etc.).\n\nBecause of interactions between neighboring cubes, the isosurface(s) generated\nby this algorithm are NOT guaranteed to be closed, particularly for\ncomplicated contours. Furthermore, this algorithm does not guarantee a single\ncontour will be returned. Indeed, ALL isosurfaces which cross `level` will be\nfound, regardless of connectivity.\n\nThe output is a triangular mesh consisting of a set of unique vertices and\nconnecting triangles. The order of these vertices and triangles in the output\nlist is determined by the position of the smallest `x,y,z` (in lexicographical\norder) coordinate in the contour. This is a side-effect of how the input array\nis traversed, but can be relied upon.\n\nThe generated mesh guarantees coherent orientation as of version 0.12.\n\nTo quantify the area of an isosurface generated by this algorithm, pass\noutputs directly into `skimage.measure.mesh_surface_area`.\n\nLorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D\nSurface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings)\n21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422\n\nLewiner marching cubes algorithm to find surfaces in 3d volumetric data.\n\nIn contrast to `marching_cubes_classic()`, this algorithm is faster, resolves\nambiguities, and guarantees topologically correct results. Therefore, this\nalgorithm generally a better choice, unless there is a specific need for the\nclassic algorithm.\n\nInput data volume to find isosurfaces. Will internally be converted to float32\nif necessary.\n\nContour value to search for isosurfaces in `volume`. If not given or None, the\naverage of the min and max of vol is used.\n\nVoxel spacing in spatial dimensions corresponding to numpy array indexing\ndimensions (M, N, P) as in `volume`.\n\nControls if the mesh was generated from an isosurface with gradient descent\ntoward objects of interest (the default), or the opposite, considering the\nleft-hand rule. The two options are: * descent : Object was greater than\nexterior * ascent : Exterior was greater than object\n\nStep size in voxels. Default 1. Larger steps yield faster but coarser results.\nThe result will always be topologically correct though.\n\nWhether to allow degenerate (i.e. zero-area) triangles in the end-result.\nDefault True. If False, degenerate triangles are removed, at the cost of\nmaking the algorithm slower.\n\nIf given and True, the classic marching cubes by Lorensen (1987) is used. This\noption is included for reference purposes. Note that this algorithm has\nambiguities and is not guaranteed to produce a topologically correct result.\nThe results with using this option are not generally the same as the\n`marching_cubes_classic()` function.\n\nBoolean array. The marching cube algorithm will be computed only on True\nelements. This will save computational time when interfaces are located within\ncertain region of the volume M, N, P-e.g. the top half of the cube-and also\nallow to compute finite surfaces-i.e. open surfaces that do not end at the\nborder of the cube.\n\nSpatial coordinates for V unique mesh vertices. Coordinate order matches input\n`volume` (M, N, P). If `allow_degenerate` is set to True, then the presence of\ndegenerate triangles in the mesh can make this array have duplicate vertices.\n\nDefine triangular faces via referencing vertex indices from `verts`. This\nalgorithm specifically outputs triangles, so each face has exactly three\nindices.\n\nThe normal direction at each vertex, as calculated from the data.\n\nGives a measure for the maximum value of the data in the local region near\neach vertex. This can be used by visualization tools to apply a colormap to\nthe mesh.\n\nSee also\n\nThe algorithm [1] is an improved version of Chernyaev\u2019s Marching Cubes 33\nalgorithm. It is an efficient algorithm that relies on heavy use of lookup\ntables to handle the many different cases, keeping the algorithm relatively\neasy. This implementation is written in Cython, ported from Lewiner\u2019s C++\nimplementation.\n\nTo quantify the area of an isosurface generated by this algorithm, pass verts\nand faces to `skimage.measure.mesh_surface_area`.\n\nRegarding visualization of algorithm output, to contour a volume named\n`myvolume` about the level 0.0, using the `mayavi` package:\n\nSimilarly using the `visvis` package:\n\nThomas Lewiner, Helio Lopes, Antonio Wilson Vieira and Geovan Tavares.\nEfficient implementation of Marching Cubes\u2019 cases with topological guarantees.\nJournal of Graphics Tools 8(2) pp. 1-15 (december 2003).\nDOI:10.1080/10867651.2003.10487582\n\nCompute surface area, given vertices & triangular faces\n\nArray containing (x, y, z) coordinates for V unique mesh vertices.\n\nList of length-3 lists of integers, referencing vertex coordinates as provided\nin `verts`\n\nSurface area of mesh. Units now [coordinate units] ** 2.\n\nSee also\n\nThe arguments expected by this function are the first two outputs from\n`skimage.measure.marching_cubes`. For unit correct output, ensure correct\n`spacing` was passed to `skimage.measure.marching_cubes`.\n\nThis algorithm works properly only if the `faces` provided are all triangles.\n\nCalculate all raw image moments up to a certain order.\n\nNote that raw moments are neither translation, scale nor rotation invariant.\n\nRasterized shape as image.\n\nMaximum order of moments. Default is 3.\n\nRaw image moments.\n\nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core\nAlgorithms. Springer-Verlag, London, 2009.\n\nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6.\nedition, 2005.\n\nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from\nLecture notes in computer science, p. 676. Springer, Berlin, 1993.\n\nhttps://en.wikipedia.org/wiki/Image_moment\n\nCalculate all central image moments up to a certain order.\n\nThe center coordinates (cr, cc) can be calculated from the raw moments as:\n{`M[1, 0] / M[0, 0]`, `M[0, 1] / M[0, 0]`}.\n\nNote that central moments are translation invariant but not scale and rotation\ninvariant.\n\nRasterized shape as image.\n\nCoordinates of the image centroid. This will be computed if it is not\nprovided.\n\nThe maximum order of moments computed.\n\nCentral image moments.\n\nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core\nAlgorithms. Springer-Verlag, London, 2009.\n\nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6.\nedition, 2005.\n\nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from\nLecture notes in computer science, p. 676. Springer, Berlin, 1993.\n\nhttps://en.wikipedia.org/wiki/Image_moment\n\nCalculate all raw image moments up to a certain order.\n\nNote that raw moments are neither translation, scale nor rotation invariant.\n\nArray of N points that describe an image of D dimensionality in Cartesian\nspace.\n\nMaximum order of moments. Default is 3.\n\nRaw image moments. (D dimensions)\n\nJohannes Kilian. Simple Image Analysis By Moments. Durham University, version\n0.2, Durham, 2001.\n\nCalculate all central image moments up to a certain order.\n\nNote that raw moments are neither translation, scale nor rotation invariant.\n\nArray of N points that describe an image of D dimensionality in Cartesian\nspace. A tuple of coordinates as returned by `np.nonzero` is also accepted as\ninput.\n\nCoordinates of the image centroid. This will be computed if it is not\nprovided.\n\nMaximum order of moments. Default is 3.\n\nCentral image moments. (D dimensions)\n\nJohannes Kilian. Simple Image Analysis By Moments. Durham University, version\n0.2, Durham, 2001.\n\nAs seen above, for symmetric objects, odd-order moments (columns 1 and 3, rows\n1 and 3) are zero when centered on the centroid, or center of mass, of the\nobject (the default). If we break the symmetry by adding a new point, this no\nlonger holds:\n\nImage moments and central image moments are equivalent (by definition) when\nthe center is (0, 0):\n\nCalculate Hu\u2019s set of image moments (2D-only).\n\nNote that this set of moments is proofed to be translation, scale and rotation\ninvariant.\n\nNormalized central image moments, where M must be >= 4.\n\nHu\u2019s set of image moments.\n\nM. K. Hu, \u201cVisual Pattern Recognition by Moment Invariants\u201d, IRE Trans. Info.\nTheory, vol. IT-8, pp. 179-187, 1962\n\nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core\nAlgorithms. Springer-Verlag, London, 2009.\n\nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6.\nedition, 2005.\n\nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from\nLecture notes in computer science, p. 676. Springer, Berlin, 1993.\n\nhttps://en.wikipedia.org/wiki/Image_moment\n\nCalculate all normalized central image moments up to a certain order.\n\nNote that normalized central moments are translation and scale invariant but\nnot rotation invariant.\n\nCentral image moments, where M must be greater than or equal to `order`.\n\nMaximum order of moments. Default is 3.\n\nNormalized central image moments.\n\nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core\nAlgorithms. Springer-Verlag, London, 2009.\n\nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6.\nedition, 2005.\n\nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from\nLecture notes in computer science, p. 676. Springer, Berlin, 1993.\n\nhttps://en.wikipedia.org/wiki/Image_moment\n\nCalculate total perimeter of all objects in binary image.\n\n2D binary image.\n\nNeighborhood connectivity for border pixel determination. It is used to\ncompute the contour. A higher neighbourhood widens the border on which the\nperimeter is computed.\n\nTotal perimeter of all objects in binary image.\n\nK. Benkrid, D. Crookes. Design and FPGA Implementation of a Perimeter\nEstimator. The Queen\u2019s University of Belfast.\nhttp://www.cs.qub.ac.uk/~d.crookes/webpubs/papers/perimeter.doc\n\nDifferent perimeters\n\nCalculate total Crofton perimeter of all objects in binary image.\n\n2D image. If image is not binary, all values strictly greater than zero are\nconsidered as the object.\n\nNumber of directions used to approximate the Crofton perimeter. By default, 4\nis used: it should be more accurate than 2. Computation time is the same in\nboth cases.\n\nTotal perimeter of all objects in binary image.\n\nThis measure is based on Crofton formula [1], which is a measure from integral\ngeometry. It is defined for general curve length evaluation via a double\nintegral along all directions. In a discrete space, 2 or 4 directions give a\nquite good approximation, 4 being more accurate than 2 for more complex\nshapes.\n\nSimilar to `perimeter()`, this function returns an approximation of the\nperimeter in continuous space.\n\nhttps://en.wikipedia.org/wiki/Crofton_formula\n\nS. Rivollier. Analyse d\u2019image geometrique et morphometrique par diagrammes de\nforme et voisinages adaptatifs generaux. PhD thesis, 2010. Ecole Nationale\nSuperieure des Mines de Saint-Etienne. https://tel.archives-\nouvertes.fr/tel-00560838\n\nDifferent perimeters\n\nTest whether points lie inside a polygon.\n\nInput points, `(x, y)`.\n\nVertices of the polygon, sorted either clockwise or anti-clockwise. The first\npoint may (but does not need to be) duplicated.\n\nTrue if corresponding point is inside the polygon.\n\nSee also\n\nReturn the intensity profile of an image measured along a scan line.\n\nThe image, either grayscale (2D array) or multichannel (3D array, where the\nfinal axis contains the channel information).\n\nThe coordinates of the start point of the scan line.\n\nThe coordinates of the end point of the scan line. The destination point is\nincluded in the profile, in contrast to standard numpy indexing.\n\nWidth of the scan, perpendicular to the line\n\nThe order of the spline interpolation, default is 0 if image.dtype is bool and\n1 otherwise. The order has to be in the range 0-5. See\n`skimage.transform.warp` for detail.\n\nHow to compute any values falling outside of the image.\n\nIf `mode` is \u2018constant\u2019, what constant value to use outside the image.\n\nFunction used to calculate the aggregation of pixel values perpendicular to\nthe profile_line direction when `linewidth` > 1\\. If set to None the unreduced\narray will be returned.\n\nThe intensity profile along the scan line. The length of the profile is the\nceil of the computed length of the scan line.\n\nThe destination point is included in the profile, in contrast to standard\nnumpy indexing. For example:\n\nFor different reduce_func inputs:\n\nThe unreduced array will be returned when `reduce_func` is None or when\n`reduce_func` acts on each pixel value individually.\n\nFit a model to data with the RANSAC (random sample consensus) algorithm.\n\nRANSAC is an iterative algorithm for the robust estimation of parameters from\na subset of inliers from the complete data set. Each iteration performs the\nfollowing tasks:\n\nThese steps are performed either a maximum number of times or until one of the\nspecial stop criteria are met. The final model is estimated using all inlier\nsamples of the previously determined best model.\n\nData set to which the model is fitted, where N is the number of data points\nand the remaining dimension are depending on model requirements. If the model\nclass requires multiple input data arrays (e.g. source and destination\ncoordinates of `skimage.transform.AffineTransform`), they can be optionally\npassed as tuple or list. Note, that in this case the functions\n`estimate(*data)`, `residuals(*data)`, `is_model_valid(model, *random_data)`\nand `is_data_valid(*random_data)` must all take each data array as separate\narguments.\n\nObject with the following object methods:\n\nwhere `success` indicates whether the model estimation succeeded (`True` or\n`None` for success, `False` for failure).\n\nThe minimum number of data points to fit a model to.\n\nMaximum distance for a data point to be classified as an inlier.\n\nThis function is called with the randomly selected data before the model is\nfitted to it: `is_data_valid(*random_data)`.\n\nThis function is called with the estimated model and the randomly selected\ndata: `is_model_valid(model, *random_data)`, .\n\nMaximum number of iterations for random sample selection.\n\nStop iteration if at least this number of inliers are found.\n\nStop iteration if sum of residuals is less than or equal to this threshold.\n\nRANSAC iteration stops if at least one outlier-free set of the training data\nis sampled with `probability >= stop_probability`, depending on the current\nbest model\u2019s inlier ratio and the number of trials. This requires to generate\nat least N samples (trials):\n\nN >= log(1 - probability) / log(1 - e**m)\n\nwhere the probability (confidence) is typically set to a high value such as\n0.99, e is the current fraction of inliers w.r.t. the total number of samples,\nand m is the min_samples value.\n\nIf int, random_state is the seed used by the random number generator; If\nRandomState instance, random_state is the random number generator; If None,\nthe random number generator is the RandomState instance used by `np.random`.\n\nInitial samples selection for model estimation\n\nBest model with largest consensus set.\n\nBoolean mask of inliers classified as `True`.\n\n\u201cRANSAC\u201d, Wikipedia, https://en.wikipedia.org/wiki/RANSAC\n\nGenerate ellipse data without tilt and add noise:\n\nAdd some faulty data:\n\nEstimate ellipse model using all available data:\n\nEstimate ellipse model using RANSAC:\n\nRANSAC can be used to robustly estimate a geometric transformation. In this\nsection, we also show how to use a proportion of the total samples, rather\nthan an absolute number.\n\nMeasure properties of labeled image regions.\n\nLabeled input image. Labels with value 0 are ignored.\n\nChanged in version 0.14.1: Previously, `label_image` was processed by\n`numpy.squeeze` and so any number of singleton dimensions was allowed. This\nresulted in inconsistent handling of images with singleton dimensions. To\nrecover the old behaviour, use `regionprops(np.squeeze(label_image), ...)`.\n\nIntensity (i.e., input) image with same size as labeled image, plus optionally\nan extra dimension for multichannel data. Default is None.\n\nChanged in version 0.18.0: The ability to provide an extra dimension for\nchannels was added.\n\nDetermine whether to cache calculated properties. The computation is much\nfaster for cached properties, whereas the memory consumption increases.\n\nThis argument is deprecated and will be removed in a future version of scikit-\nimage.\n\nSee Coordinate conventions for more details.\n\nDeprecated since version 0.16.0: Use \u201crc\u201d coordinates everywhere. It may be\nsufficient to call `numpy.transpose` on your label image to get the same\nvalues as 0.15 and earlier. However, for some properties, the transformation\nwill be less trivial. For example, the new orientation is \\\\(\\frac{\\pi}{2}\\\\)\nplus the old orientation.\n\nAdd extra property computation functions that are not included with skimage.\nThe name of the property is derived from the function name, the dtype is\ninferred by calling the function on a small sample. If the name of an extra\nproperty clashes with the name of an existing property the extra property wil\nnot be visible and a UserWarning is issued. A property computation function\nmust take a region mask as its first argument. If the property requires an\nintensity image, it must accept the intensity image as the second argument.\n\nEach item describes one labeled region, and can be accessed using the\nattributes listed below.\n\nSee also\n\nThe following properties can be accessed as attributes or keys:\n\nNumber of pixels of the region.\n\nBounding box `(min_row, min_col, max_row, max_col)`. Pixels belonging to the\nbounding box are in the half-open interval `[min_row; max_row)` and `[min_col;\nmax_col)`.\n\nNumber of pixels of bounding box.\n\nCentroid coordinate tuple `(row, col)`.\n\nNumber of pixels of convex hull image, which is the smallest convex polygon\nthat encloses the region.\n\nBinary convex hull image which has the same size as bounding box.\n\nCoordinate list `(row, col)` of the region.\n\nEccentricity of the ellipse that has the same second-moments as the region.\nThe eccentricity is the ratio of the focal distance (distance between focal\npoints) over the major axis length. The value is in the interval [0, 1). When\nit is 0, the ellipse becomes a circle.\n\nThe diameter of a circle with the same area as the region.\n\nEuler characteristic of the set of non-zero pixels. Computed as number of\nconnected components subtracted by number of holes (input.ndim connectivity).\nIn 3D, number of connected components plus number of holes subtracted by\nnumber of tunnels.\n\nRatio of pixels in the region to pixels in the total bounding box. Computed as\n`area / (rows * cols)`\n\nMaximum Feret\u2019s diameter computed as the longest distance between points\naround a region\u2019s convex hull contour as determined by `find_contours`. [5]\n\nNumber of pixels of the region will all the holes filled in. Describes the\narea of the filled_image.\n\nBinary region image with filled holes which has the same size as bounding box.\n\nSliced binary region image which has the same size as bounding box.\n\nInertia tensor of the region for the rotation around its mass.\n\nThe eigenvalues of the inertia tensor in decreasing order.\n\nImage inside region bounding box.\n\nThe label in the labeled input image.\n\nCentroid coordinate tuple `(row, col)`, relative to region bounding box.\n\nThe length of the major axis of the ellipse that has the same normalized\nsecond central moments as the region.\n\nValue with the greatest intensity in the region.\n\nValue with the mean intensity in the region.\n\nValue with the least intensity in the region.\n\nThe length of the minor axis of the ellipse that has the same normalized\nsecond central moments as the region.\n\nSpatial moments up to 3rd order:\n\nwhere the sum is over the `row`, `col` coordinates of the region.\n\nCentral moments (translation invariant) up to 3rd order:\n\nwhere the sum is over the `row`, `col` coordinates of the region, and `row_c`\nand `col_c` are the coordinates of the region\u2019s centroid.\n\nHu moments (translation, scale and rotation invariant).\n\nNormalized moments (translation and scale invariant) up to 3rd order:\n\nwhere `m_00` is the zeroth spatial moment.\n\nAngle between the 0th axis (rows) and the major axis of the ellipse that has\nthe same second moments as the region, ranging from `-pi/2` to `pi/2` counter-\nclockwise.\n\nPerimeter of object which approximates the contour as a line through the\ncenters of border pixels using a 4-connectivity.\n\nPerimeter of object approximated by the Crofton formula in 4 directions.\n\nA slice to extract the object from the source image.\n\nRatio of pixels in the region to pixels of the convex hull image.\n\nCentroid coordinate tuple `(row, col)` weighted with intensity image.\n\nCentroid coordinate tuple `(row, col)`, relative to region bounding box,\nweighted with intensity image.\n\nSpatial moments of intensity image up to 3rd order:\n\nwhere the sum is over the `row`, `col` coordinates of the region.\n\nCentral moments (translation invariant) of intensity image up to 3rd order:\n\nwhere the sum is over the `row`, `col` coordinates of the region, and `row_c`\nand `col_c` are the coordinates of the region\u2019s weighted centroid.\n\nHu moments (translation, scale and rotation invariant) of intensity image.\n\nNormalized moments (translation and scale invariant) of intensity image up to\n3rd order:\n\nwhere `wm_00` is the zeroth spatial moment (intensity-weighted area).\n\nEach region also supports iteration, so that you can do:\n\nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core\nAlgorithms. Springer-Verlag, London, 2009.\n\nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6.\nedition, 2005.\n\nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from\nLecture notes in computer science, p. 676. Springer, Berlin, 1993.\n\nhttps://en.wikipedia.org/wiki/Image_moment\n\nW. Pabst, E. Gregorov\u00e1. Characterization of particles and particle systems,\npp. 27-28. ICT Prague, 2007.\nhttps://old.vscht.cz/sil/keramika/Characterization_of_particles/CPPS%20_English%20version_.pdf\n\nAdd custom measurements by passing functions as `extra_properties`\n\nMeasure region properties\n\nCompute image properties and return them as a pandas-compatible table.\n\nThe table is a dictionary mapping column names to value arrays. See Notes\nsection below for details.\n\nNew in version 0.16.\n\nLabeled input image. Labels with value 0 are ignored.\n\nIntensity (i.e., input) image with same size as labeled image, plus optionally\nan extra dimension for multichannel data. Default is None.\n\nChanged in version 0.18.0: The ability to provide an extra dimension for\nchannels was added.\n\nProperties that will be included in the resulting dictionary For a list of\navailable properties, please see `regionprops()`. Users should remember to add\n\u201clabel\u201d to keep track of region identities.\n\nDetermine whether to cache calculated properties. The computation is much\nfaster for cached properties, whereas the memory consumption increases.\n\nFor non-scalar properties not listed in OBJECT_COLUMNS, each element will\nappear in its own column, with the index of that element separated from the\nproperty name by this separator. For example, the inertia tensor of a 2D\nregion will appear in four columns: `inertia_tensor-0-0`,\n`inertia_tensor-0-1`, `inertia_tensor-1-0`, and `inertia_tensor-1-1` (where\nthe separator is `-`).\n\nObject columns are those that cannot be split in this way because the number\nof columns would change depending on the object. For example, `image` and\n`coords`.\n\nAdd extra property computation functions that are not included with skimage.\nThe name of the property is derived from the function name, the dtype is\ninferred by calling the function on a small sample. If the name of an extra\nproperty clashes with the name of an existing property the extra property wil\nnot be visible and a UserWarning is issued. A property computation function\nmust take a region mask as its first argument. If the property requires an\nintensity image, it must accept the intensity image as the second argument.\n\nDictionary mapping property names to an array of values of that property, one\nvalue per region. This dictionary can be used as input to pandas `DataFrame`\nto map property names to columns in the frame and regions to rows. If the\nimage has no regions, the arrays will have length 0, but the correct type.\n\nEach column contains either a scalar property, an object property, or an\nelement in a multidimensional array.\n\nProperties with scalar values for each region, such as \u201ceccentricity\u201d, will\nappear as a float or int array with that property name as key.\n\nMultidimensional properties of fixed size for a given image dimension, such as\n\u201ccentroid\u201d (every centroid will have three elements in a 3D image, no matter\nthe region size), will be split into that many columns, with the name\n{property_name}{separator}{element_num} (for 1D properties),\n{property_name}{separator}{elem_num0}{separator}{elem_num1} (for 2D\nproperties), and so on.\n\nFor multidimensional properties that don\u2019t have a fixed size, such as \u201cimage\u201d\n(the image of a region varies in size depending on the region size), an object\narray will be used, with the corresponding property name as the key.\n\nThe resulting dictionary can be directly passed to pandas, if installed, to\nobtain a clean DataFrame:\n\n[5 rows x 7 columns]\n\nIf we want to measure a feature that does not come as a built-in property, we\ncan define custom functions and pass them as `extra_properties`. For example,\nwe can create a custom function that measures the intensity quartiles in a\nregion:\n\nMeasure region properties\n\nCalculate the Shannon entropy of an image.\n\nThe Shannon entropy is defined as S = -sum(pk * log(pk)), where pk are\nfrequency/probability of pixels of value k.\n\nGrayscale input image.\n\nThe logarithmic base to use.\n\nThe returned value is measured in bits or shannon (Sh) for base=2, natural\nunit (nat) for base=np.e and hartley (Hart) for base=10.\n\nhttps://en.wikipedia.org/wiki/Entropy_(information_theory)\n\nhttps://en.wiktionary.org/wiki/Shannon_entropy\n\nSubdivision of polygonal curves using B-Splines.\n\nNote that the resulting curve is always within the convex hull of the original\npolygon. Circular polygons stay closed after subdivision.\n\nCoordinate array.\n\nDegree of B-Spline. Default is 2.\n\nPreserve first and last coordinate of non-circular polygon. Default is False.\n\nSubdivided coordinate array.\n\nhttp://mrl.nyu.edu/publications/subdiv-course2000/coursenotes00.pdf\n\nBases: `skimage.measure.fit.BaseModel`\n\nTotal least squares estimator for 2D circles.\n\nThe functional model of the circle is:\n\nThis estimator minimizes the squared distances from all points to the circle:\n\nA minimum number of 3 points is required to solve for the parameters.\n\nCircle model parameters in the following order `xc`, `yc`, `r`.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nEstimate circle model from data using total least squares.\n\nN points with `(x, y)` coordinates, respectively.\n\nTrue, if model estimation succeeds.\n\nPredict x- and y-coordinates using the estimated model.\n\nAngles in circle in radians. Angles start to count from positive x-axis to\npositive y-axis in a right-handed system.\n\nOptional custom parameter set.\n\nPredicted x- and y-coordinates.\n\nDetermine residuals of data to model.\n\nFor each point the shortest distance to the circle is returned.\n\nN points with `(x, y)` coordinates, respectively.\n\nResidual for each data point.\n\nBases: `skimage.measure.fit.BaseModel`\n\nTotal least squares estimator for 2D ellipses.\n\nThe functional model of the ellipse is:\n\nwhere `(xt, yt)` is the closest point on the ellipse to `(x, y)`. Thus d is\nthe shortest distance from the point to the ellipse.\n\nThe estimator is based on a least squares minimization. The optimal solution\nis computed directly, no iterations are required. This leads to a simple,\nstable and robust fitting method.\n\nThe `params` attribute contains the parameters in the following order:\n\nEllipse model parameters in the following order `xc`, `yc`, `a`, `b`, `theta`.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nEstimate circle model from data using total least squares.\n\nN points with `(x, y)` coordinates, respectively.\n\nTrue, if model estimation succeeds.\n\nHalir, R.; Flusser, J. \u201cNumerically stable direct least squares fitting of\nellipses\u201d. In Proc. 6th International Conference in Central Europe on Computer\nGraphics and Visualization. WSCG (Vol. 98, pp. 125-132).\n\nPredict x- and y-coordinates using the estimated model.\n\nAngles in circle in radians. Angles start to count from positive x-axis to\npositive y-axis in a right-handed system.\n\nOptional custom parameter set.\n\nPredicted x- and y-coordinates.\n\nDetermine residuals of data to model.\n\nFor each point the shortest distance to the ellipse is returned.\n\nN points with `(x, y)` coordinates, respectively.\n\nResidual for each data point.\n\nBases: `skimage.measure.fit.BaseModel`\n\nTotal least squares estimator for N-dimensional lines.\n\nIn contrast to ordinary least squares line estimation, this estimator\nminimizes the orthogonal distances of points to the estimated line.\n\nLines are defined by a point (origin) and a unit vector (direction) according\nto the following vector equation:\n\nLine model parameters in the following order `origin`, `direction`.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nEstimate line model from data.\n\nThis minimizes the sum of shortest (orthogonal) distances from the given data\npoints to the estimated line.\n\nN points in a space of dimensionality dim >= 2.\n\nTrue, if model estimation succeeds.\n\nPredict intersection of the estimated line model with a hyperplane orthogonal\nto a given axis.\n\nCoordinates along an axis.\n\nAxis orthogonal to the hyperplane intersecting the line.\n\nOptional custom parameter set in the form (`origin`, `direction`).\n\nPredicted coordinates.\n\nIf the line is parallel to the given axis.\n\nPredict x-coordinates for 2D lines using the estimated model.\n\nAlias for:\n\ny-coordinates.\n\nOptional custom parameter set in the form (`origin`, `direction`).\n\nPredicted x-coordinates.\n\nPredict y-coordinates for 2D lines using the estimated model.\n\nAlias for:\n\nx-coordinates.\n\nOptional custom parameter set in the form (`origin`, `direction`).\n\nPredicted y-coordinates.\n\nDetermine residuals of data to model.\n\nFor each point, the shortest (orthogonal) distance to the line is returned. It\nis obtained by projecting the data onto the line.\n\nN points in a space of dimension dim.\n\nOptional custom parameter set in the form (`origin`, `direction`).\n\nResidual for each data point.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.approximate_polygon()", "path": "api/skimage.measure#skimage.measure.approximate_polygon", "type": "measure", "text": "\nApproximate a polygonal chain with the specified tolerance.\n\nIt is based on the Douglas-Peucker algorithm.\n\nNote that the approximated polygon is always within the convex hull of the\noriginal polygon.\n\nCoordinate array.\n\nMaximum distance from original points of polygon to approximated polygonal\nchain. If tolerance is 0, the original coordinate array is returned.\n\nApproximated polygonal chain where M <= N.\n\nhttps://en.wikipedia.org/wiki/Ramer-Douglas-Peucker_algorithm\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.block_reduce()", "path": "api/skimage.measure#skimage.measure.block_reduce", "type": "measure", "text": "\nDownsample image by applying function `func` to local blocks.\n\nThis function is useful for max and mean pooling, for example.\n\nN-dimensional input image.\n\nArray containing down-sampling integer factor along each axis.\n\nFunction object which is used to calculate the return value for each local\nblock. This function must implement an `axis` parameter. Primary functions are\n`numpy.sum`, `numpy.min`, `numpy.max`, `numpy.mean` and `numpy.median`. See\nalso `func_kwargs`.\n\nConstant padding value if image is not perfectly divisible by the block size.\n\nKeyword arguments passed to `func`. Notably useful for passing dtype argument\nto `np.mean`. Takes dictionary of inputs, e.g.: `func_kwargs={'dtype':\nnp.float16})`.\n\nDown-sampled image with same number of dimensions as input image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.CircleModel", "path": "api/skimage.measure#skimage.measure.CircleModel", "type": "measure", "text": "\nBases: `skimage.measure.fit.BaseModel`\n\nTotal least squares estimator for 2D circles.\n\nThe functional model of the circle is:\n\nThis estimator minimizes the squared distances from all points to the circle:\n\nA minimum number of 3 points is required to solve for the parameters.\n\nCircle model parameters in the following order `xc`, `yc`, `r`.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nEstimate circle model from data using total least squares.\n\nN points with `(x, y)` coordinates, respectively.\n\nTrue, if model estimation succeeds.\n\nPredict x- and y-coordinates using the estimated model.\n\nAngles in circle in radians. Angles start to count from positive x-axis to\npositive y-axis in a right-handed system.\n\nOptional custom parameter set.\n\nPredicted x- and y-coordinates.\n\nDetermine residuals of data to model.\n\nFor each point the shortest distance to the circle is returned.\n\nN points with `(x, y)` coordinates, respectively.\n\nResidual for each data point.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.CircleModel.estimate()", "path": "api/skimage.measure#skimage.measure.CircleModel.estimate", "type": "measure", "text": "\nEstimate circle model from data using total least squares.\n\nN points with `(x, y)` coordinates, respectively.\n\nTrue, if model estimation succeeds.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.CircleModel.predict_xy()", "path": "api/skimage.measure#skimage.measure.CircleModel.predict_xy", "type": "measure", "text": "\nPredict x- and y-coordinates using the estimated model.\n\nAngles in circle in radians. Angles start to count from positive x-axis to\npositive y-axis in a right-handed system.\n\nOptional custom parameter set.\n\nPredicted x- and y-coordinates.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.CircleModel.residuals()", "path": "api/skimage.measure#skimage.measure.CircleModel.residuals", "type": "measure", "text": "\nDetermine residuals of data to model.\n\nFor each point the shortest distance to the circle is returned.\n\nN points with `(x, y)` coordinates, respectively.\n\nResidual for each data point.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.CircleModel.__init__()", "path": "api/skimage.measure#skimage.measure.CircleModel.__init__", "type": "measure", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.EllipseModel", "path": "api/skimage.measure#skimage.measure.EllipseModel", "type": "measure", "text": "\nBases: `skimage.measure.fit.BaseModel`\n\nTotal least squares estimator for 2D ellipses.\n\nThe functional model of the ellipse is:\n\nwhere `(xt, yt)` is the closest point on the ellipse to `(x, y)`. Thus d is\nthe shortest distance from the point to the ellipse.\n\nThe estimator is based on a least squares minimization. The optimal solution\nis computed directly, no iterations are required. This leads to a simple,\nstable and robust fitting method.\n\nThe `params` attribute contains the parameters in the following order:\n\nEllipse model parameters in the following order `xc`, `yc`, `a`, `b`, `theta`.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nEstimate circle model from data using total least squares.\n\nN points with `(x, y)` coordinates, respectively.\n\nTrue, if model estimation succeeds.\n\nHalir, R.; Flusser, J. \u201cNumerically stable direct least squares fitting of\nellipses\u201d. In Proc. 6th International Conference in Central Europe on Computer\nGraphics and Visualization. WSCG (Vol. 98, pp. 125-132).\n\nPredict x- and y-coordinates using the estimated model.\n\nAngles in circle in radians. Angles start to count from positive x-axis to\npositive y-axis in a right-handed system.\n\nOptional custom parameter set.\n\nPredicted x- and y-coordinates.\n\nDetermine residuals of data to model.\n\nFor each point the shortest distance to the ellipse is returned.\n\nN points with `(x, y)` coordinates, respectively.\n\nResidual for each data point.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.EllipseModel.estimate()", "path": "api/skimage.measure#skimage.measure.EllipseModel.estimate", "type": "measure", "text": "\nEstimate circle model from data using total least squares.\n\nN points with `(x, y)` coordinates, respectively.\n\nTrue, if model estimation succeeds.\n\nHalir, R.; Flusser, J. \u201cNumerically stable direct least squares fitting of\nellipses\u201d. In Proc. 6th International Conference in Central Europe on Computer\nGraphics and Visualization. WSCG (Vol. 98, pp. 125-132).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.EllipseModel.predict_xy()", "path": "api/skimage.measure#skimage.measure.EllipseModel.predict_xy", "type": "measure", "text": "\nPredict x- and y-coordinates using the estimated model.\n\nAngles in circle in radians. Angles start to count from positive x-axis to\npositive y-axis in a right-handed system.\n\nOptional custom parameter set.\n\nPredicted x- and y-coordinates.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.EllipseModel.residuals()", "path": "api/skimage.measure#skimage.measure.EllipseModel.residuals", "type": "measure", "text": "\nDetermine residuals of data to model.\n\nFor each point the shortest distance to the ellipse is returned.\n\nN points with `(x, y)` coordinates, respectively.\n\nResidual for each data point.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.EllipseModel.__init__()", "path": "api/skimage.measure#skimage.measure.EllipseModel.__init__", "type": "measure", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.euler_number()", "path": "api/skimage.measure#skimage.measure.euler_number", "type": "measure", "text": "\nCalculate the Euler characteristic in binary image.\n\nFor 2D objects, the Euler number is the number of objects minus the number of\nholes. For 3D objects, the Euler number is obtained as the number of objects\nplus the number of holes, minus the number of tunnels, or loops.\n\n2D or 3D images. If image is not binary, all values strictly greater than zero\nare considered as the object.\n\nMaximum number of orthogonal hops to consider a pixel/voxel as a neighbor.\nAccepted values are ranging from 1 to input.ndim. If `None`, a full\nconnectivity of `input.ndim` is used. 4 or 8 neighborhoods are defined for 2D\nimages (connectivity 1 and 2, respectively). 6 or 26 neighborhoods are defined\nfor 3D images, (connectivity 1 and 3, respectively). Connectivity 2 is not\ndefined.\n\nEuler characteristic of the set of all objects in the image.\n\nThe Euler characteristic is an integer number that describes the topology of\nthe set of all objects in the input image. If object is 4-connected, then\nbackground is 8-connected, and conversely.\n\nThe computation of the Euler characteristic is based on an integral geometry\nformula in discretized space. In practice, a neighbourhood configuration is\nconstructed, and a LUT is applied for each configuration. The coefficients\nused are the ones of Ohser et al.\n\nIt can be useful to compute the Euler characteristic for several\nconnectivities. A large relative difference between results for different\nconnectivities suggests that the image resolution (with respect to the size of\nobjects and holes) is too low.\n\nS. Rivollier. Analyse d\u2019image geometrique et morphometrique par diagrammes de\nforme et voisinages adaptatifs generaux. PhD thesis, 2010. Ecole Nationale\nSuperieure des Mines de Saint-Etienne. https://tel.archives-\nouvertes.fr/tel-00560838\n\nOhser J., Nagel W., Schladitz K. (2002) The Euler Number of Discretized Sets -\nOn the Choice of Adjacency in Homogeneous Lattices. In: Mecke K., Stoyan D.\n(eds) Morphology of Condensed Matter. Lecture Notes in Physics, vol 600.\nSpringer, Berlin, Heidelberg.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.find_contours()", "path": "api/skimage.measure#skimage.measure.find_contours", "type": "measure", "text": "\nFind iso-valued contours in a 2D array for a given level value.\n\nUses the \u201cmarching squares\u201d method to compute a the iso-valued contours of the\ninput 2D array for a particular level value. Array values are linearly\ninterpolated to provide better precision for the output contours.\n\nInput image in which to find contours.\n\nValue along which to find contours in the array. By default, the level is set\nto (max(image) + min(image)) / 2\n\nChanged in version 0.18: This parameter is now optional.\n\nIndicates whether array elements below the given level value are to be\nconsidered fully-connected (and hence elements above the value will only be\nface connected), or vice-versa. (See notes below for details.)\n\nIndicates whether the output contours will produce positively-oriented\npolygons around islands of low- or high-valued elements. If \u2018low\u2019 then\ncontours will wind counter- clockwise around elements below the iso-value.\nAlternately, this means that low-valued elements are always on the left of the\ncontour. (See below for details.)\n\nA boolean mask, True where we want to draw contours. Note that NaN values are\nalways excluded from the considered region (`mask` is set to `False` wherever\n`array` is `NaN`).\n\nEach contour is an ndarray of shape `(n, 2)`, consisting of n `(row, column)`\ncoordinates along the contour.\n\nSee also\n\nThe marching squares algorithm is a special case of the marching cubes\nalgorithm [1]. A simple explanation is available here:\n\nhttp://users.polytech.unice.fr/~lingrand/MarchingCubes/algo.html\n\nThere is a single ambiguous case in the marching squares algorithm: when a\ngiven `2 x 2`-element square has two high-valued and two low-valued elements,\neach pair diagonally adjacent. (Where high- and low-valued is with respect to\nthe contour value sought.) In this case, either the high-valued elements can\nbe \u2018connected together\u2019 via a thin isthmus that separates the low-valued\nelements, or vice-versa. When elements are connected together across a\ndiagonal, they are considered \u2018fully connected\u2019 (also known as \u2018face+vertex-\nconnected\u2019 or \u20188-connected\u2019). Only high-valued or low-valued elements can be\nfully-connected, the other set will be considered as \u2018face-connected\u2019 or\n\u20184-connected\u2019. By default, low-valued elements are considered fully-connected;\nthis can be altered with the \u2018fully_connected\u2019 parameter.\n\nOutput contours are not guaranteed to be closed: contours which intersect the\narray edge or a masked-off region (either where mask is False or where array\nis NaN) will be left open. All other contours will be closed. (The closed-ness\nof a contours can be tested by checking whether the beginning point is the\nsame as the end point.)\n\nContours are oriented. By default, array values lower than the contour value\nare to the left of the contour and values greater than the contour value are\nto the right. This means that contours will wind counter-clockwise (i.e. in\n\u2018positive orientation\u2019) around islands of low-valued pixels. This behavior can\nbe altered with the \u2018positive_orientation\u2019 parameter.\n\nThe order of the contours in the output list is determined by the position of\nthe smallest `x,y` (in lexicographical order) coordinate in the contour. This\nis a side-effect of how the input array is traversed, but can be relied upon.\n\nWarning\n\nArray coordinates/values are assumed to refer to the center of the array\nelement. Take a simple example input: `[0, 1]`. The interpolated position of\n0.5 in this array is midway between the 0-element (at `x=0`) and the 1-element\n(at `x=1`), and thus would fall at `x=0.5`.\n\nThis means that to find reasonable contours, it is best to find contours\nmidway between the expected \u201clight\u201d and \u201cdark\u201d values. In particular, given a\nbinarized array, do not choose to find contours at the low or high value of\nthe array. This will often yield degenerate contours, especially around\nstructures that are a single array element wide. Instead choose a middle\nvalue, as above.\n\nLorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D\nSurface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings)\n21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.grid_points_in_poly()", "path": "api/skimage.measure#skimage.measure.grid_points_in_poly", "type": "measure", "text": "\nTest whether points on a specified grid are inside a polygon.\n\nFor each `(r, c)` coordinate on a grid, i.e. `(0, 0)`, `(0, 1)` etc., test\nwhether that point lies inside a polygon.\n\nShape of the grid.\n\nSpecify the V vertices of the polygon, sorted either clockwise or anti-\nclockwise. The first point may (but does not need to be) duplicated.\n\nTrue where the grid falls inside the polygon.\n\nSee also\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.inertia_tensor()", "path": "api/skimage.measure#skimage.measure.inertia_tensor", "type": "measure", "text": "\nCompute the inertia tensor of the input image.\n\nThe input image.\n\nThe pre-computed central moments of `image`. The inertia tensor computation\nrequires the central moments of the image. If an application requires both the\ncentral moments and the inertia tensor (for example,\n`skimage.measure.regionprops`), then it is more efficient to pre-compute them\nand pass them to the inertia tensor call.\n\nThe inertia tensor of the input image. \\\\(T_{i, j}\\\\) contains the covariance\nof image intensity along axes \\\\(i\\\\) and \\\\(j\\\\).\n\nhttps://en.wikipedia.org/wiki/Moment_of_inertia#Inertia_tensor\n\nBernd J\u00e4hne. Spatio-Temporal Image Processing: Theory and Scientific\nApplications. (Chapter 8: Tensor Methods) Springer, 1993.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.inertia_tensor_eigvals()", "path": "api/skimage.measure#skimage.measure.inertia_tensor_eigvals", "type": "measure", "text": "\nCompute the eigenvalues of the inertia tensor of the image.\n\nThe inertia tensor measures covariance of the image intensity along the image\naxes. (See `inertia_tensor`.) The relative magnitude of the eigenvalues of the\ntensor is thus a measure of the elongation of a (bright) object in the image.\n\nThe input image.\n\nThe pre-computed central moments of `image`.\n\nThe pre-computed inertia tensor. If `T` is given, `mu` and `image` are\nignored.\n\nThe eigenvalues of the inertia tensor of `image`, in descending order.\n\nComputing the eigenvalues requires the inertia tensor of the input image. This\nis much faster if the central moments (`mu`) are provided, or, alternatively,\none can provide the inertia tensor (`T`) directly.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.label()", "path": "api/skimage.measure#skimage.measure.label", "type": "measure", "text": "\nLabel connected regions of an integer array.\n\nTwo pixels are connected when they are neighbors and have the same value. In\n2D, they can be neighbors either in a 1- or 2-connected sense. The value\nrefers to the maximum number of orthogonal hops to consider a pixel/voxel a\nneighbor:\n\nImage to label.\n\nConsider all pixels with this value as background pixels, and label them as 0.\nBy default, 0-valued pixels are considered as background pixels.\n\nWhether to return the number of assigned labels.\n\nMaximum number of orthogonal hops to consider a pixel/voxel as a neighbor.\nAccepted values are ranging from 1 to input.ndim. If `None`, a full\nconnectivity of `input.ndim` is used.\n\nLabeled array, where all connected regions are assigned the same integer\nvalue.\n\nNumber of labels, which equals the maximum label index and is only returned if\nreturn_num is `True`.\n\nSee also\n\nChristophe Fiorio and Jens Gustedt, \u201cTwo linear time Union-Find strategies for\nimage processing\u201d, Theoretical Computer Science 154 (1996), pp. 165-181.\n\nKensheng Wu, Ekow Otoo and Arie Shoshani, \u201cOptimizing connected component\nlabeling algorithms\u201d, Paper LBNL-56864, 2005, Lawrence Berkeley National\nLaboratory (University of California),\nhttp://repositories.cdlib.org/lbnl/LBNL-56864\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.LineModelND", "path": "api/skimage.measure#skimage.measure.LineModelND", "type": "measure", "text": "\nBases: `skimage.measure.fit.BaseModel`\n\nTotal least squares estimator for N-dimensional lines.\n\nIn contrast to ordinary least squares line estimation, this estimator\nminimizes the orthogonal distances of points to the estimated line.\n\nLines are defined by a point (origin) and a unit vector (direction) according\nto the following vector equation:\n\nLine model parameters in the following order `origin`, `direction`.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nEstimate line model from data.\n\nThis minimizes the sum of shortest (orthogonal) distances from the given data\npoints to the estimated line.\n\nN points in a space of dimensionality dim >= 2.\n\nTrue, if model estimation succeeds.\n\nPredict intersection of the estimated line model with a hyperplane orthogonal\nto a given axis.\n\nCoordinates along an axis.\n\nAxis orthogonal to the hyperplane intersecting the line.\n\nOptional custom parameter set in the form (`origin`, `direction`).\n\nPredicted coordinates.\n\nIf the line is parallel to the given axis.\n\nPredict x-coordinates for 2D lines using the estimated model.\n\nAlias for:\n\ny-coordinates.\n\nOptional custom parameter set in the form (`origin`, `direction`).\n\nPredicted x-coordinates.\n\nPredict y-coordinates for 2D lines using the estimated model.\n\nAlias for:\n\nx-coordinates.\n\nOptional custom parameter set in the form (`origin`, `direction`).\n\nPredicted y-coordinates.\n\nDetermine residuals of data to model.\n\nFor each point, the shortest (orthogonal) distance to the line is returned. It\nis obtained by projecting the data onto the line.\n\nN points in a space of dimension dim.\n\nOptional custom parameter set in the form (`origin`, `direction`).\n\nResidual for each data point.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.LineModelND.estimate()", "path": "api/skimage.measure#skimage.measure.LineModelND.estimate", "type": "measure", "text": "\nEstimate line model from data.\n\nThis minimizes the sum of shortest (orthogonal) distances from the given data\npoints to the estimated line.\n\nN points in a space of dimensionality dim >= 2.\n\nTrue, if model estimation succeeds.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.LineModelND.predict()", "path": "api/skimage.measure#skimage.measure.LineModelND.predict", "type": "measure", "text": "\nPredict intersection of the estimated line model with a hyperplane orthogonal\nto a given axis.\n\nCoordinates along an axis.\n\nAxis orthogonal to the hyperplane intersecting the line.\n\nOptional custom parameter set in the form (`origin`, `direction`).\n\nPredicted coordinates.\n\nIf the line is parallel to the given axis.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.LineModelND.predict_x()", "path": "api/skimage.measure#skimage.measure.LineModelND.predict_x", "type": "measure", "text": "\nPredict x-coordinates for 2D lines using the estimated model.\n\nAlias for:\n\ny-coordinates.\n\nOptional custom parameter set in the form (`origin`, `direction`).\n\nPredicted x-coordinates.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.LineModelND.predict_y()", "path": "api/skimage.measure#skimage.measure.LineModelND.predict_y", "type": "measure", "text": "\nPredict y-coordinates for 2D lines using the estimated model.\n\nAlias for:\n\nx-coordinates.\n\nOptional custom parameter set in the form (`origin`, `direction`).\n\nPredicted y-coordinates.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.LineModelND.residuals()", "path": "api/skimage.measure#skimage.measure.LineModelND.residuals", "type": "measure", "text": "\nDetermine residuals of data to model.\n\nFor each point, the shortest (orthogonal) distance to the line is returned. It\nis obtained by projecting the data onto the line.\n\nN points in a space of dimension dim.\n\nOptional custom parameter set in the form (`origin`, `direction`).\n\nResidual for each data point.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.LineModelND.__init__()", "path": "api/skimage.measure#skimage.measure.LineModelND.__init__", "type": "measure", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.marching_cubes()", "path": "api/skimage.measure#skimage.measure.marching_cubes", "type": "measure", "text": "\nMarching cubes algorithm to find surfaces in 3d volumetric data.\n\nIn contrast with Lorensen et al. approach [2], Lewiner et al. algorithm is\nfaster, resolves ambiguities, and guarantees topologically correct results.\nTherefore, this algorithm generally a better choice.\n\nInput data volume to find isosurfaces. Will internally be converted to float32\nif necessary.\n\nContour value to search for isosurfaces in `volume`. If not given or None, the\naverage of the min and max of vol is used.\n\nVoxel spacing in spatial dimensions corresponding to numpy array indexing\ndimensions (M, N, P) as in `volume`.\n\nControls if the mesh was generated from an isosurface with gradient descent\ntoward objects of interest (the default), or the opposite, considering the\nleft-hand rule. The two options are: * descent : Object was greater than\nexterior * ascent : Exterior was greater than object\n\nStep size in voxels. Default 1. Larger steps yield faster but coarser results.\nThe result will always be topologically correct though.\n\nWhether to allow degenerate (i.e. zero-area) triangles in the end-result.\nDefault True. If False, degenerate triangles are removed, at the cost of\nmaking the algorithm slower.\n\nOne of \u2018lewiner\u2019, \u2018lorensen\u2019 or \u2018_lorensen\u2019. Specify witch of Lewiner et al.\nor Lorensen et al. method will be used. The \u2018_lorensen\u2019 flag correspond to an\nold implementation that will be deprecated in version 0.19.\n\nBoolean array. The marching cube algorithm will be computed only on True\nelements. This will save computational time when interfaces are located within\ncertain region of the volume M, N, P-e.g. the top half of the cube-and also\nallow to compute finite surfaces-i.e. open surfaces that do not end at the\nborder of the cube.\n\nSpatial coordinates for V unique mesh vertices. Coordinate order matches input\n`volume` (M, N, P). If `allow_degenerate` is set to True, then the presence of\ndegenerate triangles in the mesh can make this array have duplicate vertices.\n\nDefine triangular faces via referencing vertex indices from `verts`. This\nalgorithm specifically outputs triangles, so each face has exactly three\nindices.\n\nThe normal direction at each vertex, as calculated from the data.\n\nGives a measure for the maximum value of the data in the local region near\neach vertex. This can be used by visualization tools to apply a colormap to\nthe mesh.\n\nSee also\n\nThe algorithm [1] is an improved version of Chernyaev\u2019s Marching Cubes 33\nalgorithm. It is an efficient algorithm that relies on heavy use of lookup\ntables to handle the many different cases, keeping the algorithm relatively\neasy. This implementation is written in Cython, ported from Lewiner\u2019s C++\nimplementation.\n\nTo quantify the area of an isosurface generated by this algorithm, pass verts\nand faces to `skimage.measure.mesh_surface_area`.\n\nRegarding visualization of algorithm output, to contour a volume named\n`myvolume` about the level 0.0, using the `mayavi` package:\n\nSimilarly using the `visvis` package:\n\nTo reduce the number of triangles in the mesh for better performance, see this\nexample using the `mayavi` package.\n\nThomas Lewiner, Helio Lopes, Antonio Wilson Vieira and Geovan Tavares.\nEfficient implementation of Marching Cubes\u2019 cases with topological guarantees.\nJournal of Graphics Tools 8(2) pp. 1-15 (december 2003).\nDOI:10.1080/10867651.2003.10487582\n\nLorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D\nSurface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings)\n21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.marching_cubes_classic()", "path": "api/skimage.measure#skimage.measure.marching_cubes_classic", "type": "measure", "text": "\nClassic marching cubes algorithm to find surfaces in 3d volumetric data.\n\nNote that the `marching_cubes()` algorithm is recommended over this algorithm,\nbecause it\u2019s faster and produces better results.\n\nInput data volume to find isosurfaces. Will be cast to `np.float64`.\n\nContour value to search for isosurfaces in `volume`. If not given or None, the\naverage of the min and max of vol is used.\n\nVoxel spacing in spatial dimensions corresponding to numpy array indexing\ndimensions (M, N, P) as in `volume`.\n\nControls if the mesh was generated from an isosurface with gradient descent\ntoward objects of interest (the default), or the opposite. The two options\nare: * descent : Object was greater than exterior * ascent : Exterior was\ngreater than object\n\nSpatial coordinates for V unique mesh vertices. Coordinate order matches input\n`volume` (M, N, P). If `allow_degenerate` is set to True, then the presence of\ndegenerate triangles in the mesh can make this array have duplicate vertices.\n\nDefine triangular faces via referencing vertex indices from `verts`. This\nalgorithm specifically outputs triangles, so each face has exactly three\nindices.\n\nSee also\n\nThe marching cubes algorithm is implemented as described in [1]. A simple\nexplanation is available here:\n\nThere are several known ambiguous cases in the marching cubes algorithm. Using\npoint labeling as in [1], Figure 4, as shown:\n\nMost notably, if v4, v8, v2, and v6 are all >= `level` (or any generalization\nof this case) two parallel planes are generated by this algorithm, separating\nv4 and v8 from v2 and v6. An equally valid interpretation would be a single\nconnected thin surface enclosing all four points. This is the best known\nambiguity, though there are others.\n\nThis algorithm does not attempt to resolve such ambiguities; it is a naive\nimplementation of marching cubes as in [1], but may be a good beginning for\nwork with more recent techniques (Dual Marching Cubes, Extended Marching\nCubes, Cubic Marching Squares, etc.).\n\nBecause of interactions between neighboring cubes, the isosurface(s) generated\nby this algorithm are NOT guaranteed to be closed, particularly for\ncomplicated contours. Furthermore, this algorithm does not guarantee a single\ncontour will be returned. Indeed, ALL isosurfaces which cross `level` will be\nfound, regardless of connectivity.\n\nThe output is a triangular mesh consisting of a set of unique vertices and\nconnecting triangles. The order of these vertices and triangles in the output\nlist is determined by the position of the smallest `x,y,z` (in lexicographical\norder) coordinate in the contour. This is a side-effect of how the input array\nis traversed, but can be relied upon.\n\nThe generated mesh guarantees coherent orientation as of version 0.12.\n\nTo quantify the area of an isosurface generated by this algorithm, pass\noutputs directly into `skimage.measure.mesh_surface_area`.\n\nLorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D\nSurface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings)\n21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.marching_cubes_lewiner()", "path": "api/skimage.measure#skimage.measure.marching_cubes_lewiner", "type": "measure", "text": "\nLewiner marching cubes algorithm to find surfaces in 3d volumetric data.\n\nIn contrast to `marching_cubes_classic()`, this algorithm is faster, resolves\nambiguities, and guarantees topologically correct results. Therefore, this\nalgorithm generally a better choice, unless there is a specific need for the\nclassic algorithm.\n\nInput data volume to find isosurfaces. Will internally be converted to float32\nif necessary.\n\nContour value to search for isosurfaces in `volume`. If not given or None, the\naverage of the min and max of vol is used.\n\nVoxel spacing in spatial dimensions corresponding to numpy array indexing\ndimensions (M, N, P) as in `volume`.\n\nControls if the mesh was generated from an isosurface with gradient descent\ntoward objects of interest (the default), or the opposite, considering the\nleft-hand rule. The two options are: * descent : Object was greater than\nexterior * ascent : Exterior was greater than object\n\nStep size in voxels. Default 1. Larger steps yield faster but coarser results.\nThe result will always be topologically correct though.\n\nWhether to allow degenerate (i.e. zero-area) triangles in the end-result.\nDefault True. If False, degenerate triangles are removed, at the cost of\nmaking the algorithm slower.\n\nIf given and True, the classic marching cubes by Lorensen (1987) is used. This\noption is included for reference purposes. Note that this algorithm has\nambiguities and is not guaranteed to produce a topologically correct result.\nThe results with using this option are not generally the same as the\n`marching_cubes_classic()` function.\n\nBoolean array. The marching cube algorithm will be computed only on True\nelements. This will save computational time when interfaces are located within\ncertain region of the volume M, N, P-e.g. the top half of the cube-and also\nallow to compute finite surfaces-i.e. open surfaces that do not end at the\nborder of the cube.\n\nSpatial coordinates for V unique mesh vertices. Coordinate order matches input\n`volume` (M, N, P). If `allow_degenerate` is set to True, then the presence of\ndegenerate triangles in the mesh can make this array have duplicate vertices.\n\nDefine triangular faces via referencing vertex indices from `verts`. This\nalgorithm specifically outputs triangles, so each face has exactly three\nindices.\n\nThe normal direction at each vertex, as calculated from the data.\n\nGives a measure for the maximum value of the data in the local region near\neach vertex. This can be used by visualization tools to apply a colormap to\nthe mesh.\n\nSee also\n\nThe algorithm [1] is an improved version of Chernyaev\u2019s Marching Cubes 33\nalgorithm. It is an efficient algorithm that relies on heavy use of lookup\ntables to handle the many different cases, keeping the algorithm relatively\neasy. This implementation is written in Cython, ported from Lewiner\u2019s C++\nimplementation.\n\nTo quantify the area of an isosurface generated by this algorithm, pass verts\nand faces to `skimage.measure.mesh_surface_area`.\n\nRegarding visualization of algorithm output, to contour a volume named\n`myvolume` about the level 0.0, using the `mayavi` package:\n\nSimilarly using the `visvis` package:\n\nThomas Lewiner, Helio Lopes, Antonio Wilson Vieira and Geovan Tavares.\nEfficient implementation of Marching Cubes\u2019 cases with topological guarantees.\nJournal of Graphics Tools 8(2) pp. 1-15 (december 2003).\nDOI:10.1080/10867651.2003.10487582\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.mesh_surface_area()", "path": "api/skimage.measure#skimage.measure.mesh_surface_area", "type": "measure", "text": "\nCompute surface area, given vertices & triangular faces\n\nArray containing (x, y, z) coordinates for V unique mesh vertices.\n\nList of length-3 lists of integers, referencing vertex coordinates as provided\nin `verts`\n\nSurface area of mesh. Units now [coordinate units] ** 2.\n\nSee also\n\nThe arguments expected by this function are the first two outputs from\n`skimage.measure.marching_cubes`. For unit correct output, ensure correct\n`spacing` was passed to `skimage.measure.marching_cubes`.\n\nThis algorithm works properly only if the `faces` provided are all triangles.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.moments()", "path": "api/skimage.measure#skimage.measure.moments", "type": "measure", "text": "\nCalculate all raw image moments up to a certain order.\n\nNote that raw moments are neither translation, scale nor rotation invariant.\n\nRasterized shape as image.\n\nMaximum order of moments. Default is 3.\n\nRaw image moments.\n\nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core\nAlgorithms. Springer-Verlag, London, 2009.\n\nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6.\nedition, 2005.\n\nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from\nLecture notes in computer science, p. 676. Springer, Berlin, 1993.\n\nhttps://en.wikipedia.org/wiki/Image_moment\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.moments_central()", "path": "api/skimage.measure#skimage.measure.moments_central", "type": "measure", "text": "\nCalculate all central image moments up to a certain order.\n\nThe center coordinates (cr, cc) can be calculated from the raw moments as:\n{`M[1, 0] / M[0, 0]`, `M[0, 1] / M[0, 0]`}.\n\nNote that central moments are translation invariant but not scale and rotation\ninvariant.\n\nRasterized shape as image.\n\nCoordinates of the image centroid. This will be computed if it is not\nprovided.\n\nThe maximum order of moments computed.\n\nCentral image moments.\n\nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core\nAlgorithms. Springer-Verlag, London, 2009.\n\nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6.\nedition, 2005.\n\nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from\nLecture notes in computer science, p. 676. Springer, Berlin, 1993.\n\nhttps://en.wikipedia.org/wiki/Image_moment\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.moments_coords()", "path": "api/skimage.measure#skimage.measure.moments_coords", "type": "measure", "text": "\nCalculate all raw image moments up to a certain order.\n\nNote that raw moments are neither translation, scale nor rotation invariant.\n\nArray of N points that describe an image of D dimensionality in Cartesian\nspace.\n\nMaximum order of moments. Default is 3.\n\nRaw image moments. (D dimensions)\n\nJohannes Kilian. Simple Image Analysis By Moments. Durham University, version\n0.2, Durham, 2001.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.moments_coords_central()", "path": "api/skimage.measure#skimage.measure.moments_coords_central", "type": "measure", "text": "\nCalculate all central image moments up to a certain order.\n\nNote that raw moments are neither translation, scale nor rotation invariant.\n\nArray of N points that describe an image of D dimensionality in Cartesian\nspace. A tuple of coordinates as returned by `np.nonzero` is also accepted as\ninput.\n\nCoordinates of the image centroid. This will be computed if it is not\nprovided.\n\nMaximum order of moments. Default is 3.\n\nCentral image moments. (D dimensions)\n\nJohannes Kilian. Simple Image Analysis By Moments. Durham University, version\n0.2, Durham, 2001.\n\nAs seen above, for symmetric objects, odd-order moments (columns 1 and 3, rows\n1 and 3) are zero when centered on the centroid, or center of mass, of the\nobject (the default). If we break the symmetry by adding a new point, this no\nlonger holds:\n\nImage moments and central image moments are equivalent (by definition) when\nthe center is (0, 0):\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.moments_hu()", "path": "api/skimage.measure#skimage.measure.moments_hu", "type": "measure", "text": "\nCalculate Hu\u2019s set of image moments (2D-only).\n\nNote that this set of moments is proofed to be translation, scale and rotation\ninvariant.\n\nNormalized central image moments, where M must be >= 4.\n\nHu\u2019s set of image moments.\n\nM. K. Hu, \u201cVisual Pattern Recognition by Moment Invariants\u201d, IRE Trans. Info.\nTheory, vol. IT-8, pp. 179-187, 1962\n\nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core\nAlgorithms. Springer-Verlag, London, 2009.\n\nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6.\nedition, 2005.\n\nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from\nLecture notes in computer science, p. 676. Springer, Berlin, 1993.\n\nhttps://en.wikipedia.org/wiki/Image_moment\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.moments_normalized()", "path": "api/skimage.measure#skimage.measure.moments_normalized", "type": "measure", "text": "\nCalculate all normalized central image moments up to a certain order.\n\nNote that normalized central moments are translation and scale invariant but\nnot rotation invariant.\n\nCentral image moments, where M must be greater than or equal to `order`.\n\nMaximum order of moments. Default is 3.\n\nNormalized central image moments.\n\nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core\nAlgorithms. Springer-Verlag, London, 2009.\n\nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6.\nedition, 2005.\n\nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from\nLecture notes in computer science, p. 676. Springer, Berlin, 1993.\n\nhttps://en.wikipedia.org/wiki/Image_moment\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.perimeter()", "path": "api/skimage.measure#skimage.measure.perimeter", "type": "measure", "text": "\nCalculate total perimeter of all objects in binary image.\n\n2D binary image.\n\nNeighborhood connectivity for border pixel determination. It is used to\ncompute the contour. A higher neighbourhood widens the border on which the\nperimeter is computed.\n\nTotal perimeter of all objects in binary image.\n\nK. Benkrid, D. Crookes. Design and FPGA Implementation of a Perimeter\nEstimator. The Queen\u2019s University of Belfast.\nhttp://www.cs.qub.ac.uk/~d.crookes/webpubs/papers/perimeter.doc\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.perimeter_crofton()", "path": "api/skimage.measure#skimage.measure.perimeter_crofton", "type": "measure", "text": "\nCalculate total Crofton perimeter of all objects in binary image.\n\n2D image. If image is not binary, all values strictly greater than zero are\nconsidered as the object.\n\nNumber of directions used to approximate the Crofton perimeter. By default, 4\nis used: it should be more accurate than 2. Computation time is the same in\nboth cases.\n\nTotal perimeter of all objects in binary image.\n\nThis measure is based on Crofton formula [1], which is a measure from integral\ngeometry. It is defined for general curve length evaluation via a double\nintegral along all directions. In a discrete space, 2 or 4 directions give a\nquite good approximation, 4 being more accurate than 2 for more complex\nshapes.\n\nSimilar to `perimeter()`, this function returns an approximation of the\nperimeter in continuous space.\n\nhttps://en.wikipedia.org/wiki/Crofton_formula\n\nS. Rivollier. Analyse d\u2019image geometrique et morphometrique par diagrammes de\nforme et voisinages adaptatifs generaux. PhD thesis, 2010. Ecole Nationale\nSuperieure des Mines de Saint-Etienne. https://tel.archives-\nouvertes.fr/tel-00560838\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.points_in_poly()", "path": "api/skimage.measure#skimage.measure.points_in_poly", "type": "measure", "text": "\nTest whether points lie inside a polygon.\n\nInput points, `(x, y)`.\n\nVertices of the polygon, sorted either clockwise or anti-clockwise. The first\npoint may (but does not need to be) duplicated.\n\nTrue if corresponding point is inside the polygon.\n\nSee also\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.profile_line()", "path": "api/skimage.measure#skimage.measure.profile_line", "type": "measure", "text": "\nReturn the intensity profile of an image measured along a scan line.\n\nThe image, either grayscale (2D array) or multichannel (3D array, where the\nfinal axis contains the channel information).\n\nThe coordinates of the start point of the scan line.\n\nThe coordinates of the end point of the scan line. The destination point is\nincluded in the profile, in contrast to standard numpy indexing.\n\nWidth of the scan, perpendicular to the line\n\nThe order of the spline interpolation, default is 0 if image.dtype is bool and\n1 otherwise. The order has to be in the range 0-5. See\n`skimage.transform.warp` for detail.\n\nHow to compute any values falling outside of the image.\n\nIf `mode` is \u2018constant\u2019, what constant value to use outside the image.\n\nFunction used to calculate the aggregation of pixel values perpendicular to\nthe profile_line direction when `linewidth` > 1\\. If set to None the unreduced\narray will be returned.\n\nThe intensity profile along the scan line. The length of the profile is the\nceil of the computed length of the scan line.\n\nThe destination point is included in the profile, in contrast to standard\nnumpy indexing. For example:\n\nFor different reduce_func inputs:\n\nThe unreduced array will be returned when `reduce_func` is None or when\n`reduce_func` acts on each pixel value individually.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.ransac()", "path": "api/skimage.measure#skimage.measure.ransac", "type": "measure", "text": "\nFit a model to data with the RANSAC (random sample consensus) algorithm.\n\nRANSAC is an iterative algorithm for the robust estimation of parameters from\na subset of inliers from the complete data set. Each iteration performs the\nfollowing tasks:\n\nThese steps are performed either a maximum number of times or until one of the\nspecial stop criteria are met. The final model is estimated using all inlier\nsamples of the previously determined best model.\n\nData set to which the model is fitted, where N is the number of data points\nand the remaining dimension are depending on model requirements. If the model\nclass requires multiple input data arrays (e.g. source and destination\ncoordinates of `skimage.transform.AffineTransform`), they can be optionally\npassed as tuple or list. Note, that in this case the functions\n`estimate(*data)`, `residuals(*data)`, `is_model_valid(model, *random_data)`\nand `is_data_valid(*random_data)` must all take each data array as separate\narguments.\n\nObject with the following object methods:\n\nwhere `success` indicates whether the model estimation succeeded (`True` or\n`None` for success, `False` for failure).\n\nThe minimum number of data points to fit a model to.\n\nMaximum distance for a data point to be classified as an inlier.\n\nThis function is called with the randomly selected data before the model is\nfitted to it: `is_data_valid(*random_data)`.\n\nThis function is called with the estimated model and the randomly selected\ndata: `is_model_valid(model, *random_data)`, .\n\nMaximum number of iterations for random sample selection.\n\nStop iteration if at least this number of inliers are found.\n\nStop iteration if sum of residuals is less than or equal to this threshold.\n\nRANSAC iteration stops if at least one outlier-free set of the training data\nis sampled with `probability >= stop_probability`, depending on the current\nbest model\u2019s inlier ratio and the number of trials. This requires to generate\nat least N samples (trials):\n\nN >= log(1 - probability) / log(1 - e**m)\n\nwhere the probability (confidence) is typically set to a high value such as\n0.99, e is the current fraction of inliers w.r.t. the total number of samples,\nand m is the min_samples value.\n\nIf int, random_state is the seed used by the random number generator; If\nRandomState instance, random_state is the random number generator; If None,\nthe random number generator is the RandomState instance used by `np.random`.\n\nInitial samples selection for model estimation\n\nBest model with largest consensus set.\n\nBoolean mask of inliers classified as `True`.\n\n\u201cRANSAC\u201d, Wikipedia, https://en.wikipedia.org/wiki/RANSAC\n\nGenerate ellipse data without tilt and add noise:\n\nAdd some faulty data:\n\nEstimate ellipse model using all available data:\n\nEstimate ellipse model using RANSAC:\n\nRANSAC can be used to robustly estimate a geometric transformation. In this\nsection, we also show how to use a proportion of the total samples, rather\nthan an absolute number.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.regionprops()", "path": "api/skimage.measure#skimage.measure.regionprops", "type": "measure", "text": "\nMeasure properties of labeled image regions.\n\nLabeled input image. Labels with value 0 are ignored.\n\nChanged in version 0.14.1: Previously, `label_image` was processed by\n`numpy.squeeze` and so any number of singleton dimensions was allowed. This\nresulted in inconsistent handling of images with singleton dimensions. To\nrecover the old behaviour, use `regionprops(np.squeeze(label_image), ...)`.\n\nIntensity (i.e., input) image with same size as labeled image, plus optionally\nan extra dimension for multichannel data. Default is None.\n\nChanged in version 0.18.0: The ability to provide an extra dimension for\nchannels was added.\n\nDetermine whether to cache calculated properties. The computation is much\nfaster for cached properties, whereas the memory consumption increases.\n\nThis argument is deprecated and will be removed in a future version of scikit-\nimage.\n\nSee Coordinate conventions for more details.\n\nDeprecated since version 0.16.0: Use \u201crc\u201d coordinates everywhere. It may be\nsufficient to call `numpy.transpose` on your label image to get the same\nvalues as 0.15 and earlier. However, for some properties, the transformation\nwill be less trivial. For example, the new orientation is \\\\(\\frac{\\pi}{2}\\\\)\nplus the old orientation.\n\nAdd extra property computation functions that are not included with skimage.\nThe name of the property is derived from the function name, the dtype is\ninferred by calling the function on a small sample. If the name of an extra\nproperty clashes with the name of an existing property the extra property wil\nnot be visible and a UserWarning is issued. A property computation function\nmust take a region mask as its first argument. If the property requires an\nintensity image, it must accept the intensity image as the second argument.\n\nEach item describes one labeled region, and can be accessed using the\nattributes listed below.\n\nSee also\n\nThe following properties can be accessed as attributes or keys:\n\nNumber of pixels of the region.\n\nBounding box `(min_row, min_col, max_row, max_col)`. Pixels belonging to the\nbounding box are in the half-open interval `[min_row; max_row)` and `[min_col;\nmax_col)`.\n\nNumber of pixels of bounding box.\n\nCentroid coordinate tuple `(row, col)`.\n\nNumber of pixels of convex hull image, which is the smallest convex polygon\nthat encloses the region.\n\nBinary convex hull image which has the same size as bounding box.\n\nCoordinate list `(row, col)` of the region.\n\nEccentricity of the ellipse that has the same second-moments as the region.\nThe eccentricity is the ratio of the focal distance (distance between focal\npoints) over the major axis length. The value is in the interval [0, 1). When\nit is 0, the ellipse becomes a circle.\n\nThe diameter of a circle with the same area as the region.\n\nEuler characteristic of the set of non-zero pixels. Computed as number of\nconnected components subtracted by number of holes (input.ndim connectivity).\nIn 3D, number of connected components plus number of holes subtracted by\nnumber of tunnels.\n\nRatio of pixels in the region to pixels in the total bounding box. Computed as\n`area / (rows * cols)`\n\nMaximum Feret\u2019s diameter computed as the longest distance between points\naround a region\u2019s convex hull contour as determined by `find_contours`. [5]\n\nNumber of pixels of the region will all the holes filled in. Describes the\narea of the filled_image.\n\nBinary region image with filled holes which has the same size as bounding box.\n\nSliced binary region image which has the same size as bounding box.\n\nInertia tensor of the region for the rotation around its mass.\n\nThe eigenvalues of the inertia tensor in decreasing order.\n\nImage inside region bounding box.\n\nThe label in the labeled input image.\n\nCentroid coordinate tuple `(row, col)`, relative to region bounding box.\n\nThe length of the major axis of the ellipse that has the same normalized\nsecond central moments as the region.\n\nValue with the greatest intensity in the region.\n\nValue with the mean intensity in the region.\n\nValue with the least intensity in the region.\n\nThe length of the minor axis of the ellipse that has the same normalized\nsecond central moments as the region.\n\nSpatial moments up to 3rd order:\n\nwhere the sum is over the `row`, `col` coordinates of the region.\n\nCentral moments (translation invariant) up to 3rd order:\n\nwhere the sum is over the `row`, `col` coordinates of the region, and `row_c`\nand `col_c` are the coordinates of the region\u2019s centroid.\n\nHu moments (translation, scale and rotation invariant).\n\nNormalized moments (translation and scale invariant) up to 3rd order:\n\nwhere `m_00` is the zeroth spatial moment.\n\nAngle between the 0th axis (rows) and the major axis of the ellipse that has\nthe same second moments as the region, ranging from `-pi/2` to `pi/2` counter-\nclockwise.\n\nPerimeter of object which approximates the contour as a line through the\ncenters of border pixels using a 4-connectivity.\n\nPerimeter of object approximated by the Crofton formula in 4 directions.\n\nA slice to extract the object from the source image.\n\nRatio of pixels in the region to pixels of the convex hull image.\n\nCentroid coordinate tuple `(row, col)` weighted with intensity image.\n\nCentroid coordinate tuple `(row, col)`, relative to region bounding box,\nweighted with intensity image.\n\nSpatial moments of intensity image up to 3rd order:\n\nwhere the sum is over the `row`, `col` coordinates of the region.\n\nCentral moments (translation invariant) of intensity image up to 3rd order:\n\nwhere the sum is over the `row`, `col` coordinates of the region, and `row_c`\nand `col_c` are the coordinates of the region\u2019s weighted centroid.\n\nHu moments (translation, scale and rotation invariant) of intensity image.\n\nNormalized moments (translation and scale invariant) of intensity image up to\n3rd order:\n\nwhere `wm_00` is the zeroth spatial moment (intensity-weighted area).\n\nEach region also supports iteration, so that you can do:\n\nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core\nAlgorithms. Springer-Verlag, London, 2009.\n\nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6.\nedition, 2005.\n\nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from\nLecture notes in computer science, p. 676. Springer, Berlin, 1993.\n\nhttps://en.wikipedia.org/wiki/Image_moment\n\nW. Pabst, E. Gregorov\u00e1. Characterization of particles and particle systems,\npp. 27-28. ICT Prague, 2007.\nhttps://old.vscht.cz/sil/keramika/Characterization_of_particles/CPPS%20_English%20version_.pdf\n\nAdd custom measurements by passing functions as `extra_properties`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.regionprops_table()", "path": "api/skimage.measure#skimage.measure.regionprops_table", "type": "measure", "text": "\nCompute image properties and return them as a pandas-compatible table.\n\nThe table is a dictionary mapping column names to value arrays. See Notes\nsection below for details.\n\nNew in version 0.16.\n\nLabeled input image. Labels with value 0 are ignored.\n\nIntensity (i.e., input) image with same size as labeled image, plus optionally\nan extra dimension for multichannel data. Default is None.\n\nChanged in version 0.18.0: The ability to provide an extra dimension for\nchannels was added.\n\nProperties that will be included in the resulting dictionary For a list of\navailable properties, please see `regionprops()`. Users should remember to add\n\u201clabel\u201d to keep track of region identities.\n\nDetermine whether to cache calculated properties. The computation is much\nfaster for cached properties, whereas the memory consumption increases.\n\nFor non-scalar properties not listed in OBJECT_COLUMNS, each element will\nappear in its own column, with the index of that element separated from the\nproperty name by this separator. For example, the inertia tensor of a 2D\nregion will appear in four columns: `inertia_tensor-0-0`,\n`inertia_tensor-0-1`, `inertia_tensor-1-0`, and `inertia_tensor-1-1` (where\nthe separator is `-`).\n\nObject columns are those that cannot be split in this way because the number\nof columns would change depending on the object. For example, `image` and\n`coords`.\n\nAdd extra property computation functions that are not included with skimage.\nThe name of the property is derived from the function name, the dtype is\ninferred by calling the function on a small sample. If the name of an extra\nproperty clashes with the name of an existing property the extra property wil\nnot be visible and a UserWarning is issued. A property computation function\nmust take a region mask as its first argument. If the property requires an\nintensity image, it must accept the intensity image as the second argument.\n\nDictionary mapping property names to an array of values of that property, one\nvalue per region. This dictionary can be used as input to pandas `DataFrame`\nto map property names to columns in the frame and regions to rows. If the\nimage has no regions, the arrays will have length 0, but the correct type.\n\nEach column contains either a scalar property, an object property, or an\nelement in a multidimensional array.\n\nProperties with scalar values for each region, such as \u201ceccentricity\u201d, will\nappear as a float or int array with that property name as key.\n\nMultidimensional properties of fixed size for a given image dimension, such as\n\u201ccentroid\u201d (every centroid will have three elements in a 3D image, no matter\nthe region size), will be split into that many columns, with the name\n{property_name}{separator}{element_num} (for 1D properties),\n{property_name}{separator}{elem_num0}{separator}{elem_num1} (for 2D\nproperties), and so on.\n\nFor multidimensional properties that don\u2019t have a fixed size, such as \u201cimage\u201d\n(the image of a region varies in size depending on the region size), an object\narray will be used, with the corresponding property name as the key.\n\nThe resulting dictionary can be directly passed to pandas, if installed, to\nobtain a clean DataFrame:\n\n[5 rows x 7 columns]\n\nIf we want to measure a feature that does not come as a built-in property, we\ncan define custom functions and pass them as `extra_properties`. For example,\nwe can create a custom function that measures the intensity quartiles in a\nregion:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.shannon_entropy()", "path": "api/skimage.measure#skimage.measure.shannon_entropy", "type": "measure", "text": "\nCalculate the Shannon entropy of an image.\n\nThe Shannon entropy is defined as S = -sum(pk * log(pk)), where pk are\nfrequency/probability of pixels of value k.\n\nGrayscale input image.\n\nThe logarithmic base to use.\n\nThe returned value is measured in bits or shannon (Sh) for base=2, natural\nunit (nat) for base=np.e and hartley (Hart) for base=10.\n\nhttps://en.wikipedia.org/wiki/Entropy_(information_theory)\n\nhttps://en.wiktionary.org/wiki/Shannon_entropy\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "measure.subdivide_polygon()", "path": "api/skimage.measure#skimage.measure.subdivide_polygon", "type": "measure", "text": "\nSubdivision of polygonal curves using B-Splines.\n\nNote that the resulting curve is always within the convex hull of the original\npolygon. Circular polygons stay closed after subdivision.\n\nCoordinate array.\n\nDegree of B-Spline. Default is 2.\n\nPreserve first and last coordinate of non-circular polygon. Default is False.\n\nSubdivided coordinate array.\n\nhttp://mrl.nyu.edu/publications/subdiv-course2000/coursenotes00.pdf\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "metrics", "path": "api/skimage.metrics", "type": "metrics", "text": "\n`skimage.metrics.adapted_rand_error`([\u2026])\n\nCompute Adapted Rand error as defined by the SNEMI3D contest.\n\n`skimage.metrics.contingency_table`(im_true, \u2026)\n\nReturn the contingency table for all regions in matched segmentations.\n\n`skimage.metrics.hausdorff_distance`(image0, \u2026)\n\nCalculate the Hausdorff distance between nonzero elements of given images.\n\n`skimage.metrics.mean_squared_error`(image0, \u2026)\n\nCompute the mean-squared error between two images.\n\n`skimage.metrics.normalized_root_mse`(\u2026[, \u2026])\n\nCompute the normalized root mean-squared error (NRMSE) between two images.\n\n`skimage.metrics.peak_signal_noise_ratio`(\u2026)\n\nCompute the peak signal to noise ratio (PSNR) for an image.\n\n`skimage.metrics.structural_similarity`(im1, \u2026)\n\nCompute the mean structural similarity index between two images.\n\n`skimage.metrics.variation_of_information`([\u2026])\n\nReturn symmetric conditional entropies associated with the VI.\n\nCompute Adapted Rand error as defined by the SNEMI3D contest. [1]\n\nGround-truth label image, same shape as im_test.\n\nTest image.\n\nA contingency table built with skimage.evaluate.contingency_table. If None, it\nwill be computed on the fly.\n\nLabels to ignore. Any part of the true image labeled with any of these values\nwill not be counted in the score.\n\nThe adapted Rand error; equal to \\\\(1 - \\frac{2pr}{p + r}\\\\), where `p` and\n`r` are the precision and recall described below.\n\nThe adapted Rand precision: this is the number of pairs of pixels that have\nthe same label in the test label image and in the true image, divided by the\nnumber in the test image.\n\nThe adapted Rand recall: this is the number of pairs of pixels that have the\nsame label in the test label image and in the true image, divided by the\nnumber in the true image.\n\nPixels with label 0 in the true segmentation are ignored in the score.\n\nArganda-Carreras I, Turaga SC, Berger DR, et al. (2015) Crowdsourcing the\ncreation of image segmentation algorithms for connectomics. Front. Neuroanat.\n9:142. DOI:10.3389/fnana.2015.00142\n\nReturn the contingency table for all regions in matched segmentations.\n\nGround-truth label image, same shape as im_test.\n\nTest image.\n\nLabels to ignore. Any part of the true image labeled with any of these values\nwill not be counted in the score.\n\nDetermines if the contingency table is normalized by pixel count.\n\nA contingency table. `cont[i, j]` will equal the number of voxels labeled `i`\nin `im_true` and `j` in `im_test`.\n\nCalculate the Hausdorff distance between nonzero elements of given images.\n\nThe Hausdorff distance [1] is the maximum distance between any point on\n`image0` and its nearest point on `image1`, and vice-versa.\n\nArrays where `True` represents a point that is included in a set of points.\nBoth arrays must have the same shape.\n\nThe Hausdorff distance between coordinates of nonzero pixels in `image0` and\n`image1`, using the Euclidian distance.\n\nhttp://en.wikipedia.org/wiki/Hausdorff_distance\n\nHausdorff Distance\n\nCompute the mean-squared error between two images.\n\nImages. Any dimensionality, must have same shape.\n\nThe mean-squared error (MSE) metric.\n\nChanged in version 0.16: This function was renamed from\n`skimage.measure.compare_mse` to `skimage.metrics.mean_squared_error`.\n\nCompute the normalized root mean-squared error (NRMSE) between two images.\n\nGround-truth image, same shape as im_test.\n\nTest image.\n\nControls the normalization method to use in the denominator of the NRMSE.\nThere is no standard method of normalization across the literature [1]. The\nmethods available here are as follows:\n\n\u2018euclidean\u2019 : normalize by the averaged Euclidean norm of `im_true`:\n\nwhere || . || denotes the Frobenius norm and `N = im_true.size`. This result\nis equivalent to:\n\nThe NRMSE metric.\n\nChanged in version 0.16: This function was renamed from\n`skimage.measure.compare_nrmse` to `skimage.metrics.normalized_root_mse`.\n\nhttps://en.wikipedia.org/wiki/Root-mean-square_deviation\n\nCompute the peak signal to noise ratio (PSNR) for an image.\n\nGround-truth image, same shape as im_test.\n\nTest image.\n\nThe data range of the input image (distance between minimum and maximum\npossible values). By default, this is estimated from the image data-type.\n\nThe PSNR metric.\n\nChanged in version 0.16: This function was renamed from\n`skimage.measure.compare_psnr` to `skimage.metrics.peak_signal_noise_ratio`.\n\nhttps://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio\n\nCompute the mean structural similarity index between two images.\n\nImages. Any dimensionality with same shape.\n\nThe side-length of the sliding window used in comparison. Must be an odd\nvalue. If `gaussian_weights` is True, this is ignored and the window size will\ndepend on `sigma`.\n\nIf True, also return the gradient with respect to im2.\n\nThe data range of the input image (distance between minimum and maximum\npossible values). By default, this is estimated from the image data-type.\n\nIf True, treat the last dimension of the array as channels. Similarity\ncalculations are done independently for each channel then averaged.\n\nIf True, each patch has its mean and variance spatially weighted by a\nnormalized Gaussian kernel of width sigma=1.5.\n\nIf True, also return the full structural similarity image.\n\nThe mean structural similarity index over the image.\n\nThe gradient of the structural similarity between im1 and im2 [2]. This is\nonly returned if `gradient` is set to True.\n\nThe full SSIM image. This is only returned if `full` is set to True.\n\nIf True, normalize covariances by N-1 rather than, N where N is the number of\npixels within the sliding window.\n\nAlgorithm parameter, K1 (small constant, see [1]).\n\nAlgorithm parameter, K2 (small constant, see [1]).\n\nStandard deviation for the Gaussian when `gaussian_weights` is True.\n\nTo match the implementation of Wang et. al. [1], set `gaussian_weights` to\nTrue, `sigma` to 1.5, and `use_sample_covariance` to False.\n\nChanged in version 0.16: This function was renamed from\n`skimage.measure.compare_ssim` to `skimage.metrics.structural_similarity`.\n\nWang, Z., Bovik, A. C., Sheikh, H. R., & Simoncelli, E. P. (2004). Image\nquality assessment: From error visibility to structural similarity. IEEE\nTransactions on Image Processing, 13, 600-612.\nhttps://ece.uwaterloo.ca/~z70wang/publications/ssim.pdf,\nDOI:10.1109/TIP.2003.819861\n\nAvanaki, A. N. (2009). Exact global histogram specification optimized for\nstructural similarity. Optical Review, 16, 613-621. arXiv:0901.0065\nDOI:10.1007/s10043-009-0119-z\n\nReturn symmetric conditional entropies associated with the VI. [1]\n\nThe variation of information is defined as VI(X,Y) = H(X|Y) + H(Y|X). If X is\nthe ground-truth segmentation, then H(X|Y) can be interpreted as the amount of\nunder-segmentation and H(X|Y) as the amount of over-segmentation. In other\nwords, a perfect over-segmentation will have H(X|Y)=0 and a perfect under-\nsegmentation will have H(Y|X)=0.\n\nLabel images / segmentations, must have same shape.\n\nA contingency table built with skimage.evaluate.contingency_table. If None, it\nwill be computed with skimage.evaluate.contingency_table. If given, the\nentropies will be computed from this table and any images will be ignored.\n\nLabels to ignore. Any part of the true image labeled with any of these values\nwill not be counted in the score.\n\nThe conditional entropies of image1|image0 and image0|image1.\n\nMarina Meil\u0103 (2007), Comparing clusterings\u2014an information based distance,\nJournal of Multivariate Analysis, Volume 98, Issue 5, Pages 873-895, ISSN\n0047-259X, DOI:10.1016/j.jmva.2006.11.013.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "metrics.adapted_rand_error()", "path": "api/skimage.metrics#skimage.metrics.adapted_rand_error", "type": "metrics", "text": "\nCompute Adapted Rand error as defined by the SNEMI3D contest. [1]\n\nGround-truth label image, same shape as im_test.\n\nTest image.\n\nA contingency table built with skimage.evaluate.contingency_table. If None, it\nwill be computed on the fly.\n\nLabels to ignore. Any part of the true image labeled with any of these values\nwill not be counted in the score.\n\nThe adapted Rand error; equal to \\\\(1 - \\frac{2pr}{p + r}\\\\), where `p` and\n`r` are the precision and recall described below.\n\nThe adapted Rand precision: this is the number of pairs of pixels that have\nthe same label in the test label image and in the true image, divided by the\nnumber in the test image.\n\nThe adapted Rand recall: this is the number of pairs of pixels that have the\nsame label in the test label image and in the true image, divided by the\nnumber in the true image.\n\nPixels with label 0 in the true segmentation are ignored in the score.\n\nArganda-Carreras I, Turaga SC, Berger DR, et al. (2015) Crowdsourcing the\ncreation of image segmentation algorithms for connectomics. Front. Neuroanat.\n9:142. DOI:10.3389/fnana.2015.00142\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "metrics.contingency_table()", "path": "api/skimage.metrics#skimage.metrics.contingency_table", "type": "metrics", "text": "\nReturn the contingency table for all regions in matched segmentations.\n\nGround-truth label image, same shape as im_test.\n\nTest image.\n\nLabels to ignore. Any part of the true image labeled with any of these values\nwill not be counted in the score.\n\nDetermines if the contingency table is normalized by pixel count.\n\nA contingency table. `cont[i, j]` will equal the number of voxels labeled `i`\nin `im_true` and `j` in `im_test`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "metrics.hausdorff_distance()", "path": "api/skimage.metrics#skimage.metrics.hausdorff_distance", "type": "metrics", "text": "\nCalculate the Hausdorff distance between nonzero elements of given images.\n\nThe Hausdorff distance [1] is the maximum distance between any point on\n`image0` and its nearest point on `image1`, and vice-versa.\n\nArrays where `True` represents a point that is included in a set of points.\nBoth arrays must have the same shape.\n\nThe Hausdorff distance between coordinates of nonzero pixels in `image0` and\n`image1`, using the Euclidian distance.\n\nhttp://en.wikipedia.org/wiki/Hausdorff_distance\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "metrics.mean_squared_error()", "path": "api/skimage.metrics#skimage.metrics.mean_squared_error", "type": "metrics", "text": "\nCompute the mean-squared error between two images.\n\nImages. Any dimensionality, must have same shape.\n\nThe mean-squared error (MSE) metric.\n\nChanged in version 0.16: This function was renamed from\n`skimage.measure.compare_mse` to `skimage.metrics.mean_squared_error`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "metrics.normalized_root_mse()", "path": "api/skimage.metrics#skimage.metrics.normalized_root_mse", "type": "metrics", "text": "\nCompute the normalized root mean-squared error (NRMSE) between two images.\n\nGround-truth image, same shape as im_test.\n\nTest image.\n\nControls the normalization method to use in the denominator of the NRMSE.\nThere is no standard method of normalization across the literature [1]. The\nmethods available here are as follows:\n\n\u2018euclidean\u2019 : normalize by the averaged Euclidean norm of `im_true`:\n\nwhere || . || denotes the Frobenius norm and `N = im_true.size`. This result\nis equivalent to:\n\nThe NRMSE metric.\n\nChanged in version 0.16: This function was renamed from\n`skimage.measure.compare_nrmse` to `skimage.metrics.normalized_root_mse`.\n\nhttps://en.wikipedia.org/wiki/Root-mean-square_deviation\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "metrics.peak_signal_noise_ratio()", "path": "api/skimage.metrics#skimage.metrics.peak_signal_noise_ratio", "type": "metrics", "text": "\nCompute the peak signal to noise ratio (PSNR) for an image.\n\nGround-truth image, same shape as im_test.\n\nTest image.\n\nThe data range of the input image (distance between minimum and maximum\npossible values). By default, this is estimated from the image data-type.\n\nThe PSNR metric.\n\nChanged in version 0.16: This function was renamed from\n`skimage.measure.compare_psnr` to `skimage.metrics.peak_signal_noise_ratio`.\n\nhttps://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "metrics.structural_similarity()", "path": "api/skimage.metrics#skimage.metrics.structural_similarity", "type": "metrics", "text": "\nCompute the mean structural similarity index between two images.\n\nImages. Any dimensionality with same shape.\n\nThe side-length of the sliding window used in comparison. Must be an odd\nvalue. If `gaussian_weights` is True, this is ignored and the window size will\ndepend on `sigma`.\n\nIf True, also return the gradient with respect to im2.\n\nThe data range of the input image (distance between minimum and maximum\npossible values). By default, this is estimated from the image data-type.\n\nIf True, treat the last dimension of the array as channels. Similarity\ncalculations are done independently for each channel then averaged.\n\nIf True, each patch has its mean and variance spatially weighted by a\nnormalized Gaussian kernel of width sigma=1.5.\n\nIf True, also return the full structural similarity image.\n\nThe mean structural similarity index over the image.\n\nThe gradient of the structural similarity between im1 and im2 [2]. This is\nonly returned if `gradient` is set to True.\n\nThe full SSIM image. This is only returned if `full` is set to True.\n\nIf True, normalize covariances by N-1 rather than, N where N is the number of\npixels within the sliding window.\n\nAlgorithm parameter, K1 (small constant, see [1]).\n\nAlgorithm parameter, K2 (small constant, see [1]).\n\nStandard deviation for the Gaussian when `gaussian_weights` is True.\n\nTo match the implementation of Wang et. al. [1], set `gaussian_weights` to\nTrue, `sigma` to 1.5, and `use_sample_covariance` to False.\n\nChanged in version 0.16: This function was renamed from\n`skimage.measure.compare_ssim` to `skimage.metrics.structural_similarity`.\n\nWang, Z., Bovik, A. C., Sheikh, H. R., & Simoncelli, E. P. (2004). Image\nquality assessment: From error visibility to structural similarity. IEEE\nTransactions on Image Processing, 13, 600-612.\nhttps://ece.uwaterloo.ca/~z70wang/publications/ssim.pdf,\nDOI:10.1109/TIP.2003.819861\n\nAvanaki, A. N. (2009). Exact global histogram specification optimized for\nstructural similarity. Optical Review, 16, 613-621. arXiv:0901.0065\nDOI:10.1007/s10043-009-0119-z\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "metrics.variation_of_information()", "path": "api/skimage.metrics#skimage.metrics.variation_of_information", "type": "metrics", "text": "\nReturn symmetric conditional entropies associated with the VI. [1]\n\nThe variation of information is defined as VI(X,Y) = H(X|Y) + H(Y|X). If X is\nthe ground-truth segmentation, then H(X|Y) can be interpreted as the amount of\nunder-segmentation and H(X|Y) as the amount of over-segmentation. In other\nwords, a perfect over-segmentation will have H(X|Y)=0 and a perfect under-\nsegmentation will have H(Y|X)=0.\n\nLabel images / segmentations, must have same shape.\n\nA contingency table built with skimage.evaluate.contingency_table. If None, it\nwill be computed with skimage.evaluate.contingency_table. If given, the\nentropies will be computed from this table and any images will be ignored.\n\nLabels to ignore. Any part of the true image labeled with any of these values\nwill not be counted in the score.\n\nThe conditional entropies of image1|image0 and image0|image1.\n\nMarina Meil\u0103 (2007), Comparing clusterings\u2014an information based distance,\nJournal of Multivariate Analysis, Volume 98, Issue 5, Pages 873-895, ISSN\n0047-259X, DOI:10.1016/j.jmva.2006.11.013.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology", "path": "api/skimage.morphology", "type": "morphology", "text": "\n`skimage.morphology.area_closing`(image[, \u2026])\n\nPerform an area closing of the image.\n\n`skimage.morphology.area_opening`(image[, \u2026])\n\nPerform an area opening of the image.\n\n`skimage.morphology.ball`(radius[, dtype])\n\nGenerates a ball-shaped structuring element.\n\n`skimage.morphology.binary_closing`(image[, \u2026])\n\nReturn fast binary morphological closing of an image.\n\n`skimage.morphology.binary_dilation`(image[, \u2026])\n\nReturn fast binary morphological dilation of an image.\n\n`skimage.morphology.binary_erosion`(image[, \u2026])\n\nReturn fast binary morphological erosion of an image.\n\n`skimage.morphology.binary_opening`(image[, \u2026])\n\nReturn fast binary morphological opening of an image.\n\n`skimage.morphology.black_tophat`(image[, \u2026])\n\nReturn black top hat of an image.\n\n`skimage.morphology.closing`(image[, selem, out])\n\nReturn greyscale morphological closing of an image.\n\n`skimage.morphology.convex_hull_image`(image)\n\nCompute the convex hull image of a binary image.\n\n`skimage.morphology.convex_hull_object`(image, *)\n\nCompute the convex hull image of individual objects in a binary image.\n\n`skimage.morphology.cube`(width[, dtype])\n\nGenerates a cube-shaped structuring element.\n\n`skimage.morphology.diameter_closing`(image[, \u2026])\n\nPerform a diameter closing of the image.\n\n`skimage.morphology.diameter_opening`(image[, \u2026])\n\nPerform a diameter opening of the image.\n\n`skimage.morphology.diamond`(radius[, dtype])\n\nGenerates a flat, diamond-shaped structuring element.\n\n`skimage.morphology.dilation`(image[, selem, \u2026])\n\nReturn greyscale morphological dilation of an image.\n\n`skimage.morphology.disk`(radius[, dtype])\n\nGenerates a flat, disk-shaped structuring element.\n\n`skimage.morphology.erosion`(image[, selem, \u2026])\n\nReturn greyscale morphological erosion of an image.\n\n`skimage.morphology.flood`(image, seed_point, *)\n\nMask corresponding to a flood fill.\n\n`skimage.morphology.flood_fill`(image, \u2026[, \u2026])\n\nPerform flood filling on an image.\n\n`skimage.morphology.h_maxima`(image, h[, selem])\n\nDetermine all maxima of the image with height >= h.\n\n`skimage.morphology.h_minima`(image, h[, selem])\n\nDetermine all minima of the image with depth >= h.\n\n`skimage.morphology.label`(input[, \u2026])\n\nLabel connected regions of an integer array.\n\n`skimage.morphology.local_maxima`(image[, \u2026])\n\nFind local maxima of n-dimensional array.\n\n`skimage.morphology.local_minima`(image[, \u2026])\n\nFind local minima of n-dimensional array.\n\n`skimage.morphology.max_tree`(image[, \u2026])\n\nBuild the max tree from an image.\n\n`skimage.morphology.max_tree_local_maxima`(image)\n\nDetermine all local maxima of the image.\n\n`skimage.morphology.medial_axis`(image[, \u2026])\n\nCompute the medial axis transform of a binary image\n\n`skimage.morphology.octagon`(m, n[, dtype])\n\nGenerates an octagon shaped structuring element.\n\n`skimage.morphology.octahedron`(radius[, dtype])\n\nGenerates a octahedron-shaped structuring element.\n\n`skimage.morphology.opening`(image[, selem, out])\n\nReturn greyscale morphological opening of an image.\n\n`skimage.morphology.reconstruction`(seed, mask)\n\nPerform a morphological reconstruction of an image.\n\n`skimage.morphology.rectangle`(nrows, ncols[, \u2026])\n\nGenerates a flat, rectangular-shaped structuring element.\n\n`skimage.morphology.remove_small_holes`(ar[, \u2026])\n\nRemove contiguous holes smaller than the specified size.\n\n`skimage.morphology.remove_small_objects`(ar)\n\nRemove objects smaller than the specified size.\n\n`skimage.morphology.skeletonize`(image, *[, \u2026])\n\nCompute the skeleton of a binary image.\n\n`skimage.morphology.skeletonize_3d`(image)\n\nCompute the skeleton of a binary image.\n\n`skimage.morphology.square`(width[, dtype])\n\nGenerates a flat, square-shaped structuring element.\n\n`skimage.morphology.star`(a[, dtype])\n\nGenerates a star shaped structuring element.\n\n`skimage.morphology.thin`(image[, max_iter])\n\nPerform morphological thinning of a binary image.\n\n`skimage.morphology.watershed`(image[, \u2026])\n\nDeprecated function.\n\n`skimage.morphology.white_tophat`(image[, \u2026])\n\nReturn white top hat of an image.\n\nPerform an area closing of the image.\n\nArea closing removes all dark structures of an image with a surface smaller\nthan area_threshold. The output image is larger than or equal to the input\nimage for every pixel and all local minima have at least a surface of\narea_threshold pixels.\n\nArea closings are similar to morphological closings, but they do not use a\nfixed structuring element, but rather a deformable one, with surface =\narea_threshold.\n\nIn the binary case, area closings are equivalent to remove_small_holes; this\noperator is thus extended to gray-level images.\n\nTechnically, this operator is based on the max-tree representation of the\nimage.\n\nThe input image for which the area_closing is to be calculated. This image can\nbe of any type.\n\nThe size parameter (number of pixels). The default value is arbitrarily chosen\nto be 64.\n\nThe neighborhood connectivity. The integer represents the maximum number of\northogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and\n2 for a 8-neighborhood. Default value is 1.\n\nParent image representing the max tree of the inverted image. The value of\neach pixel is the index of its parent in the ravelled array. See Note for\nfurther details.\n\nThe ordered pixel indices (referring to the ravelled array). The pixels are\nordered such that every pixel is preceded by its parent (except for the root\nwhich has no parent).\n\nOutput image of the same shape and type as input image.\n\nSee also\n\nIf a max-tree representation (parent and tree_traverser) are given to the\nfunction, they must be calculated from the inverted image for this function,\ni.e.: >>> P, S = max_tree(invert(f)) >>> closed = diameter_closing(f, 3,\nparent=P, tree_traverser=S)\n\nVincent L., Proc. \u201cGrayscale area openings and closings, their efficient\nimplementation and applications\u201d, EURASIP Workshop on Mathematical Morphology\nand its Applications to Signal Processing, Barcelona, Spain, pp.22-27, May\n1993.\n\nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d\n(Chapter 6), 2nd edition (2003), ISBN 3540429883.\nDOI:10.1007/978-3-662-05088-0\n\nSalembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected\nOperators for Image and Sequence Processing. IEEE Transactions on Image\nProcessing, 7(4), 555-570. DOI:10.1109/83.663500\n\nNajman, L., & Couprie, M. (2006). Building the component tree in quasi-linear\ntime. IEEE Transactions on Image Processing, 15(11), 3531-3539.\nDOI:10.1109/TIP.2006.877518\n\nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree\nComputation Algorithms. IEEE Transactions on Image Processing, 23(9),\n3885-3895. DOI:10.1109/TIP.2014.2336551\n\nWe create an image (quadratic function with a minimum in the center and 4\nadditional local minima.\n\nWe can calculate the area closing:\n\nAll small minima are removed, and the remaining minima have at least a size of\n8.\n\nPerform an area opening of the image.\n\nArea opening removes all bright structures of an image with a surface smaller\nthan area_threshold. The output image is thus the largest image smaller than\nthe input for which all local maxima have at least a surface of area_threshold\npixels.\n\nArea openings are similar to morphological openings, but they do not use a\nfixed structuring element, but rather a deformable one, with surface =\narea_threshold. Consequently, the area_opening with area_threshold=1 is the\nidentity.\n\nIn the binary case, area openings are equivalent to remove_small_objects; this\noperator is thus extended to gray-level images.\n\nTechnically, this operator is based on the max-tree representation of the\nimage.\n\nThe input image for which the area_opening is to be calculated. This image can\nbe of any type.\n\nThe size parameter (number of pixels). The default value is arbitrarily chosen\nto be 64.\n\nThe neighborhood connectivity. The integer represents the maximum number of\northogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and\n2 for a 8-neighborhood. Default value is 1.\n\nParent image representing the max tree of the image. The value of each pixel\nis the index of its parent in the ravelled array.\n\nThe ordered pixel indices (referring to the ravelled array). The pixels are\nordered such that every pixel is preceded by its parent (except for the root\nwhich has no parent).\n\nOutput image of the same shape and type as the input image.\n\nSee also\n\nVincent L., Proc. \u201cGrayscale area openings and closings, their efficient\nimplementation and applications\u201d, EURASIP Workshop on Mathematical Morphology\nand its Applications to Signal Processing, Barcelona, Spain, pp.22-27, May\n1993.\n\nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d\n(Chapter 6), 2nd edition (2003), ISBN 3540429883.\n:DOI:10.1007/978-3-662-05088-0\n\nSalembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected\nOperators for Image and Sequence Processing. IEEE Transactions on Image\nProcessing, 7(4), 555-570. :DOI:10.1109/83.663500\n\nNajman, L., & Couprie, M. (2006). Building the component tree in quasi-linear\ntime. IEEE Transactions on Image Processing, 15(11), 3531-3539.\n:DOI:10.1109/TIP.2006.877518\n\nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree\nComputation Algorithms. IEEE Transactions on Image Processing, 23(9),\n3885-3895. :DOI:10.1109/TIP.2014.2336551\n\nWe create an image (quadratic function with a maximum in the center and 4\nadditional local maxima.\n\nWe can calculate the area opening:\n\nThe peaks with a surface smaller than 8 are removed.\n\nGenerates a ball-shaped structuring element.\n\nThis is the 3D equivalent of a disk. A pixel is within the neighborhood if the\nEuclidean distance between it and the origin is no greater than radius.\n\nThe radius of the ball-shaped structuring element.\n\nThe structuring element where elements of the neighborhood are 1 and 0\notherwise.\n\nThe data type of the structuring element.\n\nLocal Histogram Equalization\n\nRank filters\n\nReturn fast binary morphological closing of an image.\n\nThis function returns the same result as greyscale closing but performs faster\nfor binary images.\n\nThe morphological closing on an image is defined as a dilation followed by an\nerosion. Closing can remove small dark spots (i.e. \u201cpepper\u201d) and connect small\nbright cracks. This tends to \u201cclose\u201d up (dark) gaps between (bright) features.\n\nBinary input image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a\ncross-shaped structuring element (connectivity=1).\n\nThe array to store the result of the morphology. If None, is passed, a new\narray will be allocated.\n\nThe result of the morphological closing.\n\nFlood Fill\n\nReturn fast binary morphological dilation of an image.\n\nThis function returns the same result as greyscale dilation but performs\nfaster for binary images.\n\nMorphological dilation sets a pixel at `(i,j)` to the maximum over all pixels\nin the neighborhood centered at `(i,j)`. Dilation enlarges bright regions and\nshrinks dark regions.\n\nBinary input image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a\ncross-shaped structuring element (connectivity=1).\n\nThe array to store the result of the morphology. If None is passed, a new\narray will be allocated.\n\nThe result of the morphological dilation with values in `[False, True]`.\n\nReturn fast binary morphological erosion of an image.\n\nThis function returns the same result as greyscale erosion but performs faster\nfor binary images.\n\nMorphological erosion sets a pixel at `(i,j)` to the minimum over all pixels\nin the neighborhood centered at `(i,j)`. Erosion shrinks bright regions and\nenlarges dark regions.\n\nBinary input image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a\ncross-shaped structuring element (connectivity=1).\n\nThe array to store the result of the morphology. If None is passed, a new\narray will be allocated.\n\nThe result of the morphological erosion taking values in `[False, True]`.\n\nReturn fast binary morphological opening of an image.\n\nThis function returns the same result as greyscale opening but performs faster\nfor binary images.\n\nThe morphological opening on an image is defined as an erosion followed by a\ndilation. Opening can remove small bright spots (i.e. \u201csalt\u201d) and connect\nsmall dark cracks. This tends to \u201copen\u201d up (dark) gaps between (bright)\nfeatures.\n\nBinary input image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a\ncross-shaped structuring element (connectivity=1).\n\nThe array to store the result of the morphology. If None is passed, a new\narray will be allocated.\n\nThe result of the morphological opening.\n\nFlood Fill\n\nReturn black top hat of an image.\n\nThe black top hat of an image is defined as its morphological closing minus\nthe original image. This operation returns the dark spots of the image that\nare smaller than the structuring element. Note that dark spots in the original\nimage are bright spots after the black top hat.\n\nImage array.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use cross-\nshaped structuring element (connectivity=1).\n\nThe array to store the result of the morphology. If None is passed, a new\narray will be allocated.\n\nThe result of the morphological black top hat.\n\nSee also\n\nhttps://en.wikipedia.org/wiki/Top-hat_transform\n\nReturn greyscale morphological closing of an image.\n\nThe morphological closing on an image is defined as a dilation followed by an\nerosion. Closing can remove small dark spots (i.e. \u201cpepper\u201d) and connect small\nbright cracks. This tends to \u201cclose\u201d up (dark) gaps between (bright) features.\n\nImage array.\n\nThe neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-\nshaped structuring element (connectivity=1).\n\nThe array to store the result of the morphology. If None, is passed, a new\narray will be allocated.\n\nThe result of the morphological closing.\n\nCompute the convex hull image of a binary image.\n\nThe convex hull is the set of pixels included in the smallest convex polygon\nthat surround all white pixels in the input image.\n\nBinary input image. This array is cast to bool before processing.\n\nIf `True`, a pixel at coordinate, e.g., (4, 7) will be represented by\ncoordinates (3.5, 7), (4.5, 7), (4, 6.5), and (4, 7.5). This adds some\n\u201cextent\u201d to a pixel when computing the hull.\n\nTolerance when determining whether a point is inside the hull. Due to\nnumerical floating point errors, a tolerance of 0 can result in some points\nerroneously being classified as being outside the hull.\n\nBinary image with pixels in convex hull set to True.\n\nhttps://blogs.mathworks.com/steve/2011/10/04/binary-image-convex-hull-\nalgorithm-notes/\n\nCompute the convex hull image of individual objects in a binary image.\n\nThe convex hull is the set of pixels included in the smallest convex polygon\nthat surround all white pixels in the input image.\n\nBinary input image.\n\nDetermines the neighbors of each pixel. Adjacent elements within a squared\ndistance of `connectivity` from pixel center are considered neighbors.:\n\nBinary image with pixels inside convex hull set to `True`.\n\nThis function uses `skimage.morphology.label` to define unique objects, finds\nthe convex hull of each using `convex_hull_image`, and combines these regions\nwith logical OR. Be aware the convex hulls of unconnected objects may overlap\nin the result. If this is suspected, consider using convex_hull_image\nseparately on each object or adjust `connectivity`.\n\nGenerates a cube-shaped structuring element.\n\nThis is the 3D equivalent of a square. Every pixel along the perimeter has a\nchessboard distance no greater than radius (radius=floor(width/2)) pixels.\n\nThe width, height and depth of the cube.\n\nA structuring element consisting only of ones, i.e. every pixel belongs to the\nneighborhood.\n\nThe data type of the structuring element.\n\nPerform a diameter closing of the image.\n\nDiameter closing removes all dark structures of an image with maximal\nextension smaller than diameter_threshold. The maximal extension is defined as\nthe maximal extension of the bounding box. The operator is also called\nBounding Box Closing. In practice, the result is similar to a morphological\nclosing, but long and thin structures are not removed.\n\nTechnically, this operator is based on the max-tree representation of the\nimage.\n\nThe input image for which the diameter_closing is to be calculated. This image\ncan be of any type.\n\nThe maximal extension parameter (number of pixels). The default value is 8.\n\nThe neighborhood connectivity. The integer represents the maximum number of\northogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and\n2 for a 8-neighborhood. Default value is 1.\n\nPrecomputed parent image representing the max tree of the inverted image. This\nfunction is fast, if precomputed parent and tree_traverser are provided. See\nNote for further details.\n\nPrecomputed traverser, where the pixels are ordered such that every pixel is\npreceded by its parent (except for the root which has no parent). This\nfunction is fast, if precomputed parent and tree_traverser are provided. See\nNote for further details.\n\nOutput image of the same shape and type as input image.\n\nSee also\n\nIf a max-tree representation (parent and tree_traverser) are given to the\nfunction, they must be calculated from the inverted image for this function,\ni.e.: >>> P, S = max_tree(invert(f)) >>> closed = diameter_closing(f, 3,\nparent=P, tree_traverser=S)\n\nWalter, T., & Klein, J.-C. (2002). Automatic Detection of Microaneurysms in\nColor Fundus Images of the Human Retina by Means of the Bounding Box Closing.\nIn A. Colosimo, P. Sirabella, A. Giuliani (Eds.), Medical Data Analysis.\nLecture Notes in Computer Science, vol 2526, pp. 210-220. Springer Berlin\nHeidelberg. DOI:10.1007/3-540-36104-9_23\n\nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree\nComputation Algorithms. IEEE Transactions on Image Processing, 23(9),\n3885-3895. DOI:10.1109/TIP.2014.2336551\n\nWe create an image (quadratic function with a minimum in the center and 4\nadditional local minima.\n\nWe can calculate the diameter closing:\n\nAll small minima with a maximal extension of 2 or less are removed. The\nremaining minima have all a maximal extension of at least 3.\n\nPerform a diameter opening of the image.\n\nDiameter opening removes all bright structures of an image with maximal\nextension smaller than diameter_threshold. The maximal extension is defined as\nthe maximal extension of the bounding box. The operator is also called\nBounding Box Opening. In practice, the result is similar to a morphological\nopening, but long and thin structures are not removed.\n\nTechnically, this operator is based on the max-tree representation of the\nimage.\n\nThe input image for which the area_opening is to be calculated. This image can\nbe of any type.\n\nThe maximal extension parameter (number of pixels). The default value is 8.\n\nThe neighborhood connectivity. The integer represents the maximum number of\northogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and\n2 for a 8-neighborhood. Default value is 1.\n\nParent image representing the max tree of the image. The value of each pixel\nis the index of its parent in the ravelled array.\n\nThe ordered pixel indices (referring to the ravelled array). The pixels are\nordered such that every pixel is preceded by its parent (except for the root\nwhich has no parent).\n\nOutput image of the same shape and type as the input image.\n\nSee also\n\nWalter, T., & Klein, J.-C. (2002). Automatic Detection of Microaneurysms in\nColor Fundus Images of the Human Retina by Means of the Bounding Box Closing.\nIn A. Colosimo, P. Sirabella, A. Giuliani (Eds.), Medical Data Analysis.\nLecture Notes in Computer Science, vol 2526, pp. 210-220. Springer Berlin\nHeidelberg. DOI:10.1007/3-540-36104-9_23\n\nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree\nComputation Algorithms. IEEE Transactions on Image Processing, 23(9),\n3885-3895. DOI:10.1109/TIP.2014.2336551\n\nWe create an image (quadratic function with a maximum in the center and 4\nadditional local maxima.\n\nWe can calculate the diameter opening:\n\nThe peaks with a maximal extension of 2 or less are removed. The remaining\npeaks have all a maximal extension of at least 3.\n\nGenerates a flat, diamond-shaped structuring element.\n\nA pixel is part of the neighborhood (i.e. labeled 1) if the city\nblock/Manhattan distance between it and the center of the neighborhood is no\ngreater than radius.\n\nThe radius of the diamond-shaped structuring element.\n\nThe structuring element where elements of the neighborhood are 1 and 0\notherwise.\n\nThe data type of the structuring element.\n\nReturn greyscale morphological dilation of an image.\n\nMorphological dilation sets a pixel at (i,j) to the maximum over all pixels in\nthe neighborhood centered at (i,j). Dilation enlarges bright regions and\nshrinks dark regions.\n\nImage array.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use cross-\nshaped structuring element (connectivity=1).\n\nThe array to store the result of the morphology. If None, is passed, a new\narray will be allocated.\n\nshift structuring element about center point. This only affects eccentric\nstructuring elements (i.e. selem with even numbered sides).\n\nThe result of the morphological dilation.\n\nFor `uint8` (and `uint16` up to a certain bit-depth) data, the lower algorithm\ncomplexity makes the `skimage.filters.rank.maximum` function more efficient\nfor larger images and structuring elements.\n\nRank filters\n\nGenerates a flat, disk-shaped structuring element.\n\nA pixel is within the neighborhood if the Euclidean distance between it and\nthe origin is no greater than radius.\n\nThe radius of the disk-shaped structuring element.\n\nThe structuring element where elements of the neighborhood are 1 and 0\notherwise.\n\nThe data type of the structuring element.\n\nLocal Histogram Equalization\n\nEntropy\n\nMarkers for watershed transform\n\nFlood Fill\n\nSegment human cells (in mitosis)\n\nRank filters\n\nReturn greyscale morphological erosion of an image.\n\nMorphological erosion sets a pixel at (i,j) to the minimum over all pixels in\nthe neighborhood centered at (i,j). Erosion shrinks bright regions and\nenlarges dark regions.\n\nImage array.\n\nThe neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-\nshaped structuring element (connectivity=1).\n\nThe array to store the result of the morphology. If None is passed, a new\narray will be allocated.\n\nshift structuring element about center point. This only affects eccentric\nstructuring elements (i.e. selem with even numbered sides).\n\nThe result of the morphological erosion.\n\nFor `uint8` (and `uint16` up to a certain bit-depth) data, the lower algorithm\ncomplexity makes the `skimage.filters.rank.minimum` function more efficient\nfor larger images and structuring elements.\n\nMask corresponding to a flood fill.\n\nStarting at a specific `seed_point`, connected points equal or within\n`tolerance` of the seed value are found.\n\nAn n-dimensional array.\n\nThe point in `image` used as the starting point for the flood fill. If the\nimage is 1D, this point may be given as an integer.\n\nA structuring element used to determine the neighborhood of each evaluated\npixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as\n`image`. If not given, all adjacent pixels are considered as part of the\nneighborhood (fully connected).\n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent\npixels whose squared distance from the center is larger or equal to\n`connectivity` are considered neighbors. Ignored if `selem` is not None.\n\nIf None (default), adjacent values must be strictly equal to the initial value\nof `image` at `seed_point`. This is fastest. If a value is given, a comparison\nwill be done at every point and if within tolerance of the initial value will\nalso be filled (inclusive).\n\nA Boolean array with the same shape as `image` is returned, with True values\nfor areas connected to and equal (or within tolerance of) the seed point. All\nother values are False.\n\nThe conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many\nraster graphics programs. This function returns just the mask representing the\nfill.\n\nIf indices are desired rather than masks for memory reasons, the user can\nsimply run `numpy.nonzero` on the result, save the indices, and discard this\nmask.\n\nFill connected ones with 5, with full connectivity (diagonals included):\n\nFill connected ones with 5, excluding diagonal points (connectivity 1):\n\nFill with a tolerance:\n\nPerform flood filling on an image.\n\nStarting at a specific `seed_point`, connected points equal or within\n`tolerance` of the seed value are found, then set to `new_value`.\n\nAn n-dimensional array.\n\nThe point in `image` used as the starting point for the flood fill. If the\nimage is 1D, this point may be given as an integer.\n\nNew value to set the entire fill. This must be chosen in agreement with the\ndtype of `image`.\n\nA structuring element used to determine the neighborhood of each evaluated\npixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as\n`image`. If not given, all adjacent pixels are considered as part of the\nneighborhood (fully connected).\n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent\npixels whose squared distance from the center is less than or equal to\n`connectivity` are considered neighbors. Ignored if `selem` is not None.\n\nIf None (default), adjacent values must be strictly equal to the value of\n`image` at `seed_point` to be filled. This is fastest. If a tolerance is\nprovided, adjacent points with values within plus or minus tolerance from the\nseed point are filled (inclusive).\n\nIf True, flood filling is applied to `image` in place. If False, the flood\nfilled result is returned without modifying the input `image` (default).\n\nThis parameter is deprecated and will be removed in version 0.19.0 in favor of\nin_place. If True, flood filling is applied to `image` inplace. If False, the\nflood filled result is returned without modifying the input `image` (default).\n\nAn array with the same shape as `image` is returned, with values in areas\nconnected to and equal (or within tolerance of) the seed point replaced with\n`new_value`.\n\nThe conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many\nraster graphics programs.\n\nFill connected ones with 5, with full connectivity (diagonals included):\n\nFill connected ones with 5, excluding diagonal points (connectivity 1):\n\nFill with a tolerance:\n\nDetermine all maxima of the image with height >= h.\n\nThe local maxima are defined as connected sets of pixels with equal grey level\nstrictly greater than the grey level of all pixels in direct neighborhood of\nthe set.\n\nA local maximum M of height h is a local maximum for which there is at least\none path joining M with an equal or higher local maximum on which the minimal\nvalue is f(M) - h (i.e. the values along the path are not decreasing by more\nthan h with respect to the maximum\u2019s value) and no path to an equal or higher\nlocal maximum for which the minimal value is greater.\n\nThe global maxima of the image are also found by this function.\n\nThe input image for which the maxima are to be calculated.\n\nThe minimal height of all extracted maxima.\n\nThe neighborhood expressed as an n-D array of 1\u2019s and 0\u2019s. Default is the ball\nof radius 1 according to the maximum norm (i.e. a 3x3 square for 2D images, a\n3x3x3 cube for 3D images, etc.)\n\nThe local maxima of height >= h and the global maxima. The resulting image is\na binary image, where pixels belonging to the determined maxima take value 1,\nthe others take value 0.\n\nSee also\n\nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d\n(Chapter 6), 2nd edition (2003), ISBN 3540429883.\n\nWe create an image (quadratic function with a maximum in the center and 4\nadditional constant maxima. The heights of the maxima are: 1, 21, 41, 61, 81\n\nWe can calculate all maxima with a height of at least 40:\n\nThe resulting image will contain 3 local maxima.\n\nDetermine all minima of the image with depth >= h.\n\nThe local minima are defined as connected sets of pixels with equal grey level\nstrictly smaller than the grey levels of all pixels in direct neighborhood of\nthe set.\n\nA local minimum M of depth h is a local minimum for which there is at least\none path joining M with an equal or lower local minimum on which the maximal\nvalue is f(M) + h (i.e. the values along the path are not increasing by more\nthan h with respect to the minimum\u2019s value) and no path to an equal or lower\nlocal minimum for which the maximal value is smaller.\n\nThe global minima of the image are also found by this function.\n\nThe input image for which the minima are to be calculated.\n\nThe minimal depth of all extracted minima.\n\nThe neighborhood expressed as an n-D array of 1\u2019s and 0\u2019s. Default is the ball\nof radius 1 according to the maximum norm (i.e. a 3x3 square for 2D images, a\n3x3x3 cube for 3D images, etc.)\n\nThe local minima of depth >= h and the global minima. The resulting image is a\nbinary image, where pixels belonging to the determined minima take value 1,\nthe others take value 0.\n\nSee also\n\nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d\n(Chapter 6), 2nd edition (2003), ISBN 3540429883.\n\nWe create an image (quadratic function with a minimum in the center and 4\nadditional constant maxima. The depth of the minima are: 1, 21, 41, 61, 81\n\nWe can calculate all minima with a depth of at least 40:\n\nThe resulting image will contain 3 local minima.\n\nLabel connected regions of an integer array.\n\nTwo pixels are connected when they are neighbors and have the same value. In\n2D, they can be neighbors either in a 1- or 2-connected sense. The value\nrefers to the maximum number of orthogonal hops to consider a pixel/voxel a\nneighbor:\n\nImage to label.\n\nConsider all pixels with this value as background pixels, and label them as 0.\nBy default, 0-valued pixels are considered as background pixels.\n\nWhether to return the number of assigned labels.\n\nMaximum number of orthogonal hops to consider a pixel/voxel as a neighbor.\nAccepted values are ranging from 1 to input.ndim. If `None`, a full\nconnectivity of `input.ndim` is used.\n\nLabeled array, where all connected regions are assigned the same integer\nvalue.\n\nNumber of labels, which equals the maximum label index and is only returned if\nreturn_num is `True`.\n\nSee also\n\nChristophe Fiorio and Jens Gustedt, \u201cTwo linear time Union-Find strategies for\nimage processing\u201d, Theoretical Computer Science 154 (1996), pp. 165-181.\n\nKensheng Wu, Ekow Otoo and Arie Shoshani, \u201cOptimizing connected component\nlabeling algorithms\u201d, Paper LBNL-56864, 2005, Lawrence Berkeley National\nLaboratory (University of California),\nhttp://repositories.cdlib.org/lbnl/LBNL-56864\n\nFind local maxima of n-dimensional array.\n\nThe local maxima are defined as connected sets of pixels with equal gray level\n(plateaus) strictly greater than the gray levels of all pixels in the\nneighborhood.\n\nAn n-dimensional array.\n\nA structuring element used to determine the neighborhood of each evaluated\npixel (`True` denotes a connected pixel). It must be a boolean array and have\nthe same number of dimensions as `image`. If neither `selem` nor\n`connectivity` are given, all adjacent pixels are considered as part of the\nneighborhood.\n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent\npixels whose squared distance from the center is less than or equal to\n`connectivity` are considered neighbors. Ignored if `selem` is not None.\n\nIf True, the output will be a tuple of one-dimensional arrays representing the\nindices of local maxima in each dimension. If False, the output will be a\nboolean array with the same shape as `image`.\n\nIf true, plateaus that touch the image border are valid maxima.\n\nIf `indices` is false, a boolean array with the same shape as `image` is\nreturned with `True` indicating the position of local maxima (`False`\notherwise). If `indices` is true, a tuple of one-dimensional arrays containing\nthe coordinates (indices) of all found maxima.\n\nIf `allow_borders` is false and any dimension of the given `image` is shorter\nthan 3 samples, maxima can\u2019t exist and a warning is shown.\n\nSee also\n\nThis function operates on the following ideas:\n\nFor each candidate:\n\nFind local maxima by comparing to all neighboring pixels (maximal\nconnectivity):\n\nFind local maxima without comparing to diagonal pixels (connectivity 1):\n\nand exclude maxima that border the image edge:\n\nFind local minima of n-dimensional array.\n\nThe local minima are defined as connected sets of pixels with equal gray level\n(plateaus) strictly smaller than the gray levels of all pixels in the\nneighborhood.\n\nAn n-dimensional array.\n\nA structuring element used to determine the neighborhood of each evaluated\npixel (`True` denotes a connected pixel). It must be a boolean array and have\nthe same number of dimensions as `image`. If neither `selem` nor\n`connectivity` are given, all adjacent pixels are considered as part of the\nneighborhood.\n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent\npixels whose squared distance from the center is less than or equal to\n`connectivity` are considered neighbors. Ignored if `selem` is not None.\n\nIf True, the output will be a tuple of one-dimensional arrays representing the\nindices of local minima in each dimension. If False, the output will be a\nboolean array with the same shape as `image`.\n\nIf true, plateaus that touch the image border are valid minima.\n\nIf `indices` is false, a boolean array with the same shape as `image` is\nreturned with `True` indicating the position of local minima (`False`\notherwise). If `indices` is true, a tuple of one-dimensional arrays containing\nthe coordinates (indices) of all found minima.\n\nSee also\n\nThis function operates on the following ideas:\n\nFor each candidate:\n\nFind local minima by comparing to all neighboring pixels (maximal\nconnectivity):\n\nFind local minima without comparing to diagonal pixels (connectivity 1):\n\nand exclude minima that border the image edge:\n\nBuild the max tree from an image.\n\nComponent trees represent the hierarchical structure of the connected\ncomponents resulting from sequential thresholding operations applied to an\nimage. A connected component at one level is parent of a component at a higher\nlevel if the latter is included in the first. A max-tree is an efficient\nrepresentation of a component tree. A connected component at one level is\nrepresented by one reference pixel at this level, which is parent to all other\npixels at that level and to the reference pixel at the level above. The max-\ntree is the basis for many morphological operators, namely connected\noperators.\n\nThe input image for which the max-tree is to be calculated. This image can be\nof any type.\n\nThe neighborhood connectivity. The integer represents the maximum number of\northogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and\n2 for a 8-neighborhood. Default value is 1.\n\nArray of same shape as image. The value of each pixel is the index of its\nparent in the ravelled array.\n\nThe ordered pixel indices (referring to the ravelled array). The pixels are\nordered such that every pixel is preceded by its parent (except for the root\nwhich has no parent).\n\nSalembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected\nOperators for Image and Sequence Processing. IEEE Transactions on Image\nProcessing, 7(4), 555-570. DOI:10.1109/83.663500\n\nBerger, C., Geraud, T., Levillain, R., Widynski, N., Baillard, A., Bertin, E.\n(2007). Effective Component Tree Computation with Application to Pattern\nRecognition in Astronomical Imaging. In International Conference on Image\nProcessing (ICIP) (pp. 41-44). DOI:10.1109/ICIP.2007.4379949\n\nNajman, L., & Couprie, M. (2006). Building the component tree in quasi-linear\ntime. IEEE Transactions on Image Processing, 15(11), 3531-3539.\nDOI:10.1109/TIP.2006.877518\n\nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree\nComputation Algorithms. IEEE Transactions on Image Processing, 23(9),\n3885-3895. DOI:10.1109/TIP.2014.2336551\n\nWe create a small sample image (Figure 1 from [4]) and build the max-tree.\n\nDetermine all local maxima of the image.\n\nThe local maxima are defined as connected sets of pixels with equal gray level\nstrictly greater than the gray levels of all pixels in direct neighborhood of\nthe set. The function labels the local maxima.\n\nTechnically, the implementation is based on the max-tree representation of an\nimage. The function is very efficient if the max-tree representation has\nalready been computed. Otherwise, it is preferable to use the function\nlocal_maxima.\n\nThe input image for which the maxima are to be calculated.\n\nThe neighborhood connectivity. The integer represents the maximum number of\northogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and\n2 for a 8-neighborhood. Default value is 1.\n\nThe value of each pixel is the index of its parent in the ravelled array.\n\nThe ordered pixel indices (referring to the ravelled array). The pixels are\nordered such that every pixel is preceded by its parent (except for the root\nwhich has no parent).\n\nLabeled local maxima of the image.\n\nSee also\n\nVincent L., Proc. \u201cGrayscale area openings and closings, their efficient\nimplementation and applications\u201d, EURASIP Workshop on Mathematical Morphology\nand its Applications to Signal Processing, Barcelona, Spain, pp.22-27, May\n1993.\n\nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d\n(Chapter 6), 2nd edition (2003), ISBN 3540429883.\nDOI:10.1007/978-3-662-05088-0\n\nSalembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected\nOperators for Image and Sequence Processing. IEEE Transactions on Image\nProcessing, 7(4), 555-570. DOI:10.1109/83.663500\n\nNajman, L., & Couprie, M. (2006). Building the component tree in quasi-linear\ntime. IEEE Transactions on Image Processing, 15(11), 3531-3539.\nDOI:10.1109/TIP.2006.877518\n\nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree\nComputation Algorithms. IEEE Transactions on Image Processing, 23(9),\n3885-3895. DOI:10.1109/TIP.2014.2336551\n\nWe create an image (quadratic function with a maximum in the center and 4\nadditional constant maxima.\n\nWe can calculate all local maxima:\n\nThe resulting image contains the labeled local maxima.\n\nCompute the medial axis transform of a binary image\n\nThe image of the shape to be skeletonized.\n\nIf a mask is given, only those elements in `image` with a true value in `mask`\nare used for computing the medial axis.\n\nIf true, the distance transform is returned as well as the skeleton.\n\nMedial axis transform of the image\n\nDistance transform of the image (only returned if `return_distance` is True)\n\nSee also\n\nThis algorithm computes the medial axis transform of an image as the ridges of\nits distance transform.\n\nGenerates an octagon shaped structuring element.\n\nFor a given size of (m) horizontal and vertical sides and a given (n) height\nor width of slanted sides octagon is generated. The slanted sides are 45 or\n135 degrees to the horizontal axis and hence the widths and heights are equal.\n\nThe size of the horizontal and vertical sides.\n\nThe height or width of the slanted sides.\n\nThe structuring element where elements of the neighborhood are 1 and 0\notherwise.\n\nThe data type of the structuring element.\n\nGenerates a octahedron-shaped structuring element.\n\nThis is the 3D equivalent of a diamond. A pixel is part of the neighborhood\n(i.e. labeled 1) if the city block/Manhattan distance between it and the\ncenter of the neighborhood is no greater than radius.\n\nThe radius of the octahedron-shaped structuring element.\n\nThe structuring element where elements of the neighborhood are 1 and 0\notherwise.\n\nThe data type of the structuring element.\n\nReturn greyscale morphological opening of an image.\n\nThe morphological opening on an image is defined as an erosion followed by a\ndilation. Opening can remove small bright spots (i.e. \u201csalt\u201d) and connect\nsmall dark cracks. This tends to \u201copen\u201d up (dark) gaps between (bright)\nfeatures.\n\nImage array.\n\nThe neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-\nshaped structuring element (connectivity=1).\n\nThe array to store the result of the morphology. If None is passed, a new\narray will be allocated.\n\nThe result of the morphological opening.\n\nPerform a morphological reconstruction of an image.\n\nMorphological reconstruction by dilation is similar to basic morphological\ndilation: high-intensity values will replace nearby low-intensity values. The\nbasic dilation operator, however, uses a structuring element to determine how\nfar a value in the input image can spread. In contrast, reconstruction uses\ntwo images: a \u201cseed\u201d image, which specifies the values that spread, and a\n\u201cmask\u201d image, which gives the maximum allowed value at each pixel. The mask\nimage, like the structuring element, limits the spread of high-intensity\nvalues. Reconstruction by erosion is simply the inverse: low-intensity values\nspread from the seed image and are limited by the mask image, which represents\nthe minimum allowed value.\n\nAlternatively, you can think of reconstruction as a way to isolate the\nconnected regions of an image. For dilation, reconstruction connects regions\nmarked by local maxima in the seed image: neighboring pixels less-than-or-\nequal-to those seeds are connected to the seeded region. Local maxima with\nvalues larger than the seed image will get truncated to the seed value.\n\nThe seed image (a.k.a. marker image), which specifies the values that are\ndilated or eroded.\n\nThe maximum (dilation) / minimum (erosion) allowed value at each pixel.\n\nPerform reconstruction by dilation or erosion. In dilation (or erosion), the\nseed image is dilated (or eroded) until limited by the mask image. For\ndilation, each seed value must be less than or equal to the corresponding mask\nvalue; for erosion, the reverse is true. Default is \u2018dilation\u2019.\n\nThe neighborhood expressed as an n-D array of 1\u2019s and 0\u2019s. Default is the n-D\nsquare of radius equal to 1 (i.e. a 3x3 square for 2D images, a 3x3x3 cube for\n3D images, etc.)\n\nThe coordinates of the center of the structuring element. Default is located\non the geometrical center of the selem, in that case selem dimensions must be\nodd.\n\nThe result of morphological reconstruction.\n\nThe algorithm is taken from [1]. Applications for greyscale reconstruction are\ndiscussed in [2] and [3].\n\nRobinson, \u201cEfficient morphological reconstruction: a downhill filter\u201d, Pattern\nRecognition Letters 25 (2004) 1759-1767.\n\nVincent, L., \u201cMorphological Grayscale Reconstruction in Image Analysis:\nApplications and Efficient Algorithms\u201d, IEEE Transactions on Image Processing\n(1993)\n\nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d,\nChapter 6, 2nd edition (2003), ISBN 3540429883.\n\nFirst, we create a sinusoidal mask image with peaks at middle and ends.\n\nThen, we create a seed image initialized to the minimum mask value (for\nreconstruction by dilation, min-intensity values don\u2019t spread) and add \u201cseeds\u201d\nto the left and right peak, but at a fraction of peak value (1).\n\nThe reconstructed image (or curve, in this case) is exactly the same as the\nmask image, except that the peaks are truncated to 0.5 and 0. The middle peak\ndisappears completely: Since there were no seed values in this peak region,\nits reconstructed value is truncated to the surrounding value (-1).\n\nAs a more practical example, we try to extract the bright features of an image\nby subtracting a background image created by reconstruction.\n\nTo create the background image, set the mask image to the original image, and\nthe seed image to the original image with an intensity offset, `h`.\n\nThe resulting reconstructed image looks exactly like the original image, but\nwith the peaks of the bumps cut off. Subtracting this reconstructed image from\nthe original image leaves just the peaks of the bumps\n\nThis operation is known as the h-dome of the image and leaves features of\nheight `h` in the subtracted image.\n\nGenerates a flat, rectangular-shaped structuring element.\n\nEvery pixel in the rectangle generated for a given width and given height\nbelongs to the neighborhood.\n\nThe number of rows of the rectangle.\n\nThe number of columns of the rectangle.\n\nA structuring element consisting only of ones, i.e. every pixel belongs to the\nneighborhood.\n\nThe data type of the structuring element.\n\nRemove contiguous holes smaller than the specified size.\n\nThe array containing the connected components of interest.\n\nThe maximum area, in pixels, of a contiguous hole that will be filled.\nReplaces `min_size`.\n\nThe connectivity defining the neighborhood of a pixel.\n\nIf `True`, remove the connected components in the input array itself.\nOtherwise, make a copy.\n\nThe input array with small holes within connected components removed.\n\nIf the input array is of an invalid type, such as float or string.\n\nIf the input array contains negative values.\n\nIf the array type is int, it is assumed that it contains already-labeled\nobjects. The labels are not kept in the output image (this function always\noutputs a bool image). It is suggested that labeling is completed after using\nthis function.\n\nMeasure region properties\n\nRemove objects smaller than the specified size.\n\nExpects ar to be an array with labeled objects, and removes objects smaller\nthan min_size. If `ar` is bool, the image is first labeled. This leads to\npotentially different behavior for bool and 0-and-1 arrays.\n\nThe array containing the objects of interest. If the array type is int, the\nints must be non-negative.\n\nThe smallest allowable object size.\n\nThe connectivity defining the neighborhood of a pixel. Used during labelling\nif `ar` is bool.\n\nIf `True`, remove the objects in the input array itself. Otherwise, make a\ncopy.\n\nThe input array with small connected components removed.\n\nIf the input array is of an invalid type, such as float or string.\n\nIf the input array contains negative values.\n\nMeasure region properties\n\nCompute the skeleton of a binary image.\n\nThinning is used to reduce each connected component in a binary image to a\nsingle-pixel wide skeleton.\n\nA binary image containing the objects to be skeletonized. Zeros represent\nbackground, nonzero values are foreground.\n\nWhich algorithm to use. Zhang\u2019s algorithm [Zha84] only works for 2D images,\nand is the default for 2D. Lee\u2019s algorithm [Lee94] works for 2D or 3D images\nand is the default for 3D.\n\nThe thinned image.\n\nSee also\n\nT.-C. Lee, R.L. Kashyap and C.-N. Chu, Building skeleton models via 3-D medial\nsurface/axis thinning algorithms. Computer Vision, Graphics, and Image\nProcessing, 56(6):462-478, 1994.\n\nA fast parallel algorithm for thinning digital patterns, T. Y. Zhang and C. Y.\nSuen, Communications of the ACM, March 1984, Volume 27, Number 3.\n\nCompute the skeleton of a binary image.\n\nThinning is used to reduce each connected component in a binary image to a\nsingle-pixel wide skeleton.\n\nA binary image containing the objects to be skeletonized. Zeros represent\nbackground, nonzero values are foreground.\n\nThe thinned image.\n\nSee also\n\nThe method of [Lee94] uses an octree data structure to examine a 3x3x3\nneighborhood of a pixel. The algorithm proceeds by iteratively sweeping over\nthe image, and removing pixels at each iteration until the image stops\nchanging. Each iteration consists of two steps: first, a list of candidates\nfor removal is assembled; then pixels from this list are rechecked\nsequentially, to better preserve connectivity of the image.\n\nThe algorithm this function implements is different from the algorithms used\nby either `skeletonize` or `medial_axis`, thus for 2D images the results\nproduced by this function are generally different.\n\nT.-C. Lee, R.L. Kashyap and C.-N. Chu, Building skeleton models via 3-D medial\nsurface/axis thinning algorithms. Computer Vision, Graphics, and Image\nProcessing, 56(6):462-478, 1994.\n\nGenerates a flat, square-shaped structuring element.\n\nEvery pixel along the perimeter has a chessboard distance no greater than\nradius (radius=floor(width/2)) pixels.\n\nThe width and height of the square.\n\nA structuring element consisting only of ones, i.e. every pixel belongs to the\nneighborhood.\n\nThe data type of the structuring element.\n\nGenerates a star shaped structuring element.\n\nStart has 8 vertices and is an overlap of square of size `2*a + 1` with its 45\ndegree rotated version. The slanted sides are 45 or 135 degrees to the\nhorizontal axis.\n\nParameter deciding the size of the star structural element. The side of the\nsquare array returned is `2*a + 1 + 2*floor(a / 2)`.\n\nThe structuring element where elements of the neighborhood are 1 and 0\notherwise.\n\nThe data type of the structuring element.\n\nPerform morphological thinning of a binary image.\n\nThe image to be thinned.\n\nRegardless of the value of this parameter, the thinned image is returned\nimmediately if an iteration produces no change. If this parameter is specified\nit thus sets an upper bound on the number of iterations performed.\n\nThinned image.\n\nSee also\n\nThis algorithm [1] works by making multiple passes over the image, removing\npixels matching a set of criteria designed to thin connected regions while\npreserving eight-connected components and 2 x 2 squares [2]. In each of the\ntwo sub-iterations the algorithm correlates the intermediate skeleton image\nwith a neighborhood mask, then looks up each neighborhood in a lookup table\nindicating whether the central pixel should be deleted in that sub-iteration.\n\nZ. Guo and R. W. Hall, \u201cParallel thinning with two-subiteration algorithms,\u201d\nComm. ACM, vol. 32, no. 3, pp. 359-373, 1989. DOI:10.1145/62065.62074\n\nLam, L., Seong-Whan Lee, and Ching Y. Suen, \u201cThinning Methodologies-A\nComprehensive Survey,\u201d IEEE Transactions on Pattern Analysis and Machine\nIntelligence, Vol 14, No. 9, p. 879, 1992. DOI:10.1109/34.161346\n\nDeprecated function. Use `skimage.segmentation.watershed` instead.\n\nFind watershed basins in `image` flooded from given `markers`.\n\nData array where the lowest value points are labeled first.\n\nThe desired number of markers, or an array marking the basins with the values\nto be assigned in the label matrix. Zero means not a marker. If `None` (no\nmarkers given), the local minima of the image are used as markers.\n\nAn array with the same number of dimensions as `image` whose non-zero elements\nindicate neighbors for connection. Following the scipy convention, default is\na one-connected array of the dimension of the image.\n\noffset of the connectivity (one offset per dimension)\n\nArray of same shape as `image`. Only points at which mask == True will be\nlabeled.\n\nUse compact watershed [3] with given compactness parameter. Higher values\nresult in more regularly-shaped watershed basins.\n\nIf watershed_line is True, a one-pixel wide line separates the regions\nobtained by the watershed algorithm. The line has the label 0.\n\nA labeled matrix of the same type and shape as markers\n\nSee also\n\nrandom walker segmentation A segmentation algorithm based on anisotropic\ndiffusion, usually slower than the watershed but with good results on noisy\ndata and boundaries with holes.\n\nThis function implements a watershed algorithm [1] [2] that apportions pixels\ninto marked basins. The algorithm uses a priority queue to hold the pixels\nwith the metric for the priority queue being pixel value, then the time of\nentry into the queue - this settles ties in favor of the closest marker. Some\nideas taken from Soille, \u201cAutomated Basin Delineation from Digital Elevation\nModels Using Mathematical Morphology\u201d, Signal Processing 20 (1990) 171-182 The\nmost important insight in the paper is that entry time onto the queue solves\ntwo problems: a pixel should be assigned to the neighbor with the largest\ngradient or, if there is no gradient, pixels on a plateau should be split\nbetween markers on opposite sides. This implementation converts all arguments\nto specific, lowest common denominator types, then passes these to a C\nalgorithm. Markers can be determined manually, or automatically using for\nexample the local minima of the gradient of the image, or the local maxima of\nthe distance function to the background for separating overlapping objects\n(see example).\n\nhttps://en.wikipedia.org/wiki/Watershed_%28image_processing%29\n\nhttp://cmm.ensmp.fr/~beucher/wtshed.html\n\nPeer Neubert & Peter Protzel (2014). Compact Watershed and Preemptive SLIC: On\nImproving Trade-offs of Superpixel Segmentation Algorithms. ICPR 2014, pp\n996-1001. DOI:10.1109/ICPR.2014.181 https://www.tu-\nchemnitz.de/etit/proaut/publications/cws_pSLIC_ICPR.pdf\n\nThe watershed algorithm is useful to separate overlapping objects.\n\nWe first generate an initial image with two overlapping circles:\n\nNext, we want to separate the two circles. We generate markers at the maxima\nof the distance to the background:\n\nFinally, we run the watershed on the image and markers:\n\nThe algorithm works also for 3-D images, and can be used for example to\nseparate overlapping spheres.\n\nReturn white top hat of an image.\n\nThe white top hat of an image is defined as the image minus its morphological\nopening. This operation returns the bright spots of the image that are smaller\nthan the structuring element.\n\nImage array.\n\nThe neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-\nshaped structuring element (connectivity=1).\n\nThe array to store the result of the morphology. If None is passed, a new\narray will be allocated.\n\nThe result of the morphological white top hat.\n\nSee also\n\nhttps://en.wikipedia.org/wiki/Top-hat_transform\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.area_closing()", "path": "api/skimage.morphology#skimage.morphology.area_closing", "type": "morphology", "text": "\nPerform an area closing of the image.\n\nArea closing removes all dark structures of an image with a surface smaller\nthan area_threshold. The output image is larger than or equal to the input\nimage for every pixel and all local minima have at least a surface of\narea_threshold pixels.\n\nArea closings are similar to morphological closings, but they do not use a\nfixed structuring element, but rather a deformable one, with surface =\narea_threshold.\n\nIn the binary case, area closings are equivalent to remove_small_holes; this\noperator is thus extended to gray-level images.\n\nTechnically, this operator is based on the max-tree representation of the\nimage.\n\nThe input image for which the area_closing is to be calculated. This image can\nbe of any type.\n\nThe size parameter (number of pixels). The default value is arbitrarily chosen\nto be 64.\n\nThe neighborhood connectivity. The integer represents the maximum number of\northogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and\n2 for a 8-neighborhood. Default value is 1.\n\nParent image representing the max tree of the inverted image. The value of\neach pixel is the index of its parent in the ravelled array. See Note for\nfurther details.\n\nThe ordered pixel indices (referring to the ravelled array). The pixels are\nordered such that every pixel is preceded by its parent (except for the root\nwhich has no parent).\n\nOutput image of the same shape and type as input image.\n\nSee also\n\nIf a max-tree representation (parent and tree_traverser) are given to the\nfunction, they must be calculated from the inverted image for this function,\ni.e.: >>> P, S = max_tree(invert(f)) >>> closed = diameter_closing(f, 3,\nparent=P, tree_traverser=S)\n\nVincent L., Proc. \u201cGrayscale area openings and closings, their efficient\nimplementation and applications\u201d, EURASIP Workshop on Mathematical Morphology\nand its Applications to Signal Processing, Barcelona, Spain, pp.22-27, May\n1993.\n\nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d\n(Chapter 6), 2nd edition (2003), ISBN 3540429883.\nDOI:10.1007/978-3-662-05088-0\n\nSalembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected\nOperators for Image and Sequence Processing. IEEE Transactions on Image\nProcessing, 7(4), 555-570. DOI:10.1109/83.663500\n\nNajman, L., & Couprie, M. (2006). Building the component tree in quasi-linear\ntime. IEEE Transactions on Image Processing, 15(11), 3531-3539.\nDOI:10.1109/TIP.2006.877518\n\nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree\nComputation Algorithms. IEEE Transactions on Image Processing, 23(9),\n3885-3895. DOI:10.1109/TIP.2014.2336551\n\nWe create an image (quadratic function with a minimum in the center and 4\nadditional local minima.\n\nWe can calculate the area closing:\n\nAll small minima are removed, and the remaining minima have at least a size of\n8.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.area_opening()", "path": "api/skimage.morphology#skimage.morphology.area_opening", "type": "morphology", "text": "\nPerform an area opening of the image.\n\nArea opening removes all bright structures of an image with a surface smaller\nthan area_threshold. The output image is thus the largest image smaller than\nthe input for which all local maxima have at least a surface of area_threshold\npixels.\n\nArea openings are similar to morphological openings, but they do not use a\nfixed structuring element, but rather a deformable one, with surface =\narea_threshold. Consequently, the area_opening with area_threshold=1 is the\nidentity.\n\nIn the binary case, area openings are equivalent to remove_small_objects; this\noperator is thus extended to gray-level images.\n\nTechnically, this operator is based on the max-tree representation of the\nimage.\n\nThe input image for which the area_opening is to be calculated. This image can\nbe of any type.\n\nThe size parameter (number of pixels). The default value is arbitrarily chosen\nto be 64.\n\nThe neighborhood connectivity. The integer represents the maximum number of\northogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and\n2 for a 8-neighborhood. Default value is 1.\n\nParent image representing the max tree of the image. The value of each pixel\nis the index of its parent in the ravelled array.\n\nThe ordered pixel indices (referring to the ravelled array). The pixels are\nordered such that every pixel is preceded by its parent (except for the root\nwhich has no parent).\n\nOutput image of the same shape and type as the input image.\n\nSee also\n\nVincent L., Proc. \u201cGrayscale area openings and closings, their efficient\nimplementation and applications\u201d, EURASIP Workshop on Mathematical Morphology\nand its Applications to Signal Processing, Barcelona, Spain, pp.22-27, May\n1993.\n\nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d\n(Chapter 6), 2nd edition (2003), ISBN 3540429883.\n:DOI:10.1007/978-3-662-05088-0\n\nSalembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected\nOperators for Image and Sequence Processing. IEEE Transactions on Image\nProcessing, 7(4), 555-570. :DOI:10.1109/83.663500\n\nNajman, L., & Couprie, M. (2006). Building the component tree in quasi-linear\ntime. IEEE Transactions on Image Processing, 15(11), 3531-3539.\n:DOI:10.1109/TIP.2006.877518\n\nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree\nComputation Algorithms. IEEE Transactions on Image Processing, 23(9),\n3885-3895. :DOI:10.1109/TIP.2014.2336551\n\nWe create an image (quadratic function with a maximum in the center and 4\nadditional local maxima.\n\nWe can calculate the area opening:\n\nThe peaks with a surface smaller than 8 are removed.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.ball()", "path": "api/skimage.morphology#skimage.morphology.ball", "type": "morphology", "text": "\nGenerates a ball-shaped structuring element.\n\nThis is the 3D equivalent of a disk. A pixel is within the neighborhood if the\nEuclidean distance between it and the origin is no greater than radius.\n\nThe radius of the ball-shaped structuring element.\n\nThe structuring element where elements of the neighborhood are 1 and 0\notherwise.\n\nThe data type of the structuring element.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.binary_closing()", "path": "api/skimage.morphology#skimage.morphology.binary_closing", "type": "morphology", "text": "\nReturn fast binary morphological closing of an image.\n\nThis function returns the same result as greyscale closing but performs faster\nfor binary images.\n\nThe morphological closing on an image is defined as a dilation followed by an\nerosion. Closing can remove small dark spots (i.e. \u201cpepper\u201d) and connect small\nbright cracks. This tends to \u201cclose\u201d up (dark) gaps between (bright) features.\n\nBinary input image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a\ncross-shaped structuring element (connectivity=1).\n\nThe array to store the result of the morphology. If None, is passed, a new\narray will be allocated.\n\nThe result of the morphological closing.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.binary_dilation()", "path": "api/skimage.morphology#skimage.morphology.binary_dilation", "type": "morphology", "text": "\nReturn fast binary morphological dilation of an image.\n\nThis function returns the same result as greyscale dilation but performs\nfaster for binary images.\n\nMorphological dilation sets a pixel at `(i,j)` to the maximum over all pixels\nin the neighborhood centered at `(i,j)`. Dilation enlarges bright regions and\nshrinks dark regions.\n\nBinary input image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a\ncross-shaped structuring element (connectivity=1).\n\nThe array to store the result of the morphology. If None is passed, a new\narray will be allocated.\n\nThe result of the morphological dilation with values in `[False, True]`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.binary_erosion()", "path": "api/skimage.morphology#skimage.morphology.binary_erosion", "type": "morphology", "text": "\nReturn fast binary morphological erosion of an image.\n\nThis function returns the same result as greyscale erosion but performs faster\nfor binary images.\n\nMorphological erosion sets a pixel at `(i,j)` to the minimum over all pixels\nin the neighborhood centered at `(i,j)`. Erosion shrinks bright regions and\nenlarges dark regions.\n\nBinary input image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a\ncross-shaped structuring element (connectivity=1).\n\nThe array to store the result of the morphology. If None is passed, a new\narray will be allocated.\n\nThe result of the morphological erosion taking values in `[False, True]`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.binary_opening()", "path": "api/skimage.morphology#skimage.morphology.binary_opening", "type": "morphology", "text": "\nReturn fast binary morphological opening of an image.\n\nThis function returns the same result as greyscale opening but performs faster\nfor binary images.\n\nThe morphological opening on an image is defined as an erosion followed by a\ndilation. Opening can remove small bright spots (i.e. \u201csalt\u201d) and connect\nsmall dark cracks. This tends to \u201copen\u201d up (dark) gaps between (bright)\nfeatures.\n\nBinary input image.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a\ncross-shaped structuring element (connectivity=1).\n\nThe array to store the result of the morphology. If None is passed, a new\narray will be allocated.\n\nThe result of the morphological opening.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.black_tophat()", "path": "api/skimage.morphology#skimage.morphology.black_tophat", "type": "morphology", "text": "\nReturn black top hat of an image.\n\nThe black top hat of an image is defined as its morphological closing minus\nthe original image. This operation returns the dark spots of the image that\nare smaller than the structuring element. Note that dark spots in the original\nimage are bright spots after the black top hat.\n\nImage array.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use cross-\nshaped structuring element (connectivity=1).\n\nThe array to store the result of the morphology. If None is passed, a new\narray will be allocated.\n\nThe result of the morphological black top hat.\n\nSee also\n\nhttps://en.wikipedia.org/wiki/Top-hat_transform\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.closing()", "path": "api/skimage.morphology#skimage.morphology.closing", "type": "morphology", "text": "\nReturn greyscale morphological closing of an image.\n\nThe morphological closing on an image is defined as a dilation followed by an\nerosion. Closing can remove small dark spots (i.e. \u201cpepper\u201d) and connect small\nbright cracks. This tends to \u201cclose\u201d up (dark) gaps between (bright) features.\n\nImage array.\n\nThe neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-\nshaped structuring element (connectivity=1).\n\nThe array to store the result of the morphology. If None, is passed, a new\narray will be allocated.\n\nThe result of the morphological closing.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.convex_hull_image()", "path": "api/skimage.morphology#skimage.morphology.convex_hull_image", "type": "morphology", "text": "\nCompute the convex hull image of a binary image.\n\nThe convex hull is the set of pixels included in the smallest convex polygon\nthat surround all white pixels in the input image.\n\nBinary input image. This array is cast to bool before processing.\n\nIf `True`, a pixel at coordinate, e.g., (4, 7) will be represented by\ncoordinates (3.5, 7), (4.5, 7), (4, 6.5), and (4, 7.5). This adds some\n\u201cextent\u201d to a pixel when computing the hull.\n\nTolerance when determining whether a point is inside the hull. Due to\nnumerical floating point errors, a tolerance of 0 can result in some points\nerroneously being classified as being outside the hull.\n\nBinary image with pixels in convex hull set to True.\n\nhttps://blogs.mathworks.com/steve/2011/10/04/binary-image-convex-hull-\nalgorithm-notes/\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.convex_hull_object()", "path": "api/skimage.morphology#skimage.morphology.convex_hull_object", "type": "morphology", "text": "\nCompute the convex hull image of individual objects in a binary image.\n\nThe convex hull is the set of pixels included in the smallest convex polygon\nthat surround all white pixels in the input image.\n\nBinary input image.\n\nDetermines the neighbors of each pixel. Adjacent elements within a squared\ndistance of `connectivity` from pixel center are considered neighbors.:\n\nBinary image with pixels inside convex hull set to `True`.\n\nThis function uses `skimage.morphology.label` to define unique objects, finds\nthe convex hull of each using `convex_hull_image`, and combines these regions\nwith logical OR. Be aware the convex hulls of unconnected objects may overlap\nin the result. If this is suspected, consider using convex_hull_image\nseparately on each object or adjust `connectivity`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.cube()", "path": "api/skimage.morphology#skimage.morphology.cube", "type": "morphology", "text": "\nGenerates a cube-shaped structuring element.\n\nThis is the 3D equivalent of a square. Every pixel along the perimeter has a\nchessboard distance no greater than radius (radius=floor(width/2)) pixels.\n\nThe width, height and depth of the cube.\n\nA structuring element consisting only of ones, i.e. every pixel belongs to the\nneighborhood.\n\nThe data type of the structuring element.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.diameter_closing()", "path": "api/skimage.morphology#skimage.morphology.diameter_closing", "type": "morphology", "text": "\nPerform a diameter closing of the image.\n\nDiameter closing removes all dark structures of an image with maximal\nextension smaller than diameter_threshold. The maximal extension is defined as\nthe maximal extension of the bounding box. The operator is also called\nBounding Box Closing. In practice, the result is similar to a morphological\nclosing, but long and thin structures are not removed.\n\nTechnically, this operator is based on the max-tree representation of the\nimage.\n\nThe input image for which the diameter_closing is to be calculated. This image\ncan be of any type.\n\nThe maximal extension parameter (number of pixels). The default value is 8.\n\nThe neighborhood connectivity. The integer represents the maximum number of\northogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and\n2 for a 8-neighborhood. Default value is 1.\n\nPrecomputed parent image representing the max tree of the inverted image. This\nfunction is fast, if precomputed parent and tree_traverser are provided. See\nNote for further details.\n\nPrecomputed traverser, where the pixels are ordered such that every pixel is\npreceded by its parent (except for the root which has no parent). This\nfunction is fast, if precomputed parent and tree_traverser are provided. See\nNote for further details.\n\nOutput image of the same shape and type as input image.\n\nSee also\n\nIf a max-tree representation (parent and tree_traverser) are given to the\nfunction, they must be calculated from the inverted image for this function,\ni.e.: >>> P, S = max_tree(invert(f)) >>> closed = diameter_closing(f, 3,\nparent=P, tree_traverser=S)\n\nWalter, T., & Klein, J.-C. (2002). Automatic Detection of Microaneurysms in\nColor Fundus Images of the Human Retina by Means of the Bounding Box Closing.\nIn A. Colosimo, P. Sirabella, A. Giuliani (Eds.), Medical Data Analysis.\nLecture Notes in Computer Science, vol 2526, pp. 210-220. Springer Berlin\nHeidelberg. DOI:10.1007/3-540-36104-9_23\n\nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree\nComputation Algorithms. IEEE Transactions on Image Processing, 23(9),\n3885-3895. DOI:10.1109/TIP.2014.2336551\n\nWe create an image (quadratic function with a minimum in the center and 4\nadditional local minima.\n\nWe can calculate the diameter closing:\n\nAll small minima with a maximal extension of 2 or less are removed. The\nremaining minima have all a maximal extension of at least 3.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.diameter_opening()", "path": "api/skimage.morphology#skimage.morphology.diameter_opening", "type": "morphology", "text": "\nPerform a diameter opening of the image.\n\nDiameter opening removes all bright structures of an image with maximal\nextension smaller than diameter_threshold. The maximal extension is defined as\nthe maximal extension of the bounding box. The operator is also called\nBounding Box Opening. In practice, the result is similar to a morphological\nopening, but long and thin structures are not removed.\n\nTechnically, this operator is based on the max-tree representation of the\nimage.\n\nThe input image for which the area_opening is to be calculated. This image can\nbe of any type.\n\nThe maximal extension parameter (number of pixels). The default value is 8.\n\nThe neighborhood connectivity. The integer represents the maximum number of\northogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and\n2 for a 8-neighborhood. Default value is 1.\n\nParent image representing the max tree of the image. The value of each pixel\nis the index of its parent in the ravelled array.\n\nThe ordered pixel indices (referring to the ravelled array). The pixels are\nordered such that every pixel is preceded by its parent (except for the root\nwhich has no parent).\n\nOutput image of the same shape and type as the input image.\n\nSee also\n\nWalter, T., & Klein, J.-C. (2002). Automatic Detection of Microaneurysms in\nColor Fundus Images of the Human Retina by Means of the Bounding Box Closing.\nIn A. Colosimo, P. Sirabella, A. Giuliani (Eds.), Medical Data Analysis.\nLecture Notes in Computer Science, vol 2526, pp. 210-220. Springer Berlin\nHeidelberg. DOI:10.1007/3-540-36104-9_23\n\nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree\nComputation Algorithms. IEEE Transactions on Image Processing, 23(9),\n3885-3895. DOI:10.1109/TIP.2014.2336551\n\nWe create an image (quadratic function with a maximum in the center and 4\nadditional local maxima.\n\nWe can calculate the diameter opening:\n\nThe peaks with a maximal extension of 2 or less are removed. The remaining\npeaks have all a maximal extension of at least 3.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.diamond()", "path": "api/skimage.morphology#skimage.morphology.diamond", "type": "morphology", "text": "\nGenerates a flat, diamond-shaped structuring element.\n\nA pixel is part of the neighborhood (i.e. labeled 1) if the city\nblock/Manhattan distance between it and the center of the neighborhood is no\ngreater than radius.\n\nThe radius of the diamond-shaped structuring element.\n\nThe structuring element where elements of the neighborhood are 1 and 0\notherwise.\n\nThe data type of the structuring element.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.dilation()", "path": "api/skimage.morphology#skimage.morphology.dilation", "type": "morphology", "text": "\nReturn greyscale morphological dilation of an image.\n\nMorphological dilation sets a pixel at (i,j) to the maximum over all pixels in\nthe neighborhood centered at (i,j). Dilation enlarges bright regions and\nshrinks dark regions.\n\nImage array.\n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use cross-\nshaped structuring element (connectivity=1).\n\nThe array to store the result of the morphology. If None, is passed, a new\narray will be allocated.\n\nshift structuring element about center point. This only affects eccentric\nstructuring elements (i.e. selem with even numbered sides).\n\nThe result of the morphological dilation.\n\nFor `uint8` (and `uint16` up to a certain bit-depth) data, the lower algorithm\ncomplexity makes the `skimage.filters.rank.maximum` function more efficient\nfor larger images and structuring elements.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.disk()", "path": "api/skimage.morphology#skimage.morphology.disk", "type": "morphology", "text": "\nGenerates a flat, disk-shaped structuring element.\n\nA pixel is within the neighborhood if the Euclidean distance between it and\nthe origin is no greater than radius.\n\nThe radius of the disk-shaped structuring element.\n\nThe structuring element where elements of the neighborhood are 1 and 0\notherwise.\n\nThe data type of the structuring element.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.erosion()", "path": "api/skimage.morphology#skimage.morphology.erosion", "type": "morphology", "text": "\nReturn greyscale morphological erosion of an image.\n\nMorphological erosion sets a pixel at (i,j) to the minimum over all pixels in\nthe neighborhood centered at (i,j). Erosion shrinks bright regions and\nenlarges dark regions.\n\nImage array.\n\nThe neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-\nshaped structuring element (connectivity=1).\n\nThe array to store the result of the morphology. If None is passed, a new\narray will be allocated.\n\nshift structuring element about center point. This only affects eccentric\nstructuring elements (i.e. selem with even numbered sides).\n\nThe result of the morphological erosion.\n\nFor `uint8` (and `uint16` up to a certain bit-depth) data, the lower algorithm\ncomplexity makes the `skimage.filters.rank.minimum` function more efficient\nfor larger images and structuring elements.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.flood()", "path": "api/skimage.morphology#skimage.morphology.flood", "type": "morphology", "text": "\nMask corresponding to a flood fill.\n\nStarting at a specific `seed_point`, connected points equal or within\n`tolerance` of the seed value are found.\n\nAn n-dimensional array.\n\nThe point in `image` used as the starting point for the flood fill. If the\nimage is 1D, this point may be given as an integer.\n\nA structuring element used to determine the neighborhood of each evaluated\npixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as\n`image`. If not given, all adjacent pixels are considered as part of the\nneighborhood (fully connected).\n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent\npixels whose squared distance from the center is larger or equal to\n`connectivity` are considered neighbors. Ignored if `selem` is not None.\n\nIf None (default), adjacent values must be strictly equal to the initial value\nof `image` at `seed_point`. This is fastest. If a value is given, a comparison\nwill be done at every point and if within tolerance of the initial value will\nalso be filled (inclusive).\n\nA Boolean array with the same shape as `image` is returned, with True values\nfor areas connected to and equal (or within tolerance of) the seed point. All\nother values are False.\n\nThe conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many\nraster graphics programs. This function returns just the mask representing the\nfill.\n\nIf indices are desired rather than masks for memory reasons, the user can\nsimply run `numpy.nonzero` on the result, save the indices, and discard this\nmask.\n\nFill connected ones with 5, with full connectivity (diagonals included):\n\nFill connected ones with 5, excluding diagonal points (connectivity 1):\n\nFill with a tolerance:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.flood_fill()", "path": "api/skimage.morphology#skimage.morphology.flood_fill", "type": "morphology", "text": "\nPerform flood filling on an image.\n\nStarting at a specific `seed_point`, connected points equal or within\n`tolerance` of the seed value are found, then set to `new_value`.\n\nAn n-dimensional array.\n\nThe point in `image` used as the starting point for the flood fill. If the\nimage is 1D, this point may be given as an integer.\n\nNew value to set the entire fill. This must be chosen in agreement with the\ndtype of `image`.\n\nA structuring element used to determine the neighborhood of each evaluated\npixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as\n`image`. If not given, all adjacent pixels are considered as part of the\nneighborhood (fully connected).\n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent\npixels whose squared distance from the center is less than or equal to\n`connectivity` are considered neighbors. Ignored if `selem` is not None.\n\nIf None (default), adjacent values must be strictly equal to the value of\n`image` at `seed_point` to be filled. This is fastest. If a tolerance is\nprovided, adjacent points with values within plus or minus tolerance from the\nseed point are filled (inclusive).\n\nIf True, flood filling is applied to `image` in place. If False, the flood\nfilled result is returned without modifying the input `image` (default).\n\nThis parameter is deprecated and will be removed in version 0.19.0 in favor of\nin_place. If True, flood filling is applied to `image` inplace. If False, the\nflood filled result is returned without modifying the input `image` (default).\n\nAn array with the same shape as `image` is returned, with values in areas\nconnected to and equal (or within tolerance of) the seed point replaced with\n`new_value`.\n\nThe conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many\nraster graphics programs.\n\nFill connected ones with 5, with full connectivity (diagonals included):\n\nFill connected ones with 5, excluding diagonal points (connectivity 1):\n\nFill with a tolerance:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.h_maxima()", "path": "api/skimage.morphology#skimage.morphology.h_maxima", "type": "morphology", "text": "\nDetermine all maxima of the image with height >= h.\n\nThe local maxima are defined as connected sets of pixels with equal grey level\nstrictly greater than the grey level of all pixels in direct neighborhood of\nthe set.\n\nA local maximum M of height h is a local maximum for which there is at least\none path joining M with an equal or higher local maximum on which the minimal\nvalue is f(M) - h (i.e. the values along the path are not decreasing by more\nthan h with respect to the maximum\u2019s value) and no path to an equal or higher\nlocal maximum for which the minimal value is greater.\n\nThe global maxima of the image are also found by this function.\n\nThe input image for which the maxima are to be calculated.\n\nThe minimal height of all extracted maxima.\n\nThe neighborhood expressed as an n-D array of 1\u2019s and 0\u2019s. Default is the ball\nof radius 1 according to the maximum norm (i.e. a 3x3 square for 2D images, a\n3x3x3 cube for 3D images, etc.)\n\nThe local maxima of height >= h and the global maxima. The resulting image is\na binary image, where pixels belonging to the determined maxima take value 1,\nthe others take value 0.\n\nSee also\n\nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d\n(Chapter 6), 2nd edition (2003), ISBN 3540429883.\n\nWe create an image (quadratic function with a maximum in the center and 4\nadditional constant maxima. The heights of the maxima are: 1, 21, 41, 61, 81\n\nWe can calculate all maxima with a height of at least 40:\n\nThe resulting image will contain 3 local maxima.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.h_minima()", "path": "api/skimage.morphology#skimage.morphology.h_minima", "type": "morphology", "text": "\nDetermine all minima of the image with depth >= h.\n\nThe local minima are defined as connected sets of pixels with equal grey level\nstrictly smaller than the grey levels of all pixels in direct neighborhood of\nthe set.\n\nA local minimum M of depth h is a local minimum for which there is at least\none path joining M with an equal or lower local minimum on which the maximal\nvalue is f(M) + h (i.e. the values along the path are not increasing by more\nthan h with respect to the minimum\u2019s value) and no path to an equal or lower\nlocal minimum for which the maximal value is smaller.\n\nThe global minima of the image are also found by this function.\n\nThe input image for which the minima are to be calculated.\n\nThe minimal depth of all extracted minima.\n\nThe neighborhood expressed as an n-D array of 1\u2019s and 0\u2019s. Default is the ball\nof radius 1 according to the maximum norm (i.e. a 3x3 square for 2D images, a\n3x3x3 cube for 3D images, etc.)\n\nThe local minima of depth >= h and the global minima. The resulting image is a\nbinary image, where pixels belonging to the determined minima take value 1,\nthe others take value 0.\n\nSee also\n\nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d\n(Chapter 6), 2nd edition (2003), ISBN 3540429883.\n\nWe create an image (quadratic function with a minimum in the center and 4\nadditional constant maxima. The depth of the minima are: 1, 21, 41, 61, 81\n\nWe can calculate all minima with a depth of at least 40:\n\nThe resulting image will contain 3 local minima.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.label()", "path": "api/skimage.morphology#skimage.morphology.label", "type": "morphology", "text": "\nLabel connected regions of an integer array.\n\nTwo pixels are connected when they are neighbors and have the same value. In\n2D, they can be neighbors either in a 1- or 2-connected sense. The value\nrefers to the maximum number of orthogonal hops to consider a pixel/voxel a\nneighbor:\n\nImage to label.\n\nConsider all pixels with this value as background pixels, and label them as 0.\nBy default, 0-valued pixels are considered as background pixels.\n\nWhether to return the number of assigned labels.\n\nMaximum number of orthogonal hops to consider a pixel/voxel as a neighbor.\nAccepted values are ranging from 1 to input.ndim. If `None`, a full\nconnectivity of `input.ndim` is used.\n\nLabeled array, where all connected regions are assigned the same integer\nvalue.\n\nNumber of labels, which equals the maximum label index and is only returned if\nreturn_num is `True`.\n\nSee also\n\nChristophe Fiorio and Jens Gustedt, \u201cTwo linear time Union-Find strategies for\nimage processing\u201d, Theoretical Computer Science 154 (1996), pp. 165-181.\n\nKensheng Wu, Ekow Otoo and Arie Shoshani, \u201cOptimizing connected component\nlabeling algorithms\u201d, Paper LBNL-56864, 2005, Lawrence Berkeley National\nLaboratory (University of California),\nhttp://repositories.cdlib.org/lbnl/LBNL-56864\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.local_maxima()", "path": "api/skimage.morphology#skimage.morphology.local_maxima", "type": "morphology", "text": "\nFind local maxima of n-dimensional array.\n\nThe local maxima are defined as connected sets of pixels with equal gray level\n(plateaus) strictly greater than the gray levels of all pixels in the\nneighborhood.\n\nAn n-dimensional array.\n\nA structuring element used to determine the neighborhood of each evaluated\npixel (`True` denotes a connected pixel). It must be a boolean array and have\nthe same number of dimensions as `image`. If neither `selem` nor\n`connectivity` are given, all adjacent pixels are considered as part of the\nneighborhood.\n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent\npixels whose squared distance from the center is less than or equal to\n`connectivity` are considered neighbors. Ignored if `selem` is not None.\n\nIf True, the output will be a tuple of one-dimensional arrays representing the\nindices of local maxima in each dimension. If False, the output will be a\nboolean array with the same shape as `image`.\n\nIf true, plateaus that touch the image border are valid maxima.\n\nIf `indices` is false, a boolean array with the same shape as `image` is\nreturned with `True` indicating the position of local maxima (`False`\notherwise). If `indices` is true, a tuple of one-dimensional arrays containing\nthe coordinates (indices) of all found maxima.\n\nIf `allow_borders` is false and any dimension of the given `image` is shorter\nthan 3 samples, maxima can\u2019t exist and a warning is shown.\n\nSee also\n\nThis function operates on the following ideas:\n\nFor each candidate:\n\nFind local maxima by comparing to all neighboring pixels (maximal\nconnectivity):\n\nFind local maxima without comparing to diagonal pixels (connectivity 1):\n\nand exclude maxima that border the image edge:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.local_minima()", "path": "api/skimage.morphology#skimage.morphology.local_minima", "type": "morphology", "text": "\nFind local minima of n-dimensional array.\n\nThe local minima are defined as connected sets of pixels with equal gray level\n(plateaus) strictly smaller than the gray levels of all pixels in the\nneighborhood.\n\nAn n-dimensional array.\n\nA structuring element used to determine the neighborhood of each evaluated\npixel (`True` denotes a connected pixel). It must be a boolean array and have\nthe same number of dimensions as `image`. If neither `selem` nor\n`connectivity` are given, all adjacent pixels are considered as part of the\nneighborhood.\n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent\npixels whose squared distance from the center is less than or equal to\n`connectivity` are considered neighbors. Ignored if `selem` is not None.\n\nIf True, the output will be a tuple of one-dimensional arrays representing the\nindices of local minima in each dimension. If False, the output will be a\nboolean array with the same shape as `image`.\n\nIf true, plateaus that touch the image border are valid minima.\n\nIf `indices` is false, a boolean array with the same shape as `image` is\nreturned with `True` indicating the position of local minima (`False`\notherwise). If `indices` is true, a tuple of one-dimensional arrays containing\nthe coordinates (indices) of all found minima.\n\nSee also\n\nThis function operates on the following ideas:\n\nFor each candidate:\n\nFind local minima by comparing to all neighboring pixels (maximal\nconnectivity):\n\nFind local minima without comparing to diagonal pixels (connectivity 1):\n\nand exclude minima that border the image edge:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.max_tree()", "path": "api/skimage.morphology#skimage.morphology.max_tree", "type": "morphology", "text": "\nBuild the max tree from an image.\n\nComponent trees represent the hierarchical structure of the connected\ncomponents resulting from sequential thresholding operations applied to an\nimage. A connected component at one level is parent of a component at a higher\nlevel if the latter is included in the first. A max-tree is an efficient\nrepresentation of a component tree. A connected component at one level is\nrepresented by one reference pixel at this level, which is parent to all other\npixels at that level and to the reference pixel at the level above. The max-\ntree is the basis for many morphological operators, namely connected\noperators.\n\nThe input image for which the max-tree is to be calculated. This image can be\nof any type.\n\nThe neighborhood connectivity. The integer represents the maximum number of\northogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and\n2 for a 8-neighborhood. Default value is 1.\n\nArray of same shape as image. The value of each pixel is the index of its\nparent in the ravelled array.\n\nThe ordered pixel indices (referring to the ravelled array). The pixels are\nordered such that every pixel is preceded by its parent (except for the root\nwhich has no parent).\n\nSalembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected\nOperators for Image and Sequence Processing. IEEE Transactions on Image\nProcessing, 7(4), 555-570. DOI:10.1109/83.663500\n\nBerger, C., Geraud, T., Levillain, R., Widynski, N., Baillard, A., Bertin, E.\n(2007). Effective Component Tree Computation with Application to Pattern\nRecognition in Astronomical Imaging. In International Conference on Image\nProcessing (ICIP) (pp. 41-44). DOI:10.1109/ICIP.2007.4379949\n\nNajman, L., & Couprie, M. (2006). Building the component tree in quasi-linear\ntime. IEEE Transactions on Image Processing, 15(11), 3531-3539.\nDOI:10.1109/TIP.2006.877518\n\nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree\nComputation Algorithms. IEEE Transactions on Image Processing, 23(9),\n3885-3895. DOI:10.1109/TIP.2014.2336551\n\nWe create a small sample image (Figure 1 from [4]) and build the max-tree.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.max_tree_local_maxima()", "path": "api/skimage.morphology#skimage.morphology.max_tree_local_maxima", "type": "morphology", "text": "\nDetermine all local maxima of the image.\n\nThe local maxima are defined as connected sets of pixels with equal gray level\nstrictly greater than the gray levels of all pixels in direct neighborhood of\nthe set. The function labels the local maxima.\n\nTechnically, the implementation is based on the max-tree representation of an\nimage. The function is very efficient if the max-tree representation has\nalready been computed. Otherwise, it is preferable to use the function\nlocal_maxima.\n\nThe input image for which the maxima are to be calculated.\n\nThe neighborhood connectivity. The integer represents the maximum number of\northogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and\n2 for a 8-neighborhood. Default value is 1.\n\nThe value of each pixel is the index of its parent in the ravelled array.\n\nThe ordered pixel indices (referring to the ravelled array). The pixels are\nordered such that every pixel is preceded by its parent (except for the root\nwhich has no parent).\n\nLabeled local maxima of the image.\n\nSee also\n\nVincent L., Proc. \u201cGrayscale area openings and closings, their efficient\nimplementation and applications\u201d, EURASIP Workshop on Mathematical Morphology\nand its Applications to Signal Processing, Barcelona, Spain, pp.22-27, May\n1993.\n\nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d\n(Chapter 6), 2nd edition (2003), ISBN 3540429883.\nDOI:10.1007/978-3-662-05088-0\n\nSalembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected\nOperators for Image and Sequence Processing. IEEE Transactions on Image\nProcessing, 7(4), 555-570. DOI:10.1109/83.663500\n\nNajman, L., & Couprie, M. (2006). Building the component tree in quasi-linear\ntime. IEEE Transactions on Image Processing, 15(11), 3531-3539.\nDOI:10.1109/TIP.2006.877518\n\nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree\nComputation Algorithms. IEEE Transactions on Image Processing, 23(9),\n3885-3895. DOI:10.1109/TIP.2014.2336551\n\nWe create an image (quadratic function with a maximum in the center and 4\nadditional constant maxima.\n\nWe can calculate all local maxima:\n\nThe resulting image contains the labeled local maxima.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.medial_axis()", "path": "api/skimage.morphology#skimage.morphology.medial_axis", "type": "morphology", "text": "\nCompute the medial axis transform of a binary image\n\nThe image of the shape to be skeletonized.\n\nIf a mask is given, only those elements in `image` with a true value in `mask`\nare used for computing the medial axis.\n\nIf true, the distance transform is returned as well as the skeleton.\n\nMedial axis transform of the image\n\nDistance transform of the image (only returned if `return_distance` is True)\n\nSee also\n\nThis algorithm computes the medial axis transform of an image as the ridges of\nits distance transform.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.octagon()", "path": "api/skimage.morphology#skimage.morphology.octagon", "type": "morphology", "text": "\nGenerates an octagon shaped structuring element.\n\nFor a given size of (m) horizontal and vertical sides and a given (n) height\nor width of slanted sides octagon is generated. The slanted sides are 45 or\n135 degrees to the horizontal axis and hence the widths and heights are equal.\n\nThe size of the horizontal and vertical sides.\n\nThe height or width of the slanted sides.\n\nThe structuring element where elements of the neighborhood are 1 and 0\notherwise.\n\nThe data type of the structuring element.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.octahedron()", "path": "api/skimage.morphology#skimage.morphology.octahedron", "type": "morphology", "text": "\nGenerates a octahedron-shaped structuring element.\n\nThis is the 3D equivalent of a diamond. A pixel is part of the neighborhood\n(i.e. labeled 1) if the city block/Manhattan distance between it and the\ncenter of the neighborhood is no greater than radius.\n\nThe radius of the octahedron-shaped structuring element.\n\nThe structuring element where elements of the neighborhood are 1 and 0\notherwise.\n\nThe data type of the structuring element.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.opening()", "path": "api/skimage.morphology#skimage.morphology.opening", "type": "morphology", "text": "\nReturn greyscale morphological opening of an image.\n\nThe morphological opening on an image is defined as an erosion followed by a\ndilation. Opening can remove small bright spots (i.e. \u201csalt\u201d) and connect\nsmall dark cracks. This tends to \u201copen\u201d up (dark) gaps between (bright)\nfeatures.\n\nImage array.\n\nThe neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-\nshaped structuring element (connectivity=1).\n\nThe array to store the result of the morphology. If None is passed, a new\narray will be allocated.\n\nThe result of the morphological opening.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.reconstruction()", "path": "api/skimage.morphology#skimage.morphology.reconstruction", "type": "morphology", "text": "\nPerform a morphological reconstruction of an image.\n\nMorphological reconstruction by dilation is similar to basic morphological\ndilation: high-intensity values will replace nearby low-intensity values. The\nbasic dilation operator, however, uses a structuring element to determine how\nfar a value in the input image can spread. In contrast, reconstruction uses\ntwo images: a \u201cseed\u201d image, which specifies the values that spread, and a\n\u201cmask\u201d image, which gives the maximum allowed value at each pixel. The mask\nimage, like the structuring element, limits the spread of high-intensity\nvalues. Reconstruction by erosion is simply the inverse: low-intensity values\nspread from the seed image and are limited by the mask image, which represents\nthe minimum allowed value.\n\nAlternatively, you can think of reconstruction as a way to isolate the\nconnected regions of an image. For dilation, reconstruction connects regions\nmarked by local maxima in the seed image: neighboring pixels less-than-or-\nequal-to those seeds are connected to the seeded region. Local maxima with\nvalues larger than the seed image will get truncated to the seed value.\n\nThe seed image (a.k.a. marker image), which specifies the values that are\ndilated or eroded.\n\nThe maximum (dilation) / minimum (erosion) allowed value at each pixel.\n\nPerform reconstruction by dilation or erosion. In dilation (or erosion), the\nseed image is dilated (or eroded) until limited by the mask image. For\ndilation, each seed value must be less than or equal to the corresponding mask\nvalue; for erosion, the reverse is true. Default is \u2018dilation\u2019.\n\nThe neighborhood expressed as an n-D array of 1\u2019s and 0\u2019s. Default is the n-D\nsquare of radius equal to 1 (i.e. a 3x3 square for 2D images, a 3x3x3 cube for\n3D images, etc.)\n\nThe coordinates of the center of the structuring element. Default is located\non the geometrical center of the selem, in that case selem dimensions must be\nodd.\n\nThe result of morphological reconstruction.\n\nThe algorithm is taken from [1]. Applications for greyscale reconstruction are\ndiscussed in [2] and [3].\n\nRobinson, \u201cEfficient morphological reconstruction: a downhill filter\u201d, Pattern\nRecognition Letters 25 (2004) 1759-1767.\n\nVincent, L., \u201cMorphological Grayscale Reconstruction in Image Analysis:\nApplications and Efficient Algorithms\u201d, IEEE Transactions on Image Processing\n(1993)\n\nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d,\nChapter 6, 2nd edition (2003), ISBN 3540429883.\n\nFirst, we create a sinusoidal mask image with peaks at middle and ends.\n\nThen, we create a seed image initialized to the minimum mask value (for\nreconstruction by dilation, min-intensity values don\u2019t spread) and add \u201cseeds\u201d\nto the left and right peak, but at a fraction of peak value (1).\n\nThe reconstructed image (or curve, in this case) is exactly the same as the\nmask image, except that the peaks are truncated to 0.5 and 0. The middle peak\ndisappears completely: Since there were no seed values in this peak region,\nits reconstructed value is truncated to the surrounding value (-1).\n\nAs a more practical example, we try to extract the bright features of an image\nby subtracting a background image created by reconstruction.\n\nTo create the background image, set the mask image to the original image, and\nthe seed image to the original image with an intensity offset, `h`.\n\nThe resulting reconstructed image looks exactly like the original image, but\nwith the peaks of the bumps cut off. Subtracting this reconstructed image from\nthe original image leaves just the peaks of the bumps\n\nThis operation is known as the h-dome of the image and leaves features of\nheight `h` in the subtracted image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.rectangle()", "path": "api/skimage.morphology#skimage.morphology.rectangle", "type": "morphology", "text": "\nGenerates a flat, rectangular-shaped structuring element.\n\nEvery pixel in the rectangle generated for a given width and given height\nbelongs to the neighborhood.\n\nThe number of rows of the rectangle.\n\nThe number of columns of the rectangle.\n\nA structuring element consisting only of ones, i.e. every pixel belongs to the\nneighborhood.\n\nThe data type of the structuring element.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.remove_small_holes()", "path": "api/skimage.morphology#skimage.morphology.remove_small_holes", "type": "morphology", "text": "\nRemove contiguous holes smaller than the specified size.\n\nThe array containing the connected components of interest.\n\nThe maximum area, in pixels, of a contiguous hole that will be filled.\nReplaces `min_size`.\n\nThe connectivity defining the neighborhood of a pixel.\n\nIf `True`, remove the connected components in the input array itself.\nOtherwise, make a copy.\n\nThe input array with small holes within connected components removed.\n\nIf the input array is of an invalid type, such as float or string.\n\nIf the input array contains negative values.\n\nIf the array type is int, it is assumed that it contains already-labeled\nobjects. The labels are not kept in the output image (this function always\noutputs a bool image). It is suggested that labeling is completed after using\nthis function.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.remove_small_objects()", "path": "api/skimage.morphology#skimage.morphology.remove_small_objects", "type": "morphology", "text": "\nRemove objects smaller than the specified size.\n\nExpects ar to be an array with labeled objects, and removes objects smaller\nthan min_size. If `ar` is bool, the image is first labeled. This leads to\npotentially different behavior for bool and 0-and-1 arrays.\n\nThe array containing the objects of interest. If the array type is int, the\nints must be non-negative.\n\nThe smallest allowable object size.\n\nThe connectivity defining the neighborhood of a pixel. Used during labelling\nif `ar` is bool.\n\nIf `True`, remove the objects in the input array itself. Otherwise, make a\ncopy.\n\nThe input array with small connected components removed.\n\nIf the input array is of an invalid type, such as float or string.\n\nIf the input array contains negative values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.skeletonize()", "path": "api/skimage.morphology#skimage.morphology.skeletonize", "type": "morphology", "text": "\nCompute the skeleton of a binary image.\n\nThinning is used to reduce each connected component in a binary image to a\nsingle-pixel wide skeleton.\n\nA binary image containing the objects to be skeletonized. Zeros represent\nbackground, nonzero values are foreground.\n\nWhich algorithm to use. Zhang\u2019s algorithm [Zha84] only works for 2D images,\nand is the default for 2D. Lee\u2019s algorithm [Lee94] works for 2D or 3D images\nand is the default for 3D.\n\nThe thinned image.\n\nSee also\n\nT.-C. Lee, R.L. Kashyap and C.-N. Chu, Building skeleton models via 3-D medial\nsurface/axis thinning algorithms. Computer Vision, Graphics, and Image\nProcessing, 56(6):462-478, 1994.\n\nA fast parallel algorithm for thinning digital patterns, T. Y. Zhang and C. Y.\nSuen, Communications of the ACM, March 1984, Volume 27, Number 3.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.skeletonize_3d()", "path": "api/skimage.morphology#skimage.morphology.skeletonize_3d", "type": "morphology", "text": "\nCompute the skeleton of a binary image.\n\nThinning is used to reduce each connected component in a binary image to a\nsingle-pixel wide skeleton.\n\nA binary image containing the objects to be skeletonized. Zeros represent\nbackground, nonzero values are foreground.\n\nThe thinned image.\n\nSee also\n\nThe method of [Lee94] uses an octree data structure to examine a 3x3x3\nneighborhood of a pixel. The algorithm proceeds by iteratively sweeping over\nthe image, and removing pixels at each iteration until the image stops\nchanging. Each iteration consists of two steps: first, a list of candidates\nfor removal is assembled; then pixels from this list are rechecked\nsequentially, to better preserve connectivity of the image.\n\nThe algorithm this function implements is different from the algorithms used\nby either `skeletonize` or `medial_axis`, thus for 2D images the results\nproduced by this function are generally different.\n\nT.-C. Lee, R.L. Kashyap and C.-N. Chu, Building skeleton models via 3-D medial\nsurface/axis thinning algorithms. Computer Vision, Graphics, and Image\nProcessing, 56(6):462-478, 1994.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.square()", "path": "api/skimage.morphology#skimage.morphology.square", "type": "morphology", "text": "\nGenerates a flat, square-shaped structuring element.\n\nEvery pixel along the perimeter has a chessboard distance no greater than\nradius (radius=floor(width/2)) pixels.\n\nThe width and height of the square.\n\nA structuring element consisting only of ones, i.e. every pixel belongs to the\nneighborhood.\n\nThe data type of the structuring element.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.star()", "path": "api/skimage.morphology#skimage.morphology.star", "type": "morphology", "text": "\nGenerates a star shaped structuring element.\n\nStart has 8 vertices and is an overlap of square of size `2*a + 1` with its 45\ndegree rotated version. The slanted sides are 45 or 135 degrees to the\nhorizontal axis.\n\nParameter deciding the size of the star structural element. The side of the\nsquare array returned is `2*a + 1 + 2*floor(a / 2)`.\n\nThe structuring element where elements of the neighborhood are 1 and 0\notherwise.\n\nThe data type of the structuring element.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.thin()", "path": "api/skimage.morphology#skimage.morphology.thin", "type": "morphology", "text": "\nPerform morphological thinning of a binary image.\n\nThe image to be thinned.\n\nRegardless of the value of this parameter, the thinned image is returned\nimmediately if an iteration produces no change. If this parameter is specified\nit thus sets an upper bound on the number of iterations performed.\n\nThinned image.\n\nSee also\n\nThis algorithm [1] works by making multiple passes over the image, removing\npixels matching a set of criteria designed to thin connected regions while\npreserving eight-connected components and 2 x 2 squares [2]. In each of the\ntwo sub-iterations the algorithm correlates the intermediate skeleton image\nwith a neighborhood mask, then looks up each neighborhood in a lookup table\nindicating whether the central pixel should be deleted in that sub-iteration.\n\nZ. Guo and R. W. Hall, \u201cParallel thinning with two-subiteration algorithms,\u201d\nComm. ACM, vol. 32, no. 3, pp. 359-373, 1989. DOI:10.1145/62065.62074\n\nLam, L., Seong-Whan Lee, and Ching Y. Suen, \u201cThinning Methodologies-A\nComprehensive Survey,\u201d IEEE Transactions on Pattern Analysis and Machine\nIntelligence, Vol 14, No. 9, p. 879, 1992. DOI:10.1109/34.161346\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.watershed()", "path": "api/skimage.morphology#skimage.morphology.watershed", "type": "morphology", "text": "\nDeprecated function. Use `skimage.segmentation.watershed` instead.\n\nFind watershed basins in `image` flooded from given `markers`.\n\nData array where the lowest value points are labeled first.\n\nThe desired number of markers, or an array marking the basins with the values\nto be assigned in the label matrix. Zero means not a marker. If `None` (no\nmarkers given), the local minima of the image are used as markers.\n\nAn array with the same number of dimensions as `image` whose non-zero elements\nindicate neighbors for connection. Following the scipy convention, default is\na one-connected array of the dimension of the image.\n\noffset of the connectivity (one offset per dimension)\n\nArray of same shape as `image`. Only points at which mask == True will be\nlabeled.\n\nUse compact watershed [3] with given compactness parameter. Higher values\nresult in more regularly-shaped watershed basins.\n\nIf watershed_line is True, a one-pixel wide line separates the regions\nobtained by the watershed algorithm. The line has the label 0.\n\nA labeled matrix of the same type and shape as markers\n\nSee also\n\nrandom walker segmentation A segmentation algorithm based on anisotropic\ndiffusion, usually slower than the watershed but with good results on noisy\ndata and boundaries with holes.\n\nThis function implements a watershed algorithm [1] [2] that apportions pixels\ninto marked basins. The algorithm uses a priority queue to hold the pixels\nwith the metric for the priority queue being pixel value, then the time of\nentry into the queue - this settles ties in favor of the closest marker. Some\nideas taken from Soille, \u201cAutomated Basin Delineation from Digital Elevation\nModels Using Mathematical Morphology\u201d, Signal Processing 20 (1990) 171-182 The\nmost important insight in the paper is that entry time onto the queue solves\ntwo problems: a pixel should be assigned to the neighbor with the largest\ngradient or, if there is no gradient, pixels on a plateau should be split\nbetween markers on opposite sides. This implementation converts all arguments\nto specific, lowest common denominator types, then passes these to a C\nalgorithm. Markers can be determined manually, or automatically using for\nexample the local minima of the gradient of the image, or the local maxima of\nthe distance function to the background for separating overlapping objects\n(see example).\n\nhttps://en.wikipedia.org/wiki/Watershed_%28image_processing%29\n\nhttp://cmm.ensmp.fr/~beucher/wtshed.html\n\nPeer Neubert & Peter Protzel (2014). Compact Watershed and Preemptive SLIC: On\nImproving Trade-offs of Superpixel Segmentation Algorithms. ICPR 2014, pp\n996-1001. DOI:10.1109/ICPR.2014.181 https://www.tu-\nchemnitz.de/etit/proaut/publications/cws_pSLIC_ICPR.pdf\n\nThe watershed algorithm is useful to separate overlapping objects.\n\nWe first generate an initial image with two overlapping circles:\n\nNext, we want to separate the two circles. We generate markers at the maxima\nof the distance to the background:\n\nFinally, we run the watershed on the image and markers:\n\nThe algorithm works also for 3-D images, and can be used for example to\nseparate overlapping spheres.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "morphology.white_tophat()", "path": "api/skimage.morphology#skimage.morphology.white_tophat", "type": "morphology", "text": "\nReturn white top hat of an image.\n\nThe white top hat of an image is defined as the image minus its morphological\nopening. This operation returns the bright spots of the image that are smaller\nthan the structuring element.\n\nImage array.\n\nThe neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-\nshaped structuring element (connectivity=1).\n\nThe array to store the result of the morphology. If None is passed, a new\narray will be allocated.\n\nThe result of the morphological white top hat.\n\nSee also\n\nhttps://en.wikipedia.org/wiki/Top-hat_transform\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "registration", "path": "api/skimage.registration", "type": "registration", "text": "\n`skimage.registration.optical_flow_ilk`(\u2026[, \u2026])\n\nCoarse to fine optical flow estimator.\n\n`skimage.registration.optical_flow_tvl1`(\u2026)\n\nCoarse to fine optical flow estimator.\n\n`skimage.registration.phase_cross_correlation`(\u2026)\n\nEfficient subpixel image translation registration by cross-correlation.\n\nCoarse to fine optical flow estimator.\n\nThe iterative Lucas-Kanade (iLK) solver is applied at each level of the image\npyramid. iLK [1] is a fast and robust alternative to TVL1 algorithm although\nless accurate for rendering flat surfaces and object boundaries (see [2]).\n\nThe first gray scale image of the sequence.\n\nThe second gray scale image of the sequence.\n\nRadius of the window considered around each pixel.\n\nNumber of times moving_image is warped.\n\nIf True, a Gaussian kernel is used for the local integration. Otherwise, a\nuniform kernel is used.\n\nWhether to prefilter the estimated optical flow before each image warp. When\nTrue, a median filter with window size 3 along each axis is applied. This\nhelps to remove potential outliers.\n\nOutput data type: must be floating point. Single precision provides good\nresults and saves memory usage and computation time compared to double\nprecision.\n\nThe estimated optical flow components for each axis.\n\nLe Besnerais, G., & Champagnat, F. (2005, September). Dense optical flow by\niterative local window registration. In IEEE International Conference on Image\nProcessing 2005 (Vol. 1, pp. I-137). IEEE. DOI:10.1109/ICIP.2005.1529706\n\nPlyer, A., Le Besnerais, G., & Champagnat, F. (2016). Massively parallel Lucas\nKanade optical flow for real-time video processing applications. Journal of\nReal-Time Image Processing, 11(4), 713-730. DOI:10.1007/s11554-014-0423-0\n\nRegistration using optical flow\n\nCoarse to fine optical flow estimator.\n\nThe TV-L1 solver is applied at each level of the image pyramid. TV-L1 is a\npopular algorithm for optical flow estimation introduced by Zack et al. [1],\nimproved in [2] and detailed in [3].\n\nThe first gray scale image of the sequence.\n\nThe second gray scale image of the sequence.\n\nAttachment parameter (\\\\(\\lambda\\\\) in [1]). The smaller this parameter is,\nthe smoother the returned result will be.\n\nTightness parameter (\\\\(\\tau\\\\) in [1]). It should have a small value in order\nto maintain attachement and regularization parts in correspondence.\n\nNumber of times image1 is warped.\n\nNumber of fixed point iteration.\n\nTolerance used as stopping criterion based on the L\u00b2 distance between two\nconsecutive values of (u, v).\n\nWhether to prefilter the estimated optical flow before each image warp. When\nTrue, a median filter with window size 3 along each axis is applied. This\nhelps to remove potential outliers.\n\nOutput data type: must be floating point. Single precision provides good\nresults and saves memory usage and computation time compared to double\nprecision.\n\nThe estimated optical flow components for each axis.\n\nColor images are not supported.\n\nZach, C., Pock, T., & Bischof, H. (2007, September). A duality based approach\nfor realtime TV-L 1 optical flow. In Joint pattern recognition symposium (pp.\n214-223). Springer, Berlin, Heidelberg. DOI:10.1007/978-3-540-74936-3_22\n\nWedel, A., Pock, T., Zach, C., Bischof, H., & Cremers, D. (2009). An improved\nalgorithm for TV-L 1 optical flow. In Statistical and geometrical approaches\nto visual motion analysis (pp. 23-45). Springer, Berlin, Heidelberg.\nDOI:10.1007/978-3-642-03061-1_2\n\nP\u00e9rez, J. S., Meinhardt-Llopis, E., & Facciolo, G. (2013). TV-L1 optical flow\nestimation. Image Processing On Line, 2013, 137-150. DOI:10.5201/ipol.2013.26\n\nRegistration using optical flow\n\nEfficient subpixel image translation registration by cross-correlation.\n\nThis code gives the same precision as the FFT upsampled cross-correlation in a\nfraction of the computation time and with reduced memory requirements. It\nobtains an initial estimate of the cross-correlation peak by an FFT and then\nrefines the shift estimation by upsampling the DFT only in a small\nneighborhood of that estimate by means of a matrix-multiply DFT.\n\nReference image.\n\nImage to register. Must be same dimensionality as `reference_image`.\n\nUpsampling factor. Images will be registered to within `1 / upsample_factor`\nof a pixel. For example `upsample_factor == 20` means the images will be\nregistered within 1/20th of a pixel. Default is 1 (no upsampling). Not used if\nany of `reference_mask` or `moving_mask` is not None.\n\nDefines how the algorithm interprets input data. \u201creal\u201d means data will be\nFFT\u2019d to compute the correlation, while \u201cfourier\u201d data will bypass FFT of\ninput data. Case insensitive. Not used if any of `reference_mask` or\n`moving_mask` is not None.\n\nReturns error and phase difference if on, otherwise only shifts are returned.\nHas noeffect if any of `reference_mask` or `moving_mask` is not None. In this\ncase only shifts is returned.\n\nBoolean mask for `reference_image`. The mask should evaluate to `True` (or 1)\non valid pixels. `reference_mask` should have the same shape as\n`reference_image`.\n\nBoolean mask for `moving_image`. The mask should evaluate to `True` (or 1) on\nvalid pixels. `moving_mask` should have the same shape as `moving_image`. If\n`None`, `reference_mask` will be used.\n\nMinimum allowed overlap ratio between images. The correlation for translations\ncorresponding with an overlap ratio lower than this threshold will be ignored.\nA lower `overlap_ratio` leads to smaller maximum translation, while a higher\n`overlap_ratio` leads to greater robustness against spurious matches due to\nsmall overlap between masked images. Used only if one of `reference_mask` or\n`moving_mask` is None.\n\nShift vector (in pixels) required to register `moving_image` with\n`reference_image`. Axis ordering is consistent with numpy (e.g. Z, Y, X)\n\nTranslation invariant normalized RMS error between `reference_image` and\n`moving_image`.\n\nGlobal phase difference between the two images (should be zero if images are\nnon-negative).\n\nManuel Guizar-Sicairos, Samuel T. Thurman, and James R. Fienup, \u201cEfficient\nsubpixel image registration algorithms,\u201d Optics Letters 33, 156-158 (2008).\nDOI:10.1364/OL.33.000156\n\nJames R. Fienup, \u201cInvariant error metrics for image reconstruction\u201d Optics\nLetters 36, 8352-8357 (1997). DOI:10.1364/AO.36.008352\n\nDirk Padfield. Masked Object Registration in the Fourier Domain. IEEE\nTransactions on Image Processing, vol. 21(5), pp. 2706-2718 (2012).\nDOI:10.1109/TIP.2011.2181402\n\nD. Padfield. \u201cMasked FFT registration\u201d. In Proc. Computer Vision and Pattern\nRecognition, pp. 2918-2925 (2010). DOI:10.1109/CVPR.2010.5540032\n\nMasked Normalized Cross-Correlation\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "registration.optical_flow_ilk()", "path": "api/skimage.registration#skimage.registration.optical_flow_ilk", "type": "registration", "text": "\nCoarse to fine optical flow estimator.\n\nThe iterative Lucas-Kanade (iLK) solver is applied at each level of the image\npyramid. iLK [1] is a fast and robust alternative to TVL1 algorithm although\nless accurate for rendering flat surfaces and object boundaries (see [2]).\n\nThe first gray scale image of the sequence.\n\nThe second gray scale image of the sequence.\n\nRadius of the window considered around each pixel.\n\nNumber of times moving_image is warped.\n\nIf True, a Gaussian kernel is used for the local integration. Otherwise, a\nuniform kernel is used.\n\nWhether to prefilter the estimated optical flow before each image warp. When\nTrue, a median filter with window size 3 along each axis is applied. This\nhelps to remove potential outliers.\n\nOutput data type: must be floating point. Single precision provides good\nresults and saves memory usage and computation time compared to double\nprecision.\n\nThe estimated optical flow components for each axis.\n\nLe Besnerais, G., & Champagnat, F. (2005, September). Dense optical flow by\niterative local window registration. In IEEE International Conference on Image\nProcessing 2005 (Vol. 1, pp. I-137). IEEE. DOI:10.1109/ICIP.2005.1529706\n\nPlyer, A., Le Besnerais, G., & Champagnat, F. (2016). Massively parallel Lucas\nKanade optical flow for real-time video processing applications. Journal of\nReal-Time Image Processing, 11(4), 713-730. DOI:10.1007/s11554-014-0423-0\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "registration.optical_flow_tvl1()", "path": "api/skimage.registration#skimage.registration.optical_flow_tvl1", "type": "registration", "text": "\nCoarse to fine optical flow estimator.\n\nThe TV-L1 solver is applied at each level of the image pyramid. TV-L1 is a\npopular algorithm for optical flow estimation introduced by Zack et al. [1],\nimproved in [2] and detailed in [3].\n\nThe first gray scale image of the sequence.\n\nThe second gray scale image of the sequence.\n\nAttachment parameter (\\\\(\\lambda\\\\) in [1]). The smaller this parameter is,\nthe smoother the returned result will be.\n\nTightness parameter (\\\\(\\tau\\\\) in [1]). It should have a small value in order\nto maintain attachement and regularization parts in correspondence.\n\nNumber of times image1 is warped.\n\nNumber of fixed point iteration.\n\nTolerance used as stopping criterion based on the L\u00b2 distance between two\nconsecutive values of (u, v).\n\nWhether to prefilter the estimated optical flow before each image warp. When\nTrue, a median filter with window size 3 along each axis is applied. This\nhelps to remove potential outliers.\n\nOutput data type: must be floating point. Single precision provides good\nresults and saves memory usage and computation time compared to double\nprecision.\n\nThe estimated optical flow components for each axis.\n\nColor images are not supported.\n\nZach, C., Pock, T., & Bischof, H. (2007, September). A duality based approach\nfor realtime TV-L 1 optical flow. In Joint pattern recognition symposium (pp.\n214-223). Springer, Berlin, Heidelberg. DOI:10.1007/978-3-540-74936-3_22\n\nWedel, A., Pock, T., Zach, C., Bischof, H., & Cremers, D. (2009). An improved\nalgorithm for TV-L 1 optical flow. In Statistical and geometrical approaches\nto visual motion analysis (pp. 23-45). Springer, Berlin, Heidelberg.\nDOI:10.1007/978-3-642-03061-1_2\n\nP\u00e9rez, J. S., Meinhardt-Llopis, E., & Facciolo, G. (2013). TV-L1 optical flow\nestimation. Image Processing On Line, 2013, 137-150. DOI:10.5201/ipol.2013.26\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "registration.phase_cross_correlation()", "path": "api/skimage.registration#skimage.registration.phase_cross_correlation", "type": "registration", "text": "\nEfficient subpixel image translation registration by cross-correlation.\n\nThis code gives the same precision as the FFT upsampled cross-correlation in a\nfraction of the computation time and with reduced memory requirements. It\nobtains an initial estimate of the cross-correlation peak by an FFT and then\nrefines the shift estimation by upsampling the DFT only in a small\nneighborhood of that estimate by means of a matrix-multiply DFT.\n\nReference image.\n\nImage to register. Must be same dimensionality as `reference_image`.\n\nUpsampling factor. Images will be registered to within `1 / upsample_factor`\nof a pixel. For example `upsample_factor == 20` means the images will be\nregistered within 1/20th of a pixel. Default is 1 (no upsampling). Not used if\nany of `reference_mask` or `moving_mask` is not None.\n\nDefines how the algorithm interprets input data. \u201creal\u201d means data will be\nFFT\u2019d to compute the correlation, while \u201cfourier\u201d data will bypass FFT of\ninput data. Case insensitive. Not used if any of `reference_mask` or\n`moving_mask` is not None.\n\nReturns error and phase difference if on, otherwise only shifts are returned.\nHas noeffect if any of `reference_mask` or `moving_mask` is not None. In this\ncase only shifts is returned.\n\nBoolean mask for `reference_image`. The mask should evaluate to `True` (or 1)\non valid pixels. `reference_mask` should have the same shape as\n`reference_image`.\n\nBoolean mask for `moving_image`. The mask should evaluate to `True` (or 1) on\nvalid pixels. `moving_mask` should have the same shape as `moving_image`. If\n`None`, `reference_mask` will be used.\n\nMinimum allowed overlap ratio between images. The correlation for translations\ncorresponding with an overlap ratio lower than this threshold will be ignored.\nA lower `overlap_ratio` leads to smaller maximum translation, while a higher\n`overlap_ratio` leads to greater robustness against spurious matches due to\nsmall overlap between masked images. Used only if one of `reference_mask` or\n`moving_mask` is None.\n\nShift vector (in pixels) required to register `moving_image` with\n`reference_image`. Axis ordering is consistent with numpy (e.g. Z, Y, X)\n\nTranslation invariant normalized RMS error between `reference_image` and\n`moving_image`.\n\nGlobal phase difference between the two images (should be zero if images are\nnon-negative).\n\nManuel Guizar-Sicairos, Samuel T. Thurman, and James R. Fienup, \u201cEfficient\nsubpixel image registration algorithms,\u201d Optics Letters 33, 156-158 (2008).\nDOI:10.1364/OL.33.000156\n\nJames R. Fienup, \u201cInvariant error metrics for image reconstruction\u201d Optics\nLetters 36, 8352-8357 (1997). DOI:10.1364/AO.36.008352\n\nDirk Padfield. Masked Object Registration in the Fourier Domain. IEEE\nTransactions on Image Processing, vol. 21(5), pp. 2706-2718 (2012).\nDOI:10.1109/TIP.2011.2181402\n\nD. Padfield. \u201cMasked FFT registration\u201d. In Proc. Computer Vision and Pattern\nRecognition, pp. 2918-2925 (2010). DOI:10.1109/CVPR.2010.5540032\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "restoration", "path": "api/skimage.restoration", "type": "restoration", "text": "\nImage restoration module.\n\n`skimage.restoration.ball_kernel`(radius, ndim)\n\nCreate a ball kernel for restoration.rolling_ball.\n\n`skimage.restoration.calibrate_denoiser`(\u2026)\n\nCalibrate a denoising function and return optimal J-invariant version.\n\n`skimage.restoration.cycle_spin`(x, func, \u2026)\n\nCycle spinning (repeatedly apply func to shifted versions of x).\n\n`skimage.restoration.denoise_bilateral`(image)\n\nDenoise image using bilateral filter.\n\n`skimage.restoration.denoise_nl_means`(image)\n\nPerform non-local means denoising on 2-D or 3-D grayscale images, and 2-D RGB\nimages.\n\n`skimage.restoration.denoise_tv_bregman`(\u2026)\n\nPerform total-variation denoising using split-Bregman optimization.\n\n`skimage.restoration.denoise_tv_chambolle`(image)\n\nPerform total-variation denoising on n-dimensional images.\n\n`skimage.restoration.denoise_wavelet`(image[, \u2026])\n\nPerform wavelet denoising on an image.\n\n`skimage.restoration.ellipsoid_kernel`(shape, \u2026)\n\nCreate an ellipoid kernel for restoration.rolling_ball.\n\n`skimage.restoration.estimate_sigma`(image[, \u2026])\n\nRobust wavelet-based estimator of the (Gaussian) noise standard deviation.\n\n`skimage.restoration.inpaint_biharmonic`(\u2026)\n\nInpaint masked points in image with biharmonic equations.\n\n`skimage.restoration.richardson_lucy`(image, psf)\n\nRichardson-Lucy deconvolution.\n\n`skimage.restoration.rolling_ball`(image, *[, \u2026])\n\nEstimate background intensity by rolling/translating a kernel.\n\n`skimage.restoration.unsupervised_wiener`(\u2026)\n\nUnsupervised Wiener-Hunt deconvolution.\n\n`skimage.restoration.unwrap_phase`(image[, \u2026])\n\nRecover the original from a wrapped phase image.\n\n`skimage.restoration.wiener`(image, psf, balance)\n\nWiener-Hunt deconvolution\n\nCreate a ball kernel for restoration.rolling_ball.\n\nRadius of the ball.\n\nNumber of dimensions of the ball. `ndim` should match the dimensionality of\nthe image the kernel will be applied to.\n\nThe kernel containing the surface intensity of the top half of the ellipsoid.\n\nSee also\n\nCalibrate a denoising function and return optimal J-invariant version.\n\nThe returned function is partially evaluated with optimal parameter values set\nfor denoising the input image.\n\nInput data to be denoised (converted using `img_as_float`).\n\nDenoising function to be calibrated.\n\nRanges of parameters for `denoise_function` to be calibrated over.\n\nStride used in masking procedure that converts `denoise_function` to\nJ-invariance.\n\nWhether to approximate the self-supervised loss used to evaluate the denoiser\nby only computing it on one masked version of the image. If False, the runtime\nwill be a factor of `stride**image.ndim` longer.\n\nIf True, return parameters and losses in addition to the calibrated denoising\nfunction\n\nThe optimal J-invariant version of `denoise_function`.\n\nList of parameters tested for `denoise_function`, as a dictionary of kwargs\nSelf-supervised loss for each set of parameters in `parameters_tested`.\n\nThe calibration procedure uses a self-supervised mean-square-error loss to\nevaluate the performance of J-invariant versions of `denoise_function`. The\nminimizer of the self-supervised loss is also the minimizer of the ground-\ntruth loss (i.e., the true MSE error) [1]. The returned function can be used\non the original noisy image, or other images with similar characteristics.\n\nat the expense of increasing its runtime. It has no effect on the runtime of\nthe calibration.\n\nJ. Batson & L. Royer. Noise2Self: Blind Denoising by Self-Supervision,\nInternational Conference on Machine Learning, p. 524-533 (2019).\n\nCycle spinning (repeatedly apply func to shifted versions of x).\n\nData for input to `func`.\n\nA function to apply to circularly shifted versions of `x`. Should take `x` as\nits first argument. Any additional arguments can be supplied via `func_kw`.\n\nIf an integer, shifts in `range(0, max_shifts+1)` will be used along each axis\nof `x`. If a tuple, `range(0, max_shifts[i]+1)` will be along axis i.\n\nThe step size for the shifts applied along axis, i, are:: `range((0,\nmax_shifts[i]+1, shift_steps[i]))`. If an integer is provided, the same step\nsize is used for all axes.\n\nThe number of parallel threads to use during cycle spinning. If set to `None`,\nthe full set of available cores are used.\n\nWhether to treat the final axis as channels (no cycle shifts are performed\nover the channels axis).\n\nAdditional keyword arguments to supply to `func`.\n\nThe output of `func(x, **func_kw)` averaged over all combinations of the\nspecified axis shifts.\n\nCycle spinning was proposed as a way to approach shift-invariance via\nperforming several circular shifts of a shift-variant transform [1].\n\nFor a n-level discrete wavelet transforms, one may wish to perform all shifts\nup to `max_shifts = 2**n - 1`. In practice, much of the benefit can often be\nrealized with only a small number of shifts per axis.\n\nFor transforms such as the blockwise discrete cosine transform, one may wish\nto evaluate shifts up to the block size used by the transform.\n\nR.R. Coifman and D.L. Donoho. \u201cTranslation-Invariant De-Noising\u201d. Wavelets and\nStatistics, Lecture Notes in Statistics, vol.103. Springer, New York, 1995,\npp.125-150. DOI:10.1007/978-1-4612-2544-7_9\n\nDenoise image using bilateral filter.\n\nInput image, 2D grayscale or RGB.\n\nWindow size for filtering. If win_size is not specified, it is calculated as\n`max(5, 2 * ceil(3 * sigma_spatial) + 1)`.\n\nStandard deviation for grayvalue/color distance (radiometric similarity). A\nlarger value results in averaging of pixels with larger radiometric\ndifferences. Note, that the image will be converted using the `img_as_float`\nfunction and thus the standard deviation is in respect to the range `[0, 1]`.\nIf the value is `None` the standard deviation of the `image` will be used.\n\nStandard deviation for range distance. A larger value results in averaging of\npixels with larger spatial differences.\n\nNumber of discrete values for Gaussian weights of color filtering. A larger\nvalue results in improved accuracy.\n\nHow to handle values outside the image borders. See `numpy.pad` for detail.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nWhether the last axis of the image is to be interpreted as multiple channels\nor another spatial dimension.\n\nDenoised image.\n\nThis is an edge-preserving, denoising filter. It averages pixels based on\ntheir spatial closeness and radiometric similarity [1].\n\nSpatial closeness is measured by the Gaussian function of the Euclidean\ndistance between two pixels and a certain standard deviation\n(`sigma_spatial`).\n\nRadiometric similarity is measured by the Gaussian function of the Euclidean\ndistance between two color values and a certain standard deviation\n(`sigma_color`).\n\nC. Tomasi and R. Manduchi. \u201cBilateral Filtering for Gray and Color Images.\u201d\nIEEE International Conference on Computer Vision (1998) 839-846.\nDOI:10.1109/ICCV.1998.710815\n\nRank filters\n\nPerform non-local means denoising on 2-D or 3-D grayscale images, and 2-D RGB\nimages.\n\nInput image to be denoised, which can be 2D or 3D, and grayscale or RGB (for\n2D images only, see `multichannel` parameter).\n\nSize of patches used for denoising.\n\nMaximal distance in pixels where to search patches used for denoising.\n\nCut-off distance (in gray levels). The higher h, the more permissive one is in\naccepting patches. A higher h results in a smoother image, at the expense of\nblurring features. For a Gaussian noise of standard deviation sigma, a rule of\nthumb is to choose the value of h to be sigma of slightly less.\n\nWhether the last axis of the image is to be interpreted as multiple channels\nor another spatial dimension.\n\nIf True (default value), a fast version of the non-local means algorithm is\nused. If False, the original version of non-local means is used. See the Notes\nsection for more details about the algorithms.\n\nThe standard deviation of the (Gaussian) noise. If provided, a more robust\ncomputation of patch weights is computed that takes the expected noise\nvariance into account (see Notes below).\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nDenoised image, of same shape as `image`.\n\nThe non-local means algorithm is well suited for denoising images with\nspecific textures. The principle of the algorithm is to average the value of a\ngiven pixel with values of other pixels in a limited neighbourhood, provided\nthat the patches centered on the other pixels are similar enough to the patch\ncentered on the pixel of interest.\n\nIn the original version of the algorithm [1], corresponding to `fast=False`,\nthe computational complexity is:\n\nHence, changing the size of patches or their maximal distance has a strong\neffect on computing times, especially for 3-D images.\n\nHowever, the default behavior corresponds to `fast_mode=True`, for which\nanother version of non-local means [2] is used, corresponding to a complexity\nof:\n\nThe computing time depends only weakly on the patch size, thanks to the\ncomputation of the integral of patches distances for a given shift, that\nreduces the number of operations [1]. Therefore, this algorithm executes\nfaster than the classic algorithm (`fast_mode=False`), at the expense of using\ntwice as much memory. This implementation has been proven to be more efficient\ncompared to other alternatives, see e.g. [3].\n\nCompared to the classic algorithm, all pixels of a patch contribute to the\ndistance to another patch with the same weight, no matter their distance to\nthe center of the patch. This coarser computation of the distance can result\nin a slightly poorer denoising performance. Moreover, for small images (images\nwith a linear size that is only a few times the patch size), the classic\nalgorithm can be faster due to boundary effects.\n\nThe image is padded using the `reflect` mode of `skimage.util.pad` before\ndenoising.\n\nIf the noise standard deviation, `sigma`, is provided a more robust\ncomputation of patch weights is used. Subtracting the known noise variance\nfrom the computed patch distances improves the estimates of patch similarity,\ngiving a moderate improvement to denoising performance [4]. It was also\nmentioned as an option for the fast variant of the algorithm in [3].\n\nWhen `sigma` is provided, a smaller `h` should typically be used to avoid\noversmoothing. The optimal value for `h` depends on the image content and\nnoise level, but a reasonable starting point is `h = 0.8 * sigma` when\n`fast_mode` is `True`, or `h = 0.6 * sigma` when `fast_mode` is `False`.\n\nA. Buades, B. Coll, & J-M. Morel. A non-local algorithm for image denoising.\nIn CVPR 2005, Vol. 2, pp. 60-65, IEEE. DOI:10.1109/CVPR.2005.38\n\nJ. Darbon, A. Cunha, T.F. Chan, S. Osher, and G.J. Jensen, Fast nonlocal\nfiltering applied to electron cryomicroscopy, in 5th IEEE International\nSymposium on Biomedical Imaging: From Nano to Macro, 2008, pp. 1331-1334.\nDOI:10.1109/ISBI.2008.4541250\n\nJacques Froment. Parameter-Free Fast Pixelwise Non-Local Means Denoising.\nImage Processing On Line, 2014, vol. 4, pp. 300-326. DOI:10.5201/ipol.2014.120\n\nA. Buades, B. Coll, & J-M. Morel. Non-Local Means Denoising. Image Processing\nOn Line, 2011, vol. 1, pp. 208-212. DOI:10.5201/ipol.2011.bcm_nlm\n\nPerform total-variation denoising using split-Bregman optimization.\n\nTotal-variation denoising (also know as total-variation regularization) tries\nto find an image with less total-variation under the constraint of being\nsimilar to the input image, which is controlled by the regularization\nparameter ([1], [2], [3], [4]).\n\nInput data to be denoised (converted using img_as_float`).\n\nDenoising weight. The smaller the `weight`, the more denoising (at the expense\nof less similarity to the `input`). The regularization parameter `lambda` is\nchosen as `2 * weight`.\n\nRelative difference of the value of the cost function that determines the stop\ncriterion. The algorithm stops when:\n\nMaximal number of iterations used for the optimization.\n\nSwitch between isotropic and anisotropic TV denoising.\n\nApply total-variation denoising separately for each channel. This option\nshould be true for color images, otherwise the denoising is also applied in\nthe channels dimension.\n\nDenoised image.\n\nhttps://en.wikipedia.org/wiki/Total_variation_denoising\n\nTom Goldstein and Stanley Osher, \u201cThe Split Bregman Method For L1 Regularized\nProblems\u201d, ftp://ftp.math.ucla.edu/pub/camreport/cam08-29.pdf\n\nPascal Getreuer, \u201cRudin\u2013Osher\u2013Fatemi Total Variation Denoising using Split\nBregman\u201d in Image Processing On Line on 2012\u201305\u201319,\nhttps://www.ipol.im/pub/art/2012/g-tvd/article_lr.pdf\n\nhttps://web.math.ucsb.edu/~cgarcia/UGProjects/BregmanAlgorithms_JacquelineBush.pdf\n\nPerform total-variation denoising on n-dimensional images.\n\nInput data to be denoised. `image` can be of any numeric type, but it is cast\ninto an ndarray of floats for the computation of the denoised image.\n\nDenoising weight. The greater `weight`, the more denoising (at the expense of\nfidelity to `input`).\n\nRelative difference of the value of the cost function that determines the stop\ncriterion. The algorithm stops when:\n\n(E_(n-1) - E_n) < eps * E_0\n\nMaximal number of iterations used for the optimization.\n\nApply total-variation denoising separately for each channel. This option\nshould be true for color images, otherwise the denoising is also applied in\nthe channels dimension.\n\nDenoised image.\n\nMake sure to set the multichannel parameter appropriately for color images.\n\nThe principle of total variation denoising is explained in\nhttps://en.wikipedia.org/wiki/Total_variation_denoising\n\nThe principle of total variation denoising is to minimize the total variation\nof the image, which can be roughly described as the integral of the norm of\nthe image gradient. Total variation denoising tends to produce \u201ccartoon-like\u201d\nimages, that is, piecewise-constant images.\n\nThis code is an implementation of the algorithm of Rudin, Fatemi and Osher\nthat was proposed by Chambolle in [1].\n\nA. Chambolle, An algorithm for total variation minimization and applications,\nJournal of Mathematical Imaging and Vision, Springer, 2004, 20, 89-97.\n\n2D example on astronaut image:\n\n3D example on synthetic data:\n\nPerform wavelet denoising on an image.\n\nInput data to be denoised. `image` can be of any numeric type, but it is cast\ninto an ndarray of floats for the computation of the denoised image.\n\nThe noise standard deviation used when computing the wavelet detail\ncoefficient threshold(s). When None (default), the noise standard deviation is\nestimated via the method in [2].\n\nThe type of wavelet to perform and can be any of the options `pywt.wavelist`\noutputs. The default is `\u2018db1\u2019`. For example, `wavelet` can be any of `{'db2',\n'haar', 'sym9'}` and many more.\n\nAn optional argument to choose the type of denoising performed. It noted that\nchoosing soft thresholding given additive noise finds the best approximation\nof the original image.\n\nThe number of wavelet decomposition levels to use. The default is three less\nthan the maximum number of possible decomposition levels.\n\nApply wavelet denoising separately for each channel (where channels correspond\nto the final axis of the array).\n\nIf True and multichannel True, do the wavelet denoising in the YCbCr\ncolorspace instead of the RGB color space. This typically results in better\nperformance for RGB images.\n\nThresholding method to be used. The currently supported methods are\n\u201cBayesShrink\u201d [1] and \u201cVisuShrink\u201d [2]. Defaults to \u201cBayesShrink\u201d.\n\nIf False, no rescaling of the user-provided `sigma` will be performed. The\ndefault of `True` rescales sigma appropriately if the image is rescaled\ninternally.\n\nNew in version 0.16: `rescale_sigma` was introduced in 0.16\n\nDenoised image.\n\nThe wavelet domain is a sparse representation of the image, and can be thought\nof similarly to the frequency domain of the Fourier transform. Sparse\nrepresentations have most values zero or near-zero and truly random noise is\n(usually) represented by many small values in the wavelet domain. Setting all\nvalues below some threshold to 0 reduces the noise in the image, but larger\nthresholds also decrease the detail present in the image.\n\nIf the input is 3D, this function performs wavelet denoising on each color\nplane separately.\n\nChanged in version 0.16: For floating point inputs, the original input range\nis maintained and there is no clipping applied to the output. Other input\ntypes will be converted to a floating point value in the range [-1, 1] or [0,\n1] depending on the input image range. Unless `rescale_sigma = False`, any\ninternal rescaling applied to the `image` will also be applied to `sigma` to\nmaintain the same relative amplitude.\n\nMany wavelet coefficient thresholding approaches have been proposed. By\ndefault, `denoise_wavelet` applies BayesShrink, which is an adaptive\nthresholding method that computes separate thresholds for each wavelet sub-\nband as described in [1].\n\nIf `method == \"VisuShrink\"`, a single \u201cuniversal threshold\u201d is applied to all\nwavelet detail coefficients as described in [2]. This threshold is designed to\nremove all Gaussian noise at a given `sigma` with high probability, but tends\nto produce images that appear overly smooth.\n\nAlthough any of the wavelets from `PyWavelets` can be selected, the\nthresholding methods assume an orthogonal wavelet transform and may not choose\nthe threshold appropriately for biorthogonal wavelets. Orthogonal wavelets are\ndesirable because white noise in the input remains white noise in the\nsubbands. Biorthogonal wavelets lead to colored noise in the subbands.\nAdditionally, the orthogonal wavelets in PyWavelets are orthonormal so that\nnoise variance in the subbands remains identical to the noise variance of the\ninput. Example orthogonal wavelets are the Daubechies (e.g. \u2018db2\u2019) or symmlet\n(e.g. \u2018sym2\u2019) families.\n\nChang, S. Grace, Bin Yu, and Martin Vetterli. \u201cAdaptive wavelet thresholding\nfor image denoising and compression.\u201d Image Processing, IEEE Transactions on\n9.9 (2000): 1532-1546. DOI:10.1109/83.862633\n\nD. L. Donoho and I. M. Johnstone. \u201cIdeal spatial adaptation by wavelet\nshrinkage.\u201d Biometrika 81.3 (1994): 425-455. DOI:10.1093/biomet/81.3.425\n\nCreate an ellipoid kernel for restoration.rolling_ball.\n\nLength of the principal axis of the ellipsoid (excluding the intensity axis).\nThe kernel needs to have the same dimensionality as the image it will be\napplied to.\n\nLength of the intensity axis of the ellipsoid.\n\nThe kernel containing the surface intensity of the top half of the ellipsoid.\n\nSee also\n\nUse rolling-ball algorithm for estimating background intensity\n\nRobust wavelet-based estimator of the (Gaussian) noise standard deviation.\n\nImage for which to estimate the noise standard deviation.\n\nIf true, average the channel estimates of `sigma`. Otherwise return a list of\nsigmas corresponding to each channel.\n\nEstimate sigma separately for each channel.\n\nEstimated noise standard deviation(s). If `multichannel` is True and\n`average_sigmas` is False, a separate noise estimate for each channel is\nreturned. Otherwise, the average of the individual channel estimates is\nreturned.\n\nThis function assumes the noise follows a Gaussian distribution. The\nestimation algorithm is based on the median absolute deviation of the wavelet\ndetail coefficients as described in section 4.2 of [1].\n\nD. L. Donoho and I. M. Johnstone. \u201cIdeal spatial adaptation by wavelet\nshrinkage.\u201d Biometrika 81.3 (1994): 425-455. DOI:10.1093/biomet/81.3.425\n\nInpaint masked points in image with biharmonic equations.\n\nInput image.\n\nArray of pixels to be inpainted. Have to be the same shape as one of the\n\u2018image\u2019 channels. Unknown pixels have to be represented with 1, known pixels -\nwith 0.\n\nIf True, the last `image` dimension is considered as a color channel,\notherwise as spatial.\n\nInput image with masked pixels inpainted.\n\nN.S.Hoang, S.B.Damelin, \u201cOn surface completion and image inpainting by\nbiharmonic functions: numerical aspects\u201d, arXiv:1707.06567\n\nC. K. Chui and H. N. Mhaskar, MRA Contextual-Recovery Extension of Smooth\nFunctions on Manifolds, Appl. and Comp. Harmonic Anal., 28 (2010), 104-113,\nDOI:10.1016/j.acha.2009.04.004\n\nRichardson-Lucy deconvolution.\n\nInput degraded image (can be N dimensional).\n\nThe point spread function.\n\nNumber of iterations. This parameter plays the role of regularisation.\n\nTrue by default. If true, pixel value of the result above 1 or under -1 are\nthresholded for skimage pipeline compatibility.\n\nValue below which intermediate results become 0 to avoid division by small\nnumbers.\n\nThe deconvolved image.\n\nhttps://en.wikipedia.org/wiki/Richardson%E2%80%93Lucy_deconvolution\n\nEstimate background intensity by rolling/translating a kernel.\n\nThis rolling ball algorithm estimates background intensity for a ndimage in\ncase of uneven exposure. It is a generalization of the frequently used rolling\nball algorithm [1].\n\nThe image to be filtered.\n\nRadius of a ball shaped kernel to be rolled/translated in the image. Used if\n`kernel = None`.\n\nThe kernel to be rolled/translated in the image. It must have the same number\nof dimensions as `image`. Kernel is filled with the intensity of the kernel at\nthat position.\n\nIf `False` (default) assumes that none of the values in `image` are `np.nan`,\nand uses a faster implementation.\n\nThe maximum number of threads to use. If `None` use the OpenMP default value;\ntypically equal to the maximum number of virtual cores. Note: This is an upper\nlimit to the number of threads. The exact number is determined by the system\u2019s\nOpenMP library.\n\nThe estimated background of the image.\n\nFor the pixel that has its background intensity estimated (without loss of\ngenerality at `center`) the rolling ball method centers `kernel` under it and\nraises the kernel until the surface touches the image umbra at some\n`pos=(y,x)`. The background intensity is then estimated using the image\nintensity at that position (`image[pos]`) plus the difference of\n`kernel[center] - kernel[pos]`.\n\nThis algorithm assumes that dark pixels correspond to the background. If you\nhave a bright background, invert the image before passing it to the function,\ne.g., using `utils.invert`. See the gallery example for details.\n\nThis algorithm is sensitive to noise (in particular salt-and-pepper noise). If\nthis is a problem in your image, you can apply mild gaussian smoothing before\npassing the image to this function.\n\nSternberg, Stanley R. \u201cBiomedical image processing.\u201d Computer 1 (1983): 22-34.\nDOI:10.1109/MC.1983.1654163\n\nUse rolling-ball algorithm for estimating background intensity\n\nUnsupervised Wiener-Hunt deconvolution.\n\nReturn the deconvolution with a Wiener-Hunt approach, where the\nhyperparameters are automatically estimated. The algorithm is a stochastic\niterative process (Gibbs sampler) described in the reference below. See also\n`wiener` function.\n\nThe input degraded image.\n\nThe impulse response (input image\u2019s space) or the transfer function (Fourier\nspace). Both are accepted. The transfer function is automatically recognized\nas being complex (`np.iscomplexobj(psf)`).\n\nThe regularisation operator. The Laplacian by default. It can be an impulse\nresponse or a transfer function, as for the psf.\n\nDictionary of parameters for the Gibbs sampler. See below.\n\nTrue by default. If true, pixel values of the result above 1 or under -1 are\nthresholded for skimage pipeline compatibility.\n\nThe deconvolved image (the posterior mean).\n\nThe keys `noise` and `prior` contain the chain list of noise and prior\nprecision respectively.\n\nThe stopping criterion: the norm of the difference between to successive\napproximated solution (empirical mean of object samples, see Notes section).\n1e-4 by default.\n\nThe number of sample to ignore to start computation of the mean. 15 by\ndefault.\n\nThe minimum number of iterations. 30 by default.\n\nThe maximum number of iterations if `threshold` is not satisfied. 200 by\ndefault.\n\nA user provided callable to which is passed, if the function exists, the\ncurrent image sample for whatever purpose. The user can store the sample, or\ncompute other moments than the mean. It has no influence on the algorithm\nexecution and is only for inspection.\n\nThe estimated image is design as the posterior mean of a probability law (from\na Bayesian analysis). The mean is defined as a sum over all the possible\nimages weighted by their respective probability. Given the size of the\nproblem, the exact sum is not tractable. This algorithm use of MCMC to draw\nimage under the posterior law. The practical idea is to only draw highly\nprobable images since they have the biggest contribution to the mean. At the\nopposite, the less probable images are drawn less often since their\ncontribution is low. Finally the empirical mean of these samples give us an\nestimation of the mean, and an exact computation with an infinite sample set.\n\nFran\u00e7ois Orieux, Jean-Fran\u00e7ois Giovannelli, and Thomas Rodet, \u201cBayesian\nestimation of regularization and point spread function parameters for Wiener-\nHunt deconvolution\u201d, J. Opt. Soc. Am. A 27, 1593-1607 (2010)\n\nhttps://www.osapublishing.org/josaa/abstract.cfm?URI=josaa-27-7-1593\n\nhttp://research.orieux.fr/files/papers/OGR-JOSA10.pdf\n\nRecover the original from a wrapped phase image.\n\nFrom an image wrapped to lie in the interval [-pi, pi), recover the original,\nunwrapped image.\n\nThe values should be in the range [-pi, pi). If a masked array is provided,\nthe masked entries will not be changed, and their values will not be used to\nguide the unwrapping of neighboring, unmasked values. Masked 1D arrays are not\nallowed, and will raise a `ValueError`.\n\nWhen an element of the sequence is `True`, the unwrapping process will regard\nthe edges along the corresponding axis of the image to be connected and use\nthis connectivity to guide the phase unwrapping process. If only a single\nboolean is given, it will apply to all axes. Wrap around is not supported for\n1D arrays.\n\nUnwrapping 2D or 3D images uses random initialization. This sets the seed of\nthe PRNG to achieve deterministic behavior.\n\nUnwrapped image of the same shape as the input. If the input `image` was a\nmasked array, the mask will be preserved.\n\nIf called with a masked 1D array or called with a 1D array and\n`wrap_around=True`.\n\nMiguel Arevallilo Herraez, David R. Burton, Michael J. Lalor, and Munther A.\nGdeisat, \u201cFast two-dimensional phase-unwrapping algorithm based on sorting by\nreliability following a noncontinuous path\u201d, Journal Applied Optics, Vol. 41,\nNo. 35 (2002) 7437,\n\nAbdul-Rahman, H., Gdeisat, M., Burton, D., & Lalor, M., \u201cFast three-\ndimensional phase-unwrapping algorithm based on sorting by reliability\nfollowing a non-continuous path. In W. Osten, C. Gorecki, & E. L. Novak\n(Eds.), Optical Metrology (2005) 32\u201340, International Society for Optics and\nPhotonics.\n\nPhase Unwrapping\n\nWiener-Hunt deconvolution\n\nReturn the deconvolution with a Wiener-Hunt approach (i.e. with Fourier\ndiagonalisation).\n\nInput degraded image\n\nPoint Spread Function. This is assumed to be the impulse response (input image\nspace) if the data-type is real, or the transfer function (Fourier space) if\nthe data-type is complex. There is no constraints on the shape of the impulse\nresponse. The transfer function must be of shape `(M, N)` if `is_real is\nTrue`, `(M, N // 2 + 1)` otherwise (see `np.fft.rfftn`).\n\nThe regularisation parameter value that tunes the balance between the data\nadequacy that improve frequency restoration and the prior adequacy that reduce\nfrequency restoration (to avoid noise artifacts).\n\nThe regularisation operator. The Laplacian by default. It can be an impulse\nresponse or a transfer function, as for the psf. Shape constraint is the same\nas for the `psf` parameter.\n\nTrue by default. Specify if `psf` and `reg` are provided with hermitian\nhypothesis, that is only half of the frequency plane is provided (due to the\nredundancy of Fourier transform of real signal). It\u2019s apply only if `psf`\nand/or `reg` are provided as transfer function. For the hermitian property see\n`uft` module or `np.fft.rfftn`.\n\nTrue by default. If True, pixel values of the result above 1 or under -1 are\nthresholded for skimage pipeline compatibility.\n\nThe deconvolved image.\n\nThis function applies the Wiener filter to a noisy and degraded image by an\nimpulse response (or PSF). If the data model is\n\nwhere \\\\(n\\\\) is noise, \\\\(H\\\\) the PSF and \\\\(x\\\\) the unknown original\nimage, the Wiener filter is\n\nwhere \\\\(F\\\\) and \\\\(F^\\dagger\\\\) are the Fourier and inverse Fourier\ntransforms respectively, \\\\(\\Lambda_H\\\\) the transfer function (or the Fourier\ntransform of the PSF, see [Hunt] below) and \\\\(\\Lambda_D\\\\) the filter to\npenalize the restored image frequencies (Laplacian by default, that is\npenalization of high frequency). The parameter \\\\(\\lambda\\\\) tunes the balance\nbetween the data (that tends to increase high frequency, even those coming\nfrom noise), and the regularization.\n\nThese methods are then specific to a prior model. Consequently, the\napplication or the true image nature must corresponds to the prior model. By\ndefault, the prior model (Laplacian) introduce image smoothness or pixel\ncorrelation. It can also be interpreted as high-frequency penalization to\ncompensate the instability of the solution with respect to the data (sometimes\ncalled noise amplification or \u201cexplosive\u201d solution).\n\nFinally, the use of Fourier space implies a circulant property of \\\\(H\\\\), see\n[Hunt].\n\nFran\u00e7ois Orieux, Jean-Fran\u00e7ois Giovannelli, and Thomas Rodet, \u201cBayesian\nestimation of regularization and point spread function parameters for Wiener-\nHunt deconvolution\u201d, J. Opt. Soc. Am. A 27, 1593-1607 (2010)\n\nhttps://www.osapublishing.org/josaa/abstract.cfm?URI=josaa-27-7-1593\n\nhttp://research.orieux.fr/files/papers/OGR-JOSA10.pdf\n\nB. R. Hunt \u201cA matrix theory proof of the discrete convolution theorem\u201d, IEEE\nTrans. on Audio and Electroacoustics, vol. au-19, no. 4, pp. 285-288, dec.\n1971\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "restoration.ball_kernel()", "path": "api/skimage.restoration#skimage.restoration.ball_kernel", "type": "restoration", "text": "\nCreate a ball kernel for restoration.rolling_ball.\n\nRadius of the ball.\n\nNumber of dimensions of the ball. `ndim` should match the dimensionality of\nthe image the kernel will be applied to.\n\nThe kernel containing the surface intensity of the top half of the ellipsoid.\n\nSee also\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "restoration.calibrate_denoiser()", "path": "api/skimage.restoration#skimage.restoration.calibrate_denoiser", "type": "restoration", "text": "\nCalibrate a denoising function and return optimal J-invariant version.\n\nThe returned function is partially evaluated with optimal parameter values set\nfor denoising the input image.\n\nInput data to be denoised (converted using `img_as_float`).\n\nDenoising function to be calibrated.\n\nRanges of parameters for `denoise_function` to be calibrated over.\n\nStride used in masking procedure that converts `denoise_function` to\nJ-invariance.\n\nWhether to approximate the self-supervised loss used to evaluate the denoiser\nby only computing it on one masked version of the image. If False, the runtime\nwill be a factor of `stride**image.ndim` longer.\n\nIf True, return parameters and losses in addition to the calibrated denoising\nfunction\n\nThe optimal J-invariant version of `denoise_function`.\n\nList of parameters tested for `denoise_function`, as a dictionary of kwargs\nSelf-supervised loss for each set of parameters in `parameters_tested`.\n\nThe calibration procedure uses a self-supervised mean-square-error loss to\nevaluate the performance of J-invariant versions of `denoise_function`. The\nminimizer of the self-supervised loss is also the minimizer of the ground-\ntruth loss (i.e., the true MSE error) [1]. The returned function can be used\non the original noisy image, or other images with similar characteristics.\n\nat the expense of increasing its runtime. It has no effect on the runtime of\nthe calibration.\n\nJ. Batson & L. Royer. Noise2Self: Blind Denoising by Self-Supervision,\nInternational Conference on Machine Learning, p. 524-533 (2019).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "restoration.cycle_spin()", "path": "api/skimage.restoration#skimage.restoration.cycle_spin", "type": "restoration", "text": "\nCycle spinning (repeatedly apply func to shifted versions of x).\n\nData for input to `func`.\n\nA function to apply to circularly shifted versions of `x`. Should take `x` as\nits first argument. Any additional arguments can be supplied via `func_kw`.\n\nIf an integer, shifts in `range(0, max_shifts+1)` will be used along each axis\nof `x`. If a tuple, `range(0, max_shifts[i]+1)` will be along axis i.\n\nThe step size for the shifts applied along axis, i, are:: `range((0,\nmax_shifts[i]+1, shift_steps[i]))`. If an integer is provided, the same step\nsize is used for all axes.\n\nThe number of parallel threads to use during cycle spinning. If set to `None`,\nthe full set of available cores are used.\n\nWhether to treat the final axis as channels (no cycle shifts are performed\nover the channels axis).\n\nAdditional keyword arguments to supply to `func`.\n\nThe output of `func(x, **func_kw)` averaged over all combinations of the\nspecified axis shifts.\n\nCycle spinning was proposed as a way to approach shift-invariance via\nperforming several circular shifts of a shift-variant transform [1].\n\nFor a n-level discrete wavelet transforms, one may wish to perform all shifts\nup to `max_shifts = 2**n - 1`. In practice, much of the benefit can often be\nrealized with only a small number of shifts per axis.\n\nFor transforms such as the blockwise discrete cosine transform, one may wish\nto evaluate shifts up to the block size used by the transform.\n\nR.R. Coifman and D.L. Donoho. \u201cTranslation-Invariant De-Noising\u201d. Wavelets and\nStatistics, Lecture Notes in Statistics, vol.103. Springer, New York, 1995,\npp.125-150. DOI:10.1007/978-1-4612-2544-7_9\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "restoration.denoise_bilateral()", "path": "api/skimage.restoration#skimage.restoration.denoise_bilateral", "type": "restoration", "text": "\nDenoise image using bilateral filter.\n\nInput image, 2D grayscale or RGB.\n\nWindow size for filtering. If win_size is not specified, it is calculated as\n`max(5, 2 * ceil(3 * sigma_spatial) + 1)`.\n\nStandard deviation for grayvalue/color distance (radiometric similarity). A\nlarger value results in averaging of pixels with larger radiometric\ndifferences. Note, that the image will be converted using the `img_as_float`\nfunction and thus the standard deviation is in respect to the range `[0, 1]`.\nIf the value is `None` the standard deviation of the `image` will be used.\n\nStandard deviation for range distance. A larger value results in averaging of\npixels with larger spatial differences.\n\nNumber of discrete values for Gaussian weights of color filtering. A larger\nvalue results in improved accuracy.\n\nHow to handle values outside the image borders. See `numpy.pad` for detail.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nWhether the last axis of the image is to be interpreted as multiple channels\nor another spatial dimension.\n\nDenoised image.\n\nThis is an edge-preserving, denoising filter. It averages pixels based on\ntheir spatial closeness and radiometric similarity [1].\n\nSpatial closeness is measured by the Gaussian function of the Euclidean\ndistance between two pixels and a certain standard deviation\n(`sigma_spatial`).\n\nRadiometric similarity is measured by the Gaussian function of the Euclidean\ndistance between two color values and a certain standard deviation\n(`sigma_color`).\n\nC. Tomasi and R. Manduchi. \u201cBilateral Filtering for Gray and Color Images.\u201d\nIEEE International Conference on Computer Vision (1998) 839-846.\nDOI:10.1109/ICCV.1998.710815\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "restoration.denoise_nl_means()", "path": "api/skimage.restoration#skimage.restoration.denoise_nl_means", "type": "restoration", "text": "\nPerform non-local means denoising on 2-D or 3-D grayscale images, and 2-D RGB\nimages.\n\nInput image to be denoised, which can be 2D or 3D, and grayscale or RGB (for\n2D images only, see `multichannel` parameter).\n\nSize of patches used for denoising.\n\nMaximal distance in pixels where to search patches used for denoising.\n\nCut-off distance (in gray levels). The higher h, the more permissive one is in\naccepting patches. A higher h results in a smoother image, at the expense of\nblurring features. For a Gaussian noise of standard deviation sigma, a rule of\nthumb is to choose the value of h to be sigma of slightly less.\n\nWhether the last axis of the image is to be interpreted as multiple channels\nor another spatial dimension.\n\nIf True (default value), a fast version of the non-local means algorithm is\nused. If False, the original version of non-local means is used. See the Notes\nsection for more details about the algorithms.\n\nThe standard deviation of the (Gaussian) noise. If provided, a more robust\ncomputation of patch weights is computed that takes the expected noise\nvariance into account (see Notes below).\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nDenoised image, of same shape as `image`.\n\nThe non-local means algorithm is well suited for denoising images with\nspecific textures. The principle of the algorithm is to average the value of a\ngiven pixel with values of other pixels in a limited neighbourhood, provided\nthat the patches centered on the other pixels are similar enough to the patch\ncentered on the pixel of interest.\n\nIn the original version of the algorithm [1], corresponding to `fast=False`,\nthe computational complexity is:\n\nHence, changing the size of patches or their maximal distance has a strong\neffect on computing times, especially for 3-D images.\n\nHowever, the default behavior corresponds to `fast_mode=True`, for which\nanother version of non-local means [2] is used, corresponding to a complexity\nof:\n\nThe computing time depends only weakly on the patch size, thanks to the\ncomputation of the integral of patches distances for a given shift, that\nreduces the number of operations [1]. Therefore, this algorithm executes\nfaster than the classic algorithm (`fast_mode=False`), at the expense of using\ntwice as much memory. This implementation has been proven to be more efficient\ncompared to other alternatives, see e.g. [3].\n\nCompared to the classic algorithm, all pixels of a patch contribute to the\ndistance to another patch with the same weight, no matter their distance to\nthe center of the patch. This coarser computation of the distance can result\nin a slightly poorer denoising performance. Moreover, for small images (images\nwith a linear size that is only a few times the patch size), the classic\nalgorithm can be faster due to boundary effects.\n\nThe image is padded using the `reflect` mode of `skimage.util.pad` before\ndenoising.\n\nIf the noise standard deviation, `sigma`, is provided a more robust\ncomputation of patch weights is used. Subtracting the known noise variance\nfrom the computed patch distances improves the estimates of patch similarity,\ngiving a moderate improvement to denoising performance [4]. It was also\nmentioned as an option for the fast variant of the algorithm in [3].\n\nWhen `sigma` is provided, a smaller `h` should typically be used to avoid\noversmoothing. The optimal value for `h` depends on the image content and\nnoise level, but a reasonable starting point is `h = 0.8 * sigma` when\n`fast_mode` is `True`, or `h = 0.6 * sigma` when `fast_mode` is `False`.\n\nA. Buades, B. Coll, & J-M. Morel. A non-local algorithm for image denoising.\nIn CVPR 2005, Vol. 2, pp. 60-65, IEEE. DOI:10.1109/CVPR.2005.38\n\nJ. Darbon, A. Cunha, T.F. Chan, S. Osher, and G.J. Jensen, Fast nonlocal\nfiltering applied to electron cryomicroscopy, in 5th IEEE International\nSymposium on Biomedical Imaging: From Nano to Macro, 2008, pp. 1331-1334.\nDOI:10.1109/ISBI.2008.4541250\n\nJacques Froment. Parameter-Free Fast Pixelwise Non-Local Means Denoising.\nImage Processing On Line, 2014, vol. 4, pp. 300-326. DOI:10.5201/ipol.2014.120\n\nA. Buades, B. Coll, & J-M. Morel. Non-Local Means Denoising. Image Processing\nOn Line, 2011, vol. 1, pp. 208-212. DOI:10.5201/ipol.2011.bcm_nlm\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "restoration.denoise_tv_bregman()", "path": "api/skimage.restoration#skimage.restoration.denoise_tv_bregman", "type": "restoration", "text": "\nPerform total-variation denoising using split-Bregman optimization.\n\nTotal-variation denoising (also know as total-variation regularization) tries\nto find an image with less total-variation under the constraint of being\nsimilar to the input image, which is controlled by the regularization\nparameter ([1], [2], [3], [4]).\n\nInput data to be denoised (converted using img_as_float`).\n\nDenoising weight. The smaller the `weight`, the more denoising (at the expense\nof less similarity to the `input`). The regularization parameter `lambda` is\nchosen as `2 * weight`.\n\nRelative difference of the value of the cost function that determines the stop\ncriterion. The algorithm stops when:\n\nMaximal number of iterations used for the optimization.\n\nSwitch between isotropic and anisotropic TV denoising.\n\nApply total-variation denoising separately for each channel. This option\nshould be true for color images, otherwise the denoising is also applied in\nthe channels dimension.\n\nDenoised image.\n\nhttps://en.wikipedia.org/wiki/Total_variation_denoising\n\nTom Goldstein and Stanley Osher, \u201cThe Split Bregman Method For L1 Regularized\nProblems\u201d, ftp://ftp.math.ucla.edu/pub/camreport/cam08-29.pdf\n\nPascal Getreuer, \u201cRudin\u2013Osher\u2013Fatemi Total Variation Denoising using Split\nBregman\u201d in Image Processing On Line on 2012\u201305\u201319,\nhttps://www.ipol.im/pub/art/2012/g-tvd/article_lr.pdf\n\nhttps://web.math.ucsb.edu/~cgarcia/UGProjects/BregmanAlgorithms_JacquelineBush.pdf\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "restoration.denoise_tv_chambolle()", "path": "api/skimage.restoration#skimage.restoration.denoise_tv_chambolle", "type": "restoration", "text": "\nPerform total-variation denoising on n-dimensional images.\n\nInput data to be denoised. `image` can be of any numeric type, but it is cast\ninto an ndarray of floats for the computation of the denoised image.\n\nDenoising weight. The greater `weight`, the more denoising (at the expense of\nfidelity to `input`).\n\nRelative difference of the value of the cost function that determines the stop\ncriterion. The algorithm stops when:\n\n(E_(n-1) - E_n) < eps * E_0\n\nMaximal number of iterations used for the optimization.\n\nApply total-variation denoising separately for each channel. This option\nshould be true for color images, otherwise the denoising is also applied in\nthe channels dimension.\n\nDenoised image.\n\nMake sure to set the multichannel parameter appropriately for color images.\n\nThe principle of total variation denoising is explained in\nhttps://en.wikipedia.org/wiki/Total_variation_denoising\n\nThe principle of total variation denoising is to minimize the total variation\nof the image, which can be roughly described as the integral of the norm of\nthe image gradient. Total variation denoising tends to produce \u201ccartoon-like\u201d\nimages, that is, piecewise-constant images.\n\nThis code is an implementation of the algorithm of Rudin, Fatemi and Osher\nthat was proposed by Chambolle in [1].\n\nA. Chambolle, An algorithm for total variation minimization and applications,\nJournal of Mathematical Imaging and Vision, Springer, 2004, 20, 89-97.\n\n2D example on astronaut image:\n\n3D example on synthetic data:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "restoration.denoise_wavelet()", "path": "api/skimage.restoration#skimage.restoration.denoise_wavelet", "type": "restoration", "text": "\nPerform wavelet denoising on an image.\n\nInput data to be denoised. `image` can be of any numeric type, but it is cast\ninto an ndarray of floats for the computation of the denoised image.\n\nThe noise standard deviation used when computing the wavelet detail\ncoefficient threshold(s). When None (default), the noise standard deviation is\nestimated via the method in [2].\n\nThe type of wavelet to perform and can be any of the options `pywt.wavelist`\noutputs. The default is `\u2018db1\u2019`. For example, `wavelet` can be any of `{'db2',\n'haar', 'sym9'}` and many more.\n\nAn optional argument to choose the type of denoising performed. It noted that\nchoosing soft thresholding given additive noise finds the best approximation\nof the original image.\n\nThe number of wavelet decomposition levels to use. The default is three less\nthan the maximum number of possible decomposition levels.\n\nApply wavelet denoising separately for each channel (where channels correspond\nto the final axis of the array).\n\nIf True and multichannel True, do the wavelet denoising in the YCbCr\ncolorspace instead of the RGB color space. This typically results in better\nperformance for RGB images.\n\nThresholding method to be used. The currently supported methods are\n\u201cBayesShrink\u201d [1] and \u201cVisuShrink\u201d [2]. Defaults to \u201cBayesShrink\u201d.\n\nIf False, no rescaling of the user-provided `sigma` will be performed. The\ndefault of `True` rescales sigma appropriately if the image is rescaled\ninternally.\n\nNew in version 0.16: `rescale_sigma` was introduced in 0.16\n\nDenoised image.\n\nThe wavelet domain is a sparse representation of the image, and can be thought\nof similarly to the frequency domain of the Fourier transform. Sparse\nrepresentations have most values zero or near-zero and truly random noise is\n(usually) represented by many small values in the wavelet domain. Setting all\nvalues below some threshold to 0 reduces the noise in the image, but larger\nthresholds also decrease the detail present in the image.\n\nIf the input is 3D, this function performs wavelet denoising on each color\nplane separately.\n\nChanged in version 0.16: For floating point inputs, the original input range\nis maintained and there is no clipping applied to the output. Other input\ntypes will be converted to a floating point value in the range [-1, 1] or [0,\n1] depending on the input image range. Unless `rescale_sigma = False`, any\ninternal rescaling applied to the `image` will also be applied to `sigma` to\nmaintain the same relative amplitude.\n\nMany wavelet coefficient thresholding approaches have been proposed. By\ndefault, `denoise_wavelet` applies BayesShrink, which is an adaptive\nthresholding method that computes separate thresholds for each wavelet sub-\nband as described in [1].\n\nIf `method == \"VisuShrink\"`, a single \u201cuniversal threshold\u201d is applied to all\nwavelet detail coefficients as described in [2]. This threshold is designed to\nremove all Gaussian noise at a given `sigma` with high probability, but tends\nto produce images that appear overly smooth.\n\nAlthough any of the wavelets from `PyWavelets` can be selected, the\nthresholding methods assume an orthogonal wavelet transform and may not choose\nthe threshold appropriately for biorthogonal wavelets. Orthogonal wavelets are\ndesirable because white noise in the input remains white noise in the\nsubbands. Biorthogonal wavelets lead to colored noise in the subbands.\nAdditionally, the orthogonal wavelets in PyWavelets are orthonormal so that\nnoise variance in the subbands remains identical to the noise variance of the\ninput. Example orthogonal wavelets are the Daubechies (e.g. \u2018db2\u2019) or symmlet\n(e.g. \u2018sym2\u2019) families.\n\nChang, S. Grace, Bin Yu, and Martin Vetterli. \u201cAdaptive wavelet thresholding\nfor image denoising and compression.\u201d Image Processing, IEEE Transactions on\n9.9 (2000): 1532-1546. DOI:10.1109/83.862633\n\nD. L. Donoho and I. M. Johnstone. \u201cIdeal spatial adaptation by wavelet\nshrinkage.\u201d Biometrika 81.3 (1994): 425-455. DOI:10.1093/biomet/81.3.425\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "restoration.ellipsoid_kernel()", "path": "api/skimage.restoration#skimage.restoration.ellipsoid_kernel", "type": "restoration", "text": "\nCreate an ellipoid kernel for restoration.rolling_ball.\n\nLength of the principal axis of the ellipsoid (excluding the intensity axis).\nThe kernel needs to have the same dimensionality as the image it will be\napplied to.\n\nLength of the intensity axis of the ellipsoid.\n\nThe kernel containing the surface intensity of the top half of the ellipsoid.\n\nSee also\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "restoration.estimate_sigma()", "path": "api/skimage.restoration#skimage.restoration.estimate_sigma", "type": "restoration", "text": "\nRobust wavelet-based estimator of the (Gaussian) noise standard deviation.\n\nImage for which to estimate the noise standard deviation.\n\nIf true, average the channel estimates of `sigma`. Otherwise return a list of\nsigmas corresponding to each channel.\n\nEstimate sigma separately for each channel.\n\nEstimated noise standard deviation(s). If `multichannel` is True and\n`average_sigmas` is False, a separate noise estimate for each channel is\nreturned. Otherwise, the average of the individual channel estimates is\nreturned.\n\nThis function assumes the noise follows a Gaussian distribution. The\nestimation algorithm is based on the median absolute deviation of the wavelet\ndetail coefficients as described in section 4.2 of [1].\n\nD. L. Donoho and I. M. Johnstone. \u201cIdeal spatial adaptation by wavelet\nshrinkage.\u201d Biometrika 81.3 (1994): 425-455. DOI:10.1093/biomet/81.3.425\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "restoration.inpaint_biharmonic()", "path": "api/skimage.restoration#skimage.restoration.inpaint_biharmonic", "type": "restoration", "text": "\nInpaint masked points in image with biharmonic equations.\n\nInput image.\n\nArray of pixels to be inpainted. Have to be the same shape as one of the\n\u2018image\u2019 channels. Unknown pixels have to be represented with 1, known pixels -\nwith 0.\n\nIf True, the last `image` dimension is considered as a color channel,\notherwise as spatial.\n\nInput image with masked pixels inpainted.\n\nN.S.Hoang, S.B.Damelin, \u201cOn surface completion and image inpainting by\nbiharmonic functions: numerical aspects\u201d, arXiv:1707.06567\n\nC. K. Chui and H. N. Mhaskar, MRA Contextual-Recovery Extension of Smooth\nFunctions on Manifolds, Appl. and Comp. Harmonic Anal., 28 (2010), 104-113,\nDOI:10.1016/j.acha.2009.04.004\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "restoration.richardson_lucy()", "path": "api/skimage.restoration#skimage.restoration.richardson_lucy", "type": "restoration", "text": "\nRichardson-Lucy deconvolution.\n\nInput degraded image (can be N dimensional).\n\nThe point spread function.\n\nNumber of iterations. This parameter plays the role of regularisation.\n\nTrue by default. If true, pixel value of the result above 1 or under -1 are\nthresholded for skimage pipeline compatibility.\n\nValue below which intermediate results become 0 to avoid division by small\nnumbers.\n\nThe deconvolved image.\n\nhttps://en.wikipedia.org/wiki/Richardson%E2%80%93Lucy_deconvolution\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "restoration.rolling_ball()", "path": "api/skimage.restoration#skimage.restoration.rolling_ball", "type": "restoration", "text": "\nEstimate background intensity by rolling/translating a kernel.\n\nThis rolling ball algorithm estimates background intensity for a ndimage in\ncase of uneven exposure. It is a generalization of the frequently used rolling\nball algorithm [1].\n\nThe image to be filtered.\n\nRadius of a ball shaped kernel to be rolled/translated in the image. Used if\n`kernel = None`.\n\nThe kernel to be rolled/translated in the image. It must have the same number\nof dimensions as `image`. Kernel is filled with the intensity of the kernel at\nthat position.\n\nIf `False` (default) assumes that none of the values in `image` are `np.nan`,\nand uses a faster implementation.\n\nThe maximum number of threads to use. If `None` use the OpenMP default value;\ntypically equal to the maximum number of virtual cores. Note: This is an upper\nlimit to the number of threads. The exact number is determined by the system\u2019s\nOpenMP library.\n\nThe estimated background of the image.\n\nFor the pixel that has its background intensity estimated (without loss of\ngenerality at `center`) the rolling ball method centers `kernel` under it and\nraises the kernel until the surface touches the image umbra at some\n`pos=(y,x)`. The background intensity is then estimated using the image\nintensity at that position (`image[pos]`) plus the difference of\n`kernel[center] - kernel[pos]`.\n\nThis algorithm assumes that dark pixels correspond to the background. If you\nhave a bright background, invert the image before passing it to the function,\ne.g., using `utils.invert`. See the gallery example for details.\n\nThis algorithm is sensitive to noise (in particular salt-and-pepper noise). If\nthis is a problem in your image, you can apply mild gaussian smoothing before\npassing the image to this function.\n\nSternberg, Stanley R. \u201cBiomedical image processing.\u201d Computer 1 (1983): 22-34.\nDOI:10.1109/MC.1983.1654163\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "restoration.unsupervised_wiener()", "path": "api/skimage.restoration#skimage.restoration.unsupervised_wiener", "type": "restoration", "text": "\nUnsupervised Wiener-Hunt deconvolution.\n\nReturn the deconvolution with a Wiener-Hunt approach, where the\nhyperparameters are automatically estimated. The algorithm is a stochastic\niterative process (Gibbs sampler) described in the reference below. See also\n`wiener` function.\n\nThe input degraded image.\n\nThe impulse response (input image\u2019s space) or the transfer function (Fourier\nspace). Both are accepted. The transfer function is automatically recognized\nas being complex (`np.iscomplexobj(psf)`).\n\nThe regularisation operator. The Laplacian by default. It can be an impulse\nresponse or a transfer function, as for the psf.\n\nDictionary of parameters for the Gibbs sampler. See below.\n\nTrue by default. If true, pixel values of the result above 1 or under -1 are\nthresholded for skimage pipeline compatibility.\n\nThe deconvolved image (the posterior mean).\n\nThe keys `noise` and `prior` contain the chain list of noise and prior\nprecision respectively.\n\nThe stopping criterion: the norm of the difference between to successive\napproximated solution (empirical mean of object samples, see Notes section).\n1e-4 by default.\n\nThe number of sample to ignore to start computation of the mean. 15 by\ndefault.\n\nThe minimum number of iterations. 30 by default.\n\nThe maximum number of iterations if `threshold` is not satisfied. 200 by\ndefault.\n\nA user provided callable to which is passed, if the function exists, the\ncurrent image sample for whatever purpose. The user can store the sample, or\ncompute other moments than the mean. It has no influence on the algorithm\nexecution and is only for inspection.\n\nThe estimated image is design as the posterior mean of a probability law (from\na Bayesian analysis). The mean is defined as a sum over all the possible\nimages weighted by their respective probability. Given the size of the\nproblem, the exact sum is not tractable. This algorithm use of MCMC to draw\nimage under the posterior law. The practical idea is to only draw highly\nprobable images since they have the biggest contribution to the mean. At the\nopposite, the less probable images are drawn less often since their\ncontribution is low. Finally the empirical mean of these samples give us an\nestimation of the mean, and an exact computation with an infinite sample set.\n\nFran\u00e7ois Orieux, Jean-Fran\u00e7ois Giovannelli, and Thomas Rodet, \u201cBayesian\nestimation of regularization and point spread function parameters for Wiener-\nHunt deconvolution\u201d, J. Opt. Soc. Am. A 27, 1593-1607 (2010)\n\nhttps://www.osapublishing.org/josaa/abstract.cfm?URI=josaa-27-7-1593\n\nhttp://research.orieux.fr/files/papers/OGR-JOSA10.pdf\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "restoration.unwrap_phase()", "path": "api/skimage.restoration#skimage.restoration.unwrap_phase", "type": "restoration", "text": "\nRecover the original from a wrapped phase image.\n\nFrom an image wrapped to lie in the interval [-pi, pi), recover the original,\nunwrapped image.\n\nThe values should be in the range [-pi, pi). If a masked array is provided,\nthe masked entries will not be changed, and their values will not be used to\nguide the unwrapping of neighboring, unmasked values. Masked 1D arrays are not\nallowed, and will raise a `ValueError`.\n\nWhen an element of the sequence is `True`, the unwrapping process will regard\nthe edges along the corresponding axis of the image to be connected and use\nthis connectivity to guide the phase unwrapping process. If only a single\nboolean is given, it will apply to all axes. Wrap around is not supported for\n1D arrays.\n\nUnwrapping 2D or 3D images uses random initialization. This sets the seed of\nthe PRNG to achieve deterministic behavior.\n\nUnwrapped image of the same shape as the input. If the input `image` was a\nmasked array, the mask will be preserved.\n\nIf called with a masked 1D array or called with a 1D array and\n`wrap_around=True`.\n\nMiguel Arevallilo Herraez, David R. Burton, Michael J. Lalor, and Munther A.\nGdeisat, \u201cFast two-dimensional phase-unwrapping algorithm based on sorting by\nreliability following a noncontinuous path\u201d, Journal Applied Optics, Vol. 41,\nNo. 35 (2002) 7437,\n\nAbdul-Rahman, H., Gdeisat, M., Burton, D., & Lalor, M., \u201cFast three-\ndimensional phase-unwrapping algorithm based on sorting by reliability\nfollowing a non-continuous path. In W. Osten, C. Gorecki, & E. L. Novak\n(Eds.), Optical Metrology (2005) 32\u201340, International Society for Optics and\nPhotonics.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "restoration.wiener()", "path": "api/skimage.restoration#skimage.restoration.wiener", "type": "restoration", "text": "\nWiener-Hunt deconvolution\n\nReturn the deconvolution with a Wiener-Hunt approach (i.e. with Fourier\ndiagonalisation).\n\nInput degraded image\n\nPoint Spread Function. This is assumed to be the impulse response (input image\nspace) if the data-type is real, or the transfer function (Fourier space) if\nthe data-type is complex. There is no constraints on the shape of the impulse\nresponse. The transfer function must be of shape `(M, N)` if `is_real is\nTrue`, `(M, N // 2 + 1)` otherwise (see `np.fft.rfftn`).\n\nThe regularisation parameter value that tunes the balance between the data\nadequacy that improve frequency restoration and the prior adequacy that reduce\nfrequency restoration (to avoid noise artifacts).\n\nThe regularisation operator. The Laplacian by default. It can be an impulse\nresponse or a transfer function, as for the psf. Shape constraint is the same\nas for the `psf` parameter.\n\nTrue by default. Specify if `psf` and `reg` are provided with hermitian\nhypothesis, that is only half of the frequency plane is provided (due to the\nredundancy of Fourier transform of real signal). It\u2019s apply only if `psf`\nand/or `reg` are provided as transfer function. For the hermitian property see\n`uft` module or `np.fft.rfftn`.\n\nTrue by default. If True, pixel values of the result above 1 or under -1 are\nthresholded for skimage pipeline compatibility.\n\nThe deconvolved image.\n\nThis function applies the Wiener filter to a noisy and degraded image by an\nimpulse response (or PSF). If the data model is\n\nwhere \\\\(n\\\\) is noise, \\\\(H\\\\) the PSF and \\\\(x\\\\) the unknown original\nimage, the Wiener filter is\n\nwhere \\\\(F\\\\) and \\\\(F^\\dagger\\\\) are the Fourier and inverse Fourier\ntransforms respectively, \\\\(\\Lambda_H\\\\) the transfer function (or the Fourier\ntransform of the PSF, see [Hunt] below) and \\\\(\\Lambda_D\\\\) the filter to\npenalize the restored image frequencies (Laplacian by default, that is\npenalization of high frequency). The parameter \\\\(\\lambda\\\\) tunes the balance\nbetween the data (that tends to increase high frequency, even those coming\nfrom noise), and the regularization.\n\nThese methods are then specific to a prior model. Consequently, the\napplication or the true image nature must corresponds to the prior model. By\ndefault, the prior model (Laplacian) introduce image smoothness or pixel\ncorrelation. It can also be interpreted as high-frequency penalization to\ncompensate the instability of the solution with respect to the data (sometimes\ncalled noise amplification or \u201cexplosive\u201d solution).\n\nFinally, the use of Fourier space implies a circulant property of \\\\(H\\\\), see\n[Hunt].\n\nFran\u00e7ois Orieux, Jean-Fran\u00e7ois Giovannelli, and Thomas Rodet, \u201cBayesian\nestimation of regularization and point spread function parameters for Wiener-\nHunt deconvolution\u201d, J. Opt. Soc. Am. A 27, 1593-1607 (2010)\n\nhttps://www.osapublishing.org/josaa/abstract.cfm?URI=josaa-27-7-1593\n\nhttp://research.orieux.fr/files/papers/OGR-JOSA10.pdf\n\nB. R. Hunt \u201cA matrix theory proof of the discrete convolution theorem\u201d, IEEE\nTrans. on Audio and Electroacoustics, vol. au-19, no. 4, pp. 285-288, dec.\n1971\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "segmentation", "path": "api/skimage.segmentation", "type": "segmentation", "text": "\n`skimage.segmentation.active_contour`(image, snake)\n\nActive contour model.\n\n`skimage.segmentation.chan_vese`(image[, mu, \u2026])\n\nChan-Vese segmentation algorithm.\n\n`skimage.segmentation.checkerboard_level_set`(\u2026)\n\nCreate a checkerboard level set with binary values.\n\n`skimage.segmentation.circle_level_set`(\u2026[, \u2026])\n\nCreate a circle level set with binary values.\n\n`skimage.segmentation.clear_border`(labels[, \u2026])\n\nClear objects connected to the label image border.\n\n`skimage.segmentation.disk_level_set`(\u2026[, \u2026])\n\nCreate a disk level set with binary values.\n\n`skimage.segmentation.expand_labels`(label_image)\n\nExpand labels in label image by `distance` pixels without overlapping.\n\n`skimage.segmentation.felzenszwalb`(image[, \u2026])\n\nComputes Felsenszwalb\u2019s efficient graph based image segmentation.\n\n`skimage.segmentation.find_boundaries`(label_img)\n\nReturn bool array where boundaries between labeled regions are True.\n\n`skimage.segmentation.flood`(image, seed_point, *)\n\nMask corresponding to a flood fill.\n\n`skimage.segmentation.flood_fill`(image, \u2026)\n\nPerform flood filling on an image.\n\n`skimage.segmentation.inverse_gaussian_gradient`(image)\n\nInverse of gradient magnitude.\n\n`skimage.segmentation.join_segmentations`(s1, s2)\n\nReturn the join of the two input segmentations.\n\n`skimage.segmentation.mark_boundaries`(image, \u2026)\n\nReturn image with boundaries between labeled regions highlighted.\n\n`skimage.segmentation.morphological_chan_vese`(\u2026)\n\nMorphological Active Contours without Edges (MorphACWE)\n\n`skimage.segmentation.morphological_geodesic_active_contour`(\u2026)\n\nMorphological Geodesic Active Contours (MorphGAC).\n\n`skimage.segmentation.quickshift`(image[, \u2026])\n\nSegments image using quickshift clustering in Color-(x,y) space.\n\n`skimage.segmentation.random_walker`(data, labels)\n\nRandom walker algorithm for segmentation from markers.\n\n`skimage.segmentation.relabel_sequential`(\u2026)\n\nRelabel arbitrary labels to {`offset`, \u2026\n\n`skimage.segmentation.slic`(image[, \u2026])\n\nSegments image using k-means clustering in Color-(x,y,z) space.\n\n`skimage.segmentation.watershed`(image[, \u2026])\n\nFind watershed basins in `image` flooded from given `markers`.\n\nActive contour model.\n\nActive contours by fitting snakes to features of images. Supports single and\nmultichannel 2D images. Snakes can be periodic (for segmentation) or have\nfixed and/or free ends. The output snake has the same length as the input\nboundary. As the number of points is constant, make sure that the initial\nsnake has enough points to capture the details of the final contour.\n\nInput image.\n\nInitial snake coordinates. For periodic boundary conditions, endpoints must\nnot be duplicated.\n\nSnake length shape parameter. Higher values makes snake contract faster.\n\nSnake smoothness shape parameter. Higher values makes snake smoother.\n\nControls attraction to brightness. Use negative values to attract toward dark\nregions.\n\nControls attraction to edges. Use negative values to repel snake from edges.\n\nExplicit time stepping parameter.\n\nMaximum pixel distance to move per iteration.\n\nMaximum iterations to optimize snake shape.\n\nConvergence criteria.\n\nBoundary conditions for the contour. Can be one of \u2018periodic\u2019, \u2018free\u2019,\n\u2018fixed\u2019, \u2018free-fixed\u2019, or \u2018fixed-free\u2019. \u2018periodic\u2019 attaches the two ends of\nthe snake, \u2018fixed\u2019 holds the end-points in place, and \u2018free\u2019 allows free\nmovement of the ends. \u2018fixed\u2019 and \u2018free\u2019 can be combined by parsing \u2018fixed-\nfree\u2019, \u2018free-fixed\u2019. Parsing \u2018fixed-fixed\u2019 or \u2018free-free\u2019 yields same\nbehaviour as \u2018fixed\u2019 and \u2018free\u2019, respectively.\n\nThis option remains for compatibility purpose only and has no effect. It was\nintroduced in 0.16 with the `'xy'` option, but since 0.18, only the `'rc'`\noption is valid. Coordinates must be set in a row-column format.\n\nOptimised snake, same shape as input parameter.\n\nKass, M.; Witkin, A.; Terzopoulos, D. \u201cSnakes: Active contour models\u201d.\nInternational Journal of Computer Vision 1 (4): 321 (1988).\nDOI:10.1007/BF00133570\n\nCreate and smooth image:\n\nInitialize spline:\n\nFit spline to image:\n\nChan-Vese segmentation algorithm.\n\nActive contour model by evolving a level set. Can be used to segment objects\nwithout clearly defined boundaries.\n\nGrayscale image to be segmented.\n\n\u2018edge length\u2019 weight parameter. Higher `mu` values will produce a \u2018round\u2019\nedge, while values closer to zero will detect smaller objects.\n\n\u2018difference from average\u2019 weight parameter for the output region with value\n\u2018True\u2019. If it is lower than `lambda2`, this region will have a larger range of\nvalues than the other.\n\n\u2018difference from average\u2019 weight parameter for the output region with value\n\u2018False\u2019. If it is lower than `lambda1`, this region will have a larger range\nof values than the other.\n\nLevel set variation tolerance between iterations. If the L2 norm difference\nbetween the level sets of successive iterations normalized by the area of the\nimage is below this value, the algorithm will assume that the solution was\nreached.\n\nMaximum number of iterations allowed before the algorithm interrupts itself.\n\nA multiplication factor applied at calculations for each step, serves to\naccelerate the algorithm. While higher values may speed up the algorithm, they\nmay also lead to convergence problems.\n\nDefines the starting level set used by the algorithm. If a string is inputted,\na level set that matches the image size will automatically be generated.\nAlternatively, it is possible to define a custom level set, which should be an\narray of float values, with the same shape as \u2018image\u2019. Accepted string values\nare as follows.\n\nthe starting level set is defined as sin(x/5*pi)*sin(y/5*pi), where x and y\nare pixel coordinates. This level set has fast convergence, but may fail to\ndetect implicit edges.\n\nthe starting level set is defined as the opposite of the distance from the\ncenter of the image minus half of the minimum value between image width and\nimage height. This is somewhat slower, but is more likely to properly detect\nimplicit edges.\n\nthe starting level set is defined as the opposite of the distance from the\ncenter of the image minus a quarter of the minimum value between image width\nand image height.\n\nIf set to True, the return value will be a tuple containing the three return\nvalues (see below). If set to False which is the default value, only the\n\u2018segmentation\u2019 array will be returned.\n\nSegmentation produced by the algorithm.\n\nFinal level set computed by the algorithm.\n\nShows the evolution of the \u2018energy\u2019 for each step of the algorithm. This\nshould allow to check whether the algorithm converged.\n\nThe Chan-Vese Algorithm is designed to segment objects without clearly defined\nboundaries. This algorithm is based on level sets that are evolved iteratively\nto minimize an energy, which is defined by weighted values corresponding to\nthe sum of differences intensity from the average value outside the segmented\nregion, the sum of differences from the average value inside the segmented\nregion, and a term which is dependent on the length of the boundary of the\nsegmented region.\n\nThis algorithm was first proposed by Tony Chan and Luminita Vese, in a\npublication entitled \u201cAn Active Contour Model Without Edges\u201d [1].\n\nThis implementation of the algorithm is somewhat simplified in the sense that\nthe area factor \u2018nu\u2019 described in the original paper is not implemented, and\nis only suitable for grayscale images.\n\nTypical values for `lambda1` and `lambda2` are 1. If the \u2018background\u2019 is very\ndifferent from the segmented object in terms of distribution (for example, a\nuniform black image with figures of varying intensity), then these values\nshould be different from each other.\n\nTypical values for mu are between 0 and 1, though higher values can be used\nwhen dealing with shapes with very ill-defined contours.\n\nThe \u2018energy\u2019 which this algorithm tries to minimize is defined as the sum of\nthe differences from the average within the region squared and weighed by the\n\u2018lambda\u2019 factors to which is added the length of the contour multiplied by the\n\u2018mu\u2019 factor.\n\nSupports 2D grayscale images only, and does not implement the area term\ndescribed in the original article.\n\nAn Active Contour Model without Edges, Tony Chan and Luminita Vese, Scale-\nSpace Theories in Computer Vision, 1999, DOI:10.1007/3-540-48236-9_13\n\nChan-Vese Segmentation, Pascal Getreuer Image Processing On Line, 2 (2012),\npp. 214-224, DOI:10.5201/ipol.2012.g-cv\n\nThe Chan-Vese Algorithm - Project Report, Rami Cohen, 2011 arXiv:1107.2782\n\nCreate a checkerboard level set with binary values.\n\nShape of the image.\n\nSize of the squares of the checkerboard. It defaults to 5.\n\nBinary level set of the checkerboard.\n\nSee also\n\nCreate a circle level set with binary values.\n\nShape of the image\n\nCoordinates of the center of the circle given in (row, column). If not given,\nit defaults to the center of the image.\n\nRadius of the circle. If not given, it is set to the 75% of the smallest image\ndimension.\n\nBinary level set of the circle with the given `radius` and `center`.\n\nNew in version 0.17: This function is deprecated and will be removed in\nscikit-image 0.19. Please use the function named `disk_level_set` instead.\n\nSee also\n\nClear objects connected to the label image border.\n\nImaging data labels.\n\nThe width of the border examined. By default, only objects that touch the\noutside of the image are removed.\n\nCleared objects are set to this value.\n\nWhether or not to manipulate the labels array in-place.\n\nImage data mask. Objects in labels image overlapping with False pixels of mask\nwill be removed. If defined, the argument buffer_size will be ignored.\n\nImaging data labels with cleared borders\n\nCreate a disk level set with binary values.\n\nShape of the image\n\nCoordinates of the center of the disk given in (row, column). If not given, it\ndefaults to the center of the image.\n\nRadius of the disk. If not given, it is set to the 75% of the smallest image\ndimension.\n\nBinary level set of the disk with the given `radius` and `center`.\n\nSee also\n\nExpand labels in label image by `distance` pixels without overlapping.\n\nGiven a label image, `expand_labels` grows label regions (connected\ncomponents) outwards by up to `distance` pixels without overflowing into\nneighboring regions. More specifically, each background pixel that is within\nEuclidean distance of <= `distance` pixels of a connected component is\nassigned the label of that connected component. Where multiple connected\ncomponents are within `distance` pixels of a background pixel, the label value\nof the closest connected component will be assigned (see Notes for the case of\nmultiple labels at equal distance).\n\nlabel image\n\nEuclidean distance in pixels by which to grow the labels. Default is one.\n\nLabeled array, where all connected regions have been enlarged\n\nSee also\n\nWhere labels are spaced more than `distance` pixels are apart, this is\nequivalent to a morphological dilation with a disc or hyperball of radius\n`distance`. However, in contrast to a morphological dilation, `expand_labels`\nwill not expand a label region into a neighboring region.\n\nThis implementation of `expand_labels` is derived from CellProfiler [1], where\nit is known as module \u201cIdentifySecondaryObjects (Distance-N)\u201d [2].\n\nThere is an important edge case when a pixel has the same distance to multiple\nregions, as it is not defined which region expands into that space. Here, the\nexact behavior depends on the upstream implementation of\n`scipy.ndimage.distance_transform_edt`.\n\nhttps://cellprofiler.org\n\nhttps://github.com/CellProfiler/CellProfiler/blob/082930ea95add7b72243a4fa3d39ae5145995e9c/cellprofiler/modules/identifysecondaryobjects.py#L559\n\nLabels will not overwrite each other:\n\nIn case of ties, behavior is undefined, but currently resolves to the label\nclosest to `(0,) * ndim` in lexicographical order.\n\nComputes Felsenszwalb\u2019s efficient graph based image segmentation.\n\nProduces an oversegmentation of a multichannel (i.e. RGB) image using a fast,\nminimum spanning tree based clustering on the image grid. The parameter\n`scale` sets an observation level. Higher scale means less and larger\nsegments. `sigma` is the diameter of a Gaussian kernel, used for smoothing the\nimage prior to segmentation.\n\nThe number of produced segments as well as their size can only be controlled\nindirectly through `scale`. Segment size within an image can vary greatly\ndepending on local contrast.\n\nFor RGB images, the algorithm uses the euclidean distance between pixels in\ncolor space.\n\nInput image.\n\nFree parameter. Higher means larger clusters.\n\nWidth (standard deviation) of Gaussian kernel used in preprocessing.\n\nMinimum component size. Enforced using postprocessing.\n\nWhether the last axis of the image is to be interpreted as multiple channels.\nA value of False, for a 3D image, is not currently supported.\n\nInteger mask indicating segment labels.\n\nThe `k` parameter used in the original paper renamed to `scale` here.\n\nEfficient graph-based image segmentation, Felzenszwalb, P.F. and Huttenlocher,\nD.P. International Journal of Computer Vision, 2004\n\nReturn bool array where boundaries between labeled regions are True.\n\nAn array in which different regions are labeled with either different integers\nor boolean values.\n\nA pixel is considered a boundary pixel if any of its neighbors has a different\nlabel. `connectivity` controls which pixels are considered neighbors. A\nconnectivity of 1 (default) means pixels sharing an edge (in 2D) or a face (in\n3D) will be considered neighbors. A connectivity of `label_img.ndim` means\npixels sharing a corner will be considered neighbors.\n\nHow to mark the boundaries:\n\nFor modes \u2018inner\u2019 and \u2018outer\u2019, a definition of a background label is required.\nSee `mode` for descriptions of these two.\n\nA bool image where `True` represents a boundary pixel. For `mode` equal to\n\u2018subpixel\u2019, `boundaries.shape[i]` is equal to `2 * label_img.shape[i] - 1` for\nall `i` (a pixel is inserted in between all other pairs of pixels).\n\nMask corresponding to a flood fill.\n\nStarting at a specific `seed_point`, connected points equal or within\n`tolerance` of the seed value are found.\n\nAn n-dimensional array.\n\nThe point in `image` used as the starting point for the flood fill. If the\nimage is 1D, this point may be given as an integer.\n\nA structuring element used to determine the neighborhood of each evaluated\npixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as\n`image`. If not given, all adjacent pixels are considered as part of the\nneighborhood (fully connected).\n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent\npixels whose squared distance from the center is larger or equal to\n`connectivity` are considered neighbors. Ignored if `selem` is not None.\n\nIf None (default), adjacent values must be strictly equal to the initial value\nof `image` at `seed_point`. This is fastest. If a value is given, a comparison\nwill be done at every point and if within tolerance of the initial value will\nalso be filled (inclusive).\n\nA Boolean array with the same shape as `image` is returned, with True values\nfor areas connected to and equal (or within tolerance of) the seed point. All\nother values are False.\n\nThe conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many\nraster graphics programs. This function returns just the mask representing the\nfill.\n\nIf indices are desired rather than masks for memory reasons, the user can\nsimply run `numpy.nonzero` on the result, save the indices, and discard this\nmask.\n\nFill connected ones with 5, with full connectivity (diagonals included):\n\nFill connected ones with 5, excluding diagonal points (connectivity 1):\n\nFill with a tolerance:\n\nFlood Fill\n\nPerform flood filling on an image.\n\nStarting at a specific `seed_point`, connected points equal or within\n`tolerance` of the seed value are found, then set to `new_value`.\n\nAn n-dimensional array.\n\nThe point in `image` used as the starting point for the flood fill. If the\nimage is 1D, this point may be given as an integer.\n\nNew value to set the entire fill. This must be chosen in agreement with the\ndtype of `image`.\n\nA structuring element used to determine the neighborhood of each evaluated\npixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as\n`image`. If not given, all adjacent pixels are considered as part of the\nneighborhood (fully connected).\n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent\npixels whose squared distance from the center is less than or equal to\n`connectivity` are considered neighbors. Ignored if `selem` is not None.\n\nIf None (default), adjacent values must be strictly equal to the value of\n`image` at `seed_point` to be filled. This is fastest. If a tolerance is\nprovided, adjacent points with values within plus or minus tolerance from the\nseed point are filled (inclusive).\n\nIf True, flood filling is applied to `image` in place. If False, the flood\nfilled result is returned without modifying the input `image` (default).\n\nThis parameter is deprecated and will be removed in version 0.19.0 in favor of\nin_place. If True, flood filling is applied to `image` inplace. If False, the\nflood filled result is returned without modifying the input `image` (default).\n\nAn array with the same shape as `image` is returned, with values in areas\nconnected to and equal (or within tolerance of) the seed point replaced with\n`new_value`.\n\nThe conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many\nraster graphics programs.\n\nFill connected ones with 5, with full connectivity (diagonals included):\n\nFill connected ones with 5, excluding diagonal points (connectivity 1):\n\nFill with a tolerance:\n\nFlood Fill\n\nInverse of gradient magnitude.\n\nCompute the magnitude of the gradients in the image and then inverts the\nresult in the range [0, 1]. Flat areas are assigned values close to 1, while\nareas close to borders are assigned values close to 0.\n\nThis function or a similar one defined by the user should be applied over the\nimage as a preprocessing step before calling\n`morphological_geodesic_active_contour`.\n\nGrayscale image or volume.\n\nControls the steepness of the inversion. A larger value will make the\ntransition between the flat areas and border areas steeper in the resulting\narray.\n\nStandard deviation of the Gaussian filter applied over the image.\n\nPreprocessed image (or volume) suitable for\n`morphological_geodesic_active_contour`.\n\nReturn the join of the two input segmentations.\n\nThe join J of S1 and S2 is defined as the segmentation in which two voxels are\nin the same segment if and only if they are in the same segment in both S1 and\nS2.\n\ns1 and s2 are label fields of the same shape.\n\nThe join segmentation of s1 and s2.\n\nReturn image with boundaries between labeled regions highlighted.\n\nGrayscale or RGB image.\n\nLabel array where regions are marked by different integer values.\n\nRGB color of boundaries in the output image.\n\nRGB color surrounding boundaries in the output image. If None, no outline is\ndrawn.\n\nThe mode for finding boundaries.\n\nWhich label to consider background (this is only useful for modes `inner` and\n`outer`).\n\nAn image in which the boundaries between labels are superimposed on the\noriginal image.\n\nSee also\n\nTrainable segmentation using local features and random forests\n\nMorphological Active Contours without Edges (MorphACWE)\n\nActive contours without edges implemented with morphological operators. It can\nbe used to segment objects in images and volumes without well defined borders.\nIt is required that the inside of the object looks different on average than\nthe outside (i.e., the inner area of the object should be darker or lighter\nthan the outer area on average).\n\nGrayscale image or volume to be segmented.\n\nNumber of iterations to run\n\nInitial level set. If an array is given, it will be binarized and used as the\ninitial level set. If a string is given, it defines the method to generate a\nreasonable initial level set with the shape of the `image`. Accepted values\nare \u2018checkerboard\u2019 and \u2018circle\u2019. See the documentation of\n`checkerboard_level_set` and `circle_level_set` respectively for details about\nhow these level sets are created.\n\nNumber of times the smoothing operator is applied per iteration. Reasonable\nvalues are around 1-4. Larger values lead to smoother segmentations.\n\nWeight parameter for the outer region. If `lambda1` is larger than `lambda2`,\nthe outer region will contain a larger range of values than the inner region.\n\nWeight parameter for the inner region. If `lambda2` is larger than `lambda1`,\nthe inner region will contain a larger range of values than the outer region.\n\nIf given, this function is called once per iteration with the current level\nset as the only argument. This is useful for debugging or for plotting\nintermediate results during the evolution.\n\nFinal segmentation (i.e., the final level set)\n\nSee also\n\nThis is a version of the Chan-Vese algorithm that uses morphological operators\ninstead of solving a partial differential equation (PDE) for the evolution of\nthe contour. The set of morphological operators used in this algorithm are\nproved to be infinitesimally equivalent to the Chan-Vese PDE (see [1]).\nHowever, morphological operators are do not suffer from the numerical\nstability issues typically found in PDEs (it is not necessary to find the\nright time step for the evolution), and are computationally faster.\n\nThe algorithm and its theoretical derivation are described in [1].\n\nA Morphological Approach to Curvature-based Evolution of Curves and Surfaces,\nPablo M\u00e1rquez-Neila, Luis Baumela, Luis \u00c1lvarez. In IEEE Transactions on\nPattern Analysis and Machine Intelligence (PAMI), 2014,\nDOI:10.1109/TPAMI.2013.106\n\nMorphological Geodesic Active Contours (MorphGAC).\n\nGeodesic active contours implemented with morphological operators. It can be\nused to segment objects with visible but noisy, cluttered, broken borders.\n\nPreprocessed image or volume to be segmented. This is very rarely the original\nimage. Instead, this is usually a preprocessed version of the original image\nthat enhances and highlights the borders (or other structures) of the object\nto segment. `morphological_geodesic_active_contour` will try to stop the\ncontour evolution in areas where `gimage` is small. See\n`morphsnakes.inverse_gaussian_gradient` as an example function to perform this\npreprocessing. Note that the quality of\n`morphological_geodesic_active_contour` might greatly depend on this\npreprocessing.\n\nNumber of iterations to run.\n\nInitial level set. If an array is given, it will be binarized and used as the\ninitial level set. If a string is given, it defines the method to generate a\nreasonable initial level set with the shape of the `image`. Accepted values\nare \u2018checkerboard\u2019 and \u2018circle\u2019. See the documentation of\n`checkerboard_level_set` and `circle_level_set` respectively for details about\nhow these level sets are created.\n\nNumber of times the smoothing operator is applied per iteration. Reasonable\nvalues are around 1-4. Larger values lead to smoother segmentations.\n\nAreas of the image with a value smaller than this threshold will be considered\nborders. The evolution of the contour will stop in this areas.\n\nBalloon force to guide the contour in non-informative areas of the image,\ni.e., areas where the gradient of the image is too small to push the contour\ntowards a border. A negative value will shrink the contour, while a positive\nvalue will expand the contour in these areas. Setting this to zero will\ndisable the balloon force.\n\nIf given, this function is called once per iteration with the current level\nset as the only argument. This is useful for debugging or for plotting\nintermediate results during the evolution.\n\nFinal segmentation (i.e., the final level set)\n\nSee also\n\nThis is a version of the Geodesic Active Contours (GAC) algorithm that uses\nmorphological operators instead of solving partial differential equations\n(PDEs) for the evolution of the contour. The set of morphological operators\nused in this algorithm are proved to be infinitesimally equivalent to the GAC\nPDEs (see [1]). However, morphological operators are do not suffer from the\nnumerical stability issues typically found in PDEs (e.g., it is not necessary\nto find the right time step for the evolution), and are computationally\nfaster.\n\nThe algorithm and its theoretical derivation are described in [1].\n\nA Morphological Approach to Curvature-based Evolution of Curves and Surfaces,\nPablo M\u00e1rquez-Neila, Luis Baumela, Luis \u00c1lvarez. In IEEE Transactions on\nPattern Analysis and Machine Intelligence (PAMI), 2014,\nDOI:10.1109/TPAMI.2013.106\n\nSegments image using quickshift clustering in Color-(x,y) space.\n\nProduces an oversegmentation of the image using the quickshift mode-seeking\nalgorithm.\n\nInput image.\n\nBalances color-space proximity and image-space proximity. Higher values give\nmore weight to color-space.\n\nWidth of Gaussian kernel used in smoothing the sample density. Higher means\nfewer clusters.\n\nCut-off point for data distances. Higher means fewer clusters.\n\nWhether to return the full segmentation hierarchy tree and distances.\n\nWidth for Gaussian smoothing as preprocessing. Zero means no smoothing.\n\nWhether the input should be converted to Lab colorspace prior to segmentation.\nFor this purpose, the input is assumed to be RGB.\n\nRandom seed used for breaking ties.\n\nInteger mask indicating segment labels.\n\nThe authors advocate to convert the image to Lab color space prior to\nsegmentation, though this is not strictly necessary. For this to work, the\nimage must be given in RGB format.\n\nQuick shift and kernel methods for mode seeking, Vedaldi, A. and Soatto, S.\nEuropean Conference on Computer Vision, 2008\n\nRandom walker algorithm for segmentation from markers.\n\nRandom walker algorithm is implemented for gray-level or multichannel images.\n\nImage to be segmented in phases. Gray-level `data` can be two- or three-\ndimensional; multichannel data can be three- or four- dimensional\n(multichannel=True) with the highest dimension denoting channels. Data spacing\nis assumed isotropic unless the `spacing` keyword argument is used.\n\nArray of seed markers labeled with different positive integers for different\nphases. Zero-labeled pixels are unlabeled pixels. Negative labels correspond\nto inactive pixels that are not taken into account (they are removed from the\ngraph). If labels are not consecutive integers, the labels array will be\ntransformed so that labels are consecutive. In the multichannel case, `labels`\nshould have the same shape as a single channel of `data`, i.e. without the\nfinal dimension denoting channels.\n\nPenalization coefficient for the random walker motion (the greater `beta`, the\nmore difficult the diffusion).\n\nMode for solving the linear system in the random walker algorithm.\n\nTolerance to achieve when solving the linear system using the conjugate\ngradient based modes (\u2018cg\u2019, \u2018cg_j\u2019 and \u2018cg_mg\u2019).\n\nIf copy is False, the `labels` array will be overwritten with the result of\nthe segmentation. Use copy=False if you want to save on memory.\n\nIf True, input data is parsed as multichannel data (see \u2018data\u2019 above for\nproper input format in this case).\n\nIf True, the probability that a pixel belongs to each of the labels will be\nreturned, instead of only the most likely label.\n\nSpacing between voxels in each spatial dimension. If `None`, then the spacing\nbetween pixels/voxels in each dimension is assumed 1.\n\nTolerance on the resulting probability to be in the interval [0, 1]. If the\ntolerance is not satisfied, a warning is displayed.\n\nSee also\n\nwatershed segmentation A segmentation algorithm based on mathematical\nmorphology and \u201cflooding\u201d of regions from markers.\n\nMultichannel inputs are scaled with all channel data combined. Ensure all\nchannels are separately normalized prior to running this algorithm.\n\nThe `spacing` argument is specifically for anisotropic datasets, where data\npoints are spaced differently in one or more spatial dimensions. Anisotropic\ndata is commonly encountered in medical imaging.\n\nThe algorithm was first proposed in [1].\n\nThe algorithm solves the diffusion equation at infinite times for sources\nplaced on markers of each phase in turn. A pixel is labeled with the phase\nthat has the greatest probability to diffuse first to the pixel.\n\nThe diffusion equation is solved by minimizing x.T L x for each phase, where L\nis the Laplacian of the weighted graph of the image, and x is the probability\nthat a marker of the given phase arrives first at a pixel by diffusion (x=1 on\nmarkers of the phase, x=0 on the other markers, and the other coefficients are\nlooked for). Each pixel is attributed the label for which it has a maximal\nvalue of x. The Laplacian L of the image is defined as:\n\nThe weight w_ij is a decreasing function of the norm of the local gradient.\nThis ensures that diffusion is easier between pixels of similar values.\n\nWhen the Laplacian is decomposed into blocks of marked and unmarked pixels:\n\nwith first indices corresponding to marked pixels, and then to unmarked\npixels, minimizing x.T L x for one phase amount to solving:\n\nwhere x_m = 1 on markers of the given phase, and 0 on other markers. This\nlinear system is solved in the algorithm using a direct method for small\nimages, and an iterative method for larger images.\n\nLeo Grady, Random walks for image segmentation, IEEE Trans Pattern Anal Mach\nIntell. 2006 Nov;28(11):1768-83. DOI:10.1109/TPAMI.2006.233.\n\nRelabel arbitrary labels to {`offset`, \u2026 `offset` \\+ number_of_labels}.\n\nThis function also returns the forward map (mapping the original labels to the\nreduced labels) and the inverse map (mapping the reduced labels back to the\noriginal ones).\n\nAn array of labels, which must be non-negative integers.\n\nThe return labels will start at `offset`, which should be strictly positive.\n\nThe input label field with labels mapped to {offset, \u2026, number_of_labels +\noffset - 1}. The data type will be the same as `label_field`, except when\noffset + number_of_labels causes overflow of the current data type.\n\nThe map from the original label space to the returned label space. Can be used\nto re-apply the same mapping. See examples for usage. The output data type\nwill be the same as `relabeled`.\n\nThe map from the new label space to the original space. This can be used to\nreconstruct the original label field from the relabeled one. The output data\ntype will be the same as `label_field`.\n\nThe label 0 is assumed to denote the background and is never remapped.\n\nThe forward map can be extremely big for some inputs, since its length is\ngiven by the maximum of the label field. However, in most situations,\n`label_field.max()` is much smaller than `label_field.size`, and in these\ncases the forward map is guaranteed to be smaller than either the input or\noutput images.\n\nSegments image using k-means clustering in Color-(x,y,z) space.\n\nInput image, which can be 2D or 3D, and grayscale or multichannel (see\n`multichannel` parameter). Input image must either be NaN-free or the NaN\u2019s\nmust be masked out\n\nThe (approximate) number of labels in the segmented output image.\n\nBalances color proximity and space proximity. Higher values give more weight\nto space proximity, making superpixel shapes more square/cubic. In SLICO mode,\nthis is the initial compactness. This parameter depends strongly on image\ncontrast and on the shapes of objects in the image. We recommend exploring\npossible values on a log scale, e.g., 0.01, 0.1, 1, 10, 100, before refining\naround a chosen value.\n\nMaximum number of iterations of k-means.\n\nWidth of Gaussian smoothing kernel for pre-processing for each dimension of\nthe image. The same sigma is applied to each dimension in case of a scalar\nvalue. Zero means no smoothing. Note, that `sigma` is automatically scaled if\nit is scalar and a manual voxel spacing is provided (see Notes section).\n\nThe voxel spacing along each image dimension. By default, `slic` assumes\nuniform spacing (same voxel resolution along z, y and x). This parameter\ncontrols the weights of the distances along z, y, and x during k-means\nclustering.\n\nWhether the last axis of the image is to be interpreted as multiple channels\nor another spatial dimension.\n\nWhether the input should be converted to Lab colorspace prior to segmentation.\nThe input image must be RGB. Highly recommended. This option defaults to\n`True` when `multichannel=True` and `image.shape[-1] == 3`.\n\nWhether the generated segments are connected or not\n\nProportion of the minimum segment size to be removed with respect to the\nsupposed segment size ``depth*width*height/n_segments``\n\nProportion of the maximum connected segment size. A value of 3 works in most\nof the cases.\n\nRun SLIC-zero, the zero-parameter mode of SLIC. [2]\n\nThe labels\u2019 index start. Should be 0 or 1.\n\nNew in version 0.17: `start_label` was introduced in 0.17\n\nIf provided, superpixels are computed only where mask is True, and seed points\nare homogeneously distributed over the mask using a K-means clustering\nstrategy.\n\nNew in version 0.17: `mask` was introduced in 0.17\n\nInteger mask indicating segment labels.\n\nIf `convert2lab` is set to `True` but the last array dimension is not of\nlength 3.\n\nIf `start_label` is not 0 or 1.\n\nRadhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua,\nand Sabine S\u00fcsstrunk, SLIC Superpixels Compared to State-of-the-art Superpixel\nMethods, TPAMI, May 2012. DOI:10.1109/TPAMI.2012.120\n\nhttps://www.epfl.ch/labs/ivrl/research/slic-superpixels/#SLICO\n\nIrving, Benjamin. \u201cmaskSLIC: regional superpixel generation with application\nto local pathology characterisation in medical images.\u201d, 2016,\narXiv:1606.09518\n\nhttps://github.com/scikit-image/scikit-image/issues/3722\n\nIncreasing the compactness parameter yields more square regions:\n\nFind watershed basins in `image` flooded from given `markers`.\n\nData array where the lowest value points are labeled first.\n\nThe desired number of markers, or an array marking the basins with the values\nto be assigned in the label matrix. Zero means not a marker. If `None` (no\nmarkers given), the local minima of the image are used as markers.\n\nAn array with the same number of dimensions as `image` whose non-zero elements\nindicate neighbors for connection. Following the scipy convention, default is\na one-connected array of the dimension of the image.\n\noffset of the connectivity (one offset per dimension)\n\nArray of same shape as `image`. Only points at which mask == True will be\nlabeled.\n\nUse compact watershed [3] with given compactness parameter. Higher values\nresult in more regularly-shaped watershed basins.\n\nIf watershed_line is True, a one-pixel wide line separates the regions\nobtained by the watershed algorithm. The line has the label 0.\n\nA labeled matrix of the same type and shape as markers\n\nSee also\n\nrandom walker segmentation A segmentation algorithm based on anisotropic\ndiffusion, usually slower than the watershed but with good results on noisy\ndata and boundaries with holes.\n\nThis function implements a watershed algorithm [1] [2] that apportions pixels\ninto marked basins. The algorithm uses a priority queue to hold the pixels\nwith the metric for the priority queue being pixel value, then the time of\nentry into the queue - this settles ties in favor of the closest marker.\n\nSome ideas taken from Soille, \u201cAutomated Basin Delineation from Digital\nElevation Models Using Mathematical Morphology\u201d, Signal Processing 20 (1990)\n171-182\n\nThe most important insight in the paper is that entry time onto the queue\nsolves two problems: a pixel should be assigned to the neighbor with the\nlargest gradient or, if there is no gradient, pixels on a plateau should be\nsplit between markers on opposite sides.\n\nThis implementation converts all arguments to specific, lowest common\ndenominator types, then passes these to a C algorithm.\n\nMarkers can be determined manually, or automatically using for example the\nlocal minima of the gradient of the image, or the local maxima of the distance\nfunction to the background for separating overlapping objects (see example).\n\nhttps://en.wikipedia.org/wiki/Watershed_%28image_processing%29\n\nhttp://cmm.ensmp.fr/~beucher/wtshed.html\n\nPeer Neubert & Peter Protzel (2014). Compact Watershed and Preemptive SLIC: On\nImproving Trade-offs of Superpixel Segmentation Algorithms. ICPR 2014, pp\n996-1001. DOI:10.1109/ICPR.2014.181 https://www.tu-\nchemnitz.de/etit/proaut/publications/cws_pSLIC_ICPR.pdf\n\nThe watershed algorithm is useful to separate overlapping objects.\n\nWe first generate an initial image with two overlapping circles:\n\nNext, we want to separate the two circles. We generate markers at the maxima\nof the distance to the background:\n\nFinally, we run the watershed on the image and markers:\n\nThe algorithm works also for 3-D images, and can be used for example to\nseparate overlapping spheres.\n\nWatershed segmentation\n\nMarkers for watershed transform\n\nSegment human cells (in mitosis)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "segmentation.active_contour()", "path": "api/skimage.segmentation#skimage.segmentation.active_contour", "type": "segmentation", "text": "\nActive contour model.\n\nActive contours by fitting snakes to features of images. Supports single and\nmultichannel 2D images. Snakes can be periodic (for segmentation) or have\nfixed and/or free ends. The output snake has the same length as the input\nboundary. As the number of points is constant, make sure that the initial\nsnake has enough points to capture the details of the final contour.\n\nInput image.\n\nInitial snake coordinates. For periodic boundary conditions, endpoints must\nnot be duplicated.\n\nSnake length shape parameter. Higher values makes snake contract faster.\n\nSnake smoothness shape parameter. Higher values makes snake smoother.\n\nControls attraction to brightness. Use negative values to attract toward dark\nregions.\n\nControls attraction to edges. Use negative values to repel snake from edges.\n\nExplicit time stepping parameter.\n\nMaximum pixel distance to move per iteration.\n\nMaximum iterations to optimize snake shape.\n\nConvergence criteria.\n\nBoundary conditions for the contour. Can be one of \u2018periodic\u2019, \u2018free\u2019,\n\u2018fixed\u2019, \u2018free-fixed\u2019, or \u2018fixed-free\u2019. \u2018periodic\u2019 attaches the two ends of\nthe snake, \u2018fixed\u2019 holds the end-points in place, and \u2018free\u2019 allows free\nmovement of the ends. \u2018fixed\u2019 and \u2018free\u2019 can be combined by parsing \u2018fixed-\nfree\u2019, \u2018free-fixed\u2019. Parsing \u2018fixed-fixed\u2019 or \u2018free-free\u2019 yields same\nbehaviour as \u2018fixed\u2019 and \u2018free\u2019, respectively.\n\nThis option remains for compatibility purpose only and has no effect. It was\nintroduced in 0.16 with the `'xy'` option, but since 0.18, only the `'rc'`\noption is valid. Coordinates must be set in a row-column format.\n\nOptimised snake, same shape as input parameter.\n\nKass, M.; Witkin, A.; Terzopoulos, D. \u201cSnakes: Active contour models\u201d.\nInternational Journal of Computer Vision 1 (4): 321 (1988).\nDOI:10.1007/BF00133570\n\nCreate and smooth image:\n\nInitialize spline:\n\nFit spline to image:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "segmentation.chan_vese()", "path": "api/skimage.segmentation#skimage.segmentation.chan_vese", "type": "segmentation", "text": "\nChan-Vese segmentation algorithm.\n\nActive contour model by evolving a level set. Can be used to segment objects\nwithout clearly defined boundaries.\n\nGrayscale image to be segmented.\n\n\u2018edge length\u2019 weight parameter. Higher `mu` values will produce a \u2018round\u2019\nedge, while values closer to zero will detect smaller objects.\n\n\u2018difference from average\u2019 weight parameter for the output region with value\n\u2018True\u2019. If it is lower than `lambda2`, this region will have a larger range of\nvalues than the other.\n\n\u2018difference from average\u2019 weight parameter for the output region with value\n\u2018False\u2019. If it is lower than `lambda1`, this region will have a larger range\nof values than the other.\n\nLevel set variation tolerance between iterations. If the L2 norm difference\nbetween the level sets of successive iterations normalized by the area of the\nimage is below this value, the algorithm will assume that the solution was\nreached.\n\nMaximum number of iterations allowed before the algorithm interrupts itself.\n\nA multiplication factor applied at calculations for each step, serves to\naccelerate the algorithm. While higher values may speed up the algorithm, they\nmay also lead to convergence problems.\n\nDefines the starting level set used by the algorithm. If a string is inputted,\na level set that matches the image size will automatically be generated.\nAlternatively, it is possible to define a custom level set, which should be an\narray of float values, with the same shape as \u2018image\u2019. Accepted string values\nare as follows.\n\nthe starting level set is defined as sin(x/5*pi)*sin(y/5*pi), where x and y\nare pixel coordinates. This level set has fast convergence, but may fail to\ndetect implicit edges.\n\nthe starting level set is defined as the opposite of the distance from the\ncenter of the image minus half of the minimum value between image width and\nimage height. This is somewhat slower, but is more likely to properly detect\nimplicit edges.\n\nthe starting level set is defined as the opposite of the distance from the\ncenter of the image minus a quarter of the minimum value between image width\nand image height.\n\nIf set to True, the return value will be a tuple containing the three return\nvalues (see below). If set to False which is the default value, only the\n\u2018segmentation\u2019 array will be returned.\n\nSegmentation produced by the algorithm.\n\nFinal level set computed by the algorithm.\n\nShows the evolution of the \u2018energy\u2019 for each step of the algorithm. This\nshould allow to check whether the algorithm converged.\n\nThe Chan-Vese Algorithm is designed to segment objects without clearly defined\nboundaries. This algorithm is based on level sets that are evolved iteratively\nto minimize an energy, which is defined by weighted values corresponding to\nthe sum of differences intensity from the average value outside the segmented\nregion, the sum of differences from the average value inside the segmented\nregion, and a term which is dependent on the length of the boundary of the\nsegmented region.\n\nThis algorithm was first proposed by Tony Chan and Luminita Vese, in a\npublication entitled \u201cAn Active Contour Model Without Edges\u201d [1].\n\nThis implementation of the algorithm is somewhat simplified in the sense that\nthe area factor \u2018nu\u2019 described in the original paper is not implemented, and\nis only suitable for grayscale images.\n\nTypical values for `lambda1` and `lambda2` are 1. If the \u2018background\u2019 is very\ndifferent from the segmented object in terms of distribution (for example, a\nuniform black image with figures of varying intensity), then these values\nshould be different from each other.\n\nTypical values for mu are between 0 and 1, though higher values can be used\nwhen dealing with shapes with very ill-defined contours.\n\nThe \u2018energy\u2019 which this algorithm tries to minimize is defined as the sum of\nthe differences from the average within the region squared and weighed by the\n\u2018lambda\u2019 factors to which is added the length of the contour multiplied by the\n\u2018mu\u2019 factor.\n\nSupports 2D grayscale images only, and does not implement the area term\ndescribed in the original article.\n\nAn Active Contour Model without Edges, Tony Chan and Luminita Vese, Scale-\nSpace Theories in Computer Vision, 1999, DOI:10.1007/3-540-48236-9_13\n\nChan-Vese Segmentation, Pascal Getreuer Image Processing On Line, 2 (2012),\npp. 214-224, DOI:10.5201/ipol.2012.g-cv\n\nThe Chan-Vese Algorithm - Project Report, Rami Cohen, 2011 arXiv:1107.2782\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "segmentation.checkerboard_level_set()", "path": "api/skimage.segmentation#skimage.segmentation.checkerboard_level_set", "type": "segmentation", "text": "\nCreate a checkerboard level set with binary values.\n\nShape of the image.\n\nSize of the squares of the checkerboard. It defaults to 5.\n\nBinary level set of the checkerboard.\n\nSee also\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "segmentation.circle_level_set()", "path": "api/skimage.segmentation#skimage.segmentation.circle_level_set", "type": "segmentation", "text": "\nCreate a circle level set with binary values.\n\nShape of the image\n\nCoordinates of the center of the circle given in (row, column). If not given,\nit defaults to the center of the image.\n\nRadius of the circle. If not given, it is set to the 75% of the smallest image\ndimension.\n\nBinary level set of the circle with the given `radius` and `center`.\n\nNew in version 0.17: This function is deprecated and will be removed in\nscikit-image 0.19. Please use the function named `disk_level_set` instead.\n\nSee also\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "segmentation.clear_border()", "path": "api/skimage.segmentation#skimage.segmentation.clear_border", "type": "segmentation", "text": "\nClear objects connected to the label image border.\n\nImaging data labels.\n\nThe width of the border examined. By default, only objects that touch the\noutside of the image are removed.\n\nCleared objects are set to this value.\n\nWhether or not to manipulate the labels array in-place.\n\nImage data mask. Objects in labels image overlapping with False pixels of mask\nwill be removed. If defined, the argument buffer_size will be ignored.\n\nImaging data labels with cleared borders\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "segmentation.disk_level_set()", "path": "api/skimage.segmentation#skimage.segmentation.disk_level_set", "type": "segmentation", "text": "\nCreate a disk level set with binary values.\n\nShape of the image\n\nCoordinates of the center of the disk given in (row, column). If not given, it\ndefaults to the center of the image.\n\nRadius of the disk. If not given, it is set to the 75% of the smallest image\ndimension.\n\nBinary level set of the disk with the given `radius` and `center`.\n\nSee also\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "segmentation.expand_labels()", "path": "api/skimage.segmentation#skimage.segmentation.expand_labels", "type": "segmentation", "text": "\nExpand labels in label image by `distance` pixels without overlapping.\n\nGiven a label image, `expand_labels` grows label regions (connected\ncomponents) outwards by up to `distance` pixels without overflowing into\nneighboring regions. More specifically, each background pixel that is within\nEuclidean distance of <= `distance` pixels of a connected component is\nassigned the label of that connected component. Where multiple connected\ncomponents are within `distance` pixels of a background pixel, the label value\nof the closest connected component will be assigned (see Notes for the case of\nmultiple labels at equal distance).\n\nlabel image\n\nEuclidean distance in pixels by which to grow the labels. Default is one.\n\nLabeled array, where all connected regions have been enlarged\n\nSee also\n\nWhere labels are spaced more than `distance` pixels are apart, this is\nequivalent to a morphological dilation with a disc or hyperball of radius\n`distance`. However, in contrast to a morphological dilation, `expand_labels`\nwill not expand a label region into a neighboring region.\n\nThis implementation of `expand_labels` is derived from CellProfiler [1], where\nit is known as module \u201cIdentifySecondaryObjects (Distance-N)\u201d [2].\n\nThere is an important edge case when a pixel has the same distance to multiple\nregions, as it is not defined which region expands into that space. Here, the\nexact behavior depends on the upstream implementation of\n`scipy.ndimage.distance_transform_edt`.\n\nhttps://cellprofiler.org\n\nhttps://github.com/CellProfiler/CellProfiler/blob/082930ea95add7b72243a4fa3d39ae5145995e9c/cellprofiler/modules/identifysecondaryobjects.py#L559\n\nLabels will not overwrite each other:\n\nIn case of ties, behavior is undefined, but currently resolves to the label\nclosest to `(0,) * ndim` in lexicographical order.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "segmentation.felzenszwalb()", "path": "api/skimage.segmentation#skimage.segmentation.felzenszwalb", "type": "segmentation", "text": "\nComputes Felsenszwalb\u2019s efficient graph based image segmentation.\n\nProduces an oversegmentation of a multichannel (i.e. RGB) image using a fast,\nminimum spanning tree based clustering on the image grid. The parameter\n`scale` sets an observation level. Higher scale means less and larger\nsegments. `sigma` is the diameter of a Gaussian kernel, used for smoothing the\nimage prior to segmentation.\n\nThe number of produced segments as well as their size can only be controlled\nindirectly through `scale`. Segment size within an image can vary greatly\ndepending on local contrast.\n\nFor RGB images, the algorithm uses the euclidean distance between pixels in\ncolor space.\n\nInput image.\n\nFree parameter. Higher means larger clusters.\n\nWidth (standard deviation) of Gaussian kernel used in preprocessing.\n\nMinimum component size. Enforced using postprocessing.\n\nWhether the last axis of the image is to be interpreted as multiple channels.\nA value of False, for a 3D image, is not currently supported.\n\nInteger mask indicating segment labels.\n\nThe `k` parameter used in the original paper renamed to `scale` here.\n\nEfficient graph-based image segmentation, Felzenszwalb, P.F. and Huttenlocher,\nD.P. International Journal of Computer Vision, 2004\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "segmentation.find_boundaries()", "path": "api/skimage.segmentation#skimage.segmentation.find_boundaries", "type": "segmentation", "text": "\nReturn bool array where boundaries between labeled regions are True.\n\nAn array in which different regions are labeled with either different integers\nor boolean values.\n\nA pixel is considered a boundary pixel if any of its neighbors has a different\nlabel. `connectivity` controls which pixels are considered neighbors. A\nconnectivity of 1 (default) means pixels sharing an edge (in 2D) or a face (in\n3D) will be considered neighbors. A connectivity of `label_img.ndim` means\npixels sharing a corner will be considered neighbors.\n\nHow to mark the boundaries:\n\nFor modes \u2018inner\u2019 and \u2018outer\u2019, a definition of a background label is required.\nSee `mode` for descriptions of these two.\n\nA bool image where `True` represents a boundary pixel. For `mode` equal to\n\u2018subpixel\u2019, `boundaries.shape[i]` is equal to `2 * label_img.shape[i] - 1` for\nall `i` (a pixel is inserted in between all other pairs of pixels).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "segmentation.flood()", "path": "api/skimage.segmentation#skimage.segmentation.flood", "type": "segmentation", "text": "\nMask corresponding to a flood fill.\n\nStarting at a specific `seed_point`, connected points equal or within\n`tolerance` of the seed value are found.\n\nAn n-dimensional array.\n\nThe point in `image` used as the starting point for the flood fill. If the\nimage is 1D, this point may be given as an integer.\n\nA structuring element used to determine the neighborhood of each evaluated\npixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as\n`image`. If not given, all adjacent pixels are considered as part of the\nneighborhood (fully connected).\n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent\npixels whose squared distance from the center is larger or equal to\n`connectivity` are considered neighbors. Ignored if `selem` is not None.\n\nIf None (default), adjacent values must be strictly equal to the initial value\nof `image` at `seed_point`. This is fastest. If a value is given, a comparison\nwill be done at every point and if within tolerance of the initial value will\nalso be filled (inclusive).\n\nA Boolean array with the same shape as `image` is returned, with True values\nfor areas connected to and equal (or within tolerance of) the seed point. All\nother values are False.\n\nThe conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many\nraster graphics programs. This function returns just the mask representing the\nfill.\n\nIf indices are desired rather than masks for memory reasons, the user can\nsimply run `numpy.nonzero` on the result, save the indices, and discard this\nmask.\n\nFill connected ones with 5, with full connectivity (diagonals included):\n\nFill connected ones with 5, excluding diagonal points (connectivity 1):\n\nFill with a tolerance:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "segmentation.flood_fill()", "path": "api/skimage.segmentation#skimage.segmentation.flood_fill", "type": "segmentation", "text": "\nPerform flood filling on an image.\n\nStarting at a specific `seed_point`, connected points equal or within\n`tolerance` of the seed value are found, then set to `new_value`.\n\nAn n-dimensional array.\n\nThe point in `image` used as the starting point for the flood fill. If the\nimage is 1D, this point may be given as an integer.\n\nNew value to set the entire fill. This must be chosen in agreement with the\ndtype of `image`.\n\nA structuring element used to determine the neighborhood of each evaluated\npixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as\n`image`. If not given, all adjacent pixels are considered as part of the\nneighborhood (fully connected).\n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent\npixels whose squared distance from the center is less than or equal to\n`connectivity` are considered neighbors. Ignored if `selem` is not None.\n\nIf None (default), adjacent values must be strictly equal to the value of\n`image` at `seed_point` to be filled. This is fastest. If a tolerance is\nprovided, adjacent points with values within plus or minus tolerance from the\nseed point are filled (inclusive).\n\nIf True, flood filling is applied to `image` in place. If False, the flood\nfilled result is returned without modifying the input `image` (default).\n\nThis parameter is deprecated and will be removed in version 0.19.0 in favor of\nin_place. If True, flood filling is applied to `image` inplace. If False, the\nflood filled result is returned without modifying the input `image` (default).\n\nAn array with the same shape as `image` is returned, with values in areas\nconnected to and equal (or within tolerance of) the seed point replaced with\n`new_value`.\n\nThe conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many\nraster graphics programs.\n\nFill connected ones with 5, with full connectivity (diagonals included):\n\nFill connected ones with 5, excluding diagonal points (connectivity 1):\n\nFill with a tolerance:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "segmentation.inverse_gaussian_gradient()", "path": "api/skimage.segmentation#skimage.segmentation.inverse_gaussian_gradient", "type": "segmentation", "text": "\nInverse of gradient magnitude.\n\nCompute the magnitude of the gradients in the image and then inverts the\nresult in the range [0, 1]. Flat areas are assigned values close to 1, while\nareas close to borders are assigned values close to 0.\n\nThis function or a similar one defined by the user should be applied over the\nimage as a preprocessing step before calling\n`morphological_geodesic_active_contour`.\n\nGrayscale image or volume.\n\nControls the steepness of the inversion. A larger value will make the\ntransition between the flat areas and border areas steeper in the resulting\narray.\n\nStandard deviation of the Gaussian filter applied over the image.\n\nPreprocessed image (or volume) suitable for\n`morphological_geodesic_active_contour`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "segmentation.join_segmentations()", "path": "api/skimage.segmentation#skimage.segmentation.join_segmentations", "type": "segmentation", "text": "\nReturn the join of the two input segmentations.\n\nThe join J of S1 and S2 is defined as the segmentation in which two voxels are\nin the same segment if and only if they are in the same segment in both S1 and\nS2.\n\ns1 and s2 are label fields of the same shape.\n\nThe join segmentation of s1 and s2.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "segmentation.mark_boundaries()", "path": "api/skimage.segmentation#skimage.segmentation.mark_boundaries", "type": "segmentation", "text": "\nReturn image with boundaries between labeled regions highlighted.\n\nGrayscale or RGB image.\n\nLabel array where regions are marked by different integer values.\n\nRGB color of boundaries in the output image.\n\nRGB color surrounding boundaries in the output image. If None, no outline is\ndrawn.\n\nThe mode for finding boundaries.\n\nWhich label to consider background (this is only useful for modes `inner` and\n`outer`).\n\nAn image in which the boundaries between labels are superimposed on the\noriginal image.\n\nSee also\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "segmentation.morphological_chan_vese()", "path": "api/skimage.segmentation#skimage.segmentation.morphological_chan_vese", "type": "segmentation", "text": "\nMorphological Active Contours without Edges (MorphACWE)\n\nActive contours without edges implemented with morphological operators. It can\nbe used to segment objects in images and volumes without well defined borders.\nIt is required that the inside of the object looks different on average than\nthe outside (i.e., the inner area of the object should be darker or lighter\nthan the outer area on average).\n\nGrayscale image or volume to be segmented.\n\nNumber of iterations to run\n\nInitial level set. If an array is given, it will be binarized and used as the\ninitial level set. If a string is given, it defines the method to generate a\nreasonable initial level set with the shape of the `image`. Accepted values\nare \u2018checkerboard\u2019 and \u2018circle\u2019. See the documentation of\n`checkerboard_level_set` and `circle_level_set` respectively for details about\nhow these level sets are created.\n\nNumber of times the smoothing operator is applied per iteration. Reasonable\nvalues are around 1-4. Larger values lead to smoother segmentations.\n\nWeight parameter for the outer region. If `lambda1` is larger than `lambda2`,\nthe outer region will contain a larger range of values than the inner region.\n\nWeight parameter for the inner region. If `lambda2` is larger than `lambda1`,\nthe inner region will contain a larger range of values than the outer region.\n\nIf given, this function is called once per iteration with the current level\nset as the only argument. This is useful for debugging or for plotting\nintermediate results during the evolution.\n\nFinal segmentation (i.e., the final level set)\n\nSee also\n\nThis is a version of the Chan-Vese algorithm that uses morphological operators\ninstead of solving a partial differential equation (PDE) for the evolution of\nthe contour. The set of morphological operators used in this algorithm are\nproved to be infinitesimally equivalent to the Chan-Vese PDE (see [1]).\nHowever, morphological operators are do not suffer from the numerical\nstability issues typically found in PDEs (it is not necessary to find the\nright time step for the evolution), and are computationally faster.\n\nThe algorithm and its theoretical derivation are described in [1].\n\nA Morphological Approach to Curvature-based Evolution of Curves and Surfaces,\nPablo M\u00e1rquez-Neila, Luis Baumela, Luis \u00c1lvarez. In IEEE Transactions on\nPattern Analysis and Machine Intelligence (PAMI), 2014,\nDOI:10.1109/TPAMI.2013.106\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "segmentation.morphological_geodesic_active_contour()", "path": "api/skimage.segmentation#skimage.segmentation.morphological_geodesic_active_contour", "type": "segmentation", "text": "\nMorphological Geodesic Active Contours (MorphGAC).\n\nGeodesic active contours implemented with morphological operators. It can be\nused to segment objects with visible but noisy, cluttered, broken borders.\n\nPreprocessed image or volume to be segmented. This is very rarely the original\nimage. Instead, this is usually a preprocessed version of the original image\nthat enhances and highlights the borders (or other structures) of the object\nto segment. `morphological_geodesic_active_contour` will try to stop the\ncontour evolution in areas where `gimage` is small. See\n`morphsnakes.inverse_gaussian_gradient` as an example function to perform this\npreprocessing. Note that the quality of\n`morphological_geodesic_active_contour` might greatly depend on this\npreprocessing.\n\nNumber of iterations to run.\n\nInitial level set. If an array is given, it will be binarized and used as the\ninitial level set. If a string is given, it defines the method to generate a\nreasonable initial level set with the shape of the `image`. Accepted values\nare \u2018checkerboard\u2019 and \u2018circle\u2019. See the documentation of\n`checkerboard_level_set` and `circle_level_set` respectively for details about\nhow these level sets are created.\n\nNumber of times the smoothing operator is applied per iteration. Reasonable\nvalues are around 1-4. Larger values lead to smoother segmentations.\n\nAreas of the image with a value smaller than this threshold will be considered\nborders. The evolution of the contour will stop in this areas.\n\nBalloon force to guide the contour in non-informative areas of the image,\ni.e., areas where the gradient of the image is too small to push the contour\ntowards a border. A negative value will shrink the contour, while a positive\nvalue will expand the contour in these areas. Setting this to zero will\ndisable the balloon force.\n\nIf given, this function is called once per iteration with the current level\nset as the only argument. This is useful for debugging or for plotting\nintermediate results during the evolution.\n\nFinal segmentation (i.e., the final level set)\n\nSee also\n\nThis is a version of the Geodesic Active Contours (GAC) algorithm that uses\nmorphological operators instead of solving partial differential equations\n(PDEs) for the evolution of the contour. The set of morphological operators\nused in this algorithm are proved to be infinitesimally equivalent to the GAC\nPDEs (see [1]). However, morphological operators are do not suffer from the\nnumerical stability issues typically found in PDEs (e.g., it is not necessary\nto find the right time step for the evolution), and are computationally\nfaster.\n\nThe algorithm and its theoretical derivation are described in [1].\n\nA Morphological Approach to Curvature-based Evolution of Curves and Surfaces,\nPablo M\u00e1rquez-Neila, Luis Baumela, Luis \u00c1lvarez. In IEEE Transactions on\nPattern Analysis and Machine Intelligence (PAMI), 2014,\nDOI:10.1109/TPAMI.2013.106\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "segmentation.quickshift()", "path": "api/skimage.segmentation#skimage.segmentation.quickshift", "type": "segmentation", "text": "\nSegments image using quickshift clustering in Color-(x,y) space.\n\nProduces an oversegmentation of the image using the quickshift mode-seeking\nalgorithm.\n\nInput image.\n\nBalances color-space proximity and image-space proximity. Higher values give\nmore weight to color-space.\n\nWidth of Gaussian kernel used in smoothing the sample density. Higher means\nfewer clusters.\n\nCut-off point for data distances. Higher means fewer clusters.\n\nWhether to return the full segmentation hierarchy tree and distances.\n\nWidth for Gaussian smoothing as preprocessing. Zero means no smoothing.\n\nWhether the input should be converted to Lab colorspace prior to segmentation.\nFor this purpose, the input is assumed to be RGB.\n\nRandom seed used for breaking ties.\n\nInteger mask indicating segment labels.\n\nThe authors advocate to convert the image to Lab color space prior to\nsegmentation, though this is not strictly necessary. For this to work, the\nimage must be given in RGB format.\n\nQuick shift and kernel methods for mode seeking, Vedaldi, A. and Soatto, S.\nEuropean Conference on Computer Vision, 2008\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "segmentation.random_walker()", "path": "api/skimage.segmentation#skimage.segmentation.random_walker", "type": "segmentation", "text": "\nRandom walker algorithm for segmentation from markers.\n\nRandom walker algorithm is implemented for gray-level or multichannel images.\n\nImage to be segmented in phases. Gray-level `data` can be two- or three-\ndimensional; multichannel data can be three- or four- dimensional\n(multichannel=True) with the highest dimension denoting channels. Data spacing\nis assumed isotropic unless the `spacing` keyword argument is used.\n\nArray of seed markers labeled with different positive integers for different\nphases. Zero-labeled pixels are unlabeled pixels. Negative labels correspond\nto inactive pixels that are not taken into account (they are removed from the\ngraph). If labels are not consecutive integers, the labels array will be\ntransformed so that labels are consecutive. In the multichannel case, `labels`\nshould have the same shape as a single channel of `data`, i.e. without the\nfinal dimension denoting channels.\n\nPenalization coefficient for the random walker motion (the greater `beta`, the\nmore difficult the diffusion).\n\nMode for solving the linear system in the random walker algorithm.\n\nTolerance to achieve when solving the linear system using the conjugate\ngradient based modes (\u2018cg\u2019, \u2018cg_j\u2019 and \u2018cg_mg\u2019).\n\nIf copy is False, the `labels` array will be overwritten with the result of\nthe segmentation. Use copy=False if you want to save on memory.\n\nIf True, input data is parsed as multichannel data (see \u2018data\u2019 above for\nproper input format in this case).\n\nIf True, the probability that a pixel belongs to each of the labels will be\nreturned, instead of only the most likely label.\n\nSpacing between voxels in each spatial dimension. If `None`, then the spacing\nbetween pixels/voxels in each dimension is assumed 1.\n\nTolerance on the resulting probability to be in the interval [0, 1]. If the\ntolerance is not satisfied, a warning is displayed.\n\nSee also\n\nwatershed segmentation A segmentation algorithm based on mathematical\nmorphology and \u201cflooding\u201d of regions from markers.\n\nMultichannel inputs are scaled with all channel data combined. Ensure all\nchannels are separately normalized prior to running this algorithm.\n\nThe `spacing` argument is specifically for anisotropic datasets, where data\npoints are spaced differently in one or more spatial dimensions. Anisotropic\ndata is commonly encountered in medical imaging.\n\nThe algorithm was first proposed in [1].\n\nThe algorithm solves the diffusion equation at infinite times for sources\nplaced on markers of each phase in turn. A pixel is labeled with the phase\nthat has the greatest probability to diffuse first to the pixel.\n\nThe diffusion equation is solved by minimizing x.T L x for each phase, where L\nis the Laplacian of the weighted graph of the image, and x is the probability\nthat a marker of the given phase arrives first at a pixel by diffusion (x=1 on\nmarkers of the phase, x=0 on the other markers, and the other coefficients are\nlooked for). Each pixel is attributed the label for which it has a maximal\nvalue of x. The Laplacian L of the image is defined as:\n\nThe weight w_ij is a decreasing function of the norm of the local gradient.\nThis ensures that diffusion is easier between pixels of similar values.\n\nWhen the Laplacian is decomposed into blocks of marked and unmarked pixels:\n\nwith first indices corresponding to marked pixels, and then to unmarked\npixels, minimizing x.T L x for one phase amount to solving:\n\nwhere x_m = 1 on markers of the given phase, and 0 on other markers. This\nlinear system is solved in the algorithm using a direct method for small\nimages, and an iterative method for larger images.\n\nLeo Grady, Random walks for image segmentation, IEEE Trans Pattern Anal Mach\nIntell. 2006 Nov;28(11):1768-83. DOI:10.1109/TPAMI.2006.233.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "segmentation.relabel_sequential()", "path": "api/skimage.segmentation#skimage.segmentation.relabel_sequential", "type": "segmentation", "text": "\nRelabel arbitrary labels to {`offset`, \u2026 `offset` \\+ number_of_labels}.\n\nThis function also returns the forward map (mapping the original labels to the\nreduced labels) and the inverse map (mapping the reduced labels back to the\noriginal ones).\n\nAn array of labels, which must be non-negative integers.\n\nThe return labels will start at `offset`, which should be strictly positive.\n\nThe input label field with labels mapped to {offset, \u2026, number_of_labels +\noffset - 1}. The data type will be the same as `label_field`, except when\noffset + number_of_labels causes overflow of the current data type.\n\nThe map from the original label space to the returned label space. Can be used\nto re-apply the same mapping. See examples for usage. The output data type\nwill be the same as `relabeled`.\n\nThe map from the new label space to the original space. This can be used to\nreconstruct the original label field from the relabeled one. The output data\ntype will be the same as `label_field`.\n\nThe label 0 is assumed to denote the background and is never remapped.\n\nThe forward map can be extremely big for some inputs, since its length is\ngiven by the maximum of the label field. However, in most situations,\n`label_field.max()` is much smaller than `label_field.size`, and in these\ncases the forward map is guaranteed to be smaller than either the input or\noutput images.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "segmentation.slic()", "path": "api/skimage.segmentation#skimage.segmentation.slic", "type": "segmentation", "text": "\nSegments image using k-means clustering in Color-(x,y,z) space.\n\nInput image, which can be 2D or 3D, and grayscale or multichannel (see\n`multichannel` parameter). Input image must either be NaN-free or the NaN\u2019s\nmust be masked out\n\nThe (approximate) number of labels in the segmented output image.\n\nBalances color proximity and space proximity. Higher values give more weight\nto space proximity, making superpixel shapes more square/cubic. In SLICO mode,\nthis is the initial compactness. This parameter depends strongly on image\ncontrast and on the shapes of objects in the image. We recommend exploring\npossible values on a log scale, e.g., 0.01, 0.1, 1, 10, 100, before refining\naround a chosen value.\n\nMaximum number of iterations of k-means.\n\nWidth of Gaussian smoothing kernel for pre-processing for each dimension of\nthe image. The same sigma is applied to each dimension in case of a scalar\nvalue. Zero means no smoothing. Note, that `sigma` is automatically scaled if\nit is scalar and a manual voxel spacing is provided (see Notes section).\n\nThe voxel spacing along each image dimension. By default, `slic` assumes\nuniform spacing (same voxel resolution along z, y and x). This parameter\ncontrols the weights of the distances along z, y, and x during k-means\nclustering.\n\nWhether the last axis of the image is to be interpreted as multiple channels\nor another spatial dimension.\n\nWhether the input should be converted to Lab colorspace prior to segmentation.\nThe input image must be RGB. Highly recommended. This option defaults to\n`True` when `multichannel=True` and `image.shape[-1] == 3`.\n\nWhether the generated segments are connected or not\n\nProportion of the minimum segment size to be removed with respect to the\nsupposed segment size ``depth*width*height/n_segments``\n\nProportion of the maximum connected segment size. A value of 3 works in most\nof the cases.\n\nRun SLIC-zero, the zero-parameter mode of SLIC. [2]\n\nThe labels\u2019 index start. Should be 0 or 1.\n\nNew in version 0.17: `start_label` was introduced in 0.17\n\nIf provided, superpixels are computed only where mask is True, and seed points\nare homogeneously distributed over the mask using a K-means clustering\nstrategy.\n\nNew in version 0.17: `mask` was introduced in 0.17\n\nInteger mask indicating segment labels.\n\nIf `convert2lab` is set to `True` but the last array dimension is not of\nlength 3.\n\nIf `start_label` is not 0 or 1.\n\nRadhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua,\nand Sabine S\u00fcsstrunk, SLIC Superpixels Compared to State-of-the-art Superpixel\nMethods, TPAMI, May 2012. DOI:10.1109/TPAMI.2012.120\n\nhttps://www.epfl.ch/labs/ivrl/research/slic-superpixels/#SLICO\n\nIrving, Benjamin. \u201cmaskSLIC: regional superpixel generation with application\nto local pathology characterisation in medical images.\u201d, 2016,\narXiv:1606.09518\n\nhttps://github.com/scikit-image/scikit-image/issues/3722\n\nIncreasing the compactness parameter yields more square regions:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "segmentation.watershed()", "path": "api/skimage.segmentation#skimage.segmentation.watershed", "type": "segmentation", "text": "\nFind watershed basins in `image` flooded from given `markers`.\n\nData array where the lowest value points are labeled first.\n\nThe desired number of markers, or an array marking the basins with the values\nto be assigned in the label matrix. Zero means not a marker. If `None` (no\nmarkers given), the local minima of the image are used as markers.\n\nAn array with the same number of dimensions as `image` whose non-zero elements\nindicate neighbors for connection. Following the scipy convention, default is\na one-connected array of the dimension of the image.\n\noffset of the connectivity (one offset per dimension)\n\nArray of same shape as `image`. Only points at which mask == True will be\nlabeled.\n\nUse compact watershed [3] with given compactness parameter. Higher values\nresult in more regularly-shaped watershed basins.\n\nIf watershed_line is True, a one-pixel wide line separates the regions\nobtained by the watershed algorithm. The line has the label 0.\n\nA labeled matrix of the same type and shape as markers\n\nSee also\n\nrandom walker segmentation A segmentation algorithm based on anisotropic\ndiffusion, usually slower than the watershed but with good results on noisy\ndata and boundaries with holes.\n\nThis function implements a watershed algorithm [1] [2] that apportions pixels\ninto marked basins. The algorithm uses a priority queue to hold the pixels\nwith the metric for the priority queue being pixel value, then the time of\nentry into the queue - this settles ties in favor of the closest marker.\n\nSome ideas taken from Soille, \u201cAutomated Basin Delineation from Digital\nElevation Models Using Mathematical Morphology\u201d, Signal Processing 20 (1990)\n171-182\n\nThe most important insight in the paper is that entry time onto the queue\nsolves two problems: a pixel should be assigned to the neighbor with the\nlargest gradient or, if there is no gradient, pixels on a plateau should be\nsplit between markers on opposite sides.\n\nThis implementation converts all arguments to specific, lowest common\ndenominator types, then passes these to a C algorithm.\n\nMarkers can be determined manually, or automatically using for example the\nlocal minima of the gradient of the image, or the local maxima of the distance\nfunction to the background for separating overlapping objects (see example).\n\nhttps://en.wikipedia.org/wiki/Watershed_%28image_processing%29\n\nhttp://cmm.ensmp.fr/~beucher/wtshed.html\n\nPeer Neubert & Peter Protzel (2014). Compact Watershed and Preemptive SLIC: On\nImproving Trade-offs of Superpixel Segmentation Algorithms. ICPR 2014, pp\n996-1001. DOI:10.1109/ICPR.2014.181 https://www.tu-\nchemnitz.de/etit/proaut/publications/cws_pSLIC_ICPR.pdf\n\nThe watershed algorithm is useful to separate overlapping objects.\n\nWe first generate an initial image with two overlapping circles:\n\nNext, we want to separate the two circles. We generate markers at the maxima\nof the distance to the background:\n\nFinally, we run the watershed on the image and markers:\n\nThe algorithm works also for 3-D images, and can be used for example to\nseparate overlapping spheres.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "skimage", "path": "api/skimage", "type": "skimage", "text": "\nImage Processing for Python\n\n`scikit-image` (a.k.a. `skimage`) is a collection of algorithms for image\nprocessing and computer vision.\n\nThe main package of `skimage` only provides a few utilities for converting\nbetween image data types; for most features, you need to import one of the\nfollowing subpackages:\n\nColor space conversion.\n\nTest images and example data.\n\nDrawing primitives (lines, text, etc.) that operate on NumPy arrays.\n\nImage intensity adjustment, e.g., histogram equalization, etc.\n\nFeature detection and extraction, e.g., texture analysis corners, etc.\n\nSharpening, edge finding, rank filters, thresholding, etc.\n\nGraph-theoretic operations, e.g., shortest paths.\n\nReading, saving, and displaying images and video.\n\nMeasurement of image properties, e.g., region properties and contours.\n\nMetrics corresponding to images, e.g. distance metrics, similarity, etc.\n\nMorphological operations, e.g., opening or skeletonization.\n\nRestoration algorithms, e.g., deconvolution algorithms, denoising, etc.\n\nPartitioning an image into multiple regions.\n\nGeometric and other transforms, e.g., rotation or the Radon transform.\n\nGeneric utilities.\n\nA simple graphical user interface for visualizing results and exploring\nparameters.\n\nConvert an image to floating point format, with values in [0, 1]. Is similar\nto `img_as_float64`, but will not convert lower-precision floating point\narrays to `float64`.\n\nConvert an image to single-precision (32-bit) floating point format, with\nvalues in [0, 1].\n\nConvert an image to double-precision (64-bit) floating point format, with\nvalues in [0, 1].\n\nConvert an image to unsigned integer format, with values in [0, 65535].\n\nConvert an image to signed integer format, with values in [-32768, 32767].\n\nConvert an image to unsigned byte format, with values in [0, 255].\n\nConvert an image to boolean format, with values either True or False.\n\nReturn intensity limits, i.e. (min, max) tuple, of the image\u2019s dtype.\n\n`skimage.dtype_limits`(image[, clip_negative])\n\nReturn intensity limits, i.e.\n\n`skimage.ensure_python_version`(min_version)\n\n`skimage.img_as_bool`(image[, force_copy])\n\nConvert an image to boolean format.\n\n`skimage.img_as_float`(image[, force_copy])\n\nConvert an image to floating point format.\n\n`skimage.img_as_float32`(image[, force_copy])\n\nConvert an image to single-precision (32-bit) floating point format.\n\n`skimage.img_as_float64`(image[, force_copy])\n\nConvert an image to double-precision (64-bit) floating point format.\n\n`skimage.img_as_int`(image[, force_copy])\n\nConvert an image to 16-bit signed integer format.\n\n`skimage.img_as_ubyte`(image[, force_copy])\n\nConvert an image to 8-bit unsigned integer format.\n\n`skimage.img_as_uint`(image[, force_copy])\n\nConvert an image to 16-bit unsigned integer format.\n\n`skimage.lookfor`(what)\n\nDo a keyword search on scikit-image docstrings.\n\n`skimage.data`\n\nStandard test images.\n\n`skimage.util`\n\nReturn intensity limits, i.e. (min, max) tuple, of the image\u2019s dtype.\n\nInput image.\n\nIf True, clip the negative range (i.e. return 0 for min intensity) even if the\nimage dtype allows negative values.\n\nLower and upper intensity limits.\n\nConvert an image to boolean format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nThe upper half of the input dtype\u2019s positive range is True, and the lower half\nis False. All negative values (if present) are False.\n\nConvert an image to floating point format.\n\nThis function is similar to `img_as_float64`, but will not convert lower-\nprecision floating point arrays to `float64`.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nThe range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when\nconverting from unsigned or signed datatypes, respectively. If the input image\nhas a float type, intensity values are not modified and can be outside the\nranges [0.0, 1.0] or [-1.0, 1.0].\n\nTinting gray-scale images\n\n3D adaptive histogram equalization\n\nPhase Unwrapping\n\nFinding local maxima\n\nUse rolling-ball algorithm for estimating background intensity\n\nExplore 3D images (of cells)\n\nConvert an image to single-precision (32-bit) floating point format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nThe range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when\nconverting from unsigned or signed datatypes, respectively. If the input image\nhas a float type, intensity values are not modified and can be outside the\nranges [0.0, 1.0] or [-1.0, 1.0].\n\nConvert an image to double-precision (64-bit) floating point format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nThe range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when\nconverting from unsigned or signed datatypes, respectively. If the input image\nhas a float type, intensity values are not modified and can be outside the\nranges [0.0, 1.0] or [-1.0, 1.0].\n\nConvert an image to 16-bit signed integer format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nThe values are scaled between -32768 and 32767. If the input data-type is\npositive-only (e.g., uint8), then the output image will still only have\npositive values.\n\nConvert an image to 8-bit unsigned integer format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nNegative input values will be clipped. Positive values are scaled between 0\nand 255.\n\nLocal Histogram Equalization\n\nEntropy\n\nMarkers for watershed transform\n\nSegment human cells (in mitosis)\n\nRank filters\n\nConvert an image to 16-bit unsigned integer format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nNegative input values will be clipped. Positive values are scaled between 0\nand 65535.\n\nDo a keyword search on scikit-image docstrings.\n\nWords to look for.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform", "path": "api/skimage.transform", "type": "transform", "text": "\n`skimage.transform.downscale_local_mean`(\u2026)\n\nDown-sample N-dimensional image by local averaging.\n\n`skimage.transform.estimate_transform`(ttype, \u2026)\n\nEstimate 2D geometric transformation parameters.\n\n`skimage.transform.frt2`(a)\n\nCompute the 2-dimensional finite radon transform (FRT) for an n x n integer\narray.\n\n`skimage.transform.hough_circle`(image, radius)\n\nPerform a circular Hough transform.\n\n`skimage.transform.hough_circle_peaks`(\u2026[, \u2026])\n\nReturn peaks in a circle Hough transform.\n\n`skimage.transform.hough_ellipse`(image[, \u2026])\n\nPerform an elliptical Hough transform.\n\n`skimage.transform.hough_line`(image[, theta])\n\nPerform a straight line Hough transform.\n\n`skimage.transform.hough_line_peaks`(hspace, \u2026)\n\nReturn peaks in a straight line Hough transform.\n\n`skimage.transform.ifrt2`(a)\n\nCompute the 2-dimensional inverse finite radon transform (iFRT) for an (n+1) x\nn integer array.\n\n`skimage.transform.integral_image`(image)\n\nIntegral image / summed area table.\n\n`skimage.transform.integrate`(ii, start, end)\n\nUse an integral image to integrate over a given window.\n\n`skimage.transform.iradon`(radon_image[, \u2026])\n\nInverse radon transform.\n\n`skimage.transform.iradon_sart`(radon_image[, \u2026])\n\nInverse radon transform.\n\n`skimage.transform.matrix_transform`(coords, \u2026)\n\nApply 2D matrix transform.\n\n`skimage.transform.order_angles_golden_ratio`(theta)\n\nOrder angles to reduce the amount of correlated information in subsequent\nprojections.\n\n`skimage.transform.probabilistic_hough_line`(image)\n\nReturn lines from a progressive probabilistic line Hough transform.\n\n`skimage.transform.pyramid_expand`(image[, \u2026])\n\nUpsample and then smooth image.\n\n`skimage.transform.pyramid_gaussian`(image[, \u2026])\n\nYield images of the Gaussian pyramid formed by the input image.\n\n`skimage.transform.pyramid_laplacian`(image[, \u2026])\n\nYield images of the laplacian pyramid formed by the input image.\n\n`skimage.transform.pyramid_reduce`(image[, \u2026])\n\nSmooth and then downsample image.\n\n`skimage.transform.radon`(image[, theta, \u2026])\n\nCalculates the radon transform of an image given specified projection angles.\n\n`skimage.transform.rescale`(image, scale[, \u2026])\n\nScale image by a certain factor.\n\n`skimage.transform.resize`(image, output_shape)\n\nResize image to match a certain size.\n\n`skimage.transform.rotate`(image, angle[, \u2026])\n\nRotate image by a certain angle around its center.\n\n`skimage.transform.swirl`(image[, center, \u2026])\n\nPerform a swirl transformation.\n\n`skimage.transform.warp`(image, inverse_map[, \u2026])\n\nWarp an image according to a given coordinate transformation.\n\n`skimage.transform.warp_coords`(coord_map, shape)\n\nBuild the source coordinates for the output of a 2-D image warp.\n\n`skimage.transform.warp_polar`(image[, \u2026])\n\nRemap image to polar or log-polar coordinates space.\n\n`skimage.transform.AffineTransform`([matrix, \u2026])\n\n2D affine transformation.\n\n`skimage.transform.EssentialMatrixTransform`([\u2026])\n\nEssential matrix transformation.\n\n`skimage.transform.EuclideanTransform`([\u2026])\n\n2D Euclidean transformation.\n\n`skimage.transform.FundamentalMatrixTransform`([\u2026])\n\nFundamental matrix transformation.\n\n`skimage.transform.PiecewiseAffineTransform`()\n\n2D piecewise affine transformation.\n\n`skimage.transform.PolynomialTransform`([params])\n\n2D polynomial transformation.\n\n`skimage.transform.ProjectiveTransform`([matrix])\n\nProjective transformation.\n\n`skimage.transform.SimilarityTransform`([\u2026])\n\n2D similarity transformation.\n\nDown-sample N-dimensional image by local averaging.\n\nThe image is padded with `cval` if it is not perfectly divisible by the\ninteger factors.\n\nIn contrast to interpolation in `skimage.transform.resize` and\n`skimage.transform.rescale` this function calculates the local mean of\nelements in each block of size `factors` in the input image.\n\nN-dimensional input image.\n\nArray containing down-sampling integer factor along each axis.\n\nConstant padding value if image is not perfectly divisible by the integer\nfactors.\n\nUnused, but kept here for API consistency with the other transforms in this\nmodule. (The local mean will never fall outside the range of values in the\ninput image, assuming the provided `cval` also falls within that range.)\n\nDown-sampled image with same number of dimensions as input image. For integer\ninputs, the output dtype will be `float64`. See `numpy.mean()` for details.\n\nEstimate 2D geometric transformation parameters.\n\nYou can determine the over-, well- and under-determined parameters with the\ntotal least-squares method.\n\nNumber of source and destination coordinates must match.\n\nType of transform.\n\nFunction parameters (src, dst, n, angle):\n\nAlso see examples below.\n\nTransform object containing the transformation parameters and providing access\nto forward and inverse transformation functions.\n\nCompute the 2-dimensional finite radon transform (FRT) for an n x n integer\narray.\n\nA 2-D square n x n integer array.\n\nFinite Radon Transform array of (n+1) x n integer coefficients.\n\nSee also\n\nThe two-dimensional inverse FRT.\n\nThe FRT has a unique inverse if and only if n is prime. [FRT] The idea for\nthis algorithm is due to Vlad Negnevitski.\n\nA. Kingston and I. Svalbe, \u201cProjective transforms on periodic discrete image\narrays,\u201d in P. Hawkes (Ed), Advances in Imaging and Electron Physics, 139\n(2006)\n\nGenerate a test image: Use a prime number for the array dimensions\n\nApply the Finite Radon Transform:\n\nPerform a circular Hough transform.\n\nInput image with nonzero values representing edges.\n\nRadii at which to compute the Hough transform. Floats are converted to\nintegers.\n\nNormalize the accumulator with the number of pixels used to draw the radius.\n\nExtend the output size by twice the largest radius in order to detect centers\noutside the input picture.\n\nHough transform accumulator for each radius. R designates the larger radius if\nfull_output is True. Otherwise, R = 0.\n\nReturn peaks in a circle Hough transform.\n\nIdentifies most prominent circles separated by certain distances in given\nHough spaces. Non-maximum suppression with different sizes is applied\nseparately in the first and second dimension of the Hough space to identify\npeaks. For circles with different radius but close in distance, only the one\nwith highest peak is kept.\n\nHough spaces returned by the `hough_circle` function.\n\nRadii corresponding to Hough spaces.\n\nMinimum distance separating centers in the x dimension.\n\nMinimum distance separating centers in the y dimension.\n\nMinimum intensity of peaks in each Hough space. Default is `0.5 *\nmax(hspace)`.\n\nMaximum number of peaks in each Hough space. When the number of peaks exceeds\n`num_peaks`, only `num_peaks` coordinates based on peak intensity are\nconsidered for the corresponding radius.\n\nMaximum number of peaks. When the number of peaks exceeds `num_peaks`, return\n`num_peaks` coordinates based on peak intensity.\n\nIf True, normalize the accumulator by the radius to sort the prominent peaks.\n\nPeak values in Hough space, x and y center coordinates and radii.\n\nCircles with bigger radius have higher peaks in Hough space. If larger circles\nare preferred over smaller ones, `normalize` should be False. Otherwise,\ncircles will be returned in the order of decreasing voting number.\n\nPerform an elliptical Hough transform.\n\nInput image with nonzero values representing edges.\n\nAccumulator threshold value.\n\nBin size on the minor axis used in the accumulator.\n\nMinimal major axis length.\n\nMaximal minor axis length. If None, the value is set to the half of the\nsmaller image dimension.\n\nWhere `(yc, xc)` is the center, `(a, b)` the major and minor axes,\nrespectively. The `orientation` value follows `skimage.draw.ellipse_perimeter`\nconvention.\n\nThe accuracy must be chosen to produce a peak in the accumulator distribution.\nIn other words, a flat accumulator distribution with low values may be caused\nby a too low bin size.\n\nXie, Yonghong, and Qiang Ji. \u201cA new efficient ellipse detection method.\u201d\nPattern Recognition, 2002. Proceedings. 16th International Conference on. Vol.\n2. IEEE, 2002\n\nPerform a straight line Hough transform.\n\nInput image with nonzero values representing edges.\n\nAngles at which to compute the transform, in radians. Defaults to a vector of\n180 angles evenly spaced from -pi/2 to pi/2.\n\nHough transform accumulator.\n\nAngles at which the transform is computed, in radians.\n\nDistance values.\n\nThe origin is the top left corner of the original image. X and Y axis are\nhorizontal and vertical edges respectively. The distance is the minimal\nalgebraic distance from the origin to the detected line. The angle accuracy\ncan be improved by decreasing the step size in the `theta` array.\n\nGenerate a test image:\n\nApply the Hough transform:\n\n(Source code, png, pdf)\n\nReturn peaks in a straight line Hough transform.\n\nIdentifies most prominent lines separated by a certain angle and distance in a\nHough transform. Non-maximum suppression with different sizes is applied\nseparately in the first (distances) and second (angles) dimension of the Hough\nspace to identify peaks.\n\nHough space returned by the `hough_line` function.\n\nAngles returned by the `hough_line` function. Assumed to be continuous.\n(`angles[-1] - angles[0] == PI`).\n\nDistances returned by the `hough_line` function.\n\nMinimum distance separating lines (maximum filter size for first dimension of\nhough space).\n\nMinimum angle separating lines (maximum filter size for second dimension of\nhough space).\n\nMinimum intensity of peaks. Default is `0.5 * max(hspace)`.\n\nMaximum number of peaks. When the number of peaks exceeds `num_peaks`, return\n`num_peaks` coordinates based on peak intensity.\n\nPeak values in Hough space, angles and distances.\n\nCompute the 2-dimensional inverse finite radon transform (iFRT) for an (n+1) x\nn integer array.\n\nA 2-D (n+1) row x n column integer array.\n\nInverse Finite Radon Transform array of n x n integer coefficients.\n\nSee also\n\nThe two-dimensional FRT\n\nThe FRT has a unique inverse if and only if n is prime. See [1] for an\noverview. The idea for this algorithm is due to Vlad Negnevitski.\n\nA. Kingston and I. Svalbe, \u201cProjective transforms on periodic discrete image\narrays,\u201d in P. Hawkes (Ed), Advances in Imaging and Electron Physics, 139\n(2006)\n\nApply the Finite Radon Transform:\n\nApply the Inverse Finite Radon Transform to recover the input\n\nCheck that it\u2019s identical to the original\n\nIntegral image / summed area table.\n\nThe integral image contains the sum of all elements above and to the left of\nit, i.e.:\n\nInput image.\n\nIntegral image/summed area table of same shape as input image.\n\nF.C. Crow, \u201cSummed-area tables for texture mapping,\u201d ACM SIGGRAPH Computer\nGraphics, vol. 18, 1984, pp. 207-212.\n\nUse an integral image to integrate over a given window.\n\nIntegral image.\n\nCoordinates of top left corner of window(s). Each tuple in the list contains\nthe starting row, col, \u2026 index i.e `[(row_win1, col_win1, \u2026), (row_win2,\ncol_win2,\u2026), \u2026]`.\n\nCoordinates of bottom right corner of window(s). Each tuple in the list\ncontaining the end row, col, \u2026 index i.e `[(row_win1, col_win1, \u2026), (row_win2,\ncol_win2, \u2026), \u2026]`.\n\nIntegral (sum) over the given window(s).\n\nInverse radon transform.\n\nReconstruct an image from the radon transform, using the filtered back\nprojection algorithm.\n\nImage containing radon transform (sinogram). Each column of the image\ncorresponds to a projection along a different angle. The tomography rotation\naxis should lie at the pixel index `radon_image.shape[0] // 2` along the 0th\ndimension of `radon_image`.\n\nReconstruction angles (in degrees). Default: m angles evenly spaced between 0\nand 180 (if the shape of `radon_image` is (N, M)).\n\nNumber of rows and columns in the reconstruction.\n\nFilter used in frequency domain filtering. Ramp filter used by default.\nFilters available: ramp, shepp-logan, cosine, hamming, hann. Assign None to\nuse no filter.\n\nInterpolation method used in reconstruction. Methods available: \u2018linear\u2019,\n\u2018nearest\u2019, and \u2018cubic\u2019 (\u2018cubic\u2019 is slow).\n\nAssume the reconstructed image is zero outside the inscribed circle. Also\nchanges the default output_size to match the behaviour of `radon` called with\n`circle=True`.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nReconstructed image. The rotation axis will be located in the pixel with\nindices `(reconstructed.shape[0] // 2, reconstructed.shape[1] // 2)`.\n\nChanged in version 0.19: In `iradon`, `filter` argument is deprecated in favor\nof `filter_name`.\n\nIt applies the Fourier slice theorem to reconstruct an image by multiplying\nthe frequency domain of the filter with the FFT of the projection data. This\nalgorithm is called filtered back projection.\n\nAC Kak, M Slaney, \u201cPrinciples of Computerized Tomographic Imaging\u201d, IEEE Press\n1988.\n\nB.R. Ramesh, N. Srinivasa, K. Rajgopal, \u201cAn Algorithm for Computing the\nDiscrete Radon Transform With Some Applications\u201d, Proceedings of the Fourth\nIEEE Region 10 International Conference, TENCON \u201889, 1989\n\nInverse radon transform.\n\nReconstruct an image from the radon transform, using a single iteration of the\nSimultaneous Algebraic Reconstruction Technique (SART) algorithm.\n\nImage containing radon transform (sinogram). Each column of the image\ncorresponds to a projection along a different angle. The tomography rotation\naxis should lie at the pixel index `radon_image.shape[0] // 2` along the 0th\ndimension of `radon_image`.\n\nReconstruction angles (in degrees). Default: m angles evenly spaced between 0\nand 180 (if the shape of `radon_image` is (N, M)).\n\nImage containing an initial reconstruction estimate. Shape of this array\nshould be `(radon_image.shape[0], radon_image.shape[0])`. The default is an\narray of zeros.\n\nShift the projections contained in `radon_image` (the sinogram) by this many\npixels before reconstructing the image. The i\u2019th value defines the shift of\nthe i\u2019th column of `radon_image`.\n\nForce all values in the reconstructed tomogram to lie in the range `[clip[0],\nclip[1]]`\n\nRelaxation parameter for the update step. A higher value can improve the\nconvergence rate, but one runs the risk of instabilities. Values close to or\nhigher than 1 are not recommended.\n\nOutput data type, must be floating point. By default, if input data type is\nnot float, input is cast to double, otherwise dtype is set to input data type.\n\nReconstructed image. The rotation axis will be located in the pixel with\nindices `(reconstructed.shape[0] // 2, reconstructed.shape[1] // 2)`.\n\nAlgebraic Reconstruction Techniques are based on formulating the tomography\nreconstruction problem as a set of linear equations. Along each ray, the\nprojected value is the sum of all the values of the cross section along the\nray. A typical feature of SART (and a few other variants of algebraic\ntechniques) is that it samples the cross section at equidistant points along\nthe ray, using linear interpolation between the pixel values of the cross\nsection. The resulting set of linear equations are then solved using a\nslightly modified Kaczmarz method.\n\nWhen using SART, a single iteration is usually sufficient to obtain a good\nreconstruction. Further iterations will tend to enhance high-frequency\ninformation, but will also often increase the noise.\n\nAC Kak, M Slaney, \u201cPrinciples of Computerized Tomographic Imaging\u201d, IEEE Press\n1988.\n\nAH Andersen, AC Kak, \u201cSimultaneous algebraic reconstruction technique (SART):\na superior implementation of the ART algorithm\u201d, Ultrasonic Imaging 6 pp 81\u201394\n(1984)\n\nS Kaczmarz, \u201cAngen\u00e4herte aufl\u00f6sung von systemen linearer gleichungen\u201d,\nBulletin International de l\u2019Academie Polonaise des Sciences et des Lettres 35\npp 355\u2013357 (1937)\n\nKohler, T. \u201cA projection access scheme for iterative reconstruction based on\nthe golden section.\u201d Nuclear Science Symposium Conference Record, 2004 IEEE.\nVol. 6. IEEE, 2004.\n\nKaczmarz\u2019 method, Wikipedia, https://en.wikipedia.org/wiki/Kaczmarz_method\n\nApply 2D matrix transform.\n\nx, y coordinates to transform\n\nHomogeneous transformation matrix.\n\nTransformed coordinates.\n\nOrder angles to reduce the amount of correlated information in subsequent\nprojections.\n\nProjection angles in degrees. Duplicate angles are not allowed.\n\nThe returned generator yields indices into `theta` such that `theta[indices]`\ngives the approximate golden ratio ordering of the projections. In total,\n`len(theta)` indices are yielded. All non-negative integers < `len(theta)` are\nyielded exactly once.\n\nThe method used here is that of the golden ratio introduced by T. Kohler.\n\nKohler, T. \u201cA projection access scheme for iterative reconstruction based on\nthe golden section.\u201d Nuclear Science Symposium Conference Record, 2004 IEEE.\nVol. 6. IEEE, 2004.\n\nWinkelmann, Stefanie, et al. \u201cAn optimal radial profile order based on the\nGolden Ratio for time-resolved MRI.\u201d Medical Imaging, IEEE Transactions on\n26.1 (2007): 68-76.\n\nReturn lines from a progressive probabilistic line Hough transform.\n\nInput image with nonzero values representing edges.\n\nThreshold\n\nMinimum accepted length of detected lines. Increase the parameter to extract\nlonger lines.\n\nMaximum gap between pixels to still form a line. Increase the parameter to\nmerge broken lines more aggressively.\n\nAngles at which to compute the transform, in radians. If None, use a range\nfrom -pi/2 to pi/2.\n\nSeed to initialize the random number generator.\n\nList of lines identified, lines in format ((x0, y0), (x1, y1)), indicating\nline start and end.\n\nC. Galamhos, J. Matas and J. Kittler, \u201cProgressive probabilistic Hough\ntransform for line detection\u201d, in IEEE Computer Society Conference on Computer\nVision and Pattern Recognition, 1999.\n\nUpsample and then smooth image.\n\nInput image.\n\nUpscale factor.\n\nSigma for Gaussian filter. Default is `2 * upscale / 6.0` which corresponds to\na filter mask twice the size of the scale factor that covers more than 99% of\nthe Gaussian distribution.\n\nOrder of splines used in interpolation of upsampling. See\n`skimage.transform.warp` for detail.\n\nThe mode parameter determines how the array borders are handled, where cval is\nthe value when mode is equal to \u2018constant\u2019.\n\nValue to fill past edges of input if mode is \u2018constant\u2019.\n\nWhether the last axis of the image is to be interpreted as multiple channels\nor another spatial dimension.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nUpsampled and smoothed float image.\n\nhttp://persci.mit.edu/pub_pdfs/pyramid83.pdf\n\nYield images of the Gaussian pyramid formed by the input image.\n\nRecursively applies the `pyramid_reduce` function to the image, and yields the\ndownscaled images.\n\nNote that the first image of the pyramid will be the original, unscaled image.\nThe total number of images is `max_layer + 1`. In case all layers are\ncomputed, the last image is either a one-pixel image or the image where the\nreduction does not change its shape.\n\nInput image.\n\nNumber of layers for the pyramid. 0th layer is the original image. Default is\n-1 which builds all possible layers.\n\nDownscale factor.\n\nSigma for Gaussian filter. Default is `2 * downscale / 6.0` which corresponds\nto a filter mask twice the size of the scale factor that covers more than 99%\nof the Gaussian distribution.\n\nOrder of splines used in interpolation of downsampling. See\n`skimage.transform.warp` for detail.\n\nThe mode parameter determines how the array borders are handled, where cval is\nthe value when mode is equal to \u2018constant\u2019.\n\nValue to fill past edges of input if mode is \u2018constant\u2019.\n\nWhether the last axis of the image is to be interpreted as multiple channels\nor another spatial dimension.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nGenerator yielding pyramid layers as float images.\n\nhttp://persci.mit.edu/pub_pdfs/pyramid83.pdf\n\nYield images of the laplacian pyramid formed by the input image.\n\nEach layer contains the difference between the downsampled and the\ndownsampled, smoothed image:\n\nNote that the first image of the pyramid will be the difference between the\noriginal, unscaled image and its smoothed version. The total number of images\nis `max_layer + 1`. In case all layers are computed, the last image is either\na one-pixel image or the image where the reduction does not change its shape.\n\nInput image.\n\nNumber of layers for the pyramid. 0th layer is the original image. Default is\n-1 which builds all possible layers.\n\nDownscale factor.\n\nSigma for Gaussian filter. Default is `2 * downscale / 6.0` which corresponds\nto a filter mask twice the size of the scale factor that covers more than 99%\nof the Gaussian distribution.\n\nOrder of splines used in interpolation of downsampling. See\n`skimage.transform.warp` for detail.\n\nThe mode parameter determines how the array borders are handled, where cval is\nthe value when mode is equal to \u2018constant\u2019.\n\nValue to fill past edges of input if mode is \u2018constant\u2019.\n\nWhether the last axis of the image is to be interpreted as multiple channels\nor another spatial dimension.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nGenerator yielding pyramid layers as float images.\n\nhttp://persci.mit.edu/pub_pdfs/pyramid83.pdf\n\nhttp://sepwww.stanford.edu/data/media/public/sep/morgan/texturematch/paper_html/node3.html\n\nSmooth and then downsample image.\n\nInput image.\n\nDownscale factor.\n\nSigma for Gaussian filter. Default is `2 * downscale / 6.0` which corresponds\nto a filter mask twice the size of the scale factor that covers more than 99%\nof the Gaussian distribution.\n\nOrder of splines used in interpolation of downsampling. See\n`skimage.transform.warp` for detail.\n\nThe mode parameter determines how the array borders are handled, where cval is\nthe value when mode is equal to \u2018constant\u2019.\n\nValue to fill past edges of input if mode is \u2018constant\u2019.\n\nWhether the last axis of the image is to be interpreted as multiple channels\nor another spatial dimension.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nSmoothed and downsampled float image.\n\nhttp://persci.mit.edu/pub_pdfs/pyramid83.pdf\n\nCalculates the radon transform of an image given specified projection angles.\n\nInput image. The rotation axis will be located in the pixel with indices\n`(image.shape[0] // 2, image.shape[1] // 2)`.\n\nProjection angles (in degrees). If `None`, the value is set to np.arange(180).\n\nAssume image is zero outside the inscribed circle, making the width of each\nprojection (the first dimension of the sinogram) equal to `min(image.shape)`.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nRadon transform (sinogram). The tomography rotation axis will lie at the pixel\nindex `radon_image.shape[0] // 2` along the 0th dimension of `radon_image`.\n\nBased on code of Justin K. Romberg\n(https://www.clear.rice.edu/elec431/projects96/DSP/bpanalysis.html)\n\nAC Kak, M Slaney, \u201cPrinciples of Computerized Tomographic Imaging\u201d, IEEE Press\n1988.\n\nB.R. Ramesh, N. Srinivasa, K. Rajgopal, \u201cAn Algorithm for Computing the\nDiscrete Radon Transform With Some Applications\u201d, Proceedings of the Fourth\nIEEE Region 10 International Conference, TENCON \u201889, 1989\n\nScale image by a certain factor.\n\nPerforms interpolation to up-scale or down-scale N-dimensional images. Note\nthat anti-aliasing should be enabled when down-sizing images to avoid aliasing\nartifacts. For down-sampling with an integer factor also see\n`skimage.transform.downscale_local_mean`.\n\nInput image.\n\nScale factors. Separate scale factors can be defined as `(rows, cols[, \u2026][,\ndim])`.\n\nScaled version of the input.\n\nThe order of the spline interpolation, default is 0 if image.dtype is bool and\n1 otherwise. The order has to be in the range 0-5. See\n`skimage.transform.warp` for detail.\n\nPoints outside the boundaries of the input are filled according to the given\nmode. Modes match the behaviour of `numpy.pad`.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nWhether to clip the output to the range of values of the input image. This is\nenabled by default, since higher order interpolation may produce values\noutside the given input range.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nWhether the last axis of the image is to be interpreted as multiple channels\nor another spatial dimension.\n\nWhether to apply a Gaussian filter to smooth the image prior to down-scaling.\nIt is crucial to filter when down-sampling the image to avoid aliasing\nartifacts. If input image data type is bool, no anti-aliasing is applied.\n\nStandard deviation for Gaussian filtering to avoid aliasing artifacts. By\ndefault, this value is chosen as (s - 1) / 2 where s is the down-scaling\nfactor.\n\nModes \u2018reflect\u2019 and \u2018symmetric\u2019 are similar, but differ in whether the edge\npixels are duplicated during the reflection. As an example, if an array has\nvalues [0, 1, 2] and was padded to the right by four values using symmetric,\nthe result would be [0, 1, 2, 2, 1, 0, 0], while for reflect it would be [0,\n1, 2, 1, 0, 1, 2].\n\nResize image to match a certain size.\n\nPerforms interpolation to up-size or down-size N-dimensional images. Note that\nanti-aliasing should be enabled when down-sizing images to avoid aliasing\nartifacts. For down-sampling with an integer factor also see\n`skimage.transform.downscale_local_mean`.\n\nInput image.\n\nSize of the generated output image `(rows, cols[, \u2026][, dim])`. If `dim` is not\nprovided, the number of channels is preserved. In case the number of input\nchannels does not equal the number of output channels a n-dimensional\ninterpolation is applied.\n\nResized version of the input.\n\nThe order of the spline interpolation, default is 0 if image.dtype is bool and\n1 otherwise. The order has to be in the range 0-5. See\n`skimage.transform.warp` for detail.\n\nPoints outside the boundaries of the input are filled according to the given\nmode. Modes match the behaviour of `numpy.pad`.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nWhether to clip the output to the range of values of the input image. This is\nenabled by default, since higher order interpolation may produce values\noutside the given input range.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nWhether to apply a Gaussian filter to smooth the image prior to down-scaling.\nIt is crucial to filter when down-sampling the image to avoid aliasing\nartifacts. If input image data type is bool, no anti-aliasing is applied.\n\nStandard deviation for Gaussian filtering to avoid aliasing artifacts. By\ndefault, this value is chosen as (s - 1) / 2 where s is the down-scaling\nfactor, where s > 1\\. For the up-size case, s < 1, no anti-aliasing is\nperformed prior to rescaling.\n\nModes \u2018reflect\u2019 and \u2018symmetric\u2019 are similar, but differ in whether the edge\npixels are duplicated during the reflection. As an example, if an array has\nvalues [0, 1, 2] and was padded to the right by four values using symmetric,\nthe result would be [0, 1, 2, 2, 1, 0, 0], while for reflect it would be [0,\n1, 2, 1, 0, 1, 2].\n\nRotate image by a certain angle around its center.\n\nInput image.\n\nRotation angle in degrees in counter-clockwise direction.\n\nDetermine whether the shape of the output image will be automatically\ncalculated, so the complete rotated image exactly fits. Default is False.\n\nThe rotation center. If `center=None`, the image is rotated around its center,\ni.e. `center=(cols / 2 - 0.5, rows / 2 - 0.5)`. Please note that this\nparameter is (cols, rows), contrary to normal skimage ordering.\n\nRotated version of the input.\n\nThe order of the spline interpolation, default is 0 if image.dtype is bool and\n1 otherwise. The order has to be in the range 0-5. See\n`skimage.transform.warp` for detail.\n\nPoints outside the boundaries of the input are filled according to the given\nmode. Modes match the behaviour of `numpy.pad`.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nWhether to clip the output to the range of values of the input image. This is\nenabled by default, since higher order interpolation may produce values\noutside the given input range.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nModes \u2018reflect\u2019 and \u2018symmetric\u2019 are similar, but differ in whether the edge\npixels are duplicated during the reflection. As an example, if an array has\nvalues [0, 1, 2] and was padded to the right by four values using symmetric,\nthe result would be [0, 1, 2, 2, 1, 0, 0], while for reflect it would be [0,\n1, 2, 1, 0, 1, 2].\n\nDifferent perimeters\n\nMeasure region properties\n\nPerform a swirl transformation.\n\nInput image.\n\nCenter coordinate of transformation.\n\nThe amount of swirling applied.\n\nThe extent of the swirl in pixels. The effect dies out rapidly beyond\n`radius`.\n\nAdditional rotation applied to the image.\n\nSwirled version of the input.\n\nShape of the output image generated. By default the shape of the input image\nis preserved.\n\nThe order of the spline interpolation, default is 0 if image.dtype is bool and\n1 otherwise. The order has to be in the range 0-5. See\n`skimage.transform.warp` for detail.\n\nPoints outside the boundaries of the input are filled according to the given\nmode, with \u2018constant\u2019 used as the default. Modes match the behaviour of\n`numpy.pad`.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nWhether to clip the output to the range of values of the input image. This is\nenabled by default, since higher order interpolation may produce values\noutside the given input range.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nWarp an image according to a given coordinate transformation.\n\nInput image.\n\nInverse coordinate map, which transforms coordinates in the output images into\ntheir corresponding coordinates in the input image.\n\nThere are a number of different options to define this map, depending on the\ndimensionality of the input image. A 2-D image can have 2 dimensions for gray-\nscale images, or 3 dimensions with color information.\n\nNote, that a `(3, 3)` matrix is interpreted as a homogeneous transformation\nmatrix, so you cannot interpolate values from a 3-D input, if the output is of\nshape `(3,)`.\n\nSee example section for usage.\n\nKeyword arguments passed to `inverse_map`.\n\nShape of the output image generated. By default the shape of the input image\nis preserved. Note that, even for multi-band images, only rows and columns\nneed to be specified.\n\nDefault is 0 if image.dtype is bool and 1 otherwise.\n\nPoints outside the boundaries of the input are filled according to the given\nmode. Modes match the behaviour of `numpy.pad`.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nWhether to clip the output to the range of values of the input image. This is\nenabled by default, since higher order interpolation may produce values\noutside the given input range.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nThe warped input image.\n\nThe following image warps are all equal but differ substantially in execution\ntime. The image is shifted to the bottom.\n\nUse a geometric transform to warp an image (fast):\n\nUse a callable (slow):\n\nUse a transformation matrix to warp an image (fast):\n\nYou can also use the inverse of a geometric transformation (fast):\n\nFor N-D images you can pass a coordinate array, that specifies the coordinates\nin the input image for every element in the output image. E.g. if you want to\nrescale a 3-D cube, you can do:\n\nSetup the coordinate array, that defines the scaling:\n\nAssume that the cube contains spatial data, where the first array element\ncenter is at coordinate (0.5, 0.5, 0.5) in real space, i.e. we have to account\nfor this extra offset when scaling the image:\n\nRegistration using optical flow\n\nBuild the source coordinates for the output of a 2-D image warp.\n\nReturn input coordinates for given output coordinates. Coordinates are in the\nshape (P, 2), where P is the number of coordinates and each element is a\n`(row, col)` pair.\n\nShape of output image `(rows, cols[, bands])`.\n\ndtype for return value (sane choices: float32 or float64).\n\nCoordinates for `scipy.ndimage.map_coordinates`, that will yield an image of\nshape (orows, ocols, bands) by drawing from source points according to the\n`coord_transform_fn`.\n\nThis is a lower-level routine that produces the source coordinates for 2-D\nimages used by `warp()`.\n\nIt is provided separately from `warp` to give additional flexibility to users\nwho would like, for example, to re-use a particular coordinate mapping, to use\nspecific dtypes at various points along the the image-warping process, or to\nimplement different post-processing logic than `warp` performs after the call\nto `ndi.map_coordinates`.\n\nProduce a coordinate map that shifts an image up and to the right:\n\nRemap image to polar or log-polar coordinates space.\n\nInput image. Only 2-D arrays are accepted by default. If `multichannel=True`,\n3-D arrays are accepted and the last axis is interpreted as multiple channels.\n\nPoint in image that represents the center of the transformation (i.e., the\norigin in cartesian space). Values can be of type `float`. If no value is\ngiven, the center is assumed to be the center point of the image.\n\nRadius of the circle that bounds the area to be transformed.\n\nSpecify whether the image warp is polar or log-polar. Defaults to \u2018linear\u2019.\n\nWhether the image is a 3-D array in which the third axis is to be interpreted\nas multiple channels. If set to `False` (default), only 2-D arrays are\naccepted.\n\nPassed to `transform.warp`.\n\nThe polar or log-polar warped image.\n\nPerform a basic polar warp on a grayscale image:\n\nPerform a log-polar warp on a grayscale image:\n\nPerform a log-polar warp on a grayscale image while specifying center, radius,\nand output shape:\n\nPerform a log-polar warp on a color image:\n\nBases: `skimage.transform._geometric.ProjectiveTransform`\n\n2D affine transformation.\n\nHas the following form:\n\nwhere `sx` and `sy` are scale factors in the x and y directions, and the\nhomogeneous transformation matrix is:\n\nHomogeneous transformation matrix.\n\nScale factor(s). If a single value, it will be assigned to both sx and sy.\n\nNew in version 0.17: Added support for supplying a single scalar value.\n\nRotation angle in counter-clockwise direction as radians.\n\nShear angle in counter-clockwise direction as radians.\n\nTranslation parameters.\n\nHomogeneous transformation matrix.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nBases: `skimage.transform._geometric.FundamentalMatrixTransform`\n\nEssential matrix transformation.\n\nThe essential matrix relates corresponding points between a pair of calibrated\nimages. The matrix transforms normalized, homogeneous image points in one\nimage to epipolar lines in the other image.\n\nThe essential matrix is only defined for a pair of moving images capturing a\nnon-planar scene. In the case of pure rotation or planar scenes, the\nhomography describes the geometric relation between two images\n(`ProjectiveTransform`). If the intrinsic calibration of the images is\nunknown, the fundamental matrix describes the projective relation between the\ntwo images (`FundamentalMatrixTransform`).\n\nRotation matrix of the relative camera motion.\n\nTranslation vector of the relative camera motion. The vector must have unit\nlength.\n\nEssential matrix.\n\nHartley, Richard, and Andrew Zisserman. Multiple view geometry in computer\nvision. Cambridge university press, 2003.\n\nEssential matrix.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nEstimate essential matrix using 8-point algorithm.\n\nThe 8-point algorithm requires at least 8 corresponding point pairs for a\nwell-conditioned solution, otherwise the over-determined solution is\nestimated.\n\nSource coordinates.\n\nDestination coordinates.\n\nTrue, if model estimation succeeds.\n\nBases: `skimage.transform._geometric.ProjectiveTransform`\n\n2D Euclidean transformation.\n\nHas the following form:\n\nwhere the homogeneous transformation matrix is:\n\nThe Euclidean transformation is a rigid transformation with rotation and\ntranslation parameters. The similarity transformation extends the Euclidean\ntransformation with a single scaling factor.\n\nHomogeneous transformation matrix.\n\nRotation angle in counter-clockwise direction as radians.\n\nx, y translation parameters.\n\nHomogeneous transformation matrix.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nEstimate the transformation from a set of corresponding points.\n\nYou can determine the over-, well- and under-determined parameters with the\ntotal least-squares method.\n\nNumber of source and destination coordinates must match.\n\nSource coordinates.\n\nDestination coordinates.\n\nTrue, if model estimation succeeds.\n\nBases: `skimage.transform._geometric.GeometricTransform`\n\nFundamental matrix transformation.\n\nThe fundamental matrix relates corresponding points between a pair of\nuncalibrated images. The matrix transforms homogeneous image points in one\nimage to epipolar lines in the other image.\n\nThe fundamental matrix is only defined for a pair of moving images. In the\ncase of pure rotation or planar scenes, the homography describes the geometric\nrelation between two images (`ProjectiveTransform`). If the intrinsic\ncalibration of the images is known, the essential matrix describes the metric\nrelation between the two images (`EssentialMatrixTransform`).\n\nFundamental matrix.\n\nHartley, Richard, and Andrew Zisserman. Multiple view geometry in computer\nvision. Cambridge university press, 2003.\n\nFundamental matrix.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nEstimate fundamental matrix using 8-point algorithm.\n\nThe 8-point algorithm requires at least 8 corresponding point pairs for a\nwell-conditioned solution, otherwise the over-determined solution is\nestimated.\n\nSource coordinates.\n\nDestination coordinates.\n\nTrue, if model estimation succeeds.\n\nApply inverse transformation.\n\nDestination coordinates.\n\nEpipolar lines in the source image.\n\nCompute the Sampson distance.\n\nThe Sampson distance is the first approximation to the geometric error.\n\nSource coordinates.\n\nDestination coordinates.\n\nSampson distance.\n\nBases: `skimage.transform._geometric.GeometricTransform`\n\n2D piecewise affine transformation.\n\nControl points are used to define the mapping. The transform is based on a\nDelaunay triangulation of the points to form a mesh. Each triangle is used to\nfind a local affine transform.\n\nAffine transformations for each triangle in the mesh.\n\nInverse affine transformations for each triangle in the mesh.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nEstimate the transformation from a set of corresponding points.\n\nNumber of source and destination coordinates must match.\n\nSource coordinates.\n\nDestination coordinates.\n\nTrue, if model estimation succeeds.\n\nApply inverse transformation.\n\nCoordinates outside of the mesh will be set to `- 1`.\n\nSource coordinates.\n\nTransformed coordinates.\n\nBases: `skimage.transform._geometric.GeometricTransform`\n\n2D polynomial transformation.\n\nHas the following form:\n\nPolynomial coefficients where `N * 2 = (order + 1) * (order + 2)`. So, a_ji is\ndefined in `params[0, :]` and b_ji in `params[1, :]`.\n\nPolynomial coefficients where `N * 2 = (order + 1) * (order + 2)`. So, a_ji is\ndefined in `params[0, :]` and b_ji in `params[1, :]`.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nEstimate the transformation from a set of corresponding points.\n\nYou can determine the over-, well- and under-determined parameters with the\ntotal least-squares method.\n\nNumber of source and destination coordinates must match.\n\nThe transformation is defined as:\n\nThese equations can be transformed to the following form:\n\nwhich exist for each set of corresponding points, so we have a set of N * 2\nequations. The coefficients appear linearly so we can write A x = 0, where:\n\nIn case of total least-squares the solution of this homogeneous system of\nequations is the right singular vector of A which corresponds to the smallest\nsingular value normed by the coefficient c3.\n\nSource coordinates.\n\nDestination coordinates.\n\nPolynomial order (number of coefficients is order + 1).\n\nTrue, if model estimation succeeds.\n\nApply inverse transformation.\n\nDestination coordinates.\n\nSource coordinates.\n\nBases: `skimage.transform._geometric.GeometricTransform`\n\nProjective transformation.\n\nApply a projective transformation (homography) on coordinates.\n\nFor each homogeneous coordinate \\\\(\\mathbf{x} = [x, y, 1]^T\\\\), its target\nposition is calculated by multiplying with the given matrix, \\\\(H\\\\), to give\n\\\\(H \\mathbf{x}\\\\):\n\nE.g., to rotate by theta degrees clockwise, the matrix should be:\n\nor, to translate x by 10 and y by 20:\n\nHomogeneous transformation matrix.\n\nHomogeneous transformation matrix.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nEstimate the transformation from a set of corresponding points.\n\nYou can determine the over-, well- and under-determined parameters with the\ntotal least-squares method.\n\nNumber of source and destination coordinates must match.\n\nThe transformation is defined as:\n\nThese equations can be transformed to the following form:\n\nwhich exist for each set of corresponding points, so we have a set of N * 2\nequations. The coefficients appear linearly so we can write A x = 0, where:\n\nIn case of total least-squares the solution of this homogeneous system of\nequations is the right singular vector of A which corresponds to the smallest\nsingular value normed by the coefficient c3.\n\nIn case of the affine transformation the coefficients c0 and c1 are 0. Thus\nthe system of equations is:\n\nSource coordinates.\n\nDestination coordinates.\n\nTrue, if model estimation succeeds.\n\nApply inverse transformation.\n\nDestination coordinates.\n\nSource coordinates.\n\nBases: `skimage.transform._geometric.EuclideanTransform`\n\n2D similarity transformation.\n\nHas the following form:\n\nwhere `s` is a scale factor and the homogeneous transformation matrix is:\n\nThe similarity transformation extends the Euclidean transformation with a\nsingle scaling factor in addition to the rotation and translation parameters.\n\nHomogeneous transformation matrix.\n\nScale factor.\n\nRotation angle in counter-clockwise direction as radians.\n\nx, y translation parameters.\n\nHomogeneous transformation matrix.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nEstimate the transformation from a set of corresponding points.\n\nYou can determine the over-, well- and under-determined parameters with the\ntotal least-squares method.\n\nNumber of source and destination coordinates must match.\n\nSource coordinates.\n\nDestination coordinates.\n\nTrue, if model estimation succeeds.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.AffineTransform", "path": "api/skimage.transform#skimage.transform.AffineTransform", "type": "transform", "text": "\nBases: `skimage.transform._geometric.ProjectiveTransform`\n\n2D affine transformation.\n\nHas the following form:\n\nwhere `sx` and `sy` are scale factors in the x and y directions, and the\nhomogeneous transformation matrix is:\n\nHomogeneous transformation matrix.\n\nScale factor(s). If a single value, it will be assigned to both sx and sy.\n\nNew in version 0.17: Added support for supplying a single scalar value.\n\nRotation angle in counter-clockwise direction as radians.\n\nShear angle in counter-clockwise direction as radians.\n\nTranslation parameters.\n\nHomogeneous transformation matrix.\n\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.AffineTransform.rotation()", "path": "api/skimage.transform#skimage.transform.AffineTransform.rotation", "type": "transform", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.AffineTransform.scale()", "path": "api/skimage.transform#skimage.transform.AffineTransform.scale", "type": "transform", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.AffineTransform.shear()", "path": "api/skimage.transform#skimage.transform.AffineTransform.shear", "type": "transform", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.AffineTransform.translation()", "path": "api/skimage.transform#skimage.transform.AffineTransform.translation", "type": "transform", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.AffineTransform.__init__()", "path": "api/skimage.transform#skimage.transform.AffineTransform.__init__", "type": "transform", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.downscale_local_mean()", "path": "api/skimage.transform#skimage.transform.downscale_local_mean", "type": "transform", "text": "\nDown-sample N-dimensional image by local averaging.\n\nThe image is padded with `cval` if it is not perfectly divisible by the\ninteger factors.\n\nIn contrast to interpolation in `skimage.transform.resize` and\n`skimage.transform.rescale` this function calculates the local mean of\nelements in each block of size `factors` in the input image.\n\nN-dimensional input image.\n\nArray containing down-sampling integer factor along each axis.\n\nConstant padding value if image is not perfectly divisible by the integer\nfactors.\n\nUnused, but kept here for API consistency with the other transforms in this\nmodule. (The local mean will never fall outside the range of values in the\ninput image, assuming the provided `cval` also falls within that range.)\n\nDown-sampled image with same number of dimensions as input image. For integer\ninputs, the output dtype will be `float64`. See `numpy.mean()` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.EssentialMatrixTransform", "path": "api/skimage.transform#skimage.transform.EssentialMatrixTransform", "type": "transform", "text": "\nBases: `skimage.transform._geometric.FundamentalMatrixTransform`\n\nEssential matrix transformation.\n\nThe essential matrix relates corresponding points between a pair of calibrated\nimages. The matrix transforms normalized, homogeneous image points in one\nimage to epipolar lines in the other image.\n\nThe essential matrix is only defined for a pair of moving images capturing a\nnon-planar scene. In the case of pure rotation or planar scenes, the\nhomography describes the geometric relation between two images\n(`ProjectiveTransform`). If the intrinsic calibration of the images is\nunknown, the fundamental matrix describes the projective relation between the\ntwo images (`FundamentalMatrixTransform`).\n\nRotation matrix of the relative camera motion.\n\nTranslation vector of the relative camera motion. The vector must have unit\nlength.\n\nEssential matrix.\n\nHartley, Richard, and Andrew Zisserman. Multiple view geometry in computer\nvision. Cambridge university press, 2003.\n\nEssential matrix.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nEstimate essential matrix using 8-point algorithm.\n\nThe 8-point algorithm requires at least 8 corresponding point pairs for a\nwell-conditioned solution, otherwise the over-determined solution is\nestimated.\n\nSource coordinates.\n\nDestination coordinates.\n\nTrue, if model estimation succeeds.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.EssentialMatrixTransform.estimate()", "path": "api/skimage.transform#skimage.transform.EssentialMatrixTransform.estimate", "type": "transform", "text": "\nEstimate essential matrix using 8-point algorithm.\n\nThe 8-point algorithm requires at least 8 corresponding point pairs for a\nwell-conditioned solution, otherwise the over-determined solution is\nestimated.\n\nSource coordinates.\n\nDestination coordinates.\n\nTrue, if model estimation succeeds.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.EssentialMatrixTransform.__init__()", "path": "api/skimage.transform#skimage.transform.EssentialMatrixTransform.__init__", "type": "transform", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.estimate_transform()", "path": "api/skimage.transform#skimage.transform.estimate_transform", "type": "transform", "text": "\nEstimate 2D geometric transformation parameters.\n\nYou can determine the over-, well- and under-determined parameters with the\ntotal least-squares method.\n\nNumber of source and destination coordinates must match.\n\nType of transform.\n\nFunction parameters (src, dst, n, angle):\n\nAlso see examples below.\n\nTransform object containing the transformation parameters and providing access\nto forward and inverse transformation functions.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.EuclideanTransform", "path": "api/skimage.transform#skimage.transform.EuclideanTransform", "type": "transform", "text": "\nBases: `skimage.transform._geometric.ProjectiveTransform`\n\n2D Euclidean transformation.\n\nHas the following form:\n\nwhere the homogeneous transformation matrix is:\n\nThe Euclidean transformation is a rigid transformation with rotation and\ntranslation parameters. The similarity transformation extends the Euclidean\ntransformation with a single scaling factor.\n\nHomogeneous transformation matrix.\n\nRotation angle in counter-clockwise direction as radians.\n\nx, y translation parameters.\n\nHomogeneous transformation matrix.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nEstimate the transformation from a set of corresponding points.\n\nYou can determine the over-, well- and under-determined parameters with the\ntotal least-squares method.\n\nNumber of source and destination coordinates must match.\n\nSource coordinates.\n\nDestination coordinates.\n\nTrue, if model estimation succeeds.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.EuclideanTransform.estimate()", "path": "api/skimage.transform#skimage.transform.EuclideanTransform.estimate", "type": "transform", "text": "\nEstimate the transformation from a set of corresponding points.\n\nYou can determine the over-, well- and under-determined parameters with the\ntotal least-squares method.\n\nNumber of source and destination coordinates must match.\n\nSource coordinates.\n\nDestination coordinates.\n\nTrue, if model estimation succeeds.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.EuclideanTransform.rotation()", "path": "api/skimage.transform#skimage.transform.EuclideanTransform.rotation", "type": "transform", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.EuclideanTransform.translation()", "path": "api/skimage.transform#skimage.transform.EuclideanTransform.translation", "type": "transform", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.EuclideanTransform.__init__()", "path": "api/skimage.transform#skimage.transform.EuclideanTransform.__init__", "type": "transform", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.frt2()", "path": "api/skimage.transform#skimage.transform.frt2", "type": "transform", "text": "\nCompute the 2-dimensional finite radon transform (FRT) for an n x n integer\narray.\n\nA 2-D square n x n integer array.\n\nFinite Radon Transform array of (n+1) x n integer coefficients.\n\nSee also\n\nThe two-dimensional inverse FRT.\n\nThe FRT has a unique inverse if and only if n is prime. [FRT] The idea for\nthis algorithm is due to Vlad Negnevitski.\n\nA. Kingston and I. Svalbe, \u201cProjective transforms on periodic discrete image\narrays,\u201d in P. Hawkes (Ed), Advances in Imaging and Electron Physics, 139\n(2006)\n\nGenerate a test image: Use a prime number for the array dimensions\n\nApply the Finite Radon Transform:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.FundamentalMatrixTransform", "path": "api/skimage.transform#skimage.transform.FundamentalMatrixTransform", "type": "transform", "text": "\nBases: `skimage.transform._geometric.GeometricTransform`\n\nFundamental matrix transformation.\n\nThe fundamental matrix relates corresponding points between a pair of\nuncalibrated images. The matrix transforms homogeneous image points in one\nimage to epipolar lines in the other image.\n\nThe fundamental matrix is only defined for a pair of moving images. In the\ncase of pure rotation or planar scenes, the homography describes the geometric\nrelation between two images (`ProjectiveTransform`). If the intrinsic\ncalibration of the images is known, the essential matrix describes the metric\nrelation between the two images (`EssentialMatrixTransform`).\n\nFundamental matrix.\n\nHartley, Richard, and Andrew Zisserman. Multiple view geometry in computer\nvision. Cambridge university press, 2003.\n\nFundamental matrix.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nEstimate fundamental matrix using 8-point algorithm.\n\nThe 8-point algorithm requires at least 8 corresponding point pairs for a\nwell-conditioned solution, otherwise the over-determined solution is\nestimated.\n\nSource coordinates.\n\nDestination coordinates.\n\nTrue, if model estimation succeeds.\n\nApply inverse transformation.\n\nDestination coordinates.\n\nEpipolar lines in the source image.\n\nCompute the Sampson distance.\n\nThe Sampson distance is the first approximation to the geometric error.\n\nSource coordinates.\n\nDestination coordinates.\n\nSampson distance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.FundamentalMatrixTransform.estimate()", "path": "api/skimage.transform#skimage.transform.FundamentalMatrixTransform.estimate", "type": "transform", "text": "\nEstimate fundamental matrix using 8-point algorithm.\n\nThe 8-point algorithm requires at least 8 corresponding point pairs for a\nwell-conditioned solution, otherwise the over-determined solution is\nestimated.\n\nSource coordinates.\n\nDestination coordinates.\n\nTrue, if model estimation succeeds.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.FundamentalMatrixTransform.inverse()", "path": "api/skimage.transform#skimage.transform.FundamentalMatrixTransform.inverse", "type": "transform", "text": "\nApply inverse transformation.\n\nDestination coordinates.\n\nEpipolar lines in the source image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.FundamentalMatrixTransform.residuals()", "path": "api/skimage.transform#skimage.transform.FundamentalMatrixTransform.residuals", "type": "transform", "text": "\nCompute the Sampson distance.\n\nThe Sampson distance is the first approximation to the geometric error.\n\nSource coordinates.\n\nDestination coordinates.\n\nSampson distance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.FundamentalMatrixTransform.__init__()", "path": "api/skimage.transform#skimage.transform.FundamentalMatrixTransform.__init__", "type": "transform", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.hough_circle()", "path": "api/skimage.transform#skimage.transform.hough_circle", "type": "transform", "text": "\nPerform a circular Hough transform.\n\nInput image with nonzero values representing edges.\n\nRadii at which to compute the Hough transform. Floats are converted to\nintegers.\n\nNormalize the accumulator with the number of pixels used to draw the radius.\n\nExtend the output size by twice the largest radius in order to detect centers\noutside the input picture.\n\nHough transform accumulator for each radius. R designates the larger radius if\nfull_output is True. Otherwise, R = 0.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.hough_circle_peaks()", "path": "api/skimage.transform#skimage.transform.hough_circle_peaks", "type": "transform", "text": "\nReturn peaks in a circle Hough transform.\n\nIdentifies most prominent circles separated by certain distances in given\nHough spaces. Non-maximum suppression with different sizes is applied\nseparately in the first and second dimension of the Hough space to identify\npeaks. For circles with different radius but close in distance, only the one\nwith highest peak is kept.\n\nHough spaces returned by the `hough_circle` function.\n\nRadii corresponding to Hough spaces.\n\nMinimum distance separating centers in the x dimension.\n\nMinimum distance separating centers in the y dimension.\n\nMinimum intensity of peaks in each Hough space. Default is `0.5 *\nmax(hspace)`.\n\nMaximum number of peaks in each Hough space. When the number of peaks exceeds\n`num_peaks`, only `num_peaks` coordinates based on peak intensity are\nconsidered for the corresponding radius.\n\nMaximum number of peaks. When the number of peaks exceeds `num_peaks`, return\n`num_peaks` coordinates based on peak intensity.\n\nIf True, normalize the accumulator by the radius to sort the prominent peaks.\n\nPeak values in Hough space, x and y center coordinates and radii.\n\nCircles with bigger radius have higher peaks in Hough space. If larger circles\nare preferred over smaller ones, `normalize` should be False. Otherwise,\ncircles will be returned in the order of decreasing voting number.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.hough_ellipse()", "path": "api/skimage.transform#skimage.transform.hough_ellipse", "type": "transform", "text": "\nPerform an elliptical Hough transform.\n\nInput image with nonzero values representing edges.\n\nAccumulator threshold value.\n\nBin size on the minor axis used in the accumulator.\n\nMinimal major axis length.\n\nMaximal minor axis length. If None, the value is set to the half of the\nsmaller image dimension.\n\nWhere `(yc, xc)` is the center, `(a, b)` the major and minor axes,\nrespectively. The `orientation` value follows `skimage.draw.ellipse_perimeter`\nconvention.\n\nThe accuracy must be chosen to produce a peak in the accumulator distribution.\nIn other words, a flat accumulator distribution with low values may be caused\nby a too low bin size.\n\nXie, Yonghong, and Qiang Ji. \u201cA new efficient ellipse detection method.\u201d\nPattern Recognition, 2002. Proceedings. 16th International Conference on. Vol.\n2. IEEE, 2002\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.hough_line()", "path": "api/skimage.transform#skimage.transform.hough_line", "type": "transform", "text": "\nPerform a straight line Hough transform.\n\nInput image with nonzero values representing edges.\n\nAngles at which to compute the transform, in radians. Defaults to a vector of\n180 angles evenly spaced from -pi/2 to pi/2.\n\nHough transform accumulator.\n\nAngles at which the transform is computed, in radians.\n\nDistance values.\n\nThe origin is the top left corner of the original image. X and Y axis are\nhorizontal and vertical edges respectively. The distance is the minimal\nalgebraic distance from the origin to the detected line. The angle accuracy\ncan be improved by decreasing the step size in the `theta` array.\n\nGenerate a test image:\n\nApply the Hough transform:\n\n(Source code, png, pdf)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.hough_line_peaks()", "path": "api/skimage.transform#skimage.transform.hough_line_peaks", "type": "transform", "text": "\nReturn peaks in a straight line Hough transform.\n\nIdentifies most prominent lines separated by a certain angle and distance in a\nHough transform. Non-maximum suppression with different sizes is applied\nseparately in the first (distances) and second (angles) dimension of the Hough\nspace to identify peaks.\n\nHough space returned by the `hough_line` function.\n\nAngles returned by the `hough_line` function. Assumed to be continuous.\n(`angles[-1] - angles[0] == PI`).\n\nDistances returned by the `hough_line` function.\n\nMinimum distance separating lines (maximum filter size for first dimension of\nhough space).\n\nMinimum angle separating lines (maximum filter size for second dimension of\nhough space).\n\nMinimum intensity of peaks. Default is `0.5 * max(hspace)`.\n\nMaximum number of peaks. When the number of peaks exceeds `num_peaks`, return\n`num_peaks` coordinates based on peak intensity.\n\nPeak values in Hough space, angles and distances.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.ifrt2()", "path": "api/skimage.transform#skimage.transform.ifrt2", "type": "transform", "text": "\nCompute the 2-dimensional inverse finite radon transform (iFRT) for an (n+1) x\nn integer array.\n\nA 2-D (n+1) row x n column integer array.\n\nInverse Finite Radon Transform array of n x n integer coefficients.\n\nSee also\n\nThe two-dimensional FRT\n\nThe FRT has a unique inverse if and only if n is prime. See [1] for an\noverview. The idea for this algorithm is due to Vlad Negnevitski.\n\nA. Kingston and I. Svalbe, \u201cProjective transforms on periodic discrete image\narrays,\u201d in P. Hawkes (Ed), Advances in Imaging and Electron Physics, 139\n(2006)\n\nApply the Finite Radon Transform:\n\nApply the Inverse Finite Radon Transform to recover the input\n\nCheck that it\u2019s identical to the original\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.integral_image()", "path": "api/skimage.transform#skimage.transform.integral_image", "type": "transform", "text": "\nIntegral image / summed area table.\n\nThe integral image contains the sum of all elements above and to the left of\nit, i.e.:\n\nInput image.\n\nIntegral image/summed area table of same shape as input image.\n\nF.C. Crow, \u201cSummed-area tables for texture mapping,\u201d ACM SIGGRAPH Computer\nGraphics, vol. 18, 1984, pp. 207-212.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.integrate()", "path": "api/skimage.transform#skimage.transform.integrate", "type": "transform", "text": "\nUse an integral image to integrate over a given window.\n\nIntegral image.\n\nCoordinates of top left corner of window(s). Each tuple in the list contains\nthe starting row, col, \u2026 index i.e `[(row_win1, col_win1, \u2026), (row_win2,\ncol_win2,\u2026), \u2026]`.\n\nCoordinates of bottom right corner of window(s). Each tuple in the list\ncontaining the end row, col, \u2026 index i.e `[(row_win1, col_win1, \u2026), (row_win2,\ncol_win2, \u2026), \u2026]`.\n\nIntegral (sum) over the given window(s).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.iradon()", "path": "api/skimage.transform#skimage.transform.iradon", "type": "transform", "text": "\nInverse radon transform.\n\nReconstruct an image from the radon transform, using the filtered back\nprojection algorithm.\n\nImage containing radon transform (sinogram). Each column of the image\ncorresponds to a projection along a different angle. The tomography rotation\naxis should lie at the pixel index `radon_image.shape[0] // 2` along the 0th\ndimension of `radon_image`.\n\nReconstruction angles (in degrees). Default: m angles evenly spaced between 0\nand 180 (if the shape of `radon_image` is (N, M)).\n\nNumber of rows and columns in the reconstruction.\n\nFilter used in frequency domain filtering. Ramp filter used by default.\nFilters available: ramp, shepp-logan, cosine, hamming, hann. Assign None to\nuse no filter.\n\nInterpolation method used in reconstruction. Methods available: \u2018linear\u2019,\n\u2018nearest\u2019, and \u2018cubic\u2019 (\u2018cubic\u2019 is slow).\n\nAssume the reconstructed image is zero outside the inscribed circle. Also\nchanges the default output_size to match the behaviour of `radon` called with\n`circle=True`.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nReconstructed image. The rotation axis will be located in the pixel with\nindices `(reconstructed.shape[0] // 2, reconstructed.shape[1] // 2)`.\n\nChanged in version 0.19: In `iradon`, `filter` argument is deprecated in favor\nof `filter_name`.\n\nIt applies the Fourier slice theorem to reconstruct an image by multiplying\nthe frequency domain of the filter with the FFT of the projection data. This\nalgorithm is called filtered back projection.\n\nAC Kak, M Slaney, \u201cPrinciples of Computerized Tomographic Imaging\u201d, IEEE Press\n1988.\n\nB.R. Ramesh, N. Srinivasa, K. Rajgopal, \u201cAn Algorithm for Computing the\nDiscrete Radon Transform With Some Applications\u201d, Proceedings of the Fourth\nIEEE Region 10 International Conference, TENCON \u201889, 1989\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.iradon_sart()", "path": "api/skimage.transform#skimage.transform.iradon_sart", "type": "transform", "text": "\nInverse radon transform.\n\nReconstruct an image from the radon transform, using a single iteration of the\nSimultaneous Algebraic Reconstruction Technique (SART) algorithm.\n\nImage containing radon transform (sinogram). Each column of the image\ncorresponds to a projection along a different angle. The tomography rotation\naxis should lie at the pixel index `radon_image.shape[0] // 2` along the 0th\ndimension of `radon_image`.\n\nReconstruction angles (in degrees). Default: m angles evenly spaced between 0\nand 180 (if the shape of `radon_image` is (N, M)).\n\nImage containing an initial reconstruction estimate. Shape of this array\nshould be `(radon_image.shape[0], radon_image.shape[0])`. The default is an\narray of zeros.\n\nShift the projections contained in `radon_image` (the sinogram) by this many\npixels before reconstructing the image. The i\u2019th value defines the shift of\nthe i\u2019th column of `radon_image`.\n\nForce all values in the reconstructed tomogram to lie in the range `[clip[0],\nclip[1]]`\n\nRelaxation parameter for the update step. A higher value can improve the\nconvergence rate, but one runs the risk of instabilities. Values close to or\nhigher than 1 are not recommended.\n\nOutput data type, must be floating point. By default, if input data type is\nnot float, input is cast to double, otherwise dtype is set to input data type.\n\nReconstructed image. The rotation axis will be located in the pixel with\nindices `(reconstructed.shape[0] // 2, reconstructed.shape[1] // 2)`.\n\nAlgebraic Reconstruction Techniques are based on formulating the tomography\nreconstruction problem as a set of linear equations. Along each ray, the\nprojected value is the sum of all the values of the cross section along the\nray. A typical feature of SART (and a few other variants of algebraic\ntechniques) is that it samples the cross section at equidistant points along\nthe ray, using linear interpolation between the pixel values of the cross\nsection. The resulting set of linear equations are then solved using a\nslightly modified Kaczmarz method.\n\nWhen using SART, a single iteration is usually sufficient to obtain a good\nreconstruction. Further iterations will tend to enhance high-frequency\ninformation, but will also often increase the noise.\n\nAC Kak, M Slaney, \u201cPrinciples of Computerized Tomographic Imaging\u201d, IEEE Press\n1988.\n\nAH Andersen, AC Kak, \u201cSimultaneous algebraic reconstruction technique (SART):\na superior implementation of the ART algorithm\u201d, Ultrasonic Imaging 6 pp 81\u201394\n(1984)\n\nS Kaczmarz, \u201cAngen\u00e4herte aufl\u00f6sung von systemen linearer gleichungen\u201d,\nBulletin International de l\u2019Academie Polonaise des Sciences et des Lettres 35\npp 355\u2013357 (1937)\n\nKohler, T. \u201cA projection access scheme for iterative reconstruction based on\nthe golden section.\u201d Nuclear Science Symposium Conference Record, 2004 IEEE.\nVol. 6. IEEE, 2004.\n\nKaczmarz\u2019 method, Wikipedia, https://en.wikipedia.org/wiki/Kaczmarz_method\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.matrix_transform()", "path": "api/skimage.transform#skimage.transform.matrix_transform", "type": "transform", "text": "\nApply 2D matrix transform.\n\nx, y coordinates to transform\n\nHomogeneous transformation matrix.\n\nTransformed coordinates.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.order_angles_golden_ratio()", "path": "api/skimage.transform#skimage.transform.order_angles_golden_ratio", "type": "transform", "text": "\nOrder angles to reduce the amount of correlated information in subsequent\nprojections.\n\nProjection angles in degrees. Duplicate angles are not allowed.\n\nThe returned generator yields indices into `theta` such that `theta[indices]`\ngives the approximate golden ratio ordering of the projections. In total,\n`len(theta)` indices are yielded. All non-negative integers < `len(theta)` are\nyielded exactly once.\n\nThe method used here is that of the golden ratio introduced by T. Kohler.\n\nKohler, T. \u201cA projection access scheme for iterative reconstruction based on\nthe golden section.\u201d Nuclear Science Symposium Conference Record, 2004 IEEE.\nVol. 6. IEEE, 2004.\n\nWinkelmann, Stefanie, et al. \u201cAn optimal radial profile order based on the\nGolden Ratio for time-resolved MRI.\u201d Medical Imaging, IEEE Transactions on\n26.1 (2007): 68-76.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.PiecewiseAffineTransform", "path": "api/skimage.transform#skimage.transform.PiecewiseAffineTransform", "type": "transform", "text": "\nBases: `skimage.transform._geometric.GeometricTransform`\n\n2D piecewise affine transformation.\n\nControl points are used to define the mapping. The transform is based on a\nDelaunay triangulation of the points to form a mesh. Each triangle is used to\nfind a local affine transform.\n\nAffine transformations for each triangle in the mesh.\n\nInverse affine transformations for each triangle in the mesh.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nEstimate the transformation from a set of corresponding points.\n\nNumber of source and destination coordinates must match.\n\nSource coordinates.\n\nDestination coordinates.\n\nTrue, if model estimation succeeds.\n\nApply inverse transformation.\n\nCoordinates outside of the mesh will be set to `- 1`.\n\nSource coordinates.\n\nTransformed coordinates.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.PiecewiseAffineTransform.estimate()", "path": "api/skimage.transform#skimage.transform.PiecewiseAffineTransform.estimate", "type": "transform", "text": "\nEstimate the transformation from a set of corresponding points.\n\nNumber of source and destination coordinates must match.\n\nSource coordinates.\n\nDestination coordinates.\n\nTrue, if model estimation succeeds.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.PiecewiseAffineTransform.inverse()", "path": "api/skimage.transform#skimage.transform.PiecewiseAffineTransform.inverse", "type": "transform", "text": "\nApply inverse transformation.\n\nCoordinates outside of the mesh will be set to `- 1`.\n\nSource coordinates.\n\nTransformed coordinates.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.PiecewiseAffineTransform.__init__()", "path": "api/skimage.transform#skimage.transform.PiecewiseAffineTransform.__init__", "type": "transform", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.PolynomialTransform", "path": "api/skimage.transform#skimage.transform.PolynomialTransform", "type": "transform", "text": "\nBases: `skimage.transform._geometric.GeometricTransform`\n\n2D polynomial transformation.\n\nHas the following form:\n\nPolynomial coefficients where `N * 2 = (order + 1) * (order + 2)`. So, a_ji is\ndefined in `params[0, :]` and b_ji in `params[1, :]`.\n\nPolynomial coefficients where `N * 2 = (order + 1) * (order + 2)`. So, a_ji is\ndefined in `params[0, :]` and b_ji in `params[1, :]`.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nEstimate the transformation from a set of corresponding points.\n\nYou can determine the over-, well- and under-determined parameters with the\ntotal least-squares method.\n\nNumber of source and destination coordinates must match.\n\nThe transformation is defined as:\n\nThese equations can be transformed to the following form:\n\nwhich exist for each set of corresponding points, so we have a set of N * 2\nequations. The coefficients appear linearly so we can write A x = 0, where:\n\nIn case of total least-squares the solution of this homogeneous system of\nequations is the right singular vector of A which corresponds to the smallest\nsingular value normed by the coefficient c3.\n\nSource coordinates.\n\nDestination coordinates.\n\nPolynomial order (number of coefficients is order + 1).\n\nTrue, if model estimation succeeds.\n\nApply inverse transformation.\n\nDestination coordinates.\n\nSource coordinates.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.PolynomialTransform.estimate()", "path": "api/skimage.transform#skimage.transform.PolynomialTransform.estimate", "type": "transform", "text": "\nEstimate the transformation from a set of corresponding points.\n\nYou can determine the over-, well- and under-determined parameters with the\ntotal least-squares method.\n\nNumber of source and destination coordinates must match.\n\nThe transformation is defined as:\n\nThese equations can be transformed to the following form:\n\nwhich exist for each set of corresponding points, so we have a set of N * 2\nequations. The coefficients appear linearly so we can write A x = 0, where:\n\nIn case of total least-squares the solution of this homogeneous system of\nequations is the right singular vector of A which corresponds to the smallest\nsingular value normed by the coefficient c3.\n\nSource coordinates.\n\nDestination coordinates.\n\nPolynomial order (number of coefficients is order + 1).\n\nTrue, if model estimation succeeds.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.PolynomialTransform.inverse()", "path": "api/skimage.transform#skimage.transform.PolynomialTransform.inverse", "type": "transform", "text": "\nApply inverse transformation.\n\nDestination coordinates.\n\nSource coordinates.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.PolynomialTransform.__init__()", "path": "api/skimage.transform#skimage.transform.PolynomialTransform.__init__", "type": "transform", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.probabilistic_hough_line()", "path": "api/skimage.transform#skimage.transform.probabilistic_hough_line", "type": "transform", "text": "\nReturn lines from a progressive probabilistic line Hough transform.\n\nInput image with nonzero values representing edges.\n\nThreshold\n\nMinimum accepted length of detected lines. Increase the parameter to extract\nlonger lines.\n\nMaximum gap between pixels to still form a line. Increase the parameter to\nmerge broken lines more aggressively.\n\nAngles at which to compute the transform, in radians. If None, use a range\nfrom -pi/2 to pi/2.\n\nSeed to initialize the random number generator.\n\nList of lines identified, lines in format ((x0, y0), (x1, y1)), indicating\nline start and end.\n\nC. Galamhos, J. Matas and J. Kittler, \u201cProgressive probabilistic Hough\ntransform for line detection\u201d, in IEEE Computer Society Conference on Computer\nVision and Pattern Recognition, 1999.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.ProjectiveTransform", "path": "api/skimage.transform#skimage.transform.ProjectiveTransform", "type": "transform", "text": "\nBases: `skimage.transform._geometric.GeometricTransform`\n\nProjective transformation.\n\nApply a projective transformation (homography) on coordinates.\n\nFor each homogeneous coordinate \\\\(\\mathbf{x} = [x, y, 1]^T\\\\), its target\nposition is calculated by multiplying with the given matrix, \\\\(H\\\\), to give\n\\\\(H \\mathbf{x}\\\\):\n\nE.g., to rotate by theta degrees clockwise, the matrix should be:\n\nor, to translate x by 10 and y by 20:\n\nHomogeneous transformation matrix.\n\nHomogeneous transformation matrix.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nEstimate the transformation from a set of corresponding points.\n\nYou can determine the over-, well- and under-determined parameters with the\ntotal least-squares method.\n\nNumber of source and destination coordinates must match.\n\nThe transformation is defined as:\n\nThese equations can be transformed to the following form:\n\nwhich exist for each set of corresponding points, so we have a set of N * 2\nequations. The coefficients appear linearly so we can write A x = 0, where:\n\nIn case of total least-squares the solution of this homogeneous system of\nequations is the right singular vector of A which corresponds to the smallest\nsingular value normed by the coefficient c3.\n\nIn case of the affine transformation the coefficients c0 and c1 are 0. Thus\nthe system of equations is:\n\nSource coordinates.\n\nDestination coordinates.\n\nTrue, if model estimation succeeds.\n\nApply inverse transformation.\n\nDestination coordinates.\n\nSource coordinates.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.ProjectiveTransform.estimate()", "path": "api/skimage.transform#skimage.transform.ProjectiveTransform.estimate", "type": "transform", "text": "\nEstimate the transformation from a set of corresponding points.\n\nYou can determine the over-, well- and under-determined parameters with the\ntotal least-squares method.\n\nNumber of source and destination coordinates must match.\n\nThe transformation is defined as:\n\nThese equations can be transformed to the following form:\n\nwhich exist for each set of corresponding points, so we have a set of N * 2\nequations. The coefficients appear linearly so we can write A x = 0, where:\n\nIn case of total least-squares the solution of this homogeneous system of\nequations is the right singular vector of A which corresponds to the smallest\nsingular value normed by the coefficient c3.\n\nIn case of the affine transformation the coefficients c0 and c1 are 0. Thus\nthe system of equations is:\n\nSource coordinates.\n\nDestination coordinates.\n\nTrue, if model estimation succeeds.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.ProjectiveTransform.inverse()", "path": "api/skimage.transform#skimage.transform.ProjectiveTransform.inverse", "type": "transform", "text": "\nApply inverse transformation.\n\nDestination coordinates.\n\nSource coordinates.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.ProjectiveTransform.__init__()", "path": "api/skimage.transform#skimage.transform.ProjectiveTransform.__init__", "type": "transform", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.pyramid_expand()", "path": "api/skimage.transform#skimage.transform.pyramid_expand", "type": "transform", "text": "\nUpsample and then smooth image.\n\nInput image.\n\nUpscale factor.\n\nSigma for Gaussian filter. Default is `2 * upscale / 6.0` which corresponds to\na filter mask twice the size of the scale factor that covers more than 99% of\nthe Gaussian distribution.\n\nOrder of splines used in interpolation of upsampling. See\n`skimage.transform.warp` for detail.\n\nThe mode parameter determines how the array borders are handled, where cval is\nthe value when mode is equal to \u2018constant\u2019.\n\nValue to fill past edges of input if mode is \u2018constant\u2019.\n\nWhether the last axis of the image is to be interpreted as multiple channels\nor another spatial dimension.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nUpsampled and smoothed float image.\n\nhttp://persci.mit.edu/pub_pdfs/pyramid83.pdf\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.pyramid_gaussian()", "path": "api/skimage.transform#skimage.transform.pyramid_gaussian", "type": "transform", "text": "\nYield images of the Gaussian pyramid formed by the input image.\n\nRecursively applies the `pyramid_reduce` function to the image, and yields the\ndownscaled images.\n\nNote that the first image of the pyramid will be the original, unscaled image.\nThe total number of images is `max_layer + 1`. In case all layers are\ncomputed, the last image is either a one-pixel image or the image where the\nreduction does not change its shape.\n\nInput image.\n\nNumber of layers for the pyramid. 0th layer is the original image. Default is\n-1 which builds all possible layers.\n\nDownscale factor.\n\nSigma for Gaussian filter. Default is `2 * downscale / 6.0` which corresponds\nto a filter mask twice the size of the scale factor that covers more than 99%\nof the Gaussian distribution.\n\nOrder of splines used in interpolation of downsampling. See\n`skimage.transform.warp` for detail.\n\nThe mode parameter determines how the array borders are handled, where cval is\nthe value when mode is equal to \u2018constant\u2019.\n\nValue to fill past edges of input if mode is \u2018constant\u2019.\n\nWhether the last axis of the image is to be interpreted as multiple channels\nor another spatial dimension.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nGenerator yielding pyramid layers as float images.\n\nhttp://persci.mit.edu/pub_pdfs/pyramid83.pdf\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.pyramid_laplacian()", "path": "api/skimage.transform#skimage.transform.pyramid_laplacian", "type": "transform", "text": "\nYield images of the laplacian pyramid formed by the input image.\n\nEach layer contains the difference between the downsampled and the\ndownsampled, smoothed image:\n\nNote that the first image of the pyramid will be the difference between the\noriginal, unscaled image and its smoothed version. The total number of images\nis `max_layer + 1`. In case all layers are computed, the last image is either\na one-pixel image or the image where the reduction does not change its shape.\n\nInput image.\n\nNumber of layers for the pyramid. 0th layer is the original image. Default is\n-1 which builds all possible layers.\n\nDownscale factor.\n\nSigma for Gaussian filter. Default is `2 * downscale / 6.0` which corresponds\nto a filter mask twice the size of the scale factor that covers more than 99%\nof the Gaussian distribution.\n\nOrder of splines used in interpolation of downsampling. See\n`skimage.transform.warp` for detail.\n\nThe mode parameter determines how the array borders are handled, where cval is\nthe value when mode is equal to \u2018constant\u2019.\n\nValue to fill past edges of input if mode is \u2018constant\u2019.\n\nWhether the last axis of the image is to be interpreted as multiple channels\nor another spatial dimension.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nGenerator yielding pyramid layers as float images.\n\nhttp://persci.mit.edu/pub_pdfs/pyramid83.pdf\n\nhttp://sepwww.stanford.edu/data/media/public/sep/morgan/texturematch/paper_html/node3.html\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.pyramid_reduce()", "path": "api/skimage.transform#skimage.transform.pyramid_reduce", "type": "transform", "text": "\nSmooth and then downsample image.\n\nInput image.\n\nDownscale factor.\n\nSigma for Gaussian filter. Default is `2 * downscale / 6.0` which corresponds\nto a filter mask twice the size of the scale factor that covers more than 99%\nof the Gaussian distribution.\n\nOrder of splines used in interpolation of downsampling. See\n`skimage.transform.warp` for detail.\n\nThe mode parameter determines how the array borders are handled, where cval is\nthe value when mode is equal to \u2018constant\u2019.\n\nValue to fill past edges of input if mode is \u2018constant\u2019.\n\nWhether the last axis of the image is to be interpreted as multiple channels\nor another spatial dimension.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nSmoothed and downsampled float image.\n\nhttp://persci.mit.edu/pub_pdfs/pyramid83.pdf\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.radon()", "path": "api/skimage.transform#skimage.transform.radon", "type": "transform", "text": "\nCalculates the radon transform of an image given specified projection angles.\n\nInput image. The rotation axis will be located in the pixel with indices\n`(image.shape[0] // 2, image.shape[1] // 2)`.\n\nProjection angles (in degrees). If `None`, the value is set to np.arange(180).\n\nAssume image is zero outside the inscribed circle, making the width of each\nprojection (the first dimension of the sinogram) equal to `min(image.shape)`.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nRadon transform (sinogram). The tomography rotation axis will lie at the pixel\nindex `radon_image.shape[0] // 2` along the 0th dimension of `radon_image`.\n\nBased on code of Justin K. Romberg\n(https://www.clear.rice.edu/elec431/projects96/DSP/bpanalysis.html)\n\nAC Kak, M Slaney, \u201cPrinciples of Computerized Tomographic Imaging\u201d, IEEE Press\n1988.\n\nB.R. Ramesh, N. Srinivasa, K. Rajgopal, \u201cAn Algorithm for Computing the\nDiscrete Radon Transform With Some Applications\u201d, Proceedings of the Fourth\nIEEE Region 10 International Conference, TENCON \u201889, 1989\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.rescale()", "path": "api/skimage.transform#skimage.transform.rescale", "type": "transform", "text": "\nScale image by a certain factor.\n\nPerforms interpolation to up-scale or down-scale N-dimensional images. Note\nthat anti-aliasing should be enabled when down-sizing images to avoid aliasing\nartifacts. For down-sampling with an integer factor also see\n`skimage.transform.downscale_local_mean`.\n\nInput image.\n\nScale factors. Separate scale factors can be defined as `(rows, cols[, \u2026][,\ndim])`.\n\nScaled version of the input.\n\nThe order of the spline interpolation, default is 0 if image.dtype is bool and\n1 otherwise. The order has to be in the range 0-5. See\n`skimage.transform.warp` for detail.\n\nPoints outside the boundaries of the input are filled according to the given\nmode. Modes match the behaviour of `numpy.pad`.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nWhether to clip the output to the range of values of the input image. This is\nenabled by default, since higher order interpolation may produce values\noutside the given input range.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nWhether the last axis of the image is to be interpreted as multiple channels\nor another spatial dimension.\n\nWhether to apply a Gaussian filter to smooth the image prior to down-scaling.\nIt is crucial to filter when down-sampling the image to avoid aliasing\nartifacts. If input image data type is bool, no anti-aliasing is applied.\n\nStandard deviation for Gaussian filtering to avoid aliasing artifacts. By\ndefault, this value is chosen as (s - 1) / 2 where s is the down-scaling\nfactor.\n\nModes \u2018reflect\u2019 and \u2018symmetric\u2019 are similar, but differ in whether the edge\npixels are duplicated during the reflection. As an example, if an array has\nvalues [0, 1, 2] and was padded to the right by four values using symmetric,\nthe result would be [0, 1, 2, 2, 1, 0, 0], while for reflect it would be [0,\n1, 2, 1, 0, 1, 2].\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.resize()", "path": "api/skimage.transform#skimage.transform.resize", "type": "transform", "text": "\nResize image to match a certain size.\n\nPerforms interpolation to up-size or down-size N-dimensional images. Note that\nanti-aliasing should be enabled when down-sizing images to avoid aliasing\nartifacts. For down-sampling with an integer factor also see\n`skimage.transform.downscale_local_mean`.\n\nInput image.\n\nSize of the generated output image `(rows, cols[, \u2026][, dim])`. If `dim` is not\nprovided, the number of channels is preserved. In case the number of input\nchannels does not equal the number of output channels a n-dimensional\ninterpolation is applied.\n\nResized version of the input.\n\nThe order of the spline interpolation, default is 0 if image.dtype is bool and\n1 otherwise. The order has to be in the range 0-5. See\n`skimage.transform.warp` for detail.\n\nPoints outside the boundaries of the input are filled according to the given\nmode. Modes match the behaviour of `numpy.pad`.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nWhether to clip the output to the range of values of the input image. This is\nenabled by default, since higher order interpolation may produce values\noutside the given input range.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nWhether to apply a Gaussian filter to smooth the image prior to down-scaling.\nIt is crucial to filter when down-sampling the image to avoid aliasing\nartifacts. If input image data type is bool, no anti-aliasing is applied.\n\nStandard deviation for Gaussian filtering to avoid aliasing artifacts. By\ndefault, this value is chosen as (s - 1) / 2 where s is the down-scaling\nfactor, where s > 1\\. For the up-size case, s < 1, no anti-aliasing is\nperformed prior to rescaling.\n\nModes \u2018reflect\u2019 and \u2018symmetric\u2019 are similar, but differ in whether the edge\npixels are duplicated during the reflection. As an example, if an array has\nvalues [0, 1, 2] and was padded to the right by four values using symmetric,\nthe result would be [0, 1, 2, 2, 1, 0, 0], while for reflect it would be [0,\n1, 2, 1, 0, 1, 2].\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.rotate()", "path": "api/skimage.transform#skimage.transform.rotate", "type": "transform", "text": "\nRotate image by a certain angle around its center.\n\nInput image.\n\nRotation angle in degrees in counter-clockwise direction.\n\nDetermine whether the shape of the output image will be automatically\ncalculated, so the complete rotated image exactly fits. Default is False.\n\nThe rotation center. If `center=None`, the image is rotated around its center,\ni.e. `center=(cols / 2 - 0.5, rows / 2 - 0.5)`. Please note that this\nparameter is (cols, rows), contrary to normal skimage ordering.\n\nRotated version of the input.\n\nThe order of the spline interpolation, default is 0 if image.dtype is bool and\n1 otherwise. The order has to be in the range 0-5. See\n`skimage.transform.warp` for detail.\n\nPoints outside the boundaries of the input are filled according to the given\nmode. Modes match the behaviour of `numpy.pad`.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nWhether to clip the output to the range of values of the input image. This is\nenabled by default, since higher order interpolation may produce values\noutside the given input range.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nModes \u2018reflect\u2019 and \u2018symmetric\u2019 are similar, but differ in whether the edge\npixels are duplicated during the reflection. As an example, if an array has\nvalues [0, 1, 2] and was padded to the right by four values using symmetric,\nthe result would be [0, 1, 2, 2, 1, 0, 0], while for reflect it would be [0,\n1, 2, 1, 0, 1, 2].\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.SimilarityTransform", "path": "api/skimage.transform#skimage.transform.SimilarityTransform", "type": "transform", "text": "\nBases: `skimage.transform._geometric.EuclideanTransform`\n\n2D similarity transformation.\n\nHas the following form:\n\nwhere `s` is a scale factor and the homogeneous transformation matrix is:\n\nThe similarity transformation extends the Euclidean transformation with a\nsingle scaling factor in addition to the rotation and translation parameters.\n\nHomogeneous transformation matrix.\n\nScale factor.\n\nRotation angle in counter-clockwise direction as radians.\n\nx, y translation parameters.\n\nHomogeneous transformation matrix.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nEstimate the transformation from a set of corresponding points.\n\nYou can determine the over-, well- and under-determined parameters with the\ntotal least-squares method.\n\nNumber of source and destination coordinates must match.\n\nSource coordinates.\n\nDestination coordinates.\n\nTrue, if model estimation succeeds.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.SimilarityTransform.estimate()", "path": "api/skimage.transform#skimage.transform.SimilarityTransform.estimate", "type": "transform", "text": "\nEstimate the transformation from a set of corresponding points.\n\nYou can determine the over-, well- and under-determined parameters with the\ntotal least-squares method.\n\nNumber of source and destination coordinates must match.\n\nSource coordinates.\n\nDestination coordinates.\n\nTrue, if model estimation succeeds.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.SimilarityTransform.scale()", "path": "api/skimage.transform#skimage.transform.SimilarityTransform.scale", "type": "transform", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.SimilarityTransform.__init__()", "path": "api/skimage.transform#skimage.transform.SimilarityTransform.__init__", "type": "transform", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.swirl()", "path": "api/skimage.transform#skimage.transform.swirl", "type": "transform", "text": "\nPerform a swirl transformation.\n\nInput image.\n\nCenter coordinate of transformation.\n\nThe amount of swirling applied.\n\nThe extent of the swirl in pixels. The effect dies out rapidly beyond\n`radius`.\n\nAdditional rotation applied to the image.\n\nSwirled version of the input.\n\nShape of the output image generated. By default the shape of the input image\nis preserved.\n\nThe order of the spline interpolation, default is 0 if image.dtype is bool and\n1 otherwise. The order has to be in the range 0-5. See\n`skimage.transform.warp` for detail.\n\nPoints outside the boundaries of the input are filled according to the given\nmode, with \u2018constant\u2019 used as the default. Modes match the behaviour of\n`numpy.pad`.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nWhether to clip the output to the range of values of the input image. This is\nenabled by default, since higher order interpolation may produce values\noutside the given input range.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.warp()", "path": "api/skimage.transform#skimage.transform.warp", "type": "transform", "text": "\nWarp an image according to a given coordinate transformation.\n\nInput image.\n\nInverse coordinate map, which transforms coordinates in the output images into\ntheir corresponding coordinates in the input image.\n\nThere are a number of different options to define this map, depending on the\ndimensionality of the input image. A 2-D image can have 2 dimensions for gray-\nscale images, or 3 dimensions with color information.\n\nNote, that a `(3, 3)` matrix is interpreted as a homogeneous transformation\nmatrix, so you cannot interpolate values from a 3-D input, if the output is of\nshape `(3,)`.\n\nSee example section for usage.\n\nKeyword arguments passed to `inverse_map`.\n\nShape of the output image generated. By default the shape of the input image\nis preserved. Note that, even for multi-band images, only rows and columns\nneed to be specified.\n\nDefault is 0 if image.dtype is bool and 1 otherwise.\n\nPoints outside the boundaries of the input are filled according to the given\nmode. Modes match the behaviour of `numpy.pad`.\n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image\nboundaries.\n\nWhether to clip the output to the range of values of the input image. This is\nenabled by default, since higher order interpolation may produce values\noutside the given input range.\n\nWhether to keep the original range of values. Otherwise, the input image is\nconverted according to the conventions of `img_as_float`. Also see\nhttps://scikit-image.org/docs/dev/user_guide/data_types.html\n\nThe warped input image.\n\nThe following image warps are all equal but differ substantially in execution\ntime. The image is shifted to the bottom.\n\nUse a geometric transform to warp an image (fast):\n\nUse a callable (slow):\n\nUse a transformation matrix to warp an image (fast):\n\nYou can also use the inverse of a geometric transformation (fast):\n\nFor N-D images you can pass a coordinate array, that specifies the coordinates\nin the input image for every element in the output image. E.g. if you want to\nrescale a 3-D cube, you can do:\n\nSetup the coordinate array, that defines the scaling:\n\nAssume that the cube contains spatial data, where the first array element\ncenter is at coordinate (0.5, 0.5, 0.5) in real space, i.e. we have to account\nfor this extra offset when scaling the image:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.warp_coords()", "path": "api/skimage.transform#skimage.transform.warp_coords", "type": "transform", "text": "\nBuild the source coordinates for the output of a 2-D image warp.\n\nReturn input coordinates for given output coordinates. Coordinates are in the\nshape (P, 2), where P is the number of coordinates and each element is a\n`(row, col)` pair.\n\nShape of output image `(rows, cols[, bands])`.\n\ndtype for return value (sane choices: float32 or float64).\n\nCoordinates for `scipy.ndimage.map_coordinates`, that will yield an image of\nshape (orows, ocols, bands) by drawing from source points according to the\n`coord_transform_fn`.\n\nThis is a lower-level routine that produces the source coordinates for 2-D\nimages used by `warp()`.\n\nIt is provided separately from `warp` to give additional flexibility to users\nwho would like, for example, to re-use a particular coordinate mapping, to use\nspecific dtypes at various points along the the image-warping process, or to\nimplement different post-processing logic than `warp` performs after the call\nto `ndi.map_coordinates`.\n\nProduce a coordinate map that shifts an image up and to the right:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "transform.warp_polar()", "path": "api/skimage.transform#skimage.transform.warp_polar", "type": "transform", "text": "\nRemap image to polar or log-polar coordinates space.\n\nInput image. Only 2-D arrays are accepted by default. If `multichannel=True`,\n3-D arrays are accepted and the last axis is interpreted as multiple channels.\n\nPoint in image that represents the center of the transformation (i.e., the\norigin in cartesian space). Values can be of type `float`. If no value is\ngiven, the center is assumed to be the center point of the image.\n\nRadius of the circle that bounds the area to be transformed.\n\nSpecify whether the image warp is polar or log-polar. Defaults to \u2018linear\u2019.\n\nWhether the image is a 3-D array in which the third axis is to be interpreted\nas multiple channels. If set to `False` (default), only 2-D arrays are\naccepted.\n\nPassed to `transform.warp`.\n\nThe polar or log-polar warped image.\n\nPerform a basic polar warp on a grayscale image:\n\nPerform a log-polar warp on a grayscale image:\n\nPerform a log-polar warp on a grayscale image while specifying center, radius,\nand output shape:\n\nPerform a log-polar warp on a color image:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "Tutorials", "path": "user_guide/tutorials", "type": "Guide", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "User Guide", "path": "user_guide", "type": "Guide", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "util", "path": "api/skimage.util", "type": "util", "text": "\n`skimage.util.apply_parallel`(function, array)\n\nMap a function in parallel across an array.\n\n`skimage.util.compare_images`(image1, image2)\n\nReturn an image showing the differences between two images.\n\n`skimage.util.crop`(ar, crop_width[, copy, order])\n\nCrop array `ar` by `crop_width` along each dimension.\n\n`skimage.util.dtype_limits`(image[, clip_negative])\n\nReturn intensity limits, i.e.\n\n`skimage.util.img_as_bool`(image[, force_copy])\n\nConvert an image to boolean format.\n\n`skimage.util.img_as_float`(image[, force_copy])\n\nConvert an image to floating point format.\n\n`skimage.util.img_as_float32`(image[, force_copy])\n\nConvert an image to single-precision (32-bit) floating point format.\n\n`skimage.util.img_as_float64`(image[, force_copy])\n\nConvert an image to double-precision (64-bit) floating point format.\n\n`skimage.util.img_as_int`(image[, force_copy])\n\nConvert an image to 16-bit signed integer format.\n\n`skimage.util.img_as_ubyte`(image[, force_copy])\n\nConvert an image to 8-bit unsigned integer format.\n\n`skimage.util.img_as_uint`(image[, force_copy])\n\nConvert an image to 16-bit unsigned integer format.\n\n`skimage.util.invert`(image[, signed_float])\n\nInvert an image.\n\n`skimage.util.map_array`(input_arr, \u2026[, out])\n\nMap values from input array from input_vals to output_vals.\n\n`skimage.util.montage`(arr_in[, fill, \u2026])\n\nCreate a montage of several single- or multichannel images.\n\n`skimage.util.pad`(array, pad_width[, mode])\n\nPad an array.\n\n`skimage.util.random_noise`(image[, mode, \u2026])\n\nFunction to add random noise of various types to a floating-point image.\n\n`skimage.util.regular_grid`(ar_shape, n_points)\n\nFind `n_points` regularly spaced along `ar_shape`.\n\n`skimage.util.regular_seeds`(ar_shape, n_points)\n\nReturn an image with ~`n_points` regularly-spaced nonzero pixels.\n\n`skimage.util.unique_rows`(ar)\n\nRemove repeated rows from a 2D array.\n\n`skimage.util.view_as_blocks`(arr_in, block_shape)\n\nBlock view of the input n-dimensional array (using re-striding).\n\n`skimage.util.view_as_windows`(arr_in, \u2026[, step])\n\nRolling window view of the input n-dimensional array.\n\nMap a function in parallel across an array.\n\nSplit an array into possibly overlapping chunks of a given depth and boundary\ntype, call the given function in parallel on the chunks, combine the chunks\nand return the resulting array.\n\nFunction to be mapped which takes an array as an argument.\n\nArray which the function will be applied to.\n\nA single integer is interpreted as the length of one side of a square chunk\nthat should be tiled across the array. One tuple of length `array.ndim`\nrepresents the shape of a chunk, and it is tiled across the array. A list of\ntuples of length `ndim`, where each sub-tuple is a sequence of chunk sizes\nalong the corresponding dimension. If None, the array is broken up into chunks\nbased on the number of available cpus. More information about chunks is in the\ndocumentation here.\n\nInteger equal to the depth of the added boundary cells. Defaults to zero.\n\ntype of external boundary padding.\n\nTuple of arguments to be passed to the function.\n\nDictionary of keyword arguments to be passed to the function.\n\nThe data-type of the `function` output. If None, Dask will attempt to infer\nthis by calling the function on data of shape `(1,) * ndim`. For functions\nexpecting RGB or multichannel data this may be problematic. In such cases, the\nuser should manually specify this dtype argument instead.\n\nNew in version 0.18: `dtype` was added in 0.18.\n\nIf `chunks` is None and `multichannel` is True, this function will keep only a\nsingle chunk along the channels axis. When `depth` is specified as a scalar\nvalue, that depth will be applied only to the non-channels axes (a depth of 0\nwill be used along the channels axis). If the user manually specified both\n`chunks` and a `depth` tuple, then this argument will have no effect.\n\nNew in version 0.18: `multichannel` was added in 0.18.\n\nIf `True`, compute eagerly returning a NumPy Array. If `False`, compute lazily\nreturning a Dask Array. If `None` (default), compute based on array type\nprovided (eagerly for NumPy Arrays and lazily for Dask Arrays).\n\nReturns the result of the applying the operation. Type is dependent on the\n`compute` argument.\n\nNumpy edge modes \u2018symmetric\u2019, \u2018wrap\u2019, and \u2018edge\u2019 are converted to the\nequivalent `dask` boundary modes \u2018reflect\u2019, \u2018periodic\u2019 and \u2018nearest\u2019,\nrespectively. Setting `compute=False` can be useful for chaining later\noperations. For example region selection to preview a result or storing large\ndata to disk instead of loading in memory.\n\nReturn an image showing the differences between two images.\n\nNew in version 0.16.\n\nImages to process, must be of the same shape.\n\nMethod used for the comparison. Valid values are {\u2018diff\u2019, \u2018blend\u2019,\n\u2018checkerboard\u2019}. Details are provided in the note section.\n\nUsed only for the `checkerboard` method. Specifies the number of tiles (row,\ncolumn) to divide the image.\n\nImage showing the differences.\n\n`'diff'` computes the absolute difference between the two images. `'blend'`\ncomputes the mean value. `'checkerboard'` makes tiles of dimension `n_tiles`\nthat display alternatively the first and the second image.\n\nCrop array `ar` by `crop_width` along each dimension.\n\nInput array.\n\nNumber of values to remove from the edges of each axis. `((before_1,\nafter_1),` \u2026 `(before_N, after_N))` specifies unique crop widths at the start\nand end of each axis. `((before, after),) or (before, after)` specifies a\nfixed start and end crop for every axis. `(n,)` or `n` for integer `n` is a\nshortcut for before = after = `n` for all axes.\n\nIf `True`, ensure the returned array is a contiguous copy. Normally, a crop\noperation will return a discontiguous view of the underlying input array.\n\nIf `copy==True`, control the memory layout of the copy. See `np.copy`.\n\nThe cropped array. If `copy=False` (default), this is a sliced view of the\ninput array.\n\nReturn intensity limits, i.e. (min, max) tuple, of the image\u2019s dtype.\n\nInput image.\n\nIf True, clip the negative range (i.e. return 0 for min intensity) even if the\nimage dtype allows negative values.\n\nLower and upper intensity limits.\n\nConvert an image to boolean format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nThe upper half of the input dtype\u2019s positive range is True, and the lower half\nis False. All negative values (if present) are False.\n\nConvert an image to floating point format.\n\nThis function is similar to `img_as_float64`, but will not convert lower-\nprecision floating point arrays to `float64`.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nThe range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when\nconverting from unsigned or signed datatypes, respectively. If the input image\nhas a float type, intensity values are not modified and can be outside the\nranges [0.0, 1.0] or [-1.0, 1.0].\n\nConvert an image to single-precision (32-bit) floating point format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nThe range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when\nconverting from unsigned or signed datatypes, respectively. If the input image\nhas a float type, intensity values are not modified and can be outside the\nranges [0.0, 1.0] or [-1.0, 1.0].\n\nConvert an image to double-precision (64-bit) floating point format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nThe range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when\nconverting from unsigned or signed datatypes, respectively. If the input image\nhas a float type, intensity values are not modified and can be outside the\nranges [0.0, 1.0] or [-1.0, 1.0].\n\nConvert an image to 16-bit signed integer format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nThe values are scaled between -32768 and 32767. If the input data-type is\npositive-only (e.g., uint8), then the output image will still only have\npositive values.\n\nConvert an image to 8-bit unsigned integer format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nNegative input values will be clipped. Positive values are scaled between 0\nand 255.\n\nConvert an image to 16-bit unsigned integer format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nNegative input values will be clipped. Positive values are scaled between 0\nand 65535.\n\nInvert an image.\n\nInvert the intensity range of the input image, so that the dtype maximum is\nnow the dtype minimum, and vice-versa. This operation is slightly different\ndepending on the input dtype:\n\nSee the examples for clarification.\n\nInput image.\n\nIf True and the image is of type float, the range is assumed to be [-1, 1]. If\nFalse and the image is of type float, the range is assumed to be [0, 1].\n\nInverted image.\n\nIdeally, for signed integers we would simply multiply by -1. However, signed\ninteger ranges are asymmetric. For example, for np.int8, the range of possible\nvalues is [-128, 127], so that -128 * -1 equals -128! By subtracting from -1,\nwe correctly map the maximum dtype value to the minimum.\n\nUse rolling-ball algorithm for estimating background intensity\n\nMap values from input array from input_vals to output_vals.\n\nThe input label image.\n\nThe values to map from.\n\nThe values to map to.\n\nThe output array. Will be created if not provided. It should have the same\ndtype as `output_vals`.\n\nThe array of mapped values.\n\nCreate a montage of several single- or multichannel images.\n\nCreate a rectangular montage from an input array representing an ensemble of\nequally shaped single- (gray) or multichannel (color) images.\n\nFor example, `montage(arr_in)` called with the following `arr_in`\n\n1\n\n2\n\n3\n\nwill return\n\n1\n\n2\n\n3\n\nwhere the \u2018*\u2019 patch will be determined by the `fill` parameter.\n\nAn array representing an ensemble of `K` images of equal shape.\n\nValue to fill the padding areas and/or the extra tiles in the output array.\nHas to be `float` for single channel collections. For multichannel collections\nhas to be an array-like of shape of number of channels. If `mean`, uses the\nmean value over all images.\n\nWhether to rescale the intensity of each image to [0, 1].\n\nThe desired grid shape for the montage `(ntiles_row, ntiles_column)`. The\ndefault aspect ratio is square.\n\nThe size of the spacing between the tiles and between the tiles and the\nborders. If non-zero, makes the boundaries of individual images easier to\nperceive.\n\nIf True, the last `arr_in` dimension is threated as a color channel, otherwise\nas spatial.\n\nOutput array with input images glued together (including padding `p`).\n\nPad an array.\n\nThe array to pad.\n\nNumber of values padded to the edges of each axis. ((before_1, after_1), \u2026\n(before_N, after_N)) unique pad widths for each axis. ((before, after),)\nyields same before and after pad for each axis. (pad,) or int is a shortcut\nfor before = after = pad width for all axes.\n\nOne of the following string values or a user supplied function.\n\nPads with a constant value.\n\nPads with the edge values of array.\n\nPads with the linear ramp between end_value and the array edge value.\n\nPads with the maximum value of all or part of the vector along each axis.\n\nPads with the mean value of all or part of the vector along each axis.\n\nPads with the median value of all or part of the vector along each axis.\n\nPads with the minimum value of all or part of the vector along each axis.\n\nPads with the reflection of the vector mirrored on the first and last values\nof the vector along each axis.\n\nPads with the reflection of the vector mirrored along the edge of the array.\n\nPads with the wrap of the vector along the axis. The first values are used to\npad the end and the end values are used to pad the beginning.\n\nPads with undefined values.\n\nNew in version 1.17.\n\nPadding function, see Notes.\n\nUsed in \u2018maximum\u2019, \u2018mean\u2019, \u2018median\u2019, and \u2018minimum\u2019. Number of values at edge\nof each axis used to calculate the statistic value.\n\n((before_1, after_1), \u2026 (before_N, after_N)) unique statistic lengths for each\naxis.\n\n((before, after),) yields same before and after statistic lengths for each\naxis.\n\n(stat_length,) or int is a shortcut for before = after = statistic length for\nall axes.\n\nDefault is `None`, to use the entire axis.\n\nUsed in \u2018constant\u2019. The values to set the padded values for each axis.\n\n`((before_1, after_1), ... (before_N, after_N))` unique pad constants for each\naxis.\n\n`((before, after),)` yields same before and after constants for each axis.\n\n`(constant,)` or `constant` is a shortcut for `before = after = constant` for\nall axes.\n\nDefault is 0.\n\nUsed in \u2018linear_ramp\u2019. The values used for the ending value of the linear_ramp\nand that will form the edge of the padded array.\n\n`((before_1, after_1), ... (before_N, after_N))` unique end values for each\naxis.\n\n`((before, after),)` yields same before and after end values for each axis.\n\n`(constant,)` or `constant` is a shortcut for `before = after = constant` for\nall axes.\n\nDefault is 0.\n\nUsed in \u2018reflect\u2019, and \u2018symmetric\u2019. The \u2018even\u2019 style is the default with an\nunaltered reflection around the edge value. For the \u2018odd\u2019 style, the extended\npart of the array is created by subtracting the reflected values from two\ntimes the edge value.\n\nPadded array of rank equal to `array` with shape increased according to\n`pad_width`.\n\nNew in version 1.7.0.\n\nFor an array with rank greater than 1, some of the padding of later axes is\ncalculated from padding of previous axes. This is easiest to think about with\na rank 2 array where the corners of the padded array are calculated by using\npadded values from the first axis.\n\nThe padding function, if used, should modify a rank 1 array in-place. It has\nthe following signature:\n\nwhere\n\nA rank 1 array already padded with zeros. Padded values are\nvector[:iaxis_pad_width[0]] and vector[-iaxis_pad_width[1]:].\n\nA 2-tuple of ints, iaxis_pad_width[0] represents the number of values padded\nat the beginning of vector where iaxis_pad_width[1] represents the number of\nvalues padded at the end of vector.\n\nThe axis currently being calculated.\n\nAny keyword arguments the function requires.\n\nFunction to add random noise of various types to a floating-point image.\n\nInput image data. Will be converted to float.\n\nOne of the following strings, selecting the type of noise to add:\n\nlocal variance at each point of `image`.\n\n-1 (for signed images).\n`low_val` is 0 for unsigned images or -1 for signed images.\n\nn is Gaussian noise with specified mean & variance.\n\nIf provided, this will set the random seed before generating noise, for valid\npseudo-random comparisons.\n\nIf True (default), the output will be clipped after noise applied for modes\n`\u2018speckle\u2019`, `\u2018poisson\u2019`, and `\u2018gaussian\u2019`. This is needed to maintain the\nproper image data range. If False, clipping is not applied, and the output may\nextend beyond the range [-1, 1].\n\nMean of random distribution. Used in \u2018gaussian\u2019 and \u2018speckle\u2019. Default : 0.\n\nVariance of random distribution. Used in \u2018gaussian\u2019 and \u2018speckle\u2019. Note:\nvariance = (standard deviation) ** 2. Default : 0.01\n\nArray of positive floats, same shape as `image`, defining the local variance\nat every image point. Used in \u2018localvar\u2019.\n\nProportion of image pixels to replace with noise on range [0, 1]. Used in\n\u2018salt\u2019, \u2018pepper\u2019, and \u2018salt & pepper\u2019. Default : 0.05\n\nProportion of salt vs. pepper noise for \u2018s&p\u2019 on range [0, 1]. Higher values\nrepresent more salt. Default : 0.5 (equal amounts)\n\nOutput floating-point image data on range [0, 1] or [-1, 1] if the input\n`image` was unsigned or signed, respectively.\n\nSpeckle, Poisson, Localvar, and Gaussian noise may generate noise outside the\nvalid image range. The default is to clip (not alias) these values, but they\nmay be preserved by setting `clip=False`. Note that in this case the output\nmay contain values outside the ranges [0, 1] or [-1, 1]. Use this option with\ncare.\n\nBecause of the prevalence of exclusively positive floating-point images in\nintermediate calculations, it is not possible to intuit if an input is signed\nbased on dtype alone. Instead, negative values are explicitly searched for.\nOnly if found does this function assume signed input. Unexpected results only\noccur in rare, poorly exposes cases (e.g. if all values are above 50 percent\ngray in a signed `image`). In this event, manually scaling the input to the\npositive domain will solve the problem.\n\nThe Poisson distribution is only defined for positive integers. To apply this\nnoise type, the number of unique values in the image is found and the next\nround power of two is used to scale up the floating-point result, after which\nit is scaled back down to the floating-point image range.\n\nTo generate Poisson noise against a signed image, the signed image is\ntemporarily converted to an unsigned image in the floating point domain,\nPoisson noise is generated, then it is returned to the original range.\n\nFind `n_points` regularly spaced along `ar_shape`.\n\nThe returned points (as slices) should be as close to cubically-spaced as\npossible. Essentially, the points are spaced by the Nth root of the input\narray size, where N is the number of dimensions. However, if an array\ndimension cannot fit a full step size, it is \u201cdiscarded\u201d, and the computation\nis done for only the remaining dimensions.\n\nThe shape of the space embedding the grid. `len(ar_shape)` is the number of\ndimensions.\n\nThe (approximate) number of points to embed in the space.\n\nA slice along each dimension of `ar_shape`, such that the intersection of all\nthe slices give the coordinates of regularly spaced points.\n\nChanged in version 0.14.1: In scikit-image 0.14.1 and 0.15, the return type\nwas changed from a list to a tuple to ensure compatibility with Numpy 1.15 and\nhigher. If your code requires the returned result to be a list, you may\nconvert the output of this function to a list with:\n\nReturn an image with ~`n_points` regularly-spaced nonzero pixels.\n\nThe shape of the desired output image.\n\nThe desired number of nonzero points.\n\nThe desired data type of the output.\n\nThe desired image.\n\nRemove repeated rows from a 2D array.\n\nIn particular, if given an array of coordinates of shape (Npoints, Ndim), it\nwill remove repeated points.\n\nThe input array.\n\nA copy of the input array with repeated rows removed.\n\nThe function will generate a copy of `ar` if it is not C-contiguous, which\nwill negatively affect performance for large input arrays.\n\nBlock view of the input n-dimensional array (using re-striding).\n\nBlocks are non-overlapping views of the input array.\n\nN-d input array.\n\nThe shape of the block. Each dimension must divide evenly into the\ncorresponding dimensions of `arr_in`.\n\nBlock view of the input array.\n\nRolling window view of the input n-dimensional array.\n\nWindows are overlapping views of the input array, with adjacent windows\nshifted by a single row or column (or an index of a higher dimension).\n\nN-d input array.\n\nDefines the shape of the elementary n-dimensional orthotope (better know as\nhyperrectangle [1]) of the rolling window view. If an integer is given, the\nshape will be a hypercube of sidelength given by its value.\n\nIndicates step size at which extraction shall be performed. If integer is\ngiven, then the step is uniform in all dimensions.\n\n(rolling) window view of the input array.\n\nOne should be very careful with rolling views when it comes to memory usage.\nIndeed, although a \u2018view\u2019 has the same memory footprint as its base array, the\nactual array that emerges when this \u2018view\u2019 is used in a computation is\ngenerally a (much) larger array than the original, especially for\n2-dimensional arrays and above.\n\nFor example, let us consider a 3 dimensional array of size (100, 100, 100) of\n`float64`. This array takes about 8*100**3 Bytes for storage which is just 8\nMB. If one decides to build a rolling view on this array with a window of (3,\n3, 3) the hypothetical size of the rolling view (if one was to reshape the\nview for example) would be 8*(100-3+1)**3*3**3 which is about 203 MB! The\nscaling becomes even worse as the dimension of the input array becomes larger.\n\nhttps://en.wikipedia.org/wiki/Hyperrectangle\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "util.apply_parallel()", "path": "api/skimage.util#skimage.util.apply_parallel", "type": "util", "text": "\nMap a function in parallel across an array.\n\nSplit an array into possibly overlapping chunks of a given depth and boundary\ntype, call the given function in parallel on the chunks, combine the chunks\nand return the resulting array.\n\nFunction to be mapped which takes an array as an argument.\n\nArray which the function will be applied to.\n\nA single integer is interpreted as the length of one side of a square chunk\nthat should be tiled across the array. One tuple of length `array.ndim`\nrepresents the shape of a chunk, and it is tiled across the array. A list of\ntuples of length `ndim`, where each sub-tuple is a sequence of chunk sizes\nalong the corresponding dimension. If None, the array is broken up into chunks\nbased on the number of available cpus. More information about chunks is in the\ndocumentation here.\n\nInteger equal to the depth of the added boundary cells. Defaults to zero.\n\ntype of external boundary padding.\n\nTuple of arguments to be passed to the function.\n\nDictionary of keyword arguments to be passed to the function.\n\nThe data-type of the `function` output. If None, Dask will attempt to infer\nthis by calling the function on data of shape `(1,) * ndim`. For functions\nexpecting RGB or multichannel data this may be problematic. In such cases, the\nuser should manually specify this dtype argument instead.\n\nNew in version 0.18: `dtype` was added in 0.18.\n\nIf `chunks` is None and `multichannel` is True, this function will keep only a\nsingle chunk along the channels axis. When `depth` is specified as a scalar\nvalue, that depth will be applied only to the non-channels axes (a depth of 0\nwill be used along the channels axis). If the user manually specified both\n`chunks` and a `depth` tuple, then this argument will have no effect.\n\nNew in version 0.18: `multichannel` was added in 0.18.\n\nIf `True`, compute eagerly returning a NumPy Array. If `False`, compute lazily\nreturning a Dask Array. If `None` (default), compute based on array type\nprovided (eagerly for NumPy Arrays and lazily for Dask Arrays).\n\nReturns the result of the applying the operation. Type is dependent on the\n`compute` argument.\n\nNumpy edge modes \u2018symmetric\u2019, \u2018wrap\u2019, and \u2018edge\u2019 are converted to the\nequivalent `dask` boundary modes \u2018reflect\u2019, \u2018periodic\u2019 and \u2018nearest\u2019,\nrespectively. Setting `compute=False` can be useful for chaining later\noperations. For example region selection to preview a result or storing large\ndata to disk instead of loading in memory.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "util.compare_images()", "path": "api/skimage.util#skimage.util.compare_images", "type": "util", "text": "\nReturn an image showing the differences between two images.\n\nNew in version 0.16.\n\nImages to process, must be of the same shape.\n\nMethod used for the comparison. Valid values are {\u2018diff\u2019, \u2018blend\u2019,\n\u2018checkerboard\u2019}. Details are provided in the note section.\n\nUsed only for the `checkerboard` method. Specifies the number of tiles (row,\ncolumn) to divide the image.\n\nImage showing the differences.\n\n`'diff'` computes the absolute difference between the two images. `'blend'`\ncomputes the mean value. `'checkerboard'` makes tiles of dimension `n_tiles`\nthat display alternatively the first and the second image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "util.crop()", "path": "api/skimage.util#skimage.util.crop", "type": "util", "text": "\nCrop array `ar` by `crop_width` along each dimension.\n\nInput array.\n\nNumber of values to remove from the edges of each axis. `((before_1,\nafter_1),` \u2026 `(before_N, after_N))` specifies unique crop widths at the start\nand end of each axis. `((before, after),) or (before, after)` specifies a\nfixed start and end crop for every axis. `(n,)` or `n` for integer `n` is a\nshortcut for before = after = `n` for all axes.\n\nIf `True`, ensure the returned array is a contiguous copy. Normally, a crop\noperation will return a discontiguous view of the underlying input array.\n\nIf `copy==True`, control the memory layout of the copy. See `np.copy`.\n\nThe cropped array. If `copy=False` (default), this is a sliced view of the\ninput array.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "util.dtype_limits()", "path": "api/skimage.util#skimage.util.dtype_limits", "type": "util", "text": "\nReturn intensity limits, i.e. (min, max) tuple, of the image\u2019s dtype.\n\nInput image.\n\nIf True, clip the negative range (i.e. return 0 for min intensity) even if the\nimage dtype allows negative values.\n\nLower and upper intensity limits.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "util.img_as_bool()", "path": "api/skimage.util#skimage.util.img_as_bool", "type": "util", "text": "\nConvert an image to boolean format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nThe upper half of the input dtype\u2019s positive range is True, and the lower half\nis False. All negative values (if present) are False.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "util.img_as_float()", "path": "api/skimage.util#skimage.util.img_as_float", "type": "util", "text": "\nConvert an image to floating point format.\n\nThis function is similar to `img_as_float64`, but will not convert lower-\nprecision floating point arrays to `float64`.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nThe range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when\nconverting from unsigned or signed datatypes, respectively. If the input image\nhas a float type, intensity values are not modified and can be outside the\nranges [0.0, 1.0] or [-1.0, 1.0].\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "util.img_as_float32()", "path": "api/skimage.util#skimage.util.img_as_float32", "type": "util", "text": "\nConvert an image to single-precision (32-bit) floating point format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nThe range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when\nconverting from unsigned or signed datatypes, respectively. If the input image\nhas a float type, intensity values are not modified and can be outside the\nranges [0.0, 1.0] or [-1.0, 1.0].\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "util.img_as_float64()", "path": "api/skimage.util#skimage.util.img_as_float64", "type": "util", "text": "\nConvert an image to double-precision (64-bit) floating point format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nThe range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when\nconverting from unsigned or signed datatypes, respectively. If the input image\nhas a float type, intensity values are not modified and can be outside the\nranges [0.0, 1.0] or [-1.0, 1.0].\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "util.img_as_int()", "path": "api/skimage.util#skimage.util.img_as_int", "type": "util", "text": "\nConvert an image to 16-bit signed integer format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nThe values are scaled between -32768 and 32767. If the input data-type is\npositive-only (e.g., uint8), then the output image will still only have\npositive values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "util.img_as_ubyte()", "path": "api/skimage.util#skimage.util.img_as_ubyte", "type": "util", "text": "\nConvert an image to 8-bit unsigned integer format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nNegative input values will be clipped. Positive values are scaled between 0\nand 255.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "util.img_as_uint()", "path": "api/skimage.util#skimage.util.img_as_uint", "type": "util", "text": "\nConvert an image to 16-bit unsigned integer format.\n\nInput image.\n\nForce a copy of the data, irrespective of its current dtype.\n\nOutput image.\n\nNegative input values will be clipped. Positive values are scaled between 0\nand 65535.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "util.invert()", "path": "api/skimage.util#skimage.util.invert", "type": "util", "text": "\nInvert an image.\n\nInvert the intensity range of the input image, so that the dtype maximum is\nnow the dtype minimum, and vice-versa. This operation is slightly different\ndepending on the input dtype:\n\nSee the examples for clarification.\n\nInput image.\n\nIf True and the image is of type float, the range is assumed to be [-1, 1]. If\nFalse and the image is of type float, the range is assumed to be [0, 1].\n\nInverted image.\n\nIdeally, for signed integers we would simply multiply by -1. However, signed\ninteger ranges are asymmetric. For example, for np.int8, the range of possible\nvalues is [-128, 127], so that -128 * -1 equals -128! By subtracting from -1,\nwe correctly map the maximum dtype value to the minimum.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "util.map_array()", "path": "api/skimage.util#skimage.util.map_array", "type": "util", "text": "\nMap values from input array from input_vals to output_vals.\n\nThe input label image.\n\nThe values to map from.\n\nThe values to map to.\n\nThe output array. Will be created if not provided. It should have the same\ndtype as `output_vals`.\n\nThe array of mapped values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "util.montage()", "path": "api/skimage.util#skimage.util.montage", "type": "util", "text": "\nCreate a montage of several single- or multichannel images.\n\nCreate a rectangular montage from an input array representing an ensemble of\nequally shaped single- (gray) or multichannel (color) images.\n\nFor example, `montage(arr_in)` called with the following `arr_in`\n\n1\n\n2\n\n3\n\nwill return\n\n1\n\n2\n\n3\n\nwhere the \u2018*\u2019 patch will be determined by the `fill` parameter.\n\nAn array representing an ensemble of `K` images of equal shape.\n\nValue to fill the padding areas and/or the extra tiles in the output array.\nHas to be `float` for single channel collections. For multichannel collections\nhas to be an array-like of shape of number of channels. If `mean`, uses the\nmean value over all images.\n\nWhether to rescale the intensity of each image to [0, 1].\n\nThe desired grid shape for the montage `(ntiles_row, ntiles_column)`. The\ndefault aspect ratio is square.\n\nThe size of the spacing between the tiles and between the tiles and the\nborders. If non-zero, makes the boundaries of individual images easier to\nperceive.\n\nIf True, the last `arr_in` dimension is threated as a color channel, otherwise\nas spatial.\n\nOutput array with input images glued together (including padding `p`).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "util.pad()", "path": "api/skimage.util#skimage.util.pad", "type": "util", "text": "\nPad an array.\n\nThe array to pad.\n\nNumber of values padded to the edges of each axis. ((before_1, after_1), \u2026\n(before_N, after_N)) unique pad widths for each axis. ((before, after),)\nyields same before and after pad for each axis. (pad,) or int is a shortcut\nfor before = after = pad width for all axes.\n\nOne of the following string values or a user supplied function.\n\nPads with a constant value.\n\nPads with the edge values of array.\n\nPads with the linear ramp between end_value and the array edge value.\n\nPads with the maximum value of all or part of the vector along each axis.\n\nPads with the mean value of all or part of the vector along each axis.\n\nPads with the median value of all or part of the vector along each axis.\n\nPads with the minimum value of all or part of the vector along each axis.\n\nPads with the reflection of the vector mirrored on the first and last values\nof the vector along each axis.\n\nPads with the reflection of the vector mirrored along the edge of the array.\n\nPads with the wrap of the vector along the axis. The first values are used to\npad the end and the end values are used to pad the beginning.\n\nPads with undefined values.\n\nNew in version 1.17.\n\nPadding function, see Notes.\n\nUsed in \u2018maximum\u2019, \u2018mean\u2019, \u2018median\u2019, and \u2018minimum\u2019. Number of values at edge\nof each axis used to calculate the statistic value.\n\n((before_1, after_1), \u2026 (before_N, after_N)) unique statistic lengths for each\naxis.\n\n((before, after),) yields same before and after statistic lengths for each\naxis.\n\n(stat_length,) or int is a shortcut for before = after = statistic length for\nall axes.\n\nDefault is `None`, to use the entire axis.\n\nUsed in \u2018constant\u2019. The values to set the padded values for each axis.\n\n`((before_1, after_1), ... (before_N, after_N))` unique pad constants for each\naxis.\n\n`((before, after),)` yields same before and after constants for each axis.\n\n`(constant,)` or `constant` is a shortcut for `before = after = constant` for\nall axes.\n\nDefault is 0.\n\nUsed in \u2018linear_ramp\u2019. The values used for the ending value of the linear_ramp\nand that will form the edge of the padded array.\n\n`((before_1, after_1), ... (before_N, after_N))` unique end values for each\naxis.\n\n`((before, after),)` yields same before and after end values for each axis.\n\n`(constant,)` or `constant` is a shortcut for `before = after = constant` for\nall axes.\n\nDefault is 0.\n\nUsed in \u2018reflect\u2019, and \u2018symmetric\u2019. The \u2018even\u2019 style is the default with an\nunaltered reflection around the edge value. For the \u2018odd\u2019 style, the extended\npart of the array is created by subtracting the reflected values from two\ntimes the edge value.\n\nPadded array of rank equal to `array` with shape increased according to\n`pad_width`.\n\nNew in version 1.7.0.\n\nFor an array with rank greater than 1, some of the padding of later axes is\ncalculated from padding of previous axes. This is easiest to think about with\na rank 2 array where the corners of the padded array are calculated by using\npadded values from the first axis.\n\nThe padding function, if used, should modify a rank 1 array in-place. It has\nthe following signature:\n\nwhere\n\nA rank 1 array already padded with zeros. Padded values are\nvector[:iaxis_pad_width[0]] and vector[-iaxis_pad_width[1]:].\n\nA 2-tuple of ints, iaxis_pad_width[0] represents the number of values padded\nat the beginning of vector where iaxis_pad_width[1] represents the number of\nvalues padded at the end of vector.\n\nThe axis currently being calculated.\n\nAny keyword arguments the function requires.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "util.random_noise()", "path": "api/skimage.util#skimage.util.random_noise", "type": "util", "text": "\nFunction to add random noise of various types to a floating-point image.\n\nInput image data. Will be converted to float.\n\nOne of the following strings, selecting the type of noise to add:\n\nlocal variance at each point of `image`.\n\n-1 (for signed images).\n`low_val` is 0 for unsigned images or -1 for signed images.\n\nn is Gaussian noise with specified mean & variance.\n\nIf provided, this will set the random seed before generating noise, for valid\npseudo-random comparisons.\n\nIf True (default), the output will be clipped after noise applied for modes\n`\u2018speckle\u2019`, `\u2018poisson\u2019`, and `\u2018gaussian\u2019`. This is needed to maintain the\nproper image data range. If False, clipping is not applied, and the output may\nextend beyond the range [-1, 1].\n\nMean of random distribution. Used in \u2018gaussian\u2019 and \u2018speckle\u2019. Default : 0.\n\nVariance of random distribution. Used in \u2018gaussian\u2019 and \u2018speckle\u2019. Note:\nvariance = (standard deviation) ** 2. Default : 0.01\n\nArray of positive floats, same shape as `image`, defining the local variance\nat every image point. Used in \u2018localvar\u2019.\n\nProportion of image pixels to replace with noise on range [0, 1]. Used in\n\u2018salt\u2019, \u2018pepper\u2019, and \u2018salt & pepper\u2019. Default : 0.05\n\nProportion of salt vs. pepper noise for \u2018s&p\u2019 on range [0, 1]. Higher values\nrepresent more salt. Default : 0.5 (equal amounts)\n\nOutput floating-point image data on range [0, 1] or [-1, 1] if the input\n`image` was unsigned or signed, respectively.\n\nSpeckle, Poisson, Localvar, and Gaussian noise may generate noise outside the\nvalid image range. The default is to clip (not alias) these values, but they\nmay be preserved by setting `clip=False`. Note that in this case the output\nmay contain values outside the ranges [0, 1] or [-1, 1]. Use this option with\ncare.\n\nBecause of the prevalence of exclusively positive floating-point images in\nintermediate calculations, it is not possible to intuit if an input is signed\nbased on dtype alone. Instead, negative values are explicitly searched for.\nOnly if found does this function assume signed input. Unexpected results only\noccur in rare, poorly exposes cases (e.g. if all values are above 50 percent\ngray in a signed `image`). In this event, manually scaling the input to the\npositive domain will solve the problem.\n\nThe Poisson distribution is only defined for positive integers. To apply this\nnoise type, the number of unique values in the image is found and the next\nround power of two is used to scale up the floating-point result, after which\nit is scaled back down to the floating-point image range.\n\nTo generate Poisson noise against a signed image, the signed image is\ntemporarily converted to an unsigned image in the floating point domain,\nPoisson noise is generated, then it is returned to the original range.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "util.regular_grid()", "path": "api/skimage.util#skimage.util.regular_grid", "type": "util", "text": "\nFind `n_points` regularly spaced along `ar_shape`.\n\nThe returned points (as slices) should be as close to cubically-spaced as\npossible. Essentially, the points are spaced by the Nth root of the input\narray size, where N is the number of dimensions. However, if an array\ndimension cannot fit a full step size, it is \u201cdiscarded\u201d, and the computation\nis done for only the remaining dimensions.\n\nThe shape of the space embedding the grid. `len(ar_shape)` is the number of\ndimensions.\n\nThe (approximate) number of points to embed in the space.\n\nA slice along each dimension of `ar_shape`, such that the intersection of all\nthe slices give the coordinates of regularly spaced points.\n\nChanged in version 0.14.1: In scikit-image 0.14.1 and 0.15, the return type\nwas changed from a list to a tuple to ensure compatibility with Numpy 1.15 and\nhigher. If your code requires the returned result to be a list, you may\nconvert the output of this function to a list with:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "util.regular_seeds()", "path": "api/skimage.util#skimage.util.regular_seeds", "type": "util", "text": "\nReturn an image with ~`n_points` regularly-spaced nonzero pixels.\n\nThe shape of the desired output image.\n\nThe desired number of nonzero points.\n\nThe desired data type of the output.\n\nThe desired image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "util.unique_rows()", "path": "api/skimage.util#skimage.util.unique_rows", "type": "util", "text": "\nRemove repeated rows from a 2D array.\n\nIn particular, if given an array of coordinates of shape (Npoints, Ndim), it\nwill remove repeated points.\n\nThe input array.\n\nA copy of the input array with repeated rows removed.\n\nThe function will generate a copy of `ar` if it is not C-contiguous, which\nwill negatively affect performance for large input arrays.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "util.view_as_blocks()", "path": "api/skimage.util#skimage.util.view_as_blocks", "type": "util", "text": "\nBlock view of the input n-dimensional array (using re-striding).\n\nBlocks are non-overlapping views of the input array.\n\nN-d input array.\n\nThe shape of the block. Each dimension must divide evenly into the\ncorresponding dimensions of `arr_in`.\n\nBlock view of the input array.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "util.view_as_windows()", "path": "api/skimage.util#skimage.util.view_as_windows", "type": "util", "text": "\nRolling window view of the input n-dimensional array.\n\nWindows are overlapping views of the input array, with adjacent windows\nshifted by a single row or column (or an index of a higher dimension).\n\nN-d input array.\n\nDefines the shape of the elementary n-dimensional orthotope (better know as\nhyperrectangle [1]) of the rolling window view. If an integer is given, the\nshape will be a hypercube of sidelength given by its value.\n\nIndicates step size at which extraction shall be performed. If integer is\ngiven, then the step is uniform in all dimensions.\n\n(rolling) window view of the input array.\n\nOne should be very careful with rolling views when it comes to memory usage.\nIndeed, although a \u2018view\u2019 has the same memory footprint as its base array, the\nactual array that emerges when this \u2018view\u2019 is used in a computation is\ngenerally a (much) larger array than the original, especially for\n2-dimensional arrays and above.\n\nFor example, let us consider a 3 dimensional array of size (100, 100, 100) of\n`float64`. This array takes about 8*100**3 Bytes for storage which is just 8\nMB. If one decides to build a rolling view on this array with a window of (3,\n3, 3) the hypothetical size of the rolling view (if one was to reshape the\nview for example) would be 8*(100-3+1)**3*3**3 which is about 203 MB! The\nscaling becomes even worse as the dimension of the input array becomes larger.\n\nhttps://en.wikipedia.org/wiki/Hyperrectangle\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer", "path": "api/skimage.viewer", "type": "viewer", "text": "\n`skimage.viewer.CollectionViewer`(image_collection)\n\nViewer for displaying image collections.\n\n`skimage.viewer.ImageViewer`(image[, useblit])\n\nViewer for displaying images.\n\n`skimage.viewer.canvastools`\n\n`skimage.viewer.plugins`\n\n`skimage.viewer.qt`\n\n`skimage.viewer.utils`\n\n`skimage.viewer.viewers`\n\n`skimage.viewer.widgets`\n\nWidgets for interacting with ImageViewer.\n\nBases: `skimage.viewer.viewers.core.ImageViewer`\n\nViewer for displaying image collections.\n\nSelect the displayed frame of the image collection using the slider or with\nthe following keyboard shortcuts:\n\nPrevious/next image in collection.\n\n0% to 90% of collection. For example, \u201c5\u201d goes to the image in the middle\n(i.e. 50%) of the collection.\n\nFirst/last image in collection.\n\nList of images to be displayed.\n\nControl whether image is updated on slide or release of the image slider.\nUsing \u2018on_release\u2019 will give smoother behavior when displaying large images or\nwhen writing a plugin/subclass that requires heavy computation.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nSelect image on display using index into image collection.\n\nBases: `object`\n\nViewer for displaying images.\n\nThis viewer is a simple container object that holds a Matplotlib axes for\nshowing images. `ImageViewer` doesn\u2019t subclass the Matplotlib axes (or figure)\nbecause of the high probability of name collisions.\n\nSubclasses and plugins will likely extend the `update_image` method to add\ncustom overlays or filter the displayed image.\n\nImage being viewed.\n\nMatplotlib canvas, figure, and axes used to display image.\n\nImage being viewed. Setting this value will update the displayed frame.\n\nPlugins typically operate on (but don\u2019t change) the original image.\n\nList of attached plugins.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nConnect callback function to matplotlib event and return id.\n\nDisconnect callback by its id (returned by `connect_event`).\n\nOpen image file and display in viewer.\n\nSave current image to file.\n\nThe current behavior is not ideal: It saves the image displayed on screen, so\nall images will be converted to RGB, and the image size is not preserved\n(resizing the viewer window will alter the size of the saved image).\n\nShow ImageViewer and attached plugins.\n\nThis behaves much like `matplotlib.pyplot.show` and `QWidget.show`.\n\nUpdate displayed image.\n\nThis method can be overridden or extended in subclasses and plugins to react\nto image changes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools", "path": "api/skimage.viewer.canvastools", "type": "viewer", "text": "\n`skimage.viewer.canvastools.LineTool`(manager)\n\nWidget for line selection in a plot.\n\n`skimage.viewer.canvastools.PaintTool`(\u2026[, \u2026])\n\nWidget for painting on top of a plot.\n\n`skimage.viewer.canvastools.RectangleTool`(manager)\n\nWidget for selecting a rectangular region in a plot.\n\n`skimage.viewer.canvastools.ThickLineTool`(manager)\n\nWidget for line selection in a plot.\n\n`skimage.viewer.canvastools.base`\n\n`skimage.viewer.canvastools.linetool`\n\n`skimage.viewer.canvastools.painttool`\n\n`skimage.viewer.canvastools.recttool`\n\nBases: `skimage.viewer.canvastools.base.CanvasToolBase`\n\nWidget for line selection in a plot.\n\nSkimage viewer or plot plugin object.\n\nFunction called whenever a control handle is moved. This function must accept\nthe end points of line as the only argument.\n\nFunction called whenever the control handle is released.\n\nFunction called whenever the \u201center\u201d key is pressed.\n\nMaximum pixel distance allowed when selecting control handle.\n\nProperties for `matplotlib.lines.Line2D`.\n\nMarker properties for the handles (also see `matplotlib.lines.Line2D`).\n\nEnd points of line ((x1, y1), (x2, y2)).\n\nInitialize self. See help(type(self)) for accurate signature.\n\nGeometry information that gets passed to callback functions.\n\nBases: `skimage.viewer.canvastools.base.CanvasToolBase`\n\nWidget for painting on top of a plot.\n\nSkimage viewer or plot plugin object.\n\n2D shape tuple used to initialize overlay image.\n\nThe size of the paint cursor.\n\nOpacity of overlay.\n\nFunction called whenever a control handle is moved. This function must accept\nthe end points of line as the only argument.\n\nFunction called whenever the control handle is released.\n\nFunction called whenever the \u201center\u201d key is pressed.\n\nProperties for `matplotlib.patches.Rectangle`. This class redefines defaults\nin `matplotlib.widgets.RectangleSelector`.\n\nOverlay of painted labels displayed on top of image.\n\nCurrent paint color.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nGeometry information that gets passed to callback functions.\n\nBases: `skimage.viewer.canvastools.base.CanvasToolBase`,\n`matplotlib.widgets.RectangleSelector`\n\nWidget for selecting a rectangular region in a plot.\n\nAfter making the desired selection, press \u201cEnter\u201d to accept the selection and\ncall the `on_enter` callback function.\n\nSkimage viewer or plot plugin object.\n\nFunction called whenever a control handle is moved. This function must accept\nthe rectangle extents as the only argument.\n\nFunction called whenever the control handle is released.\n\nFunction called whenever the \u201center\u201d key is pressed.\n\nMaximum pixel distance allowed when selecting control handle.\n\nProperties for `matplotlib.patches.Rectangle`. This class redefines defaults\nin `matplotlib.widgets.RectangleSelector`.\n\nReturn (xmin, xmax, ymin, ymax).\n\nThe parent axes for the widget.\n\nA callback function that is called after a selection is completed. It must\nhave the signature:\n\nwhere eclick and erelease are the mouse click and release `MouseEvent`s that\nstart and complete the selection.\n\nWhether to draw the full rectangle box, the diagonal line of the rectangle, or\nnothing at all.\n\nSelections with an x-span less than minspanx are ignored.\n\nSelections with an y-span less than minspany are ignored.\n\nWhether to use blitting for faster drawing (if supported by the backend).\n\nProperties with which the line is drawn, if `drawtype == \"line\"`. Default:\n\nProperties with which the rectangle is drawn, if `drawtype == \"box\"`. Default:\n\nWhether to interpret minspanx and minspany in data or in pixel coordinates.\n\nButton(s) that trigger rectangle selection.\n\nDistance in pixels within which the interactive tool handles can be activated.\n\nProperties with which the interactive handles are drawn. Currently not\nimplemented and ignored.\n\nWhether to draw a set of handles that allow interaction with the widget after\nit is drawn.\n\nKeyboard modifiers which affect the widget\u2019s behavior. Values amend the\ndefaults.\n\n\u201csquare\u201d and \u201ccenter\u201d can be combined.\n\nCorners of rectangle from lower left, moving clockwise.\n\nMidpoint of rectangle edges from left, moving clockwise.\n\nReturn (xmin, xmax, ymin, ymax).\n\nGeometry information that gets passed to callback functions.\n\nBases: `skimage.viewer.canvastools.linetool.LineTool`\n\nWidget for line selection in a plot.\n\nThe thickness of the line can be varied using the mouse scroll wheel, or with\nthe \u2018+\u2019 and \u2018-\u2018 keys.\n\nSkimage viewer or plot plugin object.\n\nFunction called whenever a control handle is moved. This function must accept\nthe end points of line as the only argument.\n\nFunction called whenever the control handle is released.\n\nFunction called whenever the \u201center\u201d key is pressed.\n\nFunction called whenever the line thickness is changed.\n\nMaximum pixel distance allowed when selecting control handle.\n\nProperties for `matplotlib.lines.Line2D`.\n\nMarker properties for the handles (also see `matplotlib.lines.Line2D`).\n\nEnd points of line ((x1, y1), (x2, y2)).\n\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.LineTool", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool", "type": "viewer", "text": "\nBases: `skimage.viewer.canvastools.base.CanvasToolBase`\n\nWidget for line selection in a plot.\n\nSkimage viewer or plot plugin object.\n\nFunction called whenever a control handle is moved. This function must accept\nthe end points of line as the only argument.\n\nFunction called whenever the control handle is released.\n\nFunction called whenever the \u201center\u201d key is pressed.\n\nMaximum pixel distance allowed when selecting control handle.\n\nProperties for `matplotlib.lines.Line2D`.\n\nMarker properties for the handles (also see `matplotlib.lines.Line2D`).\n\nEnd points of line ((x1, y1), (x2, y2)).\n\nInitialize self. See help(type(self)) for accurate signature.\n\nGeometry information that gets passed to callback functions.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.LineTool.end_points()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.end_points", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.LineTool.geometry()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.geometry", "type": "viewer", "text": "\nGeometry information that gets passed to callback functions.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.LineTool.hit_test()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.hit_test", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.LineTool.on_mouse_press()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.on_mouse_press", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.LineTool.on_mouse_release()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.on_mouse_release", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.LineTool.on_move()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.on_move", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.LineTool.update()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.update", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.LineTool.__init__()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.PaintTool", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool", "type": "viewer", "text": "\nBases: `skimage.viewer.canvastools.base.CanvasToolBase`\n\nWidget for painting on top of a plot.\n\nSkimage viewer or plot plugin object.\n\n2D shape tuple used to initialize overlay image.\n\nThe size of the paint cursor.\n\nOpacity of overlay.\n\nFunction called whenever a control handle is moved. This function must accept\nthe end points of line as the only argument.\n\nFunction called whenever the control handle is released.\n\nFunction called whenever the \u201center\u201d key is pressed.\n\nProperties for `matplotlib.patches.Rectangle`. This class redefines defaults\nin `matplotlib.widgets.RectangleSelector`.\n\nOverlay of painted labels displayed on top of image.\n\nCurrent paint color.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nGeometry information that gets passed to callback functions.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.PaintTool.geometry()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.geometry", "type": "viewer", "text": "\nGeometry information that gets passed to callback functions.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.PaintTool.label()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.label", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.PaintTool.on_key_press()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.on_key_press", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.PaintTool.on_mouse_press()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.on_mouse_press", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.PaintTool.on_mouse_release()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.on_mouse_release", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.PaintTool.on_move()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.on_move", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.PaintTool.overlay()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.overlay", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.PaintTool.radius()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.radius", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.PaintTool.shape()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.shape", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.PaintTool.update_cursor()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.update_cursor", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.PaintTool.update_overlay()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.update_overlay", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.PaintTool.__init__()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.RectangleTool", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool", "type": "viewer", "text": "\nBases: `skimage.viewer.canvastools.base.CanvasToolBase`,\n`matplotlib.widgets.RectangleSelector`\n\nWidget for selecting a rectangular region in a plot.\n\nAfter making the desired selection, press \u201cEnter\u201d to accept the selection and\ncall the `on_enter` callback function.\n\nSkimage viewer or plot plugin object.\n\nFunction called whenever a control handle is moved. This function must accept\nthe rectangle extents as the only argument.\n\nFunction called whenever the control handle is released.\n\nFunction called whenever the \u201center\u201d key is pressed.\n\nMaximum pixel distance allowed when selecting control handle.\n\nProperties for `matplotlib.patches.Rectangle`. This class redefines defaults\nin `matplotlib.widgets.RectangleSelector`.\n\nReturn (xmin, xmax, ymin, ymax).\n\nThe parent axes for the widget.\n\nA callback function that is called after a selection is completed. It must\nhave the signature:\n\nwhere eclick and erelease are the mouse click and release `MouseEvent`s that\nstart and complete the selection.\n\nWhether to draw the full rectangle box, the diagonal line of the rectangle, or\nnothing at all.\n\nSelections with an x-span less than minspanx are ignored.\n\nSelections with an y-span less than minspany are ignored.\n\nWhether to use blitting for faster drawing (if supported by the backend).\n\nProperties with which the line is drawn, if `drawtype == \"line\"`. Default:\n\nProperties with which the rectangle is drawn, if `drawtype == \"box\"`. Default:\n\nWhether to interpret minspanx and minspany in data or in pixel coordinates.\n\nButton(s) that trigger rectangle selection.\n\nDistance in pixels within which the interactive tool handles can be activated.\n\nProperties with which the interactive handles are drawn. Currently not\nimplemented and ignored.\n\nWhether to draw a set of handles that allow interaction with the widget after\nit is drawn.\n\nKeyboard modifiers which affect the widget\u2019s behavior. Values amend the\ndefaults.\n\n\u201csquare\u201d and \u201ccenter\u201d can be combined.\n\nCorners of rectangle from lower left, moving clockwise.\n\nMidpoint of rectangle edges from left, moving clockwise.\n\nReturn (xmin, xmax, ymin, ymax).\n\nGeometry information that gets passed to callback functions.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.RectangleTool.corners()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.corners", "type": "viewer", "text": "\nCorners of rectangle from lower left, moving clockwise.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.RectangleTool.edge_centers()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.edge_centers", "type": "viewer", "text": "\nMidpoint of rectangle edges from left, moving clockwise.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.RectangleTool.extents()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.extents", "type": "viewer", "text": "\nReturn (xmin, xmax, ymin, ymax).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.RectangleTool.geometry()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.geometry", "type": "viewer", "text": "\nGeometry information that gets passed to callback functions.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.RectangleTool.on_mouse_press()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.on_mouse_press", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.RectangleTool.on_mouse_release()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.on_mouse_release", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.RectangleTool.on_move()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.on_move", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.RectangleTool.__init__()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.__init__", "type": "viewer", "text": "\nThe parent axes for the widget.\n\nA callback function that is called after a selection is completed. It must\nhave the signature:\n\nwhere eclick and erelease are the mouse click and release `MouseEvent`s that\nstart and complete the selection.\n\nWhether to draw the full rectangle box, the diagonal line of the rectangle, or\nnothing at all.\n\nSelections with an x-span less than minspanx are ignored.\n\nSelections with an y-span less than minspany are ignored.\n\nWhether to use blitting for faster drawing (if supported by the backend).\n\nProperties with which the line is drawn, if `drawtype == \"line\"`. Default:\n\nProperties with which the rectangle is drawn, if `drawtype == \"box\"`. Default:\n\nWhether to interpret minspanx and minspany in data or in pixel coordinates.\n\nButton(s) that trigger rectangle selection.\n\nDistance in pixels within which the interactive tool handles can be activated.\n\nProperties with which the interactive handles are drawn. Currently not\nimplemented and ignored.\n\nWhether to draw a set of handles that allow interaction with the widget after\nit is drawn.\n\nKeyboard modifiers which affect the widget\u2019s behavior. Values amend the\ndefaults.\n\n\u201csquare\u201d and \u201ccenter\u201d can be combined.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.ThickLineTool", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.ThickLineTool", "type": "viewer", "text": "\nBases: `skimage.viewer.canvastools.linetool.LineTool`\n\nWidget for line selection in a plot.\n\nThe thickness of the line can be varied using the mouse scroll wheel, or with\nthe \u2018+\u2019 and \u2018-\u2018 keys.\n\nSkimage viewer or plot plugin object.\n\nFunction called whenever a control handle is moved. This function must accept\nthe end points of line as the only argument.\n\nFunction called whenever the control handle is released.\n\nFunction called whenever the \u201center\u201d key is pressed.\n\nFunction called whenever the line thickness is changed.\n\nMaximum pixel distance allowed when selecting control handle.\n\nProperties for `matplotlib.lines.Line2D`.\n\nMarker properties for the handles (also see `matplotlib.lines.Line2D`).\n\nEnd points of line ((x1, y1), (x2, y2)).\n\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.ThickLineTool.on_key_press()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.ThickLineTool.on_key_press", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.ThickLineTool.on_scroll()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.ThickLineTool.on_scroll", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.canvastools.ThickLineTool.__init__()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.ThickLineTool.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.CollectionViewer", "path": "api/skimage.viewer#skimage.viewer.CollectionViewer", "type": "viewer", "text": "\nBases: `skimage.viewer.viewers.core.ImageViewer`\n\nViewer for displaying image collections.\n\nSelect the displayed frame of the image collection using the slider or with\nthe following keyboard shortcuts:\n\nPrevious/next image in collection.\n\n0% to 90% of collection. For example, \u201c5\u201d goes to the image in the middle\n(i.e. 50%) of the collection.\n\nFirst/last image in collection.\n\nList of images to be displayed.\n\nControl whether image is updated on slide or release of the image slider.\nUsing \u2018on_release\u2019 will give smoother behavior when displaying large images or\nwhen writing a plugin/subclass that requires heavy computation.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nSelect image on display using index into image collection.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.CollectionViewer.keyPressEvent()", "path": "api/skimage.viewer#skimage.viewer.CollectionViewer.keyPressEvent", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.CollectionViewer.update_index()", "path": "api/skimage.viewer#skimage.viewer.CollectionViewer.update_index", "type": "viewer", "text": "\nSelect image on display using index into image collection.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.CollectionViewer.__init__()", "path": "api/skimage.viewer#skimage.viewer.CollectionViewer.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.ImageViewer", "path": "api/skimage.viewer#skimage.viewer.ImageViewer", "type": "viewer", "text": "\nBases: `object`\n\nViewer for displaying images.\n\nThis viewer is a simple container object that holds a Matplotlib axes for\nshowing images. `ImageViewer` doesn\u2019t subclass the Matplotlib axes (or figure)\nbecause of the high probability of name collisions.\n\nSubclasses and plugins will likely extend the `update_image` method to add\ncustom overlays or filter the displayed image.\n\nImage being viewed.\n\nMatplotlib canvas, figure, and axes used to display image.\n\nImage being viewed. Setting this value will update the displayed frame.\n\nPlugins typically operate on (but don\u2019t change) the original image.\n\nList of attached plugins.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nConnect callback function to matplotlib event and return id.\n\nDisconnect callback by its id (returned by `connect_event`).\n\nOpen image file and display in viewer.\n\nSave current image to file.\n\nThe current behavior is not ideal: It saves the image displayed on screen, so\nall images will be converted to RGB, and the image size is not preserved\n(resizing the viewer window will alter the size of the saved image).\n\nShow ImageViewer and attached plugins.\n\nThis behaves much like `matplotlib.pyplot.show` and `QWidget.show`.\n\nUpdate displayed image.\n\nThis method can be overridden or extended in subclasses and plugins to react\nto image changes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.ImageViewer.add_tool()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.add_tool", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.ImageViewer.closeEvent()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.closeEvent", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.ImageViewer.connect_event()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.connect_event", "type": "viewer", "text": "\nConnect callback function to matplotlib event and return id.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.ImageViewer.disconnect_event()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.disconnect_event", "type": "viewer", "text": "\nDisconnect callback by its id (returned by `connect_event`).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.ImageViewer.dock_areas", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.dock_areas", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.ImageViewer.image()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.image", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.ImageViewer.open_file()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.open_file", "type": "viewer", "text": "\nOpen image file and display in viewer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.ImageViewer.original_image_changed", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.original_image_changed", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.ImageViewer.redraw()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.redraw", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.ImageViewer.remove_tool()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.remove_tool", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.ImageViewer.reset_image()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.reset_image", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.ImageViewer.save_to_file()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.save_to_file", "type": "viewer", "text": "\nSave current image to file.\n\nThe current behavior is not ideal: It saves the image displayed on screen, so\nall images will be converted to RGB, and the image size is not preserved\n(resizing the viewer window will alter the size of the saved image).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.ImageViewer.show()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.show", "type": "viewer", "text": "\nShow ImageViewer and attached plugins.\n\nThis behaves much like `matplotlib.pyplot.show` and `QWidget.show`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.ImageViewer.update_image()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.update_image", "type": "viewer", "text": "\nUpdate displayed image.\n\nThis method can be overridden or extended in subclasses and plugins to react\nto image changes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.ImageViewer.__init__()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins", "path": "api/skimage.viewer.plugins", "type": "viewer", "text": "\n`skimage.viewer.plugins.CannyPlugin`(*args, \u2026)\n\nCanny filter plugin to show edges of an image.\n\n`skimage.viewer.plugins.ColorHistogram`([max_pct])\n\n`skimage.viewer.plugins.Crop`([maxdist])\n\n`skimage.viewer.plugins.LabelPainter`([max_radius])\n\n`skimage.viewer.plugins.LineProfile`([\u2026])\n\nPlugin to compute interpolated intensity under a scan line on an image.\n\n`skimage.viewer.plugins.Measure`([maxdist])\n\n`skimage.viewer.plugins.OverlayPlugin`(**kwargs)\n\nPlugin for ImageViewer that displays an overlay on top of main image.\n\n`skimage.viewer.plugins.PlotPlugin`([\u2026])\n\nPlugin for ImageViewer that contains a plot canvas.\n\n`skimage.viewer.plugins.Plugin`([\u2026])\n\nBase class for plugins that interact with an ImageViewer.\n\n`skimage.viewer.plugins.base`\n\nBase class for Plugins that interact with ImageViewer.\n\n`skimage.viewer.plugins.canny`\n\n`skimage.viewer.plugins.color_histogram`\n\n`skimage.viewer.plugins.crop`\n\n`skimage.viewer.plugins.labelplugin`\n\n`skimage.viewer.plugins.lineprofile`\n\n`skimage.viewer.plugins.measure`\n\n`skimage.viewer.plugins.overlayplugin`\n\n`skimage.viewer.plugins.plotplugin`\n\nBases: `skimage.viewer.plugins.overlayplugin.OverlayPlugin`\n\nCanny filter plugin to show edges of an image.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\nBases: `skimage.viewer.plugins.plotplugin.PlotPlugin`\n\nInitialize self. See help(type(self)) for accurate signature.\n\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\nReturn the image mask and the histogram data.\n\nThe selected pixels.\n\nThe data describing the histogram and the selected region. The dictionary\ncontains:\n\nBases: `skimage.viewer.plugins.base.Plugin`\n\nInitialize self. See help(type(self)) for accurate signature.\n\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\nBases: `skimage.viewer.plugins.base.Plugin`\n\nInitialize self. See help(type(self)) for accurate signature.\n\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\nBases: `skimage.viewer.plugins.plotplugin.PlotPlugin`\n\nPlugin to compute interpolated intensity under a scan line on an image.\n\nSee PlotPlugin and Plugin classes for additional details.\n\nMaximum pixel distance allowed when selecting end point of scan line.\n\n(minimum, maximum) intensity limits for plotted profile. The following special\nvalues are defined:\n\nNone : rescale based on min/max intensity along selected scan line. \u2018image\u2019 :\nfixed scale based on min/max intensity in image. \u2018dtype\u2019 : fixed scale based\non min/max intensity of image dtype.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\nReturn intensity profile of the selected line.\n\nThe positions ((x1, y1), (x2, y2)) of the line ends.\n\nProfile of intensity values. Length 1 (grayscale) or 3 (rgb).\n\nReturn the drawn line and the resulting scan.\n\nAn array of 0s with the scanned line set to 255. If the linewidth of the line\ntool is greater than 1, sets the values within the profiled polygon to 128.\n\nThe line scan values across the image.\n\nBases: `skimage.viewer.plugins.base.Plugin`\n\nInitialize self. See help(type(self)) for accurate signature.\n\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\nBases: `skimage.viewer.plugins.base.Plugin`\n\nPlugin for ImageViewer that displays an overlay on top of main image.\n\nThe base Plugin class displays the filtered image directly on the viewer.\nOverlayPlugin will instead overlay an image with a transparent colormap.\n\nSee base Plugin class for additional details.\n\nOverlay displayed on top of image. This overlay defaults to a color map with\nalpha values varying linearly from 0 to 1.\n\nColor of overlay.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\nOn close disconnect all artists and events from ImageViewer.\n\nNote that artists must be appended to `self.artists`.\n\nDisplay filtered image as an overlay on top of image in viewer.\n\nReturn filtered image.\n\nThis \u201cfiltered image\u201d is used when saving from the plugin.\n\nReturn the overlaid image.\n\nThe overlay currently displayed.\n\nBases: `skimage.viewer.plugins.base.Plugin`\n\nPlugin for ImageViewer that contains a plot canvas.\n\nBase class for plugins that contain a Matplotlib plot canvas, which can, for\nexample, display an image histogram.\n\nSee base Plugin class for additional details.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\nRedraw plot.\n\nBases: `object`\n\nBase class for plugins that interact with an ImageViewer.\n\nA plugin connects an image filter (or another function) to an image viewer.\nNote that a Plugin is initialized without an image viewer and attached in a\nlater step. See example below for details.\n\nWindow containing image used in measurement/manipulation.\n\nFunction that gets called to update image in image viewer. This value can be\n`None` if, for example, you have a plugin that extracts information from an\nimage and doesn\u2019t manipulate it. Alternatively, this function can be defined\nas a method in a Plugin subclass.\n\nSize of plugin window in pixels. Note that Qt will automatically resize a\nwindow to fit components. So if you\u2019re adding rows of components, you can\nleave `height = 0` and just let Qt determine the final height.\n\nIf True, use blitting to speed up animation. Only available on some Matplotlib\nbackends. If None, set to True when using Agg backend. This only has an effect\nif you draw on top of an image viewer.\n\nThe plugin will automatically delegate parameters to `image_filter` based on\nits parameter type, i.e., `ptype` (widgets for required arguments must be\nadded in the order they appear in the function). The image attached to the\nviewer is automatically passed as the first argument to the filter function.\n\n#TODO: Add flag so image is not passed to filter function by default.\n\n`ptype = \u2018kwarg\u2019` is the default for most widgets so it\u2019s unnecessary here.\n\nWindow containing image used in measurement.\n\nName of plugin. This is displayed as the window title.\n\nList of Matplotlib artists and canvastools. Any artists created by the plugin\nshould be added to this list so that it gets cleaned up on close.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nAdd widget to plugin.\n\nAlternatively, Plugin\u2019s `__add__` method is overloaded to add widgets:\n\nWidgets can adjust required or optional arguments of filter function or\nparameters for the plugin. This is specified by the Widget\u2019s `ptype`.\n\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\nOn close disconnect all artists and events from ImageViewer.\n\nNote that artists must be appended to `self.artists`.\n\nDisplay the filtered image on image viewer.\n\nIf you don\u2019t want to simply replace the displayed image with the filtered\nimage (e.g., you want to display a transparent overlay), you can override this\nmethod.\n\nCall `image_filter` with widget args and kwargs\n\nNote: `display_filtered_image` is automatically called.\n\nReturn filtered image.\n\nReturn the plugin\u2019s representation and data.\n\nThe filtered image.\n\nAny data associated with the plugin.\n\nDerived classes should override this method to return a tuple containing an\noverlay of the same shape of the image, and a data object. Either of these is\noptional: return `None` if you don\u2019t want to return a value.\n\nRemove artists that are connected to the image viewer.\n\nShow plugin.\n\nUpdate keyword parameters of the plugin itself.\n\nThese parameters will typically be implemented as class properties so that\nthey update the image or some other component.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.CannyPlugin", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.CannyPlugin", "type": "viewer", "text": "\nBases: `skimage.viewer.plugins.overlayplugin.OverlayPlugin`\n\nCanny filter plugin to show edges of an image.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.CannyPlugin.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.CannyPlugin.attach", "type": "viewer", "text": "\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.CannyPlugin.name", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.CannyPlugin.name", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.CannyPlugin.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.CannyPlugin.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.ColorHistogram", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.ColorHistogram", "type": "viewer", "text": "\nBases: `skimage.viewer.plugins.plotplugin.PlotPlugin`\n\nInitialize self. See help(type(self)) for accurate signature.\n\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\nReturn the image mask and the histogram data.\n\nThe selected pixels.\n\nThe data describing the histogram and the selected region. The dictionary\ncontains:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.ColorHistogram.ab_selected()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.ColorHistogram.ab_selected", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.ColorHistogram.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.ColorHistogram.attach", "type": "viewer", "text": "\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.ColorHistogram.help()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.ColorHistogram.help", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.ColorHistogram.name", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.ColorHistogram.name", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.ColorHistogram.output()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.ColorHistogram.output", "type": "viewer", "text": "\nReturn the image mask and the histogram data.\n\nThe selected pixels.\n\nThe data describing the histogram and the selected region. The dictionary\ncontains:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.ColorHistogram.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.ColorHistogram.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Crop", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Crop", "type": "viewer", "text": "\nBases: `skimage.viewer.plugins.base.Plugin`\n\nInitialize self. See help(type(self)) for accurate signature.\n\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Crop.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Crop.attach", "type": "viewer", "text": "\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Crop.crop()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Crop.crop", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Crop.help()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Crop.help", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Crop.name", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Crop.name", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Crop.reset()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Crop.reset", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Crop.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Crop.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.LabelPainter", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter", "type": "viewer", "text": "\nBases: `skimage.viewer.plugins.base.Plugin`\n\nInitialize self. See help(type(self)) for accurate signature.\n\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.LabelPainter.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter.attach", "type": "viewer", "text": "\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.LabelPainter.help()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter.help", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.LabelPainter.label()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter.label", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.LabelPainter.name", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter.name", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.LabelPainter.on_enter()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter.on_enter", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.LabelPainter.radius()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter.radius", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.LabelPainter.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.LineProfile", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile", "type": "viewer", "text": "\nBases: `skimage.viewer.plugins.plotplugin.PlotPlugin`\n\nPlugin to compute interpolated intensity under a scan line on an image.\n\nSee PlotPlugin and Plugin classes for additional details.\n\nMaximum pixel distance allowed when selecting end point of scan line.\n\n(minimum, maximum) intensity limits for plotted profile. The following special\nvalues are defined:\n\nNone : rescale based on min/max intensity along selected scan line. \u2018image\u2019 :\nfixed scale based on min/max intensity in image. \u2018dtype\u2019 : fixed scale based\non min/max intensity of image dtype.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\nReturn intensity profile of the selected line.\n\nThe positions ((x1, y1), (x2, y2)) of the line ends.\n\nProfile of intensity values. Length 1 (grayscale) or 3 (rgb).\n\nReturn the drawn line and the resulting scan.\n\nAn array of 0s with the scanned line set to 255. If the linewidth of the line\ntool is greater than 1, sets the values within the profiled polygon to 128.\n\nThe line scan values across the image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.LineProfile.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.attach", "type": "viewer", "text": "\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.LineProfile.get_profiles()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.get_profiles", "type": "viewer", "text": "\nReturn intensity profile of the selected line.\n\nThe positions ((x1, y1), (x2, y2)) of the line ends.\n\nProfile of intensity values. Length 1 (grayscale) or 3 (rgb).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.LineProfile.help()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.help", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.LineProfile.line_changed()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.line_changed", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.LineProfile.name", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.name", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.LineProfile.output()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.output", "type": "viewer", "text": "\nReturn the drawn line and the resulting scan.\n\nAn array of 0s with the scanned line set to 255. If the linewidth of the line\ntool is greater than 1, sets the values within the profiled polygon to 128.\n\nThe line scan values across the image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.LineProfile.reset_axes()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.reset_axes", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.LineProfile.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Measure", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Measure", "type": "viewer", "text": "\nBases: `skimage.viewer.plugins.base.Plugin`\n\nInitialize self. See help(type(self)) for accurate signature.\n\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Measure.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Measure.attach", "type": "viewer", "text": "\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Measure.help()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Measure.help", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Measure.line_changed()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Measure.line_changed", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Measure.name", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Measure.name", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Measure.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Measure.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.OverlayPlugin", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin", "type": "viewer", "text": "\nBases: `skimage.viewer.plugins.base.Plugin`\n\nPlugin for ImageViewer that displays an overlay on top of main image.\n\nThe base Plugin class displays the filtered image directly on the viewer.\nOverlayPlugin will instead overlay an image with a transparent colormap.\n\nSee base Plugin class for additional details.\n\nOverlay displayed on top of image. This overlay defaults to a color map with\nalpha values varying linearly from 0 to 1.\n\nColor of overlay.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\nOn close disconnect all artists and events from ImageViewer.\n\nNote that artists must be appended to `self.artists`.\n\nDisplay filtered image as an overlay on top of image in viewer.\n\nReturn filtered image.\n\nThis \u201cfiltered image\u201d is used when saving from the plugin.\n\nReturn the overlaid image.\n\nThe overlay currently displayed.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.OverlayPlugin.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.attach", "type": "viewer", "text": "\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.OverlayPlugin.closeEvent()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.closeEvent", "type": "viewer", "text": "\nOn close disconnect all artists and events from ImageViewer.\n\nNote that artists must be appended to `self.artists`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.OverlayPlugin.color()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.color", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.OverlayPlugin.colors", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.colors", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.OverlayPlugin.display_filtered_image()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.display_filtered_image", "type": "viewer", "text": "\nDisplay filtered image as an overlay on top of image in viewer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.OverlayPlugin.filtered_image()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.filtered_image", "type": "viewer", "text": "\nReturn filtered image.\n\nThis \u201cfiltered image\u201d is used when saving from the plugin.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.OverlayPlugin.output()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.output", "type": "viewer", "text": "\nReturn the overlaid image.\n\nThe overlay currently displayed.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.OverlayPlugin.overlay()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.overlay", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.OverlayPlugin.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.PlotPlugin", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.PlotPlugin", "type": "viewer", "text": "\nBases: `skimage.viewer.plugins.base.Plugin`\n\nPlugin for ImageViewer that contains a plot canvas.\n\nBase class for plugins that contain a Matplotlib plot canvas, which can, for\nexample, display an image histogram.\n\nSee base Plugin class for additional details.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\nRedraw plot.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.PlotPlugin.add_plot()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.PlotPlugin.add_plot", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.PlotPlugin.add_tool()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.PlotPlugin.add_tool", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.PlotPlugin.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.PlotPlugin.attach", "type": "viewer", "text": "\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.PlotPlugin.redraw()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.PlotPlugin.redraw", "type": "viewer", "text": "\nRedraw plot.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.PlotPlugin.remove_tool()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.PlotPlugin.remove_tool", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.PlotPlugin.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.PlotPlugin.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Plugin", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin", "type": "viewer", "text": "\nBases: `object`\n\nBase class for plugins that interact with an ImageViewer.\n\nA plugin connects an image filter (or another function) to an image viewer.\nNote that a Plugin is initialized without an image viewer and attached in a\nlater step. See example below for details.\n\nWindow containing image used in measurement/manipulation.\n\nFunction that gets called to update image in image viewer. This value can be\n`None` if, for example, you have a plugin that extracts information from an\nimage and doesn\u2019t manipulate it. Alternatively, this function can be defined\nas a method in a Plugin subclass.\n\nSize of plugin window in pixels. Note that Qt will automatically resize a\nwindow to fit components. So if you\u2019re adding rows of components, you can\nleave `height = 0` and just let Qt determine the final height.\n\nIf True, use blitting to speed up animation. Only available on some Matplotlib\nbackends. If None, set to True when using Agg backend. This only has an effect\nif you draw on top of an image viewer.\n\nThe plugin will automatically delegate parameters to `image_filter` based on\nits parameter type, i.e., `ptype` (widgets for required arguments must be\nadded in the order they appear in the function). The image attached to the\nviewer is automatically passed as the first argument to the filter function.\n\n#TODO: Add flag so image is not passed to filter function by default.\n\n`ptype = \u2018kwarg\u2019` is the default for most widgets so it\u2019s unnecessary here.\n\nWindow containing image used in measurement.\n\nName of plugin. This is displayed as the window title.\n\nList of Matplotlib artists and canvastools. Any artists created by the plugin\nshould be added to this list so that it gets cleaned up on close.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nAdd widget to plugin.\n\nAlternatively, Plugin\u2019s `__add__` method is overloaded to add widgets:\n\nWidgets can adjust required or optional arguments of filter function or\nparameters for the plugin. This is specified by the Widget\u2019s `ptype`.\n\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\nOn close disconnect all artists and events from ImageViewer.\n\nNote that artists must be appended to `self.artists`.\n\nDisplay the filtered image on image viewer.\n\nIf you don\u2019t want to simply replace the displayed image with the filtered\nimage (e.g., you want to display a transparent overlay), you can override this\nmethod.\n\nCall `image_filter` with widget args and kwargs\n\nNote: `display_filtered_image` is automatically called.\n\nReturn filtered image.\n\nReturn the plugin\u2019s representation and data.\n\nThe filtered image.\n\nAny data associated with the plugin.\n\nDerived classes should override this method to return a tuple containing an\noverlay of the same shape of the image, and a data object. Either of these is\noptional: return `None` if you don\u2019t want to return a value.\n\nRemove artists that are connected to the image viewer.\n\nShow plugin.\n\nUpdate keyword parameters of the plugin itself.\n\nThese parameters will typically be implemented as class properties so that\nthey update the image or some other component.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Plugin.add_widget()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.add_widget", "type": "viewer", "text": "\nAdd widget to plugin.\n\nAlternatively, Plugin\u2019s `__add__` method is overloaded to add widgets:\n\nWidgets can adjust required or optional arguments of filter function or\nparameters for the plugin. This is specified by the Widget\u2019s `ptype`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Plugin.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.attach", "type": "viewer", "text": "\nAttach the plugin to an ImageViewer.\n\nNote that the ImageViewer will automatically call this method when the plugin\nis added to the ImageViewer. For example:\n\nAlso note that `attach` automatically calls the filter function so that the\nimage matches the filtered value specified by attached widgets.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Plugin.clean_up()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.clean_up", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Plugin.closeEvent()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.closeEvent", "type": "viewer", "text": "\nOn close disconnect all artists and events from ImageViewer.\n\nNote that artists must be appended to `self.artists`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Plugin.display_filtered_image()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.display_filtered_image", "type": "viewer", "text": "\nDisplay the filtered image on image viewer.\n\nIf you don\u2019t want to simply replace the displayed image with the filtered\nimage (e.g., you want to display a transparent overlay), you can override this\nmethod.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Plugin.filtered_image()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.filtered_image", "type": "viewer", "text": "\nReturn filtered image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Plugin.filter_image()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.filter_image", "type": "viewer", "text": "\nCall `image_filter` with widget args and kwargs\n\nNote: `display_filtered_image` is automatically called.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Plugin.image_changed", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.image_changed", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Plugin.image_viewer", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.image_viewer", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Plugin.name", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.name", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Plugin.output()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.output", "type": "viewer", "text": "\nReturn the plugin\u2019s representation and data.\n\nThe filtered image.\n\nAny data associated with the plugin.\n\nDerived classes should override this method to return a tuple containing an\noverlay of the same shape of the image, and a data object. Either of these is\noptional: return `None` if you don\u2019t want to return a value.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Plugin.remove_image_artists()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.remove_image_artists", "type": "viewer", "text": "\nRemove artists that are connected to the image viewer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Plugin.show()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.show", "type": "viewer", "text": "\nShow plugin.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Plugin.update_plugin()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.update_plugin", "type": "viewer", "text": "\nUpdate keyword parameters of the plugin itself.\n\nThese parameters will typically be implemented as class properties so that\nthey update the image or some other component.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.plugins.Plugin.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.utils", "path": "api/skimage.viewer.utils", "type": "viewer", "text": "\n`skimage.viewer.utils.figimage`(image[, \u2026])\n\nReturn figure and axes with figure tightly surrounding image.\n\n`skimage.viewer.utils.init_qtapp`()\n\nInitialize QAppliction.\n\n`skimage.viewer.utils.new_plot`([parent, \u2026])\n\nReturn new figure and axes.\n\n`skimage.viewer.utils.start_qtapp`([app])\n\nStart Qt mainloop\n\n`skimage.viewer.utils.update_axes_image`(\u2026)\n\nUpdate the image displayed by an image plot.\n\n`skimage.viewer.utils.ClearColormap`(rgb[, \u2026])\n\nColor map that varies linearly from alpha = 0 to 1\n\n`skimage.viewer.utils.FigureCanvas`(figure, \u2026)\n\nCanvas for displaying images.\n\n`skimage.viewer.utils.LinearColormap`(name, \u2026)\n\nLinearSegmentedColormap in which color varies smoothly.\n\n`skimage.viewer.utils.RequiredAttr`([init_val])\n\nA class attribute that must be set before use.\n\n`skimage.viewer.utils.canvas`\n\n`skimage.viewer.utils.core`\n\n`skimage.viewer.utils.dialogs`\n\nReturn figure and axes with figure tightly surrounding image.\n\nUnlike pyplot.figimage, this actually plots onto an axes object, which fills\nthe figure. Plotting the image onto an axes allows for subsequent overlays of\naxes artists.\n\nimage to plot\n\nIf scale is 1, the figure and axes have the same dimension as the image.\nSmaller values of `scale` will shrink the figure.\n\nDots per inch for figure. If None, use the default rcParam.\n\nInitialize QAppliction.\n\nThe QApplication needs to be initialized before creating any QWidgets\n\nReturn new figure and axes.\n\nQt widget that displays the plot objects. If None, you must manually call\n`canvas.setParent` and pass the parent widget.\n\nKeyword arguments passed `matplotlib.figure.Figure.add_subplot`.\n\nKeyword arguments passed `matplotlib.figure.Figure`.\n\nStart Qt mainloop\n\nUpdate the image displayed by an image plot.\n\nThis sets the image plot\u2019s array and updates its shape appropriately\n\nImage axes to update.\n\nImage array.\n\nBases: `skimage.viewer.utils.core.LinearColormap`\n\nColor map that varies linearly from alpha = 0 to 1\n\nCreate color map from linear mapping segments\n\nsegmentdata argument is a dictionary with a red, green and blue entries. Each\nentry should be a list of x, y0, y1 tuples, forming rows in a table. Entries\nfor alpha are optional.\n\nExample: suppose you want red to increase from 0 to 1 over the bottom half,\ngreen to do the same over the middle half, and blue over the top half. Then\nyou would use:\n\nEach row in the table for a given color is a sequence of x, y0, y1 tuples. In\neach sequence, x must increase monotonically from 0 to 1. For any input value\nz falling between x[i] and x[i+1], the output value of a given color will be\nlinearly interpolated between y1[i] and y0[i+1]:\n\nHence y0 in the first row and y1 in the last row are never used.\n\nSee also\n\nStatic method; factory function for generating a smoothly-varying\nLinearSegmentedColormap.\n\nFor information about making a mapping array.\n\nBases: `object`\n\nCanvas for displaying images.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nBases: `matplotlib.colors.LinearSegmentedColormap`\n\nLinearSegmentedColormap in which color varies smoothly.\n\nThis class is a simplification of LinearSegmentedColormap, which doesn\u2019t\nsupport jumps in color intensities.\n\nName of colormap.\n\nDictionary of \u2018red\u2019, \u2018green\u2019, \u2018blue\u2019, and (optionally) \u2018alpha\u2019 values. Each\ncolor key contains a list of `x`, `y` tuples. `x` must increase monotonically\nfrom 0 to 1 and corresponds to input values for a mappable object (e.g. an\nimage). `y` corresponds to the color intensity.\n\nCreate color map from linear mapping segments\n\nsegmentdata argument is a dictionary with a red, green and blue entries. Each\nentry should be a list of x, y0, y1 tuples, forming rows in a table. Entries\nfor alpha are optional.\n\nExample: suppose you want red to increase from 0 to 1 over the bottom half,\ngreen to do the same over the middle half, and blue over the top half. Then\nyou would use:\n\nEach row in the table for a given color is a sequence of x, y0, y1 tuples. In\neach sequence, x must increase monotonically from 0 to 1. For any input value\nz falling between x[i] and x[i+1], the output value of a given color will be\nlinearly interpolated between y1[i] and y0[i+1]:\n\nHence y0 in the first row and y1 in the last row are never used.\n\nSee also\n\nStatic method; factory function for generating a smoothly-varying\nLinearSegmentedColormap.\n\nFor information about making a mapping array.\n\nBases: `object`\n\nA class attribute that must be set before use.\n\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.utils.ClearColormap", "path": "api/skimage.viewer.utils#skimage.viewer.utils.ClearColormap", "type": "viewer", "text": "\nBases: `skimage.viewer.utils.core.LinearColormap`\n\nColor map that varies linearly from alpha = 0 to 1\n\nCreate color map from linear mapping segments\n\nsegmentdata argument is a dictionary with a red, green and blue entries. Each\nentry should be a list of x, y0, y1 tuples, forming rows in a table. Entries\nfor alpha are optional.\n\nExample: suppose you want red to increase from 0 to 1 over the bottom half,\ngreen to do the same over the middle half, and blue over the top half. Then\nyou would use:\n\nEach row in the table for a given color is a sequence of x, y0, y1 tuples. In\neach sequence, x must increase monotonically from 0 to 1. For any input value\nz falling between x[i] and x[i+1], the output value of a given color will be\nlinearly interpolated between y1[i] and y0[i+1]:\n\nHence y0 in the first row and y1 in the last row are never used.\n\nSee also\n\nStatic method; factory function for generating a smoothly-varying\nLinearSegmentedColormap.\n\nFor information about making a mapping array.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.utils.ClearColormap.__init__()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.ClearColormap.__init__", "type": "viewer", "text": "\nCreate color map from linear mapping segments\n\nsegmentdata argument is a dictionary with a red, green and blue entries. Each\nentry should be a list of x, y0, y1 tuples, forming rows in a table. Entries\nfor alpha are optional.\n\nExample: suppose you want red to increase from 0 to 1 over the bottom half,\ngreen to do the same over the middle half, and blue over the top half. Then\nyou would use:\n\nEach row in the table for a given color is a sequence of x, y0, y1 tuples. In\neach sequence, x must increase monotonically from 0 to 1. For any input value\nz falling between x[i] and x[i+1], the output value of a given color will be\nlinearly interpolated between y1[i] and y0[i+1]:\n\nHence y0 in the first row and y1 in the last row are never used.\n\nSee also\n\nStatic method; factory function for generating a smoothly-varying\nLinearSegmentedColormap.\n\nFor information about making a mapping array.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.utils.figimage()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.figimage", "type": "viewer", "text": "\nReturn figure and axes with figure tightly surrounding image.\n\nUnlike pyplot.figimage, this actually plots onto an axes object, which fills\nthe figure. Plotting the image onto an axes allows for subsequent overlays of\naxes artists.\n\nimage to plot\n\nIf scale is 1, the figure and axes have the same dimension as the image.\nSmaller values of `scale` will shrink the figure.\n\nDots per inch for figure. If None, use the default rcParam.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.utils.FigureCanvas", "path": "api/skimage.viewer.utils#skimage.viewer.utils.FigureCanvas", "type": "viewer", "text": "\nBases: `object`\n\nCanvas for displaying images.\n\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.utils.FigureCanvas.resizeEvent()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.FigureCanvas.resizeEvent", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.utils.FigureCanvas.__init__()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.FigureCanvas.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.utils.init_qtapp()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.init_qtapp", "type": "viewer", "text": "\nInitialize QAppliction.\n\nThe QApplication needs to be initialized before creating any QWidgets\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.utils.LinearColormap", "path": "api/skimage.viewer.utils#skimage.viewer.utils.LinearColormap", "type": "viewer", "text": "\nBases: `matplotlib.colors.LinearSegmentedColormap`\n\nLinearSegmentedColormap in which color varies smoothly.\n\nThis class is a simplification of LinearSegmentedColormap, which doesn\u2019t\nsupport jumps in color intensities.\n\nName of colormap.\n\nDictionary of \u2018red\u2019, \u2018green\u2019, \u2018blue\u2019, and (optionally) \u2018alpha\u2019 values. Each\ncolor key contains a list of `x`, `y` tuples. `x` must increase monotonically\nfrom 0 to 1 and corresponds to input values for a mappable object (e.g. an\nimage). `y` corresponds to the color intensity.\n\nCreate color map from linear mapping segments\n\nsegmentdata argument is a dictionary with a red, green and blue entries. Each\nentry should be a list of x, y0, y1 tuples, forming rows in a table. Entries\nfor alpha are optional.\n\nExample: suppose you want red to increase from 0 to 1 over the bottom half,\ngreen to do the same over the middle half, and blue over the top half. Then\nyou would use:\n\nEach row in the table for a given color is a sequence of x, y0, y1 tuples. In\neach sequence, x must increase monotonically from 0 to 1. For any input value\nz falling between x[i] and x[i+1], the output value of a given color will be\nlinearly interpolated between y1[i] and y0[i+1]:\n\nHence y0 in the first row and y1 in the last row are never used.\n\nSee also\n\nStatic method; factory function for generating a smoothly-varying\nLinearSegmentedColormap.\n\nFor information about making a mapping array.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.utils.LinearColormap.__init__()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.LinearColormap.__init__", "type": "viewer", "text": "\nCreate color map from linear mapping segments\n\nsegmentdata argument is a dictionary with a red, green and blue entries. Each\nentry should be a list of x, y0, y1 tuples, forming rows in a table. Entries\nfor alpha are optional.\n\nExample: suppose you want red to increase from 0 to 1 over the bottom half,\ngreen to do the same over the middle half, and blue over the top half. Then\nyou would use:\n\nEach row in the table for a given color is a sequence of x, y0, y1 tuples. In\neach sequence, x must increase monotonically from 0 to 1. For any input value\nz falling between x[i] and x[i+1], the output value of a given color will be\nlinearly interpolated between y1[i] and y0[i+1]:\n\nHence y0 in the first row and y1 in the last row are never used.\n\nSee also\n\nStatic method; factory function for generating a smoothly-varying\nLinearSegmentedColormap.\n\nFor information about making a mapping array.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.utils.new_plot()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.new_plot", "type": "viewer", "text": "\nReturn new figure and axes.\n\nQt widget that displays the plot objects. If None, you must manually call\n`canvas.setParent` and pass the parent widget.\n\nKeyword arguments passed `matplotlib.figure.Figure.add_subplot`.\n\nKeyword arguments passed `matplotlib.figure.Figure`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.utils.RequiredAttr", "path": "api/skimage.viewer.utils#skimage.viewer.utils.RequiredAttr", "type": "viewer", "text": "\nBases: `object`\n\nA class attribute that must be set before use.\n\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.utils.RequiredAttr.instances", "path": "api/skimage.viewer.utils#skimage.viewer.utils.RequiredAttr.instances", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.utils.RequiredAttr.__init__()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.RequiredAttr.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.utils.start_qtapp()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.start_qtapp", "type": "viewer", "text": "\nStart Qt mainloop\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.utils.update_axes_image()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.update_axes_image", "type": "viewer", "text": "\nUpdate the image displayed by an image plot.\n\nThis sets the image plot\u2019s array and updates its shape appropriately\n\nImage axes to update.\n\nImage array.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.viewers", "path": "api/skimage.viewer.viewers", "type": "viewer", "text": "\n`skimage.viewer.viewers.CollectionViewer`(\u2026)\n\nViewer for displaying image collections.\n\n`skimage.viewer.viewers.ImageViewer`(image[, \u2026])\n\nViewer for displaying images.\n\n`skimage.viewer.viewers.core`\n\nImageViewer class for viewing and interacting with images.\n\nBases: `skimage.viewer.viewers.core.ImageViewer`\n\nViewer for displaying image collections.\n\nSelect the displayed frame of the image collection using the slider or with\nthe following keyboard shortcuts:\n\nPrevious/next image in collection.\n\n0% to 90% of collection. For example, \u201c5\u201d goes to the image in the middle\n(i.e. 50%) of the collection.\n\nFirst/last image in collection.\n\nList of images to be displayed.\n\nControl whether image is updated on slide or release of the image slider.\nUsing \u2018on_release\u2019 will give smoother behavior when displaying large images or\nwhen writing a plugin/subclass that requires heavy computation.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nSelect image on display using index into image collection.\n\nBases: `object`\n\nViewer for displaying images.\n\nThis viewer is a simple container object that holds a Matplotlib axes for\nshowing images. `ImageViewer` doesn\u2019t subclass the Matplotlib axes (or figure)\nbecause of the high probability of name collisions.\n\nSubclasses and plugins will likely extend the `update_image` method to add\ncustom overlays or filter the displayed image.\n\nImage being viewed.\n\nMatplotlib canvas, figure, and axes used to display image.\n\nImage being viewed. Setting this value will update the displayed frame.\n\nPlugins typically operate on (but don\u2019t change) the original image.\n\nList of attached plugins.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nConnect callback function to matplotlib event and return id.\n\nDisconnect callback by its id (returned by `connect_event`).\n\nOpen image file and display in viewer.\n\nSave current image to file.\n\nThe current behavior is not ideal: It saves the image displayed on screen, so\nall images will be converted to RGB, and the image size is not preserved\n(resizing the viewer window will alter the size of the saved image).\n\nShow ImageViewer and attached plugins.\n\nThis behaves much like `matplotlib.pyplot.show` and `QWidget.show`.\n\nUpdate displayed image.\n\nThis method can be overridden or extended in subclasses and plugins to react\nto image changes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.viewers.CollectionViewer", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.CollectionViewer", "type": "viewer", "text": "\nBases: `skimage.viewer.viewers.core.ImageViewer`\n\nViewer for displaying image collections.\n\nSelect the displayed frame of the image collection using the slider or with\nthe following keyboard shortcuts:\n\nPrevious/next image in collection.\n\n0% to 90% of collection. For example, \u201c5\u201d goes to the image in the middle\n(i.e. 50%) of the collection.\n\nFirst/last image in collection.\n\nList of images to be displayed.\n\nControl whether image is updated on slide or release of the image slider.\nUsing \u2018on_release\u2019 will give smoother behavior when displaying large images or\nwhen writing a plugin/subclass that requires heavy computation.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nSelect image on display using index into image collection.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.viewers.CollectionViewer.keyPressEvent()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.CollectionViewer.keyPressEvent", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.viewers.CollectionViewer.update_index()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.CollectionViewer.update_index", "type": "viewer", "text": "\nSelect image on display using index into image collection.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.viewers.CollectionViewer.__init__()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.CollectionViewer.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.viewers.ImageViewer", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer", "type": "viewer", "text": "\nBases: `object`\n\nViewer for displaying images.\n\nThis viewer is a simple container object that holds a Matplotlib axes for\nshowing images. `ImageViewer` doesn\u2019t subclass the Matplotlib axes (or figure)\nbecause of the high probability of name collisions.\n\nSubclasses and plugins will likely extend the `update_image` method to add\ncustom overlays or filter the displayed image.\n\nImage being viewed.\n\nMatplotlib canvas, figure, and axes used to display image.\n\nImage being viewed. Setting this value will update the displayed frame.\n\nPlugins typically operate on (but don\u2019t change) the original image.\n\nList of attached plugins.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nConnect callback function to matplotlib event and return id.\n\nDisconnect callback by its id (returned by `connect_event`).\n\nOpen image file and display in viewer.\n\nSave current image to file.\n\nThe current behavior is not ideal: It saves the image displayed on screen, so\nall images will be converted to RGB, and the image size is not preserved\n(resizing the viewer window will alter the size of the saved image).\n\nShow ImageViewer and attached plugins.\n\nThis behaves much like `matplotlib.pyplot.show` and `QWidget.show`.\n\nUpdate displayed image.\n\nThis method can be overridden or extended in subclasses and plugins to react\nto image changes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.viewers.ImageViewer.add_tool()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.add_tool", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.viewers.ImageViewer.closeEvent()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.closeEvent", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.viewers.ImageViewer.connect_event()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.connect_event", "type": "viewer", "text": "\nConnect callback function to matplotlib event and return id.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.viewers.ImageViewer.disconnect_event()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.disconnect_event", "type": "viewer", "text": "\nDisconnect callback by its id (returned by `connect_event`).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.viewers.ImageViewer.dock_areas", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.dock_areas", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.viewers.ImageViewer.image()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.image", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.viewers.ImageViewer.open_file()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.open_file", "type": "viewer", "text": "\nOpen image file and display in viewer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.viewers.ImageViewer.original_image_changed", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.original_image_changed", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.viewers.ImageViewer.redraw()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.redraw", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.viewers.ImageViewer.remove_tool()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.remove_tool", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.viewers.ImageViewer.reset_image()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.reset_image", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.viewers.ImageViewer.save_to_file()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.save_to_file", "type": "viewer", "text": "\nSave current image to file.\n\nThe current behavior is not ideal: It saves the image displayed on screen, so\nall images will be converted to RGB, and the image size is not preserved\n(resizing the viewer window will alter the size of the saved image).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.viewers.ImageViewer.show()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.show", "type": "viewer", "text": "\nShow ImageViewer and attached plugins.\n\nThis behaves much like `matplotlib.pyplot.show` and `QWidget.show`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.viewers.ImageViewer.update_image()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.update_image", "type": "viewer", "text": "\nUpdate displayed image.\n\nThis method can be overridden or extended in subclasses and plugins to react\nto image changes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.viewers.ImageViewer.__init__()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets", "path": "api/skimage.viewer.widgets", "type": "viewer", "text": "\nWidgets for interacting with ImageViewer.\n\nThese widgets should be added to a Plugin subclass using its `add_widget`\nmethod or calling:\n\non a Plugin instance. The Plugin will delegate action based on the widget\u2019s\nparameter type specified by its `ptype` attribute, which can be:\n\n`skimage.viewer.widgets.BaseWidget`(name[, \u2026])\n\n`skimage.viewer.widgets.Button`(name, callback)\n\nButton which calls callback upon click.\n\n`skimage.viewer.widgets.CheckBox`(name[, \u2026])\n\nCheckBox widget\n\n`skimage.viewer.widgets.ComboBox`(name, items)\n\nComboBox widget for selecting among a list of choices.\n\n`skimage.viewer.widgets.OKCancelButtons`([\u2026])\n\nButtons that close the parent plugin.\n\n`skimage.viewer.widgets.SaveButtons`([name, \u2026])\n\nButtons to save image to io.stack or to a file.\n\n`skimage.viewer.widgets.Slider`(name[, low, \u2026])\n\nSlider widget for adjusting numeric parameters.\n\n`skimage.viewer.widgets.Text`([name, text])\n\n`skimage.viewer.widgets.core`\n\n`skimage.viewer.widgets.history`\n\nBases: `object`\n\nInitialize self. See help(type(self)) for accurate signature.\n\nBases: `skimage.viewer.widgets.core.BaseWidget`\n\nButton which calls callback upon click.\n\nName of button.\n\nFunction to call when button is clicked.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nBases: `skimage.viewer.widgets.core.BaseWidget`\n\nCheckBox widget\n\nName of CheckBox parameter. If this parameter is passed as a keyword argument,\nit must match the name of that keyword argument (spaces are replaced with\nunderscores). In addition, this name is displayed as the name of the CheckBox.\n\nInitial state of the CheckBox.\n\nCheckbox alignment\n\nParameter type\n\nCallback function called in response to checkbox changes. Note: This function\nis typically set (overridden) when the widget is added to a plugin.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nBases: `skimage.viewer.widgets.core.BaseWidget`\n\nComboBox widget for selecting among a list of choices.\n\nName of ComboBox parameter. If this parameter is passed as a keyword argument,\nit must match the name of that keyword argument (spaces are replaced with\nunderscores). In addition, this name is displayed as the name of the ComboBox.\n\nAllowed parameter values.\n\nParameter type.\n\nCallback function called in response to combobox changes. Note: This function\nis typically set (overridden) when the widget is added to a plugin.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nBases: `skimage.viewer.widgets.core.BaseWidget`\n\nButtons that close the parent plugin.\n\nOK will replace the original image with the current (filtered) image. Cancel\nwill just close the plugin.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nBases: `skimage.viewer.widgets.core.BaseWidget`\n\nButtons to save image to io.stack or to a file.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nBases: `skimage.viewer.widgets.core.BaseWidget`\n\nSlider widget for adjusting numeric parameters.\n\nName of slider parameter. If this parameter is passed as a keyword argument,\nit must match the name of that keyword argument (spaces are replaced with\nunderscores). In addition, this name is displayed as the name of the slider.\n\nRange of slider values.\n\nDefault slider value. If None, use midpoint between `low` and `high`.\n\nNumeric type of slider value.\n\nParameter type.\n\nCallback function called in response to slider changes. Note: This function is\ntypically set (overridden) when the widget is added to a plugin.\n\nSlider orientation.\n\nControl when callback function is called: on slider move or release.\n\nInitialize self. See help(type(self)) for accurate signature.\n\nBases: `skimage.viewer.widgets.core.BaseWidget`\n\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.BaseWidget", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.BaseWidget", "type": "viewer", "text": "\nBases: `object`\n\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.BaseWidget.plugin", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.BaseWidget.plugin", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.BaseWidget.val()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.BaseWidget.val", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.BaseWidget.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.BaseWidget.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.Button", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Button", "type": "viewer", "text": "\nBases: `skimage.viewer.widgets.core.BaseWidget`\n\nButton which calls callback upon click.\n\nName of button.\n\nFunction to call when button is clicked.\n\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.Button.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Button.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.CheckBox", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.CheckBox", "type": "viewer", "text": "\nBases: `skimage.viewer.widgets.core.BaseWidget`\n\nCheckBox widget\n\nName of CheckBox parameter. If this parameter is passed as a keyword argument,\nit must match the name of that keyword argument (spaces are replaced with\nunderscores). In addition, this name is displayed as the name of the CheckBox.\n\nInitial state of the CheckBox.\n\nCheckbox alignment\n\nParameter type\n\nCallback function called in response to checkbox changes. Note: This function\nis typically set (overridden) when the widget is added to a plugin.\n\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.CheckBox.val()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.CheckBox.val", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.CheckBox.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.CheckBox.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.ComboBox", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.ComboBox", "type": "viewer", "text": "\nBases: `skimage.viewer.widgets.core.BaseWidget`\n\nComboBox widget for selecting among a list of choices.\n\nName of ComboBox parameter. If this parameter is passed as a keyword argument,\nit must match the name of that keyword argument (spaces are replaced with\nunderscores). In addition, this name is displayed as the name of the ComboBox.\n\nAllowed parameter values.\n\nParameter type.\n\nCallback function called in response to combobox changes. Note: This function\nis typically set (overridden) when the widget is added to a plugin.\n\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.ComboBox.index()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.ComboBox.index", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.ComboBox.val()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.ComboBox.val", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.ComboBox.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.ComboBox.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.OKCancelButtons", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.OKCancelButtons", "type": "viewer", "text": "\nBases: `skimage.viewer.widgets.core.BaseWidget`\n\nButtons that close the parent plugin.\n\nOK will replace the original image with the current (filtered) image. Cancel\nwill just close the plugin.\n\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.OKCancelButtons.close_plugin()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.OKCancelButtons.close_plugin", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.OKCancelButtons.update_original_image()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.OKCancelButtons.update_original_image", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.OKCancelButtons.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.OKCancelButtons.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.SaveButtons", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.SaveButtons", "type": "viewer", "text": "\nBases: `skimage.viewer.widgets.core.BaseWidget`\n\nButtons to save image to io.stack or to a file.\n\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.SaveButtons.save_to_file()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.SaveButtons.save_to_file", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.SaveButtons.save_to_stack()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.SaveButtons.save_to_stack", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.SaveButtons.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.SaveButtons.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.Slider", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Slider", "type": "viewer", "text": "\nBases: `skimage.viewer.widgets.core.BaseWidget`\n\nSlider widget for adjusting numeric parameters.\n\nName of slider parameter. If this parameter is passed as a keyword argument,\nit must match the name of that keyword argument (spaces are replaced with\nunderscores). In addition, this name is displayed as the name of the slider.\n\nRange of slider values.\n\nDefault slider value. If None, use midpoint between `low` and `high`.\n\nNumeric type of slider value.\n\nParameter type.\n\nCallback function called in response to slider changes. Note: This function is\ntypically set (overridden) when the widget is added to a plugin.\n\nSlider orientation.\n\nControl when callback function is called: on slider move or release.\n\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.Slider.val()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Slider.val", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.Slider.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Slider.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.Text", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Text", "type": "viewer", "text": "\nBases: `skimage.viewer.widgets.core.BaseWidget`\n\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.Text.text()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Text.text", "type": "viewer", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "viewer.widgets.Text.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Text.__init__", "type": "viewer", "text": "\nInitialize self. See help(type(self)) for accurate signature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}]