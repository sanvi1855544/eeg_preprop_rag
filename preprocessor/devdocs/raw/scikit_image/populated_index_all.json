[{"name": "A crash course on NumPy for images", "path": "user_guide/numpy_images", "type": "Guide", "text": "A crash course on NumPy for images Images in scikit-image are represented by NumPy ndarrays. Hence, many common operations can be achieved using standard NumPy methods for manipulating arrays: >>> from skimage import data\n>>> camera = data.camera()\n>>> type(camera)\n<type 'numpy.ndarray'>\n Retrieving the geometry of the image and the number of pixels: >>> camera.shape\n(512, 512)\n>>> camera.size\n262144\n Retrieving statistical information about image intensity values: >>> camera.min(), camera.max()\n(0, 255)\n>>> camera.mean()\n118.31400299072266\n NumPy arrays representing images can be of different integer or float numerical types. See Image data types and what they mean for more information about these types and how scikit-image treats them. NumPy indexing NumPy indexing can be used both for looking at the pixel values and to modify them: >>> # Get the value of the pixel at the 10th row and 20th column\n>>> camera[10, 20]\n153\n>>> # Set to black the pixel at the 3rd row and 10th column\n>>> camera[3, 10] = 0\n Be careful! In NumPy indexing, the first dimension (camera.shape[0]) corresponds to rows, while the second (camera.shape[1]) corresponds to columns, with the origin (camera[0, 0]) at the top-left corner. This matches matrix/linear algebra notation, but is in contrast to Cartesian (x, y) coordinates. See Coordinate conventions below for more details. Beyond individual pixels, it is possible to access/modify values of whole sets of pixels using the different indexing capabilities of NumPy. Slicing: >>> # Set the first ten lines to \"black\" (0)\n>>> camera[:10] = 0\n Masking (indexing with masks of booleans): >>> mask = camera < 87\n>>> # Set to \"white\" (255) the pixels where mask is True\n>>> camera[mask] = 255\n Fancy indexing (indexing with sets of indices): >>> inds_r = np.arange(len(camera))\n>>> inds_c = 4 * inds_r % len(camera)\n>>> camera[inds_r, inds_c] = 0\n Masks are very useful when you need to select a set of pixels on which to perform the manipulations. The mask can be any boolean array of the same shape as the image (or a shape broadcastable to the image shape). This can be used to define a region of interest, for example, a disk: >>> nrows, ncols = camera.shape\n>>> row, col = np.ogrid[:nrows, :ncols]\n>>> cnt_row, cnt_col = nrows / 2, ncols / 2\n>>> outer_disk_mask = ((row - cnt_row)**2 + (col - cnt_col)**2 >\n...                    (nrows / 2)**2)\n>>> camera[outer_disk_mask] = 0\n  Boolean operations from NumPy can be used to define even more complex masks: >>> lower_half = row > cnt_row\n>>> lower_half_disk = np.logical_and(lower_half, outer_disk_mask)\n>>> camera = data.camera()\n>>> camera[lower_half_disk] = 0\n Color images All of the above remains true for color images. A color image is a NumPy array with an additional trailing dimension for the channels: >>> cat = data.chelsea()\n>>> type(cat)\n<type 'numpy.ndarray'>\n>>> cat.shape\n(300, 451, 3)\n This shows that cat is a 300-by-451 pixel image with three channels (red, green, and blue). As before, we can get and set the pixel values: >>> cat[10, 20]\narray([151, 129, 115], dtype=uint8)\n>>> # Set the pixel at (50th row, 60th column) to \"black\"\n>>> cat[50, 60] = 0\n>>> # set the pixel at (50th row, 61st column) to \"green\"\n>>> cat[50, 61] = [0, 255, 0]  # [red, green, blue]\n We can also use 2D boolean masks for 2D multichannel images, as we did with the grayscale image above: Using a 2D mask on a 2D color image >>> from skimage import data\n>>> cat = data.chelsea()\n>>> reddish = cat[:, :, 0] > 160\n>>> cat[reddish] = [0, 255, 0]\n>>> plt.imshow(cat)\n (Source code, png, pdf)    Coordinate conventions Because scikit-image represents images using NumPy arrays, the coordinate conventions must match. Two-dimensional (2D) grayscale images (such as camera above) are indexed by rows and columns (abbreviated to either (row, col) or (r, c)), with the lowest element (0, 0) at the top-left corner. In various parts of the library, you will also see rr and cc refer to lists of row and column coordinates. We distinguish this convention from (x, y), which commonly denote standard Cartesian coordinates, where x is the horizontal coordinate, y - the vertical one, and the origin is at the bottom left (Matplotlib axes, for example, use this convention). In the case of multichannel images, the last dimension is used for color channels and is denoted by channel or ch. Finally, for volumetric (3D) images, such as videos, magnetic resonance imaging (MRI) scans, confocal microscopy, etc. we refer to the leading dimension as plane, abbreviated as pln or p. These conventions are summarized below:  Dimension name and order conventions in scikit-image  \nImage type Coordinates   \n2D grayscale (row, col)  \n2D multichannel (eg. RGB) (row, col, ch)  \n3D grayscale (pln, row, col)  \n3D multichannel (pln, row, col, ch)   Many functions in scikit-image can operate on 3D images directly: >>> im3d = np.random.rand(100, 1000, 1000)\n>>> from skimage import morphology\n>>> from scipy import ndimage as ndi\n>>> seeds = ndi.label(im3d < 0.1)[0]\n>>> ws = morphology.watershed(im3d, seeds)\n In many cases, however, the third spatial dimension has lower resolution than the other two. Some scikit-image functions provide a spacing keyword argument to help handle this kind of data: >>> from skimage import segmentation\n>>> slics = segmentation.slic(im3d, spacing=[5, 1, 1], multichannel=False)\n Other times, the processing must be done plane-wise. When planes are stacked along the leading dimension (in agreement with our convention), the following syntax can be used: >>> from skimage import filters\n>>> edges = np.empty_like(im3d)\n>>> for pln, image in enumerate(im3d):\n...     # Iterate over the leading dimension\n...     edges[pln] = filters.sobel(image)\n Notes on the order of array dimensions Although the labeling of the axes might seem arbitrary, it can have a significant effect on the speed of operations. This is because modern processors never retrieve just one item from memory, but rather a whole chunk of adjacent items (an operation called prefetching). Therefore, processing of elements that are next to each other in memory is faster than processing them when they are scattered, even if the number of operations is the same: >>> def in_order_multiply(arr, scalar):\n...     for plane in list(range(arr.shape[0])):\n...         arr[plane, :, :] *= scalar\n...\n>>> def out_of_order_multiply(arr, scalar):\n...     for plane in list(range(arr.shape[2])):\n...         arr[:, :, plane] *= scalar\n...\n>>> import time\n>>> im3d = np.random.rand(100, 1024, 1024)\n>>> t0 = time.time(); x = in_order_multiply(im3d, 5); t1 = time.time()\n>>> print(\"%.2f seconds\" % (t1 - t0))  \n0.14 seconds\n>>> s0 = time.time(); x = out_of_order_multiply(im3d, 5); s1 = time.time()\n>>> print(\"%.2f seconds\" % (s1 - s0))  \n1.18 seconds\n>>> print(\"Speedup: %.1fx\" % ((s1 - s0) / (t1 - t0)))  \nSpeedup: 8.6x\n When the last/rightmost dimension becomes even larger the speedup is even more dramatic. It is worth thinking about data locality when developing algorithms. In particular, scikit-image uses C-contiguous arrays by default. When using nested loops, the last/rightmost dimension of the array should be in the innermost loop of the computation. In the example above, the *= numpy operator iterates over all remaining dimensions. A note on the time dimension Although scikit-image does not currently provide functions to work specifically with time-varying 3D data, its compatibility with NumPy arrays allows us to work quite naturally with a 5D array of the shape (t, pln, row, col, ch): >>> for timepoint in image5d:  \n...     # Each timepoint is a 3D multichannel image\n...     do_something_with(timepoint)\n We can then supplement the above table as follows:  Addendum to dimension names and orders in scikit-image  \nImage type coordinates   \n2D color video (t, row, col, ch)  \n3D multichannel video (t, pln, row, col, ch)  \n"}, {"name": "color", "path": "api/skimage.color", "type": "color", "text": "Module: color  \nskimage.color.combine_stains(stains, conv_matrix) Stain to RGB color space conversion.  \nskimage.color.convert_colorspace(arr, \u2026) Convert an image array to a new color space.  \nskimage.color.deltaE_cie76(lab1, lab2) Euclidean distance between two points in Lab color space  \nskimage.color.deltaE_ciede2000(lab1, lab2[, \u2026]) Color difference as given by the CIEDE 2000 standard.  \nskimage.color.deltaE_ciede94(lab1, lab2[, \u2026]) Color difference according to CIEDE 94 standard  \nskimage.color.deltaE_cmc(lab1, lab2[, kL, kC]) Color difference from the CMC l:c standard.  \nskimage.color.gray2rgb(image[, alpha]) Create an RGB representation of a gray-level image.  \nskimage.color.gray2rgba(image[, alpha]) Create a RGBA representation of a gray-level image.  \nskimage.color.grey2rgb(image[, alpha]) Create an RGB representation of a gray-level image.  \nskimage.color.hed2rgb(hed) Haematoxylin-Eosin-DAB (HED) to RGB color space conversion.  \nskimage.color.hsv2rgb(hsv) HSV to RGB color space conversion.  \nskimage.color.lab2lch(lab) CIE-LAB to CIE-LCH color space conversion.  \nskimage.color.lab2rgb(lab[, illuminant, \u2026]) Lab to RGB color space conversion.  \nskimage.color.lab2xyz(lab[, illuminant, \u2026]) CIE-LAB to XYZcolor space conversion.  \nskimage.color.label2rgb(label[, image, \u2026]) Return an RGB image where color-coded labels are painted over the image.  \nskimage.color.lch2lab(lch) CIE-LCH to CIE-LAB color space conversion.  \nskimage.color.rgb2gray(rgb) Compute luminance of an RGB image.  \nskimage.color.rgb2grey(rgb) Compute luminance of an RGB image.  \nskimage.color.rgb2hed(rgb) RGB to Haematoxylin-Eosin-DAB (HED) color space conversion.  \nskimage.color.rgb2hsv(rgb) RGB to HSV color space conversion.  \nskimage.color.rgb2lab(rgb[, illuminant, \u2026]) Conversion from the sRGB color space (IEC 61966-2-1:1999) to the CIE Lab colorspace under the given illuminant and observer.  \nskimage.color.rgb2rgbcie(rgb) RGB to RGB CIE color space conversion.  \nskimage.color.rgb2xyz(rgb) RGB to XYZ color space conversion.  \nskimage.color.rgb2ycbcr(rgb) RGB to YCbCr color space conversion.  \nskimage.color.rgb2ydbdr(rgb) RGB to YDbDr color space conversion.  \nskimage.color.rgb2yiq(rgb) RGB to YIQ color space conversion.  \nskimage.color.rgb2ypbpr(rgb) RGB to YPbPr color space conversion.  \nskimage.color.rgb2yuv(rgb) RGB to YUV color space conversion.  \nskimage.color.rgba2rgb(rgba[, background]) RGBA to RGB conversion using alpha blending [1].  \nskimage.color.rgbcie2rgb(rgbcie) RGB CIE to RGB color space conversion.  \nskimage.color.separate_stains(rgb, conv_matrix) RGB to stain color space conversion.  \nskimage.color.xyz2lab(xyz[, illuminant, \u2026]) XYZ to CIE-LAB color space conversion.  \nskimage.color.xyz2rgb(xyz) XYZ to RGB color space conversion.  \nskimage.color.ycbcr2rgb(ycbcr) YCbCr to RGB color space conversion.  \nskimage.color.ydbdr2rgb(ydbdr) YDbDr to RGB color space conversion.  \nskimage.color.yiq2rgb(yiq) YIQ to RGB color space conversion.  \nskimage.color.ypbpr2rgb(ypbpr) YPbPr to RGB color space conversion.  \nskimage.color.yuv2rgb(yuv) YUV to RGB color space conversion.   combine_stains  \nskimage.color.combine_stains(stains, conv_matrix) [source]\n \nStain to RGB color space conversion.  Parameters \n \nstains(\u2026, 3) array_like \n\nThe image in stain color space. Final dimension denotes channels.  conv_matrix: ndarray\n\nThe stain separation matrix as described by G. Landini [1].    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf stains is not at least 2-D with shape (\u2026, 3).     Notes Stain combination matrices available in the color module and their respective colorspace:  \nrgb_from_hed: Hematoxylin + Eosin + DAB \nrgb_from_hdx: Hematoxylin + DAB \nrgb_from_fgx: Feulgen + Light Green \nrgb_from_bex: Giemsa stain : Methyl Blue + Eosin \nrgb_from_rbd: FastRed + FastBlue + DAB \nrgb_from_gdx: Methyl Green + DAB \nrgb_from_hax: Hematoxylin + AEC \nrgb_from_bro: Blue matrix Anilline Blue + Red matrix Azocarmine + Orange matrix Orange-G \nrgb_from_bpx: Methyl Blue + Ponceau Fuchsin \nrgb_from_ahx: Alcian Blue + Hematoxylin \nrgb_from_hpx: Hematoxylin + PAS  References  \n1  \nhttps://web.archive.org/web/20160624145052/http://www.mecourse.com/landinig/software/cdeconv/cdeconv.html  \n2  \nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution,\u201d Anal. Quant. Cytol. Histol., vol. 23, no. 4, pp. 291\u2013299, Aug. 2001.   Examples >>> from skimage import data\n>>> from skimage.color import (separate_stains, combine_stains,\n...                            hdx_from_rgb, rgb_from_hdx)\n>>> ihc = data.immunohistochemistry()\n>>> ihc_hdx = separate_stains(ihc, hdx_from_rgb)\n>>> ihc_rgb = combine_stains(ihc_hdx, rgb_from_hdx)\n \n convert_colorspace  \nskimage.color.convert_colorspace(arr, fromspace, tospace) [source]\n \nConvert an image array to a new color space.  Valid color spaces are:\n\n\u2018RGB\u2019, \u2018HSV\u2019, \u2018RGB CIE\u2019, \u2018XYZ\u2019, \u2018YUV\u2019, \u2018YIQ\u2019, \u2018YPbPr\u2019, \u2018YCbCr\u2019, \u2018YDbDr\u2019    Parameters \n \narr(\u2026, 3) array_like \n\nThe image to convert. Final dimension denotes channels.  \nfromspacestr \n\nThe color space to convert from. Can be specified in lower case.  \ntospacestr \n\nThe color space to convert to. Can be specified in lower case.    Returns \n \nout(\u2026, 3) ndarray \n\nThe converted image. Same dimensions as input.    Raises \n ValueError\n\nIf fromspace is not a valid color space  ValueError\n\nIf tospace is not a valid color space     Notes Conversion is performed through the \u201ccentral\u201d RGB color space, i.e. conversion from XYZ to HSV is implemented as XYZ -> RGB -> HSV instead of directly. Examples >>> from skimage import data\n>>> img = data.astronaut()\n>>> img_hsv = convert_colorspace(img, 'RGB', 'HSV')\n \n deltaE_cie76  \nskimage.color.deltaE_cie76(lab1, lab2) [source]\n \nEuclidean distance between two points in Lab color space  Parameters \n \nlab1array_like \n\nreference color (Lab colorspace)  \nlab2array_like \n\ncomparison color (Lab colorspace)    Returns \n \ndEarray_like \n\ndistance between colors lab1 and lab2     References  \n1  \nhttps://en.wikipedia.org/wiki/Color_difference  \n2  \nA. R. Robertson, \u201cThe CIE 1976 color-difference formulae,\u201d Color Res. Appl. 2, 7-11 (1977).   \n deltaE_ciede2000  \nskimage.color.deltaE_ciede2000(lab1, lab2, kL=1, kC=1, kH=1) [source]\n \nColor difference as given by the CIEDE 2000 standard. CIEDE 2000 is a major revision of CIDE94. The perceptual calibration is largely based on experience with automotive paint on smooth surfaces.  Parameters \n \nlab1array_like \n\nreference color (Lab colorspace)  \nlab2array_like \n\ncomparison color (Lab colorspace)  \nkLfloat (range), optional \n\nlightness scale factor, 1 for \u201cacceptably close\u201d; 2 for \u201cimperceptible\u201d see deltaE_cmc  \nkCfloat (range), optional \n\nchroma scale factor, usually 1  \nkHfloat (range), optional \n\nhue scale factor, usually 1    Returns \n \ndeltaEarray_like \n\nThe distance between lab1 and lab2     Notes CIEDE 2000 assumes parametric weighting factors for the lightness, chroma, and hue (kL, kC, kH respectively). These default to 1. References  \n1  \nhttps://en.wikipedia.org/wiki/Color_difference  \n2  \nhttp://www.ece.rochester.edu/~gsharma/ciede2000/ciede2000noteCRNA.pdf DOI:10.1364/AO.33.008069  \n3  \nM. Melgosa, J. Quesada, and E. Hita, \u201cUniformity of some recent color metrics tested with an accurate color-difference tolerance dataset,\u201d Appl. Opt. 33, 8069-8077 (1994).   \n deltaE_ciede94  \nskimage.color.deltaE_ciede94(lab1, lab2, kH=1, kC=1, kL=1, k1=0.045, k2=0.015) [source]\n \nColor difference according to CIEDE 94 standard Accommodates perceptual non-uniformities through the use of application specific scale factors (kH, kC, kL, k1, and k2).  Parameters \n \nlab1array_like \n\nreference color (Lab colorspace)  \nlab2array_like \n\ncomparison color (Lab colorspace)  \nkHfloat, optional \n\nHue scale  \nkCfloat, optional \n\nChroma scale  \nkLfloat, optional \n\nLightness scale  \nk1float, optional \n\nfirst scale parameter  \nk2float, optional \n\nsecond scale parameter    Returns \n \ndEarray_like \n\ncolor difference between lab1 and lab2     Notes deltaE_ciede94 is not symmetric with respect to lab1 and lab2. CIEDE94 defines the scales for the lightness, hue, and chroma in terms of the first color. Consequently, the first color should be regarded as the \u201creference\u201d color. kL, k1, k2 depend on the application and default to the values suggested for graphic arts   \nParameter Graphic Arts Textiles   \nkL 1.000 2.000  \nk1 0.045 0.048  \nk2 0.015 0.014   References  \n1  \nhttps://en.wikipedia.org/wiki/Color_difference  \n2  \nhttp://www.brucelindbloom.com/index.html?Eqn_DeltaE_CIE94.html   \n deltaE_cmc  \nskimage.color.deltaE_cmc(lab1, lab2, kL=1, kC=1) [source]\n \nColor difference from the CMC l:c standard. This color difference was developed by the Colour Measurement Committee (CMC) of the Society of Dyers and Colourists (United Kingdom). It is intended for use in the textile industry. The scale factors kL, kC set the weight given to differences in lightness and chroma relative to differences in hue. The usual values are kL=2, kC=1 for \u201cacceptability\u201d and kL=1, kC=1 for \u201cimperceptibility\u201d. Colors with dE > 1 are \u201cdifferent\u201d for the given scale factors.  Parameters \n \nlab1array_like \n\nreference color (Lab colorspace)  \nlab2array_like \n\ncomparison color (Lab colorspace)    Returns \n \ndEarray_like \n\ndistance between colors lab1 and lab2     Notes deltaE_cmc the defines the scales for the lightness, hue, and chroma in terms of the first color. Consequently deltaE_cmc(lab1, lab2) != deltaE_cmc(lab2, lab1) References  \n1  \nhttps://en.wikipedia.org/wiki/Color_difference  \n2  \nhttp://www.brucelindbloom.com/index.html?Eqn_DeltaE_CIE94.html  \n3  \nF. J. J. Clarke, R. McDonald, and B. Rigg, \u201cModification to the JPC79 colour-difference formula,\u201d J. Soc. Dyers Colour. 100, 128-132 (1984).   \n gray2rgb  \nskimage.color.gray2rgb(image, alpha=None) [source]\n \nCreate an RGB representation of a gray-level image.  Parameters \n \nimagearray_like \n\nInput image.  \nalphabool, optional \n\nEnsure that the output image has an alpha layer. If None, alpha layers are passed through but not created.    Returns \n \nrgb(\u2026, 3) ndarray \n\nRGB image. A new dimension of length 3 is added to input image.     Notes If the input is a 1-dimensional image of shape (M, ), the output will be shape (M, 3). \n Examples using skimage.color.gray2rgb\n \n  Tinting gray-scale images   gray2rgba  \nskimage.color.gray2rgba(image, alpha=None) [source]\n \nCreate a RGBA representation of a gray-level image.  Parameters \n \nimagearray_like \n\nInput image.  \nalphaarray_like, optional \n\nAlpha channel of the output image. It may be a scalar or an array that can be broadcast to image. If not specified it is set to the maximum limit corresponding to the image dtype.    Returns \n \nrgbandarray \n\nRGBA image. A new dimension of length 4 is added to input image shape.     \n grey2rgb  \nskimage.color.grey2rgb(image, alpha=None) [source]\n \nCreate an RGB representation of a gray-level image.  Parameters \n \nimagearray_like \n\nInput image.  \nalphabool, optional \n\nEnsure that the output image has an alpha layer. If None, alpha layers are passed through but not created.    Returns \n \nrgb(\u2026, 3) ndarray \n\nRGB image. A new dimension of length 3 is added to input image.     Notes If the input is a 1-dimensional image of shape (M, ), the output will be shape (M, 3). \n hed2rgb  \nskimage.color.hed2rgb(hed) [source]\n \nHaematoxylin-Eosin-DAB (HED) to RGB color space conversion.  Parameters \n \nhed(\u2026, 3) array_like \n\nThe image in the HED color space. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB. Same dimensions as input.    Raises \n ValueError\n\nIf hed is not at least 2-D with shape (\u2026, 3).     References  \n1  \nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution.,\u201d Analytical and quantitative cytology and histology / the International Academy of Cytology [and] American Society of Cytology, vol. 23, no. 4, pp. 291-9, Aug. 2001.   Examples >>> from skimage import data\n>>> from skimage.color import rgb2hed, hed2rgb\n>>> ihc = data.immunohistochemistry()\n>>> ihc_hed = rgb2hed(ihc)\n>>> ihc_rgb = hed2rgb(ihc_hed)\n \n hsv2rgb  \nskimage.color.hsv2rgb(hsv) [source]\n \nHSV to RGB color space conversion.  Parameters \n \nhsv(\u2026, 3) array_like \n\nThe image in HSV format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf hsv is not at least 2-D with shape (\u2026, 3).     Notes Conversion between RGB and HSV color spaces results in some loss of precision, due to integer arithmetic and rounding [1]. References  \n1  \nhttps://en.wikipedia.org/wiki/HSL_and_HSV   Examples >>> from skimage import data\n>>> img = data.astronaut()\n>>> img_hsv = rgb2hsv(img)\n>>> img_rgb = hsv2rgb(img_hsv)\n \n Examples using skimage.color.hsv2rgb\n \n  Tinting gray-scale images  \n\n  Flood Fill   lab2lch  \nskimage.color.lab2lch(lab) [source]\n \nCIE-LAB to CIE-LCH color space conversion. LCH is the cylindrical representation of the LAB (Cartesian) colorspace  Parameters \n \nlab(\u2026, 3) array_like \n\nThe N-D image in CIE-LAB format. The last (N+1-th) dimension must have at least 3 elements, corresponding to the L, a, and b color channels. Subsequent elements are copied.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in LCH format, in a N-D array with same shape as input lab.    Raises \n ValueError\n\nIf lch does not have at least 3 color channels (i.e. l, a, b).     Notes The Hue is expressed as an angle between (0, 2*pi) Examples >>> from skimage import data\n>>> from skimage.color import rgb2lab, lab2lch\n>>> img = data.astronaut()\n>>> img_lab = rgb2lab(img)\n>>> img_lch = lab2lch(img_lab)\n \n lab2rgb  \nskimage.color.lab2rgb(lab, illuminant='D65', observer='2') [source]\n \nLab to RGB color space conversion.  Parameters \n \nlab(\u2026, 3) array_like \n\nThe image in Lab format. Final dimension denotes channels.  \nilluminant{\u201cA\u201d, \u201cD50\u201d, \u201cD55\u201d, \u201cD65\u201d, \u201cD75\u201d, \u201cE\u201d}, optional \n\nThe name of the illuminant (the function is NOT case sensitive).  \nobserver{\u201c2\u201d, \u201c10\u201d}, optional \n\nThe aperture angle of the observer.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf lab is not at least 2-D with shape (\u2026, 3).     Notes This function uses lab2xyz and xyz2rgb. By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref=95.047, y_ref=100., z_ref=108.883. See function get_xyz_coords for a list of supported illuminants. References  \n1  \nhttps://en.wikipedia.org/wiki/Standard_illuminant   \n lab2xyz  \nskimage.color.lab2xyz(lab, illuminant='D65', observer='2') [source]\n \nCIE-LAB to XYZcolor space conversion.  Parameters \n \nlab(\u2026, 3) array_like \n\nThe image in Lab format. Final dimension denotes channels.  \nilluminant{\u201cA\u201d, \u201cD50\u201d, \u201cD55\u201d, \u201cD65\u201d, \u201cD75\u201d, \u201cE\u201d}, optional \n\nThe name of the illuminant (the function is NOT case sensitive).  \nobserver{\u201c2\u201d, \u201c10\u201d}, optional \n\nThe aperture angle of the observer.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in XYZ format. Same dimensions as input.    Raises \n ValueError\n\nIf lab is not at least 2-D with shape (\u2026, 3).  ValueError\n\nIf either the illuminant or the observer angle are not supported or unknown.  UserWarning\n\nIf any of the pixels are invalid (Z < 0).     Notes By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref = 95.047, y_ref = 100., z_ref = 108.883. See function \u2018get_xyz_coords\u2019 for a list of supported illuminants. References  \n1  \nhttp://www.easyrgb.com/index.php?X=MATH&H=07  \n2  \nhttps://en.wikipedia.org/wiki/Lab_color_space   \n label2rgb  \nskimage.color.label2rgb(label, image=None, colors=None, alpha=0.3, bg_label=-1, bg_color=(0, 0, 0), image_alpha=1, kind='overlay') [source]\n \nReturn an RGB image where color-coded labels are painted over the image.  Parameters \n \nlabelarray, shape (M, N) \n\nInteger array of labels with the same shape as image.  \nimagearray, shape (M, N, 3), optional \n\nImage used as underlay for labels. If the input is an RGB image, it\u2019s converted to grayscale before coloring.  \ncolorslist, optional \n\nList of colors. If the number of labels exceeds the number of colors, then the colors are cycled.  \nalphafloat [0, 1], optional \n\nOpacity of colorized labels. Ignored if image is None.  \nbg_labelint, optional \n\nLabel that\u2019s treated as the background. If bg_label is specified, bg_color is None, and kind is overlay, background is not painted by any colors.  \nbg_colorstr or array, optional \n\nBackground color. Must be a name in color_dict or RGB float values between [0, 1].  \nimage_alphafloat [0, 1], optional \n\nOpacity of the image.  \nkindstring, one of {\u2018overlay\u2019, \u2018avg\u2019} \n\nThe kind of color image desired. \u2018overlay\u2019 cycles over defined colors and overlays the colored labels over the original image. \u2018avg\u2019 replaces each labeled segment with its average color, for a stained-class or pastel painting appearance.    Returns \n \nresultarray of float, shape (M, N, 3) \n\nThe result of blending a cycling colormap (colors) for each distinct value in label with the image, at a certain alpha value.     \n Examples using skimage.color.label2rgb\n \n  Segment human cells (in mitosis)   lch2lab  \nskimage.color.lch2lab(lch) [source]\n \nCIE-LCH to CIE-LAB color space conversion. LCH is the cylindrical representation of the LAB (Cartesian) colorspace  Parameters \n \nlch(\u2026, 3) array_like \n\nThe N-D image in CIE-LCH format. The last (N+1-th) dimension must have at least 3 elements, corresponding to the L, a, and b color channels. Subsequent elements are copied.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in LAB format, with same shape as input lch.    Raises \n ValueError\n\nIf lch does not have at least 3 color channels (i.e. l, c, h).     Examples >>> from skimage import data\n>>> from skimage.color import rgb2lab, lch2lab\n>>> img = data.astronaut()\n>>> img_lab = rgb2lab(img)\n>>> img_lch = lab2lch(img_lab)\n>>> img_lab2 = lch2lab(img_lch)\n \n rgb2gray  \nskimage.color.rgb2gray(rgb) [source]\n \nCompute luminance of an RGB image.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \noutndarray \n\nThe luminance image - an array which is the same size as the input array, but with the channel dimension removed.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes The weights used in this conversion are calibrated for contemporary CRT phosphors: Y = 0.2125 R + 0.7154 G + 0.0721 B\n If there is an alpha channel present, it is ignored. References  \n1  \nhttp://poynton.ca/PDFs/ColorFAQ.pdf   Examples >>> from skimage.color import rgb2gray\n>>> from skimage import data\n>>> img = data.astronaut()\n>>> img_gray = rgb2gray(img)\n \n Examples using skimage.color.rgb2gray\n \n  Registration using optical flow  \n\n  Phase Unwrapping   rgb2grey  \nskimage.color.rgb2grey(rgb) [source]\n \nCompute luminance of an RGB image.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \noutndarray \n\nThe luminance image - an array which is the same size as the input array, but with the channel dimension removed.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes The weights used in this conversion are calibrated for contemporary CRT phosphors: Y = 0.2125 R + 0.7154 G + 0.0721 B\n If there is an alpha channel present, it is ignored. References  \n1  \nhttp://poynton.ca/PDFs/ColorFAQ.pdf   Examples >>> from skimage.color import rgb2gray\n>>> from skimage import data\n>>> img = data.astronaut()\n>>> img_gray = rgb2gray(img)\n \n rgb2hed  \nskimage.color.rgb2hed(rgb) [source]\n \nRGB to Haematoxylin-Eosin-DAB (HED) color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in HED format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     References  \n1  \nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution.,\u201d Analytical and quantitative cytology and histology / the International Academy of Cytology [and] American Society of Cytology, vol. 23, no. 4, pp. 291-9, Aug. 2001.   Examples >>> from skimage import data\n>>> from skimage.color import rgb2hed\n>>> ihc = data.immunohistochemistry()\n>>> ihc_hed = rgb2hed(ihc)\n \n rgb2hsv  \nskimage.color.rgb2hsv(rgb) [source]\n \nRGB to HSV color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in HSV format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes Conversion between RGB and HSV color spaces results in some loss of precision, due to integer arithmetic and rounding [1]. References  \n1  \nhttps://en.wikipedia.org/wiki/HSL_and_HSV   Examples >>> from skimage import color\n>>> from skimage import data\n>>> img = data.astronaut()\n>>> img_hsv = color.rgb2hsv(img)\n \n Examples using skimage.color.rgb2hsv\n \n  Tinting gray-scale images  \n\n  Flood Fill   rgb2lab  \nskimage.color.rgb2lab(rgb, illuminant='D65', observer='2') [source]\n \nConversion from the sRGB color space (IEC 61966-2-1:1999) to the CIE Lab colorspace under the given illuminant and observer.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.  \nilluminant{\u201cA\u201d, \u201cD50\u201d, \u201cD55\u201d, \u201cD65\u201d, \u201cD75\u201d, \u201cE\u201d}, optional \n\nThe name of the illuminant (the function is NOT case sensitive).  \nobserver{\u201c2\u201d, \u201c10\u201d}, optional \n\nThe aperture angle of the observer.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in Lab format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes RGB is a device-dependent color space so, if you use this function, be sure that the image you are analyzing has been mapped to the sRGB color space. This function uses rgb2xyz and xyz2lab. By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref=95.047, y_ref=100., z_ref=108.883. See function get_xyz_coords for a list of supported illuminants. References  \n1  \nhttps://en.wikipedia.org/wiki/Standard_illuminant   \n rgb2rgbcie  \nskimage.color.rgb2rgbcie(rgb) [source]\n \nRGB to RGB CIE color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB CIE format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     References  \n1  \nhttps://en.wikipedia.org/wiki/CIE_1931_color_space   Examples >>> from skimage import data\n>>> from skimage.color import rgb2rgbcie\n>>> img = data.astronaut()\n>>> img_rgbcie = rgb2rgbcie(img)\n \n rgb2xyz  \nskimage.color.rgb2xyz(rgb) [source]\n \nRGB to XYZ color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in XYZ format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes The CIE XYZ color space is derived from the CIE RGB color space. Note however that this function converts from sRGB. References  \n1  \nhttps://en.wikipedia.org/wiki/CIE_1931_color_space   Examples >>> from skimage import data\n>>> img = data.astronaut()\n>>> img_xyz = rgb2xyz(img)\n \n rgb2ycbcr  \nskimage.color.rgb2ycbcr(rgb) [source]\n \nRGB to YCbCr color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in YCbCr format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes Y is between 16 and 235. This is the color space commonly used by video codecs; it is sometimes incorrectly called \u201cYUV\u201d. References  \n1  \nhttps://en.wikipedia.org/wiki/YCbCr   \n rgb2ydbdr  \nskimage.color.rgb2ydbdr(rgb) [source]\n \nRGB to YDbDr color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in YDbDr format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes This is the color space commonly used by video codecs. It is also the reversible color transform in JPEG2000. References  \n1  \nhttps://en.wikipedia.org/wiki/YDbDr   \n rgb2yiq  \nskimage.color.rgb2yiq(rgb) [source]\n \nRGB to YIQ color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in YIQ format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     \n rgb2ypbpr  \nskimage.color.rgb2ypbpr(rgb) [source]\n \nRGB to YPbPr color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in YPbPr format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     References  \n1  \nhttps://en.wikipedia.org/wiki/YPbPr   \n rgb2yuv  \nskimage.color.rgb2yuv(rgb) [source]\n \nRGB to YUV color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in YUV format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes Y is between 0 and 1. Use YCbCr instead of YUV for the color space commonly used by video codecs, where Y ranges from 16 to 235. References  \n1  \nhttps://en.wikipedia.org/wiki/YUV   \n rgba2rgb  \nskimage.color.rgba2rgb(rgba, background=(1, 1, 1)) [source]\n \nRGBA to RGB conversion using alpha blending [1].  Parameters \n \nrgba(\u2026, 4) array_like \n\nThe image in RGBA format. Final dimension denotes channels.  \nbackgroundarray_like \n\nThe color of the background to blend the image with (3 floats between 0 to 1 - the RGB value of the background).    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf rgba is not at least 2-D with shape (\u2026, 4).     References  \n1(1,2)  \nhttps://en.wikipedia.org/wiki/Alpha_compositing#Alpha_blending   Examples >>> from skimage import color\n>>> from skimage import data\n>>> img_rgba = data.logo()\n>>> img_rgb = color.rgba2rgb(img_rgba)\n \n rgbcie2rgb  \nskimage.color.rgbcie2rgb(rgbcie) [source]\n \nRGB CIE to RGB color space conversion.  Parameters \n \nrgbcie(\u2026, 3) array_like \n\nThe image in RGB CIE format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf rgbcie is not at least 2-D with shape (\u2026, 3).     References  \n1  \nhttps://en.wikipedia.org/wiki/CIE_1931_color_space   Examples >>> from skimage import data\n>>> from skimage.color import rgb2rgbcie, rgbcie2rgb\n>>> img = data.astronaut()\n>>> img_rgbcie = rgb2rgbcie(img)\n>>> img_rgb = rgbcie2rgb(img_rgbcie)\n \n separate_stains  \nskimage.color.separate_stains(rgb, conv_matrix) [source]\n \nRGB to stain color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.  conv_matrix: ndarray\n\nThe stain separation matrix as described by G. Landini [1].    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in stain color space. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes Stain separation matrices available in the color module and their respective colorspace:  \nhed_from_rgb: Hematoxylin + Eosin + DAB \nhdx_from_rgb: Hematoxylin + DAB \nfgx_from_rgb: Feulgen + Light Green \nbex_from_rgb: Giemsa stain : Methyl Blue + Eosin \nrbd_from_rgb: FastRed + FastBlue + DAB \ngdx_from_rgb: Methyl Green + DAB \nhax_from_rgb: Hematoxylin + AEC \nbro_from_rgb: Blue matrix Anilline Blue + Red matrix Azocarmine + Orange matrix Orange-G \nbpx_from_rgb: Methyl Blue + Ponceau Fuchsin \nahx_from_rgb: Alcian Blue + Hematoxylin \nhpx_from_rgb: Hematoxylin + PAS  This implementation borrows some ideas from DIPlib [2], e.g. the compensation using a small value to avoid log artifacts when calculating the Beer-Lambert law. References  \n1  \nhttps://web.archive.org/web/20160624145052/http://www.mecourse.com/landinig/software/cdeconv/cdeconv.html  \n2  \nhttps://github.com/DIPlib/diplib/  \n3  \nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution,\u201d Anal. Quant. Cytol. Histol., vol. 23, no. 4, pp. 291\u2013299, Aug. 2001.   Examples >>> from skimage import data\n>>> from skimage.color import separate_stains, hdx_from_rgb\n>>> ihc = data.immunohistochemistry()\n>>> ihc_hdx = separate_stains(ihc, hdx_from_rgb)\n \n xyz2lab  \nskimage.color.xyz2lab(xyz, illuminant='D65', observer='2') [source]\n \nXYZ to CIE-LAB color space conversion.  Parameters \n \nxyz(\u2026, 3) array_like \n\nThe image in XYZ format. Final dimension denotes channels.  \nilluminant{\u201cA\u201d, \u201cD50\u201d, \u201cD55\u201d, \u201cD65\u201d, \u201cD75\u201d, \u201cE\u201d}, optional \n\nThe name of the illuminant (the function is NOT case sensitive).  \nobserver{\u201c2\u201d, \u201c10\u201d}, optional \n\nThe aperture angle of the observer.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in CIE-LAB format. Same dimensions as input.    Raises \n ValueError\n\nIf xyz is not at least 2-D with shape (\u2026, 3).  ValueError\n\nIf either the illuminant or the observer angle is unsupported or unknown.     Notes By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref=95.047, y_ref=100., z_ref=108.883. See function get_xyz_coords for a list of supported illuminants. References  \n1  \nhttp://www.easyrgb.com/index.php?X=MATH&H=07  \n2  \nhttps://en.wikipedia.org/wiki/Lab_color_space   Examples >>> from skimage import data\n>>> from skimage.color import rgb2xyz, xyz2lab\n>>> img = data.astronaut()\n>>> img_xyz = rgb2xyz(img)\n>>> img_lab = xyz2lab(img_xyz)\n \n xyz2rgb  \nskimage.color.xyz2rgb(xyz) [source]\n \nXYZ to RGB color space conversion.  Parameters \n \nxyz(\u2026, 3) array_like \n\nThe image in XYZ format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf xyz is not at least 2-D with shape (\u2026, 3).     Notes The CIE XYZ color space is derived from the CIE RGB color space. Note however that this function converts to sRGB. References  \n1  \nhttps://en.wikipedia.org/wiki/CIE_1931_color_space   Examples >>> from skimage import data\n>>> from skimage.color import rgb2xyz, xyz2rgb\n>>> img = data.astronaut()\n>>> img_xyz = rgb2xyz(img)\n>>> img_rgb = xyz2rgb(img_xyz)\n \n ycbcr2rgb  \nskimage.color.ycbcr2rgb(ycbcr) [source]\n \nYCbCr to RGB color space conversion.  Parameters \n \nycbcr(\u2026, 3) array_like \n\nThe image in YCbCr format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf ycbcr is not at least 2-D with shape (\u2026, 3).     Notes Y is between 16 and 235. This is the color space commonly used by video codecs; it is sometimes incorrectly called \u201cYUV\u201d. References  \n1  \nhttps://en.wikipedia.org/wiki/YCbCr   \n ydbdr2rgb  \nskimage.color.ydbdr2rgb(ydbdr) [source]\n \nYDbDr to RGB color space conversion.  Parameters \n \nydbdr(\u2026, 3) array_like \n\nThe image in YDbDr format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf ydbdr is not at least 2-D with shape (\u2026, 3).     Notes This is the color space commonly used by video codecs, also called the reversible color transform in JPEG2000. References  \n1  \nhttps://en.wikipedia.org/wiki/YDbDr   \n yiq2rgb  \nskimage.color.yiq2rgb(yiq) [source]\n \nYIQ to RGB color space conversion.  Parameters \n \nyiq(\u2026, 3) array_like \n\nThe image in YIQ format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf yiq is not at least 2-D with shape (\u2026, 3).     \n ypbpr2rgb  \nskimage.color.ypbpr2rgb(ypbpr) [source]\n \nYPbPr to RGB color space conversion.  Parameters \n \nypbpr(\u2026, 3) array_like \n\nThe image in YPbPr format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf ypbpr is not at least 2-D with shape (\u2026, 3).     References  \n1  \nhttps://en.wikipedia.org/wiki/YPbPr   \n yuv2rgb  \nskimage.color.yuv2rgb(yuv) [source]\n \nYUV to RGB color space conversion.  Parameters \n \nyuv(\u2026, 3) array_like \n\nThe image in YUV format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf yuv is not at least 2-D with shape (\u2026, 3).     References  \n1  \nhttps://en.wikipedia.org/wiki/YUV   \n\n"}, {"name": "color.combine_stains()", "path": "api/skimage.color#skimage.color.combine_stains", "type": "color", "text": " \nskimage.color.combine_stains(stains, conv_matrix) [source]\n \nStain to RGB color space conversion.  Parameters \n \nstains(\u2026, 3) array_like \n\nThe image in stain color space. Final dimension denotes channels.  conv_matrix: ndarray\n\nThe stain separation matrix as described by G. Landini [1].    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf stains is not at least 2-D with shape (\u2026, 3).     Notes Stain combination matrices available in the color module and their respective colorspace:  \nrgb_from_hed: Hematoxylin + Eosin + DAB \nrgb_from_hdx: Hematoxylin + DAB \nrgb_from_fgx: Feulgen + Light Green \nrgb_from_bex: Giemsa stain : Methyl Blue + Eosin \nrgb_from_rbd: FastRed + FastBlue + DAB \nrgb_from_gdx: Methyl Green + DAB \nrgb_from_hax: Hematoxylin + AEC \nrgb_from_bro: Blue matrix Anilline Blue + Red matrix Azocarmine + Orange matrix Orange-G \nrgb_from_bpx: Methyl Blue + Ponceau Fuchsin \nrgb_from_ahx: Alcian Blue + Hematoxylin \nrgb_from_hpx: Hematoxylin + PAS  References  \n1  \nhttps://web.archive.org/web/20160624145052/http://www.mecourse.com/landinig/software/cdeconv/cdeconv.html  \n2  \nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution,\u201d Anal. Quant. Cytol. Histol., vol. 23, no. 4, pp. 291\u2013299, Aug. 2001.   Examples >>> from skimage import data\n>>> from skimage.color import (separate_stains, combine_stains,\n...                            hdx_from_rgb, rgb_from_hdx)\n>>> ihc = data.immunohistochemistry()\n>>> ihc_hdx = separate_stains(ihc, hdx_from_rgb)\n>>> ihc_rgb = combine_stains(ihc_hdx, rgb_from_hdx)\n \n"}, {"name": "color.convert_colorspace()", "path": "api/skimage.color#skimage.color.convert_colorspace", "type": "color", "text": " \nskimage.color.convert_colorspace(arr, fromspace, tospace) [source]\n \nConvert an image array to a new color space.  Valid color spaces are:\n\n\u2018RGB\u2019, \u2018HSV\u2019, \u2018RGB CIE\u2019, \u2018XYZ\u2019, \u2018YUV\u2019, \u2018YIQ\u2019, \u2018YPbPr\u2019, \u2018YCbCr\u2019, \u2018YDbDr\u2019    Parameters \n \narr(\u2026, 3) array_like \n\nThe image to convert. Final dimension denotes channels.  \nfromspacestr \n\nThe color space to convert from. Can be specified in lower case.  \ntospacestr \n\nThe color space to convert to. Can be specified in lower case.    Returns \n \nout(\u2026, 3) ndarray \n\nThe converted image. Same dimensions as input.    Raises \n ValueError\n\nIf fromspace is not a valid color space  ValueError\n\nIf tospace is not a valid color space     Notes Conversion is performed through the \u201ccentral\u201d RGB color space, i.e. conversion from XYZ to HSV is implemented as XYZ -> RGB -> HSV instead of directly. Examples >>> from skimage import data\n>>> img = data.astronaut()\n>>> img_hsv = convert_colorspace(img, 'RGB', 'HSV')\n \n"}, {"name": "color.deltaE_cie76()", "path": "api/skimage.color#skimage.color.deltaE_cie76", "type": "color", "text": " \nskimage.color.deltaE_cie76(lab1, lab2) [source]\n \nEuclidean distance between two points in Lab color space  Parameters \n \nlab1array_like \n\nreference color (Lab colorspace)  \nlab2array_like \n\ncomparison color (Lab colorspace)    Returns \n \ndEarray_like \n\ndistance between colors lab1 and lab2     References  \n1  \nhttps://en.wikipedia.org/wiki/Color_difference  \n2  \nA. R. Robertson, \u201cThe CIE 1976 color-difference formulae,\u201d Color Res. Appl. 2, 7-11 (1977).   \n"}, {"name": "color.deltaE_ciede2000()", "path": "api/skimage.color#skimage.color.deltaE_ciede2000", "type": "color", "text": " \nskimage.color.deltaE_ciede2000(lab1, lab2, kL=1, kC=1, kH=1) [source]\n \nColor difference as given by the CIEDE 2000 standard. CIEDE 2000 is a major revision of CIDE94. The perceptual calibration is largely based on experience with automotive paint on smooth surfaces.  Parameters \n \nlab1array_like \n\nreference color (Lab colorspace)  \nlab2array_like \n\ncomparison color (Lab colorspace)  \nkLfloat (range), optional \n\nlightness scale factor, 1 for \u201cacceptably close\u201d; 2 for \u201cimperceptible\u201d see deltaE_cmc  \nkCfloat (range), optional \n\nchroma scale factor, usually 1  \nkHfloat (range), optional \n\nhue scale factor, usually 1    Returns \n \ndeltaEarray_like \n\nThe distance between lab1 and lab2     Notes CIEDE 2000 assumes parametric weighting factors for the lightness, chroma, and hue (kL, kC, kH respectively). These default to 1. References  \n1  \nhttps://en.wikipedia.org/wiki/Color_difference  \n2  \nhttp://www.ece.rochester.edu/~gsharma/ciede2000/ciede2000noteCRNA.pdf DOI:10.1364/AO.33.008069  \n3  \nM. Melgosa, J. Quesada, and E. Hita, \u201cUniformity of some recent color metrics tested with an accurate color-difference tolerance dataset,\u201d Appl. Opt. 33, 8069-8077 (1994).   \n"}, {"name": "color.deltaE_ciede94()", "path": "api/skimage.color#skimage.color.deltaE_ciede94", "type": "color", "text": " \nskimage.color.deltaE_ciede94(lab1, lab2, kH=1, kC=1, kL=1, k1=0.045, k2=0.015) [source]\n \nColor difference according to CIEDE 94 standard Accommodates perceptual non-uniformities through the use of application specific scale factors (kH, kC, kL, k1, and k2).  Parameters \n \nlab1array_like \n\nreference color (Lab colorspace)  \nlab2array_like \n\ncomparison color (Lab colorspace)  \nkHfloat, optional \n\nHue scale  \nkCfloat, optional \n\nChroma scale  \nkLfloat, optional \n\nLightness scale  \nk1float, optional \n\nfirst scale parameter  \nk2float, optional \n\nsecond scale parameter    Returns \n \ndEarray_like \n\ncolor difference between lab1 and lab2     Notes deltaE_ciede94 is not symmetric with respect to lab1 and lab2. CIEDE94 defines the scales for the lightness, hue, and chroma in terms of the first color. Consequently, the first color should be regarded as the \u201creference\u201d color. kL, k1, k2 depend on the application and default to the values suggested for graphic arts   \nParameter Graphic Arts Textiles   \nkL 1.000 2.000  \nk1 0.045 0.048  \nk2 0.015 0.014   References  \n1  \nhttps://en.wikipedia.org/wiki/Color_difference  \n2  \nhttp://www.brucelindbloom.com/index.html?Eqn_DeltaE_CIE94.html   \n"}, {"name": "color.deltaE_cmc()", "path": "api/skimage.color#skimage.color.deltaE_cmc", "type": "color", "text": " \nskimage.color.deltaE_cmc(lab1, lab2, kL=1, kC=1) [source]\n \nColor difference from the CMC l:c standard. This color difference was developed by the Colour Measurement Committee (CMC) of the Society of Dyers and Colourists (United Kingdom). It is intended for use in the textile industry. The scale factors kL, kC set the weight given to differences in lightness and chroma relative to differences in hue. The usual values are kL=2, kC=1 for \u201cacceptability\u201d and kL=1, kC=1 for \u201cimperceptibility\u201d. Colors with dE > 1 are \u201cdifferent\u201d for the given scale factors.  Parameters \n \nlab1array_like \n\nreference color (Lab colorspace)  \nlab2array_like \n\ncomparison color (Lab colorspace)    Returns \n \ndEarray_like \n\ndistance between colors lab1 and lab2     Notes deltaE_cmc the defines the scales for the lightness, hue, and chroma in terms of the first color. Consequently deltaE_cmc(lab1, lab2) != deltaE_cmc(lab2, lab1) References  \n1  \nhttps://en.wikipedia.org/wiki/Color_difference  \n2  \nhttp://www.brucelindbloom.com/index.html?Eqn_DeltaE_CIE94.html  \n3  \nF. J. J. Clarke, R. McDonald, and B. Rigg, \u201cModification to the JPC79 colour-difference formula,\u201d J. Soc. Dyers Colour. 100, 128-132 (1984).   \n"}, {"name": "color.gray2rgb()", "path": "api/skimage.color#skimage.color.gray2rgb", "type": "color", "text": " \nskimage.color.gray2rgb(image, alpha=None) [source]\n \nCreate an RGB representation of a gray-level image.  Parameters \n \nimagearray_like \n\nInput image.  \nalphabool, optional \n\nEnsure that the output image has an alpha layer. If None, alpha layers are passed through but not created.    Returns \n \nrgb(\u2026, 3) ndarray \n\nRGB image. A new dimension of length 3 is added to input image.     Notes If the input is a 1-dimensional image of shape (M, ), the output will be shape (M, 3). \n"}, {"name": "color.gray2rgba()", "path": "api/skimage.color#skimage.color.gray2rgba", "type": "color", "text": " \nskimage.color.gray2rgba(image, alpha=None) [source]\n \nCreate a RGBA representation of a gray-level image.  Parameters \n \nimagearray_like \n\nInput image.  \nalphaarray_like, optional \n\nAlpha channel of the output image. It may be a scalar or an array that can be broadcast to image. If not specified it is set to the maximum limit corresponding to the image dtype.    Returns \n \nrgbandarray \n\nRGBA image. A new dimension of length 4 is added to input image shape.     \n"}, {"name": "color.grey2rgb()", "path": "api/skimage.color#skimage.color.grey2rgb", "type": "color", "text": " \nskimage.color.grey2rgb(image, alpha=None) [source]\n \nCreate an RGB representation of a gray-level image.  Parameters \n \nimagearray_like \n\nInput image.  \nalphabool, optional \n\nEnsure that the output image has an alpha layer. If None, alpha layers are passed through but not created.    Returns \n \nrgb(\u2026, 3) ndarray \n\nRGB image. A new dimension of length 3 is added to input image.     Notes If the input is a 1-dimensional image of shape (M, ), the output will be shape (M, 3). \n"}, {"name": "color.hed2rgb()", "path": "api/skimage.color#skimage.color.hed2rgb", "type": "color", "text": " \nskimage.color.hed2rgb(hed) [source]\n \nHaematoxylin-Eosin-DAB (HED) to RGB color space conversion.  Parameters \n \nhed(\u2026, 3) array_like \n\nThe image in the HED color space. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB. Same dimensions as input.    Raises \n ValueError\n\nIf hed is not at least 2-D with shape (\u2026, 3).     References  \n1  \nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution.,\u201d Analytical and quantitative cytology and histology / the International Academy of Cytology [and] American Society of Cytology, vol. 23, no. 4, pp. 291-9, Aug. 2001.   Examples >>> from skimage import data\n>>> from skimage.color import rgb2hed, hed2rgb\n>>> ihc = data.immunohistochemistry()\n>>> ihc_hed = rgb2hed(ihc)\n>>> ihc_rgb = hed2rgb(ihc_hed)\n \n"}, {"name": "color.hsv2rgb()", "path": "api/skimage.color#skimage.color.hsv2rgb", "type": "color", "text": " \nskimage.color.hsv2rgb(hsv) [source]\n \nHSV to RGB color space conversion.  Parameters \n \nhsv(\u2026, 3) array_like \n\nThe image in HSV format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf hsv is not at least 2-D with shape (\u2026, 3).     Notes Conversion between RGB and HSV color spaces results in some loss of precision, due to integer arithmetic and rounding [1]. References  \n1  \nhttps://en.wikipedia.org/wiki/HSL_and_HSV   Examples >>> from skimage import data\n>>> img = data.astronaut()\n>>> img_hsv = rgb2hsv(img)\n>>> img_rgb = hsv2rgb(img_hsv)\n \n"}, {"name": "color.lab2lch()", "path": "api/skimage.color#skimage.color.lab2lch", "type": "color", "text": " \nskimage.color.lab2lch(lab) [source]\n \nCIE-LAB to CIE-LCH color space conversion. LCH is the cylindrical representation of the LAB (Cartesian) colorspace  Parameters \n \nlab(\u2026, 3) array_like \n\nThe N-D image in CIE-LAB format. The last (N+1-th) dimension must have at least 3 elements, corresponding to the L, a, and b color channels. Subsequent elements are copied.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in LCH format, in a N-D array with same shape as input lab.    Raises \n ValueError\n\nIf lch does not have at least 3 color channels (i.e. l, a, b).     Notes The Hue is expressed as an angle between (0, 2*pi) Examples >>> from skimage import data\n>>> from skimage.color import rgb2lab, lab2lch\n>>> img = data.astronaut()\n>>> img_lab = rgb2lab(img)\n>>> img_lch = lab2lch(img_lab)\n \n"}, {"name": "color.lab2rgb()", "path": "api/skimage.color#skimage.color.lab2rgb", "type": "color", "text": " \nskimage.color.lab2rgb(lab, illuminant='D65', observer='2') [source]\n \nLab to RGB color space conversion.  Parameters \n \nlab(\u2026, 3) array_like \n\nThe image in Lab format. Final dimension denotes channels.  \nilluminant{\u201cA\u201d, \u201cD50\u201d, \u201cD55\u201d, \u201cD65\u201d, \u201cD75\u201d, \u201cE\u201d}, optional \n\nThe name of the illuminant (the function is NOT case sensitive).  \nobserver{\u201c2\u201d, \u201c10\u201d}, optional \n\nThe aperture angle of the observer.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf lab is not at least 2-D with shape (\u2026, 3).     Notes This function uses lab2xyz and xyz2rgb. By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref=95.047, y_ref=100., z_ref=108.883. See function get_xyz_coords for a list of supported illuminants. References  \n1  \nhttps://en.wikipedia.org/wiki/Standard_illuminant   \n"}, {"name": "color.lab2xyz()", "path": "api/skimage.color#skimage.color.lab2xyz", "type": "color", "text": " \nskimage.color.lab2xyz(lab, illuminant='D65', observer='2') [source]\n \nCIE-LAB to XYZcolor space conversion.  Parameters \n \nlab(\u2026, 3) array_like \n\nThe image in Lab format. Final dimension denotes channels.  \nilluminant{\u201cA\u201d, \u201cD50\u201d, \u201cD55\u201d, \u201cD65\u201d, \u201cD75\u201d, \u201cE\u201d}, optional \n\nThe name of the illuminant (the function is NOT case sensitive).  \nobserver{\u201c2\u201d, \u201c10\u201d}, optional \n\nThe aperture angle of the observer.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in XYZ format. Same dimensions as input.    Raises \n ValueError\n\nIf lab is not at least 2-D with shape (\u2026, 3).  ValueError\n\nIf either the illuminant or the observer angle are not supported or unknown.  UserWarning\n\nIf any of the pixels are invalid (Z < 0).     Notes By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref = 95.047, y_ref = 100., z_ref = 108.883. See function \u2018get_xyz_coords\u2019 for a list of supported illuminants. References  \n1  \nhttp://www.easyrgb.com/index.php?X=MATH&H=07  \n2  \nhttps://en.wikipedia.org/wiki/Lab_color_space   \n"}, {"name": "color.label2rgb()", "path": "api/skimage.color#skimage.color.label2rgb", "type": "color", "text": " \nskimage.color.label2rgb(label, image=None, colors=None, alpha=0.3, bg_label=-1, bg_color=(0, 0, 0), image_alpha=1, kind='overlay') [source]\n \nReturn an RGB image where color-coded labels are painted over the image.  Parameters \n \nlabelarray, shape (M, N) \n\nInteger array of labels with the same shape as image.  \nimagearray, shape (M, N, 3), optional \n\nImage used as underlay for labels. If the input is an RGB image, it\u2019s converted to grayscale before coloring.  \ncolorslist, optional \n\nList of colors. If the number of labels exceeds the number of colors, then the colors are cycled.  \nalphafloat [0, 1], optional \n\nOpacity of colorized labels. Ignored if image is None.  \nbg_labelint, optional \n\nLabel that\u2019s treated as the background. If bg_label is specified, bg_color is None, and kind is overlay, background is not painted by any colors.  \nbg_colorstr or array, optional \n\nBackground color. Must be a name in color_dict or RGB float values between [0, 1].  \nimage_alphafloat [0, 1], optional \n\nOpacity of the image.  \nkindstring, one of {\u2018overlay\u2019, \u2018avg\u2019} \n\nThe kind of color image desired. \u2018overlay\u2019 cycles over defined colors and overlays the colored labels over the original image. \u2018avg\u2019 replaces each labeled segment with its average color, for a stained-class or pastel painting appearance.    Returns \n \nresultarray of float, shape (M, N, 3) \n\nThe result of blending a cycling colormap (colors) for each distinct value in label with the image, at a certain alpha value.     \n"}, {"name": "color.lch2lab()", "path": "api/skimage.color#skimage.color.lch2lab", "type": "color", "text": " \nskimage.color.lch2lab(lch) [source]\n \nCIE-LCH to CIE-LAB color space conversion. LCH is the cylindrical representation of the LAB (Cartesian) colorspace  Parameters \n \nlch(\u2026, 3) array_like \n\nThe N-D image in CIE-LCH format. The last (N+1-th) dimension must have at least 3 elements, corresponding to the L, a, and b color channels. Subsequent elements are copied.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in LAB format, with same shape as input lch.    Raises \n ValueError\n\nIf lch does not have at least 3 color channels (i.e. l, c, h).     Examples >>> from skimage import data\n>>> from skimage.color import rgb2lab, lch2lab\n>>> img = data.astronaut()\n>>> img_lab = rgb2lab(img)\n>>> img_lch = lab2lch(img_lab)\n>>> img_lab2 = lch2lab(img_lch)\n \n"}, {"name": "color.rgb2gray()", "path": "api/skimage.color#skimage.color.rgb2gray", "type": "color", "text": " \nskimage.color.rgb2gray(rgb) [source]\n \nCompute luminance of an RGB image.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \noutndarray \n\nThe luminance image - an array which is the same size as the input array, but with the channel dimension removed.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes The weights used in this conversion are calibrated for contemporary CRT phosphors: Y = 0.2125 R + 0.7154 G + 0.0721 B\n If there is an alpha channel present, it is ignored. References  \n1  \nhttp://poynton.ca/PDFs/ColorFAQ.pdf   Examples >>> from skimage.color import rgb2gray\n>>> from skimage import data\n>>> img = data.astronaut()\n>>> img_gray = rgb2gray(img)\n \n"}, {"name": "color.rgb2grey()", "path": "api/skimage.color#skimage.color.rgb2grey", "type": "color", "text": " \nskimage.color.rgb2grey(rgb) [source]\n \nCompute luminance of an RGB image.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \noutndarray \n\nThe luminance image - an array which is the same size as the input array, but with the channel dimension removed.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes The weights used in this conversion are calibrated for contemporary CRT phosphors: Y = 0.2125 R + 0.7154 G + 0.0721 B\n If there is an alpha channel present, it is ignored. References  \n1  \nhttp://poynton.ca/PDFs/ColorFAQ.pdf   Examples >>> from skimage.color import rgb2gray\n>>> from skimage import data\n>>> img = data.astronaut()\n>>> img_gray = rgb2gray(img)\n \n"}, {"name": "color.rgb2hed()", "path": "api/skimage.color#skimage.color.rgb2hed", "type": "color", "text": " \nskimage.color.rgb2hed(rgb) [source]\n \nRGB to Haematoxylin-Eosin-DAB (HED) color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in HED format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     References  \n1  \nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution.,\u201d Analytical and quantitative cytology and histology / the International Academy of Cytology [and] American Society of Cytology, vol. 23, no. 4, pp. 291-9, Aug. 2001.   Examples >>> from skimage import data\n>>> from skimage.color import rgb2hed\n>>> ihc = data.immunohistochemistry()\n>>> ihc_hed = rgb2hed(ihc)\n \n"}, {"name": "color.rgb2hsv()", "path": "api/skimage.color#skimage.color.rgb2hsv", "type": "color", "text": " \nskimage.color.rgb2hsv(rgb) [source]\n \nRGB to HSV color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in HSV format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes Conversion between RGB and HSV color spaces results in some loss of precision, due to integer arithmetic and rounding [1]. References  \n1  \nhttps://en.wikipedia.org/wiki/HSL_and_HSV   Examples >>> from skimage import color\n>>> from skimage import data\n>>> img = data.astronaut()\n>>> img_hsv = color.rgb2hsv(img)\n \n"}, {"name": "color.rgb2lab()", "path": "api/skimage.color#skimage.color.rgb2lab", "type": "color", "text": " \nskimage.color.rgb2lab(rgb, illuminant='D65', observer='2') [source]\n \nConversion from the sRGB color space (IEC 61966-2-1:1999) to the CIE Lab colorspace under the given illuminant and observer.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.  \nilluminant{\u201cA\u201d, \u201cD50\u201d, \u201cD55\u201d, \u201cD65\u201d, \u201cD75\u201d, \u201cE\u201d}, optional \n\nThe name of the illuminant (the function is NOT case sensitive).  \nobserver{\u201c2\u201d, \u201c10\u201d}, optional \n\nThe aperture angle of the observer.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in Lab format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes RGB is a device-dependent color space so, if you use this function, be sure that the image you are analyzing has been mapped to the sRGB color space. This function uses rgb2xyz and xyz2lab. By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref=95.047, y_ref=100., z_ref=108.883. See function get_xyz_coords for a list of supported illuminants. References  \n1  \nhttps://en.wikipedia.org/wiki/Standard_illuminant   \n"}, {"name": "color.rgb2rgbcie()", "path": "api/skimage.color#skimage.color.rgb2rgbcie", "type": "color", "text": " \nskimage.color.rgb2rgbcie(rgb) [source]\n \nRGB to RGB CIE color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB CIE format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     References  \n1  \nhttps://en.wikipedia.org/wiki/CIE_1931_color_space   Examples >>> from skimage import data\n>>> from skimage.color import rgb2rgbcie\n>>> img = data.astronaut()\n>>> img_rgbcie = rgb2rgbcie(img)\n \n"}, {"name": "color.rgb2xyz()", "path": "api/skimage.color#skimage.color.rgb2xyz", "type": "color", "text": " \nskimage.color.rgb2xyz(rgb) [source]\n \nRGB to XYZ color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in XYZ format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes The CIE XYZ color space is derived from the CIE RGB color space. Note however that this function converts from sRGB. References  \n1  \nhttps://en.wikipedia.org/wiki/CIE_1931_color_space   Examples >>> from skimage import data\n>>> img = data.astronaut()\n>>> img_xyz = rgb2xyz(img)\n \n"}, {"name": "color.rgb2ycbcr()", "path": "api/skimage.color#skimage.color.rgb2ycbcr", "type": "color", "text": " \nskimage.color.rgb2ycbcr(rgb) [source]\n \nRGB to YCbCr color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in YCbCr format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes Y is between 16 and 235. This is the color space commonly used by video codecs; it is sometimes incorrectly called \u201cYUV\u201d. References  \n1  \nhttps://en.wikipedia.org/wiki/YCbCr   \n"}, {"name": "color.rgb2ydbdr()", "path": "api/skimage.color#skimage.color.rgb2ydbdr", "type": "color", "text": " \nskimage.color.rgb2ydbdr(rgb) [source]\n \nRGB to YDbDr color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in YDbDr format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes This is the color space commonly used by video codecs. It is also the reversible color transform in JPEG2000. References  \n1  \nhttps://en.wikipedia.org/wiki/YDbDr   \n"}, {"name": "color.rgb2yiq()", "path": "api/skimage.color#skimage.color.rgb2yiq", "type": "color", "text": " \nskimage.color.rgb2yiq(rgb) [source]\n \nRGB to YIQ color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in YIQ format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     \n"}, {"name": "color.rgb2ypbpr()", "path": "api/skimage.color#skimage.color.rgb2ypbpr", "type": "color", "text": " \nskimage.color.rgb2ypbpr(rgb) [source]\n \nRGB to YPbPr color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in YPbPr format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     References  \n1  \nhttps://en.wikipedia.org/wiki/YPbPr   \n"}, {"name": "color.rgb2yuv()", "path": "api/skimage.color#skimage.color.rgb2yuv", "type": "color", "text": " \nskimage.color.rgb2yuv(rgb) [source]\n \nRGB to YUV color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in YUV format. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes Y is between 0 and 1. Use YCbCr instead of YUV for the color space commonly used by video codecs, where Y ranges from 16 to 235. References  \n1  \nhttps://en.wikipedia.org/wiki/YUV   \n"}, {"name": "color.rgba2rgb()", "path": "api/skimage.color#skimage.color.rgba2rgb", "type": "color", "text": " \nskimage.color.rgba2rgb(rgba, background=(1, 1, 1)) [source]\n \nRGBA to RGB conversion using alpha blending [1].  Parameters \n \nrgba(\u2026, 4) array_like \n\nThe image in RGBA format. Final dimension denotes channels.  \nbackgroundarray_like \n\nThe color of the background to blend the image with (3 floats between 0 to 1 - the RGB value of the background).    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf rgba is not at least 2-D with shape (\u2026, 4).     References  \n1(1,2)  \nhttps://en.wikipedia.org/wiki/Alpha_compositing#Alpha_blending   Examples >>> from skimage import color\n>>> from skimage import data\n>>> img_rgba = data.logo()\n>>> img_rgb = color.rgba2rgb(img_rgba)\n \n"}, {"name": "color.rgbcie2rgb()", "path": "api/skimage.color#skimage.color.rgbcie2rgb", "type": "color", "text": " \nskimage.color.rgbcie2rgb(rgbcie) [source]\n \nRGB CIE to RGB color space conversion.  Parameters \n \nrgbcie(\u2026, 3) array_like \n\nThe image in RGB CIE format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf rgbcie is not at least 2-D with shape (\u2026, 3).     References  \n1  \nhttps://en.wikipedia.org/wiki/CIE_1931_color_space   Examples >>> from skimage import data\n>>> from skimage.color import rgb2rgbcie, rgbcie2rgb\n>>> img = data.astronaut()\n>>> img_rgbcie = rgb2rgbcie(img)\n>>> img_rgb = rgbcie2rgb(img_rgbcie)\n \n"}, {"name": "color.separate_stains()", "path": "api/skimage.color#skimage.color.separate_stains", "type": "color", "text": " \nskimage.color.separate_stains(rgb, conv_matrix) [source]\n \nRGB to stain color space conversion.  Parameters \n \nrgb(\u2026, 3) array_like \n\nThe image in RGB format. Final dimension denotes channels.  conv_matrix: ndarray\n\nThe stain separation matrix as described by G. Landini [1].    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in stain color space. Same dimensions as input.    Raises \n ValueError\n\nIf rgb is not at least 2-D with shape (\u2026, 3).     Notes Stain separation matrices available in the color module and their respective colorspace:  \nhed_from_rgb: Hematoxylin + Eosin + DAB \nhdx_from_rgb: Hematoxylin + DAB \nfgx_from_rgb: Feulgen + Light Green \nbex_from_rgb: Giemsa stain : Methyl Blue + Eosin \nrbd_from_rgb: FastRed + FastBlue + DAB \ngdx_from_rgb: Methyl Green + DAB \nhax_from_rgb: Hematoxylin + AEC \nbro_from_rgb: Blue matrix Anilline Blue + Red matrix Azocarmine + Orange matrix Orange-G \nbpx_from_rgb: Methyl Blue + Ponceau Fuchsin \nahx_from_rgb: Alcian Blue + Hematoxylin \nhpx_from_rgb: Hematoxylin + PAS  This implementation borrows some ideas from DIPlib [2], e.g. the compensation using a small value to avoid log artifacts when calculating the Beer-Lambert law. References  \n1  \nhttps://web.archive.org/web/20160624145052/http://www.mecourse.com/landinig/software/cdeconv/cdeconv.html  \n2  \nhttps://github.com/DIPlib/diplib/  \n3  \nA. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution,\u201d Anal. Quant. Cytol. Histol., vol. 23, no. 4, pp. 291\u2013299, Aug. 2001.   Examples >>> from skimage import data\n>>> from skimage.color import separate_stains, hdx_from_rgb\n>>> ihc = data.immunohistochemistry()\n>>> ihc_hdx = separate_stains(ihc, hdx_from_rgb)\n \n"}, {"name": "color.xyz2lab()", "path": "api/skimage.color#skimage.color.xyz2lab", "type": "color", "text": " \nskimage.color.xyz2lab(xyz, illuminant='D65', observer='2') [source]\n \nXYZ to CIE-LAB color space conversion.  Parameters \n \nxyz(\u2026, 3) array_like \n\nThe image in XYZ format. Final dimension denotes channels.  \nilluminant{\u201cA\u201d, \u201cD50\u201d, \u201cD55\u201d, \u201cD65\u201d, \u201cD75\u201d, \u201cE\u201d}, optional \n\nThe name of the illuminant (the function is NOT case sensitive).  \nobserver{\u201c2\u201d, \u201c10\u201d}, optional \n\nThe aperture angle of the observer.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in CIE-LAB format. Same dimensions as input.    Raises \n ValueError\n\nIf xyz is not at least 2-D with shape (\u2026, 3).  ValueError\n\nIf either the illuminant or the observer angle is unsupported or unknown.     Notes By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref=95.047, y_ref=100., z_ref=108.883. See function get_xyz_coords for a list of supported illuminants. References  \n1  \nhttp://www.easyrgb.com/index.php?X=MATH&H=07  \n2  \nhttps://en.wikipedia.org/wiki/Lab_color_space   Examples >>> from skimage import data\n>>> from skimage.color import rgb2xyz, xyz2lab\n>>> img = data.astronaut()\n>>> img_xyz = rgb2xyz(img)\n>>> img_lab = xyz2lab(img_xyz)\n \n"}, {"name": "color.xyz2rgb()", "path": "api/skimage.color#skimage.color.xyz2rgb", "type": "color", "text": " \nskimage.color.xyz2rgb(xyz) [source]\n \nXYZ to RGB color space conversion.  Parameters \n \nxyz(\u2026, 3) array_like \n\nThe image in XYZ format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf xyz is not at least 2-D with shape (\u2026, 3).     Notes The CIE XYZ color space is derived from the CIE RGB color space. Note however that this function converts to sRGB. References  \n1  \nhttps://en.wikipedia.org/wiki/CIE_1931_color_space   Examples >>> from skimage import data\n>>> from skimage.color import rgb2xyz, xyz2rgb\n>>> img = data.astronaut()\n>>> img_xyz = rgb2xyz(img)\n>>> img_rgb = xyz2rgb(img_xyz)\n \n"}, {"name": "color.ycbcr2rgb()", "path": "api/skimage.color#skimage.color.ycbcr2rgb", "type": "color", "text": " \nskimage.color.ycbcr2rgb(ycbcr) [source]\n \nYCbCr to RGB color space conversion.  Parameters \n \nycbcr(\u2026, 3) array_like \n\nThe image in YCbCr format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf ycbcr is not at least 2-D with shape (\u2026, 3).     Notes Y is between 16 and 235. This is the color space commonly used by video codecs; it is sometimes incorrectly called \u201cYUV\u201d. References  \n1  \nhttps://en.wikipedia.org/wiki/YCbCr   \n"}, {"name": "color.ydbdr2rgb()", "path": "api/skimage.color#skimage.color.ydbdr2rgb", "type": "color", "text": " \nskimage.color.ydbdr2rgb(ydbdr) [source]\n \nYDbDr to RGB color space conversion.  Parameters \n \nydbdr(\u2026, 3) array_like \n\nThe image in YDbDr format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf ydbdr is not at least 2-D with shape (\u2026, 3).     Notes This is the color space commonly used by video codecs, also called the reversible color transform in JPEG2000. References  \n1  \nhttps://en.wikipedia.org/wiki/YDbDr   \n"}, {"name": "color.yiq2rgb()", "path": "api/skimage.color#skimage.color.yiq2rgb", "type": "color", "text": " \nskimage.color.yiq2rgb(yiq) [source]\n \nYIQ to RGB color space conversion.  Parameters \n \nyiq(\u2026, 3) array_like \n\nThe image in YIQ format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf yiq is not at least 2-D with shape (\u2026, 3).     \n"}, {"name": "color.ypbpr2rgb()", "path": "api/skimage.color#skimage.color.ypbpr2rgb", "type": "color", "text": " \nskimage.color.ypbpr2rgb(ypbpr) [source]\n \nYPbPr to RGB color space conversion.  Parameters \n \nypbpr(\u2026, 3) array_like \n\nThe image in YPbPr format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf ypbpr is not at least 2-D with shape (\u2026, 3).     References  \n1  \nhttps://en.wikipedia.org/wiki/YPbPr   \n"}, {"name": "color.yuv2rgb()", "path": "api/skimage.color#skimage.color.yuv2rgb", "type": "color", "text": " \nskimage.color.yuv2rgb(yuv) [source]\n \nYUV to RGB color space conversion.  Parameters \n \nyuv(\u2026, 3) array_like \n\nThe image in YUV format. Final dimension denotes channels.    Returns \n \nout(\u2026, 3) ndarray \n\nThe image in RGB format. Same dimensions as input.    Raises \n ValueError\n\nIf yuv is not at least 2-D with shape (\u2026, 3).     References  \n1  \nhttps://en.wikipedia.org/wiki/YUV   \n"}, {"name": "data", "path": "api/skimage.data", "type": "data", "text": "Module: data Standard test images. For more images, see  http://sipi.usc.edu/database/database.php   \nskimage.data.astronaut() Color image of the astronaut Eileen Collins.  \nskimage.data.binary_blobs([length, \u2026]) Generate synthetic binary image with several rounded blob-like objects.  \nskimage.data.brain() Subset of data from the University of North Carolina Volume Rendering Test Data Set.  \nskimage.data.brick() Brick wall.  \nskimage.data.camera() Gray-level \u201ccamera\u201d image.  \nskimage.data.cat() Chelsea the cat.  \nskimage.data.cell() Cell floating in saline.  \nskimage.data.cells3d() 3D fluorescence microscopy image of cells.  \nskimage.data.checkerboard() Checkerboard image.  \nskimage.data.chelsea() Chelsea the cat.  \nskimage.data.clock() Motion blurred clock.  \nskimage.data.coffee() Coffee cup.  \nskimage.data.coins() Greek coins from Pompeii.  \nskimage.data.colorwheel() Color Wheel.  \nskimage.data.download_all([directory]) Download all datasets for use with scikit-image offline.  \nskimage.data.eagle() A golden eagle.  \nskimage.data.grass() Grass.  \nskimage.data.gravel() Gravel  \nskimage.data.horse() Black and white silhouette of a horse.  \nskimage.data.hubble_deep_field() Hubble eXtreme Deep Field.  \nskimage.data.human_mitosis() Image of human cells undergoing mitosis.  \nskimage.data.immunohistochemistry() Immunohistochemical (IHC) staining with hematoxylin counterstaining.  \nskimage.data.kidney() Mouse kidney tissue.  \nskimage.data.lbp_frontal_face_cascade_filename() Return the path to the XML file containing the weak classifier cascade.  \nskimage.data.lfw_subset() Subset of data from the LFW dataset.  \nskimage.data.lily() Lily of the valley plant stem.  \nskimage.data.logo() Scikit-image logo, a RGBA image.  \nskimage.data.microaneurysms() Gray-level \u201cmicroaneurysms\u201d image.  \nskimage.data.moon() Surface of the moon.  \nskimage.data.page() Scanned page.  \nskimage.data.retina() Human retina.  \nskimage.data.rocket() Launch photo of DSCOVR on Falcon 9 by SpaceX.  \nskimage.data.shepp_logan_phantom() Shepp Logan Phantom.  \nskimage.data.skin() Microscopy image of dermis and epidermis (skin layers).  \nskimage.data.stereo_motorcycle() Rectified stereo image pair with ground-truth disparities.  \nskimage.data.text() Gray-level \u201ctext\u201d image used for corner detection.   astronaut  \nskimage.data.astronaut() [source]\n \nColor image of the astronaut Eileen Collins. Photograph of Eileen Collins, an American astronaut. She was selected as an astronaut in 1992 and first piloted the space shuttle STS-63 in 1995. She retired in 2006 after spending a total of 38 days, 8 hours and 10 minutes in outer space. This image was downloaded from the NASA Great Images database <https://flic.kr/p/r9qvLn>`__. No known copyright restrictions, released into the public domain.  Returns \n \nastronaut(512, 512, 3) uint8 ndarray \n\nAstronaut image.     \n Examples using skimage.data.astronaut\n \n  Flood Fill   binary_blobs  \nskimage.data.binary_blobs(length=512, blob_size_fraction=0.1, n_dim=2, volume_fraction=0.5, seed=None) [source]\n \nGenerate synthetic binary image with several rounded blob-like objects.  Parameters \n \nlengthint, optional \n\nLinear size of output image.  \nblob_size_fractionfloat, optional \n\nTypical linear size of blob, as a fraction of length, should be smaller than 1.  \nn_dimint, optional \n\nNumber of dimensions of output image.  \nvolume_fractionfloat, default 0.5 \n\nFraction of image pixels covered by the blobs (where the output is 1). Should be in [0, 1].  \nseedint, optional \n\nSeed to initialize the random number generator. If None, a random seed from the operating system is used.    Returns \n \nblobsndarray of bools \n\nOutput binary image     Examples >>> from skimage import data\n>>> data.binary_blobs(length=5, blob_size_fraction=0.2, seed=1)\narray([[ True, False,  True,  True,  True],\n       [ True,  True,  True, False,  True],\n       [False,  True, False,  True,  True],\n       [ True, False, False,  True,  True],\n       [ True, False, False, False,  True]])\n>>> blobs = data.binary_blobs(length=256, blob_size_fraction=0.1)\n>>> # Finer structures\n>>> blobs = data.binary_blobs(length=256, blob_size_fraction=0.05)\n>>> # Blobs cover a smaller volume fraction of the image\n>>> blobs = data.binary_blobs(length=256, volume_fraction=0.3)\n \n brain  \nskimage.data.brain() [source]\n \nSubset of data from the University of North Carolina Volume Rendering Test Data Set. The full dataset is available at [1].  Returns \n \nimage(10, 256, 256) uint16 ndarray \n   Notes The 3D volume consists of 10 layers from the larger volume. References  \n1  \nhttps://graphics.stanford.edu/data/voldata/   \n Examples using skimage.data.brain\n \n  Local Histogram Equalization  \n\n  Rank filters   brick  \nskimage.data.brick() [source]\n \nBrick wall.  Returns \n \nbrick(512, 512) uint8 image \n\nA small section of a brick wall.     Notes The original image was downloaded from CC0Textures and licensed under the Creative Commons CC0 License. A perspective transform was then applied to the image, prior to rotating it by 90 degrees, cropping and scaling it to obtain the final image. \n camera  \nskimage.data.camera() [source]\n \nGray-level \u201ccamera\u201d image. Can be used for segmentation and denoising examples.  Returns \n \ncamera(512, 512) uint8 ndarray \n\nCamera image.     Notes No copyright restrictions. CC0 by the photographer (Lav Varshney).  Changed in version 0.18: This image was replaced due to copyright restrictions. For more information, please see [1].  References  \n1  \nhttps://github.com/scikit-image/scikit-image/issues/3927   \n Examples using skimage.data.camera\n \n  Tinting gray-scale images  \n\n  Masked Normalized Cross-Correlation  \n\n  Entropy  \n\n  GLCM Texture Features  \n\n  Multi-Otsu Thresholding  \n\n  Flood Fill  \n\n  Rank filters   cat  \nskimage.data.cat() [source]\n \nChelsea the cat. An example with texture, prominent edges in horizontal and diagonal directions, as well as features of differing scales.  Returns \n \nchelsea(300, 451, 3) uint8 ndarray \n\nChelsea image.     Notes No copyright restrictions. CC0 by the photographer (Stefan van der Walt). \n cell  \nskimage.data.cell() [source]\n \nCell floating in saline. This is a quantitative phase image retrieved from a digital hologram using the Python library qpformat. The image shows a cell with high phase value, above the background phase. Because of a banding pattern artifact in the background, this image is a good test of thresholding algorithms. The pixel spacing is 0.107 \u00b5m. These data were part of a comparison between several refractive index retrieval techniques for spherical objects as part of [1]. This image is CC0, dedicated to the public domain. You may copy, modify, or distribute it without asking permission.  Returns \n \ncell(660, 550) uint8 array \n\nImage of a cell.     References  \n1  \nPaul M\u00fcller, Mirjam Sch\u00fcrmann, Salvatore Girardo, Gheorghe Cojoc, and Jochen Guck. \u201cAccurate evaluation of size and refractive index for spherical objects in quantitative phase imaging.\u201d Optics Express 26(8): 10729-10743 (2018). DOI:10.1364/OE.26.010729   \n cells3d  \nskimage.data.cells3d() [source]\n \n3D fluorescence microscopy image of cells. The returned data is a 3D multichannel array with dimensions provided in (z, c, y, x) order. Each voxel has a size of (0.29 0.26 0.26) micrometer. Channel 0 contains cell membranes, channel 1 contains nuclei.  Returns \n cells3d: (60, 2, 256, 256) uint16 ndarray\n\nThe volumetric images of cells taken with an optical microscope.     Notes The data for this was provided by the Allen Institute for Cell Science. It has been downsampled by a factor of 4 in the row and column dimensions to reduce computational time. The microscope reports the following voxel spacing in microns:  Original voxel size is (0.290, 0.065, 0.065). Scaling factor is (1, 4, 4) in each dimension. After rescaling the voxel size is (0.29 0.26 0.26).  \n Examples using skimage.data.cells3d\n \n  3D adaptive histogram equalization  \n\n  Use rolling-ball algorithm for estimating background intensity  \n\n  Explore 3D images (of cells)   checkerboard  \nskimage.data.checkerboard() [source]\n \nCheckerboard image. Checkerboards are often used in image calibration, since the corner-points are easy to locate. Because of the many parallel edges, they also visualise distortions particularly well.  Returns \n \ncheckerboard(200, 200) uint8 ndarray \n\nCheckerboard image.     \n Examples using skimage.data.checkerboard\n \n  Flood Fill   chelsea  \nskimage.data.chelsea() [source]\n \nChelsea the cat. An example with texture, prominent edges in horizontal and diagonal directions, as well as features of differing scales.  Returns \n \nchelsea(300, 451, 3) uint8 ndarray \n\nChelsea image.     Notes No copyright restrictions. CC0 by the photographer (Stefan van der Walt). \n Examples using skimage.data.chelsea\n \n  Phase Unwrapping  \n\n  Flood Fill   clock  \nskimage.data.clock() [source]\n \nMotion blurred clock. This photograph of a wall clock was taken while moving the camera in an aproximately horizontal direction. It may be used to illustrate inverse filters and deconvolution. Released into the public domain by the photographer (Stefan van der Walt).  Returns \n \nclock(300, 400) uint8 ndarray \n\nClock image.     \n coffee  \nskimage.data.coffee() [source]\n \nCoffee cup. This photograph is courtesy of Pikolo Espresso Bar. It contains several elliptical shapes as well as varying texture (smooth porcelain to course wood grain).  Returns \n \ncoffee(400, 600, 3) uint8 ndarray \n\nCoffee image.     Notes No copyright restrictions. CC0 by the photographer (Rachel Michetti). \n coins  \nskimage.data.coins() [source]\n \nGreek coins from Pompeii. This image shows several coins outlined against a gray background. It is especially useful in, e.g. segmentation tests, where individual objects need to be identified against a background. The background shares enough grey levels with the coins that a simple segmentation is not sufficient.  Returns \n \ncoins(303, 384) uint8 ndarray \n\nCoins image.     Notes This image was downloaded from the Brooklyn Museum Collection. No known copyright restrictions. \n Examples using skimage.data.coins\n \n  Finding local maxima  \n\n  Measure region properties  \n\n  Use rolling-ball algorithm for estimating background intensity   colorwheel  \nskimage.data.colorwheel() [source]\n \nColor Wheel.  Returns \n \ncolorwheel(370, 371, 3) uint8 image \n\nA colorwheel.     \n download_all  \nskimage.data.download_all(directory=None) [source]\n \nDownload all datasets for use with scikit-image offline. Scikit-image datasets are no longer shipped with the library by default. This allows us to use higher quality datasets, while keeping the library download size small. This function requires the installation of an optional dependency, pooch, to download the full dataset. Follow installation instruction found at https://scikit-image.org/docs/stable/install.html Call this function to download all sample images making them available offline on your machine.  Parameters \n directory: path-like, optional\n\nThe directory where the dataset should be stored.    Raises \n ModuleNotFoundError:\n\nIf pooch is not install, this error will be raised.     Notes scikit-image will only search for images stored in the default directory. Only specify the directory if you wish to download the images to your own folder for a particular reason. You can access the location of the default data directory by inspecting the variable skimage.data.data_dir. \n eagle  \nskimage.data.eagle() [source]\n \nA golden eagle. Suitable for examples on segmentation, Hough transforms, and corner detection.  Returns \n \neagle(2019, 1826) uint8 ndarray \n\nEagle image.     Notes No copyright restrictions. CC0 by the photographer (Dayane Machado). \n Examples using skimage.data.eagle\n \n  Markers for watershed transform   grass  \nskimage.data.grass() [source]\n \nGrass.  Returns \n \ngrass(512, 512) uint8 image \n\nSome grass.     Notes The original image was downloaded from DeviantArt and licensed underthe Creative Commons CC0 License. The downloaded image was cropped to include a region of (512, 512) pixels around the top left corner, converted to grayscale, then to uint8 prior to saving the result in PNG format. \n gravel  \nskimage.data.gravel() [source]\n \nGravel  Returns \n \ngravel(512, 512) uint8 image \n\nGrayscale gravel sample.     Notes The original image was downloaded from CC0Textures and licensed under the Creative Commons CC0 License. The downloaded image was then rescaled to (1024, 1024), then the top left (512, 512) pixel region was cropped prior to converting the image to grayscale and uint8 data type. The result was saved using the PNG format. \n horse  \nskimage.data.horse() [source]\n \nBlack and white silhouette of a horse. This image was downloaded from openclipart No copyright restrictions. CC0 given by owner (Andreas Preuss (marauder)).  Returns \n \nhorse(328, 400) bool ndarray \n\nHorse image.     \n hubble_deep_field  \nskimage.data.hubble_deep_field() [source]\n \nHubble eXtreme Deep Field. This photograph contains the Hubble Telescope\u2019s farthest ever view of the universe. It can be useful as an example for multi-scale detection.  Returns \n \nhubble_deep_field(872, 1000, 3) uint8 ndarray \n\nHubble deep field image.     Notes This image was downloaded from HubbleSite. The image was captured by NASA and may be freely used in the public domain. \n human_mitosis  \nskimage.data.human_mitosis() [source]\n \nImage of human cells undergoing mitosis.  Returns \n human_mitosis: (512, 512) uint8 ndimage\n\nData of human cells undergoing mitosis taken during the preperation of the manuscript in [1].     Notes Copyright David Root. Licensed under CC-0 [2]. References  \n1  \nMoffat J, Grueneberg DA, Yang X, Kim SY, Kloepfer AM, Hinkle G, Piqani B, Eisenhaure TM, Luo B, Grenier JK, Carpenter AE, Foo SY, Stewart SA, Stockwell BR, Hacohen N, Hahn WC, Lander ES, Sabatini DM, Root DE (2006) A lentiviral RNAi library for human and mouse genes applied to an arrayed viral high-content screen. Cell, 124(6):1283-98 / :DOI: 10.1016/j.cell.2006.01.040 PMID 16564017  \n2  \nGitHub licensing discussion https://github.com/CellProfiler/examples/issues/41   \n Examples using skimage.data.human_mitosis\n \n  Segment human cells (in mitosis)   immunohistochemistry  \nskimage.data.immunohistochemistry() [source]\n \nImmunohistochemical (IHC) staining with hematoxylin counterstaining. This picture shows colonic glands where the IHC expression of FHL2 protein is revealed with DAB. Hematoxylin counterstaining is applied to enhance the negative parts of the tissue. This image was acquired at the Center for Microscopy And Molecular Imaging (CMMI). No known copyright restrictions.  Returns \n \nimmunohistochemistry(512, 512, 3) uint8 ndarray \n\nImmunohistochemistry image.     \n kidney  \nskimage.data.kidney() [source]\n \nMouse kidney tissue. This biological tissue on a pre-prepared slide was imaged with confocal fluorescence microscopy (Nikon C1 inverted microscope). Image shape is (16, 512, 512, 3). That is 512x512 pixels in X-Y, 16 image slices in Z, and 3 color channels (emission wavelengths 450nm, 515nm, and 605nm, respectively). Real-space voxel size is 1.24 microns in X-Y, and 1.25 microns in Z. Data type is unsigned 16-bit integers.  Returns \n \nkidney(16, 512, 512, 3) uint16 ndarray \n\nKidney 3D multichannel image.     Notes This image was acquired by Genevieve Buckley at Monasoh Micro Imaging in 2018. License: CC0 \n lbp_frontal_face_cascade_filename  \nskimage.data.lbp_frontal_face_cascade_filename() [source]\n \nReturn the path to the XML file containing the weak classifier cascade. These classifiers were trained using LBP features. The file is part of the OpenCV repository [1]. References  \n1  \nOpenCV lbpcascade trained files https://github.com/opencv/opencv/tree/master/data/lbpcascades   \n lfw_subset  \nskimage.data.lfw_subset() [source]\n \nSubset of data from the LFW dataset. This database is a subset of the LFW database containing:  100 faces 100 non-faces  The full dataset is available at [2].  Returns \n \nimages(200, 25, 25) uint8 ndarray \n\n100 first images are faces and subsequent 100 are non-faces.     Notes The faces were randomly selected from the LFW dataset and the non-faces were extracted from the background of the same dataset. The cropped ROIs have been resized to a 25 x 25 pixels. References  \n1  \nHuang, G., Mattar, M., Lee, H., & Learned-Miller, E. G. (2012). Learning to align from scratch. In Advances in Neural Information Processing Systems (pp. 764-772).  \n2  \nhttp://vis-www.cs.umass.edu/lfw/   \n Examples using skimage.data.lfw_subset\n \n  Specific images   lily  \nskimage.data.lily() [source]\n \nLily of the valley plant stem. This plant stem on a pre-prepared slide was imaged with confocal fluorescence microscopy (Nikon C1 inverted microscope). Image shape is (922, 922, 4). That is 922x922 pixels in X-Y, with 4 color channels. Real-space voxel size is 1.24 microns in X-Y. Data type is unsigned 16-bit integers.  Returns \n \nlily(922, 922, 4) uint16 ndarray \n\nLily 2D multichannel image.     Notes This image was acquired by Genevieve Buckley at Monasoh Micro Imaging in 2018. License: CC0 \n logo  \nskimage.data.logo() [source]\n \nScikit-image logo, a RGBA image.  Returns \n \nlogo(500, 500, 4) uint8 ndarray \n\nLogo image.     \n microaneurysms  \nskimage.data.microaneurysms() [source]\n \nGray-level \u201cmicroaneurysms\u201d image. Detail from an image of the retina (green channel). The image is a crop of image 07_dr.JPG from the High-Resolution Fundus (HRF) Image Database: https://www5.cs.fau.de/research/data/fundus-images/  Returns \n \nmicroaneurysms(102, 102) uint8 ndarray \n\nRetina image with lesions.     Notes No copyright restrictions. CC0 given by owner (Andreas Maier). References  \n1  \nBudai, A., Bock, R, Maier, A., Hornegger, J., Michelson, G. (2013). Robust Vessel Segmentation in Fundus Images. International Journal of Biomedical Imaging, vol. 2013, 2013. DOI:10.1155/2013/154860   \n moon  \nskimage.data.moon() [source]\n \nSurface of the moon. This low-contrast image of the surface of the moon is useful for illustrating histogram equalization and contrast stretching.  Returns \n \nmoon(512, 512) uint8 ndarray \n\nMoon image.     \n Examples using skimage.data.moon\n \n  Local Histogram Equalization   page  \nskimage.data.page() [source]\n \nScanned page. This image of printed text is useful for demonstrations requiring uneven background illumination.  Returns \n \npage(191, 384) uint8 ndarray \n\nPage image.     \n Examples using skimage.data.page\n \n  Use rolling-ball algorithm for estimating background intensity  \n\n  Rank filters   retina  \nskimage.data.retina() [source]\n \nHuman retina. This image of a retina is useful for demonstrations requiring circular images.  Returns \n \nretina(1411, 1411, 3) uint8 ndarray \n\nRetina image in RGB.     Notes This image was downloaded from wikimedia. This file is made available under the Creative Commons CC0 1.0 Universal Public Domain Dedication. References  \n1  \nH\u00e4ggstr\u00f6m, Mikael (2014). \u201cMedical gallery of Mikael H\u00e4ggstr\u00f6m 2014\u201d. WikiJournal of Medicine 1 (2). DOI:10.15347/wjm/2014.008. ISSN 2002-4436. Public Domain   \n rocket  \nskimage.data.rocket() [source]\n \nLaunch photo of DSCOVR on Falcon 9 by SpaceX. This is the launch photo of Falcon 9 carrying DSCOVR lifted off from SpaceX\u2019s Launch Complex 40 at Cape Canaveral Air Force Station, FL.  Returns \n \nrocket(427, 640, 3) uint8 ndarray \n\nRocket image.     Notes This image was downloaded from SpaceX Photos. The image was captured by SpaceX and released in the public domain. \n shepp_logan_phantom  \nskimage.data.shepp_logan_phantom() [source]\n \nShepp Logan Phantom.  Returns \n \nphantom(400, 400) float64 image \n\nImage of the Shepp-Logan phantom in grayscale.     References  \n1  \nL. A. Shepp and B. F. Logan, \u201cThe Fourier reconstruction of a head section,\u201d in IEEE Transactions on Nuclear Science, vol. 21, no. 3, pp. 21-43, June 1974. DOI:10.1109/TNS.1974.6499235   \n skin  \nskimage.data.skin() [source]\n \nMicroscopy image of dermis and epidermis (skin layers). Hematoxylin and eosin stained slide at 10x of normal epidermis and dermis with a benign intradermal nevus.  Returns \n \nskin(960, 1280, 3) RGB image of uint8 \n   Notes This image requires an Internet connection the first time it is called, and to have the pooch package installed, in order to fetch the image file from the scikit-image datasets repository. The source of this image is https://en.wikipedia.org/wiki/File:Normal_Epidermis_and_Dermis_with_Intradermal_Nevus_10x.JPG The image was released in the public domain by its author Kilbad. \n Examples using skimage.data.skin\n \n  Trainable segmentation using local features and random forests   stereo_motorcycle  \nskimage.data.stereo_motorcycle() [source]\n \nRectified stereo image pair with ground-truth disparities. The two images are rectified such that every pixel in the left image has its corresponding pixel on the same scanline in the right image. That means that both images are warped such that they have the same orientation but a horizontal spatial offset (baseline). The ground-truth pixel offset in column direction is specified by the included disparity map. The two images are part of the Middlebury 2014 stereo benchmark. The dataset was created by Nera Nesic, Porter Westling, Xi Wang, York Kitajima, Greg Krathwohl, and Daniel Scharstein at Middlebury College. A detailed description of the acquisition process can be found in [1]. The images included here are down-sampled versions of the default exposure images in the benchmark. The images are down-sampled by a factor of 4 using the function skimage.transform.downscale_local_mean. The calibration data in the following and the included ground-truth disparity map are valid for the down-sampled images: Focal length:           994.978px\nPrincipal point x:      311.193px\nPrincipal point y:      254.877px\nPrincipal point dx:      31.086px\nBaseline:               193.001mm\n  Returns \n \nimg_left(500, 741, 3) uint8 ndarray \n\nLeft stereo image.  \nimg_right(500, 741, 3) uint8 ndarray \n\nRight stereo image.  \ndisp(500, 741, 3) float ndarray \n\nGround-truth disparity map, where each value describes the offset in column direction between corresponding pixels in the left and the right stereo images. E.g. the corresponding pixel of img_left[10, 10 + disp[10, 10]] is img_right[10, 10]. NaNs denote pixels in the left image that do not have ground-truth.     Notes The original resolution images, images with different exposure and lighting, and ground-truth depth maps can be found at the Middlebury website [2]. References  \n1  \nD. Scharstein, H. Hirschmueller, Y. Kitajima, G. Krathwohl, N. Nesic, X. Wang, and P. Westling. High-resolution stereo datasets with subpixel-accurate ground truth. In German Conference on Pattern Recognition (GCPR 2014), Muenster, Germany, September 2014.  \n2  \nhttp://vision.middlebury.edu/stereo/data/scenes2014/   \n Examples using skimage.data.stereo_motorcycle\n \n  Specific images  \n\n  Registration using optical flow   text  \nskimage.data.text() [source]\n \nGray-level \u201ctext\u201d image used for corner detection.  Returns \n \ntext(172, 448) uint8 ndarray \n\nText image.     Notes This image was downloaded from Wikipedia <https://en.wikipedia.org/wiki/File:Corner.png>`__. No known copyright restrictions, released into the public domain. \n\n"}, {"name": "Data visualization", "path": "user_guide/visualization", "type": "Guide", "text": "Data visualization Data visualization takes an important place in image processing. Data can be a simple unique 2D image or a more complex with multidimensional aspects: 3D in space, timeslapse, multiple channels. Therefore, the visualization strategy will depend on the data complexity and a range of tools external to scikit-image can be used for this purpose. Historically, scikit-image provided viewer tools but powerful packages are now available and must be preferred. Matplotlib Matplotlib is a library able to generate static plots, which includes image visualization. Plotly Plotly is a plotting library relying on web technologies with interaction capabilities. Mayavi Mayavi can be used to visualize 3D images. Napari Napari is a multi-dimensional image viewer. It\u2019s designed for browsing, annotating, and analyzing large multi-dimensional images.\n"}, {"name": "data.astronaut()", "path": "api/skimage.data#skimage.data.astronaut", "type": "data", "text": " \nskimage.data.astronaut() [source]\n \nColor image of the astronaut Eileen Collins. Photograph of Eileen Collins, an American astronaut. She was selected as an astronaut in 1992 and first piloted the space shuttle STS-63 in 1995. She retired in 2006 after spending a total of 38 days, 8 hours and 10 minutes in outer space. This image was downloaded from the NASA Great Images database <https://flic.kr/p/r9qvLn>`__. No known copyright restrictions, released into the public domain.  Returns \n \nastronaut(512, 512, 3) uint8 ndarray \n\nAstronaut image.     \n"}, {"name": "data.binary_blobs()", "path": "api/skimage.data#skimage.data.binary_blobs", "type": "data", "text": " \nskimage.data.binary_blobs(length=512, blob_size_fraction=0.1, n_dim=2, volume_fraction=0.5, seed=None) [source]\n \nGenerate synthetic binary image with several rounded blob-like objects.  Parameters \n \nlengthint, optional \n\nLinear size of output image.  \nblob_size_fractionfloat, optional \n\nTypical linear size of blob, as a fraction of length, should be smaller than 1.  \nn_dimint, optional \n\nNumber of dimensions of output image.  \nvolume_fractionfloat, default 0.5 \n\nFraction of image pixels covered by the blobs (where the output is 1). Should be in [0, 1].  \nseedint, optional \n\nSeed to initialize the random number generator. If None, a random seed from the operating system is used.    Returns \n \nblobsndarray of bools \n\nOutput binary image     Examples >>> from skimage import data\n>>> data.binary_blobs(length=5, blob_size_fraction=0.2, seed=1)\narray([[ True, False,  True,  True,  True],\n       [ True,  True,  True, False,  True],\n       [False,  True, False,  True,  True],\n       [ True, False, False,  True,  True],\n       [ True, False, False, False,  True]])\n>>> blobs = data.binary_blobs(length=256, blob_size_fraction=0.1)\n>>> # Finer structures\n>>> blobs = data.binary_blobs(length=256, blob_size_fraction=0.05)\n>>> # Blobs cover a smaller volume fraction of the image\n>>> blobs = data.binary_blobs(length=256, volume_fraction=0.3)\n \n"}, {"name": "data.brain()", "path": "api/skimage.data#skimage.data.brain", "type": "data", "text": " \nskimage.data.brain() [source]\n \nSubset of data from the University of North Carolina Volume Rendering Test Data Set. The full dataset is available at [1].  Returns \n \nimage(10, 256, 256) uint16 ndarray \n   Notes The 3D volume consists of 10 layers from the larger volume. References  \n1  \nhttps://graphics.stanford.edu/data/voldata/   \n"}, {"name": "data.brick()", "path": "api/skimage.data#skimage.data.brick", "type": "data", "text": " \nskimage.data.brick() [source]\n \nBrick wall.  Returns \n \nbrick(512, 512) uint8 image \n\nA small section of a brick wall.     Notes The original image was downloaded from CC0Textures and licensed under the Creative Commons CC0 License. A perspective transform was then applied to the image, prior to rotating it by 90 degrees, cropping and scaling it to obtain the final image. \n"}, {"name": "data.camera()", "path": "api/skimage.data#skimage.data.camera", "type": "data", "text": " \nskimage.data.camera() [source]\n \nGray-level \u201ccamera\u201d image. Can be used for segmentation and denoising examples.  Returns \n \ncamera(512, 512) uint8 ndarray \n\nCamera image.     Notes No copyright restrictions. CC0 by the photographer (Lav Varshney).  Changed in version 0.18: This image was replaced due to copyright restrictions. For more information, please see [1].  References  \n1  \nhttps://github.com/scikit-image/scikit-image/issues/3927   \n"}, {"name": "data.cat()", "path": "api/skimage.data#skimage.data.cat", "type": "data", "text": " \nskimage.data.cat() [source]\n \nChelsea the cat. An example with texture, prominent edges in horizontal and diagonal directions, as well as features of differing scales.  Returns \n \nchelsea(300, 451, 3) uint8 ndarray \n\nChelsea image.     Notes No copyright restrictions. CC0 by the photographer (Stefan van der Walt). \n"}, {"name": "data.cell()", "path": "api/skimage.data#skimage.data.cell", "type": "data", "text": " \nskimage.data.cell() [source]\n \nCell floating in saline. This is a quantitative phase image retrieved from a digital hologram using the Python library qpformat. The image shows a cell with high phase value, above the background phase. Because of a banding pattern artifact in the background, this image is a good test of thresholding algorithms. The pixel spacing is 0.107 \u00b5m. These data were part of a comparison between several refractive index retrieval techniques for spherical objects as part of [1]. This image is CC0, dedicated to the public domain. You may copy, modify, or distribute it without asking permission.  Returns \n \ncell(660, 550) uint8 array \n\nImage of a cell.     References  \n1  \nPaul M\u00fcller, Mirjam Sch\u00fcrmann, Salvatore Girardo, Gheorghe Cojoc, and Jochen Guck. \u201cAccurate evaluation of size and refractive index for spherical objects in quantitative phase imaging.\u201d Optics Express 26(8): 10729-10743 (2018). DOI:10.1364/OE.26.010729   \n"}, {"name": "data.cells3d()", "path": "api/skimage.data#skimage.data.cells3d", "type": "data", "text": " \nskimage.data.cells3d() [source]\n \n3D fluorescence microscopy image of cells. The returned data is a 3D multichannel array with dimensions provided in (z, c, y, x) order. Each voxel has a size of (0.29 0.26 0.26) micrometer. Channel 0 contains cell membranes, channel 1 contains nuclei.  Returns \n cells3d: (60, 2, 256, 256) uint16 ndarray\n\nThe volumetric images of cells taken with an optical microscope.     Notes The data for this was provided by the Allen Institute for Cell Science. It has been downsampled by a factor of 4 in the row and column dimensions to reduce computational time. The microscope reports the following voxel spacing in microns:  Original voxel size is (0.290, 0.065, 0.065). Scaling factor is (1, 4, 4) in each dimension. After rescaling the voxel size is (0.29 0.26 0.26).  \n"}, {"name": "data.checkerboard()", "path": "api/skimage.data#skimage.data.checkerboard", "type": "data", "text": " \nskimage.data.checkerboard() [source]\n \nCheckerboard image. Checkerboards are often used in image calibration, since the corner-points are easy to locate. Because of the many parallel edges, they also visualise distortions particularly well.  Returns \n \ncheckerboard(200, 200) uint8 ndarray \n\nCheckerboard image.     \n"}, {"name": "data.chelsea()", "path": "api/skimage.data#skimage.data.chelsea", "type": "data", "text": " \nskimage.data.chelsea() [source]\n \nChelsea the cat. An example with texture, prominent edges in horizontal and diagonal directions, as well as features of differing scales.  Returns \n \nchelsea(300, 451, 3) uint8 ndarray \n\nChelsea image.     Notes No copyright restrictions. CC0 by the photographer (Stefan van der Walt). \n"}, {"name": "data.clock()", "path": "api/skimage.data#skimage.data.clock", "type": "data", "text": " \nskimage.data.clock() [source]\n \nMotion blurred clock. This photograph of a wall clock was taken while moving the camera in an aproximately horizontal direction. It may be used to illustrate inverse filters and deconvolution. Released into the public domain by the photographer (Stefan van der Walt).  Returns \n \nclock(300, 400) uint8 ndarray \n\nClock image.     \n"}, {"name": "data.coffee()", "path": "api/skimage.data#skimage.data.coffee", "type": "data", "text": " \nskimage.data.coffee() [source]\n \nCoffee cup. This photograph is courtesy of Pikolo Espresso Bar. It contains several elliptical shapes as well as varying texture (smooth porcelain to course wood grain).  Returns \n \ncoffee(400, 600, 3) uint8 ndarray \n\nCoffee image.     Notes No copyright restrictions. CC0 by the photographer (Rachel Michetti). \n"}, {"name": "data.coins()", "path": "api/skimage.data#skimage.data.coins", "type": "data", "text": " \nskimage.data.coins() [source]\n \nGreek coins from Pompeii. This image shows several coins outlined against a gray background. It is especially useful in, e.g. segmentation tests, where individual objects need to be identified against a background. The background shares enough grey levels with the coins that a simple segmentation is not sufficient.  Returns \n \ncoins(303, 384) uint8 ndarray \n\nCoins image.     Notes This image was downloaded from the Brooklyn Museum Collection. No known copyright restrictions. \n"}, {"name": "data.colorwheel()", "path": "api/skimage.data#skimage.data.colorwheel", "type": "data", "text": " \nskimage.data.colorwheel() [source]\n \nColor Wheel.  Returns \n \ncolorwheel(370, 371, 3) uint8 image \n\nA colorwheel.     \n"}, {"name": "data.download_all()", "path": "api/skimage.data#skimage.data.download_all", "type": "data", "text": " \nskimage.data.download_all(directory=None) [source]\n \nDownload all datasets for use with scikit-image offline. Scikit-image datasets are no longer shipped with the library by default. This allows us to use higher quality datasets, while keeping the library download size small. This function requires the installation of an optional dependency, pooch, to download the full dataset. Follow installation instruction found at https://scikit-image.org/docs/stable/install.html Call this function to download all sample images making them available offline on your machine.  Parameters \n directory: path-like, optional\n\nThe directory where the dataset should be stored.    Raises \n ModuleNotFoundError:\n\nIf pooch is not install, this error will be raised.     Notes scikit-image will only search for images stored in the default directory. Only specify the directory if you wish to download the images to your own folder for a particular reason. You can access the location of the default data directory by inspecting the variable skimage.data.data_dir. \n"}, {"name": "data.eagle()", "path": "api/skimage.data#skimage.data.eagle", "type": "data", "text": " \nskimage.data.eagle() [source]\n \nA golden eagle. Suitable for examples on segmentation, Hough transforms, and corner detection.  Returns \n \neagle(2019, 1826) uint8 ndarray \n\nEagle image.     Notes No copyright restrictions. CC0 by the photographer (Dayane Machado). \n"}, {"name": "data.grass()", "path": "api/skimage.data#skimage.data.grass", "type": "data", "text": " \nskimage.data.grass() [source]\n \nGrass.  Returns \n \ngrass(512, 512) uint8 image \n\nSome grass.     Notes The original image was downloaded from DeviantArt and licensed underthe Creative Commons CC0 License. The downloaded image was cropped to include a region of (512, 512) pixels around the top left corner, converted to grayscale, then to uint8 prior to saving the result in PNG format. \n"}, {"name": "data.gravel()", "path": "api/skimage.data#skimage.data.gravel", "type": "data", "text": " \nskimage.data.gravel() [source]\n \nGravel  Returns \n \ngravel(512, 512) uint8 image \n\nGrayscale gravel sample.     Notes The original image was downloaded from CC0Textures and licensed under the Creative Commons CC0 License. The downloaded image was then rescaled to (1024, 1024), then the top left (512, 512) pixel region was cropped prior to converting the image to grayscale and uint8 data type. The result was saved using the PNG format. \n"}, {"name": "data.horse()", "path": "api/skimage.data#skimage.data.horse", "type": "data", "text": " \nskimage.data.horse() [source]\n \nBlack and white silhouette of a horse. This image was downloaded from openclipart No copyright restrictions. CC0 given by owner (Andreas Preuss (marauder)).  Returns \n \nhorse(328, 400) bool ndarray \n\nHorse image.     \n"}, {"name": "data.hubble_deep_field()", "path": "api/skimage.data#skimage.data.hubble_deep_field", "type": "data", "text": " \nskimage.data.hubble_deep_field() [source]\n \nHubble eXtreme Deep Field. This photograph contains the Hubble Telescope\u2019s farthest ever view of the universe. It can be useful as an example for multi-scale detection.  Returns \n \nhubble_deep_field(872, 1000, 3) uint8 ndarray \n\nHubble deep field image.     Notes This image was downloaded from HubbleSite. The image was captured by NASA and may be freely used in the public domain. \n"}, {"name": "data.human_mitosis()", "path": "api/skimage.data#skimage.data.human_mitosis", "type": "data", "text": " \nskimage.data.human_mitosis() [source]\n \nImage of human cells undergoing mitosis.  Returns \n human_mitosis: (512, 512) uint8 ndimage\n\nData of human cells undergoing mitosis taken during the preperation of the manuscript in [1].     Notes Copyright David Root. Licensed under CC-0 [2]. References  \n1  \nMoffat J, Grueneberg DA, Yang X, Kim SY, Kloepfer AM, Hinkle G, Piqani B, Eisenhaure TM, Luo B, Grenier JK, Carpenter AE, Foo SY, Stewart SA, Stockwell BR, Hacohen N, Hahn WC, Lander ES, Sabatini DM, Root DE (2006) A lentiviral RNAi library for human and mouse genes applied to an arrayed viral high-content screen. Cell, 124(6):1283-98 / :DOI: 10.1016/j.cell.2006.01.040 PMID 16564017  \n2  \nGitHub licensing discussion https://github.com/CellProfiler/examples/issues/41   \n"}, {"name": "data.immunohistochemistry()", "path": "api/skimage.data#skimage.data.immunohistochemistry", "type": "data", "text": " \nskimage.data.immunohistochemistry() [source]\n \nImmunohistochemical (IHC) staining with hematoxylin counterstaining. This picture shows colonic glands where the IHC expression of FHL2 protein is revealed with DAB. Hematoxylin counterstaining is applied to enhance the negative parts of the tissue. This image was acquired at the Center for Microscopy And Molecular Imaging (CMMI). No known copyright restrictions.  Returns \n \nimmunohistochemistry(512, 512, 3) uint8 ndarray \n\nImmunohistochemistry image.     \n"}, {"name": "data.kidney()", "path": "api/skimage.data#skimage.data.kidney", "type": "data", "text": " \nskimage.data.kidney() [source]\n \nMouse kidney tissue. This biological tissue on a pre-prepared slide was imaged with confocal fluorescence microscopy (Nikon C1 inverted microscope). Image shape is (16, 512, 512, 3). That is 512x512 pixels in X-Y, 16 image slices in Z, and 3 color channels (emission wavelengths 450nm, 515nm, and 605nm, respectively). Real-space voxel size is 1.24 microns in X-Y, and 1.25 microns in Z. Data type is unsigned 16-bit integers.  Returns \n \nkidney(16, 512, 512, 3) uint16 ndarray \n\nKidney 3D multichannel image.     Notes This image was acquired by Genevieve Buckley at Monasoh Micro Imaging in 2018. License: CC0 \n"}, {"name": "data.lbp_frontal_face_cascade_filename()", "path": "api/skimage.data#skimage.data.lbp_frontal_face_cascade_filename", "type": "data", "text": " \nskimage.data.lbp_frontal_face_cascade_filename() [source]\n \nReturn the path to the XML file containing the weak classifier cascade. These classifiers were trained using LBP features. The file is part of the OpenCV repository [1]. References  \n1  \nOpenCV lbpcascade trained files https://github.com/opencv/opencv/tree/master/data/lbpcascades   \n"}, {"name": "data.lfw_subset()", "path": "api/skimage.data#skimage.data.lfw_subset", "type": "data", "text": " \nskimage.data.lfw_subset() [source]\n \nSubset of data from the LFW dataset. This database is a subset of the LFW database containing:  100 faces 100 non-faces  The full dataset is available at [2].  Returns \n \nimages(200, 25, 25) uint8 ndarray \n\n100 first images are faces and subsequent 100 are non-faces.     Notes The faces were randomly selected from the LFW dataset and the non-faces were extracted from the background of the same dataset. The cropped ROIs have been resized to a 25 x 25 pixels. References  \n1  \nHuang, G., Mattar, M., Lee, H., & Learned-Miller, E. G. (2012). Learning to align from scratch. In Advances in Neural Information Processing Systems (pp. 764-772).  \n2  \nhttp://vis-www.cs.umass.edu/lfw/   \n"}, {"name": "data.lily()", "path": "api/skimage.data#skimage.data.lily", "type": "data", "text": " \nskimage.data.lily() [source]\n \nLily of the valley plant stem. This plant stem on a pre-prepared slide was imaged with confocal fluorescence microscopy (Nikon C1 inverted microscope). Image shape is (922, 922, 4). That is 922x922 pixels in X-Y, with 4 color channels. Real-space voxel size is 1.24 microns in X-Y. Data type is unsigned 16-bit integers.  Returns \n \nlily(922, 922, 4) uint16 ndarray \n\nLily 2D multichannel image.     Notes This image was acquired by Genevieve Buckley at Monasoh Micro Imaging in 2018. License: CC0 \n"}, {"name": "data.logo()", "path": "api/skimage.data#skimage.data.logo", "type": "data", "text": " \nskimage.data.logo() [source]\n \nScikit-image logo, a RGBA image.  Returns \n \nlogo(500, 500, 4) uint8 ndarray \n\nLogo image.     \n"}, {"name": "data.microaneurysms()", "path": "api/skimage.data#skimage.data.microaneurysms", "type": "data", "text": " \nskimage.data.microaneurysms() [source]\n \nGray-level \u201cmicroaneurysms\u201d image. Detail from an image of the retina (green channel). The image is a crop of image 07_dr.JPG from the High-Resolution Fundus (HRF) Image Database: https://www5.cs.fau.de/research/data/fundus-images/  Returns \n \nmicroaneurysms(102, 102) uint8 ndarray \n\nRetina image with lesions.     Notes No copyright restrictions. CC0 given by owner (Andreas Maier). References  \n1  \nBudai, A., Bock, R, Maier, A., Hornegger, J., Michelson, G. (2013). Robust Vessel Segmentation in Fundus Images. International Journal of Biomedical Imaging, vol. 2013, 2013. DOI:10.1155/2013/154860   \n"}, {"name": "data.moon()", "path": "api/skimage.data#skimage.data.moon", "type": "data", "text": " \nskimage.data.moon() [source]\n \nSurface of the moon. This low-contrast image of the surface of the moon is useful for illustrating histogram equalization and contrast stretching.  Returns \n \nmoon(512, 512) uint8 ndarray \n\nMoon image.     \n"}, {"name": "data.page()", "path": "api/skimage.data#skimage.data.page", "type": "data", "text": " \nskimage.data.page() [source]\n \nScanned page. This image of printed text is useful for demonstrations requiring uneven background illumination.  Returns \n \npage(191, 384) uint8 ndarray \n\nPage image.     \n"}, {"name": "data.retina()", "path": "api/skimage.data#skimage.data.retina", "type": "data", "text": " \nskimage.data.retina() [source]\n \nHuman retina. This image of a retina is useful for demonstrations requiring circular images.  Returns \n \nretina(1411, 1411, 3) uint8 ndarray \n\nRetina image in RGB.     Notes This image was downloaded from wikimedia. This file is made available under the Creative Commons CC0 1.0 Universal Public Domain Dedication. References  \n1  \nH\u00e4ggstr\u00f6m, Mikael (2014). \u201cMedical gallery of Mikael H\u00e4ggstr\u00f6m 2014\u201d. WikiJournal of Medicine 1 (2). DOI:10.15347/wjm/2014.008. ISSN 2002-4436. Public Domain   \n"}, {"name": "data.rocket()", "path": "api/skimage.data#skimage.data.rocket", "type": "data", "text": " \nskimage.data.rocket() [source]\n \nLaunch photo of DSCOVR on Falcon 9 by SpaceX. This is the launch photo of Falcon 9 carrying DSCOVR lifted off from SpaceX\u2019s Launch Complex 40 at Cape Canaveral Air Force Station, FL.  Returns \n \nrocket(427, 640, 3) uint8 ndarray \n\nRocket image.     Notes This image was downloaded from SpaceX Photos. The image was captured by SpaceX and released in the public domain. \n"}, {"name": "data.shepp_logan_phantom()", "path": "api/skimage.data#skimage.data.shepp_logan_phantom", "type": "data", "text": " \nskimage.data.shepp_logan_phantom() [source]\n \nShepp Logan Phantom.  Returns \n \nphantom(400, 400) float64 image \n\nImage of the Shepp-Logan phantom in grayscale.     References  \n1  \nL. A. Shepp and B. F. Logan, \u201cThe Fourier reconstruction of a head section,\u201d in IEEE Transactions on Nuclear Science, vol. 21, no. 3, pp. 21-43, June 1974. DOI:10.1109/TNS.1974.6499235   \n"}, {"name": "data.skin()", "path": "api/skimage.data#skimage.data.skin", "type": "data", "text": " \nskimage.data.skin() [source]\n \nMicroscopy image of dermis and epidermis (skin layers). Hematoxylin and eosin stained slide at 10x of normal epidermis and dermis with a benign intradermal nevus.  Returns \n \nskin(960, 1280, 3) RGB image of uint8 \n   Notes This image requires an Internet connection the first time it is called, and to have the pooch package installed, in order to fetch the image file from the scikit-image datasets repository. The source of this image is https://en.wikipedia.org/wiki/File:Normal_Epidermis_and_Dermis_with_Intradermal_Nevus_10x.JPG The image was released in the public domain by its author Kilbad. \n"}, {"name": "data.stereo_motorcycle()", "path": "api/skimage.data#skimage.data.stereo_motorcycle", "type": "data", "text": " \nskimage.data.stereo_motorcycle() [source]\n \nRectified stereo image pair with ground-truth disparities. The two images are rectified such that every pixel in the left image has its corresponding pixel on the same scanline in the right image. That means that both images are warped such that they have the same orientation but a horizontal spatial offset (baseline). The ground-truth pixel offset in column direction is specified by the included disparity map. The two images are part of the Middlebury 2014 stereo benchmark. The dataset was created by Nera Nesic, Porter Westling, Xi Wang, York Kitajima, Greg Krathwohl, and Daniel Scharstein at Middlebury College. A detailed description of the acquisition process can be found in [1]. The images included here are down-sampled versions of the default exposure images in the benchmark. The images are down-sampled by a factor of 4 using the function skimage.transform.downscale_local_mean. The calibration data in the following and the included ground-truth disparity map are valid for the down-sampled images: Focal length:           994.978px\nPrincipal point x:      311.193px\nPrincipal point y:      254.877px\nPrincipal point dx:      31.086px\nBaseline:               193.001mm\n  Returns \n \nimg_left(500, 741, 3) uint8 ndarray \n\nLeft stereo image.  \nimg_right(500, 741, 3) uint8 ndarray \n\nRight stereo image.  \ndisp(500, 741, 3) float ndarray \n\nGround-truth disparity map, where each value describes the offset in column direction between corresponding pixels in the left and the right stereo images. E.g. the corresponding pixel of img_left[10, 10 + disp[10, 10]] is img_right[10, 10]. NaNs denote pixels in the left image that do not have ground-truth.     Notes The original resolution images, images with different exposure and lighting, and ground-truth depth maps can be found at the Middlebury website [2]. References  \n1  \nD. Scharstein, H. Hirschmueller, Y. Kitajima, G. Krathwohl, N. Nesic, X. Wang, and P. Westling. High-resolution stereo datasets with subpixel-accurate ground truth. In German Conference on Pattern Recognition (GCPR 2014), Muenster, Germany, September 2014.  \n2  \nhttp://vision.middlebury.edu/stereo/data/scenes2014/   \n"}, {"name": "data.text()", "path": "api/skimage.data#skimage.data.text", "type": "data", "text": " \nskimage.data.text() [source]\n \nGray-level \u201ctext\u201d image used for corner detection.  Returns \n \ntext(172, 448) uint8 ndarray \n\nText image.     Notes This image was downloaded from Wikipedia <https://en.wikipedia.org/wiki/File:Corner.png>`__. No known copyright restrictions, released into the public domain. \n"}, {"name": "draw", "path": "api/skimage.draw", "type": "draw", "text": "Module: draw  \nskimage.draw.bezier_curve(r0, c0, r1, c1, \u2026) Generate Bezier curve coordinates.  \nskimage.draw.circle(r, c, radius[, shape]) Generate coordinates of pixels within circle.  \nskimage.draw.circle_perimeter(r, c, radius) Generate circle perimeter coordinates.  \nskimage.draw.circle_perimeter_aa(r, c, radius) Generate anti-aliased circle perimeter coordinates.  \nskimage.draw.disk(center, radius, *[, shape]) Generate coordinates of pixels within circle.  \nskimage.draw.ellipse(r, c, r_radius, c_radius) Generate coordinates of pixels within ellipse.  \nskimage.draw.ellipse_perimeter(r, c, \u2026[, \u2026]) Generate ellipse perimeter coordinates.  \nskimage.draw.ellipsoid(a, b, c[, spacing, \u2026]) Generates ellipsoid with semimajor axes aligned with grid dimensions on grid with specified spacing.  \nskimage.draw.ellipsoid_stats(a, b, c) Calculates analytical surface area and volume for ellipsoid with semimajor axes aligned with grid dimensions of specified spacing.  \nskimage.draw.line(r0, c0, r1, c1) Generate line pixel coordinates.  \nskimage.draw.line_aa(r0, c0, r1, c1) Generate anti-aliased line pixel coordinates.  \nskimage.draw.line_nd(start, stop, *[, \u2026]) Draw a single-pixel thick line in n dimensions.  \nskimage.draw.polygon(r, c[, shape]) Generate coordinates of pixels within polygon.  \nskimage.draw.polygon2mask(image_shape, polygon) Compute a mask from polygon.  \nskimage.draw.polygon_perimeter(r, c[, \u2026]) Generate polygon perimeter coordinates.  \nskimage.draw.random_shapes(image_shape, \u2026) Generate an image with random shapes, labeled with bounding boxes.  \nskimage.draw.rectangle(start[, end, extent, \u2026]) Generate coordinates of pixels within a rectangle.  \nskimage.draw.rectangle_perimeter(start[, \u2026]) Generate coordinates of pixels that are exactly around a rectangle.  \nskimage.draw.set_color(image, coords, color) Set pixel color in the image at the given coordinates.   bezier_curve  \nskimage.draw.bezier_curve(r0, c0, r1, c1, r2, c2, weight, shape=None) [source]\n \nGenerate Bezier curve coordinates.  Parameters \n \nr0, c0int \n\nCoordinates of the first control point.  \nr1, c1int \n\nCoordinates of the middle control point.  \nr2, c2int \n\nCoordinates of the last control point.  \nweightdouble \n\nMiddle control point weight, it describes the line tension.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for curves that exceed the image size. If None, the full extent of the curve is used.    Returns \n \nrr, cc(N,) ndarray of int \n\nIndices of pixels that belong to the Bezier curve. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Notes The algorithm is the rational quadratic algorithm presented in reference [1]. References  \n1  \nA Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012 http://members.chello.at/easyfilter/Bresenham.pdf   Examples >>> import numpy as np\n>>> from skimage.draw import bezier_curve\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = bezier_curve(1, 5, 5, -2, 8, 8, 2)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n circle  \nskimage.draw.circle(r, c, radius, shape=None) [source]\n \nGenerate coordinates of pixels within circle.  Parameters \n \nr, cdouble \n\nCenter coordinate of disk.  \nradiusdouble \n\nRadius of disk.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for disks that exceed the image size. If None, the full extent of the disk is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, ccndarray of int \n\nPixel coordinates of disk. May be used to directly index into an array, e.g. img[rr, cc] = 1.    Warns \n Deprecated:\n\n New in version 0.17: This function is deprecated and will be removed in scikit-image 0.19. Please use the function named disk instead.      \n circle_perimeter  \nskimage.draw.circle_perimeter(r, c, radius, method='bresenham', shape=None) [source]\n \nGenerate circle perimeter coordinates.  Parameters \n \nr, cint \n\nCentre coordinate of circle.  \nradiusint \n\nRadius of circle.  \nmethod{\u2018bresenham\u2019, \u2018andres\u2019}, optional \n\nbresenham : Bresenham method (default) andres : Andres method  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for circles that exceed the image size. If None, the full extent of the circle is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, cc(N,) ndarray of int \n\nBresenham and Andres\u2019 method: Indices of pixels that belong to the circle perimeter. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Notes Andres method presents the advantage that concentric circles create a disc whereas Bresenham can make holes. There is also less distortions when Andres circles are rotated. Bresenham method is also known as midpoint circle algorithm. Anti-aliased circle generator is available with circle_perimeter_aa. References  \n1  \nJ.E. Bresenham, \u201cAlgorithm for computer control of a digital plotter\u201d, IBM Systems journal, 4 (1965) 25-30.  \n2  \nE. Andres, \u201cDiscrete circles, rings and spheres\u201d, Computers & Graphics, 18 (1994) 695-706.   Examples >>> from skimage.draw import circle_perimeter\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = circle_perimeter(4, 4, 3)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n circle_perimeter_aa  \nskimage.draw.circle_perimeter_aa(r, c, radius, shape=None) [source]\n \nGenerate anti-aliased circle perimeter coordinates.  Parameters \n \nr, cint \n\nCentre coordinate of circle.  \nradiusint \n\nRadius of circle.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for circles that exceed the image size. If None, the full extent of the circle is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, cc, val(N,) ndarray (int, int, float) \n\nIndices of pixels (rr, cc) and intensity values (val). img[rr, cc] = val.     Notes Wu\u2019s method draws anti-aliased circle. This implementation doesn\u2019t use lookup table optimization. Use the function draw.set_color to apply circle_perimeter_aa results to color images. References  \n1  \nX. Wu, \u201cAn efficient antialiasing technique\u201d, In ACM SIGGRAPH Computer Graphics, 25 (1991) 143-152.   Examples >>> from skimage.draw import circle_perimeter_aa\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc, val = circle_perimeter_aa(4, 4, 3)\n>>> img[rr, cc] = val * 255\n>>> img\narray([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,  60, 211, 255, 211,  60,   0,   0,   0],\n       [  0,  60, 194,  43,   0,  43, 194,  60,   0,   0],\n       [  0, 211,  43,   0,   0,   0,  43, 211,   0,   0],\n       [  0, 255,   0,   0,   0,   0,   0, 255,   0,   0],\n       [  0, 211,  43,   0,   0,   0,  43, 211,   0,   0],\n       [  0,  60, 194,  43,   0,  43, 194,  60,   0,   0],\n       [  0,   0,  60, 211, 255, 211,  60,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=uint8)\n >>> from skimage import data, draw\n>>> image = data.chelsea()\n>>> rr, cc, val = draw.circle_perimeter_aa(r=100, c=100, radius=75)\n>>> draw.set_color(image, (rr, cc), [1, 0, 0], alpha=val)\n \n disk  \nskimage.draw.disk(center, radius, *, shape=None) [source]\n \nGenerate coordinates of pixels within circle.  Parameters \n \ncentertuple \n\nCenter coordinate of disk.  \nradiusdouble \n\nRadius of disk.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for disks that exceed the image size. If None, the full extent of the disk is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, ccndarray of int \n\nPixel coordinates of disk. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Examples >>> from skimage.draw import disk\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = disk((4, 4), 5)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n ellipse  \nskimage.draw.ellipse(r, c, r_radius, c_radius, shape=None, rotation=0.0) [source]\n \nGenerate coordinates of pixels within ellipse.  Parameters \n \nr, cdouble \n\nCentre coordinate of ellipse.  \nr_radius, c_radiusdouble \n\nMinor and major semi-axes. (r/r_radius)**2 + (c/c_radius)**2 = 1.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for ellipses which exceed the image size. By default the full extent of the ellipse are used. Must be at least length 2. Only the first two values are used to determine the extent.  \nrotationfloat, optional (default 0.) \n\nSet the ellipse rotation (rotation) in range (-PI, PI) in contra clock wise direction, so PI/2 degree means swap ellipse axis    Returns \n \nrr, ccndarray of int \n\nPixel coordinates of ellipse. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Notes The ellipse equation: ((x * cos(alpha) + y * sin(alpha)) / x_radius) ** 2 +\n((x * sin(alpha) - y * cos(alpha)) / y_radius) ** 2 = 1\n Note that the positions of ellipse without specified shape can have also, negative values, as this is correct on the plane. On the other hand using these ellipse positions for an image afterwards may lead to appearing on the other side of image, because image[-1, -1] = image[end-1, end-1] >>> rr, cc = ellipse(1, 2, 3, 6)\n>>> img = np.zeros((6, 12), dtype=np.uint8)\n>>> img[rr, cc] = 1\n>>> img\narray([[1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1]], dtype=uint8)\n Examples >>> from skimage.draw import ellipse\n>>> img = np.zeros((10, 12), dtype=np.uint8)\n>>> rr, cc = ellipse(5, 6, 3, 5, rotation=np.deg2rad(30))\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n Examples using skimage.draw.ellipse\n \n  Masked Normalized Cross-Correlation  \n\n  Measure region properties   ellipse_perimeter  \nskimage.draw.ellipse_perimeter(r, c, r_radius, c_radius, orientation=0, shape=None) [source]\n \nGenerate ellipse perimeter coordinates.  Parameters \n \nr, cint \n\nCentre coordinate of ellipse.  \nr_radius, c_radiusint \n\nMinor and major semi-axes. (r/r_radius)**2 + (c/c_radius)**2 = 1.  \norientationdouble, optional \n\nMajor axis orientation in clockwise direction as radians.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for ellipses that exceed the image size. If None, the full extent of the ellipse is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, cc(N,) ndarray of int \n\nIndices of pixels that belong to the ellipse perimeter. May be used to directly index into an array, e.g. img[rr, cc] = 1.     References  \n1  \nA Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012 http://members.chello.at/easyfilter/Bresenham.pdf   Examples >>> from skimage.draw import ellipse_perimeter\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = ellipse_perimeter(5, 5, 3, 4)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n Note that the positions of ellipse without specified shape can have also, negative values, as this is correct on the plane. On the other hand using these ellipse positions for an image afterwards may lead to appearing on the other side of image, because image[-1, -1] = image[end-1, end-1] >>> rr, cc = ellipse_perimeter(2, 3, 4, 5)\n>>> img = np.zeros((9, 12), dtype=np.uint8)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n       [0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]], dtype=uint8)\n \n ellipsoid  \nskimage.draw.ellipsoid(a, b, c, spacing=(1.0, 1.0, 1.0), levelset=False) [source]\n \nGenerates ellipsoid with semimajor axes aligned with grid dimensions on grid with specified spacing.  Parameters \n \nafloat \n\nLength of semimajor axis aligned with x-axis.  \nbfloat \n\nLength of semimajor axis aligned with y-axis.  \ncfloat \n\nLength of semimajor axis aligned with z-axis.  \nspacingtuple of floats, length 3 \n\nSpacing in (x, y, z) spatial dimensions.  \nlevelsetbool \n\nIf True, returns the level set for this ellipsoid (signed level set about zero, with positive denoting interior) as np.float64. False returns a binarized version of said level set.    Returns \n \nellip(N, M, P) array \n\nEllipsoid centered in a correctly sized array for given spacing. Boolean dtype unless levelset=True, in which case a float array is returned with the level set above 0.0 representing the ellipsoid.     \n ellipsoid_stats  \nskimage.draw.ellipsoid_stats(a, b, c) [source]\n \nCalculates analytical surface area and volume for ellipsoid with semimajor axes aligned with grid dimensions of specified spacing.  Parameters \n \nafloat \n\nLength of semimajor axis aligned with x-axis.  \nbfloat \n\nLength of semimajor axis aligned with y-axis.  \ncfloat \n\nLength of semimajor axis aligned with z-axis.    Returns \n \nvolfloat \n\nCalculated volume of ellipsoid.  \nsurffloat \n\nCalculated surface area of ellipsoid.     \n line  \nskimage.draw.line(r0, c0, r1, c1) [source]\n \nGenerate line pixel coordinates.  Parameters \n \nr0, c0int \n\nStarting position (row, column).  \nr1, c1int \n\nEnd position (row, column).    Returns \n \nrr, cc(N,) ndarray of int \n\nIndices of pixels that belong to the line. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Notes Anti-aliased line generator is available with line_aa. Examples >>> from skimage.draw import line\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = line(1, 1, 8, 8)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n line_aa  \nskimage.draw.line_aa(r0, c0, r1, c1) [source]\n \nGenerate anti-aliased line pixel coordinates.  Parameters \n \nr0, c0int \n\nStarting position (row, column).  \nr1, c1int \n\nEnd position (row, column).    Returns \n \nrr, cc, val(N,) ndarray (int, int, float) \n\nIndices of pixels (rr, cc) and intensity values (val). img[rr, cc] = val.     References  \n1  \nA Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012 http://members.chello.at/easyfilter/Bresenham.pdf   Examples >>> from skimage.draw import line_aa\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc, val = line_aa(1, 1, 8, 8)\n>>> img[rr, cc] = val * 255\n>>> img\narray([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0, 255,  74,   0,   0,   0,   0,   0,   0,   0],\n       [  0,  74, 255,  74,   0,   0,   0,   0,   0,   0],\n       [  0,   0,  74, 255,  74,   0,   0,   0,   0,   0],\n       [  0,   0,   0,  74, 255,  74,   0,   0,   0,   0],\n       [  0,   0,   0,   0,  74, 255,  74,   0,   0,   0],\n       [  0,   0,   0,   0,   0,  74, 255,  74,   0,   0],\n       [  0,   0,   0,   0,   0,   0,  74, 255,  74,   0],\n       [  0,   0,   0,   0,   0,   0,   0,  74, 255,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=uint8)\n \n line_nd  \nskimage.draw.line_nd(start, stop, *, endpoint=False, integer=True) [source]\n \nDraw a single-pixel thick line in n dimensions. The line produced will be ndim-connected. That is, two subsequent pixels in the line will be either direct or diagonal neighbours in n dimensions.  Parameters \n \nstartarray-like, shape (N,) \n\nThe start coordinates of the line.  \nstoparray-like, shape (N,) \n\nThe end coordinates of the line.  \nendpointbool, optional \n\nWhether to include the endpoint in the returned line. Defaults to False, which allows for easy drawing of multi-point paths.  \nintegerbool, optional \n\nWhether to round the coordinates to integer. If True (default), the returned coordinates can be used to directly index into an array. False could be used for e.g. vector drawing.    Returns \n \ncoordstuple of arrays \n\nThe coordinates of points on the line.     Examples >>> lin = line_nd((1, 1), (5, 2.5), endpoint=False)\n>>> lin\n(array([1, 2, 3, 4]), array([1, 1, 2, 2]))\n>>> im = np.zeros((6, 5), dtype=int)\n>>> im[lin] = 1\n>>> im\narray([[0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0],\n       [0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0]])\n>>> line_nd([2, 1, 1], [5, 5, 2.5], endpoint=True)\n(array([2, 3, 4, 4, 5]), array([1, 2, 3, 4, 5]), array([1, 1, 2, 2, 2]))\n \n polygon  \nskimage.draw.polygon(r, c, shape=None) [source]\n \nGenerate coordinates of pixels within polygon.  Parameters \n \nr(N,) ndarray \n\nRow coordinates of vertices of polygon.  \nc(N,) ndarray \n\nColumn coordinates of vertices of polygon.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for polygons that exceed the image size. If None, the full extent of the polygon is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, ccndarray of int \n\nPixel coordinates of polygon. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Examples >>> from skimage.draw import polygon\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> r = np.array([1, 2, 8])\n>>> c = np.array([1, 7, 4])\n>>> rr, cc = polygon(r, c)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n polygon2mask  \nskimage.draw.polygon2mask(image_shape, polygon) [source]\n \nCompute a mask from polygon.  Parameters \n \nimage_shapetuple of size 2. \n\nThe shape of the mask.  \npolygonarray_like. \n\nThe polygon coordinates of shape (N, 2) where N is the number of points.    Returns \n \nmask2-D ndarray of type \u2018bool\u2019. \n\nThe mask that corresponds to the input polygon.     Notes This function does not do any border checking, so that all the vertices need to be within the given shape. Examples >>> image_shape = (128, 128)\n>>> polygon = np.array([[60, 100], [100, 40], [40, 40]])\n>>> mask = polygon2mask(image_shape, polygon)\n>>> mask.shape\n(128, 128)\n \n polygon_perimeter  \nskimage.draw.polygon_perimeter(r, c, shape=None, clip=False) [source]\n \nGenerate polygon perimeter coordinates.  Parameters \n \nr(N,) ndarray \n\nRow coordinates of vertices of polygon.  \nc(N,) ndarray \n\nColumn coordinates of vertices of polygon.  \nshapetuple, optional \n\nImage shape which is used to determine maximum extents of output pixel coordinates. This is useful for polygons that exceed the image size. If None, the full extents of the polygon is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.  \nclipbool, optional \n\nWhether to clip the polygon to the provided shape. If this is set to True, the drawn figure will always be a closed polygon with all edges visible.    Returns \n \nrr, ccndarray of int \n\nPixel coordinates of polygon. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Examples >>> from skimage.draw import polygon_perimeter\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = polygon_perimeter([5, -1, 5, 10],\n...                            [-1, 5, 11, 5],\n...                            shape=img.shape, clip=True)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 1, 1, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 1, 0, 0, 0, 1, 1, 0],\n       [0, 0, 0, 0, 1, 1, 1, 0, 0, 0]], dtype=uint8)\n \n random_shapes  \nskimage.draw.random_shapes(image_shape, max_shapes, min_shapes=1, min_size=2, max_size=None, multichannel=True, num_channels=3, shape=None, intensity_range=None, allow_overlap=False, num_trials=100, random_seed=None) [source]\n \nGenerate an image with random shapes, labeled with bounding boxes. The image is populated with random shapes with random sizes, random locations, and random colors, with or without overlap. Shapes have random (row, col) starting coordinates and random sizes bounded by min_size and max_size. It can occur that a randomly generated shape will not fit the image at all. In that case, the algorithm will try again with new starting coordinates a certain number of times. However, it also means that some shapes may be skipped altogether. In that case, this function will generate fewer shapes than requested.  Parameters \n \nimage_shapetuple \n\nThe number of rows and columns of the image to generate.  \nmax_shapesint \n\nThe maximum number of shapes to (attempt to) fit into the shape.  \nmin_shapesint, optional \n\nThe minimum number of shapes to (attempt to) fit into the shape.  \nmin_sizeint, optional \n\nThe minimum dimension of each shape to fit into the image.  \nmax_sizeint, optional \n\nThe maximum dimension of each shape to fit into the image.  \nmultichannelbool, optional \n\nIf True, the generated image has num_channels color channels, otherwise generates grayscale image.  \nnum_channelsint, optional \n\nNumber of channels in the generated image. If 1, generate monochrome images, else color images with multiple channels. Ignored if multichannel is set to False.  \nshape{rectangle, circle, triangle, ellipse, None} str, optional \n\nThe name of the shape to generate or None to pick random ones.  \nintensity_range{tuple of tuples of uint8, tuple of uint8}, optional \n\nThe range of values to sample pixel values from. For grayscale images the format is (min, max). For multichannel - ((min, max),) if the ranges are equal across the channels, and ((min_0, max_0), \u2026 (min_N, max_N)) if they differ. As the function supports generation of uint8 arrays only, the maximum range is (0, 255). If None, set to (0, 254) for each channel reserving color of intensity = 255 for background.  \nallow_overlapbool, optional \n\nIf True, allow shapes to overlap.  \nnum_trialsint, optional \n\nHow often to attempt to fit a shape into the image before skipping it.  \nrandom_seedint, optional \n\nSeed to initialize the random number generator. If None, a random seed from the operating system is used.    Returns \n \nimageuint8 array \n\nAn image with the fitted shapes.  \nlabelslist \n\nA list of labels, one per shape in the image. Each label is a (category, ((r0, r1), (c0, c1))) tuple specifying the category and bounding box coordinates of the shape.     Examples >>> import skimage.draw\n>>> image, labels = skimage.draw.random_shapes((32, 32), max_shapes=3)\n>>> image \narray([\n   [[255, 255, 255],\n    [255, 255, 255],\n    [255, 255, 255],\n    ...,\n    [255, 255, 255],\n    [255, 255, 255],\n    [255, 255, 255]]], dtype=uint8)\n>>> labels \n[('circle', ((22, 18), (25, 21))),\n ('triangle', ((5, 6), (13, 13)))]\n \n rectangle  \nskimage.draw.rectangle(start, end=None, extent=None, shape=None) [source]\n \nGenerate coordinates of pixels within a rectangle.  Parameters \n \nstarttuple \n\nOrigin point of the rectangle, e.g., ([plane,] row, column).  \nendtuple \n\nEnd point of the rectangle ([plane,] row, column). For a 2D matrix, the slice defined by the rectangle is [start:(end+1)]. Either end or extent must be specified.  \nextenttuple \n\nThe extent (size) of the drawn rectangle. E.g., ([num_planes,] num_rows, num_cols). Either end or extent must be specified. A negative extent is valid, and will result in a rectangle going along the opposite direction. If extent is negative, the start point is not included.  \nshapetuple, optional \n\nImage shape used to determine the maximum bounds of the output coordinates. This is useful for clipping rectangles that exceed the image size. By default, no clipping is done.    Returns \n \ncoordsarray of int, shape (Ndim, Npoints) \n\nThe coordinates of all pixels in the rectangle.     Notes This function can be applied to N-dimensional images, by passing start and end or extent as tuples of length N. Examples >>> import numpy as np\n>>> from skimage.draw import rectangle\n>>> img = np.zeros((5, 5), dtype=np.uint8)\n>>> start = (1, 1)\n>>> extent = (3, 3)\n>>> rr, cc = rectangle(start, extent=extent, shape=img.shape)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n >>> img = np.zeros((5, 5), dtype=np.uint8)\n>>> start = (0, 1)\n>>> end = (3, 3)\n>>> rr, cc = rectangle(start, end=end, shape=img.shape)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n >>> import numpy as np\n>>> from skimage.draw import rectangle\n>>> img = np.zeros((6, 6), dtype=np.uint8)\n>>> start = (3, 3)\n>>>\n>>> rr, cc = rectangle(start, extent=(2, 2))\n>>> img[rr, cc] = 1\n>>> rr, cc = rectangle(start, extent=(-2, 2))\n>>> img[rr, cc] = 2\n>>> rr, cc = rectangle(start, extent=(-2, -2))\n>>> img[rr, cc] = 3\n>>> rr, cc = rectangle(start, extent=(2, -2))\n>>> img[rr, cc] = 4\n>>> print(img)\n[[0 0 0 0 0 0]\n [0 3 3 2 2 0]\n [0 3 3 2 2 0]\n [0 4 4 1 1 0]\n [0 4 4 1 1 0]\n [0 0 0 0 0 0]]\n \n rectangle_perimeter  \nskimage.draw.rectangle_perimeter(start, end=None, extent=None, shape=None, clip=False) [source]\n \nGenerate coordinates of pixels that are exactly around a rectangle.  Parameters \n \nstarttuple \n\nOrigin point of the inner rectangle, e.g., (row, column).  \nendtuple \n\nEnd point of the inner rectangle (row, column). For a 2D matrix, the slice defined by inner the rectangle is [start:(end+1)]. Either end or extent must be specified.  \nextenttuple \n\nThe extent (size) of the inner rectangle. E.g., (num_rows, num_cols). Either end or extent must be specified. Negative extents are permitted. See rectangle to better understand how they behave.  \nshapetuple, optional \n\nImage shape used to determine the maximum bounds of the output coordinates. This is useful for clipping perimeters that exceed the image size. By default, no clipping is done. Must be at least length 2. Only the first two values are used to determine the extent of the input image.  \nclipbool, optional \n\nWhether to clip the perimeter to the provided shape. If this is set to True, the drawn figure will always be a closed polygon with all edges visible.    Returns \n \ncoordsarray of int, shape (2, Npoints) \n\nThe coordinates of all pixels in the rectangle.     Examples >>> import numpy as np\n>>> from skimage.draw import rectangle_perimeter\n>>> img = np.zeros((5, 6), dtype=np.uint8)\n>>> start = (2, 3)\n>>> end = (3, 4)\n>>> rr, cc = rectangle_perimeter(start, end=end, shape=img.shape)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1],\n       [0, 0, 1, 0, 0, 1],\n       [0, 0, 1, 0, 0, 1],\n       [0, 0, 1, 1, 1, 1]], dtype=uint8)\n >>> img = np.zeros((5, 5), dtype=np.uint8)\n>>> r, c = rectangle_perimeter(start, (10, 10), shape=img.shape, clip=True)\n>>> img[r, c] = 1\n>>> img\narray([[0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1],\n       [0, 0, 1, 0, 1],\n       [0, 0, 1, 0, 1],\n       [0, 0, 1, 1, 1]], dtype=uint8)\n \n set_color  \nskimage.draw.set_color(image, coords, color, alpha=1) [source]\n \nSet pixel color in the image at the given coordinates. Note that this function modifies the color of the image in-place. Coordinates that exceed the shape of the image will be ignored.  Parameters \n \nimage(M, N, D) ndarray \n\nImage  \ncoordstuple of ((P,) ndarray, (P,) ndarray) \n\nRow and column coordinates of pixels to be colored.  \ncolor(D,) ndarray \n\nColor to be assigned to coordinates in the image.  \nalphascalar or (N,) ndarray \n\nAlpha values used to blend color with image. 0 is transparent, 1 is opaque.     Examples >>> from skimage.draw import line, set_color\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = line(1, 1, 20, 20)\n>>> set_color(img, (rr, cc), 1)\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], dtype=uint8)\n \n\n"}, {"name": "draw.bezier_curve()", "path": "api/skimage.draw#skimage.draw.bezier_curve", "type": "draw", "text": " \nskimage.draw.bezier_curve(r0, c0, r1, c1, r2, c2, weight, shape=None) [source]\n \nGenerate Bezier curve coordinates.  Parameters \n \nr0, c0int \n\nCoordinates of the first control point.  \nr1, c1int \n\nCoordinates of the middle control point.  \nr2, c2int \n\nCoordinates of the last control point.  \nweightdouble \n\nMiddle control point weight, it describes the line tension.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for curves that exceed the image size. If None, the full extent of the curve is used.    Returns \n \nrr, cc(N,) ndarray of int \n\nIndices of pixels that belong to the Bezier curve. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Notes The algorithm is the rational quadratic algorithm presented in reference [1]. References  \n1  \nA Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012 http://members.chello.at/easyfilter/Bresenham.pdf   Examples >>> import numpy as np\n>>> from skimage.draw import bezier_curve\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = bezier_curve(1, 5, 5, -2, 8, 8, 2)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "draw.circle()", "path": "api/skimage.draw#skimage.draw.circle", "type": "draw", "text": " \nskimage.draw.circle(r, c, radius, shape=None) [source]\n \nGenerate coordinates of pixels within circle.  Parameters \n \nr, cdouble \n\nCenter coordinate of disk.  \nradiusdouble \n\nRadius of disk.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for disks that exceed the image size. If None, the full extent of the disk is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, ccndarray of int \n\nPixel coordinates of disk. May be used to directly index into an array, e.g. img[rr, cc] = 1.    Warns \n Deprecated:\n\n New in version 0.17: This function is deprecated and will be removed in scikit-image 0.19. Please use the function named disk instead.      \n"}, {"name": "draw.circle_perimeter()", "path": "api/skimage.draw#skimage.draw.circle_perimeter", "type": "draw", "text": " \nskimage.draw.circle_perimeter(r, c, radius, method='bresenham', shape=None) [source]\n \nGenerate circle perimeter coordinates.  Parameters \n \nr, cint \n\nCentre coordinate of circle.  \nradiusint \n\nRadius of circle.  \nmethod{\u2018bresenham\u2019, \u2018andres\u2019}, optional \n\nbresenham : Bresenham method (default) andres : Andres method  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for circles that exceed the image size. If None, the full extent of the circle is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, cc(N,) ndarray of int \n\nBresenham and Andres\u2019 method: Indices of pixels that belong to the circle perimeter. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Notes Andres method presents the advantage that concentric circles create a disc whereas Bresenham can make holes. There is also less distortions when Andres circles are rotated. Bresenham method is also known as midpoint circle algorithm. Anti-aliased circle generator is available with circle_perimeter_aa. References  \n1  \nJ.E. Bresenham, \u201cAlgorithm for computer control of a digital plotter\u201d, IBM Systems journal, 4 (1965) 25-30.  \n2  \nE. Andres, \u201cDiscrete circles, rings and spheres\u201d, Computers & Graphics, 18 (1994) 695-706.   Examples >>> from skimage.draw import circle_perimeter\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = circle_perimeter(4, 4, 3)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "draw.circle_perimeter_aa()", "path": "api/skimage.draw#skimage.draw.circle_perimeter_aa", "type": "draw", "text": " \nskimage.draw.circle_perimeter_aa(r, c, radius, shape=None) [source]\n \nGenerate anti-aliased circle perimeter coordinates.  Parameters \n \nr, cint \n\nCentre coordinate of circle.  \nradiusint \n\nRadius of circle.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for circles that exceed the image size. If None, the full extent of the circle is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, cc, val(N,) ndarray (int, int, float) \n\nIndices of pixels (rr, cc) and intensity values (val). img[rr, cc] = val.     Notes Wu\u2019s method draws anti-aliased circle. This implementation doesn\u2019t use lookup table optimization. Use the function draw.set_color to apply circle_perimeter_aa results to color images. References  \n1  \nX. Wu, \u201cAn efficient antialiasing technique\u201d, In ACM SIGGRAPH Computer Graphics, 25 (1991) 143-152.   Examples >>> from skimage.draw import circle_perimeter_aa\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc, val = circle_perimeter_aa(4, 4, 3)\n>>> img[rr, cc] = val * 255\n>>> img\narray([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,  60, 211, 255, 211,  60,   0,   0,   0],\n       [  0,  60, 194,  43,   0,  43, 194,  60,   0,   0],\n       [  0, 211,  43,   0,   0,   0,  43, 211,   0,   0],\n       [  0, 255,   0,   0,   0,   0,   0, 255,   0,   0],\n       [  0, 211,  43,   0,   0,   0,  43, 211,   0,   0],\n       [  0,  60, 194,  43,   0,  43, 194,  60,   0,   0],\n       [  0,   0,  60, 211, 255, 211,  60,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=uint8)\n >>> from skimage import data, draw\n>>> image = data.chelsea()\n>>> rr, cc, val = draw.circle_perimeter_aa(r=100, c=100, radius=75)\n>>> draw.set_color(image, (rr, cc), [1, 0, 0], alpha=val)\n \n"}, {"name": "draw.disk()", "path": "api/skimage.draw#skimage.draw.disk", "type": "draw", "text": " \nskimage.draw.disk(center, radius, *, shape=None) [source]\n \nGenerate coordinates of pixels within circle.  Parameters \n \ncentertuple \n\nCenter coordinate of disk.  \nradiusdouble \n\nRadius of disk.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for disks that exceed the image size. If None, the full extent of the disk is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, ccndarray of int \n\nPixel coordinates of disk. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Examples >>> from skimage.draw import disk\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = disk((4, 4), 5)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "draw.ellipse()", "path": "api/skimage.draw#skimage.draw.ellipse", "type": "draw", "text": " \nskimage.draw.ellipse(r, c, r_radius, c_radius, shape=None, rotation=0.0) [source]\n \nGenerate coordinates of pixels within ellipse.  Parameters \n \nr, cdouble \n\nCentre coordinate of ellipse.  \nr_radius, c_radiusdouble \n\nMinor and major semi-axes. (r/r_radius)**2 + (c/c_radius)**2 = 1.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for ellipses which exceed the image size. By default the full extent of the ellipse are used. Must be at least length 2. Only the first two values are used to determine the extent.  \nrotationfloat, optional (default 0.) \n\nSet the ellipse rotation (rotation) in range (-PI, PI) in contra clock wise direction, so PI/2 degree means swap ellipse axis    Returns \n \nrr, ccndarray of int \n\nPixel coordinates of ellipse. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Notes The ellipse equation: ((x * cos(alpha) + y * sin(alpha)) / x_radius) ** 2 +\n((x * sin(alpha) - y * cos(alpha)) / y_radius) ** 2 = 1\n Note that the positions of ellipse without specified shape can have also, negative values, as this is correct on the plane. On the other hand using these ellipse positions for an image afterwards may lead to appearing on the other side of image, because image[-1, -1] = image[end-1, end-1] >>> rr, cc = ellipse(1, 2, 3, 6)\n>>> img = np.zeros((6, 12), dtype=np.uint8)\n>>> img[rr, cc] = 1\n>>> img\narray([[1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1]], dtype=uint8)\n Examples >>> from skimage.draw import ellipse\n>>> img = np.zeros((10, 12), dtype=np.uint8)\n>>> rr, cc = ellipse(5, 6, 3, 5, rotation=np.deg2rad(30))\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "draw.ellipse_perimeter()", "path": "api/skimage.draw#skimage.draw.ellipse_perimeter", "type": "draw", "text": " \nskimage.draw.ellipse_perimeter(r, c, r_radius, c_radius, orientation=0, shape=None) [source]\n \nGenerate ellipse perimeter coordinates.  Parameters \n \nr, cint \n\nCentre coordinate of ellipse.  \nr_radius, c_radiusint \n\nMinor and major semi-axes. (r/r_radius)**2 + (c/c_radius)**2 = 1.  \norientationdouble, optional \n\nMajor axis orientation in clockwise direction as radians.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for ellipses that exceed the image size. If None, the full extent of the ellipse is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, cc(N,) ndarray of int \n\nIndices of pixels that belong to the ellipse perimeter. May be used to directly index into an array, e.g. img[rr, cc] = 1.     References  \n1  \nA Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012 http://members.chello.at/easyfilter/Bresenham.pdf   Examples >>> from skimage.draw import ellipse_perimeter\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = ellipse_perimeter(5, 5, 3, 4)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n Note that the positions of ellipse without specified shape can have also, negative values, as this is correct on the plane. On the other hand using these ellipse positions for an image afterwards may lead to appearing on the other side of image, because image[-1, -1] = image[end-1, end-1] >>> rr, cc = ellipse_perimeter(2, 3, 4, 5)\n>>> img = np.zeros((9, 12), dtype=np.uint8)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n       [0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "draw.ellipsoid()", "path": "api/skimage.draw#skimage.draw.ellipsoid", "type": "draw", "text": " \nskimage.draw.ellipsoid(a, b, c, spacing=(1.0, 1.0, 1.0), levelset=False) [source]\n \nGenerates ellipsoid with semimajor axes aligned with grid dimensions on grid with specified spacing.  Parameters \n \nafloat \n\nLength of semimajor axis aligned with x-axis.  \nbfloat \n\nLength of semimajor axis aligned with y-axis.  \ncfloat \n\nLength of semimajor axis aligned with z-axis.  \nspacingtuple of floats, length 3 \n\nSpacing in (x, y, z) spatial dimensions.  \nlevelsetbool \n\nIf True, returns the level set for this ellipsoid (signed level set about zero, with positive denoting interior) as np.float64. False returns a binarized version of said level set.    Returns \n \nellip(N, M, P) array \n\nEllipsoid centered in a correctly sized array for given spacing. Boolean dtype unless levelset=True, in which case a float array is returned with the level set above 0.0 representing the ellipsoid.     \n"}, {"name": "draw.ellipsoid_stats()", "path": "api/skimage.draw#skimage.draw.ellipsoid_stats", "type": "draw", "text": " \nskimage.draw.ellipsoid_stats(a, b, c) [source]\n \nCalculates analytical surface area and volume for ellipsoid with semimajor axes aligned with grid dimensions of specified spacing.  Parameters \n \nafloat \n\nLength of semimajor axis aligned with x-axis.  \nbfloat \n\nLength of semimajor axis aligned with y-axis.  \ncfloat \n\nLength of semimajor axis aligned with z-axis.    Returns \n \nvolfloat \n\nCalculated volume of ellipsoid.  \nsurffloat \n\nCalculated surface area of ellipsoid.     \n"}, {"name": "draw.line()", "path": "api/skimage.draw#skimage.draw.line", "type": "draw", "text": " \nskimage.draw.line(r0, c0, r1, c1) [source]\n \nGenerate line pixel coordinates.  Parameters \n \nr0, c0int \n\nStarting position (row, column).  \nr1, c1int \n\nEnd position (row, column).    Returns \n \nrr, cc(N,) ndarray of int \n\nIndices of pixels that belong to the line. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Notes Anti-aliased line generator is available with line_aa. Examples >>> from skimage.draw import line\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = line(1, 1, 8, 8)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "draw.line_aa()", "path": "api/skimage.draw#skimage.draw.line_aa", "type": "draw", "text": " \nskimage.draw.line_aa(r0, c0, r1, c1) [source]\n \nGenerate anti-aliased line pixel coordinates.  Parameters \n \nr0, c0int \n\nStarting position (row, column).  \nr1, c1int \n\nEnd position (row, column).    Returns \n \nrr, cc, val(N,) ndarray (int, int, float) \n\nIndices of pixels (rr, cc) and intensity values (val). img[rr, cc] = val.     References  \n1  \nA Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012 http://members.chello.at/easyfilter/Bresenham.pdf   Examples >>> from skimage.draw import line_aa\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc, val = line_aa(1, 1, 8, 8)\n>>> img[rr, cc] = val * 255\n>>> img\narray([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0, 255,  74,   0,   0,   0,   0,   0,   0,   0],\n       [  0,  74, 255,  74,   0,   0,   0,   0,   0,   0],\n       [  0,   0,  74, 255,  74,   0,   0,   0,   0,   0],\n       [  0,   0,   0,  74, 255,  74,   0,   0,   0,   0],\n       [  0,   0,   0,   0,  74, 255,  74,   0,   0,   0],\n       [  0,   0,   0,   0,   0,  74, 255,  74,   0,   0],\n       [  0,   0,   0,   0,   0,   0,  74, 255,  74,   0],\n       [  0,   0,   0,   0,   0,   0,   0,  74, 255,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=uint8)\n \n"}, {"name": "draw.line_nd()", "path": "api/skimage.draw#skimage.draw.line_nd", "type": "draw", "text": " \nskimage.draw.line_nd(start, stop, *, endpoint=False, integer=True) [source]\n \nDraw a single-pixel thick line in n dimensions. The line produced will be ndim-connected. That is, two subsequent pixels in the line will be either direct or diagonal neighbours in n dimensions.  Parameters \n \nstartarray-like, shape (N,) \n\nThe start coordinates of the line.  \nstoparray-like, shape (N,) \n\nThe end coordinates of the line.  \nendpointbool, optional \n\nWhether to include the endpoint in the returned line. Defaults to False, which allows for easy drawing of multi-point paths.  \nintegerbool, optional \n\nWhether to round the coordinates to integer. If True (default), the returned coordinates can be used to directly index into an array. False could be used for e.g. vector drawing.    Returns \n \ncoordstuple of arrays \n\nThe coordinates of points on the line.     Examples >>> lin = line_nd((1, 1), (5, 2.5), endpoint=False)\n>>> lin\n(array([1, 2, 3, 4]), array([1, 1, 2, 2]))\n>>> im = np.zeros((6, 5), dtype=int)\n>>> im[lin] = 1\n>>> im\narray([[0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0],\n       [0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0]])\n>>> line_nd([2, 1, 1], [5, 5, 2.5], endpoint=True)\n(array([2, 3, 4, 4, 5]), array([1, 2, 3, 4, 5]), array([1, 1, 2, 2, 2]))\n \n"}, {"name": "draw.polygon()", "path": "api/skimage.draw#skimage.draw.polygon", "type": "draw", "text": " \nskimage.draw.polygon(r, c, shape=None) [source]\n \nGenerate coordinates of pixels within polygon.  Parameters \n \nr(N,) ndarray \n\nRow coordinates of vertices of polygon.  \nc(N,) ndarray \n\nColumn coordinates of vertices of polygon.  \nshapetuple, optional \n\nImage shape which is used to determine the maximum extent of output pixel coordinates. This is useful for polygons that exceed the image size. If None, the full extent of the polygon is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.    Returns \n \nrr, ccndarray of int \n\nPixel coordinates of polygon. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Examples >>> from skimage.draw import polygon\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> r = np.array([1, 2, 8])\n>>> c = np.array([1, 7, 4])\n>>> rr, cc = polygon(r, c)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "draw.polygon2mask()", "path": "api/skimage.draw#skimage.draw.polygon2mask", "type": "draw", "text": " \nskimage.draw.polygon2mask(image_shape, polygon) [source]\n \nCompute a mask from polygon.  Parameters \n \nimage_shapetuple of size 2. \n\nThe shape of the mask.  \npolygonarray_like. \n\nThe polygon coordinates of shape (N, 2) where N is the number of points.    Returns \n \nmask2-D ndarray of type \u2018bool\u2019. \n\nThe mask that corresponds to the input polygon.     Notes This function does not do any border checking, so that all the vertices need to be within the given shape. Examples >>> image_shape = (128, 128)\n>>> polygon = np.array([[60, 100], [100, 40], [40, 40]])\n>>> mask = polygon2mask(image_shape, polygon)\n>>> mask.shape\n(128, 128)\n \n"}, {"name": "draw.polygon_perimeter()", "path": "api/skimage.draw#skimage.draw.polygon_perimeter", "type": "draw", "text": " \nskimage.draw.polygon_perimeter(r, c, shape=None, clip=False) [source]\n \nGenerate polygon perimeter coordinates.  Parameters \n \nr(N,) ndarray \n\nRow coordinates of vertices of polygon.  \nc(N,) ndarray \n\nColumn coordinates of vertices of polygon.  \nshapetuple, optional \n\nImage shape which is used to determine maximum extents of output pixel coordinates. This is useful for polygons that exceed the image size. If None, the full extents of the polygon is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.  \nclipbool, optional \n\nWhether to clip the polygon to the provided shape. If this is set to True, the drawn figure will always be a closed polygon with all edges visible.    Returns \n \nrr, ccndarray of int \n\nPixel coordinates of polygon. May be used to directly index into an array, e.g. img[rr, cc] = 1.     Examples >>> from skimage.draw import polygon_perimeter\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = polygon_perimeter([5, -1, 5, 10],\n...                            [-1, 5, 11, 5],\n...                            shape=img.shape, clip=True)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 1, 1, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 1, 0, 0, 0, 1, 1, 0],\n       [0, 0, 0, 0, 1, 1, 1, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "draw.random_shapes()", "path": "api/skimage.draw#skimage.draw.random_shapes", "type": "draw", "text": " \nskimage.draw.random_shapes(image_shape, max_shapes, min_shapes=1, min_size=2, max_size=None, multichannel=True, num_channels=3, shape=None, intensity_range=None, allow_overlap=False, num_trials=100, random_seed=None) [source]\n \nGenerate an image with random shapes, labeled with bounding boxes. The image is populated with random shapes with random sizes, random locations, and random colors, with or without overlap. Shapes have random (row, col) starting coordinates and random sizes bounded by min_size and max_size. It can occur that a randomly generated shape will not fit the image at all. In that case, the algorithm will try again with new starting coordinates a certain number of times. However, it also means that some shapes may be skipped altogether. In that case, this function will generate fewer shapes than requested.  Parameters \n \nimage_shapetuple \n\nThe number of rows and columns of the image to generate.  \nmax_shapesint \n\nThe maximum number of shapes to (attempt to) fit into the shape.  \nmin_shapesint, optional \n\nThe minimum number of shapes to (attempt to) fit into the shape.  \nmin_sizeint, optional \n\nThe minimum dimension of each shape to fit into the image.  \nmax_sizeint, optional \n\nThe maximum dimension of each shape to fit into the image.  \nmultichannelbool, optional \n\nIf True, the generated image has num_channels color channels, otherwise generates grayscale image.  \nnum_channelsint, optional \n\nNumber of channels in the generated image. If 1, generate monochrome images, else color images with multiple channels. Ignored if multichannel is set to False.  \nshape{rectangle, circle, triangle, ellipse, None} str, optional \n\nThe name of the shape to generate or None to pick random ones.  \nintensity_range{tuple of tuples of uint8, tuple of uint8}, optional \n\nThe range of values to sample pixel values from. For grayscale images the format is (min, max). For multichannel - ((min, max),) if the ranges are equal across the channels, and ((min_0, max_0), \u2026 (min_N, max_N)) if they differ. As the function supports generation of uint8 arrays only, the maximum range is (0, 255). If None, set to (0, 254) for each channel reserving color of intensity = 255 for background.  \nallow_overlapbool, optional \n\nIf True, allow shapes to overlap.  \nnum_trialsint, optional \n\nHow often to attempt to fit a shape into the image before skipping it.  \nrandom_seedint, optional \n\nSeed to initialize the random number generator. If None, a random seed from the operating system is used.    Returns \n \nimageuint8 array \n\nAn image with the fitted shapes.  \nlabelslist \n\nA list of labels, one per shape in the image. Each label is a (category, ((r0, r1), (c0, c1))) tuple specifying the category and bounding box coordinates of the shape.     Examples >>> import skimage.draw\n>>> image, labels = skimage.draw.random_shapes((32, 32), max_shapes=3)\n>>> image \narray([\n   [[255, 255, 255],\n    [255, 255, 255],\n    [255, 255, 255],\n    ...,\n    [255, 255, 255],\n    [255, 255, 255],\n    [255, 255, 255]]], dtype=uint8)\n>>> labels \n[('circle', ((22, 18), (25, 21))),\n ('triangle', ((5, 6), (13, 13)))]\n \n"}, {"name": "draw.rectangle()", "path": "api/skimage.draw#skimage.draw.rectangle", "type": "draw", "text": " \nskimage.draw.rectangle(start, end=None, extent=None, shape=None) [source]\n \nGenerate coordinates of pixels within a rectangle.  Parameters \n \nstarttuple \n\nOrigin point of the rectangle, e.g., ([plane,] row, column).  \nendtuple \n\nEnd point of the rectangle ([plane,] row, column). For a 2D matrix, the slice defined by the rectangle is [start:(end+1)]. Either end or extent must be specified.  \nextenttuple \n\nThe extent (size) of the drawn rectangle. E.g., ([num_planes,] num_rows, num_cols). Either end or extent must be specified. A negative extent is valid, and will result in a rectangle going along the opposite direction. If extent is negative, the start point is not included.  \nshapetuple, optional \n\nImage shape used to determine the maximum bounds of the output coordinates. This is useful for clipping rectangles that exceed the image size. By default, no clipping is done.    Returns \n \ncoordsarray of int, shape (Ndim, Npoints) \n\nThe coordinates of all pixels in the rectangle.     Notes This function can be applied to N-dimensional images, by passing start and end or extent as tuples of length N. Examples >>> import numpy as np\n>>> from skimage.draw import rectangle\n>>> img = np.zeros((5, 5), dtype=np.uint8)\n>>> start = (1, 1)\n>>> extent = (3, 3)\n>>> rr, cc = rectangle(start, extent=extent, shape=img.shape)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n >>> img = np.zeros((5, 5), dtype=np.uint8)\n>>> start = (0, 1)\n>>> end = (3, 3)\n>>> rr, cc = rectangle(start, end=end, shape=img.shape)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n >>> import numpy as np\n>>> from skimage.draw import rectangle\n>>> img = np.zeros((6, 6), dtype=np.uint8)\n>>> start = (3, 3)\n>>>\n>>> rr, cc = rectangle(start, extent=(2, 2))\n>>> img[rr, cc] = 1\n>>> rr, cc = rectangle(start, extent=(-2, 2))\n>>> img[rr, cc] = 2\n>>> rr, cc = rectangle(start, extent=(-2, -2))\n>>> img[rr, cc] = 3\n>>> rr, cc = rectangle(start, extent=(2, -2))\n>>> img[rr, cc] = 4\n>>> print(img)\n[[0 0 0 0 0 0]\n [0 3 3 2 2 0]\n [0 3 3 2 2 0]\n [0 4 4 1 1 0]\n [0 4 4 1 1 0]\n [0 0 0 0 0 0]]\n \n"}, {"name": "draw.rectangle_perimeter()", "path": "api/skimage.draw#skimage.draw.rectangle_perimeter", "type": "draw", "text": " \nskimage.draw.rectangle_perimeter(start, end=None, extent=None, shape=None, clip=False) [source]\n \nGenerate coordinates of pixels that are exactly around a rectangle.  Parameters \n \nstarttuple \n\nOrigin point of the inner rectangle, e.g., (row, column).  \nendtuple \n\nEnd point of the inner rectangle (row, column). For a 2D matrix, the slice defined by inner the rectangle is [start:(end+1)]. Either end or extent must be specified.  \nextenttuple \n\nThe extent (size) of the inner rectangle. E.g., (num_rows, num_cols). Either end or extent must be specified. Negative extents are permitted. See rectangle to better understand how they behave.  \nshapetuple, optional \n\nImage shape used to determine the maximum bounds of the output coordinates. This is useful for clipping perimeters that exceed the image size. By default, no clipping is done. Must be at least length 2. Only the first two values are used to determine the extent of the input image.  \nclipbool, optional \n\nWhether to clip the perimeter to the provided shape. If this is set to True, the drawn figure will always be a closed polygon with all edges visible.    Returns \n \ncoordsarray of int, shape (2, Npoints) \n\nThe coordinates of all pixels in the rectangle.     Examples >>> import numpy as np\n>>> from skimage.draw import rectangle_perimeter\n>>> img = np.zeros((5, 6), dtype=np.uint8)\n>>> start = (2, 3)\n>>> end = (3, 4)\n>>> rr, cc = rectangle_perimeter(start, end=end, shape=img.shape)\n>>> img[rr, cc] = 1\n>>> img\narray([[0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1],\n       [0, 0, 1, 0, 0, 1],\n       [0, 0, 1, 0, 0, 1],\n       [0, 0, 1, 1, 1, 1]], dtype=uint8)\n >>> img = np.zeros((5, 5), dtype=np.uint8)\n>>> r, c = rectangle_perimeter(start, (10, 10), shape=img.shape, clip=True)\n>>> img[r, c] = 1\n>>> img\narray([[0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1],\n       [0, 0, 1, 0, 1],\n       [0, 0, 1, 0, 1],\n       [0, 0, 1, 1, 1]], dtype=uint8)\n \n"}, {"name": "draw.set_color()", "path": "api/skimage.draw#skimage.draw.set_color", "type": "draw", "text": " \nskimage.draw.set_color(image, coords, color, alpha=1) [source]\n \nSet pixel color in the image at the given coordinates. Note that this function modifies the color of the image in-place. Coordinates that exceed the shape of the image will be ignored.  Parameters \n \nimage(M, N, D) ndarray \n\nImage  \ncoordstuple of ((P,) ndarray, (P,) ndarray) \n\nRow and column coordinates of pixels to be colored.  \ncolor(D,) ndarray \n\nColor to be assigned to coordinates in the image.  \nalphascalar or (N,) ndarray \n\nAlpha values used to blend color with image. 0 is transparent, 1 is opaque.     Examples >>> from skimage.draw import line, set_color\n>>> img = np.zeros((10, 10), dtype=np.uint8)\n>>> rr, cc = line(1, 1, 20, 20)\n>>> set_color(img, (rr, cc), 1)\n>>> img\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], dtype=uint8)\n \n"}, {"name": "dtype_limits()", "path": "api/skimage#skimage.dtype_limits", "type": "skimage", "text": " \nskimage.dtype_limits(image, clip_negative=False) [source]\n \nReturn intensity limits, i.e. (min, max) tuple, of the image\u2019s dtype.  Parameters \n \nimagendarray \n\nInput image.  \nclip_negativebool, optional \n\nIf True, clip the negative range (i.e. return 0 for min intensity) even if the image dtype allows negative values.    Returns \n \nimin, imaxtuple \n\nLower and upper intensity limits.     \n"}, {"name": "ensure_python_version()", "path": "api/skimage#skimage.ensure_python_version", "type": "skimage", "text": " \nskimage.ensure_python_version(min_version) [source]\n\n"}, {"name": "exposure", "path": "api/skimage.exposure", "type": "exposure", "text": "Module: exposure  \nskimage.exposure.adjust_gamma(image[, \u2026]) Performs Gamma Correction on the input image.  \nskimage.exposure.adjust_log(image[, gain, inv]) Performs Logarithmic correction on the input image.  \nskimage.exposure.adjust_sigmoid(image[, \u2026]) Performs Sigmoid Correction on the input image.  \nskimage.exposure.cumulative_distribution(image) Return cumulative distribution function (cdf) for the given image.  \nskimage.exposure.equalize_adapthist(image[, \u2026]) Contrast Limited Adaptive Histogram Equalization (CLAHE).  \nskimage.exposure.equalize_hist(image[, \u2026]) Return image after histogram equalization.  \nskimage.exposure.histogram(image[, nbins, \u2026]) Return histogram of image.  \nskimage.exposure.is_low_contrast(image[, \u2026]) Determine if an image is low contrast.  \nskimage.exposure.match_histograms(image, \u2026) Adjust an image so that its cumulative histogram matches that of another.  \nskimage.exposure.rescale_intensity(image[, \u2026]) Return image after stretching or shrinking its intensity levels.   adjust_gamma  \nskimage.exposure.adjust_gamma(image, gamma=1, gain=1) [source]\n \nPerforms Gamma Correction on the input image. Also known as Power Law Transform. This function transforms the input image pixelwise according to the equation O = I**gamma after scaling each pixel to the range 0 to 1.  Parameters \n \nimagendarray \n\nInput image.  \ngammafloat, optional \n\nNon negative real number. Default value is 1.  \ngainfloat, optional \n\nThe constant multiplier. Default value is 1.    Returns \n \noutndarray \n\nGamma corrected output image.      See also  \nadjust_log\n\n  Notes For gamma greater than 1, the histogram will shift towards left and the output image will be darker than the input image. For gamma less than 1, the histogram will shift towards right and the output image will be brighter than the input image. References  \n1  \nhttps://en.wikipedia.org/wiki/Gamma_correction   Examples >>> from skimage import data, exposure, img_as_float\n>>> image = img_as_float(data.moon())\n>>> gamma_corrected = exposure.adjust_gamma(image, 2)\n>>> # Output is darker for gamma > 1\n>>> image.mean() > gamma_corrected.mean()\nTrue\n \n Examples using skimage.exposure.adjust_gamma\n \n  Explore 3D images (of cells)   adjust_log  \nskimage.exposure.adjust_log(image, gain=1, inv=False) [source]\n \nPerforms Logarithmic correction on the input image. This function transforms the input image pixelwise according to the equation O = gain*log(1 + I) after scaling each pixel to the range 0 to 1. For inverse logarithmic correction, the equation is O = gain*(2**I - 1).  Parameters \n \nimagendarray \n\nInput image.  \ngainfloat, optional \n\nThe constant multiplier. Default value is 1.  \ninvfloat, optional \n\nIf True, it performs inverse logarithmic correction, else correction will be logarithmic. Defaults to False.    Returns \n \noutndarray \n\nLogarithm corrected output image.      See also  \nadjust_gamma\n\n  References  \n1  \nhttp://www.ece.ucsb.edu/Faculty/Manjunath/courses/ece178W03/EnhancePart1.pdf   \n adjust_sigmoid  \nskimage.exposure.adjust_sigmoid(image, cutoff=0.5, gain=10, inv=False) [source]\n \nPerforms Sigmoid Correction on the input image. Also known as Contrast Adjustment. This function transforms the input image pixelwise according to the equation O = 1/(1 + exp*(gain*(cutoff - I))) after scaling each pixel to the range 0 to 1.  Parameters \n \nimagendarray \n\nInput image.  \ncutofffloat, optional \n\nCutoff of the sigmoid function that shifts the characteristic curve in horizontal direction. Default value is 0.5.  \ngainfloat, optional \n\nThe constant multiplier in exponential\u2019s power of sigmoid function. Default value is 10.  \ninvbool, optional \n\nIf True, returns the negative sigmoid correction. Defaults to False.    Returns \n \noutndarray \n\nSigmoid corrected output image.      See also  \nadjust_gamma\n\n  References  \n1  \nGustav J. Braun, \u201cImage Lightness Rescaling Using Sigmoidal Contrast Enhancement Functions\u201d, http://www.cis.rit.edu/fairchild/PDFs/PAP07.pdf   \n cumulative_distribution  \nskimage.exposure.cumulative_distribution(image, nbins=256) [source]\n \nReturn cumulative distribution function (cdf) for the given image.  Parameters \n \nimagearray \n\nImage array.  \nnbinsint, optional \n\nNumber of bins for image histogram.    Returns \n \nimg_cdfarray \n\nValues of cumulative distribution function.  \nbin_centersarray \n\nCenters of bins.      See also  \nhistogram\n\n  References  \n1  \nhttps://en.wikipedia.org/wiki/Cumulative_distribution_function   Examples >>> from skimage import data, exposure, img_as_float\n>>> image = img_as_float(data.camera())\n>>> hi = exposure.histogram(image)\n>>> cdf = exposure.cumulative_distribution(image)\n>>> np.alltrue(cdf[0] == np.cumsum(hi[0])/float(image.size))\nTrue\n \n Examples using skimage.exposure.cumulative_distribution\n \n  Local Histogram Equalization  \n\n  Explore 3D images (of cells)   equalize_adapthist  \nskimage.exposure.equalize_adapthist(image, kernel_size=None, clip_limit=0.01, nbins=256) [source]\n \nContrast Limited Adaptive Histogram Equalization (CLAHE). An algorithm for local contrast enhancement, that uses histograms computed over different tile regions of the image. Local details can therefore be enhanced even in regions that are darker or lighter than most of the image.  Parameters \n \nimage(N1, \u2026,NN[, C]) ndarray \n\nInput image.  kernel_size: int or array_like, optional\n\nDefines the shape of contextual regions used in the algorithm. If iterable is passed, it must have the same number of elements as image.ndim (without color channel). If integer, it is broadcasted to each image dimension. By default, kernel_size is 1/8 of image height by 1/8 of its width.  \nclip_limitfloat, optional \n\nClipping limit, normalized between 0 and 1 (higher values give more contrast).  \nnbinsint, optional \n\nNumber of gray bins for histogram (\u201cdata range\u201d).    Returns \n \nout(N1, \u2026,NN[, C]) ndarray \n\nEqualized image with float64 dtype.      See also  \nequalize_hist, rescale_intensity\n\n  Notes  \n For color images, the following steps are performed:\n\n The image is converted to HSV color space The CLAHE algorithm is run on the V (Value) channel The image is converted back to RGB space and returned     For RGBA images, the original alpha channel is removed.   Changed in version 0.17: The values returned by this function are slightly shifted upwards because of an internal change in rounding behavior.  References  \n1  \nhttp://tog.acm.org/resources/GraphicsGems/  \n2  \nhttps://en.wikipedia.org/wiki/CLAHE#CLAHE   \n Examples using skimage.exposure.equalize_adapthist\n \n  3D adaptive histogram equalization   equalize_hist  \nskimage.exposure.equalize_hist(image, nbins=256, mask=None) [source]\n \nReturn image after histogram equalization.  Parameters \n \nimagearray \n\nImage array.  \nnbinsint, optional \n\nNumber of bins for image histogram. Note: this argument is ignored for integer images, for which each integer is its own bin.  mask: ndarray of bools or 0s and 1s, optional\n\nArray of same shape as image. Only points at which mask == True are used for the equalization, which is applied to the whole image.    Returns \n \noutfloat array \n\nImage array after histogram equalization.     Notes This function is adapted from [1] with the author\u2019s permission. References  \n1  \nhttp://www.janeriksolem.net/histogram-equalization-with-python-and.html  \n2  \nhttps://en.wikipedia.org/wiki/Histogram_equalization   \n Examples using skimage.exposure.equalize_hist\n \n  Local Histogram Equalization  \n\n  3D adaptive histogram equalization  \n\n  Explore 3D images (of cells)  \n\n  Rank filters   histogram  \nskimage.exposure.histogram(image, nbins=256, source_range='image', normalize=False) [source]\n \nReturn histogram of image. Unlike numpy.histogram, this function returns the centers of bins and does not rebin integer arrays. For integer arrays, each integer value has its own bin, which improves speed and intensity-resolution. The histogram is computed on the flattened image: for color images, the function should be used separately on each channel to obtain a histogram for each color channel.  Parameters \n \nimagearray \n\nInput image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.  \nsource_rangestring, optional \n\n\u2018image\u2019 (default) determines the range from the input image. \u2018dtype\u2019 determines the range from the expected range of the images of that data type.  \nnormalizebool, optional \n\nIf True, normalize the histogram by the sum of its values.    Returns \n \nhistarray \n\nThe values of the histogram.  \nbin_centersarray \n\nThe values at the center of the bins.      See also  \ncumulative_distribution\n\n  Examples >>> from skimage import data, exposure, img_as_float\n>>> image = img_as_float(data.camera())\n>>> np.histogram(image, bins=2)\n(array([ 93585, 168559]), array([0. , 0.5, 1. ]))\n>>> exposure.histogram(image, nbins=2)\n(array([ 93585, 168559]), array([0.25, 0.75]))\n \n Examples using skimage.exposure.histogram\n \n  Rank filters   is_low_contrast  \nskimage.exposure.is_low_contrast(image, fraction_threshold=0.05, lower_percentile=1, upper_percentile=99, method='linear') [source]\n \nDetermine if an image is low contrast.  Parameters \n \nimagearray-like \n\nThe image under test.  \nfraction_thresholdfloat, optional \n\nThe low contrast fraction threshold. An image is considered low- contrast when its range of brightness spans less than this fraction of its data type\u2019s full range. [1]  \nlower_percentilefloat, optional \n\nDisregard values below this percentile when computing image contrast.  \nupper_percentilefloat, optional \n\nDisregard values above this percentile when computing image contrast.  \nmethodstr, optional \n\nThe contrast determination method. Right now the only available option is \u201clinear\u201d.    Returns \n \noutbool \n\nTrue when the image is determined to be low contrast.     References  \n1  \nhttps://scikit-image.org/docs/dev/user_guide/data_types.html   Examples >>> image = np.linspace(0, 0.04, 100)\n>>> is_low_contrast(image)\nTrue\n>>> image[-1] = 1\n>>> is_low_contrast(image)\nTrue\n>>> is_low_contrast(image, upper_percentile=100)\nFalse\n \n match_histograms  \nskimage.exposure.match_histograms(image, reference, *, multichannel=False) [source]\n \nAdjust an image so that its cumulative histogram matches that of another. The adjustment is applied separately for each channel.  Parameters \n \nimagendarray \n\nInput image. Can be gray-scale or in color.  \nreferencendarray \n\nImage to match histogram of. Must have the same number of channels as image.  \nmultichannelbool, optional \n\nApply the matching separately for each channel.    Returns \n \nmatchedndarray \n\nTransformed input image.    Raises \n ValueError\n\nThrown when the number of channels in the input image and the reference differ.     References  \n1  \nhttp://paulbourke.net/miscellaneous/equalisation/   \n rescale_intensity  \nskimage.exposure.rescale_intensity(image, in_range='image', out_range='dtype') [source]\n \nReturn image after stretching or shrinking its intensity levels. The desired intensity range of the input and output, in_range and out_range respectively, are used to stretch or shrink the intensity range of the input image. See examples below.  Parameters \n \nimagearray \n\nImage array.  \nin_range, out_rangestr or 2-tuple, optional \n\nMin and max intensity values of input and output image. The possible values for this parameter are enumerated below.  \u2018image\u2019\n\nUse image min/max as the intensity range.  \u2018dtype\u2019\n\nUse min/max of the image\u2019s dtype as the intensity range.  dtype-name\n\nUse intensity range based on desired dtype. Must be valid key in DTYPE_RANGE.  2-tuple\n\nUse range_values as explicit min/max intensities.      Returns \n \noutarray \n\nImage array after rescaling its intensity. This image is the same dtype as the input image.      See also  \nequalize_hist\n\n  Notes  Changed in version 0.17: The dtype of the output array has changed to match the output dtype, or float if the output range is specified by a pair of floats.  Examples By default, the min/max intensities of the input image are stretched to the limits allowed by the image\u2019s dtype, since in_range defaults to \u2018image\u2019 and out_range defaults to \u2018dtype\u2019: >>> image = np.array([51, 102, 153], dtype=np.uint8)\n>>> rescale_intensity(image)\narray([  0, 127, 255], dtype=uint8)\n It\u2019s easy to accidentally convert an image dtype from uint8 to float: >>> 1.0 * image\narray([ 51., 102., 153.])\n Use rescale_intensity to rescale to the proper range for float dtypes: >>> image_float = 1.0 * image\n>>> rescale_intensity(image_float)\narray([0. , 0.5, 1. ])\n To maintain the low contrast of the original, use the in_range parameter: >>> rescale_intensity(image_float, in_range=(0, 255))\narray([0.2, 0.4, 0.6])\n If the min/max value of in_range is more/less than the min/max image intensity, then the intensity levels are clipped: >>> rescale_intensity(image_float, in_range=(0, 102))\narray([0.5, 1. , 1. ])\n If you have an image with signed integers but want to rescale the image to just the positive range, use the out_range parameter. In that case, the output dtype will be float: >>> image = np.array([-10, 0, 10], dtype=np.int8)\n>>> rescale_intensity(image, out_range=(0, 127))\narray([  0. ,  63.5, 127. ])\n To get the desired range with a specific dtype, use .astype(): >>> rescale_intensity(image, out_range=(0, 127)).astype(np.int8)\narray([  0,  63, 127], dtype=int8)\n If the input image is constant, the output will be clipped directly to the output range: >>> image = np.array([130, 130, 130], dtype=np.int32) >>> rescale_intensity(image, out_range=(0, 127)).astype(np.int32) array([127, 127, 127], dtype=int32) \n Examples using skimage.exposure.rescale_intensity\n \n  Phase Unwrapping  \n\n  Explore 3D images (of cells)  \n\n  Rank filters  \n"}, {"name": "exposure.adjust_gamma()", "path": "api/skimage.exposure#skimage.exposure.adjust_gamma", "type": "exposure", "text": " \nskimage.exposure.adjust_gamma(image, gamma=1, gain=1) [source]\n \nPerforms Gamma Correction on the input image. Also known as Power Law Transform. This function transforms the input image pixelwise according to the equation O = I**gamma after scaling each pixel to the range 0 to 1.  Parameters \n \nimagendarray \n\nInput image.  \ngammafloat, optional \n\nNon negative real number. Default value is 1.  \ngainfloat, optional \n\nThe constant multiplier. Default value is 1.    Returns \n \noutndarray \n\nGamma corrected output image.      See also  \nadjust_log\n\n  Notes For gamma greater than 1, the histogram will shift towards left and the output image will be darker than the input image. For gamma less than 1, the histogram will shift towards right and the output image will be brighter than the input image. References  \n1  \nhttps://en.wikipedia.org/wiki/Gamma_correction   Examples >>> from skimage import data, exposure, img_as_float\n>>> image = img_as_float(data.moon())\n>>> gamma_corrected = exposure.adjust_gamma(image, 2)\n>>> # Output is darker for gamma > 1\n>>> image.mean() > gamma_corrected.mean()\nTrue\n \n"}, {"name": "exposure.adjust_log()", "path": "api/skimage.exposure#skimage.exposure.adjust_log", "type": "exposure", "text": " \nskimage.exposure.adjust_log(image, gain=1, inv=False) [source]\n \nPerforms Logarithmic correction on the input image. This function transforms the input image pixelwise according to the equation O = gain*log(1 + I) after scaling each pixel to the range 0 to 1. For inverse logarithmic correction, the equation is O = gain*(2**I - 1).  Parameters \n \nimagendarray \n\nInput image.  \ngainfloat, optional \n\nThe constant multiplier. Default value is 1.  \ninvfloat, optional \n\nIf True, it performs inverse logarithmic correction, else correction will be logarithmic. Defaults to False.    Returns \n \noutndarray \n\nLogarithm corrected output image.      See also  \nadjust_gamma\n\n  References  \n1  \nhttp://www.ece.ucsb.edu/Faculty/Manjunath/courses/ece178W03/EnhancePart1.pdf   \n"}, {"name": "exposure.adjust_sigmoid()", "path": "api/skimage.exposure#skimage.exposure.adjust_sigmoid", "type": "exposure", "text": " \nskimage.exposure.adjust_sigmoid(image, cutoff=0.5, gain=10, inv=False) [source]\n \nPerforms Sigmoid Correction on the input image. Also known as Contrast Adjustment. This function transforms the input image pixelwise according to the equation O = 1/(1 + exp*(gain*(cutoff - I))) after scaling each pixel to the range 0 to 1.  Parameters \n \nimagendarray \n\nInput image.  \ncutofffloat, optional \n\nCutoff of the sigmoid function that shifts the characteristic curve in horizontal direction. Default value is 0.5.  \ngainfloat, optional \n\nThe constant multiplier in exponential\u2019s power of sigmoid function. Default value is 10.  \ninvbool, optional \n\nIf True, returns the negative sigmoid correction. Defaults to False.    Returns \n \noutndarray \n\nSigmoid corrected output image.      See also  \nadjust_gamma\n\n  References  \n1  \nGustav J. Braun, \u201cImage Lightness Rescaling Using Sigmoidal Contrast Enhancement Functions\u201d, http://www.cis.rit.edu/fairchild/PDFs/PAP07.pdf   \n"}, {"name": "exposure.cumulative_distribution()", "path": "api/skimage.exposure#skimage.exposure.cumulative_distribution", "type": "exposure", "text": " \nskimage.exposure.cumulative_distribution(image, nbins=256) [source]\n \nReturn cumulative distribution function (cdf) for the given image.  Parameters \n \nimagearray \n\nImage array.  \nnbinsint, optional \n\nNumber of bins for image histogram.    Returns \n \nimg_cdfarray \n\nValues of cumulative distribution function.  \nbin_centersarray \n\nCenters of bins.      See also  \nhistogram\n\n  References  \n1  \nhttps://en.wikipedia.org/wiki/Cumulative_distribution_function   Examples >>> from skimage import data, exposure, img_as_float\n>>> image = img_as_float(data.camera())\n>>> hi = exposure.histogram(image)\n>>> cdf = exposure.cumulative_distribution(image)\n>>> np.alltrue(cdf[0] == np.cumsum(hi[0])/float(image.size))\nTrue\n \n"}, {"name": "exposure.equalize_adapthist()", "path": "api/skimage.exposure#skimage.exposure.equalize_adapthist", "type": "exposure", "text": " \nskimage.exposure.equalize_adapthist(image, kernel_size=None, clip_limit=0.01, nbins=256) [source]\n \nContrast Limited Adaptive Histogram Equalization (CLAHE). An algorithm for local contrast enhancement, that uses histograms computed over different tile regions of the image. Local details can therefore be enhanced even in regions that are darker or lighter than most of the image.  Parameters \n \nimage(N1, \u2026,NN[, C]) ndarray \n\nInput image.  kernel_size: int or array_like, optional\n\nDefines the shape of contextual regions used in the algorithm. If iterable is passed, it must have the same number of elements as image.ndim (without color channel). If integer, it is broadcasted to each image dimension. By default, kernel_size is 1/8 of image height by 1/8 of its width.  \nclip_limitfloat, optional \n\nClipping limit, normalized between 0 and 1 (higher values give more contrast).  \nnbinsint, optional \n\nNumber of gray bins for histogram (\u201cdata range\u201d).    Returns \n \nout(N1, \u2026,NN[, C]) ndarray \n\nEqualized image with float64 dtype.      See also  \nequalize_hist, rescale_intensity\n\n  Notes  \n For color images, the following steps are performed:\n\n The image is converted to HSV color space The CLAHE algorithm is run on the V (Value) channel The image is converted back to RGB space and returned     For RGBA images, the original alpha channel is removed.   Changed in version 0.17: The values returned by this function are slightly shifted upwards because of an internal change in rounding behavior.  References  \n1  \nhttp://tog.acm.org/resources/GraphicsGems/  \n2  \nhttps://en.wikipedia.org/wiki/CLAHE#CLAHE   \n"}, {"name": "exposure.equalize_hist()", "path": "api/skimage.exposure#skimage.exposure.equalize_hist", "type": "exposure", "text": " \nskimage.exposure.equalize_hist(image, nbins=256, mask=None) [source]\n \nReturn image after histogram equalization.  Parameters \n \nimagearray \n\nImage array.  \nnbinsint, optional \n\nNumber of bins for image histogram. Note: this argument is ignored for integer images, for which each integer is its own bin.  mask: ndarray of bools or 0s and 1s, optional\n\nArray of same shape as image. Only points at which mask == True are used for the equalization, which is applied to the whole image.    Returns \n \noutfloat array \n\nImage array after histogram equalization.     Notes This function is adapted from [1] with the author\u2019s permission. References  \n1  \nhttp://www.janeriksolem.net/histogram-equalization-with-python-and.html  \n2  \nhttps://en.wikipedia.org/wiki/Histogram_equalization   \n"}, {"name": "exposure.histogram()", "path": "api/skimage.exposure#skimage.exposure.histogram", "type": "exposure", "text": " \nskimage.exposure.histogram(image, nbins=256, source_range='image', normalize=False) [source]\n \nReturn histogram of image. Unlike numpy.histogram, this function returns the centers of bins and does not rebin integer arrays. For integer arrays, each integer value has its own bin, which improves speed and intensity-resolution. The histogram is computed on the flattened image: for color images, the function should be used separately on each channel to obtain a histogram for each color channel.  Parameters \n \nimagearray \n\nInput image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.  \nsource_rangestring, optional \n\n\u2018image\u2019 (default) determines the range from the input image. \u2018dtype\u2019 determines the range from the expected range of the images of that data type.  \nnormalizebool, optional \n\nIf True, normalize the histogram by the sum of its values.    Returns \n \nhistarray \n\nThe values of the histogram.  \nbin_centersarray \n\nThe values at the center of the bins.      See also  \ncumulative_distribution\n\n  Examples >>> from skimage import data, exposure, img_as_float\n>>> image = img_as_float(data.camera())\n>>> np.histogram(image, bins=2)\n(array([ 93585, 168559]), array([0. , 0.5, 1. ]))\n>>> exposure.histogram(image, nbins=2)\n(array([ 93585, 168559]), array([0.25, 0.75]))\n \n"}, {"name": "exposure.is_low_contrast()", "path": "api/skimage.exposure#skimage.exposure.is_low_contrast", "type": "exposure", "text": " \nskimage.exposure.is_low_contrast(image, fraction_threshold=0.05, lower_percentile=1, upper_percentile=99, method='linear') [source]\n \nDetermine if an image is low contrast.  Parameters \n \nimagearray-like \n\nThe image under test.  \nfraction_thresholdfloat, optional \n\nThe low contrast fraction threshold. An image is considered low- contrast when its range of brightness spans less than this fraction of its data type\u2019s full range. [1]  \nlower_percentilefloat, optional \n\nDisregard values below this percentile when computing image contrast.  \nupper_percentilefloat, optional \n\nDisregard values above this percentile when computing image contrast.  \nmethodstr, optional \n\nThe contrast determination method. Right now the only available option is \u201clinear\u201d.    Returns \n \noutbool \n\nTrue when the image is determined to be low contrast.     References  \n1  \nhttps://scikit-image.org/docs/dev/user_guide/data_types.html   Examples >>> image = np.linspace(0, 0.04, 100)\n>>> is_low_contrast(image)\nTrue\n>>> image[-1] = 1\n>>> is_low_contrast(image)\nTrue\n>>> is_low_contrast(image, upper_percentile=100)\nFalse\n \n"}, {"name": "exposure.match_histograms()", "path": "api/skimage.exposure#skimage.exposure.match_histograms", "type": "exposure", "text": " \nskimage.exposure.match_histograms(image, reference, *, multichannel=False) [source]\n \nAdjust an image so that its cumulative histogram matches that of another. The adjustment is applied separately for each channel.  Parameters \n \nimagendarray \n\nInput image. Can be gray-scale or in color.  \nreferencendarray \n\nImage to match histogram of. Must have the same number of channels as image.  \nmultichannelbool, optional \n\nApply the matching separately for each channel.    Returns \n \nmatchedndarray \n\nTransformed input image.    Raises \n ValueError\n\nThrown when the number of channels in the input image and the reference differ.     References  \n1  \nhttp://paulbourke.net/miscellaneous/equalisation/   \n"}, {"name": "exposure.rescale_intensity()", "path": "api/skimage.exposure#skimage.exposure.rescale_intensity", "type": "exposure", "text": " \nskimage.exposure.rescale_intensity(image, in_range='image', out_range='dtype') [source]\n \nReturn image after stretching or shrinking its intensity levels. The desired intensity range of the input and output, in_range and out_range respectively, are used to stretch or shrink the intensity range of the input image. See examples below.  Parameters \n \nimagearray \n\nImage array.  \nin_range, out_rangestr or 2-tuple, optional \n\nMin and max intensity values of input and output image. The possible values for this parameter are enumerated below.  \u2018image\u2019\n\nUse image min/max as the intensity range.  \u2018dtype\u2019\n\nUse min/max of the image\u2019s dtype as the intensity range.  dtype-name\n\nUse intensity range based on desired dtype. Must be valid key in DTYPE_RANGE.  2-tuple\n\nUse range_values as explicit min/max intensities.      Returns \n \noutarray \n\nImage array after rescaling its intensity. This image is the same dtype as the input image.      See also  \nequalize_hist\n\n  Notes  Changed in version 0.17: The dtype of the output array has changed to match the output dtype, or float if the output range is specified by a pair of floats.  Examples By default, the min/max intensities of the input image are stretched to the limits allowed by the image\u2019s dtype, since in_range defaults to \u2018image\u2019 and out_range defaults to \u2018dtype\u2019: >>> image = np.array([51, 102, 153], dtype=np.uint8)\n>>> rescale_intensity(image)\narray([  0, 127, 255], dtype=uint8)\n It\u2019s easy to accidentally convert an image dtype from uint8 to float: >>> 1.0 * image\narray([ 51., 102., 153.])\n Use rescale_intensity to rescale to the proper range for float dtypes: >>> image_float = 1.0 * image\n>>> rescale_intensity(image_float)\narray([0. , 0.5, 1. ])\n To maintain the low contrast of the original, use the in_range parameter: >>> rescale_intensity(image_float, in_range=(0, 255))\narray([0.2, 0.4, 0.6])\n If the min/max value of in_range is more/less than the min/max image intensity, then the intensity levels are clipped: >>> rescale_intensity(image_float, in_range=(0, 102))\narray([0.5, 1. , 1. ])\n If you have an image with signed integers but want to rescale the image to just the positive range, use the out_range parameter. In that case, the output dtype will be float: >>> image = np.array([-10, 0, 10], dtype=np.int8)\n>>> rescale_intensity(image, out_range=(0, 127))\narray([  0. ,  63.5, 127. ])\n To get the desired range with a specific dtype, use .astype(): >>> rescale_intensity(image, out_range=(0, 127)).astype(np.int8)\narray([  0,  63, 127], dtype=int8)\n If the input image is constant, the output will be clipped directly to the output range: >>> image = np.array([130, 130, 130], dtype=np.int32) >>> rescale_intensity(image, out_range=(0, 127)).astype(np.int32) array([127, 127, 127], dtype=int32) \n"}, {"name": "feature", "path": "api/skimage.feature", "type": "feature", "text": "Module: feature  \nskimage.feature.blob_dog(image[, min_sigma, \u2026]) Finds blobs in the given grayscale image.  \nskimage.feature.blob_doh(image[, min_sigma, \u2026]) Finds blobs in the given grayscale image.  \nskimage.feature.blob_log(image[, min_sigma, \u2026]) Finds blobs in the given grayscale image.  \nskimage.feature.canny(image[, sigma, \u2026]) Edge filter an image using the Canny algorithm.  \nskimage.feature.corner_fast(image[, n, \u2026]) Extract FAST corners for a given image.  \nskimage.feature.corner_foerstner(image[, sigma]) Compute Foerstner corner measure response image.  \nskimage.feature.corner_harris(image[, \u2026]) Compute Harris corner measure response image.  \nskimage.feature.corner_kitchen_rosenfeld(image) Compute Kitchen and Rosenfeld corner measure response image.  \nskimage.feature.corner_moravec(image[, \u2026]) Compute Moravec corner measure response image.  \nskimage.feature.corner_orientations(image, \u2026) Compute the orientation of corners.  \nskimage.feature.corner_peaks(image[, \u2026]) Find peaks in corner measure response image.  \nskimage.feature.corner_shi_tomasi(image[, sigma]) Compute Shi-Tomasi (Kanade-Tomasi) corner measure response image.  \nskimage.feature.corner_subpix(image, corners) Determine subpixel position of corners.  \nskimage.feature.daisy(image[, step, radius, \u2026]) Extract DAISY feature descriptors densely for the given image.  \nskimage.feature.draw_haar_like_feature(\u2026) Visualization of Haar-like features.  \nskimage.feature.draw_multiblock_lbp(image, \u2026) Multi-block local binary pattern visualization.  \nskimage.feature.greycomatrix(image, \u2026[, \u2026]) Calculate the grey-level co-occurrence matrix.  \nskimage.feature.greycoprops(P[, prop]) Calculate texture properties of a GLCM.  \nskimage.feature.haar_like_feature(int_image, \u2026) Compute the Haar-like features for a region of interest (ROI) of an integral image.  \nskimage.feature.haar_like_feature_coord(\u2026) Compute the coordinates of Haar-like features.  \nskimage.feature.hessian_matrix(image[, \u2026]) Compute Hessian matrix.  \nskimage.feature.hessian_matrix_det(image[, \u2026]) Compute the approximate Hessian Determinant over an image.  \nskimage.feature.hessian_matrix_eigvals(H_elems) Compute eigenvalues of Hessian matrix.  \nskimage.feature.hog(image[, orientations, \u2026]) Extract Histogram of Oriented Gradients (HOG) for a given image.  \nskimage.feature.local_binary_pattern(image, P, R) Gray scale and rotation invariant LBP (Local Binary Patterns).  \nskimage.feature.masked_register_translation(\u2026) Deprecated function.  \nskimage.feature.match_descriptors(\u2026[, \u2026]) Brute-force matching of descriptors.  \nskimage.feature.match_template(image, template) Match a template to a 2-D or 3-D image using normalized correlation.  \nskimage.feature.multiblock_lbp(int_image, r, \u2026) Multi-block local binary pattern (MB-LBP).  \nskimage.feature.multiscale_basic_features(image) Local features for a single- or multi-channel nd image.  \nskimage.feature.peak_local_max(image[, \u2026]) Find peaks in an image as coordinate list or boolean mask.  \nskimage.feature.plot_matches(ax, image1, \u2026) Plot matched features.  \nskimage.feature.register_translation(\u2026[, \u2026]) Deprecated function.  \nskimage.feature.shape_index(image[, sigma, \u2026]) Compute the shape index.  \nskimage.feature.structure_tensor(image[, \u2026]) Compute structure tensor using sum of squared differences.  \nskimage.feature.structure_tensor_eigenvalues(A_elems) Compute eigenvalues of structure tensor.  \nskimage.feature.structure_tensor_eigvals(\u2026) Compute eigenvalues of structure tensor.  \nskimage.feature.BRIEF([descriptor_size, \u2026]) BRIEF binary descriptor extractor.  \nskimage.feature.CENSURE([min_scale, \u2026]) CENSURE keypoint detector.  \nskimage.feature.Cascade Class for cascade of classifiers that is used for object detection.  \nskimage.feature.ORB([downscale, n_scales, \u2026]) Oriented FAST and rotated BRIEF feature detector and binary descriptor extractor.   blob_dog  \nskimage.feature.blob_dog(image, min_sigma=1, max_sigma=50, sigma_ratio=1.6, threshold=2.0, overlap=0.5, *, exclude_border=False) [source]\n \nFinds blobs in the given grayscale image. Blobs are found using the Difference of Gaussian (DoG) method [1]. For each blob found, the method returns its coordinates and the standard deviation of the Gaussian kernel that detected the blob.  Parameters \n \nimage2D or 3D ndarray \n\nInput grayscale image, blobs are assumed to be light on dark background (white on black).  \nmin_sigmascalar or sequence of scalars, optional \n\nThe minimum standard deviation for Gaussian kernel. Keep this low to detect smaller blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.  \nmax_sigmascalar or sequence of scalars, optional \n\nThe maximum standard deviation for Gaussian kernel. Keep this high to detect larger blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.  \nsigma_ratiofloat, optional \n\nThe ratio between the standard deviation of Gaussian Kernels used for computing the Difference of Gaussians  \nthresholdfloat, optional. \n\nThe absolute lower bound for scale space maxima. Local maxima smaller than thresh are ignored. Reduce this to detect blobs with less intensities.  \noverlapfloat, optional \n\nA value between 0 and 1. If the area of two blobs overlaps by a fraction greater than threshold, the smaller blob is eliminated.  \nexclude_bordertuple of ints, int, or False, optional \n\nIf tuple of ints, the length of the tuple must match the input array\u2019s dimensionality. Each element of the tuple will exclude peaks from within exclude_border-pixels of the border of the image along that dimension. If nonzero int, exclude_border excludes peaks from within exclude_border-pixels of the border of the image. If zero or False, peaks are identified regardless of their distance from the border.    Returns \n \nA(n, image.ndim + sigma) ndarray \n\nA 2d array with each row representing 2 coordinate values for a 2D image, and 3 coordinate values for a 3D image, plus the sigma(s) used. When a single sigma is passed, outputs are: (r, c, sigma) or (p, r, c, sigma) where (r, c) or (p, r, c) are coordinates of the blob and sigma is the standard deviation of the Gaussian kernel which detected the blob. When an anisotropic gaussian is used (sigmas per dimension), the detected sigma is returned for each dimension.      See also  \nskimage.filters.difference_of_gaussians\n\n  Notes The radius of each blob is approximately \\(\\sqrt{2}\\sigma\\) for a 2-D image and \\(\\sqrt{3}\\sigma\\) for a 3-D image. References  \n1  \nhttps://en.wikipedia.org/wiki/Blob_detection#The_difference_of_Gaussians_approach   Examples >>> from skimage import data, feature\n>>> feature.blob_dog(data.coins(), threshold=.5, max_sigma=40)\narray([[120.      , 272.      ,  16.777216],\n       [193.      , 213.      ,  16.777216],\n       [263.      , 245.      ,  16.777216],\n       [185.      , 347.      ,  16.777216],\n       [128.      , 154.      ,  10.48576 ],\n       [198.      , 155.      ,  10.48576 ],\n       [124.      , 337.      ,  10.48576 ],\n       [ 45.      , 336.      ,  16.777216],\n       [195.      , 102.      ,  16.777216],\n       [125.      ,  45.      ,  16.777216],\n       [261.      , 173.      ,  16.777216],\n       [194.      , 277.      ,  16.777216],\n       [127.      , 102.      ,  10.48576 ],\n       [125.      , 208.      ,  10.48576 ],\n       [267.      , 115.      ,  10.48576 ],\n       [263.      , 302.      ,  16.777216],\n       [196.      ,  43.      ,  10.48576 ],\n       [260.      ,  46.      ,  16.777216],\n       [267.      , 359.      ,  16.777216],\n       [ 54.      , 276.      ,  10.48576 ],\n       [ 58.      , 100.      ,  10.48576 ],\n       [ 52.      , 155.      ,  16.777216],\n       [ 52.      , 216.      ,  16.777216],\n       [ 54.      ,  42.      ,  16.777216]])\n \n blob_doh  \nskimage.feature.blob_doh(image, min_sigma=1, max_sigma=30, num_sigma=10, threshold=0.01, overlap=0.5, log_scale=False) [source]\n \nFinds blobs in the given grayscale image. Blobs are found using the Determinant of Hessian method [1]. For each blob found, the method returns its coordinates and the standard deviation of the Gaussian Kernel used for the Hessian matrix whose determinant detected the blob. Determinant of Hessians is approximated using [2].  Parameters \n \nimage2D ndarray \n\nInput grayscale image.Blobs can either be light on dark or vice versa.  \nmin_sigmafloat, optional \n\nThe minimum standard deviation for Gaussian Kernel used to compute Hessian matrix. Keep this low to detect smaller blobs.  \nmax_sigmafloat, optional \n\nThe maximum standard deviation for Gaussian Kernel used to compute Hessian matrix. Keep this high to detect larger blobs.  \nnum_sigmaint, optional \n\nThe number of intermediate values of standard deviations to consider between min_sigma and max_sigma.  \nthresholdfloat, optional. \n\nThe absolute lower bound for scale space maxima. Local maxima smaller than thresh are ignored. Reduce this to detect less prominent blobs.  \noverlapfloat, optional \n\nA value between 0 and 1. If the area of two blobs overlaps by a fraction greater than threshold, the smaller blob is eliminated.  \nlog_scalebool, optional \n\nIf set intermediate values of standard deviations are interpolated using a logarithmic scale to the base 10. If not, linear interpolation is used.    Returns \n \nA(n, 3) ndarray \n\nA 2d array with each row representing 3 values, (y,x,sigma) where (y,x) are coordinates of the blob and sigma is the standard deviation of the Gaussian kernel of the Hessian Matrix whose determinant detected the blob.     Notes The radius of each blob is approximately sigma. Computation of Determinant of Hessians is independent of the standard deviation. Therefore detecting larger blobs won\u2019t take more time. In methods line blob_dog() and blob_log() the computation of Gaussians for larger sigma takes more time. The downside is that this method can\u2019t be used for detecting blobs of radius less than 3px due to the box filters used in the approximation of Hessian Determinant. References  \n1  \nhttps://en.wikipedia.org/wiki/Blob_detection#The_determinant_of_the_Hessian  \n2  \nHerbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool, \u201cSURF: Speeded Up Robust Features\u201d ftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf   Examples >>> from skimage import data, feature\n>>> img = data.coins()\n>>> feature.blob_doh(img)\narray([[197.        , 153.        ,  20.33333333],\n       [124.        , 336.        ,  20.33333333],\n       [126.        , 153.        ,  20.33333333],\n       [195.        , 100.        ,  23.55555556],\n       [192.        , 212.        ,  23.55555556],\n       [121.        , 271.        ,  30.        ],\n       [126.        , 101.        ,  20.33333333],\n       [193.        , 275.        ,  23.55555556],\n       [123.        , 205.        ,  20.33333333],\n       [270.        , 363.        ,  30.        ],\n       [265.        , 113.        ,  23.55555556],\n       [262.        , 243.        ,  23.55555556],\n       [185.        , 348.        ,  30.        ],\n       [156.        , 302.        ,  30.        ],\n       [123.        ,  44.        ,  23.55555556],\n       [260.        , 173.        ,  30.        ],\n       [197.        ,  44.        ,  20.33333333]])\n \n blob_log  \nskimage.feature.blob_log(image, min_sigma=1, max_sigma=50, num_sigma=10, threshold=0.2, overlap=0.5, log_scale=False, *, exclude_border=False) [source]\n \nFinds blobs in the given grayscale image. Blobs are found using the Laplacian of Gaussian (LoG) method [1]. For each blob found, the method returns its coordinates and the standard deviation of the Gaussian kernel that detected the blob.  Parameters \n \nimage2D or 3D ndarray \n\nInput grayscale image, blobs are assumed to be light on dark background (white on black).  \nmin_sigmascalar or sequence of scalars, optional \n\nthe minimum standard deviation for Gaussian kernel. Keep this low to detect smaller blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.  \nmax_sigmascalar or sequence of scalars, optional \n\nThe maximum standard deviation for Gaussian kernel. Keep this high to detect larger blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.  \nnum_sigmaint, optional \n\nThe number of intermediate values of standard deviations to consider between min_sigma and max_sigma.  \nthresholdfloat, optional. \n\nThe absolute lower bound for scale space maxima. Local maxima smaller than thresh are ignored. Reduce this to detect blobs with less intensities.  \noverlapfloat, optional \n\nA value between 0 and 1. If the area of two blobs overlaps by a fraction greater than threshold, the smaller blob is eliminated.  \nlog_scalebool, optional \n\nIf set intermediate values of standard deviations are interpolated using a logarithmic scale to the base 10. If not, linear interpolation is used.  \nexclude_bordertuple of ints, int, or False, optional \n\nIf tuple of ints, the length of the tuple must match the input array\u2019s dimensionality. Each element of the tuple will exclude peaks from within exclude_border-pixels of the border of the image along that dimension. If nonzero int, exclude_border excludes peaks from within exclude_border-pixels of the border of the image. If zero or False, peaks are identified regardless of their distance from the border.    Returns \n \nA(n, image.ndim + sigma) ndarray \n\nA 2d array with each row representing 2 coordinate values for a 2D image, and 3 coordinate values for a 3D image, plus the sigma(s) used. When a single sigma is passed, outputs are: (r, c, sigma) or (p, r, c, sigma) where (r, c) or (p, r, c) are coordinates of the blob and sigma is the standard deviation of the Gaussian kernel which detected the blob. When an anisotropic gaussian is used (sigmas per dimension), the detected sigma is returned for each dimension.     Notes The radius of each blob is approximately \\(\\sqrt{2}\\sigma\\) for a 2-D image and \\(\\sqrt{3}\\sigma\\) for a 3-D image. References  \n1  \nhttps://en.wikipedia.org/wiki/Blob_detection#The_Laplacian_of_Gaussian   Examples >>> from skimage import data, feature, exposure\n>>> img = data.coins()\n>>> img = exposure.equalize_hist(img)  # improves detection\n>>> feature.blob_log(img, threshold = .3)\narray([[124.        , 336.        ,  11.88888889],\n       [198.        , 155.        ,  11.88888889],\n       [194.        , 213.        ,  17.33333333],\n       [121.        , 272.        ,  17.33333333],\n       [263.        , 244.        ,  17.33333333],\n       [194.        , 276.        ,  17.33333333],\n       [266.        , 115.        ,  11.88888889],\n       [128.        , 154.        ,  11.88888889],\n       [260.        , 174.        ,  17.33333333],\n       [198.        , 103.        ,  11.88888889],\n       [126.        , 208.        ,  11.88888889],\n       [127.        , 102.        ,  11.88888889],\n       [263.        , 302.        ,  17.33333333],\n       [197.        ,  44.        ,  11.88888889],\n       [185.        , 344.        ,  17.33333333],\n       [126.        ,  46.        ,  11.88888889],\n       [113.        , 323.        ,   1.        ]])\n \n canny  \nskimage.feature.canny(image, sigma=1.0, low_threshold=None, high_threshold=None, mask=None, use_quantiles=False) [source]\n \nEdge filter an image using the Canny algorithm.  Parameters \n \nimage2D array \n\nGrayscale input image to detect edges on; can be of any dtype.  \nsigmafloat, optional \n\nStandard deviation of the Gaussian filter.  \nlow_thresholdfloat, optional \n\nLower bound for hysteresis thresholding (linking edges). If None, low_threshold is set to 10% of dtype\u2019s max.  \nhigh_thresholdfloat, optional \n\nUpper bound for hysteresis thresholding (linking edges). If None, high_threshold is set to 20% of dtype\u2019s max.  \nmaskarray, dtype=bool, optional \n\nMask to limit the application of Canny to a certain area.  \nuse_quantilesbool, optional \n\nIf True then treat low_threshold and high_threshold as quantiles of the edge magnitude image, rather than absolute edge magnitude values. If True then the thresholds must be in the range [0, 1].    Returns \n \noutput2D array (image) \n\nThe binary edge map.      See also  \nskimage.sobel \n  Notes The steps of the algorithm are as follows:  Smooth the image using a Gaussian with sigma width. Apply the horizontal and vertical Sobel operators to get the gradients within the image. The edge strength is the norm of the gradient. Thin potential edges to 1-pixel wide curves. First, find the normal to the edge at each point. This is done by looking at the signs and the relative magnitude of the X-Sobel and Y-Sobel to sort the points into 4 categories: horizontal, vertical, diagonal and antidiagonal. Then look in the normal and reverse directions to see if the values in either of those directions are greater than the point in question. Use interpolation to get a mix of points instead of picking the one that\u2019s the closest to the normal. Perform a hysteresis thresholding: first label all points above the high threshold as edges. Then recursively label any point above the low threshold that is 8-connected to a labeled point as an edge.  References  \n1  \nCanny, J., A Computational Approach To Edge Detection, IEEE Trans. Pattern Analysis and Machine Intelligence, 8:679-714, 1986 DOI:10.1109/TPAMI.1986.4767851  \n2  \nWilliam Green\u2019s Canny tutorial https://en.wikipedia.org/wiki/Canny_edge_detector   Examples >>> from skimage import feature\n>>> # Generate noisy image of a square\n>>> im = np.zeros((256, 256))\n>>> im[64:-64, 64:-64] = 1\n>>> im += 0.2 * np.random.rand(*im.shape)\n>>> # First trial with the Canny filter, with the default smoothing\n>>> edges1 = feature.canny(im)\n>>> # Increase the smoothing for better results\n>>> edges2 = feature.canny(im, sigma=3)\n \n corner_fast  \nskimage.feature.corner_fast(image, n=12, threshold=0.15) [source]\n \nExtract FAST corners for a given image.  Parameters \n \nimage2D ndarray \n\nInput image.  \nnint, optional \n\nMinimum number of consecutive pixels out of 16 pixels on the circle that should all be either brighter or darker w.r.t testpixel. A point c on the circle is darker w.r.t test pixel p if Ic < Ip - threshold and brighter if Ic > Ip + threshold. Also stands for the n in FAST-n corner detector.  \nthresholdfloat, optional \n\nThreshold used in deciding whether the pixels on the circle are brighter, darker or similar w.r.t. the test pixel. Decrease the threshold when more corners are desired and vice-versa.    Returns \n \nresponsendarray \n\nFAST corner response image.     References  \n1  \nRosten, E., & Drummond, T. (2006, May). Machine learning for high-speed corner detection. In European conference on computer vision (pp. 430-443). Springer, Berlin, Heidelberg. DOI:10.1007/11744023_34 http://www.edwardrosten.com/work/rosten_2006_machine.pdf  \n2  \nWikipedia, \u201cFeatures from accelerated segment test\u201d, https://en.wikipedia.org/wiki/Features_from_accelerated_segment_test   Examples >>> from skimage.feature import corner_fast, corner_peaks\n>>> square = np.zeros((12, 12))\n>>> square[3:9, 3:9] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> corner_peaks(corner_fast(square, 9), min_distance=1)\narray([[3, 3],\n       [3, 8],\n       [8, 3],\n       [8, 8]])\n \n corner_foerstner  \nskimage.feature.corner_foerstner(image, sigma=1) [source]\n \nCompute Foerstner corner measure response image. This corner detector uses information from the auto-correlation matrix A: A = [(imx**2)   (imx*imy)] = [Axx Axy]\n    [(imx*imy)   (imy**2)]   [Axy Ayy]\n Where imx and imy are first derivatives, averaged with a gaussian filter. The corner measure is then defined as: w = det(A) / trace(A)           (size of error ellipse)\nq = 4 * det(A) / trace(A)**2    (roundness of error ellipse)\n  Parameters \n \nimagendarray \n\nInput image.  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.    Returns \n \nwndarray \n\nError ellipse sizes.  \nqndarray \n\nRoundness of error ellipse.     References  \n1  \nF\u00f6rstner, W., & G\u00fclch, E. (1987, June). A fast operator for detection and precise location of distinct points, corners and centres of circular features. In Proc. ISPRS intercommission conference on fast processing of photogrammetric data (pp. 281-305). https://cseweb.ucsd.edu/classes/sp02/cse252/foerstner/foerstner.pdf  \n2  \nhttps://en.wikipedia.org/wiki/Corner_detection   Examples >>> from skimage.feature import corner_foerstner, corner_peaks\n>>> square = np.zeros([10, 10])\n>>> square[2:8, 2:8] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> w, q = corner_foerstner(square)\n>>> accuracy_thresh = 0.5\n>>> roundness_thresh = 0.3\n>>> foerstner = (q > roundness_thresh) * (w > accuracy_thresh) * w\n>>> corner_peaks(foerstner, min_distance=1)\narray([[2, 2],\n       [2, 7],\n       [7, 2],\n       [7, 7]])\n \n corner_harris  \nskimage.feature.corner_harris(image, method='k', k=0.05, eps=1e-06, sigma=1) [source]\n \nCompute Harris corner measure response image. This corner detector uses information from the auto-correlation matrix A: A = [(imx**2)   (imx*imy)] = [Axx Axy]\n    [(imx*imy)   (imy**2)]   [Axy Ayy]\n Where imx and imy are first derivatives, averaged with a gaussian filter. The corner measure is then defined as: det(A) - k * trace(A)**2\n or: 2 * det(A) / (trace(A) + eps)\n  Parameters \n \nimagendarray \n\nInput image.  \nmethod{\u2018k\u2019, \u2018eps\u2019}, optional \n\nMethod to compute the response image from the auto-correlation matrix.  \nkfloat, optional \n\nSensitivity factor to separate corners from edges, typically in range [0, 0.2]. Small values of k result in detection of sharp corners.  \nepsfloat, optional \n\nNormalisation factor (Noble\u2019s corner measure).  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.    Returns \n \nresponsendarray \n\nHarris response image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Corner_detection   Examples >>> from skimage.feature import corner_harris, corner_peaks\n>>> square = np.zeros([10, 10])\n>>> square[2:8, 2:8] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> corner_peaks(corner_harris(square), min_distance=1)\narray([[2, 2],\n       [2, 7],\n       [7, 2],\n       [7, 7]])\n \n corner_kitchen_rosenfeld  \nskimage.feature.corner_kitchen_rosenfeld(image, mode='constant', cval=0) [source]\n \nCompute Kitchen and Rosenfeld corner measure response image. The corner measure is calculated as follows: (imxx * imy**2 + imyy * imx**2 - 2 * imxy * imx * imy)\n    / (imx**2 + imy**2)\n Where imx and imy are the first and imxx, imxy, imyy the second derivatives.  Parameters \n \nimagendarray \n\nInput image.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nresponsendarray \n\nKitchen and Rosenfeld response image.     References  \n1  \nKitchen, L., & Rosenfeld, A. (1982). Gray-level corner detection. Pattern recognition letters, 1(2), 95-102. DOI:10.1016/0167-8655(82)90020-4   \n corner_moravec  \nskimage.feature.corner_moravec(image, window_size=1) [source]\n \nCompute Moravec corner measure response image. This is one of the simplest corner detectors and is comparatively fast but has several limitations (e.g. not rotation invariant).  Parameters \n \nimagendarray \n\nInput image.  \nwindow_sizeint, optional \n\nWindow size.    Returns \n \nresponsendarray \n\nMoravec response image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Corner_detection   Examples >>> from skimage.feature import corner_moravec\n>>> square = np.zeros([7, 7])\n>>> square[3, 3] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0]])\n>>> corner_moravec(square).astype(int)\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 2, 1, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0]])\n \n corner_orientations  \nskimage.feature.corner_orientations(image, corners, mask) [source]\n \nCompute the orientation of corners. The orientation of corners is computed using the first order central moment i.e. the center of mass approach. The corner orientation is the angle of the vector from the corner coordinate to the intensity centroid in the local neighborhood around the corner calculated using first order central moment.  Parameters \n \nimage2D array \n\nInput grayscale image.  \ncorners(N, 2) array \n\nCorner coordinates as (row, col).  \nmask2D array \n\nMask defining the local neighborhood of the corner used for the calculation of the central moment.    Returns \n \norientations(N, 1) array \n\nOrientations of corners in the range [-pi, pi].     References  \n1  \nEthan Rublee, Vincent Rabaud, Kurt Konolige and Gary Bradski \u201cORB : An efficient alternative to SIFT and SURF\u201d http://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf  \n2  \nPaul L. Rosin, \u201cMeasuring Corner Properties\u201d http://users.cs.cf.ac.uk/Paul.Rosin/corner2.pdf   Examples >>> from skimage.morphology import octagon\n>>> from skimage.feature import (corner_fast, corner_peaks,\n...                              corner_orientations)\n>>> square = np.zeros((12, 12))\n>>> square[3:9, 3:9] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> corners = corner_peaks(corner_fast(square, 9), min_distance=1)\n>>> corners\narray([[3, 3],\n       [3, 8],\n       [8, 3],\n       [8, 8]])\n>>> orientations = corner_orientations(square, corners, octagon(3, 2))\n>>> np.rad2deg(orientations)\narray([  45.,  135.,  -45., -135.])\n \n corner_peaks  \nskimage.feature.corner_peaks(image, min_distance=1, threshold_abs=None, threshold_rel=None, exclude_border=True, indices=True, num_peaks=inf, footprint=None, labels=None, *, num_peaks_per_label=inf, p_norm=inf) [source]\n \nFind peaks in corner measure response image. This differs from skimage.feature.peak_local_max in that it suppresses multiple connected peaks with the same accumulator value.  Parameters \n \nimagendarray \n\nInput image.  \nmin_distanceint, optional \n\nThe minimal allowed distance separating peaks.  \n** \n\nSee skimage.feature.peak_local_max().  \np_normfloat \n\nWhich Minkowski p-norm to use. Should be in the range [1, inf]. A finite large p may cause a ValueError if overflow can occur. inf corresponds to the Chebyshev distance and 2 to the Euclidean distance.    Returns \n \noutputndarray or ndarray of bools \n\n If indices = True : (row, column, \u2026) coordinates of peaks. If indices = False : Boolean array shaped like image, with peaks represented by True values.       See also  \nskimage.feature.peak_local_max\n\n  Notes  Changed in version 0.18: The default value of threshold_rel has changed to None, which corresponds to letting skimage.feature.peak_local_max decide on the default. This is equivalent to threshold_rel=0.  The num_peaks limit is applied before suppression of connected peaks. To limit the number of peaks after suppression, set num_peaks=np.inf and post-process the output of this function. Examples >>> from skimage.feature import peak_local_max\n>>> response = np.zeros((5, 5))\n>>> response[2:4, 2:4] = 1\n>>> response\narray([[0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 1., 1., 0.],\n       [0., 0., 1., 1., 0.],\n       [0., 0., 0., 0., 0.]])\n>>> peak_local_max(response)\narray([[2, 2],\n       [2, 3],\n       [3, 2],\n       [3, 3]])\n>>> corner_peaks(response)\narray([[2, 2]])\n \n corner_shi_tomasi  \nskimage.feature.corner_shi_tomasi(image, sigma=1) [source]\n \nCompute Shi-Tomasi (Kanade-Tomasi) corner measure response image. This corner detector uses information from the auto-correlation matrix A: A = [(imx**2)   (imx*imy)] = [Axx Axy]\n    [(imx*imy)   (imy**2)]   [Axy Ayy]\n Where imx and imy are first derivatives, averaged with a gaussian filter. The corner measure is then defined as the smaller eigenvalue of A: ((Axx + Ayy) - sqrt((Axx - Ayy)**2 + 4 * Axy**2)) / 2\n  Parameters \n \nimagendarray \n\nInput image.  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.    Returns \n \nresponsendarray \n\nShi-Tomasi response image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Corner_detection   Examples >>> from skimage.feature import corner_shi_tomasi, corner_peaks\n>>> square = np.zeros([10, 10])\n>>> square[2:8, 2:8] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> corner_peaks(corner_shi_tomasi(square), min_distance=1)\narray([[2, 2],\n       [2, 7],\n       [7, 2],\n       [7, 7]])\n \n corner_subpix  \nskimage.feature.corner_subpix(image, corners, window_size=11, alpha=0.99) [source]\n \nDetermine subpixel position of corners. A statistical test decides whether the corner is defined as the intersection of two edges or a single peak. Depending on the classification result, the subpixel corner location is determined based on the local covariance of the grey-values. If the significance level for either statistical test is not sufficient, the corner cannot be classified, and the output subpixel position is set to NaN.  Parameters \n \nimagendarray \n\nInput image.  \ncorners(N, 2) ndarray \n\nCorner coordinates (row, col).  \nwindow_sizeint, optional \n\nSearch window size for subpixel estimation.  \nalphafloat, optional \n\nSignificance level for corner classification.    Returns \n \npositions(N, 2) ndarray \n\nSubpixel corner positions. NaN for \u201cnot classified\u201d corners.     References  \n1  \nF\u00f6rstner, W., & G\u00fclch, E. (1987, June). A fast operator for detection and precise location of distinct points, corners and centres of circular features. In Proc. ISPRS intercommission conference on fast processing of photogrammetric data (pp. 281-305). https://cseweb.ucsd.edu/classes/sp02/cse252/foerstner/foerstner.pdf  \n2  \nhttps://en.wikipedia.org/wiki/Corner_detection   Examples >>> from skimage.feature import corner_harris, corner_peaks, corner_subpix\n>>> img = np.zeros((10, 10))\n>>> img[:5, :5] = 1\n>>> img[5:, 5:] = 1\n>>> img.astype(int)\narray([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]])\n>>> coords = corner_peaks(corner_harris(img), min_distance=2)\n>>> coords_subpix = corner_subpix(img, coords, window_size=7)\n>>> coords_subpix\narray([[4.5, 4.5]])\n \n daisy  \nskimage.feature.daisy(image, step=4, radius=15, rings=3, histograms=8, orientations=8, normalization='l1', sigmas=None, ring_radii=None, visualize=False) [source]\n \nExtract DAISY feature descriptors densely for the given image. DAISY is a feature descriptor similar to SIFT formulated in a way that allows for fast dense extraction. Typically, this is practical for bag-of-features image representations. The implementation follows Tola et al. [1] but deviate on the following points:  Histogram bin contribution are smoothed with a circular Gaussian window over the tonal range (the angular range). The sigma values of the spatial Gaussian smoothing in this code do not match the sigma values in the original code by Tola et al. [2]. In their code, spatial smoothing is applied to both the input image and the center histogram. However, this smoothing is not documented in [1] and, therefore, it is omitted.   Parameters \n \nimage(M, N) array \n\nInput image (grayscale).  \nstepint, optional \n\nDistance between descriptor sampling points.  \nradiusint, optional \n\nRadius (in pixels) of the outermost ring.  \nringsint, optional \n\nNumber of rings.  \nhistogramsint, optional \n\nNumber of histograms sampled per ring.  \norientationsint, optional \n\nNumber of orientations (bins) per histogram.  \nnormalization[ \u2018l1\u2019 | \u2018l2\u2019 | \u2018daisy\u2019 | \u2018off\u2019 ], optional \n\nHow to normalize the descriptors  \u2018l1\u2019: L1-normalization of each descriptor. \u2018l2\u2019: L2-normalization of each descriptor. \u2018daisy\u2019: L2-normalization of individual histograms. \u2018off\u2019: Disable normalization.   \nsigmas1D array of float, optional \n\nStandard deviation of spatial Gaussian smoothing for the center histogram and for each ring of histograms. The array of sigmas should be sorted from the center and out. I.e. the first sigma value defines the spatial smoothing of the center histogram and the last sigma value defines the spatial smoothing of the outermost ring. Specifying sigmas overrides the following parameter. rings = len(sigmas) - 1  \nring_radii1D array of int, optional \n\nRadius (in pixels) for each ring. Specifying ring_radii overrides the following two parameters. rings = len(ring_radii) radius = ring_radii[-1] If both sigmas and ring_radii are given, they must satisfy the following predicate since no radius is needed for the center histogram. len(ring_radii) == len(sigmas) + 1  \nvisualizebool, optional \n\nGenerate a visualization of the DAISY descriptors    Returns \n \ndescsarray \n\nGrid of DAISY descriptors for the given image as an array dimensionality (P, Q, R) where P = ceil((M - radius*2) / step) Q = ceil((N - radius*2) / step) R = (rings * histograms + 1) * orientations  \ndescs_img(M, N, 3) array (only if visualize==True) \n\nVisualization of the DAISY descriptors.     References  \n1(1,2)  \nTola et al. \u201cDaisy: An efficient dense descriptor applied to wide- baseline stereo.\u201d Pattern Analysis and Machine Intelligence, IEEE Transactions on 32.5 (2010): 815-830.  \n2  \nhttp://cvlab.epfl.ch/software/daisy   \n draw_haar_like_feature  \nskimage.feature.draw_haar_like_feature(image, r, c, width, height, feature_coord, color_positive_block=(1.0, 0.0, 0.0), color_negative_block=(0.0, 1.0, 0.0), alpha=0.5, max_n_features=None, random_state=None) [source]\n \nVisualization of Haar-like features.  Parameters \n \nimage(M, N) ndarray \n\nThe region of an integral image for which the features need to be computed.  \nrint \n\nRow-coordinate of top left corner of the detection window.  \ncint \n\nColumn-coordinate of top left corner of the detection window.  \nwidthint \n\nWidth of the detection window.  \nheightint \n\nHeight of the detection window.  \nfeature_coordndarray of list of tuples or None, optional \n\nThe array of coordinates to be extracted. This is useful when you want to recompute only a subset of features. In this case feature_type needs to be an array containing the type of each feature, as returned by haar_like_feature_coord(). By default, all coordinates are computed.  \ncolor_positive_rectangletuple of 3 floats \n\nFloats specifying the color for the positive block. Corresponding values define (R, G, B) values. Default value is red (1, 0, 0).  \ncolor_negative_blocktuple of 3 floats \n\nFloats specifying the color for the negative block Corresponding values define (R, G, B) values. Default value is blue (0, 1, 0).  \nalphafloat \n\nValue in the range [0, 1] that specifies opacity of visualization. 1 - fully transparent, 0 - opaque.  \nmax_n_featuresint, default=None \n\nThe maximum number of features to be returned. By default, all features are returned.  \nrandom_stateint, RandomState instance or None, optional \n\nIf int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. The random state is used when generating a set of features smaller than the total number of available features.    Returns \n \nfeatures(M, N), ndarray \n\nAn image in which the different features will be added.     Examples >>> import numpy as np\n>>> from skimage.feature import haar_like_feature_coord\n>>> from skimage.feature import draw_haar_like_feature\n>>> feature_coord, _ = haar_like_feature_coord(2, 2, 'type-4')\n>>> image = draw_haar_like_feature(np.zeros((2, 2)),\n...                                0, 0, 2, 2,\n...                                feature_coord,\n...                                max_n_features=1)\n>>> image\narray([[[0. , 0.5, 0. ],\n        [0.5, 0. , 0. ]],\n\n       [[0.5, 0. , 0. ],\n        [0. , 0.5, 0. ]]])\n \n draw_multiblock_lbp  \nskimage.feature.draw_multiblock_lbp(image, r, c, width, height, lbp_code=0, color_greater_block=(1, 1, 1), color_less_block=(0, 0.69, 0.96), alpha=0.5) [source]\n \nMulti-block local binary pattern visualization. Blocks with higher sums are colored with alpha-blended white rectangles, whereas blocks with lower sums are colored alpha-blended cyan. Colors and the alpha parameter can be changed.  Parameters \n \nimagendarray of float or uint \n\nImage on which to visualize the pattern.  \nrint \n\nRow-coordinate of top left corner of a rectangle containing feature.  \ncint \n\nColumn-coordinate of top left corner of a rectangle containing feature.  \nwidthint \n\nWidth of one of 9 equal rectangles that will be used to compute a feature.  \nheightint \n\nHeight of one of 9 equal rectangles that will be used to compute a feature.  \nlbp_codeint \n\nThe descriptor of feature to visualize. If not provided, the descriptor with 0 value will be used.  \ncolor_greater_blocktuple of 3 floats \n\nFloats specifying the color for the block that has greater intensity value. They should be in the range [0, 1]. Corresponding values define (R, G, B) values. Default value is white (1, 1, 1).  \ncolor_greater_blocktuple of 3 floats \n\nFloats specifying the color for the block that has greater intensity value. They should be in the range [0, 1]. Corresponding values define (R, G, B) values. Default value is cyan (0, 0.69, 0.96).  \nalphafloat \n\nValue in the range [0, 1] that specifies opacity of visualization. 1 - fully transparent, 0 - opaque.    Returns \n \noutputndarray of float \n\nImage with MB-LBP visualization.     References  \n1  \nFace Detection Based on Multi-Block LBP Representation. Lun Zhang, Rufeng Chu, Shiming Xiang, Shengcai Liao, Stan Z. Li http://www.cbsr.ia.ac.cn/users/scliao/papers/Zhang-ICB07-MBLBP.pdf   \n greycomatrix  \nskimage.feature.greycomatrix(image, distances, angles, levels=None, symmetric=False, normed=False) [source]\n \nCalculate the grey-level co-occurrence matrix. A grey level co-occurrence matrix is a histogram of co-occurring greyscale values at a given offset over an image.  Parameters \n \nimagearray_like \n\nInteger typed input image. Only positive valued images are supported. If type is other than uint8, the argument levels needs to be set.  \ndistancesarray_like \n\nList of pixel pair distance offsets.  \nanglesarray_like \n\nList of pixel pair angles in radians.  \nlevelsint, optional \n\nThe input image should contain integers in [0, levels-1], where levels indicate the number of grey-levels counted (typically 256 for an 8-bit image). This argument is required for 16-bit images or higher and is typically the maximum of the image. As the output matrix is at least levels x levels, it might be preferable to use binning of the input image rather than large values for levels.  \nsymmetricbool, optional \n\nIf True, the output matrix P[:, :, d, theta] is symmetric. This is accomplished by ignoring the order of value pairs, so both (i, j) and (j, i) are accumulated when (i, j) is encountered for a given offset. The default is False.  \nnormedbool, optional \n\nIf True, normalize each matrix P[:, :, d, theta] by dividing by the total number of accumulated co-occurrences for the given offset. The elements of the resulting matrix sum to 1. The default is False.    Returns \n \nP4-D ndarray \n\nThe grey-level co-occurrence histogram. The value P[i,j,d,theta] is the number of times that grey-level j occurs at a distance d and at an angle theta from grey-level i. If normed is False, the output is of type uint32, otherwise it is float64. The dimensions are: levels x levels x number of distances x number of angles.     References  \n1  \nThe GLCM Tutorial Home Page, http://www.fp.ucalgary.ca/mhallbey/tutorial.htm  \n2  \nHaralick, RM.; Shanmugam, K., \u201cTextural features for image classification\u201d IEEE Transactions on systems, man, and cybernetics 6 (1973): 610-621. DOI:10.1109/TSMC.1973.4309314  \n3  \nPattern Recognition Engineering, Morton Nadler & Eric P. Smith  \n4  \nWikipedia, https://en.wikipedia.org/wiki/Co-occurrence_matrix   Examples Compute 2 GLCMs: One for a 1-pixel offset to the right, and one for a 1-pixel offset upwards. >>> image = np.array([[0, 0, 1, 1],\n...                   [0, 0, 1, 1],\n...                   [0, 2, 2, 2],\n...                   [2, 2, 3, 3]], dtype=np.uint8)\n>>> result = greycomatrix(image, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4],\n...                       levels=4)\n>>> result[:, :, 0, 0]\narray([[2, 2, 1, 0],\n       [0, 2, 0, 0],\n       [0, 0, 3, 1],\n       [0, 0, 0, 1]], dtype=uint32)\n>>> result[:, :, 0, 1]\narray([[1, 1, 3, 0],\n       [0, 1, 1, 0],\n       [0, 0, 0, 2],\n       [0, 0, 0, 0]], dtype=uint32)\n>>> result[:, :, 0, 2]\narray([[3, 0, 2, 0],\n       [0, 2, 2, 0],\n       [0, 0, 1, 2],\n       [0, 0, 0, 0]], dtype=uint32)\n>>> result[:, :, 0, 3]\narray([[2, 0, 0, 0],\n       [1, 1, 2, 0],\n       [0, 0, 2, 1],\n       [0, 0, 0, 0]], dtype=uint32)\n \n Examples using skimage.feature.greycomatrix\n \n  GLCM Texture Features   greycoprops  \nskimage.feature.greycoprops(P, prop='contrast') [source]\n \nCalculate texture properties of a GLCM. Compute a feature of a grey level co-occurrence matrix to serve as a compact summary of the matrix. The properties are computed as follows:  \u2018contrast\u2019: \\(\\sum_{i,j=0}^{levels-1} P_{i,j}(i-j)^2\\)\n \u2018dissimilarity\u2019: \\(\\sum_{i,j=0}^{levels-1}P_{i,j}|i-j|\\)\n \u2018homogeneity\u2019: \\(\\sum_{i,j=0}^{levels-1}\\frac{P_{i,j}}{1+(i-j)^2}\\)\n \u2018ASM\u2019: \\(\\sum_{i,j=0}^{levels-1} P_{i,j}^2\\)\n \u2018energy\u2019: \\(\\sqrt{ASM}\\)\n \n \u2018correlation\u2019:\n\n \\[\\sum_{i,j=0}^{levels-1} P_{i,j}\\left[\\frac{(i-\\mu_i) \\ (j-\\mu_j)}{\\sqrt{(\\sigma_i^2)(\\sigma_j^2)}}\\right]\\]     Each GLCM is normalized to have a sum of 1 before the computation of texture properties.  Parameters \n \nPndarray \n\nInput array. P is the grey-level co-occurrence histogram for which to compute the specified property. The value P[i,j,d,theta] is the number of times that grey-level j occurs at a distance d and at an angle theta from grey-level i.  \nprop{\u2018contrast\u2019, \u2018dissimilarity\u2019, \u2018homogeneity\u2019, \u2018energy\u2019, \u2018correlation\u2019, \u2018ASM\u2019}, optional \n\nThe property of the GLCM to compute. The default is \u2018contrast\u2019.    Returns \n \nresults2-D ndarray \n\n2-dimensional array. results[d, a] is the property \u2018prop\u2019 for the d\u2019th distance and the a\u2019th angle.     References  \n1  \nThe GLCM Tutorial Home Page, http://www.fp.ucalgary.ca/mhallbey/tutorial.htm   Examples Compute the contrast for GLCMs with distances [1, 2] and angles [0 degrees, 90 degrees] >>> image = np.array([[0, 0, 1, 1],\n...                   [0, 0, 1, 1],\n...                   [0, 2, 2, 2],\n...                   [2, 2, 3, 3]], dtype=np.uint8)\n>>> g = greycomatrix(image, [1, 2], [0, np.pi/2], levels=4,\n...                  normed=True, symmetric=True)\n>>> contrast = greycoprops(g, 'contrast')\n>>> contrast\narray([[0.58333333, 1.        ],\n       [1.25      , 2.75      ]])\n \n Examples using skimage.feature.greycoprops\n \n  GLCM Texture Features   haar_like_feature  \nskimage.feature.haar_like_feature(int_image, r, c, width, height, feature_type=None, feature_coord=None) [source]\n \nCompute the Haar-like features for a region of interest (ROI) of an integral image. Haar-like features have been successfully used for image classification and object detection [1]. It has been used for real-time face detection algorithm proposed in [2].  Parameters \n \nint_image(M, N) ndarray \n\nIntegral image for which the features need to be computed.  \nrint \n\nRow-coordinate of top left corner of the detection window.  \ncint \n\nColumn-coordinate of top left corner of the detection window.  \nwidthint \n\nWidth of the detection window.  \nheightint \n\nHeight of the detection window.  \nfeature_typestr or list of str or None, optional \n\nThe type of feature to consider:  \u2018type-2-x\u2019: 2 rectangles varying along the x axis; \u2018type-2-y\u2019: 2 rectangles varying along the y axis; \u2018type-3-x\u2019: 3 rectangles varying along the x axis; \u2018type-3-y\u2019: 3 rectangles varying along the y axis; \u2018type-4\u2019: 4 rectangles varying along x and y axis.  By default all features are extracted. If using with feature_coord, it should correspond to the feature type of each associated coordinate feature.  \nfeature_coordndarray of list of tuples or None, optional \n\nThe array of coordinates to be extracted. This is useful when you want to recompute only a subset of features. In this case feature_type needs to be an array containing the type of each feature, as returned by haar_like_feature_coord(). By default, all coordinates are computed.    Returns \n \nhaar_features(n_features,) ndarray of int or float \n\nResulting Haar-like features. Each value is equal to the subtraction of sums of the positive and negative rectangles. The data type depends of the data type of int_image: int when the data type of int_image is uint or int and float when the data type of int_image is float.     Notes When extracting those features in parallel, be aware that the choice of the backend (i.e. multiprocessing vs threading) will have an impact on the performance. The rule of thumb is as follows: use multiprocessing when extracting features for all possible ROI in an image; use threading when extracting the feature at specific location for a limited number of ROIs. Refer to the example Face classification using Haar-like feature descriptor for more insights. References  \n1  \nhttps://en.wikipedia.org/wiki/Haar-like_feature  \n2  \nOren, M., Papageorgiou, C., Sinha, P., Osuna, E., & Poggio, T. (1997, June). Pedestrian detection using wavelet templates. In Computer Vision and Pattern Recognition, 1997. Proceedings., 1997 IEEE Computer Society Conference on (pp. 193-199). IEEE. http://tinyurl.com/y6ulxfta DOI:10.1109/CVPR.1997.609319  \n3  \nViola, Paul, and Michael J. Jones. \u201cRobust real-time face detection.\u201d International journal of computer vision 57.2 (2004): 137-154. https://www.merl.com/publications/docs/TR2004-043.pdf DOI:10.1109/CVPR.2001.990517   Examples >>> import numpy as np\n>>> from skimage.transform import integral_image\n>>> from skimage.feature import haar_like_feature\n>>> img = np.ones((5, 5), dtype=np.uint8)\n>>> img_ii = integral_image(img)\n>>> feature = haar_like_feature(img_ii, 0, 0, 5, 5, 'type-3-x')\n>>> feature\narray([-1, -2, -3, -4, -1, -2, -3, -4, -1, -2, -3, -4, -1, -2, -3, -4, -1,\n       -2, -3, -4, -1, -2, -3, -4, -1, -2, -3, -1, -2, -3, -1, -2, -3, -1,\n       -2, -1, -2, -1, -2, -1, -1, -1])\n You can compute the feature for some pre-computed coordinates. >>> from skimage.feature import haar_like_feature_coord\n>>> feature_coord, feature_type = zip(\n...     *[haar_like_feature_coord(5, 5, feat_t)\n...       for feat_t in ('type-2-x', 'type-3-x')])\n>>> # only select one feature over two\n>>> feature_coord = np.concatenate([x[::2] for x in feature_coord])\n>>> feature_type = np.concatenate([x[::2] for x in feature_type])\n>>> feature = haar_like_feature(img_ii, 0, 0, 5, 5,\n...                             feature_type=feature_type,\n...                             feature_coord=feature_coord)\n>>> feature\narray([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0, -1, -3, -1, -3, -1, -3, -1, -3, -1,\n       -3, -1, -3, -1, -3, -2, -1, -3, -2, -2, -2, -1])\n \n haar_like_feature_coord  \nskimage.feature.haar_like_feature_coord(width, height, feature_type=None) [source]\n \nCompute the coordinates of Haar-like features.  Parameters \n \nwidthint \n\nWidth of the detection window.  \nheightint \n\nHeight of the detection window.  \nfeature_typestr or list of str or None, optional \n\nThe type of feature to consider:  \u2018type-2-x\u2019: 2 rectangles varying along the x axis; \u2018type-2-y\u2019: 2 rectangles varying along the y axis; \u2018type-3-x\u2019: 3 rectangles varying along the x axis; \u2018type-3-y\u2019: 3 rectangles varying along the y axis; \u2018type-4\u2019: 4 rectangles varying along x and y axis.  By default all features are extracted.    Returns \n \nfeature_coord(n_features, n_rectangles, 2, 2), ndarray of list of tuple coord \n\nCoordinates of the rectangles for each feature.  \nfeature_type(n_features,), ndarray of str \n\nThe corresponding type for each feature.     Examples >>> import numpy as np\n>>> from skimage.transform import integral_image\n>>> from skimage.feature import haar_like_feature_coord\n>>> feat_coord, feat_type = haar_like_feature_coord(2, 2, 'type-4')\n>>> feat_coord \narray([ list([[(0, 0), (0, 0)], [(0, 1), (0, 1)],\n              [(1, 1), (1, 1)], [(1, 0), (1, 0)]])], dtype=object)\n>>> feat_type\narray(['type-4'], dtype=object)\n \n hessian_matrix  \nskimage.feature.hessian_matrix(image, sigma=1, mode='constant', cval=0, order='rc') [source]\n \nCompute Hessian matrix. The Hessian matrix is defined as: H = [Hrr Hrc]\n    [Hrc Hcc]\n which is computed by convolving the image with the second derivatives of the Gaussian kernel in the respective r- and c-directions.  Parameters \n \nimagendarray \n\nInput image.  \nsigmafloat \n\nStandard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.  \norder{\u2018rc\u2019, \u2018xy\u2019}, optional \n\nThis parameter allows for the use of reverse or forward order of the image axes in gradient computation. \u2018rc\u2019 indicates the use of the first axis initially (Hrr, Hrc, Hcc), whilst \u2018xy\u2019 indicates the usage of the last axis initially (Hxx, Hxy, Hyy)    Returns \n \nHrrndarray \n\nElement of the Hessian matrix for each pixel in the input image.  \nHrcndarray \n\nElement of the Hessian matrix for each pixel in the input image.  \nHccndarray \n\nElement of the Hessian matrix for each pixel in the input image.     Examples >>> from skimage.feature import hessian_matrix\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 4\n>>> Hrr, Hrc, Hcc = hessian_matrix(square, sigma=0.1, order='rc')\n>>> Hrc\narray([[ 0.,  0.,  0.,  0.,  0.],\n       [ 0.,  1.,  0., -1.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.],\n       [ 0., -1.,  0.,  1.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.]])\n \n hessian_matrix_det  \nskimage.feature.hessian_matrix_det(image, sigma=1, approximate=True) [source]\n \nCompute the approximate Hessian Determinant over an image. The 2D approximate method uses box filters over integral images to compute the approximate Hessian Determinant, as described in [1].  Parameters \n \nimagearray \n\nThe image over which to compute Hessian Determinant.  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, used for the Hessian matrix.  \napproximatebool, optional \n\nIf True and the image is 2D, use a much faster approximate computation. This argument has no effect on 3D and higher images.    Returns \n \noutarray \n\nThe array of the Determinant of Hessians.     Notes For 2D images when approximate=True, the running time of this method only depends on size of the image. It is independent of sigma as one would expect. The downside is that the result for sigma less than 3 is not accurate, i.e., not similar to the result obtained if someone computed the Hessian and took its determinant. References  \n1  \nHerbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool, \u201cSURF: Speeded Up Robust Features\u201d ftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf   \n hessian_matrix_eigvals  \nskimage.feature.hessian_matrix_eigvals(H_elems) [source]\n \nCompute eigenvalues of Hessian matrix.  Parameters \n \nH_elemslist of ndarray \n\nThe upper-diagonal elements of the Hessian matrix, as returned by hessian_matrix.    Returns \n \neigsndarray \n\nThe eigenvalues of the Hessian matrix, in decreasing order. The eigenvalues are the leading dimension. That is, eigs[i, j, k] contains the ith-largest eigenvalue at position (j, k).     Examples >>> from skimage.feature import hessian_matrix, hessian_matrix_eigvals\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 4\n>>> H_elems = hessian_matrix(square, sigma=0.1, order='rc')\n>>> hessian_matrix_eigvals(H_elems)[0]\narray([[ 0.,  0.,  2.,  0.,  0.],\n       [ 0.,  1.,  0.,  1.,  0.],\n       [ 2.,  0., -2.,  0.,  2.],\n       [ 0.,  1.,  0.,  1.,  0.],\n       [ 0.,  0.,  2.,  0.,  0.]])\n \n hog  \nskimage.feature.hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(3, 3), block_norm='L2-Hys', visualize=False, transform_sqrt=False, feature_vector=True, multichannel=None) [source]\n \nExtract Histogram of Oriented Gradients (HOG) for a given image. Compute a Histogram of Oriented Gradients (HOG) by  (optional) global image normalization computing the gradient image in row and col\n computing gradient histograms normalizing across blocks flattening into a feature vector   Parameters \n \nimage(M, N[, C]) ndarray \n\nInput image.  \norientationsint, optional \n\nNumber of orientation bins.  \npixels_per_cell2-tuple (int, int), optional \n\nSize (in pixels) of a cell.  \ncells_per_block2-tuple (int, int), optional \n\nNumber of cells in each block.  \nblock_normstr {\u2018L1\u2019, \u2018L1-sqrt\u2019, \u2018L2\u2019, \u2018L2-Hys\u2019}, optional \n\nBlock normalization method:  \nL1 \n\nNormalization using L1-norm.  \nL1-sqrt \n\nNormalization using L1-norm, followed by square root.  \nL2 \n\nNormalization using L2-norm.  \nL2-Hys \n\nNormalization using L2-norm, followed by limiting the maximum values to 0.2 (Hys stands for hysteresis) and renormalization using L2-norm. (default) For details, see [3], [4].    \nvisualizebool, optional \n\nAlso return an image of the HOG. For each cell and orientation bin, the image contains a line segment that is centered at the cell center, is perpendicular to the midpoint of the range of angles spanned by the orientation bin, and has intensity proportional to the corresponding histogram value.  \ntransform_sqrtbool, optional \n\nApply power law compression to normalize the image before processing. DO NOT use this if the image contains negative values. Also see notes section below.  \nfeature_vectorbool, optional \n\nReturn the data as a feature vector by calling .ravel() on the result just before returning.  \nmultichannelboolean, optional \n\nIf True, the last image dimension is considered as a color channel, otherwise as spatial.    Returns \n \nout(n_blocks_row, n_blocks_col, n_cells_row, n_cells_col, n_orient) ndarray \n\nHOG descriptor for the image. If feature_vector is True, a 1D (flattened) array is returned.  \nhog_image(M, N) ndarray, optional \n\nA visualisation of the HOG image. Only provided if visualize is True.     Notes The presented code implements the HOG extraction method from [2] with the following changes: (I) blocks of (3, 3) cells are used ((2, 2) in the paper); (II) no smoothing within cells (Gaussian spatial window with sigma=8pix in the paper); (III) L1 block normalization is used (L2-Hys in the paper). Power law compression, also known as Gamma correction, is used to reduce the effects of shadowing and illumination variations. The compression makes the dark regions lighter. When the kwarg transform_sqrt is set to True, the function computes the square root of each color channel and then applies the hog algorithm to the image. References  \n1  \nhttps://en.wikipedia.org/wiki/Histogram_of_oriented_gradients  \n2  \nDalal, N and Triggs, B, Histograms of Oriented Gradients for Human Detection, IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2005 San Diego, CA, USA, https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf, DOI:10.1109/CVPR.2005.177  \n3  \nLowe, D.G., Distinctive image features from scale-invatiant keypoints, International Journal of Computer Vision (2004) 60: 91, http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf, DOI:10.1023/B:VISI.0000029664.99615.94  \n4  \nDalal, N, Finding People in Images and Videos, Human-Computer Interaction [cs.HC], Institut National Polytechnique de Grenoble - INPG, 2006, https://tel.archives-ouvertes.fr/tel-00390303/file/NavneetDalalThesis.pdf   \n local_binary_pattern  \nskimage.feature.local_binary_pattern(image, P, R, method='default') [source]\n \nGray scale and rotation invariant LBP (Local Binary Patterns). LBP is an invariant descriptor that can be used for texture classification.  Parameters \n \nimage(N, M) array \n\nGraylevel image.  \nPint \n\nNumber of circularly symmetric neighbour set points (quantization of the angular space).  \nRfloat \n\nRadius of circle (spatial resolution of the operator).  \nmethod{\u2018default\u2019, \u2018ror\u2019, \u2018uniform\u2019, \u2018var\u2019} \n\nMethod to determine the pattern.  \n \u2018default\u2019: original local binary pattern which is gray scale but not\n\nrotation invariant.    \n \u2018ror\u2019: extension of default implementation which is gray scale and\n\nrotation invariant.    \n \u2018uniform\u2019: improved rotation invariance with uniform patterns and\n\nfiner quantization of the angular space which is gray scale and rotation invariant.    \n \u2018nri_uniform\u2019: non rotation-invariant uniform patterns variant\n\nwhich is only gray scale invariant [2].    \n \u2018var\u2019: rotation invariant variance measures of the contrast of local\n\nimage texture which is rotation but not gray scale invariant.        Returns \n \noutput(N, M) array \n\nLBP image.     References  \n1  \nMultiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns. Timo Ojala, Matti Pietikainen, Topi Maenpaa. http://www.ee.oulu.fi/research/mvmp/mvg/files/pdf/pdf_94.pdf, 2002.  \n2  \nFace recognition with local binary patterns. Timo Ahonen, Abdenour Hadid, Matti Pietikainen, http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.214.6851, 2004.   \n masked_register_translation  \nskimage.feature.masked_register_translation(src_image, target_image, src_mask, target_mask=None, overlap_ratio=0.3) [source]\n \nDeprecated function. Use skimage.registration.phase_cross_correlation instead. \n match_descriptors  \nskimage.feature.match_descriptors(descriptors1, descriptors2, metric=None, p=2, max_distance=inf, cross_check=True, max_ratio=1.0) [source]\n \nBrute-force matching of descriptors. For each descriptor in the first set this matcher finds the closest descriptor in the second set (and vice-versa in the case of enabled cross-checking).  Parameters \n \ndescriptors1(M, P) array \n\nDescriptors of size P about M keypoints in the first image.  \ndescriptors2(N, P) array \n\nDescriptors of size P about N keypoints in the second image.  \nmetric{\u2018euclidean\u2019, \u2018cityblock\u2019, \u2018minkowski\u2019, \u2018hamming\u2019, \u2026} , optional \n\nThe metric to compute the distance between two descriptors. See scipy.spatial.distance.cdist for all possible types. The hamming distance should be used for binary descriptors. By default the L2-norm is used for all descriptors of dtype float or double and the Hamming distance is used for binary descriptors automatically.  \npint, optional \n\nThe p-norm to apply for metric='minkowski'.  \nmax_distancefloat, optional \n\nMaximum allowed distance between descriptors of two keypoints in separate images to be regarded as a match.  \ncross_checkbool, optional \n\nIf True, the matched keypoints are returned after cross checking i.e. a matched pair (keypoint1, keypoint2) is returned if keypoint2 is the best match for keypoint1 in second image and keypoint1 is the best match for keypoint2 in first image.  \nmax_ratiofloat, optional \n\nMaximum ratio of distances between first and second closest descriptor in the second set of descriptors. This threshold is useful to filter ambiguous matches between the two descriptor sets. The choice of this value depends on the statistics of the chosen descriptor, e.g., for SIFT descriptors a value of 0.8 is usually chosen, see D.G. Lowe, \u201cDistinctive Image Features from Scale-Invariant Keypoints\u201d, International Journal of Computer Vision, 2004.    Returns \n \nmatches(Q, 2) array \n\nIndices of corresponding matches in first and second set of descriptors, where matches[:, 0] denote the indices in the first and matches[:, 1] the indices in the second set of descriptors.     \n match_template  \nskimage.feature.match_template(image, template, pad_input=False, mode='constant', constant_values=0) [source]\n \nMatch a template to a 2-D or 3-D image using normalized correlation. The output is an array with values between -1.0 and 1.0. The value at a given position corresponds to the correlation coefficient between the image and the template. For pad_input=True matches correspond to the center and otherwise to the top-left corner of the template. To find the best match you must search for peaks in the response (output) image.  Parameters \n \nimage(M, N[, D]) array \n\n2-D or 3-D input image.  \ntemplate(m, n[, d]) array \n\nTemplate to locate. It must be (m <= M, n <= N[, d <= D]).  \npad_inputbool \n\nIf True, pad image so that output is the same size as the image, and output values correspond to the template center. Otherwise, the output is an array with shape (M - m + 1, N - n + 1) for an (M, N) image and an (m, n) template, and matches correspond to origin (top-left corner) of the template.  \nmodesee numpy.pad, optional \n\nPadding mode.  \nconstant_valuessee numpy.pad, optional \n\nConstant values used in conjunction with mode='constant'.    Returns \n \noutputarray \n\nResponse image with correlation coefficients.     Notes Details on the cross-correlation are presented in [1]. This implementation uses FFT convolutions of the image and the template. Reference [2] presents similar derivations but the approximation presented in this reference is not used in our implementation. References  \n1  \nJ. P. Lewis, \u201cFast Normalized Cross-Correlation\u201d, Industrial Light and Magic.  \n2  \nBriechle and Hanebeck, \u201cTemplate Matching using Fast Normalized Cross Correlation\u201d, Proceedings of the SPIE (2001). DOI:10.1117/12.421129   Examples >>> template = np.zeros((3, 3))\n>>> template[1, 1] = 1\n>>> template\narray([[0., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 0.]])\n>>> image = np.zeros((6, 6))\n>>> image[1, 1] = 1\n>>> image[4, 4] = -1\n>>> image\narray([[ 0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0., -1.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.]])\n>>> result = match_template(image, template)\n>>> np.round(result, 3)\narray([[ 1.   , -0.125,  0.   ,  0.   ],\n       [-0.125, -0.125,  0.   ,  0.   ],\n       [ 0.   ,  0.   ,  0.125,  0.125],\n       [ 0.   ,  0.   ,  0.125, -1.   ]])\n>>> result = match_template(image, template, pad_input=True)\n>>> np.round(result, 3)\narray([[-0.125, -0.125, -0.125,  0.   ,  0.   ,  0.   ],\n       [-0.125,  1.   , -0.125,  0.   ,  0.   ,  0.   ],\n       [-0.125, -0.125, -0.125,  0.   ,  0.   ,  0.   ],\n       [ 0.   ,  0.   ,  0.   ,  0.125,  0.125,  0.125],\n       [ 0.   ,  0.   ,  0.   ,  0.125, -1.   ,  0.125],\n       [ 0.   ,  0.   ,  0.   ,  0.125,  0.125,  0.125]])\n \n multiblock_lbp  \nskimage.feature.multiblock_lbp(int_image, r, c, width, height) [source]\n \nMulti-block local binary pattern (MB-LBP). The features are calculated similarly to local binary patterns (LBPs), (See local_binary_pattern()) except that summed blocks are used instead of individual pixel values. MB-LBP is an extension of LBP that can be computed on multiple scales in constant time using the integral image. Nine equally-sized rectangles are used to compute a feature. For each rectangle, the sum of the pixel intensities is computed. Comparisons of these sums to that of the central rectangle determine the feature, similarly to LBP.  Parameters \n \nint_image(N, M) array \n\nIntegral image.  \nrint \n\nRow-coordinate of top left corner of a rectangle containing feature.  \ncint \n\nColumn-coordinate of top left corner of a rectangle containing feature.  \nwidthint \n\nWidth of one of the 9 equal rectangles that will be used to compute a feature.  \nheightint \n\nHeight of one of the 9 equal rectangles that will be used to compute a feature.    Returns \n \noutputint \n\n8-bit MB-LBP feature descriptor.     References  \n1  \nFace Detection Based on Multi-Block LBP Representation. Lun Zhang, Rufeng Chu, Shiming Xiang, Shengcai Liao, Stan Z. Li http://www.cbsr.ia.ac.cn/users/scliao/papers/Zhang-ICB07-MBLBP.pdf   \n multiscale_basic_features  \nskimage.feature.multiscale_basic_features(image, multichannel=False, intensity=True, edges=True, texture=True, sigma_min=0.5, sigma_max=16, num_sigma=None, num_workers=None) [source]\n \nLocal features for a single- or multi-channel nd image. Intensity, gradient intensity and local structure are computed at different scales thanks to Gaussian blurring.  Parameters \n \nimagendarray \n\nInput image, which can be grayscale or multichannel.  \nmultichannelbool, default False \n\nTrue if the last dimension corresponds to color channels.  \nintensitybool, default True \n\nIf True, pixel intensities averaged over the different scales are added to the feature set.  \nedgesbool, default True \n\nIf True, intensities of local gradients averaged over the different scales are added to the feature set.  \ntexturebool, default True \n\nIf True, eigenvalues of the Hessian matrix after Gaussian blurring at different scales are added to the feature set.  \nsigma_minfloat, optional \n\nSmallest value of the Gaussian kernel used to average local neighbourhoods before extracting features.  \nsigma_maxfloat, optional \n\nLargest value of the Gaussian kernel used to average local neighbourhoods before extracting features.  \nnum_sigmaint, optional \n\nNumber of values of the Gaussian kernel between sigma_min and sigma_max. If None, sigma_min multiplied by powers of 2 are used.  \nnum_workersint or None, optional \n\nThe number of parallel threads to use. If set to None, the full set of available cores are used.    Returns \n \nfeaturesnp.ndarray \n\nArray of shape image.shape + (n_features,)     \n Examples using skimage.feature.multiscale_basic_features\n \n  Trainable segmentation using local features and random forests   peak_local_max  \nskimage.feature.peak_local_max(image, min_distance=1, threshold_abs=None, threshold_rel=None, exclude_border=True, indices=True, num_peaks=inf, footprint=None, labels=None, num_peaks_per_label=inf, p_norm=inf) [source]\n \nFind peaks in an image as coordinate list or boolean mask. Peaks are the local maxima in a region of 2 * min_distance + 1 (i.e. peaks are separated by at least min_distance). If both threshold_abs and threshold_rel are provided, the maximum of the two is chosen as the minimum intensity threshold of peaks.  Changed in version 0.18: Prior to version 0.18, peaks of the same height within a radius of min_distance were all returned, but this could cause unexpected behaviour. From 0.18 onwards, an arbitrary peak within the region is returned. See issue gh-2592.   Parameters \n \nimagendarray \n\nInput image.  \nmin_distanceint, optional \n\nThe minimal allowed distance separating peaks. To find the maximum number of peaks, use min_distance=1.  \nthreshold_absfloat, optional \n\nMinimum intensity of peaks. By default, the absolute threshold is the minimum intensity of the image.  \nthreshold_relfloat, optional \n\nMinimum intensity of peaks, calculated as max(image) * threshold_rel.  \nexclude_borderint, tuple of ints, or bool, optional \n\nIf positive integer, exclude_border excludes peaks from within exclude_border-pixels of the border of the image. If tuple of non-negative ints, the length of the tuple must match the input array\u2019s dimensionality. Each element of the tuple will exclude peaks from within exclude_border-pixels of the border of the image along that dimension. If True, takes the min_distance parameter as value. If zero or False, peaks are identified regardless of their distance from the border.  \nindicesbool, optional \n\nIf True, the output will be an array representing peak coordinates. The coordinates are sorted according to peaks values (Larger first). If False, the output will be a boolean array shaped as image.shape with peaks present at True elements. indices is deprecated and will be removed in version 0.20. Default behavior will be to always return peak coordinates. You can obtain a mask as shown in the example below.  \nnum_peaksint, optional \n\nMaximum number of peaks. When the number of peaks exceeds num_peaks, return num_peaks peaks based on highest peak intensity.  \nfootprintndarray of bools, optional \n\nIf provided, footprint == 1 represents the local region within which to search for peaks at every point in image.  \nlabelsndarray of ints, optional \n\nIf provided, each unique region labels == value represents a unique region to search for peaks. Zero is reserved for background.  \nnum_peaks_per_labelint, optional \n\nMaximum number of peaks for each label.  \np_normfloat \n\nWhich Minkowski p-norm to use. Should be in the range [1, inf]. A finite large p may cause a ValueError if overflow can occur. inf corresponds to the Chebyshev distance and 2 to the Euclidean distance.    Returns \n \noutputndarray or ndarray of bools \n\n If indices = True : (row, column, \u2026) coordinates of peaks. If indices = False : Boolean array shaped like image, with peaks represented by True values.       See also  \nskimage.feature.corner_peaks\n\n  Notes The peak local maximum function returns the coordinates of local peaks (maxima) in an image. Internally, a maximum filter is used for finding local maxima. This operation dilates the original image. After comparison of the dilated and original image, this function returns the coordinates or a mask of the peaks where the dilated image equals the original image. Examples >>> img1 = np.zeros((7, 7))\n>>> img1[3, 4] = 1\n>>> img1[3, 2] = 1.5\n>>> img1\narray([[0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n       [0. , 0. , 1.5, 0. , 1. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 0. , 0. ]])\n >>> peak_local_max(img1, min_distance=1)\narray([[3, 2],\n       [3, 4]])\n >>> peak_local_max(img1, min_distance=2)\narray([[3, 2]])\n >>> img2 = np.zeros((20, 20, 20))\n>>> img2[10, 10, 10] = 1\n>>> img2[15, 15, 15] = 1\n>>> peak_idx = peak_local_max(img2, exclude_border=0)\n>>> peak_idx\narray([[10, 10, 10],\n       [15, 15, 15]])\n >>> peak_mask = np.zeros_like(img2, dtype=bool)\n>>> peak_mask[tuple(peak_idx.T)] = True\n>>> np.argwhere(peak_mask)\narray([[10, 10, 10],\n       [15, 15, 15]])\n \n Examples using skimage.feature.peak_local_max\n \n  Finding local maxima  \n\n  Watershed segmentation  \n\n  Segment human cells (in mitosis)   plot_matches  \nskimage.feature.plot_matches(ax, image1, image2, keypoints1, keypoints2, matches, keypoints_color='k', matches_color=None, only_matches=False, alignment='horizontal') [source]\n \nPlot matched features.  Parameters \n \naxmatplotlib.axes.Axes \n\nMatches and image are drawn in this ax.  \nimage1(N, M [, 3]) array \n\nFirst grayscale or color image.  \nimage2(N, M [, 3]) array \n\nSecond grayscale or color image.  \nkeypoints1(K1, 2) array \n\nFirst keypoint coordinates as (row, col).  \nkeypoints2(K2, 2) array \n\nSecond keypoint coordinates as (row, col).  \nmatches(Q, 2) array \n\nIndices of corresponding matches in first and second set of descriptors, where matches[:, 0] denote the indices in the first and matches[:, 1] the indices in the second set of descriptors.  \nkeypoints_colormatplotlib color, optional \n\nColor for keypoint locations.  \nmatches_colormatplotlib color, optional \n\nColor for lines which connect keypoint matches. By default the color is chosen randomly.  \nonly_matchesbool, optional \n\nWhether to only plot matches and not plot the keypoint locations.  \nalignment{\u2018horizontal\u2019, \u2018vertical\u2019}, optional \n\nWhether to show images side by side, 'horizontal', or one above the other, 'vertical'.     \n register_translation  \nskimage.feature.register_translation(src_image, target_image, upsample_factor=1, space='real', return_error=True) [source]\n \nDeprecated function. Use skimage.registration.phase_cross_correlation instead. \n shape_index  \nskimage.feature.shape_index(image, sigma=1, mode='constant', cval=0) [source]\n \nCompute the shape index. The shape index, as defined by Koenderink & van Doorn [1], is a single valued measure of local curvature, assuming the image as a 3D plane with intensities representing heights. It is derived from the eigen values of the Hessian, and its value ranges from -1 to 1 (and is undefined (=NaN) in flat regions), with following ranges representing following shapes:  Ranges of the shape index and corresponding shapes.  \nInterval (s in \u2026) Shape   \n[ -1, -7/8) Spherical cup  \n[-7/8, -5/8) Through  \n[-5/8, -3/8) Rut  \n[-3/8, -1/8) Saddle rut  \n[-1/8, +1/8) Saddle  \n[+1/8, +3/8) Saddle ridge  \n[+3/8, +5/8) Ridge  \n[+5/8, +7/8) Dome  \n[+7/8, +1] Spherical cap    Parameters \n \nimagendarray \n\nInput image.  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, which is used for smoothing the input data before Hessian eigen value calculation.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nsndarray \n\nShape index     References  \n1  \nKoenderink, J. J. & van Doorn, A. J., \u201cSurface shape and curvature scales\u201d, Image and Vision Computing, 1992, 10, 557-564. DOI:10.1016/0262-8856(92)90076-F   Examples >>> from skimage.feature import shape_index\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 4\n>>> s = shape_index(square, sigma=0.1)\n>>> s\narray([[ nan,  nan, -0.5,  nan,  nan],\n       [ nan, -0. ,  nan, -0. ,  nan],\n       [-0.5,  nan, -1. ,  nan, -0.5],\n       [ nan, -0. ,  nan, -0. ,  nan],\n       [ nan,  nan, -0.5,  nan,  nan]])\n \n structure_tensor  \nskimage.feature.structure_tensor(image, sigma=1, mode='constant', cval=0, order=None) [source]\n \nCompute structure tensor using sum of squared differences. The (2-dimensional) structure tensor A is defined as: A = [Arr Arc]\n    [Arc Acc]\n which is approximated by the weighted sum of squared differences in a local window around each pixel in the image. This formula can be extended to a larger number of dimensions (see [1]).  Parameters \n \nimagendarray \n\nInput image.  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, which is used as a weighting function for the local summation of squared differences.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.  \norder{\u2018rc\u2019, \u2018xy\u2019}, optional \n\nNOTE: Only applies in 2D. Higher dimensions must always use \u2018rc\u2019 order. This parameter allows for the use of reverse or forward order of the image axes in gradient computation. \u2018rc\u2019 indicates the use of the first axis initially (Arr, Arc, Acc), whilst \u2018xy\u2019 indicates the usage of the last axis initially (Axx, Axy, Ayy).    Returns \n \nA_elemslist of ndarray \n\nUpper-diagonal elements of the structure tensor for each pixel in the input image.      See also  \nstructure_tensor_eigenvalues\n\n  References  \n1  \nhttps://en.wikipedia.org/wiki/Structure_tensor   Examples >>> from skimage.feature import structure_tensor\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 1\n>>> Arr, Arc, Acc = structure_tensor(square, sigma=0.1, order='rc')\n>>> Acc\narray([[0., 0., 0., 0., 0.],\n       [0., 1., 0., 1., 0.],\n       [0., 4., 0., 4., 0.],\n       [0., 1., 0., 1., 0.],\n       [0., 0., 0., 0., 0.]])\n \n structure_tensor_eigenvalues  \nskimage.feature.structure_tensor_eigenvalues(A_elems) [source]\n \nCompute eigenvalues of structure tensor.  Parameters \n \nA_elemslist of ndarray \n\nThe upper-diagonal elements of the structure tensor, as returned by structure_tensor.    Returns \n ndarray\n\nThe eigenvalues of the structure tensor, in decreasing order. The eigenvalues are the leading dimension. That is, the coordinate [i, j, k] corresponds to the ith-largest eigenvalue at position (j, k).      See also  \nstructure_tensor\n\n  Examples >>> from skimage.feature import structure_tensor\n>>> from skimage.feature import structure_tensor_eigenvalues\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 1\n>>> A_elems = structure_tensor(square, sigma=0.1, order='rc')\n>>> structure_tensor_eigenvalues(A_elems)[0]\narray([[0., 0., 0., 0., 0.],\n       [0., 2., 4., 2., 0.],\n       [0., 4., 0., 4., 0.],\n       [0., 2., 4., 2., 0.],\n       [0., 0., 0., 0., 0.]])\n \n structure_tensor_eigvals  \nskimage.feature.structure_tensor_eigvals(Axx, Axy, Ayy) [source]\n \nCompute eigenvalues of structure tensor.  Parameters \n \nAxxndarray \n\nElement of the structure tensor for each pixel in the input image.  \nAxyndarray \n\nElement of the structure tensor for each pixel in the input image.  \nAyyndarray \n\nElement of the structure tensor for each pixel in the input image.    Returns \n \nl1ndarray \n\nLarger eigen value for each input matrix.  \nl2ndarray \n\nSmaller eigen value for each input matrix.     Examples >>> from skimage.feature import structure_tensor, structure_tensor_eigvals\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 1\n>>> Arr, Arc, Acc = structure_tensor(square, sigma=0.1, order='rc')\n>>> structure_tensor_eigvals(Acc, Arc, Arr)[0]\narray([[0., 0., 0., 0., 0.],\n       [0., 2., 4., 2., 0.],\n       [0., 4., 0., 4., 0.],\n       [0., 2., 4., 2., 0.],\n       [0., 0., 0., 0., 0.]])\n \n BRIEF  \nclass skimage.feature.BRIEF(descriptor_size=256, patch_size=49, mode='normal', sigma=1, sample_seed=1) [source]\n \nBases: skimage.feature.util.DescriptorExtractor BRIEF binary descriptor extractor. BRIEF (Binary Robust Independent Elementary Features) is an efficient feature point descriptor. It is highly discriminative even when using relatively few bits and is computed using simple intensity difference tests. For each keypoint, intensity comparisons are carried out for a specifically distributed number N of pixel-pairs resulting in a binary descriptor of length N. For binary descriptors the Hamming distance can be used for feature matching, which leads to lower computational cost in comparison to the L2 norm.  Parameters \n \ndescriptor_sizeint, optional \n\nSize of BRIEF descriptor for each keypoint. Sizes 128, 256 and 512 recommended by the authors. Default is 256.  \npatch_sizeint, optional \n\nLength of the two dimensional square patch sampling region around the keypoints. Default is 49.  \nmode{\u2018normal\u2019, \u2018uniform\u2019}, optional \n\nProbability distribution for sampling location of decision pixel-pairs around keypoints.  \nsample_seedint, optional \n\nSeed for the random sampling of the decision pixel-pairs. From a square window with length patch_size, pixel pairs are sampled using the mode parameter to build the descriptors using intensity comparison. The value of sample_seed must be the same for the images to be matched while building the descriptors.  \nsigmafloat, optional \n\nStandard deviation of the Gaussian low-pass filter applied to the image to alleviate noise sensitivity, which is strongly recommended to obtain discriminative and good descriptors.     Examples >>> from skimage.feature import (corner_harris, corner_peaks, BRIEF,\n...                              match_descriptors)\n>>> import numpy as np\n>>> square1 = np.zeros((8, 8), dtype=np.int32)\n>>> square1[2:6, 2:6] = 1\n>>> square1\narray([[0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)\n>>> square2 = np.zeros((9, 9), dtype=np.int32)\n>>> square2[2:7, 2:7] = 1\n>>> square2\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)\n>>> keypoints1 = corner_peaks(corner_harris(square1), min_distance=1)\n>>> keypoints2 = corner_peaks(corner_harris(square2), min_distance=1)\n>>> extractor = BRIEF(patch_size=5)\n>>> extractor.extract(square1, keypoints1)\n>>> descriptors1 = extractor.descriptors\n>>> extractor.extract(square2, keypoints2)\n>>> descriptors2 = extractor.descriptors\n>>> matches = match_descriptors(descriptors1, descriptors2)\n>>> matches\narray([[0, 0],\n       [1, 1],\n       [2, 2],\n       [3, 3]])\n>>> keypoints1[matches[:, 0]]\narray([[2, 2],\n       [2, 5],\n       [5, 2],\n       [5, 5]])\n>>> keypoints2[matches[:, 1]]\narray([[2, 2],\n       [2, 6],\n       [6, 2],\n       [6, 6]])\n  Attributes \n \ndescriptors(Q, descriptor_size) array of dtype bool \n\n2D ndarray of binary descriptors of size descriptor_size for Q keypoints after filtering out border keypoints with value at an index (i, j) either being True or False representing the outcome of the intensity comparison for i-th keypoint on j-th decision pixel-pair. It is Q == np.sum(mask).  \nmask(N, ) array of dtype bool \n\nMask indicating whether a keypoint has been filtered out (False) or is described in the descriptors array (True).      \n__init__(descriptor_size=256, patch_size=49, mode='normal', sigma=1, sample_seed=1) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nextract(image, keypoints) [source]\n \nExtract BRIEF binary descriptors for given keypoints in image.  Parameters \n \nimage2D array \n\nInput image.  \nkeypoints(N, 2) array \n\nKeypoint coordinates as (row, col).     \n \n CENSURE  \nclass skimage.feature.CENSURE(min_scale=1, max_scale=7, mode='DoB', non_max_threshold=0.15, line_threshold=10) [source]\n \nBases: skimage.feature.util.FeatureDetector CENSURE keypoint detector.  \nmin_scaleint, optional \n\nMinimum scale to extract keypoints from.  \nmax_scaleint, optional \n\nMaximum scale to extract keypoints from. The keypoints will be extracted from all the scales except the first and the last i.e. from the scales in the range [min_scale + 1, max_scale - 1]. The filter sizes for different scales is such that the two adjacent scales comprise of an octave.  \nmode{\u2018DoB\u2019, \u2018Octagon\u2019, \u2018STAR\u2019}, optional \n\nType of bi-level filter used to get the scales of the input image. Possible values are \u2018DoB\u2019, \u2018Octagon\u2019 and \u2018STAR\u2019. The three modes represent the shape of the bi-level filters i.e. box(square), octagon and star respectively. For instance, a bi-level octagon filter consists of a smaller inner octagon and a larger outer octagon with the filter weights being uniformly negative in both the inner octagon while uniformly positive in the difference region. Use STAR and Octagon for better features and DoB for better performance.  \nnon_max_thresholdfloat, optional \n\nThreshold value used to suppress maximas and minimas with a weak magnitude response obtained after Non-Maximal Suppression.  \nline_thresholdfloat, optional \n\nThreshold for rejecting interest points which have ratio of principal curvatures greater than this value.   References  \n1  \nMotilal Agrawal, Kurt Konolige and Morten Rufus Blas \u201cCENSURE: Center Surround Extremas for Realtime Feature Detection and Matching\u201d, https://link.springer.com/chapter/10.1007/978-3-540-88693-8_8 DOI:10.1007/978-3-540-88693-8_8  \n2  \nAdam Schmidt, Marek Kraft, Michal Fularz and Zuzanna Domagala \u201cComparative Assessment of Point Feature Detectors and Descriptors in the Context of Robot Navigation\u201d http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.baztech-268aaf28-0faf-4872-a4df-7e2e61cb364c/c/Schmidt_comparative.pdf DOI:10.1.1.465.1117   Examples >>> from skimage.data import astronaut\n>>> from skimage.color import rgb2gray\n>>> from skimage.feature import CENSURE\n>>> img = rgb2gray(astronaut()[100:300, 100:300])\n>>> censure = CENSURE()\n>>> censure.detect(img)\n>>> censure.keypoints\narray([[  4, 148],\n       [ 12,  73],\n       [ 21, 176],\n       [ 91,  22],\n       [ 93,  56],\n       [ 94,  22],\n       [ 95,  54],\n       [100,  51],\n       [103,  51],\n       [106,  67],\n       [108,  15],\n       [117,  20],\n       [122,  60],\n       [125,  37],\n       [129,  37],\n       [133,  76],\n       [145,  44],\n       [146,  94],\n       [150, 114],\n       [153,  33],\n       [154, 156],\n       [155, 151],\n       [184,  63]])\n>>> censure.scales\narray([2, 6, 6, 2, 4, 3, 2, 3, 2, 6, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 4, 2,\n       2])\n  Attributes \n \nkeypoints(N, 2) array \n\nKeypoint coordinates as (row, col).  \nscales(N, ) array \n\nCorresponding scales.      \n__init__(min_scale=1, max_scale=7, mode='DoB', non_max_threshold=0.15, line_threshold=10) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \ndetect(image) [source]\n \nDetect CENSURE keypoints along with the corresponding scale.  Parameters \n \nimage2D ndarray \n\nInput image.     \n \n Cascade  \nclass skimage.feature.Cascade  \nBases: object Class for cascade of classifiers that is used for object detection. The main idea behind cascade of classifiers is to create classifiers of medium accuracy and ensemble them into one strong classifier instead of just creating a strong one. The second advantage of cascade classifier is that easy examples can be classified only by evaluating some of the classifiers in the cascade, making the process much faster than the process of evaluating a one strong classifier.  Attributes \n \nepscnp.float32_t \n\nAccuracy parameter. Increasing it, makes the classifier detect less false positives but at the same time the false negative score increases.  \nstages_numberPy_ssize_t \n\nAmount of stages in a cascade. Each cascade consists of stumps i.e. trained features.  \nstumps_numberPy_ssize_t \n\nThe overall amount of stumps in all the stages of cascade.  \nfeatures_numberPy_ssize_t \n\nThe overall amount of different features used by cascade. Two stumps can use the same features but has different trained values.  \nwindow_widthPy_ssize_t \n\nThe width of a detection window that is used. Objects smaller than this window can\u2019t be detected.  \nwindow_heightPy_ssize_t \n\nThe height of a detection window.  \nstagesStage* \n\nA link to the c array that stores stages information using Stage struct.  \nfeaturesMBLBP* \n\nLink to the c array that stores MBLBP features using MBLBP struct.  \nLUTscnp.uint32_t* \n\nThe ling to the array with look-up tables that are used by trained MBLBP features (MBLBPStumps) to evaluate a particular region.      \n__init__()  \nInitialize cascade classifier.  Parameters \n \nxml_filefile\u2019s path or file\u2019s object \n\nA file in a OpenCv format from which all the cascade classifier\u2019s parameters are loaded.  \nepscnp.float32_t \n\nAccuracy parameter. Increasing it, makes the classifier detect less false positives but at the same time the false negative score increases.     \n  \ndetect_multi_scale()  \nSearch for the object on multiple scales of input image. The function takes the input image, the scale factor by which the searching window is multiplied on each step, minimum window size and maximum window size that specify the interval for the search windows that are applied to the input image to detect objects.  Parameters \n \nimg2-D or 3-D ndarray \n\nNdarray that represents the input image.  \nscale_factorcnp.float32_t \n\nThe scale by which searching window is multiplied on each step.  \nstep_ratiocnp.float32_t \n\nThe ratio by which the search step in multiplied on each scale of the image. 1 represents the exaustive search and usually is slow. By setting this parameter to higher values the results will be worse but the computation will be much faster. Usually, values in the interval [1, 1.5] give good results.  \nmin_sizetyple (int, int) \n\nMinimum size of the search window.  \nmax_sizetyple (int, int) \n\nMaximum size of the search window.  \nmin_neighbour_numberint \n\nMinimum amount of intersecting detections in order for detection to be approved by the function.  \nintersection_score_thresholdcnp.float32_t \n\nThe minimum value of value of ratio (intersection area) / (small rectangle ratio) in order to merge two detections into one.    Returns \n \noutputlist of dicts \n\nDict have form {\u2018r\u2019: int, \u2018c\u2019: int, \u2018width\u2019: int, \u2018height\u2019: int}, where \u2018r\u2019 represents row position of top left corner of detected window, \u2018c\u2019 - col position, \u2018width\u2019 - width of detected window, \u2018height\u2019 - height of detected window.     \n  \neps \n  \nfeatures_number \n  \nstages_number \n  \nstumps_number \n  \nwindow_height \n  \nwindow_width \n \n ORB  \nclass skimage.feature.ORB(downscale=1.2, n_scales=8, n_keypoints=500, fast_n=9, fast_threshold=0.08, harris_k=0.04) [source]\n \nBases: skimage.feature.util.FeatureDetector, skimage.feature.util.DescriptorExtractor Oriented FAST and rotated BRIEF feature detector and binary descriptor extractor.  Parameters \n \nn_keypointsint, optional \n\nNumber of keypoints to be returned. The function will return the best n_keypoints according to the Harris corner response if more than n_keypoints are detected. If not, then all the detected keypoints are returned.  \nfast_nint, optional \n\nThe n parameter in skimage.feature.corner_fast. Minimum number of consecutive pixels out of 16 pixels on the circle that should all be either brighter or darker w.r.t test-pixel. A point c on the circle is darker w.r.t test pixel p if Ic < Ip - threshold and brighter if Ic > Ip + threshold. Also stands for the n in FAST-n corner detector.  \nfast_thresholdfloat, optional \n\nThe threshold parameter in feature.corner_fast. Threshold used to decide whether the pixels on the circle are brighter, darker or similar w.r.t. the test pixel. Decrease the threshold when more corners are desired and vice-versa.  \nharris_kfloat, optional \n\nThe k parameter in skimage.feature.corner_harris. Sensitivity factor to separate corners from edges, typically in range [0, 0.2]. Small values of k result in detection of sharp corners.  \ndownscalefloat, optional \n\nDownscale factor for the image pyramid. Default value 1.2 is chosen so that there are more dense scales which enable robust scale invariance for a subsequent feature description.  \nn_scalesint, optional \n\nMaximum number of scales from the bottom of the image pyramid to extract the features from.     References  \n1  \nEthan Rublee, Vincent Rabaud, Kurt Konolige and Gary Bradski \u201cORB: An efficient alternative to SIFT and SURF\u201d http://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf   Examples >>> from skimage.feature import ORB, match_descriptors\n>>> img1 = np.zeros((100, 100))\n>>> img2 = np.zeros_like(img1)\n>>> np.random.seed(1)\n>>> square = np.random.rand(20, 20)\n>>> img1[40:60, 40:60] = square\n>>> img2[53:73, 53:73] = square\n>>> detector_extractor1 = ORB(n_keypoints=5)\n>>> detector_extractor2 = ORB(n_keypoints=5)\n>>> detector_extractor1.detect_and_extract(img1)\n>>> detector_extractor2.detect_and_extract(img2)\n>>> matches = match_descriptors(detector_extractor1.descriptors,\n...                             detector_extractor2.descriptors)\n>>> matches\narray([[0, 0],\n       [1, 1],\n       [2, 2],\n       [3, 3],\n       [4, 4]])\n>>> detector_extractor1.keypoints[matches[:, 0]]\narray([[42., 40.],\n       [47., 58.],\n       [44., 40.],\n       [59., 42.],\n       [45., 44.]])\n>>> detector_extractor2.keypoints[matches[:, 1]]\narray([[55., 53.],\n       [60., 71.],\n       [57., 53.],\n       [72., 55.],\n       [58., 57.]])\n  Attributes \n \nkeypoints(N, 2) array \n\nKeypoint coordinates as (row, col).  \nscales(N, ) array \n\nCorresponding scales.  \norientations(N, ) array \n\nCorresponding orientations in radians.  \nresponses(N, ) array \n\nCorresponding Harris corner responses.  \ndescriptors(Q, descriptor_size) array of dtype bool \n\n2D array of binary descriptors of size descriptor_size for Q keypoints after filtering out border keypoints with value at an index (i, j) either being True or False representing the outcome of the intensity comparison for i-th keypoint on j-th decision pixel-pair. It is Q == np.sum(mask).      \n__init__(downscale=1.2, n_scales=8, n_keypoints=500, fast_n=9, fast_threshold=0.08, harris_k=0.04) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \ndetect(image) [source]\n \nDetect oriented FAST keypoints along with the corresponding scale.  Parameters \n \nimage2D array \n\nInput image.     \n  \ndetect_and_extract(image) [source]\n \nDetect oriented FAST keypoints and extract rBRIEF descriptors. Note that this is faster than first calling detect and then extract.  Parameters \n \nimage2D array \n\nInput image.     \n  \nextract(image, keypoints, scales, orientations) [source]\n \nExtract rBRIEF binary descriptors for given keypoints in image. Note that the keypoints must be extracted using the same downscale and n_scales parameters. Additionally, if you want to extract both keypoints and descriptors you should use the faster detect_and_extract.  Parameters \n \nimage2D array \n\nInput image.  \nkeypoints(N, 2) array \n\nKeypoint coordinates as (row, col).  \nscales(N, ) array \n\nCorresponding scales.  \norientations(N, ) array \n\nCorresponding orientations in radians.     \n \n\n"}, {"name": "feature.blob_dog()", "path": "api/skimage.feature#skimage.feature.blob_dog", "type": "feature", "text": " \nskimage.feature.blob_dog(image, min_sigma=1, max_sigma=50, sigma_ratio=1.6, threshold=2.0, overlap=0.5, *, exclude_border=False) [source]\n \nFinds blobs in the given grayscale image. Blobs are found using the Difference of Gaussian (DoG) method [1]. For each blob found, the method returns its coordinates and the standard deviation of the Gaussian kernel that detected the blob.  Parameters \n \nimage2D or 3D ndarray \n\nInput grayscale image, blobs are assumed to be light on dark background (white on black).  \nmin_sigmascalar or sequence of scalars, optional \n\nThe minimum standard deviation for Gaussian kernel. Keep this low to detect smaller blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.  \nmax_sigmascalar or sequence of scalars, optional \n\nThe maximum standard deviation for Gaussian kernel. Keep this high to detect larger blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.  \nsigma_ratiofloat, optional \n\nThe ratio between the standard deviation of Gaussian Kernels used for computing the Difference of Gaussians  \nthresholdfloat, optional. \n\nThe absolute lower bound for scale space maxima. Local maxima smaller than thresh are ignored. Reduce this to detect blobs with less intensities.  \noverlapfloat, optional \n\nA value between 0 and 1. If the area of two blobs overlaps by a fraction greater than threshold, the smaller blob is eliminated.  \nexclude_bordertuple of ints, int, or False, optional \n\nIf tuple of ints, the length of the tuple must match the input array\u2019s dimensionality. Each element of the tuple will exclude peaks from within exclude_border-pixels of the border of the image along that dimension. If nonzero int, exclude_border excludes peaks from within exclude_border-pixels of the border of the image. If zero or False, peaks are identified regardless of their distance from the border.    Returns \n \nA(n, image.ndim + sigma) ndarray \n\nA 2d array with each row representing 2 coordinate values for a 2D image, and 3 coordinate values for a 3D image, plus the sigma(s) used. When a single sigma is passed, outputs are: (r, c, sigma) or (p, r, c, sigma) where (r, c) or (p, r, c) are coordinates of the blob and sigma is the standard deviation of the Gaussian kernel which detected the blob. When an anisotropic gaussian is used (sigmas per dimension), the detected sigma is returned for each dimension.      See also  \nskimage.filters.difference_of_gaussians\n\n  Notes The radius of each blob is approximately \\(\\sqrt{2}\\sigma\\) for a 2-D image and \\(\\sqrt{3}\\sigma\\) for a 3-D image. References  \n1  \nhttps://en.wikipedia.org/wiki/Blob_detection#The_difference_of_Gaussians_approach   Examples >>> from skimage import data, feature\n>>> feature.blob_dog(data.coins(), threshold=.5, max_sigma=40)\narray([[120.      , 272.      ,  16.777216],\n       [193.      , 213.      ,  16.777216],\n       [263.      , 245.      ,  16.777216],\n       [185.      , 347.      ,  16.777216],\n       [128.      , 154.      ,  10.48576 ],\n       [198.      , 155.      ,  10.48576 ],\n       [124.      , 337.      ,  10.48576 ],\n       [ 45.      , 336.      ,  16.777216],\n       [195.      , 102.      ,  16.777216],\n       [125.      ,  45.      ,  16.777216],\n       [261.      , 173.      ,  16.777216],\n       [194.      , 277.      ,  16.777216],\n       [127.      , 102.      ,  10.48576 ],\n       [125.      , 208.      ,  10.48576 ],\n       [267.      , 115.      ,  10.48576 ],\n       [263.      , 302.      ,  16.777216],\n       [196.      ,  43.      ,  10.48576 ],\n       [260.      ,  46.      ,  16.777216],\n       [267.      , 359.      ,  16.777216],\n       [ 54.      , 276.      ,  10.48576 ],\n       [ 58.      , 100.      ,  10.48576 ],\n       [ 52.      , 155.      ,  16.777216],\n       [ 52.      , 216.      ,  16.777216],\n       [ 54.      ,  42.      ,  16.777216]])\n \n"}, {"name": "feature.blob_doh()", "path": "api/skimage.feature#skimage.feature.blob_doh", "type": "feature", "text": " \nskimage.feature.blob_doh(image, min_sigma=1, max_sigma=30, num_sigma=10, threshold=0.01, overlap=0.5, log_scale=False) [source]\n \nFinds blobs in the given grayscale image. Blobs are found using the Determinant of Hessian method [1]. For each blob found, the method returns its coordinates and the standard deviation of the Gaussian Kernel used for the Hessian matrix whose determinant detected the blob. Determinant of Hessians is approximated using [2].  Parameters \n \nimage2D ndarray \n\nInput grayscale image.Blobs can either be light on dark or vice versa.  \nmin_sigmafloat, optional \n\nThe minimum standard deviation for Gaussian Kernel used to compute Hessian matrix. Keep this low to detect smaller blobs.  \nmax_sigmafloat, optional \n\nThe maximum standard deviation for Gaussian Kernel used to compute Hessian matrix. Keep this high to detect larger blobs.  \nnum_sigmaint, optional \n\nThe number of intermediate values of standard deviations to consider between min_sigma and max_sigma.  \nthresholdfloat, optional. \n\nThe absolute lower bound for scale space maxima. Local maxima smaller than thresh are ignored. Reduce this to detect less prominent blobs.  \noverlapfloat, optional \n\nA value between 0 and 1. If the area of two blobs overlaps by a fraction greater than threshold, the smaller blob is eliminated.  \nlog_scalebool, optional \n\nIf set intermediate values of standard deviations are interpolated using a logarithmic scale to the base 10. If not, linear interpolation is used.    Returns \n \nA(n, 3) ndarray \n\nA 2d array with each row representing 3 values, (y,x,sigma) where (y,x) are coordinates of the blob and sigma is the standard deviation of the Gaussian kernel of the Hessian Matrix whose determinant detected the blob.     Notes The radius of each blob is approximately sigma. Computation of Determinant of Hessians is independent of the standard deviation. Therefore detecting larger blobs won\u2019t take more time. In methods line blob_dog() and blob_log() the computation of Gaussians for larger sigma takes more time. The downside is that this method can\u2019t be used for detecting blobs of radius less than 3px due to the box filters used in the approximation of Hessian Determinant. References  \n1  \nhttps://en.wikipedia.org/wiki/Blob_detection#The_determinant_of_the_Hessian  \n2  \nHerbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool, \u201cSURF: Speeded Up Robust Features\u201d ftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf   Examples >>> from skimage import data, feature\n>>> img = data.coins()\n>>> feature.blob_doh(img)\narray([[197.        , 153.        ,  20.33333333],\n       [124.        , 336.        ,  20.33333333],\n       [126.        , 153.        ,  20.33333333],\n       [195.        , 100.        ,  23.55555556],\n       [192.        , 212.        ,  23.55555556],\n       [121.        , 271.        ,  30.        ],\n       [126.        , 101.        ,  20.33333333],\n       [193.        , 275.        ,  23.55555556],\n       [123.        , 205.        ,  20.33333333],\n       [270.        , 363.        ,  30.        ],\n       [265.        , 113.        ,  23.55555556],\n       [262.        , 243.        ,  23.55555556],\n       [185.        , 348.        ,  30.        ],\n       [156.        , 302.        ,  30.        ],\n       [123.        ,  44.        ,  23.55555556],\n       [260.        , 173.        ,  30.        ],\n       [197.        ,  44.        ,  20.33333333]])\n \n"}, {"name": "feature.blob_log()", "path": "api/skimage.feature#skimage.feature.blob_log", "type": "feature", "text": " \nskimage.feature.blob_log(image, min_sigma=1, max_sigma=50, num_sigma=10, threshold=0.2, overlap=0.5, log_scale=False, *, exclude_border=False) [source]\n \nFinds blobs in the given grayscale image. Blobs are found using the Laplacian of Gaussian (LoG) method [1]. For each blob found, the method returns its coordinates and the standard deviation of the Gaussian kernel that detected the blob.  Parameters \n \nimage2D or 3D ndarray \n\nInput grayscale image, blobs are assumed to be light on dark background (white on black).  \nmin_sigmascalar or sequence of scalars, optional \n\nthe minimum standard deviation for Gaussian kernel. Keep this low to detect smaller blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.  \nmax_sigmascalar or sequence of scalars, optional \n\nThe maximum standard deviation for Gaussian kernel. Keep this high to detect larger blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.  \nnum_sigmaint, optional \n\nThe number of intermediate values of standard deviations to consider between min_sigma and max_sigma.  \nthresholdfloat, optional. \n\nThe absolute lower bound for scale space maxima. Local maxima smaller than thresh are ignored. Reduce this to detect blobs with less intensities.  \noverlapfloat, optional \n\nA value between 0 and 1. If the area of two blobs overlaps by a fraction greater than threshold, the smaller blob is eliminated.  \nlog_scalebool, optional \n\nIf set intermediate values of standard deviations are interpolated using a logarithmic scale to the base 10. If not, linear interpolation is used.  \nexclude_bordertuple of ints, int, or False, optional \n\nIf tuple of ints, the length of the tuple must match the input array\u2019s dimensionality. Each element of the tuple will exclude peaks from within exclude_border-pixels of the border of the image along that dimension. If nonzero int, exclude_border excludes peaks from within exclude_border-pixels of the border of the image. If zero or False, peaks are identified regardless of their distance from the border.    Returns \n \nA(n, image.ndim + sigma) ndarray \n\nA 2d array with each row representing 2 coordinate values for a 2D image, and 3 coordinate values for a 3D image, plus the sigma(s) used. When a single sigma is passed, outputs are: (r, c, sigma) or (p, r, c, sigma) where (r, c) or (p, r, c) are coordinates of the blob and sigma is the standard deviation of the Gaussian kernel which detected the blob. When an anisotropic gaussian is used (sigmas per dimension), the detected sigma is returned for each dimension.     Notes The radius of each blob is approximately \\(\\sqrt{2}\\sigma\\) for a 2-D image and \\(\\sqrt{3}\\sigma\\) for a 3-D image. References  \n1  \nhttps://en.wikipedia.org/wiki/Blob_detection#The_Laplacian_of_Gaussian   Examples >>> from skimage import data, feature, exposure\n>>> img = data.coins()\n>>> img = exposure.equalize_hist(img)  # improves detection\n>>> feature.blob_log(img, threshold = .3)\narray([[124.        , 336.        ,  11.88888889],\n       [198.        , 155.        ,  11.88888889],\n       [194.        , 213.        ,  17.33333333],\n       [121.        , 272.        ,  17.33333333],\n       [263.        , 244.        ,  17.33333333],\n       [194.        , 276.        ,  17.33333333],\n       [266.        , 115.        ,  11.88888889],\n       [128.        , 154.        ,  11.88888889],\n       [260.        , 174.        ,  17.33333333],\n       [198.        , 103.        ,  11.88888889],\n       [126.        , 208.        ,  11.88888889],\n       [127.        , 102.        ,  11.88888889],\n       [263.        , 302.        ,  17.33333333],\n       [197.        ,  44.        ,  11.88888889],\n       [185.        , 344.        ,  17.33333333],\n       [126.        ,  46.        ,  11.88888889],\n       [113.        , 323.        ,   1.        ]])\n \n"}, {"name": "feature.BRIEF", "path": "api/skimage.feature#skimage.feature.BRIEF", "type": "feature", "text": " \nclass skimage.feature.BRIEF(descriptor_size=256, patch_size=49, mode='normal', sigma=1, sample_seed=1) [source]\n \nBases: skimage.feature.util.DescriptorExtractor BRIEF binary descriptor extractor. BRIEF (Binary Robust Independent Elementary Features) is an efficient feature point descriptor. It is highly discriminative even when using relatively few bits and is computed using simple intensity difference tests. For each keypoint, intensity comparisons are carried out for a specifically distributed number N of pixel-pairs resulting in a binary descriptor of length N. For binary descriptors the Hamming distance can be used for feature matching, which leads to lower computational cost in comparison to the L2 norm.  Parameters \n \ndescriptor_sizeint, optional \n\nSize of BRIEF descriptor for each keypoint. Sizes 128, 256 and 512 recommended by the authors. Default is 256.  \npatch_sizeint, optional \n\nLength of the two dimensional square patch sampling region around the keypoints. Default is 49.  \nmode{\u2018normal\u2019, \u2018uniform\u2019}, optional \n\nProbability distribution for sampling location of decision pixel-pairs around keypoints.  \nsample_seedint, optional \n\nSeed for the random sampling of the decision pixel-pairs. From a square window with length patch_size, pixel pairs are sampled using the mode parameter to build the descriptors using intensity comparison. The value of sample_seed must be the same for the images to be matched while building the descriptors.  \nsigmafloat, optional \n\nStandard deviation of the Gaussian low-pass filter applied to the image to alleviate noise sensitivity, which is strongly recommended to obtain discriminative and good descriptors.     Examples >>> from skimage.feature import (corner_harris, corner_peaks, BRIEF,\n...                              match_descriptors)\n>>> import numpy as np\n>>> square1 = np.zeros((8, 8), dtype=np.int32)\n>>> square1[2:6, 2:6] = 1\n>>> square1\narray([[0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)\n>>> square2 = np.zeros((9, 9), dtype=np.int32)\n>>> square2[2:7, 2:7] = 1\n>>> square2\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)\n>>> keypoints1 = corner_peaks(corner_harris(square1), min_distance=1)\n>>> keypoints2 = corner_peaks(corner_harris(square2), min_distance=1)\n>>> extractor = BRIEF(patch_size=5)\n>>> extractor.extract(square1, keypoints1)\n>>> descriptors1 = extractor.descriptors\n>>> extractor.extract(square2, keypoints2)\n>>> descriptors2 = extractor.descriptors\n>>> matches = match_descriptors(descriptors1, descriptors2)\n>>> matches\narray([[0, 0],\n       [1, 1],\n       [2, 2],\n       [3, 3]])\n>>> keypoints1[matches[:, 0]]\narray([[2, 2],\n       [2, 5],\n       [5, 2],\n       [5, 5]])\n>>> keypoints2[matches[:, 1]]\narray([[2, 2],\n       [2, 6],\n       [6, 2],\n       [6, 6]])\n  Attributes \n \ndescriptors(Q, descriptor_size) array of dtype bool \n\n2D ndarray of binary descriptors of size descriptor_size for Q keypoints after filtering out border keypoints with value at an index (i, j) either being True or False representing the outcome of the intensity comparison for i-th keypoint on j-th decision pixel-pair. It is Q == np.sum(mask).  \nmask(N, ) array of dtype bool \n\nMask indicating whether a keypoint has been filtered out (False) or is described in the descriptors array (True).      \n__init__(descriptor_size=256, patch_size=49, mode='normal', sigma=1, sample_seed=1) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nextract(image, keypoints) [source]\n \nExtract BRIEF binary descriptors for given keypoints in image.  Parameters \n \nimage2D array \n\nInput image.  \nkeypoints(N, 2) array \n\nKeypoint coordinates as (row, col).     \n \n"}, {"name": "feature.BRIEF.extract()", "path": "api/skimage.feature#skimage.feature.BRIEF.extract", "type": "feature", "text": " \nextract(image, keypoints) [source]\n \nExtract BRIEF binary descriptors for given keypoints in image.  Parameters \n \nimage2D array \n\nInput image.  \nkeypoints(N, 2) array \n\nKeypoint coordinates as (row, col).     \n"}, {"name": "feature.BRIEF.__init__()", "path": "api/skimage.feature#skimage.feature.BRIEF.__init__", "type": "feature", "text": " \n__init__(descriptor_size=256, patch_size=49, mode='normal', sigma=1, sample_seed=1) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "feature.canny()", "path": "api/skimage.feature#skimage.feature.canny", "type": "feature", "text": " \nskimage.feature.canny(image, sigma=1.0, low_threshold=None, high_threshold=None, mask=None, use_quantiles=False) [source]\n \nEdge filter an image using the Canny algorithm.  Parameters \n \nimage2D array \n\nGrayscale input image to detect edges on; can be of any dtype.  \nsigmafloat, optional \n\nStandard deviation of the Gaussian filter.  \nlow_thresholdfloat, optional \n\nLower bound for hysteresis thresholding (linking edges). If None, low_threshold is set to 10% of dtype\u2019s max.  \nhigh_thresholdfloat, optional \n\nUpper bound for hysteresis thresholding (linking edges). If None, high_threshold is set to 20% of dtype\u2019s max.  \nmaskarray, dtype=bool, optional \n\nMask to limit the application of Canny to a certain area.  \nuse_quantilesbool, optional \n\nIf True then treat low_threshold and high_threshold as quantiles of the edge magnitude image, rather than absolute edge magnitude values. If True then the thresholds must be in the range [0, 1].    Returns \n \noutput2D array (image) \n\nThe binary edge map.      See also  \nskimage.sobel \n  Notes The steps of the algorithm are as follows:  Smooth the image using a Gaussian with sigma width. Apply the horizontal and vertical Sobel operators to get the gradients within the image. The edge strength is the norm of the gradient. Thin potential edges to 1-pixel wide curves. First, find the normal to the edge at each point. This is done by looking at the signs and the relative magnitude of the X-Sobel and Y-Sobel to sort the points into 4 categories: horizontal, vertical, diagonal and antidiagonal. Then look in the normal and reverse directions to see if the values in either of those directions are greater than the point in question. Use interpolation to get a mix of points instead of picking the one that\u2019s the closest to the normal. Perform a hysteresis thresholding: first label all points above the high threshold as edges. Then recursively label any point above the low threshold that is 8-connected to a labeled point as an edge.  References  \n1  \nCanny, J., A Computational Approach To Edge Detection, IEEE Trans. Pattern Analysis and Machine Intelligence, 8:679-714, 1986 DOI:10.1109/TPAMI.1986.4767851  \n2  \nWilliam Green\u2019s Canny tutorial https://en.wikipedia.org/wiki/Canny_edge_detector   Examples >>> from skimage import feature\n>>> # Generate noisy image of a square\n>>> im = np.zeros((256, 256))\n>>> im[64:-64, 64:-64] = 1\n>>> im += 0.2 * np.random.rand(*im.shape)\n>>> # First trial with the Canny filter, with the default smoothing\n>>> edges1 = feature.canny(im)\n>>> # Increase the smoothing for better results\n>>> edges2 = feature.canny(im, sigma=3)\n \n"}, {"name": "feature.Cascade", "path": "api/skimage.feature#skimage.feature.Cascade", "type": "feature", "text": " \nclass skimage.feature.Cascade  \nBases: object Class for cascade of classifiers that is used for object detection. The main idea behind cascade of classifiers is to create classifiers of medium accuracy and ensemble them into one strong classifier instead of just creating a strong one. The second advantage of cascade classifier is that easy examples can be classified only by evaluating some of the classifiers in the cascade, making the process much faster than the process of evaluating a one strong classifier.  Attributes \n \nepscnp.float32_t \n\nAccuracy parameter. Increasing it, makes the classifier detect less false positives but at the same time the false negative score increases.  \nstages_numberPy_ssize_t \n\nAmount of stages in a cascade. Each cascade consists of stumps i.e. trained features.  \nstumps_numberPy_ssize_t \n\nThe overall amount of stumps in all the stages of cascade.  \nfeatures_numberPy_ssize_t \n\nThe overall amount of different features used by cascade. Two stumps can use the same features but has different trained values.  \nwindow_widthPy_ssize_t \n\nThe width of a detection window that is used. Objects smaller than this window can\u2019t be detected.  \nwindow_heightPy_ssize_t \n\nThe height of a detection window.  \nstagesStage* \n\nA link to the c array that stores stages information using Stage struct.  \nfeaturesMBLBP* \n\nLink to the c array that stores MBLBP features using MBLBP struct.  \nLUTscnp.uint32_t* \n\nThe ling to the array with look-up tables that are used by trained MBLBP features (MBLBPStumps) to evaluate a particular region.      \n__init__()  \nInitialize cascade classifier.  Parameters \n \nxml_filefile\u2019s path or file\u2019s object \n\nA file in a OpenCv format from which all the cascade classifier\u2019s parameters are loaded.  \nepscnp.float32_t \n\nAccuracy parameter. Increasing it, makes the classifier detect less false positives but at the same time the false negative score increases.     \n  \ndetect_multi_scale()  \nSearch for the object on multiple scales of input image. The function takes the input image, the scale factor by which the searching window is multiplied on each step, minimum window size and maximum window size that specify the interval for the search windows that are applied to the input image to detect objects.  Parameters \n \nimg2-D or 3-D ndarray \n\nNdarray that represents the input image.  \nscale_factorcnp.float32_t \n\nThe scale by which searching window is multiplied on each step.  \nstep_ratiocnp.float32_t \n\nThe ratio by which the search step in multiplied on each scale of the image. 1 represents the exaustive search and usually is slow. By setting this parameter to higher values the results will be worse but the computation will be much faster. Usually, values in the interval [1, 1.5] give good results.  \nmin_sizetyple (int, int) \n\nMinimum size of the search window.  \nmax_sizetyple (int, int) \n\nMaximum size of the search window.  \nmin_neighbour_numberint \n\nMinimum amount of intersecting detections in order for detection to be approved by the function.  \nintersection_score_thresholdcnp.float32_t \n\nThe minimum value of value of ratio (intersection area) / (small rectangle ratio) in order to merge two detections into one.    Returns \n \noutputlist of dicts \n\nDict have form {\u2018r\u2019: int, \u2018c\u2019: int, \u2018width\u2019: int, \u2018height\u2019: int}, where \u2018r\u2019 represents row position of top left corner of detected window, \u2018c\u2019 - col position, \u2018width\u2019 - width of detected window, \u2018height\u2019 - height of detected window.     \n  \neps \n  \nfeatures_number \n  \nstages_number \n  \nstumps_number \n  \nwindow_height \n  \nwindow_width \n \n"}, {"name": "feature.Cascade.detect_multi_scale()", "path": "api/skimage.feature#skimage.feature.Cascade.detect_multi_scale", "type": "feature", "text": " \ndetect_multi_scale()  \nSearch for the object on multiple scales of input image. The function takes the input image, the scale factor by which the searching window is multiplied on each step, minimum window size and maximum window size that specify the interval for the search windows that are applied to the input image to detect objects.  Parameters \n \nimg2-D or 3-D ndarray \n\nNdarray that represents the input image.  \nscale_factorcnp.float32_t \n\nThe scale by which searching window is multiplied on each step.  \nstep_ratiocnp.float32_t \n\nThe ratio by which the search step in multiplied on each scale of the image. 1 represents the exaustive search and usually is slow. By setting this parameter to higher values the results will be worse but the computation will be much faster. Usually, values in the interval [1, 1.5] give good results.  \nmin_sizetyple (int, int) \n\nMinimum size of the search window.  \nmax_sizetyple (int, int) \n\nMaximum size of the search window.  \nmin_neighbour_numberint \n\nMinimum amount of intersecting detections in order for detection to be approved by the function.  \nintersection_score_thresholdcnp.float32_t \n\nThe minimum value of value of ratio (intersection area) / (small rectangle ratio) in order to merge two detections into one.    Returns \n \noutputlist of dicts \n\nDict have form {\u2018r\u2019: int, \u2018c\u2019: int, \u2018width\u2019: int, \u2018height\u2019: int}, where \u2018r\u2019 represents row position of top left corner of detected window, \u2018c\u2019 - col position, \u2018width\u2019 - width of detected window, \u2018height\u2019 - height of detected window.     \n"}, {"name": "feature.Cascade.eps", "path": "api/skimage.feature#skimage.feature.Cascade.eps", "type": "feature", "text": " \neps \n"}, {"name": "feature.Cascade.features_number", "path": "api/skimage.feature#skimage.feature.Cascade.features_number", "type": "feature", "text": " \nfeatures_number \n"}, {"name": "feature.Cascade.stages_number", "path": "api/skimage.feature#skimage.feature.Cascade.stages_number", "type": "feature", "text": " \nstages_number \n"}, {"name": "feature.Cascade.stumps_number", "path": "api/skimage.feature#skimage.feature.Cascade.stumps_number", "type": "feature", "text": " \nstumps_number \n"}, {"name": "feature.Cascade.window_height", "path": "api/skimage.feature#skimage.feature.Cascade.window_height", "type": "feature", "text": " \nwindow_height \n"}, {"name": "feature.Cascade.window_width", "path": "api/skimage.feature#skimage.feature.Cascade.window_width", "type": "feature", "text": " \nwindow_width \n"}, {"name": "feature.Cascade.__init__()", "path": "api/skimage.feature#skimage.feature.Cascade.__init__", "type": "feature", "text": " \n__init__()  \nInitialize cascade classifier.  Parameters \n \nxml_filefile\u2019s path or file\u2019s object \n\nA file in a OpenCv format from which all the cascade classifier\u2019s parameters are loaded.  \nepscnp.float32_t \n\nAccuracy parameter. Increasing it, makes the classifier detect less false positives but at the same time the false negative score increases.     \n"}, {"name": "feature.CENSURE", "path": "api/skimage.feature#skimage.feature.CENSURE", "type": "feature", "text": " \nclass skimage.feature.CENSURE(min_scale=1, max_scale=7, mode='DoB', non_max_threshold=0.15, line_threshold=10) [source]\n \nBases: skimage.feature.util.FeatureDetector CENSURE keypoint detector.  \nmin_scaleint, optional \n\nMinimum scale to extract keypoints from.  \nmax_scaleint, optional \n\nMaximum scale to extract keypoints from. The keypoints will be extracted from all the scales except the first and the last i.e. from the scales in the range [min_scale + 1, max_scale - 1]. The filter sizes for different scales is such that the two adjacent scales comprise of an octave.  \nmode{\u2018DoB\u2019, \u2018Octagon\u2019, \u2018STAR\u2019}, optional \n\nType of bi-level filter used to get the scales of the input image. Possible values are \u2018DoB\u2019, \u2018Octagon\u2019 and \u2018STAR\u2019. The three modes represent the shape of the bi-level filters i.e. box(square), octagon and star respectively. For instance, a bi-level octagon filter consists of a smaller inner octagon and a larger outer octagon with the filter weights being uniformly negative in both the inner octagon while uniformly positive in the difference region. Use STAR and Octagon for better features and DoB for better performance.  \nnon_max_thresholdfloat, optional \n\nThreshold value used to suppress maximas and minimas with a weak magnitude response obtained after Non-Maximal Suppression.  \nline_thresholdfloat, optional \n\nThreshold for rejecting interest points which have ratio of principal curvatures greater than this value.   References  \n1  \nMotilal Agrawal, Kurt Konolige and Morten Rufus Blas \u201cCENSURE: Center Surround Extremas for Realtime Feature Detection and Matching\u201d, https://link.springer.com/chapter/10.1007/978-3-540-88693-8_8 DOI:10.1007/978-3-540-88693-8_8  \n2  \nAdam Schmidt, Marek Kraft, Michal Fularz and Zuzanna Domagala \u201cComparative Assessment of Point Feature Detectors and Descriptors in the Context of Robot Navigation\u201d http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.baztech-268aaf28-0faf-4872-a4df-7e2e61cb364c/c/Schmidt_comparative.pdf DOI:10.1.1.465.1117   Examples >>> from skimage.data import astronaut\n>>> from skimage.color import rgb2gray\n>>> from skimage.feature import CENSURE\n>>> img = rgb2gray(astronaut()[100:300, 100:300])\n>>> censure = CENSURE()\n>>> censure.detect(img)\n>>> censure.keypoints\narray([[  4, 148],\n       [ 12,  73],\n       [ 21, 176],\n       [ 91,  22],\n       [ 93,  56],\n       [ 94,  22],\n       [ 95,  54],\n       [100,  51],\n       [103,  51],\n       [106,  67],\n       [108,  15],\n       [117,  20],\n       [122,  60],\n       [125,  37],\n       [129,  37],\n       [133,  76],\n       [145,  44],\n       [146,  94],\n       [150, 114],\n       [153,  33],\n       [154, 156],\n       [155, 151],\n       [184,  63]])\n>>> censure.scales\narray([2, 6, 6, 2, 4, 3, 2, 3, 2, 6, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 4, 2,\n       2])\n  Attributes \n \nkeypoints(N, 2) array \n\nKeypoint coordinates as (row, col).  \nscales(N, ) array \n\nCorresponding scales.      \n__init__(min_scale=1, max_scale=7, mode='DoB', non_max_threshold=0.15, line_threshold=10) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \ndetect(image) [source]\n \nDetect CENSURE keypoints along with the corresponding scale.  Parameters \n \nimage2D ndarray \n\nInput image.     \n \n"}, {"name": "feature.CENSURE.detect()", "path": "api/skimage.feature#skimage.feature.CENSURE.detect", "type": "feature", "text": " \ndetect(image) [source]\n \nDetect CENSURE keypoints along with the corresponding scale.  Parameters \n \nimage2D ndarray \n\nInput image.     \n"}, {"name": "feature.CENSURE.__init__()", "path": "api/skimage.feature#skimage.feature.CENSURE.__init__", "type": "feature", "text": " \n__init__(min_scale=1, max_scale=7, mode='DoB', non_max_threshold=0.15, line_threshold=10) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "feature.corner_fast()", "path": "api/skimage.feature#skimage.feature.corner_fast", "type": "feature", "text": " \nskimage.feature.corner_fast(image, n=12, threshold=0.15) [source]\n \nExtract FAST corners for a given image.  Parameters \n \nimage2D ndarray \n\nInput image.  \nnint, optional \n\nMinimum number of consecutive pixels out of 16 pixels on the circle that should all be either brighter or darker w.r.t testpixel. A point c on the circle is darker w.r.t test pixel p if Ic < Ip - threshold and brighter if Ic > Ip + threshold. Also stands for the n in FAST-n corner detector.  \nthresholdfloat, optional \n\nThreshold used in deciding whether the pixels on the circle are brighter, darker or similar w.r.t. the test pixel. Decrease the threshold when more corners are desired and vice-versa.    Returns \n \nresponsendarray \n\nFAST corner response image.     References  \n1  \nRosten, E., & Drummond, T. (2006, May). Machine learning for high-speed corner detection. In European conference on computer vision (pp. 430-443). Springer, Berlin, Heidelberg. DOI:10.1007/11744023_34 http://www.edwardrosten.com/work/rosten_2006_machine.pdf  \n2  \nWikipedia, \u201cFeatures from accelerated segment test\u201d, https://en.wikipedia.org/wiki/Features_from_accelerated_segment_test   Examples >>> from skimage.feature import corner_fast, corner_peaks\n>>> square = np.zeros((12, 12))\n>>> square[3:9, 3:9] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> corner_peaks(corner_fast(square, 9), min_distance=1)\narray([[3, 3],\n       [3, 8],\n       [8, 3],\n       [8, 8]])\n \n"}, {"name": "feature.corner_foerstner()", "path": "api/skimage.feature#skimage.feature.corner_foerstner", "type": "feature", "text": " \nskimage.feature.corner_foerstner(image, sigma=1) [source]\n \nCompute Foerstner corner measure response image. This corner detector uses information from the auto-correlation matrix A: A = [(imx**2)   (imx*imy)] = [Axx Axy]\n    [(imx*imy)   (imy**2)]   [Axy Ayy]\n Where imx and imy are first derivatives, averaged with a gaussian filter. The corner measure is then defined as: w = det(A) / trace(A)           (size of error ellipse)\nq = 4 * det(A) / trace(A)**2    (roundness of error ellipse)\n  Parameters \n \nimagendarray \n\nInput image.  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.    Returns \n \nwndarray \n\nError ellipse sizes.  \nqndarray \n\nRoundness of error ellipse.     References  \n1  \nF\u00f6rstner, W., & G\u00fclch, E. (1987, June). A fast operator for detection and precise location of distinct points, corners and centres of circular features. In Proc. ISPRS intercommission conference on fast processing of photogrammetric data (pp. 281-305). https://cseweb.ucsd.edu/classes/sp02/cse252/foerstner/foerstner.pdf  \n2  \nhttps://en.wikipedia.org/wiki/Corner_detection   Examples >>> from skimage.feature import corner_foerstner, corner_peaks\n>>> square = np.zeros([10, 10])\n>>> square[2:8, 2:8] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> w, q = corner_foerstner(square)\n>>> accuracy_thresh = 0.5\n>>> roundness_thresh = 0.3\n>>> foerstner = (q > roundness_thresh) * (w > accuracy_thresh) * w\n>>> corner_peaks(foerstner, min_distance=1)\narray([[2, 2],\n       [2, 7],\n       [7, 2],\n       [7, 7]])\n \n"}, {"name": "feature.corner_harris()", "path": "api/skimage.feature#skimage.feature.corner_harris", "type": "feature", "text": " \nskimage.feature.corner_harris(image, method='k', k=0.05, eps=1e-06, sigma=1) [source]\n \nCompute Harris corner measure response image. This corner detector uses information from the auto-correlation matrix A: A = [(imx**2)   (imx*imy)] = [Axx Axy]\n    [(imx*imy)   (imy**2)]   [Axy Ayy]\n Where imx and imy are first derivatives, averaged with a gaussian filter. The corner measure is then defined as: det(A) - k * trace(A)**2\n or: 2 * det(A) / (trace(A) + eps)\n  Parameters \n \nimagendarray \n\nInput image.  \nmethod{\u2018k\u2019, \u2018eps\u2019}, optional \n\nMethod to compute the response image from the auto-correlation matrix.  \nkfloat, optional \n\nSensitivity factor to separate corners from edges, typically in range [0, 0.2]. Small values of k result in detection of sharp corners.  \nepsfloat, optional \n\nNormalisation factor (Noble\u2019s corner measure).  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.    Returns \n \nresponsendarray \n\nHarris response image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Corner_detection   Examples >>> from skimage.feature import corner_harris, corner_peaks\n>>> square = np.zeros([10, 10])\n>>> square[2:8, 2:8] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> corner_peaks(corner_harris(square), min_distance=1)\narray([[2, 2],\n       [2, 7],\n       [7, 2],\n       [7, 7]])\n \n"}, {"name": "feature.corner_kitchen_rosenfeld()", "path": "api/skimage.feature#skimage.feature.corner_kitchen_rosenfeld", "type": "feature", "text": " \nskimage.feature.corner_kitchen_rosenfeld(image, mode='constant', cval=0) [source]\n \nCompute Kitchen and Rosenfeld corner measure response image. The corner measure is calculated as follows: (imxx * imy**2 + imyy * imx**2 - 2 * imxy * imx * imy)\n    / (imx**2 + imy**2)\n Where imx and imy are the first and imxx, imxy, imyy the second derivatives.  Parameters \n \nimagendarray \n\nInput image.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nresponsendarray \n\nKitchen and Rosenfeld response image.     References  \n1  \nKitchen, L., & Rosenfeld, A. (1982). Gray-level corner detection. Pattern recognition letters, 1(2), 95-102. DOI:10.1016/0167-8655(82)90020-4   \n"}, {"name": "feature.corner_moravec()", "path": "api/skimage.feature#skimage.feature.corner_moravec", "type": "feature", "text": " \nskimage.feature.corner_moravec(image, window_size=1) [source]\n \nCompute Moravec corner measure response image. This is one of the simplest corner detectors and is comparatively fast but has several limitations (e.g. not rotation invariant).  Parameters \n \nimagendarray \n\nInput image.  \nwindow_sizeint, optional \n\nWindow size.    Returns \n \nresponsendarray \n\nMoravec response image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Corner_detection   Examples >>> from skimage.feature import corner_moravec\n>>> square = np.zeros([7, 7])\n>>> square[3, 3] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0]])\n>>> corner_moravec(square).astype(int)\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 2, 1, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0]])\n \n"}, {"name": "feature.corner_orientations()", "path": "api/skimage.feature#skimage.feature.corner_orientations", "type": "feature", "text": " \nskimage.feature.corner_orientations(image, corners, mask) [source]\n \nCompute the orientation of corners. The orientation of corners is computed using the first order central moment i.e. the center of mass approach. The corner orientation is the angle of the vector from the corner coordinate to the intensity centroid in the local neighborhood around the corner calculated using first order central moment.  Parameters \n \nimage2D array \n\nInput grayscale image.  \ncorners(N, 2) array \n\nCorner coordinates as (row, col).  \nmask2D array \n\nMask defining the local neighborhood of the corner used for the calculation of the central moment.    Returns \n \norientations(N, 1) array \n\nOrientations of corners in the range [-pi, pi].     References  \n1  \nEthan Rublee, Vincent Rabaud, Kurt Konolige and Gary Bradski \u201cORB : An efficient alternative to SIFT and SURF\u201d http://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf  \n2  \nPaul L. Rosin, \u201cMeasuring Corner Properties\u201d http://users.cs.cf.ac.uk/Paul.Rosin/corner2.pdf   Examples >>> from skimage.morphology import octagon\n>>> from skimage.feature import (corner_fast, corner_peaks,\n...                              corner_orientations)\n>>> square = np.zeros((12, 12))\n>>> square[3:9, 3:9] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> corners = corner_peaks(corner_fast(square, 9), min_distance=1)\n>>> corners\narray([[3, 3],\n       [3, 8],\n       [8, 3],\n       [8, 8]])\n>>> orientations = corner_orientations(square, corners, octagon(3, 2))\n>>> np.rad2deg(orientations)\narray([  45.,  135.,  -45., -135.])\n \n"}, {"name": "feature.corner_peaks()", "path": "api/skimage.feature#skimage.feature.corner_peaks", "type": "feature", "text": " \nskimage.feature.corner_peaks(image, min_distance=1, threshold_abs=None, threshold_rel=None, exclude_border=True, indices=True, num_peaks=inf, footprint=None, labels=None, *, num_peaks_per_label=inf, p_norm=inf) [source]\n \nFind peaks in corner measure response image. This differs from skimage.feature.peak_local_max in that it suppresses multiple connected peaks with the same accumulator value.  Parameters \n \nimagendarray \n\nInput image.  \nmin_distanceint, optional \n\nThe minimal allowed distance separating peaks.  \n** \n\nSee skimage.feature.peak_local_max().  \np_normfloat \n\nWhich Minkowski p-norm to use. Should be in the range [1, inf]. A finite large p may cause a ValueError if overflow can occur. inf corresponds to the Chebyshev distance and 2 to the Euclidean distance.    Returns \n \noutputndarray or ndarray of bools \n\n If indices = True : (row, column, \u2026) coordinates of peaks. If indices = False : Boolean array shaped like image, with peaks represented by True values.       See also  \nskimage.feature.peak_local_max\n\n  Notes  Changed in version 0.18: The default value of threshold_rel has changed to None, which corresponds to letting skimage.feature.peak_local_max decide on the default. This is equivalent to threshold_rel=0.  The num_peaks limit is applied before suppression of connected peaks. To limit the number of peaks after suppression, set num_peaks=np.inf and post-process the output of this function. Examples >>> from skimage.feature import peak_local_max\n>>> response = np.zeros((5, 5))\n>>> response[2:4, 2:4] = 1\n>>> response\narray([[0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 1., 1., 0.],\n       [0., 0., 1., 1., 0.],\n       [0., 0., 0., 0., 0.]])\n>>> peak_local_max(response)\narray([[2, 2],\n       [2, 3],\n       [3, 2],\n       [3, 3]])\n>>> corner_peaks(response)\narray([[2, 2]])\n \n"}, {"name": "feature.corner_shi_tomasi()", "path": "api/skimage.feature#skimage.feature.corner_shi_tomasi", "type": "feature", "text": " \nskimage.feature.corner_shi_tomasi(image, sigma=1) [source]\n \nCompute Shi-Tomasi (Kanade-Tomasi) corner measure response image. This corner detector uses information from the auto-correlation matrix A: A = [(imx**2)   (imx*imy)] = [Axx Axy]\n    [(imx*imy)   (imy**2)]   [Axy Ayy]\n Where imx and imy are first derivatives, averaged with a gaussian filter. The corner measure is then defined as the smaller eigenvalue of A: ((Axx + Ayy) - sqrt((Axx - Ayy)**2 + 4 * Axy**2)) / 2\n  Parameters \n \nimagendarray \n\nInput image.  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.    Returns \n \nresponsendarray \n\nShi-Tomasi response image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Corner_detection   Examples >>> from skimage.feature import corner_shi_tomasi, corner_peaks\n>>> square = np.zeros([10, 10])\n>>> square[2:8, 2:8] = 1\n>>> square.astype(int)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> corner_peaks(corner_shi_tomasi(square), min_distance=1)\narray([[2, 2],\n       [2, 7],\n       [7, 2],\n       [7, 7]])\n \n"}, {"name": "feature.corner_subpix()", "path": "api/skimage.feature#skimage.feature.corner_subpix", "type": "feature", "text": " \nskimage.feature.corner_subpix(image, corners, window_size=11, alpha=0.99) [source]\n \nDetermine subpixel position of corners. A statistical test decides whether the corner is defined as the intersection of two edges or a single peak. Depending on the classification result, the subpixel corner location is determined based on the local covariance of the grey-values. If the significance level for either statistical test is not sufficient, the corner cannot be classified, and the output subpixel position is set to NaN.  Parameters \n \nimagendarray \n\nInput image.  \ncorners(N, 2) ndarray \n\nCorner coordinates (row, col).  \nwindow_sizeint, optional \n\nSearch window size for subpixel estimation.  \nalphafloat, optional \n\nSignificance level for corner classification.    Returns \n \npositions(N, 2) ndarray \n\nSubpixel corner positions. NaN for \u201cnot classified\u201d corners.     References  \n1  \nF\u00f6rstner, W., & G\u00fclch, E. (1987, June). A fast operator for detection and precise location of distinct points, corners and centres of circular features. In Proc. ISPRS intercommission conference on fast processing of photogrammetric data (pp. 281-305). https://cseweb.ucsd.edu/classes/sp02/cse252/foerstner/foerstner.pdf  \n2  \nhttps://en.wikipedia.org/wiki/Corner_detection   Examples >>> from skimage.feature import corner_harris, corner_peaks, corner_subpix\n>>> img = np.zeros((10, 10))\n>>> img[:5, :5] = 1\n>>> img[5:, 5:] = 1\n>>> img.astype(int)\narray([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]])\n>>> coords = corner_peaks(corner_harris(img), min_distance=2)\n>>> coords_subpix = corner_subpix(img, coords, window_size=7)\n>>> coords_subpix\narray([[4.5, 4.5]])\n \n"}, {"name": "feature.daisy()", "path": "api/skimage.feature#skimage.feature.daisy", "type": "feature", "text": " \nskimage.feature.daisy(image, step=4, radius=15, rings=3, histograms=8, orientations=8, normalization='l1', sigmas=None, ring_radii=None, visualize=False) [source]\n \nExtract DAISY feature descriptors densely for the given image. DAISY is a feature descriptor similar to SIFT formulated in a way that allows for fast dense extraction. Typically, this is practical for bag-of-features image representations. The implementation follows Tola et al. [1] but deviate on the following points:  Histogram bin contribution are smoothed with a circular Gaussian window over the tonal range (the angular range). The sigma values of the spatial Gaussian smoothing in this code do not match the sigma values in the original code by Tola et al. [2]. In their code, spatial smoothing is applied to both the input image and the center histogram. However, this smoothing is not documented in [1] and, therefore, it is omitted.   Parameters \n \nimage(M, N) array \n\nInput image (grayscale).  \nstepint, optional \n\nDistance between descriptor sampling points.  \nradiusint, optional \n\nRadius (in pixels) of the outermost ring.  \nringsint, optional \n\nNumber of rings.  \nhistogramsint, optional \n\nNumber of histograms sampled per ring.  \norientationsint, optional \n\nNumber of orientations (bins) per histogram.  \nnormalization[ \u2018l1\u2019 | \u2018l2\u2019 | \u2018daisy\u2019 | \u2018off\u2019 ], optional \n\nHow to normalize the descriptors  \u2018l1\u2019: L1-normalization of each descriptor. \u2018l2\u2019: L2-normalization of each descriptor. \u2018daisy\u2019: L2-normalization of individual histograms. \u2018off\u2019: Disable normalization.   \nsigmas1D array of float, optional \n\nStandard deviation of spatial Gaussian smoothing for the center histogram and for each ring of histograms. The array of sigmas should be sorted from the center and out. I.e. the first sigma value defines the spatial smoothing of the center histogram and the last sigma value defines the spatial smoothing of the outermost ring. Specifying sigmas overrides the following parameter. rings = len(sigmas) - 1  \nring_radii1D array of int, optional \n\nRadius (in pixels) for each ring. Specifying ring_radii overrides the following two parameters. rings = len(ring_radii) radius = ring_radii[-1] If both sigmas and ring_radii are given, they must satisfy the following predicate since no radius is needed for the center histogram. len(ring_radii) == len(sigmas) + 1  \nvisualizebool, optional \n\nGenerate a visualization of the DAISY descriptors    Returns \n \ndescsarray \n\nGrid of DAISY descriptors for the given image as an array dimensionality (P, Q, R) where P = ceil((M - radius*2) / step) Q = ceil((N - radius*2) / step) R = (rings * histograms + 1) * orientations  \ndescs_img(M, N, 3) array (only if visualize==True) \n\nVisualization of the DAISY descriptors.     References  \n1(1,2)  \nTola et al. \u201cDaisy: An efficient dense descriptor applied to wide- baseline stereo.\u201d Pattern Analysis and Machine Intelligence, IEEE Transactions on 32.5 (2010): 815-830.  \n2  \nhttp://cvlab.epfl.ch/software/daisy   \n"}, {"name": "feature.draw_haar_like_feature()", "path": "api/skimage.feature#skimage.feature.draw_haar_like_feature", "type": "feature", "text": " \nskimage.feature.draw_haar_like_feature(image, r, c, width, height, feature_coord, color_positive_block=(1.0, 0.0, 0.0), color_negative_block=(0.0, 1.0, 0.0), alpha=0.5, max_n_features=None, random_state=None) [source]\n \nVisualization of Haar-like features.  Parameters \n \nimage(M, N) ndarray \n\nThe region of an integral image for which the features need to be computed.  \nrint \n\nRow-coordinate of top left corner of the detection window.  \ncint \n\nColumn-coordinate of top left corner of the detection window.  \nwidthint \n\nWidth of the detection window.  \nheightint \n\nHeight of the detection window.  \nfeature_coordndarray of list of tuples or None, optional \n\nThe array of coordinates to be extracted. This is useful when you want to recompute only a subset of features. In this case feature_type needs to be an array containing the type of each feature, as returned by haar_like_feature_coord(). By default, all coordinates are computed.  \ncolor_positive_rectangletuple of 3 floats \n\nFloats specifying the color for the positive block. Corresponding values define (R, G, B) values. Default value is red (1, 0, 0).  \ncolor_negative_blocktuple of 3 floats \n\nFloats specifying the color for the negative block Corresponding values define (R, G, B) values. Default value is blue (0, 1, 0).  \nalphafloat \n\nValue in the range [0, 1] that specifies opacity of visualization. 1 - fully transparent, 0 - opaque.  \nmax_n_featuresint, default=None \n\nThe maximum number of features to be returned. By default, all features are returned.  \nrandom_stateint, RandomState instance or None, optional \n\nIf int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. The random state is used when generating a set of features smaller than the total number of available features.    Returns \n \nfeatures(M, N), ndarray \n\nAn image in which the different features will be added.     Examples >>> import numpy as np\n>>> from skimage.feature import haar_like_feature_coord\n>>> from skimage.feature import draw_haar_like_feature\n>>> feature_coord, _ = haar_like_feature_coord(2, 2, 'type-4')\n>>> image = draw_haar_like_feature(np.zeros((2, 2)),\n...                                0, 0, 2, 2,\n...                                feature_coord,\n...                                max_n_features=1)\n>>> image\narray([[[0. , 0.5, 0. ],\n        [0.5, 0. , 0. ]],\n\n       [[0.5, 0. , 0. ],\n        [0. , 0.5, 0. ]]])\n \n"}, {"name": "feature.draw_multiblock_lbp()", "path": "api/skimage.feature#skimage.feature.draw_multiblock_lbp", "type": "feature", "text": " \nskimage.feature.draw_multiblock_lbp(image, r, c, width, height, lbp_code=0, color_greater_block=(1, 1, 1), color_less_block=(0, 0.69, 0.96), alpha=0.5) [source]\n \nMulti-block local binary pattern visualization. Blocks with higher sums are colored with alpha-blended white rectangles, whereas blocks with lower sums are colored alpha-blended cyan. Colors and the alpha parameter can be changed.  Parameters \n \nimagendarray of float or uint \n\nImage on which to visualize the pattern.  \nrint \n\nRow-coordinate of top left corner of a rectangle containing feature.  \ncint \n\nColumn-coordinate of top left corner of a rectangle containing feature.  \nwidthint \n\nWidth of one of 9 equal rectangles that will be used to compute a feature.  \nheightint \n\nHeight of one of 9 equal rectangles that will be used to compute a feature.  \nlbp_codeint \n\nThe descriptor of feature to visualize. If not provided, the descriptor with 0 value will be used.  \ncolor_greater_blocktuple of 3 floats \n\nFloats specifying the color for the block that has greater intensity value. They should be in the range [0, 1]. Corresponding values define (R, G, B) values. Default value is white (1, 1, 1).  \ncolor_greater_blocktuple of 3 floats \n\nFloats specifying the color for the block that has greater intensity value. They should be in the range [0, 1]. Corresponding values define (R, G, B) values. Default value is cyan (0, 0.69, 0.96).  \nalphafloat \n\nValue in the range [0, 1] that specifies opacity of visualization. 1 - fully transparent, 0 - opaque.    Returns \n \noutputndarray of float \n\nImage with MB-LBP visualization.     References  \n1  \nFace Detection Based on Multi-Block LBP Representation. Lun Zhang, Rufeng Chu, Shiming Xiang, Shengcai Liao, Stan Z. Li http://www.cbsr.ia.ac.cn/users/scliao/papers/Zhang-ICB07-MBLBP.pdf   \n"}, {"name": "feature.greycomatrix()", "path": "api/skimage.feature#skimage.feature.greycomatrix", "type": "feature", "text": " \nskimage.feature.greycomatrix(image, distances, angles, levels=None, symmetric=False, normed=False) [source]\n \nCalculate the grey-level co-occurrence matrix. A grey level co-occurrence matrix is a histogram of co-occurring greyscale values at a given offset over an image.  Parameters \n \nimagearray_like \n\nInteger typed input image. Only positive valued images are supported. If type is other than uint8, the argument levels needs to be set.  \ndistancesarray_like \n\nList of pixel pair distance offsets.  \nanglesarray_like \n\nList of pixel pair angles in radians.  \nlevelsint, optional \n\nThe input image should contain integers in [0, levels-1], where levels indicate the number of grey-levels counted (typically 256 for an 8-bit image). This argument is required for 16-bit images or higher and is typically the maximum of the image. As the output matrix is at least levels x levels, it might be preferable to use binning of the input image rather than large values for levels.  \nsymmetricbool, optional \n\nIf True, the output matrix P[:, :, d, theta] is symmetric. This is accomplished by ignoring the order of value pairs, so both (i, j) and (j, i) are accumulated when (i, j) is encountered for a given offset. The default is False.  \nnormedbool, optional \n\nIf True, normalize each matrix P[:, :, d, theta] by dividing by the total number of accumulated co-occurrences for the given offset. The elements of the resulting matrix sum to 1. The default is False.    Returns \n \nP4-D ndarray \n\nThe grey-level co-occurrence histogram. The value P[i,j,d,theta] is the number of times that grey-level j occurs at a distance d and at an angle theta from grey-level i. If normed is False, the output is of type uint32, otherwise it is float64. The dimensions are: levels x levels x number of distances x number of angles.     References  \n1  \nThe GLCM Tutorial Home Page, http://www.fp.ucalgary.ca/mhallbey/tutorial.htm  \n2  \nHaralick, RM.; Shanmugam, K., \u201cTextural features for image classification\u201d IEEE Transactions on systems, man, and cybernetics 6 (1973): 610-621. DOI:10.1109/TSMC.1973.4309314  \n3  \nPattern Recognition Engineering, Morton Nadler & Eric P. Smith  \n4  \nWikipedia, https://en.wikipedia.org/wiki/Co-occurrence_matrix   Examples Compute 2 GLCMs: One for a 1-pixel offset to the right, and one for a 1-pixel offset upwards. >>> image = np.array([[0, 0, 1, 1],\n...                   [0, 0, 1, 1],\n...                   [0, 2, 2, 2],\n...                   [2, 2, 3, 3]], dtype=np.uint8)\n>>> result = greycomatrix(image, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4],\n...                       levels=4)\n>>> result[:, :, 0, 0]\narray([[2, 2, 1, 0],\n       [0, 2, 0, 0],\n       [0, 0, 3, 1],\n       [0, 0, 0, 1]], dtype=uint32)\n>>> result[:, :, 0, 1]\narray([[1, 1, 3, 0],\n       [0, 1, 1, 0],\n       [0, 0, 0, 2],\n       [0, 0, 0, 0]], dtype=uint32)\n>>> result[:, :, 0, 2]\narray([[3, 0, 2, 0],\n       [0, 2, 2, 0],\n       [0, 0, 1, 2],\n       [0, 0, 0, 0]], dtype=uint32)\n>>> result[:, :, 0, 3]\narray([[2, 0, 0, 0],\n       [1, 1, 2, 0],\n       [0, 0, 2, 1],\n       [0, 0, 0, 0]], dtype=uint32)\n \n"}, {"name": "feature.greycoprops()", "path": "api/skimage.feature#skimage.feature.greycoprops", "type": "feature", "text": " \nskimage.feature.greycoprops(P, prop='contrast') [source]\n \nCalculate texture properties of a GLCM. Compute a feature of a grey level co-occurrence matrix to serve as a compact summary of the matrix. The properties are computed as follows:  \u2018contrast\u2019: \\(\\sum_{i,j=0}^{levels-1} P_{i,j}(i-j)^2\\)\n \u2018dissimilarity\u2019: \\(\\sum_{i,j=0}^{levels-1}P_{i,j}|i-j|\\)\n \u2018homogeneity\u2019: \\(\\sum_{i,j=0}^{levels-1}\\frac{P_{i,j}}{1+(i-j)^2}\\)\n \u2018ASM\u2019: \\(\\sum_{i,j=0}^{levels-1} P_{i,j}^2\\)\n \u2018energy\u2019: \\(\\sqrt{ASM}\\)\n \n \u2018correlation\u2019:\n\n \\[\\sum_{i,j=0}^{levels-1} P_{i,j}\\left[\\frac{(i-\\mu_i) \\ (j-\\mu_j)}{\\sqrt{(\\sigma_i^2)(\\sigma_j^2)}}\\right]\\]     Each GLCM is normalized to have a sum of 1 before the computation of texture properties.  Parameters \n \nPndarray \n\nInput array. P is the grey-level co-occurrence histogram for which to compute the specified property. The value P[i,j,d,theta] is the number of times that grey-level j occurs at a distance d and at an angle theta from grey-level i.  \nprop{\u2018contrast\u2019, \u2018dissimilarity\u2019, \u2018homogeneity\u2019, \u2018energy\u2019, \u2018correlation\u2019, \u2018ASM\u2019}, optional \n\nThe property of the GLCM to compute. The default is \u2018contrast\u2019.    Returns \n \nresults2-D ndarray \n\n2-dimensional array. results[d, a] is the property \u2018prop\u2019 for the d\u2019th distance and the a\u2019th angle.     References  \n1  \nThe GLCM Tutorial Home Page, http://www.fp.ucalgary.ca/mhallbey/tutorial.htm   Examples Compute the contrast for GLCMs with distances [1, 2] and angles [0 degrees, 90 degrees] >>> image = np.array([[0, 0, 1, 1],\n...                   [0, 0, 1, 1],\n...                   [0, 2, 2, 2],\n...                   [2, 2, 3, 3]], dtype=np.uint8)\n>>> g = greycomatrix(image, [1, 2], [0, np.pi/2], levels=4,\n...                  normed=True, symmetric=True)\n>>> contrast = greycoprops(g, 'contrast')\n>>> contrast\narray([[0.58333333, 1.        ],\n       [1.25      , 2.75      ]])\n \n"}, {"name": "feature.haar_like_feature()", "path": "api/skimage.feature#skimage.feature.haar_like_feature", "type": "feature", "text": " \nskimage.feature.haar_like_feature(int_image, r, c, width, height, feature_type=None, feature_coord=None) [source]\n \nCompute the Haar-like features for a region of interest (ROI) of an integral image. Haar-like features have been successfully used for image classification and object detection [1]. It has been used for real-time face detection algorithm proposed in [2].  Parameters \n \nint_image(M, N) ndarray \n\nIntegral image for which the features need to be computed.  \nrint \n\nRow-coordinate of top left corner of the detection window.  \ncint \n\nColumn-coordinate of top left corner of the detection window.  \nwidthint \n\nWidth of the detection window.  \nheightint \n\nHeight of the detection window.  \nfeature_typestr or list of str or None, optional \n\nThe type of feature to consider:  \u2018type-2-x\u2019: 2 rectangles varying along the x axis; \u2018type-2-y\u2019: 2 rectangles varying along the y axis; \u2018type-3-x\u2019: 3 rectangles varying along the x axis; \u2018type-3-y\u2019: 3 rectangles varying along the y axis; \u2018type-4\u2019: 4 rectangles varying along x and y axis.  By default all features are extracted. If using with feature_coord, it should correspond to the feature type of each associated coordinate feature.  \nfeature_coordndarray of list of tuples or None, optional \n\nThe array of coordinates to be extracted. This is useful when you want to recompute only a subset of features. In this case feature_type needs to be an array containing the type of each feature, as returned by haar_like_feature_coord(). By default, all coordinates are computed.    Returns \n \nhaar_features(n_features,) ndarray of int or float \n\nResulting Haar-like features. Each value is equal to the subtraction of sums of the positive and negative rectangles. The data type depends of the data type of int_image: int when the data type of int_image is uint or int and float when the data type of int_image is float.     Notes When extracting those features in parallel, be aware that the choice of the backend (i.e. multiprocessing vs threading) will have an impact on the performance. The rule of thumb is as follows: use multiprocessing when extracting features for all possible ROI in an image; use threading when extracting the feature at specific location for a limited number of ROIs. Refer to the example Face classification using Haar-like feature descriptor for more insights. References  \n1  \nhttps://en.wikipedia.org/wiki/Haar-like_feature  \n2  \nOren, M., Papageorgiou, C., Sinha, P., Osuna, E., & Poggio, T. (1997, June). Pedestrian detection using wavelet templates. In Computer Vision and Pattern Recognition, 1997. Proceedings., 1997 IEEE Computer Society Conference on (pp. 193-199). IEEE. http://tinyurl.com/y6ulxfta DOI:10.1109/CVPR.1997.609319  \n3  \nViola, Paul, and Michael J. Jones. \u201cRobust real-time face detection.\u201d International journal of computer vision 57.2 (2004): 137-154. https://www.merl.com/publications/docs/TR2004-043.pdf DOI:10.1109/CVPR.2001.990517   Examples >>> import numpy as np\n>>> from skimage.transform import integral_image\n>>> from skimage.feature import haar_like_feature\n>>> img = np.ones((5, 5), dtype=np.uint8)\n>>> img_ii = integral_image(img)\n>>> feature = haar_like_feature(img_ii, 0, 0, 5, 5, 'type-3-x')\n>>> feature\narray([-1, -2, -3, -4, -1, -2, -3, -4, -1, -2, -3, -4, -1, -2, -3, -4, -1,\n       -2, -3, -4, -1, -2, -3, -4, -1, -2, -3, -1, -2, -3, -1, -2, -3, -1,\n       -2, -1, -2, -1, -2, -1, -1, -1])\n You can compute the feature for some pre-computed coordinates. >>> from skimage.feature import haar_like_feature_coord\n>>> feature_coord, feature_type = zip(\n...     *[haar_like_feature_coord(5, 5, feat_t)\n...       for feat_t in ('type-2-x', 'type-3-x')])\n>>> # only select one feature over two\n>>> feature_coord = np.concatenate([x[::2] for x in feature_coord])\n>>> feature_type = np.concatenate([x[::2] for x in feature_type])\n>>> feature = haar_like_feature(img_ii, 0, 0, 5, 5,\n...                             feature_type=feature_type,\n...                             feature_coord=feature_coord)\n>>> feature\narray([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0, -1, -3, -1, -3, -1, -3, -1, -3, -1,\n       -3, -1, -3, -1, -3, -2, -1, -3, -2, -2, -2, -1])\n \n"}, {"name": "feature.haar_like_feature_coord()", "path": "api/skimage.feature#skimage.feature.haar_like_feature_coord", "type": "feature", "text": " \nskimage.feature.haar_like_feature_coord(width, height, feature_type=None) [source]\n \nCompute the coordinates of Haar-like features.  Parameters \n \nwidthint \n\nWidth of the detection window.  \nheightint \n\nHeight of the detection window.  \nfeature_typestr or list of str or None, optional \n\nThe type of feature to consider:  \u2018type-2-x\u2019: 2 rectangles varying along the x axis; \u2018type-2-y\u2019: 2 rectangles varying along the y axis; \u2018type-3-x\u2019: 3 rectangles varying along the x axis; \u2018type-3-y\u2019: 3 rectangles varying along the y axis; \u2018type-4\u2019: 4 rectangles varying along x and y axis.  By default all features are extracted.    Returns \n \nfeature_coord(n_features, n_rectangles, 2, 2), ndarray of list of tuple coord \n\nCoordinates of the rectangles for each feature.  \nfeature_type(n_features,), ndarray of str \n\nThe corresponding type for each feature.     Examples >>> import numpy as np\n>>> from skimage.transform import integral_image\n>>> from skimage.feature import haar_like_feature_coord\n>>> feat_coord, feat_type = haar_like_feature_coord(2, 2, 'type-4')\n>>> feat_coord \narray([ list([[(0, 0), (0, 0)], [(0, 1), (0, 1)],\n              [(1, 1), (1, 1)], [(1, 0), (1, 0)]])], dtype=object)\n>>> feat_type\narray(['type-4'], dtype=object)\n \n"}, {"name": "feature.hessian_matrix()", "path": "api/skimage.feature#skimage.feature.hessian_matrix", "type": "feature", "text": " \nskimage.feature.hessian_matrix(image, sigma=1, mode='constant', cval=0, order='rc') [source]\n \nCompute Hessian matrix. The Hessian matrix is defined as: H = [Hrr Hrc]\n    [Hrc Hcc]\n which is computed by convolving the image with the second derivatives of the Gaussian kernel in the respective r- and c-directions.  Parameters \n \nimagendarray \n\nInput image.  \nsigmafloat \n\nStandard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.  \norder{\u2018rc\u2019, \u2018xy\u2019}, optional \n\nThis parameter allows for the use of reverse or forward order of the image axes in gradient computation. \u2018rc\u2019 indicates the use of the first axis initially (Hrr, Hrc, Hcc), whilst \u2018xy\u2019 indicates the usage of the last axis initially (Hxx, Hxy, Hyy)    Returns \n \nHrrndarray \n\nElement of the Hessian matrix for each pixel in the input image.  \nHrcndarray \n\nElement of the Hessian matrix for each pixel in the input image.  \nHccndarray \n\nElement of the Hessian matrix for each pixel in the input image.     Examples >>> from skimage.feature import hessian_matrix\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 4\n>>> Hrr, Hrc, Hcc = hessian_matrix(square, sigma=0.1, order='rc')\n>>> Hrc\narray([[ 0.,  0.,  0.,  0.,  0.],\n       [ 0.,  1.,  0., -1.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.],\n       [ 0., -1.,  0.,  1.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.]])\n \n"}, {"name": "feature.hessian_matrix_det()", "path": "api/skimage.feature#skimage.feature.hessian_matrix_det", "type": "feature", "text": " \nskimage.feature.hessian_matrix_det(image, sigma=1, approximate=True) [source]\n \nCompute the approximate Hessian Determinant over an image. The 2D approximate method uses box filters over integral images to compute the approximate Hessian Determinant, as described in [1].  Parameters \n \nimagearray \n\nThe image over which to compute Hessian Determinant.  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, used for the Hessian matrix.  \napproximatebool, optional \n\nIf True and the image is 2D, use a much faster approximate computation. This argument has no effect on 3D and higher images.    Returns \n \noutarray \n\nThe array of the Determinant of Hessians.     Notes For 2D images when approximate=True, the running time of this method only depends on size of the image. It is independent of sigma as one would expect. The downside is that the result for sigma less than 3 is not accurate, i.e., not similar to the result obtained if someone computed the Hessian and took its determinant. References  \n1  \nHerbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool, \u201cSURF: Speeded Up Robust Features\u201d ftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf   \n"}, {"name": "feature.hessian_matrix_eigvals()", "path": "api/skimage.feature#skimage.feature.hessian_matrix_eigvals", "type": "feature", "text": " \nskimage.feature.hessian_matrix_eigvals(H_elems) [source]\n \nCompute eigenvalues of Hessian matrix.  Parameters \n \nH_elemslist of ndarray \n\nThe upper-diagonal elements of the Hessian matrix, as returned by hessian_matrix.    Returns \n \neigsndarray \n\nThe eigenvalues of the Hessian matrix, in decreasing order. The eigenvalues are the leading dimension. That is, eigs[i, j, k] contains the ith-largest eigenvalue at position (j, k).     Examples >>> from skimage.feature import hessian_matrix, hessian_matrix_eigvals\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 4\n>>> H_elems = hessian_matrix(square, sigma=0.1, order='rc')\n>>> hessian_matrix_eigvals(H_elems)[0]\narray([[ 0.,  0.,  2.,  0.,  0.],\n       [ 0.,  1.,  0.,  1.,  0.],\n       [ 2.,  0., -2.,  0.,  2.],\n       [ 0.,  1.,  0.,  1.,  0.],\n       [ 0.,  0.,  2.,  0.,  0.]])\n \n"}, {"name": "feature.hog()", "path": "api/skimage.feature#skimage.feature.hog", "type": "feature", "text": " \nskimage.feature.hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(3, 3), block_norm='L2-Hys', visualize=False, transform_sqrt=False, feature_vector=True, multichannel=None) [source]\n \nExtract Histogram of Oriented Gradients (HOG) for a given image. Compute a Histogram of Oriented Gradients (HOG) by  (optional) global image normalization computing the gradient image in row and col\n computing gradient histograms normalizing across blocks flattening into a feature vector   Parameters \n \nimage(M, N[, C]) ndarray \n\nInput image.  \norientationsint, optional \n\nNumber of orientation bins.  \npixels_per_cell2-tuple (int, int), optional \n\nSize (in pixels) of a cell.  \ncells_per_block2-tuple (int, int), optional \n\nNumber of cells in each block.  \nblock_normstr {\u2018L1\u2019, \u2018L1-sqrt\u2019, \u2018L2\u2019, \u2018L2-Hys\u2019}, optional \n\nBlock normalization method:  \nL1 \n\nNormalization using L1-norm.  \nL1-sqrt \n\nNormalization using L1-norm, followed by square root.  \nL2 \n\nNormalization using L2-norm.  \nL2-Hys \n\nNormalization using L2-norm, followed by limiting the maximum values to 0.2 (Hys stands for hysteresis) and renormalization using L2-norm. (default) For details, see [3], [4].    \nvisualizebool, optional \n\nAlso return an image of the HOG. For each cell and orientation bin, the image contains a line segment that is centered at the cell center, is perpendicular to the midpoint of the range of angles spanned by the orientation bin, and has intensity proportional to the corresponding histogram value.  \ntransform_sqrtbool, optional \n\nApply power law compression to normalize the image before processing. DO NOT use this if the image contains negative values. Also see notes section below.  \nfeature_vectorbool, optional \n\nReturn the data as a feature vector by calling .ravel() on the result just before returning.  \nmultichannelboolean, optional \n\nIf True, the last image dimension is considered as a color channel, otherwise as spatial.    Returns \n \nout(n_blocks_row, n_blocks_col, n_cells_row, n_cells_col, n_orient) ndarray \n\nHOG descriptor for the image. If feature_vector is True, a 1D (flattened) array is returned.  \nhog_image(M, N) ndarray, optional \n\nA visualisation of the HOG image. Only provided if visualize is True.     Notes The presented code implements the HOG extraction method from [2] with the following changes: (I) blocks of (3, 3) cells are used ((2, 2) in the paper); (II) no smoothing within cells (Gaussian spatial window with sigma=8pix in the paper); (III) L1 block normalization is used (L2-Hys in the paper). Power law compression, also known as Gamma correction, is used to reduce the effects of shadowing and illumination variations. The compression makes the dark regions lighter. When the kwarg transform_sqrt is set to True, the function computes the square root of each color channel and then applies the hog algorithm to the image. References  \n1  \nhttps://en.wikipedia.org/wiki/Histogram_of_oriented_gradients  \n2  \nDalal, N and Triggs, B, Histograms of Oriented Gradients for Human Detection, IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2005 San Diego, CA, USA, https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf, DOI:10.1109/CVPR.2005.177  \n3  \nLowe, D.G., Distinctive image features from scale-invatiant keypoints, International Journal of Computer Vision (2004) 60: 91, http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf, DOI:10.1023/B:VISI.0000029664.99615.94  \n4  \nDalal, N, Finding People in Images and Videos, Human-Computer Interaction [cs.HC], Institut National Polytechnique de Grenoble - INPG, 2006, https://tel.archives-ouvertes.fr/tel-00390303/file/NavneetDalalThesis.pdf   \n"}, {"name": "feature.local_binary_pattern()", "path": "api/skimage.feature#skimage.feature.local_binary_pattern", "type": "feature", "text": " \nskimage.feature.local_binary_pattern(image, P, R, method='default') [source]\n \nGray scale and rotation invariant LBP (Local Binary Patterns). LBP is an invariant descriptor that can be used for texture classification.  Parameters \n \nimage(N, M) array \n\nGraylevel image.  \nPint \n\nNumber of circularly symmetric neighbour set points (quantization of the angular space).  \nRfloat \n\nRadius of circle (spatial resolution of the operator).  \nmethod{\u2018default\u2019, \u2018ror\u2019, \u2018uniform\u2019, \u2018var\u2019} \n\nMethod to determine the pattern.  \n \u2018default\u2019: original local binary pattern which is gray scale but not\n\nrotation invariant.    \n \u2018ror\u2019: extension of default implementation which is gray scale and\n\nrotation invariant.    \n \u2018uniform\u2019: improved rotation invariance with uniform patterns and\n\nfiner quantization of the angular space which is gray scale and rotation invariant.    \n \u2018nri_uniform\u2019: non rotation-invariant uniform patterns variant\n\nwhich is only gray scale invariant [2].    \n \u2018var\u2019: rotation invariant variance measures of the contrast of local\n\nimage texture which is rotation but not gray scale invariant.        Returns \n \noutput(N, M) array \n\nLBP image.     References  \n1  \nMultiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns. Timo Ojala, Matti Pietikainen, Topi Maenpaa. http://www.ee.oulu.fi/research/mvmp/mvg/files/pdf/pdf_94.pdf, 2002.  \n2  \nFace recognition with local binary patterns. Timo Ahonen, Abdenour Hadid, Matti Pietikainen, http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.214.6851, 2004.   \n"}, {"name": "feature.masked_register_translation()", "path": "api/skimage.feature#skimage.feature.masked_register_translation", "type": "feature", "text": " \nskimage.feature.masked_register_translation(src_image, target_image, src_mask, target_mask=None, overlap_ratio=0.3) [source]\n \nDeprecated function. Use skimage.registration.phase_cross_correlation instead. \n"}, {"name": "feature.match_descriptors()", "path": "api/skimage.feature#skimage.feature.match_descriptors", "type": "feature", "text": " \nskimage.feature.match_descriptors(descriptors1, descriptors2, metric=None, p=2, max_distance=inf, cross_check=True, max_ratio=1.0) [source]\n \nBrute-force matching of descriptors. For each descriptor in the first set this matcher finds the closest descriptor in the second set (and vice-versa in the case of enabled cross-checking).  Parameters \n \ndescriptors1(M, P) array \n\nDescriptors of size P about M keypoints in the first image.  \ndescriptors2(N, P) array \n\nDescriptors of size P about N keypoints in the second image.  \nmetric{\u2018euclidean\u2019, \u2018cityblock\u2019, \u2018minkowski\u2019, \u2018hamming\u2019, \u2026} , optional \n\nThe metric to compute the distance between two descriptors. See scipy.spatial.distance.cdist for all possible types. The hamming distance should be used for binary descriptors. By default the L2-norm is used for all descriptors of dtype float or double and the Hamming distance is used for binary descriptors automatically.  \npint, optional \n\nThe p-norm to apply for metric='minkowski'.  \nmax_distancefloat, optional \n\nMaximum allowed distance between descriptors of two keypoints in separate images to be regarded as a match.  \ncross_checkbool, optional \n\nIf True, the matched keypoints are returned after cross checking i.e. a matched pair (keypoint1, keypoint2) is returned if keypoint2 is the best match for keypoint1 in second image and keypoint1 is the best match for keypoint2 in first image.  \nmax_ratiofloat, optional \n\nMaximum ratio of distances between first and second closest descriptor in the second set of descriptors. This threshold is useful to filter ambiguous matches between the two descriptor sets. The choice of this value depends on the statistics of the chosen descriptor, e.g., for SIFT descriptors a value of 0.8 is usually chosen, see D.G. Lowe, \u201cDistinctive Image Features from Scale-Invariant Keypoints\u201d, International Journal of Computer Vision, 2004.    Returns \n \nmatches(Q, 2) array \n\nIndices of corresponding matches in first and second set of descriptors, where matches[:, 0] denote the indices in the first and matches[:, 1] the indices in the second set of descriptors.     \n"}, {"name": "feature.match_template()", "path": "api/skimage.feature#skimage.feature.match_template", "type": "feature", "text": " \nskimage.feature.match_template(image, template, pad_input=False, mode='constant', constant_values=0) [source]\n \nMatch a template to a 2-D or 3-D image using normalized correlation. The output is an array with values between -1.0 and 1.0. The value at a given position corresponds to the correlation coefficient between the image and the template. For pad_input=True matches correspond to the center and otherwise to the top-left corner of the template. To find the best match you must search for peaks in the response (output) image.  Parameters \n \nimage(M, N[, D]) array \n\n2-D or 3-D input image.  \ntemplate(m, n[, d]) array \n\nTemplate to locate. It must be (m <= M, n <= N[, d <= D]).  \npad_inputbool \n\nIf True, pad image so that output is the same size as the image, and output values correspond to the template center. Otherwise, the output is an array with shape (M - m + 1, N - n + 1) for an (M, N) image and an (m, n) template, and matches correspond to origin (top-left corner) of the template.  \nmodesee numpy.pad, optional \n\nPadding mode.  \nconstant_valuessee numpy.pad, optional \n\nConstant values used in conjunction with mode='constant'.    Returns \n \noutputarray \n\nResponse image with correlation coefficients.     Notes Details on the cross-correlation are presented in [1]. This implementation uses FFT convolutions of the image and the template. Reference [2] presents similar derivations but the approximation presented in this reference is not used in our implementation. References  \n1  \nJ. P. Lewis, \u201cFast Normalized Cross-Correlation\u201d, Industrial Light and Magic.  \n2  \nBriechle and Hanebeck, \u201cTemplate Matching using Fast Normalized Cross Correlation\u201d, Proceedings of the SPIE (2001). DOI:10.1117/12.421129   Examples >>> template = np.zeros((3, 3))\n>>> template[1, 1] = 1\n>>> template\narray([[0., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 0.]])\n>>> image = np.zeros((6, 6))\n>>> image[1, 1] = 1\n>>> image[4, 4] = -1\n>>> image\narray([[ 0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  1.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0., -1.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.]])\n>>> result = match_template(image, template)\n>>> np.round(result, 3)\narray([[ 1.   , -0.125,  0.   ,  0.   ],\n       [-0.125, -0.125,  0.   ,  0.   ],\n       [ 0.   ,  0.   ,  0.125,  0.125],\n       [ 0.   ,  0.   ,  0.125, -1.   ]])\n>>> result = match_template(image, template, pad_input=True)\n>>> np.round(result, 3)\narray([[-0.125, -0.125, -0.125,  0.   ,  0.   ,  0.   ],\n       [-0.125,  1.   , -0.125,  0.   ,  0.   ,  0.   ],\n       [-0.125, -0.125, -0.125,  0.   ,  0.   ,  0.   ],\n       [ 0.   ,  0.   ,  0.   ,  0.125,  0.125,  0.125],\n       [ 0.   ,  0.   ,  0.   ,  0.125, -1.   ,  0.125],\n       [ 0.   ,  0.   ,  0.   ,  0.125,  0.125,  0.125]])\n \n"}, {"name": "feature.multiblock_lbp()", "path": "api/skimage.feature#skimage.feature.multiblock_lbp", "type": "feature", "text": " \nskimage.feature.multiblock_lbp(int_image, r, c, width, height) [source]\n \nMulti-block local binary pattern (MB-LBP). The features are calculated similarly to local binary patterns (LBPs), (See local_binary_pattern()) except that summed blocks are used instead of individual pixel values. MB-LBP is an extension of LBP that can be computed on multiple scales in constant time using the integral image. Nine equally-sized rectangles are used to compute a feature. For each rectangle, the sum of the pixel intensities is computed. Comparisons of these sums to that of the central rectangle determine the feature, similarly to LBP.  Parameters \n \nint_image(N, M) array \n\nIntegral image.  \nrint \n\nRow-coordinate of top left corner of a rectangle containing feature.  \ncint \n\nColumn-coordinate of top left corner of a rectangle containing feature.  \nwidthint \n\nWidth of one of the 9 equal rectangles that will be used to compute a feature.  \nheightint \n\nHeight of one of the 9 equal rectangles that will be used to compute a feature.    Returns \n \noutputint \n\n8-bit MB-LBP feature descriptor.     References  \n1  \nFace Detection Based on Multi-Block LBP Representation. Lun Zhang, Rufeng Chu, Shiming Xiang, Shengcai Liao, Stan Z. Li http://www.cbsr.ia.ac.cn/users/scliao/papers/Zhang-ICB07-MBLBP.pdf   \n"}, {"name": "feature.multiscale_basic_features()", "path": "api/skimage.feature#skimage.feature.multiscale_basic_features", "type": "feature", "text": " \nskimage.feature.multiscale_basic_features(image, multichannel=False, intensity=True, edges=True, texture=True, sigma_min=0.5, sigma_max=16, num_sigma=None, num_workers=None) [source]\n \nLocal features for a single- or multi-channel nd image. Intensity, gradient intensity and local structure are computed at different scales thanks to Gaussian blurring.  Parameters \n \nimagendarray \n\nInput image, which can be grayscale or multichannel.  \nmultichannelbool, default False \n\nTrue if the last dimension corresponds to color channels.  \nintensitybool, default True \n\nIf True, pixel intensities averaged over the different scales are added to the feature set.  \nedgesbool, default True \n\nIf True, intensities of local gradients averaged over the different scales are added to the feature set.  \ntexturebool, default True \n\nIf True, eigenvalues of the Hessian matrix after Gaussian blurring at different scales are added to the feature set.  \nsigma_minfloat, optional \n\nSmallest value of the Gaussian kernel used to average local neighbourhoods before extracting features.  \nsigma_maxfloat, optional \n\nLargest value of the Gaussian kernel used to average local neighbourhoods before extracting features.  \nnum_sigmaint, optional \n\nNumber of values of the Gaussian kernel between sigma_min and sigma_max. If None, sigma_min multiplied by powers of 2 are used.  \nnum_workersint or None, optional \n\nThe number of parallel threads to use. If set to None, the full set of available cores are used.    Returns \n \nfeaturesnp.ndarray \n\nArray of shape image.shape + (n_features,)     \n"}, {"name": "feature.ORB", "path": "api/skimage.feature#skimage.feature.ORB", "type": "feature", "text": " \nclass skimage.feature.ORB(downscale=1.2, n_scales=8, n_keypoints=500, fast_n=9, fast_threshold=0.08, harris_k=0.04) [source]\n \nBases: skimage.feature.util.FeatureDetector, skimage.feature.util.DescriptorExtractor Oriented FAST and rotated BRIEF feature detector and binary descriptor extractor.  Parameters \n \nn_keypointsint, optional \n\nNumber of keypoints to be returned. The function will return the best n_keypoints according to the Harris corner response if more than n_keypoints are detected. If not, then all the detected keypoints are returned.  \nfast_nint, optional \n\nThe n parameter in skimage.feature.corner_fast. Minimum number of consecutive pixels out of 16 pixels on the circle that should all be either brighter or darker w.r.t test-pixel. A point c on the circle is darker w.r.t test pixel p if Ic < Ip - threshold and brighter if Ic > Ip + threshold. Also stands for the n in FAST-n corner detector.  \nfast_thresholdfloat, optional \n\nThe threshold parameter in feature.corner_fast. Threshold used to decide whether the pixels on the circle are brighter, darker or similar w.r.t. the test pixel. Decrease the threshold when more corners are desired and vice-versa.  \nharris_kfloat, optional \n\nThe k parameter in skimage.feature.corner_harris. Sensitivity factor to separate corners from edges, typically in range [0, 0.2]. Small values of k result in detection of sharp corners.  \ndownscalefloat, optional \n\nDownscale factor for the image pyramid. Default value 1.2 is chosen so that there are more dense scales which enable robust scale invariance for a subsequent feature description.  \nn_scalesint, optional \n\nMaximum number of scales from the bottom of the image pyramid to extract the features from.     References  \n1  \nEthan Rublee, Vincent Rabaud, Kurt Konolige and Gary Bradski \u201cORB: An efficient alternative to SIFT and SURF\u201d http://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf   Examples >>> from skimage.feature import ORB, match_descriptors\n>>> img1 = np.zeros((100, 100))\n>>> img2 = np.zeros_like(img1)\n>>> np.random.seed(1)\n>>> square = np.random.rand(20, 20)\n>>> img1[40:60, 40:60] = square\n>>> img2[53:73, 53:73] = square\n>>> detector_extractor1 = ORB(n_keypoints=5)\n>>> detector_extractor2 = ORB(n_keypoints=5)\n>>> detector_extractor1.detect_and_extract(img1)\n>>> detector_extractor2.detect_and_extract(img2)\n>>> matches = match_descriptors(detector_extractor1.descriptors,\n...                             detector_extractor2.descriptors)\n>>> matches\narray([[0, 0],\n       [1, 1],\n       [2, 2],\n       [3, 3],\n       [4, 4]])\n>>> detector_extractor1.keypoints[matches[:, 0]]\narray([[42., 40.],\n       [47., 58.],\n       [44., 40.],\n       [59., 42.],\n       [45., 44.]])\n>>> detector_extractor2.keypoints[matches[:, 1]]\narray([[55., 53.],\n       [60., 71.],\n       [57., 53.],\n       [72., 55.],\n       [58., 57.]])\n  Attributes \n \nkeypoints(N, 2) array \n\nKeypoint coordinates as (row, col).  \nscales(N, ) array \n\nCorresponding scales.  \norientations(N, ) array \n\nCorresponding orientations in radians.  \nresponses(N, ) array \n\nCorresponding Harris corner responses.  \ndescriptors(Q, descriptor_size) array of dtype bool \n\n2D array of binary descriptors of size descriptor_size for Q keypoints after filtering out border keypoints with value at an index (i, j) either being True or False representing the outcome of the intensity comparison for i-th keypoint on j-th decision pixel-pair. It is Q == np.sum(mask).      \n__init__(downscale=1.2, n_scales=8, n_keypoints=500, fast_n=9, fast_threshold=0.08, harris_k=0.04) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \ndetect(image) [source]\n \nDetect oriented FAST keypoints along with the corresponding scale.  Parameters \n \nimage2D array \n\nInput image.     \n  \ndetect_and_extract(image) [source]\n \nDetect oriented FAST keypoints and extract rBRIEF descriptors. Note that this is faster than first calling detect and then extract.  Parameters \n \nimage2D array \n\nInput image.     \n  \nextract(image, keypoints, scales, orientations) [source]\n \nExtract rBRIEF binary descriptors for given keypoints in image. Note that the keypoints must be extracted using the same downscale and n_scales parameters. Additionally, if you want to extract both keypoints and descriptors you should use the faster detect_and_extract.  Parameters \n \nimage2D array \n\nInput image.  \nkeypoints(N, 2) array \n\nKeypoint coordinates as (row, col).  \nscales(N, ) array \n\nCorresponding scales.  \norientations(N, ) array \n\nCorresponding orientations in radians.     \n \n"}, {"name": "feature.ORB.detect()", "path": "api/skimage.feature#skimage.feature.ORB.detect", "type": "feature", "text": " \ndetect(image) [source]\n \nDetect oriented FAST keypoints along with the corresponding scale.  Parameters \n \nimage2D array \n\nInput image.     \n"}, {"name": "feature.ORB.detect_and_extract()", "path": "api/skimage.feature#skimage.feature.ORB.detect_and_extract", "type": "feature", "text": " \ndetect_and_extract(image) [source]\n \nDetect oriented FAST keypoints and extract rBRIEF descriptors. Note that this is faster than first calling detect and then extract.  Parameters \n \nimage2D array \n\nInput image.     \n"}, {"name": "feature.ORB.extract()", "path": "api/skimage.feature#skimage.feature.ORB.extract", "type": "feature", "text": " \nextract(image, keypoints, scales, orientations) [source]\n \nExtract rBRIEF binary descriptors for given keypoints in image. Note that the keypoints must be extracted using the same downscale and n_scales parameters. Additionally, if you want to extract both keypoints and descriptors you should use the faster detect_and_extract.  Parameters \n \nimage2D array \n\nInput image.  \nkeypoints(N, 2) array \n\nKeypoint coordinates as (row, col).  \nscales(N, ) array \n\nCorresponding scales.  \norientations(N, ) array \n\nCorresponding orientations in radians.     \n"}, {"name": "feature.ORB.__init__()", "path": "api/skimage.feature#skimage.feature.ORB.__init__", "type": "feature", "text": " \n__init__(downscale=1.2, n_scales=8, n_keypoints=500, fast_n=9, fast_threshold=0.08, harris_k=0.04) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "feature.peak_local_max()", "path": "api/skimage.feature#skimage.feature.peak_local_max", "type": "feature", "text": " \nskimage.feature.peak_local_max(image, min_distance=1, threshold_abs=None, threshold_rel=None, exclude_border=True, indices=True, num_peaks=inf, footprint=None, labels=None, num_peaks_per_label=inf, p_norm=inf) [source]\n \nFind peaks in an image as coordinate list or boolean mask. Peaks are the local maxima in a region of 2 * min_distance + 1 (i.e. peaks are separated by at least min_distance). If both threshold_abs and threshold_rel are provided, the maximum of the two is chosen as the minimum intensity threshold of peaks.  Changed in version 0.18: Prior to version 0.18, peaks of the same height within a radius of min_distance were all returned, but this could cause unexpected behaviour. From 0.18 onwards, an arbitrary peak within the region is returned. See issue gh-2592.   Parameters \n \nimagendarray \n\nInput image.  \nmin_distanceint, optional \n\nThe minimal allowed distance separating peaks. To find the maximum number of peaks, use min_distance=1.  \nthreshold_absfloat, optional \n\nMinimum intensity of peaks. By default, the absolute threshold is the minimum intensity of the image.  \nthreshold_relfloat, optional \n\nMinimum intensity of peaks, calculated as max(image) * threshold_rel.  \nexclude_borderint, tuple of ints, or bool, optional \n\nIf positive integer, exclude_border excludes peaks from within exclude_border-pixels of the border of the image. If tuple of non-negative ints, the length of the tuple must match the input array\u2019s dimensionality. Each element of the tuple will exclude peaks from within exclude_border-pixels of the border of the image along that dimension. If True, takes the min_distance parameter as value. If zero or False, peaks are identified regardless of their distance from the border.  \nindicesbool, optional \n\nIf True, the output will be an array representing peak coordinates. The coordinates are sorted according to peaks values (Larger first). If False, the output will be a boolean array shaped as image.shape with peaks present at True elements. indices is deprecated and will be removed in version 0.20. Default behavior will be to always return peak coordinates. You can obtain a mask as shown in the example below.  \nnum_peaksint, optional \n\nMaximum number of peaks. When the number of peaks exceeds num_peaks, return num_peaks peaks based on highest peak intensity.  \nfootprintndarray of bools, optional \n\nIf provided, footprint == 1 represents the local region within which to search for peaks at every point in image.  \nlabelsndarray of ints, optional \n\nIf provided, each unique region labels == value represents a unique region to search for peaks. Zero is reserved for background.  \nnum_peaks_per_labelint, optional \n\nMaximum number of peaks for each label.  \np_normfloat \n\nWhich Minkowski p-norm to use. Should be in the range [1, inf]. A finite large p may cause a ValueError if overflow can occur. inf corresponds to the Chebyshev distance and 2 to the Euclidean distance.    Returns \n \noutputndarray or ndarray of bools \n\n If indices = True : (row, column, \u2026) coordinates of peaks. If indices = False : Boolean array shaped like image, with peaks represented by True values.       See also  \nskimage.feature.corner_peaks\n\n  Notes The peak local maximum function returns the coordinates of local peaks (maxima) in an image. Internally, a maximum filter is used for finding local maxima. This operation dilates the original image. After comparison of the dilated and original image, this function returns the coordinates or a mask of the peaks where the dilated image equals the original image. Examples >>> img1 = np.zeros((7, 7))\n>>> img1[3, 4] = 1\n>>> img1[3, 2] = 1.5\n>>> img1\narray([[0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n       [0. , 0. , 1.5, 0. , 1. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n       [0. , 0. , 0. , 0. , 0. , 0. , 0. ]])\n >>> peak_local_max(img1, min_distance=1)\narray([[3, 2],\n       [3, 4]])\n >>> peak_local_max(img1, min_distance=2)\narray([[3, 2]])\n >>> img2 = np.zeros((20, 20, 20))\n>>> img2[10, 10, 10] = 1\n>>> img2[15, 15, 15] = 1\n>>> peak_idx = peak_local_max(img2, exclude_border=0)\n>>> peak_idx\narray([[10, 10, 10],\n       [15, 15, 15]])\n >>> peak_mask = np.zeros_like(img2, dtype=bool)\n>>> peak_mask[tuple(peak_idx.T)] = True\n>>> np.argwhere(peak_mask)\narray([[10, 10, 10],\n       [15, 15, 15]])\n \n"}, {"name": "feature.plot_matches()", "path": "api/skimage.feature#skimage.feature.plot_matches", "type": "feature", "text": " \nskimage.feature.plot_matches(ax, image1, image2, keypoints1, keypoints2, matches, keypoints_color='k', matches_color=None, only_matches=False, alignment='horizontal') [source]\n \nPlot matched features.  Parameters \n \naxmatplotlib.axes.Axes \n\nMatches and image are drawn in this ax.  \nimage1(N, M [, 3]) array \n\nFirst grayscale or color image.  \nimage2(N, M [, 3]) array \n\nSecond grayscale or color image.  \nkeypoints1(K1, 2) array \n\nFirst keypoint coordinates as (row, col).  \nkeypoints2(K2, 2) array \n\nSecond keypoint coordinates as (row, col).  \nmatches(Q, 2) array \n\nIndices of corresponding matches in first and second set of descriptors, where matches[:, 0] denote the indices in the first and matches[:, 1] the indices in the second set of descriptors.  \nkeypoints_colormatplotlib color, optional \n\nColor for keypoint locations.  \nmatches_colormatplotlib color, optional \n\nColor for lines which connect keypoint matches. By default the color is chosen randomly.  \nonly_matchesbool, optional \n\nWhether to only plot matches and not plot the keypoint locations.  \nalignment{\u2018horizontal\u2019, \u2018vertical\u2019}, optional \n\nWhether to show images side by side, 'horizontal', or one above the other, 'vertical'.     \n"}, {"name": "feature.register_translation()", "path": "api/skimage.feature#skimage.feature.register_translation", "type": "feature", "text": " \nskimage.feature.register_translation(src_image, target_image, upsample_factor=1, space='real', return_error=True) [source]\n \nDeprecated function. Use skimage.registration.phase_cross_correlation instead. \n"}, {"name": "feature.shape_index()", "path": "api/skimage.feature#skimage.feature.shape_index", "type": "feature", "text": " \nskimage.feature.shape_index(image, sigma=1, mode='constant', cval=0) [source]\n \nCompute the shape index. The shape index, as defined by Koenderink & van Doorn [1], is a single valued measure of local curvature, assuming the image as a 3D plane with intensities representing heights. It is derived from the eigen values of the Hessian, and its value ranges from -1 to 1 (and is undefined (=NaN) in flat regions), with following ranges representing following shapes:  Ranges of the shape index and corresponding shapes.  \nInterval (s in \u2026) Shape   \n[ -1, -7/8) Spherical cup  \n[-7/8, -5/8) Through  \n[-5/8, -3/8) Rut  \n[-3/8, -1/8) Saddle rut  \n[-1/8, +1/8) Saddle  \n[+1/8, +3/8) Saddle ridge  \n[+3/8, +5/8) Ridge  \n[+5/8, +7/8) Dome  \n[+7/8, +1] Spherical cap    Parameters \n \nimagendarray \n\nInput image.  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, which is used for smoothing the input data before Hessian eigen value calculation.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nsndarray \n\nShape index     References  \n1  \nKoenderink, J. J. & van Doorn, A. J., \u201cSurface shape and curvature scales\u201d, Image and Vision Computing, 1992, 10, 557-564. DOI:10.1016/0262-8856(92)90076-F   Examples >>> from skimage.feature import shape_index\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 4\n>>> s = shape_index(square, sigma=0.1)\n>>> s\narray([[ nan,  nan, -0.5,  nan,  nan],\n       [ nan, -0. ,  nan, -0. ,  nan],\n       [-0.5,  nan, -1. ,  nan, -0.5],\n       [ nan, -0. ,  nan, -0. ,  nan],\n       [ nan,  nan, -0.5,  nan,  nan]])\n \n"}, {"name": "feature.structure_tensor()", "path": "api/skimage.feature#skimage.feature.structure_tensor", "type": "feature", "text": " \nskimage.feature.structure_tensor(image, sigma=1, mode='constant', cval=0, order=None) [source]\n \nCompute structure tensor using sum of squared differences. The (2-dimensional) structure tensor A is defined as: A = [Arr Arc]\n    [Arc Acc]\n which is approximated by the weighted sum of squared differences in a local window around each pixel in the image. This formula can be extended to a larger number of dimensions (see [1]).  Parameters \n \nimagendarray \n\nInput image.  \nsigmafloat, optional \n\nStandard deviation used for the Gaussian kernel, which is used as a weighting function for the local summation of squared differences.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.  \norder{\u2018rc\u2019, \u2018xy\u2019}, optional \n\nNOTE: Only applies in 2D. Higher dimensions must always use \u2018rc\u2019 order. This parameter allows for the use of reverse or forward order of the image axes in gradient computation. \u2018rc\u2019 indicates the use of the first axis initially (Arr, Arc, Acc), whilst \u2018xy\u2019 indicates the usage of the last axis initially (Axx, Axy, Ayy).    Returns \n \nA_elemslist of ndarray \n\nUpper-diagonal elements of the structure tensor for each pixel in the input image.      See also  \nstructure_tensor_eigenvalues\n\n  References  \n1  \nhttps://en.wikipedia.org/wiki/Structure_tensor   Examples >>> from skimage.feature import structure_tensor\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 1\n>>> Arr, Arc, Acc = structure_tensor(square, sigma=0.1, order='rc')\n>>> Acc\narray([[0., 0., 0., 0., 0.],\n       [0., 1., 0., 1., 0.],\n       [0., 4., 0., 4., 0.],\n       [0., 1., 0., 1., 0.],\n       [0., 0., 0., 0., 0.]])\n \n"}, {"name": "feature.structure_tensor_eigenvalues()", "path": "api/skimage.feature#skimage.feature.structure_tensor_eigenvalues", "type": "feature", "text": " \nskimage.feature.structure_tensor_eigenvalues(A_elems) [source]\n \nCompute eigenvalues of structure tensor.  Parameters \n \nA_elemslist of ndarray \n\nThe upper-diagonal elements of the structure tensor, as returned by structure_tensor.    Returns \n ndarray\n\nThe eigenvalues of the structure tensor, in decreasing order. The eigenvalues are the leading dimension. That is, the coordinate [i, j, k] corresponds to the ith-largest eigenvalue at position (j, k).      See also  \nstructure_tensor\n\n  Examples >>> from skimage.feature import structure_tensor\n>>> from skimage.feature import structure_tensor_eigenvalues\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 1\n>>> A_elems = structure_tensor(square, sigma=0.1, order='rc')\n>>> structure_tensor_eigenvalues(A_elems)[0]\narray([[0., 0., 0., 0., 0.],\n       [0., 2., 4., 2., 0.],\n       [0., 4., 0., 4., 0.],\n       [0., 2., 4., 2., 0.],\n       [0., 0., 0., 0., 0.]])\n \n"}, {"name": "feature.structure_tensor_eigvals()", "path": "api/skimage.feature#skimage.feature.structure_tensor_eigvals", "type": "feature", "text": " \nskimage.feature.structure_tensor_eigvals(Axx, Axy, Ayy) [source]\n \nCompute eigenvalues of structure tensor.  Parameters \n \nAxxndarray \n\nElement of the structure tensor for each pixel in the input image.  \nAxyndarray \n\nElement of the structure tensor for each pixel in the input image.  \nAyyndarray \n\nElement of the structure tensor for each pixel in the input image.    Returns \n \nl1ndarray \n\nLarger eigen value for each input matrix.  \nl2ndarray \n\nSmaller eigen value for each input matrix.     Examples >>> from skimage.feature import structure_tensor, structure_tensor_eigvals\n>>> square = np.zeros((5, 5))\n>>> square[2, 2] = 1\n>>> Arr, Arc, Acc = structure_tensor(square, sigma=0.1, order='rc')\n>>> structure_tensor_eigvals(Acc, Arc, Arr)[0]\narray([[0., 0., 0., 0., 0.],\n       [0., 2., 4., 2., 0.],\n       [0., 4., 0., 4., 0.],\n       [0., 2., 4., 2., 0.],\n       [0., 0., 0., 0., 0.]])\n \n"}, {"name": "filters", "path": "api/skimage.filters", "type": "filters", "text": "Module: filters  \nskimage.filters.apply_hysteresis_threshold(\u2026) Apply hysteresis thresholding to image.  \nskimage.filters.correlate_sparse(image, kernel) Compute valid cross-correlation of padded_array and kernel.  \nskimage.filters.difference_of_gaussians(\u2026) Find features between low_sigma and high_sigma in size.  \nskimage.filters.farid(image, *[, mask]) Find the edge magnitude using the Farid transform.  \nskimage.filters.farid_h(image, *[, mask]) Find the horizontal edges of an image using the Farid transform.  \nskimage.filters.farid_v(image, *[, mask]) Find the vertical edges of an image using the Farid transform.  \nskimage.filters.frangi(image[, sigmas, \u2026]) Filter an image with the Frangi vesselness filter.  \nskimage.filters.gabor(image, frequency[, \u2026]) Return real and imaginary responses to Gabor filter.  \nskimage.filters.gabor_kernel(frequency[, \u2026]) Return complex 2D Gabor filter kernel.  \nskimage.filters.gaussian(image[, sigma, \u2026]) Multi-dimensional Gaussian filter.  \nskimage.filters.hessian(image[, sigmas, \u2026]) Filter an image with the Hybrid Hessian filter.  \nskimage.filters.inverse(data[, \u2026]) Apply the filter in reverse to the given data.  \nskimage.filters.laplace(image[, ksize, mask]) Find the edges of an image using the Laplace operator.  \nskimage.filters.median(image[, selem, out, \u2026]) Return local median of an image.  \nskimage.filters.meijering(image[, sigmas, \u2026]) Filter an image with the Meijering neuriteness filter.  \nskimage.filters.prewitt(image[, mask, axis, \u2026]) Find the edge magnitude using the Prewitt transform.  \nskimage.filters.prewitt_h(image[, mask]) Find the horizontal edges of an image using the Prewitt transform.  \nskimage.filters.prewitt_v(image[, mask]) Find the vertical edges of an image using the Prewitt transform.  \nskimage.filters.rank_order(image) Return an image of the same shape where each pixel is the index of the pixel value in the ascending order of the unique values of image, aka the rank-order value.  \nskimage.filters.roberts(image[, mask]) Find the edge magnitude using Roberts\u2019 cross operator.  \nskimage.filters.roberts_neg_diag(image[, mask]) Find the cross edges of an image using the Roberts\u2019 Cross operator.  \nskimage.filters.roberts_pos_diag(image[, mask]) Find the cross edges of an image using Roberts\u2019 cross operator.  \nskimage.filters.sato(image[, sigmas, \u2026]) Filter an image with the Sato tubeness filter.  \nskimage.filters.scharr(image[, mask, axis, \u2026]) Find the edge magnitude using the Scharr transform.  \nskimage.filters.scharr_h(image[, mask]) Find the horizontal edges of an image using the Scharr transform.  \nskimage.filters.scharr_v(image[, mask]) Find the vertical edges of an image using the Scharr transform.  \nskimage.filters.sobel(image[, mask, axis, \u2026]) Find edges in an image using the Sobel filter.  \nskimage.filters.sobel_h(image[, mask]) Find the horizontal edges of an image using the Sobel transform.  \nskimage.filters.sobel_v(image[, mask]) Find the vertical edges of an image using the Sobel transform.  \nskimage.filters.threshold_isodata([image, \u2026]) Return threshold value(s) based on ISODATA method.  \nskimage.filters.threshold_li(image, *[, \u2026]) Compute threshold value by Li\u2019s iterative Minimum Cross Entropy method.  \nskimage.filters.threshold_local(image, \u2026) Compute a threshold mask image based on local pixel neighborhood.  \nskimage.filters.threshold_mean(image) Return threshold value based on the mean of grayscale values.  \nskimage.filters.threshold_minimum([image, \u2026]) Return threshold value based on minimum method.  \nskimage.filters.threshold_multiotsu(image[, \u2026]) Generate classes-1 threshold values to divide gray levels in image.  \nskimage.filters.threshold_niblack(image[, \u2026]) Applies Niblack local threshold to an array.  \nskimage.filters.threshold_otsu([image, \u2026]) Return threshold value based on Otsu\u2019s method.  \nskimage.filters.threshold_sauvola(image[, \u2026]) Applies Sauvola local threshold to an array.  \nskimage.filters.threshold_triangle(image[, \u2026]) Return threshold value based on the triangle algorithm.  \nskimage.filters.threshold_yen([image, \u2026]) Return threshold value based on Yen\u2019s method.  \nskimage.filters.try_all_threshold(image[, \u2026]) Returns a figure comparing the outputs of different thresholding methods.  \nskimage.filters.unsharp_mask(image[, \u2026]) Unsharp masking filter.  \nskimage.filters.wiener(data[, \u2026]) Minimum Mean Square Error (Wiener) inverse filter.  \nskimage.filters.window(window_type, shape[, \u2026]) Return an n-dimensional window of a given size and dimensionality.  \nskimage.filters.LPIFilter2D(\u2026) Linear Position-Invariant Filter (2-dimensional)  \nskimage.filters.rank    apply_hysteresis_threshold  \nskimage.filters.apply_hysteresis_threshold(image, low, high) [source]\n \nApply hysteresis thresholding to image. This algorithm finds regions where image is greater than high OR image is greater than low and that region is connected to a region greater than high.  Parameters \n \nimagearray, shape (M,[ N, \u2026, P]) \n\nGrayscale input image.  \nlowfloat, or array of same shape as image \n\nLower threshold.  \nhighfloat, or array of same shape as image \n\nHigher threshold.    Returns \n \nthresholdedarray of bool, same shape as image \n\nArray in which True indicates the locations where image was above the hysteresis threshold.     References  \n1  \nJ. Canny. A computational approach to edge detection. IEEE Transactions on Pattern Analysis and Machine Intelligence. 1986; vol. 8, pp.679-698. DOI:10.1109/TPAMI.1986.4767851   Examples >>> image = np.array([1, 2, 3, 2, 1, 2, 1, 3, 2])\n>>> apply_hysteresis_threshold(image, 1.5, 2.5).astype(int)\narray([0, 1, 1, 1, 0, 0, 0, 1, 1])\n \n correlate_sparse  \nskimage.filters.correlate_sparse(image, kernel, mode='reflect') [source]\n \nCompute valid cross-correlation of padded_array and kernel. This function is fast when kernel is large with many zeros. See scipy.ndimage.correlate for a description of cross-correlation.  Parameters \n \nimagendarray, dtype float, shape (M, N,[ \u2026,] P) \n\nThe input array. If mode is \u2018valid\u2019, this array should already be padded, as a margin of the same shape as kernel will be stripped off.  \nkernelndarray, dtype float shape (Q, R,[ \u2026,] S) \n\nThe kernel to be correlated. Must have the same number of dimensions as padded_array. For high performance, it should be sparse (few nonzero entries).  \nmodestring, optional \n\nSee scipy.ndimage.correlate for valid modes. Additionally, mode \u2018valid\u2019 is accepted, in which case no padding is applied and the result is the result for the smaller image for which the kernel is entirely inside the original data.    Returns \n \nresultarray of float, shape (M, N,[ \u2026,] P) \n\nThe result of cross-correlating image with kernel. If mode \u2018valid\u2019 is used, the resulting shape is (M-Q+1, N-R+1,[ \u2026,] P-S+1).     \n difference_of_gaussians  \nskimage.filters.difference_of_gaussians(image, low_sigma, high_sigma=None, *, mode='nearest', cval=0, multichannel=False, truncate=4.0) [source]\n \nFind features between low_sigma and high_sigma in size. This function uses the Difference of Gaussians method for applying band-pass filters to multi-dimensional arrays. The input array is blurred with two Gaussian kernels of differing sigmas to produce two intermediate, filtered images. The more-blurred image is then subtracted from the less-blurred image. The final output image will therefore have had high-frequency components attenuated by the smaller-sigma Gaussian, and low frequency components will have been removed due to their presence in the more-blurred intermediate.  Parameters \n \nimagendarray \n\nInput array to filter.  \nlow_sigmascalar or sequence of scalars \n\nStandard deviation(s) for the Gaussian kernel with the smaller sigmas across all axes. The standard deviations are given for each axis as a sequence, or as a single number, in which case the single number is used as the standard deviation value for all axes.  \nhigh_sigmascalar or sequence of scalars, optional (default is None) \n\nStandard deviation(s) for the Gaussian kernel with the larger sigmas across all axes. The standard deviations are given for each axis as a sequence, or as a single number, in which case the single number is used as the standard deviation value for all axes. If None is given (default), sigmas for all axes are calculated as 1.6 * low_sigma.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.  \ncvalscalar, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0  \nmultichannelbool, optional (default: False) \n\nWhether the last axis of the image is to be interpreted as multiple channels. If True, each channel is filtered separately (channels are not mixed together).  \ntruncatefloat, optional (default is 4.0) \n\nTruncate the filter at this many standard deviations.    Returns \n \nfiltered_imagendarray \n\nthe filtered array.      See also  \nskimage.feature.blog_dog \n  Notes This function will subtract an array filtered with a Gaussian kernel with sigmas given by high_sigma from an array filtered with a Gaussian kernel with sigmas provided by low_sigma. The values for high_sigma must always be greater than or equal to the corresponding values in low_sigma, or a ValueError will be raised. When high_sigma is none, the values for high_sigma will be calculated as 1.6x the corresponding values in low_sigma. This ratio was originally proposed by Marr and Hildreth (1980) [1] and is commonly used when approximating the inverted Laplacian of Gaussian, which is used in edge and blob detection. Input image is converted according to the conventions of img_as_float. Except for sigma values, all parameters are used for both filters. References  \n1  \nMarr, D. and Hildreth, E. Theory of Edge Detection. Proc. R. Soc. Lond. Series B 207, 187-217 (1980). https://doi.org/10.1098/rspb.1980.0020   Examples Apply a simple Difference of Gaussians filter to a color image: >>> from skimage.data import astronaut\n>>> from skimage.filters import difference_of_gaussians\n>>> filtered_image = difference_of_gaussians(astronaut(), 2, 10,\n...                                          multichannel=True)\n Apply a Laplacian of Gaussian filter as approximated by the Difference of Gaussians filter: >>> filtered_image = difference_of_gaussians(astronaut(), 2,\n...                                          multichannel=True)\n Apply a Difference of Gaussians filter to a grayscale image using different sigma values for each axis: >>> from skimage.data import camera\n>>> filtered_image = difference_of_gaussians(camera(), (2,5), (3,20))\n \n farid  \nskimage.filters.farid(image, *, mask=None) [source]\n \nFind the edge magnitude using the Farid transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Farid edge map.      See also  \nsobel, prewitt, canny \n  Notes Take the square root of the sum of the squares of the horizontal and vertical derivatives to get a magnitude that is somewhat insensitive to direction. Similar to the Scharr operator, this operator is designed with a rotation invariance constraint. References  \n1  \nFarid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional signals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004. DOI:10.1109/TIP.2004.823819  \n2  \nWikipedia, \u201cFarid and Simoncelli Derivatives.\u201d Available at: <https://en.wikipedia.org/wiki/Image_derivatives#Farid_and_Simoncelli_Derivatives>   Examples >>> from skimage import data\n>>> camera = data.camera()\n>>> from skimage import filters\n>>> edges = filters.farid(camera)\n \n farid_h  \nskimage.filters.farid_h(image, *, mask=None) [source]\n \nFind the horizontal edges of an image using the Farid transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Farid edge map.     Notes The kernel was constructed using the 5-tap weights from [1]. References  \n1  \nFarid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional signals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004. DOI:10.1109/TIP.2004.823819  \n2  \nFarid, H. and Simoncelli, E. P. \u201cOptimally rotation-equivariant directional derivative kernels\u201d, In: 7th International Conference on Computer Analysis of Images and Patterns, Kiel, Germany. Sep, 1997.   \n farid_v  \nskimage.filters.farid_v(image, *, mask=None) [source]\n \nFind the vertical edges of an image using the Farid transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Farid edge map.     Notes The kernel was constructed using the 5-tap weights from [1]. References  \n1  \nFarid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional signals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004. DOI:10.1109/TIP.2004.823819   \n frangi  \nskimage.filters.frangi(image, sigmas=range(1, 10, 2), scale_range=None, scale_step=None, alpha=0.5, beta=0.5, gamma=15, black_ridges=True, mode='reflect', cval=0) [source]\n \nFilter an image with the Frangi vesselness filter. This filter can be used to detect continuous ridges, e.g. vessels, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects. Defined only for 2-D and 3-D images. Calculates the eigenvectors of the Hessian to compute the similarity of an image region to vessels, according to the method described in [1].  Parameters \n \nimage(N, M[, P]) ndarray \n\nArray with input image data.  \nsigmasiterable of floats, optional \n\nSigmas used as scales of filter, i.e., np.arange(scale_range[0], scale_range[1], scale_step)  \nscale_range2-tuple of floats, optional \n\nThe range of sigmas used.  \nscale_stepfloat, optional \n\nStep size between sigmas.  \nalphafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a plate-like structure.  \nbetafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a blob-like structure.  \ngammafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to areas of high variance/texture/structure.  \nblack_ridgesboolean, optional \n\nWhen True (the default), the filter detects black ridges; when False, it detects white ridges.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nout(N, M[, P]) ndarray \n\nFiltered image (maximum of pixels across all scales).      See also  \nmeijering\n\n\nsato\n\n\nhessian\n\n  Notes Written by Marc Schrijver, November 2001 Re-Written by D. J. Kroon, University of Twente, May 2009, [2] Adoption of 3D version from D. G. Ellis, Januar 20017, [3] References  \n1  \nFrangi, A. F., Niessen, W. J., Vincken, K. L., & Viergever, M. A. (1998,). Multiscale vessel enhancement filtering. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 130-137). Springer Berlin Heidelberg. DOI:10.1007/BFb0056195  \n2  \nKroon, D. J.: Hessian based Frangi vesselness filter.  \n3  \nEllis, D. G.: https://github.com/ellisdg/frangi3d/tree/master/frangi   \n gabor  \nskimage.filters.gabor(image, frequency, theta=0, bandwidth=1, sigma_x=None, sigma_y=None, n_stds=3, offset=0, mode='reflect', cval=0) [source]\n \nReturn real and imaginary responses to Gabor filter. The real and imaginary parts of the Gabor filter kernel are applied to the image and the response is returned as a pair of arrays. Gabor filter is a linear filter with a Gaussian kernel which is modulated by a sinusoidal plane wave. Frequency and orientation representations of the Gabor filter are similar to those of the human visual system. Gabor filter banks are commonly used in computer vision and image processing. They are especially suitable for edge detection and texture classification.  Parameters \n \nimage2-D array \n\nInput image.  \nfrequencyfloat \n\nSpatial frequency of the harmonic function. Specified in pixels.  \nthetafloat, optional \n\nOrientation in radians. If 0, the harmonic is in the x-direction.  \nbandwidthfloat, optional \n\nThe bandwidth captured by the filter. For fixed bandwidth, sigma_x and sigma_y will decrease with increasing frequency. This value is ignored if sigma_x and sigma_y are set by the user.  \nsigma_x, sigma_yfloat, optional \n\nStandard deviation in x- and y-directions. These directions apply to the kernel before rotation. If theta = pi/2, then the kernel is rotated 90 degrees so that sigma_x controls the vertical direction.  \nn_stdsscalar, optional \n\nThe linear size of the kernel is n_stds (3 by default) standard deviations.  \noffsetfloat, optional \n\nPhase offset of harmonic function in radians.  \nmode{\u2018constant\u2019, \u2018nearest\u2019, \u2018reflect\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional \n\nMode used to convolve image with a kernel, passed to ndi.convolve  \ncvalscalar, optional \n\nValue to fill past edges of input if mode of convolution is \u2018constant\u2019. The parameter is passed to ndi.convolve.    Returns \n \nreal, imagarrays \n\nFiltered images using the real and imaginary parts of the Gabor filter kernel. Images are of the same dimensions as the input one.     References  \n1  \nhttps://en.wikipedia.org/wiki/Gabor_filter  \n2  \nhttps://web.archive.org/web/20180127125930/http://mplab.ucsd.edu/tutorials/gabor.pdf   Examples >>> from skimage.filters import gabor\n>>> from skimage import data, io\n>>> from matplotlib import pyplot as plt  \n >>> image = data.coins()\n>>> # detecting edges in a coin image\n>>> filt_real, filt_imag = gabor(image, frequency=0.6)\n>>> plt.figure()            \n>>> io.imshow(filt_real)    \n>>> io.show()               \n >>> # less sensitivity to finer details with the lower frequency kernel\n>>> filt_real, filt_imag = gabor(image, frequency=0.1)\n>>> plt.figure()            \n>>> io.imshow(filt_real)    \n>>> io.show()               \n \n gabor_kernel  \nskimage.filters.gabor_kernel(frequency, theta=0, bandwidth=1, sigma_x=None, sigma_y=None, n_stds=3, offset=0) [source]\n \nReturn complex 2D Gabor filter kernel. Gabor kernel is a Gaussian kernel modulated by a complex harmonic function. Harmonic function consists of an imaginary sine function and a real cosine function. Spatial frequency is inversely proportional to the wavelength of the harmonic and to the standard deviation of a Gaussian kernel. The bandwidth is also inversely proportional to the standard deviation.  Parameters \n \nfrequencyfloat \n\nSpatial frequency of the harmonic function. Specified in pixels.  \nthetafloat, optional \n\nOrientation in radians. If 0, the harmonic is in the x-direction.  \nbandwidthfloat, optional \n\nThe bandwidth captured by the filter. For fixed bandwidth, sigma_x and sigma_y will decrease with increasing frequency. This value is ignored if sigma_x and sigma_y are set by the user.  \nsigma_x, sigma_yfloat, optional \n\nStandard deviation in x- and y-directions. These directions apply to the kernel before rotation. If theta = pi/2, then the kernel is rotated 90 degrees so that sigma_x controls the vertical direction.  \nn_stdsscalar, optional \n\nThe linear size of the kernel is n_stds (3 by default) standard deviations  \noffsetfloat, optional \n\nPhase offset of harmonic function in radians.    Returns \n \ngcomplex array \n\nComplex filter kernel.     References  \n1  \nhttps://en.wikipedia.org/wiki/Gabor_filter  \n2  \nhttps://web.archive.org/web/20180127125930/http://mplab.ucsd.edu/tutorials/gabor.pdf   Examples >>> from skimage.filters import gabor_kernel\n>>> from skimage import io\n>>> from matplotlib import pyplot as plt  \n >>> gk = gabor_kernel(frequency=0.2)\n>>> plt.figure()        \n>>> io.imshow(gk.real)  \n>>> io.show()           \n >>> # more ripples (equivalent to increasing the size of the\n>>> # Gaussian spread)\n>>> gk = gabor_kernel(frequency=0.2, bandwidth=0.1)\n>>> plt.figure()        \n>>> io.imshow(gk.real)  \n>>> io.show()           \n \n gaussian  \nskimage.filters.gaussian(image, sigma=1, output=None, mode='nearest', cval=0, multichannel=None, preserve_range=False, truncate=4.0) [source]\n \nMulti-dimensional Gaussian filter.  Parameters \n \nimagearray-like \n\nInput image (grayscale or color) to filter.  \nsigmascalar or sequence of scalars, optional \n\nStandard deviation for Gaussian kernel. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.  \noutputarray, optional \n\nThe output parameter passes an array in which to store the filter output.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.  \ncvalscalar, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0  \nmultichannelbool, optional (default: None) \n\nWhether the last axis of the image is to be interpreted as multiple channels. If True, each channel is filtered separately (channels are not mixed together). Only 3 channels are supported. If None, the function will attempt to guess this, and raise a warning if ambiguous, when the array has shape (M, N, 3).  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html  \ntruncatefloat, optional \n\nTruncate the filter at this many standard deviations.    Returns \n \nfiltered_imagendarray \n\nthe filtered array     Notes This function is a wrapper around scipy.ndi.gaussian_filter(). Integer arrays are converted to float. The output should be floating point data type since gaussian converts to float provided image. If output is not provided, another array will be allocated and returned as the result. The multi-dimensional filter is implemented as a sequence of one-dimensional convolution filters. The intermediate arrays are stored in the same data type as the output. Therefore, for output types with a limited precision, the results may be imprecise because intermediate results may be stored with insufficient precision. Examples >>> a = np.zeros((3, 3))\n>>> a[1, 1] = 1\n>>> a\narray([[0., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 0.]])\n>>> gaussian(a, sigma=0.4)  # mild smoothing\narray([[0.00163116, 0.03712502, 0.00163116],\n       [0.03712502, 0.84496158, 0.03712502],\n       [0.00163116, 0.03712502, 0.00163116]])\n>>> gaussian(a, sigma=1)  # more smoothing\narray([[0.05855018, 0.09653293, 0.05855018],\n       [0.09653293, 0.15915589, 0.09653293],\n       [0.05855018, 0.09653293, 0.05855018]])\n>>> # Several modes are possible for handling boundaries\n>>> gaussian(a, sigma=1, mode='reflect')\narray([[0.08767308, 0.12075024, 0.08767308],\n       [0.12075024, 0.16630671, 0.12075024],\n       [0.08767308, 0.12075024, 0.08767308]])\n>>> # For RGB images, each is filtered separately\n>>> from skimage.data import astronaut\n>>> image = astronaut()\n>>> filtered_img = gaussian(image, sigma=1, multichannel=True)\n \n hessian  \nskimage.filters.hessian(image, sigmas=range(1, 10, 2), scale_range=None, scale_step=None, alpha=0.5, beta=0.5, gamma=15, black_ridges=True, mode=None, cval=0) [source]\n \nFilter an image with the Hybrid Hessian filter. This filter can be used to detect continuous edges, e.g. vessels, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects. Defined only for 2-D and 3-D images. Almost equal to Frangi filter, but uses alternative method of smoothing. Refer to [1] to find the differences between Frangi and Hessian filters.  Parameters \n \nimage(N, M[, P]) ndarray \n\nArray with input image data.  \nsigmasiterable of floats, optional \n\nSigmas used as scales of filter, i.e., np.arange(scale_range[0], scale_range[1], scale_step)  \nscale_range2-tuple of floats, optional \n\nThe range of sigmas used.  \nscale_stepfloat, optional \n\nStep size between sigmas.  \nbetafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a blob-like structure.  \ngammafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to areas of high variance/texture/structure.  \nblack_ridgesboolean, optional \n\nWhen True (the default), the filter detects black ridges; when False, it detects white ridges.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nout(N, M[, P]) ndarray \n\nFiltered image (maximum of pixels across all scales).      See also  \nmeijering\n\n\nsato\n\n\nfrangi\n\n  Notes Written by Marc Schrijver (November 2001) Re-Written by D. J. Kroon University of Twente (May 2009) [2] References  \n1  \nNg, C. C., Yap, M. H., Costen, N., & Li, B. (2014,). Automatic wrinkle detection using hybrid Hessian filter. In Asian Conference on Computer Vision (pp. 609-622). Springer International Publishing. DOI:10.1007/978-3-319-16811-1_40  \n2  \nKroon, D. J.: Hessian based Frangi vesselness filter.   \n inverse  \nskimage.filters.inverse(data, impulse_response=None, filter_params={}, max_gain=2, predefined_filter=None) [source]\n \nApply the filter in reverse to the given data.  Parameters \n \ndata(M,N) ndarray \n\nInput data.  \nimpulse_responsecallable f(r, c, **filter_params) \n\nImpulse response of the filter. See LPIFilter2D.__init__.  \nfilter_paramsdict \n\nAdditional keyword parameters to the impulse_response function.  \nmax_gainfloat \n\nLimit the filter gain. Often, the filter contains zeros, which would cause the inverse filter to have infinite gain. High gain causes amplification of artefacts, so a conservative limit is recommended.    Other Parameters \n \npredefined_filterLPIFilter2D \n\nIf you need to apply the same filter multiple times over different images, construct the LPIFilter2D and specify it here.     \n laplace  \nskimage.filters.laplace(image, ksize=3, mask=None) [source]\n \nFind the edges of an image using the Laplace operator.  Parameters \n \nimagendarray \n\nImage to process.  \nksizeint, optional \n\nDefine the size of the discrete Laplacian operator such that it will have a size of (ksize,) * image.ndim.  \nmaskndarray, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutputndarray \n\nThe Laplace edge map.     Notes The Laplacian operator is generated using the function skimage.restoration.uft.laplacian(). \n median  \nskimage.filters.median(image, selem=None, out=None, mode='nearest', cval=0.0, behavior='ndimage') [source]\n \nReturn local median of an image.  Parameters \n \nimagearray-like \n\nInput image.  \nselemndarray, optional \n\nIf behavior=='rank', selem is a 2-D array of 1\u2019s and 0\u2019s. If behavior=='ndimage', selem is a N-D array of 1\u2019s and 0\u2019s with the same number of dimension than image. If None, selem will be a N-D array with 3 elements for each dimension (e.g., vector, square, cube, etc.)  \noutndarray, (same dtype as image), optional \n\nIf None, a new array is allocated.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018nearest\u2019, \u2018mirror\u2019,\u2019\u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.  New in version 0.15: mode is used when behavior='ndimage'.   \ncvalscalar, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0  New in version 0.15: cval was added in 0.15 is used when behavior='ndimage'.   \nbehavior{\u2018ndimage\u2019, \u2018rank\u2019}, optional \n\nEither to use the old behavior (i.e., < 0.15) or the new behavior. The old behavior will call the skimage.filters.rank.median(). The new behavior will call the scipy.ndimage.median_filter(). Default is \u2018ndimage\u2019.  New in version 0.15: behavior is introduced in 0.15   Changed in version 0.16: Default behavior has been changed from \u2018rank\u2019 to \u2018ndimage\u2019     Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.      See also  \nskimage.filters.rank.median\n\n\nRank-based implementation of the median filtering offering more flexibility with additional parameters but dedicated for unsigned integer images.    Examples >>> from skimage import data\n>>> from skimage.morphology import disk\n>>> from skimage.filters import median\n>>> img = data.camera()\n>>> med = median(img, disk(5))\n \n meijering  \nskimage.filters.meijering(image, sigmas=range(1, 10, 2), alpha=None, black_ridges=True, mode='reflect', cval=0) [source]\n \nFilter an image with the Meijering neuriteness filter. This filter can be used to detect continuous ridges, e.g. neurites, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects. Calculates the eigenvectors of the Hessian to compute the similarity of an image region to neurites, according to the method described in [1].  Parameters \n \nimage(N, M[, \u2026, P]) ndarray \n\nArray with input image data.  \nsigmasiterable of floats, optional \n\nSigmas used as scales of filter  \nalphafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a plate-like structure.  \nblack_ridgesboolean, optional \n\nWhen True (the default), the filter detects black ridges; when False, it detects white ridges.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nout(N, M[, \u2026, P]) ndarray \n\nFiltered image (maximum of pixels across all scales).      See also  \nsato\n\n\nfrangi\n\n\nhessian\n\n  References  \n1  \nMeijering, E., Jacob, M., Sarria, J. C., Steiner, P., Hirling, H., Unser, M. (2004). Design and validation of a tool for neurite tracing and analysis in fluorescence microscopy images. Cytometry Part A, 58(2), 167-176. DOI:10.1002/cyto.a.20022   \n prewitt  \nskimage.filters.prewitt(image, mask=None, *, axis=None, mode='reflect', cval=0.0) [source]\n \nFind the edge magnitude using the Prewitt transform.  Parameters \n \nimagearray \n\nThe input image.  \nmaskarray of bool, optional \n\nClip the output image to this mask. (Values where mask=0 will be set to 0.)  \naxisint or sequence of int, optional \n\nCompute the edge filter along this axis. If not provided, the edge magnitude is computed. This is defined as: prw_mag = np.sqrt(sum([prewitt(image, axis=i)**2\n                       for i in range(image.ndim)]) / image.ndim)\n The magnitude is also computed if axis is a sequence.  \nmodestr or sequence of str, optional \n\nThe boundary mode for the convolution. See scipy.ndimage.convolve for a description of the modes. This can be either a single boundary mode or one boundary mode per axis.  \ncvalfloat, optional \n\nWhen mode is 'constant', this is the constant used in values outside the boundary of the image data.    Returns \n \noutputarray of float \n\nThe Prewitt edge map.      See also  \nsobel, scharr\n\n  Notes The edge magnitude depends slightly on edge directions, since the approximation of the gradient operator by the Prewitt operator is not completely rotation invariant. For a better rotation invariance, the Scharr operator should be used. The Sobel operator has a better rotation invariance than the Prewitt operator, but a worse rotation invariance than the Scharr operator. Examples >>> from skimage import data\n>>> from skimage import filters\n>>> camera = data.camera()\n>>> edges = filters.prewitt(camera)\n \n prewitt_h  \nskimage.filters.prewitt_h(image, mask=None) [source]\n \nFind the horizontal edges of an image using the Prewitt transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Prewitt edge map.     Notes We use the following kernel:  1/3   1/3   1/3\n  0     0     0\n-1/3  -1/3  -1/3\n \n prewitt_v  \nskimage.filters.prewitt_v(image, mask=None) [source]\n \nFind the vertical edges of an image using the Prewitt transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Prewitt edge map.     Notes We use the following kernel: 1/3   0  -1/3\n1/3   0  -1/3\n1/3   0  -1/3\n \n rank_order  \nskimage.filters.rank_order(image) [source]\n \nReturn an image of the same shape where each pixel is the index of the pixel value in the ascending order of the unique values of image, aka the rank-order value.  Parameters \n \nimagendarray \n  Returns \n \nlabelsndarray of type np.uint32, of shape image.shape \n\nNew array where each pixel has the rank-order value of the corresponding pixel in image. Pixel values are between 0 and n - 1, where n is the number of distinct unique values in image.  \noriginal_values1-D ndarray \n\nUnique original values of image     Examples >>> a = np.array([[1, 4, 5], [4, 4, 1], [5, 1, 1]])\n>>> a\narray([[1, 4, 5],\n       [4, 4, 1],\n       [5, 1, 1]])\n>>> rank_order(a)\n(array([[0, 1, 2],\n       [1, 1, 0],\n       [2, 0, 0]], dtype=uint32), array([1, 4, 5]))\n>>> b = np.array([-1., 2.5, 3.1, 2.5])\n>>> rank_order(b)\n(array([0, 1, 2, 1], dtype=uint32), array([-1. ,  2.5,  3.1]))\n \n roberts  \nskimage.filters.roberts(image, mask=None) [source]\n \nFind the edge magnitude using Roberts\u2019 cross operator.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Roberts\u2019 Cross edge map.      See also  \nsobel, scharr, prewitt, feature.canny \n  Examples >>> from skimage import data\n>>> camera = data.camera()\n>>> from skimage import filters\n>>> edges = filters.roberts(camera)\n \n roberts_neg_diag  \nskimage.filters.roberts_neg_diag(image, mask=None) [source]\n \nFind the cross edges of an image using the Roberts\u2019 Cross operator. The kernel is applied to the input image to produce separate measurements of the gradient component one orientation.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Robert\u2019s edge map.     Notes We use the following kernel:  0   1\n-1   0\n \n roberts_pos_diag  \nskimage.filters.roberts_pos_diag(image, mask=None) [source]\n \nFind the cross edges of an image using Roberts\u2019 cross operator. The kernel is applied to the input image to produce separate measurements of the gradient component one orientation.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Robert\u2019s edge map.     Notes We use the following kernel: 1   0\n0  -1\n \n sato  \nskimage.filters.sato(image, sigmas=range(1, 10, 2), black_ridges=True, mode=None, cval=0) [source]\n \nFilter an image with the Sato tubeness filter. This filter can be used to detect continuous ridges, e.g. tubes, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects. Defined only for 2-D and 3-D images. Calculates the eigenvectors of the Hessian to compute the similarity of an image region to tubes, according to the method described in [1].  Parameters \n \nimage(N, M[, P]) ndarray \n\nArray with input image data.  \nsigmasiterable of floats, optional \n\nSigmas used as scales of filter.  \nblack_ridgesboolean, optional \n\nWhen True (the default), the filter detects black ridges; when False, it detects white ridges.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nout(N, M[, P]) ndarray \n\nFiltered image (maximum of pixels across all scales).      See also  \nmeijering\n\n\nfrangi\n\n\nhessian\n\n  References  \n1  \nSato, Y., Nakajima, S., Shiraga, N., Atsumi, H., Yoshida, S., Koller, T., \u2026, Kikinis, R. (1998). Three-dimensional multi-scale line filter for segmentation and visualization of curvilinear structures in medical images. Medical image analysis, 2(2), 143-168. DOI:10.1016/S1361-8415(98)80009-1   \n scharr  \nskimage.filters.scharr(image, mask=None, *, axis=None, mode='reflect', cval=0.0) [source]\n \nFind the edge magnitude using the Scharr transform.  Parameters \n \nimagearray \n\nThe input image.  \nmaskarray of bool, optional \n\nClip the output image to this mask. (Values where mask=0 will be set to 0.)  \naxisint or sequence of int, optional \n\nCompute the edge filter along this axis. If not provided, the edge magnitude is computed. This is defined as: sch_mag = np.sqrt(sum([scharr(image, axis=i)**2\n                       for i in range(image.ndim)]) / image.ndim)\n The magnitude is also computed if axis is a sequence.  \nmodestr or sequence of str, optional \n\nThe boundary mode for the convolution. See scipy.ndimage.convolve for a description of the modes. This can be either a single boundary mode or one boundary mode per axis.  \ncvalfloat, optional \n\nWhen mode is 'constant', this is the constant used in values outside the boundary of the image data.    Returns \n \noutputarray of float \n\nThe Scharr edge map.      See also  \nsobel, prewitt, canny \n  Notes The Scharr operator has a better rotation invariance than other edge filters such as the Sobel or the Prewitt operators. References  \n1  \nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives.  \n2  \nhttps://en.wikipedia.org/wiki/Sobel_operator#Alternative_operators   Examples >>> from skimage import data\n>>> from skimage import filters\n>>> camera = data.camera()\n>>> edges = filters.scharr(camera)\n \n scharr_h  \nskimage.filters.scharr_h(image, mask=None) [source]\n \nFind the horizontal edges of an image using the Scharr transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Scharr edge map.     Notes We use the following kernel:  3   10   3\n 0    0   0\n-3  -10  -3\n References  \n1  \nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives.   \n scharr_v  \nskimage.filters.scharr_v(image, mask=None) [source]\n \nFind the vertical edges of an image using the Scharr transform.  Parameters \n \nimage2-D array \n\nImage to process  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Scharr edge map.     Notes We use the following kernel:  3   0   -3\n10   0  -10\n 3   0   -3\n References  \n1  \nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives.   \n sobel  \nskimage.filters.sobel(image, mask=None, *, axis=None, mode='reflect', cval=0.0) [source]\n \nFind edges in an image using the Sobel filter.  Parameters \n \nimagearray \n\nThe input image.  \nmaskarray of bool, optional \n\nClip the output image to this mask. (Values where mask=0 will be set to 0.)  \naxisint or sequence of int, optional \n\nCompute the edge filter along this axis. If not provided, the edge magnitude is computed. This is defined as: sobel_mag = np.sqrt(sum([sobel(image, axis=i)**2\n                         for i in range(image.ndim)]) / image.ndim)\n The magnitude is also computed if axis is a sequence.  \nmodestr or sequence of str, optional \n\nThe boundary mode for the convolution. See scipy.ndimage.convolve for a description of the modes. This can be either a single boundary mode or one boundary mode per axis.  \ncvalfloat, optional \n\nWhen mode is 'constant', this is the constant used in values outside the boundary of the image data.    Returns \n \noutputarray of float \n\nThe Sobel edge map.      See also  \nscharr, prewitt, canny \n  References  \n1  \nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives.  \n2  \nhttps://en.wikipedia.org/wiki/Sobel_operator   Examples >>> from skimage import data\n>>> from skimage import filters\n>>> camera = data.camera()\n>>> edges = filters.sobel(camera)\n \n Examples using skimage.filters.sobel\n \n  Flood Fill   sobel_h  \nskimage.filters.sobel_h(image, mask=None) [source]\n \nFind the horizontal edges of an image using the Sobel transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Sobel edge map.     Notes We use the following kernel:  1   2   1\n 0   0   0\n-1  -2  -1\n \n sobel_v  \nskimage.filters.sobel_v(image, mask=None) [source]\n \nFind the vertical edges of an image using the Sobel transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Sobel edge map.     Notes We use the following kernel: 1   0  -1\n2   0  -2\n1   0  -1\n \n threshold_isodata  \nskimage.filters.threshold_isodata(image=None, nbins=256, return_all=False, *, hist=None) [source]\n \nReturn threshold value(s) based on ISODATA method. Histogram-based threshold, known as Ridler-Calvard method or inter-means. Threshold values returned satisfy the following equality: threshold = (image[image <= threshold].mean() +\n             image[image > threshold].mean()) / 2.0\n That is, returned thresholds are intensities that separate the image into two groups of pixels, where the threshold intensity is midway between the mean intensities of these groups. For integer images, the above equality holds to within one; for floating- point images, the equality holds to within the histogram bin-width. Either image or hist must be provided. In case hist is given, the actual histogram of the image is ignored.  Parameters \n \nimage(N, M) ndarray, optional \n\nInput image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.  \nreturn_allbool, optional \n\nIf False (default), return only the lowest threshold that satisfies the above equality. If True, return all valid thresholds.  \nhistarray, or 2-tuple of arrays, optional \n\nHistogram to determine the threshold from and a corresponding array of bin center intensities. Alternatively, only the histogram can be passed.    Returns \n \nthresholdfloat or int or array \n\nThreshold value(s).     References  \n1  \nRidler, TW & Calvard, S (1978), \u201cPicture thresholding using an iterative selection method\u201d IEEE Transactions on Systems, Man and Cybernetics 8: 630-632, DOI:10.1109/TSMC.1978.4310039  \n2  \nSezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and Quantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1): 146-165, http://www.busim.ee.boun.edu.tr/~sankur/SankurFolder/Threshold_survey.pdf DOI:10.1117/1.1631315  \n3  \nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold   Examples >>> from skimage.data import coins\n>>> image = coins()\n>>> thresh = threshold_isodata(image)\n>>> binary = image > thresh\n \n threshold_li  \nskimage.filters.threshold_li(image, *, tolerance=None, initial_guess=None, iter_callback=None) [source]\n \nCompute threshold value by Li\u2019s iterative Minimum Cross Entropy method.  Parameters \n \nimagendarray \n\nInput image.  \ntolerancefloat, optional \n\nFinish the computation when the change in the threshold in an iteration is less than this value. By default, this is half the smallest difference between intensity values in image.  \ninitial_guessfloat or Callable[[array[float]], float], optional \n\nLi\u2019s iterative method uses gradient descent to find the optimal threshold. If the image intensity histogram contains more than two modes (peaks), the gradient descent could get stuck in a local optimum. An initial guess for the iteration can help the algorithm find the globally-optimal threshold. A float value defines a specific start point, while a callable should take in an array of image intensities and return a float value. Example valid callables include numpy.mean (default), lambda arr: numpy.quantile(arr, 0.95), or even skimage.filters.threshold_otsu().  \niter_callbackCallable[[float], Any], optional \n\nA function that will be called on the threshold at every iteration of the algorithm.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.     References  \n1  \nLi C.H. and Lee C.K. (1993) \u201cMinimum Cross Entropy Thresholding\u201d Pattern Recognition, 26(4): 617-625 DOI:10.1016/0031-3203(93)90115-D  \n2  \nLi C.H. and Tam P.K.S. (1998) \u201cAn Iterative Algorithm for Minimum Cross Entropy Thresholding\u201d Pattern Recognition Letters, 18(8): 771-776 DOI:10.1016/S0167-8655(98)00057-9  \n3  \nSezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and Quantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1): 146-165 DOI:10.1117/1.1631315  \n4  \nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_li(image)\n>>> binary = image > thresh\n \n threshold_local  \nskimage.filters.threshold_local(image, block_size, method='gaussian', offset=0, mode='reflect', param=None, cval=0) [source]\n \nCompute a threshold mask image based on local pixel neighborhood. Also known as adaptive or dynamic thresholding. The threshold value is the weighted mean for the local neighborhood of a pixel subtracted by a constant. Alternatively the threshold can be determined dynamically by a given function, using the \u2018generic\u2019 method.  Parameters \n \nimage(N, M) ndarray \n\nInput image.  \nblock_sizeint \n\nOdd size of pixel neighborhood which is used to calculate the threshold value (e.g. 3, 5, 7, \u2026, 21, \u2026).  \nmethod{\u2018generic\u2019, \u2018gaussian\u2019, \u2018mean\u2019, \u2018median\u2019}, optional \n\nMethod used to determine adaptive threshold for local neighbourhood in weighted mean image.  \u2018generic\u2019: use custom function (see param parameter) \u2018gaussian\u2019: apply gaussian filter (see param parameter for custom sigma value) \u2018mean\u2019: apply arithmetic mean filter \u2018median\u2019: apply median rank filter  By default the \u2018gaussian\u2019 method is used.  \noffsetfloat, optional \n\nConstant subtracted from weighted mean of neighborhood to calculate the local threshold value. Default offset is 0.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018reflect\u2019.  \nparam{int, function}, optional \n\nEither specify sigma for \u2018gaussian\u2019 method or function object for \u2018generic\u2019 method. This functions takes the flat array of local neighbourhood as a single argument and returns the calculated threshold for the centre pixel.  \ncvalfloat, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019.    Returns \n \nthreshold(N, M) ndarray \n\nThreshold image. All pixels in the input image higher than the corresponding pixel in the threshold image are considered foreground.     References  \n1  \nhttps://docs.opencv.org/modules/imgproc/doc/miscellaneous_transformations.html?highlight=threshold#adaptivethreshold   Examples >>> from skimage.data import camera\n>>> image = camera()[:50, :50]\n>>> binary_image1 = image > threshold_local(image, 15, 'mean')\n>>> func = lambda arr: arr.mean()\n>>> binary_image2 = image > threshold_local(image, 15, 'generic',\n...                                         param=func)\n \n threshold_mean  \nskimage.filters.threshold_mean(image) [source]\n \nReturn threshold value based on the mean of grayscale values.  Parameters \n \nimage(N, M[, \u2026, P]) ndarray \n\nGrayscale input image.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.     References  \n1  \nC. A. Glasbey, \u201cAn analysis of histogram-based thresholding algorithms,\u201d CVGIP: Graphical Models and Image Processing, vol. 55, pp. 532-537, 1993. DOI:10.1006/cgip.1993.1040   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_mean(image)\n>>> binary = image > thresh\n \n threshold_minimum  \nskimage.filters.threshold_minimum(image=None, nbins=256, max_iter=10000, *, hist=None) [source]\n \nReturn threshold value based on minimum method. The histogram of the input image is computed if not provided and smoothed until there are only two maxima. Then the minimum in between is the threshold value. Either image or hist must be provided. In case hist is given, the actual histogram of the image is ignored.  Parameters \n \nimage(M, N) ndarray, optional \n\nInput image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.  \nmax_iterint, optional \n\nMaximum number of iterations to smooth the histogram.  \nhistarray, or 2-tuple of arrays, optional \n\nHistogram to determine the threshold from and a corresponding array of bin center intensities. Alternatively, only the histogram can be passed.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.    Raises \n RuntimeError\n\nIf unable to find two local maxima in the histogram or if the smoothing takes more than 1e4 iterations.     References  \n1  \nC. A. Glasbey, \u201cAn analysis of histogram-based thresholding algorithms,\u201d CVGIP: Graphical Models and Image Processing, vol. 55, pp. 532-537, 1993.  \n2  \nPrewitt, JMS & Mendelsohn, ML (1966), \u201cThe analysis of cell images\u201d, Annals of the New York Academy of Sciences 128: 1035-1053 DOI:10.1111/j.1749-6632.1965.tb11715.x   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_minimum(image)\n>>> binary = image > thresh\n \n threshold_multiotsu  \nskimage.filters.threshold_multiotsu(image, classes=3, nbins=256) [source]\n \nGenerate classes-1 threshold values to divide gray levels in image. The threshold values are chosen to maximize the total sum of pairwise variances between the thresholded graylevel classes. See Notes and [1] for more details.  Parameters \n \nimage(N, M) ndarray \n\nGrayscale input image.  \nclassesint, optional \n\nNumber of classes to be thresholded, i.e. the number of resulting regions.  \nnbinsint, optional \n\nNumber of bins used to calculate the histogram. This value is ignored for integer arrays.    Returns \n \nthresharray \n\nArray containing the threshold values for the desired classes.    Raises \n ValueError\n\nIf image contains less grayscale value then the desired number of classes.     Notes This implementation relies on a Cython function whose complexity is \\(O\\left(\\frac{Ch^{C-1}}{(C-1)!}\\right)\\), where \\(h\\) is the number of histogram bins and \\(C\\) is the number of classes desired. The input image must be grayscale. References  \n1  \nLiao, P-S., Chen, T-S. and Chung, P-C., \u201cA fast algorithm for multilevel thresholding\u201d, Journal of Information Science and Engineering 17 (5): 713-727, 2001. Available at: <https://ftp.iis.sinica.edu.tw/JISE/2001/200109_01.pdf> DOI:10.6688/JISE.2001.17.5.1  \n2  \nTosa, Y., \u201cMulti-Otsu Threshold\u201d, a java plugin for ImageJ. Available at: <http://imagej.net/plugins/download/Multi_OtsuThreshold.java>   Examples >>> from skimage.color import label2rgb\n>>> from skimage import data\n>>> image = data.camera()\n>>> thresholds = threshold_multiotsu(image)\n>>> regions = np.digitize(image, bins=thresholds)\n>>> regions_colorized = label2rgb(regions)\n \n Examples using skimage.filters.threshold_multiotsu\n \n  Multi-Otsu Thresholding  \n\n  Segment human cells (in mitosis)   threshold_niblack  \nskimage.filters.threshold_niblack(image, window_size=15, k=0.2) [source]\n \nApplies Niblack local threshold to an array. A threshold T is calculated for every pixel in the image using the following formula: T = m(x,y) - k * s(x,y)\n where m(x,y) and s(x,y) are the mean and standard deviation of pixel (x,y) neighborhood defined by a rectangular window with size w times w centered around the pixel. k is a configurable parameter that weights the effect of standard deviation.  Parameters \n \nimagendarray \n\nInput image.  \nwindow_sizeint, or iterable of int, optional \n\nWindow size specified as a single odd integer (3, 5, 7, \u2026), or an iterable of length image.ndim containing only odd integers (e.g. (1, 5, 5)).  \nkfloat, optional \n\nValue of parameter k in threshold formula.    Returns \n \nthreshold(N, M) ndarray \n\nThreshold mask. All pixels with an intensity higher than this value are assumed to be foreground.     Notes This algorithm is originally designed for text recognition. The Bradley threshold is a particular case of the Niblack one, being equivalent to >>> from skimage import data\n>>> image = data.page()\n>>> q = 1\n>>> threshold_image = threshold_niblack(image, k=0) * q\n for some value q. By default, Bradley and Roth use q=1. References  \n1  \nW. Niblack, An introduction to Digital Image Processing, Prentice-Hall, 1986.  \n2  \nD. Bradley and G. Roth, \u201cAdaptive thresholding using Integral Image\u201d, Journal of Graphics Tools 12(2), pp. 13-21, 2007. DOI:10.1080/2151237X.2007.10129236   Examples >>> from skimage import data\n>>> image = data.page()\n>>> threshold_image = threshold_niblack(image, window_size=7, k=0.1)\n \n threshold_otsu  \nskimage.filters.threshold_otsu(image=None, nbins=256, *, hist=None) [source]\n \nReturn threshold value based on Otsu\u2019s method. Either image or hist must be provided. If hist is provided, the actual histogram of the image is ignored.  Parameters \n \nimage(N, M) ndarray, optional \n\nGrayscale input image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.  \nhistarray, or 2-tuple of arrays, optional \n\nHistogram from which to determine the threshold, and optionally a corresponding array of bin center intensities. An alternative use of this function is to pass it only hist.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.     Notes The input image must be grayscale. References  \n1  \nWikipedia, https://en.wikipedia.org/wiki/Otsu\u2019s_Method   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_otsu(image)\n>>> binary = image <= thresh\n \n Examples using skimage.filters.threshold_otsu\n \n  Measure region properties  \n\n  Rank filters   threshold_sauvola  \nskimage.filters.threshold_sauvola(image, window_size=15, k=0.2, r=None) [source]\n \nApplies Sauvola local threshold to an array. Sauvola is a modification of Niblack technique. In the original method a threshold T is calculated for every pixel in the image using the following formula: T = m(x,y) * (1 + k * ((s(x,y) / R) - 1))\n where m(x,y) and s(x,y) are the mean and standard deviation of pixel (x,y) neighborhood defined by a rectangular window with size w times w centered around the pixel. k is a configurable parameter that weights the effect of standard deviation. R is the maximum standard deviation of a greyscale image.  Parameters \n \nimagendarray \n\nInput image.  \nwindow_sizeint, or iterable of int, optional \n\nWindow size specified as a single odd integer (3, 5, 7, \u2026), or an iterable of length image.ndim containing only odd integers (e.g. (1, 5, 5)).  \nkfloat, optional \n\nValue of the positive parameter k.  \nrfloat, optional \n\nValue of R, the dynamic range of standard deviation. If None, set to the half of the image dtype range.    Returns \n \nthreshold(N, M) ndarray \n\nThreshold mask. All pixels with an intensity higher than this value are assumed to be foreground.     Notes This algorithm is originally designed for text recognition. References  \n1  \nJ. Sauvola and M. Pietikainen, \u201cAdaptive document image binarization,\u201d Pattern Recognition 33(2), pp. 225-236, 2000. DOI:10.1016/S0031-3203(99)00055-2   Examples >>> from skimage import data\n>>> image = data.page()\n>>> t_sauvola = threshold_sauvola(image, window_size=15, k=0.2)\n>>> binary_image = image > t_sauvola\n \n threshold_triangle  \nskimage.filters.threshold_triangle(image, nbins=256) [source]\n \nReturn threshold value based on the triangle algorithm.  Parameters \n \nimage(N, M[, \u2026, P]) ndarray \n\nGrayscale input image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.     References  \n1  \nZack, G. W., Rogers, W. E. and Latt, S. A., 1977, Automatic Measurement of Sister Chromatid Exchange Frequency, Journal of Histochemistry and Cytochemistry 25 (7), pp. 741-753 DOI:10.1177/25.7.70454  \n2  \nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_triangle(image)\n>>> binary = image > thresh\n \n threshold_yen  \nskimage.filters.threshold_yen(image=None, nbins=256, *, hist=None) [source]\n \nReturn threshold value based on Yen\u2019s method. Either image or hist must be provided. In case hist is given, the actual histogram of the image is ignored.  Parameters \n \nimage(N, M) ndarray, optional \n\nInput image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.  \nhistarray, or 2-tuple of arrays, optional \n\nHistogram from which to determine the threshold, and optionally a corresponding array of bin center intensities. An alternative use of this function is to pass it only hist.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.     References  \n1  \nYen J.C., Chang F.J., and Chang S. (1995) \u201cA New Criterion for Automatic Multilevel Thresholding\u201d IEEE Trans. on Image Processing, 4(3): 370-378. DOI:10.1109/83.366472  \n2  \nSezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and Quantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1): 146-165, DOI:10.1117/1.1631315 http://www.busim.ee.boun.edu.tr/~sankur/SankurFolder/Threshold_survey.pdf  \n3  \nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_yen(image)\n>>> binary = image <= thresh\n \n try_all_threshold  \nskimage.filters.try_all_threshold(image, figsize=(8, 5), verbose=True) [source]\n \nReturns a figure comparing the outputs of different thresholding methods.  Parameters \n \nimage(N, M) ndarray \n\nInput image.  \nfigsizetuple, optional \n\nFigure size (in inches).  \nverbosebool, optional \n\nPrint function name for each method.    Returns \n \nfig, axtuple \n\nMatplotlib figure and axes.     Notes The following algorithms are used:  isodata li mean minimum otsu triangle yen  Examples >>> from skimage.data import text\n>>> fig, ax = try_all_threshold(text(), figsize=(10, 6), verbose=False)\n \n unsharp_mask  \nskimage.filters.unsharp_mask(image, radius=1.0, amount=1.0, multichannel=False, preserve_range=False) [source]\n \nUnsharp masking filter. The sharp details are identified as the difference between the original image and its blurred version. These details are then scaled, and added back to the original image.  Parameters \n \nimage[P, \u2026, ]M[, N][, C] ndarray \n\nInput image.  \nradiusscalar or sequence of scalars, optional \n\nIf a scalar is given, then its value is used for all dimensions. If sequence is given, then there must be exactly one radius for each dimension except the last dimension for multichannel images. Note that 0 radius means no blurring, and negative values are not allowed.  \namountscalar, optional \n\nThe details will be amplified with this factor. The factor could be 0 or negative. Typically, it is a small positive number, e.g. 1.0.  \nmultichannelbool, optional \n\nIf True, the last image dimension is considered as a color channel, otherwise as spatial. Color channels are processed individually.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html    Returns \n \noutput[P, \u2026, ]M[, N][, C] ndarray of float \n\nImage with unsharp mask applied.     Notes Unsharp masking is an image sharpening technique. It is a linear image operation, and numerically stable, unlike deconvolution which is an ill-posed problem. Because of this stability, it is often preferred over deconvolution. The main idea is as follows: sharp details are identified as the difference between the original image and its blurred version. These details are added back to the original image after a scaling step: enhanced image = original + amount * (original - blurred) When applying this filter to several color layers independently, color bleeding may occur. More visually pleasing result can be achieved by processing only the brightness/lightness/intensity channel in a suitable color space such as HSV, HSL, YUV, or YCbCr. Unsharp masking is described in most introductory digital image processing books. This implementation is based on [1]. References  \n1  \nMaria Petrou, Costas Petrou \u201cImage Processing: The Fundamentals\u201d, (2010), ed ii., page 357, ISBN 13: 9781119994398 DOI:10.1002/9781119994398  \n2  \nWikipedia. Unsharp masking https://en.wikipedia.org/wiki/Unsharp_masking   Examples >>> array = np.ones(shape=(5,5), dtype=np.uint8)*100\n>>> array[2,2] = 120\n>>> array\narray([[100, 100, 100, 100, 100],\n       [100, 100, 100, 100, 100],\n       [100, 100, 120, 100, 100],\n       [100, 100, 100, 100, 100],\n       [100, 100, 100, 100, 100]], dtype=uint8)\n>>> np.around(unsharp_mask(array, radius=0.5, amount=2),2)\narray([[0.39, 0.39, 0.39, 0.39, 0.39],\n       [0.39, 0.39, 0.38, 0.39, 0.39],\n       [0.39, 0.38, 0.53, 0.38, 0.39],\n       [0.39, 0.39, 0.38, 0.39, 0.39],\n       [0.39, 0.39, 0.39, 0.39, 0.39]])\n >>> array = np.ones(shape=(5,5), dtype=np.int8)*100\n>>> array[2,2] = 127\n>>> np.around(unsharp_mask(array, radius=0.5, amount=2),2)\narray([[0.79, 0.79, 0.79, 0.79, 0.79],\n       [0.79, 0.78, 0.75, 0.78, 0.79],\n       [0.79, 0.75, 1.  , 0.75, 0.79],\n       [0.79, 0.78, 0.75, 0.78, 0.79],\n       [0.79, 0.79, 0.79, 0.79, 0.79]])\n >>> np.around(unsharp_mask(array, radius=0.5, amount=2, preserve_range=True), 2)\narray([[100.  , 100.  ,  99.99, 100.  , 100.  ],\n       [100.  ,  99.39,  95.48,  99.39, 100.  ],\n       [ 99.99,  95.48, 147.59,  95.48,  99.99],\n       [100.  ,  99.39,  95.48,  99.39, 100.  ],\n       [100.  , 100.  ,  99.99, 100.  , 100.  ]])\n \n wiener  \nskimage.filters.wiener(data, impulse_response=None, filter_params={}, K=0.25, predefined_filter=None) [source]\n \nMinimum Mean Square Error (Wiener) inverse filter.  Parameters \n \ndata(M,N) ndarray \n\nInput data.  \nKfloat or (M,N) ndarray \n\nRatio between power spectrum of noise and undegraded image.  \nimpulse_responsecallable f(r, c, **filter_params) \n\nImpulse response of the filter. See LPIFilter2D.__init__.  \nfilter_paramsdict \n\nAdditional keyword parameters to the impulse_response function.    Other Parameters \n \npredefined_filterLPIFilter2D \n\nIf you need to apply the same filter multiple times over different images, construct the LPIFilter2D and specify it here.     \n window  \nskimage.filters.window(window_type, shape, warp_kwargs=None) [source]\n \nReturn an n-dimensional window of a given size and dimensionality.  Parameters \n \nwindow_typestring, float, or tuple \n\nThe type of window to be created. Any window type supported by scipy.signal.get_window is allowed here. See notes below for a current list, or the SciPy documentation for the version of SciPy on your machine.  \nshapetuple of int or int \n\nThe shape of the window along each axis. If an integer is provided, a 1D window is generated.  \nwarp_kwargsdict \n\nKeyword arguments passed to skimage.transform.warp (e.g., warp_kwargs={'order':3} to change interpolation method).    Returns \n \nnd_windowndarray \n\nA window of the specified shape. dtype is np.double.     Notes This function is based on scipy.signal.get_window and thus can access all of the window types available to that function (e.g., \"hann\", \"boxcar\"). Note that certain window types require parameters that have to be supplied with the window name as a tuple (e.g., (\"tukey\", 0.8)). If only a float is supplied, it is interpreted as the beta parameter of the Kaiser window. See https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.windows.get_window.html for more details. Note that this function generates a double precision array of the specified shape and can thus generate very large arrays that consume a large amount of available memory. The approach taken here to create nD windows is to first calculate the Euclidean distance from the center of the intended nD window to each position in the array. That distance is used to sample, with interpolation, from a 1D window returned from scipy.signal.get_window. The method of interpolation can be changed with the order keyword argument passed to skimage.transform.warp. Some coordinates in the output window will be outside of the original signal; these will be filled in with zeros. Window types: - boxcar - triang - blackman - hamming - hann - bartlett - flattop - parzen - bohman - blackmanharris - nuttall - barthann - kaiser (needs beta) - gaussian (needs standard deviation) - general_gaussian (needs power, width) - slepian (needs width) - dpss (needs normalized half-bandwidth) - chebwin (needs attenuation) - exponential (needs decay scale) - tukey (needs taper fraction) References  \n1  \nTwo-dimensional window design, Wikipedia, https://en.wikipedia.org/wiki/Two_dimensional_window_design   Examples Return a Hann window with shape (512, 512): >>> from skimage.filters import window\n>>> w = window('hann', (512, 512))\n Return a Kaiser window with beta parameter of 16 and shape (256, 256, 35): >>> w = window(16, (256, 256, 35))\n Return a Tukey window with an alpha parameter of 0.8 and shape (100, 300): >>> w = window(('tukey', 0.8), (100, 300))\n \n LPIFilter2D  \nclass skimage.filters.LPIFilter2D(impulse_response, **filter_params) [source]\n \nBases: object Linear Position-Invariant Filter (2-dimensional)  \n__init__(impulse_response, **filter_params) [source]\n \n Parameters \n \nimpulse_responsecallable f(r, c, **filter_params) \n\nFunction that yields the impulse response. r and c are 1-dimensional vectors that represent row and column positions, in other words coordinates are (r[0],c[0]),(r[0],c[1]) etc. **filter_params are passed through. In other words, impulse_response would be called like this: >>> def impulse_response(r, c, **filter_params):\n...     pass\n>>>\n>>> r = [0,0,0,1,1,1,2,2,2]\n>>> c = [0,1,2,0,1,2,0,1,2]\n>>> filter_params = {'kw1': 1, 'kw2': 2, 'kw3': 3}\n>>> impulse_response(r, c, **filter_params)\n     Examples Gaussian filter: Use a 1-D gaussian in each direction without normalization coefficients. >>> def filt_func(r, c, sigma = 1):\n...     return np.exp(-np.hypot(r, c)/sigma)\n>>> filter = LPIFilter2D(filt_func)\n \n \n\n"}, {"name": "filters.apply_hysteresis_threshold()", "path": "api/skimage.filters#skimage.filters.apply_hysteresis_threshold", "type": "filters", "text": " \nskimage.filters.apply_hysteresis_threshold(image, low, high) [source]\n \nApply hysteresis thresholding to image. This algorithm finds regions where image is greater than high OR image is greater than low and that region is connected to a region greater than high.  Parameters \n \nimagearray, shape (M,[ N, \u2026, P]) \n\nGrayscale input image.  \nlowfloat, or array of same shape as image \n\nLower threshold.  \nhighfloat, or array of same shape as image \n\nHigher threshold.    Returns \n \nthresholdedarray of bool, same shape as image \n\nArray in which True indicates the locations where image was above the hysteresis threshold.     References  \n1  \nJ. Canny. A computational approach to edge detection. IEEE Transactions on Pattern Analysis and Machine Intelligence. 1986; vol. 8, pp.679-698. DOI:10.1109/TPAMI.1986.4767851   Examples >>> image = np.array([1, 2, 3, 2, 1, 2, 1, 3, 2])\n>>> apply_hysteresis_threshold(image, 1.5, 2.5).astype(int)\narray([0, 1, 1, 1, 0, 0, 0, 1, 1])\n \n"}, {"name": "filters.correlate_sparse()", "path": "api/skimage.filters#skimage.filters.correlate_sparse", "type": "filters", "text": " \nskimage.filters.correlate_sparse(image, kernel, mode='reflect') [source]\n \nCompute valid cross-correlation of padded_array and kernel. This function is fast when kernel is large with many zeros. See scipy.ndimage.correlate for a description of cross-correlation.  Parameters \n \nimagendarray, dtype float, shape (M, N,[ \u2026,] P) \n\nThe input array. If mode is \u2018valid\u2019, this array should already be padded, as a margin of the same shape as kernel will be stripped off.  \nkernelndarray, dtype float shape (Q, R,[ \u2026,] S) \n\nThe kernel to be correlated. Must have the same number of dimensions as padded_array. For high performance, it should be sparse (few nonzero entries).  \nmodestring, optional \n\nSee scipy.ndimage.correlate for valid modes. Additionally, mode \u2018valid\u2019 is accepted, in which case no padding is applied and the result is the result for the smaller image for which the kernel is entirely inside the original data.    Returns \n \nresultarray of float, shape (M, N,[ \u2026,] P) \n\nThe result of cross-correlating image with kernel. If mode \u2018valid\u2019 is used, the resulting shape is (M-Q+1, N-R+1,[ \u2026,] P-S+1).     \n"}, {"name": "filters.difference_of_gaussians()", "path": "api/skimage.filters#skimage.filters.difference_of_gaussians", "type": "filters", "text": " \nskimage.filters.difference_of_gaussians(image, low_sigma, high_sigma=None, *, mode='nearest', cval=0, multichannel=False, truncate=4.0) [source]\n \nFind features between low_sigma and high_sigma in size. This function uses the Difference of Gaussians method for applying band-pass filters to multi-dimensional arrays. The input array is blurred with two Gaussian kernels of differing sigmas to produce two intermediate, filtered images. The more-blurred image is then subtracted from the less-blurred image. The final output image will therefore have had high-frequency components attenuated by the smaller-sigma Gaussian, and low frequency components will have been removed due to their presence in the more-blurred intermediate.  Parameters \n \nimagendarray \n\nInput array to filter.  \nlow_sigmascalar or sequence of scalars \n\nStandard deviation(s) for the Gaussian kernel with the smaller sigmas across all axes. The standard deviations are given for each axis as a sequence, or as a single number, in which case the single number is used as the standard deviation value for all axes.  \nhigh_sigmascalar or sequence of scalars, optional (default is None) \n\nStandard deviation(s) for the Gaussian kernel with the larger sigmas across all axes. The standard deviations are given for each axis as a sequence, or as a single number, in which case the single number is used as the standard deviation value for all axes. If None is given (default), sigmas for all axes are calculated as 1.6 * low_sigma.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.  \ncvalscalar, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0  \nmultichannelbool, optional (default: False) \n\nWhether the last axis of the image is to be interpreted as multiple channels. If True, each channel is filtered separately (channels are not mixed together).  \ntruncatefloat, optional (default is 4.0) \n\nTruncate the filter at this many standard deviations.    Returns \n \nfiltered_imagendarray \n\nthe filtered array.      See also  \nskimage.feature.blog_dog \n  Notes This function will subtract an array filtered with a Gaussian kernel with sigmas given by high_sigma from an array filtered with a Gaussian kernel with sigmas provided by low_sigma. The values for high_sigma must always be greater than or equal to the corresponding values in low_sigma, or a ValueError will be raised. When high_sigma is none, the values for high_sigma will be calculated as 1.6x the corresponding values in low_sigma. This ratio was originally proposed by Marr and Hildreth (1980) [1] and is commonly used when approximating the inverted Laplacian of Gaussian, which is used in edge and blob detection. Input image is converted according to the conventions of img_as_float. Except for sigma values, all parameters are used for both filters. References  \n1  \nMarr, D. and Hildreth, E. Theory of Edge Detection. Proc. R. Soc. Lond. Series B 207, 187-217 (1980). https://doi.org/10.1098/rspb.1980.0020   Examples Apply a simple Difference of Gaussians filter to a color image: >>> from skimage.data import astronaut\n>>> from skimage.filters import difference_of_gaussians\n>>> filtered_image = difference_of_gaussians(astronaut(), 2, 10,\n...                                          multichannel=True)\n Apply a Laplacian of Gaussian filter as approximated by the Difference of Gaussians filter: >>> filtered_image = difference_of_gaussians(astronaut(), 2,\n...                                          multichannel=True)\n Apply a Difference of Gaussians filter to a grayscale image using different sigma values for each axis: >>> from skimage.data import camera\n>>> filtered_image = difference_of_gaussians(camera(), (2,5), (3,20))\n \n"}, {"name": "filters.farid()", "path": "api/skimage.filters#skimage.filters.farid", "type": "filters", "text": " \nskimage.filters.farid(image, *, mask=None) [source]\n \nFind the edge magnitude using the Farid transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Farid edge map.      See also  \nsobel, prewitt, canny \n  Notes Take the square root of the sum of the squares of the horizontal and vertical derivatives to get a magnitude that is somewhat insensitive to direction. Similar to the Scharr operator, this operator is designed with a rotation invariance constraint. References  \n1  \nFarid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional signals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004. DOI:10.1109/TIP.2004.823819  \n2  \nWikipedia, \u201cFarid and Simoncelli Derivatives.\u201d Available at: <https://en.wikipedia.org/wiki/Image_derivatives#Farid_and_Simoncelli_Derivatives>   Examples >>> from skimage import data\n>>> camera = data.camera()\n>>> from skimage import filters\n>>> edges = filters.farid(camera)\n \n"}, {"name": "filters.farid_h()", "path": "api/skimage.filters#skimage.filters.farid_h", "type": "filters", "text": " \nskimage.filters.farid_h(image, *, mask=None) [source]\n \nFind the horizontal edges of an image using the Farid transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Farid edge map.     Notes The kernel was constructed using the 5-tap weights from [1]. References  \n1  \nFarid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional signals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004. DOI:10.1109/TIP.2004.823819  \n2  \nFarid, H. and Simoncelli, E. P. \u201cOptimally rotation-equivariant directional derivative kernels\u201d, In: 7th International Conference on Computer Analysis of Images and Patterns, Kiel, Germany. Sep, 1997.   \n"}, {"name": "filters.farid_v()", "path": "api/skimage.filters#skimage.filters.farid_v", "type": "filters", "text": " \nskimage.filters.farid_v(image, *, mask=None) [source]\n \nFind the vertical edges of an image using the Farid transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Farid edge map.     Notes The kernel was constructed using the 5-tap weights from [1]. References  \n1  \nFarid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional signals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004. DOI:10.1109/TIP.2004.823819   \n"}, {"name": "filters.frangi()", "path": "api/skimage.filters#skimage.filters.frangi", "type": "filters", "text": " \nskimage.filters.frangi(image, sigmas=range(1, 10, 2), scale_range=None, scale_step=None, alpha=0.5, beta=0.5, gamma=15, black_ridges=True, mode='reflect', cval=0) [source]\n \nFilter an image with the Frangi vesselness filter. This filter can be used to detect continuous ridges, e.g. vessels, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects. Defined only for 2-D and 3-D images. Calculates the eigenvectors of the Hessian to compute the similarity of an image region to vessels, according to the method described in [1].  Parameters \n \nimage(N, M[, P]) ndarray \n\nArray with input image data.  \nsigmasiterable of floats, optional \n\nSigmas used as scales of filter, i.e., np.arange(scale_range[0], scale_range[1], scale_step)  \nscale_range2-tuple of floats, optional \n\nThe range of sigmas used.  \nscale_stepfloat, optional \n\nStep size between sigmas.  \nalphafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a plate-like structure.  \nbetafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a blob-like structure.  \ngammafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to areas of high variance/texture/structure.  \nblack_ridgesboolean, optional \n\nWhen True (the default), the filter detects black ridges; when False, it detects white ridges.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nout(N, M[, P]) ndarray \n\nFiltered image (maximum of pixels across all scales).      See also  \nmeijering\n\n\nsato\n\n\nhessian\n\n  Notes Written by Marc Schrijver, November 2001 Re-Written by D. J. Kroon, University of Twente, May 2009, [2] Adoption of 3D version from D. G. Ellis, Januar 20017, [3] References  \n1  \nFrangi, A. F., Niessen, W. J., Vincken, K. L., & Viergever, M. A. (1998,). Multiscale vessel enhancement filtering. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 130-137). Springer Berlin Heidelberg. DOI:10.1007/BFb0056195  \n2  \nKroon, D. J.: Hessian based Frangi vesselness filter.  \n3  \nEllis, D. G.: https://github.com/ellisdg/frangi3d/tree/master/frangi   \n"}, {"name": "filters.gabor()", "path": "api/skimage.filters#skimage.filters.gabor", "type": "filters", "text": " \nskimage.filters.gabor(image, frequency, theta=0, bandwidth=1, sigma_x=None, sigma_y=None, n_stds=3, offset=0, mode='reflect', cval=0) [source]\n \nReturn real and imaginary responses to Gabor filter. The real and imaginary parts of the Gabor filter kernel are applied to the image and the response is returned as a pair of arrays. Gabor filter is a linear filter with a Gaussian kernel which is modulated by a sinusoidal plane wave. Frequency and orientation representations of the Gabor filter are similar to those of the human visual system. Gabor filter banks are commonly used in computer vision and image processing. They are especially suitable for edge detection and texture classification.  Parameters \n \nimage2-D array \n\nInput image.  \nfrequencyfloat \n\nSpatial frequency of the harmonic function. Specified in pixels.  \nthetafloat, optional \n\nOrientation in radians. If 0, the harmonic is in the x-direction.  \nbandwidthfloat, optional \n\nThe bandwidth captured by the filter. For fixed bandwidth, sigma_x and sigma_y will decrease with increasing frequency. This value is ignored if sigma_x and sigma_y are set by the user.  \nsigma_x, sigma_yfloat, optional \n\nStandard deviation in x- and y-directions. These directions apply to the kernel before rotation. If theta = pi/2, then the kernel is rotated 90 degrees so that sigma_x controls the vertical direction.  \nn_stdsscalar, optional \n\nThe linear size of the kernel is n_stds (3 by default) standard deviations.  \noffsetfloat, optional \n\nPhase offset of harmonic function in radians.  \nmode{\u2018constant\u2019, \u2018nearest\u2019, \u2018reflect\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional \n\nMode used to convolve image with a kernel, passed to ndi.convolve  \ncvalscalar, optional \n\nValue to fill past edges of input if mode of convolution is \u2018constant\u2019. The parameter is passed to ndi.convolve.    Returns \n \nreal, imagarrays \n\nFiltered images using the real and imaginary parts of the Gabor filter kernel. Images are of the same dimensions as the input one.     References  \n1  \nhttps://en.wikipedia.org/wiki/Gabor_filter  \n2  \nhttps://web.archive.org/web/20180127125930/http://mplab.ucsd.edu/tutorials/gabor.pdf   Examples >>> from skimage.filters import gabor\n>>> from skimage import data, io\n>>> from matplotlib import pyplot as plt  \n >>> image = data.coins()\n>>> # detecting edges in a coin image\n>>> filt_real, filt_imag = gabor(image, frequency=0.6)\n>>> plt.figure()            \n>>> io.imshow(filt_real)    \n>>> io.show()               \n >>> # less sensitivity to finer details with the lower frequency kernel\n>>> filt_real, filt_imag = gabor(image, frequency=0.1)\n>>> plt.figure()            \n>>> io.imshow(filt_real)    \n>>> io.show()               \n \n"}, {"name": "filters.gabor_kernel()", "path": "api/skimage.filters#skimage.filters.gabor_kernel", "type": "filters", "text": " \nskimage.filters.gabor_kernel(frequency, theta=0, bandwidth=1, sigma_x=None, sigma_y=None, n_stds=3, offset=0) [source]\n \nReturn complex 2D Gabor filter kernel. Gabor kernel is a Gaussian kernel modulated by a complex harmonic function. Harmonic function consists of an imaginary sine function and a real cosine function. Spatial frequency is inversely proportional to the wavelength of the harmonic and to the standard deviation of a Gaussian kernel. The bandwidth is also inversely proportional to the standard deviation.  Parameters \n \nfrequencyfloat \n\nSpatial frequency of the harmonic function. Specified in pixels.  \nthetafloat, optional \n\nOrientation in radians. If 0, the harmonic is in the x-direction.  \nbandwidthfloat, optional \n\nThe bandwidth captured by the filter. For fixed bandwidth, sigma_x and sigma_y will decrease with increasing frequency. This value is ignored if sigma_x and sigma_y are set by the user.  \nsigma_x, sigma_yfloat, optional \n\nStandard deviation in x- and y-directions. These directions apply to the kernel before rotation. If theta = pi/2, then the kernel is rotated 90 degrees so that sigma_x controls the vertical direction.  \nn_stdsscalar, optional \n\nThe linear size of the kernel is n_stds (3 by default) standard deviations  \noffsetfloat, optional \n\nPhase offset of harmonic function in radians.    Returns \n \ngcomplex array \n\nComplex filter kernel.     References  \n1  \nhttps://en.wikipedia.org/wiki/Gabor_filter  \n2  \nhttps://web.archive.org/web/20180127125930/http://mplab.ucsd.edu/tutorials/gabor.pdf   Examples >>> from skimage.filters import gabor_kernel\n>>> from skimage import io\n>>> from matplotlib import pyplot as plt  \n >>> gk = gabor_kernel(frequency=0.2)\n>>> plt.figure()        \n>>> io.imshow(gk.real)  \n>>> io.show()           \n >>> # more ripples (equivalent to increasing the size of the\n>>> # Gaussian spread)\n>>> gk = gabor_kernel(frequency=0.2, bandwidth=0.1)\n>>> plt.figure()        \n>>> io.imshow(gk.real)  \n>>> io.show()           \n \n"}, {"name": "filters.gaussian()", "path": "api/skimage.filters#skimage.filters.gaussian", "type": "filters", "text": " \nskimage.filters.gaussian(image, sigma=1, output=None, mode='nearest', cval=0, multichannel=None, preserve_range=False, truncate=4.0) [source]\n \nMulti-dimensional Gaussian filter.  Parameters \n \nimagearray-like \n\nInput image (grayscale or color) to filter.  \nsigmascalar or sequence of scalars, optional \n\nStandard deviation for Gaussian kernel. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.  \noutputarray, optional \n\nThe output parameter passes an array in which to store the filter output.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.  \ncvalscalar, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0  \nmultichannelbool, optional (default: None) \n\nWhether the last axis of the image is to be interpreted as multiple channels. If True, each channel is filtered separately (channels are not mixed together). Only 3 channels are supported. If None, the function will attempt to guess this, and raise a warning if ambiguous, when the array has shape (M, N, 3).  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html  \ntruncatefloat, optional \n\nTruncate the filter at this many standard deviations.    Returns \n \nfiltered_imagendarray \n\nthe filtered array     Notes This function is a wrapper around scipy.ndi.gaussian_filter(). Integer arrays are converted to float. The output should be floating point data type since gaussian converts to float provided image. If output is not provided, another array will be allocated and returned as the result. The multi-dimensional filter is implemented as a sequence of one-dimensional convolution filters. The intermediate arrays are stored in the same data type as the output. Therefore, for output types with a limited precision, the results may be imprecise because intermediate results may be stored with insufficient precision. Examples >>> a = np.zeros((3, 3))\n>>> a[1, 1] = 1\n>>> a\narray([[0., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 0.]])\n>>> gaussian(a, sigma=0.4)  # mild smoothing\narray([[0.00163116, 0.03712502, 0.00163116],\n       [0.03712502, 0.84496158, 0.03712502],\n       [0.00163116, 0.03712502, 0.00163116]])\n>>> gaussian(a, sigma=1)  # more smoothing\narray([[0.05855018, 0.09653293, 0.05855018],\n       [0.09653293, 0.15915589, 0.09653293],\n       [0.05855018, 0.09653293, 0.05855018]])\n>>> # Several modes are possible for handling boundaries\n>>> gaussian(a, sigma=1, mode='reflect')\narray([[0.08767308, 0.12075024, 0.08767308],\n       [0.12075024, 0.16630671, 0.12075024],\n       [0.08767308, 0.12075024, 0.08767308]])\n>>> # For RGB images, each is filtered separately\n>>> from skimage.data import astronaut\n>>> image = astronaut()\n>>> filtered_img = gaussian(image, sigma=1, multichannel=True)\n \n"}, {"name": "filters.hessian()", "path": "api/skimage.filters#skimage.filters.hessian", "type": "filters", "text": " \nskimage.filters.hessian(image, sigmas=range(1, 10, 2), scale_range=None, scale_step=None, alpha=0.5, beta=0.5, gamma=15, black_ridges=True, mode=None, cval=0) [source]\n \nFilter an image with the Hybrid Hessian filter. This filter can be used to detect continuous edges, e.g. vessels, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects. Defined only for 2-D and 3-D images. Almost equal to Frangi filter, but uses alternative method of smoothing. Refer to [1] to find the differences between Frangi and Hessian filters.  Parameters \n \nimage(N, M[, P]) ndarray \n\nArray with input image data.  \nsigmasiterable of floats, optional \n\nSigmas used as scales of filter, i.e., np.arange(scale_range[0], scale_range[1], scale_step)  \nscale_range2-tuple of floats, optional \n\nThe range of sigmas used.  \nscale_stepfloat, optional \n\nStep size between sigmas.  \nbetafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a blob-like structure.  \ngammafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to areas of high variance/texture/structure.  \nblack_ridgesboolean, optional \n\nWhen True (the default), the filter detects black ridges; when False, it detects white ridges.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nout(N, M[, P]) ndarray \n\nFiltered image (maximum of pixels across all scales).      See also  \nmeijering\n\n\nsato\n\n\nfrangi\n\n  Notes Written by Marc Schrijver (November 2001) Re-Written by D. J. Kroon University of Twente (May 2009) [2] References  \n1  \nNg, C. C., Yap, M. H., Costen, N., & Li, B. (2014,). Automatic wrinkle detection using hybrid Hessian filter. In Asian Conference on Computer Vision (pp. 609-622). Springer International Publishing. DOI:10.1007/978-3-319-16811-1_40  \n2  \nKroon, D. J.: Hessian based Frangi vesselness filter.   \n"}, {"name": "filters.inverse()", "path": "api/skimage.filters#skimage.filters.inverse", "type": "filters", "text": " \nskimage.filters.inverse(data, impulse_response=None, filter_params={}, max_gain=2, predefined_filter=None) [source]\n \nApply the filter in reverse to the given data.  Parameters \n \ndata(M,N) ndarray \n\nInput data.  \nimpulse_responsecallable f(r, c, **filter_params) \n\nImpulse response of the filter. See LPIFilter2D.__init__.  \nfilter_paramsdict \n\nAdditional keyword parameters to the impulse_response function.  \nmax_gainfloat \n\nLimit the filter gain. Often, the filter contains zeros, which would cause the inverse filter to have infinite gain. High gain causes amplification of artefacts, so a conservative limit is recommended.    Other Parameters \n \npredefined_filterLPIFilter2D \n\nIf you need to apply the same filter multiple times over different images, construct the LPIFilter2D and specify it here.     \n"}, {"name": "filters.laplace()", "path": "api/skimage.filters#skimage.filters.laplace", "type": "filters", "text": " \nskimage.filters.laplace(image, ksize=3, mask=None) [source]\n \nFind the edges of an image using the Laplace operator.  Parameters \n \nimagendarray \n\nImage to process.  \nksizeint, optional \n\nDefine the size of the discrete Laplacian operator such that it will have a size of (ksize,) * image.ndim.  \nmaskndarray, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutputndarray \n\nThe Laplace edge map.     Notes The Laplacian operator is generated using the function skimage.restoration.uft.laplacian(). \n"}, {"name": "filters.LPIFilter2D", "path": "api/skimage.filters#skimage.filters.LPIFilter2D", "type": "filters", "text": " \nclass skimage.filters.LPIFilter2D(impulse_response, **filter_params) [source]\n \nBases: object Linear Position-Invariant Filter (2-dimensional)  \n__init__(impulse_response, **filter_params) [source]\n \n Parameters \n \nimpulse_responsecallable f(r, c, **filter_params) \n\nFunction that yields the impulse response. r and c are 1-dimensional vectors that represent row and column positions, in other words coordinates are (r[0],c[0]),(r[0],c[1]) etc. **filter_params are passed through. In other words, impulse_response would be called like this: >>> def impulse_response(r, c, **filter_params):\n...     pass\n>>>\n>>> r = [0,0,0,1,1,1,2,2,2]\n>>> c = [0,1,2,0,1,2,0,1,2]\n>>> filter_params = {'kw1': 1, 'kw2': 2, 'kw3': 3}\n>>> impulse_response(r, c, **filter_params)\n     Examples Gaussian filter: Use a 1-D gaussian in each direction without normalization coefficients. >>> def filt_func(r, c, sigma = 1):\n...     return np.exp(-np.hypot(r, c)/sigma)\n>>> filter = LPIFilter2D(filt_func)\n \n \n"}, {"name": "filters.LPIFilter2D.__init__()", "path": "api/skimage.filters#skimage.filters.LPIFilter2D.__init__", "type": "filters", "text": " \n__init__(impulse_response, **filter_params) [source]\n \n Parameters \n \nimpulse_responsecallable f(r, c, **filter_params) \n\nFunction that yields the impulse response. r and c are 1-dimensional vectors that represent row and column positions, in other words coordinates are (r[0],c[0]),(r[0],c[1]) etc. **filter_params are passed through. In other words, impulse_response would be called like this: >>> def impulse_response(r, c, **filter_params):\n...     pass\n>>>\n>>> r = [0,0,0,1,1,1,2,2,2]\n>>> c = [0,1,2,0,1,2,0,1,2]\n>>> filter_params = {'kw1': 1, 'kw2': 2, 'kw3': 3}\n>>> impulse_response(r, c, **filter_params)\n     Examples Gaussian filter: Use a 1-D gaussian in each direction without normalization coefficients. >>> def filt_func(r, c, sigma = 1):\n...     return np.exp(-np.hypot(r, c)/sigma)\n>>> filter = LPIFilter2D(filt_func)\n \n"}, {"name": "filters.median()", "path": "api/skimage.filters#skimage.filters.median", "type": "filters", "text": " \nskimage.filters.median(image, selem=None, out=None, mode='nearest', cval=0.0, behavior='ndimage') [source]\n \nReturn local median of an image.  Parameters \n \nimagearray-like \n\nInput image.  \nselemndarray, optional \n\nIf behavior=='rank', selem is a 2-D array of 1\u2019s and 0\u2019s. If behavior=='ndimage', selem is a N-D array of 1\u2019s and 0\u2019s with the same number of dimension than image. If None, selem will be a N-D array with 3 elements for each dimension (e.g., vector, square, cube, etc.)  \noutndarray, (same dtype as image), optional \n\nIf None, a new array is allocated.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018nearest\u2019, \u2018mirror\u2019,\u2019\u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.  New in version 0.15: mode is used when behavior='ndimage'.   \ncvalscalar, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0  New in version 0.15: cval was added in 0.15 is used when behavior='ndimage'.   \nbehavior{\u2018ndimage\u2019, \u2018rank\u2019}, optional \n\nEither to use the old behavior (i.e., < 0.15) or the new behavior. The old behavior will call the skimage.filters.rank.median(). The new behavior will call the scipy.ndimage.median_filter(). Default is \u2018ndimage\u2019.  New in version 0.15: behavior is introduced in 0.15   Changed in version 0.16: Default behavior has been changed from \u2018rank\u2019 to \u2018ndimage\u2019     Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.      See also  \nskimage.filters.rank.median\n\n\nRank-based implementation of the median filtering offering more flexibility with additional parameters but dedicated for unsigned integer images.    Examples >>> from skimage import data\n>>> from skimage.morphology import disk\n>>> from skimage.filters import median\n>>> img = data.camera()\n>>> med = median(img, disk(5))\n \n"}, {"name": "filters.meijering()", "path": "api/skimage.filters#skimage.filters.meijering", "type": "filters", "text": " \nskimage.filters.meijering(image, sigmas=range(1, 10, 2), alpha=None, black_ridges=True, mode='reflect', cval=0) [source]\n \nFilter an image with the Meijering neuriteness filter. This filter can be used to detect continuous ridges, e.g. neurites, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects. Calculates the eigenvectors of the Hessian to compute the similarity of an image region to neurites, according to the method described in [1].  Parameters \n \nimage(N, M[, \u2026, P]) ndarray \n\nArray with input image data.  \nsigmasiterable of floats, optional \n\nSigmas used as scales of filter  \nalphafloat, optional \n\nFrangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a plate-like structure.  \nblack_ridgesboolean, optional \n\nWhen True (the default), the filter detects black ridges; when False, it detects white ridges.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nout(N, M[, \u2026, P]) ndarray \n\nFiltered image (maximum of pixels across all scales).      See also  \nsato\n\n\nfrangi\n\n\nhessian\n\n  References  \n1  \nMeijering, E., Jacob, M., Sarria, J. C., Steiner, P., Hirling, H., Unser, M. (2004). Design and validation of a tool for neurite tracing and analysis in fluorescence microscopy images. Cytometry Part A, 58(2), 167-176. DOI:10.1002/cyto.a.20022   \n"}, {"name": "filters.prewitt()", "path": "api/skimage.filters#skimage.filters.prewitt", "type": "filters", "text": " \nskimage.filters.prewitt(image, mask=None, *, axis=None, mode='reflect', cval=0.0) [source]\n \nFind the edge magnitude using the Prewitt transform.  Parameters \n \nimagearray \n\nThe input image.  \nmaskarray of bool, optional \n\nClip the output image to this mask. (Values where mask=0 will be set to 0.)  \naxisint or sequence of int, optional \n\nCompute the edge filter along this axis. If not provided, the edge magnitude is computed. This is defined as: prw_mag = np.sqrt(sum([prewitt(image, axis=i)**2\n                       for i in range(image.ndim)]) / image.ndim)\n The magnitude is also computed if axis is a sequence.  \nmodestr or sequence of str, optional \n\nThe boundary mode for the convolution. See scipy.ndimage.convolve for a description of the modes. This can be either a single boundary mode or one boundary mode per axis.  \ncvalfloat, optional \n\nWhen mode is 'constant', this is the constant used in values outside the boundary of the image data.    Returns \n \noutputarray of float \n\nThe Prewitt edge map.      See also  \nsobel, scharr\n\n  Notes The edge magnitude depends slightly on edge directions, since the approximation of the gradient operator by the Prewitt operator is not completely rotation invariant. For a better rotation invariance, the Scharr operator should be used. The Sobel operator has a better rotation invariance than the Prewitt operator, but a worse rotation invariance than the Scharr operator. Examples >>> from skimage import data\n>>> from skimage import filters\n>>> camera = data.camera()\n>>> edges = filters.prewitt(camera)\n \n"}, {"name": "filters.prewitt_h()", "path": "api/skimage.filters#skimage.filters.prewitt_h", "type": "filters", "text": " \nskimage.filters.prewitt_h(image, mask=None) [source]\n \nFind the horizontal edges of an image using the Prewitt transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Prewitt edge map.     Notes We use the following kernel:  1/3   1/3   1/3\n  0     0     0\n-1/3  -1/3  -1/3\n \n"}, {"name": "filters.prewitt_v()", "path": "api/skimage.filters#skimage.filters.prewitt_v", "type": "filters", "text": " \nskimage.filters.prewitt_v(image, mask=None) [source]\n \nFind the vertical edges of an image using the Prewitt transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Prewitt edge map.     Notes We use the following kernel: 1/3   0  -1/3\n1/3   0  -1/3\n1/3   0  -1/3\n \n"}, {"name": "filters.rank", "path": "api/skimage.filters.rank", "type": "filters", "text": "Module: filters.rank  \nskimage.filters.rank.autolevel(image, selem) Auto-level image using local histogram.  \nskimage.filters.rank.autolevel_percentile(\u2026) Return greyscale local autolevel of an image.  \nskimage.filters.rank.bottomhat(image, selem) Local bottom-hat of an image.  \nskimage.filters.rank.enhance_contrast(image, \u2026) Enhance contrast of an image.  \nskimage.filters.rank.enhance_contrast_percentile(\u2026) Enhance contrast of an image.  \nskimage.filters.rank.entropy(image, selem[, \u2026]) Local entropy.  \nskimage.filters.rank.equalize(image, selem) Equalize image using local histogram.  \nskimage.filters.rank.geometric_mean(image, selem) Return local geometric mean of an image.  \nskimage.filters.rank.gradient(image, selem) Return local gradient of an image (i.e.  \nskimage.filters.rank.gradient_percentile(\u2026) Return local gradient of an image (i.e.  \nskimage.filters.rank.majority(image, selem, *) Majority filter assign to each pixel the most occuring value within its neighborhood.  \nskimage.filters.rank.maximum(image, selem[, \u2026]) Return local maximum of an image.  \nskimage.filters.rank.mean(image, selem[, \u2026]) Return local mean of an image.  \nskimage.filters.rank.mean_bilateral(image, selem) Apply a flat kernel bilateral filter.  \nskimage.filters.rank.mean_percentile(image, \u2026) Return local mean of an image.  \nskimage.filters.rank.median(image[, selem, \u2026]) Return local median of an image.  \nskimage.filters.rank.minimum(image, selem[, \u2026]) Return local minimum of an image.  \nskimage.filters.rank.modal(image, selem[, \u2026]) Return local mode of an image.  \nskimage.filters.rank.noise_filter(image, selem) Noise feature.  \nskimage.filters.rank.otsu(image, selem[, \u2026]) Local Otsu\u2019s threshold value for each pixel.  \nskimage.filters.rank.percentile(image, selem) Return local percentile of an image.  \nskimage.filters.rank.pop(image, selem[, \u2026]) Return the local number (population) of pixels.  \nskimage.filters.rank.pop_bilateral(image, selem) Return the local number (population) of pixels.  \nskimage.filters.rank.pop_percentile(image, selem) Return the local number (population) of pixels.  \nskimage.filters.rank.subtract_mean(image, selem) Return image subtracted from its local mean.  \nskimage.filters.rank.subtract_mean_percentile(\u2026) Return image subtracted from its local mean.  \nskimage.filters.rank.sum(image, selem[, \u2026]) Return the local sum of pixels.  \nskimage.filters.rank.sum_bilateral(image, selem) Apply a flat kernel bilateral filter.  \nskimage.filters.rank.sum_percentile(image, selem) Return the local sum of pixels.  \nskimage.filters.rank.threshold(image, selem) Local threshold of an image.  \nskimage.filters.rank.threshold_percentile(\u2026) Local threshold of an image.  \nskimage.filters.rank.tophat(image, selem[, \u2026]) Local top-hat of an image.  \nskimage.filters.rank.windowed_histogram(\u2026) Normalized sliding window histogram   autolevel  \nskimage.filters.rank.autolevel(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nAuto-level image using local histogram. This filter locally stretches the histogram of gray values to cover the entire range of values from \u201cwhite\u201d to \u201cblack\u201d.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import autolevel\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> auto = autolevel(img, disk(5))\n>>> auto_vol = autolevel(volume, ball(5))\n \n Examples using skimage.filters.rank.autolevel\n \n  Rank filters   autolevel_percentile  \nskimage.filters.rank.autolevel_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn greyscale local autolevel of an image. This filter locally stretches the histogram of greyvalues to cover the entire range of values from \u201cwhite\u201d to \u201cblack\u201d. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n Examples using skimage.filters.rank.autolevel_percentile\n \n  Rank filters   bottomhat  \nskimage.filters.rank.bottomhat(image, selem, out=None, mask=None, shift_x=False, shift_y=False) [source]\n \nLocal bottom-hat of an image. This filter computes the morphological closing of the image and then subtracts the result from the original image.  Parameters \n \nimage2-D array (integer or float) \n\nInput image.  \nselem2-D array (integer or float) \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (integer or float), optional \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint, optional \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.    Warns \n Deprecated:\n\n New in version 0.17.  This function is deprecated and will be removed in scikit-image 0.19. This filter was misnamed and we believe that the usefulness is narrow.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk\n>>> from skimage.filters.rank import bottomhat\n>>> img = data.camera()\n>>> out = bottomhat(img, disk(5))  \n \n enhance_contrast  \nskimage.filters.rank.enhance_contrast(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nEnhance contrast of an image. This replaces each pixel by the local maximum if the pixel gray value is closer to the local maximum than the local minimum. Otherwise it is replaced by the local minimum.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import enhance_contrast\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = enhance_contrast(img, disk(5))\n>>> out_vol = enhance_contrast(volume, ball(5))\n \n Examples using skimage.filters.rank.enhance_contrast\n \n  Rank filters   enhance_contrast_percentile  \nskimage.filters.rank.enhance_contrast_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nEnhance contrast of an image. This replaces each pixel by the local maximum if the pixel greyvalue is closer to the local maximum than the local minimum. Otherwise it is replaced by the local minimum. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n Examples using skimage.filters.rank.enhance_contrast_percentile\n \n  Rank filters   entropy  \nskimage.filters.rank.entropy(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nLocal entropy. The entropy is computed using base 2 logarithm i.e. the filter returns the minimum number of bits needed to encode the local gray level distribution.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (float) \n\nOutput image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Entropy_(information_theory)   Examples >>> from skimage import data\n>>> from skimage.filters.rank import entropy\n>>> from skimage.morphology import disk, ball\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> ent = entropy(img, disk(5))\n>>> ent_vol = entropy(volume, ball(5))\n \n Examples using skimage.filters.rank.entropy\n \n  Tinting gray-scale images  \n\n  Entropy  \n\n  Rank filters   equalize  \nskimage.filters.rank.equalize(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nEqualize image using local histogram.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import equalize\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> equ = equalize(img, disk(5))\n>>> equ_vol = equalize(volume, ball(5))\n \n Examples using skimage.filters.rank.equalize\n \n  Local Histogram Equalization  \n\n  Rank filters   geometric_mean  \nskimage.filters.rank.geometric_mean(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local geometric mean of an image.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     References  \n1  \nGonzalez, R. C. and Wood, R. E. \u201cDigital Image Processing (3rd Edition).\u201d Prentice-Hall Inc, 2006.   Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import mean\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> avg = geometric_mean(img, disk(5))\n>>> avg_vol = geometric_mean(volume, ball(5))\n \n gradient  \nskimage.filters.rank.gradient(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local gradient of an image (i.e. local maximum - local minimum).  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import gradient\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = gradient(img, disk(5))\n>>> out_vol = gradient(volume, ball(5))\n \n Examples using skimage.filters.rank.gradient\n \n  Markers for watershed transform  \n\n  Rank filters   gradient_percentile  \nskimage.filters.rank.gradient_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn local gradient of an image (i.e. local maximum - local minimum). Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n majority  \nskimage.filters.rank.majority(image, selem, *, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nMajority filter assign to each pixel the most occuring value within its neighborhood.  Parameters \n \nimagendarray \n\nImage array (uint8, uint16 array).  \nselem2-D array (integer or float) \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \noutndarray (integer or float), optional \n\nIf None, a new array will be allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint, optional \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.filters.rank import majority\n>>> from skimage.morphology import disk, ball\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> maj_img = majority(img, disk(5))\n>>> maj_img_vol = majority(volume, ball(5))\n \n maximum  \nskimage.filters.rank.maximum(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local maximum of an image.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.      See also  \nskimage.morphology.dilation\n\n  Notes The lower algorithm complexity makes skimage.filters.rank.maximum more efficient for larger images and structuring elements. Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import maximum\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = maximum(img, disk(5))\n>>> out_vol = maximum(volume, ball(5))\n \n Examples using skimage.filters.rank.maximum\n \n  Rank filters   mean  \nskimage.filters.rank.mean(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local mean of an image.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import mean\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> avg = mean(img, disk(5))\n>>> avg_vol = mean(volume, ball(5))\n \n Examples using skimage.filters.rank.mean\n \n  Segment human cells (in mitosis)  \n\n  Rank filters   mean_bilateral  \nskimage.filters.rank.mean_bilateral(image, selem, out=None, mask=None, shift_x=False, shift_y=False, s0=10, s1=10) [source]\n \nApply a flat kernel bilateral filter. This is an edge-preserving and noise reducing denoising filter. It averages pixels based on their spatial closeness and radiometric similarity. Spatial closeness is measured by considering only the local pixel neighborhood given by a structuring element. Radiometric similarity is defined by the greylevel interval [g-s0, g+s1] where g is the current pixel greylevel. Only pixels belonging to the structuring element and having a greylevel inside this interval are averaged.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \ns0, s1int \n\nDefine the [s0, s1] interval around the greyvalue of the center pixel to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.      See also  \ndenoise_bilateral \n  Examples >>> from skimage import data\n>>> from skimage.morphology import disk\n>>> from skimage.filters.rank import mean_bilateral\n>>> img = data.camera().astype(np.uint16)\n>>> bilat_img = mean_bilateral(img, disk(20), s0=10,s1=10)\n \n Examples using skimage.filters.rank.mean_bilateral\n \n  Rank filters   mean_percentile  \nskimage.filters.rank.mean_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn local mean of an image. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n median  \nskimage.filters.rank.median(image, selem=None, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local median of an image.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s. If None, a full square of size 3 is used.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.      See also  \nskimage.filters.median\n\n\nImplementation of a median filtering which handles images with floating precision.    Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import median\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> med = median(img, disk(5))\n>>> med_vol = median(volume, ball(5))\n \n Examples using skimage.filters.rank.median\n \n  Markers for watershed transform  \n\n  Rank filters   minimum  \nskimage.filters.rank.minimum(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local minimum of an image.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.      See also  \nskimage.morphology.erosion\n\n  Notes The lower algorithm complexity makes skimage.filters.rank.minimum more efficient for larger images and structuring elements. Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import minimum\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = minimum(img, disk(5))\n>>> out_vol = minimum(volume, ball(5))\n \n Examples using skimage.filters.rank.minimum\n \n  Rank filters   modal  \nskimage.filters.rank.modal(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local mode of an image. The mode is the value that appears most often in the local histogram.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import modal\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = modal(img, disk(5))\n>>> out_vol = modal(volume, ball(5))\n \n noise_filter  \nskimage.filters.rank.noise_filter(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nNoise feature.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     References  \n1  \nN. Hashimoto et al. Referenceless image quality evaluation for whole slide imaging. J Pathol Inform 2012;3:9.   Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import noise_filter\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = noise_filter(img, disk(5))\n>>> out_vol = noise_filter(volume, ball(5))\n \n otsu  \nskimage.filters.rank.otsu(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nLocal Otsu\u2019s threshold value for each pixel.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Otsu\u2019s_method   Examples >>> from skimage import data\n>>> from skimage.filters.rank import otsu\n>>> from skimage.morphology import disk, ball\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> local_otsu = otsu(img, disk(5))\n>>> thresh_image = img >= local_otsu\n>>> local_otsu_vol = otsu(volume, ball(5))\n>>> thresh_image_vol = volume >= local_otsu_vol\n \n Examples using skimage.filters.rank.otsu\n \n  Rank filters   percentile  \nskimage.filters.rank.percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0) [source]\n \nReturn local percentile of an image. Returns the value of the p0 lower percentile of the local greyvalue distribution. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0float in [0, \u2026, 1] \n\nSet the percentile value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n pop  \nskimage.filters.rank.pop(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn the local number (population) of pixels. The number of pixels is defined as the number of pixels which are included in the structuring element and the mask.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage.morphology import square, cube # Need to add 3D example\n>>> import skimage.filters.rank as rank\n>>> img = 255 * np.array([[0, 0, 0, 0, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 0, 0, 0, 0]], dtype=np.uint8)\n>>> rank.pop(img, square(3))\narray([[4, 6, 6, 6, 4],\n       [6, 9, 9, 9, 6],\n       [6, 9, 9, 9, 6],\n       [6, 9, 9, 9, 6],\n       [4, 6, 6, 6, 4]], dtype=uint8)\n \n pop_bilateral  \nskimage.filters.rank.pop_bilateral(image, selem, out=None, mask=None, shift_x=False, shift_y=False, s0=10, s1=10) [source]\n \nReturn the local number (population) of pixels. The number of pixels is defined as the number of pixels which are included in the structuring element and the mask. Additionally pixels must have a greylevel inside the interval [g-s0, g+s1] where g is the greyvalue of the center pixel.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \ns0, s1int \n\nDefine the [s0, s1] interval around the greyvalue of the center pixel to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     Examples >>> from skimage.morphology import square\n>>> import skimage.filters.rank as rank\n>>> img = 255 * np.array([[0, 0, 0, 0, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 0, 0, 0, 0]], dtype=np.uint16)\n>>> rank.pop_bilateral(img, square(3), s0=10, s1=10)\narray([[3, 4, 3, 4, 3],\n       [4, 4, 6, 4, 4],\n       [3, 6, 9, 6, 3],\n       [4, 4, 6, 4, 4],\n       [3, 4, 3, 4, 3]], dtype=uint16)\n \n pop_percentile  \nskimage.filters.rank.pop_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn the local number (population) of pixels. The number of pixels is defined as the number of pixels which are included in the structuring element and the mask. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n subtract_mean  \nskimage.filters.rank.subtract_mean(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn image subtracted from its local mean.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Notes Subtracting the mean value may introduce underflow. To compensate this potential underflow, the obtained difference is downscaled by a factor of 2 and shifted by n_bins / 2 - 1, the median value of the local histogram (n_bins = max(3, image.max()) +1 for 16-bits images and 256 otherwise). Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import subtract_mean\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = subtract_mean(img, disk(5))\n>>> out_vol = subtract_mean(volume, ball(5))\n \n subtract_mean_percentile  \nskimage.filters.rank.subtract_mean_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn image subtracted from its local mean. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n sum  \nskimage.filters.rank.sum(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn the local sum of pixels. Note that the sum may overflow depending on the data type of the input array.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage.morphology import square, cube # Need to add 3D example\n>>> import skimage.filters.rank as rank         # Cube seems to fail but\n>>> img = np.array([[0, 0, 0, 0, 0],            # Ball can pass\n...                 [0, 1, 1, 1, 0],\n...                 [0, 1, 1, 1, 0],\n...                 [0, 1, 1, 1, 0],\n...                 [0, 0, 0, 0, 0]], dtype=np.uint8)\n>>> rank.sum(img, square(3))\narray([[1, 2, 3, 2, 1],\n       [2, 4, 6, 4, 2],\n       [3, 6, 9, 6, 3],\n       [2, 4, 6, 4, 2],\n       [1, 2, 3, 2, 1]], dtype=uint8)\n \n sum_bilateral  \nskimage.filters.rank.sum_bilateral(image, selem, out=None, mask=None, shift_x=False, shift_y=False, s0=10, s1=10) [source]\n \nApply a flat kernel bilateral filter. This is an edge-preserving and noise reducing denoising filter. It averages pixels based on their spatial closeness and radiometric similarity. Spatial closeness is measured by considering only the local pixel neighborhood given by a structuring element (selem). Radiometric similarity is defined by the greylevel interval [g-s0, g+s1] where g is the current pixel greylevel. Only pixels belonging to the structuring element AND having a greylevel inside this interval are summed. Note that the sum may overflow depending on the data type of the input array.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \ns0, s1int \n\nDefine the [s0, s1] interval around the greyvalue of the center pixel to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.      See also  \ndenoise_bilateral \n  Examples >>> from skimage import data\n>>> from skimage.morphology import disk\n>>> from skimage.filters.rank import sum_bilateral\n>>> img = data.camera().astype(np.uint16)\n>>> bilat_img = sum_bilateral(img, disk(10), s0=10, s1=10)\n \n sum_percentile  \nskimage.filters.rank.sum_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn the local sum of pixels. Only greyvalues between percentiles [p0, p1] are considered in the filter. Note that the sum may overflow depending on the data type of the input array.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n threshold  \nskimage.filters.rank.threshold(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nLocal threshold of an image. The resulting binary mask is True if the gray value of the center pixel is greater than the local mean.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage.morphology import square, cube # Need to add 3D example\n>>> from skimage.filters.rank import threshold\n>>> img = 255 * np.array([[0, 0, 0, 0, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 0, 0, 0, 0]], dtype=np.uint8)\n>>> threshold(img, square(3))\narray([[0, 0, 0, 0, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 0, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n \n threshold_percentile  \nskimage.filters.rank.threshold_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0) [source]\n \nLocal threshold of an image. The resulting binary mask is True if the greyvalue of the center pixel is greater than the local mean. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0float in [0, \u2026, 1] \n\nSet the percentile value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n tophat  \nskimage.filters.rank.tophat(image, selem, out=None, mask=None, shift_x=False, shift_y=False) [source]\n \nLocal top-hat of an image. This filter computes the morphological opening of the image and then subtracts the result from the original image.  Parameters \n \nimage2-D array (integer or float) \n\nInput image.  \nselem2-D array (integer or float) \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (integer or float), optional \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint, optional \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.    Warns \n Deprecated:\n\n New in version 0.17.  This function is deprecated and will be removed in scikit-image 0.19. This filter was misnamed and we believe that the usefulness is narrow.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk\n>>> from skimage.filters.rank import tophat\n>>> img = data.camera()\n>>> out = tophat(img, disk(5))  \n \n windowed_histogram  \nskimage.filters.rank.windowed_histogram(image, selem, out=None, mask=None, shift_x=False, shift_y=False, n_bins=None) [source]\n \nNormalized sliding window histogram  Parameters \n \nimage2-D array (integer or float) \n\nInput image.  \nselem2-D array (integer or float) \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (integer or float), optional \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint, optional \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \nn_binsint or None \n\nThe number of histogram bins. Will default to image.max() + 1 if None is passed.    Returns \n \nout3-D array (float) \n\nArray of dimensions (H,W,N), where (H,W) are the dimensions of the input image and N is n_bins or image.max() + 1 if no value is provided as a parameter. Effectively, each pixel is a N-D feature vector that is the histogram. The sum of the elements in the feature vector will be 1, unless no pixels in the window were covered by both selem and mask, in which case all elements will be 0.     Examples >>> from skimage import data\n>>> from skimage.filters.rank import windowed_histogram\n>>> from skimage.morphology import disk, ball\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> hist_img = windowed_histogram(img, disk(5))\n \n\n"}, {"name": "filters.rank.autolevel()", "path": "api/skimage.filters.rank#skimage.filters.rank.autolevel", "type": "filters", "text": " \nskimage.filters.rank.autolevel(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nAuto-level image using local histogram. This filter locally stretches the histogram of gray values to cover the entire range of values from \u201cwhite\u201d to \u201cblack\u201d.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import autolevel\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> auto = autolevel(img, disk(5))\n>>> auto_vol = autolevel(volume, ball(5))\n \n"}, {"name": "filters.rank.autolevel_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.autolevel_percentile", "type": "filters", "text": " \nskimage.filters.rank.autolevel_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn greyscale local autolevel of an image. This filter locally stretches the histogram of greyvalues to cover the entire range of values from \u201cwhite\u201d to \u201cblack\u201d. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n"}, {"name": "filters.rank.bottomhat()", "path": "api/skimage.filters.rank#skimage.filters.rank.bottomhat", "type": "filters", "text": " \nskimage.filters.rank.bottomhat(image, selem, out=None, mask=None, shift_x=False, shift_y=False) [source]\n \nLocal bottom-hat of an image. This filter computes the morphological closing of the image and then subtracts the result from the original image.  Parameters \n \nimage2-D array (integer or float) \n\nInput image.  \nselem2-D array (integer or float) \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (integer or float), optional \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint, optional \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.    Warns \n Deprecated:\n\n New in version 0.17.  This function is deprecated and will be removed in scikit-image 0.19. This filter was misnamed and we believe that the usefulness is narrow.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk\n>>> from skimage.filters.rank import bottomhat\n>>> img = data.camera()\n>>> out = bottomhat(img, disk(5))  \n \n"}, {"name": "filters.rank.enhance_contrast()", "path": "api/skimage.filters.rank#skimage.filters.rank.enhance_contrast", "type": "filters", "text": " \nskimage.filters.rank.enhance_contrast(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nEnhance contrast of an image. This replaces each pixel by the local maximum if the pixel gray value is closer to the local maximum than the local minimum. Otherwise it is replaced by the local minimum.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import enhance_contrast\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = enhance_contrast(img, disk(5))\n>>> out_vol = enhance_contrast(volume, ball(5))\n \n"}, {"name": "filters.rank.enhance_contrast_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.enhance_contrast_percentile", "type": "filters", "text": " \nskimage.filters.rank.enhance_contrast_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nEnhance contrast of an image. This replaces each pixel by the local maximum if the pixel greyvalue is closer to the local maximum than the local minimum. Otherwise it is replaced by the local minimum. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n"}, {"name": "filters.rank.entropy()", "path": "api/skimage.filters.rank#skimage.filters.rank.entropy", "type": "filters", "text": " \nskimage.filters.rank.entropy(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nLocal entropy. The entropy is computed using base 2 logarithm i.e. the filter returns the minimum number of bits needed to encode the local gray level distribution.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (float) \n\nOutput image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Entropy_(information_theory)   Examples >>> from skimage import data\n>>> from skimage.filters.rank import entropy\n>>> from skimage.morphology import disk, ball\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> ent = entropy(img, disk(5))\n>>> ent_vol = entropy(volume, ball(5))\n \n"}, {"name": "filters.rank.equalize()", "path": "api/skimage.filters.rank#skimage.filters.rank.equalize", "type": "filters", "text": " \nskimage.filters.rank.equalize(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nEqualize image using local histogram.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import equalize\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> equ = equalize(img, disk(5))\n>>> equ_vol = equalize(volume, ball(5))\n \n"}, {"name": "filters.rank.geometric_mean()", "path": "api/skimage.filters.rank#skimage.filters.rank.geometric_mean", "type": "filters", "text": " \nskimage.filters.rank.geometric_mean(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local geometric mean of an image.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     References  \n1  \nGonzalez, R. C. and Wood, R. E. \u201cDigital Image Processing (3rd Edition).\u201d Prentice-Hall Inc, 2006.   Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import mean\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> avg = geometric_mean(img, disk(5))\n>>> avg_vol = geometric_mean(volume, ball(5))\n \n"}, {"name": "filters.rank.gradient()", "path": "api/skimage.filters.rank#skimage.filters.rank.gradient", "type": "filters", "text": " \nskimage.filters.rank.gradient(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local gradient of an image (i.e. local maximum - local minimum).  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import gradient\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = gradient(img, disk(5))\n>>> out_vol = gradient(volume, ball(5))\n \n"}, {"name": "filters.rank.gradient_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.gradient_percentile", "type": "filters", "text": " \nskimage.filters.rank.gradient_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn local gradient of an image (i.e. local maximum - local minimum). Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n"}, {"name": "filters.rank.majority()", "path": "api/skimage.filters.rank#skimage.filters.rank.majority", "type": "filters", "text": " \nskimage.filters.rank.majority(image, selem, *, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nMajority filter assign to each pixel the most occuring value within its neighborhood.  Parameters \n \nimagendarray \n\nImage array (uint8, uint16 array).  \nselem2-D array (integer or float) \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \noutndarray (integer or float), optional \n\nIf None, a new array will be allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint, optional \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.filters.rank import majority\n>>> from skimage.morphology import disk, ball\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> maj_img = majority(img, disk(5))\n>>> maj_img_vol = majority(volume, ball(5))\n \n"}, {"name": "filters.rank.maximum()", "path": "api/skimage.filters.rank#skimage.filters.rank.maximum", "type": "filters", "text": " \nskimage.filters.rank.maximum(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local maximum of an image.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.      See also  \nskimage.morphology.dilation\n\n  Notes The lower algorithm complexity makes skimage.filters.rank.maximum more efficient for larger images and structuring elements. Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import maximum\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = maximum(img, disk(5))\n>>> out_vol = maximum(volume, ball(5))\n \n"}, {"name": "filters.rank.mean()", "path": "api/skimage.filters.rank#skimage.filters.rank.mean", "type": "filters", "text": " \nskimage.filters.rank.mean(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local mean of an image.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import mean\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> avg = mean(img, disk(5))\n>>> avg_vol = mean(volume, ball(5))\n \n"}, {"name": "filters.rank.mean_bilateral()", "path": "api/skimage.filters.rank#skimage.filters.rank.mean_bilateral", "type": "filters", "text": " \nskimage.filters.rank.mean_bilateral(image, selem, out=None, mask=None, shift_x=False, shift_y=False, s0=10, s1=10) [source]\n \nApply a flat kernel bilateral filter. This is an edge-preserving and noise reducing denoising filter. It averages pixels based on their spatial closeness and radiometric similarity. Spatial closeness is measured by considering only the local pixel neighborhood given by a structuring element. Radiometric similarity is defined by the greylevel interval [g-s0, g+s1] where g is the current pixel greylevel. Only pixels belonging to the structuring element and having a greylevel inside this interval are averaged.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \ns0, s1int \n\nDefine the [s0, s1] interval around the greyvalue of the center pixel to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.      See also  \ndenoise_bilateral \n  Examples >>> from skimage import data\n>>> from skimage.morphology import disk\n>>> from skimage.filters.rank import mean_bilateral\n>>> img = data.camera().astype(np.uint16)\n>>> bilat_img = mean_bilateral(img, disk(20), s0=10,s1=10)\n \n"}, {"name": "filters.rank.mean_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.mean_percentile", "type": "filters", "text": " \nskimage.filters.rank.mean_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn local mean of an image. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n"}, {"name": "filters.rank.median()", "path": "api/skimage.filters.rank#skimage.filters.rank.median", "type": "filters", "text": " \nskimage.filters.rank.median(image, selem=None, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local median of an image.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s. If None, a full square of size 3 is used.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.      See also  \nskimage.filters.median\n\n\nImplementation of a median filtering which handles images with floating precision.    Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import median\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> med = median(img, disk(5))\n>>> med_vol = median(volume, ball(5))\n \n"}, {"name": "filters.rank.minimum()", "path": "api/skimage.filters.rank#skimage.filters.rank.minimum", "type": "filters", "text": " \nskimage.filters.rank.minimum(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local minimum of an image.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.      See also  \nskimage.morphology.erosion\n\n  Notes The lower algorithm complexity makes skimage.filters.rank.minimum more efficient for larger images and structuring elements. Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import minimum\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = minimum(img, disk(5))\n>>> out_vol = minimum(volume, ball(5))\n \n"}, {"name": "filters.rank.modal()", "path": "api/skimage.filters.rank#skimage.filters.rank.modal", "type": "filters", "text": " \nskimage.filters.rank.modal(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn local mode of an image. The mode is the value that appears most often in the local histogram.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import modal\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = modal(img, disk(5))\n>>> out_vol = modal(volume, ball(5))\n \n"}, {"name": "filters.rank.noise_filter()", "path": "api/skimage.filters.rank#skimage.filters.rank.noise_filter", "type": "filters", "text": " \nskimage.filters.rank.noise_filter(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nNoise feature.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     References  \n1  \nN. Hashimoto et al. Referenceless image quality evaluation for whole slide imaging. J Pathol Inform 2012;3:9.   Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import noise_filter\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = noise_filter(img, disk(5))\n>>> out_vol = noise_filter(volume, ball(5))\n \n"}, {"name": "filters.rank.otsu()", "path": "api/skimage.filters.rank#skimage.filters.rank.otsu", "type": "filters", "text": " \nskimage.filters.rank.otsu(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nLocal Otsu\u2019s threshold value for each pixel.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Otsu\u2019s_method   Examples >>> from skimage import data\n>>> from skimage.filters.rank import otsu\n>>> from skimage.morphology import disk, ball\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> local_otsu = otsu(img, disk(5))\n>>> thresh_image = img >= local_otsu\n>>> local_otsu_vol = otsu(volume, ball(5))\n>>> thresh_image_vol = volume >= local_otsu_vol\n \n"}, {"name": "filters.rank.percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.percentile", "type": "filters", "text": " \nskimage.filters.rank.percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0) [source]\n \nReturn local percentile of an image. Returns the value of the p0 lower percentile of the local greyvalue distribution. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0float in [0, \u2026, 1] \n\nSet the percentile value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n"}, {"name": "filters.rank.pop()", "path": "api/skimage.filters.rank#skimage.filters.rank.pop", "type": "filters", "text": " \nskimage.filters.rank.pop(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn the local number (population) of pixels. The number of pixels is defined as the number of pixels which are included in the structuring element and the mask.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage.morphology import square, cube # Need to add 3D example\n>>> import skimage.filters.rank as rank\n>>> img = 255 * np.array([[0, 0, 0, 0, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 0, 0, 0, 0]], dtype=np.uint8)\n>>> rank.pop(img, square(3))\narray([[4, 6, 6, 6, 4],\n       [6, 9, 9, 9, 6],\n       [6, 9, 9, 9, 6],\n       [6, 9, 9, 9, 6],\n       [4, 6, 6, 6, 4]], dtype=uint8)\n \n"}, {"name": "filters.rank.pop_bilateral()", "path": "api/skimage.filters.rank#skimage.filters.rank.pop_bilateral", "type": "filters", "text": " \nskimage.filters.rank.pop_bilateral(image, selem, out=None, mask=None, shift_x=False, shift_y=False, s0=10, s1=10) [source]\n \nReturn the local number (population) of pixels. The number of pixels is defined as the number of pixels which are included in the structuring element and the mask. Additionally pixels must have a greylevel inside the interval [g-s0, g+s1] where g is the greyvalue of the center pixel.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \ns0, s1int \n\nDefine the [s0, s1] interval around the greyvalue of the center pixel to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     Examples >>> from skimage.morphology import square\n>>> import skimage.filters.rank as rank\n>>> img = 255 * np.array([[0, 0, 0, 0, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 0, 0, 0, 0]], dtype=np.uint16)\n>>> rank.pop_bilateral(img, square(3), s0=10, s1=10)\narray([[3, 4, 3, 4, 3],\n       [4, 4, 6, 4, 4],\n       [3, 6, 9, 6, 3],\n       [4, 4, 6, 4, 4],\n       [3, 4, 3, 4, 3]], dtype=uint16)\n \n"}, {"name": "filters.rank.pop_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.pop_percentile", "type": "filters", "text": " \nskimage.filters.rank.pop_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn the local number (population) of pixels. The number of pixels is defined as the number of pixels which are included in the structuring element and the mask. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n"}, {"name": "filters.rank.subtract_mean()", "path": "api/skimage.filters.rank#skimage.filters.rank.subtract_mean", "type": "filters", "text": " \nskimage.filters.rank.subtract_mean(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn image subtracted from its local mean.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Notes Subtracting the mean value may introduce underflow. To compensate this potential underflow, the obtained difference is downscaled by a factor of 2 and shifted by n_bins / 2 - 1, the median value of the local histogram (n_bins = max(3, image.max()) +1 for 16-bits images and 256 otherwise). Examples >>> from skimage import data\n>>> from skimage.morphology import disk, ball\n>>> from skimage.filters.rank import subtract_mean\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> out = subtract_mean(img, disk(5))\n>>> out_vol = subtract_mean(volume, ball(5))\n \n"}, {"name": "filters.rank.subtract_mean_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.subtract_mean_percentile", "type": "filters", "text": " \nskimage.filters.rank.subtract_mean_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn image subtracted from its local mean. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n"}, {"name": "filters.rank.sum()", "path": "api/skimage.filters.rank#skimage.filters.rank.sum", "type": "filters", "text": " \nskimage.filters.rank.sum(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nReturn the local sum of pixels. Note that the sum may overflow depending on the data type of the input array.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage.morphology import square, cube # Need to add 3D example\n>>> import skimage.filters.rank as rank         # Cube seems to fail but\n>>> img = np.array([[0, 0, 0, 0, 0],            # Ball can pass\n...                 [0, 1, 1, 1, 0],\n...                 [0, 1, 1, 1, 0],\n...                 [0, 1, 1, 1, 0],\n...                 [0, 0, 0, 0, 0]], dtype=np.uint8)\n>>> rank.sum(img, square(3))\narray([[1, 2, 3, 2, 1],\n       [2, 4, 6, 4, 2],\n       [3, 6, 9, 6, 3],\n       [2, 4, 6, 4, 2],\n       [1, 2, 3, 2, 1]], dtype=uint8)\n \n"}, {"name": "filters.rank.sum_bilateral()", "path": "api/skimage.filters.rank#skimage.filters.rank.sum_bilateral", "type": "filters", "text": " \nskimage.filters.rank.sum_bilateral(image, selem, out=None, mask=None, shift_x=False, shift_y=False, s0=10, s1=10) [source]\n \nApply a flat kernel bilateral filter. This is an edge-preserving and noise reducing denoising filter. It averages pixels based on their spatial closeness and radiometric similarity. Spatial closeness is measured by considering only the local pixel neighborhood given by a structuring element (selem). Radiometric similarity is defined by the greylevel interval [g-s0, g+s1] where g is the current pixel greylevel. Only pixels belonging to the structuring element AND having a greylevel inside this interval are summed. Note that the sum may overflow depending on the data type of the input array.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \ns0, s1int \n\nDefine the [s0, s1] interval around the greyvalue of the center pixel to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.      See also  \ndenoise_bilateral \n  Examples >>> from skimage import data\n>>> from skimage.morphology import disk\n>>> from skimage.filters.rank import sum_bilateral\n>>> img = data.camera().astype(np.uint16)\n>>> bilat_img = sum_bilateral(img, disk(10), s0=10, s1=10)\n \n"}, {"name": "filters.rank.sum_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.sum_percentile", "type": "filters", "text": " \nskimage.filters.rank.sum_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0, p1=1) [source]\n \nReturn the local sum of pixels. Only greyvalues between percentiles [p0, p1] are considered in the filter. Note that the sum may overflow depending on the data type of the input array.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0, p1float in [0, \u2026, 1] \n\nDefine the [p0, p1] percentile interval to be considered for computing the value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n"}, {"name": "filters.rank.threshold()", "path": "api/skimage.filters.rank#skimage.filters.rank.threshold", "type": "filters", "text": " \nskimage.filters.rank.threshold(image, selem, out=None, mask=None, shift_x=False, shift_y=False, shift_z=False) [source]\n \nLocal threshold of an image. The resulting binary mask is True if the gray value of the center pixel is greater than the local mean.  Parameters \n \nimage([P,] M, N) ndarray (uint8, uint16) \n\nInput image.  \nselemndarray \n\nThe neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.  \nout([P,] M, N) array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_y, shift_zint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout([P,] M, N) ndarray (same dtype as input image) \n\nOutput image.     Examples >>> from skimage.morphology import square, cube # Need to add 3D example\n>>> from skimage.filters.rank import threshold\n>>> img = 255 * np.array([[0, 0, 0, 0, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 1, 1, 1, 0],\n...                       [0, 0, 0, 0, 0]], dtype=np.uint8)\n>>> threshold(img, square(3))\narray([[0, 0, 0, 0, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 0, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "filters.rank.threshold_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.threshold_percentile", "type": "filters", "text": " \nskimage.filters.rank.threshold_percentile(image, selem, out=None, mask=None, shift_x=False, shift_y=False, p0=0) [source]\n \nLocal threshold of an image. The resulting binary mask is True if the greyvalue of the center pixel is greater than the local mean. Only greyvalues between percentiles [p0, p1] are considered in the filter.  Parameters \n \nimage2-D array (uint8, uint16) \n\nInput image.  \nselem2-D array \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (same dtype as input) \n\nIf None, a new array is allocated.  \nmaskndarray \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \np0float in [0, \u2026, 1] \n\nSet the percentile value.    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.     \n"}, {"name": "filters.rank.tophat()", "path": "api/skimage.filters.rank#skimage.filters.rank.tophat", "type": "filters", "text": " \nskimage.filters.rank.tophat(image, selem, out=None, mask=None, shift_x=False, shift_y=False) [source]\n \nLocal top-hat of an image. This filter computes the morphological opening of the image and then subtracts the result from the original image.  Parameters \n \nimage2-D array (integer or float) \n\nInput image.  \nselem2-D array (integer or float) \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (integer or float), optional \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint, optional \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).    Returns \n \nout2-D array (same dtype as input image) \n\nOutput image.    Warns \n Deprecated:\n\n New in version 0.17.  This function is deprecated and will be removed in scikit-image 0.19. This filter was misnamed and we believe that the usefulness is narrow.     Examples >>> from skimage import data\n>>> from skimage.morphology import disk\n>>> from skimage.filters.rank import tophat\n>>> img = data.camera()\n>>> out = tophat(img, disk(5))  \n \n"}, {"name": "filters.rank.windowed_histogram()", "path": "api/skimage.filters.rank#skimage.filters.rank.windowed_histogram", "type": "filters", "text": " \nskimage.filters.rank.windowed_histogram(image, selem, out=None, mask=None, shift_x=False, shift_y=False, n_bins=None) [source]\n \nNormalized sliding window histogram  Parameters \n \nimage2-D array (integer or float) \n\nInput image.  \nselem2-D array (integer or float) \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.  \nout2-D array (integer or float), optional \n\nIf None, a new array is allocated.  \nmaskndarray (integer or float), optional \n\nMask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).  \nshift_x, shift_yint, optional \n\nOffset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).  \nn_binsint or None \n\nThe number of histogram bins. Will default to image.max() + 1 if None is passed.    Returns \n \nout3-D array (float) \n\nArray of dimensions (H,W,N), where (H,W) are the dimensions of the input image and N is n_bins or image.max() + 1 if no value is provided as a parameter. Effectively, each pixel is a N-D feature vector that is the histogram. The sum of the elements in the feature vector will be 1, unless no pixels in the window were covered by both selem and mask, in which case all elements will be 0.     Examples >>> from skimage import data\n>>> from skimage.filters.rank import windowed_histogram\n>>> from skimage.morphology import disk, ball\n>>> import numpy as np\n>>> img = data.camera()\n>>> volume = np.random.randint(0, 255, size=(10,10,10), dtype=np.uint8)\n>>> hist_img = windowed_histogram(img, disk(5))\n \n"}, {"name": "filters.rank_order()", "path": "api/skimage.filters#skimage.filters.rank_order", "type": "filters", "text": " \nskimage.filters.rank_order(image) [source]\n \nReturn an image of the same shape where each pixel is the index of the pixel value in the ascending order of the unique values of image, aka the rank-order value.  Parameters \n \nimagendarray \n  Returns \n \nlabelsndarray of type np.uint32, of shape image.shape \n\nNew array where each pixel has the rank-order value of the corresponding pixel in image. Pixel values are between 0 and n - 1, where n is the number of distinct unique values in image.  \noriginal_values1-D ndarray \n\nUnique original values of image     Examples >>> a = np.array([[1, 4, 5], [4, 4, 1], [5, 1, 1]])\n>>> a\narray([[1, 4, 5],\n       [4, 4, 1],\n       [5, 1, 1]])\n>>> rank_order(a)\n(array([[0, 1, 2],\n       [1, 1, 0],\n       [2, 0, 0]], dtype=uint32), array([1, 4, 5]))\n>>> b = np.array([-1., 2.5, 3.1, 2.5])\n>>> rank_order(b)\n(array([0, 1, 2, 1], dtype=uint32), array([-1. ,  2.5,  3.1]))\n \n"}, {"name": "filters.roberts()", "path": "api/skimage.filters#skimage.filters.roberts", "type": "filters", "text": " \nskimage.filters.roberts(image, mask=None) [source]\n \nFind the edge magnitude using Roberts\u2019 cross operator.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Roberts\u2019 Cross edge map.      See also  \nsobel, scharr, prewitt, feature.canny \n  Examples >>> from skimage import data\n>>> camera = data.camera()\n>>> from skimage import filters\n>>> edges = filters.roberts(camera)\n \n"}, {"name": "filters.roberts_neg_diag()", "path": "api/skimage.filters#skimage.filters.roberts_neg_diag", "type": "filters", "text": " \nskimage.filters.roberts_neg_diag(image, mask=None) [source]\n \nFind the cross edges of an image using the Roberts\u2019 Cross operator. The kernel is applied to the input image to produce separate measurements of the gradient component one orientation.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Robert\u2019s edge map.     Notes We use the following kernel:  0   1\n-1   0\n \n"}, {"name": "filters.roberts_pos_diag()", "path": "api/skimage.filters#skimage.filters.roberts_pos_diag", "type": "filters", "text": " \nskimage.filters.roberts_pos_diag(image, mask=None) [source]\n \nFind the cross edges of an image using Roberts\u2019 cross operator. The kernel is applied to the input image to produce separate measurements of the gradient component one orientation.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Robert\u2019s edge map.     Notes We use the following kernel: 1   0\n0  -1\n \n"}, {"name": "filters.sato()", "path": "api/skimage.filters#skimage.filters.sato", "type": "filters", "text": " \nskimage.filters.sato(image, sigmas=range(1, 10, 2), black_ridges=True, mode=None, cval=0) [source]\n \nFilter an image with the Sato tubeness filter. This filter can be used to detect continuous ridges, e.g. tubes, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects. Defined only for 2-D and 3-D images. Calculates the eigenvectors of the Hessian to compute the similarity of an image region to tubes, according to the method described in [1].  Parameters \n \nimage(N, M[, P]) ndarray \n\nArray with input image data.  \nsigmasiterable of floats, optional \n\nSigmas used as scales of filter.  \nblack_ridgesboolean, optional \n\nWhen True (the default), the filter detects black ridges; when False, it detects white ridges.  \nmode{\u2018constant\u2019, \u2018reflect\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018mirror\u2019}, optional \n\nHow to handle values outside the image borders.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.    Returns \n \nout(N, M[, P]) ndarray \n\nFiltered image (maximum of pixels across all scales).      See also  \nmeijering\n\n\nfrangi\n\n\nhessian\n\n  References  \n1  \nSato, Y., Nakajima, S., Shiraga, N., Atsumi, H., Yoshida, S., Koller, T., \u2026, Kikinis, R. (1998). Three-dimensional multi-scale line filter for segmentation and visualization of curvilinear structures in medical images. Medical image analysis, 2(2), 143-168. DOI:10.1016/S1361-8415(98)80009-1   \n"}, {"name": "filters.scharr()", "path": "api/skimage.filters#skimage.filters.scharr", "type": "filters", "text": " \nskimage.filters.scharr(image, mask=None, *, axis=None, mode='reflect', cval=0.0) [source]\n \nFind the edge magnitude using the Scharr transform.  Parameters \n \nimagearray \n\nThe input image.  \nmaskarray of bool, optional \n\nClip the output image to this mask. (Values where mask=0 will be set to 0.)  \naxisint or sequence of int, optional \n\nCompute the edge filter along this axis. If not provided, the edge magnitude is computed. This is defined as: sch_mag = np.sqrt(sum([scharr(image, axis=i)**2\n                       for i in range(image.ndim)]) / image.ndim)\n The magnitude is also computed if axis is a sequence.  \nmodestr or sequence of str, optional \n\nThe boundary mode for the convolution. See scipy.ndimage.convolve for a description of the modes. This can be either a single boundary mode or one boundary mode per axis.  \ncvalfloat, optional \n\nWhen mode is 'constant', this is the constant used in values outside the boundary of the image data.    Returns \n \noutputarray of float \n\nThe Scharr edge map.      See also  \nsobel, prewitt, canny \n  Notes The Scharr operator has a better rotation invariance than other edge filters such as the Sobel or the Prewitt operators. References  \n1  \nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives.  \n2  \nhttps://en.wikipedia.org/wiki/Sobel_operator#Alternative_operators   Examples >>> from skimage import data\n>>> from skimage import filters\n>>> camera = data.camera()\n>>> edges = filters.scharr(camera)\n \n"}, {"name": "filters.scharr_h()", "path": "api/skimage.filters#skimage.filters.scharr_h", "type": "filters", "text": " \nskimage.filters.scharr_h(image, mask=None) [source]\n \nFind the horizontal edges of an image using the Scharr transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Scharr edge map.     Notes We use the following kernel:  3   10   3\n 0    0   0\n-3  -10  -3\n References  \n1  \nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives.   \n"}, {"name": "filters.scharr_v()", "path": "api/skimage.filters#skimage.filters.scharr_v", "type": "filters", "text": " \nskimage.filters.scharr_v(image, mask=None) [source]\n \nFind the vertical edges of an image using the Scharr transform.  Parameters \n \nimage2-D array \n\nImage to process  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Scharr edge map.     Notes We use the following kernel:  3   0   -3\n10   0  -10\n 3   0   -3\n References  \n1  \nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives.   \n"}, {"name": "filters.sobel()", "path": "api/skimage.filters#skimage.filters.sobel", "type": "filters", "text": " \nskimage.filters.sobel(image, mask=None, *, axis=None, mode='reflect', cval=0.0) [source]\n \nFind edges in an image using the Sobel filter.  Parameters \n \nimagearray \n\nThe input image.  \nmaskarray of bool, optional \n\nClip the output image to this mask. (Values where mask=0 will be set to 0.)  \naxisint or sequence of int, optional \n\nCompute the edge filter along this axis. If not provided, the edge magnitude is computed. This is defined as: sobel_mag = np.sqrt(sum([sobel(image, axis=i)**2\n                         for i in range(image.ndim)]) / image.ndim)\n The magnitude is also computed if axis is a sequence.  \nmodestr or sequence of str, optional \n\nThe boundary mode for the convolution. See scipy.ndimage.convolve for a description of the modes. This can be either a single boundary mode or one boundary mode per axis.  \ncvalfloat, optional \n\nWhen mode is 'constant', this is the constant used in values outside the boundary of the image data.    Returns \n \noutputarray of float \n\nThe Sobel edge map.      See also  \nscharr, prewitt, canny \n  References  \n1  \nD. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives.  \n2  \nhttps://en.wikipedia.org/wiki/Sobel_operator   Examples >>> from skimage import data\n>>> from skimage import filters\n>>> camera = data.camera()\n>>> edges = filters.sobel(camera)\n \n"}, {"name": "filters.sobel_h()", "path": "api/skimage.filters#skimage.filters.sobel_h", "type": "filters", "text": " \nskimage.filters.sobel_h(image, mask=None) [source]\n \nFind the horizontal edges of an image using the Sobel transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Sobel edge map.     Notes We use the following kernel:  1   2   1\n 0   0   0\n-1  -2  -1\n \n"}, {"name": "filters.sobel_v()", "path": "api/skimage.filters#skimage.filters.sobel_v", "type": "filters", "text": " \nskimage.filters.sobel_v(image, mask=None) [source]\n \nFind the vertical edges of an image using the Sobel transform.  Parameters \n \nimage2-D array \n\nImage to process.  \nmask2-D array, optional \n\nAn optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.    Returns \n \noutput2-D array \n\nThe Sobel edge map.     Notes We use the following kernel: 1   0  -1\n2   0  -2\n1   0  -1\n \n"}, {"name": "filters.threshold_isodata()", "path": "api/skimage.filters#skimage.filters.threshold_isodata", "type": "filters", "text": " \nskimage.filters.threshold_isodata(image=None, nbins=256, return_all=False, *, hist=None) [source]\n \nReturn threshold value(s) based on ISODATA method. Histogram-based threshold, known as Ridler-Calvard method or inter-means. Threshold values returned satisfy the following equality: threshold = (image[image <= threshold].mean() +\n             image[image > threshold].mean()) / 2.0\n That is, returned thresholds are intensities that separate the image into two groups of pixels, where the threshold intensity is midway between the mean intensities of these groups. For integer images, the above equality holds to within one; for floating- point images, the equality holds to within the histogram bin-width. Either image or hist must be provided. In case hist is given, the actual histogram of the image is ignored.  Parameters \n \nimage(N, M) ndarray, optional \n\nInput image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.  \nreturn_allbool, optional \n\nIf False (default), return only the lowest threshold that satisfies the above equality. If True, return all valid thresholds.  \nhistarray, or 2-tuple of arrays, optional \n\nHistogram to determine the threshold from and a corresponding array of bin center intensities. Alternatively, only the histogram can be passed.    Returns \n \nthresholdfloat or int or array \n\nThreshold value(s).     References  \n1  \nRidler, TW & Calvard, S (1978), \u201cPicture thresholding using an iterative selection method\u201d IEEE Transactions on Systems, Man and Cybernetics 8: 630-632, DOI:10.1109/TSMC.1978.4310039  \n2  \nSezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and Quantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1): 146-165, http://www.busim.ee.boun.edu.tr/~sankur/SankurFolder/Threshold_survey.pdf DOI:10.1117/1.1631315  \n3  \nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold   Examples >>> from skimage.data import coins\n>>> image = coins()\n>>> thresh = threshold_isodata(image)\n>>> binary = image > thresh\n \n"}, {"name": "filters.threshold_li()", "path": "api/skimage.filters#skimage.filters.threshold_li", "type": "filters", "text": " \nskimage.filters.threshold_li(image, *, tolerance=None, initial_guess=None, iter_callback=None) [source]\n \nCompute threshold value by Li\u2019s iterative Minimum Cross Entropy method.  Parameters \n \nimagendarray \n\nInput image.  \ntolerancefloat, optional \n\nFinish the computation when the change in the threshold in an iteration is less than this value. By default, this is half the smallest difference between intensity values in image.  \ninitial_guessfloat or Callable[[array[float]], float], optional \n\nLi\u2019s iterative method uses gradient descent to find the optimal threshold. If the image intensity histogram contains more than two modes (peaks), the gradient descent could get stuck in a local optimum. An initial guess for the iteration can help the algorithm find the globally-optimal threshold. A float value defines a specific start point, while a callable should take in an array of image intensities and return a float value. Example valid callables include numpy.mean (default), lambda arr: numpy.quantile(arr, 0.95), or even skimage.filters.threshold_otsu().  \niter_callbackCallable[[float], Any], optional \n\nA function that will be called on the threshold at every iteration of the algorithm.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.     References  \n1  \nLi C.H. and Lee C.K. (1993) \u201cMinimum Cross Entropy Thresholding\u201d Pattern Recognition, 26(4): 617-625 DOI:10.1016/0031-3203(93)90115-D  \n2  \nLi C.H. and Tam P.K.S. (1998) \u201cAn Iterative Algorithm for Minimum Cross Entropy Thresholding\u201d Pattern Recognition Letters, 18(8): 771-776 DOI:10.1016/S0167-8655(98)00057-9  \n3  \nSezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and Quantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1): 146-165 DOI:10.1117/1.1631315  \n4  \nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_li(image)\n>>> binary = image > thresh\n \n"}, {"name": "filters.threshold_local()", "path": "api/skimage.filters#skimage.filters.threshold_local", "type": "filters", "text": " \nskimage.filters.threshold_local(image, block_size, method='gaussian', offset=0, mode='reflect', param=None, cval=0) [source]\n \nCompute a threshold mask image based on local pixel neighborhood. Also known as adaptive or dynamic thresholding. The threshold value is the weighted mean for the local neighborhood of a pixel subtracted by a constant. Alternatively the threshold can be determined dynamically by a given function, using the \u2018generic\u2019 method.  Parameters \n \nimage(N, M) ndarray \n\nInput image.  \nblock_sizeint \n\nOdd size of pixel neighborhood which is used to calculate the threshold value (e.g. 3, 5, 7, \u2026, 21, \u2026).  \nmethod{\u2018generic\u2019, \u2018gaussian\u2019, \u2018mean\u2019, \u2018median\u2019}, optional \n\nMethod used to determine adaptive threshold for local neighbourhood in weighted mean image.  \u2018generic\u2019: use custom function (see param parameter) \u2018gaussian\u2019: apply gaussian filter (see param parameter for custom sigma value) \u2018mean\u2019: apply arithmetic mean filter \u2018median\u2019: apply median rank filter  By default the \u2018gaussian\u2019 method is used.  \noffsetfloat, optional \n\nConstant subtracted from weighted mean of neighborhood to calculate the local threshold value. Default offset is 0.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018reflect\u2019.  \nparam{int, function}, optional \n\nEither specify sigma for \u2018gaussian\u2019 method or function object for \u2018generic\u2019 method. This functions takes the flat array of local neighbourhood as a single argument and returns the calculated threshold for the centre pixel.  \ncvalfloat, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019.    Returns \n \nthreshold(N, M) ndarray \n\nThreshold image. All pixels in the input image higher than the corresponding pixel in the threshold image are considered foreground.     References  \n1  \nhttps://docs.opencv.org/modules/imgproc/doc/miscellaneous_transformations.html?highlight=threshold#adaptivethreshold   Examples >>> from skimage.data import camera\n>>> image = camera()[:50, :50]\n>>> binary_image1 = image > threshold_local(image, 15, 'mean')\n>>> func = lambda arr: arr.mean()\n>>> binary_image2 = image > threshold_local(image, 15, 'generic',\n...                                         param=func)\n \n"}, {"name": "filters.threshold_mean()", "path": "api/skimage.filters#skimage.filters.threshold_mean", "type": "filters", "text": " \nskimage.filters.threshold_mean(image) [source]\n \nReturn threshold value based on the mean of grayscale values.  Parameters \n \nimage(N, M[, \u2026, P]) ndarray \n\nGrayscale input image.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.     References  \n1  \nC. A. Glasbey, \u201cAn analysis of histogram-based thresholding algorithms,\u201d CVGIP: Graphical Models and Image Processing, vol. 55, pp. 532-537, 1993. DOI:10.1006/cgip.1993.1040   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_mean(image)\n>>> binary = image > thresh\n \n"}, {"name": "filters.threshold_minimum()", "path": "api/skimage.filters#skimage.filters.threshold_minimum", "type": "filters", "text": " \nskimage.filters.threshold_minimum(image=None, nbins=256, max_iter=10000, *, hist=None) [source]\n \nReturn threshold value based on minimum method. The histogram of the input image is computed if not provided and smoothed until there are only two maxima. Then the minimum in between is the threshold value. Either image or hist must be provided. In case hist is given, the actual histogram of the image is ignored.  Parameters \n \nimage(M, N) ndarray, optional \n\nInput image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.  \nmax_iterint, optional \n\nMaximum number of iterations to smooth the histogram.  \nhistarray, or 2-tuple of arrays, optional \n\nHistogram to determine the threshold from and a corresponding array of bin center intensities. Alternatively, only the histogram can be passed.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.    Raises \n RuntimeError\n\nIf unable to find two local maxima in the histogram or if the smoothing takes more than 1e4 iterations.     References  \n1  \nC. A. Glasbey, \u201cAn analysis of histogram-based thresholding algorithms,\u201d CVGIP: Graphical Models and Image Processing, vol. 55, pp. 532-537, 1993.  \n2  \nPrewitt, JMS & Mendelsohn, ML (1966), \u201cThe analysis of cell images\u201d, Annals of the New York Academy of Sciences 128: 1035-1053 DOI:10.1111/j.1749-6632.1965.tb11715.x   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_minimum(image)\n>>> binary = image > thresh\n \n"}, {"name": "filters.threshold_multiotsu()", "path": "api/skimage.filters#skimage.filters.threshold_multiotsu", "type": "filters", "text": " \nskimage.filters.threshold_multiotsu(image, classes=3, nbins=256) [source]\n \nGenerate classes-1 threshold values to divide gray levels in image. The threshold values are chosen to maximize the total sum of pairwise variances between the thresholded graylevel classes. See Notes and [1] for more details.  Parameters \n \nimage(N, M) ndarray \n\nGrayscale input image.  \nclassesint, optional \n\nNumber of classes to be thresholded, i.e. the number of resulting regions.  \nnbinsint, optional \n\nNumber of bins used to calculate the histogram. This value is ignored for integer arrays.    Returns \n \nthresharray \n\nArray containing the threshold values for the desired classes.    Raises \n ValueError\n\nIf image contains less grayscale value then the desired number of classes.     Notes This implementation relies on a Cython function whose complexity is \\(O\\left(\\frac{Ch^{C-1}}{(C-1)!}\\right)\\), where \\(h\\) is the number of histogram bins and \\(C\\) is the number of classes desired. The input image must be grayscale. References  \n1  \nLiao, P-S., Chen, T-S. and Chung, P-C., \u201cA fast algorithm for multilevel thresholding\u201d, Journal of Information Science and Engineering 17 (5): 713-727, 2001. Available at: <https://ftp.iis.sinica.edu.tw/JISE/2001/200109_01.pdf> DOI:10.6688/JISE.2001.17.5.1  \n2  \nTosa, Y., \u201cMulti-Otsu Threshold\u201d, a java plugin for ImageJ. Available at: <http://imagej.net/plugins/download/Multi_OtsuThreshold.java>   Examples >>> from skimage.color import label2rgb\n>>> from skimage import data\n>>> image = data.camera()\n>>> thresholds = threshold_multiotsu(image)\n>>> regions = np.digitize(image, bins=thresholds)\n>>> regions_colorized = label2rgb(regions)\n \n"}, {"name": "filters.threshold_niblack()", "path": "api/skimage.filters#skimage.filters.threshold_niblack", "type": "filters", "text": " \nskimage.filters.threshold_niblack(image, window_size=15, k=0.2) [source]\n \nApplies Niblack local threshold to an array. A threshold T is calculated for every pixel in the image using the following formula: T = m(x,y) - k * s(x,y)\n where m(x,y) and s(x,y) are the mean and standard deviation of pixel (x,y) neighborhood defined by a rectangular window with size w times w centered around the pixel. k is a configurable parameter that weights the effect of standard deviation.  Parameters \n \nimagendarray \n\nInput image.  \nwindow_sizeint, or iterable of int, optional \n\nWindow size specified as a single odd integer (3, 5, 7, \u2026), or an iterable of length image.ndim containing only odd integers (e.g. (1, 5, 5)).  \nkfloat, optional \n\nValue of parameter k in threshold formula.    Returns \n \nthreshold(N, M) ndarray \n\nThreshold mask. All pixels with an intensity higher than this value are assumed to be foreground.     Notes This algorithm is originally designed for text recognition. The Bradley threshold is a particular case of the Niblack one, being equivalent to >>> from skimage import data\n>>> image = data.page()\n>>> q = 1\n>>> threshold_image = threshold_niblack(image, k=0) * q\n for some value q. By default, Bradley and Roth use q=1. References  \n1  \nW. Niblack, An introduction to Digital Image Processing, Prentice-Hall, 1986.  \n2  \nD. Bradley and G. Roth, \u201cAdaptive thresholding using Integral Image\u201d, Journal of Graphics Tools 12(2), pp. 13-21, 2007. DOI:10.1080/2151237X.2007.10129236   Examples >>> from skimage import data\n>>> image = data.page()\n>>> threshold_image = threshold_niblack(image, window_size=7, k=0.1)\n \n"}, {"name": "filters.threshold_otsu()", "path": "api/skimage.filters#skimage.filters.threshold_otsu", "type": "filters", "text": " \nskimage.filters.threshold_otsu(image=None, nbins=256, *, hist=None) [source]\n \nReturn threshold value based on Otsu\u2019s method. Either image or hist must be provided. If hist is provided, the actual histogram of the image is ignored.  Parameters \n \nimage(N, M) ndarray, optional \n\nGrayscale input image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.  \nhistarray, or 2-tuple of arrays, optional \n\nHistogram from which to determine the threshold, and optionally a corresponding array of bin center intensities. An alternative use of this function is to pass it only hist.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.     Notes The input image must be grayscale. References  \n1  \nWikipedia, https://en.wikipedia.org/wiki/Otsu\u2019s_Method   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_otsu(image)\n>>> binary = image <= thresh\n \n"}, {"name": "filters.threshold_sauvola()", "path": "api/skimage.filters#skimage.filters.threshold_sauvola", "type": "filters", "text": " \nskimage.filters.threshold_sauvola(image, window_size=15, k=0.2, r=None) [source]\n \nApplies Sauvola local threshold to an array. Sauvola is a modification of Niblack technique. In the original method a threshold T is calculated for every pixel in the image using the following formula: T = m(x,y) * (1 + k * ((s(x,y) / R) - 1))\n where m(x,y) and s(x,y) are the mean and standard deviation of pixel (x,y) neighborhood defined by a rectangular window with size w times w centered around the pixel. k is a configurable parameter that weights the effect of standard deviation. R is the maximum standard deviation of a greyscale image.  Parameters \n \nimagendarray \n\nInput image.  \nwindow_sizeint, or iterable of int, optional \n\nWindow size specified as a single odd integer (3, 5, 7, \u2026), or an iterable of length image.ndim containing only odd integers (e.g. (1, 5, 5)).  \nkfloat, optional \n\nValue of the positive parameter k.  \nrfloat, optional \n\nValue of R, the dynamic range of standard deviation. If None, set to the half of the image dtype range.    Returns \n \nthreshold(N, M) ndarray \n\nThreshold mask. All pixels with an intensity higher than this value are assumed to be foreground.     Notes This algorithm is originally designed for text recognition. References  \n1  \nJ. Sauvola and M. Pietikainen, \u201cAdaptive document image binarization,\u201d Pattern Recognition 33(2), pp. 225-236, 2000. DOI:10.1016/S0031-3203(99)00055-2   Examples >>> from skimage import data\n>>> image = data.page()\n>>> t_sauvola = threshold_sauvola(image, window_size=15, k=0.2)\n>>> binary_image = image > t_sauvola\n \n"}, {"name": "filters.threshold_triangle()", "path": "api/skimage.filters#skimage.filters.threshold_triangle", "type": "filters", "text": " \nskimage.filters.threshold_triangle(image, nbins=256) [source]\n \nReturn threshold value based on the triangle algorithm.  Parameters \n \nimage(N, M[, \u2026, P]) ndarray \n\nGrayscale input image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.     References  \n1  \nZack, G. W., Rogers, W. E. and Latt, S. A., 1977, Automatic Measurement of Sister Chromatid Exchange Frequency, Journal of Histochemistry and Cytochemistry 25 (7), pp. 741-753 DOI:10.1177/25.7.70454  \n2  \nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_triangle(image)\n>>> binary = image > thresh\n \n"}, {"name": "filters.threshold_yen()", "path": "api/skimage.filters#skimage.filters.threshold_yen", "type": "filters", "text": " \nskimage.filters.threshold_yen(image=None, nbins=256, *, hist=None) [source]\n \nReturn threshold value based on Yen\u2019s method. Either image or hist must be provided. In case hist is given, the actual histogram of the image is ignored.  Parameters \n \nimage(N, M) ndarray, optional \n\nInput image.  \nnbinsint, optional \n\nNumber of bins used to calculate histogram. This value is ignored for integer arrays.  \nhistarray, or 2-tuple of arrays, optional \n\nHistogram from which to determine the threshold, and optionally a corresponding array of bin center intensities. An alternative use of this function is to pass it only hist.    Returns \n \nthresholdfloat \n\nUpper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.     References  \n1  \nYen J.C., Chang F.J., and Chang S. (1995) \u201cA New Criterion for Automatic Multilevel Thresholding\u201d IEEE Trans. on Image Processing, 4(3): 370-378. DOI:10.1109/83.366472  \n2  \nSezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and Quantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1): 146-165, DOI:10.1117/1.1631315 http://www.busim.ee.boun.edu.tr/~sankur/SankurFolder/Threshold_survey.pdf  \n3  \nImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold   Examples >>> from skimage.data import camera\n>>> image = camera()\n>>> thresh = threshold_yen(image)\n>>> binary = image <= thresh\n \n"}, {"name": "filters.try_all_threshold()", "path": "api/skimage.filters#skimage.filters.try_all_threshold", "type": "filters", "text": " \nskimage.filters.try_all_threshold(image, figsize=(8, 5), verbose=True) [source]\n \nReturns a figure comparing the outputs of different thresholding methods.  Parameters \n \nimage(N, M) ndarray \n\nInput image.  \nfigsizetuple, optional \n\nFigure size (in inches).  \nverbosebool, optional \n\nPrint function name for each method.    Returns \n \nfig, axtuple \n\nMatplotlib figure and axes.     Notes The following algorithms are used:  isodata li mean minimum otsu triangle yen  Examples >>> from skimage.data import text\n>>> fig, ax = try_all_threshold(text(), figsize=(10, 6), verbose=False)\n \n"}, {"name": "filters.unsharp_mask()", "path": "api/skimage.filters#skimage.filters.unsharp_mask", "type": "filters", "text": " \nskimage.filters.unsharp_mask(image, radius=1.0, amount=1.0, multichannel=False, preserve_range=False) [source]\n \nUnsharp masking filter. The sharp details are identified as the difference between the original image and its blurred version. These details are then scaled, and added back to the original image.  Parameters \n \nimage[P, \u2026, ]M[, N][, C] ndarray \n\nInput image.  \nradiusscalar or sequence of scalars, optional \n\nIf a scalar is given, then its value is used for all dimensions. If sequence is given, then there must be exactly one radius for each dimension except the last dimension for multichannel images. Note that 0 radius means no blurring, and negative values are not allowed.  \namountscalar, optional \n\nThe details will be amplified with this factor. The factor could be 0 or negative. Typically, it is a small positive number, e.g. 1.0.  \nmultichannelbool, optional \n\nIf True, the last image dimension is considered as a color channel, otherwise as spatial. Color channels are processed individually.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html    Returns \n \noutput[P, \u2026, ]M[, N][, C] ndarray of float \n\nImage with unsharp mask applied.     Notes Unsharp masking is an image sharpening technique. It is a linear image operation, and numerically stable, unlike deconvolution which is an ill-posed problem. Because of this stability, it is often preferred over deconvolution. The main idea is as follows: sharp details are identified as the difference between the original image and its blurred version. These details are added back to the original image after a scaling step: enhanced image = original + amount * (original - blurred) When applying this filter to several color layers independently, color bleeding may occur. More visually pleasing result can be achieved by processing only the brightness/lightness/intensity channel in a suitable color space such as HSV, HSL, YUV, or YCbCr. Unsharp masking is described in most introductory digital image processing books. This implementation is based on [1]. References  \n1  \nMaria Petrou, Costas Petrou \u201cImage Processing: The Fundamentals\u201d, (2010), ed ii., page 357, ISBN 13: 9781119994398 DOI:10.1002/9781119994398  \n2  \nWikipedia. Unsharp masking https://en.wikipedia.org/wiki/Unsharp_masking   Examples >>> array = np.ones(shape=(5,5), dtype=np.uint8)*100\n>>> array[2,2] = 120\n>>> array\narray([[100, 100, 100, 100, 100],\n       [100, 100, 100, 100, 100],\n       [100, 100, 120, 100, 100],\n       [100, 100, 100, 100, 100],\n       [100, 100, 100, 100, 100]], dtype=uint8)\n>>> np.around(unsharp_mask(array, radius=0.5, amount=2),2)\narray([[0.39, 0.39, 0.39, 0.39, 0.39],\n       [0.39, 0.39, 0.38, 0.39, 0.39],\n       [0.39, 0.38, 0.53, 0.38, 0.39],\n       [0.39, 0.39, 0.38, 0.39, 0.39],\n       [0.39, 0.39, 0.39, 0.39, 0.39]])\n >>> array = np.ones(shape=(5,5), dtype=np.int8)*100\n>>> array[2,2] = 127\n>>> np.around(unsharp_mask(array, radius=0.5, amount=2),2)\narray([[0.79, 0.79, 0.79, 0.79, 0.79],\n       [0.79, 0.78, 0.75, 0.78, 0.79],\n       [0.79, 0.75, 1.  , 0.75, 0.79],\n       [0.79, 0.78, 0.75, 0.78, 0.79],\n       [0.79, 0.79, 0.79, 0.79, 0.79]])\n >>> np.around(unsharp_mask(array, radius=0.5, amount=2, preserve_range=True), 2)\narray([[100.  , 100.  ,  99.99, 100.  , 100.  ],\n       [100.  ,  99.39,  95.48,  99.39, 100.  ],\n       [ 99.99,  95.48, 147.59,  95.48,  99.99],\n       [100.  ,  99.39,  95.48,  99.39, 100.  ],\n       [100.  , 100.  ,  99.99, 100.  , 100.  ]])\n \n"}, {"name": "filters.wiener()", "path": "api/skimage.filters#skimage.filters.wiener", "type": "filters", "text": " \nskimage.filters.wiener(data, impulse_response=None, filter_params={}, K=0.25, predefined_filter=None) [source]\n \nMinimum Mean Square Error (Wiener) inverse filter.  Parameters \n \ndata(M,N) ndarray \n\nInput data.  \nKfloat or (M,N) ndarray \n\nRatio between power spectrum of noise and undegraded image.  \nimpulse_responsecallable f(r, c, **filter_params) \n\nImpulse response of the filter. See LPIFilter2D.__init__.  \nfilter_paramsdict \n\nAdditional keyword parameters to the impulse_response function.    Other Parameters \n \npredefined_filterLPIFilter2D \n\nIf you need to apply the same filter multiple times over different images, construct the LPIFilter2D and specify it here.     \n"}, {"name": "filters.window()", "path": "api/skimage.filters#skimage.filters.window", "type": "filters", "text": " \nskimage.filters.window(window_type, shape, warp_kwargs=None) [source]\n \nReturn an n-dimensional window of a given size and dimensionality.  Parameters \n \nwindow_typestring, float, or tuple \n\nThe type of window to be created. Any window type supported by scipy.signal.get_window is allowed here. See notes below for a current list, or the SciPy documentation for the version of SciPy on your machine.  \nshapetuple of int or int \n\nThe shape of the window along each axis. If an integer is provided, a 1D window is generated.  \nwarp_kwargsdict \n\nKeyword arguments passed to skimage.transform.warp (e.g., warp_kwargs={'order':3} to change interpolation method).    Returns \n \nnd_windowndarray \n\nA window of the specified shape. dtype is np.double.     Notes This function is based on scipy.signal.get_window and thus can access all of the window types available to that function (e.g., \"hann\", \"boxcar\"). Note that certain window types require parameters that have to be supplied with the window name as a tuple (e.g., (\"tukey\", 0.8)). If only a float is supplied, it is interpreted as the beta parameter of the Kaiser window. See https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.windows.get_window.html for more details. Note that this function generates a double precision array of the specified shape and can thus generate very large arrays that consume a large amount of available memory. The approach taken here to create nD windows is to first calculate the Euclidean distance from the center of the intended nD window to each position in the array. That distance is used to sample, with interpolation, from a 1D window returned from scipy.signal.get_window. The method of interpolation can be changed with the order keyword argument passed to skimage.transform.warp. Some coordinates in the output window will be outside of the original signal; these will be filled in with zeros. Window types: - boxcar - triang - blackman - hamming - hann - bartlett - flattop - parzen - bohman - blackmanharris - nuttall - barthann - kaiser (needs beta) - gaussian (needs standard deviation) - general_gaussian (needs power, width) - slepian (needs width) - dpss (needs normalized half-bandwidth) - chebwin (needs attenuation) - exponential (needs decay scale) - tukey (needs taper fraction) References  \n1  \nTwo-dimensional window design, Wikipedia, https://en.wikipedia.org/wiki/Two_dimensional_window_design   Examples Return a Hann window with shape (512, 512): >>> from skimage.filters import window\n>>> w = window('hann', (512, 512))\n Return a Kaiser window with beta parameter of 16 and shape (256, 256, 35): >>> w = window(16, (256, 256, 35))\n Return a Tukey window with an alpha parameter of 0.8 and shape (100, 300): >>> w = window(('tukey', 0.8), (100, 300))\n \n"}, {"name": "future", "path": "api/skimage.future", "type": "future", "text": "Module: future Functionality with an experimental API. Although you can count on the functions in this package being around in the future, the API may change with any version update and will not follow the skimage two-version deprecation path. Therefore, use the functions herein with care, and do not use them in production code that will depend on updated skimage versions.  \nskimage.future.fit_segmenter(labels, \u2026) Segmentation using labeled parts of the image and a classifier.  \nskimage.future.manual_lasso_segmentation(image) Return a label image based on freeform selections made with the mouse.  \nskimage.future.manual_polygon_segmentation(image) Return a label image based on polygon selections made with the mouse.  \nskimage.future.predict_segmenter(features, clf) Segmentation of images using a pretrained classifier.  \nskimage.future.TrainableSegmenter([clf, \u2026]) Estimator for classifying pixels.  \nskimage.future.graph    fit_segmenter  \nskimage.future.fit_segmenter(labels, features, clf) [source]\n \nSegmentation using labeled parts of the image and a classifier.  Parameters \n \nlabelsndarray of ints \n\nImage of labels. Labels >= 1 correspond to the training set and label 0 to unlabeled pixels to be segmented.  \nfeaturesndarray \n\nArray of features, with the first dimension corresponding to the number of features, and the other dimensions correspond to labels.shape.  \nclfclassifier object \n\nclassifier object, exposing a fit and a predict method as in scikit-learn\u2019s API, for example an instance of RandomForestClassifier or LogisticRegression classifier.    Returns \n \nclfclassifier object \n\nclassifier trained on labels    Raises \n \nNotFittedError if self.clf has not been fitted yet (use self.fit). \n   \n Examples using skimage.future.fit_segmenter\n \n  Trainable segmentation using local features and random forests   manual_lasso_segmentation  \nskimage.future.manual_lasso_segmentation(image, alpha=0.4, return_all=False) [source]\n \nReturn a label image based on freeform selections made with the mouse.  Parameters \n \nimage(M, N[, 3]) array \n\nGrayscale or RGB image.  \nalphafloat, optional \n\nTransparency value for polygons drawn over the image.  \nreturn_allbool, optional \n\nIf True, an array containing each separate polygon drawn is returned. (The polygons may overlap.) If False (default), latter polygons \u201coverwrite\u201d earlier ones where they overlap.    Returns \n \nlabelsarray of int, shape ([Q, ]M, N) \n\nThe segmented regions. If mode is \u2018separate\u2019, the leading dimension of the array corresponds to the number of regions that the user drew.     Notes Press and hold the left mouse button to draw around each object. Examples >>> from skimage import data, future, io\n>>> camera = data.camera()\n>>> mask = future.manual_lasso_segmentation(camera)  \n>>> io.imshow(mask)  \n>>> io.show()  \n \n manual_polygon_segmentation  \nskimage.future.manual_polygon_segmentation(image, alpha=0.4, return_all=False) [source]\n \nReturn a label image based on polygon selections made with the mouse.  Parameters \n \nimage(M, N[, 3]) array \n\nGrayscale or RGB image.  \nalphafloat, optional \n\nTransparency value for polygons drawn over the image.  \nreturn_allbool, optional \n\nIf True, an array containing each separate polygon drawn is returned. (The polygons may overlap.) If False (default), latter polygons \u201coverwrite\u201d earlier ones where they overlap.    Returns \n \nlabelsarray of int, shape ([Q, ]M, N) \n\nThe segmented regions. If mode is \u2018separate\u2019, the leading dimension of the array corresponds to the number of regions that the user drew.     Notes Use left click to select the vertices of the polygon and right click to confirm the selection once all vertices are selected. Examples >>> from skimage import data, future, io\n>>> camera = data.camera()\n>>> mask = future.manual_polygon_segmentation(camera)  \n>>> io.imshow(mask)  \n>>> io.show()  \n \n predict_segmenter  \nskimage.future.predict_segmenter(features, clf) [source]\n \nSegmentation of images using a pretrained classifier.  Parameters \n \nfeaturesndarray \n\nArray of features, with the last dimension corresponding to the number of features, and the other dimensions are compatible with the shape of the image to segment, or a flattened image.  \nclfclassifier object \n\ntrained classifier object, exposing a predict method as in scikit-learn\u2019s API, for example an instance of RandomForestClassifier or LogisticRegression classifier. The classifier must be already trained, for example with skimage.segmentation.fit_segmenter().    Returns \n \noutputndarray \n\nLabeled array, built from the prediction of the classifier.     \n Examples using skimage.future.predict_segmenter\n \n  Trainable segmentation using local features and random forests   TrainableSegmenter  \nclass skimage.future.TrainableSegmenter(clf=None, features_func=None) [source]\n \nBases: object Estimator for classifying pixels.  Parameters \n \nclfclassifier object, optional \n\nclassifier object, exposing a fit and a predict method as in scikit-learn\u2019s API, for example an instance of RandomForestClassifier or LogisticRegression classifier.  \nfeatures_funcfunction, optional \n\nfunction computing features on all pixels of the image, to be passed to the classifier. The output should be of shape (m_features, *labels.shape). If None, skimage.segmentation.multiscale_basic_features() is used.     Methods  \nfit(image, labels) Train classifier using partially labeled (annotated) image.  \npredict(image) Segment new image using trained internal classifier.    \ncompute_features     \n__init__(clf=None, features_func=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \ncompute_features(image) [source]\n\n  \nfit(image, labels) [source]\n \nTrain classifier using partially labeled (annotated) image.  Parameters \n \nimagendarray \n\nInput image, which can be grayscale or multichannel, and must have a number of dimensions compatible with self.features_func.  \nlabelsndarray of ints \n\nLabeled array of shape compatible with image (same shape for a single-channel image). Labels >= 1 correspond to the training set and label 0 to unlabeled pixels to be segmented.     \n  \npredict(image) [source]\n \nSegment new image using trained internal classifier.  Parameters \n \nimagendarray \n\nInput image, which can be grayscale or multichannel, and must have a number of dimensions compatible with self.features_func.    Raises \n \nNotFittedError if self.clf has not been fitted yet (use self.fit). \n   \n \n\n"}, {"name": "future.fit_segmenter()", "path": "api/skimage.future#skimage.future.fit_segmenter", "type": "future", "text": " \nskimage.future.fit_segmenter(labels, features, clf) [source]\n \nSegmentation using labeled parts of the image and a classifier.  Parameters \n \nlabelsndarray of ints \n\nImage of labels. Labels >= 1 correspond to the training set and label 0 to unlabeled pixels to be segmented.  \nfeaturesndarray \n\nArray of features, with the first dimension corresponding to the number of features, and the other dimensions correspond to labels.shape.  \nclfclassifier object \n\nclassifier object, exposing a fit and a predict method as in scikit-learn\u2019s API, for example an instance of RandomForestClassifier or LogisticRegression classifier.    Returns \n \nclfclassifier object \n\nclassifier trained on labels    Raises \n \nNotFittedError if self.clf has not been fitted yet (use self.fit). \n   \n"}, {"name": "future.graph", "path": "api/skimage.future.graph", "type": "future", "text": "Module: future.graph  \nskimage.future.graph.cut_normalized(labels, rag) Perform Normalized Graph cut on the Region Adjacency Graph.  \nskimage.future.graph.cut_threshold(labels, \u2026) Combine regions separated by weight less than threshold.  \nskimage.future.graph.merge_hierarchical(\u2026) Perform hierarchical merging of a RAG.  \nskimage.future.graph.ncut(labels, rag[, \u2026]) Perform Normalized Graph cut on the Region Adjacency Graph.  \nskimage.future.graph.rag_boundary(labels, \u2026) Comouter RAG based on region boundaries  \nskimage.future.graph.rag_mean_color(image, \u2026) Compute the Region Adjacency Graph using mean colors.  \nskimage.future.graph.show_rag(labels, rag, image) Show a Region Adjacency Graph on an image.  \nskimage.future.graph.RAG([label_image, \u2026]) The Region Adjacency Graph (RAG) of an image, subclasses networx.Graph   cut_normalized  \nskimage.future.graph.cut_normalized(labels, rag, thresh=0.001, num_cuts=10, in_place=True, max_edge=1.0, *, random_state=None) [source]\n \nPerform Normalized Graph cut on the Region Adjacency Graph. Given an image\u2019s labels and its similarity RAG, recursively perform a 2-way normalized cut on it. All nodes belonging to a subgraph that cannot be cut further are assigned a unique label in the output.  Parameters \n \nlabelsndarray \n\nThe array of labels.  \nragRAG \n\nThe region adjacency graph.  \nthreshfloat \n\nThe threshold. A subgraph won\u2019t be further subdivided if the value of the N-cut exceeds thresh.  \nnum_cutsint \n\nThe number or N-cuts to perform before determining the optimal one.  \nin_placebool \n\nIf set, modifies rag in place. For each node n the function will set a new attribute rag.nodes[n]['ncut label'].  \nmax_edgefloat, optional \n\nThe maximum possible value of an edge in the RAG. This corresponds to an edge between identical regions. This is used to put self edges in the RAG.  \nrandom_stateint, RandomState instance or None, optional \n\nIf int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. The random state is used for the starting point of scipy.sparse.linalg.eigsh.    Returns \n \noutndarray \n\nThe new labeled array.     References  \n1  \nShi, J.; Malik, J., \u201cNormalized cuts and image segmentation\u201d, Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 888-905, August 2000.   Examples >>> from skimage import data, segmentation\n>>> from skimage.future import graph\n>>> img = data.astronaut()\n>>> labels = segmentation.slic(img)\n>>> rag = graph.rag_mean_color(img, labels, mode='similarity')\n>>> new_labels = graph.cut_normalized(labels, rag)\n \n cut_threshold  \nskimage.future.graph.cut_threshold(labels, rag, thresh, in_place=True) [source]\n \nCombine regions separated by weight less than threshold. Given an image\u2019s labels and its RAG, output new labels by combining regions whose nodes are separated by a weight less than the given threshold.  Parameters \n \nlabelsndarray \n\nThe array of labels.  \nragRAG \n\nThe region adjacency graph.  \nthreshfloat \n\nThe threshold. Regions connected by edges with smaller weights are combined.  \nin_placebool \n\nIf set, modifies rag in place. The function will remove the edges with weights less that thresh. If set to False the function makes a copy of rag before proceeding.    Returns \n \noutndarray \n\nThe new labelled array.     References  \n1  \nAlain Tremeau and Philippe Colantoni \u201cRegions Adjacency Graph Applied To Color Image Segmentation\u201d DOI:10.1109/83.841950   Examples >>> from skimage import data, segmentation\n>>> from skimage.future import graph\n>>> img = data.astronaut()\n>>> labels = segmentation.slic(img)\n>>> rag = graph.rag_mean_color(img, labels)\n>>> new_labels = graph.cut_threshold(labels, rag, 10)\n \n merge_hierarchical  \nskimage.future.graph.merge_hierarchical(labels, rag, thresh, rag_copy, in_place_merge, merge_func, weight_func) [source]\n \nPerform hierarchical merging of a RAG. Greedily merges the most similar pair of nodes until no edges lower than thresh remain.  Parameters \n \nlabelsndarray \n\nThe array of labels.  \nragRAG \n\nThe Region Adjacency Graph.  \nthreshfloat \n\nRegions connected by an edge with weight smaller than thresh are merged.  \nrag_copybool \n\nIf set, the RAG copied before modifying.  \nin_place_mergebool \n\nIf set, the nodes are merged in place. Otherwise, a new node is created for each merge..  \nmerge_funccallable \n\nThis function is called before merging two nodes. For the RAG graph while merging src and dst, it is called as follows merge_func(graph, src, dst).  \nweight_funccallable \n\nThe function to compute the new weights of the nodes adjacent to the merged node. This is directly supplied as the argument weight_func to merge_nodes.    Returns \n \noutndarray \n\nThe new labeled array.     \n ncut  \nskimage.future.graph.ncut(labels, rag, thresh=0.001, num_cuts=10, in_place=True, max_edge=1.0, *, random_state=None) [source]\n \nPerform Normalized Graph cut on the Region Adjacency Graph. Given an image\u2019s labels and its similarity RAG, recursively perform a 2-way normalized cut on it. All nodes belonging to a subgraph that cannot be cut further are assigned a unique label in the output.  Parameters \n \nlabelsndarray \n\nThe array of labels.  \nragRAG \n\nThe region adjacency graph.  \nthreshfloat \n\nThe threshold. A subgraph won\u2019t be further subdivided if the value of the N-cut exceeds thresh.  \nnum_cutsint \n\nThe number or N-cuts to perform before determining the optimal one.  \nin_placebool \n\nIf set, modifies rag in place. For each node n the function will set a new attribute rag.nodes[n]['ncut label'].  \nmax_edgefloat, optional \n\nThe maximum possible value of an edge in the RAG. This corresponds to an edge between identical regions. This is used to put self edges in the RAG.  \nrandom_stateint, RandomState instance or None, optional \n\nIf int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. The random state is used for the starting point of scipy.sparse.linalg.eigsh.    Returns \n \noutndarray \n\nThe new labeled array.     References  \n1  \nShi, J.; Malik, J., \u201cNormalized cuts and image segmentation\u201d, Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 888-905, August 2000.   Examples >>> from skimage import data, segmentation\n>>> from skimage.future import graph\n>>> img = data.astronaut()\n>>> labels = segmentation.slic(img)\n>>> rag = graph.rag_mean_color(img, labels, mode='similarity')\n>>> new_labels = graph.cut_normalized(labels, rag)\n \n rag_boundary  \nskimage.future.graph.rag_boundary(labels, edge_map, connectivity=2) [source]\n \nComouter RAG based on region boundaries Given an image\u2019s initial segmentation and its edge map this method constructs the corresponding Region Adjacency Graph (RAG). Each node in the RAG represents a set of pixels within the image with the same label in labels. The weight between two adjacent regions is the average value in edge_map along their boundary.  \nlabelsndarray \n\nThe labelled image.  \nedge_mapndarray \n\nThis should have the same shape as that of labels. For all pixels along the boundary between 2 adjacent regions, the average value of the corresponding pixels in edge_map is the edge weight between them.  \nconnectivityint, optional \n\nPixels with a squared distance less than connectivity from each other are considered adjacent. It can range from 1 to labels.ndim. Its behavior is the same as connectivity parameter in scipy.ndimage.filters.generate_binary_structure.   Examples >>> from skimage import data, segmentation, filters, color\n>>> from skimage.future import graph\n>>> img = data.chelsea()\n>>> labels = segmentation.slic(img)\n>>> edge_map = filters.sobel(color.rgb2gray(img))\n>>> rag = graph.rag_boundary(labels, edge_map)\n \n rag_mean_color  \nskimage.future.graph.rag_mean_color(image, labels, connectivity=2, mode='distance', sigma=255.0) [source]\n \nCompute the Region Adjacency Graph using mean colors. Given an image and its initial segmentation, this method constructs the corresponding Region Adjacency Graph (RAG). Each node in the RAG represents a set of pixels within image with the same label in labels. The weight between two adjacent regions represents how similar or dissimilar two regions are depending on the mode parameter.  Parameters \n \nimagendarray, shape(M, N, [\u2026, P,] 3) \n\nInput image.  \nlabelsndarray, shape(M, N, [\u2026, P]) \n\nThe labelled image. This should have one dimension less than image. If image has dimensions (M, N, 3) labels should have dimensions (M, N).  \nconnectivityint, optional \n\nPixels with a squared distance less than connectivity from each other are considered adjacent. It can range from 1 to labels.ndim. Its behavior is the same as connectivity parameter in scipy.ndimage.generate_binary_structure.  \nmode{\u2018distance\u2019, \u2018similarity\u2019}, optional \n\nThe strategy to assign edge weights. \u2018distance\u2019 : The weight between two adjacent regions is the \\(|c_1 - c_2|\\), where \\(c_1\\) and \\(c_2\\) are the mean colors of the two regions. It represents the Euclidean distance in their average color. \u2018similarity\u2019 : The weight between two adjacent is \\(e^{-d^2/sigma}\\) where \\(d=|c_1 - c_2|\\), where \\(c_1\\) and \\(c_2\\) are the mean colors of the two regions. It represents how similar two regions are.  \nsigmafloat, optional \n\nUsed for computation when mode is \u201csimilarity\u201d. It governs how close to each other two colors should be, for their corresponding edge weight to be significant. A very large value of sigma could make any two colors behave as though they were similar.    Returns \n \noutRAG \n\nThe region adjacency graph.     References  \n1  \nAlain Tremeau and Philippe Colantoni \u201cRegions Adjacency Graph Applied To Color Image Segmentation\u201d DOI:10.1109/83.841950   Examples >>> from skimage import data, segmentation\n>>> from skimage.future import graph\n>>> img = data.astronaut()\n>>> labels = segmentation.slic(img)\n>>> rag = graph.rag_mean_color(img, labels)\n \n show_rag  \nskimage.future.graph.show_rag(labels, rag, image, border_color='black', edge_width=1.5, edge_cmap='magma', img_cmap='bone', in_place=True, ax=None) [source]\n \nShow a Region Adjacency Graph on an image. Given a labelled image and its corresponding RAG, show the nodes and edges of the RAG on the image with the specified colors. Edges are displayed between the centroid of the 2 adjacent regions in the image.  Parameters \n \nlabelsndarray, shape (M, N) \n\nThe labelled image.  \nragRAG \n\nThe Region Adjacency Graph.  \nimagendarray, shape (M, N[, 3]) \n\nInput image. If colormap is None, the image should be in RGB format.  \nborder_colorcolor spec, optional \n\nColor with which the borders between regions are drawn.  \nedge_widthfloat, optional \n\nThe thickness with which the RAG edges are drawn.  \nedge_cmapmatplotlib.colors.Colormap, optional \n\nAny matplotlib colormap with which the edges are drawn.  \nimg_cmapmatplotlib.colors.Colormap, optional \n\nAny matplotlib colormap with which the image is draw. If set to None the image is drawn as it is.  \nin_placebool, optional \n\nIf set, the RAG is modified in place. For each node n the function will set a new attribute rag.nodes[n]['centroid'].  \naxmatplotlib.axes.Axes, optional \n\nThe axes to draw on. If not specified, new axes are created and drawn on.    Returns \n \nlcmatplotlib.collections.LineCollection \n\nA colection of lines that represent the edges of the graph. It can be passed to the matplotlib.figure.Figure.colorbar() function.     Examples >>> from skimage import data, segmentation\n>>> from skimage.future import graph\n>>> import matplotlib.pyplot as plt\n>>>\n>>> img = data.coffee()\n>>> labels = segmentation.slic(img)\n>>> g =  graph.rag_mean_color(img, labels)\n>>> lc = graph.show_rag(labels, g, img)\n>>> cbar = plt.colorbar(lc)\n \n RAG  \nclass skimage.future.graph.RAG(label_image=None, connectivity=1, data=None, **attr) [source]\n \nBases: networkx.classes.graph.Graph The Region Adjacency Graph (RAG) of an image, subclasses networx.Graph  Parameters \n \nlabel_imagearray of int \n\nAn initial segmentation, with each region labeled as a different integer. Every unique value in label_image will correspond to a node in the graph.  \nconnectivityint in {1, \u2026, label_image.ndim}, optional \n\nThe connectivity between pixels in label_image. For a 2D image, a connectivity of 1 corresponds to immediate neighbors up, down, left, and right, while a connectivity of 2 also includes diagonal neighbors. See scipy.ndimage.generate_binary_structure.  \ndatanetworkx Graph specification, optional \n\nInitial or additional edges to pass to the NetworkX Graph constructor. See networkx.Graph. Valid edge specifications include edge list (list of tuples), NumPy arrays, and SciPy sparse matrices.  \n**attrkeyword arguments, optional \n\nAdditional attributes to add to the graph.      \n__init__(label_image=None, connectivity=1, data=None, **attr) [source]\n \nInitialize a graph with edges, name, or graph attributes.  Parameters \n \nincoming_graph_datainput graph (optional, default: None) \n\nData to initialize graph. If None (default) an empty graph is created. The data can be an edge list, or any NetworkX graph object. If the corresponding optional Python packages are installed the data can also be a NumPy matrix or 2d ndarray, a SciPy sparse matrix, or a PyGraphviz graph.  \nattrkeyword arguments, optional (default= no attributes) \n\nAttributes to add to graph as key=value pairs.      See also  \nconvert \n  Examples >>> G = nx.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc\n>>> G = nx.Graph(name=\"my graph\")\n>>> e = [(1, 2), (2, 3), (3, 4)]  # list of edges\n>>> G = nx.Graph(e)\n Arbitrary graph attribute pairs (key=value) may be assigned >>> G = nx.Graph(e, day=\"Friday\")\n>>> G.graph\n{'day': 'Friday'}\n \n  \nadd_edge(u, v, attr_dict=None, **attr) [source]\n \nAdd an edge between u and v while updating max node id.  See also networkx.Graph.add_edge().  \n  \nadd_node(n, attr_dict=None, **attr) [source]\n \nAdd node n while updating the maximum node id.  See also networkx.Graph.add_node().  \n  \ncopy() [source]\n \nCopy the graph with its max node id.  See also networkx.Graph.copy().  \n  \nfresh_copy() [source]\n \nReturn a fresh copy graph with the same data structure. A fresh copy has no nodes, edges or graph attributes. It is the same data structure as the current graph. This method is typically used to create an empty version of the graph. This is required when subclassing Graph with networkx v2 and does not cause problems for v1. Here is more detail from the network migrating from 1.x to 2.x document: With the new GraphViews (SubGraph, ReversedGraph, etc)\nyou can't assume that ``G.__class__()`` will create a new\ninstance of the same graph type as ``G``. In fact, the\ncall signature for ``__class__`` differs depending on\nwhether ``G`` is a view or a base class. For v2.x you\nshould use ``G.fresh_copy()`` to create a null graph of\nthe correct type---ready to fill with nodes and edges.\n \n  \nmerge_nodes(src, dst, weight_func=<function min_weight>, in_place=True, extra_arguments=[], extra_keywords={}) [source]\n \nMerge node src and dst. The new combined node is adjacent to all the neighbors of src and dst. weight_func is called to decide the weight of edges incident on the new node.  Parameters \n \nsrc, dstint \n\nNodes to be merged.  \nweight_funccallable, optional \n\nFunction to decide the attributes of edges incident on the new node. For each neighbor n for src and `dst, weight_func will be called as follows: weight_func(src, dst, n, *extra_arguments, **extra_keywords). src, dst and n are IDs of vertices in the RAG object which is in turn a subclass of networkx.Graph. It is expected to return a dict of attributes of the resulting edge.  \nin_placebool, optional \n\nIf set to True, the merged node has the id dst, else merged node has a new id which is returned.  \nextra_argumentssequence, optional \n\nThe sequence of extra positional arguments passed to weight_func.  \nextra_keywordsdictionary, optional \n\nThe dict of keyword arguments passed to the weight_func.    Returns \n \nidint \n\nThe id of the new node.     Notes If in_place is False the resulting node has a new id, rather than dst. \n  \nnext_id() [source]\n \nReturns the id for the new node to be inserted. The current implementation returns one more than the maximum id.  Returns \n \nidint \n\nThe id of the new node to be inserted.     \n \n\n"}, {"name": "future.graph.cut_normalized()", "path": "api/skimage.future.graph#skimage.future.graph.cut_normalized", "type": "future", "text": " \nskimage.future.graph.cut_normalized(labels, rag, thresh=0.001, num_cuts=10, in_place=True, max_edge=1.0, *, random_state=None) [source]\n \nPerform Normalized Graph cut on the Region Adjacency Graph. Given an image\u2019s labels and its similarity RAG, recursively perform a 2-way normalized cut on it. All nodes belonging to a subgraph that cannot be cut further are assigned a unique label in the output.  Parameters \n \nlabelsndarray \n\nThe array of labels.  \nragRAG \n\nThe region adjacency graph.  \nthreshfloat \n\nThe threshold. A subgraph won\u2019t be further subdivided if the value of the N-cut exceeds thresh.  \nnum_cutsint \n\nThe number or N-cuts to perform before determining the optimal one.  \nin_placebool \n\nIf set, modifies rag in place. For each node n the function will set a new attribute rag.nodes[n]['ncut label'].  \nmax_edgefloat, optional \n\nThe maximum possible value of an edge in the RAG. This corresponds to an edge between identical regions. This is used to put self edges in the RAG.  \nrandom_stateint, RandomState instance or None, optional \n\nIf int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. The random state is used for the starting point of scipy.sparse.linalg.eigsh.    Returns \n \noutndarray \n\nThe new labeled array.     References  \n1  \nShi, J.; Malik, J., \u201cNormalized cuts and image segmentation\u201d, Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 888-905, August 2000.   Examples >>> from skimage import data, segmentation\n>>> from skimage.future import graph\n>>> img = data.astronaut()\n>>> labels = segmentation.slic(img)\n>>> rag = graph.rag_mean_color(img, labels, mode='similarity')\n>>> new_labels = graph.cut_normalized(labels, rag)\n \n"}, {"name": "future.graph.cut_threshold()", "path": "api/skimage.future.graph#skimage.future.graph.cut_threshold", "type": "future", "text": " \nskimage.future.graph.cut_threshold(labels, rag, thresh, in_place=True) [source]\n \nCombine regions separated by weight less than threshold. Given an image\u2019s labels and its RAG, output new labels by combining regions whose nodes are separated by a weight less than the given threshold.  Parameters \n \nlabelsndarray \n\nThe array of labels.  \nragRAG \n\nThe region adjacency graph.  \nthreshfloat \n\nThe threshold. Regions connected by edges with smaller weights are combined.  \nin_placebool \n\nIf set, modifies rag in place. The function will remove the edges with weights less that thresh. If set to False the function makes a copy of rag before proceeding.    Returns \n \noutndarray \n\nThe new labelled array.     References  \n1  \nAlain Tremeau and Philippe Colantoni \u201cRegions Adjacency Graph Applied To Color Image Segmentation\u201d DOI:10.1109/83.841950   Examples >>> from skimage import data, segmentation\n>>> from skimage.future import graph\n>>> img = data.astronaut()\n>>> labels = segmentation.slic(img)\n>>> rag = graph.rag_mean_color(img, labels)\n>>> new_labels = graph.cut_threshold(labels, rag, 10)\n \n"}, {"name": "future.graph.merge_hierarchical()", "path": "api/skimage.future.graph#skimage.future.graph.merge_hierarchical", "type": "future", "text": " \nskimage.future.graph.merge_hierarchical(labels, rag, thresh, rag_copy, in_place_merge, merge_func, weight_func) [source]\n \nPerform hierarchical merging of a RAG. Greedily merges the most similar pair of nodes until no edges lower than thresh remain.  Parameters \n \nlabelsndarray \n\nThe array of labels.  \nragRAG \n\nThe Region Adjacency Graph.  \nthreshfloat \n\nRegions connected by an edge with weight smaller than thresh are merged.  \nrag_copybool \n\nIf set, the RAG copied before modifying.  \nin_place_mergebool \n\nIf set, the nodes are merged in place. Otherwise, a new node is created for each merge..  \nmerge_funccallable \n\nThis function is called before merging two nodes. For the RAG graph while merging src and dst, it is called as follows merge_func(graph, src, dst).  \nweight_funccallable \n\nThe function to compute the new weights of the nodes adjacent to the merged node. This is directly supplied as the argument weight_func to merge_nodes.    Returns \n \noutndarray \n\nThe new labeled array.     \n"}, {"name": "future.graph.ncut()", "path": "api/skimage.future.graph#skimage.future.graph.ncut", "type": "future", "text": " \nskimage.future.graph.ncut(labels, rag, thresh=0.001, num_cuts=10, in_place=True, max_edge=1.0, *, random_state=None) [source]\n \nPerform Normalized Graph cut on the Region Adjacency Graph. Given an image\u2019s labels and its similarity RAG, recursively perform a 2-way normalized cut on it. All nodes belonging to a subgraph that cannot be cut further are assigned a unique label in the output.  Parameters \n \nlabelsndarray \n\nThe array of labels.  \nragRAG \n\nThe region adjacency graph.  \nthreshfloat \n\nThe threshold. A subgraph won\u2019t be further subdivided if the value of the N-cut exceeds thresh.  \nnum_cutsint \n\nThe number or N-cuts to perform before determining the optimal one.  \nin_placebool \n\nIf set, modifies rag in place. For each node n the function will set a new attribute rag.nodes[n]['ncut label'].  \nmax_edgefloat, optional \n\nThe maximum possible value of an edge in the RAG. This corresponds to an edge between identical regions. This is used to put self edges in the RAG.  \nrandom_stateint, RandomState instance or None, optional \n\nIf int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. The random state is used for the starting point of scipy.sparse.linalg.eigsh.    Returns \n \noutndarray \n\nThe new labeled array.     References  \n1  \nShi, J.; Malik, J., \u201cNormalized cuts and image segmentation\u201d, Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 888-905, August 2000.   Examples >>> from skimage import data, segmentation\n>>> from skimage.future import graph\n>>> img = data.astronaut()\n>>> labels = segmentation.slic(img)\n>>> rag = graph.rag_mean_color(img, labels, mode='similarity')\n>>> new_labels = graph.cut_normalized(labels, rag)\n \n"}, {"name": "future.graph.RAG", "path": "api/skimage.future.graph#skimage.future.graph.RAG", "type": "future", "text": " \nclass skimage.future.graph.RAG(label_image=None, connectivity=1, data=None, **attr) [source]\n \nBases: networkx.classes.graph.Graph The Region Adjacency Graph (RAG) of an image, subclasses networx.Graph  Parameters \n \nlabel_imagearray of int \n\nAn initial segmentation, with each region labeled as a different integer. Every unique value in label_image will correspond to a node in the graph.  \nconnectivityint in {1, \u2026, label_image.ndim}, optional \n\nThe connectivity between pixels in label_image. For a 2D image, a connectivity of 1 corresponds to immediate neighbors up, down, left, and right, while a connectivity of 2 also includes diagonal neighbors. See scipy.ndimage.generate_binary_structure.  \ndatanetworkx Graph specification, optional \n\nInitial or additional edges to pass to the NetworkX Graph constructor. See networkx.Graph. Valid edge specifications include edge list (list of tuples), NumPy arrays, and SciPy sparse matrices.  \n**attrkeyword arguments, optional \n\nAdditional attributes to add to the graph.      \n__init__(label_image=None, connectivity=1, data=None, **attr) [source]\n \nInitialize a graph with edges, name, or graph attributes.  Parameters \n \nincoming_graph_datainput graph (optional, default: None) \n\nData to initialize graph. If None (default) an empty graph is created. The data can be an edge list, or any NetworkX graph object. If the corresponding optional Python packages are installed the data can also be a NumPy matrix or 2d ndarray, a SciPy sparse matrix, or a PyGraphviz graph.  \nattrkeyword arguments, optional (default= no attributes) \n\nAttributes to add to graph as key=value pairs.      See also  \nconvert \n  Examples >>> G = nx.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc\n>>> G = nx.Graph(name=\"my graph\")\n>>> e = [(1, 2), (2, 3), (3, 4)]  # list of edges\n>>> G = nx.Graph(e)\n Arbitrary graph attribute pairs (key=value) may be assigned >>> G = nx.Graph(e, day=\"Friday\")\n>>> G.graph\n{'day': 'Friday'}\n \n  \nadd_edge(u, v, attr_dict=None, **attr) [source]\n \nAdd an edge between u and v while updating max node id.  See also networkx.Graph.add_edge().  \n  \nadd_node(n, attr_dict=None, **attr) [source]\n \nAdd node n while updating the maximum node id.  See also networkx.Graph.add_node().  \n  \ncopy() [source]\n \nCopy the graph with its max node id.  See also networkx.Graph.copy().  \n  \nfresh_copy() [source]\n \nReturn a fresh copy graph with the same data structure. A fresh copy has no nodes, edges or graph attributes. It is the same data structure as the current graph. This method is typically used to create an empty version of the graph. This is required when subclassing Graph with networkx v2 and does not cause problems for v1. Here is more detail from the network migrating from 1.x to 2.x document: With the new GraphViews (SubGraph, ReversedGraph, etc)\nyou can't assume that ``G.__class__()`` will create a new\ninstance of the same graph type as ``G``. In fact, the\ncall signature for ``__class__`` differs depending on\nwhether ``G`` is a view or a base class. For v2.x you\nshould use ``G.fresh_copy()`` to create a null graph of\nthe correct type---ready to fill with nodes and edges.\n \n  \nmerge_nodes(src, dst, weight_func=<function min_weight>, in_place=True, extra_arguments=[], extra_keywords={}) [source]\n \nMerge node src and dst. The new combined node is adjacent to all the neighbors of src and dst. weight_func is called to decide the weight of edges incident on the new node.  Parameters \n \nsrc, dstint \n\nNodes to be merged.  \nweight_funccallable, optional \n\nFunction to decide the attributes of edges incident on the new node. For each neighbor n for src and `dst, weight_func will be called as follows: weight_func(src, dst, n, *extra_arguments, **extra_keywords). src, dst and n are IDs of vertices in the RAG object which is in turn a subclass of networkx.Graph. It is expected to return a dict of attributes of the resulting edge.  \nin_placebool, optional \n\nIf set to True, the merged node has the id dst, else merged node has a new id which is returned.  \nextra_argumentssequence, optional \n\nThe sequence of extra positional arguments passed to weight_func.  \nextra_keywordsdictionary, optional \n\nThe dict of keyword arguments passed to the weight_func.    Returns \n \nidint \n\nThe id of the new node.     Notes If in_place is False the resulting node has a new id, rather than dst. \n  \nnext_id() [source]\n \nReturns the id for the new node to be inserted. The current implementation returns one more than the maximum id.  Returns \n \nidint \n\nThe id of the new node to be inserted.     \n \n"}, {"name": "future.graph.RAG.add_edge()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.add_edge", "type": "future", "text": " \nadd_edge(u, v, attr_dict=None, **attr) [source]\n \nAdd an edge between u and v while updating max node id.  See also networkx.Graph.add_edge().  \n"}, {"name": "future.graph.RAG.add_node()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.add_node", "type": "future", "text": " \nadd_node(n, attr_dict=None, **attr) [source]\n \nAdd node n while updating the maximum node id.  See also networkx.Graph.add_node().  \n"}, {"name": "future.graph.RAG.copy()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.copy", "type": "future", "text": " \ncopy() [source]\n \nCopy the graph with its max node id.  See also networkx.Graph.copy().  \n"}, {"name": "future.graph.RAG.fresh_copy()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.fresh_copy", "type": "future", "text": " \nfresh_copy() [source]\n \nReturn a fresh copy graph with the same data structure. A fresh copy has no nodes, edges or graph attributes. It is the same data structure as the current graph. This method is typically used to create an empty version of the graph. This is required when subclassing Graph with networkx v2 and does not cause problems for v1. Here is more detail from the network migrating from 1.x to 2.x document: With the new GraphViews (SubGraph, ReversedGraph, etc)\nyou can't assume that ``G.__class__()`` will create a new\ninstance of the same graph type as ``G``. In fact, the\ncall signature for ``__class__`` differs depending on\nwhether ``G`` is a view or a base class. For v2.x you\nshould use ``G.fresh_copy()`` to create a null graph of\nthe correct type---ready to fill with nodes and edges.\n \n"}, {"name": "future.graph.RAG.merge_nodes()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.merge_nodes", "type": "future", "text": " \nmerge_nodes(src, dst, weight_func=<function min_weight>, in_place=True, extra_arguments=[], extra_keywords={}) [source]\n \nMerge node src and dst. The new combined node is adjacent to all the neighbors of src and dst. weight_func is called to decide the weight of edges incident on the new node.  Parameters \n \nsrc, dstint \n\nNodes to be merged.  \nweight_funccallable, optional \n\nFunction to decide the attributes of edges incident on the new node. For each neighbor n for src and `dst, weight_func will be called as follows: weight_func(src, dst, n, *extra_arguments, **extra_keywords). src, dst and n are IDs of vertices in the RAG object which is in turn a subclass of networkx.Graph. It is expected to return a dict of attributes of the resulting edge.  \nin_placebool, optional \n\nIf set to True, the merged node has the id dst, else merged node has a new id which is returned.  \nextra_argumentssequence, optional \n\nThe sequence of extra positional arguments passed to weight_func.  \nextra_keywordsdictionary, optional \n\nThe dict of keyword arguments passed to the weight_func.    Returns \n \nidint \n\nThe id of the new node.     Notes If in_place is False the resulting node has a new id, rather than dst. \n"}, {"name": "future.graph.RAG.next_id()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.next_id", "type": "future", "text": " \nnext_id() [source]\n \nReturns the id for the new node to be inserted. The current implementation returns one more than the maximum id.  Returns \n \nidint \n\nThe id of the new node to be inserted.     \n"}, {"name": "future.graph.RAG.__init__()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.__init__", "type": "future", "text": " \n__init__(label_image=None, connectivity=1, data=None, **attr) [source]\n \nInitialize a graph with edges, name, or graph attributes.  Parameters \n \nincoming_graph_datainput graph (optional, default: None) \n\nData to initialize graph. If None (default) an empty graph is created. The data can be an edge list, or any NetworkX graph object. If the corresponding optional Python packages are installed the data can also be a NumPy matrix or 2d ndarray, a SciPy sparse matrix, or a PyGraphviz graph.  \nattrkeyword arguments, optional (default= no attributes) \n\nAttributes to add to graph as key=value pairs.      See also  \nconvert \n  Examples >>> G = nx.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc\n>>> G = nx.Graph(name=\"my graph\")\n>>> e = [(1, 2), (2, 3), (3, 4)]  # list of edges\n>>> G = nx.Graph(e)\n Arbitrary graph attribute pairs (key=value) may be assigned >>> G = nx.Graph(e, day=\"Friday\")\n>>> G.graph\n{'day': 'Friday'}\n \n"}, {"name": "future.graph.rag_boundary()", "path": "api/skimage.future.graph#skimage.future.graph.rag_boundary", "type": "future", "text": " \nskimage.future.graph.rag_boundary(labels, edge_map, connectivity=2) [source]\n \nComouter RAG based on region boundaries Given an image\u2019s initial segmentation and its edge map this method constructs the corresponding Region Adjacency Graph (RAG). Each node in the RAG represents a set of pixels within the image with the same label in labels. The weight between two adjacent regions is the average value in edge_map along their boundary.  \nlabelsndarray \n\nThe labelled image.  \nedge_mapndarray \n\nThis should have the same shape as that of labels. For all pixels along the boundary between 2 adjacent regions, the average value of the corresponding pixels in edge_map is the edge weight between them.  \nconnectivityint, optional \n\nPixels with a squared distance less than connectivity from each other are considered adjacent. It can range from 1 to labels.ndim. Its behavior is the same as connectivity parameter in scipy.ndimage.filters.generate_binary_structure.   Examples >>> from skimage import data, segmentation, filters, color\n>>> from skimage.future import graph\n>>> img = data.chelsea()\n>>> labels = segmentation.slic(img)\n>>> edge_map = filters.sobel(color.rgb2gray(img))\n>>> rag = graph.rag_boundary(labels, edge_map)\n \n"}, {"name": "future.graph.rag_mean_color()", "path": "api/skimage.future.graph#skimage.future.graph.rag_mean_color", "type": "future", "text": " \nskimage.future.graph.rag_mean_color(image, labels, connectivity=2, mode='distance', sigma=255.0) [source]\n \nCompute the Region Adjacency Graph using mean colors. Given an image and its initial segmentation, this method constructs the corresponding Region Adjacency Graph (RAG). Each node in the RAG represents a set of pixels within image with the same label in labels. The weight between two adjacent regions represents how similar or dissimilar two regions are depending on the mode parameter.  Parameters \n \nimagendarray, shape(M, N, [\u2026, P,] 3) \n\nInput image.  \nlabelsndarray, shape(M, N, [\u2026, P]) \n\nThe labelled image. This should have one dimension less than image. If image has dimensions (M, N, 3) labels should have dimensions (M, N).  \nconnectivityint, optional \n\nPixels with a squared distance less than connectivity from each other are considered adjacent. It can range from 1 to labels.ndim. Its behavior is the same as connectivity parameter in scipy.ndimage.generate_binary_structure.  \nmode{\u2018distance\u2019, \u2018similarity\u2019}, optional \n\nThe strategy to assign edge weights. \u2018distance\u2019 : The weight between two adjacent regions is the \\(|c_1 - c_2|\\), where \\(c_1\\) and \\(c_2\\) are the mean colors of the two regions. It represents the Euclidean distance in their average color. \u2018similarity\u2019 : The weight between two adjacent is \\(e^{-d^2/sigma}\\) where \\(d=|c_1 - c_2|\\), where \\(c_1\\) and \\(c_2\\) are the mean colors of the two regions. It represents how similar two regions are.  \nsigmafloat, optional \n\nUsed for computation when mode is \u201csimilarity\u201d. It governs how close to each other two colors should be, for their corresponding edge weight to be significant. A very large value of sigma could make any two colors behave as though they were similar.    Returns \n \noutRAG \n\nThe region adjacency graph.     References  \n1  \nAlain Tremeau and Philippe Colantoni \u201cRegions Adjacency Graph Applied To Color Image Segmentation\u201d DOI:10.1109/83.841950   Examples >>> from skimage import data, segmentation\n>>> from skimage.future import graph\n>>> img = data.astronaut()\n>>> labels = segmentation.slic(img)\n>>> rag = graph.rag_mean_color(img, labels)\n \n"}, {"name": "future.graph.show_rag()", "path": "api/skimage.future.graph#skimage.future.graph.show_rag", "type": "future", "text": " \nskimage.future.graph.show_rag(labels, rag, image, border_color='black', edge_width=1.5, edge_cmap='magma', img_cmap='bone', in_place=True, ax=None) [source]\n \nShow a Region Adjacency Graph on an image. Given a labelled image and its corresponding RAG, show the nodes and edges of the RAG on the image with the specified colors. Edges are displayed between the centroid of the 2 adjacent regions in the image.  Parameters \n \nlabelsndarray, shape (M, N) \n\nThe labelled image.  \nragRAG \n\nThe Region Adjacency Graph.  \nimagendarray, shape (M, N[, 3]) \n\nInput image. If colormap is None, the image should be in RGB format.  \nborder_colorcolor spec, optional \n\nColor with which the borders between regions are drawn.  \nedge_widthfloat, optional \n\nThe thickness with which the RAG edges are drawn.  \nedge_cmapmatplotlib.colors.Colormap, optional \n\nAny matplotlib colormap with which the edges are drawn.  \nimg_cmapmatplotlib.colors.Colormap, optional \n\nAny matplotlib colormap with which the image is draw. If set to None the image is drawn as it is.  \nin_placebool, optional \n\nIf set, the RAG is modified in place. For each node n the function will set a new attribute rag.nodes[n]['centroid'].  \naxmatplotlib.axes.Axes, optional \n\nThe axes to draw on. If not specified, new axes are created and drawn on.    Returns \n \nlcmatplotlib.collections.LineCollection \n\nA colection of lines that represent the edges of the graph. It can be passed to the matplotlib.figure.Figure.colorbar() function.     Examples >>> from skimage import data, segmentation\n>>> from skimage.future import graph\n>>> import matplotlib.pyplot as plt\n>>>\n>>> img = data.coffee()\n>>> labels = segmentation.slic(img)\n>>> g =  graph.rag_mean_color(img, labels)\n>>> lc = graph.show_rag(labels, g, img)\n>>> cbar = plt.colorbar(lc)\n \n"}, {"name": "future.manual_lasso_segmentation()", "path": "api/skimage.future#skimage.future.manual_lasso_segmentation", "type": "future", "text": " \nskimage.future.manual_lasso_segmentation(image, alpha=0.4, return_all=False) [source]\n \nReturn a label image based on freeform selections made with the mouse.  Parameters \n \nimage(M, N[, 3]) array \n\nGrayscale or RGB image.  \nalphafloat, optional \n\nTransparency value for polygons drawn over the image.  \nreturn_allbool, optional \n\nIf True, an array containing each separate polygon drawn is returned. (The polygons may overlap.) If False (default), latter polygons \u201coverwrite\u201d earlier ones where they overlap.    Returns \n \nlabelsarray of int, shape ([Q, ]M, N) \n\nThe segmented regions. If mode is \u2018separate\u2019, the leading dimension of the array corresponds to the number of regions that the user drew.     Notes Press and hold the left mouse button to draw around each object. Examples >>> from skimage import data, future, io\n>>> camera = data.camera()\n>>> mask = future.manual_lasso_segmentation(camera)  \n>>> io.imshow(mask)  \n>>> io.show()  \n \n"}, {"name": "future.manual_polygon_segmentation()", "path": "api/skimage.future#skimage.future.manual_polygon_segmentation", "type": "future", "text": " \nskimage.future.manual_polygon_segmentation(image, alpha=0.4, return_all=False) [source]\n \nReturn a label image based on polygon selections made with the mouse.  Parameters \n \nimage(M, N[, 3]) array \n\nGrayscale or RGB image.  \nalphafloat, optional \n\nTransparency value for polygons drawn over the image.  \nreturn_allbool, optional \n\nIf True, an array containing each separate polygon drawn is returned. (The polygons may overlap.) If False (default), latter polygons \u201coverwrite\u201d earlier ones where they overlap.    Returns \n \nlabelsarray of int, shape ([Q, ]M, N) \n\nThe segmented regions. If mode is \u2018separate\u2019, the leading dimension of the array corresponds to the number of regions that the user drew.     Notes Use left click to select the vertices of the polygon and right click to confirm the selection once all vertices are selected. Examples >>> from skimage import data, future, io\n>>> camera = data.camera()\n>>> mask = future.manual_polygon_segmentation(camera)  \n>>> io.imshow(mask)  \n>>> io.show()  \n \n"}, {"name": "future.predict_segmenter()", "path": "api/skimage.future#skimage.future.predict_segmenter", "type": "future", "text": " \nskimage.future.predict_segmenter(features, clf) [source]\n \nSegmentation of images using a pretrained classifier.  Parameters \n \nfeaturesndarray \n\nArray of features, with the last dimension corresponding to the number of features, and the other dimensions are compatible with the shape of the image to segment, or a flattened image.  \nclfclassifier object \n\ntrained classifier object, exposing a predict method as in scikit-learn\u2019s API, for example an instance of RandomForestClassifier or LogisticRegression classifier. The classifier must be already trained, for example with skimage.segmentation.fit_segmenter().    Returns \n \noutputndarray \n\nLabeled array, built from the prediction of the classifier.     \n"}, {"name": "future.TrainableSegmenter", "path": "api/skimage.future#skimage.future.TrainableSegmenter", "type": "future", "text": " \nclass skimage.future.TrainableSegmenter(clf=None, features_func=None) [source]\n \nBases: object Estimator for classifying pixels.  Parameters \n \nclfclassifier object, optional \n\nclassifier object, exposing a fit and a predict method as in scikit-learn\u2019s API, for example an instance of RandomForestClassifier or LogisticRegression classifier.  \nfeatures_funcfunction, optional \n\nfunction computing features on all pixels of the image, to be passed to the classifier. The output should be of shape (m_features, *labels.shape). If None, skimage.segmentation.multiscale_basic_features() is used.     Methods  \nfit(image, labels) Train classifier using partially labeled (annotated) image.  \npredict(image) Segment new image using trained internal classifier.    \ncompute_features     \n__init__(clf=None, features_func=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \ncompute_features(image) [source]\n\n  \nfit(image, labels) [source]\n \nTrain classifier using partially labeled (annotated) image.  Parameters \n \nimagendarray \n\nInput image, which can be grayscale or multichannel, and must have a number of dimensions compatible with self.features_func.  \nlabelsndarray of ints \n\nLabeled array of shape compatible with image (same shape for a single-channel image). Labels >= 1 correspond to the training set and label 0 to unlabeled pixels to be segmented.     \n  \npredict(image) [source]\n \nSegment new image using trained internal classifier.  Parameters \n \nimagendarray \n\nInput image, which can be grayscale or multichannel, and must have a number of dimensions compatible with self.features_func.    Raises \n \nNotFittedError if self.clf has not been fitted yet (use self.fit). \n   \n \n"}, {"name": "future.TrainableSegmenter.compute_features()", "path": "api/skimage.future#skimage.future.TrainableSegmenter.compute_features", "type": "future", "text": " \ncompute_features(image) [source]\n\n"}, {"name": "future.TrainableSegmenter.fit()", "path": "api/skimage.future#skimage.future.TrainableSegmenter.fit", "type": "future", "text": " \nfit(image, labels) [source]\n \nTrain classifier using partially labeled (annotated) image.  Parameters \n \nimagendarray \n\nInput image, which can be grayscale or multichannel, and must have a number of dimensions compatible with self.features_func.  \nlabelsndarray of ints \n\nLabeled array of shape compatible with image (same shape for a single-channel image). Labels >= 1 correspond to the training set and label 0 to unlabeled pixels to be segmented.     \n"}, {"name": "future.TrainableSegmenter.predict()", "path": "api/skimage.future#skimage.future.TrainableSegmenter.predict", "type": "future", "text": " \npredict(image) [source]\n \nSegment new image using trained internal classifier.  Parameters \n \nimagendarray \n\nInput image, which can be grayscale or multichannel, and must have a number of dimensions compatible with self.features_func.    Raises \n \nNotFittedError if self.clf has not been fitted yet (use self.fit). \n   \n"}, {"name": "future.TrainableSegmenter.__init__()", "path": "api/skimage.future#skimage.future.TrainableSegmenter.__init__", "type": "future", "text": " \n__init__(clf=None, features_func=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "Geometrical transformations of images", "path": "user_guide/geometrical_transform", "type": "Guide", "text": "Geometrical transformations of images Cropping, resizing and rescaling images Images being NumPy arrays (as described in the A crash course on NumPy for images section), cropping an image can be done with simple slicing operations. Below we crop a 100x100 square corresponding to the top-left corner of the astronaut image. Note that this operation is done for all color channels (the color dimension is the last, third dimension): >>> from skimage import data\n>>> img = data.astronaut()\n>>> top_left = img[:100, :100]\n In order to change the shape of the image, skimage.color provides several functions described in Rescale, resize, and downscale . \nfrom skimage import data, color\nfrom skimage.transform import rescale, resize, downscale_local_mean\n\nimage = color.rgb2gray(data.astronaut())\n\nimage_rescaled = rescale(image, 0.25, anti_aliasing=False)\nimage_resized = resize(image, (image.shape[0] // 4, image.shape[1] // 4),\n                       anti_aliasing=True)\nimage_downscaled = downscale_local_mean(image, (4, 3))\n\n  Projective transforms (homographies) Homographies are transformations of a Euclidean space that preserve the alignment of points. Specific cases of homographies correspond to the conservation of more properties, such as parallelism (affine transformation), shape (similar transformation) or distances (Euclidean transformation). The different types of homographies available in scikit-image are presented in Types of homographies. Projective transformations can either be created using the explicit parameters (e.g. scale, shear, rotation and translation): from skimage import data\nfrom skimage import transform\nfrom skimage import img_as_float\n\ntform = transform.EuclideanTransform(\n   rotation=np.pi / 12.,\n   translation = (100, -20)\n   )\n or the full transformation matrix: from skimage import data\nfrom skimage import transform\nfrom skimage import img_as_float\n\nmatrix = np.array([[np.cos(np.pi/12), -np.sin(np.pi/12), 100],\n                   [np.sin(np.pi/12), np.cos(np.pi/12), -20],\n                   [0, 0, 1]])\ntform = transform.EuclideanTransform(matrix)\n The transformation matrix of a transform is available as its tform.params attribute. Transformations can be composed by multiplying matrices with the @ matrix multiplication operator. Transformation matrices use Homogeneous coordinates, which are the extension of Cartesian coordinates used in Euclidean geometry to the more general projective geometry. In particular, points at infinity can be represented with finite coordinates. Transformations can be applied to images using skimage.transform.warp(): img = img_as_float(data.chelsea())\ntf_img = transform.warp(img, tform.inverse)\n  The different transformations in skimage.transform have a estimate method in order to estimate the parameters of the transformation from two sets of points (the source and the destination), as explained in the Using geometric transformations tutorial: text = data.text()\n\nsrc = np.array([[0, 0], [0, 50], [300, 50], [300, 0]])\ndst = np.array([[155, 15], [65, 40], [260, 130], [360, 95]])\n\ntform3 = transform.ProjectiveTransform()\ntform3.estimate(src, dst)\nwarped = transform.warp(text, tform3, output_shape=(50, 300))\n  The estimate method uses least-squares optimization to minimize the distance between source and optimization. Source and destination points can be determined manually, or using the different methods for feature detection available in skimage.feature, such as  \nCorner detection, \nORB feature detector and binary descriptor, \nBRIEF binary descriptor, etc.  and matching points using skimage.feature.match_descriptors() before estimating transformation parameters. However, spurious matches are often made, and it is advisable to use the RANSAC algorithm (instead of simple least-squares optimization) to improve the robustness to outliers, as explained in Robust matching using RANSAC.  Examples showing applications of transformation estimation are  stereo matching Fundamental matrix estimation and image rectification Using geometric transformations\n  The estimate method is point-based, that is, it uses only a set of points from the source and destination images. For estimating translations (shifts), it is also possible to use a full-field method using all pixels, based on Fourier-space cross-correlation. This method is implemented by skimage.registration.register_translation() and explained in the Image Registration tutorial.  The Using Polar and Log-Polar Transformations for Registration tutorial explains a variant of this full-field method for estimating a rotation, by using first a log-polar transformation.\n"}, {"name": "Getting help on using skimage", "path": "user_guide/getting_help", "type": "Guide", "text": "Getting help on using skimage Besides the user guide, there exist other opportunities to get help on using skimage. Examples gallery The General examples gallery provides graphical examples of typical image processing tasks. By a quick glance at the different thumbnails, the user may find an example close to a typical use case of interest. Each graphical example page displays an introductory paragraph, a figure, and the source code that generated the figure. Downloading the Python source code enables one to modify quickly the example into a case closer to one\u2019s image processing applications. Users are warmly encouraged to report on their use of skimage on the Mailing-list, in order to propose more examples in the future. Contributing examples to the gallery can be done on github (see How to contribute to scikit-image). Search field The quick search field located in the navigation bar of the html documentation can be used to search for specific keywords (segmentation, rescaling, denoising, etc.). API Discovery NumPy provides a lookfor function to search API functions. By default lookfor will search the NumPy API. NumPy lookfor example: `np.lookfor('eigenvector') ` But it can be used to search in modules, by passing in the module name as a string: ` np.lookfor('boundaries', 'skimage') ` or the module itself. `\n> import skimage\n> np.lookfor('boundaries', skimage)\n` Docstrings Docstrings of skimage functions are formatted using Numpy\u2019s documentation standard, starting with a Parameters section for the arguments and a Returns section for the objects returned by the function. Also, most functions include one or more examples. Mailing-list The scikit-image mailing-list is scikit-image@python.org (users should join before posting). This mailing-list is shared by users and developers, and it is the right place to ask any question about skimage, or in general, image processing using Python. Posting snippets of code with minimal examples ensures to get more relevant and focused answers. We would love to hear from how you use skimage for your work on the mailing-list!\n"}, {"name": "Getting started", "path": "user_guide/getting_started", "type": "Guide", "text": "Getting started scikit-image is an image processing Python package that works with numpy arrays. The package is imported as skimage: >>> import skimage\n Most functions of skimage are found within submodules: >>> from skimage import data\n>>> camera = data.camera()\n A list of submodules and functions is found on the API reference webpage. Within scikit-image, images are represented as NumPy arrays, for example 2-D arrays for grayscale 2-D images >>> type(camera)\n<type 'numpy.ndarray'>\n>>> # An image with 512 rows and 512 columns\n>>> camera.shape\n(512, 512)\n The skimage.data submodule provides a set of functions returning example images, that can be used to get started quickly on using scikit-image\u2019s functions: >>> coins = data.coins()\n>>> from skimage import filters\n>>> threshold_value = filters.threshold_otsu(coins)\n>>> threshold_value\n107\n Of course, it is also possible to load your own images as NumPy arrays from image files, using skimage.io.imread(): >>> import os\n>>> filename = os.path.join(skimage.data_dir, 'moon.png')\n>>> from skimage import io\n>>> moon = io.imread(filename)\n Use natsort to load multiple images >>> import os\n>>> from natsort import natsorted, ns\n>>> from skimage import io\n>>> list_files = os.listdir('.')\n>>> list_files\n['01.png', '010.png', '0101.png', '0190.png', '02.png']\n>>> list_files = natsorted(list_files)\n>>> list_files\n['01.png', '02.png', '010.png', '0101.png', '0190.png']\n>>> image_list = []\n>>> for filename in list_files:\n...   image_list.append(io.imread(filename))\n\n"}, {"name": "graph", "path": "api/skimage.graph", "type": "graph", "text": "Module: graph  \nskimage.graph.route_through_array(array, \u2026) Simple example of how to use the MCP and MCP_Geometric classes.  \nskimage.graph.shortest_path(arr[, reach, \u2026]) Find the shortest path through an n-d array from one side to another.  \nskimage.graph.MCP(costs[, offsets, \u2026]) A class for finding the minimum cost path through a given n-d costs array.  \nskimage.graph.MCP_Connect(costs[, offsets, \u2026]) Connect source points using the distance-weighted minimum cost function.  \nskimage.graph.MCP_Flexible(costs[, offsets, \u2026]) Find minimum cost paths through an N-d costs array.  \nskimage.graph.MCP_Geometric(costs[, \u2026]) Find distance-weighted minimum cost paths through an n-d costs array.   route_through_array  \nskimage.graph.route_through_array(array, start, end, fully_connected=True, geometric=True) [source]\n \nSimple example of how to use the MCP and MCP_Geometric classes. See the MCP and MCP_Geometric class documentation for explanation of the path-finding algorithm.  Parameters \n \narrayndarray \n\nArray of costs.  \nstartiterable \n\nn-d index into array defining the starting point  \nenditerable \n\nn-d index into array defining the end point  \nfully_connectedbool (optional) \n\nIf True, diagonal moves are permitted, if False, only axial moves.  \ngeometricbool (optional) \n\nIf True, the MCP_Geometric class is used to calculate costs, if False, the MCP base class is used. See the class documentation for an explanation of the differences between MCP and MCP_Geometric.    Returns \n \npathlist \n\nList of n-d index tuples defining the path from start to end.  \ncostfloat \n\nCost of the path. If geometric is False, the cost of the path is the sum of the values of array along the path. If geometric is True, a finer computation is made (see the documentation of the MCP_Geometric class).      See also  \nMCP, MCP_Geometric\n\n  Examples >>> import numpy as np\n>>> from skimage.graph import route_through_array\n>>>\n>>> image = np.array([[1, 3], [10, 12]])\n>>> image\narray([[ 1,  3],\n       [10, 12]])\n>>> # Forbid diagonal steps\n>>> route_through_array(image, [0, 0], [1, 1], fully_connected=False)\n([(0, 0), (0, 1), (1, 1)], 9.5)\n>>> # Now allow diagonal steps: the path goes directly from start to end\n>>> route_through_array(image, [0, 0], [1, 1])\n([(0, 0), (1, 1)], 9.19238815542512)\n>>> # Cost is the sum of array values along the path (16 = 1 + 3 + 12)\n>>> route_through_array(image, [0, 0], [1, 1], fully_connected=False,\n... geometric=False)\n([(0, 0), (0, 1), (1, 1)], 16.0)\n>>> # Larger array where we display the path that is selected\n>>> image = np.arange((36)).reshape((6, 6))\n>>> image\narray([[ 0,  1,  2,  3,  4,  5],\n       [ 6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17],\n       [18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35]])\n>>> # Find the path with lowest cost\n>>> indices, weight = route_through_array(image, (0, 0), (5, 5))\n>>> indices = np.stack(indices, axis=-1)\n>>> path = np.zeros_like(image)\n>>> path[indices[0], indices[1]] = 1\n>>> path\narray([[1, 1, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1]])\n \n shortest_path  \nskimage.graph.shortest_path(arr, reach=1, axis=-1, output_indexlist=False) [source]\n \nFind the shortest path through an n-d array from one side to another.  Parameters \n \narrndarray of float64 \n\nreachint, optional \n\nBy default (reach = 1), the shortest path can only move one row up or down for every step it moves forward (i.e., the path gradient is limited to 1). reach defines the number of elements that can be skipped along each non-axis dimension at each step.  \naxisint, optional \n\nThe axis along which the path must always move forward (default -1)  \noutput_indexlistbool, optional \n\nSee return value p for explanation.    Returns \n \npiterable of int \n\nFor each step along axis, the coordinate of the shortest path. If output_indexlist is True, then the path is returned as a list of n-d tuples that index into arr. If False, then the path is returned as an array listing the coordinates of the path along the non-axis dimensions for each step along the axis dimension. That is, p.shape == (arr.shape[axis], arr.ndim-1) except that p is squeezed before returning so if arr.ndim == 2, then p.shape == (arr.shape[axis],)  \ncostfloat \n\nCost of path. This is the absolute sum of all the differences along the path.     \n MCP  \nclass skimage.graph.MCP(costs, offsets=None, fully_connected=True, sampling=None)  \nBases: object A class for finding the minimum cost path through a given n-d costs array. Given an n-d costs array, this class can be used to find the minimum-cost path through that array from any set of points to any other set of points. Basic usage is to initialize the class and call find_costs() with a one or more starting indices (and an optional list of end indices). After that, call traceback() one or more times to find the path from any given end-position to the closest starting index. New paths through the same costs array can be found by calling find_costs() repeatedly. The cost of a path is calculated simply as the sum of the values of the costs array at each point on the path. The class MCP_Geometric, on the other hand, accounts for the fact that diagonal vs. axial moves are of different lengths, and weights the path cost accordingly. Array elements with infinite or negative costs will simply be ignored, as will paths whose cumulative cost overflows to infinite.  Parameters \n \ncostsndarray \n\noffsetsiterable, optional \n\nA list of offset tuples: each offset specifies a valid move from a given n-d position. If not provided, offsets corresponding to a singly- or fully-connected n-d neighborhood will be constructed with make_offsets(), using the fully_connected parameter value.  \nfully_connectedbool, optional \n\nIf no offsets are provided, this determines the connectivity of the generated neighborhood. If true, the path may go along diagonals between elements of the costs array; otherwise only axial moves are permitted.  \nsamplingtuple, optional \n\nFor each dimension, specifies the distance between two cells/voxels. If not given or None, the distance is assumed unit.    Attributes \n \noffsetsndarray \n\nEquivalent to the offsets provided to the constructor, or if none were so provided, the offsets created for the requested n-d neighborhood. These are useful for interpreting the traceback array returned by the find_costs() method.      \n__init__(costs, offsets=None, fully_connected=True, sampling=None)  \nSee class documentation. \n  \nfind_costs()  \nFind the minimum-cost path from the given starting points. This method finds the minimum-cost path to the specified ending indices from any one of the specified starting indices. If no end positions are given, then the minimum-cost path to every position in the costs array will be found.  Parameters \n \nstartsiterable \n\nA list of n-d starting indices (where n is the dimension of the costs array). The minimum cost path to the closest/cheapest starting point will be found.  \nendsiterable, optional \n\nA list of n-d ending indices.  \nfind_all_endsbool, optional \n\nIf \u2018True\u2019 (default), the minimum-cost-path to every specified end-position will be found; otherwise the algorithm will stop when a a path is found to any end-position. (If no ends were specified, then this parameter has no effect.)    Returns \n \ncumulative_costsndarray \n\nSame shape as the costs array; this array records the minimum cost path from the nearest/cheapest starting index to each index considered. (If ends were specified, not all elements in the array will necessarily be considered: positions not evaluated will have a cumulative cost of inf. If find_all_ends is \u2018False\u2019, only one of the specified end-positions will have a finite cumulative cost.)  \ntracebackndarray \n\nSame shape as the costs array; this array contains the offset to any given index from its predecessor index. The offset indices index into the offsets attribute, which is a array of n-d offsets. In the 2-d case, if offsets[traceback[x, y]] is (-1, -1), that means that the predecessor of [x, y] in the minimum cost path to some start position is [x+1, y+1]. Note that if the offset_index is -1, then the given index was not considered.     \n  \ngoal_reached()  \nint goal_reached(int index, float cumcost) This method is called each iteration after popping an index from the heap, before examining the neighbours. This method can be overloaded to modify the behavior of the MCP algorithm. An example might be to stop the algorithm when a certain cumulative cost is reached, or when the front is a certain distance away from the seed point. This method should return 1 if the algorithm should not check the current point\u2019s neighbours and 2 if the algorithm is now done. \n  \ntraceback(end)  \nTrace a minimum cost path through the pre-calculated traceback array. This convenience function reconstructs the the minimum cost path to a given end position from one of the starting indices provided to find_costs(), which must have been called previously. This function can be called as many times as desired after find_costs() has been run.  Parameters \n \nenditerable \n\nAn n-d index into the costs array.    Returns \n \ntracebacklist of n-d tuples \n\nA list of indices into the costs array, starting with one of the start positions passed to find_costs(), and ending with the given end index. These indices specify the minimum-cost path from any given start index to the end index. (The total cost of that path can be read out from the cumulative_costs array returned by find_costs().)     \n \n MCP_Connect  \nclass skimage.graph.MCP_Connect(costs, offsets=None, fully_connected=True)  \nBases: skimage.graph._mcp.MCP Connect source points using the distance-weighted minimum cost function. A front is grown from each seed point simultaneously, while the origin of the front is tracked as well. When two fronts meet, create_connection() is called. This method must be overloaded to deal with the found edges in a way that is appropriate for the application.  \n__init__(*args, **kwargs)  \nInitialize self. See help(type(self)) for accurate signature. \n  \ncreate_connection()  \ncreate_connection id1, id2, pos1, pos2, cost1, cost2) Overload this method to keep track of the connections that are found during MCP processing. Note that a connection with the same ids can be found multiple times (but with different positions and costs). At the time that this method is called, both points are \u201cfrozen\u201d and will not be visited again by the MCP algorithm.  Parameters \n \nid1int \n\nThe seed point id where the first neighbor originated from.  \nid2int \n\nThe seed point id where the second neighbor originated from.  \npos1tuple \n\nThe index of of the first neighbour in the connection.  \npos2tuple \n\nThe index of of the second neighbour in the connection.  \ncost1float \n\nThe cumulative cost at pos1.  \ncost2float \n\nThe cumulative costs at pos2.     \n \n MCP_Flexible  \nclass skimage.graph.MCP_Flexible(costs, offsets=None, fully_connected=True)  \nBases: skimage.graph._mcp.MCP Find minimum cost paths through an N-d costs array. See the documentation for MCP for full details. This class differs from MCP in that several methods can be overloaded (from pure Python) to modify the behavior of the algorithm and/or create custom algorithms based on MCP. Note that goal_reached can also be overloaded in the MCP class.  \n__init__(costs, offsets=None, fully_connected=True, sampling=None)  \nSee class documentation. \n  \nexamine_neighbor(index, new_index, offset_length)  \nThis method is called once for every pair of neighboring nodes, as soon as both nodes are frozen. This method can be overloaded to obtain information about neightboring nodes, and/or to modify the behavior of the MCP algorithm. One example is the MCP_Connect class, which checks for meeting fronts using this hook. \n  \ntravel_cost(old_cost, new_cost, offset_length)  \nThis method calculates the travel cost for going from the current node to the next. The default implementation returns new_cost. Overload this method to adapt the behaviour of the algorithm. \n  \nupdate_node(index, new_index, offset_length)  \nThis method is called when a node is updated, right after new_index is pushed onto the heap and the traceback map is updated. This method can be overloaded to keep track of other arrays that are used by a specific implementation of the algorithm. For instance the MCP_Connect class uses it to update an id map. \n \n MCP_Geometric  \nclass skimage.graph.MCP_Geometric(costs, offsets=None, fully_connected=True)  \nBases: skimage.graph._mcp.MCP Find distance-weighted minimum cost paths through an n-d costs array. See the documentation for MCP for full details. This class differs from MCP in that the cost of a path is not simply the sum of the costs along that path. This class instead assumes that the costs array contains at each position the \u201ccost\u201d of a unit distance of travel through that position. For example, a move (in 2-d) from (1, 1) to (1, 2) is assumed to originate in the center of the pixel (1, 1) and terminate in the center of (1, 2). The entire move is of distance 1, half through (1, 1) and half through (1, 2); thus the cost of that move is (1/2)*costs[1,1] + (1/2)*costs[1,2]. On the other hand, a move from (1, 1) to (2, 2) is along the diagonal and is sqrt(2) in length. Half of this move is within the pixel (1, 1) and the other half in (2, 2), so the cost of this move is calculated as (sqrt(2)/2)*costs[1,1] + (sqrt(2)/2)*costs[2,2]. These calculations don\u2019t make a lot of sense with offsets of magnitude greater than 1. Use the sampling argument in order to deal with anisotropic data.  \n__init__(costs, offsets=None, fully_connected=True, sampling=None)  \nSee class documentation. \n \n\n"}, {"name": "graph.MCP", "path": "api/skimage.graph#skimage.graph.MCP", "type": "graph", "text": " \nclass skimage.graph.MCP(costs, offsets=None, fully_connected=True, sampling=None)  \nBases: object A class for finding the minimum cost path through a given n-d costs array. Given an n-d costs array, this class can be used to find the minimum-cost path through that array from any set of points to any other set of points. Basic usage is to initialize the class and call find_costs() with a one or more starting indices (and an optional list of end indices). After that, call traceback() one or more times to find the path from any given end-position to the closest starting index. New paths through the same costs array can be found by calling find_costs() repeatedly. The cost of a path is calculated simply as the sum of the values of the costs array at each point on the path. The class MCP_Geometric, on the other hand, accounts for the fact that diagonal vs. axial moves are of different lengths, and weights the path cost accordingly. Array elements with infinite or negative costs will simply be ignored, as will paths whose cumulative cost overflows to infinite.  Parameters \n \ncostsndarray \n\noffsetsiterable, optional \n\nA list of offset tuples: each offset specifies a valid move from a given n-d position. If not provided, offsets corresponding to a singly- or fully-connected n-d neighborhood will be constructed with make_offsets(), using the fully_connected parameter value.  \nfully_connectedbool, optional \n\nIf no offsets are provided, this determines the connectivity of the generated neighborhood. If true, the path may go along diagonals between elements of the costs array; otherwise only axial moves are permitted.  \nsamplingtuple, optional \n\nFor each dimension, specifies the distance between two cells/voxels. If not given or None, the distance is assumed unit.    Attributes \n \noffsetsndarray \n\nEquivalent to the offsets provided to the constructor, or if none were so provided, the offsets created for the requested n-d neighborhood. These are useful for interpreting the traceback array returned by the find_costs() method.      \n__init__(costs, offsets=None, fully_connected=True, sampling=None)  \nSee class documentation. \n  \nfind_costs()  \nFind the minimum-cost path from the given starting points. This method finds the minimum-cost path to the specified ending indices from any one of the specified starting indices. If no end positions are given, then the minimum-cost path to every position in the costs array will be found.  Parameters \n \nstartsiterable \n\nA list of n-d starting indices (where n is the dimension of the costs array). The minimum cost path to the closest/cheapest starting point will be found.  \nendsiterable, optional \n\nA list of n-d ending indices.  \nfind_all_endsbool, optional \n\nIf \u2018True\u2019 (default), the minimum-cost-path to every specified end-position will be found; otherwise the algorithm will stop when a a path is found to any end-position. (If no ends were specified, then this parameter has no effect.)    Returns \n \ncumulative_costsndarray \n\nSame shape as the costs array; this array records the minimum cost path from the nearest/cheapest starting index to each index considered. (If ends were specified, not all elements in the array will necessarily be considered: positions not evaluated will have a cumulative cost of inf. If find_all_ends is \u2018False\u2019, only one of the specified end-positions will have a finite cumulative cost.)  \ntracebackndarray \n\nSame shape as the costs array; this array contains the offset to any given index from its predecessor index. The offset indices index into the offsets attribute, which is a array of n-d offsets. In the 2-d case, if offsets[traceback[x, y]] is (-1, -1), that means that the predecessor of [x, y] in the minimum cost path to some start position is [x+1, y+1]. Note that if the offset_index is -1, then the given index was not considered.     \n  \ngoal_reached()  \nint goal_reached(int index, float cumcost) This method is called each iteration after popping an index from the heap, before examining the neighbours. This method can be overloaded to modify the behavior of the MCP algorithm. An example might be to stop the algorithm when a certain cumulative cost is reached, or when the front is a certain distance away from the seed point. This method should return 1 if the algorithm should not check the current point\u2019s neighbours and 2 if the algorithm is now done. \n  \ntraceback(end)  \nTrace a minimum cost path through the pre-calculated traceback array. This convenience function reconstructs the the minimum cost path to a given end position from one of the starting indices provided to find_costs(), which must have been called previously. This function can be called as many times as desired after find_costs() has been run.  Parameters \n \nenditerable \n\nAn n-d index into the costs array.    Returns \n \ntracebacklist of n-d tuples \n\nA list of indices into the costs array, starting with one of the start positions passed to find_costs(), and ending with the given end index. These indices specify the minimum-cost path from any given start index to the end index. (The total cost of that path can be read out from the cumulative_costs array returned by find_costs().)     \n \n"}, {"name": "graph.MCP.find_costs()", "path": "api/skimage.graph#skimage.graph.MCP.find_costs", "type": "graph", "text": " \nfind_costs()  \nFind the minimum-cost path from the given starting points. This method finds the minimum-cost path to the specified ending indices from any one of the specified starting indices. If no end positions are given, then the minimum-cost path to every position in the costs array will be found.  Parameters \n \nstartsiterable \n\nA list of n-d starting indices (where n is the dimension of the costs array). The minimum cost path to the closest/cheapest starting point will be found.  \nendsiterable, optional \n\nA list of n-d ending indices.  \nfind_all_endsbool, optional \n\nIf \u2018True\u2019 (default), the minimum-cost-path to every specified end-position will be found; otherwise the algorithm will stop when a a path is found to any end-position. (If no ends were specified, then this parameter has no effect.)    Returns \n \ncumulative_costsndarray \n\nSame shape as the costs array; this array records the minimum cost path from the nearest/cheapest starting index to each index considered. (If ends were specified, not all elements in the array will necessarily be considered: positions not evaluated will have a cumulative cost of inf. If find_all_ends is \u2018False\u2019, only one of the specified end-positions will have a finite cumulative cost.)  \ntracebackndarray \n\nSame shape as the costs array; this array contains the offset to any given index from its predecessor index. The offset indices index into the offsets attribute, which is a array of n-d offsets. In the 2-d case, if offsets[traceback[x, y]] is (-1, -1), that means that the predecessor of [x, y] in the minimum cost path to some start position is [x+1, y+1]. Note that if the offset_index is -1, then the given index was not considered.     \n"}, {"name": "graph.MCP.goal_reached()", "path": "api/skimage.graph#skimage.graph.MCP.goal_reached", "type": "graph", "text": " \ngoal_reached()  \nint goal_reached(int index, float cumcost) This method is called each iteration after popping an index from the heap, before examining the neighbours. This method can be overloaded to modify the behavior of the MCP algorithm. An example might be to stop the algorithm when a certain cumulative cost is reached, or when the front is a certain distance away from the seed point. This method should return 1 if the algorithm should not check the current point\u2019s neighbours and 2 if the algorithm is now done. \n"}, {"name": "graph.MCP.traceback()", "path": "api/skimage.graph#skimage.graph.MCP.traceback", "type": "graph", "text": " \ntraceback(end)  \nTrace a minimum cost path through the pre-calculated traceback array. This convenience function reconstructs the the minimum cost path to a given end position from one of the starting indices provided to find_costs(), which must have been called previously. This function can be called as many times as desired after find_costs() has been run.  Parameters \n \nenditerable \n\nAn n-d index into the costs array.    Returns \n \ntracebacklist of n-d tuples \n\nA list of indices into the costs array, starting with one of the start positions passed to find_costs(), and ending with the given end index. These indices specify the minimum-cost path from any given start index to the end index. (The total cost of that path can be read out from the cumulative_costs array returned by find_costs().)     \n"}, {"name": "graph.MCP.__init__()", "path": "api/skimage.graph#skimage.graph.MCP.__init__", "type": "graph", "text": " \n__init__(costs, offsets=None, fully_connected=True, sampling=None)  \nSee class documentation. \n"}, {"name": "graph.MCP_Connect", "path": "api/skimage.graph#skimage.graph.MCP_Connect", "type": "graph", "text": " \nclass skimage.graph.MCP_Connect(costs, offsets=None, fully_connected=True)  \nBases: skimage.graph._mcp.MCP Connect source points using the distance-weighted minimum cost function. A front is grown from each seed point simultaneously, while the origin of the front is tracked as well. When two fronts meet, create_connection() is called. This method must be overloaded to deal with the found edges in a way that is appropriate for the application.  \n__init__(*args, **kwargs)  \nInitialize self. See help(type(self)) for accurate signature. \n  \ncreate_connection()  \ncreate_connection id1, id2, pos1, pos2, cost1, cost2) Overload this method to keep track of the connections that are found during MCP processing. Note that a connection with the same ids can be found multiple times (but with different positions and costs). At the time that this method is called, both points are \u201cfrozen\u201d and will not be visited again by the MCP algorithm.  Parameters \n \nid1int \n\nThe seed point id where the first neighbor originated from.  \nid2int \n\nThe seed point id where the second neighbor originated from.  \npos1tuple \n\nThe index of of the first neighbour in the connection.  \npos2tuple \n\nThe index of of the second neighbour in the connection.  \ncost1float \n\nThe cumulative cost at pos1.  \ncost2float \n\nThe cumulative costs at pos2.     \n \n"}, {"name": "graph.MCP_Connect.create_connection()", "path": "api/skimage.graph#skimage.graph.MCP_Connect.create_connection", "type": "graph", "text": " \ncreate_connection()  \ncreate_connection id1, id2, pos1, pos2, cost1, cost2) Overload this method to keep track of the connections that are found during MCP processing. Note that a connection with the same ids can be found multiple times (but with different positions and costs). At the time that this method is called, both points are \u201cfrozen\u201d and will not be visited again by the MCP algorithm.  Parameters \n \nid1int \n\nThe seed point id where the first neighbor originated from.  \nid2int \n\nThe seed point id where the second neighbor originated from.  \npos1tuple \n\nThe index of of the first neighbour in the connection.  \npos2tuple \n\nThe index of of the second neighbour in the connection.  \ncost1float \n\nThe cumulative cost at pos1.  \ncost2float \n\nThe cumulative costs at pos2.     \n"}, {"name": "graph.MCP_Connect.__init__()", "path": "api/skimage.graph#skimage.graph.MCP_Connect.__init__", "type": "graph", "text": " \n__init__(*args, **kwargs)  \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "graph.MCP_Flexible", "path": "api/skimage.graph#skimage.graph.MCP_Flexible", "type": "graph", "text": " \nclass skimage.graph.MCP_Flexible(costs, offsets=None, fully_connected=True)  \nBases: skimage.graph._mcp.MCP Find minimum cost paths through an N-d costs array. See the documentation for MCP for full details. This class differs from MCP in that several methods can be overloaded (from pure Python) to modify the behavior of the algorithm and/or create custom algorithms based on MCP. Note that goal_reached can also be overloaded in the MCP class.  \n__init__(costs, offsets=None, fully_connected=True, sampling=None)  \nSee class documentation. \n  \nexamine_neighbor(index, new_index, offset_length)  \nThis method is called once for every pair of neighboring nodes, as soon as both nodes are frozen. This method can be overloaded to obtain information about neightboring nodes, and/or to modify the behavior of the MCP algorithm. One example is the MCP_Connect class, which checks for meeting fronts using this hook. \n  \ntravel_cost(old_cost, new_cost, offset_length)  \nThis method calculates the travel cost for going from the current node to the next. The default implementation returns new_cost. Overload this method to adapt the behaviour of the algorithm. \n  \nupdate_node(index, new_index, offset_length)  \nThis method is called when a node is updated, right after new_index is pushed onto the heap and the traceback map is updated. This method can be overloaded to keep track of other arrays that are used by a specific implementation of the algorithm. For instance the MCP_Connect class uses it to update an id map. \n \n"}, {"name": "graph.MCP_Flexible.examine_neighbor()", "path": "api/skimage.graph#skimage.graph.MCP_Flexible.examine_neighbor", "type": "graph", "text": " \nexamine_neighbor(index, new_index, offset_length)  \nThis method is called once for every pair of neighboring nodes, as soon as both nodes are frozen. This method can be overloaded to obtain information about neightboring nodes, and/or to modify the behavior of the MCP algorithm. One example is the MCP_Connect class, which checks for meeting fronts using this hook. \n"}, {"name": "graph.MCP_Flexible.travel_cost()", "path": "api/skimage.graph#skimage.graph.MCP_Flexible.travel_cost", "type": "graph", "text": " \ntravel_cost(old_cost, new_cost, offset_length)  \nThis method calculates the travel cost for going from the current node to the next. The default implementation returns new_cost. Overload this method to adapt the behaviour of the algorithm. \n"}, {"name": "graph.MCP_Flexible.update_node()", "path": "api/skimage.graph#skimage.graph.MCP_Flexible.update_node", "type": "graph", "text": " \nupdate_node(index, new_index, offset_length)  \nThis method is called when a node is updated, right after new_index is pushed onto the heap and the traceback map is updated. This method can be overloaded to keep track of other arrays that are used by a specific implementation of the algorithm. For instance the MCP_Connect class uses it to update an id map. \n"}, {"name": "graph.MCP_Flexible.__init__()", "path": "api/skimage.graph#skimage.graph.MCP_Flexible.__init__", "type": "graph", "text": " \n__init__(costs, offsets=None, fully_connected=True, sampling=None)  \nSee class documentation. \n"}, {"name": "graph.MCP_Geometric", "path": "api/skimage.graph#skimage.graph.MCP_Geometric", "type": "graph", "text": " \nclass skimage.graph.MCP_Geometric(costs, offsets=None, fully_connected=True)  \nBases: skimage.graph._mcp.MCP Find distance-weighted minimum cost paths through an n-d costs array. See the documentation for MCP for full details. This class differs from MCP in that the cost of a path is not simply the sum of the costs along that path. This class instead assumes that the costs array contains at each position the \u201ccost\u201d of a unit distance of travel through that position. For example, a move (in 2-d) from (1, 1) to (1, 2) is assumed to originate in the center of the pixel (1, 1) and terminate in the center of (1, 2). The entire move is of distance 1, half through (1, 1) and half through (1, 2); thus the cost of that move is (1/2)*costs[1,1] + (1/2)*costs[1,2]. On the other hand, a move from (1, 1) to (2, 2) is along the diagonal and is sqrt(2) in length. Half of this move is within the pixel (1, 1) and the other half in (2, 2), so the cost of this move is calculated as (sqrt(2)/2)*costs[1,1] + (sqrt(2)/2)*costs[2,2]. These calculations don\u2019t make a lot of sense with offsets of magnitude greater than 1. Use the sampling argument in order to deal with anisotropic data.  \n__init__(costs, offsets=None, fully_connected=True, sampling=None)  \nSee class documentation. \n \n"}, {"name": "graph.MCP_Geometric.__init__()", "path": "api/skimage.graph#skimage.graph.MCP_Geometric.__init__", "type": "graph", "text": " \n__init__(costs, offsets=None, fully_connected=True, sampling=None)  \nSee class documentation. \n"}, {"name": "graph.route_through_array()", "path": "api/skimage.graph#skimage.graph.route_through_array", "type": "graph", "text": " \nskimage.graph.route_through_array(array, start, end, fully_connected=True, geometric=True) [source]\n \nSimple example of how to use the MCP and MCP_Geometric classes. See the MCP and MCP_Geometric class documentation for explanation of the path-finding algorithm.  Parameters \n \narrayndarray \n\nArray of costs.  \nstartiterable \n\nn-d index into array defining the starting point  \nenditerable \n\nn-d index into array defining the end point  \nfully_connectedbool (optional) \n\nIf True, diagonal moves are permitted, if False, only axial moves.  \ngeometricbool (optional) \n\nIf True, the MCP_Geometric class is used to calculate costs, if False, the MCP base class is used. See the class documentation for an explanation of the differences between MCP and MCP_Geometric.    Returns \n \npathlist \n\nList of n-d index tuples defining the path from start to end.  \ncostfloat \n\nCost of the path. If geometric is False, the cost of the path is the sum of the values of array along the path. If geometric is True, a finer computation is made (see the documentation of the MCP_Geometric class).      See also  \nMCP, MCP_Geometric\n\n  Examples >>> import numpy as np\n>>> from skimage.graph import route_through_array\n>>>\n>>> image = np.array([[1, 3], [10, 12]])\n>>> image\narray([[ 1,  3],\n       [10, 12]])\n>>> # Forbid diagonal steps\n>>> route_through_array(image, [0, 0], [1, 1], fully_connected=False)\n([(0, 0), (0, 1), (1, 1)], 9.5)\n>>> # Now allow diagonal steps: the path goes directly from start to end\n>>> route_through_array(image, [0, 0], [1, 1])\n([(0, 0), (1, 1)], 9.19238815542512)\n>>> # Cost is the sum of array values along the path (16 = 1 + 3 + 12)\n>>> route_through_array(image, [0, 0], [1, 1], fully_connected=False,\n... geometric=False)\n([(0, 0), (0, 1), (1, 1)], 16.0)\n>>> # Larger array where we display the path that is selected\n>>> image = np.arange((36)).reshape((6, 6))\n>>> image\narray([[ 0,  1,  2,  3,  4,  5],\n       [ 6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17],\n       [18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35]])\n>>> # Find the path with lowest cost\n>>> indices, weight = route_through_array(image, (0, 0), (5, 5))\n>>> indices = np.stack(indices, axis=-1)\n>>> path = np.zeros_like(image)\n>>> path[indices[0], indices[1]] = 1\n>>> path\narray([[1, 1, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1]])\n \n"}, {"name": "graph.shortest_path()", "path": "api/skimage.graph#skimage.graph.shortest_path", "type": "graph", "text": " \nskimage.graph.shortest_path(arr, reach=1, axis=-1, output_indexlist=False) [source]\n \nFind the shortest path through an n-d array from one side to another.  Parameters \n \narrndarray of float64 \n\nreachint, optional \n\nBy default (reach = 1), the shortest path can only move one row up or down for every step it moves forward (i.e., the path gradient is limited to 1). reach defines the number of elements that can be skipped along each non-axis dimension at each step.  \naxisint, optional \n\nThe axis along which the path must always move forward (default -1)  \noutput_indexlistbool, optional \n\nSee return value p for explanation.    Returns \n \npiterable of int \n\nFor each step along axis, the coordinate of the shortest path. If output_indexlist is True, then the path is returned as a list of n-d tuples that index into arr. If False, then the path is returned as an array listing the coordinates of the path along the non-axis dimensions for each step along the axis dimension. That is, p.shape == (arr.shape[axis], arr.ndim-1) except that p is squeezed before returning so if arr.ndim == 2, then p.shape == (arr.shape[axis],)  \ncostfloat \n\nCost of path. This is the absolute sum of all the differences along the path.     \n"}, {"name": "Handling Video Files", "path": "user_guide/video", "type": "Guide", "text": "Handling Video Files Sometimes it is necessary to read a sequence of images from a standard video file, such as .avi and .mov files. In a scientific context, it is usually better to avoid these formats in favor of a simple directory of images or a multi-dimensional TIF. Video formats are more difficult to read piecemeal, typically do not support random frame access or research-minded meta data, and use lossy compression if not carefully configured. But video files are in widespread use, and they are easy to share, so it is convenient to be equipped to read and write them when necessary. Tools for reading video files vary in their ease of installation and use, their disk and memory usage, and their cross-platform compatibility. This is a practical guide. A Workaround: Convert the Video to an Image Sequence For a one-off solution, the simplest, surest route is to convert the video to a collection of sequentially-numbered image files, often called an image sequence. Then the images files can be read into an ImageCollection by skimage.io.imread_collection. Converting the video to frames can be done easily in ImageJ, a cross-platform, GUI-based program from the bio-imaging community, or FFmpeg, a powerful command-line utility for manipulating video files. In FFmpeg, the following command generates an image file from each frame in a video. The files are numbered with five digits, padded on the left with zeros. ffmpeg -i \"video.mov\" -f image2 \"video-frame%05d.png\"\n More information is available in an FFmpeg tutorial on image sequences. Generating an image sequence has disadvantages: they can be large and unwieldy, and generating them can take some time. It is generally preferable to work directly with the original video file. For a more direct solution, we need to execute FFmpeg or LibAV from Python to read frames from the video. FFmpeg and LibAV are two large open-source projects that decode video from the sprawling variety of formats used in the wild. There are several ways to use them from Python. Each, unfortunately, has some disadvantages. PyAV PyAV uses FFmpeg\u2019s (or LibAV\u2019s) libraries to read image data directly from the video file. It invokes them using Cython bindings, so it is very fast. import av\nv = av.open('path/to/video.mov')\n PyAV\u2019s API reflects the way frames are stored in a video file. for packet in container.demux():\n    for frame in packet.decode():\n        if frame.type == 'video':\n            img = frame.to_image()  # PIL/Pillow image\n            arr = np.asarray(img)  # numpy array\n            # Do something!\n Adding Random Access to PyAV The Video class in PIMS invokes PyAV and adds additional functionality to solve a common problem in scientific applications, accessing a video by frame number. Video file formats are designed to be searched in an approximate way, by time, and they do not support an efficient means of seeking a specific frame number. PIMS adds this missing functionality by decoding (but not reading) the entire video at and producing an internal table of contents that supports indexing by frame. import pims\nv = pims.Video('path/to/video.mov')\nv[-1]  # a 2D numpy array representing the last frame\n MoviePy Moviepy invokes FFmpeg through a subprocess, pipes the decoded video from FFmpeg into RAM, and reads it out. This approach is straightforward, but it can be brittle, and it\u2019s not workable for large videos that exceed available RAM. It works on all platforms if FFmpeg is installed. Since it does not link to FFmpeg\u2019s underlying libraries, it is easier to install but about half as fast. from moviepy.editor import VideoFileClip\nmyclip = VideoFileClip(\"some_video.avi\")\n Imageio Imageio takes the same approach as MoviePy. It supports a wide range of other image file formats as well. import imageio\nfilename = '/tmp/file.mp4'\nvid = imageio.get_reader(filename,  'ffmpeg')\n\nfor num, image in vid.iter_data():\n    print(image.mean())\n\nmetadata = vid.get_meta_data()\n OpenCV Finally, another solution is the VideoReader class in OpenCV, which has bindings to FFmpeg. If you need OpenCV for other reasons, then this may be the best approach.\n"}, {"name": "How to parallelize loops", "path": "user_guide/tutorial_parallelization", "type": "Guide", "text": "How to parallelize loops In image processing, we frequently apply the same algorithm on a large batch of images. In this paragraph, we propose to use joblib to parallelize loops. Here is an example of such repetitive tasks: from skimage import data, color, util\nfrom skimage.restoration import denoise_tv_chambolle\nfrom skimage.feature import hog\n\ndef task(image):\n    \"\"\"\n    Apply some functions and return an image.\n    \"\"\"\n    image = denoise_tv_chambolle(image[0][0], weight=0.1, multichannel=True)\n    fd, hog_image = hog(color.rgb2gray(image), orientations=8,\n                        pixels_per_cell=(16, 16), cells_per_block=(1, 1),\n                        visualize=True)\n    return hog_image\n\n\n# Prepare images\nhubble = data.hubble_deep_field()\nwidth = 10\npics = util.view_as_windows(hubble, (width, hubble.shape[1], hubble.shape[2]), step=width)\n To call the function task on each element of the list pics, it is usual to write a for loop. To measure the execution time of this loop, you can use ipython and measure the execution time with %timeit. def classic_loop():\n    for image in pics:\n        task(image)\n\n\n%timeit classic_loop()\n Another equivalent way to code this loop is to use a comprehension list which has the same efficiency. def comprehension_loop():\n    [task(image) for image in pics]\n\n%timeit comprehension_loop()\n joblib is a library providing an easy way to parallelize for loops once we have a comprehension list. The number of jobs can be specified. from joblib import Parallel, delayed\ndef joblib_loop():\n    Parallel(n_jobs=4)(delayed(task)(i) for i in pics)\n\n%timeit joblib_loop()\n\n"}, {"name": "I/O Plugin Infrastructure", "path": "user_guide/plugins", "type": "Guide", "text": "I/O Plugin Infrastructure A plugin consists of two files, the source and the descriptor .ini. Let\u2019s say we\u2019d like to provide a plugin for imshow using matplotlib. We\u2019ll call our plugin mpl: skimage/io/_plugins/mpl.py\nskimage/io/_plugins/mpl.ini\n The name of the .py and .ini files must correspond. Inside the .ini file, we give the plugin meta-data: [mpl] <-- name of the plugin, may be anything\ndescription = Matplotlib image I/O plugin\nprovides = imshow <-- a comma-separated list, one or more of\n                      imshow, imsave, imread, _app_show\n The \u201cprovides\u201d-line lists all the functions provided by the plugin. Since our plugin provides imshow, we have to define it inside mpl.py: # This is mpl.py\n\nimport matplotlib.pyplot as plt\n\ndef imshow(img):\n    plt.imshow(img)\n Note that, by default, imshow is non-blocking, so a special function _app_show must be provided to block the GUI. We can modify our plugin to provide it as follows: [mpl]\nprovides = imshow, _app_show\n # This is mpl.py\n\nimport matplotlib.pyplot as plt\n\ndef imshow(img):\n    plt.imshow(img)\n\ndef _app_show():\n    plt.show()\n Any plugin in the _plugins directory is automatically examined by skimage.io upon import. You may list all the plugins on your system: >>> import skimage.io as io\n>>> io.find_available_plugins()\n{'gtk': ['imshow'],\n 'matplotlib': ['imshow', 'imread', 'imread_collection'],\n 'pil': ['imread', 'imsave', 'imread_collection'],\n 'qt': ['imshow', 'imsave', 'imread', 'imread_collection'],\n 'test': ['imsave', 'imshow', 'imread', 'imread_collection'],}\n or only those already loaded: >>> io.find_available_plugins(loaded=True)\n{'matplotlib': ['imshow', 'imread', 'imread_collection'],\n 'pil': ['imread', 'imsave', 'imread_collection']}\n A plugin is loaded using the use_plugin command: >>> import skimage.io as io\n>>> io.use_plugin('pil') # Use all capabilities provided by PIL\n or >>> io.use_plugin('pil', 'imread') # Use only the imread capability of PIL\n Note that, if more than one plugin provides certain functionality, the last plugin loaded is used. To query a plugin\u2019s capabilities, use plugin_info: >>> io.plugin_info('pil')\n>>>\n{'description': 'Image reading via the Python Imaging Library',\n 'provides': 'imread, imsave'}\n\n"}, {"name": "Image adjustment: transforming image content", "path": "user_guide/transforming_image_data", "type": "Guide", "text": "Image adjustment: transforming image content Color manipulation Most functions for manipulating color channels are found in the submodule skimage.color. Conversion between color models Color images can be represented using different color spaces. One of the most common color spaces is the RGB space, where an image has red, green and blue channels. However, other color models are widely used, such as the HSV color model, where hue, saturation and value are independent channels, or the CMYK model used for printing. skimage.color provides utility functions to convert images to and from different color spaces. Integer-type arrays can be transformed to floating-point type by the conversion operation: >>> # bright saturated red\n>>> red_pixel_rgb = np.array([[[255, 0, 0]]], dtype=np.uint8)\n>>> color.rgb2hsv(red_pixel_rgb)\narray([[[ 0.,  1.,  1.]]])\n>>> #\u00a0darker saturated blue\n>>> dark_blue_pixel_rgb = np.array([[[0, 0, 100]]], dtype=np.uint8)\n>>> color.rgb2hsv(dark_blue_pixel_rgb)\narray([[[ 0.66666667,  1.        ,  0.39215686]]])\n>>> # less saturated pink\n>>> pink_pixel_rgb = np.array([[[255, 100, 255]]], dtype=np.uint8)\n>>> color.rgb2hsv(pink_pixel_rgb)\narray([[[ 0.83333333,  0.60784314,  1.        ]]])\n Conversion from RGBA to RGB - Removing alpha channel through alpha blending Converting an RGBA image to an RGB image by alpha blending it with a background is realized with rgba2rgb() >>> from skimage.color import rgba2rgb\n>>> from skimage import data\n>>> img_rgba = data.logo()\n>>> img_rgb = rgba2rgb(img_rgba)\n Conversion between color and gray values Converting an RGB image to a grayscale image is realized with rgb2gray() >>> from skimage.color import rgb2gray\n>>> from skimage import data\n>>> img = data.astronaut()\n>>> img_gray = rgb2gray(img)\n rgb2gray() uses a non-uniform weighting of color channels, because of the different sensitivity of the human eye to different colors. Therefore, such a weighting ensures luminance preservation from RGB to grayscale: >>> red_pixel = np.array([[[255, 0, 0]]], dtype=np.uint8)\n>>> color.rgb2gray(red_pixel)\narray([[ 0.2125]])\n>>> green_pixel = np.array([[[0, 255, 0]]], dtype=np.uint8)\n>>> color.rgb2gray(green_pixel)\narray([[ 0.7154]])\n Converting a grayscale image to RGB with gray2rgb() simply duplicates the gray values over the three color channels. Image inversion An inverted image is also called complementary image. For binary images, True values become False and conversely. For grayscale images, pixel values are replaced by the difference of the maximum value of the data type and the actual value. For RGB images, the same operation is done for each channel. This operation can be achieved with skimage.util.invert(): >>> from skimage import util\n>>> img = data.camera()\n>>> inverted_img = util.invert(img)\n Painting images with labels label2rgb() can be used to superimpose colors on a grayscale image using an array of labels to encode the regions to be represented with the same color.   Examples:  Tinting gray-scale images Find the intersection of two segmentations RAG Thresholding   Contrast and exposure Image pixels can take values determined by the dtype of the image (see Image data types and what they mean), such as 0 to 255 for uint8 images or [0,\n1] for floating-point images. However, most images either have a narrower range of values (because of poor contrast), or have most pixel values concentrated in a subrange of the accessible values. skimage.exposure provides functions that spread the intensity values over a larger range. A first class of methods compute a nonlinear function of the intensity, that is independent of the pixel values of a specific image. Such methods are often used for correcting a known non-linearity of sensors, or receptors such as the human eye. A well-known example is Gamma correction, implemented in adjust_gamma(). Other methods re-distribute pixel values according to the histogram of the image. The histogram of pixel values is computed with skimage.exposure.histogram(): >>> image = np.array([[1, 3], [1, 1]])\n>>> exposure.histogram(image)\n(array([3, 0, 1]), array([1, 2, 3]))\n histogram() returns the number of pixels for each value bin, and the centers of the bins. The behavior of histogram() is therefore slightly different from the one of numpy.histogram(), which returns the boundaries of the bins. The simplest contrast enhancement rescale_intensity() consists in stretching pixel values to the whole allowed range, using a linear transformation: >>> from skimage import exposure\n>>> text = data.text()\n>>> text.min(), text.max()\n(10, 197)\n>>> better_contrast = exposure.rescale_intensity(text)\n>>> better_contrast.min(), better_contrast.max()\n(0, 255)\n Even if an image uses the whole value range, sometimes there is very little weight at the ends of the value range. In such a case, clipping pixel values using percentiles of the image improves the contrast (at the expense of some loss of information, because some pixels are saturated by this operation): >>> moon = data.moon()\n>>> v_min, v_max = np.percentile(moon, (0.2, 99.8))\n>>> v_min, v_max\n(10.0, 186.0)\n>>> better_contrast = exposure.rescale_intensity(\n...                                     moon, in_range=(v_min, v_max))\n The function equalize_hist() maps the cumulative distribution function (cdf) of pixel values onto a linear cdf, ensuring that all parts of the value range are equally represented in the image. As a result, details are enhanced in large regions with poor contrast. As a further refinement, histogram equalization can be performed in subregions of the image with equalize_adapthist(), in order to correct for exposure gradients across the image. See the example Histogram Equalization.   Examples:  Histogram Equalization  \n"}, {"name": "Image data types and what they mean", "path": "user_guide/data_types", "type": "Guide", "text": "Image data types and what they mean In skimage, images are simply numpy arrays, which support a variety of data types 1, i.e. \u201cdtypes\u201d. To avoid distorting image intensities (see Rescaling intensity values), we assume that images use the following dtype ranges:   \nData type Range   \nuint8 0 to 255  \nuint16 0 to 65535  \nuint32 0 to 232 - 1  \nfloat -1 to 1 or 0 to 1  \nint8 -128 to 127  \nint16 -32768 to 32767  \nint32 -231 to 231 - 1   Note that float images should be restricted to the range -1 to 1 even though the data type itself can exceed this range; all integer dtypes, on the other hand, have pixel intensities that can span the entire data type range. With a few exceptions, 64-bit (u)int images are not supported. Functions in skimage are designed so that they accept any of these dtypes, but, for efficiency, may return an image of a different dtype (see Output types). If you need a particular dtype, skimage provides utility functions that convert dtypes and properly rescale image intensities (see Input types). You should never use astype on an image, because it violates these assumptions about the dtype range: >>> from skimage.util import img_as_float\n>>> image = np.arange(0, 50, 10, dtype=np.uint8)\n>>> print(image.astype(float)) # These float values are out of range.\n[  0.  10.  20.  30.  40.]\n>>> print(img_as_float(image))\n[ 0.          0.03921569  0.07843137  0.11764706  0.15686275]\n Input types Although we aim to preserve the data range and type of input images, functions may support only a subset of these data-types. In such a case, the input will be converted to the required type (if possible), and a warning message printed to the log if a memory copy is needed. Type requirements should be noted in the docstrings. The following utility functions in the main package are available to developers and users:   \nFunction name Description   \nimg_as_float Convert to 64-bit floating point.  \nimg_as_ubyte Convert to 8-bit uint.  \nimg_as_uint Convert to 16-bit uint.  \nimg_as_int Convert to 16-bit int.   These functions convert images to the desired dtype and properly rescale their values: >>> from skimage.util import img_as_ubyte\n>>> image = np.array([0, 0.5, 1], dtype=float)\n>>> img_as_ubyte(image)\narray([  0, 128, 255], dtype=uint8)\n Be careful! These conversions can result in a loss of precision, since 8 bits cannot hold the same amount of information as 64 bits: >>> image = np.array([0, 0.5, 0.503, 1], dtype=float)\n>>> image_as_ubyte(image)\narray([  0, 128, 128, 255], dtype=uint8)\n Additionally, some functions take a preserve_range argument where a range conversion is convenient but not necessary. For example, interpolation in transform.warp requires an image of type float, which should have a range in [0, 1]. So, by default, input images will be rescaled to this range. However, in some cases, the image values represent physical measurements, such as temperature or rainfall values, that the user does not want rescaled. With preserve_range=True, the original range of the data will be preserved, even though the output is a float image. Users must then ensure this non-standard image is properly processed by downstream functions, which may expect an image in [0, 1]. >>> from skimage import data\n>>> from skimage.transform import rescale\n>>> image = data.coins()\n>>> image.dtype, image.min(), image.max(), image.shape\n(dtype('uint8'), 1, 252, (303, 384))\n>>> rescaled = rescale(image, 0.5)\n>>> (rescaled.dtype, np.round(rescaled.min(), 4),\n...  np.round(rescaled.max(), 4), rescaled.shape)\n(dtype('float64'), 0.0147, 0.9456, (152, 192))\n>>> rescaled = rescale(image, 0.5, preserve_range=True)\n>>> (rescaled.dtype, np.round(rescaled.min()),\n...  np.round(rescaled.max()), rescaled.shape\n(dtype('float64'), 4.0, 241.0, (152, 192))\n Output types The output type of a function is determined by the function author and is documented for the benefit of the user. While this requires the user to explicitly convert the output to whichever format is needed, it ensures that no unnecessary data copies take place. A user that requires a specific type of output (e.g., for display purposes), may write: >>> from skimage.util import img_as_uint\n>>> out = img_as_uint(sobel(image))\n>>> plt.imshow(out)\n Working with OpenCV It is possible that you may need to use an image created using skimage with OpenCV or vice versa. OpenCV image data can be accessed (without copying) in NumPy (and, thus, in scikit-image). OpenCV uses BGR (instead of scikit-image\u2019s RGB) for color images, and its dtype is uint8 by default (See Image data types and what they mean). BGR stands for Blue Green Red. Converting BGR to RGB or vice versa The color images in skimage and OpenCV have 3 dimensions: width, height and color. RGB and BGR use the same color space, except the order of colors is reversed. Note that in scikit-image we usually refer to rows and columns instead of width and height (see Coordinate conventions). The following instruction effectively reverses the order of the colors, leaving the rows and columns unaffected. >>> image = image[:, :, ::-1]\n Using an image from OpenCV with skimage\n If cv_image is an array of unsigned bytes, skimage will understand it by default. If you prefer working with floating point images, img_as_float() can be used to convert the image: >>> from skimage.util import img_as_float\n>>> image = img_as_float(any_opencv_image)\n Using an image from skimage with OpenCV The reverse can be achieved with img_as_ubyte(): >>> from skimage.util import img_as_ubyte\n>>> cv_image = img_as_ubyte(any_skimage_image)\n Image processing pipeline This dtype behavior allows you to string together any skimage function without worrying about the image dtype. On the other hand, if you want to use a custom function that requires a particular dtype, you should call one of the dtype conversion functions (here, func1 and func2 are skimage functions): >>> from skimage.util import img_as_float\n>>> image = img_as_float(func1(func2(image)))\n>>> processed_image = custom_func(image)\n Better yet, you can convert the image internally and use a simplified processing pipeline: >>> def custom_func(image):\n...     image = img_as_float(image)\n...     # do something\n...\n>>> processed_image = custom_func(func1(func2(image)))\n Rescaling intensity values When possible, functions should avoid blindly stretching image intensities (e.g. rescaling a float image so that the min and max intensities are 0 and 1), since this can heavily distort an image. For example, if you\u2019re looking for bright markers in dark images, there may be an image where no markers are present; stretching its input intensity to span the full range would make background noise look like markers. Sometimes, however, you have images that should span the entire intensity range but do not. For example, some cameras store images with 10-, 12-, or 14-bit depth per pixel. If these images are stored in an array with dtype uint16, then the image won\u2019t extend over the full intensity range, and thus, would appear dimmer than it should. To correct for this, you can use the rescale_intensity function to rescale the image so that it uses the full dtype range: >>> from skimage import exposure\n>>> image = exposure.rescale_intensity(img10bit, in_range=(0, 2**10 - 1))\n Here, the in_range argument is set to the maximum range for a 10-bit image. By default, rescale_intensity stretches the values of in_range to match the range of the dtype. rescale_intensity also accepts strings as inputs to in_range and out_range, so the example above could also be written as: >>> image = exposure.rescale_intensity(img10bit, in_range='uint10')\n Note about negative values People very often represent images in signed dtypes, even though they only manipulate the positive values of the image (e.g., using only 0-127 in an int8 image). For this reason, conversion functions only spread the positive values of a signed dtype over the entire range of an unsigned dtype. In other words, negative values are clipped to 0 when converting from signed to unsigned dtypes. (Negative values are preserved when converting between signed dtypes.) To prevent this clipping behavior, you should rescale your image beforehand: >>> image = exposure.rescale_intensity(img_int32, out_range=(0, 2**31 - 1))\n>>> img_uint8 = img_as_ubyte(image)\n This behavior is symmetric: The values in an unsigned dtype are spread over just the positive range of a signed dtype. References  \n1  \nhttps://docs.scipy.org/doc/numpy/user/basics.types.html  \n"}, {"name": "Image Segmentation", "path": "user_guide/tutorial_segmentation", "type": "Guide", "text": "Image Segmentation Image segmentation is the task of labeling the pixels of objects of interest in an image. In this tutorial, we will see how to segment objects from a background. We use the coins image from skimage.data. This image shows several coins outlined against a darker background. The segmentation of the coins cannot be done directly from the histogram of grey values, because the background shares enough grey levels with the coins that a thresholding segmentation is not sufficient.  >>> from skimage import data\n>>> from skimage.exposure import histogram\n>>> coins = data.coins()\n>>> hist, hist_centers = histogram(coins)\n Simply thresholding the image leads either to missing significant parts of the coins, or to merging parts of the background with the coins. This is due to the inhomogeneous lighting of the image.  A first idea is to take advantage of the local contrast, that is, to use the gradients rather than the grey values. Edge-based segmentation Let us first try to detect edges that enclose the coins. For edge detection, we use the Canny detector of skimage.feature.canny >>> from skimage.feature import canny\n>>> edges = canny(coins/255.)\n As the background is very smooth, almost all edges are found at the boundary of the coins, or inside the coins. >>> from scipy import ndimage as ndi\n>>> fill_coins = ndi.binary_fill_holes(edges)\n  Now that we have contours that delineate the outer boundary of the coins, we fill the inner part of the coins using the ndi.binary_fill_holes function, which uses mathematical morphology to fill the holes.  Most coins are well segmented out of the background. Small objects from the background can be easily removed using the ndi.label function to remove objects smaller than a small threshold. >>> label_objects, nb_labels = ndi.label(fill_coins)\n>>> sizes = np.bincount(label_objects.ravel())\n>>> mask_sizes = sizes > 20\n>>> mask_sizes[0] = 0\n>>> coins_cleaned = mask_sizes[label_objects]\n However, the segmentation is not very satisfying, since one of the coins has not been segmented correctly at all. The reason is that the contour that we got from the Canny detector was not completely closed, therefore the filling function did not fill the inner part of the coin.  Therefore, this segmentation method is not very robust: if we miss a single pixel of the contour of the object, we will not be able to fill it. Of course, we could try to dilate the contours in order to close them. However, it is preferable to try a more robust method. Region-based segmentation Let us first determine markers of the coins and the background. These markers are pixels that we can label unambiguously as either object or background. Here, the markers are found at the two extreme parts of the histogram of grey values: >>> markers = np.zeros_like(coins)\n>>> markers[coins < 30] = 1\n>>> markers[coins > 150] = 2\n We will use these markers in a watershed segmentation. The name watershed comes from an analogy with hydrology. The watershed transform floods an image of elevation starting from markers, in order to determine the catchment basins of these markers. Watershed lines separate these catchment basins, and correspond to the desired segmentation. The choice of the elevation map is critical for good segmentation. Here, the amplitude of the gradient provides a good elevation map. We use the Sobel operator for computing the amplitude of the gradient: >>> from skimage.filters import sobel\n>>> elevation_map = sobel(coins)\n From the 3-D surface plot shown below, we see that high barriers effectively separate the coins from the background.  and here is the corresponding 2-D plot:  The next step is to find markers of the background and the coins based on the extreme parts of the histogram of grey values: >>> markers = np.zeros_like(coins)\n>>> markers[coins < 30] = 1\n>>> markers[coins > 150] = 2\n  Let us now compute the watershed transform: >>> from skimage.segmentation import watershed\n>>> segmentation = watershed(elevation_map, markers)\n  With this method, the result is satisfying for all coins. Even if the markers for the background were not well distributed, the barriers in the elevation map were high enough for these markers to flood the entire background. We remove a few small holes with mathematical morphology: >>> segmentation = ndi.binary_fill_holes(segmentation - 1)\n We can now label all the coins one by one using ndi.label: >>> labeled_coins, _ = ndi.label(segmentation)\n \n"}, {"name": "Image Viewer", "path": "user_guide/viewer", "type": "Guide", "text": "Image Viewer  Warning The scikit-image viewer is deprecated since 0.18 and will be removed in 0.20. Please, refer to the visualization software page for alternatives.  Quick Start skimage.viewer provides a matplotlib-based canvas for displaying images and a Qt-based GUI-toolkit, with the goal of making it easy to create interactive image editors. You can simply use it to display an image: from skimage import data\nfrom skimage.viewer import ImageViewer\n\nimage = data.coins()\nviewer = ImageViewer(image)\nviewer.show()\n Of course, you could just as easily use imshow from matplotlib (or alternatively, skimage.io.imshow which adds support for multiple io-plugins) to display images. The advantage of ImageViewer is that you can easily add plugins for manipulating images. Currently, only a few plugins are implemented, but it is easy to write your own. Before going into the details, let\u2019s see an example of how a pre-defined plugin is added to the viewer: from skimage.viewer.plugins.lineprofile import LineProfile\n\nviewer = ImageViewer(image)\nviewer += LineProfile(viewer)\noverlay, data = viewer.show()[0]\n The viewer\u2019s show() method returns a list of tuples, one for each attached plugin. Each tuple contains two elements: an overlay of the same shape as the input image, and a data field (which may be None). A plugin class documents its return value in its output method. In this example, only one plugin is attached, so the list returned by show will have length 1. We extract the single tuple and bind its overlay and data elements to individual variables. Here, overlay contains an image of the line drawn on the viewer, and data contains the 1-dimensional intensity profile along that line. At the moment, there are not many plugins pre-defined, but there is a really simple interface for creating your own plugin. First, let us create a plugin to call the total-variation denoising function, denoise_tv_bregman: from skimage.filters import denoise_tv_bregman\nfrom skimage.viewer.plugins.base import Plugin\n\ndenoise_plugin = Plugin(image_filter=denoise_tv_bregman)\n  Note The Plugin assumes the first argument given to the image filter is the image from the image viewer. In the future, this should be changed so you can pass the image to a different argument of the filter function.  To actually interact with the filter, you have to add widgets that adjust the parameters of the function. Typically, that means adding a slider widget and connecting it to the filter parameter and the minimum and maximum values of the slider: from skimage.viewer.widgets import Slider\nfrom skimage.viewer.widgets.history import SaveButtons\n\ndenoise_plugin += Slider('weight', 0.01, 0.5, update_on='release')\ndenoise_plugin += SaveButtons()\n Here, we connect a slider widget to the filter\u2019s \u2018weight\u2019 argument. We also added some buttons for saving the image to file or to the scikit-image image stack (see skimage.io.push and skimage.io.pop). All that\u2019s left is to create an image viewer and add the plugin to that viewer. viewer = ImageViewer(image)\nviewer += denoise_plugin\ndenoised = viewer.show()[0][0]\n Here, we access only the overlay returned by the plugin, which contains the filtered image for the last used setting of weight.  \n"}, {"name": "img_as_bool()", "path": "api/skimage#skimage.img_as_bool", "type": "skimage", "text": " \nskimage.img_as_bool(image, force_copy=False) [source]\n \nConvert an image to boolean format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of bool (bool_) \n\nOutput image.     Notes The upper half of the input dtype\u2019s positive range is True, and the lower half is False. All negative values (if present) are False. \n"}, {"name": "img_as_float()", "path": "api/skimage#skimage.img_as_float", "type": "skimage", "text": " \nskimage.img_as_float(image, force_copy=False) [source]\n \nConvert an image to floating point format. This function is similar to img_as_float64, but will not convert lower-precision floating point arrays to float64.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of float \n\nOutput image.     Notes The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0]. \n"}, {"name": "img_as_float32()", "path": "api/skimage#skimage.img_as_float32", "type": "skimage", "text": " \nskimage.img_as_float32(image, force_copy=False) [source]\n \nConvert an image to single-precision (32-bit) floating point format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of float32 \n\nOutput image.     Notes The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0]. \n"}, {"name": "img_as_float64()", "path": "api/skimage#skimage.img_as_float64", "type": "skimage", "text": " \nskimage.img_as_float64(image, force_copy=False) [source]\n \nConvert an image to double-precision (64-bit) floating point format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of float64 \n\nOutput image.     Notes The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0]. \n"}, {"name": "img_as_int()", "path": "api/skimage#skimage.img_as_int", "type": "skimage", "text": " \nskimage.img_as_int(image, force_copy=False) [source]\n \nConvert an image to 16-bit signed integer format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of int16 \n\nOutput image.     Notes The values are scaled between -32768 and 32767. If the input data-type is positive-only (e.g., uint8), then the output image will still only have positive values. \n"}, {"name": "img_as_ubyte()", "path": "api/skimage#skimage.img_as_ubyte", "type": "skimage", "text": " \nskimage.img_as_ubyte(image, force_copy=False) [source]\n \nConvert an image to 8-bit unsigned integer format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of ubyte (uint8) \n\nOutput image.     Notes Negative input values will be clipped. Positive values are scaled between 0 and 255. \n"}, {"name": "img_as_uint()", "path": "api/skimage#skimage.img_as_uint", "type": "skimage", "text": " \nskimage.img_as_uint(image, force_copy=False) [source]\n \nConvert an image to 16-bit unsigned integer format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of uint16 \n\nOutput image.     Notes Negative input values will be clipped. Positive values are scaled between 0 and 65535. \n"}, {"name": "io", "path": "api/skimage.io", "type": "io", "text": "Module: io Utilities to read and write images in various formats. The following plug-ins are available:  \nPlugin Description  \nqt Fast image display using the Qt library. Deprecated since 0.18. Will be removed in 0.20.  \nimread Image reading and writing via imread  \ngdal Image reading via the GDAL Library (www.gdal.org)  \nsimpleitk Image reading and writing via SimpleITK  \ngtk Fast image display using the GTK library  \npil Image reading via the Python Imaging Library  \nfits FITS image reading via PyFITS  \nmatplotlib Display or save images using Matplotlib  \ntifffile Load and save TIFF and TIFF-based images using tifffile.py  \nimageio Image reading via the ImageIO Library    \nskimage.io.call_plugin(kind, *args, **kwargs) Find the appropriate plugin of \u2018kind\u2019 and execute it.  \nskimage.io.concatenate_images(ic) Concatenate all images in the image collection into an array.  \nskimage.io.find_available_plugins([loaded]) List available plugins.  \nskimage.io.imread(fname[, as_gray, plugin]) Load an image from file.  \nskimage.io.imread_collection(load_pattern[, \u2026]) Load a collection of images.  \nskimage.io.imread_collection_wrapper(imread)   \nskimage.io.imsave(fname, arr[, plugin, \u2026]) Save an image to file.  \nskimage.io.imshow(arr[, plugin]) Display an image.  \nskimage.io.imshow_collection(ic[, plugin]) Display a collection of images.  \nskimage.io.load_sift(f) Read SIFT or SURF features from externally generated file.  \nskimage.io.load_surf(f) Read SIFT or SURF features from externally generated file.  \nskimage.io.plugin_info(plugin) Return plugin meta-data.  \nskimage.io.plugin_order() Return the currently preferred plugin order.  \nskimage.io.pop() Pop an image from the shared image stack.  \nskimage.io.push(img) Push an image onto the shared image stack.  \nskimage.io.reset_plugins()   \nskimage.io.show() Display pending images.  \nskimage.io.use_plugin(name[, kind]) Set the default plugin for a specified operation.  \nskimage.io.ImageCollection(load_pattern[, \u2026]) Load and manage a collection of image files.  \nskimage.io.MultiImage(filename[, \u2026]) A class containing all frames from multi-frame images.  \nskimage.io.collection Data structures to hold collections of images, with optional caching.  \nskimage.io.manage_plugins Handle image reading, writing and plotting plugins.  \nskimage.io.sift   \nskimage.io.util    call_plugin  \nskimage.io.call_plugin(kind, *args, **kwargs) [source]\n \nFind the appropriate plugin of \u2018kind\u2019 and execute it.  Parameters \n \nkind{\u2018imshow\u2019, \u2018imsave\u2019, \u2018imread\u2019, \u2018imread_collection\u2019} \n\nFunction to look up.  \npluginstr, optional \n\nPlugin to load. Defaults to None, in which case the first matching plugin is used.  \n*args, **kwargsarguments and keyword arguments \n\nPassed to the plugin function.     \n concatenate_images  \nskimage.io.concatenate_images(ic) [source]\n \nConcatenate all images in the image collection into an array.  Parameters \n \nican iterable of images \n\nThe images to be concatenated.    Returns \n \narray_catndarray \n\nAn array having one more dimension than the images in ic.    Raises \n ValueError\n\nIf images in ic don\u2019t have identical shapes.      See also  \nImageCollection.concatenate, MultiImage.concatenate \n  Notes concatenate_images receives any iterable object containing images, including ImageCollection and MultiImage, and returns a NumPy array. \n find_available_plugins  \nskimage.io.find_available_plugins(loaded=False) [source]\n \nList available plugins.  Parameters \n \nloadedbool \n\nIf True, show only those plugins currently loaded. By default, all plugins are shown.    Returns \n \npdict \n\nDictionary with plugin names as keys and exposed functions as values.     \n imread  \nskimage.io.imread(fname, as_gray=False, plugin=None, **plugin_args) [source]\n \nLoad an image from file.  Parameters \n \nfnamestring \n\nImage file name, e.g. test.jpg or URL.  \nas_graybool, optional \n\nIf True, convert color images to gray-scale (64-bit floats). Images that are already in gray-scale format are not converted.  \npluginstr, optional \n\nName of plugin to use. By default, the different plugins are tried (starting with imageio) until a suitable candidate is found. If not given and fname is a tiff file, the tifffile plugin will be used.    Returns \n \nimg_arrayndarray \n\nThe different color bands/channels are stored in the third dimension, such that a gray-image is MxN, an RGB-image MxNx3 and an RGBA-image MxNx4.    Other Parameters \n \nplugin_argskeywords \n\nPassed to the given plugin.     \n imread_collection  \nskimage.io.imread_collection(load_pattern, conserve_memory=True, plugin=None, **plugin_args) [source]\n \nLoad a collection of images.  Parameters \n \nload_patternstr or list \n\nList of objects to load. These are usually filenames, but may vary depending on the currently active plugin. See the docstring for ImageCollection for the default behaviour of this parameter.  \nconserve_memorybool, optional \n\nIf True, never keep more than one in memory at a specific time. Otherwise, images will be cached once they are loaded.    Returns \n \nicImageCollection \n\nCollection of images.    Other Parameters \n \nplugin_argskeywords \n\nPassed to the given plugin.     \n imread_collection_wrapper  \nskimage.io.imread_collection_wrapper(imread) [source]\n\n imsave  \nskimage.io.imsave(fname, arr, plugin=None, check_contrast=True, **plugin_args) [source]\n \nSave an image to file.  Parameters \n \nfnamestr \n\nTarget filename.  \narrndarray of shape (M,N) or (M,N,3) or (M,N,4) \n\nImage data.  \npluginstr, optional \n\nName of plugin to use. By default, the different plugins are tried (starting with imageio) until a suitable candidate is found. If not given and fname is a tiff file, the tifffile plugin will be used.  \ncheck_contrastbool, optional \n\nCheck for low contrast and print warning (default: True).    Other Parameters \n \nplugin_argskeywords \n\nPassed to the given plugin.     Notes When saving a JPEG, the compression ratio may be controlled using the quality keyword argument which is an integer with values in [1, 100] where 1 is worst quality and smallest file size, and 100 is best quality and largest file size (default 75). This is only available when using the PIL and imageio plugins. \n imshow  \nskimage.io.imshow(arr, plugin=None, **plugin_args) [source]\n \nDisplay an image.  Parameters \n \narrndarray or str \n\nImage data or name of image file.  \npluginstr \n\nName of plugin to use. By default, the different plugins are tried (starting with imageio) until a suitable candidate is found.    Other Parameters \n \nplugin_argskeywords \n\nPassed to the given plugin.     \n Examples using skimage.io.imshow\n \n  Explore 3D images (of cells)   imshow_collection  \nskimage.io.imshow_collection(ic, plugin=None, **plugin_args) [source]\n \nDisplay a collection of images.  Parameters \n \nicImageCollection \n\nCollection to display.  \npluginstr \n\nName of plugin to use. By default, the different plugins are tried until a suitable candidate is found.    Other Parameters \n \nplugin_argskeywords \n\nPassed to the given plugin.     \n load_sift  \nskimage.io.load_sift(f) [source]\n \nRead SIFT or SURF features from externally generated file. This routine reads SIFT or SURF files generated by binary utilities from http://people.cs.ubc.ca/~lowe/keypoints/ and http://www.vision.ee.ethz.ch/~surf/. This routine does not generate SIFT/SURF features from an image. These algorithms are patent encumbered. Please use skimage.feature.CENSURE instead.  Parameters \n \nfilelikestring or open file \n\nInput file generated by the feature detectors from http://people.cs.ubc.ca/~lowe/keypoints/ or http://www.vision.ee.ethz.ch/~surf/ .  \nmode{\u2018SIFT\u2019, \u2018SURF\u2019}, optional \n\nKind of descriptor used to generate filelike.    Returns \n \ndatarecord array with fields \n\n \n row: int\n\nrow position of feature    \n column: int\n\ncolumn position of feature    \n scale: float\n\nfeature scale    \n orientation: float\n\nfeature orientation    \n data: array\n\nfeature values         \n load_surf  \nskimage.io.load_surf(f) [source]\n \nRead SIFT or SURF features from externally generated file. This routine reads SIFT or SURF files generated by binary utilities from http://people.cs.ubc.ca/~lowe/keypoints/ and http://www.vision.ee.ethz.ch/~surf/. This routine does not generate SIFT/SURF features from an image. These algorithms are patent encumbered. Please use skimage.feature.CENSURE instead.  Parameters \n \nfilelikestring or open file \n\nInput file generated by the feature detectors from http://people.cs.ubc.ca/~lowe/keypoints/ or http://www.vision.ee.ethz.ch/~surf/ .  \nmode{\u2018SIFT\u2019, \u2018SURF\u2019}, optional \n\nKind of descriptor used to generate filelike.    Returns \n \ndatarecord array with fields \n\n \n row: int\n\nrow position of feature    \n column: int\n\ncolumn position of feature    \n scale: float\n\nfeature scale    \n orientation: float\n\nfeature orientation    \n data: array\n\nfeature values         \n plugin_info  \nskimage.io.plugin_info(plugin) [source]\n \nReturn plugin meta-data.  Parameters \n \npluginstr \n\nName of plugin.    Returns \n \nmdict \n\nMeta data as specified in plugin .ini.     \n plugin_order  \nskimage.io.plugin_order() [source]\n \nReturn the currently preferred plugin order.  Returns \n \npdict \n\nDictionary of preferred plugin order, with function name as key and plugins (in order of preference) as value.     \n pop  \nskimage.io.pop() [source]\n \nPop an image from the shared image stack.  Returns \n \nimgndarray \n\nImage popped from the stack.     \n push  \nskimage.io.push(img) [source]\n \nPush an image onto the shared image stack.  Parameters \n \nimgndarray \n\nImage to push.     \n reset_plugins  \nskimage.io.reset_plugins() [source]\n\n show  \nskimage.io.show() [source]\n \nDisplay pending images. Launch the event loop of the current gui plugin, and display all pending images, queued via imshow. This is required when using imshow from non-interactive scripts. A call to show will block execution of code until all windows have been closed. Examples >>> import skimage.io as io\n >>> for i in range(4):\n...     ax_im = io.imshow(np.random.rand(50, 50))\n>>> io.show() \n \n use_plugin  \nskimage.io.use_plugin(name, kind=None) [source]\n \nSet the default plugin for a specified operation. The plugin will be loaded if it hasn\u2019t been already.  Parameters \n \nnamestr \n\nName of plugin.  \nkind{\u2018imsave\u2019, \u2018imread\u2019, \u2018imshow\u2019, \u2018imread_collection\u2019, \u2018imshow_collection\u2019}, optional \n\nSet the plugin for this function. By default, the plugin is set for all functions.      See also  \navailable_plugins \n\nList of available plugins    Examples To use Matplotlib as the default image reader, you would write: >>> from skimage import io\n>>> io.use_plugin('matplotlib', 'imread')\n To see a list of available plugins run io.available_plugins. Note that this lists plugins that are defined, but the full list may not be usable if your system does not have the required libraries installed. \n ImageCollection  \nclass skimage.io.ImageCollection(load_pattern, conserve_memory=True, load_func=None, **load_func_kwargs) [source]\n \nBases: object Load and manage a collection of image files.  Parameters \n \nload_patternstr or list of str \n\nPattern string or list of strings to load. The filename path can be absolute or relative.  \nconserve_memorybool, optional \n\nIf True, ImageCollection does not keep more than one in memory at a specific time. Otherwise, images will be cached once they are loaded.    Other Parameters \n \nload_funccallable \n\nimread by default. See notes below.     Notes Note that files are always returned in alphanumerical order. Also note that slicing returns a new ImageCollection, not a view into the data. ImageCollection can be modified to load images from an arbitrary source by specifying a combination of load_pattern and load_func. For an ImageCollection ic, ic[5] uses load_func(load_pattern[5]) to load the image. Imagine, for example, an ImageCollection that loads every third frame from a video file: video_file = 'no_time_for_that_tiny.gif'\n\ndef vidread_step(f, step):\n    vid = imageio.get_reader(f)\n    seq = [v for v in vid.iter_data()]\n    return seq[::step]\n\nic = ImageCollection(video_file, load_func=vidread_step, step=3)\n\nic  # is an ImageCollection object of length 1 because there is 1 file\n\nx = ic[0]  # calls vidread_step(video_file, step=3)\nx[5]  # is the sixth element of a list of length 8 (24 / 3)\n Another use of load_func would be to convert all images to uint8: def imread_convert(f):\n    return imread(f).astype(np.uint8)\n\nic = ImageCollection('/tmp/*.png', load_func=imread_convert)\n Examples >>> import skimage.io as io\n>>> from skimage import data_dir\n >>> coll = io.ImageCollection(data_dir + '/chess*.png')\n>>> len(coll)\n2\n>>> coll[0].shape\n(200, 200)\n >>> ic = io.ImageCollection(['/tmp/work/*.png', '/tmp/other/*.jpg'])\n  Attributes \n \nfileslist of str \n\nIf a pattern string is given for load_pattern, this attribute stores the expanded file list. Otherwise, this is equal to load_pattern.      \n__init__(load_pattern, conserve_memory=True, load_func=None, **load_func_kwargs) [source]\n \nLoad and manage a collection of images. \n  \nconcatenate() [source]\n \nConcatenate all images in the collection into an array.  Returns \n \narnp.ndarray \n\nAn array having one more dimension than the images in self.    Raises \n ValueError\n\nIf images in the ImageCollection don\u2019t have identical shapes.      See also  \nconcatenate_images\n\n  \n  \nproperty conserve_memory \n  \nproperty files \n  \nreload(n=None) [source]\n \nClear the image cache.  Parameters \n \nnNone or int \n\nClear the cache for this image only. By default, the entire cache is erased.     \n \n MultiImage  \nclass skimage.io.MultiImage(filename, conserve_memory=True, dtype=None, **imread_kwargs) [source]\n \nBases: skimage.io.collection.ImageCollection A class containing all frames from multi-frame images.  Parameters \n \nload_patternstr or list of str \n\nPattern glob or filenames to load. The path can be absolute or relative.  \nconserve_memorybool, optional \n\nWhether to conserve memory by only caching a single frame. Default is True.    Other Parameters \n \nload_funccallable \n\nimread by default. See notes below.     Notes If conserve_memory=True the memory footprint can be reduced, however the performance can be affected because frames have to be read from file more often. The last accessed frame is cached, all other frames will have to be read from file. The current implementation makes use of tifffile for Tiff files and PIL otherwise. Examples >>> from skimage import data_dir\n >>> img = MultiImage(data_dir + '/multipage.tif') \n>>> len(img) \n2\n>>> for frame in img: \n...     print(frame.shape) \n(15, 10)\n(15, 10)\n  \n__init__(filename, conserve_memory=True, dtype=None, **imread_kwargs) [source]\n \nLoad a multi-img. \n  \nproperty filename \n \n\n"}, {"name": "io.call_plugin()", "path": "api/skimage.io#skimage.io.call_plugin", "type": "io", "text": " \nskimage.io.call_plugin(kind, *args, **kwargs) [source]\n \nFind the appropriate plugin of \u2018kind\u2019 and execute it.  Parameters \n \nkind{\u2018imshow\u2019, \u2018imsave\u2019, \u2018imread\u2019, \u2018imread_collection\u2019} \n\nFunction to look up.  \npluginstr, optional \n\nPlugin to load. Defaults to None, in which case the first matching plugin is used.  \n*args, **kwargsarguments and keyword arguments \n\nPassed to the plugin function.     \n"}, {"name": "io.concatenate_images()", "path": "api/skimage.io#skimage.io.concatenate_images", "type": "io", "text": " \nskimage.io.concatenate_images(ic) [source]\n \nConcatenate all images in the image collection into an array.  Parameters \n \nican iterable of images \n\nThe images to be concatenated.    Returns \n \narray_catndarray \n\nAn array having one more dimension than the images in ic.    Raises \n ValueError\n\nIf images in ic don\u2019t have identical shapes.      See also  \nImageCollection.concatenate, MultiImage.concatenate \n  Notes concatenate_images receives any iterable object containing images, including ImageCollection and MultiImage, and returns a NumPy array. \n"}, {"name": "io.find_available_plugins()", "path": "api/skimage.io#skimage.io.find_available_plugins", "type": "io", "text": " \nskimage.io.find_available_plugins(loaded=False) [source]\n \nList available plugins.  Parameters \n \nloadedbool \n\nIf True, show only those plugins currently loaded. By default, all plugins are shown.    Returns \n \npdict \n\nDictionary with plugin names as keys and exposed functions as values.     \n"}, {"name": "io.ImageCollection", "path": "api/skimage.io#skimage.io.ImageCollection", "type": "io", "text": " \nclass skimage.io.ImageCollection(load_pattern, conserve_memory=True, load_func=None, **load_func_kwargs) [source]\n \nBases: object Load and manage a collection of image files.  Parameters \n \nload_patternstr or list of str \n\nPattern string or list of strings to load. The filename path can be absolute or relative.  \nconserve_memorybool, optional \n\nIf True, ImageCollection does not keep more than one in memory at a specific time. Otherwise, images will be cached once they are loaded.    Other Parameters \n \nload_funccallable \n\nimread by default. See notes below.     Notes Note that files are always returned in alphanumerical order. Also note that slicing returns a new ImageCollection, not a view into the data. ImageCollection can be modified to load images from an arbitrary source by specifying a combination of load_pattern and load_func. For an ImageCollection ic, ic[5] uses load_func(load_pattern[5]) to load the image. Imagine, for example, an ImageCollection that loads every third frame from a video file: video_file = 'no_time_for_that_tiny.gif'\n\ndef vidread_step(f, step):\n    vid = imageio.get_reader(f)\n    seq = [v for v in vid.iter_data()]\n    return seq[::step]\n\nic = ImageCollection(video_file, load_func=vidread_step, step=3)\n\nic  # is an ImageCollection object of length 1 because there is 1 file\n\nx = ic[0]  # calls vidread_step(video_file, step=3)\nx[5]  # is the sixth element of a list of length 8 (24 / 3)\n Another use of load_func would be to convert all images to uint8: def imread_convert(f):\n    return imread(f).astype(np.uint8)\n\nic = ImageCollection('/tmp/*.png', load_func=imread_convert)\n Examples >>> import skimage.io as io\n>>> from skimage import data_dir\n >>> coll = io.ImageCollection(data_dir + '/chess*.png')\n>>> len(coll)\n2\n>>> coll[0].shape\n(200, 200)\n >>> ic = io.ImageCollection(['/tmp/work/*.png', '/tmp/other/*.jpg'])\n  Attributes \n \nfileslist of str \n\nIf a pattern string is given for load_pattern, this attribute stores the expanded file list. Otherwise, this is equal to load_pattern.      \n__init__(load_pattern, conserve_memory=True, load_func=None, **load_func_kwargs) [source]\n \nLoad and manage a collection of images. \n  \nconcatenate() [source]\n \nConcatenate all images in the collection into an array.  Returns \n \narnp.ndarray \n\nAn array having one more dimension than the images in self.    Raises \n ValueError\n\nIf images in the ImageCollection don\u2019t have identical shapes.      See also  \nconcatenate_images\n\n  \n  \nproperty conserve_memory \n  \nproperty files \n  \nreload(n=None) [source]\n \nClear the image cache.  Parameters \n \nnNone or int \n\nClear the cache for this image only. By default, the entire cache is erased.     \n \n"}, {"name": "io.ImageCollection.concatenate()", "path": "api/skimage.io#skimage.io.ImageCollection.concatenate", "type": "io", "text": " \nconcatenate() [source]\n \nConcatenate all images in the collection into an array.  Returns \n \narnp.ndarray \n\nAn array having one more dimension than the images in self.    Raises \n ValueError\n\nIf images in the ImageCollection don\u2019t have identical shapes.      See also  \nconcatenate_images\n\n  \n"}, {"name": "io.ImageCollection.conserve_memory()", "path": "api/skimage.io#skimage.io.ImageCollection.conserve_memory", "type": "io", "text": " \nproperty conserve_memory \n"}, {"name": "io.ImageCollection.files()", "path": "api/skimage.io#skimage.io.ImageCollection.files", "type": "io", "text": " \nproperty files \n"}, {"name": "io.ImageCollection.reload()", "path": "api/skimage.io#skimage.io.ImageCollection.reload", "type": "io", "text": " \nreload(n=None) [source]\n \nClear the image cache.  Parameters \n \nnNone or int \n\nClear the cache for this image only. By default, the entire cache is erased.     \n"}, {"name": "io.ImageCollection.__init__()", "path": "api/skimage.io#skimage.io.ImageCollection.__init__", "type": "io", "text": " \n__init__(load_pattern, conserve_memory=True, load_func=None, **load_func_kwargs) [source]\n \nLoad and manage a collection of images. \n"}, {"name": "io.imread()", "path": "api/skimage.io#skimage.io.imread", "type": "io", "text": " \nskimage.io.imread(fname, as_gray=False, plugin=None, **plugin_args) [source]\n \nLoad an image from file.  Parameters \n \nfnamestring \n\nImage file name, e.g. test.jpg or URL.  \nas_graybool, optional \n\nIf True, convert color images to gray-scale (64-bit floats). Images that are already in gray-scale format are not converted.  \npluginstr, optional \n\nName of plugin to use. By default, the different plugins are tried (starting with imageio) until a suitable candidate is found. If not given and fname is a tiff file, the tifffile plugin will be used.    Returns \n \nimg_arrayndarray \n\nThe different color bands/channels are stored in the third dimension, such that a gray-image is MxN, an RGB-image MxNx3 and an RGBA-image MxNx4.    Other Parameters \n \nplugin_argskeywords \n\nPassed to the given plugin.     \n"}, {"name": "io.imread_collection()", "path": "api/skimage.io#skimage.io.imread_collection", "type": "io", "text": " \nskimage.io.imread_collection(load_pattern, conserve_memory=True, plugin=None, **plugin_args) [source]\n \nLoad a collection of images.  Parameters \n \nload_patternstr or list \n\nList of objects to load. These are usually filenames, but may vary depending on the currently active plugin. See the docstring for ImageCollection for the default behaviour of this parameter.  \nconserve_memorybool, optional \n\nIf True, never keep more than one in memory at a specific time. Otherwise, images will be cached once they are loaded.    Returns \n \nicImageCollection \n\nCollection of images.    Other Parameters \n \nplugin_argskeywords \n\nPassed to the given plugin.     \n"}, {"name": "io.imread_collection_wrapper()", "path": "api/skimage.io#skimage.io.imread_collection_wrapper", "type": "io", "text": " \nskimage.io.imread_collection_wrapper(imread) [source]\n\n"}, {"name": "io.imsave()", "path": "api/skimage.io#skimage.io.imsave", "type": "io", "text": " \nskimage.io.imsave(fname, arr, plugin=None, check_contrast=True, **plugin_args) [source]\n \nSave an image to file.  Parameters \n \nfnamestr \n\nTarget filename.  \narrndarray of shape (M,N) or (M,N,3) or (M,N,4) \n\nImage data.  \npluginstr, optional \n\nName of plugin to use. By default, the different plugins are tried (starting with imageio) until a suitable candidate is found. If not given and fname is a tiff file, the tifffile plugin will be used.  \ncheck_contrastbool, optional \n\nCheck for low contrast and print warning (default: True).    Other Parameters \n \nplugin_argskeywords \n\nPassed to the given plugin.     Notes When saving a JPEG, the compression ratio may be controlled using the quality keyword argument which is an integer with values in [1, 100] where 1 is worst quality and smallest file size, and 100 is best quality and largest file size (default 75). This is only available when using the PIL and imageio plugins. \n"}, {"name": "io.imshow()", "path": "api/skimage.io#skimage.io.imshow", "type": "io", "text": " \nskimage.io.imshow(arr, plugin=None, **plugin_args) [source]\n \nDisplay an image.  Parameters \n \narrndarray or str \n\nImage data or name of image file.  \npluginstr \n\nName of plugin to use. By default, the different plugins are tried (starting with imageio) until a suitable candidate is found.    Other Parameters \n \nplugin_argskeywords \n\nPassed to the given plugin.     \n"}, {"name": "io.imshow_collection()", "path": "api/skimage.io#skimage.io.imshow_collection", "type": "io", "text": " \nskimage.io.imshow_collection(ic, plugin=None, **plugin_args) [source]\n \nDisplay a collection of images.  Parameters \n \nicImageCollection \n\nCollection to display.  \npluginstr \n\nName of plugin to use. By default, the different plugins are tried until a suitable candidate is found.    Other Parameters \n \nplugin_argskeywords \n\nPassed to the given plugin.     \n"}, {"name": "io.load_sift()", "path": "api/skimage.io#skimage.io.load_sift", "type": "io", "text": " \nskimage.io.load_sift(f) [source]\n \nRead SIFT or SURF features from externally generated file. This routine reads SIFT or SURF files generated by binary utilities from http://people.cs.ubc.ca/~lowe/keypoints/ and http://www.vision.ee.ethz.ch/~surf/. This routine does not generate SIFT/SURF features from an image. These algorithms are patent encumbered. Please use skimage.feature.CENSURE instead.  Parameters \n \nfilelikestring or open file \n\nInput file generated by the feature detectors from http://people.cs.ubc.ca/~lowe/keypoints/ or http://www.vision.ee.ethz.ch/~surf/ .  \nmode{\u2018SIFT\u2019, \u2018SURF\u2019}, optional \n\nKind of descriptor used to generate filelike.    Returns \n \ndatarecord array with fields \n\n \n row: int\n\nrow position of feature    \n column: int\n\ncolumn position of feature    \n scale: float\n\nfeature scale    \n orientation: float\n\nfeature orientation    \n data: array\n\nfeature values         \n"}, {"name": "io.load_surf()", "path": "api/skimage.io#skimage.io.load_surf", "type": "io", "text": " \nskimage.io.load_surf(f) [source]\n \nRead SIFT or SURF features from externally generated file. This routine reads SIFT or SURF files generated by binary utilities from http://people.cs.ubc.ca/~lowe/keypoints/ and http://www.vision.ee.ethz.ch/~surf/. This routine does not generate SIFT/SURF features from an image. These algorithms are patent encumbered. Please use skimage.feature.CENSURE instead.  Parameters \n \nfilelikestring or open file \n\nInput file generated by the feature detectors from http://people.cs.ubc.ca/~lowe/keypoints/ or http://www.vision.ee.ethz.ch/~surf/ .  \nmode{\u2018SIFT\u2019, \u2018SURF\u2019}, optional \n\nKind of descriptor used to generate filelike.    Returns \n \ndatarecord array with fields \n\n \n row: int\n\nrow position of feature    \n column: int\n\ncolumn position of feature    \n scale: float\n\nfeature scale    \n orientation: float\n\nfeature orientation    \n data: array\n\nfeature values         \n"}, {"name": "io.MultiImage", "path": "api/skimage.io#skimage.io.MultiImage", "type": "io", "text": " \nclass skimage.io.MultiImage(filename, conserve_memory=True, dtype=None, **imread_kwargs) [source]\n \nBases: skimage.io.collection.ImageCollection A class containing all frames from multi-frame images.  Parameters \n \nload_patternstr or list of str \n\nPattern glob or filenames to load. The path can be absolute or relative.  \nconserve_memorybool, optional \n\nWhether to conserve memory by only caching a single frame. Default is True.    Other Parameters \n \nload_funccallable \n\nimread by default. See notes below.     Notes If conserve_memory=True the memory footprint can be reduced, however the performance can be affected because frames have to be read from file more often. The last accessed frame is cached, all other frames will have to be read from file. The current implementation makes use of tifffile for Tiff files and PIL otherwise. Examples >>> from skimage import data_dir\n >>> img = MultiImage(data_dir + '/multipage.tif') \n>>> len(img) \n2\n>>> for frame in img: \n...     print(frame.shape) \n(15, 10)\n(15, 10)\n  \n__init__(filename, conserve_memory=True, dtype=None, **imread_kwargs) [source]\n \nLoad a multi-img. \n  \nproperty filename \n \n"}, {"name": "io.MultiImage.filename()", "path": "api/skimage.io#skimage.io.MultiImage.filename", "type": "io", "text": " \nproperty filename \n"}, {"name": "io.MultiImage.__init__()", "path": "api/skimage.io#skimage.io.MultiImage.__init__", "type": "io", "text": " \n__init__(filename, conserve_memory=True, dtype=None, **imread_kwargs) [source]\n \nLoad a multi-img. \n"}, {"name": "io.plugin_info()", "path": "api/skimage.io#skimage.io.plugin_info", "type": "io", "text": " \nskimage.io.plugin_info(plugin) [source]\n \nReturn plugin meta-data.  Parameters \n \npluginstr \n\nName of plugin.    Returns \n \nmdict \n\nMeta data as specified in plugin .ini.     \n"}, {"name": "io.plugin_order()", "path": "api/skimage.io#skimage.io.plugin_order", "type": "io", "text": " \nskimage.io.plugin_order() [source]\n \nReturn the currently preferred plugin order.  Returns \n \npdict \n\nDictionary of preferred plugin order, with function name as key and plugins (in order of preference) as value.     \n"}, {"name": "io.pop()", "path": "api/skimage.io#skimage.io.pop", "type": "io", "text": " \nskimage.io.pop() [source]\n \nPop an image from the shared image stack.  Returns \n \nimgndarray \n\nImage popped from the stack.     \n"}, {"name": "io.push()", "path": "api/skimage.io#skimage.io.push", "type": "io", "text": " \nskimage.io.push(img) [source]\n \nPush an image onto the shared image stack.  Parameters \n \nimgndarray \n\nImage to push.     \n"}, {"name": "io.reset_plugins()", "path": "api/skimage.io#skimage.io.reset_plugins", "type": "io", "text": " \nskimage.io.reset_plugins() [source]\n\n"}, {"name": "io.show()", "path": "api/skimage.io#skimage.io.show", "type": "io", "text": " \nskimage.io.show() [source]\n \nDisplay pending images. Launch the event loop of the current gui plugin, and display all pending images, queued via imshow. This is required when using imshow from non-interactive scripts. A call to show will block execution of code until all windows have been closed. Examples >>> import skimage.io as io\n >>> for i in range(4):\n...     ax_im = io.imshow(np.random.rand(50, 50))\n>>> io.show() \n \n"}, {"name": "io.use_plugin()", "path": "api/skimage.io#skimage.io.use_plugin", "type": "io", "text": " \nskimage.io.use_plugin(name, kind=None) [source]\n \nSet the default plugin for a specified operation. The plugin will be loaded if it hasn\u2019t been already.  Parameters \n \nnamestr \n\nName of plugin.  \nkind{\u2018imsave\u2019, \u2018imread\u2019, \u2018imshow\u2019, \u2018imread_collection\u2019, \u2018imshow_collection\u2019}, optional \n\nSet the plugin for this function. By default, the plugin is set for all functions.      See also  \navailable_plugins \n\nList of available plugins    Examples To use Matplotlib as the default image reader, you would write: >>> from skimage import io\n>>> io.use_plugin('matplotlib', 'imread')\n To see a list of available plugins run io.available_plugins. Note that this lists plugins that are defined, but the full list may not be usable if your system does not have the required libraries installed. \n"}, {"name": "lookfor()", "path": "api/skimage#skimage.lookfor", "type": "skimage", "text": " \nskimage.lookfor(what) [source]\n \nDo a keyword search on scikit-image docstrings.  Parameters \n \nwhatstr \n\nWords to look for.     Examples >>> import skimage\n>>> skimage.lookfor('regular_grid')\nSearch results for 'regular_grid'\n---------------------------------\nskimage.lookfor\n    Do a keyword search on scikit-image docstrings.\nskimage.util.regular_grid\n    Find `n_points` regularly spaced along `ar_shape`.\n \n"}, {"name": "measure", "path": "api/skimage.measure", "type": "measure", "text": "Module: measure  \nskimage.measure.approximate_polygon(coords, \u2026) Approximate a polygonal chain with the specified tolerance.  \nskimage.measure.block_reduce(image, block_size) Downsample image by applying function func to local blocks.  \nskimage.measure.euler_number(image[, \u2026]) Calculate the Euler characteristic in binary image.  \nskimage.measure.find_contours(image[, \u2026]) Find iso-valued contours in a 2D array for a given level value.  \nskimage.measure.grid_points_in_poly(shape, verts) Test whether points on a specified grid are inside a polygon.  \nskimage.measure.inertia_tensor(image[, mu]) Compute the inertia tensor of the input image.  \nskimage.measure.inertia_tensor_eigvals(image) Compute the eigenvalues of the inertia tensor of the image.  \nskimage.measure.label(input[, background, \u2026]) Label connected regions of an integer array.  \nskimage.measure.marching_cubes(volume[, \u2026]) Marching cubes algorithm to find surfaces in 3d volumetric data.  \nskimage.measure.marching_cubes_classic(volume) Classic marching cubes algorithm to find surfaces in 3d volumetric data.  \nskimage.measure.marching_cubes_lewiner(volume) Lewiner marching cubes algorithm to find surfaces in 3d volumetric data.  \nskimage.measure.mesh_surface_area(verts, faces) Compute surface area, given vertices & triangular faces  \nskimage.measure.moments(image[, order]) Calculate all raw image moments up to a certain order.  \nskimage.measure.moments_central(image[, \u2026]) Calculate all central image moments up to a certain order.  \nskimage.measure.moments_coords(coords[, order]) Calculate all raw image moments up to a certain order.  \nskimage.measure.moments_coords_central(coords) Calculate all central image moments up to a certain order.  \nskimage.measure.moments_hu(nu) Calculate Hu\u2019s set of image moments (2D-only).  \nskimage.measure.moments_normalized(mu[, order]) Calculate all normalized central image moments up to a certain order.  \nskimage.measure.perimeter(image[, neighbourhood]) Calculate total perimeter of all objects in binary image.  \nskimage.measure.perimeter_crofton(image[, \u2026]) Calculate total Crofton perimeter of all objects in binary image.  \nskimage.measure.points_in_poly(points, verts) Test whether points lie inside a polygon.  \nskimage.measure.profile_line(image, src, dst) Return the intensity profile of an image measured along a scan line.  \nskimage.measure.ransac(data, model_class, \u2026) Fit a model to data with the RANSAC (random sample consensus) algorithm.  \nskimage.measure.regionprops(label_image[, \u2026]) Measure properties of labeled image regions.  \nskimage.measure.regionprops_table(label_image) Compute image properties and return them as a pandas-compatible table.  \nskimage.measure.shannon_entropy(image[, base]) Calculate the Shannon entropy of an image.  \nskimage.measure.subdivide_polygon(coords[, \u2026]) Subdivision of polygonal curves using B-Splines.  \nskimage.measure.CircleModel() Total least squares estimator for 2D circles.  \nskimage.measure.EllipseModel() Total least squares estimator for 2D ellipses.  \nskimage.measure.LineModelND() Total least squares estimator for N-dimensional lines.   approximate_polygon  \nskimage.measure.approximate_polygon(coords, tolerance) [source]\n \nApproximate a polygonal chain with the specified tolerance. It is based on the Douglas-Peucker algorithm. Note that the approximated polygon is always within the convex hull of the original polygon.  Parameters \n \ncoords(N, 2) array \n\nCoordinate array.  \ntolerancefloat \n\nMaximum distance from original points of polygon to approximated polygonal chain. If tolerance is 0, the original coordinate array is returned.    Returns \n \ncoords(M, 2) array \n\nApproximated polygonal chain where M <= N.     References  \n1  \nhttps://en.wikipedia.org/wiki/Ramer-Douglas-Peucker_algorithm   \n block_reduce  \nskimage.measure.block_reduce(image, block_size, func=<function sum>, cval=0, func_kwargs=None) [source]\n \nDownsample image by applying function func to local blocks. This function is useful for max and mean pooling, for example.  Parameters \n \nimagendarray \n\nN-dimensional input image.  \nblock_sizearray_like \n\nArray containing down-sampling integer factor along each axis.  \nfunccallable \n\nFunction object which is used to calculate the return value for each local block. This function must implement an axis parameter. Primary functions are numpy.sum, numpy.min, numpy.max, numpy.mean and numpy.median. See also func_kwargs.  \ncvalfloat \n\nConstant padding value if image is not perfectly divisible by the block size.  \nfunc_kwargsdict \n\nKeyword arguments passed to func. Notably useful for passing dtype argument to np.mean. Takes dictionary of inputs, e.g.: func_kwargs={'dtype': np.float16}).    Returns \n \nimagendarray \n\nDown-sampled image with same number of dimensions as input image.     Examples >>> from skimage.measure import block_reduce\n>>> image = np.arange(3*3*4).reshape(3, 3, 4)\n>>> image \narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]],\n       [[24, 25, 26, 27],\n        [28, 29, 30, 31],\n        [32, 33, 34, 35]]])\n>>> block_reduce(image, block_size=(3, 3, 1), func=np.mean)\narray([[[16., 17., 18., 19.]]])\n>>> image_max1 = block_reduce(image, block_size=(1, 3, 4), func=np.max)\n>>> image_max1 \narray([[[11]],\n       [[23]],\n       [[35]]])\n>>> image_max2 = block_reduce(image, block_size=(3, 1, 4), func=np.max)\n>>> image_max2 \narray([[[27],\n        [31],\n        [35]]])\n \n euler_number  \nskimage.measure.euler_number(image, connectivity=None) [source]\n \nCalculate the Euler characteristic in binary image. For 2D objects, the Euler number is the number of objects minus the number of holes. For 3D objects, the Euler number is obtained as the number of objects plus the number of holes, minus the number of tunnels, or loops.  Parameters \n image: (N, M) ndarray or (N, M, D) ndarray.\n\n2D or 3D images. If image is not binary, all values strictly greater than zero are considered as the object.  \nconnectivityint, optional \n\nMaximum number of orthogonal hops to consider a pixel/voxel as a neighbor. Accepted values are ranging from 1 to input.ndim. If None, a full connectivity of input.ndim is used. 4 or 8 neighborhoods are defined for 2D images (connectivity 1 and 2, respectively). 6 or 26 neighborhoods are defined for 3D images, (connectivity 1 and 3, respectively). Connectivity 2 is not defined.    Returns \n \neuler_numberint \n\nEuler characteristic of the set of all objects in the image.     Notes The Euler characteristic is an integer number that describes the topology of the set of all objects in the input image. If object is 4-connected, then background is 8-connected, and conversely. The computation of the Euler characteristic is based on an integral geometry formula in discretized space. In practice, a neighbourhood configuration is constructed, and a LUT is applied for each configuration. The coefficients used are the ones of Ohser et al. It can be useful to compute the Euler characteristic for several connectivities. A large relative difference between results for different connectivities suggests that the image resolution (with respect to the size of objects and holes) is too low. References  \n1  \nS. Rivollier. Analyse d\u2019image geometrique et morphometrique par diagrammes de forme et voisinages adaptatifs generaux. PhD thesis, 2010. Ecole Nationale Superieure des Mines de Saint-Etienne. https://tel.archives-ouvertes.fr/tel-00560838  \n2  \nOhser J., Nagel W., Schladitz K. (2002) The Euler Number of Discretized Sets - On the Choice of Adjacency in Homogeneous Lattices. In: Mecke K., Stoyan D. (eds) Morphology of Condensed Matter. Lecture Notes in Physics, vol 600. Springer, Berlin, Heidelberg.   Examples >>> import numpy as np\n>>> SAMPLE = np.zeros((100,100,100));\n>>> SAMPLE[40:60, 40:60, 40:60]=1\n>>> euler_number(SAMPLE) \n1...\n>>> SAMPLE[45:55,45:55,45:55] = 0;\n>>> euler_number(SAMPLE) \n2...\n>>> SAMPLE = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n...                    [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n...                    [1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0],\n...                    [0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1],\n...                    [0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]])\n>>> euler_number(SAMPLE)  # doctest:\n0\n>>> euler_number(SAMPLE, connectivity=1)  # doctest:\n2\n \n Examples using skimage.measure.euler_number\n \n  Euler number   find_contours  \nskimage.measure.find_contours(image, level=None, fully_connected='low', positive_orientation='low', *, mask=None) [source]\n \nFind iso-valued contours in a 2D array for a given level value. Uses the \u201cmarching squares\u201d method to compute a the iso-valued contours of the input 2D array for a particular level value. Array values are linearly interpolated to provide better precision for the output contours.  Parameters \n \nimage2D ndarray of double \n\nInput image in which to find contours.  \nlevelfloat, optional \n\nValue along which to find contours in the array. By default, the level is set to (max(image) + min(image)) / 2  Changed in version 0.18: This parameter is now optional.   \nfully_connectedstr, {\u2018low\u2019, \u2018high\u2019} \n\nIndicates whether array elements below the given level value are to be considered fully-connected (and hence elements above the value will only be face connected), or vice-versa. (See notes below for details.)  \npositive_orientationstr, {\u2018low\u2019, \u2018high\u2019} \n\nIndicates whether the output contours will produce positively-oriented polygons around islands of low- or high-valued elements. If \u2018low\u2019 then contours will wind counter- clockwise around elements below the iso-value. Alternately, this means that low-valued elements are always on the left of the contour. (See below for details.)  \nmask2D ndarray of bool, or None \n\nA boolean mask, True where we want to draw contours. Note that NaN values are always excluded from the considered region (mask is set to False wherever array is NaN).    Returns \n \ncontourslist of (n,2)-ndarrays \n\nEach contour is an ndarray of shape (n, 2), consisting of n (row, column) coordinates along the contour.      See also  \nskimage.measure.marching_cubes\n\n  Notes The marching squares algorithm is a special case of the marching cubes algorithm [1]. A simple explanation is available here: http://users.polytech.unice.fr/~lingrand/MarchingCubes/algo.html There is a single ambiguous case in the marching squares algorithm: when a given 2 x 2-element square has two high-valued and two low-valued elements, each pair diagonally adjacent. (Where high- and low-valued is with respect to the contour value sought.) In this case, either the high-valued elements can be \u2018connected together\u2019 via a thin isthmus that separates the low-valued elements, or vice-versa. When elements are connected together across a diagonal, they are considered \u2018fully connected\u2019 (also known as \u2018face+vertex-connected\u2019 or \u20188-connected\u2019). Only high-valued or low-valued elements can be fully-connected, the other set will be considered as \u2018face-connected\u2019 or \u20184-connected\u2019. By default, low-valued elements are considered fully-connected; this can be altered with the \u2018fully_connected\u2019 parameter. Output contours are not guaranteed to be closed: contours which intersect the array edge or a masked-off region (either where mask is False or where array is NaN) will be left open. All other contours will be closed. (The closed-ness of a contours can be tested by checking whether the beginning point is the same as the end point.) Contours are oriented. By default, array values lower than the contour value are to the left of the contour and values greater than the contour value are to the right. This means that contours will wind counter-clockwise (i.e. in \u2018positive orientation\u2019) around islands of low-valued pixels. This behavior can be altered with the \u2018positive_orientation\u2019 parameter. The order of the contours in the output list is determined by the position of the smallest x,y (in lexicographical order) coordinate in the contour. This is a side-effect of how the input array is traversed, but can be relied upon.  Warning Array coordinates/values are assumed to refer to the center of the array element. Take a simple example input: [0, 1]. The interpolated position of 0.5 in this array is midway between the 0-element (at x=0) and the 1-element (at x=1), and thus would fall at x=0.5.  This means that to find reasonable contours, it is best to find contours midway between the expected \u201clight\u201d and \u201cdark\u201d values. In particular, given a binarized array, do not choose to find contours at the low or high value of the array. This will often yield degenerate contours, especially around structures that are a single array element wide. Instead choose a middle value, as above. References  \n1  \nLorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings) 21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422   Examples >>> a = np.zeros((3, 3))\n>>> a[0, 0] = 1\n>>> a\narray([[1., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n>>> find_contours(a, 0.5)\n[array([[0. , 0.5],\n       [0.5, 0. ]])]\n \n Examples using skimage.measure.find_contours\n \n  Contour finding  \n\n  Measure region properties   grid_points_in_poly  \nskimage.measure.grid_points_in_poly(shape, verts) [source]\n \nTest whether points on a specified grid are inside a polygon. For each (r, c) coordinate on a grid, i.e. (0, 0), (0, 1) etc., test whether that point lies inside a polygon.  Parameters \n \nshapetuple (M, N) \n\nShape of the grid.  \nverts(V, 2) array \n\nSpecify the V vertices of the polygon, sorted either clockwise or anti-clockwise. The first point may (but does not need to be) duplicated.    Returns \n \nmask(M, N) ndarray of bool \n\nTrue where the grid falls inside the polygon.      See also  \npoints_in_poly\n\n  \n inertia_tensor  \nskimage.measure.inertia_tensor(image, mu=None) [source]\n \nCompute the inertia tensor of the input image.  Parameters \n \nimagearray \n\nThe input image.  \nmuarray, optional \n\nThe pre-computed central moments of image. The inertia tensor computation requires the central moments of the image. If an application requires both the central moments and the inertia tensor (for example, skimage.measure.regionprops), then it is more efficient to pre-compute them and pass them to the inertia tensor call.    Returns \n \nTarray, shape (image.ndim, image.ndim) \n\nThe inertia tensor of the input image. \\(T_{i, j}\\) contains the covariance of image intensity along axes \\(i\\) and \\(j\\).     References  \n1  \nhttps://en.wikipedia.org/wiki/Moment_of_inertia#Inertia_tensor  \n2  \nBernd J\u00e4hne. Spatio-Temporal Image Processing: Theory and Scientific Applications. (Chapter 8: Tensor Methods) Springer, 1993.   \n inertia_tensor_eigvals  \nskimage.measure.inertia_tensor_eigvals(image, mu=None, T=None) [source]\n \nCompute the eigenvalues of the inertia tensor of the image. The inertia tensor measures covariance of the image intensity along the image axes. (See inertia_tensor.) The relative magnitude of the eigenvalues of the tensor is thus a measure of the elongation of a (bright) object in the image.  Parameters \n \nimagearray \n\nThe input image.  \nmuarray, optional \n\nThe pre-computed central moments of image.  \nTarray, shape (image.ndim, image.ndim) \n\nThe pre-computed inertia tensor. If T is given, mu and image are ignored.    Returns \n \neigvalslist of float, length image.ndim \n\nThe eigenvalues of the inertia tensor of image, in descending order.     Notes Computing the eigenvalues requires the inertia tensor of the input image. This is much faster if the central moments (mu) are provided, or, alternatively, one can provide the inertia tensor (T) directly. \n label  \nskimage.measure.label(input, background=None, return_num=False, connectivity=None) [source]\n \nLabel connected regions of an integer array. Two pixels are connected when they are neighbors and have the same value. In 2D, they can be neighbors either in a 1- or 2-connected sense. The value refers to the maximum number of orthogonal hops to consider a pixel/voxel a neighbor: 1-connectivity     2-connectivity     diagonal connection close-up\n\n     [ ]           [ ]  [ ]  [ ]             [ ]\n      |               \\  |  /                 |  <- hop 2\n[ ]--[x]--[ ]      [ ]--[x]--[ ]        [x]--[ ]\n      |               /  |  \\             hop 1\n     [ ]           [ ]  [ ]  [ ]\n  Parameters \n \ninputndarray of dtype int \n\nImage to label.  \nbackgroundint, optional \n\nConsider all pixels with this value as background pixels, and label them as 0. By default, 0-valued pixels are considered as background pixels.  \nreturn_numbool, optional \n\nWhether to return the number of assigned labels.  \nconnectivityint, optional \n\nMaximum number of orthogonal hops to consider a pixel/voxel as a neighbor. Accepted values are ranging from 1 to input.ndim. If None, a full connectivity of input.ndim is used.    Returns \n \nlabelsndarray of dtype int \n\nLabeled array, where all connected regions are assigned the same integer value.  \nnumint, optional \n\nNumber of labels, which equals the maximum label index and is only returned if return_num is True.      See also  \nregionprops\n\n\nregionprops_table\n\n  References  \n1  \nChristophe Fiorio and Jens Gustedt, \u201cTwo linear time Union-Find strategies for image processing\u201d, Theoretical Computer Science 154 (1996), pp. 165-181.  \n2  \nKensheng Wu, Ekow Otoo and Arie Shoshani, \u201cOptimizing connected component labeling algorithms\u201d, Paper LBNL-56864, 2005, Lawrence Berkeley National Laboratory (University of California), http://repositories.cdlib.org/lbnl/LBNL-56864   Examples >>> import numpy as np\n>>> x = np.eye(3).astype(int)\n>>> print(x)\n[[1 0 0]\n [0 1 0]\n [0 0 1]]\n>>> print(label(x, connectivity=1))\n[[1 0 0]\n [0 2 0]\n [0 0 3]]\n>>> print(label(x, connectivity=2))\n[[1 0 0]\n [0 1 0]\n [0 0 1]]\n>>> print(label(x, background=-1))\n[[1 2 2]\n [2 1 2]\n [2 2 1]]\n>>> x = np.array([[1, 0, 0],\n...               [1, 1, 5],\n...               [0, 0, 0]])\n>>> print(label(x))\n[[1 0 0]\n [1 1 2]\n [0 0 0]]\n \n Examples using skimage.measure.label\n \n  Measure region properties  \n\n  Euler number  \n\n  Segment human cells (in mitosis)   marching_cubes  \nskimage.measure.marching_cubes(volume, level=None, *, spacing=(1.0, 1.0, 1.0), gradient_direction='descent', step_size=1, allow_degenerate=True, method='lewiner', mask=None) [source]\n \nMarching cubes algorithm to find surfaces in 3d volumetric data. In contrast with Lorensen et al. approach [2], Lewiner et al. algorithm is faster, resolves ambiguities, and guarantees topologically correct results. Therefore, this algorithm generally a better choice.  Parameters \n \nvolume(M, N, P) array \n\nInput data volume to find isosurfaces. Will internally be converted to float32 if necessary.  \nlevelfloat, optional \n\nContour value to search for isosurfaces in volume. If not given or None, the average of the min and max of vol is used.  \nspacinglength-3 tuple of floats, optional \n\nVoxel spacing in spatial dimensions corresponding to numpy array indexing dimensions (M, N, P) as in volume.  \ngradient_directionstring, optional \n\nControls if the mesh was generated from an isosurface with gradient descent toward objects of interest (the default), or the opposite, considering the left-hand rule. The two options are: * descent : Object was greater than exterior * ascent : Exterior was greater than object  \nstep_sizeint, optional \n\nStep size in voxels. Default 1. Larger steps yield faster but coarser results. The result will always be topologically correct though.  \nallow_degeneratebool, optional \n\nWhether to allow degenerate (i.e. zero-area) triangles in the end-result. Default True. If False, degenerate triangles are removed, at the cost of making the algorithm slower.  method: str, optional\n\nOne of \u2018lewiner\u2019, \u2018lorensen\u2019 or \u2018_lorensen\u2019. Specify witch of Lewiner et al. or Lorensen et al. method will be used. The \u2018_lorensen\u2019 flag correspond to an old implementation that will be deprecated in version 0.19.  \nmask(M, N, P) array, optional \n\nBoolean array. The marching cube algorithm will be computed only on True elements. This will save computational time when interfaces are located within certain region of the volume M, N, P-e.g. the top half of the cube-and also allow to compute finite surfaces-i.e. open surfaces that do not end at the border of the cube.    Returns \n \nverts(V, 3) array \n\nSpatial coordinates for V unique mesh vertices. Coordinate order matches input volume (M, N, P). If allow_degenerate is set to True, then the presence of degenerate triangles in the mesh can make this array have duplicate vertices.  \nfaces(F, 3) array \n\nDefine triangular faces via referencing vertex indices from verts. This algorithm specifically outputs triangles, so each face has exactly three indices.  \nnormals(V, 3) array \n\nThe normal direction at each vertex, as calculated from the data.  \nvalues(V, ) array \n\nGives a measure for the maximum value of the data in the local region near each vertex. This can be used by visualization tools to apply a colormap to the mesh.      See also  \nskimage.measure.mesh_surface_area\n\n\nskimage.measure.find_contours\n\n  Notes The algorithm [1] is an improved version of Chernyaev\u2019s Marching Cubes 33 algorithm. It is an efficient algorithm that relies on heavy use of lookup tables to handle the many different cases, keeping the algorithm relatively easy. This implementation is written in Cython, ported from Lewiner\u2019s C++ implementation. To quantify the area of an isosurface generated by this algorithm, pass verts and faces to skimage.measure.mesh_surface_area. Regarding visualization of algorithm output, to contour a volume named myvolume about the level 0.0, using the mayavi package: >>>\n>> from mayavi import mlab\n>> verts, faces, _, _ = marching_cubes(myvolume, 0.0)\n>> mlab.triangular_mesh([vert[0] for vert in verts],\n                        [vert[1] for vert in verts],\n                        [vert[2] for vert in verts],\n                        faces)\n>> mlab.show()\n Similarly using the visvis package: >>>\n>> import visvis as vv\n>> verts, faces, normals, values = marching_cubes(myvolume, 0.0)\n>> vv.mesh(np.fliplr(verts), faces, normals, values)\n>> vv.use().Run()\n To reduce the number of triangles in the mesh for better performance, see this example using the mayavi package. References  \n1  \nThomas Lewiner, Helio Lopes, Antonio Wilson Vieira and Geovan Tavares. Efficient implementation of Marching Cubes\u2019 cases with topological guarantees. Journal of Graphics Tools 8(2) pp. 1-15 (december 2003). DOI:10.1080/10867651.2003.10487582  \n2  \nLorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings) 21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422   \n marching_cubes_classic  \nskimage.measure.marching_cubes_classic(volume, level=None, spacing=(1.0, 1.0, 1.0), gradient_direction='descent') [source]\n \nClassic marching cubes algorithm to find surfaces in 3d volumetric data. Note that the marching_cubes() algorithm is recommended over this algorithm, because it\u2019s faster and produces better results.  Parameters \n \nvolume(M, N, P) array of doubles \n\nInput data volume to find isosurfaces. Will be cast to np.float64.  \nlevelfloat \n\nContour value to search for isosurfaces in volume. If not given or None, the average of the min and max of vol is used.  \nspacinglength-3 tuple of floats \n\nVoxel spacing in spatial dimensions corresponding to numpy array indexing dimensions (M, N, P) as in volume.  \ngradient_directionstring \n\nControls if the mesh was generated from an isosurface with gradient descent toward objects of interest (the default), or the opposite. The two options are: * descent : Object was greater than exterior * ascent : Exterior was greater than object    Returns \n \nverts(V, 3) array \n\nSpatial coordinates for V unique mesh vertices. Coordinate order matches input volume (M, N, P). If allow_degenerate is set to True, then the presence of degenerate triangles in the mesh can make this array have duplicate vertices.  \nfaces(F, 3) array \n\nDefine triangular faces via referencing vertex indices from verts. This algorithm specifically outputs triangles, so each face has exactly three indices.      See also  \nskimage.measure.marching_cubes\n\n\nskimage.measure.mesh_surface_area\n\n  Notes The marching cubes algorithm is implemented as described in [1]. A simple explanation is available here: http://users.polytech.unice.fr/~lingrand/MarchingCubes/algo.html\n There are several known ambiguous cases in the marching cubes algorithm. Using point labeling as in [1], Figure 4, as shown:     v8 ------ v7\n   / |       / |        y\n  /  |      /  |        ^  z\nv4 ------ v3   |        | /\n |  v5 ----|- v6        |/          (note: NOT right handed!)\n |  /      |  /          ----> x\n | /       | /\nv1 ------ v2\n Most notably, if v4, v8, v2, and v6 are all >= level (or any generalization of this case) two parallel planes are generated by this algorithm, separating v4 and v8 from v2 and v6. An equally valid interpretation would be a single connected thin surface enclosing all four points. This is the best known ambiguity, though there are others. This algorithm does not attempt to resolve such ambiguities; it is a naive implementation of marching cubes as in [1], but may be a good beginning for work with more recent techniques (Dual Marching Cubes, Extended Marching Cubes, Cubic Marching Squares, etc.). Because of interactions between neighboring cubes, the isosurface(s) generated by this algorithm are NOT guaranteed to be closed, particularly for complicated contours. Furthermore, this algorithm does not guarantee a single contour will be returned. Indeed, ALL isosurfaces which cross level will be found, regardless of connectivity. The output is a triangular mesh consisting of a set of unique vertices and connecting triangles. The order of these vertices and triangles in the output list is determined by the position of the smallest x,y,z (in lexicographical order) coordinate in the contour. This is a side-effect of how the input array is traversed, but can be relied upon. The generated mesh guarantees coherent orientation as of version 0.12. To quantify the area of an isosurface generated by this algorithm, pass outputs directly into skimage.measure.mesh_surface_area. References  \n1(1,2,3)  \nLorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings) 21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422   \n marching_cubes_lewiner  \nskimage.measure.marching_cubes_lewiner(volume, level=None, spacing=(1.0, 1.0, 1.0), gradient_direction='descent', step_size=1, allow_degenerate=True, use_classic=False, mask=None) [source]\n \nLewiner marching cubes algorithm to find surfaces in 3d volumetric data. In contrast to marching_cubes_classic(), this algorithm is faster, resolves ambiguities, and guarantees topologically correct results. Therefore, this algorithm generally a better choice, unless there is a specific need for the classic algorithm.  Parameters \n \nvolume(M, N, P) array \n\nInput data volume to find isosurfaces. Will internally be converted to float32 if necessary.  \nlevelfloat \n\nContour value to search for isosurfaces in volume. If not given or None, the average of the min and max of vol is used.  \nspacinglength-3 tuple of floats \n\nVoxel spacing in spatial dimensions corresponding to numpy array indexing dimensions (M, N, P) as in volume.  \ngradient_directionstring \n\nControls if the mesh was generated from an isosurface with gradient descent toward objects of interest (the default), or the opposite, considering the left-hand rule. The two options are: * descent : Object was greater than exterior * ascent : Exterior was greater than object  \nstep_sizeint \n\nStep size in voxels. Default 1. Larger steps yield faster but coarser results. The result will always be topologically correct though.  \nallow_degeneratebool \n\nWhether to allow degenerate (i.e. zero-area) triangles in the end-result. Default True. If False, degenerate triangles are removed, at the cost of making the algorithm slower.  \nuse_classicbool \n\nIf given and True, the classic marching cubes by Lorensen (1987) is used. This option is included for reference purposes. Note that this algorithm has ambiguities and is not guaranteed to produce a topologically correct result. The results with using this option are not generally the same as the marching_cubes_classic() function.  \nmask(M, N, P) array \n\nBoolean array. The marching cube algorithm will be computed only on True elements. This will save computational time when interfaces are located within certain region of the volume M, N, P-e.g. the top half of the cube-and also allow to compute finite surfaces-i.e. open surfaces that do not end at the border of the cube.    Returns \n \nverts(V, 3) array \n\nSpatial coordinates for V unique mesh vertices. Coordinate order matches input volume (M, N, P). If allow_degenerate is set to True, then the presence of degenerate triangles in the mesh can make this array have duplicate vertices.  \nfaces(F, 3) array \n\nDefine triangular faces via referencing vertex indices from verts. This algorithm specifically outputs triangles, so each face has exactly three indices.  \nnormals(V, 3) array \n\nThe normal direction at each vertex, as calculated from the data.  \nvalues(V, ) array \n\nGives a measure for the maximum value of the data in the local region near each vertex. This can be used by visualization tools to apply a colormap to the mesh.      See also  \nskimage.measure.marching_cubes\n\n\nskimage.measure.mesh_surface_area\n\n  Notes The algorithm [1] is an improved version of Chernyaev\u2019s Marching Cubes 33 algorithm. It is an efficient algorithm that relies on heavy use of lookup tables to handle the many different cases, keeping the algorithm relatively easy. This implementation is written in Cython, ported from Lewiner\u2019s C++ implementation. To quantify the area of an isosurface generated by this algorithm, pass verts and faces to skimage.measure.mesh_surface_area. Regarding visualization of algorithm output, to contour a volume named myvolume about the level 0.0, using the mayavi package: >>> from mayavi import mlab \n>>> verts, faces, normals, values = marching_cubes_lewiner(myvolume, 0.0) \n>>> mlab.triangular_mesh([vert[0] for vert in verts],\n...                      [vert[1] for vert in verts],\n...                      [vert[2] for vert in verts],\n...                      faces) \n>>> mlab.show() \n Similarly using the visvis package: >>> import visvis as vv \n>>> verts, faces, normals, values = marching_cubes_lewiner(myvolume, 0.0) \n>>> vv.mesh(np.fliplr(verts), faces, normals, values) \n>>> vv.use().Run() \n References  \n1  \nThomas Lewiner, Helio Lopes, Antonio Wilson Vieira and Geovan Tavares. Efficient implementation of Marching Cubes\u2019 cases with topological guarantees. Journal of Graphics Tools 8(2) pp. 1-15 (december 2003). DOI:10.1080/10867651.2003.10487582   \n mesh_surface_area  \nskimage.measure.mesh_surface_area(verts, faces) [source]\n \nCompute surface area, given vertices & triangular faces  Parameters \n \nverts(V, 3) array of floats \n\nArray containing (x, y, z) coordinates for V unique mesh vertices.  \nfaces(F, 3) array of ints \n\nList of length-3 lists of integers, referencing vertex coordinates as provided in verts    Returns \n \nareafloat \n\nSurface area of mesh. Units now [coordinate units] ** 2.      See also  \nskimage.measure.marching_cubes\n\n\nskimage.measure.marching_cubes_classic\n\n  Notes The arguments expected by this function are the first two outputs from skimage.measure.marching_cubes. For unit correct output, ensure correct spacing was passed to skimage.measure.marching_cubes. This algorithm works properly only if the faces provided are all triangles. \n moments  \nskimage.measure.moments(image, order=3) [source]\n \nCalculate all raw image moments up to a certain order.  The following properties can be calculated from raw image moments:\n\n Area as: M[0, 0]. Centroid as: {M[1, 0] / M[0, 0], M[0, 1] / M[0, 0]}.    Note that raw moments are neither translation, scale nor rotation invariant.  Parameters \n \nimagenD double or uint8 array \n\nRasterized shape as image.  \norderint, optional \n\nMaximum order of moments. Default is 3.    Returns \n \nm(order + 1, order + 1) array \n\nRaw image moments.     References  \n1  \nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.  \n2  \nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.  \n3  \nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.  \n4  \nhttps://en.wikipedia.org/wiki/Image_moment   Examples >>> image = np.zeros((20, 20), dtype=np.double)\n>>> image[13:17, 13:17] = 1\n>>> M = moments(image)\n>>> centroid = (M[1, 0] / M[0, 0], M[0, 1] / M[0, 0])\n>>> centroid\n(14.5, 14.5)\n \n moments_central  \nskimage.measure.moments_central(image, center=None, order=3, **kwargs) [source]\n \nCalculate all central image moments up to a certain order. The center coordinates (cr, cc) can be calculated from the raw moments as: {M[1, 0] / M[0, 0], M[0, 1] / M[0, 0]}. Note that central moments are translation invariant but not scale and rotation invariant.  Parameters \n \nimagenD double or uint8 array \n\nRasterized shape as image.  \ncentertuple of float, optional \n\nCoordinates of the image centroid. This will be computed if it is not provided.  \norderint, optional \n\nThe maximum order of moments computed.    Returns \n \nmu(order + 1, order + 1) array \n\nCentral image moments.     References  \n1  \nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.  \n2  \nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.  \n3  \nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.  \n4  \nhttps://en.wikipedia.org/wiki/Image_moment   Examples >>> image = np.zeros((20, 20), dtype=np.double)\n>>> image[13:17, 13:17] = 1\n>>> M = moments(image)\n>>> centroid = (M[1, 0] / M[0, 0], M[0, 1] / M[0, 0])\n>>> moments_central(image, centroid)\narray([[16.,  0., 20.,  0.],\n       [ 0.,  0.,  0.,  0.],\n       [20.,  0., 25.,  0.],\n       [ 0.,  0.,  0.,  0.]])\n \n moments_coords  \nskimage.measure.moments_coords(coords, order=3) [source]\n \nCalculate all raw image moments up to a certain order.  The following properties can be calculated from raw image moments:\n\n Area as: M[0, 0]. Centroid as: {M[1, 0] / M[0, 0], M[0, 1] / M[0, 0]}.    Note that raw moments are neither translation, scale nor rotation invariant.  Parameters \n \ncoords(N, D) double or uint8 array \n\nArray of N points that describe an image of D dimensionality in Cartesian space.  \norderint, optional \n\nMaximum order of moments. Default is 3.    Returns \n \nM(order + 1, order + 1, \u2026) array \n\nRaw image moments. (D dimensions)     References  \n1  \nJohannes Kilian. Simple Image Analysis By Moments. Durham University, version 0.2, Durham, 2001.   Examples >>> coords = np.array([[row, col]\n...                    for row in range(13, 17)\n...                    for col in range(14, 18)], dtype=np.double)\n>>> M = moments_coords(coords)\n>>> centroid = (M[1, 0] / M[0, 0], M[0, 1] / M[0, 0])\n>>> centroid\n(14.5, 15.5)\n \n moments_coords_central  \nskimage.measure.moments_coords_central(coords, center=None, order=3) [source]\n \nCalculate all central image moments up to a certain order.  The following properties can be calculated from raw image moments:\n\n Area as: M[0, 0]. Centroid as: {M[1, 0] / M[0, 0], M[0, 1] / M[0, 0]}.    Note that raw moments are neither translation, scale nor rotation invariant.  Parameters \n \ncoords(N, D) double or uint8 array \n\nArray of N points that describe an image of D dimensionality in Cartesian space. A tuple of coordinates as returned by np.nonzero is also accepted as input.  \ncentertuple of float, optional \n\nCoordinates of the image centroid. This will be computed if it is not provided.  \norderint, optional \n\nMaximum order of moments. Default is 3.    Returns \n \nMc(order + 1, order + 1, \u2026) array \n\nCentral image moments. (D dimensions)     References  \n1  \nJohannes Kilian. Simple Image Analysis By Moments. Durham University, version 0.2, Durham, 2001.   Examples >>> coords = np.array([[row, col]\n...                    for row in range(13, 17)\n...                    for col in range(14, 18)])\n>>> moments_coords_central(coords)\narray([[16.,  0., 20.,  0.],\n       [ 0.,  0.,  0.,  0.],\n       [20.,  0., 25.,  0.],\n       [ 0.,  0.,  0.,  0.]])\n As seen above, for symmetric objects, odd-order moments (columns 1 and 3, rows 1 and 3) are zero when centered on the centroid, or center of mass, of the object (the default). If we break the symmetry by adding a new point, this no longer holds: >>> coords2 = np.concatenate((coords, [[17, 17]]), axis=0)\n>>> np.round(moments_coords_central(coords2),\n...          decimals=2)  \narray([[17.  ,  0.  , 22.12, -2.49],\n       [ 0.  ,  3.53,  1.73,  7.4 ],\n       [25.88,  6.02, 36.63,  8.83],\n       [ 4.15, 19.17, 14.8 , 39.6 ]])\n Image moments and central image moments are equivalent (by definition) when the center is (0, 0): >>> np.allclose(moments_coords(coords),\n...             moments_coords_central(coords, (0, 0)))\nTrue\n \n moments_hu  \nskimage.measure.moments_hu(nu) [source]\n \nCalculate Hu\u2019s set of image moments (2D-only). Note that this set of moments is proofed to be translation, scale and rotation invariant.  Parameters \n \nnu(M, M) array \n\nNormalized central image moments, where M must be >= 4.    Returns \n \nnu(7,) array \n\nHu\u2019s set of image moments.     References  \n1  \nM. K. Hu, \u201cVisual Pattern Recognition by Moment Invariants\u201d, IRE Trans. Info. Theory, vol. IT-8, pp. 179-187, 1962  \n2  \nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.  \n3  \nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.  \n4  \nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.  \n5  \nhttps://en.wikipedia.org/wiki/Image_moment   Examples >>> image = np.zeros((20, 20), dtype=np.double)\n>>> image[13:17, 13:17] = 0.5\n>>> image[10:12, 10:12] = 1\n>>> mu = moments_central(image)\n>>> nu = moments_normalized(mu)\n>>> moments_hu(nu)\narray([7.45370370e-01, 3.51165981e-01, 1.04049179e-01, 4.06442107e-02,\n       2.64312299e-03, 2.40854582e-02, 4.33680869e-19])\n \n moments_normalized  \nskimage.measure.moments_normalized(mu, order=3) [source]\n \nCalculate all normalized central image moments up to a certain order. Note that normalized central moments are translation and scale invariant but not rotation invariant.  Parameters \n \nmu(M,[ \u2026,] M) array \n\nCentral image moments, where M must be greater than or equal to order.  \norderint, optional \n\nMaximum order of moments. Default is 3.    Returns \n \nnu(order + 1,[ \u2026,] order + 1) array \n\nNormalized central image moments.     References  \n1  \nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.  \n2  \nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.  \n3  \nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.  \n4  \nhttps://en.wikipedia.org/wiki/Image_moment   Examples >>> image = np.zeros((20, 20), dtype=np.double)\n>>> image[13:17, 13:17] = 1\n>>> m = moments(image)\n>>> centroid = (m[0, 1] / m[0, 0], m[1, 0] / m[0, 0])\n>>> mu = moments_central(image, centroid)\n>>> moments_normalized(mu)\narray([[       nan,        nan, 0.078125  , 0.        ],\n       [       nan, 0.        , 0.        , 0.        ],\n       [0.078125  , 0.        , 0.00610352, 0.        ],\n       [0.        , 0.        , 0.        , 0.        ]])\n \n perimeter  \nskimage.measure.perimeter(image, neighbourhood=4) [source]\n \nCalculate total perimeter of all objects in binary image.  Parameters \n \nimage(N, M) ndarray \n\n2D binary image.  \nneighbourhood4 or 8, optional \n\nNeighborhood connectivity for border pixel determination. It is used to compute the contour. A higher neighbourhood widens the border on which the perimeter is computed.    Returns \n \nperimeterfloat \n\nTotal perimeter of all objects in binary image.     References  \n1  \nK. Benkrid, D. Crookes. Design and FPGA Implementation of a Perimeter Estimator. The Queen\u2019s University of Belfast. http://www.cs.qub.ac.uk/~d.crookes/webpubs/papers/perimeter.doc   Examples >>> from skimage import data, util\n>>> from skimage.measure import label\n>>> # coins image (binary)\n>>> img_coins = data.coins() > 110\n>>> # total perimeter of all objects in the image\n>>> perimeter(img_coins, neighbourhood=4)  \n7796.867...\n>>> perimeter(img_coins, neighbourhood=8)  \n8806.268...\n \n Examples using skimage.measure.perimeter\n \n  Different perimeters   perimeter_crofton  \nskimage.measure.perimeter_crofton(image, directions=4) [source]\n \nCalculate total Crofton perimeter of all objects in binary image.  Parameters \n \nimage(N, M) ndarray \n\n2D image. If image is not binary, all values strictly greater than zero are considered as the object.  \ndirections2 or 4, optional \n\nNumber of directions used to approximate the Crofton perimeter. By default, 4 is used: it should be more accurate than 2. Computation time is the same in both cases.    Returns \n \nperimeterfloat \n\nTotal perimeter of all objects in binary image.     Notes This measure is based on Crofton formula [1], which is a measure from integral geometry. It is defined for general curve length evaluation via a double integral along all directions. In a discrete space, 2 or 4 directions give a quite good approximation, 4 being more accurate than 2 for more complex shapes. Similar to perimeter(), this function returns an approximation of the perimeter in continuous space. References  \n1  \nhttps://en.wikipedia.org/wiki/Crofton_formula  \n2  \nS. Rivollier. Analyse d\u2019image geometrique et morphometrique par diagrammes de forme et voisinages adaptatifs generaux. PhD thesis, 2010. Ecole Nationale Superieure des Mines de Saint-Etienne. https://tel.archives-ouvertes.fr/tel-00560838   Examples >>> from skimage import data, util\n>>> from skimage.measure import label\n>>> # coins image (binary)\n>>> img_coins = data.coins() > 110\n>>> # total perimeter of all objects in the image\n>>> perimeter_crofton(img_coins, directions=2)  \n8144.578...\n>>> perimeter_crofton(img_coins, directions=4)  \n7837.077...\n \n Examples using skimage.measure.perimeter_crofton\n \n  Different perimeters   points_in_poly  \nskimage.measure.points_in_poly(points, verts) [source]\n \nTest whether points lie inside a polygon.  Parameters \n \npoints(N, 2) array \n\nInput points, (x, y).  \nverts(M, 2) array \n\nVertices of the polygon, sorted either clockwise or anti-clockwise. The first point may (but does not need to be) duplicated.    Returns \n \nmask(N,) array of bool \n\nTrue if corresponding point is inside the polygon.      See also  \ngrid_points_in_poly\n\n  \n profile_line  \nskimage.measure.profile_line(image, src, dst, linewidth=1, order=None, mode=None, cval=0.0, *, reduce_func=<function mean>) [source]\n \nReturn the intensity profile of an image measured along a scan line.  Parameters \n \nimagendarray, shape (M, N[, C]) \n\nThe image, either grayscale (2D array) or multichannel (3D array, where the final axis contains the channel information).  \nsrcarray_like, shape (2, ) \n\nThe coordinates of the start point of the scan line.  \ndstarray_like, shape (2, ) \n\nThe coordinates of the end point of the scan line. The destination point is included in the profile, in contrast to standard numpy indexing.  \nlinewidthint, optional \n\nWidth of the scan, perpendicular to the line  \norderint in {0, 1, 2, 3, 4, 5}, optional \n\nThe order of the spline interpolation, default is 0 if image.dtype is bool and 1 otherwise. The order has to be in the range 0-5. See skimage.transform.warp for detail.  \nmode{\u2018constant\u2019, \u2018nearest\u2019, \u2018reflect\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional \n\nHow to compute any values falling outside of the image.  \ncvalfloat, optional \n\nIf mode is \u2018constant\u2019, what constant value to use outside the image.  \nreduce_funccallable, optional \n\nFunction used to calculate the aggregation of pixel values perpendicular to the profile_line direction when linewidth > 1. If set to None the unreduced array will be returned.    Returns \n \nreturn_valuearray \n\nThe intensity profile along the scan line. The length of the profile is the ceil of the computed length of the scan line.     Examples >>> x = np.array([[1, 1, 1, 2, 2, 2]])\n>>> img = np.vstack([np.zeros_like(x), x, x, x, np.zeros_like(x)])\n>>> img\narray([[0, 0, 0, 0, 0, 0],\n       [1, 1, 1, 2, 2, 2],\n       [1, 1, 1, 2, 2, 2],\n       [1, 1, 1, 2, 2, 2],\n       [0, 0, 0, 0, 0, 0]])\n>>> profile_line(img, (2, 1), (2, 4))\narray([1., 1., 2., 2.])\n>>> profile_line(img, (1, 0), (1, 6), cval=4)\narray([1., 1., 1., 2., 2., 2., 4.])\n The destination point is included in the profile, in contrast to standard numpy indexing. For example: >>> profile_line(img, (1, 0), (1, 6))  # The final point is out of bounds\narray([1., 1., 1., 2., 2., 2., 0.])\n>>> profile_line(img, (1, 0), (1, 5))  # This accesses the full first row\narray([1., 1., 1., 2., 2., 2.])\n For different reduce_func inputs: >>> profile_line(img, (1, 0), (1, 3), linewidth=3, reduce_func=np.mean)\narray([0.66666667, 0.66666667, 0.66666667, 1.33333333])\n>>> profile_line(img, (1, 0), (1, 3), linewidth=3, reduce_func=np.max)\narray([1, 1, 1, 2])\n>>> profile_line(img, (1, 0), (1, 3), linewidth=3, reduce_func=np.sum)\narray([2, 2, 2, 4])\n The unreduced array will be returned when reduce_func is None or when reduce_func acts on each pixel value individually. >>> profile_line(img, (1, 2), (4, 2), linewidth=3, order=0,\n...     reduce_func=None)\narray([[1, 1, 2],\n       [1, 1, 2],\n       [1, 1, 2],\n       [0, 0, 0]])\n>>> profile_line(img, (1, 0), (1, 3), linewidth=3, reduce_func=np.sqrt)\narray([[1.        , 1.        , 0.        ],\n       [1.        , 1.        , 0.        ],\n       [1.        , 1.        , 0.        ],\n       [1.41421356, 1.41421356, 0.        ]])\n \n ransac  \nskimage.measure.ransac(data, model_class, min_samples, residual_threshold, is_data_valid=None, is_model_valid=None, max_trials=100, stop_sample_num=inf, stop_residuals_sum=0, stop_probability=1, random_state=None, initial_inliers=None) [source]\n \nFit a model to data with the RANSAC (random sample consensus) algorithm. RANSAC is an iterative algorithm for the robust estimation of parameters from a subset of inliers from the complete data set. Each iteration performs the following tasks:  Select min_samples random samples from the original data and check whether the set of data is valid (see is_data_valid). Estimate a model to the random subset (model_cls.estimate(*data[random_subset]) and check whether the estimated model is valid (see is_model_valid). Classify all data as inliers or outliers by calculating the residuals to the estimated model (model_cls.residuals(*data)) - all data samples with residuals smaller than the residual_threshold are considered as inliers. Save estimated model as best model if number of inlier samples is maximal. In case the current estimated model has the same number of inliers, it is only considered as the best model if it has less sum of residuals.  These steps are performed either a maximum number of times or until one of the special stop criteria are met. The final model is estimated using all inlier samples of the previously determined best model.  Parameters \n \ndata[list, tuple of] (N, \u2026) array \n\nData set to which the model is fitted, where N is the number of data points and the remaining dimension are depending on model requirements. If the model class requires multiple input data arrays (e.g. source and destination coordinates of skimage.transform.AffineTransform), they can be optionally passed as tuple or list. Note, that in this case the functions estimate(*data), residuals(*data), is_model_valid(model, *random_data) and is_data_valid(*random_data) must all take each data array as separate arguments.  \nmodel_classobject \n\nObject with the following object methods:  success = estimate(*data) residuals(*data)  where success indicates whether the model estimation succeeded (True or None for success, False for failure).  \nmin_samplesint in range (0, N) \n\nThe minimum number of data points to fit a model to.  \nresidual_thresholdfloat larger than 0 \n\nMaximum distance for a data point to be classified as an inlier.  \nis_data_validfunction, optional \n\nThis function is called with the randomly selected data before the model is fitted to it: is_data_valid(*random_data).  \nis_model_validfunction, optional \n\nThis function is called with the estimated model and the randomly selected data: is_model_valid(model, *random_data), .  \nmax_trialsint, optional \n\nMaximum number of iterations for random sample selection.  \nstop_sample_numint, optional \n\nStop iteration if at least this number of inliers are found.  \nstop_residuals_sumfloat, optional \n\nStop iteration if sum of residuals is less than or equal to this threshold.  \nstop_probabilityfloat in range [0, 1], optional \n\nRANSAC iteration stops if at least one outlier-free set of the training data is sampled with probability >= stop_probability, depending on the current best model\u2019s inlier ratio and the number of trials. This requires to generate at least N samples (trials): N >= log(1 - probability) / log(1 - e**m) where the probability (confidence) is typically set to a high value such as 0.99, e is the current fraction of inliers w.r.t. the total number of samples, and m is the min_samples value.  \nrandom_stateint, RandomState instance or None, optional \n\nIf int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.  \ninitial_inliersarray-like of bool, shape (N,), optional \n\nInitial samples selection for model estimation    Returns \n \nmodelobject \n\nBest model with largest consensus set.  \ninliers(N, ) array \n\nBoolean mask of inliers classified as True.     References  \n1  \n\u201cRANSAC\u201d, Wikipedia, https://en.wikipedia.org/wiki/RANSAC   Examples Generate ellipse data without tilt and add noise: >>> t = np.linspace(0, 2 * np.pi, 50)\n>>> xc, yc = 20, 30\n>>> a, b = 5, 10\n>>> x = xc + a * np.cos(t)\n>>> y = yc + b * np.sin(t)\n>>> data = np.column_stack([x, y])\n>>> np.random.seed(seed=1234)\n>>> data += np.random.normal(size=data.shape)\n Add some faulty data: >>> data[0] = (100, 100)\n>>> data[1] = (110, 120)\n>>> data[2] = (120, 130)\n>>> data[3] = (140, 130)\n Estimate ellipse model using all available data: >>> model = EllipseModel()\n>>> model.estimate(data)\nTrue\n>>> np.round(model.params)  \narray([ 72.,  75.,  77.,  14.,   1.])\n Estimate ellipse model using RANSAC: >>> ransac_model, inliers = ransac(data, EllipseModel, 20, 3, max_trials=50)\n>>> abs(np.round(ransac_model.params))\narray([20., 30.,  5., 10.,  0.])\n>>> inliers \narray([False, False, False, False,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True], dtype=bool)\n>>> sum(inliers) > 40\nTrue\n RANSAC can be used to robustly estimate a geometric transformation. In this section, we also show how to use a proportion of the total samples, rather than an absolute number. >>> from skimage.transform import SimilarityTransform\n>>> np.random.seed(0)\n>>> src = 100 * np.random.rand(50, 2)\n>>> model0 = SimilarityTransform(scale=0.5, rotation=1, translation=(10, 20))\n>>> dst = model0(src)\n>>> dst[0] = (10000, 10000)\n>>> dst[1] = (-100, 100)\n>>> dst[2] = (50, 50)\n>>> ratio = 0.5  # use half of the samples\n>>> min_samples = int(ratio * len(src))\n>>> model, inliers = ransac((src, dst), SimilarityTransform, min_samples, 10,\n...                         initial_inliers=np.ones(len(src), dtype=bool))\n>>> inliers\narray([False, False, False,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True])\n \n regionprops  \nskimage.measure.regionprops(label_image, intensity_image=None, cache=True, coordinates=None, *, extra_properties=None) [source]\n \nMeasure properties of labeled image regions.  Parameters \n \nlabel_image(M, N[, P]) ndarray \n\nLabeled input image. Labels with value 0 are ignored.  Changed in version 0.14.1: Previously, label_image was processed by numpy.squeeze and so any number of singleton dimensions was allowed. This resulted in inconsistent handling of images with singleton dimensions. To recover the old behaviour, use regionprops(np.squeeze(label_image), ...).   \nintensity_image(M, N[, P][, C]) ndarray, optional \n\nIntensity (i.e., input) image with same size as labeled image, plus optionally an extra dimension for multichannel data. Default is None.  Changed in version 0.18.0: The ability to provide an extra dimension for channels was added.   \ncachebool, optional \n\nDetermine whether to cache calculated properties. The computation is much faster for cached properties, whereas the memory consumption increases.  \ncoordinatesDEPRECATED \n\nThis argument is deprecated and will be removed in a future version of scikit-image. See Coordinate conventions for more details.  Deprecated since version 0.16.0: Use \u201crc\u201d coordinates everywhere. It may be sufficient to call numpy.transpose on your label image to get the same values as 0.15 and earlier. However, for some properties, the transformation will be less trivial. For example, the new orientation is \\(\\frac{\\pi}{2}\\) plus the old orientation.   \nextra_propertiesIterable of callables \n\nAdd extra property computation functions that are not included with skimage. The name of the property is derived from the function name, the dtype is inferred by calling the function on a small sample. If the name of an extra property clashes with the name of an existing property the extra property wil not be visible and a UserWarning is issued. A property computation function must take a region mask as its first argument. If the property requires an intensity image, it must accept the intensity image as the second argument.    Returns \n \npropertieslist of RegionProperties \n\nEach item describes one labeled region, and can be accessed using the attributes listed below.      See also  \nlabel\n\n  Notes The following properties can be accessed as attributes or keys:  \nareaint \n\nNumber of pixels of the region.  \nbboxtuple \n\nBounding box (min_row, min_col, max_row, max_col). Pixels belonging to the bounding box are in the half-open interval [min_row; max_row) and [min_col; max_col).  \nbbox_areaint \n\nNumber of pixels of bounding box.  \ncentroidarray \n\nCentroid coordinate tuple (row, col).  \nconvex_areaint \n\nNumber of pixels of convex hull image, which is the smallest convex polygon that encloses the region.  \nconvex_image(H, J) ndarray \n\nBinary convex hull image which has the same size as bounding box.  \ncoords(N, 2) ndarray \n\nCoordinate list (row, col) of the region.  \neccentricityfloat \n\nEccentricity of the ellipse that has the same second-moments as the region. The eccentricity is the ratio of the focal distance (distance between focal points) over the major axis length. The value is in the interval [0, 1). When it is 0, the ellipse becomes a circle.  \nequivalent_diameterfloat \n\nThe diameter of a circle with the same area as the region.  \neuler_numberint \n\nEuler characteristic of the set of non-zero pixels. Computed as number of connected components subtracted by number of holes (input.ndim connectivity). In 3D, number of connected components plus number of holes subtracted by number of tunnels.  \nextentfloat \n\nRatio of pixels in the region to pixels in the total bounding box. Computed as area / (rows * cols)  \nferet_diameter_maxfloat \n\nMaximum Feret\u2019s diameter computed as the longest distance between points around a region\u2019s convex hull contour as determined by find_contours. [5]  \nfilled_areaint \n\nNumber of pixels of the region will all the holes filled in. Describes the area of the filled_image.  \nfilled_image(H, J) ndarray \n\nBinary region image with filled holes which has the same size as bounding box.  \nimage(H, J) ndarray \n\nSliced binary region image which has the same size as bounding box.  \ninertia_tensorndarray \n\nInertia tensor of the region for the rotation around its mass.  \ninertia_tensor_eigvalstuple \n\nThe eigenvalues of the inertia tensor in decreasing order.  \nintensity_imagendarray \n\nImage inside region bounding box.  \nlabelint \n\nThe label in the labeled input image.  \nlocal_centroidarray \n\nCentroid coordinate tuple (row, col), relative to region bounding box.  \nmajor_axis_lengthfloat \n\nThe length of the major axis of the ellipse that has the same normalized second central moments as the region.  \nmax_intensityfloat \n\nValue with the greatest intensity in the region.  \nmean_intensityfloat \n\nValue with the mean intensity in the region.  \nmin_intensityfloat \n\nValue with the least intensity in the region.  \nminor_axis_lengthfloat \n\nThe length of the minor axis of the ellipse that has the same normalized second central moments as the region.  \nmoments(3, 3) ndarray \n\nSpatial moments up to 3rd order: m_ij = sum{ array(row, col) * row^i * col^j }\n where the sum is over the row, col coordinates of the region.  \nmoments_central(3, 3) ndarray \n\nCentral moments (translation invariant) up to 3rd order: mu_ij = sum{ array(row, col) * (row - row_c)^i * (col - col_c)^j }\n where the sum is over the row, col coordinates of the region, and row_c and col_c are the coordinates of the region\u2019s centroid.  \nmoments_hutuple \n\nHu moments (translation, scale and rotation invariant).  \nmoments_normalized(3, 3) ndarray \n\nNormalized moments (translation and scale invariant) up to 3rd order: nu_ij = mu_ij / m_00^[(i+j)/2 + 1]\n where m_00 is the zeroth spatial moment.  \norientationfloat \n\nAngle between the 0th axis (rows) and the major axis of the ellipse that has the same second moments as the region, ranging from -pi/2 to pi/2 counter-clockwise.  \nperimeterfloat \n\nPerimeter of object which approximates the contour as a line through the centers of border pixels using a 4-connectivity.  \nperimeter_croftonfloat \n\nPerimeter of object approximated by the Crofton formula in 4 directions.  \nslicetuple of slices \n\nA slice to extract the object from the source image.  \nsolidityfloat \n\nRatio of pixels in the region to pixels of the convex hull image.  \nweighted_centroidarray \n\nCentroid coordinate tuple (row, col) weighted with intensity image.  \nweighted_local_centroidarray \n\nCentroid coordinate tuple (row, col), relative to region bounding box, weighted with intensity image.  \nweighted_moments(3, 3) ndarray \n\nSpatial moments of intensity image up to 3rd order: wm_ij = sum{ array(row, col) * row^i * col^j }\n where the sum is over the row, col coordinates of the region.  \nweighted_moments_central(3, 3) ndarray \n\nCentral moments (translation invariant) of intensity image up to 3rd order: wmu_ij = sum{ array(row, col) * (row - row_c)^i * (col - col_c)^j }\n where the sum is over the row, col coordinates of the region, and row_c and col_c are the coordinates of the region\u2019s weighted centroid.  \nweighted_moments_hutuple \n\nHu moments (translation, scale and rotation invariant) of intensity image.  \nweighted_moments_normalized(3, 3) ndarray \n\nNormalized moments (translation and scale invariant) of intensity image up to 3rd order: wnu_ij = wmu_ij / wm_00^[(i+j)/2 + 1]\n where wm_00 is the zeroth spatial moment (intensity-weighted area).   Each region also supports iteration, so that you can do: for prop in region:\n    print(prop, region[prop])\n References  \n1  \nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.  \n2  \nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.  \n3  \nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.  \n4  \nhttps://en.wikipedia.org/wiki/Image_moment  \n5  \nW. Pabst, E. Gregorov\u00e1. Characterization of particles and particle systems, pp. 27-28. ICT Prague, 2007. https://old.vscht.cz/sil/keramika/Characterization_of_particles/CPPS%20_English%20version_.pdf   Examples >>> from skimage import data, util\n>>> from skimage.measure import label, regionprops\n>>> img = util.img_as_ubyte(data.coins()) > 110\n>>> label_img = label(img, connectivity=img.ndim)\n>>> props = regionprops(label_img)\n>>> # centroid of first labeled object\n>>> props[0].centroid\n(22.72987986048314, 81.91228523446583)\n>>> # centroid of first labeled object\n>>> props[0]['centroid']\n(22.72987986048314, 81.91228523446583)\n Add custom measurements by passing functions as extra_properties >>> from skimage import data, util\n>>> from skimage.measure import label, regionprops\n>>> import numpy as np\n>>> img = util.img_as_ubyte(data.coins()) > 110\n>>> label_img = label(img, connectivity=img.ndim)\n>>> def pixelcount(regionmask):\n...     return np.sum(regionmask)\n>>> props = regionprops(label_img, extra_properties=(pixelcount,))\n>>> props[0].pixelcount\n7741\n>>> props[1]['pixelcount']\n42\n \n Examples using skimage.measure.regionprops\n \n  Measure region properties   regionprops_table  \nskimage.measure.regionprops_table(label_image, intensity_image=None, properties=('label', 'bbox'), *, cache=True, separator='-', extra_properties=None) [source]\n \nCompute image properties and return them as a pandas-compatible table. The table is a dictionary mapping column names to value arrays. See Notes section below for details.  New in version 0.16.   Parameters \n \nlabel_image(N, M[, P]) ndarray \n\nLabeled input image. Labels with value 0 are ignored.  \nintensity_image(M, N[, P][, C]) ndarray, optional \n\nIntensity (i.e., input) image with same size as labeled image, plus optionally an extra dimension for multichannel data. Default is None.  Changed in version 0.18.0: The ability to provide an extra dimension for channels was added.   \npropertiestuple or list of str, optional \n\nProperties that will be included in the resulting dictionary For a list of available properties, please see regionprops(). Users should remember to add \u201clabel\u201d to keep track of region identities.  \ncachebool, optional \n\nDetermine whether to cache calculated properties. The computation is much faster for cached properties, whereas the memory consumption increases.  \nseparatorstr, optional \n\nFor non-scalar properties not listed in OBJECT_COLUMNS, each element will appear in its own column, with the index of that element separated from the property name by this separator. For example, the inertia tensor of a 2D region will appear in four columns: inertia_tensor-0-0, inertia_tensor-0-1, inertia_tensor-1-0, and inertia_tensor-1-1 (where the separator is -). Object columns are those that cannot be split in this way because the number of columns would change depending on the object. For example, image and coords.  \nextra_propertiesIterable of callables \n\nAdd extra property computation functions that are not included with skimage. The name of the property is derived from the function name, the dtype is inferred by calling the function on a small sample. If the name of an extra property clashes with the name of an existing property the extra property wil not be visible and a UserWarning is issued. A property computation function must take a region mask as its first argument. If the property requires an intensity image, it must accept the intensity image as the second argument.    Returns \n \nout_dictdict \n\nDictionary mapping property names to an array of values of that property, one value per region. This dictionary can be used as input to pandas DataFrame to map property names to columns in the frame and regions to rows. If the image has no regions, the arrays will have length 0, but the correct type.     Notes Each column contains either a scalar property, an object property, or an element in a multidimensional array. Properties with scalar values for each region, such as \u201ceccentricity\u201d, will appear as a float or int array with that property name as key. Multidimensional properties of fixed size for a given image dimension, such as \u201ccentroid\u201d (every centroid will have three elements in a 3D image, no matter the region size), will be split into that many columns, with the name {property_name}{separator}{element_num} (for 1D properties), {property_name}{separator}{elem_num0}{separator}{elem_num1} (for 2D properties), and so on. For multidimensional properties that don\u2019t have a fixed size, such as \u201cimage\u201d (the image of a region varies in size depending on the region size), an object array will be used, with the corresponding property name as the key. Examples >>> from skimage import data, util, measure\n>>> image = data.coins()\n>>> label_image = measure.label(image > 110, connectivity=image.ndim)\n>>> props = measure.regionprops_table(label_image, image,\n...                           properties=['label', 'inertia_tensor',\n...                                       'inertia_tensor_eigvals'])\n>>> props  \n{'label': array([ 1,  2, ...]), ...\n 'inertia_tensor-0-0': array([  4.012...e+03,   8.51..., ...]), ...\n ...,\n 'inertia_tensor_eigvals-1': array([  2.67...e+02,   2.83..., ...])}\n The resulting dictionary can be directly passed to pandas, if installed, to obtain a clean DataFrame: >>> import pandas as pd  \n>>> data = pd.DataFrame(props)  \n>>> data.head()  \n   label  inertia_tensor-0-0  ...  inertia_tensor_eigvals-1\n0      1         4012.909888  ...                267.065503\n1      2            8.514739  ...                  2.834806\n2      3            0.666667  ...                  0.000000\n3      4            0.000000  ...                  0.000000\n4      5            0.222222  ...                  0.111111\n [5 rows x 7 columns] If we want to measure a feature that does not come as a built-in property, we can define custom functions and pass them as extra_properties. For example, we can create a custom function that measures the intensity quartiles in a region: >>> from skimage import data, util, measure\n>>> import numpy as np\n>>> def quartiles(regionmask, intensity):\n...     return np.percentile(intensity[regionmask], q=(25, 50, 75))\n>>>\n>>> image = data.coins()\n>>> label_image = measure.label(image > 110, connectivity=image.ndim)\n>>> props = measure.regionprops_table(label_image, intensity_image=image,\n...                                   properties=('label',),\n...                                   extra_properties=(quartiles,))\n>>> import pandas as pd \n>>> pd.DataFrame(props).head() \n       label  quartiles-0  quartiles-1  quartiles-2\n0      1       117.00        123.0        130.0\n1      2       111.25        112.0        114.0\n2      3       111.00        111.0        111.0\n3      4       111.00        111.5        112.5\n4      5       112.50        113.0        114.0\n \n Examples using skimage.measure.regionprops_table\n \n  Measure region properties   shannon_entropy  \nskimage.measure.shannon_entropy(image, base=2) [source]\n \nCalculate the Shannon entropy of an image. The Shannon entropy is defined as S = -sum(pk * log(pk)), where pk are frequency/probability of pixels of value k.  Parameters \n \nimage(N, M) ndarray \n\nGrayscale input image.  \nbasefloat, optional \n\nThe logarithmic base to use.    Returns \n \nentropyfloat \n   Notes The returned value is measured in bits or shannon (Sh) for base=2, natural unit (nat) for base=np.e and hartley (Hart) for base=10. References  \n1  \nhttps://en.wikipedia.org/wiki/Entropy_(information_theory)  \n2  \nhttps://en.wiktionary.org/wiki/Shannon_entropy   Examples >>> from skimage import data\n>>> from skimage.measure import shannon_entropy\n>>> shannon_entropy(data.camera())\n7.231695011055706\n \n subdivide_polygon  \nskimage.measure.subdivide_polygon(coords, degree=2, preserve_ends=False) [source]\n \nSubdivision of polygonal curves using B-Splines. Note that the resulting curve is always within the convex hull of the original polygon. Circular polygons stay closed after subdivision.  Parameters \n \ncoords(N, 2) array \n\nCoordinate array.  \ndegree{1, 2, 3, 4, 5, 6, 7}, optional \n\nDegree of B-Spline. Default is 2.  \npreserve_endsbool, optional \n\nPreserve first and last coordinate of non-circular polygon. Default is False.    Returns \n \ncoords(M, 2) array \n\nSubdivided coordinate array.     References  \n1  \nhttp://mrl.nyu.edu/publications/subdiv-course2000/coursenotes00.pdf   \n CircleModel  \nclass skimage.measure.CircleModel [source]\n \nBases: skimage.measure.fit.BaseModel Total least squares estimator for 2D circles. The functional model of the circle is: r**2 = (x - xc)**2 + (y - yc)**2\n This estimator minimizes the squared distances from all points to the circle: min{ sum((r - sqrt((x_i - xc)**2 + (y_i - yc)**2))**2) }\n A minimum number of 3 points is required to solve for the parameters. Examples >>> t = np.linspace(0, 2 * np.pi, 25)\n>>> xy = CircleModel().predict_xy(t, params=(2, 3, 4))\n>>> model = CircleModel()\n>>> model.estimate(xy)\nTrue\n>>> tuple(np.round(model.params, 5))\n(2.0, 3.0, 4.0)\n>>> res = model.residuals(xy)\n>>> np.abs(np.round(res, 9))\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0.])\n  Attributes \n \nparamstuple \n\nCircle model parameters in the following order xc, yc, r.      \n__init__() [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(data) [source]\n \nEstimate circle model from data using total least squares.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n  \npredict_xy(t, params=None) [source]\n \nPredict x- and y-coordinates using the estimated model.  Parameters \n \ntarray \n\nAngles in circle in radians. Angles start to count from positive x-axis to positive y-axis in a right-handed system.  \nparams(3, ) array, optional \n\nOptional custom parameter set.    Returns \n \nxy(\u2026, 2) array \n\nPredicted x- and y-coordinates.     \n  \nresiduals(data) [source]\n \nDetermine residuals of data to model. For each point the shortest distance to the circle is returned.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nresiduals(N, ) array \n\nResidual for each data point.     \n \n EllipseModel  \nclass skimage.measure.EllipseModel [source]\n \nBases: skimage.measure.fit.BaseModel Total least squares estimator for 2D ellipses. The functional model of the ellipse is: xt = xc + a*cos(theta)*cos(t) - b*sin(theta)*sin(t)\nyt = yc + a*sin(theta)*cos(t) + b*cos(theta)*sin(t)\nd = sqrt((x - xt)**2 + (y - yt)**2)\n where (xt, yt) is the closest point on the ellipse to (x, y). Thus d is the shortest distance from the point to the ellipse. The estimator is based on a least squares minimization. The optimal solution is computed directly, no iterations are required. This leads to a simple, stable and robust fitting method. The params attribute contains the parameters in the following order: xc, yc, a, b, theta\n Examples >>> xy = EllipseModel().predict_xy(np.linspace(0, 2 * np.pi, 25),\n...                                params=(10, 15, 4, 8, np.deg2rad(30)))\n>>> ellipse = EllipseModel()\n>>> ellipse.estimate(xy)\nTrue\n>>> np.round(ellipse.params, 2)\narray([10.  , 15.  ,  4.  ,  8.  ,  0.52])\n>>> np.round(abs(ellipse.residuals(xy)), 5)\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0.])\n  Attributes \n \nparamstuple \n\nEllipse model parameters in the following order xc, yc, a, b, theta.      \n__init__() [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(data) [source]\n \nEstimate circle model from data using total least squares.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     References  \n1  \nHalir, R.; Flusser, J. \u201cNumerically stable direct least squares fitting of ellipses\u201d. In Proc. 6th International Conference in Central Europe on Computer Graphics and Visualization. WSCG (Vol. 98, pp. 125-132).   \n  \npredict_xy(t, params=None) [source]\n \nPredict x- and y-coordinates using the estimated model.  Parameters \n \ntarray \n\nAngles in circle in radians. Angles start to count from positive x-axis to positive y-axis in a right-handed system.  \nparams(5, ) array, optional \n\nOptional custom parameter set.    Returns \n \nxy(\u2026, 2) array \n\nPredicted x- and y-coordinates.     \n  \nresiduals(data) [source]\n \nDetermine residuals of data to model. For each point the shortest distance to the ellipse is returned.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nresiduals(N, ) array \n\nResidual for each data point.     \n \n LineModelND  \nclass skimage.measure.LineModelND [source]\n \nBases: skimage.measure.fit.BaseModel Total least squares estimator for N-dimensional lines. In contrast to ordinary least squares line estimation, this estimator minimizes the orthogonal distances of points to the estimated line. Lines are defined by a point (origin) and a unit vector (direction) according to the following vector equation: X = origin + lambda * direction\n Examples >>> x = np.linspace(1, 2, 25)\n>>> y = 1.5 * x + 3\n>>> lm = LineModelND()\n>>> lm.estimate(np.stack([x, y], axis=-1))\nTrue\n>>> tuple(np.round(lm.params, 5))\n(array([1.5 , 5.25]), array([0.5547 , 0.83205]))\n>>> res = lm.residuals(np.stack([x, y], axis=-1))\n>>> np.abs(np.round(res, 9))\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0.])\n>>> np.round(lm.predict_y(x[:5]), 3)\narray([4.5  , 4.562, 4.625, 4.688, 4.75 ])\n>>> np.round(lm.predict_x(y[:5]), 3)\narray([1.   , 1.042, 1.083, 1.125, 1.167])\n  Attributes \n \nparamstuple \n\nLine model parameters in the following order origin, direction.      \n__init__() [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(data) [source]\n \nEstimate line model from data. This minimizes the sum of shortest (orthogonal) distances from the given data points to the estimated line.  Parameters \n \ndata(N, dim) array \n\nN points in a space of dimensionality dim >= 2.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n  \npredict(x, axis=0, params=None) [source]\n \nPredict intersection of the estimated line model with a hyperplane orthogonal to a given axis.  Parameters \n \nx(n, 1) array \n\nCoordinates along an axis.  \naxisint \n\nAxis orthogonal to the hyperplane intersecting the line.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \ndata(n, m) array \n\nPredicted coordinates.    Raises \n ValueError\n\nIf the line is parallel to the given axis.     \n  \npredict_x(y, params=None) [source]\n \nPredict x-coordinates for 2D lines using the estimated model. Alias for: predict(y, axis=1)[:, 0]\n  Parameters \n \nyarray \n\ny-coordinates.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \nxarray \n\nPredicted x-coordinates.     \n  \npredict_y(x, params=None) [source]\n \nPredict y-coordinates for 2D lines using the estimated model. Alias for: predict(x, axis=0)[:, 1]\n  Parameters \n \nxarray \n\nx-coordinates.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \nyarray \n\nPredicted y-coordinates.     \n  \nresiduals(data, params=None) [source]\n \nDetermine residuals of data to model. For each point, the shortest (orthogonal) distance to the line is returned. It is obtained by projecting the data onto the line.  Parameters \n \ndata(N, dim) array \n\nN points in a space of dimension dim.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \nresiduals(N, ) array \n\nResidual for each data point.     \n \n\n"}, {"name": "measure.approximate_polygon()", "path": "api/skimage.measure#skimage.measure.approximate_polygon", "type": "measure", "text": " \nskimage.measure.approximate_polygon(coords, tolerance) [source]\n \nApproximate a polygonal chain with the specified tolerance. It is based on the Douglas-Peucker algorithm. Note that the approximated polygon is always within the convex hull of the original polygon.  Parameters \n \ncoords(N, 2) array \n\nCoordinate array.  \ntolerancefloat \n\nMaximum distance from original points of polygon to approximated polygonal chain. If tolerance is 0, the original coordinate array is returned.    Returns \n \ncoords(M, 2) array \n\nApproximated polygonal chain where M <= N.     References  \n1  \nhttps://en.wikipedia.org/wiki/Ramer-Douglas-Peucker_algorithm   \n"}, {"name": "measure.block_reduce()", "path": "api/skimage.measure#skimage.measure.block_reduce", "type": "measure", "text": " \nskimage.measure.block_reduce(image, block_size, func=<function sum>, cval=0, func_kwargs=None) [source]\n \nDownsample image by applying function func to local blocks. This function is useful for max and mean pooling, for example.  Parameters \n \nimagendarray \n\nN-dimensional input image.  \nblock_sizearray_like \n\nArray containing down-sampling integer factor along each axis.  \nfunccallable \n\nFunction object which is used to calculate the return value for each local block. This function must implement an axis parameter. Primary functions are numpy.sum, numpy.min, numpy.max, numpy.mean and numpy.median. See also func_kwargs.  \ncvalfloat \n\nConstant padding value if image is not perfectly divisible by the block size.  \nfunc_kwargsdict \n\nKeyword arguments passed to func. Notably useful for passing dtype argument to np.mean. Takes dictionary of inputs, e.g.: func_kwargs={'dtype': np.float16}).    Returns \n \nimagendarray \n\nDown-sampled image with same number of dimensions as input image.     Examples >>> from skimage.measure import block_reduce\n>>> image = np.arange(3*3*4).reshape(3, 3, 4)\n>>> image \narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]],\n       [[24, 25, 26, 27],\n        [28, 29, 30, 31],\n        [32, 33, 34, 35]]])\n>>> block_reduce(image, block_size=(3, 3, 1), func=np.mean)\narray([[[16., 17., 18., 19.]]])\n>>> image_max1 = block_reduce(image, block_size=(1, 3, 4), func=np.max)\n>>> image_max1 \narray([[[11]],\n       [[23]],\n       [[35]]])\n>>> image_max2 = block_reduce(image, block_size=(3, 1, 4), func=np.max)\n>>> image_max2 \narray([[[27],\n        [31],\n        [35]]])\n \n"}, {"name": "measure.CircleModel", "path": "api/skimage.measure#skimage.measure.CircleModel", "type": "measure", "text": " \nclass skimage.measure.CircleModel [source]\n \nBases: skimage.measure.fit.BaseModel Total least squares estimator for 2D circles. The functional model of the circle is: r**2 = (x - xc)**2 + (y - yc)**2\n This estimator minimizes the squared distances from all points to the circle: min{ sum((r - sqrt((x_i - xc)**2 + (y_i - yc)**2))**2) }\n A minimum number of 3 points is required to solve for the parameters. Examples >>> t = np.linspace(0, 2 * np.pi, 25)\n>>> xy = CircleModel().predict_xy(t, params=(2, 3, 4))\n>>> model = CircleModel()\n>>> model.estimate(xy)\nTrue\n>>> tuple(np.round(model.params, 5))\n(2.0, 3.0, 4.0)\n>>> res = model.residuals(xy)\n>>> np.abs(np.round(res, 9))\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0.])\n  Attributes \n \nparamstuple \n\nCircle model parameters in the following order xc, yc, r.      \n__init__() [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(data) [source]\n \nEstimate circle model from data using total least squares.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n  \npredict_xy(t, params=None) [source]\n \nPredict x- and y-coordinates using the estimated model.  Parameters \n \ntarray \n\nAngles in circle in radians. Angles start to count from positive x-axis to positive y-axis in a right-handed system.  \nparams(3, ) array, optional \n\nOptional custom parameter set.    Returns \n \nxy(\u2026, 2) array \n\nPredicted x- and y-coordinates.     \n  \nresiduals(data) [source]\n \nDetermine residuals of data to model. For each point the shortest distance to the circle is returned.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nresiduals(N, ) array \n\nResidual for each data point.     \n \n"}, {"name": "measure.CircleModel.estimate()", "path": "api/skimage.measure#skimage.measure.CircleModel.estimate", "type": "measure", "text": " \nestimate(data) [source]\n \nEstimate circle model from data using total least squares.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n"}, {"name": "measure.CircleModel.predict_xy()", "path": "api/skimage.measure#skimage.measure.CircleModel.predict_xy", "type": "measure", "text": " \npredict_xy(t, params=None) [source]\n \nPredict x- and y-coordinates using the estimated model.  Parameters \n \ntarray \n\nAngles in circle in radians. Angles start to count from positive x-axis to positive y-axis in a right-handed system.  \nparams(3, ) array, optional \n\nOptional custom parameter set.    Returns \n \nxy(\u2026, 2) array \n\nPredicted x- and y-coordinates.     \n"}, {"name": "measure.CircleModel.residuals()", "path": "api/skimage.measure#skimage.measure.CircleModel.residuals", "type": "measure", "text": " \nresiduals(data) [source]\n \nDetermine residuals of data to model. For each point the shortest distance to the circle is returned.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nresiduals(N, ) array \n\nResidual for each data point.     \n"}, {"name": "measure.CircleModel.__init__()", "path": "api/skimage.measure#skimage.measure.CircleModel.__init__", "type": "measure", "text": " \n__init__() [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "measure.EllipseModel", "path": "api/skimage.measure#skimage.measure.EllipseModel", "type": "measure", "text": " \nclass skimage.measure.EllipseModel [source]\n \nBases: skimage.measure.fit.BaseModel Total least squares estimator for 2D ellipses. The functional model of the ellipse is: xt = xc + a*cos(theta)*cos(t) - b*sin(theta)*sin(t)\nyt = yc + a*sin(theta)*cos(t) + b*cos(theta)*sin(t)\nd = sqrt((x - xt)**2 + (y - yt)**2)\n where (xt, yt) is the closest point on the ellipse to (x, y). Thus d is the shortest distance from the point to the ellipse. The estimator is based on a least squares minimization. The optimal solution is computed directly, no iterations are required. This leads to a simple, stable and robust fitting method. The params attribute contains the parameters in the following order: xc, yc, a, b, theta\n Examples >>> xy = EllipseModel().predict_xy(np.linspace(0, 2 * np.pi, 25),\n...                                params=(10, 15, 4, 8, np.deg2rad(30)))\n>>> ellipse = EllipseModel()\n>>> ellipse.estimate(xy)\nTrue\n>>> np.round(ellipse.params, 2)\narray([10.  , 15.  ,  4.  ,  8.  ,  0.52])\n>>> np.round(abs(ellipse.residuals(xy)), 5)\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0.])\n  Attributes \n \nparamstuple \n\nEllipse model parameters in the following order xc, yc, a, b, theta.      \n__init__() [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(data) [source]\n \nEstimate circle model from data using total least squares.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     References  \n1  \nHalir, R.; Flusser, J. \u201cNumerically stable direct least squares fitting of ellipses\u201d. In Proc. 6th International Conference in Central Europe on Computer Graphics and Visualization. WSCG (Vol. 98, pp. 125-132).   \n  \npredict_xy(t, params=None) [source]\n \nPredict x- and y-coordinates using the estimated model.  Parameters \n \ntarray \n\nAngles in circle in radians. Angles start to count from positive x-axis to positive y-axis in a right-handed system.  \nparams(5, ) array, optional \n\nOptional custom parameter set.    Returns \n \nxy(\u2026, 2) array \n\nPredicted x- and y-coordinates.     \n  \nresiduals(data) [source]\n \nDetermine residuals of data to model. For each point the shortest distance to the ellipse is returned.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nresiduals(N, ) array \n\nResidual for each data point.     \n \n"}, {"name": "measure.EllipseModel.estimate()", "path": "api/skimage.measure#skimage.measure.EllipseModel.estimate", "type": "measure", "text": " \nestimate(data) [source]\n \nEstimate circle model from data using total least squares.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     References  \n1  \nHalir, R.; Flusser, J. \u201cNumerically stable direct least squares fitting of ellipses\u201d. In Proc. 6th International Conference in Central Europe on Computer Graphics and Visualization. WSCG (Vol. 98, pp. 125-132).   \n"}, {"name": "measure.EllipseModel.predict_xy()", "path": "api/skimage.measure#skimage.measure.EllipseModel.predict_xy", "type": "measure", "text": " \npredict_xy(t, params=None) [source]\n \nPredict x- and y-coordinates using the estimated model.  Parameters \n \ntarray \n\nAngles in circle in radians. Angles start to count from positive x-axis to positive y-axis in a right-handed system.  \nparams(5, ) array, optional \n\nOptional custom parameter set.    Returns \n \nxy(\u2026, 2) array \n\nPredicted x- and y-coordinates.     \n"}, {"name": "measure.EllipseModel.residuals()", "path": "api/skimage.measure#skimage.measure.EllipseModel.residuals", "type": "measure", "text": " \nresiduals(data) [source]\n \nDetermine residuals of data to model. For each point the shortest distance to the ellipse is returned.  Parameters \n \ndata(N, 2) array \n\nN points with (x, y) coordinates, respectively.    Returns \n \nresiduals(N, ) array \n\nResidual for each data point.     \n"}, {"name": "measure.EllipseModel.__init__()", "path": "api/skimage.measure#skimage.measure.EllipseModel.__init__", "type": "measure", "text": " \n__init__() [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "measure.euler_number()", "path": "api/skimage.measure#skimage.measure.euler_number", "type": "measure", "text": " \nskimage.measure.euler_number(image, connectivity=None) [source]\n \nCalculate the Euler characteristic in binary image. For 2D objects, the Euler number is the number of objects minus the number of holes. For 3D objects, the Euler number is obtained as the number of objects plus the number of holes, minus the number of tunnels, or loops.  Parameters \n image: (N, M) ndarray or (N, M, D) ndarray.\n\n2D or 3D images. If image is not binary, all values strictly greater than zero are considered as the object.  \nconnectivityint, optional \n\nMaximum number of orthogonal hops to consider a pixel/voxel as a neighbor. Accepted values are ranging from 1 to input.ndim. If None, a full connectivity of input.ndim is used. 4 or 8 neighborhoods are defined for 2D images (connectivity 1 and 2, respectively). 6 or 26 neighborhoods are defined for 3D images, (connectivity 1 and 3, respectively). Connectivity 2 is not defined.    Returns \n \neuler_numberint \n\nEuler characteristic of the set of all objects in the image.     Notes The Euler characteristic is an integer number that describes the topology of the set of all objects in the input image. If object is 4-connected, then background is 8-connected, and conversely. The computation of the Euler characteristic is based on an integral geometry formula in discretized space. In practice, a neighbourhood configuration is constructed, and a LUT is applied for each configuration. The coefficients used are the ones of Ohser et al. It can be useful to compute the Euler characteristic for several connectivities. A large relative difference between results for different connectivities suggests that the image resolution (with respect to the size of objects and holes) is too low. References  \n1  \nS. Rivollier. Analyse d\u2019image geometrique et morphometrique par diagrammes de forme et voisinages adaptatifs generaux. PhD thesis, 2010. Ecole Nationale Superieure des Mines de Saint-Etienne. https://tel.archives-ouvertes.fr/tel-00560838  \n2  \nOhser J., Nagel W., Schladitz K. (2002) The Euler Number of Discretized Sets - On the Choice of Adjacency in Homogeneous Lattices. In: Mecke K., Stoyan D. (eds) Morphology of Condensed Matter. Lecture Notes in Physics, vol 600. Springer, Berlin, Heidelberg.   Examples >>> import numpy as np\n>>> SAMPLE = np.zeros((100,100,100));\n>>> SAMPLE[40:60, 40:60, 40:60]=1\n>>> euler_number(SAMPLE) \n1...\n>>> SAMPLE[45:55,45:55,45:55] = 0;\n>>> euler_number(SAMPLE) \n2...\n>>> SAMPLE = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n...                    [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n...                    [1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0],\n...                    [0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1],\n...                    [0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]])\n>>> euler_number(SAMPLE)  # doctest:\n0\n>>> euler_number(SAMPLE, connectivity=1)  # doctest:\n2\n \n"}, {"name": "measure.find_contours()", "path": "api/skimage.measure#skimage.measure.find_contours", "type": "measure", "text": " \nskimage.measure.find_contours(image, level=None, fully_connected='low', positive_orientation='low', *, mask=None) [source]\n \nFind iso-valued contours in a 2D array for a given level value. Uses the \u201cmarching squares\u201d method to compute a the iso-valued contours of the input 2D array for a particular level value. Array values are linearly interpolated to provide better precision for the output contours.  Parameters \n \nimage2D ndarray of double \n\nInput image in which to find contours.  \nlevelfloat, optional \n\nValue along which to find contours in the array. By default, the level is set to (max(image) + min(image)) / 2  Changed in version 0.18: This parameter is now optional.   \nfully_connectedstr, {\u2018low\u2019, \u2018high\u2019} \n\nIndicates whether array elements below the given level value are to be considered fully-connected (and hence elements above the value will only be face connected), or vice-versa. (See notes below for details.)  \npositive_orientationstr, {\u2018low\u2019, \u2018high\u2019} \n\nIndicates whether the output contours will produce positively-oriented polygons around islands of low- or high-valued elements. If \u2018low\u2019 then contours will wind counter- clockwise around elements below the iso-value. Alternately, this means that low-valued elements are always on the left of the contour. (See below for details.)  \nmask2D ndarray of bool, or None \n\nA boolean mask, True where we want to draw contours. Note that NaN values are always excluded from the considered region (mask is set to False wherever array is NaN).    Returns \n \ncontourslist of (n,2)-ndarrays \n\nEach contour is an ndarray of shape (n, 2), consisting of n (row, column) coordinates along the contour.      See also  \nskimage.measure.marching_cubes\n\n  Notes The marching squares algorithm is a special case of the marching cubes algorithm [1]. A simple explanation is available here: http://users.polytech.unice.fr/~lingrand/MarchingCubes/algo.html There is a single ambiguous case in the marching squares algorithm: when a given 2 x 2-element square has two high-valued and two low-valued elements, each pair diagonally adjacent. (Where high- and low-valued is with respect to the contour value sought.) In this case, either the high-valued elements can be \u2018connected together\u2019 via a thin isthmus that separates the low-valued elements, or vice-versa. When elements are connected together across a diagonal, they are considered \u2018fully connected\u2019 (also known as \u2018face+vertex-connected\u2019 or \u20188-connected\u2019). Only high-valued or low-valued elements can be fully-connected, the other set will be considered as \u2018face-connected\u2019 or \u20184-connected\u2019. By default, low-valued elements are considered fully-connected; this can be altered with the \u2018fully_connected\u2019 parameter. Output contours are not guaranteed to be closed: contours which intersect the array edge or a masked-off region (either where mask is False or where array is NaN) will be left open. All other contours will be closed. (The closed-ness of a contours can be tested by checking whether the beginning point is the same as the end point.) Contours are oriented. By default, array values lower than the contour value are to the left of the contour and values greater than the contour value are to the right. This means that contours will wind counter-clockwise (i.e. in \u2018positive orientation\u2019) around islands of low-valued pixels. This behavior can be altered with the \u2018positive_orientation\u2019 parameter. The order of the contours in the output list is determined by the position of the smallest x,y (in lexicographical order) coordinate in the contour. This is a side-effect of how the input array is traversed, but can be relied upon.  Warning Array coordinates/values are assumed to refer to the center of the array element. Take a simple example input: [0, 1]. The interpolated position of 0.5 in this array is midway between the 0-element (at x=0) and the 1-element (at x=1), and thus would fall at x=0.5.  This means that to find reasonable contours, it is best to find contours midway between the expected \u201clight\u201d and \u201cdark\u201d values. In particular, given a binarized array, do not choose to find contours at the low or high value of the array. This will often yield degenerate contours, especially around structures that are a single array element wide. Instead choose a middle value, as above. References  \n1  \nLorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings) 21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422   Examples >>> a = np.zeros((3, 3))\n>>> a[0, 0] = 1\n>>> a\narray([[1., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n>>> find_contours(a, 0.5)\n[array([[0. , 0.5],\n       [0.5, 0. ]])]\n \n"}, {"name": "measure.grid_points_in_poly()", "path": "api/skimage.measure#skimage.measure.grid_points_in_poly", "type": "measure", "text": " \nskimage.measure.grid_points_in_poly(shape, verts) [source]\n \nTest whether points on a specified grid are inside a polygon. For each (r, c) coordinate on a grid, i.e. (0, 0), (0, 1) etc., test whether that point lies inside a polygon.  Parameters \n \nshapetuple (M, N) \n\nShape of the grid.  \nverts(V, 2) array \n\nSpecify the V vertices of the polygon, sorted either clockwise or anti-clockwise. The first point may (but does not need to be) duplicated.    Returns \n \nmask(M, N) ndarray of bool \n\nTrue where the grid falls inside the polygon.      See also  \npoints_in_poly\n\n  \n"}, {"name": "measure.inertia_tensor()", "path": "api/skimage.measure#skimage.measure.inertia_tensor", "type": "measure", "text": " \nskimage.measure.inertia_tensor(image, mu=None) [source]\n \nCompute the inertia tensor of the input image.  Parameters \n \nimagearray \n\nThe input image.  \nmuarray, optional \n\nThe pre-computed central moments of image. The inertia tensor computation requires the central moments of the image. If an application requires both the central moments and the inertia tensor (for example, skimage.measure.regionprops), then it is more efficient to pre-compute them and pass them to the inertia tensor call.    Returns \n \nTarray, shape (image.ndim, image.ndim) \n\nThe inertia tensor of the input image. \\(T_{i, j}\\) contains the covariance of image intensity along axes \\(i\\) and \\(j\\).     References  \n1  \nhttps://en.wikipedia.org/wiki/Moment_of_inertia#Inertia_tensor  \n2  \nBernd J\u00e4hne. Spatio-Temporal Image Processing: Theory and Scientific Applications. (Chapter 8: Tensor Methods) Springer, 1993.   \n"}, {"name": "measure.inertia_tensor_eigvals()", "path": "api/skimage.measure#skimage.measure.inertia_tensor_eigvals", "type": "measure", "text": " \nskimage.measure.inertia_tensor_eigvals(image, mu=None, T=None) [source]\n \nCompute the eigenvalues of the inertia tensor of the image. The inertia tensor measures covariance of the image intensity along the image axes. (See inertia_tensor.) The relative magnitude of the eigenvalues of the tensor is thus a measure of the elongation of a (bright) object in the image.  Parameters \n \nimagearray \n\nThe input image.  \nmuarray, optional \n\nThe pre-computed central moments of image.  \nTarray, shape (image.ndim, image.ndim) \n\nThe pre-computed inertia tensor. If T is given, mu and image are ignored.    Returns \n \neigvalslist of float, length image.ndim \n\nThe eigenvalues of the inertia tensor of image, in descending order.     Notes Computing the eigenvalues requires the inertia tensor of the input image. This is much faster if the central moments (mu) are provided, or, alternatively, one can provide the inertia tensor (T) directly. \n"}, {"name": "measure.label()", "path": "api/skimage.measure#skimage.measure.label", "type": "measure", "text": " \nskimage.measure.label(input, background=None, return_num=False, connectivity=None) [source]\n \nLabel connected regions of an integer array. Two pixels are connected when they are neighbors and have the same value. In 2D, they can be neighbors either in a 1- or 2-connected sense. The value refers to the maximum number of orthogonal hops to consider a pixel/voxel a neighbor: 1-connectivity     2-connectivity     diagonal connection close-up\n\n     [ ]           [ ]  [ ]  [ ]             [ ]\n      |               \\  |  /                 |  <- hop 2\n[ ]--[x]--[ ]      [ ]--[x]--[ ]        [x]--[ ]\n      |               /  |  \\             hop 1\n     [ ]           [ ]  [ ]  [ ]\n  Parameters \n \ninputndarray of dtype int \n\nImage to label.  \nbackgroundint, optional \n\nConsider all pixels with this value as background pixels, and label them as 0. By default, 0-valued pixels are considered as background pixels.  \nreturn_numbool, optional \n\nWhether to return the number of assigned labels.  \nconnectivityint, optional \n\nMaximum number of orthogonal hops to consider a pixel/voxel as a neighbor. Accepted values are ranging from 1 to input.ndim. If None, a full connectivity of input.ndim is used.    Returns \n \nlabelsndarray of dtype int \n\nLabeled array, where all connected regions are assigned the same integer value.  \nnumint, optional \n\nNumber of labels, which equals the maximum label index and is only returned if return_num is True.      See also  \nregionprops\n\n\nregionprops_table\n\n  References  \n1  \nChristophe Fiorio and Jens Gustedt, \u201cTwo linear time Union-Find strategies for image processing\u201d, Theoretical Computer Science 154 (1996), pp. 165-181.  \n2  \nKensheng Wu, Ekow Otoo and Arie Shoshani, \u201cOptimizing connected component labeling algorithms\u201d, Paper LBNL-56864, 2005, Lawrence Berkeley National Laboratory (University of California), http://repositories.cdlib.org/lbnl/LBNL-56864   Examples >>> import numpy as np\n>>> x = np.eye(3).astype(int)\n>>> print(x)\n[[1 0 0]\n [0 1 0]\n [0 0 1]]\n>>> print(label(x, connectivity=1))\n[[1 0 0]\n [0 2 0]\n [0 0 3]]\n>>> print(label(x, connectivity=2))\n[[1 0 0]\n [0 1 0]\n [0 0 1]]\n>>> print(label(x, background=-1))\n[[1 2 2]\n [2 1 2]\n [2 2 1]]\n>>> x = np.array([[1, 0, 0],\n...               [1, 1, 5],\n...               [0, 0, 0]])\n>>> print(label(x))\n[[1 0 0]\n [1 1 2]\n [0 0 0]]\n \n"}, {"name": "measure.LineModelND", "path": "api/skimage.measure#skimage.measure.LineModelND", "type": "measure", "text": " \nclass skimage.measure.LineModelND [source]\n \nBases: skimage.measure.fit.BaseModel Total least squares estimator for N-dimensional lines. In contrast to ordinary least squares line estimation, this estimator minimizes the orthogonal distances of points to the estimated line. Lines are defined by a point (origin) and a unit vector (direction) according to the following vector equation: X = origin + lambda * direction\n Examples >>> x = np.linspace(1, 2, 25)\n>>> y = 1.5 * x + 3\n>>> lm = LineModelND()\n>>> lm.estimate(np.stack([x, y], axis=-1))\nTrue\n>>> tuple(np.round(lm.params, 5))\n(array([1.5 , 5.25]), array([0.5547 , 0.83205]))\n>>> res = lm.residuals(np.stack([x, y], axis=-1))\n>>> np.abs(np.round(res, 9))\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0.])\n>>> np.round(lm.predict_y(x[:5]), 3)\narray([4.5  , 4.562, 4.625, 4.688, 4.75 ])\n>>> np.round(lm.predict_x(y[:5]), 3)\narray([1.   , 1.042, 1.083, 1.125, 1.167])\n  Attributes \n \nparamstuple \n\nLine model parameters in the following order origin, direction.      \n__init__() [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(data) [source]\n \nEstimate line model from data. This minimizes the sum of shortest (orthogonal) distances from the given data points to the estimated line.  Parameters \n \ndata(N, dim) array \n\nN points in a space of dimensionality dim >= 2.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n  \npredict(x, axis=0, params=None) [source]\n \nPredict intersection of the estimated line model with a hyperplane orthogonal to a given axis.  Parameters \n \nx(n, 1) array \n\nCoordinates along an axis.  \naxisint \n\nAxis orthogonal to the hyperplane intersecting the line.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \ndata(n, m) array \n\nPredicted coordinates.    Raises \n ValueError\n\nIf the line is parallel to the given axis.     \n  \npredict_x(y, params=None) [source]\n \nPredict x-coordinates for 2D lines using the estimated model. Alias for: predict(y, axis=1)[:, 0]\n  Parameters \n \nyarray \n\ny-coordinates.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \nxarray \n\nPredicted x-coordinates.     \n  \npredict_y(x, params=None) [source]\n \nPredict y-coordinates for 2D lines using the estimated model. Alias for: predict(x, axis=0)[:, 1]\n  Parameters \n \nxarray \n\nx-coordinates.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \nyarray \n\nPredicted y-coordinates.     \n  \nresiduals(data, params=None) [source]\n \nDetermine residuals of data to model. For each point, the shortest (orthogonal) distance to the line is returned. It is obtained by projecting the data onto the line.  Parameters \n \ndata(N, dim) array \n\nN points in a space of dimension dim.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \nresiduals(N, ) array \n\nResidual for each data point.     \n \n"}, {"name": "measure.LineModelND.estimate()", "path": "api/skimage.measure#skimage.measure.LineModelND.estimate", "type": "measure", "text": " \nestimate(data) [source]\n \nEstimate line model from data. This minimizes the sum of shortest (orthogonal) distances from the given data points to the estimated line.  Parameters \n \ndata(N, dim) array \n\nN points in a space of dimensionality dim >= 2.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n"}, {"name": "measure.LineModelND.predict()", "path": "api/skimage.measure#skimage.measure.LineModelND.predict", "type": "measure", "text": " \npredict(x, axis=0, params=None) [source]\n \nPredict intersection of the estimated line model with a hyperplane orthogonal to a given axis.  Parameters \n \nx(n, 1) array \n\nCoordinates along an axis.  \naxisint \n\nAxis orthogonal to the hyperplane intersecting the line.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \ndata(n, m) array \n\nPredicted coordinates.    Raises \n ValueError\n\nIf the line is parallel to the given axis.     \n"}, {"name": "measure.LineModelND.predict_x()", "path": "api/skimage.measure#skimage.measure.LineModelND.predict_x", "type": "measure", "text": " \npredict_x(y, params=None) [source]\n \nPredict x-coordinates for 2D lines using the estimated model. Alias for: predict(y, axis=1)[:, 0]\n  Parameters \n \nyarray \n\ny-coordinates.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \nxarray \n\nPredicted x-coordinates.     \n"}, {"name": "measure.LineModelND.predict_y()", "path": "api/skimage.measure#skimage.measure.LineModelND.predict_y", "type": "measure", "text": " \npredict_y(x, params=None) [source]\n \nPredict y-coordinates for 2D lines using the estimated model. Alias for: predict(x, axis=0)[:, 1]\n  Parameters \n \nxarray \n\nx-coordinates.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \nyarray \n\nPredicted y-coordinates.     \n"}, {"name": "measure.LineModelND.residuals()", "path": "api/skimage.measure#skimage.measure.LineModelND.residuals", "type": "measure", "text": " \nresiduals(data, params=None) [source]\n \nDetermine residuals of data to model. For each point, the shortest (orthogonal) distance to the line is returned. It is obtained by projecting the data onto the line.  Parameters \n \ndata(N, dim) array \n\nN points in a space of dimension dim.  \nparams(2, ) array, optional \n\nOptional custom parameter set in the form (origin, direction).    Returns \n \nresiduals(N, ) array \n\nResidual for each data point.     \n"}, {"name": "measure.LineModelND.__init__()", "path": "api/skimage.measure#skimage.measure.LineModelND.__init__", "type": "measure", "text": " \n__init__() [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "measure.marching_cubes()", "path": "api/skimage.measure#skimage.measure.marching_cubes", "type": "measure", "text": " \nskimage.measure.marching_cubes(volume, level=None, *, spacing=(1.0, 1.0, 1.0), gradient_direction='descent', step_size=1, allow_degenerate=True, method='lewiner', mask=None) [source]\n \nMarching cubes algorithm to find surfaces in 3d volumetric data. In contrast with Lorensen et al. approach [2], Lewiner et al. algorithm is faster, resolves ambiguities, and guarantees topologically correct results. Therefore, this algorithm generally a better choice.  Parameters \n \nvolume(M, N, P) array \n\nInput data volume to find isosurfaces. Will internally be converted to float32 if necessary.  \nlevelfloat, optional \n\nContour value to search for isosurfaces in volume. If not given or None, the average of the min and max of vol is used.  \nspacinglength-3 tuple of floats, optional \n\nVoxel spacing in spatial dimensions corresponding to numpy array indexing dimensions (M, N, P) as in volume.  \ngradient_directionstring, optional \n\nControls if the mesh was generated from an isosurface with gradient descent toward objects of interest (the default), or the opposite, considering the left-hand rule. The two options are: * descent : Object was greater than exterior * ascent : Exterior was greater than object  \nstep_sizeint, optional \n\nStep size in voxels. Default 1. Larger steps yield faster but coarser results. The result will always be topologically correct though.  \nallow_degeneratebool, optional \n\nWhether to allow degenerate (i.e. zero-area) triangles in the end-result. Default True. If False, degenerate triangles are removed, at the cost of making the algorithm slower.  method: str, optional\n\nOne of \u2018lewiner\u2019, \u2018lorensen\u2019 or \u2018_lorensen\u2019. Specify witch of Lewiner et al. or Lorensen et al. method will be used. The \u2018_lorensen\u2019 flag correspond to an old implementation that will be deprecated in version 0.19.  \nmask(M, N, P) array, optional \n\nBoolean array. The marching cube algorithm will be computed only on True elements. This will save computational time when interfaces are located within certain region of the volume M, N, P-e.g. the top half of the cube-and also allow to compute finite surfaces-i.e. open surfaces that do not end at the border of the cube.    Returns \n \nverts(V, 3) array \n\nSpatial coordinates for V unique mesh vertices. Coordinate order matches input volume (M, N, P). If allow_degenerate is set to True, then the presence of degenerate triangles in the mesh can make this array have duplicate vertices.  \nfaces(F, 3) array \n\nDefine triangular faces via referencing vertex indices from verts. This algorithm specifically outputs triangles, so each face has exactly three indices.  \nnormals(V, 3) array \n\nThe normal direction at each vertex, as calculated from the data.  \nvalues(V, ) array \n\nGives a measure for the maximum value of the data in the local region near each vertex. This can be used by visualization tools to apply a colormap to the mesh.      See also  \nskimage.measure.mesh_surface_area\n\n\nskimage.measure.find_contours\n\n  Notes The algorithm [1] is an improved version of Chernyaev\u2019s Marching Cubes 33 algorithm. It is an efficient algorithm that relies on heavy use of lookup tables to handle the many different cases, keeping the algorithm relatively easy. This implementation is written in Cython, ported from Lewiner\u2019s C++ implementation. To quantify the area of an isosurface generated by this algorithm, pass verts and faces to skimage.measure.mesh_surface_area. Regarding visualization of algorithm output, to contour a volume named myvolume about the level 0.0, using the mayavi package: >>>\n>> from mayavi import mlab\n>> verts, faces, _, _ = marching_cubes(myvolume, 0.0)\n>> mlab.triangular_mesh([vert[0] for vert in verts],\n                        [vert[1] for vert in verts],\n                        [vert[2] for vert in verts],\n                        faces)\n>> mlab.show()\n Similarly using the visvis package: >>>\n>> import visvis as vv\n>> verts, faces, normals, values = marching_cubes(myvolume, 0.0)\n>> vv.mesh(np.fliplr(verts), faces, normals, values)\n>> vv.use().Run()\n To reduce the number of triangles in the mesh for better performance, see this example using the mayavi package. References  \n1  \nThomas Lewiner, Helio Lopes, Antonio Wilson Vieira and Geovan Tavares. Efficient implementation of Marching Cubes\u2019 cases with topological guarantees. Journal of Graphics Tools 8(2) pp. 1-15 (december 2003). DOI:10.1080/10867651.2003.10487582  \n2  \nLorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings) 21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422   \n"}, {"name": "measure.marching_cubes_classic()", "path": "api/skimage.measure#skimage.measure.marching_cubes_classic", "type": "measure", "text": " \nskimage.measure.marching_cubes_classic(volume, level=None, spacing=(1.0, 1.0, 1.0), gradient_direction='descent') [source]\n \nClassic marching cubes algorithm to find surfaces in 3d volumetric data. Note that the marching_cubes() algorithm is recommended over this algorithm, because it\u2019s faster and produces better results.  Parameters \n \nvolume(M, N, P) array of doubles \n\nInput data volume to find isosurfaces. Will be cast to np.float64.  \nlevelfloat \n\nContour value to search for isosurfaces in volume. If not given or None, the average of the min and max of vol is used.  \nspacinglength-3 tuple of floats \n\nVoxel spacing in spatial dimensions corresponding to numpy array indexing dimensions (M, N, P) as in volume.  \ngradient_directionstring \n\nControls if the mesh was generated from an isosurface with gradient descent toward objects of interest (the default), or the opposite. The two options are: * descent : Object was greater than exterior * ascent : Exterior was greater than object    Returns \n \nverts(V, 3) array \n\nSpatial coordinates for V unique mesh vertices. Coordinate order matches input volume (M, N, P). If allow_degenerate is set to True, then the presence of degenerate triangles in the mesh can make this array have duplicate vertices.  \nfaces(F, 3) array \n\nDefine triangular faces via referencing vertex indices from verts. This algorithm specifically outputs triangles, so each face has exactly three indices.      See also  \nskimage.measure.marching_cubes\n\n\nskimage.measure.mesh_surface_area\n\n  Notes The marching cubes algorithm is implemented as described in [1]. A simple explanation is available here: http://users.polytech.unice.fr/~lingrand/MarchingCubes/algo.html\n There are several known ambiguous cases in the marching cubes algorithm. Using point labeling as in [1], Figure 4, as shown:     v8 ------ v7\n   / |       / |        y\n  /  |      /  |        ^  z\nv4 ------ v3   |        | /\n |  v5 ----|- v6        |/          (note: NOT right handed!)\n |  /      |  /          ----> x\n | /       | /\nv1 ------ v2\n Most notably, if v4, v8, v2, and v6 are all >= level (or any generalization of this case) two parallel planes are generated by this algorithm, separating v4 and v8 from v2 and v6. An equally valid interpretation would be a single connected thin surface enclosing all four points. This is the best known ambiguity, though there are others. This algorithm does not attempt to resolve such ambiguities; it is a naive implementation of marching cubes as in [1], but may be a good beginning for work with more recent techniques (Dual Marching Cubes, Extended Marching Cubes, Cubic Marching Squares, etc.). Because of interactions between neighboring cubes, the isosurface(s) generated by this algorithm are NOT guaranteed to be closed, particularly for complicated contours. Furthermore, this algorithm does not guarantee a single contour will be returned. Indeed, ALL isosurfaces which cross level will be found, regardless of connectivity. The output is a triangular mesh consisting of a set of unique vertices and connecting triangles. The order of these vertices and triangles in the output list is determined by the position of the smallest x,y,z (in lexicographical order) coordinate in the contour. This is a side-effect of how the input array is traversed, but can be relied upon. The generated mesh guarantees coherent orientation as of version 0.12. To quantify the area of an isosurface generated by this algorithm, pass outputs directly into skimage.measure.mesh_surface_area. References  \n1(1,2,3)  \nLorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings) 21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422   \n"}, {"name": "measure.marching_cubes_lewiner()", "path": "api/skimage.measure#skimage.measure.marching_cubes_lewiner", "type": "measure", "text": " \nskimage.measure.marching_cubes_lewiner(volume, level=None, spacing=(1.0, 1.0, 1.0), gradient_direction='descent', step_size=1, allow_degenerate=True, use_classic=False, mask=None) [source]\n \nLewiner marching cubes algorithm to find surfaces in 3d volumetric data. In contrast to marching_cubes_classic(), this algorithm is faster, resolves ambiguities, and guarantees topologically correct results. Therefore, this algorithm generally a better choice, unless there is a specific need for the classic algorithm.  Parameters \n \nvolume(M, N, P) array \n\nInput data volume to find isosurfaces. Will internally be converted to float32 if necessary.  \nlevelfloat \n\nContour value to search for isosurfaces in volume. If not given or None, the average of the min and max of vol is used.  \nspacinglength-3 tuple of floats \n\nVoxel spacing in spatial dimensions corresponding to numpy array indexing dimensions (M, N, P) as in volume.  \ngradient_directionstring \n\nControls if the mesh was generated from an isosurface with gradient descent toward objects of interest (the default), or the opposite, considering the left-hand rule. The two options are: * descent : Object was greater than exterior * ascent : Exterior was greater than object  \nstep_sizeint \n\nStep size in voxels. Default 1. Larger steps yield faster but coarser results. The result will always be topologically correct though.  \nallow_degeneratebool \n\nWhether to allow degenerate (i.e. zero-area) triangles in the end-result. Default True. If False, degenerate triangles are removed, at the cost of making the algorithm slower.  \nuse_classicbool \n\nIf given and True, the classic marching cubes by Lorensen (1987) is used. This option is included for reference purposes. Note that this algorithm has ambiguities and is not guaranteed to produce a topologically correct result. The results with using this option are not generally the same as the marching_cubes_classic() function.  \nmask(M, N, P) array \n\nBoolean array. The marching cube algorithm will be computed only on True elements. This will save computational time when interfaces are located within certain region of the volume M, N, P-e.g. the top half of the cube-and also allow to compute finite surfaces-i.e. open surfaces that do not end at the border of the cube.    Returns \n \nverts(V, 3) array \n\nSpatial coordinates for V unique mesh vertices. Coordinate order matches input volume (M, N, P). If allow_degenerate is set to True, then the presence of degenerate triangles in the mesh can make this array have duplicate vertices.  \nfaces(F, 3) array \n\nDefine triangular faces via referencing vertex indices from verts. This algorithm specifically outputs triangles, so each face has exactly three indices.  \nnormals(V, 3) array \n\nThe normal direction at each vertex, as calculated from the data.  \nvalues(V, ) array \n\nGives a measure for the maximum value of the data in the local region near each vertex. This can be used by visualization tools to apply a colormap to the mesh.      See also  \nskimage.measure.marching_cubes\n\n\nskimage.measure.mesh_surface_area\n\n  Notes The algorithm [1] is an improved version of Chernyaev\u2019s Marching Cubes 33 algorithm. It is an efficient algorithm that relies on heavy use of lookup tables to handle the many different cases, keeping the algorithm relatively easy. This implementation is written in Cython, ported from Lewiner\u2019s C++ implementation. To quantify the area of an isosurface generated by this algorithm, pass verts and faces to skimage.measure.mesh_surface_area. Regarding visualization of algorithm output, to contour a volume named myvolume about the level 0.0, using the mayavi package: >>> from mayavi import mlab \n>>> verts, faces, normals, values = marching_cubes_lewiner(myvolume, 0.0) \n>>> mlab.triangular_mesh([vert[0] for vert in verts],\n...                      [vert[1] for vert in verts],\n...                      [vert[2] for vert in verts],\n...                      faces) \n>>> mlab.show() \n Similarly using the visvis package: >>> import visvis as vv \n>>> verts, faces, normals, values = marching_cubes_lewiner(myvolume, 0.0) \n>>> vv.mesh(np.fliplr(verts), faces, normals, values) \n>>> vv.use().Run() \n References  \n1  \nThomas Lewiner, Helio Lopes, Antonio Wilson Vieira and Geovan Tavares. Efficient implementation of Marching Cubes\u2019 cases with topological guarantees. Journal of Graphics Tools 8(2) pp. 1-15 (december 2003). DOI:10.1080/10867651.2003.10487582   \n"}, {"name": "measure.mesh_surface_area()", "path": "api/skimage.measure#skimage.measure.mesh_surface_area", "type": "measure", "text": " \nskimage.measure.mesh_surface_area(verts, faces) [source]\n \nCompute surface area, given vertices & triangular faces  Parameters \n \nverts(V, 3) array of floats \n\nArray containing (x, y, z) coordinates for V unique mesh vertices.  \nfaces(F, 3) array of ints \n\nList of length-3 lists of integers, referencing vertex coordinates as provided in verts    Returns \n \nareafloat \n\nSurface area of mesh. Units now [coordinate units] ** 2.      See also  \nskimage.measure.marching_cubes\n\n\nskimage.measure.marching_cubes_classic\n\n  Notes The arguments expected by this function are the first two outputs from skimage.measure.marching_cubes. For unit correct output, ensure correct spacing was passed to skimage.measure.marching_cubes. This algorithm works properly only if the faces provided are all triangles. \n"}, {"name": "measure.moments()", "path": "api/skimage.measure#skimage.measure.moments", "type": "measure", "text": " \nskimage.measure.moments(image, order=3) [source]\n \nCalculate all raw image moments up to a certain order.  The following properties can be calculated from raw image moments:\n\n Area as: M[0, 0]. Centroid as: {M[1, 0] / M[0, 0], M[0, 1] / M[0, 0]}.    Note that raw moments are neither translation, scale nor rotation invariant.  Parameters \n \nimagenD double or uint8 array \n\nRasterized shape as image.  \norderint, optional \n\nMaximum order of moments. Default is 3.    Returns \n \nm(order + 1, order + 1) array \n\nRaw image moments.     References  \n1  \nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.  \n2  \nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.  \n3  \nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.  \n4  \nhttps://en.wikipedia.org/wiki/Image_moment   Examples >>> image = np.zeros((20, 20), dtype=np.double)\n>>> image[13:17, 13:17] = 1\n>>> M = moments(image)\n>>> centroid = (M[1, 0] / M[0, 0], M[0, 1] / M[0, 0])\n>>> centroid\n(14.5, 14.5)\n \n"}, {"name": "measure.moments_central()", "path": "api/skimage.measure#skimage.measure.moments_central", "type": "measure", "text": " \nskimage.measure.moments_central(image, center=None, order=3, **kwargs) [source]\n \nCalculate all central image moments up to a certain order. The center coordinates (cr, cc) can be calculated from the raw moments as: {M[1, 0] / M[0, 0], M[0, 1] / M[0, 0]}. Note that central moments are translation invariant but not scale and rotation invariant.  Parameters \n \nimagenD double or uint8 array \n\nRasterized shape as image.  \ncentertuple of float, optional \n\nCoordinates of the image centroid. This will be computed if it is not provided.  \norderint, optional \n\nThe maximum order of moments computed.    Returns \n \nmu(order + 1, order + 1) array \n\nCentral image moments.     References  \n1  \nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.  \n2  \nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.  \n3  \nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.  \n4  \nhttps://en.wikipedia.org/wiki/Image_moment   Examples >>> image = np.zeros((20, 20), dtype=np.double)\n>>> image[13:17, 13:17] = 1\n>>> M = moments(image)\n>>> centroid = (M[1, 0] / M[0, 0], M[0, 1] / M[0, 0])\n>>> moments_central(image, centroid)\narray([[16.,  0., 20.,  0.],\n       [ 0.,  0.,  0.,  0.],\n       [20.,  0., 25.,  0.],\n       [ 0.,  0.,  0.,  0.]])\n \n"}, {"name": "measure.moments_coords()", "path": "api/skimage.measure#skimage.measure.moments_coords", "type": "measure", "text": " \nskimage.measure.moments_coords(coords, order=3) [source]\n \nCalculate all raw image moments up to a certain order.  The following properties can be calculated from raw image moments:\n\n Area as: M[0, 0]. Centroid as: {M[1, 0] / M[0, 0], M[0, 1] / M[0, 0]}.    Note that raw moments are neither translation, scale nor rotation invariant.  Parameters \n \ncoords(N, D) double or uint8 array \n\nArray of N points that describe an image of D dimensionality in Cartesian space.  \norderint, optional \n\nMaximum order of moments. Default is 3.    Returns \n \nM(order + 1, order + 1, \u2026) array \n\nRaw image moments. (D dimensions)     References  \n1  \nJohannes Kilian. Simple Image Analysis By Moments. Durham University, version 0.2, Durham, 2001.   Examples >>> coords = np.array([[row, col]\n...                    for row in range(13, 17)\n...                    for col in range(14, 18)], dtype=np.double)\n>>> M = moments_coords(coords)\n>>> centroid = (M[1, 0] / M[0, 0], M[0, 1] / M[0, 0])\n>>> centroid\n(14.5, 15.5)\n \n"}, {"name": "measure.moments_coords_central()", "path": "api/skimage.measure#skimage.measure.moments_coords_central", "type": "measure", "text": " \nskimage.measure.moments_coords_central(coords, center=None, order=3) [source]\n \nCalculate all central image moments up to a certain order.  The following properties can be calculated from raw image moments:\n\n Area as: M[0, 0]. Centroid as: {M[1, 0] / M[0, 0], M[0, 1] / M[0, 0]}.    Note that raw moments are neither translation, scale nor rotation invariant.  Parameters \n \ncoords(N, D) double or uint8 array \n\nArray of N points that describe an image of D dimensionality in Cartesian space. A tuple of coordinates as returned by np.nonzero is also accepted as input.  \ncentertuple of float, optional \n\nCoordinates of the image centroid. This will be computed if it is not provided.  \norderint, optional \n\nMaximum order of moments. Default is 3.    Returns \n \nMc(order + 1, order + 1, \u2026) array \n\nCentral image moments. (D dimensions)     References  \n1  \nJohannes Kilian. Simple Image Analysis By Moments. Durham University, version 0.2, Durham, 2001.   Examples >>> coords = np.array([[row, col]\n...                    for row in range(13, 17)\n...                    for col in range(14, 18)])\n>>> moments_coords_central(coords)\narray([[16.,  0., 20.,  0.],\n       [ 0.,  0.,  0.,  0.],\n       [20.,  0., 25.,  0.],\n       [ 0.,  0.,  0.,  0.]])\n As seen above, for symmetric objects, odd-order moments (columns 1 and 3, rows 1 and 3) are zero when centered on the centroid, or center of mass, of the object (the default). If we break the symmetry by adding a new point, this no longer holds: >>> coords2 = np.concatenate((coords, [[17, 17]]), axis=0)\n>>> np.round(moments_coords_central(coords2),\n...          decimals=2)  \narray([[17.  ,  0.  , 22.12, -2.49],\n       [ 0.  ,  3.53,  1.73,  7.4 ],\n       [25.88,  6.02, 36.63,  8.83],\n       [ 4.15, 19.17, 14.8 , 39.6 ]])\n Image moments and central image moments are equivalent (by definition) when the center is (0, 0): >>> np.allclose(moments_coords(coords),\n...             moments_coords_central(coords, (0, 0)))\nTrue\n \n"}, {"name": "measure.moments_hu()", "path": "api/skimage.measure#skimage.measure.moments_hu", "type": "measure", "text": " \nskimage.measure.moments_hu(nu) [source]\n \nCalculate Hu\u2019s set of image moments (2D-only). Note that this set of moments is proofed to be translation, scale and rotation invariant.  Parameters \n \nnu(M, M) array \n\nNormalized central image moments, where M must be >= 4.    Returns \n \nnu(7,) array \n\nHu\u2019s set of image moments.     References  \n1  \nM. K. Hu, \u201cVisual Pattern Recognition by Moment Invariants\u201d, IRE Trans. Info. Theory, vol. IT-8, pp. 179-187, 1962  \n2  \nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.  \n3  \nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.  \n4  \nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.  \n5  \nhttps://en.wikipedia.org/wiki/Image_moment   Examples >>> image = np.zeros((20, 20), dtype=np.double)\n>>> image[13:17, 13:17] = 0.5\n>>> image[10:12, 10:12] = 1\n>>> mu = moments_central(image)\n>>> nu = moments_normalized(mu)\n>>> moments_hu(nu)\narray([7.45370370e-01, 3.51165981e-01, 1.04049179e-01, 4.06442107e-02,\n       2.64312299e-03, 2.40854582e-02, 4.33680869e-19])\n \n"}, {"name": "measure.moments_normalized()", "path": "api/skimage.measure#skimage.measure.moments_normalized", "type": "measure", "text": " \nskimage.measure.moments_normalized(mu, order=3) [source]\n \nCalculate all normalized central image moments up to a certain order. Note that normalized central moments are translation and scale invariant but not rotation invariant.  Parameters \n \nmu(M,[ \u2026,] M) array \n\nCentral image moments, where M must be greater than or equal to order.  \norderint, optional \n\nMaximum order of moments. Default is 3.    Returns \n \nnu(order + 1,[ \u2026,] order + 1) array \n\nNormalized central image moments.     References  \n1  \nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.  \n2  \nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.  \n3  \nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.  \n4  \nhttps://en.wikipedia.org/wiki/Image_moment   Examples >>> image = np.zeros((20, 20), dtype=np.double)\n>>> image[13:17, 13:17] = 1\n>>> m = moments(image)\n>>> centroid = (m[0, 1] / m[0, 0], m[1, 0] / m[0, 0])\n>>> mu = moments_central(image, centroid)\n>>> moments_normalized(mu)\narray([[       nan,        nan, 0.078125  , 0.        ],\n       [       nan, 0.        , 0.        , 0.        ],\n       [0.078125  , 0.        , 0.00610352, 0.        ],\n       [0.        , 0.        , 0.        , 0.        ]])\n \n"}, {"name": "measure.perimeter()", "path": "api/skimage.measure#skimage.measure.perimeter", "type": "measure", "text": " \nskimage.measure.perimeter(image, neighbourhood=4) [source]\n \nCalculate total perimeter of all objects in binary image.  Parameters \n \nimage(N, M) ndarray \n\n2D binary image.  \nneighbourhood4 or 8, optional \n\nNeighborhood connectivity for border pixel determination. It is used to compute the contour. A higher neighbourhood widens the border on which the perimeter is computed.    Returns \n \nperimeterfloat \n\nTotal perimeter of all objects in binary image.     References  \n1  \nK. Benkrid, D. Crookes. Design and FPGA Implementation of a Perimeter Estimator. The Queen\u2019s University of Belfast. http://www.cs.qub.ac.uk/~d.crookes/webpubs/papers/perimeter.doc   Examples >>> from skimage import data, util\n>>> from skimage.measure import label\n>>> # coins image (binary)\n>>> img_coins = data.coins() > 110\n>>> # total perimeter of all objects in the image\n>>> perimeter(img_coins, neighbourhood=4)  \n7796.867...\n>>> perimeter(img_coins, neighbourhood=8)  \n8806.268...\n \n"}, {"name": "measure.perimeter_crofton()", "path": "api/skimage.measure#skimage.measure.perimeter_crofton", "type": "measure", "text": " \nskimage.measure.perimeter_crofton(image, directions=4) [source]\n \nCalculate total Crofton perimeter of all objects in binary image.  Parameters \n \nimage(N, M) ndarray \n\n2D image. If image is not binary, all values strictly greater than zero are considered as the object.  \ndirections2 or 4, optional \n\nNumber of directions used to approximate the Crofton perimeter. By default, 4 is used: it should be more accurate than 2. Computation time is the same in both cases.    Returns \n \nperimeterfloat \n\nTotal perimeter of all objects in binary image.     Notes This measure is based on Crofton formula [1], which is a measure from integral geometry. It is defined for general curve length evaluation via a double integral along all directions. In a discrete space, 2 or 4 directions give a quite good approximation, 4 being more accurate than 2 for more complex shapes. Similar to perimeter(), this function returns an approximation of the perimeter in continuous space. References  \n1  \nhttps://en.wikipedia.org/wiki/Crofton_formula  \n2  \nS. Rivollier. Analyse d\u2019image geometrique et morphometrique par diagrammes de forme et voisinages adaptatifs generaux. PhD thesis, 2010. Ecole Nationale Superieure des Mines de Saint-Etienne. https://tel.archives-ouvertes.fr/tel-00560838   Examples >>> from skimage import data, util\n>>> from skimage.measure import label\n>>> # coins image (binary)\n>>> img_coins = data.coins() > 110\n>>> # total perimeter of all objects in the image\n>>> perimeter_crofton(img_coins, directions=2)  \n8144.578...\n>>> perimeter_crofton(img_coins, directions=4)  \n7837.077...\n \n"}, {"name": "measure.points_in_poly()", "path": "api/skimage.measure#skimage.measure.points_in_poly", "type": "measure", "text": " \nskimage.measure.points_in_poly(points, verts) [source]\n \nTest whether points lie inside a polygon.  Parameters \n \npoints(N, 2) array \n\nInput points, (x, y).  \nverts(M, 2) array \n\nVertices of the polygon, sorted either clockwise or anti-clockwise. The first point may (but does not need to be) duplicated.    Returns \n \nmask(N,) array of bool \n\nTrue if corresponding point is inside the polygon.      See also  \ngrid_points_in_poly\n\n  \n"}, {"name": "measure.profile_line()", "path": "api/skimage.measure#skimage.measure.profile_line", "type": "measure", "text": " \nskimage.measure.profile_line(image, src, dst, linewidth=1, order=None, mode=None, cval=0.0, *, reduce_func=<function mean>) [source]\n \nReturn the intensity profile of an image measured along a scan line.  Parameters \n \nimagendarray, shape (M, N[, C]) \n\nThe image, either grayscale (2D array) or multichannel (3D array, where the final axis contains the channel information).  \nsrcarray_like, shape (2, ) \n\nThe coordinates of the start point of the scan line.  \ndstarray_like, shape (2, ) \n\nThe coordinates of the end point of the scan line. The destination point is included in the profile, in contrast to standard numpy indexing.  \nlinewidthint, optional \n\nWidth of the scan, perpendicular to the line  \norderint in {0, 1, 2, 3, 4, 5}, optional \n\nThe order of the spline interpolation, default is 0 if image.dtype is bool and 1 otherwise. The order has to be in the range 0-5. See skimage.transform.warp for detail.  \nmode{\u2018constant\u2019, \u2018nearest\u2019, \u2018reflect\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional \n\nHow to compute any values falling outside of the image.  \ncvalfloat, optional \n\nIf mode is \u2018constant\u2019, what constant value to use outside the image.  \nreduce_funccallable, optional \n\nFunction used to calculate the aggregation of pixel values perpendicular to the profile_line direction when linewidth > 1. If set to None the unreduced array will be returned.    Returns \n \nreturn_valuearray \n\nThe intensity profile along the scan line. The length of the profile is the ceil of the computed length of the scan line.     Examples >>> x = np.array([[1, 1, 1, 2, 2, 2]])\n>>> img = np.vstack([np.zeros_like(x), x, x, x, np.zeros_like(x)])\n>>> img\narray([[0, 0, 0, 0, 0, 0],\n       [1, 1, 1, 2, 2, 2],\n       [1, 1, 1, 2, 2, 2],\n       [1, 1, 1, 2, 2, 2],\n       [0, 0, 0, 0, 0, 0]])\n>>> profile_line(img, (2, 1), (2, 4))\narray([1., 1., 2., 2.])\n>>> profile_line(img, (1, 0), (1, 6), cval=4)\narray([1., 1., 1., 2., 2., 2., 4.])\n The destination point is included in the profile, in contrast to standard numpy indexing. For example: >>> profile_line(img, (1, 0), (1, 6))  # The final point is out of bounds\narray([1., 1., 1., 2., 2., 2., 0.])\n>>> profile_line(img, (1, 0), (1, 5))  # This accesses the full first row\narray([1., 1., 1., 2., 2., 2.])\n For different reduce_func inputs: >>> profile_line(img, (1, 0), (1, 3), linewidth=3, reduce_func=np.mean)\narray([0.66666667, 0.66666667, 0.66666667, 1.33333333])\n>>> profile_line(img, (1, 0), (1, 3), linewidth=3, reduce_func=np.max)\narray([1, 1, 1, 2])\n>>> profile_line(img, (1, 0), (1, 3), linewidth=3, reduce_func=np.sum)\narray([2, 2, 2, 4])\n The unreduced array will be returned when reduce_func is None or when reduce_func acts on each pixel value individually. >>> profile_line(img, (1, 2), (4, 2), linewidth=3, order=0,\n...     reduce_func=None)\narray([[1, 1, 2],\n       [1, 1, 2],\n       [1, 1, 2],\n       [0, 0, 0]])\n>>> profile_line(img, (1, 0), (1, 3), linewidth=3, reduce_func=np.sqrt)\narray([[1.        , 1.        , 0.        ],\n       [1.        , 1.        , 0.        ],\n       [1.        , 1.        , 0.        ],\n       [1.41421356, 1.41421356, 0.        ]])\n \n"}, {"name": "measure.ransac()", "path": "api/skimage.measure#skimage.measure.ransac", "type": "measure", "text": " \nskimage.measure.ransac(data, model_class, min_samples, residual_threshold, is_data_valid=None, is_model_valid=None, max_trials=100, stop_sample_num=inf, stop_residuals_sum=0, stop_probability=1, random_state=None, initial_inliers=None) [source]\n \nFit a model to data with the RANSAC (random sample consensus) algorithm. RANSAC is an iterative algorithm for the robust estimation of parameters from a subset of inliers from the complete data set. Each iteration performs the following tasks:  Select min_samples random samples from the original data and check whether the set of data is valid (see is_data_valid). Estimate a model to the random subset (model_cls.estimate(*data[random_subset]) and check whether the estimated model is valid (see is_model_valid). Classify all data as inliers or outliers by calculating the residuals to the estimated model (model_cls.residuals(*data)) - all data samples with residuals smaller than the residual_threshold are considered as inliers. Save estimated model as best model if number of inlier samples is maximal. In case the current estimated model has the same number of inliers, it is only considered as the best model if it has less sum of residuals.  These steps are performed either a maximum number of times or until one of the special stop criteria are met. The final model is estimated using all inlier samples of the previously determined best model.  Parameters \n \ndata[list, tuple of] (N, \u2026) array \n\nData set to which the model is fitted, where N is the number of data points and the remaining dimension are depending on model requirements. If the model class requires multiple input data arrays (e.g. source and destination coordinates of skimage.transform.AffineTransform), they can be optionally passed as tuple or list. Note, that in this case the functions estimate(*data), residuals(*data), is_model_valid(model, *random_data) and is_data_valid(*random_data) must all take each data array as separate arguments.  \nmodel_classobject \n\nObject with the following object methods:  success = estimate(*data) residuals(*data)  where success indicates whether the model estimation succeeded (True or None for success, False for failure).  \nmin_samplesint in range (0, N) \n\nThe minimum number of data points to fit a model to.  \nresidual_thresholdfloat larger than 0 \n\nMaximum distance for a data point to be classified as an inlier.  \nis_data_validfunction, optional \n\nThis function is called with the randomly selected data before the model is fitted to it: is_data_valid(*random_data).  \nis_model_validfunction, optional \n\nThis function is called with the estimated model and the randomly selected data: is_model_valid(model, *random_data), .  \nmax_trialsint, optional \n\nMaximum number of iterations for random sample selection.  \nstop_sample_numint, optional \n\nStop iteration if at least this number of inliers are found.  \nstop_residuals_sumfloat, optional \n\nStop iteration if sum of residuals is less than or equal to this threshold.  \nstop_probabilityfloat in range [0, 1], optional \n\nRANSAC iteration stops if at least one outlier-free set of the training data is sampled with probability >= stop_probability, depending on the current best model\u2019s inlier ratio and the number of trials. This requires to generate at least N samples (trials): N >= log(1 - probability) / log(1 - e**m) where the probability (confidence) is typically set to a high value such as 0.99, e is the current fraction of inliers w.r.t. the total number of samples, and m is the min_samples value.  \nrandom_stateint, RandomState instance or None, optional \n\nIf int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.  \ninitial_inliersarray-like of bool, shape (N,), optional \n\nInitial samples selection for model estimation    Returns \n \nmodelobject \n\nBest model with largest consensus set.  \ninliers(N, ) array \n\nBoolean mask of inliers classified as True.     References  \n1  \n\u201cRANSAC\u201d, Wikipedia, https://en.wikipedia.org/wiki/RANSAC   Examples Generate ellipse data without tilt and add noise: >>> t = np.linspace(0, 2 * np.pi, 50)\n>>> xc, yc = 20, 30\n>>> a, b = 5, 10\n>>> x = xc + a * np.cos(t)\n>>> y = yc + b * np.sin(t)\n>>> data = np.column_stack([x, y])\n>>> np.random.seed(seed=1234)\n>>> data += np.random.normal(size=data.shape)\n Add some faulty data: >>> data[0] = (100, 100)\n>>> data[1] = (110, 120)\n>>> data[2] = (120, 130)\n>>> data[3] = (140, 130)\n Estimate ellipse model using all available data: >>> model = EllipseModel()\n>>> model.estimate(data)\nTrue\n>>> np.round(model.params)  \narray([ 72.,  75.,  77.,  14.,   1.])\n Estimate ellipse model using RANSAC: >>> ransac_model, inliers = ransac(data, EllipseModel, 20, 3, max_trials=50)\n>>> abs(np.round(ransac_model.params))\narray([20., 30.,  5., 10.,  0.])\n>>> inliers \narray([False, False, False, False,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True], dtype=bool)\n>>> sum(inliers) > 40\nTrue\n RANSAC can be used to robustly estimate a geometric transformation. In this section, we also show how to use a proportion of the total samples, rather than an absolute number. >>> from skimage.transform import SimilarityTransform\n>>> np.random.seed(0)\n>>> src = 100 * np.random.rand(50, 2)\n>>> model0 = SimilarityTransform(scale=0.5, rotation=1, translation=(10, 20))\n>>> dst = model0(src)\n>>> dst[0] = (10000, 10000)\n>>> dst[1] = (-100, 100)\n>>> dst[2] = (50, 50)\n>>> ratio = 0.5  # use half of the samples\n>>> min_samples = int(ratio * len(src))\n>>> model, inliers = ransac((src, dst), SimilarityTransform, min_samples, 10,\n...                         initial_inliers=np.ones(len(src), dtype=bool))\n>>> inliers\narray([False, False, False,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True])\n \n"}, {"name": "measure.regionprops()", "path": "api/skimage.measure#skimage.measure.regionprops", "type": "measure", "text": " \nskimage.measure.regionprops(label_image, intensity_image=None, cache=True, coordinates=None, *, extra_properties=None) [source]\n \nMeasure properties of labeled image regions.  Parameters \n \nlabel_image(M, N[, P]) ndarray \n\nLabeled input image. Labels with value 0 are ignored.  Changed in version 0.14.1: Previously, label_image was processed by numpy.squeeze and so any number of singleton dimensions was allowed. This resulted in inconsistent handling of images with singleton dimensions. To recover the old behaviour, use regionprops(np.squeeze(label_image), ...).   \nintensity_image(M, N[, P][, C]) ndarray, optional \n\nIntensity (i.e., input) image with same size as labeled image, plus optionally an extra dimension for multichannel data. Default is None.  Changed in version 0.18.0: The ability to provide an extra dimension for channels was added.   \ncachebool, optional \n\nDetermine whether to cache calculated properties. The computation is much faster for cached properties, whereas the memory consumption increases.  \ncoordinatesDEPRECATED \n\nThis argument is deprecated and will be removed in a future version of scikit-image. See Coordinate conventions for more details.  Deprecated since version 0.16.0: Use \u201crc\u201d coordinates everywhere. It may be sufficient to call numpy.transpose on your label image to get the same values as 0.15 and earlier. However, for some properties, the transformation will be less trivial. For example, the new orientation is \\(\\frac{\\pi}{2}\\) plus the old orientation.   \nextra_propertiesIterable of callables \n\nAdd extra property computation functions that are not included with skimage. The name of the property is derived from the function name, the dtype is inferred by calling the function on a small sample. If the name of an extra property clashes with the name of an existing property the extra property wil not be visible and a UserWarning is issued. A property computation function must take a region mask as its first argument. If the property requires an intensity image, it must accept the intensity image as the second argument.    Returns \n \npropertieslist of RegionProperties \n\nEach item describes one labeled region, and can be accessed using the attributes listed below.      See also  \nlabel\n\n  Notes The following properties can be accessed as attributes or keys:  \nareaint \n\nNumber of pixels of the region.  \nbboxtuple \n\nBounding box (min_row, min_col, max_row, max_col). Pixels belonging to the bounding box are in the half-open interval [min_row; max_row) and [min_col; max_col).  \nbbox_areaint \n\nNumber of pixels of bounding box.  \ncentroidarray \n\nCentroid coordinate tuple (row, col).  \nconvex_areaint \n\nNumber of pixels of convex hull image, which is the smallest convex polygon that encloses the region.  \nconvex_image(H, J) ndarray \n\nBinary convex hull image which has the same size as bounding box.  \ncoords(N, 2) ndarray \n\nCoordinate list (row, col) of the region.  \neccentricityfloat \n\nEccentricity of the ellipse that has the same second-moments as the region. The eccentricity is the ratio of the focal distance (distance between focal points) over the major axis length. The value is in the interval [0, 1). When it is 0, the ellipse becomes a circle.  \nequivalent_diameterfloat \n\nThe diameter of a circle with the same area as the region.  \neuler_numberint \n\nEuler characteristic of the set of non-zero pixels. Computed as number of connected components subtracted by number of holes (input.ndim connectivity). In 3D, number of connected components plus number of holes subtracted by number of tunnels.  \nextentfloat \n\nRatio of pixels in the region to pixels in the total bounding box. Computed as area / (rows * cols)  \nferet_diameter_maxfloat \n\nMaximum Feret\u2019s diameter computed as the longest distance between points around a region\u2019s convex hull contour as determined by find_contours. [5]  \nfilled_areaint \n\nNumber of pixels of the region will all the holes filled in. Describes the area of the filled_image.  \nfilled_image(H, J) ndarray \n\nBinary region image with filled holes which has the same size as bounding box.  \nimage(H, J) ndarray \n\nSliced binary region image which has the same size as bounding box.  \ninertia_tensorndarray \n\nInertia tensor of the region for the rotation around its mass.  \ninertia_tensor_eigvalstuple \n\nThe eigenvalues of the inertia tensor in decreasing order.  \nintensity_imagendarray \n\nImage inside region bounding box.  \nlabelint \n\nThe label in the labeled input image.  \nlocal_centroidarray \n\nCentroid coordinate tuple (row, col), relative to region bounding box.  \nmajor_axis_lengthfloat \n\nThe length of the major axis of the ellipse that has the same normalized second central moments as the region.  \nmax_intensityfloat \n\nValue with the greatest intensity in the region.  \nmean_intensityfloat \n\nValue with the mean intensity in the region.  \nmin_intensityfloat \n\nValue with the least intensity in the region.  \nminor_axis_lengthfloat \n\nThe length of the minor axis of the ellipse that has the same normalized second central moments as the region.  \nmoments(3, 3) ndarray \n\nSpatial moments up to 3rd order: m_ij = sum{ array(row, col) * row^i * col^j }\n where the sum is over the row, col coordinates of the region.  \nmoments_central(3, 3) ndarray \n\nCentral moments (translation invariant) up to 3rd order: mu_ij = sum{ array(row, col) * (row - row_c)^i * (col - col_c)^j }\n where the sum is over the row, col coordinates of the region, and row_c and col_c are the coordinates of the region\u2019s centroid.  \nmoments_hutuple \n\nHu moments (translation, scale and rotation invariant).  \nmoments_normalized(3, 3) ndarray \n\nNormalized moments (translation and scale invariant) up to 3rd order: nu_ij = mu_ij / m_00^[(i+j)/2 + 1]\n where m_00 is the zeroth spatial moment.  \norientationfloat \n\nAngle between the 0th axis (rows) and the major axis of the ellipse that has the same second moments as the region, ranging from -pi/2 to pi/2 counter-clockwise.  \nperimeterfloat \n\nPerimeter of object which approximates the contour as a line through the centers of border pixels using a 4-connectivity.  \nperimeter_croftonfloat \n\nPerimeter of object approximated by the Crofton formula in 4 directions.  \nslicetuple of slices \n\nA slice to extract the object from the source image.  \nsolidityfloat \n\nRatio of pixels in the region to pixels of the convex hull image.  \nweighted_centroidarray \n\nCentroid coordinate tuple (row, col) weighted with intensity image.  \nweighted_local_centroidarray \n\nCentroid coordinate tuple (row, col), relative to region bounding box, weighted with intensity image.  \nweighted_moments(3, 3) ndarray \n\nSpatial moments of intensity image up to 3rd order: wm_ij = sum{ array(row, col) * row^i * col^j }\n where the sum is over the row, col coordinates of the region.  \nweighted_moments_central(3, 3) ndarray \n\nCentral moments (translation invariant) of intensity image up to 3rd order: wmu_ij = sum{ array(row, col) * (row - row_c)^i * (col - col_c)^j }\n where the sum is over the row, col coordinates of the region, and row_c and col_c are the coordinates of the region\u2019s weighted centroid.  \nweighted_moments_hutuple \n\nHu moments (translation, scale and rotation invariant) of intensity image.  \nweighted_moments_normalized(3, 3) ndarray \n\nNormalized moments (translation and scale invariant) of intensity image up to 3rd order: wnu_ij = wmu_ij / wm_00^[(i+j)/2 + 1]\n where wm_00 is the zeroth spatial moment (intensity-weighted area).   Each region also supports iteration, so that you can do: for prop in region:\n    print(prop, region[prop])\n References  \n1  \nWilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.  \n2  \nB. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.  \n3  \nT. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.  \n4  \nhttps://en.wikipedia.org/wiki/Image_moment  \n5  \nW. Pabst, E. Gregorov\u00e1. Characterization of particles and particle systems, pp. 27-28. ICT Prague, 2007. https://old.vscht.cz/sil/keramika/Characterization_of_particles/CPPS%20_English%20version_.pdf   Examples >>> from skimage import data, util\n>>> from skimage.measure import label, regionprops\n>>> img = util.img_as_ubyte(data.coins()) > 110\n>>> label_img = label(img, connectivity=img.ndim)\n>>> props = regionprops(label_img)\n>>> # centroid of first labeled object\n>>> props[0].centroid\n(22.72987986048314, 81.91228523446583)\n>>> # centroid of first labeled object\n>>> props[0]['centroid']\n(22.72987986048314, 81.91228523446583)\n Add custom measurements by passing functions as extra_properties >>> from skimage import data, util\n>>> from skimage.measure import label, regionprops\n>>> import numpy as np\n>>> img = util.img_as_ubyte(data.coins()) > 110\n>>> label_img = label(img, connectivity=img.ndim)\n>>> def pixelcount(regionmask):\n...     return np.sum(regionmask)\n>>> props = regionprops(label_img, extra_properties=(pixelcount,))\n>>> props[0].pixelcount\n7741\n>>> props[1]['pixelcount']\n42\n \n"}, {"name": "measure.regionprops_table()", "path": "api/skimage.measure#skimage.measure.regionprops_table", "type": "measure", "text": " \nskimage.measure.regionprops_table(label_image, intensity_image=None, properties=('label', 'bbox'), *, cache=True, separator='-', extra_properties=None) [source]\n \nCompute image properties and return them as a pandas-compatible table. The table is a dictionary mapping column names to value arrays. See Notes section below for details.  New in version 0.16.   Parameters \n \nlabel_image(N, M[, P]) ndarray \n\nLabeled input image. Labels with value 0 are ignored.  \nintensity_image(M, N[, P][, C]) ndarray, optional \n\nIntensity (i.e., input) image with same size as labeled image, plus optionally an extra dimension for multichannel data. Default is None.  Changed in version 0.18.0: The ability to provide an extra dimension for channels was added.   \npropertiestuple or list of str, optional \n\nProperties that will be included in the resulting dictionary For a list of available properties, please see regionprops(). Users should remember to add \u201clabel\u201d to keep track of region identities.  \ncachebool, optional \n\nDetermine whether to cache calculated properties. The computation is much faster for cached properties, whereas the memory consumption increases.  \nseparatorstr, optional \n\nFor non-scalar properties not listed in OBJECT_COLUMNS, each element will appear in its own column, with the index of that element separated from the property name by this separator. For example, the inertia tensor of a 2D region will appear in four columns: inertia_tensor-0-0, inertia_tensor-0-1, inertia_tensor-1-0, and inertia_tensor-1-1 (where the separator is -). Object columns are those that cannot be split in this way because the number of columns would change depending on the object. For example, image and coords.  \nextra_propertiesIterable of callables \n\nAdd extra property computation functions that are not included with skimage. The name of the property is derived from the function name, the dtype is inferred by calling the function on a small sample. If the name of an extra property clashes with the name of an existing property the extra property wil not be visible and a UserWarning is issued. A property computation function must take a region mask as its first argument. If the property requires an intensity image, it must accept the intensity image as the second argument.    Returns \n \nout_dictdict \n\nDictionary mapping property names to an array of values of that property, one value per region. This dictionary can be used as input to pandas DataFrame to map property names to columns in the frame and regions to rows. If the image has no regions, the arrays will have length 0, but the correct type.     Notes Each column contains either a scalar property, an object property, or an element in a multidimensional array. Properties with scalar values for each region, such as \u201ceccentricity\u201d, will appear as a float or int array with that property name as key. Multidimensional properties of fixed size for a given image dimension, such as \u201ccentroid\u201d (every centroid will have three elements in a 3D image, no matter the region size), will be split into that many columns, with the name {property_name}{separator}{element_num} (for 1D properties), {property_name}{separator}{elem_num0}{separator}{elem_num1} (for 2D properties), and so on. For multidimensional properties that don\u2019t have a fixed size, such as \u201cimage\u201d (the image of a region varies in size depending on the region size), an object array will be used, with the corresponding property name as the key. Examples >>> from skimage import data, util, measure\n>>> image = data.coins()\n>>> label_image = measure.label(image > 110, connectivity=image.ndim)\n>>> props = measure.regionprops_table(label_image, image,\n...                           properties=['label', 'inertia_tensor',\n...                                       'inertia_tensor_eigvals'])\n>>> props  \n{'label': array([ 1,  2, ...]), ...\n 'inertia_tensor-0-0': array([  4.012...e+03,   8.51..., ...]), ...\n ...,\n 'inertia_tensor_eigvals-1': array([  2.67...e+02,   2.83..., ...])}\n The resulting dictionary can be directly passed to pandas, if installed, to obtain a clean DataFrame: >>> import pandas as pd  \n>>> data = pd.DataFrame(props)  \n>>> data.head()  \n   label  inertia_tensor-0-0  ...  inertia_tensor_eigvals-1\n0      1         4012.909888  ...                267.065503\n1      2            8.514739  ...                  2.834806\n2      3            0.666667  ...                  0.000000\n3      4            0.000000  ...                  0.000000\n4      5            0.222222  ...                  0.111111\n [5 rows x 7 columns] If we want to measure a feature that does not come as a built-in property, we can define custom functions and pass them as extra_properties. For example, we can create a custom function that measures the intensity quartiles in a region: >>> from skimage import data, util, measure\n>>> import numpy as np\n>>> def quartiles(regionmask, intensity):\n...     return np.percentile(intensity[regionmask], q=(25, 50, 75))\n>>>\n>>> image = data.coins()\n>>> label_image = measure.label(image > 110, connectivity=image.ndim)\n>>> props = measure.regionprops_table(label_image, intensity_image=image,\n...                                   properties=('label',),\n...                                   extra_properties=(quartiles,))\n>>> import pandas as pd \n>>> pd.DataFrame(props).head() \n       label  quartiles-0  quartiles-1  quartiles-2\n0      1       117.00        123.0        130.0\n1      2       111.25        112.0        114.0\n2      3       111.00        111.0        111.0\n3      4       111.00        111.5        112.5\n4      5       112.50        113.0        114.0\n \n"}, {"name": "measure.shannon_entropy()", "path": "api/skimage.measure#skimage.measure.shannon_entropy", "type": "measure", "text": " \nskimage.measure.shannon_entropy(image, base=2) [source]\n \nCalculate the Shannon entropy of an image. The Shannon entropy is defined as S = -sum(pk * log(pk)), where pk are frequency/probability of pixels of value k.  Parameters \n \nimage(N, M) ndarray \n\nGrayscale input image.  \nbasefloat, optional \n\nThe logarithmic base to use.    Returns \n \nentropyfloat \n   Notes The returned value is measured in bits or shannon (Sh) for base=2, natural unit (nat) for base=np.e and hartley (Hart) for base=10. References  \n1  \nhttps://en.wikipedia.org/wiki/Entropy_(information_theory)  \n2  \nhttps://en.wiktionary.org/wiki/Shannon_entropy   Examples >>> from skimage import data\n>>> from skimage.measure import shannon_entropy\n>>> shannon_entropy(data.camera())\n7.231695011055706\n \n"}, {"name": "measure.subdivide_polygon()", "path": "api/skimage.measure#skimage.measure.subdivide_polygon", "type": "measure", "text": " \nskimage.measure.subdivide_polygon(coords, degree=2, preserve_ends=False) [source]\n \nSubdivision of polygonal curves using B-Splines. Note that the resulting curve is always within the convex hull of the original polygon. Circular polygons stay closed after subdivision.  Parameters \n \ncoords(N, 2) array \n\nCoordinate array.  \ndegree{1, 2, 3, 4, 5, 6, 7}, optional \n\nDegree of B-Spline. Default is 2.  \npreserve_endsbool, optional \n\nPreserve first and last coordinate of non-circular polygon. Default is False.    Returns \n \ncoords(M, 2) array \n\nSubdivided coordinate array.     References  \n1  \nhttp://mrl.nyu.edu/publications/subdiv-course2000/coursenotes00.pdf   \n"}, {"name": "metrics", "path": "api/skimage.metrics", "type": "metrics", "text": "Module: metrics  \nskimage.metrics.adapted_rand_error([\u2026]) Compute Adapted Rand error as defined by the SNEMI3D contest.  \nskimage.metrics.contingency_table(im_true, \u2026) Return the contingency table for all regions in matched segmentations.  \nskimage.metrics.hausdorff_distance(image0, \u2026) Calculate the Hausdorff distance between nonzero elements of given images.  \nskimage.metrics.mean_squared_error(image0, \u2026) Compute the mean-squared error between two images.  \nskimage.metrics.normalized_root_mse(\u2026[, \u2026]) Compute the normalized root mean-squared error (NRMSE) between two images.  \nskimage.metrics.peak_signal_noise_ratio(\u2026) Compute the peak signal to noise ratio (PSNR) for an image.  \nskimage.metrics.structural_similarity(im1, \u2026) Compute the mean structural similarity index between two images.  \nskimage.metrics.variation_of_information([\u2026]) Return symmetric conditional entropies associated with the VI.   adapted_rand_error  \nskimage.metrics.adapted_rand_error(image_true=None, image_test=None, *, table=None, ignore_labels=(0, )) [source]\n \nCompute Adapted Rand error as defined by the SNEMI3D contest. [1]  Parameters \n \nimage_truendarray of int \n\nGround-truth label image, same shape as im_test.  \nimage_testndarray of int \n\nTest image.  \ntablescipy.sparse array in crs format, optional \n\nA contingency table built with skimage.evaluate.contingency_table. If None, it will be computed on the fly.  \nignore_labelssequence of int, optional \n\nLabels to ignore. Any part of the true image labeled with any of these values will not be counted in the score.    Returns \n \narefloat \n\nThe adapted Rand error; equal to \\(1 - \\frac{2pr}{p + r}\\), where p and r are the precision and recall described below.  \nprecfloat \n\nThe adapted Rand precision: this is the number of pairs of pixels that have the same label in the test label image and in the true image, divided by the number in the test image.  \nrecfloat \n\nThe adapted Rand recall: this is the number of pairs of pixels that have the same label in the test label image and in the true image, divided by the number in the true image.     Notes Pixels with label 0 in the true segmentation are ignored in the score. References  \n1  \nArganda-Carreras I, Turaga SC, Berger DR, et al. (2015) Crowdsourcing the creation of image segmentation algorithms for connectomics. Front. Neuroanat. 9:142. DOI:10.3389/fnana.2015.00142   \n contingency_table  \nskimage.metrics.contingency_table(im_true, im_test, *, ignore_labels=None, normalize=False) [source]\n \nReturn the contingency table for all regions in matched segmentations.  Parameters \n \nim_truendarray of int \n\nGround-truth label image, same shape as im_test.  \nim_testndarray of int \n\nTest image.  \nignore_labelssequence of int, optional \n\nLabels to ignore. Any part of the true image labeled with any of these values will not be counted in the score.  \nnormalizebool \n\nDetermines if the contingency table is normalized by pixel count.    Returns \n \ncontscipy.sparse.csr_matrix \n\nA contingency table. cont[i, j] will equal the number of voxels labeled i in im_true and j in im_test.     \n hausdorff_distance  \nskimage.metrics.hausdorff_distance(image0, image1) [source]\n \nCalculate the Hausdorff distance between nonzero elements of given images. The Hausdorff distance [1] is the maximum distance between any point on image0 and its nearest point on image1, and vice-versa.  Parameters \n \nimage0, image1ndarray \n\nArrays where True represents a point that is included in a set of points. Both arrays must have the same shape.    Returns \n \ndistancefloat \n\nThe Hausdorff distance between coordinates of nonzero pixels in image0 and image1, using the Euclidian distance.     References  \n1  \nhttp://en.wikipedia.org/wiki/Hausdorff_distance   Examples >>> points_a = (3, 0)\n>>> points_b = (6, 0)\n>>> shape = (7, 1)\n>>> image_a = np.zeros(shape, dtype=bool)\n>>> image_b = np.zeros(shape, dtype=bool)\n>>> image_a[points_a] = True\n>>> image_b[points_b] = True\n>>> hausdorff_distance(image_a, image_b)\n3.0\n \n Examples using skimage.metrics.hausdorff_distance\n \n  Hausdorff Distance   mean_squared_error  \nskimage.metrics.mean_squared_error(image0, image1) [source]\n \nCompute the mean-squared error between two images.  Parameters \n \nimage0, image1ndarray \n\nImages. Any dimensionality, must have same shape.    Returns \n \nmsefloat \n\nThe mean-squared error (MSE) metric.     Notes  Changed in version 0.16: This function was renamed from skimage.measure.compare_mse to skimage.metrics.mean_squared_error.  \n normalized_root_mse  \nskimage.metrics.normalized_root_mse(image_true, image_test, *, normalization='euclidean') [source]\n \nCompute the normalized root mean-squared error (NRMSE) between two images.  Parameters \n \nimage_truendarray \n\nGround-truth image, same shape as im_test.  \nimage_testndarray \n\nTest image.  \nnormalization{\u2018euclidean\u2019, \u2018min-max\u2019, \u2018mean\u2019}, optional \n\nControls the normalization method to use in the denominator of the NRMSE. There is no standard method of normalization across the literature [1]. The methods available here are as follows:  \n\u2018euclidean\u2019 : normalize by the averaged Euclidean norm of im_true: NRMSE = RMSE * sqrt(N) / || im_true ||\n where || . || denotes the Frobenius norm and N = im_true.size. This result is equivalent to: NRMSE = || im_true - im_test || / || im_true ||.\n  \u2018min-max\u2019 : normalize by the intensity range of im_true. \u2018mean\u2019 : normalize by the mean of im_true\n     Returns \n \nnrmsefloat \n\nThe NRMSE metric.     Notes  Changed in version 0.16: This function was renamed from skimage.measure.compare_nrmse to skimage.metrics.normalized_root_mse.  References  \n1  \nhttps://en.wikipedia.org/wiki/Root-mean-square_deviation   \n peak_signal_noise_ratio  \nskimage.metrics.peak_signal_noise_ratio(image_true, image_test, *, data_range=None) [source]\n \nCompute the peak signal to noise ratio (PSNR) for an image.  Parameters \n \nimage_truendarray \n\nGround-truth image, same shape as im_test.  \nimage_testndarray \n\nTest image.  \ndata_rangeint, optional \n\nThe data range of the input image (distance between minimum and maximum possible values). By default, this is estimated from the image data-type.    Returns \n \npsnrfloat \n\nThe PSNR metric.     Notes  Changed in version 0.16: This function was renamed from skimage.measure.compare_psnr to skimage.metrics.peak_signal_noise_ratio.  References  \n1  \nhttps://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio   \n structural_similarity  \nskimage.metrics.structural_similarity(im1, im2, *, win_size=None, gradient=False, data_range=None, multichannel=False, gaussian_weights=False, full=False, **kwargs) [source]\n \nCompute the mean structural similarity index between two images.  Parameters \n \nim1, im2ndarray \n\nImages. Any dimensionality with same shape.  \nwin_sizeint or None, optional \n\nThe side-length of the sliding window used in comparison. Must be an odd value. If gaussian_weights is True, this is ignored and the window size will depend on sigma.  \ngradientbool, optional \n\nIf True, also return the gradient with respect to im2.  \ndata_rangefloat, optional \n\nThe data range of the input image (distance between minimum and maximum possible values). By default, this is estimated from the image data-type.  \nmultichannelbool, optional \n\nIf True, treat the last dimension of the array as channels. Similarity calculations are done independently for each channel then averaged.  \ngaussian_weightsbool, optional \n\nIf True, each patch has its mean and variance spatially weighted by a normalized Gaussian kernel of width sigma=1.5.  \nfullbool, optional \n\nIf True, also return the full structural similarity image.    Returns \n \nmssimfloat \n\nThe mean structural similarity index over the image.  \ngradndarray \n\nThe gradient of the structural similarity between im1 and im2 [2]. This is only returned if gradient is set to True.  \nSndarray \n\nThe full SSIM image. This is only returned if full is set to True.    Other Parameters \n \nuse_sample_covariancebool \n\nIf True, normalize covariances by N-1 rather than, N where N is the number of pixels within the sliding window.  \nK1float \n\nAlgorithm parameter, K1 (small constant, see [1]).  \nK2float \n\nAlgorithm parameter, K2 (small constant, see [1]).  \nsigmafloat \n\nStandard deviation for the Gaussian when gaussian_weights is True.     Notes To match the implementation of Wang et. al. [1], set gaussian_weights to True, sigma to 1.5, and use_sample_covariance to False.  Changed in version 0.16: This function was renamed from skimage.measure.compare_ssim to skimage.metrics.structural_similarity.  References  \n1(1,2,3)  \nWang, Z., Bovik, A. C., Sheikh, H. R., & Simoncelli, E. P. (2004). Image quality assessment: From error visibility to structural similarity. IEEE Transactions on Image Processing, 13, 600-612. https://ece.uwaterloo.ca/~z70wang/publications/ssim.pdf, DOI:10.1109/TIP.2003.819861  \n2  \nAvanaki, A. N. (2009). Exact global histogram specification optimized for structural similarity. Optical Review, 16, 613-621. arXiv:0901.0065 DOI:10.1007/s10043-009-0119-z   \n variation_of_information  \nskimage.metrics.variation_of_information(image0=None, image1=None, *, table=None, ignore_labels=()) [source]\n \nReturn symmetric conditional entropies associated with the VI. [1] The variation of information is defined as VI(X,Y) = H(X|Y) + H(Y|X). If X is the ground-truth segmentation, then H(X|Y) can be interpreted as the amount of under-segmentation and H(X|Y) as the amount of over-segmentation. In other words, a perfect over-segmentation will have H(X|Y)=0 and a perfect under-segmentation will have H(Y|X)=0.  Parameters \n \nimage0, image1ndarray of int \n\nLabel images / segmentations, must have same shape.  \ntablescipy.sparse array in csr format, optional \n\nA contingency table built with skimage.evaluate.contingency_table. If None, it will be computed with skimage.evaluate.contingency_table. If given, the entropies will be computed from this table and any images will be ignored.  \nignore_labelssequence of int, optional \n\nLabels to ignore. Any part of the true image labeled with any of these values will not be counted in the score.    Returns \n \nvindarray of float, shape (2,) \n\nThe conditional entropies of image1|image0 and image0|image1.     References  \n1  \nMarina Meil\u0103 (2007), Comparing clusterings\u2014an information based distance, Journal of Multivariate Analysis, Volume 98, Issue 5, Pages 873-895, ISSN 0047-259X, DOI:10.1016/j.jmva.2006.11.013.   \n\n"}, {"name": "metrics.adapted_rand_error()", "path": "api/skimage.metrics#skimage.metrics.adapted_rand_error", "type": "metrics", "text": " \nskimage.metrics.adapted_rand_error(image_true=None, image_test=None, *, table=None, ignore_labels=(0, )) [source]\n \nCompute Adapted Rand error as defined by the SNEMI3D contest. [1]  Parameters \n \nimage_truendarray of int \n\nGround-truth label image, same shape as im_test.  \nimage_testndarray of int \n\nTest image.  \ntablescipy.sparse array in crs format, optional \n\nA contingency table built with skimage.evaluate.contingency_table. If None, it will be computed on the fly.  \nignore_labelssequence of int, optional \n\nLabels to ignore. Any part of the true image labeled with any of these values will not be counted in the score.    Returns \n \narefloat \n\nThe adapted Rand error; equal to \\(1 - \\frac{2pr}{p + r}\\), where p and r are the precision and recall described below.  \nprecfloat \n\nThe adapted Rand precision: this is the number of pairs of pixels that have the same label in the test label image and in the true image, divided by the number in the test image.  \nrecfloat \n\nThe adapted Rand recall: this is the number of pairs of pixels that have the same label in the test label image and in the true image, divided by the number in the true image.     Notes Pixels with label 0 in the true segmentation are ignored in the score. References  \n1  \nArganda-Carreras I, Turaga SC, Berger DR, et al. (2015) Crowdsourcing the creation of image segmentation algorithms for connectomics. Front. Neuroanat. 9:142. DOI:10.3389/fnana.2015.00142   \n"}, {"name": "metrics.contingency_table()", "path": "api/skimage.metrics#skimage.metrics.contingency_table", "type": "metrics", "text": " \nskimage.metrics.contingency_table(im_true, im_test, *, ignore_labels=None, normalize=False) [source]\n \nReturn the contingency table for all regions in matched segmentations.  Parameters \n \nim_truendarray of int \n\nGround-truth label image, same shape as im_test.  \nim_testndarray of int \n\nTest image.  \nignore_labelssequence of int, optional \n\nLabels to ignore. Any part of the true image labeled with any of these values will not be counted in the score.  \nnormalizebool \n\nDetermines if the contingency table is normalized by pixel count.    Returns \n \ncontscipy.sparse.csr_matrix \n\nA contingency table. cont[i, j] will equal the number of voxels labeled i in im_true and j in im_test.     \n"}, {"name": "metrics.hausdorff_distance()", "path": "api/skimage.metrics#skimage.metrics.hausdorff_distance", "type": "metrics", "text": " \nskimage.metrics.hausdorff_distance(image0, image1) [source]\n \nCalculate the Hausdorff distance between nonzero elements of given images. The Hausdorff distance [1] is the maximum distance between any point on image0 and its nearest point on image1, and vice-versa.  Parameters \n \nimage0, image1ndarray \n\nArrays where True represents a point that is included in a set of points. Both arrays must have the same shape.    Returns \n \ndistancefloat \n\nThe Hausdorff distance between coordinates of nonzero pixels in image0 and image1, using the Euclidian distance.     References  \n1  \nhttp://en.wikipedia.org/wiki/Hausdorff_distance   Examples >>> points_a = (3, 0)\n>>> points_b = (6, 0)\n>>> shape = (7, 1)\n>>> image_a = np.zeros(shape, dtype=bool)\n>>> image_b = np.zeros(shape, dtype=bool)\n>>> image_a[points_a] = True\n>>> image_b[points_b] = True\n>>> hausdorff_distance(image_a, image_b)\n3.0\n \n"}, {"name": "metrics.mean_squared_error()", "path": "api/skimage.metrics#skimage.metrics.mean_squared_error", "type": "metrics", "text": " \nskimage.metrics.mean_squared_error(image0, image1) [source]\n \nCompute the mean-squared error between two images.  Parameters \n \nimage0, image1ndarray \n\nImages. Any dimensionality, must have same shape.    Returns \n \nmsefloat \n\nThe mean-squared error (MSE) metric.     Notes  Changed in version 0.16: This function was renamed from skimage.measure.compare_mse to skimage.metrics.mean_squared_error.  \n"}, {"name": "metrics.normalized_root_mse()", "path": "api/skimage.metrics#skimage.metrics.normalized_root_mse", "type": "metrics", "text": " \nskimage.metrics.normalized_root_mse(image_true, image_test, *, normalization='euclidean') [source]\n \nCompute the normalized root mean-squared error (NRMSE) between two images.  Parameters \n \nimage_truendarray \n\nGround-truth image, same shape as im_test.  \nimage_testndarray \n\nTest image.  \nnormalization{\u2018euclidean\u2019, \u2018min-max\u2019, \u2018mean\u2019}, optional \n\nControls the normalization method to use in the denominator of the NRMSE. There is no standard method of normalization across the literature [1]. The methods available here are as follows:  \n\u2018euclidean\u2019 : normalize by the averaged Euclidean norm of im_true: NRMSE = RMSE * sqrt(N) / || im_true ||\n where || . || denotes the Frobenius norm and N = im_true.size. This result is equivalent to: NRMSE = || im_true - im_test || / || im_true ||.\n  \u2018min-max\u2019 : normalize by the intensity range of im_true. \u2018mean\u2019 : normalize by the mean of im_true\n     Returns \n \nnrmsefloat \n\nThe NRMSE metric.     Notes  Changed in version 0.16: This function was renamed from skimage.measure.compare_nrmse to skimage.metrics.normalized_root_mse.  References  \n1  \nhttps://en.wikipedia.org/wiki/Root-mean-square_deviation   \n"}, {"name": "metrics.peak_signal_noise_ratio()", "path": "api/skimage.metrics#skimage.metrics.peak_signal_noise_ratio", "type": "metrics", "text": " \nskimage.metrics.peak_signal_noise_ratio(image_true, image_test, *, data_range=None) [source]\n \nCompute the peak signal to noise ratio (PSNR) for an image.  Parameters \n \nimage_truendarray \n\nGround-truth image, same shape as im_test.  \nimage_testndarray \n\nTest image.  \ndata_rangeint, optional \n\nThe data range of the input image (distance between minimum and maximum possible values). By default, this is estimated from the image data-type.    Returns \n \npsnrfloat \n\nThe PSNR metric.     Notes  Changed in version 0.16: This function was renamed from skimage.measure.compare_psnr to skimage.metrics.peak_signal_noise_ratio.  References  \n1  \nhttps://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio   \n"}, {"name": "metrics.structural_similarity()", "path": "api/skimage.metrics#skimage.metrics.structural_similarity", "type": "metrics", "text": " \nskimage.metrics.structural_similarity(im1, im2, *, win_size=None, gradient=False, data_range=None, multichannel=False, gaussian_weights=False, full=False, **kwargs) [source]\n \nCompute the mean structural similarity index between two images.  Parameters \n \nim1, im2ndarray \n\nImages. Any dimensionality with same shape.  \nwin_sizeint or None, optional \n\nThe side-length of the sliding window used in comparison. Must be an odd value. If gaussian_weights is True, this is ignored and the window size will depend on sigma.  \ngradientbool, optional \n\nIf True, also return the gradient with respect to im2.  \ndata_rangefloat, optional \n\nThe data range of the input image (distance between minimum and maximum possible values). By default, this is estimated from the image data-type.  \nmultichannelbool, optional \n\nIf True, treat the last dimension of the array as channels. Similarity calculations are done independently for each channel then averaged.  \ngaussian_weightsbool, optional \n\nIf True, each patch has its mean and variance spatially weighted by a normalized Gaussian kernel of width sigma=1.5.  \nfullbool, optional \n\nIf True, also return the full structural similarity image.    Returns \n \nmssimfloat \n\nThe mean structural similarity index over the image.  \ngradndarray \n\nThe gradient of the structural similarity between im1 and im2 [2]. This is only returned if gradient is set to True.  \nSndarray \n\nThe full SSIM image. This is only returned if full is set to True.    Other Parameters \n \nuse_sample_covariancebool \n\nIf True, normalize covariances by N-1 rather than, N where N is the number of pixels within the sliding window.  \nK1float \n\nAlgorithm parameter, K1 (small constant, see [1]).  \nK2float \n\nAlgorithm parameter, K2 (small constant, see [1]).  \nsigmafloat \n\nStandard deviation for the Gaussian when gaussian_weights is True.     Notes To match the implementation of Wang et. al. [1], set gaussian_weights to True, sigma to 1.5, and use_sample_covariance to False.  Changed in version 0.16: This function was renamed from skimage.measure.compare_ssim to skimage.metrics.structural_similarity.  References  \n1(1,2,3)  \nWang, Z., Bovik, A. C., Sheikh, H. R., & Simoncelli, E. P. (2004). Image quality assessment: From error visibility to structural similarity. IEEE Transactions on Image Processing, 13, 600-612. https://ece.uwaterloo.ca/~z70wang/publications/ssim.pdf, DOI:10.1109/TIP.2003.819861  \n2  \nAvanaki, A. N. (2009). Exact global histogram specification optimized for structural similarity. Optical Review, 16, 613-621. arXiv:0901.0065 DOI:10.1007/s10043-009-0119-z   \n"}, {"name": "metrics.variation_of_information()", "path": "api/skimage.metrics#skimage.metrics.variation_of_information", "type": "metrics", "text": " \nskimage.metrics.variation_of_information(image0=None, image1=None, *, table=None, ignore_labels=()) [source]\n \nReturn symmetric conditional entropies associated with the VI. [1] The variation of information is defined as VI(X,Y) = H(X|Y) + H(Y|X). If X is the ground-truth segmentation, then H(X|Y) can be interpreted as the amount of under-segmentation and H(X|Y) as the amount of over-segmentation. In other words, a perfect over-segmentation will have H(X|Y)=0 and a perfect under-segmentation will have H(Y|X)=0.  Parameters \n \nimage0, image1ndarray of int \n\nLabel images / segmentations, must have same shape.  \ntablescipy.sparse array in csr format, optional \n\nA contingency table built with skimage.evaluate.contingency_table. If None, it will be computed with skimage.evaluate.contingency_table. If given, the entropies will be computed from this table and any images will be ignored.  \nignore_labelssequence of int, optional \n\nLabels to ignore. Any part of the true image labeled with any of these values will not be counted in the score.    Returns \n \nvindarray of float, shape (2,) \n\nThe conditional entropies of image1|image0 and image0|image1.     References  \n1  \nMarina Meil\u0103 (2007), Comparing clusterings\u2014an information based distance, Journal of Multivariate Analysis, Volume 98, Issue 5, Pages 873-895, ISSN 0047-259X, DOI:10.1016/j.jmva.2006.11.013.   \n"}, {"name": "morphology", "path": "api/skimage.morphology", "type": "morphology", "text": "Module: morphology  \nskimage.morphology.area_closing(image[, \u2026]) Perform an area closing of the image.  \nskimage.morphology.area_opening(image[, \u2026]) Perform an area opening of the image.  \nskimage.morphology.ball(radius[, dtype]) Generates a ball-shaped structuring element.  \nskimage.morphology.binary_closing(image[, \u2026]) Return fast binary morphological closing of an image.  \nskimage.morphology.binary_dilation(image[, \u2026]) Return fast binary morphological dilation of an image.  \nskimage.morphology.binary_erosion(image[, \u2026]) Return fast binary morphological erosion of an image.  \nskimage.morphology.binary_opening(image[, \u2026]) Return fast binary morphological opening of an image.  \nskimage.morphology.black_tophat(image[, \u2026]) Return black top hat of an image.  \nskimage.morphology.closing(image[, selem, out]) Return greyscale morphological closing of an image.  \nskimage.morphology.convex_hull_image(image) Compute the convex hull image of a binary image.  \nskimage.morphology.convex_hull_object(image, *) Compute the convex hull image of individual objects in a binary image.  \nskimage.morphology.cube(width[, dtype]) Generates a cube-shaped structuring element.  \nskimage.morphology.diameter_closing(image[, \u2026]) Perform a diameter closing of the image.  \nskimage.morphology.diameter_opening(image[, \u2026]) Perform a diameter opening of the image.  \nskimage.morphology.diamond(radius[, dtype]) Generates a flat, diamond-shaped structuring element.  \nskimage.morphology.dilation(image[, selem, \u2026]) Return greyscale morphological dilation of an image.  \nskimage.morphology.disk(radius[, dtype]) Generates a flat, disk-shaped structuring element.  \nskimage.morphology.erosion(image[, selem, \u2026]) Return greyscale morphological erosion of an image.  \nskimage.morphology.flood(image, seed_point, *) Mask corresponding to a flood fill.  \nskimage.morphology.flood_fill(image, \u2026[, \u2026]) Perform flood filling on an image.  \nskimage.morphology.h_maxima(image, h[, selem]) Determine all maxima of the image with height >= h.  \nskimage.morphology.h_minima(image, h[, selem]) Determine all minima of the image with depth >= h.  \nskimage.morphology.label(input[, \u2026]) Label connected regions of an integer array.  \nskimage.morphology.local_maxima(image[, \u2026]) Find local maxima of n-dimensional array.  \nskimage.morphology.local_minima(image[, \u2026]) Find local minima of n-dimensional array.  \nskimage.morphology.max_tree(image[, \u2026]) Build the max tree from an image.  \nskimage.morphology.max_tree_local_maxima(image) Determine all local maxima of the image.  \nskimage.morphology.medial_axis(image[, \u2026]) Compute the medial axis transform of a binary image  \nskimage.morphology.octagon(m, n[, dtype]) Generates an octagon shaped structuring element.  \nskimage.morphology.octahedron(radius[, dtype]) Generates a octahedron-shaped structuring element.  \nskimage.morphology.opening(image[, selem, out]) Return greyscale morphological opening of an image.  \nskimage.morphology.reconstruction(seed, mask) Perform a morphological reconstruction of an image.  \nskimage.morphology.rectangle(nrows, ncols[, \u2026]) Generates a flat, rectangular-shaped structuring element.  \nskimage.morphology.remove_small_holes(ar[, \u2026]) Remove contiguous holes smaller than the specified size.  \nskimage.morphology.remove_small_objects(ar) Remove objects smaller than the specified size.  \nskimage.morphology.skeletonize(image, *[, \u2026]) Compute the skeleton of a binary image.  \nskimage.morphology.skeletonize_3d(image) Compute the skeleton of a binary image.  \nskimage.morphology.square(width[, dtype]) Generates a flat, square-shaped structuring element.  \nskimage.morphology.star(a[, dtype]) Generates a star shaped structuring element.  \nskimage.morphology.thin(image[, max_iter]) Perform morphological thinning of a binary image.  \nskimage.morphology.watershed(image[, \u2026]) Deprecated function.  \nskimage.morphology.white_tophat(image[, \u2026]) Return white top hat of an image.   area_closing  \nskimage.morphology.area_closing(image, area_threshold=64, connectivity=1, parent=None, tree_traverser=None) [source]\n \nPerform an area closing of the image. Area closing removes all dark structures of an image with a surface smaller than area_threshold. The output image is larger than or equal to the input image for every pixel and all local minima have at least a surface of area_threshold pixels. Area closings are similar to morphological closings, but they do not use a fixed structuring element, but rather a deformable one, with surface = area_threshold. In the binary case, area closings are equivalent to remove_small_holes; this operator is thus extended to gray-level images. Technically, this operator is based on the max-tree representation of the image.  Parameters \n \nimagendarray \n\nThe input image for which the area_closing is to be calculated. This image can be of any type.  \narea_thresholdunsigned int \n\nThe size parameter (number of pixels). The default value is arbitrarily chosen to be 64.  \nconnectivityunsigned int, optional \n\nThe neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.  \nparentndarray, int64, optional \n\nParent image representing the max tree of the inverted image. The value of each pixel is the index of its parent in the ravelled array. See Note for further details.  \ntree_traverser1D array, int64, optional \n\nThe ordered pixel indices (referring to the ravelled array). The pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent).    Returns \n \noutputndarray \n\nOutput image of the same shape and type as input image.      See also  \nskimage.morphology.area_opening\n\n\nskimage.morphology.diameter_opening\n\n\nskimage.morphology.diameter_closing\n\n\nskimage.morphology.max_tree\n\n\nskimage.morphology.remove_small_objects\n\n\nskimage.morphology.remove_small_holes\n\n  Notes If a max-tree representation (parent and tree_traverser) are given to the function, they must be calculated from the inverted image for this function, i.e.: >>> P, S = max_tree(invert(f)) >>> closed = diameter_closing(f, 3, parent=P, tree_traverser=S) References  \n1  \nVincent L., Proc. \u201cGrayscale area openings and closings, their efficient implementation and applications\u201d, EURASIP Workshop on Mathematical Morphology and its Applications to Signal Processing, Barcelona, Spain, pp.22-27, May 1993.  \n2  \nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d (Chapter 6), 2nd edition (2003), ISBN 3540429883. DOI:10.1007/978-3-662-05088-0  \n3  \nSalembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected Operators for Image and Sequence Processing. IEEE Transactions on Image Processing, 7(4), 555-570. DOI:10.1109/83.663500  \n4  \nNajman, L., & Couprie, M. (2006). Building the component tree in quasi-linear time. IEEE Transactions on Image Processing, 15(11), 3531-3539. DOI:10.1109/TIP.2006.877518  \n5  \nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. DOI:10.1109/TIP.2014.2336551   Examples We create an image (quadratic function with a minimum in the center and 4 additional local minima. >>> w = 12\n>>> x, y = np.mgrid[0:w,0:w]\n>>> f = 180 + 0.2*((x - w/2)**2 + (y-w/2)**2)\n>>> f[2:3,1:5] = 160; f[2:4,9:11] = 140; f[9:11,2:4] = 120\n>>> f[9:10,9:11] = 100; f[10,10] = 100\n>>> f = f.astype(int)\n We can calculate the area closing: >>> closed = area_closing(f, 8, connectivity=1)\n All small minima are removed, and the remaining minima have at least a size of 8. \n area_opening  \nskimage.morphology.area_opening(image, area_threshold=64, connectivity=1, parent=None, tree_traverser=None) [source]\n \nPerform an area opening of the image. Area opening removes all bright structures of an image with a surface smaller than area_threshold. The output image is thus the largest image smaller than the input for which all local maxima have at least a surface of area_threshold pixels. Area openings are similar to morphological openings, but they do not use a fixed structuring element, but rather a deformable one, with surface = area_threshold. Consequently, the area_opening with area_threshold=1 is the identity. In the binary case, area openings are equivalent to remove_small_objects; this operator is thus extended to gray-level images. Technically, this operator is based on the max-tree representation of the image.  Parameters \n \nimagendarray \n\nThe input image for which the area_opening is to be calculated. This image can be of any type.  \narea_thresholdunsigned int \n\nThe size parameter (number of pixels). The default value is arbitrarily chosen to be 64.  \nconnectivityunsigned int, optional \n\nThe neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.  \nparentndarray, int64, optional \n\nParent image representing the max tree of the image. The value of each pixel is the index of its parent in the ravelled array.  \ntree_traverser1D array, int64, optional \n\nThe ordered pixel indices (referring to the ravelled array). The pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent).    Returns \n \noutputndarray \n\nOutput image of the same shape and type as the input image.      See also  \nskimage.morphology.area_closing\n\n\nskimage.morphology.diameter_opening\n\n\nskimage.morphology.diameter_closing\n\n\nskimage.morphology.max_tree\n\n\nskimage.morphology.remove_small_objects\n\n\nskimage.morphology.remove_small_holes\n\n  References  \n1  \nVincent L., Proc. \u201cGrayscale area openings and closings, their efficient implementation and applications\u201d, EURASIP Workshop on Mathematical Morphology and its Applications to Signal Processing, Barcelona, Spain, pp.22-27, May 1993.  \n2  \nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d (Chapter 6), 2nd edition (2003), ISBN 3540429883. :DOI:10.1007/978-3-662-05088-0  \n3  \nSalembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected Operators for Image and Sequence Processing. IEEE Transactions on Image Processing, 7(4), 555-570. :DOI:10.1109/83.663500  \n4  \nNajman, L., & Couprie, M. (2006). Building the component tree in quasi-linear time. IEEE Transactions on Image Processing, 15(11), 3531-3539. :DOI:10.1109/TIP.2006.877518  \n5  \nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. :DOI:10.1109/TIP.2014.2336551   Examples We create an image (quadratic function with a maximum in the center and 4 additional local maxima. >>> w = 12\n>>> x, y = np.mgrid[0:w,0:w]\n>>> f = 20 - 0.2*((x - w/2)**2 + (y-w/2)**2)\n>>> f[2:3,1:5] = 40; f[2:4,9:11] = 60; f[9:11,2:4] = 80\n>>> f[9:10,9:11] = 100; f[10,10] = 100\n>>> f = f.astype(int)\n We can calculate the area opening: >>> open = area_opening(f, 8, connectivity=1)\n The peaks with a surface smaller than 8 are removed. \n ball  \nskimage.morphology.ball(radius, dtype=<class 'numpy.uint8'>) [source]\n \nGenerates a ball-shaped structuring element. This is the 3D equivalent of a disk. A pixel is within the neighborhood if the Euclidean distance between it and the origin is no greater than radius.  Parameters \n \nradiusint \n\nThe radius of the ball-shaped structuring element.    Returns \n \nselemndarray \n\nThe structuring element where elements of the neighborhood are 1 and 0 otherwise.    Other Parameters \n \ndtypedata-type \n\nThe data type of the structuring element.     \n Examples using skimage.morphology.ball\n \n  Local Histogram Equalization  \n\n  Rank filters   binary_closing  \nskimage.morphology.binary_closing(image, selem=None, out=None) [source]\n \nReturn fast binary morphological closing of an image. This function returns the same result as greyscale closing but performs faster for binary images. The morphological closing on an image is defined as a dilation followed by an erosion. Closing can remove small dark spots (i.e. \u201cpepper\u201d) and connect small bright cracks. This tends to \u201cclose\u201d up (dark) gaps between (bright) features.  Parameters \n \nimagendarray \n\nBinary input image.  \nselemndarray, optional \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a cross-shaped structuring element (connectivity=1).  \noutndarray of bool, optional \n\nThe array to store the result of the morphology. If None, is passed, a new array will be allocated.    Returns \n \nclosingndarray of bool \n\nThe result of the morphological closing.     \n Examples using skimage.morphology.binary_closing\n \n  Flood Fill   binary_dilation  \nskimage.morphology.binary_dilation(image, selem=None, out=None) [source]\n \nReturn fast binary morphological dilation of an image. This function returns the same result as greyscale dilation but performs faster for binary images. Morphological dilation sets a pixel at (i,j) to the maximum over all pixels in the neighborhood centered at (i,j). Dilation enlarges bright regions and shrinks dark regions.  Parameters \n \nimagendarray \n\nBinary input image.  \nselemndarray, optional \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a cross-shaped structuring element (connectivity=1).  \noutndarray of bool, optional \n\nThe array to store the result of the morphology. If None is passed, a new array will be allocated.    Returns \n \ndilatedndarray of bool or uint \n\nThe result of the morphological dilation with values in [False, True].     \n binary_erosion  \nskimage.morphology.binary_erosion(image, selem=None, out=None) [source]\n \nReturn fast binary morphological erosion of an image. This function returns the same result as greyscale erosion but performs faster for binary images. Morphological erosion sets a pixel at (i,j) to the minimum over all pixels in the neighborhood centered at (i,j). Erosion shrinks bright regions and enlarges dark regions.  Parameters \n \nimagendarray \n\nBinary input image.  \nselemndarray, optional \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a cross-shaped structuring element (connectivity=1).  \noutndarray of bool, optional \n\nThe array to store the result of the morphology. If None is passed, a new array will be allocated.    Returns \n \nerodedndarray of bool or uint \n\nThe result of the morphological erosion taking values in [False, True].     \n binary_opening  \nskimage.morphology.binary_opening(image, selem=None, out=None) [source]\n \nReturn fast binary morphological opening of an image. This function returns the same result as greyscale opening but performs faster for binary images. The morphological opening on an image is defined as an erosion followed by a dilation. Opening can remove small bright spots (i.e. \u201csalt\u201d) and connect small dark cracks. This tends to \u201copen\u201d up (dark) gaps between (bright) features.  Parameters \n \nimagendarray \n\nBinary input image.  \nselemndarray, optional \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a cross-shaped structuring element (connectivity=1).  \noutndarray of bool, optional \n\nThe array to store the result of the morphology. If None is passed, a new array will be allocated.    Returns \n \nopeningndarray of bool \n\nThe result of the morphological opening.     \n Examples using skimage.morphology.binary_opening\n \n  Flood Fill   black_tophat  \nskimage.morphology.black_tophat(image, selem=None, out=None) [source]\n \nReturn black top hat of an image. The black top hat of an image is defined as its morphological closing minus the original image. This operation returns the dark spots of the image that are smaller than the structuring element. Note that dark spots in the original image are bright spots after the black top hat.  Parameters \n \nimagendarray \n\nImage array.  \nselemndarray, optional \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).  \noutndarray, optional \n\nThe array to store the result of the morphology. If None is passed, a new array will be allocated.    Returns \n \noutarray, same shape and type as image \n\nThe result of the morphological black top hat.      See also  \nwhite_tophat\n\n  References  \n1  \nhttps://en.wikipedia.org/wiki/Top-hat_transform   Examples >>> # Change dark peak to bright peak and subtract background\n>>> import numpy as np\n>>> from skimage.morphology import square\n>>> dark_on_grey = np.array([[7, 6, 6, 6, 7],\n...                          [6, 5, 4, 5, 6],\n...                          [6, 4, 0, 4, 6],\n...                          [6, 5, 4, 5, 6],\n...                          [7, 6, 6, 6, 7]], dtype=np.uint8)\n>>> black_tophat(dark_on_grey, square(3))\narray([[0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 1, 5, 1, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n \n closing  \nskimage.morphology.closing(image, selem=None, out=None) [source]\n \nReturn greyscale morphological closing of an image. The morphological closing on an image is defined as a dilation followed by an erosion. Closing can remove small dark spots (i.e. \u201cpepper\u201d) and connect small bright cracks. This tends to \u201cclose\u201d up (dark) gaps between (bright) features.  Parameters \n \nimagendarray \n\nImage array.  \nselemndarray, optional \n\nThe neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).  \noutndarray, optional \n\nThe array to store the result of the morphology. If None, is passed, a new array will be allocated.    Returns \n \nclosingarray, same shape and type as image \n\nThe result of the morphological closing.     Examples >>> # Close a gap between two bright lines\n>>> import numpy as np\n>>> from skimage.morphology import square\n>>> broken_line = np.array([[0, 0, 0, 0, 0],\n...                         [0, 0, 0, 0, 0],\n...                         [1, 1, 0, 1, 1],\n...                         [0, 0, 0, 0, 0],\n...                         [0, 0, 0, 0, 0]], dtype=np.uint8)\n>>> closing(broken_line, square(3))\narray([[0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1],\n       [0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n \n convex_hull_image  \nskimage.morphology.convex_hull_image(image, offset_coordinates=True, tolerance=1e-10) [source]\n \nCompute the convex hull image of a binary image. The convex hull is the set of pixels included in the smallest convex polygon that surround all white pixels in the input image.  Parameters \n \nimagearray \n\nBinary input image. This array is cast to bool before processing.  \noffset_coordinatesbool, optional \n\nIf True, a pixel at coordinate, e.g., (4, 7) will be represented by coordinates (3.5, 7), (4.5, 7), (4, 6.5), and (4, 7.5). This adds some \u201cextent\u201d to a pixel when computing the hull.  \ntolerancefloat, optional \n\nTolerance when determining whether a point is inside the hull. Due to numerical floating point errors, a tolerance of 0 can result in some points erroneously being classified as being outside the hull.    Returns \n \nhull(M, N) array of bool \n\nBinary image with pixels in convex hull set to True.     References  \n1  \nhttps://blogs.mathworks.com/steve/2011/10/04/binary-image-convex-hull-algorithm-notes/   \n convex_hull_object  \nskimage.morphology.convex_hull_object(image, *, connectivity=2) [source]\n \nCompute the convex hull image of individual objects in a binary image. The convex hull is the set of pixels included in the smallest convex polygon that surround all white pixels in the input image.  Parameters \n \nimage(M, N) ndarray \n\nBinary input image.  \nconnectivity{1, 2}, int, optional \n\nDetermines the neighbors of each pixel. Adjacent elements within a squared distance of connectivity from pixel center are considered neighbors.: 1-connectivity      2-connectivity\n      [ ]           [ ]  [ ]  [ ]\n       |               \\  |  /\n [ ]--[x]--[ ]      [ ]--[x]--[ ]\n       |               /  |  \\\n      [ ]           [ ]  [ ]  [ ]\n    Returns \n \nhullndarray of bool \n\nBinary image with pixels inside convex hull set to True.     Notes This function uses skimage.morphology.label to define unique objects, finds the convex hull of each using convex_hull_image, and combines these regions with logical OR. Be aware the convex hulls of unconnected objects may overlap in the result. If this is suspected, consider using convex_hull_image separately on each object or adjust connectivity. \n cube  \nskimage.morphology.cube(width, dtype=<class 'numpy.uint8'>) [source]\n \nGenerates a cube-shaped structuring element. This is the 3D equivalent of a square. Every pixel along the perimeter has a chessboard distance no greater than radius (radius=floor(width/2)) pixels.  Parameters \n \nwidthint \n\nThe width, height and depth of the cube.    Returns \n \nselemndarray \n\nA structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.    Other Parameters \n \ndtypedata-type \n\nThe data type of the structuring element.     \n diameter_closing  \nskimage.morphology.diameter_closing(image, diameter_threshold=8, connectivity=1, parent=None, tree_traverser=None) [source]\n \nPerform a diameter closing of the image. Diameter closing removes all dark structures of an image with maximal extension smaller than diameter_threshold. The maximal extension is defined as the maximal extension of the bounding box. The operator is also called Bounding Box Closing. In practice, the result is similar to a morphological closing, but long and thin structures are not removed. Technically, this operator is based on the max-tree representation of the image.  Parameters \n \nimagendarray \n\nThe input image for which the diameter_closing is to be calculated. This image can be of any type.  \ndiameter_thresholdunsigned int \n\nThe maximal extension parameter (number of pixels). The default value is 8.  \nconnectivityunsigned int, optional \n\nThe neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.  \nparentndarray, int64, optional \n\nPrecomputed parent image representing the max tree of the inverted image. This function is fast, if precomputed parent and tree_traverser are provided. See Note for further details.  \ntree_traverser1D array, int64, optional \n\nPrecomputed traverser, where the pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent). This function is fast, if precomputed parent and tree_traverser are provided. See Note for further details.    Returns \n \noutputndarray \n\nOutput image of the same shape and type as input image.      See also  \nskimage.morphology.area_opening\n\n\nskimage.morphology.area_closing\n\n\nskimage.morphology.diameter_opening\n\n\nskimage.morphology.max_tree\n\n  Notes If a max-tree representation (parent and tree_traverser) are given to the function, they must be calculated from the inverted image for this function, i.e.: >>> P, S = max_tree(invert(f)) >>> closed = diameter_closing(f, 3, parent=P, tree_traverser=S) References  \n1  \nWalter, T., & Klein, J.-C. (2002). Automatic Detection of Microaneurysms in Color Fundus Images of the Human Retina by Means of the Bounding Box Closing. In A. Colosimo, P. Sirabella, A. Giuliani (Eds.), Medical Data Analysis. Lecture Notes in Computer Science, vol 2526, pp. 210-220. Springer Berlin Heidelberg. DOI:10.1007/3-540-36104-9_23  \n2  \nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. DOI:10.1109/TIP.2014.2336551   Examples We create an image (quadratic function with a minimum in the center and 4 additional local minima. >>> w = 12\n>>> x, y = np.mgrid[0:w,0:w]\n>>> f = 180 + 0.2*((x - w/2)**2 + (y-w/2)**2)\n>>> f[2:3,1:5] = 160; f[2:4,9:11] = 140; f[9:11,2:4] = 120\n>>> f[9:10,9:11] = 100; f[10,10] = 100\n>>> f = f.astype(int)\n We can calculate the diameter closing: >>> closed = diameter_closing(f, 3, connectivity=1)\n All small minima with a maximal extension of 2 or less are removed. The remaining minima have all a maximal extension of at least 3. \n diameter_opening  \nskimage.morphology.diameter_opening(image, diameter_threshold=8, connectivity=1, parent=None, tree_traverser=None) [source]\n \nPerform a diameter opening of the image. Diameter opening removes all bright structures of an image with maximal extension smaller than diameter_threshold. The maximal extension is defined as the maximal extension of the bounding box. The operator is also called Bounding Box Opening. In practice, the result is similar to a morphological opening, but long and thin structures are not removed. Technically, this operator is based on the max-tree representation of the image.  Parameters \n \nimagendarray \n\nThe input image for which the area_opening is to be calculated. This image can be of any type.  \ndiameter_thresholdunsigned int \n\nThe maximal extension parameter (number of pixels). The default value is 8.  \nconnectivityunsigned int, optional \n\nThe neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.  \nparentndarray, int64, optional \n\nParent image representing the max tree of the image. The value of each pixel is the index of its parent in the ravelled array.  \ntree_traverser1D array, int64, optional \n\nThe ordered pixel indices (referring to the ravelled array). The pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent).    Returns \n \noutputndarray \n\nOutput image of the same shape and type as the input image.      See also  \nskimage.morphology.area_opening\n\n\nskimage.morphology.area_closing\n\n\nskimage.morphology.diameter_closing\n\n\nskimage.morphology.max_tree\n\n  References  \n1  \nWalter, T., & Klein, J.-C. (2002). Automatic Detection of Microaneurysms in Color Fundus Images of the Human Retina by Means of the Bounding Box Closing. In A. Colosimo, P. Sirabella, A. Giuliani (Eds.), Medical Data Analysis. Lecture Notes in Computer Science, vol 2526, pp. 210-220. Springer Berlin Heidelberg. DOI:10.1007/3-540-36104-9_23  \n2  \nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. DOI:10.1109/TIP.2014.2336551   Examples We create an image (quadratic function with a maximum in the center and 4 additional local maxima. >>> w = 12\n>>> x, y = np.mgrid[0:w,0:w]\n>>> f = 20 - 0.2*((x - w/2)**2 + (y-w/2)**2)\n>>> f[2:3,1:5] = 40; f[2:4,9:11] = 60; f[9:11,2:4] = 80\n>>> f[9:10,9:11] = 100; f[10,10] = 100\n>>> f = f.astype(int)\n We can calculate the diameter opening: >>> open = diameter_opening(f, 3, connectivity=1)\n The peaks with a maximal extension of 2 or less are removed. The remaining peaks have all a maximal extension of at least 3. \n diamond  \nskimage.morphology.diamond(radius, dtype=<class 'numpy.uint8'>) [source]\n \nGenerates a flat, diamond-shaped structuring element. A pixel is part of the neighborhood (i.e. labeled 1) if the city block/Manhattan distance between it and the center of the neighborhood is no greater than radius.  Parameters \n \nradiusint \n\nThe radius of the diamond-shaped structuring element.    Returns \n \nselemndarray \n\nThe structuring element where elements of the neighborhood are 1 and 0 otherwise.    Other Parameters \n \ndtypedata-type \n\nThe data type of the structuring element.     \n dilation  \nskimage.morphology.dilation(image, selem=None, out=None, shift_x=False, shift_y=False) [source]\n \nReturn greyscale morphological dilation of an image. Morphological dilation sets a pixel at (i,j) to the maximum over all pixels in the neighborhood centered at (i,j). Dilation enlarges bright regions and shrinks dark regions.  Parameters \n \nimagendarray \n\nImage array.  \nselemndarray, optional \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).  \noutndarray, optional \n\nThe array to store the result of the morphology. If None, is passed, a new array will be allocated.  \nshift_x, shift_ybool, optional \n\nshift structuring element about center point. This only affects eccentric structuring elements (i.e. selem with even numbered sides).    Returns \n \ndilateduint8 array, same shape and type as image \n\nThe result of the morphological dilation.     Notes For uint8 (and uint16 up to a certain bit-depth) data, the lower algorithm complexity makes the skimage.filters.rank.maximum function more efficient for larger images and structuring elements. Examples >>> # Dilation enlarges bright regions\n>>> import numpy as np\n>>> from skimage.morphology import square\n>>> bright_pixel = np.array([[0, 0, 0, 0, 0],\n...                          [0, 0, 0, 0, 0],\n...                          [0, 0, 1, 0, 0],\n...                          [0, 0, 0, 0, 0],\n...                          [0, 0, 0, 0, 0]], dtype=np.uint8)\n>>> dilation(bright_pixel, square(3))\narray([[0, 0, 0, 0, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n \n Examples using skimage.morphology.dilation\n \n  Rank filters   disk  \nskimage.morphology.disk(radius, dtype=<class 'numpy.uint8'>) [source]\n \nGenerates a flat, disk-shaped structuring element. A pixel is within the neighborhood if the Euclidean distance between it and the origin is no greater than radius.  Parameters \n \nradiusint \n\nThe radius of the disk-shaped structuring element.    Returns \n \nselemndarray \n\nThe structuring element where elements of the neighborhood are 1 and 0 otherwise.    Other Parameters \n \ndtypedata-type \n\nThe data type of the structuring element.     \n Examples using skimage.morphology.disk\n \n  Local Histogram Equalization  \n\n  Entropy  \n\n  Markers for watershed transform  \n\n  Flood Fill  \n\n  Segment human cells (in mitosis)  \n\n  Rank filters   erosion  \nskimage.morphology.erosion(image, selem=None, out=None, shift_x=False, shift_y=False) [source]\n \nReturn greyscale morphological erosion of an image. Morphological erosion sets a pixel at (i,j) to the minimum over all pixels in the neighborhood centered at (i,j). Erosion shrinks bright regions and enlarges dark regions.  Parameters \n \nimagendarray \n\nImage array.  \nselemndarray, optional \n\nThe neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).  \noutndarrays, optional \n\nThe array to store the result of the morphology. If None is passed, a new array will be allocated.  \nshift_x, shift_ybool, optional \n\nshift structuring element about center point. This only affects eccentric structuring elements (i.e. selem with even numbered sides).    Returns \n \nerodedarray, same shape as image \n\nThe result of the morphological erosion.     Notes For uint8 (and uint16 up to a certain bit-depth) data, the lower algorithm complexity makes the skimage.filters.rank.minimum function more efficient for larger images and structuring elements. Examples >>> # Erosion shrinks bright regions\n>>> import numpy as np\n>>> from skimage.morphology import square\n>>> bright_square = np.array([[0, 0, 0, 0, 0],\n...                           [0, 1, 1, 1, 0],\n...                           [0, 1, 1, 1, 0],\n...                           [0, 1, 1, 1, 0],\n...                           [0, 0, 0, 0, 0]], dtype=np.uint8)\n>>> erosion(bright_square, square(3))\narray([[0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n \n flood  \nskimage.morphology.flood(image, seed_point, *, selem=None, connectivity=None, tolerance=None) [source]\n \nMask corresponding to a flood fill. Starting at a specific seed_point, connected points equal or within tolerance of the seed value are found.  Parameters \n \nimagendarray \n\nAn n-dimensional array.  \nseed_pointtuple or int \n\nThe point in image used as the starting point for the flood fill. If the image is 1D, this point may be given as an integer.  \nselemndarray, optional \n\nA structuring element used to determine the neighborhood of each evaluated pixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as image. If not given, all adjacent pixels are considered as part of the neighborhood (fully connected).  \nconnectivityint, optional \n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is larger or equal to connectivity are considered neighbors. Ignored if selem is not None.  \ntolerancefloat or int, optional \n\nIf None (default), adjacent values must be strictly equal to the initial value of image at seed_point. This is fastest. If a value is given, a comparison will be done at every point and if within tolerance of the initial value will also be filled (inclusive).    Returns \n \nmaskndarray \n\nA Boolean array with the same shape as image is returned, with True values for areas connected to and equal (or within tolerance of) the seed point. All other values are False.     Notes The conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many raster graphics programs. This function returns just the mask representing the fill. If indices are desired rather than masks for memory reasons, the user can simply run numpy.nonzero on the result, save the indices, and discard this mask. Examples >>> from skimage.morphology import flood\n>>> image = np.zeros((4, 7), dtype=int)\n>>> image[1:3, 1:3] = 1\n>>> image[3, 0] = 1\n>>> image[1:3, 4:6] = 2\n>>> image[3, 6] = 3\n>>> image\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 1, 0, 2, 2, 0],\n       [0, 1, 1, 0, 2, 2, 0],\n       [1, 0, 0, 0, 0, 0, 3]])\n Fill connected ones with 5, with full connectivity (diagonals included): >>> mask = flood(image, (1, 1))\n>>> image_flooded = image.copy()\n>>> image_flooded[mask] = 5\n>>> image_flooded\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [5, 0, 0, 0, 0, 0, 3]])\n Fill connected ones with 5, excluding diagonal points (connectivity 1): >>> mask = flood(image, (1, 1), connectivity=1)\n>>> image_flooded = image.copy()\n>>> image_flooded[mask] = 5\n>>> image_flooded\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [1, 0, 0, 0, 0, 0, 3]])\n Fill with a tolerance: >>> mask = flood(image, (0, 0), tolerance=1)\n>>> image_flooded = image.copy()\n>>> image_flooded[mask] = 5\n>>> image_flooded\narray([[5, 5, 5, 5, 5, 5, 5],\n       [5, 5, 5, 5, 2, 2, 5],\n       [5, 5, 5, 5, 2, 2, 5],\n       [5, 5, 5, 5, 5, 5, 3]])\n \n flood_fill  \nskimage.morphology.flood_fill(image, seed_point, new_value, *, selem=None, connectivity=None, tolerance=None, in_place=False, inplace=None) [source]\n \nPerform flood filling on an image. Starting at a specific seed_point, connected points equal or within tolerance of the seed value are found, then set to new_value.  Parameters \n \nimagendarray \n\nAn n-dimensional array.  \nseed_pointtuple or int \n\nThe point in image used as the starting point for the flood fill. If the image is 1D, this point may be given as an integer.  \nnew_valueimage type \n\nNew value to set the entire fill. This must be chosen in agreement with the dtype of image.  \nselemndarray, optional \n\nA structuring element used to determine the neighborhood of each evaluated pixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as image. If not given, all adjacent pixels are considered as part of the neighborhood (fully connected).  \nconnectivityint, optional \n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is less than or equal to connectivity are considered neighbors. Ignored if selem is not None.  \ntolerancefloat or int, optional \n\nIf None (default), adjacent values must be strictly equal to the value of image at seed_point to be filled. This is fastest. If a tolerance is provided, adjacent points with values within plus or minus tolerance from the seed point are filled (inclusive).  \nin_placebool, optional \n\nIf True, flood filling is applied to image in place. If False, the flood filled result is returned without modifying the input image (default).  \ninplacebool, optional \n\nThis parameter is deprecated and will be removed in version 0.19.0 in favor of in_place. If True, flood filling is applied to image inplace. If False, the flood filled result is returned without modifying the input image (default).    Returns \n \nfilledndarray \n\nAn array with the same shape as image is returned, with values in areas connected to and equal (or within tolerance of) the seed point replaced with new_value.     Notes The conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many raster graphics programs. Examples >>> from skimage.morphology import flood_fill\n>>> image = np.zeros((4, 7), dtype=int)\n>>> image[1:3, 1:3] = 1\n>>> image[3, 0] = 1\n>>> image[1:3, 4:6] = 2\n>>> image[3, 6] = 3\n>>> image\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 1, 0, 2, 2, 0],\n       [0, 1, 1, 0, 2, 2, 0],\n       [1, 0, 0, 0, 0, 0, 3]])\n Fill connected ones with 5, with full connectivity (diagonals included): >>> flood_fill(image, (1, 1), 5)\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [5, 0, 0, 0, 0, 0, 3]])\n Fill connected ones with 5, excluding diagonal points (connectivity 1): >>> flood_fill(image, (1, 1), 5, connectivity=1)\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [1, 0, 0, 0, 0, 0, 3]])\n Fill with a tolerance: >>> flood_fill(image, (0, 0), 5, tolerance=1)\narray([[5, 5, 5, 5, 5, 5, 5],\n       [5, 5, 5, 5, 2, 2, 5],\n       [5, 5, 5, 5, 2, 2, 5],\n       [5, 5, 5, 5, 5, 5, 3]])\n \n h_maxima  \nskimage.morphology.h_maxima(image, h, selem=None) [source]\n \nDetermine all maxima of the image with height >= h. The local maxima are defined as connected sets of pixels with equal grey level strictly greater than the grey level of all pixels in direct neighborhood of the set. A local maximum M of height h is a local maximum for which there is at least one path joining M with an equal or higher local maximum on which the minimal value is f(M) - h (i.e. the values along the path are not decreasing by more than h with respect to the maximum\u2019s value) and no path to an equal or higher local maximum for which the minimal value is greater. The global maxima of the image are also found by this function.  Parameters \n \nimagendarray \n\nThe input image for which the maxima are to be calculated.  \nhunsigned integer \n\nThe minimal height of all extracted maxima.  \nselemndarray, optional \n\nThe neighborhood expressed as an n-D array of 1\u2019s and 0\u2019s. Default is the ball of radius 1 according to the maximum norm (i.e. a 3x3 square for 2D images, a 3x3x3 cube for 3D images, etc.)    Returns \n \nh_maxndarray \n\nThe local maxima of height >= h and the global maxima. The resulting image is a binary image, where pixels belonging to the determined maxima take value 1, the others take value 0.      See also  \nskimage.morphology.extrema.h_minima \n\nskimage.morphology.extrema.local_maxima \n\nskimage.morphology.extrema.local_minima \n  References  \n1  \nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d (Chapter 6), 2nd edition (2003), ISBN 3540429883.   Examples >>> import numpy as np\n>>> from skimage.morphology import extrema\n We create an image (quadratic function with a maximum in the center and 4 additional constant maxima. The heights of the maxima are: 1, 21, 41, 61, 81 >>> w = 10\n>>> x, y = np.mgrid[0:w,0:w]\n>>> f = 20 - 0.2*((x - w/2)**2 + (y-w/2)**2)\n>>> f[2:4,2:4] = 40; f[2:4,7:9] = 60; f[7:9,2:4] = 80; f[7:9,7:9] = 100\n>>> f = f.astype(int)\n We can calculate all maxima with a height of at least 40: >>> maxima = extrema.h_maxima(f, 40)\n The resulting image will contain 3 local maxima. \n h_minima  \nskimage.morphology.h_minima(image, h, selem=None) [source]\n \nDetermine all minima of the image with depth >= h. The local minima are defined as connected sets of pixels with equal grey level strictly smaller than the grey levels of all pixels in direct neighborhood of the set. A local minimum M of depth h is a local minimum for which there is at least one path joining M with an equal or lower local minimum on which the maximal value is f(M) + h (i.e. the values along the path are not increasing by more than h with respect to the minimum\u2019s value) and no path to an equal or lower local minimum for which the maximal value is smaller. The global minima of the image are also found by this function.  Parameters \n \nimagendarray \n\nThe input image for which the minima are to be calculated.  \nhunsigned integer \n\nThe minimal depth of all extracted minima.  \nselemndarray, optional \n\nThe neighborhood expressed as an n-D array of 1\u2019s and 0\u2019s. Default is the ball of radius 1 according to the maximum norm (i.e. a 3x3 square for 2D images, a 3x3x3 cube for 3D images, etc.)    Returns \n \nh_minndarray \n\nThe local minima of depth >= h and the global minima. The resulting image is a binary image, where pixels belonging to the determined minima take value 1, the others take value 0.      See also  \nskimage.morphology.extrema.h_maxima \n\nskimage.morphology.extrema.local_maxima \n\nskimage.morphology.extrema.local_minima \n  References  \n1  \nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d (Chapter 6), 2nd edition (2003), ISBN 3540429883.   Examples >>> import numpy as np\n>>> from skimage.morphology import extrema\n We create an image (quadratic function with a minimum in the center and 4 additional constant maxima. The depth of the minima are: 1, 21, 41, 61, 81 >>> w = 10\n>>> x, y = np.mgrid[0:w,0:w]\n>>> f = 180 + 0.2*((x - w/2)**2 + (y-w/2)**2)\n>>> f[2:4,2:4] = 160; f[2:4,7:9] = 140; f[7:9,2:4] = 120; f[7:9,7:9] = 100\n>>> f = f.astype(int)\n We can calculate all minima with a depth of at least 40: >>> minima = extrema.h_minima(f, 40)\n The resulting image will contain 3 local minima. \n label  \nskimage.morphology.label(input, background=None, return_num=False, connectivity=None) [source]\n \nLabel connected regions of an integer array. Two pixels are connected when they are neighbors and have the same value. In 2D, they can be neighbors either in a 1- or 2-connected sense. The value refers to the maximum number of orthogonal hops to consider a pixel/voxel a neighbor: 1-connectivity     2-connectivity     diagonal connection close-up\n\n     [ ]           [ ]  [ ]  [ ]             [ ]\n      |               \\  |  /                 |  <- hop 2\n[ ]--[x]--[ ]      [ ]--[x]--[ ]        [x]--[ ]\n      |               /  |  \\             hop 1\n     [ ]           [ ]  [ ]  [ ]\n  Parameters \n \ninputndarray of dtype int \n\nImage to label.  \nbackgroundint, optional \n\nConsider all pixels with this value as background pixels, and label them as 0. By default, 0-valued pixels are considered as background pixels.  \nreturn_numbool, optional \n\nWhether to return the number of assigned labels.  \nconnectivityint, optional \n\nMaximum number of orthogonal hops to consider a pixel/voxel as a neighbor. Accepted values are ranging from 1 to input.ndim. If None, a full connectivity of input.ndim is used.    Returns \n \nlabelsndarray of dtype int \n\nLabeled array, where all connected regions are assigned the same integer value.  \nnumint, optional \n\nNumber of labels, which equals the maximum label index and is only returned if return_num is True.      See also  \nregionprops \n\nregionprops_table \n  References  \n1  \nChristophe Fiorio and Jens Gustedt, \u201cTwo linear time Union-Find strategies for image processing\u201d, Theoretical Computer Science 154 (1996), pp. 165-181.  \n2  \nKensheng Wu, Ekow Otoo and Arie Shoshani, \u201cOptimizing connected component labeling algorithms\u201d, Paper LBNL-56864, 2005, Lawrence Berkeley National Laboratory (University of California), http://repositories.cdlib.org/lbnl/LBNL-56864   Examples >>> import numpy as np\n>>> x = np.eye(3).astype(int)\n>>> print(x)\n[[1 0 0]\n [0 1 0]\n [0 0 1]]\n>>> print(label(x, connectivity=1))\n[[1 0 0]\n [0 2 0]\n [0 0 3]]\n>>> print(label(x, connectivity=2))\n[[1 0 0]\n [0 1 0]\n [0 0 1]]\n>>> print(label(x, background=-1))\n[[1 2 2]\n [2 1 2]\n [2 2 1]]\n>>> x = np.array([[1, 0, 0],\n...               [1, 1, 5],\n...               [0, 0, 0]])\n>>> print(label(x))\n[[1 0 0]\n [1 1 2]\n [0 0 0]]\n \n local_maxima  \nskimage.morphology.local_maxima(image, selem=None, connectivity=None, indices=False, allow_borders=True) [source]\n \nFind local maxima of n-dimensional array. The local maxima are defined as connected sets of pixels with equal gray level (plateaus) strictly greater than the gray levels of all pixels in the neighborhood.  Parameters \n \nimagendarray \n\nAn n-dimensional array.  \nselemndarray, optional \n\nA structuring element used to determine the neighborhood of each evaluated pixel (True denotes a connected pixel). It must be a boolean array and have the same number of dimensions as image. If neither selem nor connectivity are given, all adjacent pixels are considered as part of the neighborhood.  \nconnectivityint, optional \n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is less than or equal to connectivity are considered neighbors. Ignored if selem is not None.  \nindicesbool, optional \n\nIf True, the output will be a tuple of one-dimensional arrays representing the indices of local maxima in each dimension. If False, the output will be a boolean array with the same shape as image.  \nallow_bordersbool, optional \n\nIf true, plateaus that touch the image border are valid maxima.    Returns \n \nmaximandarray or tuple[ndarray] \n\nIf indices is false, a boolean array with the same shape as image is returned with True indicating the position of local maxima (False otherwise). If indices is true, a tuple of one-dimensional arrays containing the coordinates (indices) of all found maxima.    Warns \n UserWarning\n\nIf allow_borders is false and any dimension of the given image is shorter than 3 samples, maxima can\u2019t exist and a warning is shown.      See also  \nskimage.morphology.local_minima\n\n\nskimage.morphology.h_maxima\n\n\nskimage.morphology.h_minima\n\n  Notes This function operates on the following ideas:  Make a first pass over the image\u2019s last dimension and flag candidates for local maxima by comparing pixels in only one direction. If the pixels aren\u2019t connected in the last dimension all pixels are flagged as candidates instead.  For each candidate:  Perform a flood-fill to find all connected pixels that have the same gray value and are part of the plateau. Consider the connected neighborhood of a plateau: if no bordering sample has a higher gray level, mark the plateau as a definite local maximum.  Examples >>> from skimage.morphology import local_maxima\n>>> image = np.zeros((4, 7), dtype=int)\n>>> image[1:3, 1:3] = 1\n>>> image[3, 0] = 1\n>>> image[1:3, 4:6] = 2\n>>> image[3, 6] = 3\n>>> image\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 1, 0, 2, 2, 0],\n       [0, 1, 1, 0, 2, 2, 0],\n       [1, 0, 0, 0, 0, 0, 3]])\n Find local maxima by comparing to all neighboring pixels (maximal connectivity): >>> local_maxima(image)\narray([[False, False, False, False, False, False, False],\n       [False,  True,  True, False, False, False, False],\n       [False,  True,  True, False, False, False, False],\n       [ True, False, False, False, False, False,  True]])\n>>> local_maxima(image, indices=True)\n(array([1, 1, 2, 2, 3, 3]), array([1, 2, 1, 2, 0, 6]))\n Find local maxima without comparing to diagonal pixels (connectivity 1): >>> local_maxima(image, connectivity=1)\narray([[False, False, False, False, False, False, False],\n       [False,  True,  True, False,  True,  True, False],\n       [False,  True,  True, False,  True,  True, False],\n       [ True, False, False, False, False, False,  True]])\n and exclude maxima that border the image edge: >>> local_maxima(image, connectivity=1, allow_borders=False)\narray([[False, False, False, False, False, False, False],\n       [False,  True,  True, False,  True,  True, False],\n       [False,  True,  True, False,  True,  True, False],\n       [False, False, False, False, False, False, False]])\n \n local_minima  \nskimage.morphology.local_minima(image, selem=None, connectivity=None, indices=False, allow_borders=True) [source]\n \nFind local minima of n-dimensional array. The local minima are defined as connected sets of pixels with equal gray level (plateaus) strictly smaller than the gray levels of all pixels in the neighborhood.  Parameters \n \nimagendarray \n\nAn n-dimensional array.  \nselemndarray, optional \n\nA structuring element used to determine the neighborhood of each evaluated pixel (True denotes a connected pixel). It must be a boolean array and have the same number of dimensions as image. If neither selem nor connectivity are given, all adjacent pixels are considered as part of the neighborhood.  \nconnectivityint, optional \n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is less than or equal to connectivity are considered neighbors. Ignored if selem is not None.  \nindicesbool, optional \n\nIf True, the output will be a tuple of one-dimensional arrays representing the indices of local minima in each dimension. If False, the output will be a boolean array with the same shape as image.  \nallow_bordersbool, optional \n\nIf true, plateaus that touch the image border are valid minima.    Returns \n \nminimandarray or tuple[ndarray] \n\nIf indices is false, a boolean array with the same shape as image is returned with True indicating the position of local minima (False otherwise). If indices is true, a tuple of one-dimensional arrays containing the coordinates (indices) of all found minima.      See also  \nskimage.morphology.local_maxima\n\n\nskimage.morphology.h_maxima\n\n\nskimage.morphology.h_minima\n\n  Notes This function operates on the following ideas:  Make a first pass over the image\u2019s last dimension and flag candidates for local minima by comparing pixels in only one direction. If the pixels aren\u2019t connected in the last dimension all pixels are flagged as candidates instead.  For each candidate:  Perform a flood-fill to find all connected pixels that have the same gray value and are part of the plateau. Consider the connected neighborhood of a plateau: if no bordering sample has a smaller gray level, mark the plateau as a definite local minimum.  Examples >>> from skimage.morphology import local_minima\n>>> image = np.zeros((4, 7), dtype=int)\n>>> image[1:3, 1:3] = -1\n>>> image[3, 0] = -1\n>>> image[1:3, 4:6] = -2\n>>> image[3, 6] = -3\n>>> image\narray([[ 0,  0,  0,  0,  0,  0,  0],\n       [ 0, -1, -1,  0, -2, -2,  0],\n       [ 0, -1, -1,  0, -2, -2,  0],\n       [-1,  0,  0,  0,  0,  0, -3]])\n Find local minima by comparing to all neighboring pixels (maximal connectivity): >>> local_minima(image)\narray([[False, False, False, False, False, False, False],\n       [False,  True,  True, False, False, False, False],\n       [False,  True,  True, False, False, False, False],\n       [ True, False, False, False, False, False,  True]])\n>>> local_minima(image, indices=True)\n(array([1, 1, 2, 2, 3, 3]), array([1, 2, 1, 2, 0, 6]))\n Find local minima without comparing to diagonal pixels (connectivity 1): >>> local_minima(image, connectivity=1)\narray([[False, False, False, False, False, False, False],\n       [False,  True,  True, False,  True,  True, False],\n       [False,  True,  True, False,  True,  True, False],\n       [ True, False, False, False, False, False,  True]])\n and exclude minima that border the image edge: >>> local_minima(image, connectivity=1, allow_borders=False)\narray([[False, False, False, False, False, False, False],\n       [False,  True,  True, False,  True,  True, False],\n       [False,  True,  True, False,  True,  True, False],\n       [False, False, False, False, False, False, False]])\n \n max_tree  \nskimage.morphology.max_tree(image, connectivity=1) [source]\n \nBuild the max tree from an image. Component trees represent the hierarchical structure of the connected components resulting from sequential thresholding operations applied to an image. A connected component at one level is parent of a component at a higher level if the latter is included in the first. A max-tree is an efficient representation of a component tree. A connected component at one level is represented by one reference pixel at this level, which is parent to all other pixels at that level and to the reference pixel at the level above. The max-tree is the basis for many morphological operators, namely connected operators.  Parameters \n \nimagendarray \n\nThe input image for which the max-tree is to be calculated. This image can be of any type.  \nconnectivityunsigned int, optional \n\nThe neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.    Returns \n \nparentndarray, int64 \n\nArray of same shape as image. The value of each pixel is the index of its parent in the ravelled array.  \ntree_traverser1D array, int64 \n\nThe ordered pixel indices (referring to the ravelled array). The pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent).     References  \n1  \nSalembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected Operators for Image and Sequence Processing. IEEE Transactions on Image Processing, 7(4), 555-570. DOI:10.1109/83.663500  \n2  \nBerger, C., Geraud, T., Levillain, R., Widynski, N., Baillard, A., Bertin, E. (2007). Effective Component Tree Computation with Application to Pattern Recognition in Astronomical Imaging. In International Conference on Image Processing (ICIP) (pp. 41-44). DOI:10.1109/ICIP.2007.4379949  \n3  \nNajman, L., & Couprie, M. (2006). Building the component tree in quasi-linear time. IEEE Transactions on Image Processing, 15(11), 3531-3539. DOI:10.1109/TIP.2006.877518  \n4  \nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. DOI:10.1109/TIP.2014.2336551   Examples We create a small sample image (Figure 1 from [4]) and build the max-tree. >>> image = np.array([[15, 13, 16], [12, 12, 10], [16, 12, 14]])\n>>> P, S = max_tree(image, connectivity=2)\n \n max_tree_local_maxima  \nskimage.morphology.max_tree_local_maxima(image, connectivity=1, parent=None, tree_traverser=None) [source]\n \nDetermine all local maxima of the image. The local maxima are defined as connected sets of pixels with equal gray level strictly greater than the gray levels of all pixels in direct neighborhood of the set. The function labels the local maxima. Technically, the implementation is based on the max-tree representation of an image. The function is very efficient if the max-tree representation has already been computed. Otherwise, it is preferable to use the function local_maxima.  Parameters \n \nimagendarray \n\nThe input image for which the maxima are to be calculated.  connectivity: unsigned int, optional\n\nThe neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.  parent: ndarray, int64, optional\n\nThe value of each pixel is the index of its parent in the ravelled array.  tree_traverser: 1D array, int64, optional\n\nThe ordered pixel indices (referring to the ravelled array). The pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent).    Returns \n \nlocal_maxndarray, uint64 \n\nLabeled local maxima of the image.      See also  \nskimage.morphology.local_maxima\n\n\nskimage.morphology.max_tree\n\n  References  \n1  \nVincent L., Proc. \u201cGrayscale area openings and closings, their efficient implementation and applications\u201d, EURASIP Workshop on Mathematical Morphology and its Applications to Signal Processing, Barcelona, Spain, pp.22-27, May 1993.  \n2  \nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d (Chapter 6), 2nd edition (2003), ISBN 3540429883. DOI:10.1007/978-3-662-05088-0  \n3  \nSalembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected Operators for Image and Sequence Processing. IEEE Transactions on Image Processing, 7(4), 555-570. DOI:10.1109/83.663500  \n4  \nNajman, L., & Couprie, M. (2006). Building the component tree in quasi-linear time. IEEE Transactions on Image Processing, 15(11), 3531-3539. DOI:10.1109/TIP.2006.877518  \n5  \nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. DOI:10.1109/TIP.2014.2336551   Examples We create an image (quadratic function with a maximum in the center and 4 additional constant maxima. >>> w = 10\n>>> x, y = np.mgrid[0:w,0:w]\n>>> f = 20 - 0.2*((x - w/2)**2 + (y-w/2)**2)\n>>> f[2:4,2:4] = 40; f[2:4,7:9] = 60; f[7:9,2:4] = 80; f[7:9,7:9] = 100\n>>> f = f.astype(int)\n We can calculate all local maxima: >>> maxima = max_tree_local_maxima(f)\n The resulting image contains the labeled local maxima. \n medial_axis  \nskimage.morphology.medial_axis(image, mask=None, return_distance=False) [source]\n \nCompute the medial axis transform of a binary image  Parameters \n \nimagebinary ndarray, shape (M, N) \n\nThe image of the shape to be skeletonized.  \nmaskbinary ndarray, shape (M, N), optional \n\nIf a mask is given, only those elements in image with a true value in mask are used for computing the medial axis.  \nreturn_distancebool, optional \n\nIf true, the distance transform is returned as well as the skeleton.    Returns \n \noutndarray of bools \n\nMedial axis transform of the image  \ndistndarray of ints, optional \n\nDistance transform of the image (only returned if return_distance is True)      See also  \nskeletonize\n\n  Notes This algorithm computes the medial axis transform of an image as the ridges of its distance transform.  The different steps of the algorithm are as follows\n\n A lookup table is used, that assigns 0 or 1 to each configuration of the 3x3 binary square, whether the central pixel should be removed or kept. We want a point to be removed if it has more than one neighbor and if removing it does not change the number of connected components. The distance transform to the background is computed, as well as the cornerness of the pixel. The foreground (value of 1) points are ordered by the distance transform, then the cornerness. A cython function is called to reduce the image to its skeleton. It processes pixels in the order determined at the previous step, and removes or maintains a pixel according to the lookup table. Because of the ordering, it is possible to process all pixels in only one pass.    Examples >>> square = np.zeros((7, 7), dtype=np.uint8)\n>>> square[1:-1, 2:-2] = 1\n>>> square\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n>>> medial_axis(square).astype(np.uint8)\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n octagon  \nskimage.morphology.octagon(m, n, dtype=<class 'numpy.uint8'>) [source]\n \nGenerates an octagon shaped structuring element. For a given size of (m) horizontal and vertical sides and a given (n) height or width of slanted sides octagon is generated. The slanted sides are 45 or 135 degrees to the horizontal axis and hence the widths and heights are equal.  Parameters \n \nmint \n\nThe size of the horizontal and vertical sides.  \nnint \n\nThe height or width of the slanted sides.    Returns \n \nselemndarray \n\nThe structuring element where elements of the neighborhood are 1 and 0 otherwise.    Other Parameters \n \ndtypedata-type \n\nThe data type of the structuring element.     \n octahedron  \nskimage.morphology.octahedron(radius, dtype=<class 'numpy.uint8'>) [source]\n \nGenerates a octahedron-shaped structuring element. This is the 3D equivalent of a diamond. A pixel is part of the neighborhood (i.e. labeled 1) if the city block/Manhattan distance between it and the center of the neighborhood is no greater than radius.  Parameters \n \nradiusint \n\nThe radius of the octahedron-shaped structuring element.    Returns \n \nselemndarray \n\nThe structuring element where elements of the neighborhood are 1 and 0 otherwise.    Other Parameters \n \ndtypedata-type \n\nThe data type of the structuring element.     \n opening  \nskimage.morphology.opening(image, selem=None, out=None) [source]\n \nReturn greyscale morphological opening of an image. The morphological opening on an image is defined as an erosion followed by a dilation. Opening can remove small bright spots (i.e. \u201csalt\u201d) and connect small dark cracks. This tends to \u201copen\u201d up (dark) gaps between (bright) features.  Parameters \n \nimagendarray \n\nImage array.  \nselemndarray, optional \n\nThe neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).  \noutndarray, optional \n\nThe array to store the result of the morphology. If None is passed, a new array will be allocated.    Returns \n \nopeningarray, same shape and type as image \n\nThe result of the morphological opening.     Examples >>> # Open up gap between two bright regions (but also shrink regions)\n>>> import numpy as np\n>>> from skimage.morphology import square\n>>> bad_connection = np.array([[1, 0, 0, 0, 1],\n...                            [1, 1, 0, 1, 1],\n...                            [1, 1, 1, 1, 1],\n...                            [1, 1, 0, 1, 1],\n...                            [1, 0, 0, 0, 1]], dtype=np.uint8)\n>>> opening(bad_connection, square(3))\narray([[0, 0, 0, 0, 0],\n       [1, 1, 0, 1, 1],\n       [1, 1, 0, 1, 1],\n       [1, 1, 0, 1, 1],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n \n reconstruction  \nskimage.morphology.reconstruction(seed, mask, method='dilation', selem=None, offset=None) [source]\n \nPerform a morphological reconstruction of an image. Morphological reconstruction by dilation is similar to basic morphological dilation: high-intensity values will replace nearby low-intensity values. The basic dilation operator, however, uses a structuring element to determine how far a value in the input image can spread. In contrast, reconstruction uses two images: a \u201cseed\u201d image, which specifies the values that spread, and a \u201cmask\u201d image, which gives the maximum allowed value at each pixel. The mask image, like the structuring element, limits the spread of high-intensity values. Reconstruction by erosion is simply the inverse: low-intensity values spread from the seed image and are limited by the mask image, which represents the minimum allowed value. Alternatively, you can think of reconstruction as a way to isolate the connected regions of an image. For dilation, reconstruction connects regions marked by local maxima in the seed image: neighboring pixels less-than-or-equal-to those seeds are connected to the seeded region. Local maxima with values larger than the seed image will get truncated to the seed value.  Parameters \n \nseedndarray \n\nThe seed image (a.k.a. marker image), which specifies the values that are dilated or eroded.  \nmaskndarray \n\nThe maximum (dilation) / minimum (erosion) allowed value at each pixel.  \nmethod{\u2018dilation\u2019|\u2019erosion\u2019}, optional \n\nPerform reconstruction by dilation or erosion. In dilation (or erosion), the seed image is dilated (or eroded) until limited by the mask image. For dilation, each seed value must be less than or equal to the corresponding mask value; for erosion, the reverse is true. Default is \u2018dilation\u2019.  \nselemndarray, optional \n\nThe neighborhood expressed as an n-D array of 1\u2019s and 0\u2019s. Default is the n-D square of radius equal to 1 (i.e. a 3x3 square for 2D images, a 3x3x3 cube for 3D images, etc.)  \noffsetndarray, optional \n\nThe coordinates of the center of the structuring element. Default is located on the geometrical center of the selem, in that case selem dimensions must be odd.    Returns \n \nreconstructedndarray \n\nThe result of morphological reconstruction.     Notes The algorithm is taken from [1]. Applications for greyscale reconstruction are discussed in [2] and [3]. References  \n1  \nRobinson, \u201cEfficient morphological reconstruction: a downhill filter\u201d, Pattern Recognition Letters 25 (2004) 1759-1767.  \n2  \nVincent, L., \u201cMorphological Grayscale Reconstruction in Image Analysis: Applications and Efficient Algorithms\u201d, IEEE Transactions on Image Processing (1993)  \n3  \nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d, Chapter 6, 2nd edition (2003), ISBN 3540429883.   Examples >>> import numpy as np\n>>> from skimage.morphology import reconstruction\n First, we create a sinusoidal mask image with peaks at middle and ends. >>> x = np.linspace(0, 4 * np.pi)\n>>> y_mask = np.cos(x)\n Then, we create a seed image initialized to the minimum mask value (for reconstruction by dilation, min-intensity values don\u2019t spread) and add \u201cseeds\u201d to the left and right peak, but at a fraction of peak value (1). >>> y_seed = y_mask.min() * np.ones_like(x)\n>>> y_seed[0] = 0.5\n>>> y_seed[-1] = 0\n>>> y_rec = reconstruction(y_seed, y_mask)\n The reconstructed image (or curve, in this case) is exactly the same as the mask image, except that the peaks are truncated to 0.5 and 0. The middle peak disappears completely: Since there were no seed values in this peak region, its reconstructed value is truncated to the surrounding value (-1). As a more practical example, we try to extract the bright features of an image by subtracting a background image created by reconstruction. >>> y, x = np.mgrid[:20:0.5, :20:0.5]\n>>> bumps = np.sin(x) + np.sin(y)\n To create the background image, set the mask image to the original image, and the seed image to the original image with an intensity offset, h. >>> h = 0.3\n>>> seed = bumps - h\n>>> background = reconstruction(seed, bumps)\n The resulting reconstructed image looks exactly like the original image, but with the peaks of the bumps cut off. Subtracting this reconstructed image from the original image leaves just the peaks of the bumps >>> hdome = bumps - background\n This operation is known as the h-dome of the image and leaves features of height h in the subtracted image. \n rectangle  \nskimage.morphology.rectangle(nrows, ncols, dtype=<class 'numpy.uint8'>) [source]\n \nGenerates a flat, rectangular-shaped structuring element. Every pixel in the rectangle generated for a given width and given height belongs to the neighborhood.  Parameters \n \nnrowsint \n\nThe number of rows of the rectangle.  \nncolsint \n\nThe number of columns of the rectangle.    Returns \n \nselemndarray \n\nA structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.    Other Parameters \n \ndtypedata-type \n\nThe data type of the structuring element.     Notes  The use of width and height has been deprecated in version 0.18.0. Use nrows and ncols instead.  \n remove_small_holes  \nskimage.morphology.remove_small_holes(ar, area_threshold=64, connectivity=1, in_place=False) [source]\n \nRemove contiguous holes smaller than the specified size.  Parameters \n \narndarray (arbitrary shape, int or bool type) \n\nThe array containing the connected components of interest.  \narea_thresholdint, optional (default: 64) \n\nThe maximum area, in pixels, of a contiguous hole that will be filled. Replaces min_size.  \nconnectivityint, {1, 2, \u2026, ar.ndim}, optional (default: 1) \n\nThe connectivity defining the neighborhood of a pixel.  \nin_placebool, optional (default: False) \n\nIf True, remove the connected components in the input array itself. Otherwise, make a copy.    Returns \n \noutndarray, same shape and type as input ar \n\nThe input array with small holes within connected components removed.    Raises \n TypeError\n\nIf the input array is of an invalid type, such as float or string.  ValueError\n\nIf the input array contains negative values.     Notes If the array type is int, it is assumed that it contains already-labeled objects. The labels are not kept in the output image (this function always outputs a bool image). It is suggested that labeling is completed after using this function. Examples >>> from skimage import morphology\n>>> a = np.array([[1, 1, 1, 1, 1, 0],\n...               [1, 1, 1, 0, 1, 0],\n...               [1, 0, 0, 1, 1, 0],\n...               [1, 1, 1, 1, 1, 0]], bool)\n>>> b = morphology.remove_small_holes(a, 2)\n>>> b\narray([[ True,  True,  True,  True,  True, False],\n       [ True,  True,  True,  True,  True, False],\n       [ True, False, False,  True,  True, False],\n       [ True,  True,  True,  True,  True, False]])\n>>> c = morphology.remove_small_holes(a, 2, connectivity=2)\n>>> c\narray([[ True,  True,  True,  True,  True, False],\n       [ True,  True,  True, False,  True, False],\n       [ True, False, False,  True,  True, False],\n       [ True,  True,  True,  True,  True, False]])\n>>> d = morphology.remove_small_holes(a, 2, in_place=True)\n>>> d is a\nTrue\n \n Examples using skimage.morphology.remove_small_holes\n \n  Measure region properties   remove_small_objects  \nskimage.morphology.remove_small_objects(ar, min_size=64, connectivity=1, in_place=False) [source]\n \nRemove objects smaller than the specified size. Expects ar to be an array with labeled objects, and removes objects smaller than min_size. If ar is bool, the image is first labeled. This leads to potentially different behavior for bool and 0-and-1 arrays.  Parameters \n \narndarray (arbitrary shape, int or bool type) \n\nThe array containing the objects of interest. If the array type is int, the ints must be non-negative.  \nmin_sizeint, optional (default: 64) \n\nThe smallest allowable object size.  \nconnectivityint, {1, 2, \u2026, ar.ndim}, optional (default: 1) \n\nThe connectivity defining the neighborhood of a pixel. Used during labelling if ar is bool.  \nin_placebool, optional (default: False) \n\nIf True, remove the objects in the input array itself. Otherwise, make a copy.    Returns \n \noutndarray, same shape and type as input ar \n\nThe input array with small connected components removed.    Raises \n TypeError\n\nIf the input array is of an invalid type, such as float or string.  ValueError\n\nIf the input array contains negative values.     Examples >>> from skimage import morphology\n>>> a = np.array([[0, 0, 0, 1, 0],\n...               [1, 1, 1, 0, 0],\n...               [1, 1, 1, 0, 1]], bool)\n>>> b = morphology.remove_small_objects(a, 6)\n>>> b\narray([[False, False, False, False, False],\n       [ True,  True,  True, False, False],\n       [ True,  True,  True, False, False]])\n>>> c = morphology.remove_small_objects(a, 7, connectivity=2)\n>>> c\narray([[False, False, False,  True, False],\n       [ True,  True,  True, False, False],\n       [ True,  True,  True, False, False]])\n>>> d = morphology.remove_small_objects(a, 6, in_place=True)\n>>> d is a\nTrue\n \n Examples using skimage.morphology.remove_small_objects\n \n  Measure region properties   skeletonize  \nskimage.morphology.skeletonize(image, *, method=None) [source]\n \nCompute the skeleton of a binary image. Thinning is used to reduce each connected component in a binary image to a single-pixel wide skeleton.  Parameters \n \nimagendarray, 2D or 3D \n\nA binary image containing the objects to be skeletonized. Zeros represent background, nonzero values are foreground.  \nmethod{\u2018zhang\u2019, \u2018lee\u2019}, optional \n\nWhich algorithm to use. Zhang\u2019s algorithm [Zha84] only works for 2D images, and is the default for 2D. Lee\u2019s algorithm [Lee94] works for 2D or 3D images and is the default for 3D.    Returns \n \nskeletonndarray \n\nThe thinned image.      See also  \nmedial_axis\n\n  References  \nLee94  \nT.-C. Lee, R.L. Kashyap and C.-N. Chu, Building skeleton models via 3-D medial surface/axis thinning algorithms. Computer Vision, Graphics, and Image Processing, 56(6):462-478, 1994.  \nZha84  \nA fast parallel algorithm for thinning digital patterns, T. Y. Zhang and C. Y. Suen, Communications of the ACM, March 1984, Volume 27, Number 3.   Examples >>> X, Y = np.ogrid[0:9, 0:9]\n>>> ellipse = (1./3 * (X - 4)**2 + (Y - 4)**2 < 3**2).astype(np.uint8)\n>>> ellipse\narray([[0, 0, 0, 1, 1, 1, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 1, 1, 1, 0, 0, 0]], dtype=uint8)\n>>> skel = skeletonize(ellipse)\n>>> skel.astype(np.uint8)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n skeletonize_3d  \nskimage.morphology.skeletonize_3d(image) [source]\n \nCompute the skeleton of a binary image. Thinning is used to reduce each connected component in a binary image to a single-pixel wide skeleton.  Parameters \n \nimagendarray, 2D or 3D \n\nA binary image containing the objects to be skeletonized. Zeros represent background, nonzero values are foreground.    Returns \n \nskeletonndarray \n\nThe thinned image.      See also  \nskeletonize, medial_axis\n\n  Notes The method of [Lee94] uses an octree data structure to examine a 3x3x3 neighborhood of a pixel. The algorithm proceeds by iteratively sweeping over the image, and removing pixels at each iteration until the image stops changing. Each iteration consists of two steps: first, a list of candidates for removal is assembled; then pixels from this list are rechecked sequentially, to better preserve connectivity of the image. The algorithm this function implements is different from the algorithms used by either skeletonize or medial_axis, thus for 2D images the results produced by this function are generally different. References  \nLee94  \nT.-C. Lee, R.L. Kashyap and C.-N. Chu, Building skeleton models via 3-D medial surface/axis thinning algorithms. Computer Vision, Graphics, and Image Processing, 56(6):462-478, 1994.   \n square  \nskimage.morphology.square(width, dtype=<class 'numpy.uint8'>) [source]\n \nGenerates a flat, square-shaped structuring element. Every pixel along the perimeter has a chessboard distance no greater than radius (radius=floor(width/2)) pixels.  Parameters \n \nwidthint \n\nThe width and height of the square.    Returns \n \nselemndarray \n\nA structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.    Other Parameters \n \ndtypedata-type \n\nThe data type of the structuring element.     \n star  \nskimage.morphology.star(a, dtype=<class 'numpy.uint8'>) [source]\n \nGenerates a star shaped structuring element. Start has 8 vertices and is an overlap of square of size 2*a + 1 with its 45 degree rotated version. The slanted sides are 45 or 135 degrees to the horizontal axis.  Parameters \n \naint \n\nParameter deciding the size of the star structural element. The side of the square array returned is 2*a + 1 + 2*floor(a / 2).    Returns \n \nselemndarray \n\nThe structuring element where elements of the neighborhood are 1 and 0 otherwise.    Other Parameters \n \ndtypedata-type \n\nThe data type of the structuring element.     \n thin  \nskimage.morphology.thin(image, max_iter=None) [source]\n \nPerform morphological thinning of a binary image.  Parameters \n \nimagebinary (M, N) ndarray \n\nThe image to be thinned.  \nmax_iterint, number of iterations, optional \n\nRegardless of the value of this parameter, the thinned image is returned immediately if an iteration produces no change. If this parameter is specified it thus sets an upper bound on the number of iterations performed.    Returns \n \noutndarray of bool \n\nThinned image.      See also  \nskeletonize, medial_axis\n\n  Notes This algorithm [1] works by making multiple passes over the image, removing pixels matching a set of criteria designed to thin connected regions while preserving eight-connected components and 2 x 2 squares [2]. In each of the two sub-iterations the algorithm correlates the intermediate skeleton image with a neighborhood mask, then looks up each neighborhood in a lookup table indicating whether the central pixel should be deleted in that sub-iteration. References  \n1  \nZ. Guo and R. W. Hall, \u201cParallel thinning with two-subiteration algorithms,\u201d Comm. ACM, vol. 32, no. 3, pp. 359-373, 1989. DOI:10.1145/62065.62074  \n2  \nLam, L., Seong-Whan Lee, and Ching Y. Suen, \u201cThinning Methodologies-A Comprehensive Survey,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol 14, No. 9, p. 879, 1992. DOI:10.1109/34.161346   Examples >>> square = np.zeros((7, 7), dtype=np.uint8)\n>>> square[1:-1, 2:-2] = 1\n>>> square[0, 1] =  1\n>>> square\narray([[0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n>>> skel = thin(square)\n>>> skel.astype(np.uint8)\narray([[0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n watershed  \nskimage.morphology.watershed(image, markers=None, connectivity=1, offset=None, mask=None, compactness=0, watershed_line=False) [source]\n \nDeprecated function. Use skimage.segmentation.watershed instead. Find watershed basins in image flooded from given markers.  Parameters \n \nimagendarray (2-D, 3-D, \u2026) of integers \n\nData array where the lowest value points are labeled first.  \nmarkersint, or ndarray of int, same shape as image, optional \n\nThe desired number of markers, or an array marking the basins with the values to be assigned in the label matrix. Zero means not a marker. If None (no markers given), the local minima of the image are used as markers.  \nconnectivityndarray, optional \n\nAn array with the same number of dimensions as image whose non-zero elements indicate neighbors for connection. Following the scipy convention, default is a one-connected array of the dimension of the image.  \noffsetarray_like of shape image.ndim, optional \n\noffset of the connectivity (one offset per dimension)  \nmaskndarray of bools or 0s and 1s, optional \n\nArray of same shape as image. Only points at which mask == True will be labeled.  \ncompactnessfloat, optional \n\nUse compact watershed [3] with given compactness parameter. Higher values result in more regularly-shaped watershed basins.  \nwatershed_linebool, optional \n\nIf watershed_line is True, a one-pixel wide line separates the regions obtained by the watershed algorithm. The line has the label 0.    Returns \n out: ndarray\n\nA labeled matrix of the same type and shape as markers      See also  \nskimage.segmentation.random_walker\n\n\nrandom walker segmentation A segmentation algorithm based on anisotropic diffusion, usually slower than the watershed but with good results on noisy data and boundaries with holes.    Notes This function implements a watershed algorithm [1] [2] that apportions pixels into marked basins. The algorithm uses a priority queue to hold the pixels with the metric for the priority queue being pixel value, then the time of entry into the queue - this settles ties in favor of the closest marker. Some ideas taken from Soille, \u201cAutomated Basin Delineation from Digital Elevation Models Using Mathematical Morphology\u201d, Signal Processing 20 (1990) 171-182 The most important insight in the paper is that entry time onto the queue solves two problems: a pixel should be assigned to the neighbor with the largest gradient or, if there is no gradient, pixels on a plateau should be split between markers on opposite sides. This implementation converts all arguments to specific, lowest common denominator types, then passes these to a C algorithm. Markers can be determined manually, or automatically using for example the local minima of the gradient of the image, or the local maxima of the distance function to the background for separating overlapping objects (see example). References  \n1  \nhttps://en.wikipedia.org/wiki/Watershed_%28image_processing%29  \n2  \nhttp://cmm.ensmp.fr/~beucher/wtshed.html  \n3  \nPeer Neubert & Peter Protzel (2014). Compact Watershed and Preemptive SLIC: On Improving Trade-offs of Superpixel Segmentation Algorithms. ICPR 2014, pp 996-1001. DOI:10.1109/ICPR.2014.181 https://www.tu-chemnitz.de/etit/proaut/publications/cws_pSLIC_ICPR.pdf   Examples The watershed algorithm is useful to separate overlapping objects. We first generate an initial image with two overlapping circles: >>> import numpy as np\n>>> x, y = np.indices((80, 80))\n>>> x1, y1, x2, y2 = 28, 28, 44, 52\n>>> r1, r2 = 16, 20\n>>> mask_circle1 = (x - x1)**2 + (y - y1)**2 < r1**2\n>>> mask_circle2 = (x - x2)**2 + (y - y2)**2 < r2**2\n>>> image = np.logical_or(mask_circle1, mask_circle2)\n Next, we want to separate the two circles. We generate markers at the maxima of the distance to the background: >>> from scipy import ndimage as ndi\n>>> distance = ndi.distance_transform_edt(image)\n>>> from skimage.feature import peak_local_max\n>>> local_maxi = peak_local_max(distance, labels=image,\n...                             footprint=np.ones((3, 3)),\n...                             indices=False)\n>>> markers = ndi.label(local_maxi)[0]\n Finally, we run the watershed on the image and markers: >>> labels = watershed(-distance, markers, mask=image)  \n The algorithm works also for 3-D images, and can be used for example to separate overlapping spheres. \n white_tophat  \nskimage.morphology.white_tophat(image, selem=None, out=None) [source]\n \nReturn white top hat of an image. The white top hat of an image is defined as the image minus its morphological opening. This operation returns the bright spots of the image that are smaller than the structuring element.  Parameters \n \nimagendarray \n\nImage array.  \nselemndarray, optional \n\nThe neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).  \noutndarray, optional \n\nThe array to store the result of the morphology. If None is passed, a new array will be allocated.    Returns \n \noutarray, same shape and type as image \n\nThe result of the morphological white top hat.      See also  \nblack_tophat\n\n  References  \n1  \nhttps://en.wikipedia.org/wiki/Top-hat_transform   Examples >>> # Subtract grey background from bright peak\n>>> import numpy as np\n>>> from skimage.morphology import square\n>>> bright_on_grey = np.array([[2, 3, 3, 3, 2],\n...                            [3, 4, 5, 4, 3],\n...                            [3, 5, 9, 5, 3],\n...                            [3, 4, 5, 4, 3],\n...                            [2, 3, 3, 3, 2]], dtype=np.uint8)\n>>> white_tophat(bright_on_grey, square(3))\narray([[0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 1, 5, 1, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n \n\n"}, {"name": "morphology.area_closing()", "path": "api/skimage.morphology#skimage.morphology.area_closing", "type": "morphology", "text": " \nskimage.morphology.area_closing(image, area_threshold=64, connectivity=1, parent=None, tree_traverser=None) [source]\n \nPerform an area closing of the image. Area closing removes all dark structures of an image with a surface smaller than area_threshold. The output image is larger than or equal to the input image for every pixel and all local minima have at least a surface of area_threshold pixels. Area closings are similar to morphological closings, but they do not use a fixed structuring element, but rather a deformable one, with surface = area_threshold. In the binary case, area closings are equivalent to remove_small_holes; this operator is thus extended to gray-level images. Technically, this operator is based on the max-tree representation of the image.  Parameters \n \nimagendarray \n\nThe input image for which the area_closing is to be calculated. This image can be of any type.  \narea_thresholdunsigned int \n\nThe size parameter (number of pixels). The default value is arbitrarily chosen to be 64.  \nconnectivityunsigned int, optional \n\nThe neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.  \nparentndarray, int64, optional \n\nParent image representing the max tree of the inverted image. The value of each pixel is the index of its parent in the ravelled array. See Note for further details.  \ntree_traverser1D array, int64, optional \n\nThe ordered pixel indices (referring to the ravelled array). The pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent).    Returns \n \noutputndarray \n\nOutput image of the same shape and type as input image.      See also  \nskimage.morphology.area_opening\n\n\nskimage.morphology.diameter_opening\n\n\nskimage.morphology.diameter_closing\n\n\nskimage.morphology.max_tree\n\n\nskimage.morphology.remove_small_objects\n\n\nskimage.morphology.remove_small_holes\n\n  Notes If a max-tree representation (parent and tree_traverser) are given to the function, they must be calculated from the inverted image for this function, i.e.: >>> P, S = max_tree(invert(f)) >>> closed = diameter_closing(f, 3, parent=P, tree_traverser=S) References  \n1  \nVincent L., Proc. \u201cGrayscale area openings and closings, their efficient implementation and applications\u201d, EURASIP Workshop on Mathematical Morphology and its Applications to Signal Processing, Barcelona, Spain, pp.22-27, May 1993.  \n2  \nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d (Chapter 6), 2nd edition (2003), ISBN 3540429883. DOI:10.1007/978-3-662-05088-0  \n3  \nSalembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected Operators for Image and Sequence Processing. IEEE Transactions on Image Processing, 7(4), 555-570. DOI:10.1109/83.663500  \n4  \nNajman, L., & Couprie, M. (2006). Building the component tree in quasi-linear time. IEEE Transactions on Image Processing, 15(11), 3531-3539. DOI:10.1109/TIP.2006.877518  \n5  \nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. DOI:10.1109/TIP.2014.2336551   Examples We create an image (quadratic function with a minimum in the center and 4 additional local minima. >>> w = 12\n>>> x, y = np.mgrid[0:w,0:w]\n>>> f = 180 + 0.2*((x - w/2)**2 + (y-w/2)**2)\n>>> f[2:3,1:5] = 160; f[2:4,9:11] = 140; f[9:11,2:4] = 120\n>>> f[9:10,9:11] = 100; f[10,10] = 100\n>>> f = f.astype(int)\n We can calculate the area closing: >>> closed = area_closing(f, 8, connectivity=1)\n All small minima are removed, and the remaining minima have at least a size of 8. \n"}, {"name": "morphology.area_opening()", "path": "api/skimage.morphology#skimage.morphology.area_opening", "type": "morphology", "text": " \nskimage.morphology.area_opening(image, area_threshold=64, connectivity=1, parent=None, tree_traverser=None) [source]\n \nPerform an area opening of the image. Area opening removes all bright structures of an image with a surface smaller than area_threshold. The output image is thus the largest image smaller than the input for which all local maxima have at least a surface of area_threshold pixels. Area openings are similar to morphological openings, but they do not use a fixed structuring element, but rather a deformable one, with surface = area_threshold. Consequently, the area_opening with area_threshold=1 is the identity. In the binary case, area openings are equivalent to remove_small_objects; this operator is thus extended to gray-level images. Technically, this operator is based on the max-tree representation of the image.  Parameters \n \nimagendarray \n\nThe input image for which the area_opening is to be calculated. This image can be of any type.  \narea_thresholdunsigned int \n\nThe size parameter (number of pixels). The default value is arbitrarily chosen to be 64.  \nconnectivityunsigned int, optional \n\nThe neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.  \nparentndarray, int64, optional \n\nParent image representing the max tree of the image. The value of each pixel is the index of its parent in the ravelled array.  \ntree_traverser1D array, int64, optional \n\nThe ordered pixel indices (referring to the ravelled array). The pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent).    Returns \n \noutputndarray \n\nOutput image of the same shape and type as the input image.      See also  \nskimage.morphology.area_closing\n\n\nskimage.morphology.diameter_opening\n\n\nskimage.morphology.diameter_closing\n\n\nskimage.morphology.max_tree\n\n\nskimage.morphology.remove_small_objects\n\n\nskimage.morphology.remove_small_holes\n\n  References  \n1  \nVincent L., Proc. \u201cGrayscale area openings and closings, their efficient implementation and applications\u201d, EURASIP Workshop on Mathematical Morphology and its Applications to Signal Processing, Barcelona, Spain, pp.22-27, May 1993.  \n2  \nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d (Chapter 6), 2nd edition (2003), ISBN 3540429883. :DOI:10.1007/978-3-662-05088-0  \n3  \nSalembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected Operators for Image and Sequence Processing. IEEE Transactions on Image Processing, 7(4), 555-570. :DOI:10.1109/83.663500  \n4  \nNajman, L., & Couprie, M. (2006). Building the component tree in quasi-linear time. IEEE Transactions on Image Processing, 15(11), 3531-3539. :DOI:10.1109/TIP.2006.877518  \n5  \nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. :DOI:10.1109/TIP.2014.2336551   Examples We create an image (quadratic function with a maximum in the center and 4 additional local maxima. >>> w = 12\n>>> x, y = np.mgrid[0:w,0:w]\n>>> f = 20 - 0.2*((x - w/2)**2 + (y-w/2)**2)\n>>> f[2:3,1:5] = 40; f[2:4,9:11] = 60; f[9:11,2:4] = 80\n>>> f[9:10,9:11] = 100; f[10,10] = 100\n>>> f = f.astype(int)\n We can calculate the area opening: >>> open = area_opening(f, 8, connectivity=1)\n The peaks with a surface smaller than 8 are removed. \n"}, {"name": "morphology.ball()", "path": "api/skimage.morphology#skimage.morphology.ball", "type": "morphology", "text": " \nskimage.morphology.ball(radius, dtype=<class 'numpy.uint8'>) [source]\n \nGenerates a ball-shaped structuring element. This is the 3D equivalent of a disk. A pixel is within the neighborhood if the Euclidean distance between it and the origin is no greater than radius.  Parameters \n \nradiusint \n\nThe radius of the ball-shaped structuring element.    Returns \n \nselemndarray \n\nThe structuring element where elements of the neighborhood are 1 and 0 otherwise.    Other Parameters \n \ndtypedata-type \n\nThe data type of the structuring element.     \n"}, {"name": "morphology.binary_closing()", "path": "api/skimage.morphology#skimage.morphology.binary_closing", "type": "morphology", "text": " \nskimage.morphology.binary_closing(image, selem=None, out=None) [source]\n \nReturn fast binary morphological closing of an image. This function returns the same result as greyscale closing but performs faster for binary images. The morphological closing on an image is defined as a dilation followed by an erosion. Closing can remove small dark spots (i.e. \u201cpepper\u201d) and connect small bright cracks. This tends to \u201cclose\u201d up (dark) gaps between (bright) features.  Parameters \n \nimagendarray \n\nBinary input image.  \nselemndarray, optional \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a cross-shaped structuring element (connectivity=1).  \noutndarray of bool, optional \n\nThe array to store the result of the morphology. If None, is passed, a new array will be allocated.    Returns \n \nclosingndarray of bool \n\nThe result of the morphological closing.     \n"}, {"name": "morphology.binary_dilation()", "path": "api/skimage.morphology#skimage.morphology.binary_dilation", "type": "morphology", "text": " \nskimage.morphology.binary_dilation(image, selem=None, out=None) [source]\n \nReturn fast binary morphological dilation of an image. This function returns the same result as greyscale dilation but performs faster for binary images. Morphological dilation sets a pixel at (i,j) to the maximum over all pixels in the neighborhood centered at (i,j). Dilation enlarges bright regions and shrinks dark regions.  Parameters \n \nimagendarray \n\nBinary input image.  \nselemndarray, optional \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a cross-shaped structuring element (connectivity=1).  \noutndarray of bool, optional \n\nThe array to store the result of the morphology. If None is passed, a new array will be allocated.    Returns \n \ndilatedndarray of bool or uint \n\nThe result of the morphological dilation with values in [False, True].     \n"}, {"name": "morphology.binary_erosion()", "path": "api/skimage.morphology#skimage.morphology.binary_erosion", "type": "morphology", "text": " \nskimage.morphology.binary_erosion(image, selem=None, out=None) [source]\n \nReturn fast binary morphological erosion of an image. This function returns the same result as greyscale erosion but performs faster for binary images. Morphological erosion sets a pixel at (i,j) to the minimum over all pixels in the neighborhood centered at (i,j). Erosion shrinks bright regions and enlarges dark regions.  Parameters \n \nimagendarray \n\nBinary input image.  \nselemndarray, optional \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a cross-shaped structuring element (connectivity=1).  \noutndarray of bool, optional \n\nThe array to store the result of the morphology. If None is passed, a new array will be allocated.    Returns \n \nerodedndarray of bool or uint \n\nThe result of the morphological erosion taking values in [False, True].     \n"}, {"name": "morphology.binary_opening()", "path": "api/skimage.morphology#skimage.morphology.binary_opening", "type": "morphology", "text": " \nskimage.morphology.binary_opening(image, selem=None, out=None) [source]\n \nReturn fast binary morphological opening of an image. This function returns the same result as greyscale opening but performs faster for binary images. The morphological opening on an image is defined as an erosion followed by a dilation. Opening can remove small bright spots (i.e. \u201csalt\u201d) and connect small dark cracks. This tends to \u201copen\u201d up (dark) gaps between (bright) features.  Parameters \n \nimagendarray \n\nBinary input image.  \nselemndarray, optional \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a cross-shaped structuring element (connectivity=1).  \noutndarray of bool, optional \n\nThe array to store the result of the morphology. If None is passed, a new array will be allocated.    Returns \n \nopeningndarray of bool \n\nThe result of the morphological opening.     \n"}, {"name": "morphology.black_tophat()", "path": "api/skimage.morphology#skimage.morphology.black_tophat", "type": "morphology", "text": " \nskimage.morphology.black_tophat(image, selem=None, out=None) [source]\n \nReturn black top hat of an image. The black top hat of an image is defined as its morphological closing minus the original image. This operation returns the dark spots of the image that are smaller than the structuring element. Note that dark spots in the original image are bright spots after the black top hat.  Parameters \n \nimagendarray \n\nImage array.  \nselemndarray, optional \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).  \noutndarray, optional \n\nThe array to store the result of the morphology. If None is passed, a new array will be allocated.    Returns \n \noutarray, same shape and type as image \n\nThe result of the morphological black top hat.      See also  \nwhite_tophat\n\n  References  \n1  \nhttps://en.wikipedia.org/wiki/Top-hat_transform   Examples >>> # Change dark peak to bright peak and subtract background\n>>> import numpy as np\n>>> from skimage.morphology import square\n>>> dark_on_grey = np.array([[7, 6, 6, 6, 7],\n...                          [6, 5, 4, 5, 6],\n...                          [6, 4, 0, 4, 6],\n...                          [6, 5, 4, 5, 6],\n...                          [7, 6, 6, 6, 7]], dtype=np.uint8)\n>>> black_tophat(dark_on_grey, square(3))\narray([[0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 1, 5, 1, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "morphology.closing()", "path": "api/skimage.morphology#skimage.morphology.closing", "type": "morphology", "text": " \nskimage.morphology.closing(image, selem=None, out=None) [source]\n \nReturn greyscale morphological closing of an image. The morphological closing on an image is defined as a dilation followed by an erosion. Closing can remove small dark spots (i.e. \u201cpepper\u201d) and connect small bright cracks. This tends to \u201cclose\u201d up (dark) gaps between (bright) features.  Parameters \n \nimagendarray \n\nImage array.  \nselemndarray, optional \n\nThe neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).  \noutndarray, optional \n\nThe array to store the result of the morphology. If None, is passed, a new array will be allocated.    Returns \n \nclosingarray, same shape and type as image \n\nThe result of the morphological closing.     Examples >>> # Close a gap between two bright lines\n>>> import numpy as np\n>>> from skimage.morphology import square\n>>> broken_line = np.array([[0, 0, 0, 0, 0],\n...                         [0, 0, 0, 0, 0],\n...                         [1, 1, 0, 1, 1],\n...                         [0, 0, 0, 0, 0],\n...                         [0, 0, 0, 0, 0]], dtype=np.uint8)\n>>> closing(broken_line, square(3))\narray([[0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1],\n       [0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "morphology.convex_hull_image()", "path": "api/skimage.morphology#skimage.morphology.convex_hull_image", "type": "morphology", "text": " \nskimage.morphology.convex_hull_image(image, offset_coordinates=True, tolerance=1e-10) [source]\n \nCompute the convex hull image of a binary image. The convex hull is the set of pixels included in the smallest convex polygon that surround all white pixels in the input image.  Parameters \n \nimagearray \n\nBinary input image. This array is cast to bool before processing.  \noffset_coordinatesbool, optional \n\nIf True, a pixel at coordinate, e.g., (4, 7) will be represented by coordinates (3.5, 7), (4.5, 7), (4, 6.5), and (4, 7.5). This adds some \u201cextent\u201d to a pixel when computing the hull.  \ntolerancefloat, optional \n\nTolerance when determining whether a point is inside the hull. Due to numerical floating point errors, a tolerance of 0 can result in some points erroneously being classified as being outside the hull.    Returns \n \nhull(M, N) array of bool \n\nBinary image with pixels in convex hull set to True.     References  \n1  \nhttps://blogs.mathworks.com/steve/2011/10/04/binary-image-convex-hull-algorithm-notes/   \n"}, {"name": "morphology.convex_hull_object()", "path": "api/skimage.morphology#skimage.morphology.convex_hull_object", "type": "morphology", "text": " \nskimage.morphology.convex_hull_object(image, *, connectivity=2) [source]\n \nCompute the convex hull image of individual objects in a binary image. The convex hull is the set of pixels included in the smallest convex polygon that surround all white pixels in the input image.  Parameters \n \nimage(M, N) ndarray \n\nBinary input image.  \nconnectivity{1, 2}, int, optional \n\nDetermines the neighbors of each pixel. Adjacent elements within a squared distance of connectivity from pixel center are considered neighbors.: 1-connectivity      2-connectivity\n      [ ]           [ ]  [ ]  [ ]\n       |               \\  |  /\n [ ]--[x]--[ ]      [ ]--[x]--[ ]\n       |               /  |  \\\n      [ ]           [ ]  [ ]  [ ]\n    Returns \n \nhullndarray of bool \n\nBinary image with pixels inside convex hull set to True.     Notes This function uses skimage.morphology.label to define unique objects, finds the convex hull of each using convex_hull_image, and combines these regions with logical OR. Be aware the convex hulls of unconnected objects may overlap in the result. If this is suspected, consider using convex_hull_image separately on each object or adjust connectivity. \n"}, {"name": "morphology.cube()", "path": "api/skimage.morphology#skimage.morphology.cube", "type": "morphology", "text": " \nskimage.morphology.cube(width, dtype=<class 'numpy.uint8'>) [source]\n \nGenerates a cube-shaped structuring element. This is the 3D equivalent of a square. Every pixel along the perimeter has a chessboard distance no greater than radius (radius=floor(width/2)) pixels.  Parameters \n \nwidthint \n\nThe width, height and depth of the cube.    Returns \n \nselemndarray \n\nA structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.    Other Parameters \n \ndtypedata-type \n\nThe data type of the structuring element.     \n"}, {"name": "morphology.diameter_closing()", "path": "api/skimage.morphology#skimage.morphology.diameter_closing", "type": "morphology", "text": " \nskimage.morphology.diameter_closing(image, diameter_threshold=8, connectivity=1, parent=None, tree_traverser=None) [source]\n \nPerform a diameter closing of the image. Diameter closing removes all dark structures of an image with maximal extension smaller than diameter_threshold. The maximal extension is defined as the maximal extension of the bounding box. The operator is also called Bounding Box Closing. In practice, the result is similar to a morphological closing, but long and thin structures are not removed. Technically, this operator is based on the max-tree representation of the image.  Parameters \n \nimagendarray \n\nThe input image for which the diameter_closing is to be calculated. This image can be of any type.  \ndiameter_thresholdunsigned int \n\nThe maximal extension parameter (number of pixels). The default value is 8.  \nconnectivityunsigned int, optional \n\nThe neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.  \nparentndarray, int64, optional \n\nPrecomputed parent image representing the max tree of the inverted image. This function is fast, if precomputed parent and tree_traverser are provided. See Note for further details.  \ntree_traverser1D array, int64, optional \n\nPrecomputed traverser, where the pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent). This function is fast, if precomputed parent and tree_traverser are provided. See Note for further details.    Returns \n \noutputndarray \n\nOutput image of the same shape and type as input image.      See also  \nskimage.morphology.area_opening\n\n\nskimage.morphology.area_closing\n\n\nskimage.morphology.diameter_opening\n\n\nskimage.morphology.max_tree\n\n  Notes If a max-tree representation (parent and tree_traverser) are given to the function, they must be calculated from the inverted image for this function, i.e.: >>> P, S = max_tree(invert(f)) >>> closed = diameter_closing(f, 3, parent=P, tree_traverser=S) References  \n1  \nWalter, T., & Klein, J.-C. (2002). Automatic Detection of Microaneurysms in Color Fundus Images of the Human Retina by Means of the Bounding Box Closing. In A. Colosimo, P. Sirabella, A. Giuliani (Eds.), Medical Data Analysis. Lecture Notes in Computer Science, vol 2526, pp. 210-220. Springer Berlin Heidelberg. DOI:10.1007/3-540-36104-9_23  \n2  \nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. DOI:10.1109/TIP.2014.2336551   Examples We create an image (quadratic function with a minimum in the center and 4 additional local minima. >>> w = 12\n>>> x, y = np.mgrid[0:w,0:w]\n>>> f = 180 + 0.2*((x - w/2)**2 + (y-w/2)**2)\n>>> f[2:3,1:5] = 160; f[2:4,9:11] = 140; f[9:11,2:4] = 120\n>>> f[9:10,9:11] = 100; f[10,10] = 100\n>>> f = f.astype(int)\n We can calculate the diameter closing: >>> closed = diameter_closing(f, 3, connectivity=1)\n All small minima with a maximal extension of 2 or less are removed. The remaining minima have all a maximal extension of at least 3. \n"}, {"name": "morphology.diameter_opening()", "path": "api/skimage.morphology#skimage.morphology.diameter_opening", "type": "morphology", "text": " \nskimage.morphology.diameter_opening(image, diameter_threshold=8, connectivity=1, parent=None, tree_traverser=None) [source]\n \nPerform a diameter opening of the image. Diameter opening removes all bright structures of an image with maximal extension smaller than diameter_threshold. The maximal extension is defined as the maximal extension of the bounding box. The operator is also called Bounding Box Opening. In practice, the result is similar to a morphological opening, but long and thin structures are not removed. Technically, this operator is based on the max-tree representation of the image.  Parameters \n \nimagendarray \n\nThe input image for which the area_opening is to be calculated. This image can be of any type.  \ndiameter_thresholdunsigned int \n\nThe maximal extension parameter (number of pixels). The default value is 8.  \nconnectivityunsigned int, optional \n\nThe neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.  \nparentndarray, int64, optional \n\nParent image representing the max tree of the image. The value of each pixel is the index of its parent in the ravelled array.  \ntree_traverser1D array, int64, optional \n\nThe ordered pixel indices (referring to the ravelled array). The pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent).    Returns \n \noutputndarray \n\nOutput image of the same shape and type as the input image.      See also  \nskimage.morphology.area_opening\n\n\nskimage.morphology.area_closing\n\n\nskimage.morphology.diameter_closing\n\n\nskimage.morphology.max_tree\n\n  References  \n1  \nWalter, T., & Klein, J.-C. (2002). Automatic Detection of Microaneurysms in Color Fundus Images of the Human Retina by Means of the Bounding Box Closing. In A. Colosimo, P. Sirabella, A. Giuliani (Eds.), Medical Data Analysis. Lecture Notes in Computer Science, vol 2526, pp. 210-220. Springer Berlin Heidelberg. DOI:10.1007/3-540-36104-9_23  \n2  \nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. DOI:10.1109/TIP.2014.2336551   Examples We create an image (quadratic function with a maximum in the center and 4 additional local maxima. >>> w = 12\n>>> x, y = np.mgrid[0:w,0:w]\n>>> f = 20 - 0.2*((x - w/2)**2 + (y-w/2)**2)\n>>> f[2:3,1:5] = 40; f[2:4,9:11] = 60; f[9:11,2:4] = 80\n>>> f[9:10,9:11] = 100; f[10,10] = 100\n>>> f = f.astype(int)\n We can calculate the diameter opening: >>> open = diameter_opening(f, 3, connectivity=1)\n The peaks with a maximal extension of 2 or less are removed. The remaining peaks have all a maximal extension of at least 3. \n"}, {"name": "morphology.diamond()", "path": "api/skimage.morphology#skimage.morphology.diamond", "type": "morphology", "text": " \nskimage.morphology.diamond(radius, dtype=<class 'numpy.uint8'>) [source]\n \nGenerates a flat, diamond-shaped structuring element. A pixel is part of the neighborhood (i.e. labeled 1) if the city block/Manhattan distance between it and the center of the neighborhood is no greater than radius.  Parameters \n \nradiusint \n\nThe radius of the diamond-shaped structuring element.    Returns \n \nselemndarray \n\nThe structuring element where elements of the neighborhood are 1 and 0 otherwise.    Other Parameters \n \ndtypedata-type \n\nThe data type of the structuring element.     \n"}, {"name": "morphology.dilation()", "path": "api/skimage.morphology#skimage.morphology.dilation", "type": "morphology", "text": " \nskimage.morphology.dilation(image, selem=None, out=None, shift_x=False, shift_y=False) [source]\n \nReturn greyscale morphological dilation of an image. Morphological dilation sets a pixel at (i,j) to the maximum over all pixels in the neighborhood centered at (i,j). Dilation enlarges bright regions and shrinks dark regions.  Parameters \n \nimagendarray \n\nImage array.  \nselemndarray, optional \n\nThe neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).  \noutndarray, optional \n\nThe array to store the result of the morphology. If None, is passed, a new array will be allocated.  \nshift_x, shift_ybool, optional \n\nshift structuring element about center point. This only affects eccentric structuring elements (i.e. selem with even numbered sides).    Returns \n \ndilateduint8 array, same shape and type as image \n\nThe result of the morphological dilation.     Notes For uint8 (and uint16 up to a certain bit-depth) data, the lower algorithm complexity makes the skimage.filters.rank.maximum function more efficient for larger images and structuring elements. Examples >>> # Dilation enlarges bright regions\n>>> import numpy as np\n>>> from skimage.morphology import square\n>>> bright_pixel = np.array([[0, 0, 0, 0, 0],\n...                          [0, 0, 0, 0, 0],\n...                          [0, 0, 1, 0, 0],\n...                          [0, 0, 0, 0, 0],\n...                          [0, 0, 0, 0, 0]], dtype=np.uint8)\n>>> dilation(bright_pixel, square(3))\narray([[0, 0, 0, 0, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "morphology.disk()", "path": "api/skimage.morphology#skimage.morphology.disk", "type": "morphology", "text": " \nskimage.morphology.disk(radius, dtype=<class 'numpy.uint8'>) [source]\n \nGenerates a flat, disk-shaped structuring element. A pixel is within the neighborhood if the Euclidean distance between it and the origin is no greater than radius.  Parameters \n \nradiusint \n\nThe radius of the disk-shaped structuring element.    Returns \n \nselemndarray \n\nThe structuring element where elements of the neighborhood are 1 and 0 otherwise.    Other Parameters \n \ndtypedata-type \n\nThe data type of the structuring element.     \n"}, {"name": "morphology.erosion()", "path": "api/skimage.morphology#skimage.morphology.erosion", "type": "morphology", "text": " \nskimage.morphology.erosion(image, selem=None, out=None, shift_x=False, shift_y=False) [source]\n \nReturn greyscale morphological erosion of an image. Morphological erosion sets a pixel at (i,j) to the minimum over all pixels in the neighborhood centered at (i,j). Erosion shrinks bright regions and enlarges dark regions.  Parameters \n \nimagendarray \n\nImage array.  \nselemndarray, optional \n\nThe neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).  \noutndarrays, optional \n\nThe array to store the result of the morphology. If None is passed, a new array will be allocated.  \nshift_x, shift_ybool, optional \n\nshift structuring element about center point. This only affects eccentric structuring elements (i.e. selem with even numbered sides).    Returns \n \nerodedarray, same shape as image \n\nThe result of the morphological erosion.     Notes For uint8 (and uint16 up to a certain bit-depth) data, the lower algorithm complexity makes the skimage.filters.rank.minimum function more efficient for larger images and structuring elements. Examples >>> # Erosion shrinks bright regions\n>>> import numpy as np\n>>> from skimage.morphology import square\n>>> bright_square = np.array([[0, 0, 0, 0, 0],\n...                           [0, 1, 1, 1, 0],\n...                           [0, 1, 1, 1, 0],\n...                           [0, 1, 1, 1, 0],\n...                           [0, 0, 0, 0, 0]], dtype=np.uint8)\n>>> erosion(bright_square, square(3))\narray([[0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "morphology.flood()", "path": "api/skimage.morphology#skimage.morphology.flood", "type": "morphology", "text": " \nskimage.morphology.flood(image, seed_point, *, selem=None, connectivity=None, tolerance=None) [source]\n \nMask corresponding to a flood fill. Starting at a specific seed_point, connected points equal or within tolerance of the seed value are found.  Parameters \n \nimagendarray \n\nAn n-dimensional array.  \nseed_pointtuple or int \n\nThe point in image used as the starting point for the flood fill. If the image is 1D, this point may be given as an integer.  \nselemndarray, optional \n\nA structuring element used to determine the neighborhood of each evaluated pixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as image. If not given, all adjacent pixels are considered as part of the neighborhood (fully connected).  \nconnectivityint, optional \n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is larger or equal to connectivity are considered neighbors. Ignored if selem is not None.  \ntolerancefloat or int, optional \n\nIf None (default), adjacent values must be strictly equal to the initial value of image at seed_point. This is fastest. If a value is given, a comparison will be done at every point and if within tolerance of the initial value will also be filled (inclusive).    Returns \n \nmaskndarray \n\nA Boolean array with the same shape as image is returned, with True values for areas connected to and equal (or within tolerance of) the seed point. All other values are False.     Notes The conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many raster graphics programs. This function returns just the mask representing the fill. If indices are desired rather than masks for memory reasons, the user can simply run numpy.nonzero on the result, save the indices, and discard this mask. Examples >>> from skimage.morphology import flood\n>>> image = np.zeros((4, 7), dtype=int)\n>>> image[1:3, 1:3] = 1\n>>> image[3, 0] = 1\n>>> image[1:3, 4:6] = 2\n>>> image[3, 6] = 3\n>>> image\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 1, 0, 2, 2, 0],\n       [0, 1, 1, 0, 2, 2, 0],\n       [1, 0, 0, 0, 0, 0, 3]])\n Fill connected ones with 5, with full connectivity (diagonals included): >>> mask = flood(image, (1, 1))\n>>> image_flooded = image.copy()\n>>> image_flooded[mask] = 5\n>>> image_flooded\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [5, 0, 0, 0, 0, 0, 3]])\n Fill connected ones with 5, excluding diagonal points (connectivity 1): >>> mask = flood(image, (1, 1), connectivity=1)\n>>> image_flooded = image.copy()\n>>> image_flooded[mask] = 5\n>>> image_flooded\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [1, 0, 0, 0, 0, 0, 3]])\n Fill with a tolerance: >>> mask = flood(image, (0, 0), tolerance=1)\n>>> image_flooded = image.copy()\n>>> image_flooded[mask] = 5\n>>> image_flooded\narray([[5, 5, 5, 5, 5, 5, 5],\n       [5, 5, 5, 5, 2, 2, 5],\n       [5, 5, 5, 5, 2, 2, 5],\n       [5, 5, 5, 5, 5, 5, 3]])\n \n"}, {"name": "morphology.flood_fill()", "path": "api/skimage.morphology#skimage.morphology.flood_fill", "type": "morphology", "text": " \nskimage.morphology.flood_fill(image, seed_point, new_value, *, selem=None, connectivity=None, tolerance=None, in_place=False, inplace=None) [source]\n \nPerform flood filling on an image. Starting at a specific seed_point, connected points equal or within tolerance of the seed value are found, then set to new_value.  Parameters \n \nimagendarray \n\nAn n-dimensional array.  \nseed_pointtuple or int \n\nThe point in image used as the starting point for the flood fill. If the image is 1D, this point may be given as an integer.  \nnew_valueimage type \n\nNew value to set the entire fill. This must be chosen in agreement with the dtype of image.  \nselemndarray, optional \n\nA structuring element used to determine the neighborhood of each evaluated pixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as image. If not given, all adjacent pixels are considered as part of the neighborhood (fully connected).  \nconnectivityint, optional \n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is less than or equal to connectivity are considered neighbors. Ignored if selem is not None.  \ntolerancefloat or int, optional \n\nIf None (default), adjacent values must be strictly equal to the value of image at seed_point to be filled. This is fastest. If a tolerance is provided, adjacent points with values within plus or minus tolerance from the seed point are filled (inclusive).  \nin_placebool, optional \n\nIf True, flood filling is applied to image in place. If False, the flood filled result is returned without modifying the input image (default).  \ninplacebool, optional \n\nThis parameter is deprecated and will be removed in version 0.19.0 in favor of in_place. If True, flood filling is applied to image inplace. If False, the flood filled result is returned without modifying the input image (default).    Returns \n \nfilledndarray \n\nAn array with the same shape as image is returned, with values in areas connected to and equal (or within tolerance of) the seed point replaced with new_value.     Notes The conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many raster graphics programs. Examples >>> from skimage.morphology import flood_fill\n>>> image = np.zeros((4, 7), dtype=int)\n>>> image[1:3, 1:3] = 1\n>>> image[3, 0] = 1\n>>> image[1:3, 4:6] = 2\n>>> image[3, 6] = 3\n>>> image\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 1, 0, 2, 2, 0],\n       [0, 1, 1, 0, 2, 2, 0],\n       [1, 0, 0, 0, 0, 0, 3]])\n Fill connected ones with 5, with full connectivity (diagonals included): >>> flood_fill(image, (1, 1), 5)\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [5, 0, 0, 0, 0, 0, 3]])\n Fill connected ones with 5, excluding diagonal points (connectivity 1): >>> flood_fill(image, (1, 1), 5, connectivity=1)\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [1, 0, 0, 0, 0, 0, 3]])\n Fill with a tolerance: >>> flood_fill(image, (0, 0), 5, tolerance=1)\narray([[5, 5, 5, 5, 5, 5, 5],\n       [5, 5, 5, 5, 2, 2, 5],\n       [5, 5, 5, 5, 2, 2, 5],\n       [5, 5, 5, 5, 5, 5, 3]])\n \n"}, {"name": "morphology.h_maxima()", "path": "api/skimage.morphology#skimage.morphology.h_maxima", "type": "morphology", "text": " \nskimage.morphology.h_maxima(image, h, selem=None) [source]\n \nDetermine all maxima of the image with height >= h. The local maxima are defined as connected sets of pixels with equal grey level strictly greater than the grey level of all pixels in direct neighborhood of the set. A local maximum M of height h is a local maximum for which there is at least one path joining M with an equal or higher local maximum on which the minimal value is f(M) - h (i.e. the values along the path are not decreasing by more than h with respect to the maximum\u2019s value) and no path to an equal or higher local maximum for which the minimal value is greater. The global maxima of the image are also found by this function.  Parameters \n \nimagendarray \n\nThe input image for which the maxima are to be calculated.  \nhunsigned integer \n\nThe minimal height of all extracted maxima.  \nselemndarray, optional \n\nThe neighborhood expressed as an n-D array of 1\u2019s and 0\u2019s. Default is the ball of radius 1 according to the maximum norm (i.e. a 3x3 square for 2D images, a 3x3x3 cube for 3D images, etc.)    Returns \n \nh_maxndarray \n\nThe local maxima of height >= h and the global maxima. The resulting image is a binary image, where pixels belonging to the determined maxima take value 1, the others take value 0.      See also  \nskimage.morphology.extrema.h_minima \n\nskimage.morphology.extrema.local_maxima \n\nskimage.morphology.extrema.local_minima \n  References  \n1  \nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d (Chapter 6), 2nd edition (2003), ISBN 3540429883.   Examples >>> import numpy as np\n>>> from skimage.morphology import extrema\n We create an image (quadratic function with a maximum in the center and 4 additional constant maxima. The heights of the maxima are: 1, 21, 41, 61, 81 >>> w = 10\n>>> x, y = np.mgrid[0:w,0:w]\n>>> f = 20 - 0.2*((x - w/2)**2 + (y-w/2)**2)\n>>> f[2:4,2:4] = 40; f[2:4,7:9] = 60; f[7:9,2:4] = 80; f[7:9,7:9] = 100\n>>> f = f.astype(int)\n We can calculate all maxima with a height of at least 40: >>> maxima = extrema.h_maxima(f, 40)\n The resulting image will contain 3 local maxima. \n"}, {"name": "morphology.h_minima()", "path": "api/skimage.morphology#skimage.morphology.h_minima", "type": "morphology", "text": " \nskimage.morphology.h_minima(image, h, selem=None) [source]\n \nDetermine all minima of the image with depth >= h. The local minima are defined as connected sets of pixels with equal grey level strictly smaller than the grey levels of all pixels in direct neighborhood of the set. A local minimum M of depth h is a local minimum for which there is at least one path joining M with an equal or lower local minimum on which the maximal value is f(M) + h (i.e. the values along the path are not increasing by more than h with respect to the minimum\u2019s value) and no path to an equal or lower local minimum for which the maximal value is smaller. The global minima of the image are also found by this function.  Parameters \n \nimagendarray \n\nThe input image for which the minima are to be calculated.  \nhunsigned integer \n\nThe minimal depth of all extracted minima.  \nselemndarray, optional \n\nThe neighborhood expressed as an n-D array of 1\u2019s and 0\u2019s. Default is the ball of radius 1 according to the maximum norm (i.e. a 3x3 square for 2D images, a 3x3x3 cube for 3D images, etc.)    Returns \n \nh_minndarray \n\nThe local minima of depth >= h and the global minima. The resulting image is a binary image, where pixels belonging to the determined minima take value 1, the others take value 0.      See also  \nskimage.morphology.extrema.h_maxima \n\nskimage.morphology.extrema.local_maxima \n\nskimage.morphology.extrema.local_minima \n  References  \n1  \nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d (Chapter 6), 2nd edition (2003), ISBN 3540429883.   Examples >>> import numpy as np\n>>> from skimage.morphology import extrema\n We create an image (quadratic function with a minimum in the center and 4 additional constant maxima. The depth of the minima are: 1, 21, 41, 61, 81 >>> w = 10\n>>> x, y = np.mgrid[0:w,0:w]\n>>> f = 180 + 0.2*((x - w/2)**2 + (y-w/2)**2)\n>>> f[2:4,2:4] = 160; f[2:4,7:9] = 140; f[7:9,2:4] = 120; f[7:9,7:9] = 100\n>>> f = f.astype(int)\n We can calculate all minima with a depth of at least 40: >>> minima = extrema.h_minima(f, 40)\n The resulting image will contain 3 local minima. \n"}, {"name": "morphology.label()", "path": "api/skimage.morphology#skimage.morphology.label", "type": "morphology", "text": " \nskimage.morphology.label(input, background=None, return_num=False, connectivity=None) [source]\n \nLabel connected regions of an integer array. Two pixels are connected when they are neighbors and have the same value. In 2D, they can be neighbors either in a 1- or 2-connected sense. The value refers to the maximum number of orthogonal hops to consider a pixel/voxel a neighbor: 1-connectivity     2-connectivity     diagonal connection close-up\n\n     [ ]           [ ]  [ ]  [ ]             [ ]\n      |               \\  |  /                 |  <- hop 2\n[ ]--[x]--[ ]      [ ]--[x]--[ ]        [x]--[ ]\n      |               /  |  \\             hop 1\n     [ ]           [ ]  [ ]  [ ]\n  Parameters \n \ninputndarray of dtype int \n\nImage to label.  \nbackgroundint, optional \n\nConsider all pixels with this value as background pixels, and label them as 0. By default, 0-valued pixels are considered as background pixels.  \nreturn_numbool, optional \n\nWhether to return the number of assigned labels.  \nconnectivityint, optional \n\nMaximum number of orthogonal hops to consider a pixel/voxel as a neighbor. Accepted values are ranging from 1 to input.ndim. If None, a full connectivity of input.ndim is used.    Returns \n \nlabelsndarray of dtype int \n\nLabeled array, where all connected regions are assigned the same integer value.  \nnumint, optional \n\nNumber of labels, which equals the maximum label index and is only returned if return_num is True.      See also  \nregionprops \n\nregionprops_table \n  References  \n1  \nChristophe Fiorio and Jens Gustedt, \u201cTwo linear time Union-Find strategies for image processing\u201d, Theoretical Computer Science 154 (1996), pp. 165-181.  \n2  \nKensheng Wu, Ekow Otoo and Arie Shoshani, \u201cOptimizing connected component labeling algorithms\u201d, Paper LBNL-56864, 2005, Lawrence Berkeley National Laboratory (University of California), http://repositories.cdlib.org/lbnl/LBNL-56864   Examples >>> import numpy as np\n>>> x = np.eye(3).astype(int)\n>>> print(x)\n[[1 0 0]\n [0 1 0]\n [0 0 1]]\n>>> print(label(x, connectivity=1))\n[[1 0 0]\n [0 2 0]\n [0 0 3]]\n>>> print(label(x, connectivity=2))\n[[1 0 0]\n [0 1 0]\n [0 0 1]]\n>>> print(label(x, background=-1))\n[[1 2 2]\n [2 1 2]\n [2 2 1]]\n>>> x = np.array([[1, 0, 0],\n...               [1, 1, 5],\n...               [0, 0, 0]])\n>>> print(label(x))\n[[1 0 0]\n [1 1 2]\n [0 0 0]]\n \n"}, {"name": "morphology.local_maxima()", "path": "api/skimage.morphology#skimage.morphology.local_maxima", "type": "morphology", "text": " \nskimage.morphology.local_maxima(image, selem=None, connectivity=None, indices=False, allow_borders=True) [source]\n \nFind local maxima of n-dimensional array. The local maxima are defined as connected sets of pixels with equal gray level (plateaus) strictly greater than the gray levels of all pixels in the neighborhood.  Parameters \n \nimagendarray \n\nAn n-dimensional array.  \nselemndarray, optional \n\nA structuring element used to determine the neighborhood of each evaluated pixel (True denotes a connected pixel). It must be a boolean array and have the same number of dimensions as image. If neither selem nor connectivity are given, all adjacent pixels are considered as part of the neighborhood.  \nconnectivityint, optional \n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is less than or equal to connectivity are considered neighbors. Ignored if selem is not None.  \nindicesbool, optional \n\nIf True, the output will be a tuple of one-dimensional arrays representing the indices of local maxima in each dimension. If False, the output will be a boolean array with the same shape as image.  \nallow_bordersbool, optional \n\nIf true, plateaus that touch the image border are valid maxima.    Returns \n \nmaximandarray or tuple[ndarray] \n\nIf indices is false, a boolean array with the same shape as image is returned with True indicating the position of local maxima (False otherwise). If indices is true, a tuple of one-dimensional arrays containing the coordinates (indices) of all found maxima.    Warns \n UserWarning\n\nIf allow_borders is false and any dimension of the given image is shorter than 3 samples, maxima can\u2019t exist and a warning is shown.      See also  \nskimage.morphology.local_minima\n\n\nskimage.morphology.h_maxima\n\n\nskimage.morphology.h_minima\n\n  Notes This function operates on the following ideas:  Make a first pass over the image\u2019s last dimension and flag candidates for local maxima by comparing pixels in only one direction. If the pixels aren\u2019t connected in the last dimension all pixels are flagged as candidates instead.  For each candidate:  Perform a flood-fill to find all connected pixels that have the same gray value and are part of the plateau. Consider the connected neighborhood of a plateau: if no bordering sample has a higher gray level, mark the plateau as a definite local maximum.  Examples >>> from skimage.morphology import local_maxima\n>>> image = np.zeros((4, 7), dtype=int)\n>>> image[1:3, 1:3] = 1\n>>> image[3, 0] = 1\n>>> image[1:3, 4:6] = 2\n>>> image[3, 6] = 3\n>>> image\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 1, 0, 2, 2, 0],\n       [0, 1, 1, 0, 2, 2, 0],\n       [1, 0, 0, 0, 0, 0, 3]])\n Find local maxima by comparing to all neighboring pixels (maximal connectivity): >>> local_maxima(image)\narray([[False, False, False, False, False, False, False],\n       [False,  True,  True, False, False, False, False],\n       [False,  True,  True, False, False, False, False],\n       [ True, False, False, False, False, False,  True]])\n>>> local_maxima(image, indices=True)\n(array([1, 1, 2, 2, 3, 3]), array([1, 2, 1, 2, 0, 6]))\n Find local maxima without comparing to diagonal pixels (connectivity 1): >>> local_maxima(image, connectivity=1)\narray([[False, False, False, False, False, False, False],\n       [False,  True,  True, False,  True,  True, False],\n       [False,  True,  True, False,  True,  True, False],\n       [ True, False, False, False, False, False,  True]])\n and exclude maxima that border the image edge: >>> local_maxima(image, connectivity=1, allow_borders=False)\narray([[False, False, False, False, False, False, False],\n       [False,  True,  True, False,  True,  True, False],\n       [False,  True,  True, False,  True,  True, False],\n       [False, False, False, False, False, False, False]])\n \n"}, {"name": "morphology.local_minima()", "path": "api/skimage.morphology#skimage.morphology.local_minima", "type": "morphology", "text": " \nskimage.morphology.local_minima(image, selem=None, connectivity=None, indices=False, allow_borders=True) [source]\n \nFind local minima of n-dimensional array. The local minima are defined as connected sets of pixels with equal gray level (plateaus) strictly smaller than the gray levels of all pixels in the neighborhood.  Parameters \n \nimagendarray \n\nAn n-dimensional array.  \nselemndarray, optional \n\nA structuring element used to determine the neighborhood of each evaluated pixel (True denotes a connected pixel). It must be a boolean array and have the same number of dimensions as image. If neither selem nor connectivity are given, all adjacent pixels are considered as part of the neighborhood.  \nconnectivityint, optional \n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is less than or equal to connectivity are considered neighbors. Ignored if selem is not None.  \nindicesbool, optional \n\nIf True, the output will be a tuple of one-dimensional arrays representing the indices of local minima in each dimension. If False, the output will be a boolean array with the same shape as image.  \nallow_bordersbool, optional \n\nIf true, plateaus that touch the image border are valid minima.    Returns \n \nminimandarray or tuple[ndarray] \n\nIf indices is false, a boolean array with the same shape as image is returned with True indicating the position of local minima (False otherwise). If indices is true, a tuple of one-dimensional arrays containing the coordinates (indices) of all found minima.      See also  \nskimage.morphology.local_maxima\n\n\nskimage.morphology.h_maxima\n\n\nskimage.morphology.h_minima\n\n  Notes This function operates on the following ideas:  Make a first pass over the image\u2019s last dimension and flag candidates for local minima by comparing pixels in only one direction. If the pixels aren\u2019t connected in the last dimension all pixels are flagged as candidates instead.  For each candidate:  Perform a flood-fill to find all connected pixels that have the same gray value and are part of the plateau. Consider the connected neighborhood of a plateau: if no bordering sample has a smaller gray level, mark the plateau as a definite local minimum.  Examples >>> from skimage.morphology import local_minima\n>>> image = np.zeros((4, 7), dtype=int)\n>>> image[1:3, 1:3] = -1\n>>> image[3, 0] = -1\n>>> image[1:3, 4:6] = -2\n>>> image[3, 6] = -3\n>>> image\narray([[ 0,  0,  0,  0,  0,  0,  0],\n       [ 0, -1, -1,  0, -2, -2,  0],\n       [ 0, -1, -1,  0, -2, -2,  0],\n       [-1,  0,  0,  0,  0,  0, -3]])\n Find local minima by comparing to all neighboring pixels (maximal connectivity): >>> local_minima(image)\narray([[False, False, False, False, False, False, False],\n       [False,  True,  True, False, False, False, False],\n       [False,  True,  True, False, False, False, False],\n       [ True, False, False, False, False, False,  True]])\n>>> local_minima(image, indices=True)\n(array([1, 1, 2, 2, 3, 3]), array([1, 2, 1, 2, 0, 6]))\n Find local minima without comparing to diagonal pixels (connectivity 1): >>> local_minima(image, connectivity=1)\narray([[False, False, False, False, False, False, False],\n       [False,  True,  True, False,  True,  True, False],\n       [False,  True,  True, False,  True,  True, False],\n       [ True, False, False, False, False, False,  True]])\n and exclude minima that border the image edge: >>> local_minima(image, connectivity=1, allow_borders=False)\narray([[False, False, False, False, False, False, False],\n       [False,  True,  True, False,  True,  True, False],\n       [False,  True,  True, False,  True,  True, False],\n       [False, False, False, False, False, False, False]])\n \n"}, {"name": "morphology.max_tree()", "path": "api/skimage.morphology#skimage.morphology.max_tree", "type": "morphology", "text": " \nskimage.morphology.max_tree(image, connectivity=1) [source]\n \nBuild the max tree from an image. Component trees represent the hierarchical structure of the connected components resulting from sequential thresholding operations applied to an image. A connected component at one level is parent of a component at a higher level if the latter is included in the first. A max-tree is an efficient representation of a component tree. A connected component at one level is represented by one reference pixel at this level, which is parent to all other pixels at that level and to the reference pixel at the level above. The max-tree is the basis for many morphological operators, namely connected operators.  Parameters \n \nimagendarray \n\nThe input image for which the max-tree is to be calculated. This image can be of any type.  \nconnectivityunsigned int, optional \n\nThe neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.    Returns \n \nparentndarray, int64 \n\nArray of same shape as image. The value of each pixel is the index of its parent in the ravelled array.  \ntree_traverser1D array, int64 \n\nThe ordered pixel indices (referring to the ravelled array). The pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent).     References  \n1  \nSalembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected Operators for Image and Sequence Processing. IEEE Transactions on Image Processing, 7(4), 555-570. DOI:10.1109/83.663500  \n2  \nBerger, C., Geraud, T., Levillain, R., Widynski, N., Baillard, A., Bertin, E. (2007). Effective Component Tree Computation with Application to Pattern Recognition in Astronomical Imaging. In International Conference on Image Processing (ICIP) (pp. 41-44). DOI:10.1109/ICIP.2007.4379949  \n3  \nNajman, L., & Couprie, M. (2006). Building the component tree in quasi-linear time. IEEE Transactions on Image Processing, 15(11), 3531-3539. DOI:10.1109/TIP.2006.877518  \n4  \nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. DOI:10.1109/TIP.2014.2336551   Examples We create a small sample image (Figure 1 from [4]) and build the max-tree. >>> image = np.array([[15, 13, 16], [12, 12, 10], [16, 12, 14]])\n>>> P, S = max_tree(image, connectivity=2)\n \n"}, {"name": "morphology.max_tree_local_maxima()", "path": "api/skimage.morphology#skimage.morphology.max_tree_local_maxima", "type": "morphology", "text": " \nskimage.morphology.max_tree_local_maxima(image, connectivity=1, parent=None, tree_traverser=None) [source]\n \nDetermine all local maxima of the image. The local maxima are defined as connected sets of pixels with equal gray level strictly greater than the gray levels of all pixels in direct neighborhood of the set. The function labels the local maxima. Technically, the implementation is based on the max-tree representation of an image. The function is very efficient if the max-tree representation has already been computed. Otherwise, it is preferable to use the function local_maxima.  Parameters \n \nimagendarray \n\nThe input image for which the maxima are to be calculated.  connectivity: unsigned int, optional\n\nThe neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.  parent: ndarray, int64, optional\n\nThe value of each pixel is the index of its parent in the ravelled array.  tree_traverser: 1D array, int64, optional\n\nThe ordered pixel indices (referring to the ravelled array). The pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent).    Returns \n \nlocal_maxndarray, uint64 \n\nLabeled local maxima of the image.      See also  \nskimage.morphology.local_maxima\n\n\nskimage.morphology.max_tree\n\n  References  \n1  \nVincent L., Proc. \u201cGrayscale area openings and closings, their efficient implementation and applications\u201d, EURASIP Workshop on Mathematical Morphology and its Applications to Signal Processing, Barcelona, Spain, pp.22-27, May 1993.  \n2  \nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d (Chapter 6), 2nd edition (2003), ISBN 3540429883. DOI:10.1007/978-3-662-05088-0  \n3  \nSalembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected Operators for Image and Sequence Processing. IEEE Transactions on Image Processing, 7(4), 555-570. DOI:10.1109/83.663500  \n4  \nNajman, L., & Couprie, M. (2006). Building the component tree in quasi-linear time. IEEE Transactions on Image Processing, 15(11), 3531-3539. DOI:10.1109/TIP.2006.877518  \n5  \nCarlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. DOI:10.1109/TIP.2014.2336551   Examples We create an image (quadratic function with a maximum in the center and 4 additional constant maxima. >>> w = 10\n>>> x, y = np.mgrid[0:w,0:w]\n>>> f = 20 - 0.2*((x - w/2)**2 + (y-w/2)**2)\n>>> f[2:4,2:4] = 40; f[2:4,7:9] = 60; f[7:9,2:4] = 80; f[7:9,7:9] = 100\n>>> f = f.astype(int)\n We can calculate all local maxima: >>> maxima = max_tree_local_maxima(f)\n The resulting image contains the labeled local maxima. \n"}, {"name": "morphology.medial_axis()", "path": "api/skimage.morphology#skimage.morphology.medial_axis", "type": "morphology", "text": " \nskimage.morphology.medial_axis(image, mask=None, return_distance=False) [source]\n \nCompute the medial axis transform of a binary image  Parameters \n \nimagebinary ndarray, shape (M, N) \n\nThe image of the shape to be skeletonized.  \nmaskbinary ndarray, shape (M, N), optional \n\nIf a mask is given, only those elements in image with a true value in mask are used for computing the medial axis.  \nreturn_distancebool, optional \n\nIf true, the distance transform is returned as well as the skeleton.    Returns \n \noutndarray of bools \n\nMedial axis transform of the image  \ndistndarray of ints, optional \n\nDistance transform of the image (only returned if return_distance is True)      See also  \nskeletonize\n\n  Notes This algorithm computes the medial axis transform of an image as the ridges of its distance transform.  The different steps of the algorithm are as follows\n\n A lookup table is used, that assigns 0 or 1 to each configuration of the 3x3 binary square, whether the central pixel should be removed or kept. We want a point to be removed if it has more than one neighbor and if removing it does not change the number of connected components. The distance transform to the background is computed, as well as the cornerness of the pixel. The foreground (value of 1) points are ordered by the distance transform, then the cornerness. A cython function is called to reduce the image to its skeleton. It processes pixels in the order determined at the previous step, and removes or maintains a pixel according to the lookup table. Because of the ordering, it is possible to process all pixels in only one pass.    Examples >>> square = np.zeros((7, 7), dtype=np.uint8)\n>>> square[1:-1, 2:-2] = 1\n>>> square\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n>>> medial_axis(square).astype(np.uint8)\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "morphology.octagon()", "path": "api/skimage.morphology#skimage.morphology.octagon", "type": "morphology", "text": " \nskimage.morphology.octagon(m, n, dtype=<class 'numpy.uint8'>) [source]\n \nGenerates an octagon shaped structuring element. For a given size of (m) horizontal and vertical sides and a given (n) height or width of slanted sides octagon is generated. The slanted sides are 45 or 135 degrees to the horizontal axis and hence the widths and heights are equal.  Parameters \n \nmint \n\nThe size of the horizontal and vertical sides.  \nnint \n\nThe height or width of the slanted sides.    Returns \n \nselemndarray \n\nThe structuring element where elements of the neighborhood are 1 and 0 otherwise.    Other Parameters \n \ndtypedata-type \n\nThe data type of the structuring element.     \n"}, {"name": "morphology.octahedron()", "path": "api/skimage.morphology#skimage.morphology.octahedron", "type": "morphology", "text": " \nskimage.morphology.octahedron(radius, dtype=<class 'numpy.uint8'>) [source]\n \nGenerates a octahedron-shaped structuring element. This is the 3D equivalent of a diamond. A pixel is part of the neighborhood (i.e. labeled 1) if the city block/Manhattan distance between it and the center of the neighborhood is no greater than radius.  Parameters \n \nradiusint \n\nThe radius of the octahedron-shaped structuring element.    Returns \n \nselemndarray \n\nThe structuring element where elements of the neighborhood are 1 and 0 otherwise.    Other Parameters \n \ndtypedata-type \n\nThe data type of the structuring element.     \n"}, {"name": "morphology.opening()", "path": "api/skimage.morphology#skimage.morphology.opening", "type": "morphology", "text": " \nskimage.morphology.opening(image, selem=None, out=None) [source]\n \nReturn greyscale morphological opening of an image. The morphological opening on an image is defined as an erosion followed by a dilation. Opening can remove small bright spots (i.e. \u201csalt\u201d) and connect small dark cracks. This tends to \u201copen\u201d up (dark) gaps between (bright) features.  Parameters \n \nimagendarray \n\nImage array.  \nselemndarray, optional \n\nThe neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).  \noutndarray, optional \n\nThe array to store the result of the morphology. If None is passed, a new array will be allocated.    Returns \n \nopeningarray, same shape and type as image \n\nThe result of the morphological opening.     Examples >>> # Open up gap between two bright regions (but also shrink regions)\n>>> import numpy as np\n>>> from skimage.morphology import square\n>>> bad_connection = np.array([[1, 0, 0, 0, 1],\n...                            [1, 1, 0, 1, 1],\n...                            [1, 1, 1, 1, 1],\n...                            [1, 1, 0, 1, 1],\n...                            [1, 0, 0, 0, 1]], dtype=np.uint8)\n>>> opening(bad_connection, square(3))\narray([[0, 0, 0, 0, 0],\n       [1, 1, 0, 1, 1],\n       [1, 1, 0, 1, 1],\n       [1, 1, 0, 1, 1],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "morphology.reconstruction()", "path": "api/skimage.morphology#skimage.morphology.reconstruction", "type": "morphology", "text": " \nskimage.morphology.reconstruction(seed, mask, method='dilation', selem=None, offset=None) [source]\n \nPerform a morphological reconstruction of an image. Morphological reconstruction by dilation is similar to basic morphological dilation: high-intensity values will replace nearby low-intensity values. The basic dilation operator, however, uses a structuring element to determine how far a value in the input image can spread. In contrast, reconstruction uses two images: a \u201cseed\u201d image, which specifies the values that spread, and a \u201cmask\u201d image, which gives the maximum allowed value at each pixel. The mask image, like the structuring element, limits the spread of high-intensity values. Reconstruction by erosion is simply the inverse: low-intensity values spread from the seed image and are limited by the mask image, which represents the minimum allowed value. Alternatively, you can think of reconstruction as a way to isolate the connected regions of an image. For dilation, reconstruction connects regions marked by local maxima in the seed image: neighboring pixels less-than-or-equal-to those seeds are connected to the seeded region. Local maxima with values larger than the seed image will get truncated to the seed value.  Parameters \n \nseedndarray \n\nThe seed image (a.k.a. marker image), which specifies the values that are dilated or eroded.  \nmaskndarray \n\nThe maximum (dilation) / minimum (erosion) allowed value at each pixel.  \nmethod{\u2018dilation\u2019|\u2019erosion\u2019}, optional \n\nPerform reconstruction by dilation or erosion. In dilation (or erosion), the seed image is dilated (or eroded) until limited by the mask image. For dilation, each seed value must be less than or equal to the corresponding mask value; for erosion, the reverse is true. Default is \u2018dilation\u2019.  \nselemndarray, optional \n\nThe neighborhood expressed as an n-D array of 1\u2019s and 0\u2019s. Default is the n-D square of radius equal to 1 (i.e. a 3x3 square for 2D images, a 3x3x3 cube for 3D images, etc.)  \noffsetndarray, optional \n\nThe coordinates of the center of the structuring element. Default is located on the geometrical center of the selem, in that case selem dimensions must be odd.    Returns \n \nreconstructedndarray \n\nThe result of morphological reconstruction.     Notes The algorithm is taken from [1]. Applications for greyscale reconstruction are discussed in [2] and [3]. References  \n1  \nRobinson, \u201cEfficient morphological reconstruction: a downhill filter\u201d, Pattern Recognition Letters 25 (2004) 1759-1767.  \n2  \nVincent, L., \u201cMorphological Grayscale Reconstruction in Image Analysis: Applications and Efficient Algorithms\u201d, IEEE Transactions on Image Processing (1993)  \n3  \nSoille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d, Chapter 6, 2nd edition (2003), ISBN 3540429883.   Examples >>> import numpy as np\n>>> from skimage.morphology import reconstruction\n First, we create a sinusoidal mask image with peaks at middle and ends. >>> x = np.linspace(0, 4 * np.pi)\n>>> y_mask = np.cos(x)\n Then, we create a seed image initialized to the minimum mask value (for reconstruction by dilation, min-intensity values don\u2019t spread) and add \u201cseeds\u201d to the left and right peak, but at a fraction of peak value (1). >>> y_seed = y_mask.min() * np.ones_like(x)\n>>> y_seed[0] = 0.5\n>>> y_seed[-1] = 0\n>>> y_rec = reconstruction(y_seed, y_mask)\n The reconstructed image (or curve, in this case) is exactly the same as the mask image, except that the peaks are truncated to 0.5 and 0. The middle peak disappears completely: Since there were no seed values in this peak region, its reconstructed value is truncated to the surrounding value (-1). As a more practical example, we try to extract the bright features of an image by subtracting a background image created by reconstruction. >>> y, x = np.mgrid[:20:0.5, :20:0.5]\n>>> bumps = np.sin(x) + np.sin(y)\n To create the background image, set the mask image to the original image, and the seed image to the original image with an intensity offset, h. >>> h = 0.3\n>>> seed = bumps - h\n>>> background = reconstruction(seed, bumps)\n The resulting reconstructed image looks exactly like the original image, but with the peaks of the bumps cut off. Subtracting this reconstructed image from the original image leaves just the peaks of the bumps >>> hdome = bumps - background\n This operation is known as the h-dome of the image and leaves features of height h in the subtracted image. \n"}, {"name": "morphology.rectangle()", "path": "api/skimage.morphology#skimage.morphology.rectangle", "type": "morphology", "text": " \nskimage.morphology.rectangle(nrows, ncols, dtype=<class 'numpy.uint8'>) [source]\n \nGenerates a flat, rectangular-shaped structuring element. Every pixel in the rectangle generated for a given width and given height belongs to the neighborhood.  Parameters \n \nnrowsint \n\nThe number of rows of the rectangle.  \nncolsint \n\nThe number of columns of the rectangle.    Returns \n \nselemndarray \n\nA structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.    Other Parameters \n \ndtypedata-type \n\nThe data type of the structuring element.     Notes  The use of width and height has been deprecated in version 0.18.0. Use nrows and ncols instead.  \n"}, {"name": "morphology.remove_small_holes()", "path": "api/skimage.morphology#skimage.morphology.remove_small_holes", "type": "morphology", "text": " \nskimage.morphology.remove_small_holes(ar, area_threshold=64, connectivity=1, in_place=False) [source]\n \nRemove contiguous holes smaller than the specified size.  Parameters \n \narndarray (arbitrary shape, int or bool type) \n\nThe array containing the connected components of interest.  \narea_thresholdint, optional (default: 64) \n\nThe maximum area, in pixels, of a contiguous hole that will be filled. Replaces min_size.  \nconnectivityint, {1, 2, \u2026, ar.ndim}, optional (default: 1) \n\nThe connectivity defining the neighborhood of a pixel.  \nin_placebool, optional (default: False) \n\nIf True, remove the connected components in the input array itself. Otherwise, make a copy.    Returns \n \noutndarray, same shape and type as input ar \n\nThe input array with small holes within connected components removed.    Raises \n TypeError\n\nIf the input array is of an invalid type, such as float or string.  ValueError\n\nIf the input array contains negative values.     Notes If the array type is int, it is assumed that it contains already-labeled objects. The labels are not kept in the output image (this function always outputs a bool image). It is suggested that labeling is completed after using this function. Examples >>> from skimage import morphology\n>>> a = np.array([[1, 1, 1, 1, 1, 0],\n...               [1, 1, 1, 0, 1, 0],\n...               [1, 0, 0, 1, 1, 0],\n...               [1, 1, 1, 1, 1, 0]], bool)\n>>> b = morphology.remove_small_holes(a, 2)\n>>> b\narray([[ True,  True,  True,  True,  True, False],\n       [ True,  True,  True,  True,  True, False],\n       [ True, False, False,  True,  True, False],\n       [ True,  True,  True,  True,  True, False]])\n>>> c = morphology.remove_small_holes(a, 2, connectivity=2)\n>>> c\narray([[ True,  True,  True,  True,  True, False],\n       [ True,  True,  True, False,  True, False],\n       [ True, False, False,  True,  True, False],\n       [ True,  True,  True,  True,  True, False]])\n>>> d = morphology.remove_small_holes(a, 2, in_place=True)\n>>> d is a\nTrue\n \n"}, {"name": "morphology.remove_small_objects()", "path": "api/skimage.morphology#skimage.morphology.remove_small_objects", "type": "morphology", "text": " \nskimage.morphology.remove_small_objects(ar, min_size=64, connectivity=1, in_place=False) [source]\n \nRemove objects smaller than the specified size. Expects ar to be an array with labeled objects, and removes objects smaller than min_size. If ar is bool, the image is first labeled. This leads to potentially different behavior for bool and 0-and-1 arrays.  Parameters \n \narndarray (arbitrary shape, int or bool type) \n\nThe array containing the objects of interest. If the array type is int, the ints must be non-negative.  \nmin_sizeint, optional (default: 64) \n\nThe smallest allowable object size.  \nconnectivityint, {1, 2, \u2026, ar.ndim}, optional (default: 1) \n\nThe connectivity defining the neighborhood of a pixel. Used during labelling if ar is bool.  \nin_placebool, optional (default: False) \n\nIf True, remove the objects in the input array itself. Otherwise, make a copy.    Returns \n \noutndarray, same shape and type as input ar \n\nThe input array with small connected components removed.    Raises \n TypeError\n\nIf the input array is of an invalid type, such as float or string.  ValueError\n\nIf the input array contains negative values.     Examples >>> from skimage import morphology\n>>> a = np.array([[0, 0, 0, 1, 0],\n...               [1, 1, 1, 0, 0],\n...               [1, 1, 1, 0, 1]], bool)\n>>> b = morphology.remove_small_objects(a, 6)\n>>> b\narray([[False, False, False, False, False],\n       [ True,  True,  True, False, False],\n       [ True,  True,  True, False, False]])\n>>> c = morphology.remove_small_objects(a, 7, connectivity=2)\n>>> c\narray([[False, False, False,  True, False],\n       [ True,  True,  True, False, False],\n       [ True,  True,  True, False, False]])\n>>> d = morphology.remove_small_objects(a, 6, in_place=True)\n>>> d is a\nTrue\n \n"}, {"name": "morphology.skeletonize()", "path": "api/skimage.morphology#skimage.morphology.skeletonize", "type": "morphology", "text": " \nskimage.morphology.skeletonize(image, *, method=None) [source]\n \nCompute the skeleton of a binary image. Thinning is used to reduce each connected component in a binary image to a single-pixel wide skeleton.  Parameters \n \nimagendarray, 2D or 3D \n\nA binary image containing the objects to be skeletonized. Zeros represent background, nonzero values are foreground.  \nmethod{\u2018zhang\u2019, \u2018lee\u2019}, optional \n\nWhich algorithm to use. Zhang\u2019s algorithm [Zha84] only works for 2D images, and is the default for 2D. Lee\u2019s algorithm [Lee94] works for 2D or 3D images and is the default for 3D.    Returns \n \nskeletonndarray \n\nThe thinned image.      See also  \nmedial_axis\n\n  References  \nLee94  \nT.-C. Lee, R.L. Kashyap and C.-N. Chu, Building skeleton models via 3-D medial surface/axis thinning algorithms. Computer Vision, Graphics, and Image Processing, 56(6):462-478, 1994.  \nZha84  \nA fast parallel algorithm for thinning digital patterns, T. Y. Zhang and C. Y. Suen, Communications of the ACM, March 1984, Volume 27, Number 3.   Examples >>> X, Y = np.ogrid[0:9, 0:9]\n>>> ellipse = (1./3 * (X - 4)**2 + (Y - 4)**2 < 3**2).astype(np.uint8)\n>>> ellipse\narray([[0, 0, 0, 1, 1, 1, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 0, 0],\n       [0, 0, 0, 1, 1, 1, 0, 0, 0]], dtype=uint8)\n>>> skel = skeletonize(ellipse)\n>>> skel.astype(np.uint8)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "morphology.skeletonize_3d()", "path": "api/skimage.morphology#skimage.morphology.skeletonize_3d", "type": "morphology", "text": " \nskimage.morphology.skeletonize_3d(image) [source]\n \nCompute the skeleton of a binary image. Thinning is used to reduce each connected component in a binary image to a single-pixel wide skeleton.  Parameters \n \nimagendarray, 2D or 3D \n\nA binary image containing the objects to be skeletonized. Zeros represent background, nonzero values are foreground.    Returns \n \nskeletonndarray \n\nThe thinned image.      See also  \nskeletonize, medial_axis\n\n  Notes The method of [Lee94] uses an octree data structure to examine a 3x3x3 neighborhood of a pixel. The algorithm proceeds by iteratively sweeping over the image, and removing pixels at each iteration until the image stops changing. Each iteration consists of two steps: first, a list of candidates for removal is assembled; then pixels from this list are rechecked sequentially, to better preserve connectivity of the image. The algorithm this function implements is different from the algorithms used by either skeletonize or medial_axis, thus for 2D images the results produced by this function are generally different. References  \nLee94  \nT.-C. Lee, R.L. Kashyap and C.-N. Chu, Building skeleton models via 3-D medial surface/axis thinning algorithms. Computer Vision, Graphics, and Image Processing, 56(6):462-478, 1994.   \n"}, {"name": "morphology.square()", "path": "api/skimage.morphology#skimage.morphology.square", "type": "morphology", "text": " \nskimage.morphology.square(width, dtype=<class 'numpy.uint8'>) [source]\n \nGenerates a flat, square-shaped structuring element. Every pixel along the perimeter has a chessboard distance no greater than radius (radius=floor(width/2)) pixels.  Parameters \n \nwidthint \n\nThe width and height of the square.    Returns \n \nselemndarray \n\nA structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.    Other Parameters \n \ndtypedata-type \n\nThe data type of the structuring element.     \n"}, {"name": "morphology.star()", "path": "api/skimage.morphology#skimage.morphology.star", "type": "morphology", "text": " \nskimage.morphology.star(a, dtype=<class 'numpy.uint8'>) [source]\n \nGenerates a star shaped structuring element. Start has 8 vertices and is an overlap of square of size 2*a + 1 with its 45 degree rotated version. The slanted sides are 45 or 135 degrees to the horizontal axis.  Parameters \n \naint \n\nParameter deciding the size of the star structural element. The side of the square array returned is 2*a + 1 + 2*floor(a / 2).    Returns \n \nselemndarray \n\nThe structuring element where elements of the neighborhood are 1 and 0 otherwise.    Other Parameters \n \ndtypedata-type \n\nThe data type of the structuring element.     \n"}, {"name": "morphology.thin()", "path": "api/skimage.morphology#skimage.morphology.thin", "type": "morphology", "text": " \nskimage.morphology.thin(image, max_iter=None) [source]\n \nPerform morphological thinning of a binary image.  Parameters \n \nimagebinary (M, N) ndarray \n\nThe image to be thinned.  \nmax_iterint, number of iterations, optional \n\nRegardless of the value of this parameter, the thinned image is returned immediately if an iteration produces no change. If this parameter is specified it thus sets an upper bound on the number of iterations performed.    Returns \n \noutndarray of bool \n\nThinned image.      See also  \nskeletonize, medial_axis\n\n  Notes This algorithm [1] works by making multiple passes over the image, removing pixels matching a set of criteria designed to thin connected regions while preserving eight-connected components and 2 x 2 squares [2]. In each of the two sub-iterations the algorithm correlates the intermediate skeleton image with a neighborhood mask, then looks up each neighborhood in a lookup table indicating whether the central pixel should be deleted in that sub-iteration. References  \n1  \nZ. Guo and R. W. Hall, \u201cParallel thinning with two-subiteration algorithms,\u201d Comm. ACM, vol. 32, no. 3, pp. 359-373, 1989. DOI:10.1145/62065.62074  \n2  \nLam, L., Seong-Whan Lee, and Ching Y. Suen, \u201cThinning Methodologies-A Comprehensive Survey,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol 14, No. 9, p. 879, 1992. DOI:10.1109/34.161346   Examples >>> square = np.zeros((7, 7), dtype=np.uint8)\n>>> square[1:-1, 2:-2] = 1\n>>> square[0, 1] =  1\n>>> square\narray([[0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n>>> skel = thin(square)\n>>> skel.astype(np.uint8)\narray([[0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "morphology.watershed()", "path": "api/skimage.morphology#skimage.morphology.watershed", "type": "morphology", "text": " \nskimage.morphology.watershed(image, markers=None, connectivity=1, offset=None, mask=None, compactness=0, watershed_line=False) [source]\n \nDeprecated function. Use skimage.segmentation.watershed instead. Find watershed basins in image flooded from given markers.  Parameters \n \nimagendarray (2-D, 3-D, \u2026) of integers \n\nData array where the lowest value points are labeled first.  \nmarkersint, or ndarray of int, same shape as image, optional \n\nThe desired number of markers, or an array marking the basins with the values to be assigned in the label matrix. Zero means not a marker. If None (no markers given), the local minima of the image are used as markers.  \nconnectivityndarray, optional \n\nAn array with the same number of dimensions as image whose non-zero elements indicate neighbors for connection. Following the scipy convention, default is a one-connected array of the dimension of the image.  \noffsetarray_like of shape image.ndim, optional \n\noffset of the connectivity (one offset per dimension)  \nmaskndarray of bools or 0s and 1s, optional \n\nArray of same shape as image. Only points at which mask == True will be labeled.  \ncompactnessfloat, optional \n\nUse compact watershed [3] with given compactness parameter. Higher values result in more regularly-shaped watershed basins.  \nwatershed_linebool, optional \n\nIf watershed_line is True, a one-pixel wide line separates the regions obtained by the watershed algorithm. The line has the label 0.    Returns \n out: ndarray\n\nA labeled matrix of the same type and shape as markers      See also  \nskimage.segmentation.random_walker\n\n\nrandom walker segmentation A segmentation algorithm based on anisotropic diffusion, usually slower than the watershed but with good results on noisy data and boundaries with holes.    Notes This function implements a watershed algorithm [1] [2] that apportions pixels into marked basins. The algorithm uses a priority queue to hold the pixels with the metric for the priority queue being pixel value, then the time of entry into the queue - this settles ties in favor of the closest marker. Some ideas taken from Soille, \u201cAutomated Basin Delineation from Digital Elevation Models Using Mathematical Morphology\u201d, Signal Processing 20 (1990) 171-182 The most important insight in the paper is that entry time onto the queue solves two problems: a pixel should be assigned to the neighbor with the largest gradient or, if there is no gradient, pixels on a plateau should be split between markers on opposite sides. This implementation converts all arguments to specific, lowest common denominator types, then passes these to a C algorithm. Markers can be determined manually, or automatically using for example the local minima of the gradient of the image, or the local maxima of the distance function to the background for separating overlapping objects (see example). References  \n1  \nhttps://en.wikipedia.org/wiki/Watershed_%28image_processing%29  \n2  \nhttp://cmm.ensmp.fr/~beucher/wtshed.html  \n3  \nPeer Neubert & Peter Protzel (2014). Compact Watershed and Preemptive SLIC: On Improving Trade-offs of Superpixel Segmentation Algorithms. ICPR 2014, pp 996-1001. DOI:10.1109/ICPR.2014.181 https://www.tu-chemnitz.de/etit/proaut/publications/cws_pSLIC_ICPR.pdf   Examples The watershed algorithm is useful to separate overlapping objects. We first generate an initial image with two overlapping circles: >>> import numpy as np\n>>> x, y = np.indices((80, 80))\n>>> x1, y1, x2, y2 = 28, 28, 44, 52\n>>> r1, r2 = 16, 20\n>>> mask_circle1 = (x - x1)**2 + (y - y1)**2 < r1**2\n>>> mask_circle2 = (x - x2)**2 + (y - y2)**2 < r2**2\n>>> image = np.logical_or(mask_circle1, mask_circle2)\n Next, we want to separate the two circles. We generate markers at the maxima of the distance to the background: >>> from scipy import ndimage as ndi\n>>> distance = ndi.distance_transform_edt(image)\n>>> from skimage.feature import peak_local_max\n>>> local_maxi = peak_local_max(distance, labels=image,\n...                             footprint=np.ones((3, 3)),\n...                             indices=False)\n>>> markers = ndi.label(local_maxi)[0]\n Finally, we run the watershed on the image and markers: >>> labels = watershed(-distance, markers, mask=image)  \n The algorithm works also for 3-D images, and can be used for example to separate overlapping spheres. \n"}, {"name": "morphology.white_tophat()", "path": "api/skimage.morphology#skimage.morphology.white_tophat", "type": "morphology", "text": " \nskimage.morphology.white_tophat(image, selem=None, out=None) [source]\n \nReturn white top hat of an image. The white top hat of an image is defined as the image minus its morphological opening. This operation returns the bright spots of the image that are smaller than the structuring element.  Parameters \n \nimagendarray \n\nImage array.  \nselemndarray, optional \n\nThe neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).  \noutndarray, optional \n\nThe array to store the result of the morphology. If None is passed, a new array will be allocated.    Returns \n \noutarray, same shape and type as image \n\nThe result of the morphological white top hat.      See also  \nblack_tophat\n\n  References  \n1  \nhttps://en.wikipedia.org/wiki/Top-hat_transform   Examples >>> # Subtract grey background from bright peak\n>>> import numpy as np\n>>> from skimage.morphology import square\n>>> bright_on_grey = np.array([[2, 3, 3, 3, 2],\n...                            [3, 4, 5, 4, 3],\n...                            [3, 5, 9, 5, 3],\n...                            [3, 4, 5, 4, 3],\n...                            [2, 3, 3, 3, 2]], dtype=np.uint8)\n>>> white_tophat(bright_on_grey, square(3))\narray([[0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 1, 5, 1, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0]], dtype=uint8)\n \n"}, {"name": "registration", "path": "api/skimage.registration", "type": "registration", "text": "Module: registration  \nskimage.registration.optical_flow_ilk(\u2026[, \u2026]) Coarse to fine optical flow estimator.  \nskimage.registration.optical_flow_tvl1(\u2026) Coarse to fine optical flow estimator.  \nskimage.registration.phase_cross_correlation(\u2026) Efficient subpixel image translation registration by cross-correlation.   optical_flow_ilk  \nskimage.registration.optical_flow_ilk(reference_image, moving_image, *, radius=7, num_warp=10, gaussian=False, prefilter=False, dtype=<class 'numpy.float32'>) [source]\n \nCoarse to fine optical flow estimator. The iterative Lucas-Kanade (iLK) solver is applied at each level of the image pyramid. iLK [1] is a fast and robust alternative to TVL1 algorithm although less accurate for rendering flat surfaces and object boundaries (see [2]).  Parameters \n \nreference_imagendarray, shape (M, N[, P[, \u2026]]) \n\nThe first gray scale image of the sequence.  \nmoving_imagendarray, shape (M, N[, P[, \u2026]]) \n\nThe second gray scale image of the sequence.  \nradiusint, optional \n\nRadius of the window considered around each pixel.  \nnum_warpint, optional \n\nNumber of times moving_image is warped.  \ngaussianbool, optional \n\nIf True, a Gaussian kernel is used for the local integration. Otherwise, a uniform kernel is used.  \nprefilterbool, optional \n\nWhether to prefilter the estimated optical flow before each image warp. When True, a median filter with window size 3 along each axis is applied. This helps to remove potential outliers.  \ndtypedtype, optional \n\nOutput data type: must be floating point. Single precision provides good results and saves memory usage and computation time compared to double precision.    Returns \n \nflowndarray, shape ((reference_image.ndim, M, N[, P[, \u2026]]) \n\nThe estimated optical flow components for each axis.     Notes  The implemented algorithm is described in Table2 of [1]. Color images are not supported.  References  \n1(1,2)  \nLe Besnerais, G., & Champagnat, F. (2005, September). Dense optical flow by iterative local window registration. In IEEE International Conference on Image Processing 2005 (Vol. 1, pp. I-137). IEEE. DOI:10.1109/ICIP.2005.1529706  \n2  \nPlyer, A., Le Besnerais, G., & Champagnat, F. (2016). Massively parallel Lucas Kanade optical flow for real-time video processing applications. Journal of Real-Time Image Processing, 11(4), 713-730. DOI:10.1007/s11554-014-0423-0   Examples >>> from skimage.color import rgb2gray\n>>> from skimage.data import stereo_motorcycle\n>>> from skimage.registration import optical_flow_ilk\n>>> reference_image, moving_image, disp = stereo_motorcycle()\n>>> # --- Convert the images to gray level: color is not supported.\n>>> reference_image = rgb2gray(reference_image)\n>>> moving_image = rgb2gray(moving_image)\n>>> flow = optical_flow_ilk(moving_image, reference_image)\n \n Examples using skimage.registration.optical_flow_ilk\n \n  Registration using optical flow   optical_flow_tvl1  \nskimage.registration.optical_flow_tvl1(reference_image, moving_image, *, attachment=15, tightness=0.3, num_warp=5, num_iter=10, tol=0.0001, prefilter=False, dtype=<class 'numpy.float32'>) [source]\n \nCoarse to fine optical flow estimator. The TV-L1 solver is applied at each level of the image pyramid. TV-L1 is a popular algorithm for optical flow estimation introduced by Zack et al. [1], improved in [2] and detailed in [3].  Parameters \n \nreference_imagendarray, shape (M, N[, P[, \u2026]]) \n\nThe first gray scale image of the sequence.  \nmoving_imagendarray, shape (M, N[, P[, \u2026]]) \n\nThe second gray scale image of the sequence.  \nattachmentfloat, optional \n\nAttachment parameter (\\(\\lambda\\) in [1]). The smaller this parameter is, the smoother the returned result will be.  \ntightnessfloat, optional \n\nTightness parameter (\\(\\tau\\) in [1]). It should have a small value in order to maintain attachement and regularization parts in correspondence.  \nnum_warpint, optional \n\nNumber of times image1 is warped.  \nnum_iterint, optional \n\nNumber of fixed point iteration.  \ntolfloat, optional \n\nTolerance used as stopping criterion based on the L\u00b2 distance between two consecutive values of (u, v).  \nprefilterbool, optional \n\nWhether to prefilter the estimated optical flow before each image warp. When True, a median filter with window size 3 along each axis is applied. This helps to remove potential outliers.  \ndtypedtype, optional \n\nOutput data type: must be floating point. Single precision provides good results and saves memory usage and computation time compared to double precision.    Returns \n \nflowndarray, shape ((image0.ndim, M, N[, P[, \u2026]]) \n\nThe estimated optical flow components for each axis.     Notes Color images are not supported. References  \n1(1,2,3)  \nZach, C., Pock, T., & Bischof, H. (2007, September). A duality based approach for realtime TV-L 1 optical flow. In Joint pattern recognition symposium (pp. 214-223). Springer, Berlin, Heidelberg. DOI:10.1007/978-3-540-74936-3_22  \n2  \nWedel, A., Pock, T., Zach, C., Bischof, H., & Cremers, D. (2009). An improved algorithm for TV-L 1 optical flow. In Statistical and geometrical approaches to visual motion analysis (pp. 23-45). Springer, Berlin, Heidelberg. DOI:10.1007/978-3-642-03061-1_2  \n3  \nP\u00e9rez, J. S., Meinhardt-Llopis, E., & Facciolo, G. (2013). TV-L1 optical flow estimation. Image Processing On Line, 2013, 137-150. DOI:10.5201/ipol.2013.26   Examples >>> from skimage.color import rgb2gray\n>>> from skimage.data import stereo_motorcycle\n>>> from skimage.registration import optical_flow_tvl1\n>>> image0, image1, disp = stereo_motorcycle()\n>>> # --- Convert the images to gray level: color is not supported.\n>>> image0 = rgb2gray(image0)\n>>> image1 = rgb2gray(image1)\n>>> flow = optical_flow_tvl1(image1, image0)\n \n Examples using skimage.registration.optical_flow_tvl1\n \n  Registration using optical flow   phase_cross_correlation  \nskimage.registration.phase_cross_correlation(reference_image, moving_image, *, upsample_factor=1, space='real', return_error=True, reference_mask=None, moving_mask=None, overlap_ratio=0.3) [source]\n \nEfficient subpixel image translation registration by cross-correlation. This code gives the same precision as the FFT upsampled cross-correlation in a fraction of the computation time and with reduced memory requirements. It obtains an initial estimate of the cross-correlation peak by an FFT and then refines the shift estimation by upsampling the DFT only in a small neighborhood of that estimate by means of a matrix-multiply DFT.  Parameters \n \nreference_imagearray \n\nReference image.  \nmoving_imagearray \n\nImage to register. Must be same dimensionality as reference_image.  \nupsample_factorint, optional \n\nUpsampling factor. Images will be registered to within 1 / upsample_factor of a pixel. For example upsample_factor == 20 means the images will be registered within 1/20th of a pixel. Default is 1 (no upsampling). Not used if any of reference_mask or moving_mask is not None.  \nspacestring, one of \u201creal\u201d or \u201cfourier\u201d, optional \n\nDefines how the algorithm interprets input data. \u201creal\u201d means data will be FFT\u2019d to compute the correlation, while \u201cfourier\u201d data will bypass FFT of input data. Case insensitive. Not used if any of reference_mask or moving_mask is not None.  \nreturn_errorbool, optional \n\nReturns error and phase difference if on, otherwise only shifts are returned. Has noeffect if any of reference_mask or moving_mask is not None. In this case only shifts is returned.  \nreference_maskndarray \n\nBoolean mask for reference_image. The mask should evaluate to True (or 1) on valid pixels. reference_mask should have the same shape as reference_image.  \nmoving_maskndarray or None, optional \n\nBoolean mask for moving_image. The mask should evaluate to True (or 1) on valid pixels. moving_mask should have the same shape as moving_image. If None, reference_mask will be used.  \noverlap_ratiofloat, optional \n\nMinimum allowed overlap ratio between images. The correlation for translations corresponding with an overlap ratio lower than this threshold will be ignored. A lower overlap_ratio leads to smaller maximum translation, while a higher overlap_ratio leads to greater robustness against spurious matches due to small overlap between masked images. Used only if one of reference_mask or moving_mask is None.    Returns \n \nshiftsndarray \n\nShift vector (in pixels) required to register moving_image with reference_image. Axis ordering is consistent with numpy (e.g. Z, Y, X)  \nerrorfloat \n\nTranslation invariant normalized RMS error between reference_image and moving_image.  \nphasedifffloat \n\nGlobal phase difference between the two images (should be zero if images are non-negative).     References  \n1  \nManuel Guizar-Sicairos, Samuel T. Thurman, and James R. Fienup, \u201cEfficient subpixel image registration algorithms,\u201d Optics Letters 33, 156-158 (2008). DOI:10.1364/OL.33.000156  \n2  \nJames R. Fienup, \u201cInvariant error metrics for image reconstruction\u201d Optics Letters 36, 8352-8357 (1997). DOI:10.1364/AO.36.008352  \n3  \nDirk Padfield. Masked Object Registration in the Fourier Domain. IEEE Transactions on Image Processing, vol. 21(5), pp. 2706-2718 (2012). DOI:10.1109/TIP.2011.2181402  \n4  \nD. Padfield. \u201cMasked FFT registration\u201d. In Proc. Computer Vision and Pattern Recognition, pp. 2918-2925 (2010). DOI:10.1109/CVPR.2010.5540032   \n Examples using skimage.registration.phase_cross_correlation\n \n  Masked Normalized Cross-Correlation  \n"}, {"name": "registration.optical_flow_ilk()", "path": "api/skimage.registration#skimage.registration.optical_flow_ilk", "type": "registration", "text": " \nskimage.registration.optical_flow_ilk(reference_image, moving_image, *, radius=7, num_warp=10, gaussian=False, prefilter=False, dtype=<class 'numpy.float32'>) [source]\n \nCoarse to fine optical flow estimator. The iterative Lucas-Kanade (iLK) solver is applied at each level of the image pyramid. iLK [1] is a fast and robust alternative to TVL1 algorithm although less accurate for rendering flat surfaces and object boundaries (see [2]).  Parameters \n \nreference_imagendarray, shape (M, N[, P[, \u2026]]) \n\nThe first gray scale image of the sequence.  \nmoving_imagendarray, shape (M, N[, P[, \u2026]]) \n\nThe second gray scale image of the sequence.  \nradiusint, optional \n\nRadius of the window considered around each pixel.  \nnum_warpint, optional \n\nNumber of times moving_image is warped.  \ngaussianbool, optional \n\nIf True, a Gaussian kernel is used for the local integration. Otherwise, a uniform kernel is used.  \nprefilterbool, optional \n\nWhether to prefilter the estimated optical flow before each image warp. When True, a median filter with window size 3 along each axis is applied. This helps to remove potential outliers.  \ndtypedtype, optional \n\nOutput data type: must be floating point. Single precision provides good results and saves memory usage and computation time compared to double precision.    Returns \n \nflowndarray, shape ((reference_image.ndim, M, N[, P[, \u2026]]) \n\nThe estimated optical flow components for each axis.     Notes  The implemented algorithm is described in Table2 of [1]. Color images are not supported.  References  \n1(1,2)  \nLe Besnerais, G., & Champagnat, F. (2005, September). Dense optical flow by iterative local window registration. In IEEE International Conference on Image Processing 2005 (Vol. 1, pp. I-137). IEEE. DOI:10.1109/ICIP.2005.1529706  \n2  \nPlyer, A., Le Besnerais, G., & Champagnat, F. (2016). Massively parallel Lucas Kanade optical flow for real-time video processing applications. Journal of Real-Time Image Processing, 11(4), 713-730. DOI:10.1007/s11554-014-0423-0   Examples >>> from skimage.color import rgb2gray\n>>> from skimage.data import stereo_motorcycle\n>>> from skimage.registration import optical_flow_ilk\n>>> reference_image, moving_image, disp = stereo_motorcycle()\n>>> # --- Convert the images to gray level: color is not supported.\n>>> reference_image = rgb2gray(reference_image)\n>>> moving_image = rgb2gray(moving_image)\n>>> flow = optical_flow_ilk(moving_image, reference_image)\n \n"}, {"name": "registration.optical_flow_tvl1()", "path": "api/skimage.registration#skimage.registration.optical_flow_tvl1", "type": "registration", "text": " \nskimage.registration.optical_flow_tvl1(reference_image, moving_image, *, attachment=15, tightness=0.3, num_warp=5, num_iter=10, tol=0.0001, prefilter=False, dtype=<class 'numpy.float32'>) [source]\n \nCoarse to fine optical flow estimator. The TV-L1 solver is applied at each level of the image pyramid. TV-L1 is a popular algorithm for optical flow estimation introduced by Zack et al. [1], improved in [2] and detailed in [3].  Parameters \n \nreference_imagendarray, shape (M, N[, P[, \u2026]]) \n\nThe first gray scale image of the sequence.  \nmoving_imagendarray, shape (M, N[, P[, \u2026]]) \n\nThe second gray scale image of the sequence.  \nattachmentfloat, optional \n\nAttachment parameter (\\(\\lambda\\) in [1]). The smaller this parameter is, the smoother the returned result will be.  \ntightnessfloat, optional \n\nTightness parameter (\\(\\tau\\) in [1]). It should have a small value in order to maintain attachement and regularization parts in correspondence.  \nnum_warpint, optional \n\nNumber of times image1 is warped.  \nnum_iterint, optional \n\nNumber of fixed point iteration.  \ntolfloat, optional \n\nTolerance used as stopping criterion based on the L\u00b2 distance between two consecutive values of (u, v).  \nprefilterbool, optional \n\nWhether to prefilter the estimated optical flow before each image warp. When True, a median filter with window size 3 along each axis is applied. This helps to remove potential outliers.  \ndtypedtype, optional \n\nOutput data type: must be floating point. Single precision provides good results and saves memory usage and computation time compared to double precision.    Returns \n \nflowndarray, shape ((image0.ndim, M, N[, P[, \u2026]]) \n\nThe estimated optical flow components for each axis.     Notes Color images are not supported. References  \n1(1,2,3)  \nZach, C., Pock, T., & Bischof, H. (2007, September). A duality based approach for realtime TV-L 1 optical flow. In Joint pattern recognition symposium (pp. 214-223). Springer, Berlin, Heidelberg. DOI:10.1007/978-3-540-74936-3_22  \n2  \nWedel, A., Pock, T., Zach, C., Bischof, H., & Cremers, D. (2009). An improved algorithm for TV-L 1 optical flow. In Statistical and geometrical approaches to visual motion analysis (pp. 23-45). Springer, Berlin, Heidelberg. DOI:10.1007/978-3-642-03061-1_2  \n3  \nP\u00e9rez, J. S., Meinhardt-Llopis, E., & Facciolo, G. (2013). TV-L1 optical flow estimation. Image Processing On Line, 2013, 137-150. DOI:10.5201/ipol.2013.26   Examples >>> from skimage.color import rgb2gray\n>>> from skimage.data import stereo_motorcycle\n>>> from skimage.registration import optical_flow_tvl1\n>>> image0, image1, disp = stereo_motorcycle()\n>>> # --- Convert the images to gray level: color is not supported.\n>>> image0 = rgb2gray(image0)\n>>> image1 = rgb2gray(image1)\n>>> flow = optical_flow_tvl1(image1, image0)\n \n"}, {"name": "registration.phase_cross_correlation()", "path": "api/skimage.registration#skimage.registration.phase_cross_correlation", "type": "registration", "text": " \nskimage.registration.phase_cross_correlation(reference_image, moving_image, *, upsample_factor=1, space='real', return_error=True, reference_mask=None, moving_mask=None, overlap_ratio=0.3) [source]\n \nEfficient subpixel image translation registration by cross-correlation. This code gives the same precision as the FFT upsampled cross-correlation in a fraction of the computation time and with reduced memory requirements. It obtains an initial estimate of the cross-correlation peak by an FFT and then refines the shift estimation by upsampling the DFT only in a small neighborhood of that estimate by means of a matrix-multiply DFT.  Parameters \n \nreference_imagearray \n\nReference image.  \nmoving_imagearray \n\nImage to register. Must be same dimensionality as reference_image.  \nupsample_factorint, optional \n\nUpsampling factor. Images will be registered to within 1 / upsample_factor of a pixel. For example upsample_factor == 20 means the images will be registered within 1/20th of a pixel. Default is 1 (no upsampling). Not used if any of reference_mask or moving_mask is not None.  \nspacestring, one of \u201creal\u201d or \u201cfourier\u201d, optional \n\nDefines how the algorithm interprets input data. \u201creal\u201d means data will be FFT\u2019d to compute the correlation, while \u201cfourier\u201d data will bypass FFT of input data. Case insensitive. Not used if any of reference_mask or moving_mask is not None.  \nreturn_errorbool, optional \n\nReturns error and phase difference if on, otherwise only shifts are returned. Has noeffect if any of reference_mask or moving_mask is not None. In this case only shifts is returned.  \nreference_maskndarray \n\nBoolean mask for reference_image. The mask should evaluate to True (or 1) on valid pixels. reference_mask should have the same shape as reference_image.  \nmoving_maskndarray or None, optional \n\nBoolean mask for moving_image. The mask should evaluate to True (or 1) on valid pixels. moving_mask should have the same shape as moving_image. If None, reference_mask will be used.  \noverlap_ratiofloat, optional \n\nMinimum allowed overlap ratio between images. The correlation for translations corresponding with an overlap ratio lower than this threshold will be ignored. A lower overlap_ratio leads to smaller maximum translation, while a higher overlap_ratio leads to greater robustness against spurious matches due to small overlap between masked images. Used only if one of reference_mask or moving_mask is None.    Returns \n \nshiftsndarray \n\nShift vector (in pixels) required to register moving_image with reference_image. Axis ordering is consistent with numpy (e.g. Z, Y, X)  \nerrorfloat \n\nTranslation invariant normalized RMS error between reference_image and moving_image.  \nphasedifffloat \n\nGlobal phase difference between the two images (should be zero if images are non-negative).     References  \n1  \nManuel Guizar-Sicairos, Samuel T. Thurman, and James R. Fienup, \u201cEfficient subpixel image registration algorithms,\u201d Optics Letters 33, 156-158 (2008). DOI:10.1364/OL.33.000156  \n2  \nJames R. Fienup, \u201cInvariant error metrics for image reconstruction\u201d Optics Letters 36, 8352-8357 (1997). DOI:10.1364/AO.36.008352  \n3  \nDirk Padfield. Masked Object Registration in the Fourier Domain. IEEE Transactions on Image Processing, vol. 21(5), pp. 2706-2718 (2012). DOI:10.1109/TIP.2011.2181402  \n4  \nD. Padfield. \u201cMasked FFT registration\u201d. In Proc. Computer Vision and Pattern Recognition, pp. 2918-2925 (2010). DOI:10.1109/CVPR.2010.5540032   \n"}, {"name": "restoration", "path": "api/skimage.restoration", "type": "restoration", "text": "Module: restoration Image restoration module.  \nskimage.restoration.ball_kernel(radius, ndim) Create a ball kernel for restoration.rolling_ball.  \nskimage.restoration.calibrate_denoiser(\u2026) Calibrate a denoising function and return optimal J-invariant version.  \nskimage.restoration.cycle_spin(x, func, \u2026) Cycle spinning (repeatedly apply func to shifted versions of x).  \nskimage.restoration.denoise_bilateral(image) Denoise image using bilateral filter.  \nskimage.restoration.denoise_nl_means(image) Perform non-local means denoising on 2-D or 3-D grayscale images, and 2-D RGB images.  \nskimage.restoration.denoise_tv_bregman(\u2026) Perform total-variation denoising using split-Bregman optimization.  \nskimage.restoration.denoise_tv_chambolle(image) Perform total-variation denoising on n-dimensional images.  \nskimage.restoration.denoise_wavelet(image[, \u2026]) Perform wavelet denoising on an image.  \nskimage.restoration.ellipsoid_kernel(shape, \u2026) Create an ellipoid kernel for restoration.rolling_ball.  \nskimage.restoration.estimate_sigma(image[, \u2026]) Robust wavelet-based estimator of the (Gaussian) noise standard deviation.  \nskimage.restoration.inpaint_biharmonic(\u2026) Inpaint masked points in image with biharmonic equations.  \nskimage.restoration.richardson_lucy(image, psf) Richardson-Lucy deconvolution.  \nskimage.restoration.rolling_ball(image, *[, \u2026]) Estimate background intensity by rolling/translating a kernel.  \nskimage.restoration.unsupervised_wiener(\u2026) Unsupervised Wiener-Hunt deconvolution.  \nskimage.restoration.unwrap_phase(image[, \u2026]) Recover the original from a wrapped phase image.  \nskimage.restoration.wiener(image, psf, balance) Wiener-Hunt deconvolution   ball_kernel  \nskimage.restoration.ball_kernel(radius, ndim) [source]\n \nCreate a ball kernel for restoration.rolling_ball.  Parameters \n \nradiusint \n\nRadius of the ball.  \nndimint \n\nNumber of dimensions of the ball. ndim should match the dimensionality of the image the kernel will be applied to.    Returns \n \nkernelndarray \n\nThe kernel containing the surface intensity of the top half of the ellipsoid.      See also  \nrolling_ball\n\n  \n calibrate_denoiser  \nskimage.restoration.calibrate_denoiser(image, denoise_function, denoise_parameters, *, stride=4, approximate_loss=True, extra_output=False) [source]\n \nCalibrate a denoising function and return optimal J-invariant version. The returned function is partially evaluated with optimal parameter values set for denoising the input image.  Parameters \n \nimagendarray \n\nInput data to be denoised (converted using img_as_float).  \ndenoise_functionfunction \n\nDenoising function to be calibrated.  \ndenoise_parametersdict of list \n\nRanges of parameters for denoise_function to be calibrated over.  \nstrideint, optional \n\nStride used in masking procedure that converts denoise_function to J-invariance.  \napproximate_lossbool, optional \n\nWhether to approximate the self-supervised loss used to evaluate the denoiser by only computing it on one masked version of the image. If False, the runtime will be a factor of stride**image.ndim longer.  \nextra_outputbool, optional \n\nIf True, return parameters and losses in addition to the calibrated denoising function    Returns \n \nbest_denoise_functionfunction \n\nThe optimal J-invariant version of denoise_function.  \nIf extra_output is True, the following tuple is also returned: \n\n(parameters_tested, losses)tuple (list of dict, list of int) \n\nList of parameters tested for denoise_function, as a dictionary of kwargs Self-supervised loss for each set of parameters in parameters_tested.     Notes The calibration procedure uses a self-supervised mean-square-error loss to evaluate the performance of J-invariant versions of denoise_function. The minimizer of the self-supervised loss is also the minimizer of the ground-truth loss (i.e., the true MSE error) [1]. The returned function can be used on the original noisy image, or other images with similar characteristics.  \nIncreasing the stride increases the performance of best_denoise_function \n\nat the expense of increasing its runtime. It has no effect on the runtime of the calibration.   References  \n1  \nJ. Batson & L. Royer. Noise2Self: Blind Denoising by Self-Supervision, International Conference on Machine Learning, p. 524-533 (2019).   Examples >>> from skimage import color, data\n>>> from skimage.restoration import denoise_wavelet\n>>> import numpy as np\n>>> img = color.rgb2gray(data.astronaut()[:50, :50])\n>>> noisy = img + 0.5 * img.std() * np.random.randn(*img.shape)\n>>> parameters = {'sigma': np.arange(0.1, 0.4, 0.02)}\n>>> denoising_function = calibrate_denoiser(noisy, denoise_wavelet,\n...                                         denoise_parameters=parameters)\n>>> denoised_img = denoising_function(img)\n \n cycle_spin  \nskimage.restoration.cycle_spin(x, func, max_shifts, shift_steps=1, num_workers=None, multichannel=False, func_kw={}) [source]\n \nCycle spinning (repeatedly apply func to shifted versions of x).  Parameters \n \nxarray-like \n\nData for input to func.  \nfuncfunction \n\nA function to apply to circularly shifted versions of x. Should take x as its first argument. Any additional arguments can be supplied via func_kw.  \nmax_shiftsint or tuple \n\nIf an integer, shifts in range(0, max_shifts+1) will be used along each axis of x. If a tuple, range(0, max_shifts[i]+1) will be along axis i.  \nshift_stepsint or tuple, optional \n\nThe step size for the shifts applied along axis, i, are:: range((0, max_shifts[i]+1, shift_steps[i])). If an integer is provided, the same step size is used for all axes.  \nnum_workersint or None, optional \n\nThe number of parallel threads to use during cycle spinning. If set to None, the full set of available cores are used.  \nmultichannelbool, optional \n\nWhether to treat the final axis as channels (no cycle shifts are performed over the channels axis).  \nfunc_kwdict, optional \n\nAdditional keyword arguments to supply to func.    Returns \n \navg_ynp.ndarray \n\nThe output of func(x, **func_kw) averaged over all combinations of the specified axis shifts.     Notes Cycle spinning was proposed as a way to approach shift-invariance via performing several circular shifts of a shift-variant transform [1]. For a n-level discrete wavelet transforms, one may wish to perform all shifts up to max_shifts = 2**n - 1. In practice, much of the benefit can often be realized with only a small number of shifts per axis. For transforms such as the blockwise discrete cosine transform, one may wish to evaluate shifts up to the block size used by the transform. References  \n1  \nR.R. Coifman and D.L. Donoho. \u201cTranslation-Invariant De-Noising\u201d. Wavelets and Statistics, Lecture Notes in Statistics, vol.103. Springer, New York, 1995, pp.125-150. DOI:10.1007/978-1-4612-2544-7_9   Examples >>> import skimage.data\n>>> from skimage import img_as_float\n>>> from skimage.restoration import denoise_wavelet, cycle_spin\n>>> img = img_as_float(skimage.data.camera())\n>>> sigma = 0.1\n>>> img = img + sigma * np.random.standard_normal(img.shape)\n>>> denoised = cycle_spin(img, func=denoise_wavelet,\n...                       max_shifts=3)\n \n denoise_bilateral  \nskimage.restoration.denoise_bilateral(image, win_size=None, sigma_color=None, sigma_spatial=1, bins=10000, mode='constant', cval=0, multichannel=False) [source]\n \nDenoise image using bilateral filter.  Parameters \n \nimagendarray, shape (M, N[, 3]) \n\nInput image, 2D grayscale or RGB.  \nwin_sizeint \n\nWindow size for filtering. If win_size is not specified, it is calculated as max(5, 2 * ceil(3 * sigma_spatial) + 1).  \nsigma_colorfloat \n\nStandard deviation for grayvalue/color distance (radiometric similarity). A larger value results in averaging of pixels with larger radiometric differences. Note, that the image will be converted using the img_as_float function and thus the standard deviation is in respect to the range [0, 1]. If the value is None the standard deviation of the image will be used.  \nsigma_spatialfloat \n\nStandard deviation for range distance. A larger value results in averaging of pixels with larger spatial differences.  \nbinsint \n\nNumber of discrete values for Gaussian weights of color filtering. A larger value results in improved accuracy.  \nmode{\u2018constant\u2019, \u2018edge\u2019, \u2018symmetric\u2019, \u2018reflect\u2019, \u2018wrap\u2019} \n\nHow to handle values outside the image borders. See numpy.pad for detail.  \ncvalstring \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.  \nmultichannelbool \n\nWhether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.    Returns \n \ndenoisedndarray \n\nDenoised image.     Notes This is an edge-preserving, denoising filter. It averages pixels based on their spatial closeness and radiometric similarity [1]. Spatial closeness is measured by the Gaussian function of the Euclidean distance between two pixels and a certain standard deviation (sigma_spatial). Radiometric similarity is measured by the Gaussian function of the Euclidean distance between two color values and a certain standard deviation (sigma_color). References  \n1  \nC. Tomasi and R. Manduchi. \u201cBilateral Filtering for Gray and Color Images.\u201d IEEE International Conference on Computer Vision (1998) 839-846. DOI:10.1109/ICCV.1998.710815   Examples >>> from skimage import data, img_as_float\n>>> astro = img_as_float(data.astronaut())\n>>> astro = astro[220:300, 220:320]\n>>> noisy = astro + 0.6 * astro.std() * np.random.random(astro.shape)\n>>> noisy = np.clip(noisy, 0, 1)\n>>> denoised = denoise_bilateral(noisy, sigma_color=0.05, sigma_spatial=15,\n...                              multichannel=True)\n \n Examples using skimage.restoration.denoise_bilateral\n \n  Rank filters   denoise_nl_means  \nskimage.restoration.denoise_nl_means(image, patch_size=7, patch_distance=11, h=0.1, multichannel=False, fast_mode=True, sigma=0.0, *, preserve_range=None) [source]\n \nPerform non-local means denoising on 2-D or 3-D grayscale images, and 2-D RGB images.  Parameters \n \nimage2D or 3D ndarray \n\nInput image to be denoised, which can be 2D or 3D, and grayscale or RGB (for 2D images only, see multichannel parameter).  \npatch_sizeint, optional \n\nSize of patches used for denoising.  \npatch_distanceint, optional \n\nMaximal distance in pixels where to search patches used for denoising.  \nhfloat, optional \n\nCut-off distance (in gray levels). The higher h, the more permissive one is in accepting patches. A higher h results in a smoother image, at the expense of blurring features. For a Gaussian noise of standard deviation sigma, a rule of thumb is to choose the value of h to be sigma of slightly less.  \nmultichannelbool, optional \n\nWhether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.  \nfast_modebool, optional \n\nIf True (default value), a fast version of the non-local means algorithm is used. If False, the original version of non-local means is used. See the Notes section for more details about the algorithms.  \nsigmafloat, optional \n\nThe standard deviation of the (Gaussian) noise. If provided, a more robust computation of patch weights is computed that takes the expected noise variance into account (see Notes below).  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html    Returns \n \nresultndarray \n\nDenoised image, of same shape as image.     Notes The non-local means algorithm is well suited for denoising images with specific textures. The principle of the algorithm is to average the value of a given pixel with values of other pixels in a limited neighbourhood, provided that the patches centered on the other pixels are similar enough to the patch centered on the pixel of interest. In the original version of the algorithm [1], corresponding to fast=False, the computational complexity is: image.size * patch_size ** image.ndim * patch_distance ** image.ndim\n Hence, changing the size of patches or their maximal distance has a strong effect on computing times, especially for 3-D images. However, the default behavior corresponds to fast_mode=True, for which another version of non-local means [2] is used, corresponding to a complexity of: image.size * patch_distance ** image.ndim\n The computing time depends only weakly on the patch size, thanks to the computation of the integral of patches distances for a given shift, that reduces the number of operations [1]. Therefore, this algorithm executes faster than the classic algorithm (fast_mode=False), at the expense of using twice as much memory. This implementation has been proven to be more efficient compared to other alternatives, see e.g. [3]. Compared to the classic algorithm, all pixels of a patch contribute to the distance to another patch with the same weight, no matter their distance to the center of the patch. This coarser computation of the distance can result in a slightly poorer denoising performance. Moreover, for small images (images with a linear size that is only a few times the patch size), the classic algorithm can be faster due to boundary effects. The image is padded using the reflect mode of skimage.util.pad before denoising. If the noise standard deviation, sigma, is provided a more robust computation of patch weights is used. Subtracting the known noise variance from the computed patch distances improves the estimates of patch similarity, giving a moderate improvement to denoising performance [4]. It was also mentioned as an option for the fast variant of the algorithm in [3]. When sigma is provided, a smaller h should typically be used to avoid oversmoothing. The optimal value for h depends on the image content and noise level, but a reasonable starting point is h = 0.8 * sigma when fast_mode is True, or h = 0.6 * sigma when fast_mode is False. References  \n1(1,2)  \nA. Buades, B. Coll, & J-M. Morel. A non-local algorithm for image denoising. In CVPR 2005, Vol. 2, pp. 60-65, IEEE. DOI:10.1109/CVPR.2005.38  \n2  \nJ. Darbon, A. Cunha, T.F. Chan, S. Osher, and G.J. Jensen, Fast nonlocal filtering applied to electron cryomicroscopy, in 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro, 2008, pp. 1331-1334. DOI:10.1109/ISBI.2008.4541250  \n3(1,2)  \nJacques Froment. Parameter-Free Fast Pixelwise Non-Local Means Denoising. Image Processing On Line, 2014, vol. 4, pp. 300-326. DOI:10.5201/ipol.2014.120  \n4  \nA. Buades, B. Coll, & J-M. Morel. Non-Local Means Denoising. Image Processing On Line, 2011, vol. 1, pp. 208-212. DOI:10.5201/ipol.2011.bcm_nlm   Examples >>> a = np.zeros((40, 40))\n>>> a[10:-10, 10:-10] = 1.\n>>> a += 0.3 * np.random.randn(*a.shape)\n>>> denoised_a = denoise_nl_means(a, 7, 5, 0.1)\n \n denoise_tv_bregman  \nskimage.restoration.denoise_tv_bregman(image, weight, max_iter=100, eps=0.001, isotropic=True, *, multichannel=False) [source]\n \nPerform total-variation denoising using split-Bregman optimization. Total-variation denoising (also know as total-variation regularization) tries to find an image with less total-variation under the constraint of being similar to the input image, which is controlled by the regularization parameter ([1], [2], [3], [4]).  Parameters \n \nimagendarray \n\nInput data to be denoised (converted using img_as_float`).  \nweightfloat \n\nDenoising weight. The smaller the weight, the more denoising (at the expense of less similarity to the input). The regularization parameter lambda is chosen as 2 * weight.  \nepsfloat, optional \n\nRelative difference of the value of the cost function that determines the stop criterion. The algorithm stops when: SUM((u(n) - u(n-1))**2) < eps\n  \nmax_iterint, optional \n\nMaximal number of iterations used for the optimization.  \nisotropicboolean, optional \n\nSwitch between isotropic and anisotropic TV denoising.  \nmultichannelbool, optional \n\nApply total-variation denoising separately for each channel. This option should be true for color images, otherwise the denoising is also applied in the channels dimension.    Returns \n \nundarray \n\nDenoised image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Total_variation_denoising  \n2  \nTom Goldstein and Stanley Osher, \u201cThe Split Bregman Method For L1 Regularized Problems\u201d, ftp://ftp.math.ucla.edu/pub/camreport/cam08-29.pdf  \n3  \nPascal Getreuer, \u201cRudin\u2013Osher\u2013Fatemi Total Variation Denoising using Split Bregman\u201d in Image Processing On Line on 2012\u201305\u201319, https://www.ipol.im/pub/art/2012/g-tvd/article_lr.pdf  \n4  \nhttps://web.math.ucsb.edu/~cgarcia/UGProjects/BregmanAlgorithms_JacquelineBush.pdf   \n denoise_tv_chambolle  \nskimage.restoration.denoise_tv_chambolle(image, weight=0.1, eps=0.0002, n_iter_max=200, multichannel=False) [source]\n \nPerform total-variation denoising on n-dimensional images.  Parameters \n \nimagendarray of ints, uints or floats \n\nInput data to be denoised. image can be of any numeric type, but it is cast into an ndarray of floats for the computation of the denoised image.  \nweightfloat, optional \n\nDenoising weight. The greater weight, the more denoising (at the expense of fidelity to input).  \nepsfloat, optional \n\nRelative difference of the value of the cost function that determines the stop criterion. The algorithm stops when: (E_(n-1) - E_n) < eps * E_0  \nn_iter_maxint, optional \n\nMaximal number of iterations used for the optimization.  \nmultichannelbool, optional \n\nApply total-variation denoising separately for each channel. This option should be true for color images, otherwise the denoising is also applied in the channels dimension.    Returns \n \noutndarray \n\nDenoised image.     Notes Make sure to set the multichannel parameter appropriately for color images. The principle of total variation denoising is explained in https://en.wikipedia.org/wiki/Total_variation_denoising The principle of total variation denoising is to minimize the total variation of the image, which can be roughly described as the integral of the norm of the image gradient. Total variation denoising tends to produce \u201ccartoon-like\u201d images, that is, piecewise-constant images. This code is an implementation of the algorithm of Rudin, Fatemi and Osher that was proposed by Chambolle in [1]. References  \n1  \nA. Chambolle, An algorithm for total variation minimization and applications, Journal of Mathematical Imaging and Vision, Springer, 2004, 20, 89-97.   Examples 2D example on astronaut image: >>> from skimage import color, data\n>>> img = color.rgb2gray(data.astronaut())[:50, :50]\n>>> img += 0.5 * img.std() * np.random.randn(*img.shape)\n>>> denoised_img = denoise_tv_chambolle(img, weight=60)\n 3D example on synthetic data: >>> x, y, z = np.ogrid[0:20, 0:20, 0:20]\n>>> mask = (x - 22)**2 + (y - 20)**2 + (z - 17)**2 < 8**2\n>>> mask = mask.astype(float)\n>>> mask += 0.2*np.random.randn(*mask.shape)\n>>> res = denoise_tv_chambolle(mask, weight=100)\n \n denoise_wavelet  \nskimage.restoration.denoise_wavelet(image, sigma=None, wavelet='db1', mode='soft', wavelet_levels=None, multichannel=False, convert2ycbcr=False, method='BayesShrink', rescale_sigma=True) [source]\n \nPerform wavelet denoising on an image.  Parameters \n \nimagendarray ([M[, N[, \u2026P]][, C]) of ints, uints or floats \n\nInput data to be denoised. image can be of any numeric type, but it is cast into an ndarray of floats for the computation of the denoised image.  \nsigmafloat or list, optional \n\nThe noise standard deviation used when computing the wavelet detail coefficient threshold(s). When None (default), the noise standard deviation is estimated via the method in [2].  \nwaveletstring, optional \n\nThe type of wavelet to perform and can be any of the options pywt.wavelist outputs. The default is \u2018db1\u2019. For example, wavelet can be any of {'db2', 'haar', 'sym9'} and many more.  \nmode{\u2018soft\u2019, \u2018hard\u2019}, optional \n\nAn optional argument to choose the type of denoising performed. It noted that choosing soft thresholding given additive noise finds the best approximation of the original image.  \nwavelet_levelsint or None, optional \n\nThe number of wavelet decomposition levels to use. The default is three less than the maximum number of possible decomposition levels.  \nmultichannelbool, optional \n\nApply wavelet denoising separately for each channel (where channels correspond to the final axis of the array).  \nconvert2ycbcrbool, optional \n\nIf True and multichannel True, do the wavelet denoising in the YCbCr colorspace instead of the RGB color space. This typically results in better performance for RGB images.  \nmethod{\u2018BayesShrink\u2019, \u2018VisuShrink\u2019}, optional \n\nThresholding method to be used. The currently supported methods are \u201cBayesShrink\u201d [1] and \u201cVisuShrink\u201d [2]. Defaults to \u201cBayesShrink\u201d.  \nrescale_sigmabool, optional \n\nIf False, no rescaling of the user-provided sigma will be performed. The default of True rescales sigma appropriately if the image is rescaled internally.  New in version 0.16: rescale_sigma was introduced in 0.16     Returns \n \noutndarray \n\nDenoised image.     Notes The wavelet domain is a sparse representation of the image, and can be thought of similarly to the frequency domain of the Fourier transform. Sparse representations have most values zero or near-zero and truly random noise is (usually) represented by many small values in the wavelet domain. Setting all values below some threshold to 0 reduces the noise in the image, but larger thresholds also decrease the detail present in the image. If the input is 3D, this function performs wavelet denoising on each color plane separately.  Changed in version 0.16: For floating point inputs, the original input range is maintained and there is no clipping applied to the output. Other input types will be converted to a floating point value in the range [-1, 1] or [0, 1] depending on the input image range. Unless rescale_sigma = False, any internal rescaling applied to the image will also be applied to sigma to maintain the same relative amplitude.  Many wavelet coefficient thresholding approaches have been proposed. By default, denoise_wavelet applies BayesShrink, which is an adaptive thresholding method that computes separate thresholds for each wavelet sub-band as described in [1]. If method == \"VisuShrink\", a single \u201cuniversal threshold\u201d is applied to all wavelet detail coefficients as described in [2]. This threshold is designed to remove all Gaussian noise at a given sigma with high probability, but tends to produce images that appear overly smooth. Although any of the wavelets from PyWavelets can be selected, the thresholding methods assume an orthogonal wavelet transform and may not choose the threshold appropriately for biorthogonal wavelets. Orthogonal wavelets are desirable because white noise in the input remains white noise in the subbands. Biorthogonal wavelets lead to colored noise in the subbands. Additionally, the orthogonal wavelets in PyWavelets are orthonormal so that noise variance in the subbands remains identical to the noise variance of the input. Example orthogonal wavelets are the Daubechies (e.g. \u2018db2\u2019) or symmlet (e.g. \u2018sym2\u2019) families. References  \n1(1,2)  \nChang, S. Grace, Bin Yu, and Martin Vetterli. \u201cAdaptive wavelet thresholding for image denoising and compression.\u201d Image Processing, IEEE Transactions on 9.9 (2000): 1532-1546. DOI:10.1109/83.862633  \n2(1,2,3)  \nD. L. Donoho and I. M. Johnstone. \u201cIdeal spatial adaptation by wavelet shrinkage.\u201d Biometrika 81.3 (1994): 425-455. DOI:10.1093/biomet/81.3.425   Examples >>> from skimage import color, data\n>>> img = img_as_float(data.astronaut())\n>>> img = color.rgb2gray(img)\n>>> img += 0.1 * np.random.randn(*img.shape)\n>>> img = np.clip(img, 0, 1)\n>>> denoised_img = denoise_wavelet(img, sigma=0.1, rescale_sigma=True)\n \n ellipsoid_kernel  \nskimage.restoration.ellipsoid_kernel(shape, intensity) [source]\n \nCreate an ellipoid kernel for restoration.rolling_ball.  Parameters \n \nshapearraylike \n\nLength of the principal axis of the ellipsoid (excluding the intensity axis). The kernel needs to have the same dimensionality as the image it will be applied to.  \nintensityint \n\nLength of the intensity axis of the ellipsoid.    Returns \n \nkernelndarray \n\nThe kernel containing the surface intensity of the top half of the ellipsoid.      See also  \nrolling_ball\n\n  \n Examples using skimage.restoration.ellipsoid_kernel\n \n  Use rolling-ball algorithm for estimating background intensity   estimate_sigma  \nskimage.restoration.estimate_sigma(image, average_sigmas=False, multichannel=False) [source]\n \nRobust wavelet-based estimator of the (Gaussian) noise standard deviation.  Parameters \n \nimagendarray \n\nImage for which to estimate the noise standard deviation.  \naverage_sigmasbool, optional \n\nIf true, average the channel estimates of sigma. Otherwise return a list of sigmas corresponding to each channel.  \nmultichannelbool \n\nEstimate sigma separately for each channel.    Returns \n \nsigmafloat or list \n\nEstimated noise standard deviation(s). If multichannel is True and average_sigmas is False, a separate noise estimate for each channel is returned. Otherwise, the average of the individual channel estimates is returned.     Notes This function assumes the noise follows a Gaussian distribution. The estimation algorithm is based on the median absolute deviation of the wavelet detail coefficients as described in section 4.2 of [1]. References  \n1  \nD. L. Donoho and I. M. Johnstone. \u201cIdeal spatial adaptation by wavelet shrinkage.\u201d Biometrika 81.3 (1994): 425-455. DOI:10.1093/biomet/81.3.425   Examples >>> import skimage.data\n>>> from skimage import img_as_float\n>>> img = img_as_float(skimage.data.camera())\n>>> sigma = 0.1\n>>> img = img + sigma * np.random.standard_normal(img.shape)\n>>> sigma_hat = estimate_sigma(img, multichannel=False)\n \n inpaint_biharmonic  \nskimage.restoration.inpaint_biharmonic(image, mask, multichannel=False) [source]\n \nInpaint masked points in image with biharmonic equations.  Parameters \n \nimage(M[, N[, \u2026, P]][, C]) ndarray \n\nInput image.  \nmask(M[, N[, \u2026, P]]) ndarray \n\nArray of pixels to be inpainted. Have to be the same shape as one of the \u2018image\u2019 channels. Unknown pixels have to be represented with 1, known pixels - with 0.  \nmultichannelboolean, optional \n\nIf True, the last image dimension is considered as a color channel, otherwise as spatial.    Returns \n \nout(M[, N[, \u2026, P]][, C]) ndarray \n\nInput image with masked pixels inpainted.     References  \n1  \nN.S.Hoang, S.B.Damelin, \u201cOn surface completion and image inpainting by biharmonic functions: numerical aspects\u201d, arXiv:1707.06567  \n2  \nC. K. Chui and H. N. Mhaskar, MRA Contextual-Recovery Extension of Smooth Functions on Manifolds, Appl. and Comp. Harmonic Anal., 28 (2010), 104-113, DOI:10.1016/j.acha.2009.04.004   Examples >>> img = np.tile(np.square(np.linspace(0, 1, 5)), (5, 1))\n>>> mask = np.zeros_like(img)\n>>> mask[2, 2:] = 1\n>>> mask[1, 3:] = 1\n>>> mask[0, 4:] = 1\n>>> out = inpaint_biharmonic(img, mask)\n \n richardson_lucy  \nskimage.restoration.richardson_lucy(image, psf, iterations=50, clip=True, filter_epsilon=None) [source]\n \nRichardson-Lucy deconvolution.  Parameters \n \nimagendarray \n\nInput degraded image (can be N dimensional).  \npsfndarray \n\nThe point spread function.  \niterationsint, optional \n\nNumber of iterations. This parameter plays the role of regularisation.  \nclipboolean, optional \n\nTrue by default. If true, pixel value of the result above 1 or under -1 are thresholded for skimage pipeline compatibility.  filter_epsilon: float, optional\n\nValue below which intermediate results become 0 to avoid division by small numbers.    Returns \n \nim_deconvndarray \n\nThe deconvolved image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Richardson%E2%80%93Lucy_deconvolution   Examples >>> from skimage import img_as_float, data, restoration\n>>> camera = img_as_float(data.camera())\n>>> from scipy.signal import convolve2d\n>>> psf = np.ones((5, 5)) / 25\n>>> camera = convolve2d(camera, psf, 'same')\n>>> camera += 0.1 * camera.std() * np.random.standard_normal(camera.shape)\n>>> deconvolved = restoration.richardson_lucy(camera, psf, 5)\n \n rolling_ball  \nskimage.restoration.rolling_ball(image, *, radius=100, kernel=None, nansafe=False, num_threads=None) [source]\n \nEstimate background intensity by rolling/translating a kernel. This rolling ball algorithm estimates background intensity for a ndimage in case of uneven exposure. It is a generalization of the frequently used rolling ball algorithm [1].  Parameters \n \nimagendarray \n\nThe image to be filtered.  \nradiusint, optional \n\nRadius of a ball shaped kernel to be rolled/translated in the image. Used if kernel = None.  \nkernelndarray, optional \n\nThe kernel to be rolled/translated in the image. It must have the same number of dimensions as image. Kernel is filled with the intensity of the kernel at that position.  nansafe: bool, optional\n\nIf False (default) assumes that none of the values in image are np.nan, and uses a faster implementation.  num_threads: int, optional\n\nThe maximum number of threads to use. If None use the OpenMP default value; typically equal to the maximum number of virtual cores. Note: This is an upper limit to the number of threads. The exact number is determined by the system\u2019s OpenMP library.    Returns \n \nbackgroundndarray \n\nThe estimated background of the image.     Notes For the pixel that has its background intensity estimated (without loss of generality at center) the rolling ball method centers kernel under it and raises the kernel until the surface touches the image umbra at some pos=(y,x). The background intensity is then estimated using the image intensity at that position (image[pos]) plus the difference of kernel[center] - kernel[pos]. This algorithm assumes that dark pixels correspond to the background. If you have a bright background, invert the image before passing it to the function, e.g., using utils.invert. See the gallery example for details. This algorithm is sensitive to noise (in particular salt-and-pepper noise). If this is a problem in your image, you can apply mild gaussian smoothing before passing the image to this function. References  \n1  \nSternberg, Stanley R. \u201cBiomedical image processing.\u201d Computer 1 (1983): 22-34. DOI:10.1109/MC.1983.1654163   Examples >>> import numpy as np\n>>> from skimage import data\n>>> from skimage.restoration import rolling_ball\n>>> image = data.coins()\n>>> background = rolling_ball(data.coins())\n>>> filtered_image = image - background\n >>> import numpy as np\n>>> from skimage import data\n>>> from skimage.restoration import rolling_ball, ellipsoid_kernel\n>>> image = data.coins()\n>>> kernel = ellipsoid_kernel((101, 101), 75)\n>>> background = rolling_ball(data.coins(), kernel=kernel)\n>>> filtered_image = image - background\n \n Examples using skimage.restoration.rolling_ball\n \n  Use rolling-ball algorithm for estimating background intensity   unsupervised_wiener  \nskimage.restoration.unsupervised_wiener(image, psf, reg=None, user_params=None, is_real=True, clip=True) [source]\n \nUnsupervised Wiener-Hunt deconvolution. Return the deconvolution with a Wiener-Hunt approach, where the hyperparameters are automatically estimated. The algorithm is a stochastic iterative process (Gibbs sampler) described in the reference below. See also wiener function.  Parameters \n \nimage(M, N) ndarray \n\nThe input degraded image.  \npsfndarray \n\nThe impulse response (input image\u2019s space) or the transfer function (Fourier space). Both are accepted. The transfer function is automatically recognized as being complex (np.iscomplexobj(psf)).  \nregndarray, optional \n\nThe regularisation operator. The Laplacian by default. It can be an impulse response or a transfer function, as for the psf.  \nuser_paramsdict, optional \n\nDictionary of parameters for the Gibbs sampler. See below.  \nclipboolean, optional \n\nTrue by default. If true, pixel values of the result above 1 or under -1 are thresholded for skimage pipeline compatibility.    Returns \n \nx_postmean(M, N) ndarray \n\nThe deconvolved image (the posterior mean).  \nchainsdict \n\nThe keys noise and prior contain the chain list of noise and prior precision respectively.    Other Parameters \n The keys of ``user_params`` are:\n\nthresholdfloat \n\nThe stopping criterion: the norm of the difference between to successive approximated solution (empirical mean of object samples, see Notes section). 1e-4 by default.  \nburninint \n\nThe number of sample to ignore to start computation of the mean. 15 by default.  \nmin_iterint \n\nThe minimum number of iterations. 30 by default.  \nmax_iterint \n\nThe maximum number of iterations if threshold is not satisfied. 200 by default.  \ncallbackcallable (None by default) \n\nA user provided callable to which is passed, if the function exists, the current image sample for whatever purpose. The user can store the sample, or compute other moments than the mean. It has no influence on the algorithm execution and is only for inspection.     Notes The estimated image is design as the posterior mean of a probability law (from a Bayesian analysis). The mean is defined as a sum over all the possible images weighted by their respective probability. Given the size of the problem, the exact sum is not tractable. This algorithm use of MCMC to draw image under the posterior law. The practical idea is to only draw highly probable images since they have the biggest contribution to the mean. At the opposite, the less probable images are drawn less often since their contribution is low. Finally the empirical mean of these samples give us an estimation of the mean, and an exact computation with an infinite sample set. References  \n1  \nFran\u00e7ois Orieux, Jean-Fran\u00e7ois Giovannelli, and Thomas Rodet, \u201cBayesian estimation of regularization and point spread function parameters for Wiener-Hunt deconvolution\u201d, J. Opt. Soc. Am. A 27, 1593-1607 (2010) https://www.osapublishing.org/josaa/abstract.cfm?URI=josaa-27-7-1593 http://research.orieux.fr/files/papers/OGR-JOSA10.pdf   Examples >>> from skimage import color, data, restoration\n>>> img = color.rgb2gray(data.astronaut())\n>>> from scipy.signal import convolve2d\n>>> psf = np.ones((5, 5)) / 25\n>>> img = convolve2d(img, psf, 'same')\n>>> img += 0.1 * img.std() * np.random.standard_normal(img.shape)\n>>> deconvolved_img = restoration.unsupervised_wiener(img, psf)\n \n unwrap_phase  \nskimage.restoration.unwrap_phase(image, wrap_around=False, seed=None) [source]\n \nRecover the original from a wrapped phase image. From an image wrapped to lie in the interval [-pi, pi), recover the original, unwrapped image.  Parameters \n \nimage1D, 2D or 3D ndarray of floats, optionally a masked array \n\nThe values should be in the range [-pi, pi). If a masked array is provided, the masked entries will not be changed, and their values will not be used to guide the unwrapping of neighboring, unmasked values. Masked 1D arrays are not allowed, and will raise a ValueError.  \nwrap_aroundbool or sequence of bool, optional \n\nWhen an element of the sequence is True, the unwrapping process will regard the edges along the corresponding axis of the image to be connected and use this connectivity to guide the phase unwrapping process. If only a single boolean is given, it will apply to all axes. Wrap around is not supported for 1D arrays.  \nseedint, optional \n\nUnwrapping 2D or 3D images uses random initialization. This sets the seed of the PRNG to achieve deterministic behavior.    Returns \n \nimage_unwrappedarray_like, double \n\nUnwrapped image of the same shape as the input. If the input image was a masked array, the mask will be preserved.    Raises \n ValueError\n\nIf called with a masked 1D array or called with a 1D array and wrap_around=True.     References  \n1  \nMiguel Arevallilo Herraez, David R. Burton, Michael J. Lalor, and Munther A. Gdeisat, \u201cFast two-dimensional phase-unwrapping algorithm based on sorting by reliability following a noncontinuous path\u201d, Journal Applied Optics, Vol. 41, No. 35 (2002) 7437,  \n2  \nAbdul-Rahman, H., Gdeisat, M., Burton, D., & Lalor, M., \u201cFast three-dimensional phase-unwrapping algorithm based on sorting by reliability following a non-continuous path. In W. Osten, C. Gorecki, & E. L. Novak (Eds.), Optical Metrology (2005) 32\u201340, International Society for Optics and Photonics.   Examples >>> c0, c1 = np.ogrid[-1:1:128j, -1:1:128j]\n>>> image = 12 * np.pi * np.exp(-(c0**2 + c1**2))\n>>> image_wrapped = np.angle(np.exp(1j * image))\n>>> image_unwrapped = unwrap_phase(image_wrapped)\n>>> np.std(image_unwrapped - image) < 1e-6   # A constant offset is normal\nTrue\n \n Examples using skimage.restoration.unwrap_phase\n \n  Phase Unwrapping   wiener  \nskimage.restoration.wiener(image, psf, balance, reg=None, is_real=True, clip=True) [source]\n \nWiener-Hunt deconvolution Return the deconvolution with a Wiener-Hunt approach (i.e. with Fourier diagonalisation).  Parameters \n \nimage(M, N) ndarray \n\nInput degraded image  \npsfndarray \n\nPoint Spread Function. This is assumed to be the impulse response (input image space) if the data-type is real, or the transfer function (Fourier space) if the data-type is complex. There is no constraints on the shape of the impulse response. The transfer function must be of shape (M, N) if is_real is True, (M, N // 2 + 1) otherwise (see np.fft.rfftn).  \nbalancefloat \n\nThe regularisation parameter value that tunes the balance between the data adequacy that improve frequency restoration and the prior adequacy that reduce frequency restoration (to avoid noise artifacts).  \nregndarray, optional \n\nThe regularisation operator. The Laplacian by default. It can be an impulse response or a transfer function, as for the psf. Shape constraint is the same as for the psf parameter.  \nis_realboolean, optional \n\nTrue by default. Specify if psf and reg are provided with hermitian hypothesis, that is only half of the frequency plane is provided (due to the redundancy of Fourier transform of real signal). It\u2019s apply only if psf and/or reg are provided as transfer function. For the hermitian property see uft module or np.fft.rfftn.  \nclipboolean, optional \n\nTrue by default. If True, pixel values of the result above 1 or under -1 are thresholded for skimage pipeline compatibility.    Returns \n \nim_deconv(M, N) ndarray \n\nThe deconvolved image.     Notes This function applies the Wiener filter to a noisy and degraded image by an impulse response (or PSF). If the data model is  \\[y = Hx + n\\] where \\(n\\) is noise, \\(H\\) the PSF and \\(x\\) the unknown original image, the Wiener filter is  \\[\\hat x = F^\\dagger (|\\Lambda_H|^2 + \\lambda |\\Lambda_D|^2) \\Lambda_H^\\dagger F y\\] where \\(F\\) and \\(F^\\dagger\\) are the Fourier and inverse Fourier transforms respectively, \\(\\Lambda_H\\) the transfer function (or the Fourier transform of the PSF, see [Hunt] below) and \\(\\Lambda_D\\) the filter to penalize the restored image frequencies (Laplacian by default, that is penalization of high frequency). The parameter \\(\\lambda\\) tunes the balance between the data (that tends to increase high frequency, even those coming from noise), and the regularization. These methods are then specific to a prior model. Consequently, the application or the true image nature must corresponds to the prior model. By default, the prior model (Laplacian) introduce image smoothness or pixel correlation. It can also be interpreted as high-frequency penalization to compensate the instability of the solution with respect to the data (sometimes called noise amplification or \u201cexplosive\u201d solution). Finally, the use of Fourier space implies a circulant property of \\(H\\), see [Hunt]. References  \n1  \nFran\u00e7ois Orieux, Jean-Fran\u00e7ois Giovannelli, and Thomas Rodet, \u201cBayesian estimation of regularization and point spread function parameters for Wiener-Hunt deconvolution\u201d, J. Opt. Soc. Am. A 27, 1593-1607 (2010) https://www.osapublishing.org/josaa/abstract.cfm?URI=josaa-27-7-1593 http://research.orieux.fr/files/papers/OGR-JOSA10.pdf  \n2  \nB. R. Hunt \u201cA matrix theory proof of the discrete convolution theorem\u201d, IEEE Trans. on Audio and Electroacoustics, vol. au-19, no. 4, pp. 285-288, dec. 1971   Examples >>> from skimage import color, data, restoration\n>>> img = color.rgb2gray(data.astronaut())\n>>> from scipy.signal import convolve2d\n>>> psf = np.ones((5, 5)) / 25\n>>> img = convolve2d(img, psf, 'same')\n>>> img += 0.1 * img.std() * np.random.standard_normal(img.shape)\n>>> deconvolved_img = restoration.wiener(img, psf, 1100)\n \n\n"}, {"name": "restoration.ball_kernel()", "path": "api/skimage.restoration#skimage.restoration.ball_kernel", "type": "restoration", "text": " \nskimage.restoration.ball_kernel(radius, ndim) [source]\n \nCreate a ball kernel for restoration.rolling_ball.  Parameters \n \nradiusint \n\nRadius of the ball.  \nndimint \n\nNumber of dimensions of the ball. ndim should match the dimensionality of the image the kernel will be applied to.    Returns \n \nkernelndarray \n\nThe kernel containing the surface intensity of the top half of the ellipsoid.      See also  \nrolling_ball\n\n  \n"}, {"name": "restoration.calibrate_denoiser()", "path": "api/skimage.restoration#skimage.restoration.calibrate_denoiser", "type": "restoration", "text": " \nskimage.restoration.calibrate_denoiser(image, denoise_function, denoise_parameters, *, stride=4, approximate_loss=True, extra_output=False) [source]\n \nCalibrate a denoising function and return optimal J-invariant version. The returned function is partially evaluated with optimal parameter values set for denoising the input image.  Parameters \n \nimagendarray \n\nInput data to be denoised (converted using img_as_float).  \ndenoise_functionfunction \n\nDenoising function to be calibrated.  \ndenoise_parametersdict of list \n\nRanges of parameters for denoise_function to be calibrated over.  \nstrideint, optional \n\nStride used in masking procedure that converts denoise_function to J-invariance.  \napproximate_lossbool, optional \n\nWhether to approximate the self-supervised loss used to evaluate the denoiser by only computing it on one masked version of the image. If False, the runtime will be a factor of stride**image.ndim longer.  \nextra_outputbool, optional \n\nIf True, return parameters and losses in addition to the calibrated denoising function    Returns \n \nbest_denoise_functionfunction \n\nThe optimal J-invariant version of denoise_function.  \nIf extra_output is True, the following tuple is also returned: \n\n(parameters_tested, losses)tuple (list of dict, list of int) \n\nList of parameters tested for denoise_function, as a dictionary of kwargs Self-supervised loss for each set of parameters in parameters_tested.     Notes The calibration procedure uses a self-supervised mean-square-error loss to evaluate the performance of J-invariant versions of denoise_function. The minimizer of the self-supervised loss is also the minimizer of the ground-truth loss (i.e., the true MSE error) [1]. The returned function can be used on the original noisy image, or other images with similar characteristics.  \nIncreasing the stride increases the performance of best_denoise_function \n\nat the expense of increasing its runtime. It has no effect on the runtime of the calibration.   References  \n1  \nJ. Batson & L. Royer. Noise2Self: Blind Denoising by Self-Supervision, International Conference on Machine Learning, p. 524-533 (2019).   Examples >>> from skimage import color, data\n>>> from skimage.restoration import denoise_wavelet\n>>> import numpy as np\n>>> img = color.rgb2gray(data.astronaut()[:50, :50])\n>>> noisy = img + 0.5 * img.std() * np.random.randn(*img.shape)\n>>> parameters = {'sigma': np.arange(0.1, 0.4, 0.02)}\n>>> denoising_function = calibrate_denoiser(noisy, denoise_wavelet,\n...                                         denoise_parameters=parameters)\n>>> denoised_img = denoising_function(img)\n \n"}, {"name": "restoration.cycle_spin()", "path": "api/skimage.restoration#skimage.restoration.cycle_spin", "type": "restoration", "text": " \nskimage.restoration.cycle_spin(x, func, max_shifts, shift_steps=1, num_workers=None, multichannel=False, func_kw={}) [source]\n \nCycle spinning (repeatedly apply func to shifted versions of x).  Parameters \n \nxarray-like \n\nData for input to func.  \nfuncfunction \n\nA function to apply to circularly shifted versions of x. Should take x as its first argument. Any additional arguments can be supplied via func_kw.  \nmax_shiftsint or tuple \n\nIf an integer, shifts in range(0, max_shifts+1) will be used along each axis of x. If a tuple, range(0, max_shifts[i]+1) will be along axis i.  \nshift_stepsint or tuple, optional \n\nThe step size for the shifts applied along axis, i, are:: range((0, max_shifts[i]+1, shift_steps[i])). If an integer is provided, the same step size is used for all axes.  \nnum_workersint or None, optional \n\nThe number of parallel threads to use during cycle spinning. If set to None, the full set of available cores are used.  \nmultichannelbool, optional \n\nWhether to treat the final axis as channels (no cycle shifts are performed over the channels axis).  \nfunc_kwdict, optional \n\nAdditional keyword arguments to supply to func.    Returns \n \navg_ynp.ndarray \n\nThe output of func(x, **func_kw) averaged over all combinations of the specified axis shifts.     Notes Cycle spinning was proposed as a way to approach shift-invariance via performing several circular shifts of a shift-variant transform [1]. For a n-level discrete wavelet transforms, one may wish to perform all shifts up to max_shifts = 2**n - 1. In practice, much of the benefit can often be realized with only a small number of shifts per axis. For transforms such as the blockwise discrete cosine transform, one may wish to evaluate shifts up to the block size used by the transform. References  \n1  \nR.R. Coifman and D.L. Donoho. \u201cTranslation-Invariant De-Noising\u201d. Wavelets and Statistics, Lecture Notes in Statistics, vol.103. Springer, New York, 1995, pp.125-150. DOI:10.1007/978-1-4612-2544-7_9   Examples >>> import skimage.data\n>>> from skimage import img_as_float\n>>> from skimage.restoration import denoise_wavelet, cycle_spin\n>>> img = img_as_float(skimage.data.camera())\n>>> sigma = 0.1\n>>> img = img + sigma * np.random.standard_normal(img.shape)\n>>> denoised = cycle_spin(img, func=denoise_wavelet,\n...                       max_shifts=3)\n \n"}, {"name": "restoration.denoise_bilateral()", "path": "api/skimage.restoration#skimage.restoration.denoise_bilateral", "type": "restoration", "text": " \nskimage.restoration.denoise_bilateral(image, win_size=None, sigma_color=None, sigma_spatial=1, bins=10000, mode='constant', cval=0, multichannel=False) [source]\n \nDenoise image using bilateral filter.  Parameters \n \nimagendarray, shape (M, N[, 3]) \n\nInput image, 2D grayscale or RGB.  \nwin_sizeint \n\nWindow size for filtering. If win_size is not specified, it is calculated as max(5, 2 * ceil(3 * sigma_spatial) + 1).  \nsigma_colorfloat \n\nStandard deviation for grayvalue/color distance (radiometric similarity). A larger value results in averaging of pixels with larger radiometric differences. Note, that the image will be converted using the img_as_float function and thus the standard deviation is in respect to the range [0, 1]. If the value is None the standard deviation of the image will be used.  \nsigma_spatialfloat \n\nStandard deviation for range distance. A larger value results in averaging of pixels with larger spatial differences.  \nbinsint \n\nNumber of discrete values for Gaussian weights of color filtering. A larger value results in improved accuracy.  \nmode{\u2018constant\u2019, \u2018edge\u2019, \u2018symmetric\u2019, \u2018reflect\u2019, \u2018wrap\u2019} \n\nHow to handle values outside the image borders. See numpy.pad for detail.  \ncvalstring \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.  \nmultichannelbool \n\nWhether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.    Returns \n \ndenoisedndarray \n\nDenoised image.     Notes This is an edge-preserving, denoising filter. It averages pixels based on their spatial closeness and radiometric similarity [1]. Spatial closeness is measured by the Gaussian function of the Euclidean distance between two pixels and a certain standard deviation (sigma_spatial). Radiometric similarity is measured by the Gaussian function of the Euclidean distance between two color values and a certain standard deviation (sigma_color). References  \n1  \nC. Tomasi and R. Manduchi. \u201cBilateral Filtering for Gray and Color Images.\u201d IEEE International Conference on Computer Vision (1998) 839-846. DOI:10.1109/ICCV.1998.710815   Examples >>> from skimage import data, img_as_float\n>>> astro = img_as_float(data.astronaut())\n>>> astro = astro[220:300, 220:320]\n>>> noisy = astro + 0.6 * astro.std() * np.random.random(astro.shape)\n>>> noisy = np.clip(noisy, 0, 1)\n>>> denoised = denoise_bilateral(noisy, sigma_color=0.05, sigma_spatial=15,\n...                              multichannel=True)\n \n"}, {"name": "restoration.denoise_nl_means()", "path": "api/skimage.restoration#skimage.restoration.denoise_nl_means", "type": "restoration", "text": " \nskimage.restoration.denoise_nl_means(image, patch_size=7, patch_distance=11, h=0.1, multichannel=False, fast_mode=True, sigma=0.0, *, preserve_range=None) [source]\n \nPerform non-local means denoising on 2-D or 3-D grayscale images, and 2-D RGB images.  Parameters \n \nimage2D or 3D ndarray \n\nInput image to be denoised, which can be 2D or 3D, and grayscale or RGB (for 2D images only, see multichannel parameter).  \npatch_sizeint, optional \n\nSize of patches used for denoising.  \npatch_distanceint, optional \n\nMaximal distance in pixels where to search patches used for denoising.  \nhfloat, optional \n\nCut-off distance (in gray levels). The higher h, the more permissive one is in accepting patches. A higher h results in a smoother image, at the expense of blurring features. For a Gaussian noise of standard deviation sigma, a rule of thumb is to choose the value of h to be sigma of slightly less.  \nmultichannelbool, optional \n\nWhether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.  \nfast_modebool, optional \n\nIf True (default value), a fast version of the non-local means algorithm is used. If False, the original version of non-local means is used. See the Notes section for more details about the algorithms.  \nsigmafloat, optional \n\nThe standard deviation of the (Gaussian) noise. If provided, a more robust computation of patch weights is computed that takes the expected noise variance into account (see Notes below).  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html    Returns \n \nresultndarray \n\nDenoised image, of same shape as image.     Notes The non-local means algorithm is well suited for denoising images with specific textures. The principle of the algorithm is to average the value of a given pixel with values of other pixels in a limited neighbourhood, provided that the patches centered on the other pixels are similar enough to the patch centered on the pixel of interest. In the original version of the algorithm [1], corresponding to fast=False, the computational complexity is: image.size * patch_size ** image.ndim * patch_distance ** image.ndim\n Hence, changing the size of patches or their maximal distance has a strong effect on computing times, especially for 3-D images. However, the default behavior corresponds to fast_mode=True, for which another version of non-local means [2] is used, corresponding to a complexity of: image.size * patch_distance ** image.ndim\n The computing time depends only weakly on the patch size, thanks to the computation of the integral of patches distances for a given shift, that reduces the number of operations [1]. Therefore, this algorithm executes faster than the classic algorithm (fast_mode=False), at the expense of using twice as much memory. This implementation has been proven to be more efficient compared to other alternatives, see e.g. [3]. Compared to the classic algorithm, all pixels of a patch contribute to the distance to another patch with the same weight, no matter their distance to the center of the patch. This coarser computation of the distance can result in a slightly poorer denoising performance. Moreover, for small images (images with a linear size that is only a few times the patch size), the classic algorithm can be faster due to boundary effects. The image is padded using the reflect mode of skimage.util.pad before denoising. If the noise standard deviation, sigma, is provided a more robust computation of patch weights is used. Subtracting the known noise variance from the computed patch distances improves the estimates of patch similarity, giving a moderate improvement to denoising performance [4]. It was also mentioned as an option for the fast variant of the algorithm in [3]. When sigma is provided, a smaller h should typically be used to avoid oversmoothing. The optimal value for h depends on the image content and noise level, but a reasonable starting point is h = 0.8 * sigma when fast_mode is True, or h = 0.6 * sigma when fast_mode is False. References  \n1(1,2)  \nA. Buades, B. Coll, & J-M. Morel. A non-local algorithm for image denoising. In CVPR 2005, Vol. 2, pp. 60-65, IEEE. DOI:10.1109/CVPR.2005.38  \n2  \nJ. Darbon, A. Cunha, T.F. Chan, S. Osher, and G.J. Jensen, Fast nonlocal filtering applied to electron cryomicroscopy, in 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro, 2008, pp. 1331-1334. DOI:10.1109/ISBI.2008.4541250  \n3(1,2)  \nJacques Froment. Parameter-Free Fast Pixelwise Non-Local Means Denoising. Image Processing On Line, 2014, vol. 4, pp. 300-326. DOI:10.5201/ipol.2014.120  \n4  \nA. Buades, B. Coll, & J-M. Morel. Non-Local Means Denoising. Image Processing On Line, 2011, vol. 1, pp. 208-212. DOI:10.5201/ipol.2011.bcm_nlm   Examples >>> a = np.zeros((40, 40))\n>>> a[10:-10, 10:-10] = 1.\n>>> a += 0.3 * np.random.randn(*a.shape)\n>>> denoised_a = denoise_nl_means(a, 7, 5, 0.1)\n \n"}, {"name": "restoration.denoise_tv_bregman()", "path": "api/skimage.restoration#skimage.restoration.denoise_tv_bregman", "type": "restoration", "text": " \nskimage.restoration.denoise_tv_bregman(image, weight, max_iter=100, eps=0.001, isotropic=True, *, multichannel=False) [source]\n \nPerform total-variation denoising using split-Bregman optimization. Total-variation denoising (also know as total-variation regularization) tries to find an image with less total-variation under the constraint of being similar to the input image, which is controlled by the regularization parameter ([1], [2], [3], [4]).  Parameters \n \nimagendarray \n\nInput data to be denoised (converted using img_as_float`).  \nweightfloat \n\nDenoising weight. The smaller the weight, the more denoising (at the expense of less similarity to the input). The regularization parameter lambda is chosen as 2 * weight.  \nepsfloat, optional \n\nRelative difference of the value of the cost function that determines the stop criterion. The algorithm stops when: SUM((u(n) - u(n-1))**2) < eps\n  \nmax_iterint, optional \n\nMaximal number of iterations used for the optimization.  \nisotropicboolean, optional \n\nSwitch between isotropic and anisotropic TV denoising.  \nmultichannelbool, optional \n\nApply total-variation denoising separately for each channel. This option should be true for color images, otherwise the denoising is also applied in the channels dimension.    Returns \n \nundarray \n\nDenoised image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Total_variation_denoising  \n2  \nTom Goldstein and Stanley Osher, \u201cThe Split Bregman Method For L1 Regularized Problems\u201d, ftp://ftp.math.ucla.edu/pub/camreport/cam08-29.pdf  \n3  \nPascal Getreuer, \u201cRudin\u2013Osher\u2013Fatemi Total Variation Denoising using Split Bregman\u201d in Image Processing On Line on 2012\u201305\u201319, https://www.ipol.im/pub/art/2012/g-tvd/article_lr.pdf  \n4  \nhttps://web.math.ucsb.edu/~cgarcia/UGProjects/BregmanAlgorithms_JacquelineBush.pdf   \n"}, {"name": "restoration.denoise_tv_chambolle()", "path": "api/skimage.restoration#skimage.restoration.denoise_tv_chambolle", "type": "restoration", "text": " \nskimage.restoration.denoise_tv_chambolle(image, weight=0.1, eps=0.0002, n_iter_max=200, multichannel=False) [source]\n \nPerform total-variation denoising on n-dimensional images.  Parameters \n \nimagendarray of ints, uints or floats \n\nInput data to be denoised. image can be of any numeric type, but it is cast into an ndarray of floats for the computation of the denoised image.  \nweightfloat, optional \n\nDenoising weight. The greater weight, the more denoising (at the expense of fidelity to input).  \nepsfloat, optional \n\nRelative difference of the value of the cost function that determines the stop criterion. The algorithm stops when: (E_(n-1) - E_n) < eps * E_0  \nn_iter_maxint, optional \n\nMaximal number of iterations used for the optimization.  \nmultichannelbool, optional \n\nApply total-variation denoising separately for each channel. This option should be true for color images, otherwise the denoising is also applied in the channels dimension.    Returns \n \noutndarray \n\nDenoised image.     Notes Make sure to set the multichannel parameter appropriately for color images. The principle of total variation denoising is explained in https://en.wikipedia.org/wiki/Total_variation_denoising The principle of total variation denoising is to minimize the total variation of the image, which can be roughly described as the integral of the norm of the image gradient. Total variation denoising tends to produce \u201ccartoon-like\u201d images, that is, piecewise-constant images. This code is an implementation of the algorithm of Rudin, Fatemi and Osher that was proposed by Chambolle in [1]. References  \n1  \nA. Chambolle, An algorithm for total variation minimization and applications, Journal of Mathematical Imaging and Vision, Springer, 2004, 20, 89-97.   Examples 2D example on astronaut image: >>> from skimage import color, data\n>>> img = color.rgb2gray(data.astronaut())[:50, :50]\n>>> img += 0.5 * img.std() * np.random.randn(*img.shape)\n>>> denoised_img = denoise_tv_chambolle(img, weight=60)\n 3D example on synthetic data: >>> x, y, z = np.ogrid[0:20, 0:20, 0:20]\n>>> mask = (x - 22)**2 + (y - 20)**2 + (z - 17)**2 < 8**2\n>>> mask = mask.astype(float)\n>>> mask += 0.2*np.random.randn(*mask.shape)\n>>> res = denoise_tv_chambolle(mask, weight=100)\n \n"}, {"name": "restoration.denoise_wavelet()", "path": "api/skimage.restoration#skimage.restoration.denoise_wavelet", "type": "restoration", "text": " \nskimage.restoration.denoise_wavelet(image, sigma=None, wavelet='db1', mode='soft', wavelet_levels=None, multichannel=False, convert2ycbcr=False, method='BayesShrink', rescale_sigma=True) [source]\n \nPerform wavelet denoising on an image.  Parameters \n \nimagendarray ([M[, N[, \u2026P]][, C]) of ints, uints or floats \n\nInput data to be denoised. image can be of any numeric type, but it is cast into an ndarray of floats for the computation of the denoised image.  \nsigmafloat or list, optional \n\nThe noise standard deviation used when computing the wavelet detail coefficient threshold(s). When None (default), the noise standard deviation is estimated via the method in [2].  \nwaveletstring, optional \n\nThe type of wavelet to perform and can be any of the options pywt.wavelist outputs. The default is \u2018db1\u2019. For example, wavelet can be any of {'db2', 'haar', 'sym9'} and many more.  \nmode{\u2018soft\u2019, \u2018hard\u2019}, optional \n\nAn optional argument to choose the type of denoising performed. It noted that choosing soft thresholding given additive noise finds the best approximation of the original image.  \nwavelet_levelsint or None, optional \n\nThe number of wavelet decomposition levels to use. The default is three less than the maximum number of possible decomposition levels.  \nmultichannelbool, optional \n\nApply wavelet denoising separately for each channel (where channels correspond to the final axis of the array).  \nconvert2ycbcrbool, optional \n\nIf True and multichannel True, do the wavelet denoising in the YCbCr colorspace instead of the RGB color space. This typically results in better performance for RGB images.  \nmethod{\u2018BayesShrink\u2019, \u2018VisuShrink\u2019}, optional \n\nThresholding method to be used. The currently supported methods are \u201cBayesShrink\u201d [1] and \u201cVisuShrink\u201d [2]. Defaults to \u201cBayesShrink\u201d.  \nrescale_sigmabool, optional \n\nIf False, no rescaling of the user-provided sigma will be performed. The default of True rescales sigma appropriately if the image is rescaled internally.  New in version 0.16: rescale_sigma was introduced in 0.16     Returns \n \noutndarray \n\nDenoised image.     Notes The wavelet domain is a sparse representation of the image, and can be thought of similarly to the frequency domain of the Fourier transform. Sparse representations have most values zero or near-zero and truly random noise is (usually) represented by many small values in the wavelet domain. Setting all values below some threshold to 0 reduces the noise in the image, but larger thresholds also decrease the detail present in the image. If the input is 3D, this function performs wavelet denoising on each color plane separately.  Changed in version 0.16: For floating point inputs, the original input range is maintained and there is no clipping applied to the output. Other input types will be converted to a floating point value in the range [-1, 1] or [0, 1] depending on the input image range. Unless rescale_sigma = False, any internal rescaling applied to the image will also be applied to sigma to maintain the same relative amplitude.  Many wavelet coefficient thresholding approaches have been proposed. By default, denoise_wavelet applies BayesShrink, which is an adaptive thresholding method that computes separate thresholds for each wavelet sub-band as described in [1]. If method == \"VisuShrink\", a single \u201cuniversal threshold\u201d is applied to all wavelet detail coefficients as described in [2]. This threshold is designed to remove all Gaussian noise at a given sigma with high probability, but tends to produce images that appear overly smooth. Although any of the wavelets from PyWavelets can be selected, the thresholding methods assume an orthogonal wavelet transform and may not choose the threshold appropriately for biorthogonal wavelets. Orthogonal wavelets are desirable because white noise in the input remains white noise in the subbands. Biorthogonal wavelets lead to colored noise in the subbands. Additionally, the orthogonal wavelets in PyWavelets are orthonormal so that noise variance in the subbands remains identical to the noise variance of the input. Example orthogonal wavelets are the Daubechies (e.g. \u2018db2\u2019) or symmlet (e.g. \u2018sym2\u2019) families. References  \n1(1,2)  \nChang, S. Grace, Bin Yu, and Martin Vetterli. \u201cAdaptive wavelet thresholding for image denoising and compression.\u201d Image Processing, IEEE Transactions on 9.9 (2000): 1532-1546. DOI:10.1109/83.862633  \n2(1,2,3)  \nD. L. Donoho and I. M. Johnstone. \u201cIdeal spatial adaptation by wavelet shrinkage.\u201d Biometrika 81.3 (1994): 425-455. DOI:10.1093/biomet/81.3.425   Examples >>> from skimage import color, data\n>>> img = img_as_float(data.astronaut())\n>>> img = color.rgb2gray(img)\n>>> img += 0.1 * np.random.randn(*img.shape)\n>>> img = np.clip(img, 0, 1)\n>>> denoised_img = denoise_wavelet(img, sigma=0.1, rescale_sigma=True)\n \n"}, {"name": "restoration.ellipsoid_kernel()", "path": "api/skimage.restoration#skimage.restoration.ellipsoid_kernel", "type": "restoration", "text": " \nskimage.restoration.ellipsoid_kernel(shape, intensity) [source]\n \nCreate an ellipoid kernel for restoration.rolling_ball.  Parameters \n \nshapearraylike \n\nLength of the principal axis of the ellipsoid (excluding the intensity axis). The kernel needs to have the same dimensionality as the image it will be applied to.  \nintensityint \n\nLength of the intensity axis of the ellipsoid.    Returns \n \nkernelndarray \n\nThe kernel containing the surface intensity of the top half of the ellipsoid.      See also  \nrolling_ball\n\n  \n"}, {"name": "restoration.estimate_sigma()", "path": "api/skimage.restoration#skimage.restoration.estimate_sigma", "type": "restoration", "text": " \nskimage.restoration.estimate_sigma(image, average_sigmas=False, multichannel=False) [source]\n \nRobust wavelet-based estimator of the (Gaussian) noise standard deviation.  Parameters \n \nimagendarray \n\nImage for which to estimate the noise standard deviation.  \naverage_sigmasbool, optional \n\nIf true, average the channel estimates of sigma. Otherwise return a list of sigmas corresponding to each channel.  \nmultichannelbool \n\nEstimate sigma separately for each channel.    Returns \n \nsigmafloat or list \n\nEstimated noise standard deviation(s). If multichannel is True and average_sigmas is False, a separate noise estimate for each channel is returned. Otherwise, the average of the individual channel estimates is returned.     Notes This function assumes the noise follows a Gaussian distribution. The estimation algorithm is based on the median absolute deviation of the wavelet detail coefficients as described in section 4.2 of [1]. References  \n1  \nD. L. Donoho and I. M. Johnstone. \u201cIdeal spatial adaptation by wavelet shrinkage.\u201d Biometrika 81.3 (1994): 425-455. DOI:10.1093/biomet/81.3.425   Examples >>> import skimage.data\n>>> from skimage import img_as_float\n>>> img = img_as_float(skimage.data.camera())\n>>> sigma = 0.1\n>>> img = img + sigma * np.random.standard_normal(img.shape)\n>>> sigma_hat = estimate_sigma(img, multichannel=False)\n \n"}, {"name": "restoration.inpaint_biharmonic()", "path": "api/skimage.restoration#skimage.restoration.inpaint_biharmonic", "type": "restoration", "text": " \nskimage.restoration.inpaint_biharmonic(image, mask, multichannel=False) [source]\n \nInpaint masked points in image with biharmonic equations.  Parameters \n \nimage(M[, N[, \u2026, P]][, C]) ndarray \n\nInput image.  \nmask(M[, N[, \u2026, P]]) ndarray \n\nArray of pixels to be inpainted. Have to be the same shape as one of the \u2018image\u2019 channels. Unknown pixels have to be represented with 1, known pixels - with 0.  \nmultichannelboolean, optional \n\nIf True, the last image dimension is considered as a color channel, otherwise as spatial.    Returns \n \nout(M[, N[, \u2026, P]][, C]) ndarray \n\nInput image with masked pixels inpainted.     References  \n1  \nN.S.Hoang, S.B.Damelin, \u201cOn surface completion and image inpainting by biharmonic functions: numerical aspects\u201d, arXiv:1707.06567  \n2  \nC. K. Chui and H. N. Mhaskar, MRA Contextual-Recovery Extension of Smooth Functions on Manifolds, Appl. and Comp. Harmonic Anal., 28 (2010), 104-113, DOI:10.1016/j.acha.2009.04.004   Examples >>> img = np.tile(np.square(np.linspace(0, 1, 5)), (5, 1))\n>>> mask = np.zeros_like(img)\n>>> mask[2, 2:] = 1\n>>> mask[1, 3:] = 1\n>>> mask[0, 4:] = 1\n>>> out = inpaint_biharmonic(img, mask)\n \n"}, {"name": "restoration.richardson_lucy()", "path": "api/skimage.restoration#skimage.restoration.richardson_lucy", "type": "restoration", "text": " \nskimage.restoration.richardson_lucy(image, psf, iterations=50, clip=True, filter_epsilon=None) [source]\n \nRichardson-Lucy deconvolution.  Parameters \n \nimagendarray \n\nInput degraded image (can be N dimensional).  \npsfndarray \n\nThe point spread function.  \niterationsint, optional \n\nNumber of iterations. This parameter plays the role of regularisation.  \nclipboolean, optional \n\nTrue by default. If true, pixel value of the result above 1 or under -1 are thresholded for skimage pipeline compatibility.  filter_epsilon: float, optional\n\nValue below which intermediate results become 0 to avoid division by small numbers.    Returns \n \nim_deconvndarray \n\nThe deconvolved image.     References  \n1  \nhttps://en.wikipedia.org/wiki/Richardson%E2%80%93Lucy_deconvolution   Examples >>> from skimage import img_as_float, data, restoration\n>>> camera = img_as_float(data.camera())\n>>> from scipy.signal import convolve2d\n>>> psf = np.ones((5, 5)) / 25\n>>> camera = convolve2d(camera, psf, 'same')\n>>> camera += 0.1 * camera.std() * np.random.standard_normal(camera.shape)\n>>> deconvolved = restoration.richardson_lucy(camera, psf, 5)\n \n"}, {"name": "restoration.rolling_ball()", "path": "api/skimage.restoration#skimage.restoration.rolling_ball", "type": "restoration", "text": " \nskimage.restoration.rolling_ball(image, *, radius=100, kernel=None, nansafe=False, num_threads=None) [source]\n \nEstimate background intensity by rolling/translating a kernel. This rolling ball algorithm estimates background intensity for a ndimage in case of uneven exposure. It is a generalization of the frequently used rolling ball algorithm [1].  Parameters \n \nimagendarray \n\nThe image to be filtered.  \nradiusint, optional \n\nRadius of a ball shaped kernel to be rolled/translated in the image. Used if kernel = None.  \nkernelndarray, optional \n\nThe kernel to be rolled/translated in the image. It must have the same number of dimensions as image. Kernel is filled with the intensity of the kernel at that position.  nansafe: bool, optional\n\nIf False (default) assumes that none of the values in image are np.nan, and uses a faster implementation.  num_threads: int, optional\n\nThe maximum number of threads to use. If None use the OpenMP default value; typically equal to the maximum number of virtual cores. Note: This is an upper limit to the number of threads. The exact number is determined by the system\u2019s OpenMP library.    Returns \n \nbackgroundndarray \n\nThe estimated background of the image.     Notes For the pixel that has its background intensity estimated (without loss of generality at center) the rolling ball method centers kernel under it and raises the kernel until the surface touches the image umbra at some pos=(y,x). The background intensity is then estimated using the image intensity at that position (image[pos]) plus the difference of kernel[center] - kernel[pos]. This algorithm assumes that dark pixels correspond to the background. If you have a bright background, invert the image before passing it to the function, e.g., using utils.invert. See the gallery example for details. This algorithm is sensitive to noise (in particular salt-and-pepper noise). If this is a problem in your image, you can apply mild gaussian smoothing before passing the image to this function. References  \n1  \nSternberg, Stanley R. \u201cBiomedical image processing.\u201d Computer 1 (1983): 22-34. DOI:10.1109/MC.1983.1654163   Examples >>> import numpy as np\n>>> from skimage import data\n>>> from skimage.restoration import rolling_ball\n>>> image = data.coins()\n>>> background = rolling_ball(data.coins())\n>>> filtered_image = image - background\n >>> import numpy as np\n>>> from skimage import data\n>>> from skimage.restoration import rolling_ball, ellipsoid_kernel\n>>> image = data.coins()\n>>> kernel = ellipsoid_kernel((101, 101), 75)\n>>> background = rolling_ball(data.coins(), kernel=kernel)\n>>> filtered_image = image - background\n \n"}, {"name": "restoration.unsupervised_wiener()", "path": "api/skimage.restoration#skimage.restoration.unsupervised_wiener", "type": "restoration", "text": " \nskimage.restoration.unsupervised_wiener(image, psf, reg=None, user_params=None, is_real=True, clip=True) [source]\n \nUnsupervised Wiener-Hunt deconvolution. Return the deconvolution with a Wiener-Hunt approach, where the hyperparameters are automatically estimated. The algorithm is a stochastic iterative process (Gibbs sampler) described in the reference below. See also wiener function.  Parameters \n \nimage(M, N) ndarray \n\nThe input degraded image.  \npsfndarray \n\nThe impulse response (input image\u2019s space) or the transfer function (Fourier space). Both are accepted. The transfer function is automatically recognized as being complex (np.iscomplexobj(psf)).  \nregndarray, optional \n\nThe regularisation operator. The Laplacian by default. It can be an impulse response or a transfer function, as for the psf.  \nuser_paramsdict, optional \n\nDictionary of parameters for the Gibbs sampler. See below.  \nclipboolean, optional \n\nTrue by default. If true, pixel values of the result above 1 or under -1 are thresholded for skimage pipeline compatibility.    Returns \n \nx_postmean(M, N) ndarray \n\nThe deconvolved image (the posterior mean).  \nchainsdict \n\nThe keys noise and prior contain the chain list of noise and prior precision respectively.    Other Parameters \n The keys of ``user_params`` are:\n\nthresholdfloat \n\nThe stopping criterion: the norm of the difference between to successive approximated solution (empirical mean of object samples, see Notes section). 1e-4 by default.  \nburninint \n\nThe number of sample to ignore to start computation of the mean. 15 by default.  \nmin_iterint \n\nThe minimum number of iterations. 30 by default.  \nmax_iterint \n\nThe maximum number of iterations if threshold is not satisfied. 200 by default.  \ncallbackcallable (None by default) \n\nA user provided callable to which is passed, if the function exists, the current image sample for whatever purpose. The user can store the sample, or compute other moments than the mean. It has no influence on the algorithm execution and is only for inspection.     Notes The estimated image is design as the posterior mean of a probability law (from a Bayesian analysis). The mean is defined as a sum over all the possible images weighted by their respective probability. Given the size of the problem, the exact sum is not tractable. This algorithm use of MCMC to draw image under the posterior law. The practical idea is to only draw highly probable images since they have the biggest contribution to the mean. At the opposite, the less probable images are drawn less often since their contribution is low. Finally the empirical mean of these samples give us an estimation of the mean, and an exact computation with an infinite sample set. References  \n1  \nFran\u00e7ois Orieux, Jean-Fran\u00e7ois Giovannelli, and Thomas Rodet, \u201cBayesian estimation of regularization and point spread function parameters for Wiener-Hunt deconvolution\u201d, J. Opt. Soc. Am. A 27, 1593-1607 (2010) https://www.osapublishing.org/josaa/abstract.cfm?URI=josaa-27-7-1593 http://research.orieux.fr/files/papers/OGR-JOSA10.pdf   Examples >>> from skimage import color, data, restoration\n>>> img = color.rgb2gray(data.astronaut())\n>>> from scipy.signal import convolve2d\n>>> psf = np.ones((5, 5)) / 25\n>>> img = convolve2d(img, psf, 'same')\n>>> img += 0.1 * img.std() * np.random.standard_normal(img.shape)\n>>> deconvolved_img = restoration.unsupervised_wiener(img, psf)\n \n"}, {"name": "restoration.unwrap_phase()", "path": "api/skimage.restoration#skimage.restoration.unwrap_phase", "type": "restoration", "text": " \nskimage.restoration.unwrap_phase(image, wrap_around=False, seed=None) [source]\n \nRecover the original from a wrapped phase image. From an image wrapped to lie in the interval [-pi, pi), recover the original, unwrapped image.  Parameters \n \nimage1D, 2D or 3D ndarray of floats, optionally a masked array \n\nThe values should be in the range [-pi, pi). If a masked array is provided, the masked entries will not be changed, and their values will not be used to guide the unwrapping of neighboring, unmasked values. Masked 1D arrays are not allowed, and will raise a ValueError.  \nwrap_aroundbool or sequence of bool, optional \n\nWhen an element of the sequence is True, the unwrapping process will regard the edges along the corresponding axis of the image to be connected and use this connectivity to guide the phase unwrapping process. If only a single boolean is given, it will apply to all axes. Wrap around is not supported for 1D arrays.  \nseedint, optional \n\nUnwrapping 2D or 3D images uses random initialization. This sets the seed of the PRNG to achieve deterministic behavior.    Returns \n \nimage_unwrappedarray_like, double \n\nUnwrapped image of the same shape as the input. If the input image was a masked array, the mask will be preserved.    Raises \n ValueError\n\nIf called with a masked 1D array or called with a 1D array and wrap_around=True.     References  \n1  \nMiguel Arevallilo Herraez, David R. Burton, Michael J. Lalor, and Munther A. Gdeisat, \u201cFast two-dimensional phase-unwrapping algorithm based on sorting by reliability following a noncontinuous path\u201d, Journal Applied Optics, Vol. 41, No. 35 (2002) 7437,  \n2  \nAbdul-Rahman, H., Gdeisat, M., Burton, D., & Lalor, M., \u201cFast three-dimensional phase-unwrapping algorithm based on sorting by reliability following a non-continuous path. In W. Osten, C. Gorecki, & E. L. Novak (Eds.), Optical Metrology (2005) 32\u201340, International Society for Optics and Photonics.   Examples >>> c0, c1 = np.ogrid[-1:1:128j, -1:1:128j]\n>>> image = 12 * np.pi * np.exp(-(c0**2 + c1**2))\n>>> image_wrapped = np.angle(np.exp(1j * image))\n>>> image_unwrapped = unwrap_phase(image_wrapped)\n>>> np.std(image_unwrapped - image) < 1e-6   # A constant offset is normal\nTrue\n \n"}, {"name": "restoration.wiener()", "path": "api/skimage.restoration#skimage.restoration.wiener", "type": "restoration", "text": " \nskimage.restoration.wiener(image, psf, balance, reg=None, is_real=True, clip=True) [source]\n \nWiener-Hunt deconvolution Return the deconvolution with a Wiener-Hunt approach (i.e. with Fourier diagonalisation).  Parameters \n \nimage(M, N) ndarray \n\nInput degraded image  \npsfndarray \n\nPoint Spread Function. This is assumed to be the impulse response (input image space) if the data-type is real, or the transfer function (Fourier space) if the data-type is complex. There is no constraints on the shape of the impulse response. The transfer function must be of shape (M, N) if is_real is True, (M, N // 2 + 1) otherwise (see np.fft.rfftn).  \nbalancefloat \n\nThe regularisation parameter value that tunes the balance between the data adequacy that improve frequency restoration and the prior adequacy that reduce frequency restoration (to avoid noise artifacts).  \nregndarray, optional \n\nThe regularisation operator. The Laplacian by default. It can be an impulse response or a transfer function, as for the psf. Shape constraint is the same as for the psf parameter.  \nis_realboolean, optional \n\nTrue by default. Specify if psf and reg are provided with hermitian hypothesis, that is only half of the frequency plane is provided (due to the redundancy of Fourier transform of real signal). It\u2019s apply only if psf and/or reg are provided as transfer function. For the hermitian property see uft module or np.fft.rfftn.  \nclipboolean, optional \n\nTrue by default. If True, pixel values of the result above 1 or under -1 are thresholded for skimage pipeline compatibility.    Returns \n \nim_deconv(M, N) ndarray \n\nThe deconvolved image.     Notes This function applies the Wiener filter to a noisy and degraded image by an impulse response (or PSF). If the data model is  \\[y = Hx + n\\] where \\(n\\) is noise, \\(H\\) the PSF and \\(x\\) the unknown original image, the Wiener filter is  \\[\\hat x = F^\\dagger (|\\Lambda_H|^2 + \\lambda |\\Lambda_D|^2) \\Lambda_H^\\dagger F y\\] where \\(F\\) and \\(F^\\dagger\\) are the Fourier and inverse Fourier transforms respectively, \\(\\Lambda_H\\) the transfer function (or the Fourier transform of the PSF, see [Hunt] below) and \\(\\Lambda_D\\) the filter to penalize the restored image frequencies (Laplacian by default, that is penalization of high frequency). The parameter \\(\\lambda\\) tunes the balance between the data (that tends to increase high frequency, even those coming from noise), and the regularization. These methods are then specific to a prior model. Consequently, the application or the true image nature must corresponds to the prior model. By default, the prior model (Laplacian) introduce image smoothness or pixel correlation. It can also be interpreted as high-frequency penalization to compensate the instability of the solution with respect to the data (sometimes called noise amplification or \u201cexplosive\u201d solution). Finally, the use of Fourier space implies a circulant property of \\(H\\), see [Hunt]. References  \n1  \nFran\u00e7ois Orieux, Jean-Fran\u00e7ois Giovannelli, and Thomas Rodet, \u201cBayesian estimation of regularization and point spread function parameters for Wiener-Hunt deconvolution\u201d, J. Opt. Soc. Am. A 27, 1593-1607 (2010) https://www.osapublishing.org/josaa/abstract.cfm?URI=josaa-27-7-1593 http://research.orieux.fr/files/papers/OGR-JOSA10.pdf  \n2  \nB. R. Hunt \u201cA matrix theory proof of the discrete convolution theorem\u201d, IEEE Trans. on Audio and Electroacoustics, vol. au-19, no. 4, pp. 285-288, dec. 1971   Examples >>> from skimage import color, data, restoration\n>>> img = color.rgb2gray(data.astronaut())\n>>> from scipy.signal import convolve2d\n>>> psf = np.ones((5, 5)) / 25\n>>> img = convolve2d(img, psf, 'same')\n>>> img += 0.1 * img.std() * np.random.standard_normal(img.shape)\n>>> deconvolved_img = restoration.wiener(img, psf, 1100)\n \n"}, {"name": "segmentation", "path": "api/skimage.segmentation", "type": "segmentation", "text": "Module: segmentation  \nskimage.segmentation.active_contour(image, snake) Active contour model.  \nskimage.segmentation.chan_vese(image[, mu, \u2026]) Chan-Vese segmentation algorithm.  \nskimage.segmentation.checkerboard_level_set(\u2026) Create a checkerboard level set with binary values.  \nskimage.segmentation.circle_level_set(\u2026[, \u2026]) Create a circle level set with binary values.  \nskimage.segmentation.clear_border(labels[, \u2026]) Clear objects connected to the label image border.  \nskimage.segmentation.disk_level_set(\u2026[, \u2026]) Create a disk level set with binary values.  \nskimage.segmentation.expand_labels(label_image) Expand labels in label image by distance pixels without overlapping.  \nskimage.segmentation.felzenszwalb(image[, \u2026]) Computes Felsenszwalb\u2019s efficient graph based image segmentation.  \nskimage.segmentation.find_boundaries(label_img) Return bool array where boundaries between labeled regions are True.  \nskimage.segmentation.flood(image, seed_point, *) Mask corresponding to a flood fill.  \nskimage.segmentation.flood_fill(image, \u2026) Perform flood filling on an image.  \nskimage.segmentation.inverse_gaussian_gradient(image) Inverse of gradient magnitude.  \nskimage.segmentation.join_segmentations(s1, s2) Return the join of the two input segmentations.  \nskimage.segmentation.mark_boundaries(image, \u2026) Return image with boundaries between labeled regions highlighted.  \nskimage.segmentation.morphological_chan_vese(\u2026) Morphological Active Contours without Edges (MorphACWE)  \nskimage.segmentation.morphological_geodesic_active_contour(\u2026) Morphological Geodesic Active Contours (MorphGAC).  \nskimage.segmentation.quickshift(image[, \u2026]) Segments image using quickshift clustering in Color-(x,y) space.  \nskimage.segmentation.random_walker(data, labels) Random walker algorithm for segmentation from markers.  \nskimage.segmentation.relabel_sequential(\u2026) Relabel arbitrary labels to {offset, \u2026  \nskimage.segmentation.slic(image[, \u2026]) Segments image using k-means clustering in Color-(x,y,z) space.  \nskimage.segmentation.watershed(image[, \u2026]) Find watershed basins in image flooded from given markers.   active_contour  \nskimage.segmentation.active_contour(image, snake, alpha=0.01, beta=0.1, w_line=0, w_edge=1, gamma=0.01, max_px_move=1.0, max_iterations=2500, convergence=0.1, *, boundary_condition='periodic', coordinates='rc') [source]\n \nActive contour model. Active contours by fitting snakes to features of images. Supports single and multichannel 2D images. Snakes can be periodic (for segmentation) or have fixed and/or free ends. The output snake has the same length as the input boundary. As the number of points is constant, make sure that the initial snake has enough points to capture the details of the final contour.  Parameters \n \nimage(N, M) or (N, M, 3) ndarray \n\nInput image.  \nsnake(N, 2) ndarray \n\nInitial snake coordinates. For periodic boundary conditions, endpoints must not be duplicated.  \nalphafloat, optional \n\nSnake length shape parameter. Higher values makes snake contract faster.  \nbetafloat, optional \n\nSnake smoothness shape parameter. Higher values makes snake smoother.  \nw_linefloat, optional \n\nControls attraction to brightness. Use negative values to attract toward dark regions.  \nw_edgefloat, optional \n\nControls attraction to edges. Use negative values to repel snake from edges.  \ngammafloat, optional \n\nExplicit time stepping parameter.  \nmax_px_movefloat, optional \n\nMaximum pixel distance to move per iteration.  \nmax_iterationsint, optional \n\nMaximum iterations to optimize snake shape.  \nconvergencefloat, optional \n\nConvergence criteria.  \nboundary_conditionstring, optional \n\nBoundary conditions for the contour. Can be one of \u2018periodic\u2019, \u2018free\u2019, \u2018fixed\u2019, \u2018free-fixed\u2019, or \u2018fixed-free\u2019. \u2018periodic\u2019 attaches the two ends of the snake, \u2018fixed\u2019 holds the end-points in place, and \u2018free\u2019 allows free movement of the ends. \u2018fixed\u2019 and \u2018free\u2019 can be combined by parsing \u2018fixed-free\u2019, \u2018free-fixed\u2019. Parsing \u2018fixed-fixed\u2019 or \u2018free-free\u2019 yields same behaviour as \u2018fixed\u2019 and \u2018free\u2019, respectively.  \ncoordinates{\u2018rc\u2019}, optional \n\nThis option remains for compatibility purpose only and has no effect. It was introduced in 0.16 with the 'xy' option, but since 0.18, only the 'rc' option is valid. Coordinates must be set in a row-column format.    Returns \n \nsnake(N, 2) ndarray \n\nOptimised snake, same shape as input parameter.     References  \n1  \nKass, M.; Witkin, A.; Terzopoulos, D. \u201cSnakes: Active contour models\u201d. International Journal of Computer Vision 1 (4): 321 (1988). DOI:10.1007/BF00133570   Examples >>> from skimage.draw import circle_perimeter\n>>> from skimage.filters import gaussian\n Create and smooth image: >>> img = np.zeros((100, 100))\n>>> rr, cc = circle_perimeter(35, 45, 25)\n>>> img[rr, cc] = 1\n>>> img = gaussian(img, 2)\n Initialize spline: >>> s = np.linspace(0, 2*np.pi, 100)\n>>> init = 50 * np.array([np.sin(s), np.cos(s)]).T + 50\n Fit spline to image: >>> snake = active_contour(img, init, w_edge=0, w_line=1, coordinates='rc')  \n>>> dist = np.sqrt((45-snake[:, 0])**2 + (35-snake[:, 1])**2)  \n>>> int(np.mean(dist))  \n25\n \n chan_vese  \nskimage.segmentation.chan_vese(image, mu=0.25, lambda1=1.0, lambda2=1.0, tol=0.001, max_iter=500, dt=0.5, init_level_set='checkerboard', extended_output=False) [source]\n \nChan-Vese segmentation algorithm. Active contour model by evolving a level set. Can be used to segment objects without clearly defined boundaries.  Parameters \n \nimage(M, N) ndarray \n\nGrayscale image to be segmented.  \nmufloat, optional \n\n\u2018edge length\u2019 weight parameter. Higher mu values will produce a \u2018round\u2019 edge, while values closer to zero will detect smaller objects.  \nlambda1float, optional \n\n\u2018difference from average\u2019 weight parameter for the output region with value \u2018True\u2019. If it is lower than lambda2, this region will have a larger range of values than the other.  \nlambda2float, optional \n\n\u2018difference from average\u2019 weight parameter for the output region with value \u2018False\u2019. If it is lower than lambda1, this region will have a larger range of values than the other.  \ntolfloat, positive, optional \n\nLevel set variation tolerance between iterations. If the L2 norm difference between the level sets of successive iterations normalized by the area of the image is below this value, the algorithm will assume that the solution was reached.  \nmax_iteruint, optional \n\nMaximum number of iterations allowed before the algorithm interrupts itself.  \ndtfloat, optional \n\nA multiplication factor applied at calculations for each step, serves to accelerate the algorithm. While higher values may speed up the algorithm, they may also lead to convergence problems.  \ninit_level_setstr or (M, N) ndarray, optional \n\nDefines the starting level set used by the algorithm. If a string is inputted, a level set that matches the image size will automatically be generated. Alternatively, it is possible to define a custom level set, which should be an array of float values, with the same shape as \u2018image\u2019. Accepted string values are as follows.  \u2018checkerboard\u2019\n\nthe starting level set is defined as sin(x/5*pi)*sin(y/5*pi), where x and y are pixel coordinates. This level set has fast convergence, but may fail to detect implicit edges.  \u2018disk\u2019\n\nthe starting level set is defined as the opposite of the distance from the center of the image minus half of the minimum value between image width and image height. This is somewhat slower, but is more likely to properly detect implicit edges.  \u2018small disk\u2019\n\nthe starting level set is defined as the opposite of the distance from the center of the image minus a quarter of the minimum value between image width and image height.    \nextended_outputbool, optional \n\nIf set to True, the return value will be a tuple containing the three return values (see below). If set to False which is the default value, only the \u2018segmentation\u2019 array will be returned.    Returns \n \nsegmentation(M, N) ndarray, bool \n\nSegmentation produced by the algorithm.  \nphi(M, N) ndarray of floats \n\nFinal level set computed by the algorithm.  \nenergieslist of floats \n\nShows the evolution of the \u2018energy\u2019 for each step of the algorithm. This should allow to check whether the algorithm converged.     Notes The Chan-Vese Algorithm is designed to segment objects without clearly defined boundaries. This algorithm is based on level sets that are evolved iteratively to minimize an energy, which is defined by weighted values corresponding to the sum of differences intensity from the average value outside the segmented region, the sum of differences from the average value inside the segmented region, and a term which is dependent on the length of the boundary of the segmented region. This algorithm was first proposed by Tony Chan and Luminita Vese, in a publication entitled \u201cAn Active Contour Model Without Edges\u201d [1]. This implementation of the algorithm is somewhat simplified in the sense that the area factor \u2018nu\u2019 described in the original paper is not implemented, and is only suitable for grayscale images. Typical values for lambda1 and lambda2 are 1. If the \u2018background\u2019 is very different from the segmented object in terms of distribution (for example, a uniform black image with figures of varying intensity), then these values should be different from each other. Typical values for mu are between 0 and 1, though higher values can be used when dealing with shapes with very ill-defined contours. The \u2018energy\u2019 which this algorithm tries to minimize is defined as the sum of the differences from the average within the region squared and weighed by the \u2018lambda\u2019 factors to which is added the length of the contour multiplied by the \u2018mu\u2019 factor. Supports 2D grayscale images only, and does not implement the area term described in the original article. References  \n1  \nAn Active Contour Model without Edges, Tony Chan and Luminita Vese, Scale-Space Theories in Computer Vision, 1999, DOI:10.1007/3-540-48236-9_13  \n2  \nChan-Vese Segmentation, Pascal Getreuer Image Processing On Line, 2 (2012), pp. 214-224, DOI:10.5201/ipol.2012.g-cv  \n3  \nThe Chan-Vese Algorithm - Project Report, Rami Cohen, 2011 arXiv:1107.2782   \n checkerboard_level_set  \nskimage.segmentation.checkerboard_level_set(image_shape, square_size=5) [source]\n \nCreate a checkerboard level set with binary values.  Parameters \n \nimage_shapetuple of positive integers \n\nShape of the image.  \nsquare_sizeint, optional \n\nSize of the squares of the checkerboard. It defaults to 5.    Returns \n \noutarray with shape image_shape \n\nBinary level set of the checkerboard.      See also  \ncircle_level_set\n\n  \n circle_level_set  \nskimage.segmentation.circle_level_set(image_shape, center=None, radius=None) [source]\n \nCreate a circle level set with binary values.  Parameters \n \nimage_shapetuple of positive integers \n\nShape of the image  \ncentertuple of positive integers, optional \n\nCoordinates of the center of the circle given in (row, column). If not given, it defaults to the center of the image.  \nradiusfloat, optional \n\nRadius of the circle. If not given, it is set to the 75% of the smallest image dimension.    Returns \n \noutarray with shape image_shape \n\nBinary level set of the circle with the given radius and center.    Warns \n Deprecated:\n\n New in version 0.17: This function is deprecated and will be removed in scikit-image 0.19. Please use the function named disk_level_set instead.       See also  \ncheckerboard_level_set\n\n  \n clear_border  \nskimage.segmentation.clear_border(labels, buffer_size=0, bgval=0, in_place=False, mask=None) [source]\n \nClear objects connected to the label image border.  Parameters \n \nlabels(M[, N[, \u2026, P]]) array of int or bool \n\nImaging data labels.  \nbuffer_sizeint, optional \n\nThe width of the border examined. By default, only objects that touch the outside of the image are removed.  \nbgvalfloat or int, optional \n\nCleared objects are set to this value.  \nin_placebool, optional \n\nWhether or not to manipulate the labels array in-place.  \nmaskndarray of bool, same shape as image, optional. \n\nImage data mask. Objects in labels image overlapping with False pixels of mask will be removed. If defined, the argument buffer_size will be ignored.    Returns \n \nout(M[, N[, \u2026, P]]) array \n\nImaging data labels with cleared borders     Examples >>> import numpy as np\n>>> from skimage.segmentation import clear_border\n>>> labels = np.array([[0, 0, 0, 0, 0, 0, 0, 1, 0],\n...                    [1, 1, 0, 0, 1, 0, 0, 1, 0],\n...                    [1, 1, 0, 1, 0, 1, 0, 0, 0],\n...                    [0, 0, 0, 1, 1, 1, 1, 0, 0],\n...                    [0, 1, 1, 1, 1, 1, 1, 1, 0],\n...                    [0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> clear_border(labels)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> mask = np.array([[0, 0, 1, 1, 1, 1, 1, 1, 1],\n...                  [0, 0, 1, 1, 1, 1, 1, 1, 1],\n...                  [1, 1, 1, 1, 1, 1, 1, 1, 1],\n...                  [1, 1, 1, 1, 1, 1, 1, 1, 1],\n...                  [1, 1, 1, 1, 1, 1, 1, 1, 1],\n...                  [1, 1, 1, 1, 1, 1, 1, 1, 1]]).astype(bool)\n>>> clear_border(labels, mask=mask)\narray([[0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 1, 0, 0, 1, 0],\n       [0, 0, 0, 1, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0]])\n \n disk_level_set  \nskimage.segmentation.disk_level_set(image_shape, *, center=None, radius=None) [source]\n \nCreate a disk level set with binary values.  Parameters \n \nimage_shapetuple of positive integers \n\nShape of the image  \ncentertuple of positive integers, optional \n\nCoordinates of the center of the disk given in (row, column). If not given, it defaults to the center of the image.  \nradiusfloat, optional \n\nRadius of the disk. If not given, it is set to the 75% of the smallest image dimension.    Returns \n \noutarray with shape image_shape \n\nBinary level set of the disk with the given radius and center.      See also  \ncheckerboard_level_set\n\n  \n expand_labels  \nskimage.segmentation.expand_labels(label_image, distance=1) [source]\n \nExpand labels in label image by distance pixels without overlapping. Given a label image, expand_labels grows label regions (connected components) outwards by up to distance pixels without overflowing into neighboring regions. More specifically, each background pixel that is within Euclidean distance of <= distance pixels of a connected component is assigned the label of that connected component. Where multiple connected components are within distance pixels of a background pixel, the label value of the closest connected component will be assigned (see Notes for the case of multiple labels at equal distance).  Parameters \n \nlabel_imagendarray of dtype int \n\nlabel image  \ndistancefloat \n\nEuclidean distance in pixels by which to grow the labels. Default is one.    Returns \n \nenlarged_labelsndarray of dtype int \n\nLabeled array, where all connected regions have been enlarged      See also  \nskimage.measure.label(), skimage.segmentation.watershed(), skimage.morphology.dilation()\n\n  Notes Where labels are spaced more than distance pixels are apart, this is equivalent to a morphological dilation with a disc or hyperball of radius distance. However, in contrast to a morphological dilation, expand_labels will not expand a label region into a neighboring region. This implementation of expand_labels is derived from CellProfiler [1], where it is known as module \u201cIdentifySecondaryObjects (Distance-N)\u201d [2]. There is an important edge case when a pixel has the same distance to multiple regions, as it is not defined which region expands into that space. Here, the exact behavior depends on the upstream implementation of scipy.ndimage.distance_transform_edt. References  \n1  \nhttps://cellprofiler.org  \n2  \nhttps://github.com/CellProfiler/CellProfiler/blob/082930ea95add7b72243a4fa3d39ae5145995e9c/cellprofiler/modules/identifysecondaryobjects.py#L559   Examples >>> labels = np.array([0, 1, 0, 0, 0, 0, 2])\n>>> expand_labels(labels, distance=1)\narray([1, 1, 1, 0, 0, 2, 2])\n Labels will not overwrite each other: >>> expand_labels(labels, distance=3)\narray([1, 1, 1, 1, 2, 2, 2])\n In case of ties, behavior is undefined, but currently resolves to the label closest to (0,) * ndim in lexicographical order. >>> labels_tied = np.array([0, 1, 0, 2, 0])\n>>> expand_labels(labels_tied, 1)\narray([1, 1, 1, 2, 2])\n>>> labels2d = np.array(\n...     [[0, 1, 0, 0],\n...      [2, 0, 0, 0],\n...      [0, 3, 0, 0]]\n... )\n>>> expand_labels(labels2d, 1)\narray([[2, 1, 1, 0],\n       [2, 2, 0, 0],\n       [2, 3, 3, 0]])\n \n felzenszwalb  \nskimage.segmentation.felzenszwalb(image, scale=1, sigma=0.8, min_size=20, multichannel=True) [source]\n \nComputes Felsenszwalb\u2019s efficient graph based image segmentation. Produces an oversegmentation of a multichannel (i.e. RGB) image using a fast, minimum spanning tree based clustering on the image grid. The parameter scale sets an observation level. Higher scale means less and larger segments. sigma is the diameter of a Gaussian kernel, used for smoothing the image prior to segmentation. The number of produced segments as well as their size can only be controlled indirectly through scale. Segment size within an image can vary greatly depending on local contrast. For RGB images, the algorithm uses the euclidean distance between pixels in color space.  Parameters \n \nimage(width, height, 3) or (width, height) ndarray \n\nInput image.  \nscalefloat \n\nFree parameter. Higher means larger clusters.  \nsigmafloat \n\nWidth (standard deviation) of Gaussian kernel used in preprocessing.  \nmin_sizeint \n\nMinimum component size. Enforced using postprocessing.  \nmultichannelbool, optional (default: True) \n\nWhether the last axis of the image is to be interpreted as multiple channels. A value of False, for a 3D image, is not currently supported.    Returns \n \nsegment_mask(width, height) ndarray \n\nInteger mask indicating segment labels.     Notes The k parameter used in the original paper renamed to scale here. References  \n1  \nEfficient graph-based image segmentation, Felzenszwalb, P.F. and Huttenlocher, D.P. International Journal of Computer Vision, 2004   Examples >>> from skimage.segmentation import felzenszwalb\n>>> from skimage.data import coffee\n>>> img = coffee()\n>>> segments = felzenszwalb(img, scale=3.0, sigma=0.95, min_size=5)\n \n find_boundaries  \nskimage.segmentation.find_boundaries(label_img, connectivity=1, mode='thick', background=0) [source]\n \nReturn bool array where boundaries between labeled regions are True.  Parameters \n \nlabel_imgarray of int or bool \n\nAn array in which different regions are labeled with either different integers or boolean values.  \nconnectivityint in {1, \u2026, label_img.ndim}, optional \n\nA pixel is considered a boundary pixel if any of its neighbors has a different label. connectivity controls which pixels are considered neighbors. A connectivity of 1 (default) means pixels sharing an edge (in 2D) or a face (in 3D) will be considered neighbors. A connectivity of label_img.ndim means pixels sharing a corner will be considered neighbors.  \nmodestring in {\u2018thick\u2019, \u2018inner\u2019, \u2018outer\u2019, \u2018subpixel\u2019} \n\nHow to mark the boundaries:  thick: any pixel not completely surrounded by pixels of the same label (defined by connectivity) is marked as a boundary. This results in boundaries that are 2 pixels thick. inner: outline the pixels just inside of objects, leaving background pixels untouched. outer: outline pixels in the background around object boundaries. When two objects touch, their boundary is also marked. subpixel: return a doubled image, with pixels between the original pixels marked as boundary where appropriate.   \nbackgroundint, optional \n\nFor modes \u2018inner\u2019 and \u2018outer\u2019, a definition of a background label is required. See mode for descriptions of these two.    Returns \n \nboundariesarray of bool, same shape as label_img \n\nA bool image where True represents a boundary pixel. For mode equal to \u2018subpixel\u2019, boundaries.shape[i] is equal to 2 * label_img.shape[i] - 1 for all i (a pixel is inserted in between all other pairs of pixels).     Examples >>> labels = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 5, 5, 5, 0, 0],\n...                    [0, 0, 1, 1, 1, 5, 5, 5, 0, 0],\n...                    [0, 0, 1, 1, 1, 5, 5, 5, 0, 0],\n...                    [0, 0, 1, 1, 1, 5, 5, 5, 0, 0],\n...                    [0, 0, 0, 0, 0, 5, 5, 5, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=np.uint8)\n>>> find_boundaries(labels, mode='thick').astype(np.uint8)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 1, 1, 1, 1, 1, 0, 1, 1, 0],\n       [0, 1, 1, 0, 1, 1, 0, 1, 1, 0],\n       [0, 1, 1, 1, 1, 1, 0, 1, 1, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n>>> find_boundaries(labels, mode='inner').astype(np.uint8)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0],\n       [0, 0, 1, 0, 1, 1, 0, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n>>> find_boundaries(labels, mode='outer').astype(np.uint8)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 0, 1, 0],\n       [0, 1, 0, 0, 1, 1, 0, 0, 1, 0],\n       [0, 1, 0, 0, 1, 1, 0, 0, 1, 0],\n       [0, 1, 0, 0, 1, 1, 0, 0, 1, 0],\n       [0, 0, 1, 1, 1, 1, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n>>> labels_small = labels[::2, ::3]\n>>> labels_small\narray([[0, 0, 0, 0],\n       [0, 0, 5, 0],\n       [0, 1, 5, 0],\n       [0, 0, 5, 0],\n       [0, 0, 0, 0]], dtype=uint8)\n>>> find_boundaries(labels_small, mode='subpixel').astype(np.uint8)\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 0],\n       [0, 0, 0, 1, 0, 1, 0],\n       [0, 1, 1, 1, 0, 1, 0],\n       [0, 1, 0, 1, 0, 1, 0],\n       [0, 1, 1, 1, 0, 1, 0],\n       [0, 0, 0, 1, 0, 1, 0],\n       [0, 0, 0, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n>>> bool_image = np.array([[False, False, False, False, False],\n...                        [False, False, False, False, False],\n...                        [False, False,  True,  True,  True],\n...                        [False, False,  True,  True,  True],\n...                        [False, False,  True,  True,  True]],\n...                       dtype=bool)\n>>> find_boundaries(bool_image)\narray([[False, False, False, False, False],\n       [False, False,  True,  True,  True],\n       [False,  True,  True,  True,  True],\n       [False,  True,  True, False, False],\n       [False,  True,  True, False, False]])\n \n flood  \nskimage.segmentation.flood(image, seed_point, *, selem=None, connectivity=None, tolerance=None) [source]\n \nMask corresponding to a flood fill. Starting at a specific seed_point, connected points equal or within tolerance of the seed value are found.  Parameters \n \nimagendarray \n\nAn n-dimensional array.  \nseed_pointtuple or int \n\nThe point in image used as the starting point for the flood fill. If the image is 1D, this point may be given as an integer.  \nselemndarray, optional \n\nA structuring element used to determine the neighborhood of each evaluated pixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as image. If not given, all adjacent pixels are considered as part of the neighborhood (fully connected).  \nconnectivityint, optional \n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is larger or equal to connectivity are considered neighbors. Ignored if selem is not None.  \ntolerancefloat or int, optional \n\nIf None (default), adjacent values must be strictly equal to the initial value of image at seed_point. This is fastest. If a value is given, a comparison will be done at every point and if within tolerance of the initial value will also be filled (inclusive).    Returns \n \nmaskndarray \n\nA Boolean array with the same shape as image is returned, with True values for areas connected to and equal (or within tolerance of) the seed point. All other values are False.     Notes The conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many raster graphics programs. This function returns just the mask representing the fill. If indices are desired rather than masks for memory reasons, the user can simply run numpy.nonzero on the result, save the indices, and discard this mask. Examples >>> from skimage.morphology import flood\n>>> image = np.zeros((4, 7), dtype=int)\n>>> image[1:3, 1:3] = 1\n>>> image[3, 0] = 1\n>>> image[1:3, 4:6] = 2\n>>> image[3, 6] = 3\n>>> image\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 1, 0, 2, 2, 0],\n       [0, 1, 1, 0, 2, 2, 0],\n       [1, 0, 0, 0, 0, 0, 3]])\n Fill connected ones with 5, with full connectivity (diagonals included): >>> mask = flood(image, (1, 1))\n>>> image_flooded = image.copy()\n>>> image_flooded[mask] = 5\n>>> image_flooded\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [5, 0, 0, 0, 0, 0, 3]])\n Fill connected ones with 5, excluding diagonal points (connectivity 1): >>> mask = flood(image, (1, 1), connectivity=1)\n>>> image_flooded = image.copy()\n>>> image_flooded[mask] = 5\n>>> image_flooded\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [1, 0, 0, 0, 0, 0, 3]])\n Fill with a tolerance: >>> mask = flood(image, (0, 0), tolerance=1)\n>>> image_flooded = image.copy()\n>>> image_flooded[mask] = 5\n>>> image_flooded\narray([[5, 5, 5, 5, 5, 5, 5],\n       [5, 5, 5, 5, 2, 2, 5],\n       [5, 5, 5, 5, 2, 2, 5],\n       [5, 5, 5, 5, 5, 5, 3]])\n \n Examples using skimage.segmentation.flood\n \n  Flood Fill   flood_fill  \nskimage.segmentation.flood_fill(image, seed_point, new_value, *, selem=None, connectivity=None, tolerance=None, in_place=False, inplace=None) [source]\n \nPerform flood filling on an image. Starting at a specific seed_point, connected points equal or within tolerance of the seed value are found, then set to new_value.  Parameters \n \nimagendarray \n\nAn n-dimensional array.  \nseed_pointtuple or int \n\nThe point in image used as the starting point for the flood fill. If the image is 1D, this point may be given as an integer.  \nnew_valueimage type \n\nNew value to set the entire fill. This must be chosen in agreement with the dtype of image.  \nselemndarray, optional \n\nA structuring element used to determine the neighborhood of each evaluated pixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as image. If not given, all adjacent pixels are considered as part of the neighborhood (fully connected).  \nconnectivityint, optional \n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is less than or equal to connectivity are considered neighbors. Ignored if selem is not None.  \ntolerancefloat or int, optional \n\nIf None (default), adjacent values must be strictly equal to the value of image at seed_point to be filled. This is fastest. If a tolerance is provided, adjacent points with values within plus or minus tolerance from the seed point are filled (inclusive).  \nin_placebool, optional \n\nIf True, flood filling is applied to image in place. If False, the flood filled result is returned without modifying the input image (default).  \ninplacebool, optional \n\nThis parameter is deprecated and will be removed in version 0.19.0 in favor of in_place. If True, flood filling is applied to image inplace. If False, the flood filled result is returned without modifying the input image (default).    Returns \n \nfilledndarray \n\nAn array with the same shape as image is returned, with values in areas connected to and equal (or within tolerance of) the seed point replaced with new_value.     Notes The conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many raster graphics programs. Examples >>> from skimage.morphology import flood_fill\n>>> image = np.zeros((4, 7), dtype=int)\n>>> image[1:3, 1:3] = 1\n>>> image[3, 0] = 1\n>>> image[1:3, 4:6] = 2\n>>> image[3, 6] = 3\n>>> image\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 1, 0, 2, 2, 0],\n       [0, 1, 1, 0, 2, 2, 0],\n       [1, 0, 0, 0, 0, 0, 3]])\n Fill connected ones with 5, with full connectivity (diagonals included): >>> flood_fill(image, (1, 1), 5)\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [5, 0, 0, 0, 0, 0, 3]])\n Fill connected ones with 5, excluding diagonal points (connectivity 1): >>> flood_fill(image, (1, 1), 5, connectivity=1)\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [1, 0, 0, 0, 0, 0, 3]])\n Fill with a tolerance: >>> flood_fill(image, (0, 0), 5, tolerance=1)\narray([[5, 5, 5, 5, 5, 5, 5],\n       [5, 5, 5, 5, 2, 2, 5],\n       [5, 5, 5, 5, 2, 2, 5],\n       [5, 5, 5, 5, 5, 5, 3]])\n \n Examples using skimage.segmentation.flood_fill\n \n  Flood Fill   inverse_gaussian_gradient  \nskimage.segmentation.inverse_gaussian_gradient(image, alpha=100.0, sigma=5.0) [source]\n \nInverse of gradient magnitude. Compute the magnitude of the gradients in the image and then inverts the result in the range [0, 1]. Flat areas are assigned values close to 1, while areas close to borders are assigned values close to 0. This function or a similar one defined by the user should be applied over the image as a preprocessing step before calling morphological_geodesic_active_contour.  Parameters \n \nimage(M, N) or (L, M, N) array \n\nGrayscale image or volume.  \nalphafloat, optional \n\nControls the steepness of the inversion. A larger value will make the transition between the flat areas and border areas steeper in the resulting array.  \nsigmafloat, optional \n\nStandard deviation of the Gaussian filter applied over the image.    Returns \n \ngimage(M, N) or (L, M, N) array \n\nPreprocessed image (or volume) suitable for morphological_geodesic_active_contour.     \n join_segmentations  \nskimage.segmentation.join_segmentations(s1, s2) [source]\n \nReturn the join of the two input segmentations. The join J of S1 and S2 is defined as the segmentation in which two voxels are in the same segment if and only if they are in the same segment in both S1 and S2.  Parameters \n \ns1, s2numpy arrays \n\ns1 and s2 are label fields of the same shape.    Returns \n \njnumpy array \n\nThe join segmentation of s1 and s2.     Examples >>> from skimage.segmentation import join_segmentations\n>>> s1 = np.array([[0, 0, 1, 1],\n...                [0, 2, 1, 1],\n...                [2, 2, 2, 1]])\n>>> s2 = np.array([[0, 1, 1, 0],\n...                [0, 1, 1, 0],\n...                [0, 1, 1, 1]])\n>>> join_segmentations(s1, s2)\narray([[0, 1, 3, 2],\n       [0, 5, 3, 2],\n       [4, 5, 5, 3]])\n \n mark_boundaries  \nskimage.segmentation.mark_boundaries(image, label_img, color=(1, 1, 0), outline_color=None, mode='outer', background_label=0) [source]\n \nReturn image with boundaries between labeled regions highlighted.  Parameters \n \nimage(M, N[, 3]) array \n\nGrayscale or RGB image.  \nlabel_img(M, N) array of int \n\nLabel array where regions are marked by different integer values.  \ncolorlength-3 sequence, optional \n\nRGB color of boundaries in the output image.  \noutline_colorlength-3 sequence, optional \n\nRGB color surrounding boundaries in the output image. If None, no outline is drawn.  \nmodestring in {\u2018thick\u2019, \u2018inner\u2019, \u2018outer\u2019, \u2018subpixel\u2019}, optional \n\nThe mode for finding boundaries.  \nbackground_labelint, optional \n\nWhich label to consider background (this is only useful for modes inner and outer).    Returns \n \nmarked(M, N, 3) array of float \n\nAn image in which the boundaries between labels are superimposed on the original image.      See also  \nfind_boundaries\n\n  \n Examples using skimage.segmentation.mark_boundaries\n \n  Trainable segmentation using local features and random forests   morphological_chan_vese  \nskimage.segmentation.morphological_chan_vese(image, iterations, init_level_set='checkerboard', smoothing=1, lambda1=1, lambda2=1, iter_callback=<function <lambda>>) [source]\n \nMorphological Active Contours without Edges (MorphACWE) Active contours without edges implemented with morphological operators. It can be used to segment objects in images and volumes without well defined borders. It is required that the inside of the object looks different on average than the outside (i.e., the inner area of the object should be darker or lighter than the outer area on average).  Parameters \n \nimage(M, N) or (L, M, N) array \n\nGrayscale image or volume to be segmented.  \niterationsuint \n\nNumber of iterations to run  \ninit_level_setstr, (M, N) array, or (L, M, N) array \n\nInitial level set. If an array is given, it will be binarized and used as the initial level set. If a string is given, it defines the method to generate a reasonable initial level set with the shape of the image. Accepted values are \u2018checkerboard\u2019 and \u2018circle\u2019. See the documentation of checkerboard_level_set and circle_level_set respectively for details about how these level sets are created.  \nsmoothinguint, optional \n\nNumber of times the smoothing operator is applied per iteration. Reasonable values are around 1-4. Larger values lead to smoother segmentations.  \nlambda1float, optional \n\nWeight parameter for the outer region. If lambda1 is larger than lambda2, the outer region will contain a larger range of values than the inner region.  \nlambda2float, optional \n\nWeight parameter for the inner region. If lambda2 is larger than lambda1, the inner region will contain a larger range of values than the outer region.  \niter_callbackfunction, optional \n\nIf given, this function is called once per iteration with the current level set as the only argument. This is useful for debugging or for plotting intermediate results during the evolution.    Returns \n \nout(M, N) or (L, M, N) array \n\nFinal segmentation (i.e., the final level set)      See also  \ncircle_level_set, checkerboard_level_set\n\n  Notes This is a version of the Chan-Vese algorithm that uses morphological operators instead of solving a partial differential equation (PDE) for the evolution of the contour. The set of morphological operators used in this algorithm are proved to be infinitesimally equivalent to the Chan-Vese PDE (see [1]). However, morphological operators are do not suffer from the numerical stability issues typically found in PDEs (it is not necessary to find the right time step for the evolution), and are computationally faster. The algorithm and its theoretical derivation are described in [1]. References  \n1(1,2)  \nA Morphological Approach to Curvature-based Evolution of Curves and Surfaces, Pablo M\u00e1rquez-Neila, Luis Baumela, Luis \u00c1lvarez. In IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2014, DOI:10.1109/TPAMI.2013.106   \n morphological_geodesic_active_contour  \nskimage.segmentation.morphological_geodesic_active_contour(gimage, iterations, init_level_set='circle', smoothing=1, threshold='auto', balloon=0, iter_callback=<function <lambda>>) [source]\n \nMorphological Geodesic Active Contours (MorphGAC). Geodesic active contours implemented with morphological operators. It can be used to segment objects with visible but noisy, cluttered, broken borders.  Parameters \n \ngimage(M, N) or (L, M, N) array \n\nPreprocessed image or volume to be segmented. This is very rarely the original image. Instead, this is usually a preprocessed version of the original image that enhances and highlights the borders (or other structures) of the object to segment. morphological_geodesic_active_contour will try to stop the contour evolution in areas where gimage is small. See morphsnakes.inverse_gaussian_gradient as an example function to perform this preprocessing. Note that the quality of morphological_geodesic_active_contour might greatly depend on this preprocessing.  \niterationsuint \n\nNumber of iterations to run.  \ninit_level_setstr, (M, N) array, or (L, M, N) array \n\nInitial level set. If an array is given, it will be binarized and used as the initial level set. If a string is given, it defines the method to generate a reasonable initial level set with the shape of the image. Accepted values are \u2018checkerboard\u2019 and \u2018circle\u2019. See the documentation of checkerboard_level_set and circle_level_set respectively for details about how these level sets are created.  \nsmoothinguint, optional \n\nNumber of times the smoothing operator is applied per iteration. Reasonable values are around 1-4. Larger values lead to smoother segmentations.  \nthresholdfloat, optional \n\nAreas of the image with a value smaller than this threshold will be considered borders. The evolution of the contour will stop in this areas.  \nballoonfloat, optional \n\nBalloon force to guide the contour in non-informative areas of the image, i.e., areas where the gradient of the image is too small to push the contour towards a border. A negative value will shrink the contour, while a positive value will expand the contour in these areas. Setting this to zero will disable the balloon force.  \niter_callbackfunction, optional \n\nIf given, this function is called once per iteration with the current level set as the only argument. This is useful for debugging or for plotting intermediate results during the evolution.    Returns \n \nout(M, N) or (L, M, N) array \n\nFinal segmentation (i.e., the final level set)      See also  \ninverse_gaussian_gradient, circle_level_set, checkerboard_level_set\n\n  Notes This is a version of the Geodesic Active Contours (GAC) algorithm that uses morphological operators instead of solving partial differential equations (PDEs) for the evolution of the contour. The set of morphological operators used in this algorithm are proved to be infinitesimally equivalent to the GAC PDEs (see [1]). However, morphological operators are do not suffer from the numerical stability issues typically found in PDEs (e.g., it is not necessary to find the right time step for the evolution), and are computationally faster. The algorithm and its theoretical derivation are described in [1]. References  \n1(1,2)  \nA Morphological Approach to Curvature-based Evolution of Curves and Surfaces, Pablo M\u00e1rquez-Neila, Luis Baumela, Luis \u00c1lvarez. In IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2014, DOI:10.1109/TPAMI.2013.106   \n quickshift  \nskimage.segmentation.quickshift(image, ratio=1.0, kernel_size=5, max_dist=10, return_tree=False, sigma=0, convert2lab=True, random_seed=42) [source]\n \nSegments image using quickshift clustering in Color-(x,y) space. Produces an oversegmentation of the image using the quickshift mode-seeking algorithm.  Parameters \n \nimage(width, height, channels) ndarray \n\nInput image.  \nratiofloat, optional, between 0 and 1 \n\nBalances color-space proximity and image-space proximity. Higher values give more weight to color-space.  \nkernel_sizefloat, optional \n\nWidth of Gaussian kernel used in smoothing the sample density. Higher means fewer clusters.  \nmax_distfloat, optional \n\nCut-off point for data distances. Higher means fewer clusters.  \nreturn_treebool, optional \n\nWhether to return the full segmentation hierarchy tree and distances.  \nsigmafloat, optional \n\nWidth for Gaussian smoothing as preprocessing. Zero means no smoothing.  \nconvert2labbool, optional \n\nWhether the input should be converted to Lab colorspace prior to segmentation. For this purpose, the input is assumed to be RGB.  \nrandom_seedint, optional \n\nRandom seed used for breaking ties.    Returns \n \nsegment_mask(width, height) ndarray \n\nInteger mask indicating segment labels.     Notes The authors advocate to convert the image to Lab color space prior to segmentation, though this is not strictly necessary. For this to work, the image must be given in RGB format. References  \n1  \nQuick shift and kernel methods for mode seeking, Vedaldi, A. and Soatto, S. European Conference on Computer Vision, 2008   \n random_walker  \nskimage.segmentation.random_walker(data, labels, beta=130, mode='cg_j', tol=0.001, copy=True, multichannel=False, return_full_prob=False, spacing=None, *, prob_tol=0.001) [source]\n \nRandom walker algorithm for segmentation from markers. Random walker algorithm is implemented for gray-level or multichannel images.  Parameters \n \ndataarray_like \n\nImage to be segmented in phases. Gray-level data can be two- or three-dimensional; multichannel data can be three- or four- dimensional (multichannel=True) with the highest dimension denoting channels. Data spacing is assumed isotropic unless the spacing keyword argument is used.  \nlabelsarray of ints, of same shape as data without channels dimension \n\nArray of seed markers labeled with different positive integers for different phases. Zero-labeled pixels are unlabeled pixels. Negative labels correspond to inactive pixels that are not taken into account (they are removed from the graph). If labels are not consecutive integers, the labels array will be transformed so that labels are consecutive. In the multichannel case, labels should have the same shape as a single channel of data, i.e. without the final dimension denoting channels.  \nbetafloat, optional \n\nPenalization coefficient for the random walker motion (the greater beta, the more difficult the diffusion).  \nmodestring, available options {\u2018cg\u2019, \u2018cg_j\u2019, \u2018cg_mg\u2019, \u2018bf\u2019} \n\nMode for solving the linear system in the random walker algorithm.  \u2018bf\u2019 (brute force): an LU factorization of the Laplacian is computed. This is fast for small images (<1024x1024), but very slow and memory-intensive for large images (e.g., 3-D volumes). \u2018cg\u2019 (conjugate gradient): the linear system is solved iteratively using the Conjugate Gradient method from scipy.sparse.linalg. This is less memory-consuming than the brute force method for large images, but it is quite slow. \u2018cg_j\u2019 (conjugate gradient with Jacobi preconditionner): the Jacobi preconditionner is applyed during the Conjugate gradient method iterations. This may accelerate the convergence of the \u2018cg\u2019 method. \u2018cg_mg\u2019 (conjugate gradient with multigrid preconditioner): a preconditioner is computed using a multigrid solver, then the solution is computed with the Conjugate Gradient method. This mode requires that the pyamg module is installed.   \ntolfloat, optional \n\nTolerance to achieve when solving the linear system using the conjugate gradient based modes (\u2018cg\u2019, \u2018cg_j\u2019 and \u2018cg_mg\u2019).  \ncopybool, optional \n\nIf copy is False, the labels array will be overwritten with the result of the segmentation. Use copy=False if you want to save on memory.  \nmultichannelbool, optional \n\nIf True, input data is parsed as multichannel data (see \u2018data\u2019 above for proper input format in this case).  \nreturn_full_probbool, optional \n\nIf True, the probability that a pixel belongs to each of the labels will be returned, instead of only the most likely label.  \nspacingiterable of floats, optional \n\nSpacing between voxels in each spatial dimension. If None, then the spacing between pixels/voxels in each dimension is assumed 1.  \nprob_tolfloat, optional \n\nTolerance on the resulting probability to be in the interval [0, 1]. If the tolerance is not satisfied, a warning is displayed.    Returns \n \noutputndarray \n\n If return_full_prob is False, array of ints of same shape and data type as labels, in which each pixel has been labeled according to the marker that reached the pixel first by anisotropic diffusion. If return_full_prob is True, array of floats of shape (nlabels, labels.shape). output[label_nb, i, j] is the probability that label label_nb reaches the pixel (i, j) first.       See also  \nskimage.morphology.watershed\n\n\nwatershed segmentation A segmentation algorithm based on mathematical morphology and \u201cflooding\u201d of regions from markers.    Notes Multichannel inputs are scaled with all channel data combined. Ensure all channels are separately normalized prior to running this algorithm. The spacing argument is specifically for anisotropic datasets, where data points are spaced differently in one or more spatial dimensions. Anisotropic data is commonly encountered in medical imaging. The algorithm was first proposed in [1]. The algorithm solves the diffusion equation at infinite times for sources placed on markers of each phase in turn. A pixel is labeled with the phase that has the greatest probability to diffuse first to the pixel. The diffusion equation is solved by minimizing x.T L x for each phase, where L is the Laplacian of the weighted graph of the image, and x is the probability that a marker of the given phase arrives first at a pixel by diffusion (x=1 on markers of the phase, x=0 on the other markers, and the other coefficients are looked for). Each pixel is attributed the label for which it has a maximal value of x. The Laplacian L of the image is defined as:  L_ii = d_i, the number of neighbors of pixel i (the degree of i) L_ij = -w_ij if i and j are adjacent pixels  The weight w_ij is a decreasing function of the norm of the local gradient. This ensures that diffusion is easier between pixels of similar values. When the Laplacian is decomposed into blocks of marked and unmarked pixels: L = M B.T\n    B A\n with first indices corresponding to marked pixels, and then to unmarked pixels, minimizing x.T L x for one phase amount to solving: A x = - B x_m\n where x_m = 1 on markers of the given phase, and 0 on other markers. This linear system is solved in the algorithm using a direct method for small images, and an iterative method for larger images. References  \n1  \nLeo Grady, Random walks for image segmentation, IEEE Trans Pattern Anal Mach Intell. 2006 Nov;28(11):1768-83. DOI:10.1109/TPAMI.2006.233.   Examples >>> np.random.seed(0)\n>>> a = np.zeros((10, 10)) + 0.2 * np.random.rand(10, 10)\n>>> a[5:8, 5:8] += 1\n>>> b = np.zeros_like(a, dtype=np.int32)\n>>> b[3, 3] = 1  # Marker for first phase\n>>> b[6, 6] = 2  # Marker for second phase\n>>> random_walker(a, b)\narray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],\n       [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],\n       [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)\n \n relabel_sequential  \nskimage.segmentation.relabel_sequential(label_field, offset=1) [source]\n \nRelabel arbitrary labels to {offset, \u2026 offset + number_of_labels}. This function also returns the forward map (mapping the original labels to the reduced labels) and the inverse map (mapping the reduced labels back to the original ones).  Parameters \n \nlabel_fieldnumpy array of int, arbitrary shape \n\nAn array of labels, which must be non-negative integers.  \noffsetint, optional \n\nThe return labels will start at offset, which should be strictly positive.    Returns \n \nrelabelednumpy array of int, same shape as label_field \n\nThe input label field with labels mapped to {offset, \u2026, number_of_labels + offset - 1}. The data type will be the same as label_field, except when offset + number_of_labels causes overflow of the current data type.  \nforward_mapArrayMap \n\nThe map from the original label space to the returned label space. Can be used to re-apply the same mapping. See examples for usage. The output data type will be the same as relabeled.  \ninverse_mapArrayMap \n\nThe map from the new label space to the original space. This can be used to reconstruct the original label field from the relabeled one. The output data type will be the same as label_field.     Notes The label 0 is assumed to denote the background and is never remapped. The forward map can be extremely big for some inputs, since its length is given by the maximum of the label field. However, in most situations, label_field.max() is much smaller than label_field.size, and in these cases the forward map is guaranteed to be smaller than either the input or output images. Examples >>> from skimage.segmentation import relabel_sequential\n>>> label_field = np.array([1, 1, 5, 5, 8, 99, 42])\n>>> relab, fw, inv = relabel_sequential(label_field)\n>>> relab\narray([1, 1, 2, 2, 3, 5, 4])\n>>> print(fw)\nArrayMap:\n  1 \u2192 1\n  5 \u2192 2\n  8 \u2192 3\n  42 \u2192 4\n  99 \u2192 5\n>>> np.array(fw)\narray([0, 1, 0, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5])\n>>> np.array(inv)\narray([ 0,  1,  5,  8, 42, 99])\n>>> (fw[label_field] == relab).all()\nTrue\n>>> (inv[relab] == label_field).all()\nTrue\n>>> relab, fw, inv = relabel_sequential(label_field, offset=5)\n>>> relab\narray([5, 5, 6, 6, 7, 9, 8])\n \n slic  \nskimage.segmentation.slic(image, n_segments=100, compactness=10.0, max_iter=10, sigma=0, spacing=None, multichannel=True, convert2lab=None, enforce_connectivity=True, min_size_factor=0.5, max_size_factor=3, slic_zero=False, start_label=None, mask=None) [source]\n \nSegments image using k-means clustering in Color-(x,y,z) space.  Parameters \n \nimage2D, 3D or 4D ndarray \n\nInput image, which can be 2D or 3D, and grayscale or multichannel (see multichannel parameter). Input image must either be NaN-free or the NaN\u2019s must be masked out  \nn_segmentsint, optional \n\nThe (approximate) number of labels in the segmented output image.  \ncompactnessfloat, optional \n\nBalances color proximity and space proximity. Higher values give more weight to space proximity, making superpixel shapes more square/cubic. In SLICO mode, this is the initial compactness. This parameter depends strongly on image contrast and on the shapes of objects in the image. We recommend exploring possible values on a log scale, e.g., 0.01, 0.1, 1, 10, 100, before refining around a chosen value.  \nmax_iterint, optional \n\nMaximum number of iterations of k-means.  \nsigmafloat or (3,) array-like of floats, optional \n\nWidth of Gaussian smoothing kernel for pre-processing for each dimension of the image. The same sigma is applied to each dimension in case of a scalar value. Zero means no smoothing. Note, that sigma is automatically scaled if it is scalar and a manual voxel spacing is provided (see Notes section).  \nspacing(3,) array-like of floats, optional \n\nThe voxel spacing along each image dimension. By default, slic assumes uniform spacing (same voxel resolution along z, y and x). This parameter controls the weights of the distances along z, y, and x during k-means clustering.  \nmultichannelbool, optional \n\nWhether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.  \nconvert2labbool, optional \n\nWhether the input should be converted to Lab colorspace prior to segmentation. The input image must be RGB. Highly recommended. This option defaults to True when multichannel=True and image.shape[-1] == 3.  \nenforce_connectivitybool, optional \n\nWhether the generated segments are connected or not  \nmin_size_factorfloat, optional \n\nProportion of the minimum segment size to be removed with respect to the supposed segment size `depth*width*height/n_segments`  \nmax_size_factorfloat, optional \n\nProportion of the maximum connected segment size. A value of 3 works in most of the cases.  \nslic_zerobool, optional \n\nRun SLIC-zero, the zero-parameter mode of SLIC. [2]  start_label: int, optional\n\nThe labels\u2019 index start. Should be 0 or 1.  New in version 0.17: start_label was introduced in 0.17   \nmask2D ndarray, optional \n\nIf provided, superpixels are computed only where mask is True, and seed points are homogeneously distributed over the mask using a K-means clustering strategy.  New in version 0.17: mask was introduced in 0.17     Returns \n \nlabels2D or 3D array \n\nInteger mask indicating segment labels.    Raises \n ValueError\n\nIf convert2lab is set to True but the last array dimension is not of length 3.  ValueError\n\nIf start_label is not 0 or 1.     Notes  If sigma > 0, the image is smoothed using a Gaussian kernel prior to segmentation. If sigma is scalar and spacing is provided, the kernel width is divided along each dimension by the spacing. For example, if sigma=1 and spacing=[5, 1, 1], the effective sigma is [0.2, 1, 1]. This ensures sensible smoothing for anisotropic images. The image is rescaled to be in [0, 1] prior to processing. Images of shape (M, N, 3) are interpreted as 2D RGB images by default. To interpret them as 3D with the last dimension having length 3, use multichannel=False. \nstart_label is introduced to handle the issue [4]. The labels indexing starting at 0 will be deprecated in future versions. If mask is not None labels indexing starts at 1 and masked area is set to 0.  References  \n1  \nRadhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine S\u00fcsstrunk, SLIC Superpixels Compared to State-of-the-art Superpixel Methods, TPAMI, May 2012. DOI:10.1109/TPAMI.2012.120  \n2  \nhttps://www.epfl.ch/labs/ivrl/research/slic-superpixels/#SLICO  \n3  \nIrving, Benjamin. \u201cmaskSLIC: regional superpixel generation with application to local pathology characterisation in medical images.\u201d, 2016, arXiv:1606.09518  \n4  \nhttps://github.com/scikit-image/scikit-image/issues/3722   Examples >>> from skimage.segmentation import slic\n>>> from skimage.data import astronaut\n>>> img = astronaut()\n>>> segments = slic(img, n_segments=100, compactness=10)\n Increasing the compactness parameter yields more square regions: >>> segments = slic(img, n_segments=100, compactness=20)\n \n watershed  \nskimage.segmentation.watershed(image, markers=None, connectivity=1, offset=None, mask=None, compactness=0, watershed_line=False) [source]\n \nFind watershed basins in image flooded from given markers.  Parameters \n \nimagendarray (2-D, 3-D, \u2026) of integers \n\nData array where the lowest value points are labeled first.  \nmarkersint, or ndarray of int, same shape as image, optional \n\nThe desired number of markers, or an array marking the basins with the values to be assigned in the label matrix. Zero means not a marker. If None (no markers given), the local minima of the image are used as markers.  \nconnectivityndarray, optional \n\nAn array with the same number of dimensions as image whose non-zero elements indicate neighbors for connection. Following the scipy convention, default is a one-connected array of the dimension of the image.  \noffsetarray_like of shape image.ndim, optional \n\noffset of the connectivity (one offset per dimension)  \nmaskndarray of bools or 0s and 1s, optional \n\nArray of same shape as image. Only points at which mask == True will be labeled.  \ncompactnessfloat, optional \n\nUse compact watershed [3] with given compactness parameter. Higher values result in more regularly-shaped watershed basins.  \nwatershed_linebool, optional \n\nIf watershed_line is True, a one-pixel wide line separates the regions obtained by the watershed algorithm. The line has the label 0.    Returns \n \noutndarray \n\nA labeled matrix of the same type and shape as markers      See also  \nskimage.segmentation.random_walker\n\n\nrandom walker segmentation A segmentation algorithm based on anisotropic diffusion, usually slower than the watershed but with good results on noisy data and boundaries with holes.    Notes This function implements a watershed algorithm [1] [2] that apportions pixels into marked basins. The algorithm uses a priority queue to hold the pixels with the metric for the priority queue being pixel value, then the time of entry into the queue - this settles ties in favor of the closest marker. Some ideas taken from Soille, \u201cAutomated Basin Delineation from Digital Elevation Models Using Mathematical Morphology\u201d, Signal Processing 20 (1990) 171-182 The most important insight in the paper is that entry time onto the queue solves two problems: a pixel should be assigned to the neighbor with the largest gradient or, if there is no gradient, pixels on a plateau should be split between markers on opposite sides. This implementation converts all arguments to specific, lowest common denominator types, then passes these to a C algorithm. Markers can be determined manually, or automatically using for example the local minima of the gradient of the image, or the local maxima of the distance function to the background for separating overlapping objects (see example). References  \n1  \nhttps://en.wikipedia.org/wiki/Watershed_%28image_processing%29  \n2  \nhttp://cmm.ensmp.fr/~beucher/wtshed.html  \n3  \nPeer Neubert & Peter Protzel (2014). Compact Watershed and Preemptive SLIC: On Improving Trade-offs of Superpixel Segmentation Algorithms. ICPR 2014, pp 996-1001. DOI:10.1109/ICPR.2014.181 https://www.tu-chemnitz.de/etit/proaut/publications/cws_pSLIC_ICPR.pdf   Examples The watershed algorithm is useful to separate overlapping objects. We first generate an initial image with two overlapping circles: >>> x, y = np.indices((80, 80))\n>>> x1, y1, x2, y2 = 28, 28, 44, 52\n>>> r1, r2 = 16, 20\n>>> mask_circle1 = (x - x1)**2 + (y - y1)**2 < r1**2\n>>> mask_circle2 = (x - x2)**2 + (y - y2)**2 < r2**2\n>>> image = np.logical_or(mask_circle1, mask_circle2)\n Next, we want to separate the two circles. We generate markers at the maxima of the distance to the background: >>> from scipy import ndimage as ndi\n>>> distance = ndi.distance_transform_edt(image)\n>>> from skimage.feature import peak_local_max\n>>> local_maxi = peak_local_max(distance, labels=image,\n...                             footprint=np.ones((3, 3)),\n...                             indices=False)\n>>> markers = ndi.label(local_maxi)[0]\n Finally, we run the watershed on the image and markers: >>> labels = watershed(-distance, markers, mask=image)\n The algorithm works also for 3-D images, and can be used for example to separate overlapping spheres. \n Examples using skimage.segmentation.watershed\n \n  Watershed segmentation  \n\n  Markers for watershed transform  \n\n  Segment human cells (in mitosis)  \n"}, {"name": "segmentation.active_contour()", "path": "api/skimage.segmentation#skimage.segmentation.active_contour", "type": "segmentation", "text": " \nskimage.segmentation.active_contour(image, snake, alpha=0.01, beta=0.1, w_line=0, w_edge=1, gamma=0.01, max_px_move=1.0, max_iterations=2500, convergence=0.1, *, boundary_condition='periodic', coordinates='rc') [source]\n \nActive contour model. Active contours by fitting snakes to features of images. Supports single and multichannel 2D images. Snakes can be periodic (for segmentation) or have fixed and/or free ends. The output snake has the same length as the input boundary. As the number of points is constant, make sure that the initial snake has enough points to capture the details of the final contour.  Parameters \n \nimage(N, M) or (N, M, 3) ndarray \n\nInput image.  \nsnake(N, 2) ndarray \n\nInitial snake coordinates. For periodic boundary conditions, endpoints must not be duplicated.  \nalphafloat, optional \n\nSnake length shape parameter. Higher values makes snake contract faster.  \nbetafloat, optional \n\nSnake smoothness shape parameter. Higher values makes snake smoother.  \nw_linefloat, optional \n\nControls attraction to brightness. Use negative values to attract toward dark regions.  \nw_edgefloat, optional \n\nControls attraction to edges. Use negative values to repel snake from edges.  \ngammafloat, optional \n\nExplicit time stepping parameter.  \nmax_px_movefloat, optional \n\nMaximum pixel distance to move per iteration.  \nmax_iterationsint, optional \n\nMaximum iterations to optimize snake shape.  \nconvergencefloat, optional \n\nConvergence criteria.  \nboundary_conditionstring, optional \n\nBoundary conditions for the contour. Can be one of \u2018periodic\u2019, \u2018free\u2019, \u2018fixed\u2019, \u2018free-fixed\u2019, or \u2018fixed-free\u2019. \u2018periodic\u2019 attaches the two ends of the snake, \u2018fixed\u2019 holds the end-points in place, and \u2018free\u2019 allows free movement of the ends. \u2018fixed\u2019 and \u2018free\u2019 can be combined by parsing \u2018fixed-free\u2019, \u2018free-fixed\u2019. Parsing \u2018fixed-fixed\u2019 or \u2018free-free\u2019 yields same behaviour as \u2018fixed\u2019 and \u2018free\u2019, respectively.  \ncoordinates{\u2018rc\u2019}, optional \n\nThis option remains for compatibility purpose only and has no effect. It was introduced in 0.16 with the 'xy' option, but since 0.18, only the 'rc' option is valid. Coordinates must be set in a row-column format.    Returns \n \nsnake(N, 2) ndarray \n\nOptimised snake, same shape as input parameter.     References  \n1  \nKass, M.; Witkin, A.; Terzopoulos, D. \u201cSnakes: Active contour models\u201d. International Journal of Computer Vision 1 (4): 321 (1988). DOI:10.1007/BF00133570   Examples >>> from skimage.draw import circle_perimeter\n>>> from skimage.filters import gaussian\n Create and smooth image: >>> img = np.zeros((100, 100))\n>>> rr, cc = circle_perimeter(35, 45, 25)\n>>> img[rr, cc] = 1\n>>> img = gaussian(img, 2)\n Initialize spline: >>> s = np.linspace(0, 2*np.pi, 100)\n>>> init = 50 * np.array([np.sin(s), np.cos(s)]).T + 50\n Fit spline to image: >>> snake = active_contour(img, init, w_edge=0, w_line=1, coordinates='rc')  \n>>> dist = np.sqrt((45-snake[:, 0])**2 + (35-snake[:, 1])**2)  \n>>> int(np.mean(dist))  \n25\n \n"}, {"name": "segmentation.chan_vese()", "path": "api/skimage.segmentation#skimage.segmentation.chan_vese", "type": "segmentation", "text": " \nskimage.segmentation.chan_vese(image, mu=0.25, lambda1=1.0, lambda2=1.0, tol=0.001, max_iter=500, dt=0.5, init_level_set='checkerboard', extended_output=False) [source]\n \nChan-Vese segmentation algorithm. Active contour model by evolving a level set. Can be used to segment objects without clearly defined boundaries.  Parameters \n \nimage(M, N) ndarray \n\nGrayscale image to be segmented.  \nmufloat, optional \n\n\u2018edge length\u2019 weight parameter. Higher mu values will produce a \u2018round\u2019 edge, while values closer to zero will detect smaller objects.  \nlambda1float, optional \n\n\u2018difference from average\u2019 weight parameter for the output region with value \u2018True\u2019. If it is lower than lambda2, this region will have a larger range of values than the other.  \nlambda2float, optional \n\n\u2018difference from average\u2019 weight parameter for the output region with value \u2018False\u2019. If it is lower than lambda1, this region will have a larger range of values than the other.  \ntolfloat, positive, optional \n\nLevel set variation tolerance between iterations. If the L2 norm difference between the level sets of successive iterations normalized by the area of the image is below this value, the algorithm will assume that the solution was reached.  \nmax_iteruint, optional \n\nMaximum number of iterations allowed before the algorithm interrupts itself.  \ndtfloat, optional \n\nA multiplication factor applied at calculations for each step, serves to accelerate the algorithm. While higher values may speed up the algorithm, they may also lead to convergence problems.  \ninit_level_setstr or (M, N) ndarray, optional \n\nDefines the starting level set used by the algorithm. If a string is inputted, a level set that matches the image size will automatically be generated. Alternatively, it is possible to define a custom level set, which should be an array of float values, with the same shape as \u2018image\u2019. Accepted string values are as follows.  \u2018checkerboard\u2019\n\nthe starting level set is defined as sin(x/5*pi)*sin(y/5*pi), where x and y are pixel coordinates. This level set has fast convergence, but may fail to detect implicit edges.  \u2018disk\u2019\n\nthe starting level set is defined as the opposite of the distance from the center of the image minus half of the minimum value between image width and image height. This is somewhat slower, but is more likely to properly detect implicit edges.  \u2018small disk\u2019\n\nthe starting level set is defined as the opposite of the distance from the center of the image minus a quarter of the minimum value between image width and image height.    \nextended_outputbool, optional \n\nIf set to True, the return value will be a tuple containing the three return values (see below). If set to False which is the default value, only the \u2018segmentation\u2019 array will be returned.    Returns \n \nsegmentation(M, N) ndarray, bool \n\nSegmentation produced by the algorithm.  \nphi(M, N) ndarray of floats \n\nFinal level set computed by the algorithm.  \nenergieslist of floats \n\nShows the evolution of the \u2018energy\u2019 for each step of the algorithm. This should allow to check whether the algorithm converged.     Notes The Chan-Vese Algorithm is designed to segment objects without clearly defined boundaries. This algorithm is based on level sets that are evolved iteratively to minimize an energy, which is defined by weighted values corresponding to the sum of differences intensity from the average value outside the segmented region, the sum of differences from the average value inside the segmented region, and a term which is dependent on the length of the boundary of the segmented region. This algorithm was first proposed by Tony Chan and Luminita Vese, in a publication entitled \u201cAn Active Contour Model Without Edges\u201d [1]. This implementation of the algorithm is somewhat simplified in the sense that the area factor \u2018nu\u2019 described in the original paper is not implemented, and is only suitable for grayscale images. Typical values for lambda1 and lambda2 are 1. If the \u2018background\u2019 is very different from the segmented object in terms of distribution (for example, a uniform black image with figures of varying intensity), then these values should be different from each other. Typical values for mu are between 0 and 1, though higher values can be used when dealing with shapes with very ill-defined contours. The \u2018energy\u2019 which this algorithm tries to minimize is defined as the sum of the differences from the average within the region squared and weighed by the \u2018lambda\u2019 factors to which is added the length of the contour multiplied by the \u2018mu\u2019 factor. Supports 2D grayscale images only, and does not implement the area term described in the original article. References  \n1  \nAn Active Contour Model without Edges, Tony Chan and Luminita Vese, Scale-Space Theories in Computer Vision, 1999, DOI:10.1007/3-540-48236-9_13  \n2  \nChan-Vese Segmentation, Pascal Getreuer Image Processing On Line, 2 (2012), pp. 214-224, DOI:10.5201/ipol.2012.g-cv  \n3  \nThe Chan-Vese Algorithm - Project Report, Rami Cohen, 2011 arXiv:1107.2782   \n"}, {"name": "segmentation.checkerboard_level_set()", "path": "api/skimage.segmentation#skimage.segmentation.checkerboard_level_set", "type": "segmentation", "text": " \nskimage.segmentation.checkerboard_level_set(image_shape, square_size=5) [source]\n \nCreate a checkerboard level set with binary values.  Parameters \n \nimage_shapetuple of positive integers \n\nShape of the image.  \nsquare_sizeint, optional \n\nSize of the squares of the checkerboard. It defaults to 5.    Returns \n \noutarray with shape image_shape \n\nBinary level set of the checkerboard.      See also  \ncircle_level_set\n\n  \n"}, {"name": "segmentation.circle_level_set()", "path": "api/skimage.segmentation#skimage.segmentation.circle_level_set", "type": "segmentation", "text": " \nskimage.segmentation.circle_level_set(image_shape, center=None, radius=None) [source]\n \nCreate a circle level set with binary values.  Parameters \n \nimage_shapetuple of positive integers \n\nShape of the image  \ncentertuple of positive integers, optional \n\nCoordinates of the center of the circle given in (row, column). If not given, it defaults to the center of the image.  \nradiusfloat, optional \n\nRadius of the circle. If not given, it is set to the 75% of the smallest image dimension.    Returns \n \noutarray with shape image_shape \n\nBinary level set of the circle with the given radius and center.    Warns \n Deprecated:\n\n New in version 0.17: This function is deprecated and will be removed in scikit-image 0.19. Please use the function named disk_level_set instead.       See also  \ncheckerboard_level_set\n\n  \n"}, {"name": "segmentation.clear_border()", "path": "api/skimage.segmentation#skimage.segmentation.clear_border", "type": "segmentation", "text": " \nskimage.segmentation.clear_border(labels, buffer_size=0, bgval=0, in_place=False, mask=None) [source]\n \nClear objects connected to the label image border.  Parameters \n \nlabels(M[, N[, \u2026, P]]) array of int or bool \n\nImaging data labels.  \nbuffer_sizeint, optional \n\nThe width of the border examined. By default, only objects that touch the outside of the image are removed.  \nbgvalfloat or int, optional \n\nCleared objects are set to this value.  \nin_placebool, optional \n\nWhether or not to manipulate the labels array in-place.  \nmaskndarray of bool, same shape as image, optional. \n\nImage data mask. Objects in labels image overlapping with False pixels of mask will be removed. If defined, the argument buffer_size will be ignored.    Returns \n \nout(M[, N[, \u2026, P]]) array \n\nImaging data labels with cleared borders     Examples >>> import numpy as np\n>>> from skimage.segmentation import clear_border\n>>> labels = np.array([[0, 0, 0, 0, 0, 0, 0, 1, 0],\n...                    [1, 1, 0, 0, 1, 0, 0, 1, 0],\n...                    [1, 1, 0, 1, 0, 1, 0, 0, 0],\n...                    [0, 0, 0, 1, 1, 1, 1, 0, 0],\n...                    [0, 1, 1, 1, 1, 1, 1, 1, 0],\n...                    [0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> clear_border(labels)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0]])\n>>> mask = np.array([[0, 0, 1, 1, 1, 1, 1, 1, 1],\n...                  [0, 0, 1, 1, 1, 1, 1, 1, 1],\n...                  [1, 1, 1, 1, 1, 1, 1, 1, 1],\n...                  [1, 1, 1, 1, 1, 1, 1, 1, 1],\n...                  [1, 1, 1, 1, 1, 1, 1, 1, 1],\n...                  [1, 1, 1, 1, 1, 1, 1, 1, 1]]).astype(bool)\n>>> clear_border(labels, mask=mask)\narray([[0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 1, 0, 0, 1, 0],\n       [0, 0, 0, 1, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 1, 0, 0],\n       [0, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0]])\n \n"}, {"name": "segmentation.disk_level_set()", "path": "api/skimage.segmentation#skimage.segmentation.disk_level_set", "type": "segmentation", "text": " \nskimage.segmentation.disk_level_set(image_shape, *, center=None, radius=None) [source]\n \nCreate a disk level set with binary values.  Parameters \n \nimage_shapetuple of positive integers \n\nShape of the image  \ncentertuple of positive integers, optional \n\nCoordinates of the center of the disk given in (row, column). If not given, it defaults to the center of the image.  \nradiusfloat, optional \n\nRadius of the disk. If not given, it is set to the 75% of the smallest image dimension.    Returns \n \noutarray with shape image_shape \n\nBinary level set of the disk with the given radius and center.      See also  \ncheckerboard_level_set\n\n  \n"}, {"name": "segmentation.expand_labels()", "path": "api/skimage.segmentation#skimage.segmentation.expand_labels", "type": "segmentation", "text": " \nskimage.segmentation.expand_labels(label_image, distance=1) [source]\n \nExpand labels in label image by distance pixels without overlapping. Given a label image, expand_labels grows label regions (connected components) outwards by up to distance pixels without overflowing into neighboring regions. More specifically, each background pixel that is within Euclidean distance of <= distance pixels of a connected component is assigned the label of that connected component. Where multiple connected components are within distance pixels of a background pixel, the label value of the closest connected component will be assigned (see Notes for the case of multiple labels at equal distance).  Parameters \n \nlabel_imagendarray of dtype int \n\nlabel image  \ndistancefloat \n\nEuclidean distance in pixels by which to grow the labels. Default is one.    Returns \n \nenlarged_labelsndarray of dtype int \n\nLabeled array, where all connected regions have been enlarged      See also  \nskimage.measure.label(), skimage.segmentation.watershed(), skimage.morphology.dilation()\n\n  Notes Where labels are spaced more than distance pixels are apart, this is equivalent to a morphological dilation with a disc or hyperball of radius distance. However, in contrast to a morphological dilation, expand_labels will not expand a label region into a neighboring region. This implementation of expand_labels is derived from CellProfiler [1], where it is known as module \u201cIdentifySecondaryObjects (Distance-N)\u201d [2]. There is an important edge case when a pixel has the same distance to multiple regions, as it is not defined which region expands into that space. Here, the exact behavior depends on the upstream implementation of scipy.ndimage.distance_transform_edt. References  \n1  \nhttps://cellprofiler.org  \n2  \nhttps://github.com/CellProfiler/CellProfiler/blob/082930ea95add7b72243a4fa3d39ae5145995e9c/cellprofiler/modules/identifysecondaryobjects.py#L559   Examples >>> labels = np.array([0, 1, 0, 0, 0, 0, 2])\n>>> expand_labels(labels, distance=1)\narray([1, 1, 1, 0, 0, 2, 2])\n Labels will not overwrite each other: >>> expand_labels(labels, distance=3)\narray([1, 1, 1, 1, 2, 2, 2])\n In case of ties, behavior is undefined, but currently resolves to the label closest to (0,) * ndim in lexicographical order. >>> labels_tied = np.array([0, 1, 0, 2, 0])\n>>> expand_labels(labels_tied, 1)\narray([1, 1, 1, 2, 2])\n>>> labels2d = np.array(\n...     [[0, 1, 0, 0],\n...      [2, 0, 0, 0],\n...      [0, 3, 0, 0]]\n... )\n>>> expand_labels(labels2d, 1)\narray([[2, 1, 1, 0],\n       [2, 2, 0, 0],\n       [2, 3, 3, 0]])\n \n"}, {"name": "segmentation.felzenszwalb()", "path": "api/skimage.segmentation#skimage.segmentation.felzenszwalb", "type": "segmentation", "text": " \nskimage.segmentation.felzenszwalb(image, scale=1, sigma=0.8, min_size=20, multichannel=True) [source]\n \nComputes Felsenszwalb\u2019s efficient graph based image segmentation. Produces an oversegmentation of a multichannel (i.e. RGB) image using a fast, minimum spanning tree based clustering on the image grid. The parameter scale sets an observation level. Higher scale means less and larger segments. sigma is the diameter of a Gaussian kernel, used for smoothing the image prior to segmentation. The number of produced segments as well as their size can only be controlled indirectly through scale. Segment size within an image can vary greatly depending on local contrast. For RGB images, the algorithm uses the euclidean distance between pixels in color space.  Parameters \n \nimage(width, height, 3) or (width, height) ndarray \n\nInput image.  \nscalefloat \n\nFree parameter. Higher means larger clusters.  \nsigmafloat \n\nWidth (standard deviation) of Gaussian kernel used in preprocessing.  \nmin_sizeint \n\nMinimum component size. Enforced using postprocessing.  \nmultichannelbool, optional (default: True) \n\nWhether the last axis of the image is to be interpreted as multiple channels. A value of False, for a 3D image, is not currently supported.    Returns \n \nsegment_mask(width, height) ndarray \n\nInteger mask indicating segment labels.     Notes The k parameter used in the original paper renamed to scale here. References  \n1  \nEfficient graph-based image segmentation, Felzenszwalb, P.F. and Huttenlocher, D.P. International Journal of Computer Vision, 2004   Examples >>> from skimage.segmentation import felzenszwalb\n>>> from skimage.data import coffee\n>>> img = coffee()\n>>> segments = felzenszwalb(img, scale=3.0, sigma=0.95, min_size=5)\n \n"}, {"name": "segmentation.find_boundaries()", "path": "api/skimage.segmentation#skimage.segmentation.find_boundaries", "type": "segmentation", "text": " \nskimage.segmentation.find_boundaries(label_img, connectivity=1, mode='thick', background=0) [source]\n \nReturn bool array where boundaries between labeled regions are True.  Parameters \n \nlabel_imgarray of int or bool \n\nAn array in which different regions are labeled with either different integers or boolean values.  \nconnectivityint in {1, \u2026, label_img.ndim}, optional \n\nA pixel is considered a boundary pixel if any of its neighbors has a different label. connectivity controls which pixels are considered neighbors. A connectivity of 1 (default) means pixels sharing an edge (in 2D) or a face (in 3D) will be considered neighbors. A connectivity of label_img.ndim means pixels sharing a corner will be considered neighbors.  \nmodestring in {\u2018thick\u2019, \u2018inner\u2019, \u2018outer\u2019, \u2018subpixel\u2019} \n\nHow to mark the boundaries:  thick: any pixel not completely surrounded by pixels of the same label (defined by connectivity) is marked as a boundary. This results in boundaries that are 2 pixels thick. inner: outline the pixels just inside of objects, leaving background pixels untouched. outer: outline pixels in the background around object boundaries. When two objects touch, their boundary is also marked. subpixel: return a doubled image, with pixels between the original pixels marked as boundary where appropriate.   \nbackgroundint, optional \n\nFor modes \u2018inner\u2019 and \u2018outer\u2019, a definition of a background label is required. See mode for descriptions of these two.    Returns \n \nboundariesarray of bool, same shape as label_img \n\nA bool image where True represents a boundary pixel. For mode equal to \u2018subpixel\u2019, boundaries.shape[i] is equal to 2 * label_img.shape[i] - 1 for all i (a pixel is inserted in between all other pairs of pixels).     Examples >>> labels = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 5, 5, 5, 0, 0],\n...                    [0, 0, 1, 1, 1, 5, 5, 5, 0, 0],\n...                    [0, 0, 1, 1, 1, 5, 5, 5, 0, 0],\n...                    [0, 0, 1, 1, 1, 5, 5, 5, 0, 0],\n...                    [0, 0, 0, 0, 0, 5, 5, 5, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n...                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=np.uint8)\n>>> find_boundaries(labels, mode='thick').astype(np.uint8)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 1, 1, 1, 1, 1, 0, 1, 1, 0],\n       [0, 1, 1, 0, 1, 1, 0, 1, 1, 0],\n       [0, 1, 1, 1, 1, 1, 0, 1, 1, 0],\n       [0, 0, 1, 1, 1, 1, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n>>> find_boundaries(labels, mode='inner').astype(np.uint8)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0],\n       [0, 0, 1, 0, 1, 1, 0, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n>>> find_boundaries(labels, mode='outer').astype(np.uint8)\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 0, 1, 0],\n       [0, 1, 0, 0, 1, 1, 0, 0, 1, 0],\n       [0, 1, 0, 0, 1, 1, 0, 0, 1, 0],\n       [0, 1, 0, 0, 1, 1, 0, 0, 1, 0],\n       [0, 0, 1, 1, 1, 1, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n>>> labels_small = labels[::2, ::3]\n>>> labels_small\narray([[0, 0, 0, 0],\n       [0, 0, 5, 0],\n       [0, 1, 5, 0],\n       [0, 0, 5, 0],\n       [0, 0, 0, 0]], dtype=uint8)\n>>> find_boundaries(labels_small, mode='subpixel').astype(np.uint8)\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 1, 1, 0],\n       [0, 0, 0, 1, 0, 1, 0],\n       [0, 1, 1, 1, 0, 1, 0],\n       [0, 1, 0, 1, 0, 1, 0],\n       [0, 1, 1, 1, 0, 1, 0],\n       [0, 0, 0, 1, 0, 1, 0],\n       [0, 0, 0, 1, 1, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n>>> bool_image = np.array([[False, False, False, False, False],\n...                        [False, False, False, False, False],\n...                        [False, False,  True,  True,  True],\n...                        [False, False,  True,  True,  True],\n...                        [False, False,  True,  True,  True]],\n...                       dtype=bool)\n>>> find_boundaries(bool_image)\narray([[False, False, False, False, False],\n       [False, False,  True,  True,  True],\n       [False,  True,  True,  True,  True],\n       [False,  True,  True, False, False],\n       [False,  True,  True, False, False]])\n \n"}, {"name": "segmentation.flood()", "path": "api/skimage.segmentation#skimage.segmentation.flood", "type": "segmentation", "text": " \nskimage.segmentation.flood(image, seed_point, *, selem=None, connectivity=None, tolerance=None) [source]\n \nMask corresponding to a flood fill. Starting at a specific seed_point, connected points equal or within tolerance of the seed value are found.  Parameters \n \nimagendarray \n\nAn n-dimensional array.  \nseed_pointtuple or int \n\nThe point in image used as the starting point for the flood fill. If the image is 1D, this point may be given as an integer.  \nselemndarray, optional \n\nA structuring element used to determine the neighborhood of each evaluated pixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as image. If not given, all adjacent pixels are considered as part of the neighborhood (fully connected).  \nconnectivityint, optional \n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is larger or equal to connectivity are considered neighbors. Ignored if selem is not None.  \ntolerancefloat or int, optional \n\nIf None (default), adjacent values must be strictly equal to the initial value of image at seed_point. This is fastest. If a value is given, a comparison will be done at every point and if within tolerance of the initial value will also be filled (inclusive).    Returns \n \nmaskndarray \n\nA Boolean array with the same shape as image is returned, with True values for areas connected to and equal (or within tolerance of) the seed point. All other values are False.     Notes The conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many raster graphics programs. This function returns just the mask representing the fill. If indices are desired rather than masks for memory reasons, the user can simply run numpy.nonzero on the result, save the indices, and discard this mask. Examples >>> from skimage.morphology import flood\n>>> image = np.zeros((4, 7), dtype=int)\n>>> image[1:3, 1:3] = 1\n>>> image[3, 0] = 1\n>>> image[1:3, 4:6] = 2\n>>> image[3, 6] = 3\n>>> image\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 1, 0, 2, 2, 0],\n       [0, 1, 1, 0, 2, 2, 0],\n       [1, 0, 0, 0, 0, 0, 3]])\n Fill connected ones with 5, with full connectivity (diagonals included): >>> mask = flood(image, (1, 1))\n>>> image_flooded = image.copy()\n>>> image_flooded[mask] = 5\n>>> image_flooded\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [5, 0, 0, 0, 0, 0, 3]])\n Fill connected ones with 5, excluding diagonal points (connectivity 1): >>> mask = flood(image, (1, 1), connectivity=1)\n>>> image_flooded = image.copy()\n>>> image_flooded[mask] = 5\n>>> image_flooded\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [1, 0, 0, 0, 0, 0, 3]])\n Fill with a tolerance: >>> mask = flood(image, (0, 0), tolerance=1)\n>>> image_flooded = image.copy()\n>>> image_flooded[mask] = 5\n>>> image_flooded\narray([[5, 5, 5, 5, 5, 5, 5],\n       [5, 5, 5, 5, 2, 2, 5],\n       [5, 5, 5, 5, 2, 2, 5],\n       [5, 5, 5, 5, 5, 5, 3]])\n \n"}, {"name": "segmentation.flood_fill()", "path": "api/skimage.segmentation#skimage.segmentation.flood_fill", "type": "segmentation", "text": " \nskimage.segmentation.flood_fill(image, seed_point, new_value, *, selem=None, connectivity=None, tolerance=None, in_place=False, inplace=None) [source]\n \nPerform flood filling on an image. Starting at a specific seed_point, connected points equal or within tolerance of the seed value are found, then set to new_value.  Parameters \n \nimagendarray \n\nAn n-dimensional array.  \nseed_pointtuple or int \n\nThe point in image used as the starting point for the flood fill. If the image is 1D, this point may be given as an integer.  \nnew_valueimage type \n\nNew value to set the entire fill. This must be chosen in agreement with the dtype of image.  \nselemndarray, optional \n\nA structuring element used to determine the neighborhood of each evaluated pixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as image. If not given, all adjacent pixels are considered as part of the neighborhood (fully connected).  \nconnectivityint, optional \n\nA number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is less than or equal to connectivity are considered neighbors. Ignored if selem is not None.  \ntolerancefloat or int, optional \n\nIf None (default), adjacent values must be strictly equal to the value of image at seed_point to be filled. This is fastest. If a tolerance is provided, adjacent points with values within plus or minus tolerance from the seed point are filled (inclusive).  \nin_placebool, optional \n\nIf True, flood filling is applied to image in place. If False, the flood filled result is returned without modifying the input image (default).  \ninplacebool, optional \n\nThis parameter is deprecated and will be removed in version 0.19.0 in favor of in_place. If True, flood filling is applied to image inplace. If False, the flood filled result is returned without modifying the input image (default).    Returns \n \nfilledndarray \n\nAn array with the same shape as image is returned, with values in areas connected to and equal (or within tolerance of) the seed point replaced with new_value.     Notes The conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many raster graphics programs. Examples >>> from skimage.morphology import flood_fill\n>>> image = np.zeros((4, 7), dtype=int)\n>>> image[1:3, 1:3] = 1\n>>> image[3, 0] = 1\n>>> image[1:3, 4:6] = 2\n>>> image[3, 6] = 3\n>>> image\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 1, 0, 2, 2, 0],\n       [0, 1, 1, 0, 2, 2, 0],\n       [1, 0, 0, 0, 0, 0, 3]])\n Fill connected ones with 5, with full connectivity (diagonals included): >>> flood_fill(image, (1, 1), 5)\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [5, 0, 0, 0, 0, 0, 3]])\n Fill connected ones with 5, excluding diagonal points (connectivity 1): >>> flood_fill(image, (1, 1), 5, connectivity=1)\narray([[0, 0, 0, 0, 0, 0, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [0, 5, 5, 0, 2, 2, 0],\n       [1, 0, 0, 0, 0, 0, 3]])\n Fill with a tolerance: >>> flood_fill(image, (0, 0), 5, tolerance=1)\narray([[5, 5, 5, 5, 5, 5, 5],\n       [5, 5, 5, 5, 2, 2, 5],\n       [5, 5, 5, 5, 2, 2, 5],\n       [5, 5, 5, 5, 5, 5, 3]])\n \n"}, {"name": "segmentation.inverse_gaussian_gradient()", "path": "api/skimage.segmentation#skimage.segmentation.inverse_gaussian_gradient", "type": "segmentation", "text": " \nskimage.segmentation.inverse_gaussian_gradient(image, alpha=100.0, sigma=5.0) [source]\n \nInverse of gradient magnitude. Compute the magnitude of the gradients in the image and then inverts the result in the range [0, 1]. Flat areas are assigned values close to 1, while areas close to borders are assigned values close to 0. This function or a similar one defined by the user should be applied over the image as a preprocessing step before calling morphological_geodesic_active_contour.  Parameters \n \nimage(M, N) or (L, M, N) array \n\nGrayscale image or volume.  \nalphafloat, optional \n\nControls the steepness of the inversion. A larger value will make the transition between the flat areas and border areas steeper in the resulting array.  \nsigmafloat, optional \n\nStandard deviation of the Gaussian filter applied over the image.    Returns \n \ngimage(M, N) or (L, M, N) array \n\nPreprocessed image (or volume) suitable for morphological_geodesic_active_contour.     \n"}, {"name": "segmentation.join_segmentations()", "path": "api/skimage.segmentation#skimage.segmentation.join_segmentations", "type": "segmentation", "text": " \nskimage.segmentation.join_segmentations(s1, s2) [source]\n \nReturn the join of the two input segmentations. The join J of S1 and S2 is defined as the segmentation in which two voxels are in the same segment if and only if they are in the same segment in both S1 and S2.  Parameters \n \ns1, s2numpy arrays \n\ns1 and s2 are label fields of the same shape.    Returns \n \njnumpy array \n\nThe join segmentation of s1 and s2.     Examples >>> from skimage.segmentation import join_segmentations\n>>> s1 = np.array([[0, 0, 1, 1],\n...                [0, 2, 1, 1],\n...                [2, 2, 2, 1]])\n>>> s2 = np.array([[0, 1, 1, 0],\n...                [0, 1, 1, 0],\n...                [0, 1, 1, 1]])\n>>> join_segmentations(s1, s2)\narray([[0, 1, 3, 2],\n       [0, 5, 3, 2],\n       [4, 5, 5, 3]])\n \n"}, {"name": "segmentation.mark_boundaries()", "path": "api/skimage.segmentation#skimage.segmentation.mark_boundaries", "type": "segmentation", "text": " \nskimage.segmentation.mark_boundaries(image, label_img, color=(1, 1, 0), outline_color=None, mode='outer', background_label=0) [source]\n \nReturn image with boundaries between labeled regions highlighted.  Parameters \n \nimage(M, N[, 3]) array \n\nGrayscale or RGB image.  \nlabel_img(M, N) array of int \n\nLabel array where regions are marked by different integer values.  \ncolorlength-3 sequence, optional \n\nRGB color of boundaries in the output image.  \noutline_colorlength-3 sequence, optional \n\nRGB color surrounding boundaries in the output image. If None, no outline is drawn.  \nmodestring in {\u2018thick\u2019, \u2018inner\u2019, \u2018outer\u2019, \u2018subpixel\u2019}, optional \n\nThe mode for finding boundaries.  \nbackground_labelint, optional \n\nWhich label to consider background (this is only useful for modes inner and outer).    Returns \n \nmarked(M, N, 3) array of float \n\nAn image in which the boundaries between labels are superimposed on the original image.      See also  \nfind_boundaries\n\n  \n"}, {"name": "segmentation.morphological_chan_vese()", "path": "api/skimage.segmentation#skimage.segmentation.morphological_chan_vese", "type": "segmentation", "text": " \nskimage.segmentation.morphological_chan_vese(image, iterations, init_level_set='checkerboard', smoothing=1, lambda1=1, lambda2=1, iter_callback=<function <lambda>>) [source]\n \nMorphological Active Contours without Edges (MorphACWE) Active contours without edges implemented with morphological operators. It can be used to segment objects in images and volumes without well defined borders. It is required that the inside of the object looks different on average than the outside (i.e., the inner area of the object should be darker or lighter than the outer area on average).  Parameters \n \nimage(M, N) or (L, M, N) array \n\nGrayscale image or volume to be segmented.  \niterationsuint \n\nNumber of iterations to run  \ninit_level_setstr, (M, N) array, or (L, M, N) array \n\nInitial level set. If an array is given, it will be binarized and used as the initial level set. If a string is given, it defines the method to generate a reasonable initial level set with the shape of the image. Accepted values are \u2018checkerboard\u2019 and \u2018circle\u2019. See the documentation of checkerboard_level_set and circle_level_set respectively for details about how these level sets are created.  \nsmoothinguint, optional \n\nNumber of times the smoothing operator is applied per iteration. Reasonable values are around 1-4. Larger values lead to smoother segmentations.  \nlambda1float, optional \n\nWeight parameter for the outer region. If lambda1 is larger than lambda2, the outer region will contain a larger range of values than the inner region.  \nlambda2float, optional \n\nWeight parameter for the inner region. If lambda2 is larger than lambda1, the inner region will contain a larger range of values than the outer region.  \niter_callbackfunction, optional \n\nIf given, this function is called once per iteration with the current level set as the only argument. This is useful for debugging or for plotting intermediate results during the evolution.    Returns \n \nout(M, N) or (L, M, N) array \n\nFinal segmentation (i.e., the final level set)      See also  \ncircle_level_set, checkerboard_level_set\n\n  Notes This is a version of the Chan-Vese algorithm that uses morphological operators instead of solving a partial differential equation (PDE) for the evolution of the contour. The set of morphological operators used in this algorithm are proved to be infinitesimally equivalent to the Chan-Vese PDE (see [1]). However, morphological operators are do not suffer from the numerical stability issues typically found in PDEs (it is not necessary to find the right time step for the evolution), and are computationally faster. The algorithm and its theoretical derivation are described in [1]. References  \n1(1,2)  \nA Morphological Approach to Curvature-based Evolution of Curves and Surfaces, Pablo M\u00e1rquez-Neila, Luis Baumela, Luis \u00c1lvarez. In IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2014, DOI:10.1109/TPAMI.2013.106   \n"}, {"name": "segmentation.morphological_geodesic_active_contour()", "path": "api/skimage.segmentation#skimage.segmentation.morphological_geodesic_active_contour", "type": "segmentation", "text": " \nskimage.segmentation.morphological_geodesic_active_contour(gimage, iterations, init_level_set='circle', smoothing=1, threshold='auto', balloon=0, iter_callback=<function <lambda>>) [source]\n \nMorphological Geodesic Active Contours (MorphGAC). Geodesic active contours implemented with morphological operators. It can be used to segment objects with visible but noisy, cluttered, broken borders.  Parameters \n \ngimage(M, N) or (L, M, N) array \n\nPreprocessed image or volume to be segmented. This is very rarely the original image. Instead, this is usually a preprocessed version of the original image that enhances and highlights the borders (or other structures) of the object to segment. morphological_geodesic_active_contour will try to stop the contour evolution in areas where gimage is small. See morphsnakes.inverse_gaussian_gradient as an example function to perform this preprocessing. Note that the quality of morphological_geodesic_active_contour might greatly depend on this preprocessing.  \niterationsuint \n\nNumber of iterations to run.  \ninit_level_setstr, (M, N) array, or (L, M, N) array \n\nInitial level set. If an array is given, it will be binarized and used as the initial level set. If a string is given, it defines the method to generate a reasonable initial level set with the shape of the image. Accepted values are \u2018checkerboard\u2019 and \u2018circle\u2019. See the documentation of checkerboard_level_set and circle_level_set respectively for details about how these level sets are created.  \nsmoothinguint, optional \n\nNumber of times the smoothing operator is applied per iteration. Reasonable values are around 1-4. Larger values lead to smoother segmentations.  \nthresholdfloat, optional \n\nAreas of the image with a value smaller than this threshold will be considered borders. The evolution of the contour will stop in this areas.  \nballoonfloat, optional \n\nBalloon force to guide the contour in non-informative areas of the image, i.e., areas where the gradient of the image is too small to push the contour towards a border. A negative value will shrink the contour, while a positive value will expand the contour in these areas. Setting this to zero will disable the balloon force.  \niter_callbackfunction, optional \n\nIf given, this function is called once per iteration with the current level set as the only argument. This is useful for debugging or for plotting intermediate results during the evolution.    Returns \n \nout(M, N) or (L, M, N) array \n\nFinal segmentation (i.e., the final level set)      See also  \ninverse_gaussian_gradient, circle_level_set, checkerboard_level_set\n\n  Notes This is a version of the Geodesic Active Contours (GAC) algorithm that uses morphological operators instead of solving partial differential equations (PDEs) for the evolution of the contour. The set of morphological operators used in this algorithm are proved to be infinitesimally equivalent to the GAC PDEs (see [1]). However, morphological operators are do not suffer from the numerical stability issues typically found in PDEs (e.g., it is not necessary to find the right time step for the evolution), and are computationally faster. The algorithm and its theoretical derivation are described in [1]. References  \n1(1,2)  \nA Morphological Approach to Curvature-based Evolution of Curves and Surfaces, Pablo M\u00e1rquez-Neila, Luis Baumela, Luis \u00c1lvarez. In IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2014, DOI:10.1109/TPAMI.2013.106   \n"}, {"name": "segmentation.quickshift()", "path": "api/skimage.segmentation#skimage.segmentation.quickshift", "type": "segmentation", "text": " \nskimage.segmentation.quickshift(image, ratio=1.0, kernel_size=5, max_dist=10, return_tree=False, sigma=0, convert2lab=True, random_seed=42) [source]\n \nSegments image using quickshift clustering in Color-(x,y) space. Produces an oversegmentation of the image using the quickshift mode-seeking algorithm.  Parameters \n \nimage(width, height, channels) ndarray \n\nInput image.  \nratiofloat, optional, between 0 and 1 \n\nBalances color-space proximity and image-space proximity. Higher values give more weight to color-space.  \nkernel_sizefloat, optional \n\nWidth of Gaussian kernel used in smoothing the sample density. Higher means fewer clusters.  \nmax_distfloat, optional \n\nCut-off point for data distances. Higher means fewer clusters.  \nreturn_treebool, optional \n\nWhether to return the full segmentation hierarchy tree and distances.  \nsigmafloat, optional \n\nWidth for Gaussian smoothing as preprocessing. Zero means no smoothing.  \nconvert2labbool, optional \n\nWhether the input should be converted to Lab colorspace prior to segmentation. For this purpose, the input is assumed to be RGB.  \nrandom_seedint, optional \n\nRandom seed used for breaking ties.    Returns \n \nsegment_mask(width, height) ndarray \n\nInteger mask indicating segment labels.     Notes The authors advocate to convert the image to Lab color space prior to segmentation, though this is not strictly necessary. For this to work, the image must be given in RGB format. References  \n1  \nQuick shift and kernel methods for mode seeking, Vedaldi, A. and Soatto, S. European Conference on Computer Vision, 2008   \n"}, {"name": "segmentation.random_walker()", "path": "api/skimage.segmentation#skimage.segmentation.random_walker", "type": "segmentation", "text": " \nskimage.segmentation.random_walker(data, labels, beta=130, mode='cg_j', tol=0.001, copy=True, multichannel=False, return_full_prob=False, spacing=None, *, prob_tol=0.001) [source]\n \nRandom walker algorithm for segmentation from markers. Random walker algorithm is implemented for gray-level or multichannel images.  Parameters \n \ndataarray_like \n\nImage to be segmented in phases. Gray-level data can be two- or three-dimensional; multichannel data can be three- or four- dimensional (multichannel=True) with the highest dimension denoting channels. Data spacing is assumed isotropic unless the spacing keyword argument is used.  \nlabelsarray of ints, of same shape as data without channels dimension \n\nArray of seed markers labeled with different positive integers for different phases. Zero-labeled pixels are unlabeled pixels. Negative labels correspond to inactive pixels that are not taken into account (they are removed from the graph). If labels are not consecutive integers, the labels array will be transformed so that labels are consecutive. In the multichannel case, labels should have the same shape as a single channel of data, i.e. without the final dimension denoting channels.  \nbetafloat, optional \n\nPenalization coefficient for the random walker motion (the greater beta, the more difficult the diffusion).  \nmodestring, available options {\u2018cg\u2019, \u2018cg_j\u2019, \u2018cg_mg\u2019, \u2018bf\u2019} \n\nMode for solving the linear system in the random walker algorithm.  \u2018bf\u2019 (brute force): an LU factorization of the Laplacian is computed. This is fast for small images (<1024x1024), but very slow and memory-intensive for large images (e.g., 3-D volumes). \u2018cg\u2019 (conjugate gradient): the linear system is solved iteratively using the Conjugate Gradient method from scipy.sparse.linalg. This is less memory-consuming than the brute force method for large images, but it is quite slow. \u2018cg_j\u2019 (conjugate gradient with Jacobi preconditionner): the Jacobi preconditionner is applyed during the Conjugate gradient method iterations. This may accelerate the convergence of the \u2018cg\u2019 method. \u2018cg_mg\u2019 (conjugate gradient with multigrid preconditioner): a preconditioner is computed using a multigrid solver, then the solution is computed with the Conjugate Gradient method. This mode requires that the pyamg module is installed.   \ntolfloat, optional \n\nTolerance to achieve when solving the linear system using the conjugate gradient based modes (\u2018cg\u2019, \u2018cg_j\u2019 and \u2018cg_mg\u2019).  \ncopybool, optional \n\nIf copy is False, the labels array will be overwritten with the result of the segmentation. Use copy=False if you want to save on memory.  \nmultichannelbool, optional \n\nIf True, input data is parsed as multichannel data (see \u2018data\u2019 above for proper input format in this case).  \nreturn_full_probbool, optional \n\nIf True, the probability that a pixel belongs to each of the labels will be returned, instead of only the most likely label.  \nspacingiterable of floats, optional \n\nSpacing between voxels in each spatial dimension. If None, then the spacing between pixels/voxels in each dimension is assumed 1.  \nprob_tolfloat, optional \n\nTolerance on the resulting probability to be in the interval [0, 1]. If the tolerance is not satisfied, a warning is displayed.    Returns \n \noutputndarray \n\n If return_full_prob is False, array of ints of same shape and data type as labels, in which each pixel has been labeled according to the marker that reached the pixel first by anisotropic diffusion. If return_full_prob is True, array of floats of shape (nlabels, labels.shape). output[label_nb, i, j] is the probability that label label_nb reaches the pixel (i, j) first.       See also  \nskimage.morphology.watershed\n\n\nwatershed segmentation A segmentation algorithm based on mathematical morphology and \u201cflooding\u201d of regions from markers.    Notes Multichannel inputs are scaled with all channel data combined. Ensure all channels are separately normalized prior to running this algorithm. The spacing argument is specifically for anisotropic datasets, where data points are spaced differently in one or more spatial dimensions. Anisotropic data is commonly encountered in medical imaging. The algorithm was first proposed in [1]. The algorithm solves the diffusion equation at infinite times for sources placed on markers of each phase in turn. A pixel is labeled with the phase that has the greatest probability to diffuse first to the pixel. The diffusion equation is solved by minimizing x.T L x for each phase, where L is the Laplacian of the weighted graph of the image, and x is the probability that a marker of the given phase arrives first at a pixel by diffusion (x=1 on markers of the phase, x=0 on the other markers, and the other coefficients are looked for). Each pixel is attributed the label for which it has a maximal value of x. The Laplacian L of the image is defined as:  L_ii = d_i, the number of neighbors of pixel i (the degree of i) L_ij = -w_ij if i and j are adjacent pixels  The weight w_ij is a decreasing function of the norm of the local gradient. This ensures that diffusion is easier between pixels of similar values. When the Laplacian is decomposed into blocks of marked and unmarked pixels: L = M B.T\n    B A\n with first indices corresponding to marked pixels, and then to unmarked pixels, minimizing x.T L x for one phase amount to solving: A x = - B x_m\n where x_m = 1 on markers of the given phase, and 0 on other markers. This linear system is solved in the algorithm using a direct method for small images, and an iterative method for larger images. References  \n1  \nLeo Grady, Random walks for image segmentation, IEEE Trans Pattern Anal Mach Intell. 2006 Nov;28(11):1768-83. DOI:10.1109/TPAMI.2006.233.   Examples >>> np.random.seed(0)\n>>> a = np.zeros((10, 10)) + 0.2 * np.random.rand(10, 10)\n>>> a[5:8, 5:8] += 1\n>>> b = np.zeros_like(a, dtype=np.int32)\n>>> b[3, 3] = 1  # Marker for first phase\n>>> b[6, 6] = 2  # Marker for second phase\n>>> random_walker(a, b)\narray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],\n       [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],\n       [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)\n \n"}, {"name": "segmentation.relabel_sequential()", "path": "api/skimage.segmentation#skimage.segmentation.relabel_sequential", "type": "segmentation", "text": " \nskimage.segmentation.relabel_sequential(label_field, offset=1) [source]\n \nRelabel arbitrary labels to {offset, \u2026 offset + number_of_labels}. This function also returns the forward map (mapping the original labels to the reduced labels) and the inverse map (mapping the reduced labels back to the original ones).  Parameters \n \nlabel_fieldnumpy array of int, arbitrary shape \n\nAn array of labels, which must be non-negative integers.  \noffsetint, optional \n\nThe return labels will start at offset, which should be strictly positive.    Returns \n \nrelabelednumpy array of int, same shape as label_field \n\nThe input label field with labels mapped to {offset, \u2026, number_of_labels + offset - 1}. The data type will be the same as label_field, except when offset + number_of_labels causes overflow of the current data type.  \nforward_mapArrayMap \n\nThe map from the original label space to the returned label space. Can be used to re-apply the same mapping. See examples for usage. The output data type will be the same as relabeled.  \ninverse_mapArrayMap \n\nThe map from the new label space to the original space. This can be used to reconstruct the original label field from the relabeled one. The output data type will be the same as label_field.     Notes The label 0 is assumed to denote the background and is never remapped. The forward map can be extremely big for some inputs, since its length is given by the maximum of the label field. However, in most situations, label_field.max() is much smaller than label_field.size, and in these cases the forward map is guaranteed to be smaller than either the input or output images. Examples >>> from skimage.segmentation import relabel_sequential\n>>> label_field = np.array([1, 1, 5, 5, 8, 99, 42])\n>>> relab, fw, inv = relabel_sequential(label_field)\n>>> relab\narray([1, 1, 2, 2, 3, 5, 4])\n>>> print(fw)\nArrayMap:\n  1 \u2192 1\n  5 \u2192 2\n  8 \u2192 3\n  42 \u2192 4\n  99 \u2192 5\n>>> np.array(fw)\narray([0, 1, 0, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5])\n>>> np.array(inv)\narray([ 0,  1,  5,  8, 42, 99])\n>>> (fw[label_field] == relab).all()\nTrue\n>>> (inv[relab] == label_field).all()\nTrue\n>>> relab, fw, inv = relabel_sequential(label_field, offset=5)\n>>> relab\narray([5, 5, 6, 6, 7, 9, 8])\n \n"}, {"name": "segmentation.slic()", "path": "api/skimage.segmentation#skimage.segmentation.slic", "type": "segmentation", "text": " \nskimage.segmentation.slic(image, n_segments=100, compactness=10.0, max_iter=10, sigma=0, spacing=None, multichannel=True, convert2lab=None, enforce_connectivity=True, min_size_factor=0.5, max_size_factor=3, slic_zero=False, start_label=None, mask=None) [source]\n \nSegments image using k-means clustering in Color-(x,y,z) space.  Parameters \n \nimage2D, 3D or 4D ndarray \n\nInput image, which can be 2D or 3D, and grayscale or multichannel (see multichannel parameter). Input image must either be NaN-free or the NaN\u2019s must be masked out  \nn_segmentsint, optional \n\nThe (approximate) number of labels in the segmented output image.  \ncompactnessfloat, optional \n\nBalances color proximity and space proximity. Higher values give more weight to space proximity, making superpixel shapes more square/cubic. In SLICO mode, this is the initial compactness. This parameter depends strongly on image contrast and on the shapes of objects in the image. We recommend exploring possible values on a log scale, e.g., 0.01, 0.1, 1, 10, 100, before refining around a chosen value.  \nmax_iterint, optional \n\nMaximum number of iterations of k-means.  \nsigmafloat or (3,) array-like of floats, optional \n\nWidth of Gaussian smoothing kernel for pre-processing for each dimension of the image. The same sigma is applied to each dimension in case of a scalar value. Zero means no smoothing. Note, that sigma is automatically scaled if it is scalar and a manual voxel spacing is provided (see Notes section).  \nspacing(3,) array-like of floats, optional \n\nThe voxel spacing along each image dimension. By default, slic assumes uniform spacing (same voxel resolution along z, y and x). This parameter controls the weights of the distances along z, y, and x during k-means clustering.  \nmultichannelbool, optional \n\nWhether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.  \nconvert2labbool, optional \n\nWhether the input should be converted to Lab colorspace prior to segmentation. The input image must be RGB. Highly recommended. This option defaults to True when multichannel=True and image.shape[-1] == 3.  \nenforce_connectivitybool, optional \n\nWhether the generated segments are connected or not  \nmin_size_factorfloat, optional \n\nProportion of the minimum segment size to be removed with respect to the supposed segment size `depth*width*height/n_segments`  \nmax_size_factorfloat, optional \n\nProportion of the maximum connected segment size. A value of 3 works in most of the cases.  \nslic_zerobool, optional \n\nRun SLIC-zero, the zero-parameter mode of SLIC. [2]  start_label: int, optional\n\nThe labels\u2019 index start. Should be 0 or 1.  New in version 0.17: start_label was introduced in 0.17   \nmask2D ndarray, optional \n\nIf provided, superpixels are computed only where mask is True, and seed points are homogeneously distributed over the mask using a K-means clustering strategy.  New in version 0.17: mask was introduced in 0.17     Returns \n \nlabels2D or 3D array \n\nInteger mask indicating segment labels.    Raises \n ValueError\n\nIf convert2lab is set to True but the last array dimension is not of length 3.  ValueError\n\nIf start_label is not 0 or 1.     Notes  If sigma > 0, the image is smoothed using a Gaussian kernel prior to segmentation. If sigma is scalar and spacing is provided, the kernel width is divided along each dimension by the spacing. For example, if sigma=1 and spacing=[5, 1, 1], the effective sigma is [0.2, 1, 1]. This ensures sensible smoothing for anisotropic images. The image is rescaled to be in [0, 1] prior to processing. Images of shape (M, N, 3) are interpreted as 2D RGB images by default. To interpret them as 3D with the last dimension having length 3, use multichannel=False. \nstart_label is introduced to handle the issue [4]. The labels indexing starting at 0 will be deprecated in future versions. If mask is not None labels indexing starts at 1 and masked area is set to 0.  References  \n1  \nRadhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine S\u00fcsstrunk, SLIC Superpixels Compared to State-of-the-art Superpixel Methods, TPAMI, May 2012. DOI:10.1109/TPAMI.2012.120  \n2  \nhttps://www.epfl.ch/labs/ivrl/research/slic-superpixels/#SLICO  \n3  \nIrving, Benjamin. \u201cmaskSLIC: regional superpixel generation with application to local pathology characterisation in medical images.\u201d, 2016, arXiv:1606.09518  \n4  \nhttps://github.com/scikit-image/scikit-image/issues/3722   Examples >>> from skimage.segmentation import slic\n>>> from skimage.data import astronaut\n>>> img = astronaut()\n>>> segments = slic(img, n_segments=100, compactness=10)\n Increasing the compactness parameter yields more square regions: >>> segments = slic(img, n_segments=100, compactness=20)\n \n"}, {"name": "segmentation.watershed()", "path": "api/skimage.segmentation#skimage.segmentation.watershed", "type": "segmentation", "text": " \nskimage.segmentation.watershed(image, markers=None, connectivity=1, offset=None, mask=None, compactness=0, watershed_line=False) [source]\n \nFind watershed basins in image flooded from given markers.  Parameters \n \nimagendarray (2-D, 3-D, \u2026) of integers \n\nData array where the lowest value points are labeled first.  \nmarkersint, or ndarray of int, same shape as image, optional \n\nThe desired number of markers, or an array marking the basins with the values to be assigned in the label matrix. Zero means not a marker. If None (no markers given), the local minima of the image are used as markers.  \nconnectivityndarray, optional \n\nAn array with the same number of dimensions as image whose non-zero elements indicate neighbors for connection. Following the scipy convention, default is a one-connected array of the dimension of the image.  \noffsetarray_like of shape image.ndim, optional \n\noffset of the connectivity (one offset per dimension)  \nmaskndarray of bools or 0s and 1s, optional \n\nArray of same shape as image. Only points at which mask == True will be labeled.  \ncompactnessfloat, optional \n\nUse compact watershed [3] with given compactness parameter. Higher values result in more regularly-shaped watershed basins.  \nwatershed_linebool, optional \n\nIf watershed_line is True, a one-pixel wide line separates the regions obtained by the watershed algorithm. The line has the label 0.    Returns \n \noutndarray \n\nA labeled matrix of the same type and shape as markers      See also  \nskimage.segmentation.random_walker\n\n\nrandom walker segmentation A segmentation algorithm based on anisotropic diffusion, usually slower than the watershed but with good results on noisy data and boundaries with holes.    Notes This function implements a watershed algorithm [1] [2] that apportions pixels into marked basins. The algorithm uses a priority queue to hold the pixels with the metric for the priority queue being pixel value, then the time of entry into the queue - this settles ties in favor of the closest marker. Some ideas taken from Soille, \u201cAutomated Basin Delineation from Digital Elevation Models Using Mathematical Morphology\u201d, Signal Processing 20 (1990) 171-182 The most important insight in the paper is that entry time onto the queue solves two problems: a pixel should be assigned to the neighbor with the largest gradient or, if there is no gradient, pixels on a plateau should be split between markers on opposite sides. This implementation converts all arguments to specific, lowest common denominator types, then passes these to a C algorithm. Markers can be determined manually, or automatically using for example the local minima of the gradient of the image, or the local maxima of the distance function to the background for separating overlapping objects (see example). References  \n1  \nhttps://en.wikipedia.org/wiki/Watershed_%28image_processing%29  \n2  \nhttp://cmm.ensmp.fr/~beucher/wtshed.html  \n3  \nPeer Neubert & Peter Protzel (2014). Compact Watershed and Preemptive SLIC: On Improving Trade-offs of Superpixel Segmentation Algorithms. ICPR 2014, pp 996-1001. DOI:10.1109/ICPR.2014.181 https://www.tu-chemnitz.de/etit/proaut/publications/cws_pSLIC_ICPR.pdf   Examples The watershed algorithm is useful to separate overlapping objects. We first generate an initial image with two overlapping circles: >>> x, y = np.indices((80, 80))\n>>> x1, y1, x2, y2 = 28, 28, 44, 52\n>>> r1, r2 = 16, 20\n>>> mask_circle1 = (x - x1)**2 + (y - y1)**2 < r1**2\n>>> mask_circle2 = (x - x2)**2 + (y - y2)**2 < r2**2\n>>> image = np.logical_or(mask_circle1, mask_circle2)\n Next, we want to separate the two circles. We generate markers at the maxima of the distance to the background: >>> from scipy import ndimage as ndi\n>>> distance = ndi.distance_transform_edt(image)\n>>> from skimage.feature import peak_local_max\n>>> local_maxi = peak_local_max(distance, labels=image,\n...                             footprint=np.ones((3, 3)),\n...                             indices=False)\n>>> markers = ndi.label(local_maxi)[0]\n Finally, we run the watershed on the image and markers: >>> labels = watershed(-distance, markers, mask=image)\n The algorithm works also for 3-D images, and can be used for example to separate overlapping spheres. \n"}, {"name": "skimage", "path": "api/skimage", "type": "skimage", "text": "skimage Image Processing for Python scikit-image (a.k.a. skimage) is a collection of algorithms for image processing and computer vision. The main package of skimage only provides a few utilities for converting between image data types; for most features, you need to import one of the following subpackages: Subpackages  color\n\nColor space conversion.  data\n\nTest images and example data.  draw\n\nDrawing primitives (lines, text, etc.) that operate on NumPy arrays.  exposure\n\nImage intensity adjustment, e.g., histogram equalization, etc.  feature\n\nFeature detection and extraction, e.g., texture analysis corners, etc.  filters\n\nSharpening, edge finding, rank filters, thresholding, etc.  graph\n\nGraph-theoretic operations, e.g., shortest paths.  io\n\nReading, saving, and displaying images and video.  measure\n\nMeasurement of image properties, e.g., region properties and contours.  metrics\n\nMetrics corresponding to images, e.g. distance metrics, similarity, etc.  morphology\n\nMorphological operations, e.g., opening or skeletonization.  restoration\n\nRestoration algorithms, e.g., deconvolution algorithms, denoising, etc.  segmentation\n\nPartitioning an image into multiple regions.  transform\n\nGeometric and other transforms, e.g., rotation or the Radon transform.  util\n\nGeneric utilities.  viewer\n\nA simple graphical user interface for visualizing results and exploring parameters.   Utility Functions  img_as_float\n\nConvert an image to floating point format, with values in [0, 1]. Is similar to img_as_float64, but will not convert lower-precision floating point arrays to float64.  img_as_float32\n\nConvert an image to single-precision (32-bit) floating point format, with values in [0, 1].  img_as_float64\n\nConvert an image to double-precision (64-bit) floating point format, with values in [0, 1].  img_as_uint\n\nConvert an image to unsigned integer format, with values in [0, 65535].  img_as_int\n\nConvert an image to signed integer format, with values in [-32768, 32767].  img_as_ubyte\n\nConvert an image to unsigned byte format, with values in [0, 255].  img_as_bool\n\nConvert an image to boolean format, with values either True or False.  dtype_limits\n\nReturn intensity limits, i.e. (min, max) tuple, of the image\u2019s dtype.    \nskimage.dtype_limits(image[, clip_negative]) Return intensity limits, i.e.  \nskimage.ensure_python_version(min_version)   \nskimage.img_as_bool(image[, force_copy]) Convert an image to boolean format.  \nskimage.img_as_float(image[, force_copy]) Convert an image to floating point format.  \nskimage.img_as_float32(image[, force_copy]) Convert an image to single-precision (32-bit) floating point format.  \nskimage.img_as_float64(image[, force_copy]) Convert an image to double-precision (64-bit) floating point format.  \nskimage.img_as_int(image[, force_copy]) Convert an image to 16-bit signed integer format.  \nskimage.img_as_ubyte(image[, force_copy]) Convert an image to 8-bit unsigned integer format.  \nskimage.img_as_uint(image[, force_copy]) Convert an image to 16-bit unsigned integer format.  \nskimage.lookfor(what) Do a keyword search on scikit-image docstrings.  \nskimage.data Standard test images.  \nskimage.util    dtype_limits  \nskimage.dtype_limits(image, clip_negative=False) [source]\n \nReturn intensity limits, i.e. (min, max) tuple, of the image\u2019s dtype.  Parameters \n \nimagendarray \n\nInput image.  \nclip_negativebool, optional \n\nIf True, clip the negative range (i.e. return 0 for min intensity) even if the image dtype allows negative values.    Returns \n \nimin, imaxtuple \n\nLower and upper intensity limits.     \n ensure_python_version  \nskimage.ensure_python_version(min_version) [source]\n\n img_as_bool  \nskimage.img_as_bool(image, force_copy=False) [source]\n \nConvert an image to boolean format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of bool (bool_) \n\nOutput image.     Notes The upper half of the input dtype\u2019s positive range is True, and the lower half is False. All negative values (if present) are False. \n img_as_float  \nskimage.img_as_float(image, force_copy=False) [source]\n \nConvert an image to floating point format. This function is similar to img_as_float64, but will not convert lower-precision floating point arrays to float64.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of float \n\nOutput image.     Notes The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0]. \n Examples using skimage.img_as_float\n \n  Tinting gray-scale images  \n\n  3D adaptive histogram equalization  \n\n  Phase Unwrapping  \n\n  Finding local maxima  \n\n  Use rolling-ball algorithm for estimating background intensity  \n\n  Explore 3D images (of cells)   img_as_float32  \nskimage.img_as_float32(image, force_copy=False) [source]\n \nConvert an image to single-precision (32-bit) floating point format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of float32 \n\nOutput image.     Notes The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0]. \n img_as_float64  \nskimage.img_as_float64(image, force_copy=False) [source]\n \nConvert an image to double-precision (64-bit) floating point format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of float64 \n\nOutput image.     Notes The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0]. \n img_as_int  \nskimage.img_as_int(image, force_copy=False) [source]\n \nConvert an image to 16-bit signed integer format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of int16 \n\nOutput image.     Notes The values are scaled between -32768 and 32767. If the input data-type is positive-only (e.g., uint8), then the output image will still only have positive values. \n img_as_ubyte  \nskimage.img_as_ubyte(image, force_copy=False) [source]\n \nConvert an image to 8-bit unsigned integer format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of ubyte (uint8) \n\nOutput image.     Notes Negative input values will be clipped. Positive values are scaled between 0 and 255. \n Examples using skimage.img_as_ubyte\n \n  Local Histogram Equalization  \n\n  Entropy  \n\n  Markers for watershed transform  \n\n  Segment human cells (in mitosis)  \n\n  Rank filters   img_as_uint  \nskimage.img_as_uint(image, force_copy=False) [source]\n \nConvert an image to 16-bit unsigned integer format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of uint16 \n\nOutput image.     Notes Negative input values will be clipped. Positive values are scaled between 0 and 65535. \n lookfor  \nskimage.lookfor(what) [source]\n \nDo a keyword search on scikit-image docstrings.  Parameters \n \nwhatstr \n\nWords to look for.     Examples >>> import skimage\n>>> skimage.lookfor('regular_grid')\nSearch results for 'regular_grid'\n---------------------------------\nskimage.lookfor\n    Do a keyword search on scikit-image docstrings.\nskimage.util.regular_grid\n    Find `n_points` regularly spaced along `ar_shape`.\n \n\n"}, {"name": "transform", "path": "api/skimage.transform", "type": "transform", "text": "Module: transform  \nskimage.transform.downscale_local_mean(\u2026) Down-sample N-dimensional image by local averaging.  \nskimage.transform.estimate_transform(ttype, \u2026) Estimate 2D geometric transformation parameters.  \nskimage.transform.frt2(a) Compute the 2-dimensional finite radon transform (FRT) for an n x n integer array.  \nskimage.transform.hough_circle(image, radius) Perform a circular Hough transform.  \nskimage.transform.hough_circle_peaks(\u2026[, \u2026]) Return peaks in a circle Hough transform.  \nskimage.transform.hough_ellipse(image[, \u2026]) Perform an elliptical Hough transform.  \nskimage.transform.hough_line(image[, theta]) Perform a straight line Hough transform.  \nskimage.transform.hough_line_peaks(hspace, \u2026) Return peaks in a straight line Hough transform.  \nskimage.transform.ifrt2(a) Compute the 2-dimensional inverse finite radon transform (iFRT) for an (n+1) x n integer array.  \nskimage.transform.integral_image(image) Integral image / summed area table.  \nskimage.transform.integrate(ii, start, end) Use an integral image to integrate over a given window.  \nskimage.transform.iradon(radon_image[, \u2026]) Inverse radon transform.  \nskimage.transform.iradon_sart(radon_image[, \u2026]) Inverse radon transform.  \nskimage.transform.matrix_transform(coords, \u2026) Apply 2D matrix transform.  \nskimage.transform.order_angles_golden_ratio(theta) Order angles to reduce the amount of correlated information in subsequent projections.  \nskimage.transform.probabilistic_hough_line(image) Return lines from a progressive probabilistic line Hough transform.  \nskimage.transform.pyramid_expand(image[, \u2026]) Upsample and then smooth image.  \nskimage.transform.pyramid_gaussian(image[, \u2026]) Yield images of the Gaussian pyramid formed by the input image.  \nskimage.transform.pyramid_laplacian(image[, \u2026]) Yield images of the laplacian pyramid formed by the input image.  \nskimage.transform.pyramid_reduce(image[, \u2026]) Smooth and then downsample image.  \nskimage.transform.radon(image[, theta, \u2026]) Calculates the radon transform of an image given specified projection angles.  \nskimage.transform.rescale(image, scale[, \u2026]) Scale image by a certain factor.  \nskimage.transform.resize(image, output_shape) Resize image to match a certain size.  \nskimage.transform.rotate(image, angle[, \u2026]) Rotate image by a certain angle around its center.  \nskimage.transform.swirl(image[, center, \u2026]) Perform a swirl transformation.  \nskimage.transform.warp(image, inverse_map[, \u2026]) Warp an image according to a given coordinate transformation.  \nskimage.transform.warp_coords(coord_map, shape) Build the source coordinates for the output of a 2-D image warp.  \nskimage.transform.warp_polar(image[, \u2026]) Remap image to polar or log-polar coordinates space.  \nskimage.transform.AffineTransform([matrix, \u2026]) 2D affine transformation.  \nskimage.transform.EssentialMatrixTransform([\u2026]) Essential matrix transformation.  \nskimage.transform.EuclideanTransform([\u2026]) 2D Euclidean transformation.  \nskimage.transform.FundamentalMatrixTransform([\u2026]) Fundamental matrix transformation.  \nskimage.transform.PiecewiseAffineTransform() 2D piecewise affine transformation.  \nskimage.transform.PolynomialTransform([params]) 2D polynomial transformation.  \nskimage.transform.ProjectiveTransform([matrix]) Projective transformation.  \nskimage.transform.SimilarityTransform([\u2026]) 2D similarity transformation.   downscale_local_mean  \nskimage.transform.downscale_local_mean(image, factors, cval=0, clip=True) [source]\n \nDown-sample N-dimensional image by local averaging. The image is padded with cval if it is not perfectly divisible by the integer factors. In contrast to interpolation in skimage.transform.resize and skimage.transform.rescale this function calculates the local mean of elements in each block of size factors in the input image.  Parameters \n \nimagendarray \n\nN-dimensional input image.  \nfactorsarray_like \n\nArray containing down-sampling integer factor along each axis.  \ncvalfloat, optional \n\nConstant padding value if image is not perfectly divisible by the integer factors.  \nclipbool, optional \n\nUnused, but kept here for API consistency with the other transforms in this module. (The local mean will never fall outside the range of values in the input image, assuming the provided cval also falls within that range.)    Returns \n \nimagendarray \n\nDown-sampled image with same number of dimensions as input image. For integer inputs, the output dtype will be float64. See numpy.mean() for details.     Examples >>> a = np.arange(15).reshape(3, 5)\n>>> a\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])\n>>> downscale_local_mean(a, (2, 3))\narray([[3.5, 4. ],\n       [5.5, 4.5]])\n \n estimate_transform  \nskimage.transform.estimate_transform(ttype, src, dst, **kwargs) [source]\n \nEstimate 2D geometric transformation parameters. You can determine the over-, well- and under-determined parameters with the total least-squares method. Number of source and destination coordinates must match.  Parameters \n \nttype{\u2018euclidean\u2019, similarity\u2019, \u2018affine\u2019, \u2018piecewise-affine\u2019, \u2018projective\u2019, \u2018polynomial\u2019} \n\nType of transform.  \nkwargsarray or int \n\nFunction parameters (src, dst, n, angle): NAME / TTYPE        FUNCTION PARAMETERS\n'euclidean'         `src, `dst`\n'similarity'        `src, `dst`\n'affine'            `src, `dst`\n'piecewise-affine'  `src, `dst`\n'projective'        `src, `dst`\n'polynomial'        `src, `dst`, `order` (polynomial order,\n                                          default order is 2)\n Also see examples below.    Returns \n \ntformGeometricTransform \n\nTransform object containing the transformation parameters and providing access to forward and inverse transformation functions.     Examples >>> import numpy as np\n>>> from skimage import transform\n >>> # estimate transformation parameters\n>>> src = np.array([0, 0, 10, 10]).reshape((2, 2))\n>>> dst = np.array([12, 14, 1, -20]).reshape((2, 2))\n >>> tform = transform.estimate_transform('similarity', src, dst)\n >>> np.allclose(tform.inverse(tform(src)), src)\nTrue\n >>> # warp image using the estimated transformation\n>>> from skimage import data\n>>> image = data.camera()\n >>> warp(image, inverse_map=tform.inverse) \n >>> # create transformation with explicit parameters\n>>> tform2 = transform.SimilarityTransform(scale=1.1, rotation=1,\n...     translation=(10, 20))\n >>> # unite transformations, applied in order from left to right\n>>> tform3 = tform + tform2\n>>> np.allclose(tform3(src), tform2(tform(src)))\nTrue\n \n frt2  \nskimage.transform.frt2(a) [source]\n \nCompute the 2-dimensional finite radon transform (FRT) for an n x n integer array.  Parameters \n \naarray_like \n\nA 2-D square n x n integer array.    Returns \n \nFRT2-D ndarray \n\nFinite Radon Transform array of (n+1) x n integer coefficients.      See also  \nifrt2\n\n\nThe two-dimensional inverse FRT.    Notes The FRT has a unique inverse if and only if n is prime. [FRT] The idea for this algorithm is due to Vlad Negnevitski. References  \nFRT  \nA. Kingston and I. Svalbe, \u201cProjective transforms on periodic discrete image arrays,\u201d in P. Hawkes (Ed), Advances in Imaging and Electron Physics, 139 (2006)   Examples Generate a test image: Use a prime number for the array dimensions >>> SIZE = 59\n>>> img = np.tri(SIZE, dtype=np.int32)\n Apply the Finite Radon Transform: >>> f = frt2(img)\n \n hough_circle  \nskimage.transform.hough_circle(image, radius, normalize=True, full_output=False) [source]\n \nPerform a circular Hough transform.  Parameters \n \nimage(M, N) ndarray \n\nInput image with nonzero values representing edges.  \nradiusscalar or sequence of scalars \n\nRadii at which to compute the Hough transform. Floats are converted to integers.  \nnormalizeboolean, optional (default True) \n\nNormalize the accumulator with the number of pixels used to draw the radius.  \nfull_outputboolean, optional (default False) \n\nExtend the output size by twice the largest radius in order to detect centers outside the input picture.    Returns \n \nH3D ndarray (radius index, (M + 2R, N + 2R) ndarray) \n\nHough transform accumulator for each radius. R designates the larger radius if full_output is True. Otherwise, R = 0.     Examples >>> from skimage.transform import hough_circle\n>>> from skimage.draw import circle_perimeter\n>>> img = np.zeros((100, 100), dtype=bool)\n>>> rr, cc = circle_perimeter(25, 35, 23)\n>>> img[rr, cc] = 1\n>>> try_radii = np.arange(5, 50)\n>>> res = hough_circle(img, try_radii)\n>>> ridx, r, c = np.unravel_index(np.argmax(res), res.shape)\n>>> r, c, try_radii[ridx]\n(25, 35, 23)\n \n hough_circle_peaks  \nskimage.transform.hough_circle_peaks(hspaces, radii, min_xdistance=1, min_ydistance=1, threshold=None, num_peaks=inf, total_num_peaks=inf, normalize=False) [source]\n \nReturn peaks in a circle Hough transform. Identifies most prominent circles separated by certain distances in given Hough spaces. Non-maximum suppression with different sizes is applied separately in the first and second dimension of the Hough space to identify peaks. For circles with different radius but close in distance, only the one with highest peak is kept.  Parameters \n \nhspaces(N, M) array \n\nHough spaces returned by the hough_circle function.  \nradii(M,) array \n\nRadii corresponding to Hough spaces.  \nmin_xdistanceint, optional \n\nMinimum distance separating centers in the x dimension.  \nmin_ydistanceint, optional \n\nMinimum distance separating centers in the y dimension.  \nthresholdfloat, optional \n\nMinimum intensity of peaks in each Hough space. Default is 0.5 * max(hspace).  \nnum_peaksint, optional \n\nMaximum number of peaks in each Hough space. When the number of peaks exceeds num_peaks, only num_peaks coordinates based on peak intensity are considered for the corresponding radius.  \ntotal_num_peaksint, optional \n\nMaximum number of peaks. When the number of peaks exceeds num_peaks, return num_peaks coordinates based on peak intensity.  \nnormalizebool, optional \n\nIf True, normalize the accumulator by the radius to sort the prominent peaks.    Returns \n \naccum, cx, cy, radtuple of array \n\nPeak values in Hough space, x and y center coordinates and radii.     Notes Circles with bigger radius have higher peaks in Hough space. If larger circles are preferred over smaller ones, normalize should be False. Otherwise, circles will be returned in the order of decreasing voting number. Examples >>> from skimage import transform, draw\n>>> img = np.zeros((120, 100), dtype=int)\n>>> radius, x_0, y_0 = (20, 99, 50)\n>>> y, x = draw.circle_perimeter(y_0, x_0, radius)\n>>> img[x, y] = 1\n>>> hspaces = transform.hough_circle(img, radius)\n>>> accum, cx, cy, rad = hough_circle_peaks(hspaces, [radius,])\n \n hough_ellipse  \nskimage.transform.hough_ellipse(image, threshold=4, accuracy=1, min_size=4, max_size=None) [source]\n \nPerform an elliptical Hough transform.  Parameters \n \nimage(M, N) ndarray \n\nInput image with nonzero values representing edges.  \nthresholdint, optional \n\nAccumulator threshold value.  \naccuracydouble, optional \n\nBin size on the minor axis used in the accumulator.  \nmin_sizeint, optional \n\nMinimal major axis length.  \nmax_sizeint, optional \n\nMaximal minor axis length. If None, the value is set to the half of the smaller image dimension.    Returns \n \nresultndarray with fields [(accumulator, yc, xc, a, b, orientation)]. \n\nWhere (yc, xc) is the center, (a, b) the major and minor axes, respectively. The orientation value follows skimage.draw.ellipse_perimeter convention.     Notes The accuracy must be chosen to produce a peak in the accumulator distribution. In other words, a flat accumulator distribution with low values may be caused by a too low bin size. References  \n1  \nXie, Yonghong, and Qiang Ji. \u201cA new efficient ellipse detection method.\u201d Pattern Recognition, 2002. Proceedings. 16th International Conference on. Vol. 2. IEEE, 2002   Examples >>> from skimage.transform import hough_ellipse\n>>> from skimage.draw import ellipse_perimeter\n>>> img = np.zeros((25, 25), dtype=np.uint8)\n>>> rr, cc = ellipse_perimeter(10, 10, 6, 8)\n>>> img[cc, rr] = 1\n>>> result = hough_ellipse(img, threshold=8)\n>>> result.tolist()\n[(10, 10.0, 10.0, 8.0, 6.0, 0.0)]\n \n hough_line  \nskimage.transform.hough_line(image, theta=None) [source]\n \nPerform a straight line Hough transform.  Parameters \n \nimage(M, N) ndarray \n\nInput image with nonzero values representing edges.  \ntheta1D ndarray of double, optional \n\nAngles at which to compute the transform, in radians. Defaults to a vector of 180 angles evenly spaced from -pi/2 to pi/2.    Returns \n \nhspace2-D ndarray of uint64 \n\nHough transform accumulator.  \nanglesndarray \n\nAngles at which the transform is computed, in radians.  \ndistancesndarray \n\nDistance values.     Notes The origin is the top left corner of the original image. X and Y axis are horizontal and vertical edges respectively. The distance is the minimal algebraic distance from the origin to the detected line. The angle accuracy can be improved by decreasing the step size in the theta array. Examples Generate a test image: >>> img = np.zeros((100, 150), dtype=bool)\n>>> img[30, :] = 1\n>>> img[:, 65] = 1\n>>> img[35:45, 35:50] = 1\n>>> for i in range(90):\n...     img[i, i] = 1\n>>> img += np.random.random(img.shape) > 0.95\n Apply the Hough transform: >>> out, angles, d = hough_line(img)\n import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom skimage.transform import hough_line\nfrom skimage.draw import line\n\nimg = np.zeros((100, 150), dtype=bool)\nimg[30, :] = 1\nimg[:, 65] = 1\nimg[35:45, 35:50] = 1\nrr, cc = line(60, 130, 80, 10)\nimg[rr, cc] = 1\nimg += np.random.random(img.shape) > 0.95\n\nout, angles, d = hough_line(img)\n\nfix, axes = plt.subplots(1, 2, figsize=(7, 4))\n\naxes[0].imshow(img, cmap=plt.cm.gray)\naxes[0].set_title('Input image')\n\naxes[1].imshow(\n    out, cmap=plt.cm.bone,\n    extent=(np.rad2deg(angles[-1]), np.rad2deg(angles[0]), d[-1], d[0]))\naxes[1].set_title('Hough transform')\naxes[1].set_xlabel('Angle (degree)')\naxes[1].set_ylabel('Distance (pixel)')\n\nplt.tight_layout()\nplt.show()\n (Source code, png, pdf)    \n hough_line_peaks  \nskimage.transform.hough_line_peaks(hspace, angles, dists, min_distance=9, min_angle=10, threshold=None, num_peaks=inf) [source]\n \nReturn peaks in a straight line Hough transform. Identifies most prominent lines separated by a certain angle and distance in a Hough transform. Non-maximum suppression with different sizes is applied separately in the first (distances) and second (angles) dimension of the Hough space to identify peaks.  Parameters \n \nhspace(N, M) array \n\nHough space returned by the hough_line function.  \nangles(M,) array \n\nAngles returned by the hough_line function. Assumed to be continuous. (angles[-1] - angles[0] == PI).  \ndists(N, ) array \n\nDistances returned by the hough_line function.  \nmin_distanceint, optional \n\nMinimum distance separating lines (maximum filter size for first dimension of hough space).  \nmin_angleint, optional \n\nMinimum angle separating lines (maximum filter size for second dimension of hough space).  \nthresholdfloat, optional \n\nMinimum intensity of peaks. Default is 0.5 * max(hspace).  \nnum_peaksint, optional \n\nMaximum number of peaks. When the number of peaks exceeds num_peaks, return num_peaks coordinates based on peak intensity.    Returns \n \naccum, angles, diststuple of array \n\nPeak values in Hough space, angles and distances.     Examples >>> from skimage.transform import hough_line, hough_line_peaks\n>>> from skimage.draw import line\n>>> img = np.zeros((15, 15), dtype=bool)\n>>> rr, cc = line(0, 0, 14, 14)\n>>> img[rr, cc] = 1\n>>> rr, cc = line(0, 14, 14, 0)\n>>> img[cc, rr] = 1\n>>> hspace, angles, dists = hough_line(img)\n>>> hspace, angles, dists = hough_line_peaks(hspace, angles, dists)\n>>> len(angles)\n2\n \n ifrt2  \nskimage.transform.ifrt2(a) [source]\n \nCompute the 2-dimensional inverse finite radon transform (iFRT) for an (n+1) x n integer array.  Parameters \n \naarray_like \n\nA 2-D (n+1) row x n column integer array.    Returns \n \niFRT2-D n x n ndarray \n\nInverse Finite Radon Transform array of n x n integer coefficients.      See also  \nfrt2\n\n\nThe two-dimensional FRT    Notes The FRT has a unique inverse if and only if n is prime. See [1] for an overview. The idea for this algorithm is due to Vlad Negnevitski. References  \n1  \nA. Kingston and I. Svalbe, \u201cProjective transforms on periodic discrete image arrays,\u201d in P. Hawkes (Ed), Advances in Imaging and Electron Physics, 139 (2006)   Examples >>> SIZE = 59\n>>> img = np.tri(SIZE, dtype=np.int32)\n Apply the Finite Radon Transform: >>> f = frt2(img)\n Apply the Inverse Finite Radon Transform to recover the input >>> fi = ifrt2(f)\n Check that it\u2019s identical to the original >>> assert len(np.nonzero(img-fi)[0]) == 0\n \n integral_image  \nskimage.transform.integral_image(image) [source]\n \nIntegral image / summed area table. The integral image contains the sum of all elements above and to the left of it, i.e.:  \\[S[m, n] = \\sum_{i \\leq m} \\sum_{j \\leq n} X[i, j]\\]  Parameters \n \nimagendarray \n\nInput image.    Returns \n \nSndarray \n\nIntegral image/summed area table of same shape as input image.     References  \n1  \nF.C. Crow, \u201cSummed-area tables for texture mapping,\u201d ACM SIGGRAPH Computer Graphics, vol. 18, 1984, pp. 207-212.   \n integrate  \nskimage.transform.integrate(ii, start, end) [source]\n \nUse an integral image to integrate over a given window.  Parameters \n \niindarray \n\nIntegral image.  \nstartList of tuples, each tuple of length equal to dimension of ii \n\nCoordinates of top left corner of window(s). Each tuple in the list contains the starting row, col, \u2026 index i.e [(row_win1, col_win1, \u2026), (row_win2, col_win2,\u2026), \u2026].  \nendList of tuples, each tuple of length equal to dimension of ii \n\nCoordinates of bottom right corner of window(s). Each tuple in the list containing the end row, col, \u2026 index i.e [(row_win1, col_win1, \u2026), (row_win2, col_win2, \u2026), \u2026].    Returns \n \nSscalar or ndarray \n\nIntegral (sum) over the given window(s).     Examples >>> arr = np.ones((5, 6), dtype=float)\n>>> ii = integral_image(arr)\n>>> integrate(ii, (1, 0), (1, 2))  # sum from (1, 0) to (1, 2)\narray([3.])\n>>> integrate(ii, [(3, 3)], [(4, 5)])  # sum from (3, 3) to (4, 5)\narray([6.])\n>>> # sum from (1, 0) to (1, 2) and from (3, 3) to (4, 5)\n>>> integrate(ii, [(1, 0), (3, 3)], [(1, 2), (4, 5)])\narray([3., 6.])\n \n iradon  \nskimage.transform.iradon(radon_image, theta=None, output_size=None, filter_name='ramp', interpolation='linear', circle=True, preserve_range=True) [source]\n \nInverse radon transform. Reconstruct an image from the radon transform, using the filtered back projection algorithm.  Parameters \n \nradon_imagearray \n\nImage containing radon transform (sinogram). Each column of the image corresponds to a projection along a different angle. The tomography rotation axis should lie at the pixel index radon_image.shape[0] // 2 along the 0th dimension of radon_image.  \nthetaarray_like, optional \n\nReconstruction angles (in degrees). Default: m angles evenly spaced between 0 and 180 (if the shape of radon_image is (N, M)).  \noutput_sizeint, optional \n\nNumber of rows and columns in the reconstruction.  \nfilter_namestr, optional \n\nFilter used in frequency domain filtering. Ramp filter used by default. Filters available: ramp, shepp-logan, cosine, hamming, hann. Assign None to use no filter.  \ninterpolationstr, optional \n\nInterpolation method used in reconstruction. Methods available: \u2018linear\u2019, \u2018nearest\u2019, and \u2018cubic\u2019 (\u2018cubic\u2019 is slow).  \ncircleboolean, optional \n\nAssume the reconstructed image is zero outside the inscribed circle. Also changes the default output_size to match the behaviour of radon called with circle=True.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html    Returns \n \nreconstructedndarray \n\nReconstructed image. The rotation axis will be located in the pixel with indices (reconstructed.shape[0] // 2, reconstructed.shape[1] // 2).    Changed in version 0.19: In iradon, filter argument is deprecated in favor of filter_name.    Notes It applies the Fourier slice theorem to reconstruct an image by multiplying the frequency domain of the filter with the FFT of the projection data. This algorithm is called filtered back projection. References  \n1  \nAC Kak, M Slaney, \u201cPrinciples of Computerized Tomographic Imaging\u201d, IEEE Press 1988.  \n2  \nB.R. Ramesh, N. Srinivasa, K. Rajgopal, \u201cAn Algorithm for Computing the Discrete Radon Transform With Some Applications\u201d, Proceedings of the Fourth IEEE Region 10 International Conference, TENCON \u201889, 1989   \n iradon_sart  \nskimage.transform.iradon_sart(radon_image, theta=None, image=None, projection_shifts=None, clip=None, relaxation=0.15, dtype=None) [source]\n \nInverse radon transform. Reconstruct an image from the radon transform, using a single iteration of the Simultaneous Algebraic Reconstruction Technique (SART) algorithm.  Parameters \n \nradon_image2D array \n\nImage containing radon transform (sinogram). Each column of the image corresponds to a projection along a different angle. The tomography rotation axis should lie at the pixel index radon_image.shape[0] // 2 along the 0th dimension of radon_image.  \ntheta1D array, optional \n\nReconstruction angles (in degrees). Default: m angles evenly spaced between 0 and 180 (if the shape of radon_image is (N, M)).  \nimage2D array, optional \n\nImage containing an initial reconstruction estimate. Shape of this array should be (radon_image.shape[0], radon_image.shape[0]). The default is an array of zeros.  \nprojection_shifts1D array, optional \n\nShift the projections contained in radon_image (the sinogram) by this many pixels before reconstructing the image. The i\u2019th value defines the shift of the i\u2019th column of radon_image.  \ncliplength-2 sequence of floats, optional \n\nForce all values in the reconstructed tomogram to lie in the range [clip[0], clip[1]]  \nrelaxationfloat, optional \n\nRelaxation parameter for the update step. A higher value can improve the convergence rate, but one runs the risk of instabilities. Values close to or higher than 1 are not recommended.  \ndtypedtype, optional \n\nOutput data type, must be floating point. By default, if input data type is not float, input is cast to double, otherwise dtype is set to input data type.    Returns \n \nreconstructedndarray \n\nReconstructed image. The rotation axis will be located in the pixel with indices (reconstructed.shape[0] // 2, reconstructed.shape[1] // 2).     Notes Algebraic Reconstruction Techniques are based on formulating the tomography reconstruction problem as a set of linear equations. Along each ray, the projected value is the sum of all the values of the cross section along the ray. A typical feature of SART (and a few other variants of algebraic techniques) is that it samples the cross section at equidistant points along the ray, using linear interpolation between the pixel values of the cross section. The resulting set of linear equations are then solved using a slightly modified Kaczmarz method. When using SART, a single iteration is usually sufficient to obtain a good reconstruction. Further iterations will tend to enhance high-frequency information, but will also often increase the noise. References  \n1  \nAC Kak, M Slaney, \u201cPrinciples of Computerized Tomographic Imaging\u201d, IEEE Press 1988.  \n2  \nAH Andersen, AC Kak, \u201cSimultaneous algebraic reconstruction technique (SART): a superior implementation of the ART algorithm\u201d, Ultrasonic Imaging 6 pp 81\u201394 (1984)  \n3  \nS Kaczmarz, \u201cAngen\u00e4herte aufl\u00f6sung von systemen linearer gleichungen\u201d, Bulletin International de l\u2019Academie Polonaise des Sciences et des Lettres 35 pp 355\u2013357 (1937)  \n4  \nKohler, T. \u201cA projection access scheme for iterative reconstruction based on the golden section.\u201d Nuclear Science Symposium Conference Record, 2004 IEEE. Vol. 6. IEEE, 2004.  \n5  \nKaczmarz\u2019 method, Wikipedia, https://en.wikipedia.org/wiki/Kaczmarz_method   \n matrix_transform  \nskimage.transform.matrix_transform(coords, matrix) [source]\n \nApply 2D matrix transform.  Parameters \n \ncoords(N, 2) array \n\nx, y coordinates to transform  \nmatrix(3, 3) array \n\nHomogeneous transformation matrix.    Returns \n \ncoords(N, 2) array \n\nTransformed coordinates.     \n order_angles_golden_ratio  \nskimage.transform.order_angles_golden_ratio(theta) [source]\n \nOrder angles to reduce the amount of correlated information in subsequent projections.  Parameters \n \ntheta1D array of floats \n\nProjection angles in degrees. Duplicate angles are not allowed.    Returns \n \nindices_generatorgenerator yielding unsigned integers \n\nThe returned generator yields indices into theta such that theta[indices] gives the approximate golden ratio ordering of the projections. In total, len(theta) indices are yielded. All non-negative integers < len(theta) are yielded exactly once.     Notes The method used here is that of the golden ratio introduced by T. Kohler. References  \n1  \nKohler, T. \u201cA projection access scheme for iterative reconstruction based on the golden section.\u201d Nuclear Science Symposium Conference Record, 2004 IEEE. Vol. 6. IEEE, 2004.  \n2  \nWinkelmann, Stefanie, et al. \u201cAn optimal radial profile order based on the Golden Ratio for time-resolved MRI.\u201d Medical Imaging, IEEE Transactions on 26.1 (2007): 68-76.   \n probabilistic_hough_line  \nskimage.transform.probabilistic_hough_line(image, threshold=10, line_length=50, line_gap=10, theta=None, seed=None) [source]\n \nReturn lines from a progressive probabilistic line Hough transform.  Parameters \n \nimage(M, N) ndarray \n\nInput image with nonzero values representing edges.  \nthresholdint, optional \n\nThreshold  \nline_lengthint, optional \n\nMinimum accepted length of detected lines. Increase the parameter to extract longer lines.  \nline_gapint, optional \n\nMaximum gap between pixels to still form a line. Increase the parameter to merge broken lines more aggressively.  \ntheta1D ndarray, dtype=double, optional \n\nAngles at which to compute the transform, in radians. If None, use a range from -pi/2 to pi/2.  \nseedint, optional \n\nSeed to initialize the random number generator.    Returns \n \nlineslist \n\nList of lines identified, lines in format ((x0, y0), (x1, y1)), indicating line start and end.     References  \n1  \nC. Galamhos, J. Matas and J. Kittler, \u201cProgressive probabilistic Hough transform for line detection\u201d, in IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 1999.   \n pyramid_expand  \nskimage.transform.pyramid_expand(image, upscale=2, sigma=None, order=1, mode='reflect', cval=0, multichannel=False, preserve_range=False) [source]\n \nUpsample and then smooth image.  Parameters \n \nimagendarray \n\nInput image.  \nupscalefloat, optional \n\nUpscale factor.  \nsigmafloat, optional \n\nSigma for Gaussian filter. Default is 2 * upscale / 6.0 which corresponds to a filter mask twice the size of the scale factor that covers more than 99% of the Gaussian distribution.  \norderint, optional \n\nOrder of splines used in interpolation of upsampling. See skimage.transform.warp for detail.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018edge\u2019, \u2018symmetric\u2019, \u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019.  \ncvalfloat, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019.  \nmultichannelbool, optional \n\nWhether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html    Returns \n \noutarray \n\nUpsampled and smoothed float image.     References  \n1  \nhttp://persci.mit.edu/pub_pdfs/pyramid83.pdf   \n pyramid_gaussian  \nskimage.transform.pyramid_gaussian(image, max_layer=-1, downscale=2, sigma=None, order=1, mode='reflect', cval=0, multichannel=False, preserve_range=False) [source]\n \nYield images of the Gaussian pyramid formed by the input image. Recursively applies the pyramid_reduce function to the image, and yields the downscaled images. Note that the first image of the pyramid will be the original, unscaled image. The total number of images is max_layer + 1. In case all layers are computed, the last image is either a one-pixel image or the image where the reduction does not change its shape.  Parameters \n \nimagendarray \n\nInput image.  \nmax_layerint, optional \n\nNumber of layers for the pyramid. 0th layer is the original image. Default is -1 which builds all possible layers.  \ndownscalefloat, optional \n\nDownscale factor.  \nsigmafloat, optional \n\nSigma for Gaussian filter. Default is 2 * downscale / 6.0 which corresponds to a filter mask twice the size of the scale factor that covers more than 99% of the Gaussian distribution.  \norderint, optional \n\nOrder of splines used in interpolation of downsampling. See skimage.transform.warp for detail.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018edge\u2019, \u2018symmetric\u2019, \u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019.  \ncvalfloat, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019.  \nmultichannelbool, optional \n\nWhether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html    Returns \n \npyramidgenerator \n\nGenerator yielding pyramid layers as float images.     References  \n1  \nhttp://persci.mit.edu/pub_pdfs/pyramid83.pdf   \n pyramid_laplacian  \nskimage.transform.pyramid_laplacian(image, max_layer=-1, downscale=2, sigma=None, order=1, mode='reflect', cval=0, multichannel=False, preserve_range=False) [source]\n \nYield images of the laplacian pyramid formed by the input image. Each layer contains the difference between the downsampled and the downsampled, smoothed image: layer = resize(prev_layer) - smooth(resize(prev_layer))\n Note that the first image of the pyramid will be the difference between the original, unscaled image and its smoothed version. The total number of images is max_layer + 1. In case all layers are computed, the last image is either a one-pixel image or the image where the reduction does not change its shape.  Parameters \n \nimagendarray \n\nInput image.  \nmax_layerint, optional \n\nNumber of layers for the pyramid. 0th layer is the original image. Default is -1 which builds all possible layers.  \ndownscalefloat, optional \n\nDownscale factor.  \nsigmafloat, optional \n\nSigma for Gaussian filter. Default is 2 * downscale / 6.0 which corresponds to a filter mask twice the size of the scale factor that covers more than 99% of the Gaussian distribution.  \norderint, optional \n\nOrder of splines used in interpolation of downsampling. See skimage.transform.warp for detail.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018edge\u2019, \u2018symmetric\u2019, \u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019.  \ncvalfloat, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019.  \nmultichannelbool, optional \n\nWhether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html    Returns \n \npyramidgenerator \n\nGenerator yielding pyramid layers as float images.     References  \n1  \nhttp://persci.mit.edu/pub_pdfs/pyramid83.pdf  \n2  \nhttp://sepwww.stanford.edu/data/media/public/sep/morgan/texturematch/paper_html/node3.html   \n pyramid_reduce  \nskimage.transform.pyramid_reduce(image, downscale=2, sigma=None, order=1, mode='reflect', cval=0, multichannel=False, preserve_range=False) [source]\n \nSmooth and then downsample image.  Parameters \n \nimagendarray \n\nInput image.  \ndownscalefloat, optional \n\nDownscale factor.  \nsigmafloat, optional \n\nSigma for Gaussian filter. Default is 2 * downscale / 6.0 which corresponds to a filter mask twice the size of the scale factor that covers more than 99% of the Gaussian distribution.  \norderint, optional \n\nOrder of splines used in interpolation of downsampling. See skimage.transform.warp for detail.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018edge\u2019, \u2018symmetric\u2019, \u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019.  \ncvalfloat, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019.  \nmultichannelbool, optional \n\nWhether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html    Returns \n \noutarray \n\nSmoothed and downsampled float image.     References  \n1  \nhttp://persci.mit.edu/pub_pdfs/pyramid83.pdf   \n radon  \nskimage.transform.radon(image, theta=None, circle=True, *, preserve_range=False) [source]\n \nCalculates the radon transform of an image given specified projection angles.  Parameters \n \nimagearray_like \n\nInput image. The rotation axis will be located in the pixel with indices (image.shape[0] // 2, image.shape[1] // 2).  \nthetaarray_like, optional \n\nProjection angles (in degrees). If None, the value is set to np.arange(180).  \ncircleboolean, optional \n\nAssume image is zero outside the inscribed circle, making the width of each projection (the first dimension of the sinogram) equal to min(image.shape).  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html    Returns \n \nradon_imagendarray \n\nRadon transform (sinogram). The tomography rotation axis will lie at the pixel index radon_image.shape[0] // 2 along the 0th dimension of radon_image.     Notes Based on code of Justin K. Romberg (https://www.clear.rice.edu/elec431/projects96/DSP/bpanalysis.html) References  \n1  \nAC Kak, M Slaney, \u201cPrinciples of Computerized Tomographic Imaging\u201d, IEEE Press 1988.  \n2  \nB.R. Ramesh, N. Srinivasa, K. Rajgopal, \u201cAn Algorithm for Computing the Discrete Radon Transform With Some Applications\u201d, Proceedings of the Fourth IEEE Region 10 International Conference, TENCON \u201889, 1989   \n rescale  \nskimage.transform.rescale(image, scale, order=None, mode='reflect', cval=0, clip=True, preserve_range=False, multichannel=False, anti_aliasing=None, anti_aliasing_sigma=None) [source]\n \nScale image by a certain factor. Performs interpolation to up-scale or down-scale N-dimensional images. Note that anti-aliasing should be enabled when down-sizing images to avoid aliasing artifacts. For down-sampling with an integer factor also see skimage.transform.downscale_local_mean.  Parameters \n \nimagendarray \n\nInput image.  \nscale{float, tuple of floats} \n\nScale factors. Separate scale factors can be defined as (rows, cols[, \u2026][, dim]).    Returns \n \nscaledndarray \n\nScaled version of the input.    Other Parameters \n \norderint, optional \n\nThe order of the spline interpolation, default is 0 if image.dtype is bool and 1 otherwise. The order has to be in the range 0-5. See skimage.transform.warp for detail.  \nmode{\u2018constant\u2019, \u2018edge\u2019, \u2018symmetric\u2019, \u2018reflect\u2019, \u2018wrap\u2019}, optional \n\nPoints outside the boundaries of the input are filled according to the given mode. Modes match the behaviour of numpy.pad.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.  \nclipbool, optional \n\nWhether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html  \nmultichannelbool, optional \n\nWhether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.  \nanti_aliasingbool, optional \n\nWhether to apply a Gaussian filter to smooth the image prior to down-scaling. It is crucial to filter when down-sampling the image to avoid aliasing artifacts. If input image data type is bool, no anti-aliasing is applied.  \nanti_aliasing_sigma{float, tuple of floats}, optional \n\nStandard deviation for Gaussian filtering to avoid aliasing artifacts. By default, this value is chosen as (s - 1) / 2 where s is the down-scaling factor.     Notes Modes \u2018reflect\u2019 and \u2018symmetric\u2019 are similar, but differ in whether the edge pixels are duplicated during the reflection. As an example, if an array has values [0, 1, 2] and was padded to the right by four values using symmetric, the result would be [0, 1, 2, 2, 1, 0, 0], while for reflect it would be [0, 1, 2, 1, 0, 1, 2]. Examples >>> from skimage import data\n>>> from skimage.transform import rescale\n>>> image = data.camera()\n>>> rescale(image, 0.1).shape\n(51, 51)\n>>> rescale(image, 0.5).shape\n(256, 256)\n \n resize  \nskimage.transform.resize(image, output_shape, order=None, mode='reflect', cval=0, clip=True, preserve_range=False, anti_aliasing=None, anti_aliasing_sigma=None) [source]\n \nResize image to match a certain size. Performs interpolation to up-size or down-size N-dimensional images. Note that anti-aliasing should be enabled when down-sizing images to avoid aliasing artifacts. For down-sampling with an integer factor also see skimage.transform.downscale_local_mean.  Parameters \n \nimagendarray \n\nInput image.  \noutput_shapetuple or ndarray \n\nSize of the generated output image (rows, cols[, \u2026][, dim]). If dim is not provided, the number of channels is preserved. In case the number of input channels does not equal the number of output channels a n-dimensional interpolation is applied.    Returns \n \nresizedndarray \n\nResized version of the input.    Other Parameters \n \norderint, optional \n\nThe order of the spline interpolation, default is 0 if image.dtype is bool and 1 otherwise. The order has to be in the range 0-5. See skimage.transform.warp for detail.  \nmode{\u2018constant\u2019, \u2018edge\u2019, \u2018symmetric\u2019, \u2018reflect\u2019, \u2018wrap\u2019}, optional \n\nPoints outside the boundaries of the input are filled according to the given mode. Modes match the behaviour of numpy.pad.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.  \nclipbool, optional \n\nWhether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html  \nanti_aliasingbool, optional \n\nWhether to apply a Gaussian filter to smooth the image prior to down-scaling. It is crucial to filter when down-sampling the image to avoid aliasing artifacts. If input image data type is bool, no anti-aliasing is applied.  \nanti_aliasing_sigma{float, tuple of floats}, optional \n\nStandard deviation for Gaussian filtering to avoid aliasing artifacts. By default, this value is chosen as (s - 1) / 2 where s is the down-scaling factor, where s > 1. For the up-size case, s < 1, no anti-aliasing is performed prior to rescaling.     Notes Modes \u2018reflect\u2019 and \u2018symmetric\u2019 are similar, but differ in whether the edge pixels are duplicated during the reflection. As an example, if an array has values [0, 1, 2] and was padded to the right by four values using symmetric, the result would be [0, 1, 2, 2, 1, 0, 0], while for reflect it would be [0, 1, 2, 1, 0, 1, 2]. Examples >>> from skimage import data\n>>> from skimage.transform import resize\n>>> image = data.camera()\n>>> resize(image, (100, 100)).shape\n(100, 100)\n \n rotate  \nskimage.transform.rotate(image, angle, resize=False, center=None, order=None, mode='constant', cval=0, clip=True, preserve_range=False) [source]\n \nRotate image by a certain angle around its center.  Parameters \n \nimagendarray \n\nInput image.  \nanglefloat \n\nRotation angle in degrees in counter-clockwise direction.  \nresizebool, optional \n\nDetermine whether the shape of the output image will be automatically calculated, so the complete rotated image exactly fits. Default is False.  \ncenteriterable of length 2 \n\nThe rotation center. If center=None, the image is rotated around its center, i.e. center=(cols / 2 - 0.5, rows / 2 - 0.5). Please note that this parameter is (cols, rows), contrary to normal skimage ordering.    Returns \n \nrotatedndarray \n\nRotated version of the input.    Other Parameters \n \norderint, optional \n\nThe order of the spline interpolation, default is 0 if image.dtype is bool and 1 otherwise. The order has to be in the range 0-5. See skimage.transform.warp for detail.  \nmode{\u2018constant\u2019, \u2018edge\u2019, \u2018symmetric\u2019, \u2018reflect\u2019, \u2018wrap\u2019}, optional \n\nPoints outside the boundaries of the input are filled according to the given mode. Modes match the behaviour of numpy.pad.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.  \nclipbool, optional \n\nWhether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html     Notes Modes \u2018reflect\u2019 and \u2018symmetric\u2019 are similar, but differ in whether the edge pixels are duplicated during the reflection. As an example, if an array has values [0, 1, 2] and was padded to the right by four values using symmetric, the result would be [0, 1, 2, 2, 1, 0, 0], while for reflect it would be [0, 1, 2, 1, 0, 1, 2]. Examples >>> from skimage import data\n>>> from skimage.transform import rotate\n>>> image = data.camera()\n>>> rotate(image, 2).shape\n(512, 512)\n>>> rotate(image, 2, resize=True).shape\n(530, 530)\n>>> rotate(image, 90, resize=True).shape\n(512, 512)\n \n Examples using skimage.transform.rotate\n \n  Different perimeters  \n\n  Measure region properties   swirl  \nskimage.transform.swirl(image, center=None, strength=1, radius=100, rotation=0, output_shape=None, order=None, mode='reflect', cval=0, clip=True, preserve_range=False) [source]\n \nPerform a swirl transformation.  Parameters \n \nimagendarray \n\nInput image.  \ncenter(column, row) tuple or (2,) ndarray, optional \n\nCenter coordinate of transformation.  \nstrengthfloat, optional \n\nThe amount of swirling applied.  \nradiusfloat, optional \n\nThe extent of the swirl in pixels. The effect dies out rapidly beyond radius.  \nrotationfloat, optional \n\nAdditional rotation applied to the image.    Returns \n \nswirledndarray \n\nSwirled version of the input.    Other Parameters \n \noutput_shapetuple (rows, cols), optional \n\nShape of the output image generated. By default the shape of the input image is preserved.  \norderint, optional \n\nThe order of the spline interpolation, default is 0 if image.dtype is bool and 1 otherwise. The order has to be in the range 0-5. See skimage.transform.warp for detail.  \nmode{\u2018constant\u2019, \u2018edge\u2019, \u2018symmetric\u2019, \u2018reflect\u2019, \u2018wrap\u2019}, optional \n\nPoints outside the boundaries of the input are filled according to the given mode, with \u2018constant\u2019 used as the default. Modes match the behaviour of numpy.pad.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.  \nclipbool, optional \n\nWhether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html     \n warp  \nskimage.transform.warp(image, inverse_map, map_args={}, output_shape=None, order=None, mode='constant', cval=0.0, clip=True, preserve_range=False) [source]\n \nWarp an image according to a given coordinate transformation.  Parameters \n \nimagendarray \n\nInput image.  \ninverse_maptransformation object, callable cr = f(cr, **kwargs), or ndarray \n\nInverse coordinate map, which transforms coordinates in the output images into their corresponding coordinates in the input image. There are a number of different options to define this map, depending on the dimensionality of the input image. A 2-D image can have 2 dimensions for gray-scale images, or 3 dimensions with color information.  For 2-D images, you can directly pass a transformation object, e.g. skimage.transform.SimilarityTransform, or its inverse. For 2-D images, you can pass a (3, 3) homogeneous transformation matrix, e.g. skimage.transform.SimilarityTransform.params. For 2-D images, a function that transforms a (M, 2) array of (col, row) coordinates in the output image to their corresponding coordinates in the input image. Extra parameters to the function can be specified through map_args. For N-D images, you can directly pass an array of coordinates. The first dimension specifies the coordinates in the input image, while the subsequent dimensions determine the position in the output image. E.g. in case of 2-D images, you need to pass an array of shape (2, rows, cols), where rows and cols determine the shape of the output image, and the first dimension contains the (row, col) coordinate in the input image. See scipy.ndimage.map_coordinates for further documentation.  Note, that a (3, 3) matrix is interpreted as a homogeneous transformation matrix, so you cannot interpolate values from a 3-D input, if the output is of shape (3,). See example section for usage.  \nmap_argsdict, optional \n\nKeyword arguments passed to inverse_map.  \noutput_shapetuple (rows, cols), optional \n\nShape of the output image generated. By default the shape of the input image is preserved. Note that, even for multi-band images, only rows and columns need to be specified.  \norderint, optional \n\n The order of interpolation. The order has to be in the range 0-5:\n\n 0: Nearest-neighbor 1: Bi-linear (default) 2: Bi-quadratic 3: Bi-cubic 4: Bi-quartic 5: Bi-quintic  Default is 0 if image.dtype is bool and 1 otherwise.    \nmode{\u2018constant\u2019, \u2018edge\u2019, \u2018symmetric\u2019, \u2018reflect\u2019, \u2018wrap\u2019}, optional \n\nPoints outside the boundaries of the input are filled according to the given mode. Modes match the behaviour of numpy.pad.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.  \nclipbool, optional \n\nWhether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html    Returns \n \nwarpeddouble ndarray \n\nThe warped input image.     Notes  The input image is converted to a double image. In case of a SimilarityTransform, AffineTransform and ProjectiveTransform and order in [0, 3] this function uses the underlying transformation matrix to warp the image with a much faster routine.  Examples >>> from skimage.transform import warp\n>>> from skimage import data\n>>> image = data.camera()\n The following image warps are all equal but differ substantially in execution time. The image is shifted to the bottom. Use a geometric transform to warp an image (fast): >>> from skimage.transform import SimilarityTransform\n>>> tform = SimilarityTransform(translation=(0, -10))\n>>> warped = warp(image, tform)\n Use a callable (slow): >>> def shift_down(xy):\n...     xy[:, 1] -= 10\n...     return xy\n>>> warped = warp(image, shift_down)\n Use a transformation matrix to warp an image (fast): >>> matrix = np.array([[1, 0, 0], [0, 1, -10], [0, 0, 1]])\n>>> warped = warp(image, matrix)\n>>> from skimage.transform import ProjectiveTransform\n>>> warped = warp(image, ProjectiveTransform(matrix=matrix))\n You can also use the inverse of a geometric transformation (fast): >>> warped = warp(image, tform.inverse)\n For N-D images you can pass a coordinate array, that specifies the coordinates in the input image for every element in the output image. E.g. if you want to rescale a 3-D cube, you can do: >>> cube_shape = np.array([30, 30, 30])\n>>> cube = np.random.rand(*cube_shape)\n Setup the coordinate array, that defines the scaling: >>> scale = 0.1\n>>> output_shape = (scale * cube_shape).astype(int)\n>>> coords0, coords1, coords2 = np.mgrid[:output_shape[0],\n...                    :output_shape[1], :output_shape[2]]\n>>> coords = np.array([coords0, coords1, coords2])\n Assume that the cube contains spatial data, where the first array element center is at coordinate (0.5, 0.5, 0.5) in real space, i.e. we have to account for this extra offset when scaling the image: >>> coords = (coords + 0.5) / scale - 0.5\n>>> warped = warp(cube, coords)\n \n Examples using skimage.transform.warp\n \n  Registration using optical flow   warp_coords  \nskimage.transform.warp_coords(coord_map, shape, dtype=<class 'numpy.float64'>) [source]\n \nBuild the source coordinates for the output of a 2-D image warp.  Parameters \n \ncoord_mapcallable like GeometricTransform.inverse \n\nReturn input coordinates for given output coordinates. Coordinates are in the shape (P, 2), where P is the number of coordinates and each element is a (row, col) pair.  \nshapetuple \n\nShape of output image (rows, cols[, bands]).  \ndtypenp.dtype or string \n\ndtype for return value (sane choices: float32 or float64).    Returns \n \ncoords(ndim, rows, cols[, bands]) array of dtype dtype \n\nCoordinates for scipy.ndimage.map_coordinates, that will yield an image of shape (orows, ocols, bands) by drawing from source points according to the coord_transform_fn.     Notes This is a lower-level routine that produces the source coordinates for 2-D images used by warp(). It is provided separately from warp to give additional flexibility to users who would like, for example, to re-use a particular coordinate mapping, to use specific dtypes at various points along the the image-warping process, or to implement different post-processing logic than warp performs after the call to ndi.map_coordinates. Examples Produce a coordinate map that shifts an image up and to the right: >>> from skimage import data\n>>> from scipy.ndimage import map_coordinates\n>>>\n>>> def shift_up10_left20(xy):\n...     return xy - np.array([-20, 10])[None, :]\n>>>\n>>> image = data.astronaut().astype(np.float32)\n>>> coords = warp_coords(shift_up10_left20, image.shape)\n>>> warped_image = map_coordinates(image, coords)\n \n warp_polar  \nskimage.transform.warp_polar(image, center=None, *, radius=None, output_shape=None, scaling='linear', multichannel=False, **kwargs) [source]\n \nRemap image to polar or log-polar coordinates space.  Parameters \n \nimagendarray \n\nInput image. Only 2-D arrays are accepted by default. If multichannel=True, 3-D arrays are accepted and the last axis is interpreted as multiple channels.  \ncentertuple (row, col), optional \n\nPoint in image that represents the center of the transformation (i.e., the origin in cartesian space). Values can be of type float. If no value is given, the center is assumed to be the center point of the image.  \nradiusfloat, optional \n\nRadius of the circle that bounds the area to be transformed.  \noutput_shapetuple (row, col), optional \n\nscaling{\u2018linear\u2019, \u2018log\u2019}, optional \n\nSpecify whether the image warp is polar or log-polar. Defaults to \u2018linear\u2019.  \nmultichannelbool, optional \n\nWhether the image is a 3-D array in which the third axis is to be interpreted as multiple channels. If set to False (default), only 2-D arrays are accepted.  \n**kwargskeyword arguments \n\nPassed to transform.warp.    Returns \n \nwarpedndarray \n\nThe polar or log-polar warped image.     Examples Perform a basic polar warp on a grayscale image: >>> from skimage import data\n>>> from skimage.transform import warp_polar\n>>> image = data.checkerboard()\n>>> warped = warp_polar(image)\n Perform a log-polar warp on a grayscale image: >>> warped = warp_polar(image, scaling='log')\n Perform a log-polar warp on a grayscale image while specifying center, radius, and output shape: >>> warped = warp_polar(image, (100,100), radius=100,\n...                     output_shape=image.shape, scaling='log')\n Perform a log-polar warp on a color image: >>> image = data.astronaut()\n>>> warped = warp_polar(image, scaling='log', multichannel=True)\n \n AffineTransform  \nclass skimage.transform.AffineTransform(matrix=None, scale=None, rotation=None, shear=None, translation=None) [source]\n \nBases: skimage.transform._geometric.ProjectiveTransform 2D affine transformation. Has the following form: X = a0*x + a1*y + a2 =\n  = sx*x*cos(rotation) - sy*y*sin(rotation + shear) + a2\n\nY = b0*x + b1*y + b2 =\n  = sx*x*sin(rotation) + sy*y*cos(rotation + shear) + b2\n where sx and sy are scale factors in the x and y directions, and the homogeneous transformation matrix is: [[a0  a1  a2]\n [b0  b1  b2]\n [0   0    1]]\n  Parameters \n \nmatrix(3, 3) array, optional \n\nHomogeneous transformation matrix.  \nscale{s as float or (sx, sy) as array, list or tuple}, optional \n\nScale factor(s). If a single value, it will be assigned to both sx and sy.  New in version 0.17: Added support for supplying a single scalar value.   \nrotationfloat, optional \n\nRotation angle in counter-clockwise direction as radians.  \nshearfloat, optional \n\nShear angle in counter-clockwise direction as radians.  \ntranslation(tx, ty) as array, list or tuple, optional \n\nTranslation parameters.    Attributes \n \nparams(3, 3) array \n\nHomogeneous transformation matrix.      \n__init__(matrix=None, scale=None, rotation=None, shear=None, translation=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nproperty rotation \n  \nproperty scale \n  \nproperty shear \n  \nproperty translation \n \n EssentialMatrixTransform  \nclass skimage.transform.EssentialMatrixTransform(rotation=None, translation=None, matrix=None) [source]\n \nBases: skimage.transform._geometric.FundamentalMatrixTransform Essential matrix transformation. The essential matrix relates corresponding points between a pair of calibrated images. The matrix transforms normalized, homogeneous image points in one image to epipolar lines in the other image. The essential matrix is only defined for a pair of moving images capturing a non-planar scene. In the case of pure rotation or planar scenes, the homography describes the geometric relation between two images (ProjectiveTransform). If the intrinsic calibration of the images is unknown, the fundamental matrix describes the projective relation between the two images (FundamentalMatrixTransform).  Parameters \n \nrotation(3, 3) array, optional \n\nRotation matrix of the relative camera motion.  \ntranslation(3, 1) array, optional \n\nTranslation vector of the relative camera motion. The vector must have unit length.  \nmatrix(3, 3) array, optional \n\nEssential matrix.     References  \n1  \nHartley, Richard, and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003.    Attributes \n \nparams(3, 3) array \n\nEssential matrix.      \n__init__(rotation=None, translation=None, matrix=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(src, dst) [source]\n \nEstimate essential matrix using 8-point algorithm. The 8-point algorithm requires at least 8 corresponding point pairs for a well-conditioned solution, otherwise the over-determined solution is estimated.  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n \n EuclideanTransform  \nclass skimage.transform.EuclideanTransform(matrix=None, rotation=None, translation=None) [source]\n \nBases: skimage.transform._geometric.ProjectiveTransform 2D Euclidean transformation. Has the following form: X = a0 * x - b0 * y + a1 =\n  = x * cos(rotation) - y * sin(rotation) + a1\n\nY = b0 * x + a0 * y + b1 =\n  = x * sin(rotation) + y * cos(rotation) + b1\n where the homogeneous transformation matrix is: [[a0  b0  a1]\n [b0  a0  b1]\n [0   0    1]]\n The Euclidean transformation is a rigid transformation with rotation and translation parameters. The similarity transformation extends the Euclidean transformation with a single scaling factor.  Parameters \n \nmatrix(3, 3) array, optional \n\nHomogeneous transformation matrix.  \nrotationfloat, optional \n\nRotation angle in counter-clockwise direction as radians.  \ntranslation(tx, ty) as array, list or tuple, optional \n\nx, y translation parameters.    Attributes \n \nparams(3, 3) array \n\nHomogeneous transformation matrix.      \n__init__(matrix=None, rotation=None, translation=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(src, dst) [source]\n \nEstimate the transformation from a set of corresponding points. You can determine the over-, well- and under-determined parameters with the total least-squares method. Number of source and destination coordinates must match.  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n  \nproperty rotation \n  \nproperty translation \n \n FundamentalMatrixTransform  \nclass skimage.transform.FundamentalMatrixTransform(matrix=None) [source]\n \nBases: skimage.transform._geometric.GeometricTransform Fundamental matrix transformation. The fundamental matrix relates corresponding points between a pair of uncalibrated images. The matrix transforms homogeneous image points in one image to epipolar lines in the other image. The fundamental matrix is only defined for a pair of moving images. In the case of pure rotation or planar scenes, the homography describes the geometric relation between two images (ProjectiveTransform). If the intrinsic calibration of the images is known, the essential matrix describes the metric relation between the two images (EssentialMatrixTransform).  Parameters \n \nmatrix(3, 3) array, optional \n\nFundamental matrix.     References  \n1  \nHartley, Richard, and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003.    Attributes \n \nparams(3, 3) array \n\nFundamental matrix.      \n__init__(matrix=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(src, dst) [source]\n \nEstimate fundamental matrix using 8-point algorithm. The 8-point algorithm requires at least 8 corresponding point pairs for a well-conditioned solution, otherwise the over-determined solution is estimated.  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n  \ninverse(coords) [source]\n \nApply inverse transformation.  Parameters \n \ncoords(N, 2) array \n\nDestination coordinates.    Returns \n \ncoords(N, 3) array \n\nEpipolar lines in the source image.     \n  \nresiduals(src, dst) [source]\n \nCompute the Sampson distance. The Sampson distance is the first approximation to the geometric error.  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.    Returns \n \nresiduals(N, ) array \n\nSampson distance.     \n \n PiecewiseAffineTransform  \nclass skimage.transform.PiecewiseAffineTransform [source]\n \nBases: skimage.transform._geometric.GeometricTransform 2D piecewise affine transformation. Control points are used to define the mapping. The transform is based on a Delaunay triangulation of the points to form a mesh. Each triangle is used to find a local affine transform.  Attributes \n \naffineslist of AffineTransform objects \n\nAffine transformations for each triangle in the mesh.  \ninverse_affineslist of AffineTransform objects \n\nInverse affine transformations for each triangle in the mesh.      \n__init__() [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(src, dst) [source]\n \nEstimate the transformation from a set of corresponding points. Number of source and destination coordinates must match.  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n  \ninverse(coords) [source]\n \nApply inverse transformation. Coordinates outside of the mesh will be set to - 1.  Parameters \n \ncoords(N, 2) array \n\nSource coordinates.    Returns \n \ncoords(N, 2) array \n\nTransformed coordinates.     \n \n PolynomialTransform  \nclass skimage.transform.PolynomialTransform(params=None) [source]\n \nBases: skimage.transform._geometric.GeometricTransform 2D polynomial transformation. Has the following form: X = sum[j=0:order]( sum[i=0:j]( a_ji * x**(j - i) * y**i ))\nY = sum[j=0:order]( sum[i=0:j]( b_ji * x**(j - i) * y**i ))\n  Parameters \n \nparams(2, N) array, optional \n\nPolynomial coefficients where N * 2 = (order + 1) * (order + 2). So, a_ji is defined in params[0, :] and b_ji in params[1, :].    Attributes \n \nparams(2, N) array \n\nPolynomial coefficients where N * 2 = (order + 1) * (order + 2). So, a_ji is defined in params[0, :] and b_ji in params[1, :].      \n__init__(params=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(src, dst, order=2) [source]\n \nEstimate the transformation from a set of corresponding points. You can determine the over-, well- and under-determined parameters with the total least-squares method. Number of source and destination coordinates must match. The transformation is defined as: X = sum[j=0:order]( sum[i=0:j]( a_ji * x**(j - i) * y**i ))\nY = sum[j=0:order]( sum[i=0:j]( b_ji * x**(j - i) * y**i ))\n These equations can be transformed to the following form: 0 = sum[j=0:order]( sum[i=0:j]( a_ji * x**(j - i) * y**i )) - X\n0 = sum[j=0:order]( sum[i=0:j]( b_ji * x**(j - i) * y**i )) - Y\n which exist for each set of corresponding points, so we have a set of N * 2 equations. The coefficients appear linearly so we can write A x = 0, where: A   = [[1 x y x**2 x*y y**2 ... 0 ...             0 -X]\n       [0 ...                 0 1 x y x**2 x*y y**2 -Y]\n        ...\n        ...\n      ]\nx.T = [a00 a10 a11 a20 a21 a22 ... ann\n       b00 b10 b11 b20 b21 b22 ... bnn c3]\n In case of total least-squares the solution of this homogeneous system of equations is the right singular vector of A which corresponds to the smallest singular value normed by the coefficient c3.  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.  \norderint, optional \n\nPolynomial order (number of coefficients is order + 1).    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n  \ninverse(coords) [source]\n \nApply inverse transformation.  Parameters \n \ncoords(N, 2) array \n\nDestination coordinates.    Returns \n \ncoords(N, 2) array \n\nSource coordinates.     \n \n ProjectiveTransform  \nclass skimage.transform.ProjectiveTransform(matrix=None) [source]\n \nBases: skimage.transform._geometric.GeometricTransform Projective transformation. Apply a projective transformation (homography) on coordinates. For each homogeneous coordinate \\(\\mathbf{x} = [x, y, 1]^T\\), its target position is calculated by multiplying with the given matrix, \\(H\\), to give \\(H \\mathbf{x}\\): [[a0 a1 a2]\n [b0 b1 b2]\n [c0 c1 1 ]].\n E.g., to rotate by theta degrees clockwise, the matrix should be: [[cos(theta) -sin(theta) 0]\n [sin(theta)  cos(theta) 0]\n [0            0         1]]\n or, to translate x by 10 and y by 20: [[1 0 10]\n [0 1 20]\n [0 0 1 ]].\n  Parameters \n \nmatrix(3, 3) array, optional \n\nHomogeneous transformation matrix.    Attributes \n \nparams(3, 3) array \n\nHomogeneous transformation matrix.      \n__init__(matrix=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(src, dst) [source]\n \nEstimate the transformation from a set of corresponding points. You can determine the over-, well- and under-determined parameters with the total least-squares method. Number of source and destination coordinates must match. The transformation is defined as: X = (a0*x + a1*y + a2) / (c0*x + c1*y + 1)\nY = (b0*x + b1*y + b2) / (c0*x + c1*y + 1)\n These equations can be transformed to the following form: 0 = a0*x + a1*y + a2 - c0*x*X - c1*y*X - X\n0 = b0*x + b1*y + b2 - c0*x*Y - c1*y*Y - Y\n which exist for each set of corresponding points, so we have a set of N * 2 equations. The coefficients appear linearly so we can write A x = 0, where: A   = [[x y 1 0 0 0 -x*X -y*X -X]\n       [0 0 0 x y 1 -x*Y -y*Y -Y]\n        ...\n        ...\n      ]\nx.T = [a0 a1 a2 b0 b1 b2 c0 c1 c3]\n In case of total least-squares the solution of this homogeneous system of equations is the right singular vector of A which corresponds to the smallest singular value normed by the coefficient c3. In case of the affine transformation the coefficients c0 and c1 are 0. Thus the system of equations is: A   = [[x y 1 0 0 0 -X]\n       [0 0 0 x y 1 -Y]\n        ...\n        ...\n      ]\nx.T = [a0 a1 a2 b0 b1 b2 c3]\n  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n  \ninverse(coords) [source]\n \nApply inverse transformation.  Parameters \n \ncoords(N, 2) array \n\nDestination coordinates.    Returns \n \ncoords(N, 2) array \n\nSource coordinates.     \n \n SimilarityTransform  \nclass skimage.transform.SimilarityTransform(matrix=None, scale=None, rotation=None, translation=None) [source]\n \nBases: skimage.transform._geometric.EuclideanTransform 2D similarity transformation. Has the following form: X = a0 * x - b0 * y + a1 =\n  = s * x * cos(rotation) - s * y * sin(rotation) + a1\n\nY = b0 * x + a0 * y + b1 =\n  = s * x * sin(rotation) + s * y * cos(rotation) + b1\n where s is a scale factor and the homogeneous transformation matrix is: [[a0  b0  a1]\n [b0  a0  b1]\n [0   0    1]]\n The similarity transformation extends the Euclidean transformation with a single scaling factor in addition to the rotation and translation parameters.  Parameters \n \nmatrix(3, 3) array, optional \n\nHomogeneous transformation matrix.  \nscalefloat, optional \n\nScale factor.  \nrotationfloat, optional \n\nRotation angle in counter-clockwise direction as radians.  \ntranslation(tx, ty) as array, list or tuple, optional \n\nx, y translation parameters.    Attributes \n \nparams(3, 3) array \n\nHomogeneous transformation matrix.      \n__init__(matrix=None, scale=None, rotation=None, translation=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(src, dst) [source]\n \nEstimate the transformation from a set of corresponding points. You can determine the over-, well- and under-determined parameters with the total least-squares method. Number of source and destination coordinates must match.  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n  \nproperty scale \n \n\n"}, {"name": "transform.AffineTransform", "path": "api/skimage.transform#skimage.transform.AffineTransform", "type": "transform", "text": " \nclass skimage.transform.AffineTransform(matrix=None, scale=None, rotation=None, shear=None, translation=None) [source]\n \nBases: skimage.transform._geometric.ProjectiveTransform 2D affine transformation. Has the following form: X = a0*x + a1*y + a2 =\n  = sx*x*cos(rotation) - sy*y*sin(rotation + shear) + a2\n\nY = b0*x + b1*y + b2 =\n  = sx*x*sin(rotation) + sy*y*cos(rotation + shear) + b2\n where sx and sy are scale factors in the x and y directions, and the homogeneous transformation matrix is: [[a0  a1  a2]\n [b0  b1  b2]\n [0   0    1]]\n  Parameters \n \nmatrix(3, 3) array, optional \n\nHomogeneous transformation matrix.  \nscale{s as float or (sx, sy) as array, list or tuple}, optional \n\nScale factor(s). If a single value, it will be assigned to both sx and sy.  New in version 0.17: Added support for supplying a single scalar value.   \nrotationfloat, optional \n\nRotation angle in counter-clockwise direction as radians.  \nshearfloat, optional \n\nShear angle in counter-clockwise direction as radians.  \ntranslation(tx, ty) as array, list or tuple, optional \n\nTranslation parameters.    Attributes \n \nparams(3, 3) array \n\nHomogeneous transformation matrix.      \n__init__(matrix=None, scale=None, rotation=None, shear=None, translation=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nproperty rotation \n  \nproperty scale \n  \nproperty shear \n  \nproperty translation \n \n"}, {"name": "transform.AffineTransform.rotation()", "path": "api/skimage.transform#skimage.transform.AffineTransform.rotation", "type": "transform", "text": " \nproperty rotation \n"}, {"name": "transform.AffineTransform.scale()", "path": "api/skimage.transform#skimage.transform.AffineTransform.scale", "type": "transform", "text": " \nproperty scale \n"}, {"name": "transform.AffineTransform.shear()", "path": "api/skimage.transform#skimage.transform.AffineTransform.shear", "type": "transform", "text": " \nproperty shear \n"}, {"name": "transform.AffineTransform.translation()", "path": "api/skimage.transform#skimage.transform.AffineTransform.translation", "type": "transform", "text": " \nproperty translation \n"}, {"name": "transform.AffineTransform.__init__()", "path": "api/skimage.transform#skimage.transform.AffineTransform.__init__", "type": "transform", "text": " \n__init__(matrix=None, scale=None, rotation=None, shear=None, translation=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "transform.downscale_local_mean()", "path": "api/skimage.transform#skimage.transform.downscale_local_mean", "type": "transform", "text": " \nskimage.transform.downscale_local_mean(image, factors, cval=0, clip=True) [source]\n \nDown-sample N-dimensional image by local averaging. The image is padded with cval if it is not perfectly divisible by the integer factors. In contrast to interpolation in skimage.transform.resize and skimage.transform.rescale this function calculates the local mean of elements in each block of size factors in the input image.  Parameters \n \nimagendarray \n\nN-dimensional input image.  \nfactorsarray_like \n\nArray containing down-sampling integer factor along each axis.  \ncvalfloat, optional \n\nConstant padding value if image is not perfectly divisible by the integer factors.  \nclipbool, optional \n\nUnused, but kept here for API consistency with the other transforms in this module. (The local mean will never fall outside the range of values in the input image, assuming the provided cval also falls within that range.)    Returns \n \nimagendarray \n\nDown-sampled image with same number of dimensions as input image. For integer inputs, the output dtype will be float64. See numpy.mean() for details.     Examples >>> a = np.arange(15).reshape(3, 5)\n>>> a\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])\n>>> downscale_local_mean(a, (2, 3))\narray([[3.5, 4. ],\n       [5.5, 4.5]])\n \n"}, {"name": "transform.EssentialMatrixTransform", "path": "api/skimage.transform#skimage.transform.EssentialMatrixTransform", "type": "transform", "text": " \nclass skimage.transform.EssentialMatrixTransform(rotation=None, translation=None, matrix=None) [source]\n \nBases: skimage.transform._geometric.FundamentalMatrixTransform Essential matrix transformation. The essential matrix relates corresponding points between a pair of calibrated images. The matrix transforms normalized, homogeneous image points in one image to epipolar lines in the other image. The essential matrix is only defined for a pair of moving images capturing a non-planar scene. In the case of pure rotation or planar scenes, the homography describes the geometric relation between two images (ProjectiveTransform). If the intrinsic calibration of the images is unknown, the fundamental matrix describes the projective relation between the two images (FundamentalMatrixTransform).  Parameters \n \nrotation(3, 3) array, optional \n\nRotation matrix of the relative camera motion.  \ntranslation(3, 1) array, optional \n\nTranslation vector of the relative camera motion. The vector must have unit length.  \nmatrix(3, 3) array, optional \n\nEssential matrix.     References  \n1  \nHartley, Richard, and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003.    Attributes \n \nparams(3, 3) array \n\nEssential matrix.      \n__init__(rotation=None, translation=None, matrix=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(src, dst) [source]\n \nEstimate essential matrix using 8-point algorithm. The 8-point algorithm requires at least 8 corresponding point pairs for a well-conditioned solution, otherwise the over-determined solution is estimated.  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n \n"}, {"name": "transform.EssentialMatrixTransform.estimate()", "path": "api/skimage.transform#skimage.transform.EssentialMatrixTransform.estimate", "type": "transform", "text": " \nestimate(src, dst) [source]\n \nEstimate essential matrix using 8-point algorithm. The 8-point algorithm requires at least 8 corresponding point pairs for a well-conditioned solution, otherwise the over-determined solution is estimated.  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n"}, {"name": "transform.EssentialMatrixTransform.__init__()", "path": "api/skimage.transform#skimage.transform.EssentialMatrixTransform.__init__", "type": "transform", "text": " \n__init__(rotation=None, translation=None, matrix=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "transform.estimate_transform()", "path": "api/skimage.transform#skimage.transform.estimate_transform", "type": "transform", "text": " \nskimage.transform.estimate_transform(ttype, src, dst, **kwargs) [source]\n \nEstimate 2D geometric transformation parameters. You can determine the over-, well- and under-determined parameters with the total least-squares method. Number of source and destination coordinates must match.  Parameters \n \nttype{\u2018euclidean\u2019, similarity\u2019, \u2018affine\u2019, \u2018piecewise-affine\u2019, \u2018projective\u2019, \u2018polynomial\u2019} \n\nType of transform.  \nkwargsarray or int \n\nFunction parameters (src, dst, n, angle): NAME / TTYPE        FUNCTION PARAMETERS\n'euclidean'         `src, `dst`\n'similarity'        `src, `dst`\n'affine'            `src, `dst`\n'piecewise-affine'  `src, `dst`\n'projective'        `src, `dst`\n'polynomial'        `src, `dst`, `order` (polynomial order,\n                                          default order is 2)\n Also see examples below.    Returns \n \ntformGeometricTransform \n\nTransform object containing the transformation parameters and providing access to forward and inverse transformation functions.     Examples >>> import numpy as np\n>>> from skimage import transform\n >>> # estimate transformation parameters\n>>> src = np.array([0, 0, 10, 10]).reshape((2, 2))\n>>> dst = np.array([12, 14, 1, -20]).reshape((2, 2))\n >>> tform = transform.estimate_transform('similarity', src, dst)\n >>> np.allclose(tform.inverse(tform(src)), src)\nTrue\n >>> # warp image using the estimated transformation\n>>> from skimage import data\n>>> image = data.camera()\n >>> warp(image, inverse_map=tform.inverse) \n >>> # create transformation with explicit parameters\n>>> tform2 = transform.SimilarityTransform(scale=1.1, rotation=1,\n...     translation=(10, 20))\n >>> # unite transformations, applied in order from left to right\n>>> tform3 = tform + tform2\n>>> np.allclose(tform3(src), tform2(tform(src)))\nTrue\n \n"}, {"name": "transform.EuclideanTransform", "path": "api/skimage.transform#skimage.transform.EuclideanTransform", "type": "transform", "text": " \nclass skimage.transform.EuclideanTransform(matrix=None, rotation=None, translation=None) [source]\n \nBases: skimage.transform._geometric.ProjectiveTransform 2D Euclidean transformation. Has the following form: X = a0 * x - b0 * y + a1 =\n  = x * cos(rotation) - y * sin(rotation) + a1\n\nY = b0 * x + a0 * y + b1 =\n  = x * sin(rotation) + y * cos(rotation) + b1\n where the homogeneous transformation matrix is: [[a0  b0  a1]\n [b0  a0  b1]\n [0   0    1]]\n The Euclidean transformation is a rigid transformation with rotation and translation parameters. The similarity transformation extends the Euclidean transformation with a single scaling factor.  Parameters \n \nmatrix(3, 3) array, optional \n\nHomogeneous transformation matrix.  \nrotationfloat, optional \n\nRotation angle in counter-clockwise direction as radians.  \ntranslation(tx, ty) as array, list or tuple, optional \n\nx, y translation parameters.    Attributes \n \nparams(3, 3) array \n\nHomogeneous transformation matrix.      \n__init__(matrix=None, rotation=None, translation=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(src, dst) [source]\n \nEstimate the transformation from a set of corresponding points. You can determine the over-, well- and under-determined parameters with the total least-squares method. Number of source and destination coordinates must match.  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n  \nproperty rotation \n  \nproperty translation \n \n"}, {"name": "transform.EuclideanTransform.estimate()", "path": "api/skimage.transform#skimage.transform.EuclideanTransform.estimate", "type": "transform", "text": " \nestimate(src, dst) [source]\n \nEstimate the transformation from a set of corresponding points. You can determine the over-, well- and under-determined parameters with the total least-squares method. Number of source and destination coordinates must match.  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n"}, {"name": "transform.EuclideanTransform.rotation()", "path": "api/skimage.transform#skimage.transform.EuclideanTransform.rotation", "type": "transform", "text": " \nproperty rotation \n"}, {"name": "transform.EuclideanTransform.translation()", "path": "api/skimage.transform#skimage.transform.EuclideanTransform.translation", "type": "transform", "text": " \nproperty translation \n"}, {"name": "transform.EuclideanTransform.__init__()", "path": "api/skimage.transform#skimage.transform.EuclideanTransform.__init__", "type": "transform", "text": " \n__init__(matrix=None, rotation=None, translation=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "transform.frt2()", "path": "api/skimage.transform#skimage.transform.frt2", "type": "transform", "text": " \nskimage.transform.frt2(a) [source]\n \nCompute the 2-dimensional finite radon transform (FRT) for an n x n integer array.  Parameters \n \naarray_like \n\nA 2-D square n x n integer array.    Returns \n \nFRT2-D ndarray \n\nFinite Radon Transform array of (n+1) x n integer coefficients.      See also  \nifrt2\n\n\nThe two-dimensional inverse FRT.    Notes The FRT has a unique inverse if and only if n is prime. [FRT] The idea for this algorithm is due to Vlad Negnevitski. References  \nFRT  \nA. Kingston and I. Svalbe, \u201cProjective transforms on periodic discrete image arrays,\u201d in P. Hawkes (Ed), Advances in Imaging and Electron Physics, 139 (2006)   Examples Generate a test image: Use a prime number for the array dimensions >>> SIZE = 59\n>>> img = np.tri(SIZE, dtype=np.int32)\n Apply the Finite Radon Transform: >>> f = frt2(img)\n \n"}, {"name": "transform.FundamentalMatrixTransform", "path": "api/skimage.transform#skimage.transform.FundamentalMatrixTransform", "type": "transform", "text": " \nclass skimage.transform.FundamentalMatrixTransform(matrix=None) [source]\n \nBases: skimage.transform._geometric.GeometricTransform Fundamental matrix transformation. The fundamental matrix relates corresponding points between a pair of uncalibrated images. The matrix transforms homogeneous image points in one image to epipolar lines in the other image. The fundamental matrix is only defined for a pair of moving images. In the case of pure rotation or planar scenes, the homography describes the geometric relation between two images (ProjectiveTransform). If the intrinsic calibration of the images is known, the essential matrix describes the metric relation between the two images (EssentialMatrixTransform).  Parameters \n \nmatrix(3, 3) array, optional \n\nFundamental matrix.     References  \n1  \nHartley, Richard, and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003.    Attributes \n \nparams(3, 3) array \n\nFundamental matrix.      \n__init__(matrix=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(src, dst) [source]\n \nEstimate fundamental matrix using 8-point algorithm. The 8-point algorithm requires at least 8 corresponding point pairs for a well-conditioned solution, otherwise the over-determined solution is estimated.  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n  \ninverse(coords) [source]\n \nApply inverse transformation.  Parameters \n \ncoords(N, 2) array \n\nDestination coordinates.    Returns \n \ncoords(N, 3) array \n\nEpipolar lines in the source image.     \n  \nresiduals(src, dst) [source]\n \nCompute the Sampson distance. The Sampson distance is the first approximation to the geometric error.  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.    Returns \n \nresiduals(N, ) array \n\nSampson distance.     \n \n"}, {"name": "transform.FundamentalMatrixTransform.estimate()", "path": "api/skimage.transform#skimage.transform.FundamentalMatrixTransform.estimate", "type": "transform", "text": " \nestimate(src, dst) [source]\n \nEstimate fundamental matrix using 8-point algorithm. The 8-point algorithm requires at least 8 corresponding point pairs for a well-conditioned solution, otherwise the over-determined solution is estimated.  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n"}, {"name": "transform.FundamentalMatrixTransform.inverse()", "path": "api/skimage.transform#skimage.transform.FundamentalMatrixTransform.inverse", "type": "transform", "text": " \ninverse(coords) [source]\n \nApply inverse transformation.  Parameters \n \ncoords(N, 2) array \n\nDestination coordinates.    Returns \n \ncoords(N, 3) array \n\nEpipolar lines in the source image.     \n"}, {"name": "transform.FundamentalMatrixTransform.residuals()", "path": "api/skimage.transform#skimage.transform.FundamentalMatrixTransform.residuals", "type": "transform", "text": " \nresiduals(src, dst) [source]\n \nCompute the Sampson distance. The Sampson distance is the first approximation to the geometric error.  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.    Returns \n \nresiduals(N, ) array \n\nSampson distance.     \n"}, {"name": "transform.FundamentalMatrixTransform.__init__()", "path": "api/skimage.transform#skimage.transform.FundamentalMatrixTransform.__init__", "type": "transform", "text": " \n__init__(matrix=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "transform.hough_circle()", "path": "api/skimage.transform#skimage.transform.hough_circle", "type": "transform", "text": " \nskimage.transform.hough_circle(image, radius, normalize=True, full_output=False) [source]\n \nPerform a circular Hough transform.  Parameters \n \nimage(M, N) ndarray \n\nInput image with nonzero values representing edges.  \nradiusscalar or sequence of scalars \n\nRadii at which to compute the Hough transform. Floats are converted to integers.  \nnormalizeboolean, optional (default True) \n\nNormalize the accumulator with the number of pixels used to draw the radius.  \nfull_outputboolean, optional (default False) \n\nExtend the output size by twice the largest radius in order to detect centers outside the input picture.    Returns \n \nH3D ndarray (radius index, (M + 2R, N + 2R) ndarray) \n\nHough transform accumulator for each radius. R designates the larger radius if full_output is True. Otherwise, R = 0.     Examples >>> from skimage.transform import hough_circle\n>>> from skimage.draw import circle_perimeter\n>>> img = np.zeros((100, 100), dtype=bool)\n>>> rr, cc = circle_perimeter(25, 35, 23)\n>>> img[rr, cc] = 1\n>>> try_radii = np.arange(5, 50)\n>>> res = hough_circle(img, try_radii)\n>>> ridx, r, c = np.unravel_index(np.argmax(res), res.shape)\n>>> r, c, try_radii[ridx]\n(25, 35, 23)\n \n"}, {"name": "transform.hough_circle_peaks()", "path": "api/skimage.transform#skimage.transform.hough_circle_peaks", "type": "transform", "text": " \nskimage.transform.hough_circle_peaks(hspaces, radii, min_xdistance=1, min_ydistance=1, threshold=None, num_peaks=inf, total_num_peaks=inf, normalize=False) [source]\n \nReturn peaks in a circle Hough transform. Identifies most prominent circles separated by certain distances in given Hough spaces. Non-maximum suppression with different sizes is applied separately in the first and second dimension of the Hough space to identify peaks. For circles with different radius but close in distance, only the one with highest peak is kept.  Parameters \n \nhspaces(N, M) array \n\nHough spaces returned by the hough_circle function.  \nradii(M,) array \n\nRadii corresponding to Hough spaces.  \nmin_xdistanceint, optional \n\nMinimum distance separating centers in the x dimension.  \nmin_ydistanceint, optional \n\nMinimum distance separating centers in the y dimension.  \nthresholdfloat, optional \n\nMinimum intensity of peaks in each Hough space. Default is 0.5 * max(hspace).  \nnum_peaksint, optional \n\nMaximum number of peaks in each Hough space. When the number of peaks exceeds num_peaks, only num_peaks coordinates based on peak intensity are considered for the corresponding radius.  \ntotal_num_peaksint, optional \n\nMaximum number of peaks. When the number of peaks exceeds num_peaks, return num_peaks coordinates based on peak intensity.  \nnormalizebool, optional \n\nIf True, normalize the accumulator by the radius to sort the prominent peaks.    Returns \n \naccum, cx, cy, radtuple of array \n\nPeak values in Hough space, x and y center coordinates and radii.     Notes Circles with bigger radius have higher peaks in Hough space. If larger circles are preferred over smaller ones, normalize should be False. Otherwise, circles will be returned in the order of decreasing voting number. Examples >>> from skimage import transform, draw\n>>> img = np.zeros((120, 100), dtype=int)\n>>> radius, x_0, y_0 = (20, 99, 50)\n>>> y, x = draw.circle_perimeter(y_0, x_0, radius)\n>>> img[x, y] = 1\n>>> hspaces = transform.hough_circle(img, radius)\n>>> accum, cx, cy, rad = hough_circle_peaks(hspaces, [radius,])\n \n"}, {"name": "transform.hough_ellipse()", "path": "api/skimage.transform#skimage.transform.hough_ellipse", "type": "transform", "text": " \nskimage.transform.hough_ellipse(image, threshold=4, accuracy=1, min_size=4, max_size=None) [source]\n \nPerform an elliptical Hough transform.  Parameters \n \nimage(M, N) ndarray \n\nInput image with nonzero values representing edges.  \nthresholdint, optional \n\nAccumulator threshold value.  \naccuracydouble, optional \n\nBin size on the minor axis used in the accumulator.  \nmin_sizeint, optional \n\nMinimal major axis length.  \nmax_sizeint, optional \n\nMaximal minor axis length. If None, the value is set to the half of the smaller image dimension.    Returns \n \nresultndarray with fields [(accumulator, yc, xc, a, b, orientation)]. \n\nWhere (yc, xc) is the center, (a, b) the major and minor axes, respectively. The orientation value follows skimage.draw.ellipse_perimeter convention.     Notes The accuracy must be chosen to produce a peak in the accumulator distribution. In other words, a flat accumulator distribution with low values may be caused by a too low bin size. References  \n1  \nXie, Yonghong, and Qiang Ji. \u201cA new efficient ellipse detection method.\u201d Pattern Recognition, 2002. Proceedings. 16th International Conference on. Vol. 2. IEEE, 2002   Examples >>> from skimage.transform import hough_ellipse\n>>> from skimage.draw import ellipse_perimeter\n>>> img = np.zeros((25, 25), dtype=np.uint8)\n>>> rr, cc = ellipse_perimeter(10, 10, 6, 8)\n>>> img[cc, rr] = 1\n>>> result = hough_ellipse(img, threshold=8)\n>>> result.tolist()\n[(10, 10.0, 10.0, 8.0, 6.0, 0.0)]\n \n"}, {"name": "transform.hough_line()", "path": "api/skimage.transform#skimage.transform.hough_line", "type": "transform", "text": " \nskimage.transform.hough_line(image, theta=None) [source]\n \nPerform a straight line Hough transform.  Parameters \n \nimage(M, N) ndarray \n\nInput image with nonzero values representing edges.  \ntheta1D ndarray of double, optional \n\nAngles at which to compute the transform, in radians. Defaults to a vector of 180 angles evenly spaced from -pi/2 to pi/2.    Returns \n \nhspace2-D ndarray of uint64 \n\nHough transform accumulator.  \nanglesndarray \n\nAngles at which the transform is computed, in radians.  \ndistancesndarray \n\nDistance values.     Notes The origin is the top left corner of the original image. X and Y axis are horizontal and vertical edges respectively. The distance is the minimal algebraic distance from the origin to the detected line. The angle accuracy can be improved by decreasing the step size in the theta array. Examples Generate a test image: >>> img = np.zeros((100, 150), dtype=bool)\n>>> img[30, :] = 1\n>>> img[:, 65] = 1\n>>> img[35:45, 35:50] = 1\n>>> for i in range(90):\n...     img[i, i] = 1\n>>> img += np.random.random(img.shape) > 0.95\n Apply the Hough transform: >>> out, angles, d = hough_line(img)\n import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom skimage.transform import hough_line\nfrom skimage.draw import line\n\nimg = np.zeros((100, 150), dtype=bool)\nimg[30, :] = 1\nimg[:, 65] = 1\nimg[35:45, 35:50] = 1\nrr, cc = line(60, 130, 80, 10)\nimg[rr, cc] = 1\nimg += np.random.random(img.shape) > 0.95\n\nout, angles, d = hough_line(img)\n\nfix, axes = plt.subplots(1, 2, figsize=(7, 4))\n\naxes[0].imshow(img, cmap=plt.cm.gray)\naxes[0].set_title('Input image')\n\naxes[1].imshow(\n    out, cmap=plt.cm.bone,\n    extent=(np.rad2deg(angles[-1]), np.rad2deg(angles[0]), d[-1], d[0]))\naxes[1].set_title('Hough transform')\naxes[1].set_xlabel('Angle (degree)')\naxes[1].set_ylabel('Distance (pixel)')\n\nplt.tight_layout()\nplt.show()\n (Source code, png, pdf)    \n"}, {"name": "transform.hough_line_peaks()", "path": "api/skimage.transform#skimage.transform.hough_line_peaks", "type": "transform", "text": " \nskimage.transform.hough_line_peaks(hspace, angles, dists, min_distance=9, min_angle=10, threshold=None, num_peaks=inf) [source]\n \nReturn peaks in a straight line Hough transform. Identifies most prominent lines separated by a certain angle and distance in a Hough transform. Non-maximum suppression with different sizes is applied separately in the first (distances) and second (angles) dimension of the Hough space to identify peaks.  Parameters \n \nhspace(N, M) array \n\nHough space returned by the hough_line function.  \nangles(M,) array \n\nAngles returned by the hough_line function. Assumed to be continuous. (angles[-1] - angles[0] == PI).  \ndists(N, ) array \n\nDistances returned by the hough_line function.  \nmin_distanceint, optional \n\nMinimum distance separating lines (maximum filter size for first dimension of hough space).  \nmin_angleint, optional \n\nMinimum angle separating lines (maximum filter size for second dimension of hough space).  \nthresholdfloat, optional \n\nMinimum intensity of peaks. Default is 0.5 * max(hspace).  \nnum_peaksint, optional \n\nMaximum number of peaks. When the number of peaks exceeds num_peaks, return num_peaks coordinates based on peak intensity.    Returns \n \naccum, angles, diststuple of array \n\nPeak values in Hough space, angles and distances.     Examples >>> from skimage.transform import hough_line, hough_line_peaks\n>>> from skimage.draw import line\n>>> img = np.zeros((15, 15), dtype=bool)\n>>> rr, cc = line(0, 0, 14, 14)\n>>> img[rr, cc] = 1\n>>> rr, cc = line(0, 14, 14, 0)\n>>> img[cc, rr] = 1\n>>> hspace, angles, dists = hough_line(img)\n>>> hspace, angles, dists = hough_line_peaks(hspace, angles, dists)\n>>> len(angles)\n2\n \n"}, {"name": "transform.ifrt2()", "path": "api/skimage.transform#skimage.transform.ifrt2", "type": "transform", "text": " \nskimage.transform.ifrt2(a) [source]\n \nCompute the 2-dimensional inverse finite radon transform (iFRT) for an (n+1) x n integer array.  Parameters \n \naarray_like \n\nA 2-D (n+1) row x n column integer array.    Returns \n \niFRT2-D n x n ndarray \n\nInverse Finite Radon Transform array of n x n integer coefficients.      See also  \nfrt2\n\n\nThe two-dimensional FRT    Notes The FRT has a unique inverse if and only if n is prime. See [1] for an overview. The idea for this algorithm is due to Vlad Negnevitski. References  \n1  \nA. Kingston and I. Svalbe, \u201cProjective transforms on periodic discrete image arrays,\u201d in P. Hawkes (Ed), Advances in Imaging and Electron Physics, 139 (2006)   Examples >>> SIZE = 59\n>>> img = np.tri(SIZE, dtype=np.int32)\n Apply the Finite Radon Transform: >>> f = frt2(img)\n Apply the Inverse Finite Radon Transform to recover the input >>> fi = ifrt2(f)\n Check that it\u2019s identical to the original >>> assert len(np.nonzero(img-fi)[0]) == 0\n \n"}, {"name": "transform.integral_image()", "path": "api/skimage.transform#skimage.transform.integral_image", "type": "transform", "text": " \nskimage.transform.integral_image(image) [source]\n \nIntegral image / summed area table. The integral image contains the sum of all elements above and to the left of it, i.e.:  \\[S[m, n] = \\sum_{i \\leq m} \\sum_{j \\leq n} X[i, j]\\]  Parameters \n \nimagendarray \n\nInput image.    Returns \n \nSndarray \n\nIntegral image/summed area table of same shape as input image.     References  \n1  \nF.C. Crow, \u201cSummed-area tables for texture mapping,\u201d ACM SIGGRAPH Computer Graphics, vol. 18, 1984, pp. 207-212.   \n"}, {"name": "transform.integrate()", "path": "api/skimage.transform#skimage.transform.integrate", "type": "transform", "text": " \nskimage.transform.integrate(ii, start, end) [source]\n \nUse an integral image to integrate over a given window.  Parameters \n \niindarray \n\nIntegral image.  \nstartList of tuples, each tuple of length equal to dimension of ii \n\nCoordinates of top left corner of window(s). Each tuple in the list contains the starting row, col, \u2026 index i.e [(row_win1, col_win1, \u2026), (row_win2, col_win2,\u2026), \u2026].  \nendList of tuples, each tuple of length equal to dimension of ii \n\nCoordinates of bottom right corner of window(s). Each tuple in the list containing the end row, col, \u2026 index i.e [(row_win1, col_win1, \u2026), (row_win2, col_win2, \u2026), \u2026].    Returns \n \nSscalar or ndarray \n\nIntegral (sum) over the given window(s).     Examples >>> arr = np.ones((5, 6), dtype=float)\n>>> ii = integral_image(arr)\n>>> integrate(ii, (1, 0), (1, 2))  # sum from (1, 0) to (1, 2)\narray([3.])\n>>> integrate(ii, [(3, 3)], [(4, 5)])  # sum from (3, 3) to (4, 5)\narray([6.])\n>>> # sum from (1, 0) to (1, 2) and from (3, 3) to (4, 5)\n>>> integrate(ii, [(1, 0), (3, 3)], [(1, 2), (4, 5)])\narray([3., 6.])\n \n"}, {"name": "transform.iradon()", "path": "api/skimage.transform#skimage.transform.iradon", "type": "transform", "text": " \nskimage.transform.iradon(radon_image, theta=None, output_size=None, filter_name='ramp', interpolation='linear', circle=True, preserve_range=True) [source]\n \nInverse radon transform. Reconstruct an image from the radon transform, using the filtered back projection algorithm.  Parameters \n \nradon_imagearray \n\nImage containing radon transform (sinogram). Each column of the image corresponds to a projection along a different angle. The tomography rotation axis should lie at the pixel index radon_image.shape[0] // 2 along the 0th dimension of radon_image.  \nthetaarray_like, optional \n\nReconstruction angles (in degrees). Default: m angles evenly spaced between 0 and 180 (if the shape of radon_image is (N, M)).  \noutput_sizeint, optional \n\nNumber of rows and columns in the reconstruction.  \nfilter_namestr, optional \n\nFilter used in frequency domain filtering. Ramp filter used by default. Filters available: ramp, shepp-logan, cosine, hamming, hann. Assign None to use no filter.  \ninterpolationstr, optional \n\nInterpolation method used in reconstruction. Methods available: \u2018linear\u2019, \u2018nearest\u2019, and \u2018cubic\u2019 (\u2018cubic\u2019 is slow).  \ncircleboolean, optional \n\nAssume the reconstructed image is zero outside the inscribed circle. Also changes the default output_size to match the behaviour of radon called with circle=True.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html    Returns \n \nreconstructedndarray \n\nReconstructed image. The rotation axis will be located in the pixel with indices (reconstructed.shape[0] // 2, reconstructed.shape[1] // 2).    Changed in version 0.19: In iradon, filter argument is deprecated in favor of filter_name.    Notes It applies the Fourier slice theorem to reconstruct an image by multiplying the frequency domain of the filter with the FFT of the projection data. This algorithm is called filtered back projection. References  \n1  \nAC Kak, M Slaney, \u201cPrinciples of Computerized Tomographic Imaging\u201d, IEEE Press 1988.  \n2  \nB.R. Ramesh, N. Srinivasa, K. Rajgopal, \u201cAn Algorithm for Computing the Discrete Radon Transform With Some Applications\u201d, Proceedings of the Fourth IEEE Region 10 International Conference, TENCON \u201889, 1989   \n"}, {"name": "transform.iradon_sart()", "path": "api/skimage.transform#skimage.transform.iradon_sart", "type": "transform", "text": " \nskimage.transform.iradon_sart(radon_image, theta=None, image=None, projection_shifts=None, clip=None, relaxation=0.15, dtype=None) [source]\n \nInverse radon transform. Reconstruct an image from the radon transform, using a single iteration of the Simultaneous Algebraic Reconstruction Technique (SART) algorithm.  Parameters \n \nradon_image2D array \n\nImage containing radon transform (sinogram). Each column of the image corresponds to a projection along a different angle. The tomography rotation axis should lie at the pixel index radon_image.shape[0] // 2 along the 0th dimension of radon_image.  \ntheta1D array, optional \n\nReconstruction angles (in degrees). Default: m angles evenly spaced between 0 and 180 (if the shape of radon_image is (N, M)).  \nimage2D array, optional \n\nImage containing an initial reconstruction estimate. Shape of this array should be (radon_image.shape[0], radon_image.shape[0]). The default is an array of zeros.  \nprojection_shifts1D array, optional \n\nShift the projections contained in radon_image (the sinogram) by this many pixels before reconstructing the image. The i\u2019th value defines the shift of the i\u2019th column of radon_image.  \ncliplength-2 sequence of floats, optional \n\nForce all values in the reconstructed tomogram to lie in the range [clip[0], clip[1]]  \nrelaxationfloat, optional \n\nRelaxation parameter for the update step. A higher value can improve the convergence rate, but one runs the risk of instabilities. Values close to or higher than 1 are not recommended.  \ndtypedtype, optional \n\nOutput data type, must be floating point. By default, if input data type is not float, input is cast to double, otherwise dtype is set to input data type.    Returns \n \nreconstructedndarray \n\nReconstructed image. The rotation axis will be located in the pixel with indices (reconstructed.shape[0] // 2, reconstructed.shape[1] // 2).     Notes Algebraic Reconstruction Techniques are based on formulating the tomography reconstruction problem as a set of linear equations. Along each ray, the projected value is the sum of all the values of the cross section along the ray. A typical feature of SART (and a few other variants of algebraic techniques) is that it samples the cross section at equidistant points along the ray, using linear interpolation between the pixel values of the cross section. The resulting set of linear equations are then solved using a slightly modified Kaczmarz method. When using SART, a single iteration is usually sufficient to obtain a good reconstruction. Further iterations will tend to enhance high-frequency information, but will also often increase the noise. References  \n1  \nAC Kak, M Slaney, \u201cPrinciples of Computerized Tomographic Imaging\u201d, IEEE Press 1988.  \n2  \nAH Andersen, AC Kak, \u201cSimultaneous algebraic reconstruction technique (SART): a superior implementation of the ART algorithm\u201d, Ultrasonic Imaging 6 pp 81\u201394 (1984)  \n3  \nS Kaczmarz, \u201cAngen\u00e4herte aufl\u00f6sung von systemen linearer gleichungen\u201d, Bulletin International de l\u2019Academie Polonaise des Sciences et des Lettres 35 pp 355\u2013357 (1937)  \n4  \nKohler, T. \u201cA projection access scheme for iterative reconstruction based on the golden section.\u201d Nuclear Science Symposium Conference Record, 2004 IEEE. Vol. 6. IEEE, 2004.  \n5  \nKaczmarz\u2019 method, Wikipedia, https://en.wikipedia.org/wiki/Kaczmarz_method   \n"}, {"name": "transform.matrix_transform()", "path": "api/skimage.transform#skimage.transform.matrix_transform", "type": "transform", "text": " \nskimage.transform.matrix_transform(coords, matrix) [source]\n \nApply 2D matrix transform.  Parameters \n \ncoords(N, 2) array \n\nx, y coordinates to transform  \nmatrix(3, 3) array \n\nHomogeneous transformation matrix.    Returns \n \ncoords(N, 2) array \n\nTransformed coordinates.     \n"}, {"name": "transform.order_angles_golden_ratio()", "path": "api/skimage.transform#skimage.transform.order_angles_golden_ratio", "type": "transform", "text": " \nskimage.transform.order_angles_golden_ratio(theta) [source]\n \nOrder angles to reduce the amount of correlated information in subsequent projections.  Parameters \n \ntheta1D array of floats \n\nProjection angles in degrees. Duplicate angles are not allowed.    Returns \n \nindices_generatorgenerator yielding unsigned integers \n\nThe returned generator yields indices into theta such that theta[indices] gives the approximate golden ratio ordering of the projections. In total, len(theta) indices are yielded. All non-negative integers < len(theta) are yielded exactly once.     Notes The method used here is that of the golden ratio introduced by T. Kohler. References  \n1  \nKohler, T. \u201cA projection access scheme for iterative reconstruction based on the golden section.\u201d Nuclear Science Symposium Conference Record, 2004 IEEE. Vol. 6. IEEE, 2004.  \n2  \nWinkelmann, Stefanie, et al. \u201cAn optimal radial profile order based on the Golden Ratio for time-resolved MRI.\u201d Medical Imaging, IEEE Transactions on 26.1 (2007): 68-76.   \n"}, {"name": "transform.PiecewiseAffineTransform", "path": "api/skimage.transform#skimage.transform.PiecewiseAffineTransform", "type": "transform", "text": " \nclass skimage.transform.PiecewiseAffineTransform [source]\n \nBases: skimage.transform._geometric.GeometricTransform 2D piecewise affine transformation. Control points are used to define the mapping. The transform is based on a Delaunay triangulation of the points to form a mesh. Each triangle is used to find a local affine transform.  Attributes \n \naffineslist of AffineTransform objects \n\nAffine transformations for each triangle in the mesh.  \ninverse_affineslist of AffineTransform objects \n\nInverse affine transformations for each triangle in the mesh.      \n__init__() [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(src, dst) [source]\n \nEstimate the transformation from a set of corresponding points. Number of source and destination coordinates must match.  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n  \ninverse(coords) [source]\n \nApply inverse transformation. Coordinates outside of the mesh will be set to - 1.  Parameters \n \ncoords(N, 2) array \n\nSource coordinates.    Returns \n \ncoords(N, 2) array \n\nTransformed coordinates.     \n \n"}, {"name": "transform.PiecewiseAffineTransform.estimate()", "path": "api/skimage.transform#skimage.transform.PiecewiseAffineTransform.estimate", "type": "transform", "text": " \nestimate(src, dst) [source]\n \nEstimate the transformation from a set of corresponding points. Number of source and destination coordinates must match.  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n"}, {"name": "transform.PiecewiseAffineTransform.inverse()", "path": "api/skimage.transform#skimage.transform.PiecewiseAffineTransform.inverse", "type": "transform", "text": " \ninverse(coords) [source]\n \nApply inverse transformation. Coordinates outside of the mesh will be set to - 1.  Parameters \n \ncoords(N, 2) array \n\nSource coordinates.    Returns \n \ncoords(N, 2) array \n\nTransformed coordinates.     \n"}, {"name": "transform.PiecewiseAffineTransform.__init__()", "path": "api/skimage.transform#skimage.transform.PiecewiseAffineTransform.__init__", "type": "transform", "text": " \n__init__() [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "transform.PolynomialTransform", "path": "api/skimage.transform#skimage.transform.PolynomialTransform", "type": "transform", "text": " \nclass skimage.transform.PolynomialTransform(params=None) [source]\n \nBases: skimage.transform._geometric.GeometricTransform 2D polynomial transformation. Has the following form: X = sum[j=0:order]( sum[i=0:j]( a_ji * x**(j - i) * y**i ))\nY = sum[j=0:order]( sum[i=0:j]( b_ji * x**(j - i) * y**i ))\n  Parameters \n \nparams(2, N) array, optional \n\nPolynomial coefficients where N * 2 = (order + 1) * (order + 2). So, a_ji is defined in params[0, :] and b_ji in params[1, :].    Attributes \n \nparams(2, N) array \n\nPolynomial coefficients where N * 2 = (order + 1) * (order + 2). So, a_ji is defined in params[0, :] and b_ji in params[1, :].      \n__init__(params=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(src, dst, order=2) [source]\n \nEstimate the transformation from a set of corresponding points. You can determine the over-, well- and under-determined parameters with the total least-squares method. Number of source and destination coordinates must match. The transformation is defined as: X = sum[j=0:order]( sum[i=0:j]( a_ji * x**(j - i) * y**i ))\nY = sum[j=0:order]( sum[i=0:j]( b_ji * x**(j - i) * y**i ))\n These equations can be transformed to the following form: 0 = sum[j=0:order]( sum[i=0:j]( a_ji * x**(j - i) * y**i )) - X\n0 = sum[j=0:order]( sum[i=0:j]( b_ji * x**(j - i) * y**i )) - Y\n which exist for each set of corresponding points, so we have a set of N * 2 equations. The coefficients appear linearly so we can write A x = 0, where: A   = [[1 x y x**2 x*y y**2 ... 0 ...             0 -X]\n       [0 ...                 0 1 x y x**2 x*y y**2 -Y]\n        ...\n        ...\n      ]\nx.T = [a00 a10 a11 a20 a21 a22 ... ann\n       b00 b10 b11 b20 b21 b22 ... bnn c3]\n In case of total least-squares the solution of this homogeneous system of equations is the right singular vector of A which corresponds to the smallest singular value normed by the coefficient c3.  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.  \norderint, optional \n\nPolynomial order (number of coefficients is order + 1).    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n  \ninverse(coords) [source]\n \nApply inverse transformation.  Parameters \n \ncoords(N, 2) array \n\nDestination coordinates.    Returns \n \ncoords(N, 2) array \n\nSource coordinates.     \n \n"}, {"name": "transform.PolynomialTransform.estimate()", "path": "api/skimage.transform#skimage.transform.PolynomialTransform.estimate", "type": "transform", "text": " \nestimate(src, dst, order=2) [source]\n \nEstimate the transformation from a set of corresponding points. You can determine the over-, well- and under-determined parameters with the total least-squares method. Number of source and destination coordinates must match. The transformation is defined as: X = sum[j=0:order]( sum[i=0:j]( a_ji * x**(j - i) * y**i ))\nY = sum[j=0:order]( sum[i=0:j]( b_ji * x**(j - i) * y**i ))\n These equations can be transformed to the following form: 0 = sum[j=0:order]( sum[i=0:j]( a_ji * x**(j - i) * y**i )) - X\n0 = sum[j=0:order]( sum[i=0:j]( b_ji * x**(j - i) * y**i )) - Y\n which exist for each set of corresponding points, so we have a set of N * 2 equations. The coefficients appear linearly so we can write A x = 0, where: A   = [[1 x y x**2 x*y y**2 ... 0 ...             0 -X]\n       [0 ...                 0 1 x y x**2 x*y y**2 -Y]\n        ...\n        ...\n      ]\nx.T = [a00 a10 a11 a20 a21 a22 ... ann\n       b00 b10 b11 b20 b21 b22 ... bnn c3]\n In case of total least-squares the solution of this homogeneous system of equations is the right singular vector of A which corresponds to the smallest singular value normed by the coefficient c3.  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.  \norderint, optional \n\nPolynomial order (number of coefficients is order + 1).    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n"}, {"name": "transform.PolynomialTransform.inverse()", "path": "api/skimage.transform#skimage.transform.PolynomialTransform.inverse", "type": "transform", "text": " \ninverse(coords) [source]\n \nApply inverse transformation.  Parameters \n \ncoords(N, 2) array \n\nDestination coordinates.    Returns \n \ncoords(N, 2) array \n\nSource coordinates.     \n"}, {"name": "transform.PolynomialTransform.__init__()", "path": "api/skimage.transform#skimage.transform.PolynomialTransform.__init__", "type": "transform", "text": " \n__init__(params=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "transform.probabilistic_hough_line()", "path": "api/skimage.transform#skimage.transform.probabilistic_hough_line", "type": "transform", "text": " \nskimage.transform.probabilistic_hough_line(image, threshold=10, line_length=50, line_gap=10, theta=None, seed=None) [source]\n \nReturn lines from a progressive probabilistic line Hough transform.  Parameters \n \nimage(M, N) ndarray \n\nInput image with nonzero values representing edges.  \nthresholdint, optional \n\nThreshold  \nline_lengthint, optional \n\nMinimum accepted length of detected lines. Increase the parameter to extract longer lines.  \nline_gapint, optional \n\nMaximum gap between pixels to still form a line. Increase the parameter to merge broken lines more aggressively.  \ntheta1D ndarray, dtype=double, optional \n\nAngles at which to compute the transform, in radians. If None, use a range from -pi/2 to pi/2.  \nseedint, optional \n\nSeed to initialize the random number generator.    Returns \n \nlineslist \n\nList of lines identified, lines in format ((x0, y0), (x1, y1)), indicating line start and end.     References  \n1  \nC. Galamhos, J. Matas and J. Kittler, \u201cProgressive probabilistic Hough transform for line detection\u201d, in IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 1999.   \n"}, {"name": "transform.ProjectiveTransform", "path": "api/skimage.transform#skimage.transform.ProjectiveTransform", "type": "transform", "text": " \nclass skimage.transform.ProjectiveTransform(matrix=None) [source]\n \nBases: skimage.transform._geometric.GeometricTransform Projective transformation. Apply a projective transformation (homography) on coordinates. For each homogeneous coordinate \\(\\mathbf{x} = [x, y, 1]^T\\), its target position is calculated by multiplying with the given matrix, \\(H\\), to give \\(H \\mathbf{x}\\): [[a0 a1 a2]\n [b0 b1 b2]\n [c0 c1 1 ]].\n E.g., to rotate by theta degrees clockwise, the matrix should be: [[cos(theta) -sin(theta) 0]\n [sin(theta)  cos(theta) 0]\n [0            0         1]]\n or, to translate x by 10 and y by 20: [[1 0 10]\n [0 1 20]\n [0 0 1 ]].\n  Parameters \n \nmatrix(3, 3) array, optional \n\nHomogeneous transformation matrix.    Attributes \n \nparams(3, 3) array \n\nHomogeneous transformation matrix.      \n__init__(matrix=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(src, dst) [source]\n \nEstimate the transformation from a set of corresponding points. You can determine the over-, well- and under-determined parameters with the total least-squares method. Number of source and destination coordinates must match. The transformation is defined as: X = (a0*x + a1*y + a2) / (c0*x + c1*y + 1)\nY = (b0*x + b1*y + b2) / (c0*x + c1*y + 1)\n These equations can be transformed to the following form: 0 = a0*x + a1*y + a2 - c0*x*X - c1*y*X - X\n0 = b0*x + b1*y + b2 - c0*x*Y - c1*y*Y - Y\n which exist for each set of corresponding points, so we have a set of N * 2 equations. The coefficients appear linearly so we can write A x = 0, where: A   = [[x y 1 0 0 0 -x*X -y*X -X]\n       [0 0 0 x y 1 -x*Y -y*Y -Y]\n        ...\n        ...\n      ]\nx.T = [a0 a1 a2 b0 b1 b2 c0 c1 c3]\n In case of total least-squares the solution of this homogeneous system of equations is the right singular vector of A which corresponds to the smallest singular value normed by the coefficient c3. In case of the affine transformation the coefficients c0 and c1 are 0. Thus the system of equations is: A   = [[x y 1 0 0 0 -X]\n       [0 0 0 x y 1 -Y]\n        ...\n        ...\n      ]\nx.T = [a0 a1 a2 b0 b1 b2 c3]\n  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n  \ninverse(coords) [source]\n \nApply inverse transformation.  Parameters \n \ncoords(N, 2) array \n\nDestination coordinates.    Returns \n \ncoords(N, 2) array \n\nSource coordinates.     \n \n"}, {"name": "transform.ProjectiveTransform.estimate()", "path": "api/skimage.transform#skimage.transform.ProjectiveTransform.estimate", "type": "transform", "text": " \nestimate(src, dst) [source]\n \nEstimate the transformation from a set of corresponding points. You can determine the over-, well- and under-determined parameters with the total least-squares method. Number of source and destination coordinates must match. The transformation is defined as: X = (a0*x + a1*y + a2) / (c0*x + c1*y + 1)\nY = (b0*x + b1*y + b2) / (c0*x + c1*y + 1)\n These equations can be transformed to the following form: 0 = a0*x + a1*y + a2 - c0*x*X - c1*y*X - X\n0 = b0*x + b1*y + b2 - c0*x*Y - c1*y*Y - Y\n which exist for each set of corresponding points, so we have a set of N * 2 equations. The coefficients appear linearly so we can write A x = 0, where: A   = [[x y 1 0 0 0 -x*X -y*X -X]\n       [0 0 0 x y 1 -x*Y -y*Y -Y]\n        ...\n        ...\n      ]\nx.T = [a0 a1 a2 b0 b1 b2 c0 c1 c3]\n In case of total least-squares the solution of this homogeneous system of equations is the right singular vector of A which corresponds to the smallest singular value normed by the coefficient c3. In case of the affine transformation the coefficients c0 and c1 are 0. Thus the system of equations is: A   = [[x y 1 0 0 0 -X]\n       [0 0 0 x y 1 -Y]\n        ...\n        ...\n      ]\nx.T = [a0 a1 a2 b0 b1 b2 c3]\n  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n"}, {"name": "transform.ProjectiveTransform.inverse()", "path": "api/skimage.transform#skimage.transform.ProjectiveTransform.inverse", "type": "transform", "text": " \ninverse(coords) [source]\n \nApply inverse transformation.  Parameters \n \ncoords(N, 2) array \n\nDestination coordinates.    Returns \n \ncoords(N, 2) array \n\nSource coordinates.     \n"}, {"name": "transform.ProjectiveTransform.__init__()", "path": "api/skimage.transform#skimage.transform.ProjectiveTransform.__init__", "type": "transform", "text": " \n__init__(matrix=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "transform.pyramid_expand()", "path": "api/skimage.transform#skimage.transform.pyramid_expand", "type": "transform", "text": " \nskimage.transform.pyramid_expand(image, upscale=2, sigma=None, order=1, mode='reflect', cval=0, multichannel=False, preserve_range=False) [source]\n \nUpsample and then smooth image.  Parameters \n \nimagendarray \n\nInput image.  \nupscalefloat, optional \n\nUpscale factor.  \nsigmafloat, optional \n\nSigma for Gaussian filter. Default is 2 * upscale / 6.0 which corresponds to a filter mask twice the size of the scale factor that covers more than 99% of the Gaussian distribution.  \norderint, optional \n\nOrder of splines used in interpolation of upsampling. See skimage.transform.warp for detail.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018edge\u2019, \u2018symmetric\u2019, \u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019.  \ncvalfloat, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019.  \nmultichannelbool, optional \n\nWhether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html    Returns \n \noutarray \n\nUpsampled and smoothed float image.     References  \n1  \nhttp://persci.mit.edu/pub_pdfs/pyramid83.pdf   \n"}, {"name": "transform.pyramid_gaussian()", "path": "api/skimage.transform#skimage.transform.pyramid_gaussian", "type": "transform", "text": " \nskimage.transform.pyramid_gaussian(image, max_layer=-1, downscale=2, sigma=None, order=1, mode='reflect', cval=0, multichannel=False, preserve_range=False) [source]\n \nYield images of the Gaussian pyramid formed by the input image. Recursively applies the pyramid_reduce function to the image, and yields the downscaled images. Note that the first image of the pyramid will be the original, unscaled image. The total number of images is max_layer + 1. In case all layers are computed, the last image is either a one-pixel image or the image where the reduction does not change its shape.  Parameters \n \nimagendarray \n\nInput image.  \nmax_layerint, optional \n\nNumber of layers for the pyramid. 0th layer is the original image. Default is -1 which builds all possible layers.  \ndownscalefloat, optional \n\nDownscale factor.  \nsigmafloat, optional \n\nSigma for Gaussian filter. Default is 2 * downscale / 6.0 which corresponds to a filter mask twice the size of the scale factor that covers more than 99% of the Gaussian distribution.  \norderint, optional \n\nOrder of splines used in interpolation of downsampling. See skimage.transform.warp for detail.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018edge\u2019, \u2018symmetric\u2019, \u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019.  \ncvalfloat, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019.  \nmultichannelbool, optional \n\nWhether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html    Returns \n \npyramidgenerator \n\nGenerator yielding pyramid layers as float images.     References  \n1  \nhttp://persci.mit.edu/pub_pdfs/pyramid83.pdf   \n"}, {"name": "transform.pyramid_laplacian()", "path": "api/skimage.transform#skimage.transform.pyramid_laplacian", "type": "transform", "text": " \nskimage.transform.pyramid_laplacian(image, max_layer=-1, downscale=2, sigma=None, order=1, mode='reflect', cval=0, multichannel=False, preserve_range=False) [source]\n \nYield images of the laplacian pyramid formed by the input image. Each layer contains the difference between the downsampled and the downsampled, smoothed image: layer = resize(prev_layer) - smooth(resize(prev_layer))\n Note that the first image of the pyramid will be the difference between the original, unscaled image and its smoothed version. The total number of images is max_layer + 1. In case all layers are computed, the last image is either a one-pixel image or the image where the reduction does not change its shape.  Parameters \n \nimagendarray \n\nInput image.  \nmax_layerint, optional \n\nNumber of layers for the pyramid. 0th layer is the original image. Default is -1 which builds all possible layers.  \ndownscalefloat, optional \n\nDownscale factor.  \nsigmafloat, optional \n\nSigma for Gaussian filter. Default is 2 * downscale / 6.0 which corresponds to a filter mask twice the size of the scale factor that covers more than 99% of the Gaussian distribution.  \norderint, optional \n\nOrder of splines used in interpolation of downsampling. See skimage.transform.warp for detail.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018edge\u2019, \u2018symmetric\u2019, \u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019.  \ncvalfloat, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019.  \nmultichannelbool, optional \n\nWhether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html    Returns \n \npyramidgenerator \n\nGenerator yielding pyramid layers as float images.     References  \n1  \nhttp://persci.mit.edu/pub_pdfs/pyramid83.pdf  \n2  \nhttp://sepwww.stanford.edu/data/media/public/sep/morgan/texturematch/paper_html/node3.html   \n"}, {"name": "transform.pyramid_reduce()", "path": "api/skimage.transform#skimage.transform.pyramid_reduce", "type": "transform", "text": " \nskimage.transform.pyramid_reduce(image, downscale=2, sigma=None, order=1, mode='reflect', cval=0, multichannel=False, preserve_range=False) [source]\n \nSmooth and then downsample image.  Parameters \n \nimagendarray \n\nInput image.  \ndownscalefloat, optional \n\nDownscale factor.  \nsigmafloat, optional \n\nSigma for Gaussian filter. Default is 2 * downscale / 6.0 which corresponds to a filter mask twice the size of the scale factor that covers more than 99% of the Gaussian distribution.  \norderint, optional \n\nOrder of splines used in interpolation of downsampling. See skimage.transform.warp for detail.  \nmode{\u2018reflect\u2019, \u2018constant\u2019, \u2018edge\u2019, \u2018symmetric\u2019, \u2018wrap\u2019}, optional \n\nThe mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019.  \ncvalfloat, optional \n\nValue to fill past edges of input if mode is \u2018constant\u2019.  \nmultichannelbool, optional \n\nWhether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html    Returns \n \noutarray \n\nSmoothed and downsampled float image.     References  \n1  \nhttp://persci.mit.edu/pub_pdfs/pyramid83.pdf   \n"}, {"name": "transform.radon()", "path": "api/skimage.transform#skimage.transform.radon", "type": "transform", "text": " \nskimage.transform.radon(image, theta=None, circle=True, *, preserve_range=False) [source]\n \nCalculates the radon transform of an image given specified projection angles.  Parameters \n \nimagearray_like \n\nInput image. The rotation axis will be located in the pixel with indices (image.shape[0] // 2, image.shape[1] // 2).  \nthetaarray_like, optional \n\nProjection angles (in degrees). If None, the value is set to np.arange(180).  \ncircleboolean, optional \n\nAssume image is zero outside the inscribed circle, making the width of each projection (the first dimension of the sinogram) equal to min(image.shape).  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html    Returns \n \nradon_imagendarray \n\nRadon transform (sinogram). The tomography rotation axis will lie at the pixel index radon_image.shape[0] // 2 along the 0th dimension of radon_image.     Notes Based on code of Justin K. Romberg (https://www.clear.rice.edu/elec431/projects96/DSP/bpanalysis.html) References  \n1  \nAC Kak, M Slaney, \u201cPrinciples of Computerized Tomographic Imaging\u201d, IEEE Press 1988.  \n2  \nB.R. Ramesh, N. Srinivasa, K. Rajgopal, \u201cAn Algorithm for Computing the Discrete Radon Transform With Some Applications\u201d, Proceedings of the Fourth IEEE Region 10 International Conference, TENCON \u201889, 1989   \n"}, {"name": "transform.rescale()", "path": "api/skimage.transform#skimage.transform.rescale", "type": "transform", "text": " \nskimage.transform.rescale(image, scale, order=None, mode='reflect', cval=0, clip=True, preserve_range=False, multichannel=False, anti_aliasing=None, anti_aliasing_sigma=None) [source]\n \nScale image by a certain factor. Performs interpolation to up-scale or down-scale N-dimensional images. Note that anti-aliasing should be enabled when down-sizing images to avoid aliasing artifacts. For down-sampling with an integer factor also see skimage.transform.downscale_local_mean.  Parameters \n \nimagendarray \n\nInput image.  \nscale{float, tuple of floats} \n\nScale factors. Separate scale factors can be defined as (rows, cols[, \u2026][, dim]).    Returns \n \nscaledndarray \n\nScaled version of the input.    Other Parameters \n \norderint, optional \n\nThe order of the spline interpolation, default is 0 if image.dtype is bool and 1 otherwise. The order has to be in the range 0-5. See skimage.transform.warp for detail.  \nmode{\u2018constant\u2019, \u2018edge\u2019, \u2018symmetric\u2019, \u2018reflect\u2019, \u2018wrap\u2019}, optional \n\nPoints outside the boundaries of the input are filled according to the given mode. Modes match the behaviour of numpy.pad.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.  \nclipbool, optional \n\nWhether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html  \nmultichannelbool, optional \n\nWhether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.  \nanti_aliasingbool, optional \n\nWhether to apply a Gaussian filter to smooth the image prior to down-scaling. It is crucial to filter when down-sampling the image to avoid aliasing artifacts. If input image data type is bool, no anti-aliasing is applied.  \nanti_aliasing_sigma{float, tuple of floats}, optional \n\nStandard deviation for Gaussian filtering to avoid aliasing artifacts. By default, this value is chosen as (s - 1) / 2 where s is the down-scaling factor.     Notes Modes \u2018reflect\u2019 and \u2018symmetric\u2019 are similar, but differ in whether the edge pixels are duplicated during the reflection. As an example, if an array has values [0, 1, 2] and was padded to the right by four values using symmetric, the result would be [0, 1, 2, 2, 1, 0, 0], while for reflect it would be [0, 1, 2, 1, 0, 1, 2]. Examples >>> from skimage import data\n>>> from skimage.transform import rescale\n>>> image = data.camera()\n>>> rescale(image, 0.1).shape\n(51, 51)\n>>> rescale(image, 0.5).shape\n(256, 256)\n \n"}, {"name": "transform.resize()", "path": "api/skimage.transform#skimage.transform.resize", "type": "transform", "text": " \nskimage.transform.resize(image, output_shape, order=None, mode='reflect', cval=0, clip=True, preserve_range=False, anti_aliasing=None, anti_aliasing_sigma=None) [source]\n \nResize image to match a certain size. Performs interpolation to up-size or down-size N-dimensional images. Note that anti-aliasing should be enabled when down-sizing images to avoid aliasing artifacts. For down-sampling with an integer factor also see skimage.transform.downscale_local_mean.  Parameters \n \nimagendarray \n\nInput image.  \noutput_shapetuple or ndarray \n\nSize of the generated output image (rows, cols[, \u2026][, dim]). If dim is not provided, the number of channels is preserved. In case the number of input channels does not equal the number of output channels a n-dimensional interpolation is applied.    Returns \n \nresizedndarray \n\nResized version of the input.    Other Parameters \n \norderint, optional \n\nThe order of the spline interpolation, default is 0 if image.dtype is bool and 1 otherwise. The order has to be in the range 0-5. See skimage.transform.warp for detail.  \nmode{\u2018constant\u2019, \u2018edge\u2019, \u2018symmetric\u2019, \u2018reflect\u2019, \u2018wrap\u2019}, optional \n\nPoints outside the boundaries of the input are filled according to the given mode. Modes match the behaviour of numpy.pad.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.  \nclipbool, optional \n\nWhether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html  \nanti_aliasingbool, optional \n\nWhether to apply a Gaussian filter to smooth the image prior to down-scaling. It is crucial to filter when down-sampling the image to avoid aliasing artifacts. If input image data type is bool, no anti-aliasing is applied.  \nanti_aliasing_sigma{float, tuple of floats}, optional \n\nStandard deviation for Gaussian filtering to avoid aliasing artifacts. By default, this value is chosen as (s - 1) / 2 where s is the down-scaling factor, where s > 1. For the up-size case, s < 1, no anti-aliasing is performed prior to rescaling.     Notes Modes \u2018reflect\u2019 and \u2018symmetric\u2019 are similar, but differ in whether the edge pixels are duplicated during the reflection. As an example, if an array has values [0, 1, 2] and was padded to the right by four values using symmetric, the result would be [0, 1, 2, 2, 1, 0, 0], while for reflect it would be [0, 1, 2, 1, 0, 1, 2]. Examples >>> from skimage import data\n>>> from skimage.transform import resize\n>>> image = data.camera()\n>>> resize(image, (100, 100)).shape\n(100, 100)\n \n"}, {"name": "transform.rotate()", "path": "api/skimage.transform#skimage.transform.rotate", "type": "transform", "text": " \nskimage.transform.rotate(image, angle, resize=False, center=None, order=None, mode='constant', cval=0, clip=True, preserve_range=False) [source]\n \nRotate image by a certain angle around its center.  Parameters \n \nimagendarray \n\nInput image.  \nanglefloat \n\nRotation angle in degrees in counter-clockwise direction.  \nresizebool, optional \n\nDetermine whether the shape of the output image will be automatically calculated, so the complete rotated image exactly fits. Default is False.  \ncenteriterable of length 2 \n\nThe rotation center. If center=None, the image is rotated around its center, i.e. center=(cols / 2 - 0.5, rows / 2 - 0.5). Please note that this parameter is (cols, rows), contrary to normal skimage ordering.    Returns \n \nrotatedndarray \n\nRotated version of the input.    Other Parameters \n \norderint, optional \n\nThe order of the spline interpolation, default is 0 if image.dtype is bool and 1 otherwise. The order has to be in the range 0-5. See skimage.transform.warp for detail.  \nmode{\u2018constant\u2019, \u2018edge\u2019, \u2018symmetric\u2019, \u2018reflect\u2019, \u2018wrap\u2019}, optional \n\nPoints outside the boundaries of the input are filled according to the given mode. Modes match the behaviour of numpy.pad.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.  \nclipbool, optional \n\nWhether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html     Notes Modes \u2018reflect\u2019 and \u2018symmetric\u2019 are similar, but differ in whether the edge pixels are duplicated during the reflection. As an example, if an array has values [0, 1, 2] and was padded to the right by four values using symmetric, the result would be [0, 1, 2, 2, 1, 0, 0], while for reflect it would be [0, 1, 2, 1, 0, 1, 2]. Examples >>> from skimage import data\n>>> from skimage.transform import rotate\n>>> image = data.camera()\n>>> rotate(image, 2).shape\n(512, 512)\n>>> rotate(image, 2, resize=True).shape\n(530, 530)\n>>> rotate(image, 90, resize=True).shape\n(512, 512)\n \n"}, {"name": "transform.SimilarityTransform", "path": "api/skimage.transform#skimage.transform.SimilarityTransform", "type": "transform", "text": " \nclass skimage.transform.SimilarityTransform(matrix=None, scale=None, rotation=None, translation=None) [source]\n \nBases: skimage.transform._geometric.EuclideanTransform 2D similarity transformation. Has the following form: X = a0 * x - b0 * y + a1 =\n  = s * x * cos(rotation) - s * y * sin(rotation) + a1\n\nY = b0 * x + a0 * y + b1 =\n  = s * x * sin(rotation) + s * y * cos(rotation) + b1\n where s is a scale factor and the homogeneous transformation matrix is: [[a0  b0  a1]\n [b0  a0  b1]\n [0   0    1]]\n The similarity transformation extends the Euclidean transformation with a single scaling factor in addition to the rotation and translation parameters.  Parameters \n \nmatrix(3, 3) array, optional \n\nHomogeneous transformation matrix.  \nscalefloat, optional \n\nScale factor.  \nrotationfloat, optional \n\nRotation angle in counter-clockwise direction as radians.  \ntranslation(tx, ty) as array, list or tuple, optional \n\nx, y translation parameters.    Attributes \n \nparams(3, 3) array \n\nHomogeneous transformation matrix.      \n__init__(matrix=None, scale=None, rotation=None, translation=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nestimate(src, dst) [source]\n \nEstimate the transformation from a set of corresponding points. You can determine the over-, well- and under-determined parameters with the total least-squares method. Number of source and destination coordinates must match.  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n  \nproperty scale \n \n"}, {"name": "transform.SimilarityTransform.estimate()", "path": "api/skimage.transform#skimage.transform.SimilarityTransform.estimate", "type": "transform", "text": " \nestimate(src, dst) [source]\n \nEstimate the transformation from a set of corresponding points. You can determine the over-, well- and under-determined parameters with the total least-squares method. Number of source and destination coordinates must match.  Parameters \n \nsrc(N, 2) array \n\nSource coordinates.  \ndst(N, 2) array \n\nDestination coordinates.    Returns \n \nsuccessbool \n\nTrue, if model estimation succeeds.     \n"}, {"name": "transform.SimilarityTransform.scale()", "path": "api/skimage.transform#skimage.transform.SimilarityTransform.scale", "type": "transform", "text": " \nproperty scale \n"}, {"name": "transform.SimilarityTransform.__init__()", "path": "api/skimage.transform#skimage.transform.SimilarityTransform.__init__", "type": "transform", "text": " \n__init__(matrix=None, scale=None, rotation=None, translation=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "transform.swirl()", "path": "api/skimage.transform#skimage.transform.swirl", "type": "transform", "text": " \nskimage.transform.swirl(image, center=None, strength=1, radius=100, rotation=0, output_shape=None, order=None, mode='reflect', cval=0, clip=True, preserve_range=False) [source]\n \nPerform a swirl transformation.  Parameters \n \nimagendarray \n\nInput image.  \ncenter(column, row) tuple or (2,) ndarray, optional \n\nCenter coordinate of transformation.  \nstrengthfloat, optional \n\nThe amount of swirling applied.  \nradiusfloat, optional \n\nThe extent of the swirl in pixels. The effect dies out rapidly beyond radius.  \nrotationfloat, optional \n\nAdditional rotation applied to the image.    Returns \n \nswirledndarray \n\nSwirled version of the input.    Other Parameters \n \noutput_shapetuple (rows, cols), optional \n\nShape of the output image generated. By default the shape of the input image is preserved.  \norderint, optional \n\nThe order of the spline interpolation, default is 0 if image.dtype is bool and 1 otherwise. The order has to be in the range 0-5. See skimage.transform.warp for detail.  \nmode{\u2018constant\u2019, \u2018edge\u2019, \u2018symmetric\u2019, \u2018reflect\u2019, \u2018wrap\u2019}, optional \n\nPoints outside the boundaries of the input are filled according to the given mode, with \u2018constant\u2019 used as the default. Modes match the behaviour of numpy.pad.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.  \nclipbool, optional \n\nWhether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html     \n"}, {"name": "transform.warp()", "path": "api/skimage.transform#skimage.transform.warp", "type": "transform", "text": " \nskimage.transform.warp(image, inverse_map, map_args={}, output_shape=None, order=None, mode='constant', cval=0.0, clip=True, preserve_range=False) [source]\n \nWarp an image according to a given coordinate transformation.  Parameters \n \nimagendarray \n\nInput image.  \ninverse_maptransformation object, callable cr = f(cr, **kwargs), or ndarray \n\nInverse coordinate map, which transforms coordinates in the output images into their corresponding coordinates in the input image. There are a number of different options to define this map, depending on the dimensionality of the input image. A 2-D image can have 2 dimensions for gray-scale images, or 3 dimensions with color information.  For 2-D images, you can directly pass a transformation object, e.g. skimage.transform.SimilarityTransform, or its inverse. For 2-D images, you can pass a (3, 3) homogeneous transformation matrix, e.g. skimage.transform.SimilarityTransform.params. For 2-D images, a function that transforms a (M, 2) array of (col, row) coordinates in the output image to their corresponding coordinates in the input image. Extra parameters to the function can be specified through map_args. For N-D images, you can directly pass an array of coordinates. The first dimension specifies the coordinates in the input image, while the subsequent dimensions determine the position in the output image. E.g. in case of 2-D images, you need to pass an array of shape (2, rows, cols), where rows and cols determine the shape of the output image, and the first dimension contains the (row, col) coordinate in the input image. See scipy.ndimage.map_coordinates for further documentation.  Note, that a (3, 3) matrix is interpreted as a homogeneous transformation matrix, so you cannot interpolate values from a 3-D input, if the output is of shape (3,). See example section for usage.  \nmap_argsdict, optional \n\nKeyword arguments passed to inverse_map.  \noutput_shapetuple (rows, cols), optional \n\nShape of the output image generated. By default the shape of the input image is preserved. Note that, even for multi-band images, only rows and columns need to be specified.  \norderint, optional \n\n The order of interpolation. The order has to be in the range 0-5:\n\n 0: Nearest-neighbor 1: Bi-linear (default) 2: Bi-quadratic 3: Bi-cubic 4: Bi-quartic 5: Bi-quintic  Default is 0 if image.dtype is bool and 1 otherwise.    \nmode{\u2018constant\u2019, \u2018edge\u2019, \u2018symmetric\u2019, \u2018reflect\u2019, \u2018wrap\u2019}, optional \n\nPoints outside the boundaries of the input are filled according to the given mode. Modes match the behaviour of numpy.pad.  \ncvalfloat, optional \n\nUsed in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.  \nclipbool, optional \n\nWhether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.  \npreserve_rangebool, optional \n\nWhether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html    Returns \n \nwarpeddouble ndarray \n\nThe warped input image.     Notes  The input image is converted to a double image. In case of a SimilarityTransform, AffineTransform and ProjectiveTransform and order in [0, 3] this function uses the underlying transformation matrix to warp the image with a much faster routine.  Examples >>> from skimage.transform import warp\n>>> from skimage import data\n>>> image = data.camera()\n The following image warps are all equal but differ substantially in execution time. The image is shifted to the bottom. Use a geometric transform to warp an image (fast): >>> from skimage.transform import SimilarityTransform\n>>> tform = SimilarityTransform(translation=(0, -10))\n>>> warped = warp(image, tform)\n Use a callable (slow): >>> def shift_down(xy):\n...     xy[:, 1] -= 10\n...     return xy\n>>> warped = warp(image, shift_down)\n Use a transformation matrix to warp an image (fast): >>> matrix = np.array([[1, 0, 0], [0, 1, -10], [0, 0, 1]])\n>>> warped = warp(image, matrix)\n>>> from skimage.transform import ProjectiveTransform\n>>> warped = warp(image, ProjectiveTransform(matrix=matrix))\n You can also use the inverse of a geometric transformation (fast): >>> warped = warp(image, tform.inverse)\n For N-D images you can pass a coordinate array, that specifies the coordinates in the input image for every element in the output image. E.g. if you want to rescale a 3-D cube, you can do: >>> cube_shape = np.array([30, 30, 30])\n>>> cube = np.random.rand(*cube_shape)\n Setup the coordinate array, that defines the scaling: >>> scale = 0.1\n>>> output_shape = (scale * cube_shape).astype(int)\n>>> coords0, coords1, coords2 = np.mgrid[:output_shape[0],\n...                    :output_shape[1], :output_shape[2]]\n>>> coords = np.array([coords0, coords1, coords2])\n Assume that the cube contains spatial data, where the first array element center is at coordinate (0.5, 0.5, 0.5) in real space, i.e. we have to account for this extra offset when scaling the image: >>> coords = (coords + 0.5) / scale - 0.5\n>>> warped = warp(cube, coords)\n \n"}, {"name": "transform.warp_coords()", "path": "api/skimage.transform#skimage.transform.warp_coords", "type": "transform", "text": " \nskimage.transform.warp_coords(coord_map, shape, dtype=<class 'numpy.float64'>) [source]\n \nBuild the source coordinates for the output of a 2-D image warp.  Parameters \n \ncoord_mapcallable like GeometricTransform.inverse \n\nReturn input coordinates for given output coordinates. Coordinates are in the shape (P, 2), where P is the number of coordinates and each element is a (row, col) pair.  \nshapetuple \n\nShape of output image (rows, cols[, bands]).  \ndtypenp.dtype or string \n\ndtype for return value (sane choices: float32 or float64).    Returns \n \ncoords(ndim, rows, cols[, bands]) array of dtype dtype \n\nCoordinates for scipy.ndimage.map_coordinates, that will yield an image of shape (orows, ocols, bands) by drawing from source points according to the coord_transform_fn.     Notes This is a lower-level routine that produces the source coordinates for 2-D images used by warp(). It is provided separately from warp to give additional flexibility to users who would like, for example, to re-use a particular coordinate mapping, to use specific dtypes at various points along the the image-warping process, or to implement different post-processing logic than warp performs after the call to ndi.map_coordinates. Examples Produce a coordinate map that shifts an image up and to the right: >>> from skimage import data\n>>> from scipy.ndimage import map_coordinates\n>>>\n>>> def shift_up10_left20(xy):\n...     return xy - np.array([-20, 10])[None, :]\n>>>\n>>> image = data.astronaut().astype(np.float32)\n>>> coords = warp_coords(shift_up10_left20, image.shape)\n>>> warped_image = map_coordinates(image, coords)\n \n"}, {"name": "transform.warp_polar()", "path": "api/skimage.transform#skimage.transform.warp_polar", "type": "transform", "text": " \nskimage.transform.warp_polar(image, center=None, *, radius=None, output_shape=None, scaling='linear', multichannel=False, **kwargs) [source]\n \nRemap image to polar or log-polar coordinates space.  Parameters \n \nimagendarray \n\nInput image. Only 2-D arrays are accepted by default. If multichannel=True, 3-D arrays are accepted and the last axis is interpreted as multiple channels.  \ncentertuple (row, col), optional \n\nPoint in image that represents the center of the transformation (i.e., the origin in cartesian space). Values can be of type float. If no value is given, the center is assumed to be the center point of the image.  \nradiusfloat, optional \n\nRadius of the circle that bounds the area to be transformed.  \noutput_shapetuple (row, col), optional \n\nscaling{\u2018linear\u2019, \u2018log\u2019}, optional \n\nSpecify whether the image warp is polar or log-polar. Defaults to \u2018linear\u2019.  \nmultichannelbool, optional \n\nWhether the image is a 3-D array in which the third axis is to be interpreted as multiple channels. If set to False (default), only 2-D arrays are accepted.  \n**kwargskeyword arguments \n\nPassed to transform.warp.    Returns \n \nwarpedndarray \n\nThe polar or log-polar warped image.     Examples Perform a basic polar warp on a grayscale image: >>> from skimage import data\n>>> from skimage.transform import warp_polar\n>>> image = data.checkerboard()\n>>> warped = warp_polar(image)\n Perform a log-polar warp on a grayscale image: >>> warped = warp_polar(image, scaling='log')\n Perform a log-polar warp on a grayscale image while specifying center, radius, and output shape: >>> warped = warp_polar(image, (100,100), radius=100,\n...                     output_shape=image.shape, scaling='log')\n Perform a log-polar warp on a color image: >>> image = data.astronaut()\n>>> warped = warp_polar(image, scaling='log', multichannel=True)\n \n"}, {"name": "Tutorials", "path": "user_guide/tutorials", "type": "Guide", "text": "Tutorials  Image Segmentation How to parallelize loops \n"}, {"name": "User Guide", "path": "user_guide", "type": "Guide", "text": "User Guide  Getting started \nA crash course on NumPy for images NumPy indexing Color images Coordinate conventions Notes on the order of array dimensions A note on the time dimension   \nImage data types and what they mean Input types Output types Working with OpenCV Image processing pipeline Rescaling intensity values Note about negative values References   I/O Plugin Infrastructure \nHandling Video Files A Workaround: Convert the Video to an Image Sequence PyAV Adding Random Access to PyAV MoviePy Imageio OpenCV   \nData visualization Matplotlib Plotly Mayavi Napari   \nImage adjustment: transforming image content Color manipulation Contrast and exposure   \nGeometrical transformations of images Cropping, resizing and rescaling images Projective transforms (homographies)   \nTutorials Image Segmentation How to parallelize loops   \nGetting help on using skimage Examples gallery Search field API Discovery Docstrings Mailing-list   \nImage Viewer Quick Start   \n"}, {"name": "util", "path": "api/skimage.util", "type": "util", "text": "Module: util  \nskimage.util.apply_parallel(function, array) Map a function in parallel across an array.  \nskimage.util.compare_images(image1, image2) Return an image showing the differences between two images.  \nskimage.util.crop(ar, crop_width[, copy, order]) Crop array ar by crop_width along each dimension.  \nskimage.util.dtype_limits(image[, clip_negative]) Return intensity limits, i.e.  \nskimage.util.img_as_bool(image[, force_copy]) Convert an image to boolean format.  \nskimage.util.img_as_float(image[, force_copy]) Convert an image to floating point format.  \nskimage.util.img_as_float32(image[, force_copy]) Convert an image to single-precision (32-bit) floating point format.  \nskimage.util.img_as_float64(image[, force_copy]) Convert an image to double-precision (64-bit) floating point format.  \nskimage.util.img_as_int(image[, force_copy]) Convert an image to 16-bit signed integer format.  \nskimage.util.img_as_ubyte(image[, force_copy]) Convert an image to 8-bit unsigned integer format.  \nskimage.util.img_as_uint(image[, force_copy]) Convert an image to 16-bit unsigned integer format.  \nskimage.util.invert(image[, signed_float]) Invert an image.  \nskimage.util.map_array(input_arr, \u2026[, out]) Map values from input array from input_vals to output_vals.  \nskimage.util.montage(arr_in[, fill, \u2026]) Create a montage of several single- or multichannel images.  \nskimage.util.pad(array, pad_width[, mode]) Pad an array.  \nskimage.util.random_noise(image[, mode, \u2026]) Function to add random noise of various types to a floating-point image.  \nskimage.util.regular_grid(ar_shape, n_points) Find n_points regularly spaced along ar_shape.  \nskimage.util.regular_seeds(ar_shape, n_points) Return an image with ~`n_points` regularly-spaced nonzero pixels.  \nskimage.util.unique_rows(ar) Remove repeated rows from a 2D array.  \nskimage.util.view_as_blocks(arr_in, block_shape) Block view of the input n-dimensional array (using re-striding).  \nskimage.util.view_as_windows(arr_in, \u2026[, step]) Rolling window view of the input n-dimensional array.   apply_parallel  \nskimage.util.apply_parallel(function, array, chunks=None, depth=0, mode=None, extra_arguments=(), extra_keywords={}, *, dtype=None, multichannel=False, compute=None) [source]\n \nMap a function in parallel across an array. Split an array into possibly overlapping chunks of a given depth and boundary type, call the given function in parallel on the chunks, combine the chunks and return the resulting array.  Parameters \n \nfunctionfunction \n\nFunction to be mapped which takes an array as an argument.  \narraynumpy array or dask array \n\nArray which the function will be applied to.  \nchunksint, tuple, or tuple of tuples, optional \n\nA single integer is interpreted as the length of one side of a square chunk that should be tiled across the array. One tuple of length array.ndim represents the shape of a chunk, and it is tiled across the array. A list of tuples of length ndim, where each sub-tuple is a sequence of chunk sizes along the corresponding dimension. If None, the array is broken up into chunks based on the number of available cpus. More information about chunks is in the documentation here.  \ndepthint, optional \n\nInteger equal to the depth of the added boundary cells. Defaults to zero.  \nmode{\u2018reflect\u2019, \u2018symmetric\u2019, \u2018periodic\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018edge\u2019}, optional \n\ntype of external boundary padding.  \nextra_argumentstuple, optional \n\nTuple of arguments to be passed to the function.  \nextra_keywordsdictionary, optional \n\nDictionary of keyword arguments to be passed to the function.  \ndtypedata-type or None, optional \n\nThe data-type of the function output. If None, Dask will attempt to infer this by calling the function on data of shape (1,) * ndim. For functions expecting RGB or multichannel data this may be problematic. In such cases, the user should manually specify this dtype argument instead.  New in version 0.18: dtype was added in 0.18.   \nmultichannelbool, optional \n\nIf chunks is None and multichannel is True, this function will keep only a single chunk along the channels axis. When depth is specified as a scalar value, that depth will be applied only to the non-channels axes (a depth of 0 will be used along the channels axis). If the user manually specified both chunks and a depth tuple, then this argument will have no effect.  New in version 0.18: multichannel was added in 0.18.   \ncomputebool, optional \n\nIf True, compute eagerly returning a NumPy Array. If False, compute lazily returning a Dask Array. If None (default), compute based on array type provided (eagerly for NumPy Arrays and lazily for Dask Arrays).    Returns \n \noutndarray or dask Array \n\nReturns the result of the applying the operation. Type is dependent on the compute argument.     Notes Numpy edge modes \u2018symmetric\u2019, \u2018wrap\u2019, and \u2018edge\u2019 are converted to the equivalent dask boundary modes \u2018reflect\u2019, \u2018periodic\u2019 and \u2018nearest\u2019, respectively. Setting compute=False can be useful for chaining later operations. For example region selection to preview a result or storing large data to disk instead of loading in memory. \n compare_images  \nskimage.util.compare_images(image1, image2, method='diff', *, n_tiles=(8, 8)) [source]\n \nReturn an image showing the differences between two images.  New in version 0.16.   Parameters \n \nimage1, image22-D array \n\nImages to process, must be of the same shape.  \nmethodstring, optional \n\nMethod used for the comparison. Valid values are {\u2018diff\u2019, \u2018blend\u2019, \u2018checkerboard\u2019}. Details are provided in the note section.  \nn_tilestuple, optional \n\nUsed only for the checkerboard method. Specifies the number of tiles (row, column) to divide the image.    Returns \n \ncomparison2-D array \n\nImage showing the differences.     Notes 'diff' computes the absolute difference between the two images. 'blend' computes the mean value. 'checkerboard' makes tiles of dimension n_tiles that display alternatively the first and the second image. \n crop  \nskimage.util.crop(ar, crop_width, copy=False, order='K') [source]\n \nCrop array ar by crop_width along each dimension.  Parameters \n \nararray-like of rank N \n\nInput array.  \ncrop_width{sequence, int} \n\nNumber of values to remove from the edges of each axis. ((before_1, after_1), \u2026 (before_N, after_N)) specifies unique crop widths at the start and end of each axis. ((before, after),) or (before, after) specifies a fixed start and end crop for every axis. (n,) or n for integer n is a shortcut for before = after = n for all axes.  \ncopybool, optional \n\nIf True, ensure the returned array is a contiguous copy. Normally, a crop operation will return a discontiguous view of the underlying input array.  \norder{\u2018C\u2019, \u2018F\u2019, \u2018A\u2019, \u2018K\u2019}, optional \n\nIf copy==True, control the memory layout of the copy. See np.copy.    Returns \n \ncroppedarray \n\nThe cropped array. If copy=False (default), this is a sliced view of the input array.     \n dtype_limits  \nskimage.util.dtype_limits(image, clip_negative=False) [source]\n \nReturn intensity limits, i.e. (min, max) tuple, of the image\u2019s dtype.  Parameters \n \nimagendarray \n\nInput image.  \nclip_negativebool, optional \n\nIf True, clip the negative range (i.e. return 0 for min intensity) even if the image dtype allows negative values.    Returns \n \nimin, imaxtuple \n\nLower and upper intensity limits.     \n img_as_bool  \nskimage.util.img_as_bool(image, force_copy=False) [source]\n \nConvert an image to boolean format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of bool (bool_) \n\nOutput image.     Notes The upper half of the input dtype\u2019s positive range is True, and the lower half is False. All negative values (if present) are False. \n img_as_float  \nskimage.util.img_as_float(image, force_copy=False) [source]\n \nConvert an image to floating point format. This function is similar to img_as_float64, but will not convert lower-precision floating point arrays to float64.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of float \n\nOutput image.     Notes The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0]. \n img_as_float32  \nskimage.util.img_as_float32(image, force_copy=False) [source]\n \nConvert an image to single-precision (32-bit) floating point format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of float32 \n\nOutput image.     Notes The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0]. \n img_as_float64  \nskimage.util.img_as_float64(image, force_copy=False) [source]\n \nConvert an image to double-precision (64-bit) floating point format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of float64 \n\nOutput image.     Notes The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0]. \n img_as_int  \nskimage.util.img_as_int(image, force_copy=False) [source]\n \nConvert an image to 16-bit signed integer format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of int16 \n\nOutput image.     Notes The values are scaled between -32768 and 32767. If the input data-type is positive-only (e.g., uint8), then the output image will still only have positive values. \n img_as_ubyte  \nskimage.util.img_as_ubyte(image, force_copy=False) [source]\n \nConvert an image to 8-bit unsigned integer format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of ubyte (uint8) \n\nOutput image.     Notes Negative input values will be clipped. Positive values are scaled between 0 and 255. \n img_as_uint  \nskimage.util.img_as_uint(image, force_copy=False) [source]\n \nConvert an image to 16-bit unsigned integer format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of uint16 \n\nOutput image.     Notes Negative input values will be clipped. Positive values are scaled between 0 and 65535. \n invert  \nskimage.util.invert(image, signed_float=False) [source]\n \nInvert an image. Invert the intensity range of the input image, so that the dtype maximum is now the dtype minimum, and vice-versa. This operation is slightly different depending on the input dtype:  unsigned integers: subtract the image from the dtype maximum signed integers: subtract the image from -1 (see Notes) floats: subtract the image from 1 (if signed_float is False, so we assume the image is unsigned), or from 0 (if signed_float is True).  See the examples for clarification.  Parameters \n \nimagendarray \n\nInput image.  \nsigned_floatbool, optional \n\nIf True and the image is of type float, the range is assumed to be [-1, 1]. If False and the image is of type float, the range is assumed to be [0, 1].    Returns \n \ninvertedndarray \n\nInverted image.     Notes Ideally, for signed integers we would simply multiply by -1. However, signed integer ranges are asymmetric. For example, for np.int8, the range of possible values is [-128, 127], so that -128 * -1 equals -128! By subtracting from -1, we correctly map the maximum dtype value to the minimum. Examples >>> img = np.array([[100,  0, 200],\n...                 [  0, 50,   0],\n...                 [ 30,  0, 255]], np.uint8)\n>>> invert(img)\narray([[155, 255,  55],\n       [255, 205, 255],\n       [225, 255,   0]], dtype=uint8)\n>>> img2 = np.array([[ -2, 0, -128],\n...                  [127, 0,    5]], np.int8)\n>>> invert(img2)\narray([[   1,   -1,  127],\n       [-128,   -1,   -6]], dtype=int8)\n>>> img3 = np.array([[ 0., 1., 0.5, 0.75]])\n>>> invert(img3)\narray([[1.  , 0.  , 0.5 , 0.25]])\n>>> img4 = np.array([[ 0., 1., -1., -0.25]])\n>>> invert(img4, signed_float=True)\narray([[-0.  , -1.  ,  1.  ,  0.25]])\n \n Examples using skimage.util.invert\n \n  Use rolling-ball algorithm for estimating background intensity   map_array  \nskimage.util.map_array(input_arr, input_vals, output_vals, out=None) [source]\n \nMap values from input array from input_vals to output_vals.  Parameters \n \ninput_arrarray of int, shape (M[, N][, P][, \u2026]) \n\nThe input label image.  \ninput_valsarray of int, shape (N,) \n\nThe values to map from.  \noutput_valsarray, shape (N,) \n\nThe values to map to.  out: array, same shape as `input_arr`\n\nThe output array. Will be created if not provided. It should have the same dtype as output_vals.    Returns \n \noutarray, same shape as input_arr \n\nThe array of mapped values.     \n montage  \nskimage.util.montage(arr_in, fill='mean', rescale_intensity=False, grid_shape=None, padding_width=0, multichannel=False) [source]\n \nCreate a montage of several single- or multichannel images. Create a rectangular montage from an input array representing an ensemble of equally shaped single- (gray) or multichannel (color) images. For example, montage(arr_in) called with the following arr_in  \n1 2 3   will return  \n1 2  \n3 \n      where the \u2018*\u2019 patch will be determined by the fill parameter.  Parameters \n \narr_in(K, M, N[, C]) ndarray \n\nAn array representing an ensemble of K images of equal shape.  \nfillfloat or array-like of floats or \u2018mean\u2019, optional \n\nValue to fill the padding areas and/or the extra tiles in the output array. Has to be float for single channel collections. For multichannel collections has to be an array-like of shape of number of channels. If mean, uses the mean value over all images.  \nrescale_intensitybool, optional \n\nWhether to rescale the intensity of each image to [0, 1].  \ngrid_shapetuple, optional \n\nThe desired grid shape for the montage (ntiles_row, ntiles_column). The default aspect ratio is square.  \npadding_widthint, optional \n\nThe size of the spacing between the tiles and between the tiles and the borders. If non-zero, makes the boundaries of individual images easier to perceive.  \nmultichannelboolean, optional \n\nIf True, the last arr_in dimension is threated as a color channel, otherwise as spatial.    Returns \n \narr_out(K*(M+p)+p, K*(N+p)+p[, C]) ndarray \n\nOutput array with input images glued together (including padding p).     Examples >>> import numpy as np\n>>> from skimage.util import montage\n>>> arr_in = np.arange(3 * 2 * 2).reshape(3, 2, 2)\n>>> arr_in  \narray([[[ 0,  1],\n        [ 2,  3]],\n       [[ 4,  5],\n        [ 6,  7]],\n       [[ 8,  9],\n        [10, 11]]])\n>>> arr_out = montage(arr_in)\n>>> arr_out.shape\n(4, 4)\n>>> arr_out\narray([[ 0,  1,  4,  5],\n       [ 2,  3,  6,  7],\n       [ 8,  9,  5,  5],\n       [10, 11,  5,  5]])\n>>> arr_in.mean()\n5.5\n>>> arr_out_nonsquare = montage(arr_in, grid_shape=(1, 3))\n>>> arr_out_nonsquare\narray([[ 0,  1,  4,  5,  8,  9],\n       [ 2,  3,  6,  7, 10, 11]])\n>>> arr_out_nonsquare.shape\n(2, 6)\n \n pad  \nskimage.util.pad(array, pad_width, mode='constant', **kwargs) [source]\n \nPad an array.  Parameters \n \narrayarray_like of rank N \n\nThe array to pad.  \npad_width{sequence, array_like, int} \n\nNumber of values padded to the edges of each axis. ((before_1, after_1), \u2026 (before_N, after_N)) unique pad widths for each axis. ((before, after),) yields same before and after pad for each axis. (pad,) or int is a shortcut for before = after = pad width for all axes.  \nmodestr or function, optional \n\nOne of the following string values or a user supplied function.  \u2018constant\u2019 (default)\n\nPads with a constant value.  \u2018edge\u2019\n\nPads with the edge values of array.  \u2018linear_ramp\u2019\n\nPads with the linear ramp between end_value and the array edge value.  \u2018maximum\u2019\n\nPads with the maximum value of all or part of the vector along each axis.  \u2018mean\u2019\n\nPads with the mean value of all or part of the vector along each axis.  \u2018median\u2019\n\nPads with the median value of all or part of the vector along each axis.  \u2018minimum\u2019\n\nPads with the minimum value of all or part of the vector along each axis.  \u2018reflect\u2019\n\nPads with the reflection of the vector mirrored on the first and last values of the vector along each axis.  \u2018symmetric\u2019\n\nPads with the reflection of the vector mirrored along the edge of the array.  \u2018wrap\u2019\n\nPads with the wrap of the vector along the axis. The first values are used to pad the end and the end values are used to pad the beginning.  \u2018empty\u2019\n\nPads with undefined values.  New in version 1.17.   <function>\n\nPadding function, see Notes.    \nstat_lengthsequence or int, optional \n\nUsed in \u2018maximum\u2019, \u2018mean\u2019, \u2018median\u2019, and \u2018minimum\u2019. Number of values at edge of each axis used to calculate the statistic value. ((before_1, after_1), \u2026 (before_N, after_N)) unique statistic lengths for each axis. ((before, after),) yields same before and after statistic lengths for each axis. (stat_length,) or int is a shortcut for before = after = statistic length for all axes. Default is None, to use the entire axis.  \nconstant_valuessequence or scalar, optional \n\nUsed in \u2018constant\u2019. The values to set the padded values for each axis. ((before_1, after_1), ... (before_N, after_N)) unique pad constants for each axis. ((before, after),) yields same before and after constants for each axis. (constant,) or constant is a shortcut for before = after = constant for all axes. Default is 0.  \nend_valuessequence or scalar, optional \n\nUsed in \u2018linear_ramp\u2019. The values used for the ending value of the linear_ramp and that will form the edge of the padded array. ((before_1, after_1), ... (before_N, after_N)) unique end values for each axis. ((before, after),) yields same before and after end values for each axis. (constant,) or constant is a shortcut for before = after = constant for all axes. Default is 0.  \nreflect_type{\u2018even\u2019, \u2018odd\u2019}, optional \n\nUsed in \u2018reflect\u2019, and \u2018symmetric\u2019. The \u2018even\u2019 style is the default with an unaltered reflection around the edge value. For the \u2018odd\u2019 style, the extended part of the array is created by subtracting the reflected values from two times the edge value.    Returns \n \npadndarray \n\nPadded array of rank equal to array with shape increased according to pad_width.     Notes  New in version 1.7.0.  For an array with rank greater than 1, some of the padding of later axes is calculated from padding of previous axes. This is easiest to think about with a rank 2 array where the corners of the padded array are calculated by using padded values from the first axis. The padding function, if used, should modify a rank 1 array in-place. It has the following signature: padding_func(vector, iaxis_pad_width, iaxis, kwargs)\n where  \nvectorndarray \n\nA rank 1 array already padded with zeros. Padded values are vector[:iaxis_pad_width[0]] and vector[-iaxis_pad_width[1]:].  \niaxis_pad_widthtuple \n\nA 2-tuple of ints, iaxis_pad_width[0] represents the number of values padded at the beginning of vector where iaxis_pad_width[1] represents the number of values padded at the end of vector.  \niaxisint \n\nThe axis currently being calculated.  \nkwargsdict \n\nAny keyword arguments the function requires.   Examples >>> a = [1, 2, 3, 4, 5]\n>>> np.pad(a, (2, 3), 'constant', constant_values=(4, 6))\narray([4, 4, 1, ..., 6, 6, 6])\n >>> np.pad(a, (2, 3), 'edge')\narray([1, 1, 1, ..., 5, 5, 5])\n >>> np.pad(a, (2, 3), 'linear_ramp', end_values=(5, -4))\narray([ 5,  3,  1,  2,  3,  4,  5,  2, -1, -4])\n >>> np.pad(a, (2,), 'maximum')\narray([5, 5, 1, 2, 3, 4, 5, 5, 5])\n >>> np.pad(a, (2,), 'mean')\narray([3, 3, 1, 2, 3, 4, 5, 3, 3])\n >>> np.pad(a, (2,), 'median')\narray([3, 3, 1, 2, 3, 4, 5, 3, 3])\n >>> a = [[1, 2], [3, 4]]\n>>> np.pad(a, ((3, 2), (2, 3)), 'minimum')\narray([[1, 1, 1, 2, 1, 1, 1],\n       [1, 1, 1, 2, 1, 1, 1],\n       [1, 1, 1, 2, 1, 1, 1],\n       [1, 1, 1, 2, 1, 1, 1],\n       [3, 3, 3, 4, 3, 3, 3],\n       [1, 1, 1, 2, 1, 1, 1],\n       [1, 1, 1, 2, 1, 1, 1]])\n >>> a = [1, 2, 3, 4, 5]\n>>> np.pad(a, (2, 3), 'reflect')\narray([3, 2, 1, 2, 3, 4, 5, 4, 3, 2])\n >>> np.pad(a, (2, 3), 'reflect', reflect_type='odd')\narray([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8])\n >>> np.pad(a, (2, 3), 'symmetric')\narray([2, 1, 1, 2, 3, 4, 5, 5, 4, 3])\n >>> np.pad(a, (2, 3), 'symmetric', reflect_type='odd')\narray([0, 1, 1, 2, 3, 4, 5, 5, 6, 7])\n >>> np.pad(a, (2, 3), 'wrap')\narray([4, 5, 1, 2, 3, 4, 5, 1, 2, 3])\n >>> def pad_with(vector, pad_width, iaxis, kwargs):\n...     pad_value = kwargs.get('padder', 10)\n...     vector[:pad_width[0]] = pad_value\n...     vector[-pad_width[1]:] = pad_value\n>>> a = np.arange(6)\n>>> a = a.reshape((2, 3))\n>>> np.pad(a, 2, pad_with)\narray([[10, 10, 10, 10, 10, 10, 10],\n       [10, 10, 10, 10, 10, 10, 10],\n       [10, 10,  0,  1,  2, 10, 10],\n       [10, 10,  3,  4,  5, 10, 10],\n       [10, 10, 10, 10, 10, 10, 10],\n       [10, 10, 10, 10, 10, 10, 10]])\n>>> np.pad(a, 2, pad_with, padder=100)\narray([[100, 100, 100, 100, 100, 100, 100],\n       [100, 100, 100, 100, 100, 100, 100],\n       [100, 100,   0,   1,   2, 100, 100],\n       [100, 100,   3,   4,   5, 100, 100],\n       [100, 100, 100, 100, 100, 100, 100],\n       [100, 100, 100, 100, 100, 100, 100]])\n \n random_noise  \nskimage.util.random_noise(image, mode='gaussian', seed=None, clip=True, **kwargs) [source]\n \nFunction to add random noise of various types to a floating-point image.  Parameters \n \nimagendarray \n\nInput image data. Will be converted to float.  \nmodestr, optional \n\nOne of the following strings, selecting the type of noise to add:  \u2018gaussian\u2019 Gaussian-distributed additive noise. \n \u2018localvar\u2019 Gaussian-distributed additive noise, with specified\n\nlocal variance at each point of image.    \u2018poisson\u2019 Poisson-distributed noise generated from the data. \u2018salt\u2019 Replaces random pixels with 1. \n \u2018pepper\u2019 Replaces random pixels with 0 (for unsigned images) or\n\n-1 (for signed images).    \n \n\u2018s&p\u2019 Replaces random pixels with either 1 or low_val, where \n\nlow_val is 0 for unsigned images or -1 for signed images.    \n \u2018speckle\u2019 Multiplicative noise using out = image + n*image, where\n\nn is Gaussian noise with specified mean & variance.      \nseedint, optional \n\nIf provided, this will set the random seed before generating noise, for valid pseudo-random comparisons.  \nclipbool, optional \n\nIf True (default), the output will be clipped after noise applied for modes \u2018speckle\u2019, \u2018poisson\u2019, and \u2018gaussian\u2019. This is needed to maintain the proper image data range. If False, clipping is not applied, and the output may extend beyond the range [-1, 1].  \nmeanfloat, optional \n\nMean of random distribution. Used in \u2018gaussian\u2019 and \u2018speckle\u2019. Default : 0.  \nvarfloat, optional \n\nVariance of random distribution. Used in \u2018gaussian\u2019 and \u2018speckle\u2019. Note: variance = (standard deviation) ** 2. Default : 0.01  \nlocal_varsndarray, optional \n\nArray of positive floats, same shape as image, defining the local variance at every image point. Used in \u2018localvar\u2019.  \namountfloat, optional \n\nProportion of image pixels to replace with noise on range [0, 1]. Used in \u2018salt\u2019, \u2018pepper\u2019, and \u2018salt & pepper\u2019. Default : 0.05  \nsalt_vs_pepperfloat, optional \n\nProportion of salt vs. pepper noise for \u2018s&p\u2019 on range [0, 1]. Higher values represent more salt. Default : 0.5 (equal amounts)    Returns \n \noutndarray \n\nOutput floating-point image data on range [0, 1] or [-1, 1] if the input image was unsigned or signed, respectively.     Notes Speckle, Poisson, Localvar, and Gaussian noise may generate noise outside the valid image range. The default is to clip (not alias) these values, but they may be preserved by setting clip=False. Note that in this case the output may contain values outside the ranges [0, 1] or [-1, 1]. Use this option with care. Because of the prevalence of exclusively positive floating-point images in intermediate calculations, it is not possible to intuit if an input is signed based on dtype alone. Instead, negative values are explicitly searched for. Only if found does this function assume signed input. Unexpected results only occur in rare, poorly exposes cases (e.g. if all values are above 50 percent gray in a signed image). In this event, manually scaling the input to the positive domain will solve the problem. The Poisson distribution is only defined for positive integers. To apply this noise type, the number of unique values in the image is found and the next round power of two is used to scale up the floating-point result, after which it is scaled back down to the floating-point image range. To generate Poisson noise against a signed image, the signed image is temporarily converted to an unsigned image in the floating point domain, Poisson noise is generated, then it is returned to the original range. \n regular_grid  \nskimage.util.regular_grid(ar_shape, n_points) [source]\n \nFind n_points regularly spaced along ar_shape. The returned points (as slices) should be as close to cubically-spaced as possible. Essentially, the points are spaced by the Nth root of the input array size, where N is the number of dimensions. However, if an array dimension cannot fit a full step size, it is \u201cdiscarded\u201d, and the computation is done for only the remaining dimensions.  Parameters \n \nar_shapearray-like of ints \n\nThe shape of the space embedding the grid. len(ar_shape) is the number of dimensions.  \nn_pointsint \n\nThe (approximate) number of points to embed in the space.    Returns \n \nslicestuple of slice objects \n\nA slice along each dimension of ar_shape, such that the intersection of all the slices give the coordinates of regularly spaced points.  Changed in version 0.14.1: In scikit-image 0.14.1 and 0.15, the return type was changed from a list to a tuple to ensure compatibility with Numpy 1.15 and higher. If your code requires the returned result to be a list, you may convert the output of this function to a list with: >>> result = list(regular_grid(ar_shape=(3, 20, 40), n_points=8))\n      Examples >>> ar = np.zeros((20, 40))\n>>> g = regular_grid(ar.shape, 8)\n>>> g\n(slice(5, None, 10), slice(5, None, 10))\n>>> ar[g] = 1\n>>> ar.sum()\n8.0\n>>> ar = np.zeros((20, 40))\n>>> g = regular_grid(ar.shape, 32)\n>>> g\n(slice(2, None, 5), slice(2, None, 5))\n>>> ar[g] = 1\n>>> ar.sum()\n32.0\n>>> ar = np.zeros((3, 20, 40))\n>>> g = regular_grid(ar.shape, 8)\n>>> g\n(slice(1, None, 3), slice(5, None, 10), slice(5, None, 10))\n>>> ar[g] = 1\n>>> ar.sum()\n8.0\n \n regular_seeds  \nskimage.util.regular_seeds(ar_shape, n_points, dtype=<class 'int'>) [source]\n \nReturn an image with ~`n_points` regularly-spaced nonzero pixels.  Parameters \n \nar_shapetuple of int \n\nThe shape of the desired output image.  \nn_pointsint \n\nThe desired number of nonzero points.  \ndtypenumpy data type, optional \n\nThe desired data type of the output.    Returns \n \nseed_imgarray of int or bool \n\nThe desired image.     Examples >>> regular_seeds((5, 5), 4)\narray([[0, 0, 0, 0, 0],\n       [0, 1, 0, 2, 0],\n       [0, 0, 0, 0, 0],\n       [0, 3, 0, 4, 0],\n       [0, 0, 0, 0, 0]])\n \n unique_rows  \nskimage.util.unique_rows(ar) [source]\n \nRemove repeated rows from a 2D array. In particular, if given an array of coordinates of shape (Npoints, Ndim), it will remove repeated points.  Parameters \n \nar2-D ndarray \n\nThe input array.    Returns \n \nar_out2-D ndarray \n\nA copy of the input array with repeated rows removed.    Raises \n \nValueErrorif ar is not two-dimensional. \n   Notes The function will generate a copy of ar if it is not C-contiguous, which will negatively affect performance for large input arrays. Examples >>> ar = np.array([[1, 0, 1],\n...                [0, 1, 0],\n...                [1, 0, 1]], np.uint8)\n>>> unique_rows(ar)\narray([[0, 1, 0],\n       [1, 0, 1]], dtype=uint8)\n \n view_as_blocks  \nskimage.util.view_as_blocks(arr_in, block_shape) [source]\n \nBlock view of the input n-dimensional array (using re-striding). Blocks are non-overlapping views of the input array.  Parameters \n \narr_inndarray \n\nN-d input array.  \nblock_shapetuple \n\nThe shape of the block. Each dimension must divide evenly into the corresponding dimensions of arr_in.    Returns \n \narr_outndarray \n\nBlock view of the input array.     Examples >>> import numpy as np\n>>> from skimage.util.shape import view_as_blocks\n>>> A = np.arange(4*4).reshape(4,4)\n>>> A\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12, 13, 14, 15]])\n>>> B = view_as_blocks(A, block_shape=(2, 2))\n>>> B[0, 0]\narray([[0, 1],\n       [4, 5]])\n>>> B[0, 1]\narray([[2, 3],\n       [6, 7]])\n>>> B[1, 0, 1, 1]\n13\n >>> A = np.arange(4*4*6).reshape(4,4,6)\n>>> A  \narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11],\n        [12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23]],\n       [[24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35],\n        [36, 37, 38, 39, 40, 41],\n        [42, 43, 44, 45, 46, 47]],\n       [[48, 49, 50, 51, 52, 53],\n        [54, 55, 56, 57, 58, 59],\n        [60, 61, 62, 63, 64, 65],\n        [66, 67, 68, 69, 70, 71]],\n       [[72, 73, 74, 75, 76, 77],\n        [78, 79, 80, 81, 82, 83],\n        [84, 85, 86, 87, 88, 89],\n        [90, 91, 92, 93, 94, 95]]])\n>>> B = view_as_blocks(A, block_shape=(1, 2, 2))\n>>> B.shape\n(4, 2, 3, 1, 2, 2)\n>>> B[2:, 0, 2]  \narray([[[[52, 53],\n         [58, 59]]],\n       [[[76, 77],\n         [82, 83]]]])\n \n view_as_windows  \nskimage.util.view_as_windows(arr_in, window_shape, step=1) [source]\n \nRolling window view of the input n-dimensional array. Windows are overlapping views of the input array, with adjacent windows shifted by a single row or column (or an index of a higher dimension).  Parameters \n \narr_inndarray \n\nN-d input array.  \nwindow_shapeinteger or tuple of length arr_in.ndim \n\nDefines the shape of the elementary n-dimensional orthotope (better know as hyperrectangle [1]) of the rolling window view. If an integer is given, the shape will be a hypercube of sidelength given by its value.  \nstepinteger or tuple of length arr_in.ndim \n\nIndicates step size at which extraction shall be performed. If integer is given, then the step is uniform in all dimensions.    Returns \n \narr_outndarray \n\n(rolling) window view of the input array.     Notes One should be very careful with rolling views when it comes to memory usage. Indeed, although a \u2018view\u2019 has the same memory footprint as its base array, the actual array that emerges when this \u2018view\u2019 is used in a computation is generally a (much) larger array than the original, especially for 2-dimensional arrays and above. For example, let us consider a 3 dimensional array of size (100, 100, 100) of float64. This array takes about 8*100**3 Bytes for storage which is just 8 MB. If one decides to build a rolling view on this array with a window of (3, 3, 3) the hypothetical size of the rolling view (if one was to reshape the view for example) would be 8*(100-3+1)**3*3**3 which is about 203 MB! The scaling becomes even worse as the dimension of the input array becomes larger. References  \n1  \nhttps://en.wikipedia.org/wiki/Hyperrectangle   Examples >>> import numpy as np\n>>> from skimage.util.shape import view_as_windows\n>>> A = np.arange(4*4).reshape(4,4)\n>>> A\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12, 13, 14, 15]])\n>>> window_shape = (2, 2)\n>>> B = view_as_windows(A, window_shape)\n>>> B[0, 0]\narray([[0, 1],\n       [4, 5]])\n>>> B[0, 1]\narray([[1, 2],\n       [5, 6]])\n >>> A = np.arange(10)\n>>> A\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n>>> window_shape = (3,)\n>>> B = view_as_windows(A, window_shape)\n>>> B.shape\n(8, 3)\n>>> B\narray([[0, 1, 2],\n       [1, 2, 3],\n       [2, 3, 4],\n       [3, 4, 5],\n       [4, 5, 6],\n       [5, 6, 7],\n       [6, 7, 8],\n       [7, 8, 9]])\n >>> A = np.arange(5*4).reshape(5, 4)\n>>> A\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12, 13, 14, 15],\n       [16, 17, 18, 19]])\n>>> window_shape = (4, 3)\n>>> B = view_as_windows(A, window_shape)\n>>> B.shape\n(2, 2, 4, 3)\n>>> B  \narray([[[[ 0,  1,  2],\n         [ 4,  5,  6],\n         [ 8,  9, 10],\n         [12, 13, 14]],\n        [[ 1,  2,  3],\n         [ 5,  6,  7],\n         [ 9, 10, 11],\n         [13, 14, 15]]],\n       [[[ 4,  5,  6],\n         [ 8,  9, 10],\n         [12, 13, 14],\n         [16, 17, 18]],\n        [[ 5,  6,  7],\n         [ 9, 10, 11],\n         [13, 14, 15],\n         [17, 18, 19]]]])\n \n\n"}, {"name": "util.apply_parallel()", "path": "api/skimage.util#skimage.util.apply_parallel", "type": "util", "text": " \nskimage.util.apply_parallel(function, array, chunks=None, depth=0, mode=None, extra_arguments=(), extra_keywords={}, *, dtype=None, multichannel=False, compute=None) [source]\n \nMap a function in parallel across an array. Split an array into possibly overlapping chunks of a given depth and boundary type, call the given function in parallel on the chunks, combine the chunks and return the resulting array.  Parameters \n \nfunctionfunction \n\nFunction to be mapped which takes an array as an argument.  \narraynumpy array or dask array \n\nArray which the function will be applied to.  \nchunksint, tuple, or tuple of tuples, optional \n\nA single integer is interpreted as the length of one side of a square chunk that should be tiled across the array. One tuple of length array.ndim represents the shape of a chunk, and it is tiled across the array. A list of tuples of length ndim, where each sub-tuple is a sequence of chunk sizes along the corresponding dimension. If None, the array is broken up into chunks based on the number of available cpus. More information about chunks is in the documentation here.  \ndepthint, optional \n\nInteger equal to the depth of the added boundary cells. Defaults to zero.  \nmode{\u2018reflect\u2019, \u2018symmetric\u2019, \u2018periodic\u2019, \u2018wrap\u2019, \u2018nearest\u2019, \u2018edge\u2019}, optional \n\ntype of external boundary padding.  \nextra_argumentstuple, optional \n\nTuple of arguments to be passed to the function.  \nextra_keywordsdictionary, optional \n\nDictionary of keyword arguments to be passed to the function.  \ndtypedata-type or None, optional \n\nThe data-type of the function output. If None, Dask will attempt to infer this by calling the function on data of shape (1,) * ndim. For functions expecting RGB or multichannel data this may be problematic. In such cases, the user should manually specify this dtype argument instead.  New in version 0.18: dtype was added in 0.18.   \nmultichannelbool, optional \n\nIf chunks is None and multichannel is True, this function will keep only a single chunk along the channels axis. When depth is specified as a scalar value, that depth will be applied only to the non-channels axes (a depth of 0 will be used along the channels axis). If the user manually specified both chunks and a depth tuple, then this argument will have no effect.  New in version 0.18: multichannel was added in 0.18.   \ncomputebool, optional \n\nIf True, compute eagerly returning a NumPy Array. If False, compute lazily returning a Dask Array. If None (default), compute based on array type provided (eagerly for NumPy Arrays and lazily for Dask Arrays).    Returns \n \noutndarray or dask Array \n\nReturns the result of the applying the operation. Type is dependent on the compute argument.     Notes Numpy edge modes \u2018symmetric\u2019, \u2018wrap\u2019, and \u2018edge\u2019 are converted to the equivalent dask boundary modes \u2018reflect\u2019, \u2018periodic\u2019 and \u2018nearest\u2019, respectively. Setting compute=False can be useful for chaining later operations. For example region selection to preview a result or storing large data to disk instead of loading in memory. \n"}, {"name": "util.compare_images()", "path": "api/skimage.util#skimage.util.compare_images", "type": "util", "text": " \nskimage.util.compare_images(image1, image2, method='diff', *, n_tiles=(8, 8)) [source]\n \nReturn an image showing the differences between two images.  New in version 0.16.   Parameters \n \nimage1, image22-D array \n\nImages to process, must be of the same shape.  \nmethodstring, optional \n\nMethod used for the comparison. Valid values are {\u2018diff\u2019, \u2018blend\u2019, \u2018checkerboard\u2019}. Details are provided in the note section.  \nn_tilestuple, optional \n\nUsed only for the checkerboard method. Specifies the number of tiles (row, column) to divide the image.    Returns \n \ncomparison2-D array \n\nImage showing the differences.     Notes 'diff' computes the absolute difference between the two images. 'blend' computes the mean value. 'checkerboard' makes tiles of dimension n_tiles that display alternatively the first and the second image. \n"}, {"name": "util.crop()", "path": "api/skimage.util#skimage.util.crop", "type": "util", "text": " \nskimage.util.crop(ar, crop_width, copy=False, order='K') [source]\n \nCrop array ar by crop_width along each dimension.  Parameters \n \nararray-like of rank N \n\nInput array.  \ncrop_width{sequence, int} \n\nNumber of values to remove from the edges of each axis. ((before_1, after_1), \u2026 (before_N, after_N)) specifies unique crop widths at the start and end of each axis. ((before, after),) or (before, after) specifies a fixed start and end crop for every axis. (n,) or n for integer n is a shortcut for before = after = n for all axes.  \ncopybool, optional \n\nIf True, ensure the returned array is a contiguous copy. Normally, a crop operation will return a discontiguous view of the underlying input array.  \norder{\u2018C\u2019, \u2018F\u2019, \u2018A\u2019, \u2018K\u2019}, optional \n\nIf copy==True, control the memory layout of the copy. See np.copy.    Returns \n \ncroppedarray \n\nThe cropped array. If copy=False (default), this is a sliced view of the input array.     \n"}, {"name": "util.dtype_limits()", "path": "api/skimage.util#skimage.util.dtype_limits", "type": "util", "text": " \nskimage.util.dtype_limits(image, clip_negative=False) [source]\n \nReturn intensity limits, i.e. (min, max) tuple, of the image\u2019s dtype.  Parameters \n \nimagendarray \n\nInput image.  \nclip_negativebool, optional \n\nIf True, clip the negative range (i.e. return 0 for min intensity) even if the image dtype allows negative values.    Returns \n \nimin, imaxtuple \n\nLower and upper intensity limits.     \n"}, {"name": "util.img_as_bool()", "path": "api/skimage.util#skimage.util.img_as_bool", "type": "util", "text": " \nskimage.util.img_as_bool(image, force_copy=False) [source]\n \nConvert an image to boolean format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of bool (bool_) \n\nOutput image.     Notes The upper half of the input dtype\u2019s positive range is True, and the lower half is False. All negative values (if present) are False. \n"}, {"name": "util.img_as_float()", "path": "api/skimage.util#skimage.util.img_as_float", "type": "util", "text": " \nskimage.util.img_as_float(image, force_copy=False) [source]\n \nConvert an image to floating point format. This function is similar to img_as_float64, but will not convert lower-precision floating point arrays to float64.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of float \n\nOutput image.     Notes The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0]. \n"}, {"name": "util.img_as_float32()", "path": "api/skimage.util#skimage.util.img_as_float32", "type": "util", "text": " \nskimage.util.img_as_float32(image, force_copy=False) [source]\n \nConvert an image to single-precision (32-bit) floating point format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of float32 \n\nOutput image.     Notes The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0]. \n"}, {"name": "util.img_as_float64()", "path": "api/skimage.util#skimage.util.img_as_float64", "type": "util", "text": " \nskimage.util.img_as_float64(image, force_copy=False) [source]\n \nConvert an image to double-precision (64-bit) floating point format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of float64 \n\nOutput image.     Notes The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0]. \n"}, {"name": "util.img_as_int()", "path": "api/skimage.util#skimage.util.img_as_int", "type": "util", "text": " \nskimage.util.img_as_int(image, force_copy=False) [source]\n \nConvert an image to 16-bit signed integer format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of int16 \n\nOutput image.     Notes The values are scaled between -32768 and 32767. If the input data-type is positive-only (e.g., uint8), then the output image will still only have positive values. \n"}, {"name": "util.img_as_ubyte()", "path": "api/skimage.util#skimage.util.img_as_ubyte", "type": "util", "text": " \nskimage.util.img_as_ubyte(image, force_copy=False) [source]\n \nConvert an image to 8-bit unsigned integer format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of ubyte (uint8) \n\nOutput image.     Notes Negative input values will be clipped. Positive values are scaled between 0 and 255. \n"}, {"name": "util.img_as_uint()", "path": "api/skimage.util#skimage.util.img_as_uint", "type": "util", "text": " \nskimage.util.img_as_uint(image, force_copy=False) [source]\n \nConvert an image to 16-bit unsigned integer format.  Parameters \n \nimagendarray \n\nInput image.  \nforce_copybool, optional \n\nForce a copy of the data, irrespective of its current dtype.    Returns \n \noutndarray of uint16 \n\nOutput image.     Notes Negative input values will be clipped. Positive values are scaled between 0 and 65535. \n"}, {"name": "util.invert()", "path": "api/skimage.util#skimage.util.invert", "type": "util", "text": " \nskimage.util.invert(image, signed_float=False) [source]\n \nInvert an image. Invert the intensity range of the input image, so that the dtype maximum is now the dtype minimum, and vice-versa. This operation is slightly different depending on the input dtype:  unsigned integers: subtract the image from the dtype maximum signed integers: subtract the image from -1 (see Notes) floats: subtract the image from 1 (if signed_float is False, so we assume the image is unsigned), or from 0 (if signed_float is True).  See the examples for clarification.  Parameters \n \nimagendarray \n\nInput image.  \nsigned_floatbool, optional \n\nIf True and the image is of type float, the range is assumed to be [-1, 1]. If False and the image is of type float, the range is assumed to be [0, 1].    Returns \n \ninvertedndarray \n\nInverted image.     Notes Ideally, for signed integers we would simply multiply by -1. However, signed integer ranges are asymmetric. For example, for np.int8, the range of possible values is [-128, 127], so that -128 * -1 equals -128! By subtracting from -1, we correctly map the maximum dtype value to the minimum. Examples >>> img = np.array([[100,  0, 200],\n...                 [  0, 50,   0],\n...                 [ 30,  0, 255]], np.uint8)\n>>> invert(img)\narray([[155, 255,  55],\n       [255, 205, 255],\n       [225, 255,   0]], dtype=uint8)\n>>> img2 = np.array([[ -2, 0, -128],\n...                  [127, 0,    5]], np.int8)\n>>> invert(img2)\narray([[   1,   -1,  127],\n       [-128,   -1,   -6]], dtype=int8)\n>>> img3 = np.array([[ 0., 1., 0.5, 0.75]])\n>>> invert(img3)\narray([[1.  , 0.  , 0.5 , 0.25]])\n>>> img4 = np.array([[ 0., 1., -1., -0.25]])\n>>> invert(img4, signed_float=True)\narray([[-0.  , -1.  ,  1.  ,  0.25]])\n \n"}, {"name": "util.map_array()", "path": "api/skimage.util#skimage.util.map_array", "type": "util", "text": " \nskimage.util.map_array(input_arr, input_vals, output_vals, out=None) [source]\n \nMap values from input array from input_vals to output_vals.  Parameters \n \ninput_arrarray of int, shape (M[, N][, P][, \u2026]) \n\nThe input label image.  \ninput_valsarray of int, shape (N,) \n\nThe values to map from.  \noutput_valsarray, shape (N,) \n\nThe values to map to.  out: array, same shape as `input_arr`\n\nThe output array. Will be created if not provided. It should have the same dtype as output_vals.    Returns \n \noutarray, same shape as input_arr \n\nThe array of mapped values.     \n"}, {"name": "util.montage()", "path": "api/skimage.util#skimage.util.montage", "type": "util", "text": " \nskimage.util.montage(arr_in, fill='mean', rescale_intensity=False, grid_shape=None, padding_width=0, multichannel=False) [source]\n \nCreate a montage of several single- or multichannel images. Create a rectangular montage from an input array representing an ensemble of equally shaped single- (gray) or multichannel (color) images. For example, montage(arr_in) called with the following arr_in  \n1 2 3   will return  \n1 2  \n3 \n      where the \u2018*\u2019 patch will be determined by the fill parameter.  Parameters \n \narr_in(K, M, N[, C]) ndarray \n\nAn array representing an ensemble of K images of equal shape.  \nfillfloat or array-like of floats or \u2018mean\u2019, optional \n\nValue to fill the padding areas and/or the extra tiles in the output array. Has to be float for single channel collections. For multichannel collections has to be an array-like of shape of number of channels. If mean, uses the mean value over all images.  \nrescale_intensitybool, optional \n\nWhether to rescale the intensity of each image to [0, 1].  \ngrid_shapetuple, optional \n\nThe desired grid shape for the montage (ntiles_row, ntiles_column). The default aspect ratio is square.  \npadding_widthint, optional \n\nThe size of the spacing between the tiles and between the tiles and the borders. If non-zero, makes the boundaries of individual images easier to perceive.  \nmultichannelboolean, optional \n\nIf True, the last arr_in dimension is threated as a color channel, otherwise as spatial.    Returns \n \narr_out(K*(M+p)+p, K*(N+p)+p[, C]) ndarray \n\nOutput array with input images glued together (including padding p).     Examples >>> import numpy as np\n>>> from skimage.util import montage\n>>> arr_in = np.arange(3 * 2 * 2).reshape(3, 2, 2)\n>>> arr_in  \narray([[[ 0,  1],\n        [ 2,  3]],\n       [[ 4,  5],\n        [ 6,  7]],\n       [[ 8,  9],\n        [10, 11]]])\n>>> arr_out = montage(arr_in)\n>>> arr_out.shape\n(4, 4)\n>>> arr_out\narray([[ 0,  1,  4,  5],\n       [ 2,  3,  6,  7],\n       [ 8,  9,  5,  5],\n       [10, 11,  5,  5]])\n>>> arr_in.mean()\n5.5\n>>> arr_out_nonsquare = montage(arr_in, grid_shape=(1, 3))\n>>> arr_out_nonsquare\narray([[ 0,  1,  4,  5,  8,  9],\n       [ 2,  3,  6,  7, 10, 11]])\n>>> arr_out_nonsquare.shape\n(2, 6)\n \n"}, {"name": "util.pad()", "path": "api/skimage.util#skimage.util.pad", "type": "util", "text": " \nskimage.util.pad(array, pad_width, mode='constant', **kwargs) [source]\n \nPad an array.  Parameters \n \narrayarray_like of rank N \n\nThe array to pad.  \npad_width{sequence, array_like, int} \n\nNumber of values padded to the edges of each axis. ((before_1, after_1), \u2026 (before_N, after_N)) unique pad widths for each axis. ((before, after),) yields same before and after pad for each axis. (pad,) or int is a shortcut for before = after = pad width for all axes.  \nmodestr or function, optional \n\nOne of the following string values or a user supplied function.  \u2018constant\u2019 (default)\n\nPads with a constant value.  \u2018edge\u2019\n\nPads with the edge values of array.  \u2018linear_ramp\u2019\n\nPads with the linear ramp between end_value and the array edge value.  \u2018maximum\u2019\n\nPads with the maximum value of all or part of the vector along each axis.  \u2018mean\u2019\n\nPads with the mean value of all or part of the vector along each axis.  \u2018median\u2019\n\nPads with the median value of all or part of the vector along each axis.  \u2018minimum\u2019\n\nPads with the minimum value of all or part of the vector along each axis.  \u2018reflect\u2019\n\nPads with the reflection of the vector mirrored on the first and last values of the vector along each axis.  \u2018symmetric\u2019\n\nPads with the reflection of the vector mirrored along the edge of the array.  \u2018wrap\u2019\n\nPads with the wrap of the vector along the axis. The first values are used to pad the end and the end values are used to pad the beginning.  \u2018empty\u2019\n\nPads with undefined values.  New in version 1.17.   <function>\n\nPadding function, see Notes.    \nstat_lengthsequence or int, optional \n\nUsed in \u2018maximum\u2019, \u2018mean\u2019, \u2018median\u2019, and \u2018minimum\u2019. Number of values at edge of each axis used to calculate the statistic value. ((before_1, after_1), \u2026 (before_N, after_N)) unique statistic lengths for each axis. ((before, after),) yields same before and after statistic lengths for each axis. (stat_length,) or int is a shortcut for before = after = statistic length for all axes. Default is None, to use the entire axis.  \nconstant_valuessequence or scalar, optional \n\nUsed in \u2018constant\u2019. The values to set the padded values for each axis. ((before_1, after_1), ... (before_N, after_N)) unique pad constants for each axis. ((before, after),) yields same before and after constants for each axis. (constant,) or constant is a shortcut for before = after = constant for all axes. Default is 0.  \nend_valuessequence or scalar, optional \n\nUsed in \u2018linear_ramp\u2019. The values used for the ending value of the linear_ramp and that will form the edge of the padded array. ((before_1, after_1), ... (before_N, after_N)) unique end values for each axis. ((before, after),) yields same before and after end values for each axis. (constant,) or constant is a shortcut for before = after = constant for all axes. Default is 0.  \nreflect_type{\u2018even\u2019, \u2018odd\u2019}, optional \n\nUsed in \u2018reflect\u2019, and \u2018symmetric\u2019. The \u2018even\u2019 style is the default with an unaltered reflection around the edge value. For the \u2018odd\u2019 style, the extended part of the array is created by subtracting the reflected values from two times the edge value.    Returns \n \npadndarray \n\nPadded array of rank equal to array with shape increased according to pad_width.     Notes  New in version 1.7.0.  For an array with rank greater than 1, some of the padding of later axes is calculated from padding of previous axes. This is easiest to think about with a rank 2 array where the corners of the padded array are calculated by using padded values from the first axis. The padding function, if used, should modify a rank 1 array in-place. It has the following signature: padding_func(vector, iaxis_pad_width, iaxis, kwargs)\n where  \nvectorndarray \n\nA rank 1 array already padded with zeros. Padded values are vector[:iaxis_pad_width[0]] and vector[-iaxis_pad_width[1]:].  \niaxis_pad_widthtuple \n\nA 2-tuple of ints, iaxis_pad_width[0] represents the number of values padded at the beginning of vector where iaxis_pad_width[1] represents the number of values padded at the end of vector.  \niaxisint \n\nThe axis currently being calculated.  \nkwargsdict \n\nAny keyword arguments the function requires.   Examples >>> a = [1, 2, 3, 4, 5]\n>>> np.pad(a, (2, 3), 'constant', constant_values=(4, 6))\narray([4, 4, 1, ..., 6, 6, 6])\n >>> np.pad(a, (2, 3), 'edge')\narray([1, 1, 1, ..., 5, 5, 5])\n >>> np.pad(a, (2, 3), 'linear_ramp', end_values=(5, -4))\narray([ 5,  3,  1,  2,  3,  4,  5,  2, -1, -4])\n >>> np.pad(a, (2,), 'maximum')\narray([5, 5, 1, 2, 3, 4, 5, 5, 5])\n >>> np.pad(a, (2,), 'mean')\narray([3, 3, 1, 2, 3, 4, 5, 3, 3])\n >>> np.pad(a, (2,), 'median')\narray([3, 3, 1, 2, 3, 4, 5, 3, 3])\n >>> a = [[1, 2], [3, 4]]\n>>> np.pad(a, ((3, 2), (2, 3)), 'minimum')\narray([[1, 1, 1, 2, 1, 1, 1],\n       [1, 1, 1, 2, 1, 1, 1],\n       [1, 1, 1, 2, 1, 1, 1],\n       [1, 1, 1, 2, 1, 1, 1],\n       [3, 3, 3, 4, 3, 3, 3],\n       [1, 1, 1, 2, 1, 1, 1],\n       [1, 1, 1, 2, 1, 1, 1]])\n >>> a = [1, 2, 3, 4, 5]\n>>> np.pad(a, (2, 3), 'reflect')\narray([3, 2, 1, 2, 3, 4, 5, 4, 3, 2])\n >>> np.pad(a, (2, 3), 'reflect', reflect_type='odd')\narray([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8])\n >>> np.pad(a, (2, 3), 'symmetric')\narray([2, 1, 1, 2, 3, 4, 5, 5, 4, 3])\n >>> np.pad(a, (2, 3), 'symmetric', reflect_type='odd')\narray([0, 1, 1, 2, 3, 4, 5, 5, 6, 7])\n >>> np.pad(a, (2, 3), 'wrap')\narray([4, 5, 1, 2, 3, 4, 5, 1, 2, 3])\n >>> def pad_with(vector, pad_width, iaxis, kwargs):\n...     pad_value = kwargs.get('padder', 10)\n...     vector[:pad_width[0]] = pad_value\n...     vector[-pad_width[1]:] = pad_value\n>>> a = np.arange(6)\n>>> a = a.reshape((2, 3))\n>>> np.pad(a, 2, pad_with)\narray([[10, 10, 10, 10, 10, 10, 10],\n       [10, 10, 10, 10, 10, 10, 10],\n       [10, 10,  0,  1,  2, 10, 10],\n       [10, 10,  3,  4,  5, 10, 10],\n       [10, 10, 10, 10, 10, 10, 10],\n       [10, 10, 10, 10, 10, 10, 10]])\n>>> np.pad(a, 2, pad_with, padder=100)\narray([[100, 100, 100, 100, 100, 100, 100],\n       [100, 100, 100, 100, 100, 100, 100],\n       [100, 100,   0,   1,   2, 100, 100],\n       [100, 100,   3,   4,   5, 100, 100],\n       [100, 100, 100, 100, 100, 100, 100],\n       [100, 100, 100, 100, 100, 100, 100]])\n \n"}, {"name": "util.random_noise()", "path": "api/skimage.util#skimage.util.random_noise", "type": "util", "text": " \nskimage.util.random_noise(image, mode='gaussian', seed=None, clip=True, **kwargs) [source]\n \nFunction to add random noise of various types to a floating-point image.  Parameters \n \nimagendarray \n\nInput image data. Will be converted to float.  \nmodestr, optional \n\nOne of the following strings, selecting the type of noise to add:  \u2018gaussian\u2019 Gaussian-distributed additive noise. \n \u2018localvar\u2019 Gaussian-distributed additive noise, with specified\n\nlocal variance at each point of image.    \u2018poisson\u2019 Poisson-distributed noise generated from the data. \u2018salt\u2019 Replaces random pixels with 1. \n \u2018pepper\u2019 Replaces random pixels with 0 (for unsigned images) or\n\n-1 (for signed images).    \n \n\u2018s&p\u2019 Replaces random pixels with either 1 or low_val, where \n\nlow_val is 0 for unsigned images or -1 for signed images.    \n \u2018speckle\u2019 Multiplicative noise using out = image + n*image, where\n\nn is Gaussian noise with specified mean & variance.      \nseedint, optional \n\nIf provided, this will set the random seed before generating noise, for valid pseudo-random comparisons.  \nclipbool, optional \n\nIf True (default), the output will be clipped after noise applied for modes \u2018speckle\u2019, \u2018poisson\u2019, and \u2018gaussian\u2019. This is needed to maintain the proper image data range. If False, clipping is not applied, and the output may extend beyond the range [-1, 1].  \nmeanfloat, optional \n\nMean of random distribution. Used in \u2018gaussian\u2019 and \u2018speckle\u2019. Default : 0.  \nvarfloat, optional \n\nVariance of random distribution. Used in \u2018gaussian\u2019 and \u2018speckle\u2019. Note: variance = (standard deviation) ** 2. Default : 0.01  \nlocal_varsndarray, optional \n\nArray of positive floats, same shape as image, defining the local variance at every image point. Used in \u2018localvar\u2019.  \namountfloat, optional \n\nProportion of image pixels to replace with noise on range [0, 1]. Used in \u2018salt\u2019, \u2018pepper\u2019, and \u2018salt & pepper\u2019. Default : 0.05  \nsalt_vs_pepperfloat, optional \n\nProportion of salt vs. pepper noise for \u2018s&p\u2019 on range [0, 1]. Higher values represent more salt. Default : 0.5 (equal amounts)    Returns \n \noutndarray \n\nOutput floating-point image data on range [0, 1] or [-1, 1] if the input image was unsigned or signed, respectively.     Notes Speckle, Poisson, Localvar, and Gaussian noise may generate noise outside the valid image range. The default is to clip (not alias) these values, but they may be preserved by setting clip=False. Note that in this case the output may contain values outside the ranges [0, 1] or [-1, 1]. Use this option with care. Because of the prevalence of exclusively positive floating-point images in intermediate calculations, it is not possible to intuit if an input is signed based on dtype alone. Instead, negative values are explicitly searched for. Only if found does this function assume signed input. Unexpected results only occur in rare, poorly exposes cases (e.g. if all values are above 50 percent gray in a signed image). In this event, manually scaling the input to the positive domain will solve the problem. The Poisson distribution is only defined for positive integers. To apply this noise type, the number of unique values in the image is found and the next round power of two is used to scale up the floating-point result, after which it is scaled back down to the floating-point image range. To generate Poisson noise against a signed image, the signed image is temporarily converted to an unsigned image in the floating point domain, Poisson noise is generated, then it is returned to the original range. \n"}, {"name": "util.regular_grid()", "path": "api/skimage.util#skimage.util.regular_grid", "type": "util", "text": " \nskimage.util.regular_grid(ar_shape, n_points) [source]\n \nFind n_points regularly spaced along ar_shape. The returned points (as slices) should be as close to cubically-spaced as possible. Essentially, the points are spaced by the Nth root of the input array size, where N is the number of dimensions. However, if an array dimension cannot fit a full step size, it is \u201cdiscarded\u201d, and the computation is done for only the remaining dimensions.  Parameters \n \nar_shapearray-like of ints \n\nThe shape of the space embedding the grid. len(ar_shape) is the number of dimensions.  \nn_pointsint \n\nThe (approximate) number of points to embed in the space.    Returns \n \nslicestuple of slice objects \n\nA slice along each dimension of ar_shape, such that the intersection of all the slices give the coordinates of regularly spaced points.  Changed in version 0.14.1: In scikit-image 0.14.1 and 0.15, the return type was changed from a list to a tuple to ensure compatibility with Numpy 1.15 and higher. If your code requires the returned result to be a list, you may convert the output of this function to a list with: >>> result = list(regular_grid(ar_shape=(3, 20, 40), n_points=8))\n      Examples >>> ar = np.zeros((20, 40))\n>>> g = regular_grid(ar.shape, 8)\n>>> g\n(slice(5, None, 10), slice(5, None, 10))\n>>> ar[g] = 1\n>>> ar.sum()\n8.0\n>>> ar = np.zeros((20, 40))\n>>> g = regular_grid(ar.shape, 32)\n>>> g\n(slice(2, None, 5), slice(2, None, 5))\n>>> ar[g] = 1\n>>> ar.sum()\n32.0\n>>> ar = np.zeros((3, 20, 40))\n>>> g = regular_grid(ar.shape, 8)\n>>> g\n(slice(1, None, 3), slice(5, None, 10), slice(5, None, 10))\n>>> ar[g] = 1\n>>> ar.sum()\n8.0\n \n"}, {"name": "util.regular_seeds()", "path": "api/skimage.util#skimage.util.regular_seeds", "type": "util", "text": " \nskimage.util.regular_seeds(ar_shape, n_points, dtype=<class 'int'>) [source]\n \nReturn an image with ~`n_points` regularly-spaced nonzero pixels.  Parameters \n \nar_shapetuple of int \n\nThe shape of the desired output image.  \nn_pointsint \n\nThe desired number of nonzero points.  \ndtypenumpy data type, optional \n\nThe desired data type of the output.    Returns \n \nseed_imgarray of int or bool \n\nThe desired image.     Examples >>> regular_seeds((5, 5), 4)\narray([[0, 0, 0, 0, 0],\n       [0, 1, 0, 2, 0],\n       [0, 0, 0, 0, 0],\n       [0, 3, 0, 4, 0],\n       [0, 0, 0, 0, 0]])\n \n"}, {"name": "util.unique_rows()", "path": "api/skimage.util#skimage.util.unique_rows", "type": "util", "text": " \nskimage.util.unique_rows(ar) [source]\n \nRemove repeated rows from a 2D array. In particular, if given an array of coordinates of shape (Npoints, Ndim), it will remove repeated points.  Parameters \n \nar2-D ndarray \n\nThe input array.    Returns \n \nar_out2-D ndarray \n\nA copy of the input array with repeated rows removed.    Raises \n \nValueErrorif ar is not two-dimensional. \n   Notes The function will generate a copy of ar if it is not C-contiguous, which will negatively affect performance for large input arrays. Examples >>> ar = np.array([[1, 0, 1],\n...                [0, 1, 0],\n...                [1, 0, 1]], np.uint8)\n>>> unique_rows(ar)\narray([[0, 1, 0],\n       [1, 0, 1]], dtype=uint8)\n \n"}, {"name": "util.view_as_blocks()", "path": "api/skimage.util#skimage.util.view_as_blocks", "type": "util", "text": " \nskimage.util.view_as_blocks(arr_in, block_shape) [source]\n \nBlock view of the input n-dimensional array (using re-striding). Blocks are non-overlapping views of the input array.  Parameters \n \narr_inndarray \n\nN-d input array.  \nblock_shapetuple \n\nThe shape of the block. Each dimension must divide evenly into the corresponding dimensions of arr_in.    Returns \n \narr_outndarray \n\nBlock view of the input array.     Examples >>> import numpy as np\n>>> from skimage.util.shape import view_as_blocks\n>>> A = np.arange(4*4).reshape(4,4)\n>>> A\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12, 13, 14, 15]])\n>>> B = view_as_blocks(A, block_shape=(2, 2))\n>>> B[0, 0]\narray([[0, 1],\n       [4, 5]])\n>>> B[0, 1]\narray([[2, 3],\n       [6, 7]])\n>>> B[1, 0, 1, 1]\n13\n >>> A = np.arange(4*4*6).reshape(4,4,6)\n>>> A  \narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11],\n        [12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23]],\n       [[24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35],\n        [36, 37, 38, 39, 40, 41],\n        [42, 43, 44, 45, 46, 47]],\n       [[48, 49, 50, 51, 52, 53],\n        [54, 55, 56, 57, 58, 59],\n        [60, 61, 62, 63, 64, 65],\n        [66, 67, 68, 69, 70, 71]],\n       [[72, 73, 74, 75, 76, 77],\n        [78, 79, 80, 81, 82, 83],\n        [84, 85, 86, 87, 88, 89],\n        [90, 91, 92, 93, 94, 95]]])\n>>> B = view_as_blocks(A, block_shape=(1, 2, 2))\n>>> B.shape\n(4, 2, 3, 1, 2, 2)\n>>> B[2:, 0, 2]  \narray([[[[52, 53],\n         [58, 59]]],\n       [[[76, 77],\n         [82, 83]]]])\n \n"}, {"name": "util.view_as_windows()", "path": "api/skimage.util#skimage.util.view_as_windows", "type": "util", "text": " \nskimage.util.view_as_windows(arr_in, window_shape, step=1) [source]\n \nRolling window view of the input n-dimensional array. Windows are overlapping views of the input array, with adjacent windows shifted by a single row or column (or an index of a higher dimension).  Parameters \n \narr_inndarray \n\nN-d input array.  \nwindow_shapeinteger or tuple of length arr_in.ndim \n\nDefines the shape of the elementary n-dimensional orthotope (better know as hyperrectangle [1]) of the rolling window view. If an integer is given, the shape will be a hypercube of sidelength given by its value.  \nstepinteger or tuple of length arr_in.ndim \n\nIndicates step size at which extraction shall be performed. If integer is given, then the step is uniform in all dimensions.    Returns \n \narr_outndarray \n\n(rolling) window view of the input array.     Notes One should be very careful with rolling views when it comes to memory usage. Indeed, although a \u2018view\u2019 has the same memory footprint as its base array, the actual array that emerges when this \u2018view\u2019 is used in a computation is generally a (much) larger array than the original, especially for 2-dimensional arrays and above. For example, let us consider a 3 dimensional array of size (100, 100, 100) of float64. This array takes about 8*100**3 Bytes for storage which is just 8 MB. If one decides to build a rolling view on this array with a window of (3, 3, 3) the hypothetical size of the rolling view (if one was to reshape the view for example) would be 8*(100-3+1)**3*3**3 which is about 203 MB! The scaling becomes even worse as the dimension of the input array becomes larger. References  \n1  \nhttps://en.wikipedia.org/wiki/Hyperrectangle   Examples >>> import numpy as np\n>>> from skimage.util.shape import view_as_windows\n>>> A = np.arange(4*4).reshape(4,4)\n>>> A\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12, 13, 14, 15]])\n>>> window_shape = (2, 2)\n>>> B = view_as_windows(A, window_shape)\n>>> B[0, 0]\narray([[0, 1],\n       [4, 5]])\n>>> B[0, 1]\narray([[1, 2],\n       [5, 6]])\n >>> A = np.arange(10)\n>>> A\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n>>> window_shape = (3,)\n>>> B = view_as_windows(A, window_shape)\n>>> B.shape\n(8, 3)\n>>> B\narray([[0, 1, 2],\n       [1, 2, 3],\n       [2, 3, 4],\n       [3, 4, 5],\n       [4, 5, 6],\n       [5, 6, 7],\n       [6, 7, 8],\n       [7, 8, 9]])\n >>> A = np.arange(5*4).reshape(5, 4)\n>>> A\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12, 13, 14, 15],\n       [16, 17, 18, 19]])\n>>> window_shape = (4, 3)\n>>> B = view_as_windows(A, window_shape)\n>>> B.shape\n(2, 2, 4, 3)\n>>> B  \narray([[[[ 0,  1,  2],\n         [ 4,  5,  6],\n         [ 8,  9, 10],\n         [12, 13, 14]],\n        [[ 1,  2,  3],\n         [ 5,  6,  7],\n         [ 9, 10, 11],\n         [13, 14, 15]]],\n       [[[ 4,  5,  6],\n         [ 8,  9, 10],\n         [12, 13, 14],\n         [16, 17, 18]],\n        [[ 5,  6,  7],\n         [ 9, 10, 11],\n         [13, 14, 15],\n         [17, 18, 19]]]])\n \n"}, {"name": "viewer", "path": "api/skimage.viewer", "type": "viewer", "text": "Module: viewer  \nskimage.viewer.CollectionViewer(image_collection) Viewer for displaying image collections.  \nskimage.viewer.ImageViewer(image[, useblit]) Viewer for displaying images.  \nskimage.viewer.canvastools   \nskimage.viewer.plugins   \nskimage.viewer.qt   \nskimage.viewer.utils   \nskimage.viewer.viewers   \nskimage.viewer.widgets Widgets for interacting with ImageViewer.   CollectionViewer  \nclass skimage.viewer.CollectionViewer(image_collection, update_on='move', **kwargs) [source]\n \nBases: skimage.viewer.viewers.core.ImageViewer Viewer for displaying image collections. Select the displayed frame of the image collection using the slider or with the following keyboard shortcuts:  left/right arrows\n\nPrevious/next image in collection.  number keys, 0\u20139\n\n0% to 90% of collection. For example, \u201c5\u201d goes to the image in the middle (i.e. 50%) of the collection.  home/end keys\n\nFirst/last image in collection.    Parameters \n \nimage_collectionlist of images \n\nList of images to be displayed.  \nupdate_on{\u2018move\u2019 | \u2018release\u2019} \n\nControl whether image is updated on slide or release of the image slider. Using \u2018on_release\u2019 will give smoother behavior when displaying large images or when writing a plugin/subclass that requires heavy computation.      \n__init__(image_collection, update_on='move', **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nkeyPressEvent(event) [source]\n\n  \nupdate_index(name, index) [source]\n \nSelect image on display using index into image collection. \n \n ImageViewer  \nclass skimage.viewer.ImageViewer(image, useblit=True) [source]\n \nBases: object Viewer for displaying images. This viewer is a simple container object that holds a Matplotlib axes for showing images. ImageViewer doesn\u2019t subclass the Matplotlib axes (or figure) because of the high probability of name collisions. Subclasses and plugins will likely extend the update_image method to add custom overlays or filter the displayed image.  Parameters \n \nimagearray \n\nImage being viewed.     Examples >>> from skimage import data\n>>> image = data.coins()\n>>> viewer = ImageViewer(image) \n>>> viewer.show()               \n  Attributes \n \ncanvas, fig, axMatplotlib canvas, figure, and axes \n\nMatplotlib canvas, figure, and axes used to display image.  \nimagearray \n\nImage being viewed. Setting this value will update the displayed frame.  \noriginal_imagearray \n\nPlugins typically operate on (but don\u2019t change) the original image.  \npluginslist \n\nList of attached plugins.      \n__init__(image, useblit=True) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nadd_tool(tool) [source]\n\n  \ncloseEvent(event) [source]\n\n  \nconnect_event(event, callback) [source]\n \nConnect callback function to matplotlib event and return id. \n  \ndisconnect_event(callback_id) [source]\n \nDisconnect callback by its id (returned by connect_event). \n  \ndock_areas = {'bottom': None, 'left': None, 'right': None, 'top': None} \n  \nproperty image \n  \nopen_file(filename=None) [source]\n \nOpen image file and display in viewer. \n  \noriginal_image_changed = None \n  \nredraw() [source]\n\n  \nremove_tool(tool) [source]\n\n  \nreset_image() [source]\n\n  \nsave_to_file(filename=None) [source]\n \nSave current image to file. The current behavior is not ideal: It saves the image displayed on screen, so all images will be converted to RGB, and the image size is not preserved (resizing the viewer window will alter the size of the saved image). \n  \nshow(main_window=True) [source]\n \nShow ImageViewer and attached plugins. This behaves much like matplotlib.pyplot.show and QWidget.show. \n  \nupdate_image(image) [source]\n \nUpdate displayed image. This method can be overridden or extended in subclasses and plugins to react to image changes. \n \n\n"}, {"name": "viewer.canvastools", "path": "api/skimage.viewer.canvastools", "type": "viewer", "text": "Module: viewer.canvastools  \nskimage.viewer.canvastools.LineTool(manager) Widget for line selection in a plot.  \nskimage.viewer.canvastools.PaintTool(\u2026[, \u2026]) Widget for painting on top of a plot.  \nskimage.viewer.canvastools.RectangleTool(manager) Widget for selecting a rectangular region in a plot.  \nskimage.viewer.canvastools.ThickLineTool(manager) Widget for line selection in a plot.  \nskimage.viewer.canvastools.base   \nskimage.viewer.canvastools.linetool   \nskimage.viewer.canvastools.painttool   \nskimage.viewer.canvastools.recttool    LineTool  \nclass skimage.viewer.canvastools.LineTool(manager, on_move=None, on_release=None, on_enter=None, maxdist=10, line_props=None, handle_props=None, **kwargs) [source]\n \nBases: skimage.viewer.canvastools.base.CanvasToolBase Widget for line selection in a plot.  Parameters \n \nmanagerViewer or PlotPlugin. \n\nSkimage viewer or plot plugin object.  \non_movefunction \n\nFunction called whenever a control handle is moved. This function must accept the end points of line as the only argument.  \non_releasefunction \n\nFunction called whenever the control handle is released.  \non_enterfunction \n\nFunction called whenever the \u201center\u201d key is pressed.  \nmaxdistfloat \n\nMaximum pixel distance allowed when selecting control handle.  \nline_propsdict \n\nProperties for matplotlib.lines.Line2D.  \nhandle_propsdict \n\nMarker properties for the handles (also see matplotlib.lines.Line2D).    Attributes \n \nend_points2D array \n\nEnd points of line ((x1, y1), (x2, y2)).      \n__init__(manager, on_move=None, on_release=None, on_enter=None, maxdist=10, line_props=None, handle_props=None, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nproperty end_points \n  \nproperty geometry  \nGeometry information that gets passed to callback functions. \n  \nhit_test(event) [source]\n\n  \non_mouse_press(event) [source]\n\n  \non_mouse_release(event) [source]\n\n  \non_move(event) [source]\n\n  \nupdate(x=None, y=None) [source]\n\n \n PaintTool  \nclass skimage.viewer.canvastools.PaintTool(manager, overlay_shape, radius=5, alpha=0.3, on_move=None, on_release=None, on_enter=None, rect_props=None) [source]\n \nBases: skimage.viewer.canvastools.base.CanvasToolBase Widget for painting on top of a plot.  Parameters \n \nmanagerViewer or PlotPlugin. \n\nSkimage viewer or plot plugin object.  \noverlay_shapeshape tuple \n\n2D shape tuple used to initialize overlay image.  \nradiusint \n\nThe size of the paint cursor.  \nalphafloat (between [0, 1]) \n\nOpacity of overlay.  \non_movefunction \n\nFunction called whenever a control handle is moved. This function must accept the end points of line as the only argument.  \non_releasefunction \n\nFunction called whenever the control handle is released.  \non_enterfunction \n\nFunction called whenever the \u201center\u201d key is pressed.  \nrect_propsdict \n\nProperties for matplotlib.patches.Rectangle. This class redefines defaults in matplotlib.widgets.RectangleSelector.     Examples >>> from skimage.data import camera\n>>> import matplotlib.pyplot as plt\n>>> from skimage.viewer.canvastools import PaintTool\n>>> import numpy as np\n >>> img = camera() \n >>> ax = plt.subplot(111) \n>>> plt.imshow(img, cmap=plt.cm.gray) \n>>> p = PaintTool(ax,np.shape(img[:-1]),10,0.2) \n>>> plt.show() \n >>> mask = p.overlay \n>>> plt.imshow(mask,cmap=plt.cm.gray) \n>>> plt.show() \n  Attributes \n \noverlayarray \n\nOverlay of painted labels displayed on top of image.  \nlabelint \n\nCurrent paint color.      \n__init__(manager, overlay_shape, radius=5, alpha=0.3, on_move=None, on_release=None, on_enter=None, rect_props=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nproperty geometry  \nGeometry information that gets passed to callback functions. \n  \nproperty label \n  \non_key_press(event) [source]\n\n  \non_mouse_press(event) [source]\n\n  \non_mouse_release(event) [source]\n\n  \non_move(event) [source]\n\n  \nproperty overlay \n  \nproperty radius \n  \nproperty shape \n  \nupdate_cursor(x, y) [source]\n\n  \nupdate_overlay(x, y) [source]\n\n \n RectangleTool  \nclass skimage.viewer.canvastools.RectangleTool(manager, on_move=None, on_release=None, on_enter=None, maxdist=10, rect_props=None) [source]\n \nBases: skimage.viewer.canvastools.base.CanvasToolBase, matplotlib.widgets.RectangleSelector Widget for selecting a rectangular region in a plot. After making the desired selection, press \u201cEnter\u201d to accept the selection and call the on_enter callback function.  Parameters \n \nmanagerViewer or PlotPlugin. \n\nSkimage viewer or plot plugin object.  \non_movefunction \n\nFunction called whenever a control handle is moved. This function must accept the rectangle extents as the only argument.  \non_releasefunction \n\nFunction called whenever the control handle is released.  \non_enterfunction \n\nFunction called whenever the \u201center\u201d key is pressed.  \nmaxdistfloat \n\nMaximum pixel distance allowed when selecting control handle.  \nrect_propsdict \n\nProperties for matplotlib.patches.Rectangle. This class redefines defaults in matplotlib.widgets.RectangleSelector.     Examples >>> from skimage import data\n>>> from skimage.viewer import ImageViewer\n>>> from skimage.viewer.canvastools import RectangleTool\n>>> from skimage.draw import line\n>>> from skimage.draw import set_color\n >>> viewer = ImageViewer(data.coffee())  \n >>> def print_the_rect(extents):\n...     global viewer\n...     im = viewer.image\n...     coord = np.int64(extents)\n...     [rr1, cc1] = line(coord[2],coord[0],coord[2],coord[1])\n...     [rr2, cc2] = line(coord[2],coord[1],coord[3],coord[1])\n...     [rr3, cc3] = line(coord[3],coord[1],coord[3],coord[0])\n...     [rr4, cc4] = line(coord[3],coord[0],coord[2],coord[0])\n...     set_color(im, (rr1, cc1), [255, 255, 0])\n...     set_color(im, (rr2, cc2), [0, 255, 255])\n...     set_color(im, (rr3, cc3), [255, 0, 255])\n...     set_color(im, (rr4, cc4), [0, 0, 0])\n...     viewer.image=im\n >>> rect_tool = RectangleTool(viewer, on_enter=print_the_rect) \n>>> viewer.show() \n  Attributes \n \nextentstuple \n\nReturn (xmin, xmax, ymin, ymax).      \n__init__(manager, on_move=None, on_release=None, on_enter=None, maxdist=10, rect_props=None) [source]\n \n Parameters \n \naxAxes \n\nThe parent axes for the widget.  \nonselectfunction \n\nA callback function that is called after a selection is completed. It must have the signature: def onselect(eclick: MouseEvent, erelease: MouseEvent)\n where eclick and erelease are the mouse click and release MouseEvents that start and complete the selection.  \ndrawtype{\u201cbox\u201d, \u201cline\u201d, \u201cnone\u201d}, default: \u201cbox\u201d \n\nWhether to draw the full rectangle box, the diagonal line of the rectangle, or nothing at all.  \nminspanxfloat, default: 0 \n\nSelections with an x-span less than minspanx are ignored.  \nminspanyfloat, default: 0 \n\nSelections with an y-span less than minspany are ignored.  \nuseblitbool, default: False \n\nWhether to use blitting for faster drawing (if supported by the backend).  \nlinepropsdict, optional \n\nProperties with which the line is drawn, if drawtype == \"line\". Default: dict(color=\"black\", linestyle=\"-\", linewidth=2, alpha=0.5)\n  \nrectpropsdict, optional \n\nProperties with which the rectangle is drawn, if drawtype ==\n\"box\". Default: dict(facecolor=\"red\", edgecolor=\"black\", alpha=0.2, fill=True)\n  \nspancoords{\u201cdata\u201d, \u201cpixels\u201d}, default: \u201cdata\u201d \n\nWhether to interpret minspanx and minspany in data or in pixel coordinates.  \nbuttonMouseButton, list of MouseButton, default: all buttons \n\nButton(s) that trigger rectangle selection.  \nmaxdistfloat, default: 10 \n\nDistance in pixels within which the interactive tool handles can be activated.  \nmarker_propsdict \n\nProperties with which the interactive handles are drawn. Currently not implemented and ignored.  \ninteractivebool, default: False \n\nWhether to draw a set of handles that allow interaction with the widget after it is drawn.  \nstate_modifier_keysdict, optional \n\nKeyboard modifiers which affect the widget\u2019s behavior. Values amend the defaults.  \u201cmove\u201d: Move the existing shape, default: no modifier. \u201cclear\u201d: Clear the current shape, default: \u201cescape\u201d. \u201csquare\u201d: Makes the shape square, default: \u201cshift\u201d. \u201ccenter\u201d: Make the initial point the center of the shape, default: \u201cctrl\u201d.  \u201csquare\u201d and \u201ccenter\u201d can be combined.     \n  \nproperty corners  \nCorners of rectangle from lower left, moving clockwise. \n  \nproperty edge_centers  \nMidpoint of rectangle edges from left, moving clockwise. \n  \nproperty extents  \nReturn (xmin, xmax, ymin, ymax). \n  \nproperty geometry  \nGeometry information that gets passed to callback functions. \n  \non_mouse_press(event) [source]\n\n  \non_mouse_release(event) [source]\n\n  \non_move(event) [source]\n\n \n ThickLineTool  \nclass skimage.viewer.canvastools.ThickLineTool(manager, on_move=None, on_enter=None, on_release=None, on_change=None, maxdist=10, line_props=None, handle_props=None) [source]\n \nBases: skimage.viewer.canvastools.linetool.LineTool Widget for line selection in a plot. The thickness of the line can be varied using the mouse scroll wheel, or with the \u2018+\u2019 and \u2018-\u2018 keys.  Parameters \n \nmanagerViewer or PlotPlugin. \n\nSkimage viewer or plot plugin object.  \non_movefunction \n\nFunction called whenever a control handle is moved. This function must accept the end points of line as the only argument.  \non_releasefunction \n\nFunction called whenever the control handle is released.  \non_enterfunction \n\nFunction called whenever the \u201center\u201d key is pressed.  \non_changefunction \n\nFunction called whenever the line thickness is changed.  \nmaxdistfloat \n\nMaximum pixel distance allowed when selecting control handle.  \nline_propsdict \n\nProperties for matplotlib.lines.Line2D.  \nhandle_propsdict \n\nMarker properties for the handles (also see matplotlib.lines.Line2D).    Attributes \n \nend_points2D array \n\nEnd points of line ((x1, y1), (x2, y2)).      \n__init__(manager, on_move=None, on_enter=None, on_release=None, on_change=None, maxdist=10, line_props=None, handle_props=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \non_key_press(event) [source]\n\n  \non_scroll(event) [source]\n\n \n\n"}, {"name": "viewer.canvastools.LineTool", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool", "type": "viewer", "text": " \nclass skimage.viewer.canvastools.LineTool(manager, on_move=None, on_release=None, on_enter=None, maxdist=10, line_props=None, handle_props=None, **kwargs) [source]\n \nBases: skimage.viewer.canvastools.base.CanvasToolBase Widget for line selection in a plot.  Parameters \n \nmanagerViewer or PlotPlugin. \n\nSkimage viewer or plot plugin object.  \non_movefunction \n\nFunction called whenever a control handle is moved. This function must accept the end points of line as the only argument.  \non_releasefunction \n\nFunction called whenever the control handle is released.  \non_enterfunction \n\nFunction called whenever the \u201center\u201d key is pressed.  \nmaxdistfloat \n\nMaximum pixel distance allowed when selecting control handle.  \nline_propsdict \n\nProperties for matplotlib.lines.Line2D.  \nhandle_propsdict \n\nMarker properties for the handles (also see matplotlib.lines.Line2D).    Attributes \n \nend_points2D array \n\nEnd points of line ((x1, y1), (x2, y2)).      \n__init__(manager, on_move=None, on_release=None, on_enter=None, maxdist=10, line_props=None, handle_props=None, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nproperty end_points \n  \nproperty geometry  \nGeometry information that gets passed to callback functions. \n  \nhit_test(event) [source]\n\n  \non_mouse_press(event) [source]\n\n  \non_mouse_release(event) [source]\n\n  \non_move(event) [source]\n\n  \nupdate(x=None, y=None) [source]\n\n \n"}, {"name": "viewer.canvastools.LineTool.end_points()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.end_points", "type": "viewer", "text": " \nproperty end_points \n"}, {"name": "viewer.canvastools.LineTool.geometry()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.geometry", "type": "viewer", "text": " \nproperty geometry  \nGeometry information that gets passed to callback functions. \n"}, {"name": "viewer.canvastools.LineTool.hit_test()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.hit_test", "type": "viewer", "text": " \nhit_test(event) [source]\n\n"}, {"name": "viewer.canvastools.LineTool.on_mouse_press()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.on_mouse_press", "type": "viewer", "text": " \non_mouse_press(event) [source]\n\n"}, {"name": "viewer.canvastools.LineTool.on_mouse_release()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.on_mouse_release", "type": "viewer", "text": " \non_mouse_release(event) [source]\n\n"}, {"name": "viewer.canvastools.LineTool.on_move()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.on_move", "type": "viewer", "text": " \non_move(event) [source]\n\n"}, {"name": "viewer.canvastools.LineTool.update()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.update", "type": "viewer", "text": " \nupdate(x=None, y=None) [source]\n\n"}, {"name": "viewer.canvastools.LineTool.__init__()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.__init__", "type": "viewer", "text": " \n__init__(manager, on_move=None, on_release=None, on_enter=None, maxdist=10, line_props=None, handle_props=None, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.canvastools.PaintTool", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool", "type": "viewer", "text": " \nclass skimage.viewer.canvastools.PaintTool(manager, overlay_shape, radius=5, alpha=0.3, on_move=None, on_release=None, on_enter=None, rect_props=None) [source]\n \nBases: skimage.viewer.canvastools.base.CanvasToolBase Widget for painting on top of a plot.  Parameters \n \nmanagerViewer or PlotPlugin. \n\nSkimage viewer or plot plugin object.  \noverlay_shapeshape tuple \n\n2D shape tuple used to initialize overlay image.  \nradiusint \n\nThe size of the paint cursor.  \nalphafloat (between [0, 1]) \n\nOpacity of overlay.  \non_movefunction \n\nFunction called whenever a control handle is moved. This function must accept the end points of line as the only argument.  \non_releasefunction \n\nFunction called whenever the control handle is released.  \non_enterfunction \n\nFunction called whenever the \u201center\u201d key is pressed.  \nrect_propsdict \n\nProperties for matplotlib.patches.Rectangle. This class redefines defaults in matplotlib.widgets.RectangleSelector.     Examples >>> from skimage.data import camera\n>>> import matplotlib.pyplot as plt\n>>> from skimage.viewer.canvastools import PaintTool\n>>> import numpy as np\n >>> img = camera() \n >>> ax = plt.subplot(111) \n>>> plt.imshow(img, cmap=plt.cm.gray) \n>>> p = PaintTool(ax,np.shape(img[:-1]),10,0.2) \n>>> plt.show() \n >>> mask = p.overlay \n>>> plt.imshow(mask,cmap=plt.cm.gray) \n>>> plt.show() \n  Attributes \n \noverlayarray \n\nOverlay of painted labels displayed on top of image.  \nlabelint \n\nCurrent paint color.      \n__init__(manager, overlay_shape, radius=5, alpha=0.3, on_move=None, on_release=None, on_enter=None, rect_props=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nproperty geometry  \nGeometry information that gets passed to callback functions. \n  \nproperty label \n  \non_key_press(event) [source]\n\n  \non_mouse_press(event) [source]\n\n  \non_mouse_release(event) [source]\n\n  \non_move(event) [source]\n\n  \nproperty overlay \n  \nproperty radius \n  \nproperty shape \n  \nupdate_cursor(x, y) [source]\n\n  \nupdate_overlay(x, y) [source]\n\n \n"}, {"name": "viewer.canvastools.PaintTool.geometry()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.geometry", "type": "viewer", "text": " \nproperty geometry  \nGeometry information that gets passed to callback functions. \n"}, {"name": "viewer.canvastools.PaintTool.label()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.label", "type": "viewer", "text": " \nproperty label \n"}, {"name": "viewer.canvastools.PaintTool.on_key_press()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.on_key_press", "type": "viewer", "text": " \non_key_press(event) [source]\n\n"}, {"name": "viewer.canvastools.PaintTool.on_mouse_press()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.on_mouse_press", "type": "viewer", "text": " \non_mouse_press(event) [source]\n\n"}, {"name": "viewer.canvastools.PaintTool.on_mouse_release()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.on_mouse_release", "type": "viewer", "text": " \non_mouse_release(event) [source]\n\n"}, {"name": "viewer.canvastools.PaintTool.on_move()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.on_move", "type": "viewer", "text": " \non_move(event) [source]\n\n"}, {"name": "viewer.canvastools.PaintTool.overlay()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.overlay", "type": "viewer", "text": " \nproperty overlay \n"}, {"name": "viewer.canvastools.PaintTool.radius()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.radius", "type": "viewer", "text": " \nproperty radius \n"}, {"name": "viewer.canvastools.PaintTool.shape()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.shape", "type": "viewer", "text": " \nproperty shape \n"}, {"name": "viewer.canvastools.PaintTool.update_cursor()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.update_cursor", "type": "viewer", "text": " \nupdate_cursor(x, y) [source]\n\n"}, {"name": "viewer.canvastools.PaintTool.update_overlay()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.update_overlay", "type": "viewer", "text": " \nupdate_overlay(x, y) [source]\n\n"}, {"name": "viewer.canvastools.PaintTool.__init__()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.__init__", "type": "viewer", "text": " \n__init__(manager, overlay_shape, radius=5, alpha=0.3, on_move=None, on_release=None, on_enter=None, rect_props=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.canvastools.RectangleTool", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool", "type": "viewer", "text": " \nclass skimage.viewer.canvastools.RectangleTool(manager, on_move=None, on_release=None, on_enter=None, maxdist=10, rect_props=None) [source]\n \nBases: skimage.viewer.canvastools.base.CanvasToolBase, matplotlib.widgets.RectangleSelector Widget for selecting a rectangular region in a plot. After making the desired selection, press \u201cEnter\u201d to accept the selection and call the on_enter callback function.  Parameters \n \nmanagerViewer or PlotPlugin. \n\nSkimage viewer or plot plugin object.  \non_movefunction \n\nFunction called whenever a control handle is moved. This function must accept the rectangle extents as the only argument.  \non_releasefunction \n\nFunction called whenever the control handle is released.  \non_enterfunction \n\nFunction called whenever the \u201center\u201d key is pressed.  \nmaxdistfloat \n\nMaximum pixel distance allowed when selecting control handle.  \nrect_propsdict \n\nProperties for matplotlib.patches.Rectangle. This class redefines defaults in matplotlib.widgets.RectangleSelector.     Examples >>> from skimage import data\n>>> from skimage.viewer import ImageViewer\n>>> from skimage.viewer.canvastools import RectangleTool\n>>> from skimage.draw import line\n>>> from skimage.draw import set_color\n >>> viewer = ImageViewer(data.coffee())  \n >>> def print_the_rect(extents):\n...     global viewer\n...     im = viewer.image\n...     coord = np.int64(extents)\n...     [rr1, cc1] = line(coord[2],coord[0],coord[2],coord[1])\n...     [rr2, cc2] = line(coord[2],coord[1],coord[3],coord[1])\n...     [rr3, cc3] = line(coord[3],coord[1],coord[3],coord[0])\n...     [rr4, cc4] = line(coord[3],coord[0],coord[2],coord[0])\n...     set_color(im, (rr1, cc1), [255, 255, 0])\n...     set_color(im, (rr2, cc2), [0, 255, 255])\n...     set_color(im, (rr3, cc3), [255, 0, 255])\n...     set_color(im, (rr4, cc4), [0, 0, 0])\n...     viewer.image=im\n >>> rect_tool = RectangleTool(viewer, on_enter=print_the_rect) \n>>> viewer.show() \n  Attributes \n \nextentstuple \n\nReturn (xmin, xmax, ymin, ymax).      \n__init__(manager, on_move=None, on_release=None, on_enter=None, maxdist=10, rect_props=None) [source]\n \n Parameters \n \naxAxes \n\nThe parent axes for the widget.  \nonselectfunction \n\nA callback function that is called after a selection is completed. It must have the signature: def onselect(eclick: MouseEvent, erelease: MouseEvent)\n where eclick and erelease are the mouse click and release MouseEvents that start and complete the selection.  \ndrawtype{\u201cbox\u201d, \u201cline\u201d, \u201cnone\u201d}, default: \u201cbox\u201d \n\nWhether to draw the full rectangle box, the diagonal line of the rectangle, or nothing at all.  \nminspanxfloat, default: 0 \n\nSelections with an x-span less than minspanx are ignored.  \nminspanyfloat, default: 0 \n\nSelections with an y-span less than minspany are ignored.  \nuseblitbool, default: False \n\nWhether to use blitting for faster drawing (if supported by the backend).  \nlinepropsdict, optional \n\nProperties with which the line is drawn, if drawtype == \"line\". Default: dict(color=\"black\", linestyle=\"-\", linewidth=2, alpha=0.5)\n  \nrectpropsdict, optional \n\nProperties with which the rectangle is drawn, if drawtype ==\n\"box\". Default: dict(facecolor=\"red\", edgecolor=\"black\", alpha=0.2, fill=True)\n  \nspancoords{\u201cdata\u201d, \u201cpixels\u201d}, default: \u201cdata\u201d \n\nWhether to interpret minspanx and minspany in data or in pixel coordinates.  \nbuttonMouseButton, list of MouseButton, default: all buttons \n\nButton(s) that trigger rectangle selection.  \nmaxdistfloat, default: 10 \n\nDistance in pixels within which the interactive tool handles can be activated.  \nmarker_propsdict \n\nProperties with which the interactive handles are drawn. Currently not implemented and ignored.  \ninteractivebool, default: False \n\nWhether to draw a set of handles that allow interaction with the widget after it is drawn.  \nstate_modifier_keysdict, optional \n\nKeyboard modifiers which affect the widget\u2019s behavior. Values amend the defaults.  \u201cmove\u201d: Move the existing shape, default: no modifier. \u201cclear\u201d: Clear the current shape, default: \u201cescape\u201d. \u201csquare\u201d: Makes the shape square, default: \u201cshift\u201d. \u201ccenter\u201d: Make the initial point the center of the shape, default: \u201cctrl\u201d.  \u201csquare\u201d and \u201ccenter\u201d can be combined.     \n  \nproperty corners  \nCorners of rectangle from lower left, moving clockwise. \n  \nproperty edge_centers  \nMidpoint of rectangle edges from left, moving clockwise. \n  \nproperty extents  \nReturn (xmin, xmax, ymin, ymax). \n  \nproperty geometry  \nGeometry information that gets passed to callback functions. \n  \non_mouse_press(event) [source]\n\n  \non_mouse_release(event) [source]\n\n  \non_move(event) [source]\n\n \n"}, {"name": "viewer.canvastools.RectangleTool.corners()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.corners", "type": "viewer", "text": " \nproperty corners  \nCorners of rectangle from lower left, moving clockwise. \n"}, {"name": "viewer.canvastools.RectangleTool.edge_centers()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.edge_centers", "type": "viewer", "text": " \nproperty edge_centers  \nMidpoint of rectangle edges from left, moving clockwise. \n"}, {"name": "viewer.canvastools.RectangleTool.extents()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.extents", "type": "viewer", "text": " \nproperty extents  \nReturn (xmin, xmax, ymin, ymax). \n"}, {"name": "viewer.canvastools.RectangleTool.geometry()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.geometry", "type": "viewer", "text": " \nproperty geometry  \nGeometry information that gets passed to callback functions. \n"}, {"name": "viewer.canvastools.RectangleTool.on_mouse_press()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.on_mouse_press", "type": "viewer", "text": " \non_mouse_press(event) [source]\n\n"}, {"name": "viewer.canvastools.RectangleTool.on_mouse_release()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.on_mouse_release", "type": "viewer", "text": " \non_mouse_release(event) [source]\n\n"}, {"name": "viewer.canvastools.RectangleTool.on_move()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.on_move", "type": "viewer", "text": " \non_move(event) [source]\n\n"}, {"name": "viewer.canvastools.RectangleTool.__init__()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.__init__", "type": "viewer", "text": " \n__init__(manager, on_move=None, on_release=None, on_enter=None, maxdist=10, rect_props=None) [source]\n \n Parameters \n \naxAxes \n\nThe parent axes for the widget.  \nonselectfunction \n\nA callback function that is called after a selection is completed. It must have the signature: def onselect(eclick: MouseEvent, erelease: MouseEvent)\n where eclick and erelease are the mouse click and release MouseEvents that start and complete the selection.  \ndrawtype{\u201cbox\u201d, \u201cline\u201d, \u201cnone\u201d}, default: \u201cbox\u201d \n\nWhether to draw the full rectangle box, the diagonal line of the rectangle, or nothing at all.  \nminspanxfloat, default: 0 \n\nSelections with an x-span less than minspanx are ignored.  \nminspanyfloat, default: 0 \n\nSelections with an y-span less than minspany are ignored.  \nuseblitbool, default: False \n\nWhether to use blitting for faster drawing (if supported by the backend).  \nlinepropsdict, optional \n\nProperties with which the line is drawn, if drawtype == \"line\". Default: dict(color=\"black\", linestyle=\"-\", linewidth=2, alpha=0.5)\n  \nrectpropsdict, optional \n\nProperties with which the rectangle is drawn, if drawtype ==\n\"box\". Default: dict(facecolor=\"red\", edgecolor=\"black\", alpha=0.2, fill=True)\n  \nspancoords{\u201cdata\u201d, \u201cpixels\u201d}, default: \u201cdata\u201d \n\nWhether to interpret minspanx and minspany in data or in pixel coordinates.  \nbuttonMouseButton, list of MouseButton, default: all buttons \n\nButton(s) that trigger rectangle selection.  \nmaxdistfloat, default: 10 \n\nDistance in pixels within which the interactive tool handles can be activated.  \nmarker_propsdict \n\nProperties with which the interactive handles are drawn. Currently not implemented and ignored.  \ninteractivebool, default: False \n\nWhether to draw a set of handles that allow interaction with the widget after it is drawn.  \nstate_modifier_keysdict, optional \n\nKeyboard modifiers which affect the widget\u2019s behavior. Values amend the defaults.  \u201cmove\u201d: Move the existing shape, default: no modifier. \u201cclear\u201d: Clear the current shape, default: \u201cescape\u201d. \u201csquare\u201d: Makes the shape square, default: \u201cshift\u201d. \u201ccenter\u201d: Make the initial point the center of the shape, default: \u201cctrl\u201d.  \u201csquare\u201d and \u201ccenter\u201d can be combined.     \n"}, {"name": "viewer.canvastools.ThickLineTool", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.ThickLineTool", "type": "viewer", "text": " \nclass skimage.viewer.canvastools.ThickLineTool(manager, on_move=None, on_enter=None, on_release=None, on_change=None, maxdist=10, line_props=None, handle_props=None) [source]\n \nBases: skimage.viewer.canvastools.linetool.LineTool Widget for line selection in a plot. The thickness of the line can be varied using the mouse scroll wheel, or with the \u2018+\u2019 and \u2018-\u2018 keys.  Parameters \n \nmanagerViewer or PlotPlugin. \n\nSkimage viewer or plot plugin object.  \non_movefunction \n\nFunction called whenever a control handle is moved. This function must accept the end points of line as the only argument.  \non_releasefunction \n\nFunction called whenever the control handle is released.  \non_enterfunction \n\nFunction called whenever the \u201center\u201d key is pressed.  \non_changefunction \n\nFunction called whenever the line thickness is changed.  \nmaxdistfloat \n\nMaximum pixel distance allowed when selecting control handle.  \nline_propsdict \n\nProperties for matplotlib.lines.Line2D.  \nhandle_propsdict \n\nMarker properties for the handles (also see matplotlib.lines.Line2D).    Attributes \n \nend_points2D array \n\nEnd points of line ((x1, y1), (x2, y2)).      \n__init__(manager, on_move=None, on_enter=None, on_release=None, on_change=None, maxdist=10, line_props=None, handle_props=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \non_key_press(event) [source]\n\n  \non_scroll(event) [source]\n\n \n"}, {"name": "viewer.canvastools.ThickLineTool.on_key_press()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.ThickLineTool.on_key_press", "type": "viewer", "text": " \non_key_press(event) [source]\n\n"}, {"name": "viewer.canvastools.ThickLineTool.on_scroll()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.ThickLineTool.on_scroll", "type": "viewer", "text": " \non_scroll(event) [source]\n\n"}, {"name": "viewer.canvastools.ThickLineTool.__init__()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.ThickLineTool.__init__", "type": "viewer", "text": " \n__init__(manager, on_move=None, on_enter=None, on_release=None, on_change=None, maxdist=10, line_props=None, handle_props=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.CollectionViewer", "path": "api/skimage.viewer#skimage.viewer.CollectionViewer", "type": "viewer", "text": " \nclass skimage.viewer.CollectionViewer(image_collection, update_on='move', **kwargs) [source]\n \nBases: skimage.viewer.viewers.core.ImageViewer Viewer for displaying image collections. Select the displayed frame of the image collection using the slider or with the following keyboard shortcuts:  left/right arrows\n\nPrevious/next image in collection.  number keys, 0\u20139\n\n0% to 90% of collection. For example, \u201c5\u201d goes to the image in the middle (i.e. 50%) of the collection.  home/end keys\n\nFirst/last image in collection.    Parameters \n \nimage_collectionlist of images \n\nList of images to be displayed.  \nupdate_on{\u2018move\u2019 | \u2018release\u2019} \n\nControl whether image is updated on slide or release of the image slider. Using \u2018on_release\u2019 will give smoother behavior when displaying large images or when writing a plugin/subclass that requires heavy computation.      \n__init__(image_collection, update_on='move', **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nkeyPressEvent(event) [source]\n\n  \nupdate_index(name, index) [source]\n \nSelect image on display using index into image collection. \n \n"}, {"name": "viewer.CollectionViewer.keyPressEvent()", "path": "api/skimage.viewer#skimage.viewer.CollectionViewer.keyPressEvent", "type": "viewer", "text": " \nkeyPressEvent(event) [source]\n\n"}, {"name": "viewer.CollectionViewer.update_index()", "path": "api/skimage.viewer#skimage.viewer.CollectionViewer.update_index", "type": "viewer", "text": " \nupdate_index(name, index) [source]\n \nSelect image on display using index into image collection. \n"}, {"name": "viewer.CollectionViewer.__init__()", "path": "api/skimage.viewer#skimage.viewer.CollectionViewer.__init__", "type": "viewer", "text": " \n__init__(image_collection, update_on='move', **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.ImageViewer", "path": "api/skimage.viewer#skimage.viewer.ImageViewer", "type": "viewer", "text": " \nclass skimage.viewer.ImageViewer(image, useblit=True) [source]\n \nBases: object Viewer for displaying images. This viewer is a simple container object that holds a Matplotlib axes for showing images. ImageViewer doesn\u2019t subclass the Matplotlib axes (or figure) because of the high probability of name collisions. Subclasses and plugins will likely extend the update_image method to add custom overlays or filter the displayed image.  Parameters \n \nimagearray \n\nImage being viewed.     Examples >>> from skimage import data\n>>> image = data.coins()\n>>> viewer = ImageViewer(image) \n>>> viewer.show()               \n  Attributes \n \ncanvas, fig, axMatplotlib canvas, figure, and axes \n\nMatplotlib canvas, figure, and axes used to display image.  \nimagearray \n\nImage being viewed. Setting this value will update the displayed frame.  \noriginal_imagearray \n\nPlugins typically operate on (but don\u2019t change) the original image.  \npluginslist \n\nList of attached plugins.      \n__init__(image, useblit=True) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nadd_tool(tool) [source]\n\n  \ncloseEvent(event) [source]\n\n  \nconnect_event(event, callback) [source]\n \nConnect callback function to matplotlib event and return id. \n  \ndisconnect_event(callback_id) [source]\n \nDisconnect callback by its id (returned by connect_event). \n  \ndock_areas = {'bottom': None, 'left': None, 'right': None, 'top': None} \n  \nproperty image \n  \nopen_file(filename=None) [source]\n \nOpen image file and display in viewer. \n  \noriginal_image_changed = None \n  \nredraw() [source]\n\n  \nremove_tool(tool) [source]\n\n  \nreset_image() [source]\n\n  \nsave_to_file(filename=None) [source]\n \nSave current image to file. The current behavior is not ideal: It saves the image displayed on screen, so all images will be converted to RGB, and the image size is not preserved (resizing the viewer window will alter the size of the saved image). \n  \nshow(main_window=True) [source]\n \nShow ImageViewer and attached plugins. This behaves much like matplotlib.pyplot.show and QWidget.show. \n  \nupdate_image(image) [source]\n \nUpdate displayed image. This method can be overridden or extended in subclasses and plugins to react to image changes. \n \n"}, {"name": "viewer.ImageViewer.add_tool()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.add_tool", "type": "viewer", "text": " \nadd_tool(tool) [source]\n\n"}, {"name": "viewer.ImageViewer.closeEvent()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.closeEvent", "type": "viewer", "text": " \ncloseEvent(event) [source]\n\n"}, {"name": "viewer.ImageViewer.connect_event()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.connect_event", "type": "viewer", "text": " \nconnect_event(event, callback) [source]\n \nConnect callback function to matplotlib event and return id. \n"}, {"name": "viewer.ImageViewer.disconnect_event()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.disconnect_event", "type": "viewer", "text": " \ndisconnect_event(callback_id) [source]\n \nDisconnect callback by its id (returned by connect_event). \n"}, {"name": "viewer.ImageViewer.dock_areas", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.dock_areas", "type": "viewer", "text": " \ndock_areas = {'bottom': None, 'left': None, 'right': None, 'top': None} \n"}, {"name": "viewer.ImageViewer.image()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.image", "type": "viewer", "text": " \nproperty image \n"}, {"name": "viewer.ImageViewer.open_file()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.open_file", "type": "viewer", "text": " \nopen_file(filename=None) [source]\n \nOpen image file and display in viewer. \n"}, {"name": "viewer.ImageViewer.original_image_changed", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.original_image_changed", "type": "viewer", "text": " \noriginal_image_changed = None \n"}, {"name": "viewer.ImageViewer.redraw()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.redraw", "type": "viewer", "text": " \nredraw() [source]\n\n"}, {"name": "viewer.ImageViewer.remove_tool()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.remove_tool", "type": "viewer", "text": " \nremove_tool(tool) [source]\n\n"}, {"name": "viewer.ImageViewer.reset_image()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.reset_image", "type": "viewer", "text": " \nreset_image() [source]\n\n"}, {"name": "viewer.ImageViewer.save_to_file()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.save_to_file", "type": "viewer", "text": " \nsave_to_file(filename=None) [source]\n \nSave current image to file. The current behavior is not ideal: It saves the image displayed on screen, so all images will be converted to RGB, and the image size is not preserved (resizing the viewer window will alter the size of the saved image). \n"}, {"name": "viewer.ImageViewer.show()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.show", "type": "viewer", "text": " \nshow(main_window=True) [source]\n \nShow ImageViewer and attached plugins. This behaves much like matplotlib.pyplot.show and QWidget.show. \n"}, {"name": "viewer.ImageViewer.update_image()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.update_image", "type": "viewer", "text": " \nupdate_image(image) [source]\n \nUpdate displayed image. This method can be overridden or extended in subclasses and plugins to react to image changes. \n"}, {"name": "viewer.ImageViewer.__init__()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.__init__", "type": "viewer", "text": " \n__init__(image, useblit=True) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.plugins", "path": "api/skimage.viewer.plugins", "type": "viewer", "text": "Module: viewer.plugins  \nskimage.viewer.plugins.CannyPlugin(*args, \u2026) Canny filter plugin to show edges of an image.  \nskimage.viewer.plugins.ColorHistogram([max_pct])   \nskimage.viewer.plugins.Crop([maxdist])   \nskimage.viewer.plugins.LabelPainter([max_radius])   \nskimage.viewer.plugins.LineProfile([\u2026]) Plugin to compute interpolated intensity under a scan line on an image.  \nskimage.viewer.plugins.Measure([maxdist])   \nskimage.viewer.plugins.OverlayPlugin(**kwargs) Plugin for ImageViewer that displays an overlay on top of main image.  \nskimage.viewer.plugins.PlotPlugin([\u2026]) Plugin for ImageViewer that contains a plot canvas.  \nskimage.viewer.plugins.Plugin([\u2026]) Base class for plugins that interact with an ImageViewer.  \nskimage.viewer.plugins.base Base class for Plugins that interact with ImageViewer.  \nskimage.viewer.plugins.canny   \nskimage.viewer.plugins.color_histogram   \nskimage.viewer.plugins.crop   \nskimage.viewer.plugins.labelplugin   \nskimage.viewer.plugins.lineprofile   \nskimage.viewer.plugins.measure   \nskimage.viewer.plugins.overlayplugin   \nskimage.viewer.plugins.plotplugin    CannyPlugin  \nclass skimage.viewer.plugins.CannyPlugin(*args, **kwargs) [source]\n \nBases: skimage.viewer.plugins.overlayplugin.OverlayPlugin Canny filter plugin to show edges of an image.  \n__init__(*args, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n  \nname = 'Canny Filter' \n \n ColorHistogram  \nclass skimage.viewer.plugins.ColorHistogram(max_pct=0.99, **kwargs) [source]\n \nBases: skimage.viewer.plugins.plotplugin.PlotPlugin  \n__init__(max_pct=0.99, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nab_selected(extents) [source]\n\n  \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n  \nhelp() [source]\n\n  \nname = 'Color Histogram' \n  \noutput() [source]\n \nReturn the image mask and the histogram data.  Returns \n \nmaskarray of bool, same shape as image \n\nThe selected pixels.  \ndatadict \n\nThe data describing the histogram and the selected region. The dictionary contains:  \u2018bins\u2019 : array of float The bin boundaries for both a and b channels. \u2018hist\u2019 : 2D array of float The normalized histogram. \u2018edges\u2019 : tuple of array of float The bin edges along each dimension \u2018extents\u2019 : tuple of float The left and right and top and bottom of the selected region.      \n \n Crop  \nclass skimage.viewer.plugins.Crop(maxdist=10, **kwargs) [source]\n \nBases: skimage.viewer.plugins.base.Plugin  \n__init__(maxdist=10, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n  \ncrop(extents) [source]\n\n  \nhelp() [source]\n\n  \nname = 'Crop' \n  \nreset() [source]\n\n \n LabelPainter  \nclass skimage.viewer.plugins.LabelPainter(max_radius=20, **kwargs) [source]\n \nBases: skimage.viewer.plugins.base.Plugin  \n__init__(max_radius=20, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n  \nhelp() [source]\n\n  \nproperty label \n  \nname = 'LabelPainter' \n  \non_enter(overlay) [source]\n\n  \nproperty radius \n \n LineProfile  \nclass skimage.viewer.plugins.LineProfile(maxdist=10, epsilon='deprecated', limits='image', **kwargs) [source]\n \nBases: skimage.viewer.plugins.plotplugin.PlotPlugin Plugin to compute interpolated intensity under a scan line on an image. See PlotPlugin and Plugin classes for additional details.  Parameters \n \nmaxdistfloat \n\nMaximum pixel distance allowed when selecting end point of scan line.  \nlimitstuple or {None, \u2018image\u2019, \u2018dtype\u2019} \n\n(minimum, maximum) intensity limits for plotted profile. The following special values are defined: None : rescale based on min/max intensity along selected scan line. \u2018image\u2019 : fixed scale based on min/max intensity in image. \u2018dtype\u2019 : fixed scale based on min/max intensity of image dtype.      \n__init__(maxdist=10, epsilon='deprecated', limits='image', **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n  \nget_profiles() [source]\n \nReturn intensity profile of the selected line.  Returns \n end_points: (2, 2) array\n\nThe positions ((x1, y1), (x2, y2)) of the line ends.  profile: list of 1d arrays\n\nProfile of intensity values. Length 1 (grayscale) or 3 (rgb).     \n  \nhelp() [source]\n\n  \nline_changed(end_points) [source]\n\n  \nname = 'Line Profile' \n  \noutput() [source]\n \nReturn the drawn line and the resulting scan.  Returns \n \nline_image(M, N) uint8 array, same shape as image \n\nAn array of 0s with the scanned line set to 255. If the linewidth of the line tool is greater than 1, sets the values within the profiled polygon to 128.  \nscan(P,) or (P, 3) array of int or float \n\nThe line scan values across the image.     \n  \nreset_axes(scan_data) [source]\n\n \n Measure  \nclass skimage.viewer.plugins.Measure(maxdist=10, **kwargs) [source]\n \nBases: skimage.viewer.plugins.base.Plugin  \n__init__(maxdist=10, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n  \nhelp() [source]\n\n  \nline_changed(end_points) [source]\n\n  \nname = 'Measure' \n \n OverlayPlugin  \nclass skimage.viewer.plugins.OverlayPlugin(**kwargs) [source]\n \nBases: skimage.viewer.plugins.base.Plugin Plugin for ImageViewer that displays an overlay on top of main image. The base Plugin class displays the filtered image directly on the viewer. OverlayPlugin will instead overlay an image with a transparent colormap. See base Plugin class for additional details.  Attributes \n \noverlayarray \n\nOverlay displayed on top of image. This overlay defaults to a color map with alpha values varying linearly from 0 to 1.  \ncolorint \n\nColor of overlay.      \n__init__(**kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n  \ncloseEvent(event) [source]\n \nOn close disconnect all artists and events from ImageViewer. Note that artists must be appended to self.artists. \n  \nproperty color \n  \ncolors = {'cyan': (0, 1, 1), 'green': (0, 1, 0), 'red': (1, 0, 0), 'yellow': (1, 1, 0)} \n  \ndisplay_filtered_image(image) [source]\n \nDisplay filtered image as an overlay on top of image in viewer. \n  \nproperty filtered_image  \nReturn filtered image. This \u201cfiltered image\u201d is used when saving from the plugin. \n  \noutput() [source]\n \nReturn the overlaid image.  Returns \n \noverlayarray, same shape as image \n\nThe overlay currently displayed.  \ndataNone \n   \n  \nproperty overlay \n \n PlotPlugin  \nclass skimage.viewer.plugins.PlotPlugin(image_filter=None, height=150, width=400, **kwargs) [source]\n \nBases: skimage.viewer.plugins.base.Plugin Plugin for ImageViewer that contains a plot canvas. Base class for plugins that contain a Matplotlib plot canvas, which can, for example, display an image histogram. See base Plugin class for additional details.  \n__init__(image_filter=None, height=150, width=400, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nadd_plot() [source]\n\n  \nadd_tool(tool) [source]\n\n  \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n  \nredraw() [source]\n \nRedraw plot. \n  \nremove_tool(tool) [source]\n\n \n Plugin  \nclass skimage.viewer.plugins.Plugin(image_filter=None, height=0, width=400, useblit=True, dock='bottom') [source]\n \nBases: object Base class for plugins that interact with an ImageViewer. A plugin connects an image filter (or another function) to an image viewer. Note that a Plugin is initialized without an image viewer and attached in a later step. See example below for details.  Parameters \n \nimage_viewerImageViewer \n\nWindow containing image used in measurement/manipulation.  \nimage_filterfunction \n\nFunction that gets called to update image in image viewer. This value can be None if, for example, you have a plugin that extracts information from an image and doesn\u2019t manipulate it. Alternatively, this function can be defined as a method in a Plugin subclass.  \nheight, widthint \n\nSize of plugin window in pixels. Note that Qt will automatically resize a window to fit components. So if you\u2019re adding rows of components, you can leave height = 0 and just let Qt determine the final height.  \nuseblitbool \n\nIf True, use blitting to speed up animation. Only available on some Matplotlib backends. If None, set to True when using Agg backend. This only has an effect if you draw on top of an image viewer.     Examples >>> from skimage.viewer import ImageViewer\n>>> from skimage.viewer.widgets import Slider\n>>> from skimage import data\n>>>\n>>> plugin = Plugin(image_filter=lambda img,\n...                 threshold: img > threshold) \n>>> plugin += Slider('threshold', 0, 255)       \n>>>\n>>> image = data.coins()\n>>> viewer = ImageViewer(image)       \n>>> viewer += plugin                  \n>>> thresholded = viewer.show()[0][0] \n The plugin will automatically delegate parameters to image_filter based on its parameter type, i.e., ptype (widgets for required arguments must be added in the order they appear in the function). The image attached to the viewer is automatically passed as the first argument to the filter function. #TODO: Add flag so image is not passed to filter function by default. ptype = \u2018kwarg\u2019 is the default for most widgets so it\u2019s unnecessary here.  Attributes \n \nimage_viewerImageViewer \n\nWindow containing image used in measurement.  \nnamestr \n\nName of plugin. This is displayed as the window title.  \nartistlist \n\nList of Matplotlib artists and canvastools. Any artists created by the plugin should be added to this list so that it gets cleaned up on close.      \n__init__(image_filter=None, height=0, width=400, useblit=True, dock='bottom') [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nadd_widget(widget) [source]\n \nAdd widget to plugin. Alternatively, Plugin\u2019s __add__ method is overloaded to add widgets: plugin += Widget(...)\n Widgets can adjust required or optional arguments of filter function or parameters for the plugin. This is specified by the Widget\u2019s ptype. \n  \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n  \nclean_up() [source]\n\n  \ncloseEvent(event) [source]\n \nOn close disconnect all artists and events from ImageViewer. Note that artists must be appended to self.artists. \n  \ndisplay_filtered_image(image) [source]\n \nDisplay the filtered image on image viewer. If you don\u2019t want to simply replace the displayed image with the filtered image (e.g., you want to display a transparent overlay), you can override this method. \n  \nfilter_image(*widget_arg) [source]\n \nCall image_filter with widget args and kwargs Note: display_filtered_image is automatically called. \n  \nproperty filtered_image  \nReturn filtered image. \n  \nimage_changed = None \n  \nimage_viewer = 'Plugin is not attached to ImageViewer' \n  \nname = 'Plugin' \n  \noutput() [source]\n \nReturn the plugin\u2019s representation and data.  Returns \n \nimagearray, same shape as self.image_viewer.image, or None \n\nThe filtered image.  \ndataNone \n\nAny data associated with the plugin.     Notes Derived classes should override this method to return a tuple containing an overlay of the same shape of the image, and a data object. Either of these is optional: return None if you don\u2019t want to return a value. \n  \nremove_image_artists() [source]\n \nRemove artists that are connected to the image viewer. \n  \nshow(main_window=True) [source]\n \nShow plugin. \n  \nupdate_plugin(name, value) [source]\n \nUpdate keyword parameters of the plugin itself. These parameters will typically be implemented as class properties so that they update the image or some other component. \n \n\n"}, {"name": "viewer.plugins.CannyPlugin", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.CannyPlugin", "type": "viewer", "text": " \nclass skimage.viewer.plugins.CannyPlugin(*args, **kwargs) [source]\n \nBases: skimage.viewer.plugins.overlayplugin.OverlayPlugin Canny filter plugin to show edges of an image.  \n__init__(*args, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n  \nname = 'Canny Filter' \n \n"}, {"name": "viewer.plugins.CannyPlugin.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.CannyPlugin.attach", "type": "viewer", "text": " \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n"}, {"name": "viewer.plugins.CannyPlugin.name", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.CannyPlugin.name", "type": "viewer", "text": " \nname = 'Canny Filter' \n"}, {"name": "viewer.plugins.CannyPlugin.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.CannyPlugin.__init__", "type": "viewer", "text": " \n__init__(*args, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.plugins.ColorHistogram", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.ColorHistogram", "type": "viewer", "text": " \nclass skimage.viewer.plugins.ColorHistogram(max_pct=0.99, **kwargs) [source]\n \nBases: skimage.viewer.plugins.plotplugin.PlotPlugin  \n__init__(max_pct=0.99, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nab_selected(extents) [source]\n\n  \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n  \nhelp() [source]\n\n  \nname = 'Color Histogram' \n  \noutput() [source]\n \nReturn the image mask and the histogram data.  Returns \n \nmaskarray of bool, same shape as image \n\nThe selected pixels.  \ndatadict \n\nThe data describing the histogram and the selected region. The dictionary contains:  \u2018bins\u2019 : array of float The bin boundaries for both a and b channels. \u2018hist\u2019 : 2D array of float The normalized histogram. \u2018edges\u2019 : tuple of array of float The bin edges along each dimension \u2018extents\u2019 : tuple of float The left and right and top and bottom of the selected region.      \n \n"}, {"name": "viewer.plugins.ColorHistogram.ab_selected()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.ColorHistogram.ab_selected", "type": "viewer", "text": " \nab_selected(extents) [source]\n\n"}, {"name": "viewer.plugins.ColorHistogram.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.ColorHistogram.attach", "type": "viewer", "text": " \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n"}, {"name": "viewer.plugins.ColorHistogram.help()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.ColorHistogram.help", "type": "viewer", "text": " \nhelp() [source]\n\n"}, {"name": "viewer.plugins.ColorHistogram.name", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.ColorHistogram.name", "type": "viewer", "text": " \nname = 'Color Histogram' \n"}, {"name": "viewer.plugins.ColorHistogram.output()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.ColorHistogram.output", "type": "viewer", "text": " \noutput() [source]\n \nReturn the image mask and the histogram data.  Returns \n \nmaskarray of bool, same shape as image \n\nThe selected pixels.  \ndatadict \n\nThe data describing the histogram and the selected region. The dictionary contains:  \u2018bins\u2019 : array of float The bin boundaries for both a and b channels. \u2018hist\u2019 : 2D array of float The normalized histogram. \u2018edges\u2019 : tuple of array of float The bin edges along each dimension \u2018extents\u2019 : tuple of float The left and right and top and bottom of the selected region.      \n"}, {"name": "viewer.plugins.ColorHistogram.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.ColorHistogram.__init__", "type": "viewer", "text": " \n__init__(max_pct=0.99, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.plugins.Crop", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Crop", "type": "viewer", "text": " \nclass skimage.viewer.plugins.Crop(maxdist=10, **kwargs) [source]\n \nBases: skimage.viewer.plugins.base.Plugin  \n__init__(maxdist=10, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n  \ncrop(extents) [source]\n\n  \nhelp() [source]\n\n  \nname = 'Crop' \n  \nreset() [source]\n\n \n"}, {"name": "viewer.plugins.Crop.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Crop.attach", "type": "viewer", "text": " \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n"}, {"name": "viewer.plugins.Crop.crop()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Crop.crop", "type": "viewer", "text": " \ncrop(extents) [source]\n\n"}, {"name": "viewer.plugins.Crop.help()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Crop.help", "type": "viewer", "text": " \nhelp() [source]\n\n"}, {"name": "viewer.plugins.Crop.name", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Crop.name", "type": "viewer", "text": " \nname = 'Crop' \n"}, {"name": "viewer.plugins.Crop.reset()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Crop.reset", "type": "viewer", "text": " \nreset() [source]\n\n"}, {"name": "viewer.plugins.Crop.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Crop.__init__", "type": "viewer", "text": " \n__init__(maxdist=10, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.plugins.LabelPainter", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter", "type": "viewer", "text": " \nclass skimage.viewer.plugins.LabelPainter(max_radius=20, **kwargs) [source]\n \nBases: skimage.viewer.plugins.base.Plugin  \n__init__(max_radius=20, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n  \nhelp() [source]\n\n  \nproperty label \n  \nname = 'LabelPainter' \n  \non_enter(overlay) [source]\n\n  \nproperty radius \n \n"}, {"name": "viewer.plugins.LabelPainter.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter.attach", "type": "viewer", "text": " \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n"}, {"name": "viewer.plugins.LabelPainter.help()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter.help", "type": "viewer", "text": " \nhelp() [source]\n\n"}, {"name": "viewer.plugins.LabelPainter.label()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter.label", "type": "viewer", "text": " \nproperty label \n"}, {"name": "viewer.plugins.LabelPainter.name", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter.name", "type": "viewer", "text": " \nname = 'LabelPainter' \n"}, {"name": "viewer.plugins.LabelPainter.on_enter()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter.on_enter", "type": "viewer", "text": " \non_enter(overlay) [source]\n\n"}, {"name": "viewer.plugins.LabelPainter.radius()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter.radius", "type": "viewer", "text": " \nproperty radius \n"}, {"name": "viewer.plugins.LabelPainter.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter.__init__", "type": "viewer", "text": " \n__init__(max_radius=20, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.plugins.LineProfile", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile", "type": "viewer", "text": " \nclass skimage.viewer.plugins.LineProfile(maxdist=10, epsilon='deprecated', limits='image', **kwargs) [source]\n \nBases: skimage.viewer.plugins.plotplugin.PlotPlugin Plugin to compute interpolated intensity under a scan line on an image. See PlotPlugin and Plugin classes for additional details.  Parameters \n \nmaxdistfloat \n\nMaximum pixel distance allowed when selecting end point of scan line.  \nlimitstuple or {None, \u2018image\u2019, \u2018dtype\u2019} \n\n(minimum, maximum) intensity limits for plotted profile. The following special values are defined: None : rescale based on min/max intensity along selected scan line. \u2018image\u2019 : fixed scale based on min/max intensity in image. \u2018dtype\u2019 : fixed scale based on min/max intensity of image dtype.      \n__init__(maxdist=10, epsilon='deprecated', limits='image', **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n  \nget_profiles() [source]\n \nReturn intensity profile of the selected line.  Returns \n end_points: (2, 2) array\n\nThe positions ((x1, y1), (x2, y2)) of the line ends.  profile: list of 1d arrays\n\nProfile of intensity values. Length 1 (grayscale) or 3 (rgb).     \n  \nhelp() [source]\n\n  \nline_changed(end_points) [source]\n\n  \nname = 'Line Profile' \n  \noutput() [source]\n \nReturn the drawn line and the resulting scan.  Returns \n \nline_image(M, N) uint8 array, same shape as image \n\nAn array of 0s with the scanned line set to 255. If the linewidth of the line tool is greater than 1, sets the values within the profiled polygon to 128.  \nscan(P,) or (P, 3) array of int or float \n\nThe line scan values across the image.     \n  \nreset_axes(scan_data) [source]\n\n \n"}, {"name": "viewer.plugins.LineProfile.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.attach", "type": "viewer", "text": " \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n"}, {"name": "viewer.plugins.LineProfile.get_profiles()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.get_profiles", "type": "viewer", "text": " \nget_profiles() [source]\n \nReturn intensity profile of the selected line.  Returns \n end_points: (2, 2) array\n\nThe positions ((x1, y1), (x2, y2)) of the line ends.  profile: list of 1d arrays\n\nProfile of intensity values. Length 1 (grayscale) or 3 (rgb).     \n"}, {"name": "viewer.plugins.LineProfile.help()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.help", "type": "viewer", "text": " \nhelp() [source]\n\n"}, {"name": "viewer.plugins.LineProfile.line_changed()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.line_changed", "type": "viewer", "text": " \nline_changed(end_points) [source]\n\n"}, {"name": "viewer.plugins.LineProfile.name", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.name", "type": "viewer", "text": " \nname = 'Line Profile' \n"}, {"name": "viewer.plugins.LineProfile.output()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.output", "type": "viewer", "text": " \noutput() [source]\n \nReturn the drawn line and the resulting scan.  Returns \n \nline_image(M, N) uint8 array, same shape as image \n\nAn array of 0s with the scanned line set to 255. If the linewidth of the line tool is greater than 1, sets the values within the profiled polygon to 128.  \nscan(P,) or (P, 3) array of int or float \n\nThe line scan values across the image.     \n"}, {"name": "viewer.plugins.LineProfile.reset_axes()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.reset_axes", "type": "viewer", "text": " \nreset_axes(scan_data) [source]\n\n"}, {"name": "viewer.plugins.LineProfile.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.__init__", "type": "viewer", "text": " \n__init__(maxdist=10, epsilon='deprecated', limits='image', **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.plugins.Measure", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Measure", "type": "viewer", "text": " \nclass skimage.viewer.plugins.Measure(maxdist=10, **kwargs) [source]\n \nBases: skimage.viewer.plugins.base.Plugin  \n__init__(maxdist=10, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n  \nhelp() [source]\n\n  \nline_changed(end_points) [source]\n\n  \nname = 'Measure' \n \n"}, {"name": "viewer.plugins.Measure.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Measure.attach", "type": "viewer", "text": " \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n"}, {"name": "viewer.plugins.Measure.help()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Measure.help", "type": "viewer", "text": " \nhelp() [source]\n\n"}, {"name": "viewer.plugins.Measure.line_changed()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Measure.line_changed", "type": "viewer", "text": " \nline_changed(end_points) [source]\n\n"}, {"name": "viewer.plugins.Measure.name", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Measure.name", "type": "viewer", "text": " \nname = 'Measure' \n"}, {"name": "viewer.plugins.Measure.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Measure.__init__", "type": "viewer", "text": " \n__init__(maxdist=10, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.plugins.OverlayPlugin", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin", "type": "viewer", "text": " \nclass skimage.viewer.plugins.OverlayPlugin(**kwargs) [source]\n \nBases: skimage.viewer.plugins.base.Plugin Plugin for ImageViewer that displays an overlay on top of main image. The base Plugin class displays the filtered image directly on the viewer. OverlayPlugin will instead overlay an image with a transparent colormap. See base Plugin class for additional details.  Attributes \n \noverlayarray \n\nOverlay displayed on top of image. This overlay defaults to a color map with alpha values varying linearly from 0 to 1.  \ncolorint \n\nColor of overlay.      \n__init__(**kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n  \ncloseEvent(event) [source]\n \nOn close disconnect all artists and events from ImageViewer. Note that artists must be appended to self.artists. \n  \nproperty color \n  \ncolors = {'cyan': (0, 1, 1), 'green': (0, 1, 0), 'red': (1, 0, 0), 'yellow': (1, 1, 0)} \n  \ndisplay_filtered_image(image) [source]\n \nDisplay filtered image as an overlay on top of image in viewer. \n  \nproperty filtered_image  \nReturn filtered image. This \u201cfiltered image\u201d is used when saving from the plugin. \n  \noutput() [source]\n \nReturn the overlaid image.  Returns \n \noverlayarray, same shape as image \n\nThe overlay currently displayed.  \ndataNone \n   \n  \nproperty overlay \n \n"}, {"name": "viewer.plugins.OverlayPlugin.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.attach", "type": "viewer", "text": " \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n"}, {"name": "viewer.plugins.OverlayPlugin.closeEvent()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.closeEvent", "type": "viewer", "text": " \ncloseEvent(event) [source]\n \nOn close disconnect all artists and events from ImageViewer. Note that artists must be appended to self.artists. \n"}, {"name": "viewer.plugins.OverlayPlugin.color()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.color", "type": "viewer", "text": " \nproperty color \n"}, {"name": "viewer.plugins.OverlayPlugin.colors", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.colors", "type": "viewer", "text": " \ncolors = {'cyan': (0, 1, 1), 'green': (0, 1, 0), 'red': (1, 0, 0), 'yellow': (1, 1, 0)} \n"}, {"name": "viewer.plugins.OverlayPlugin.display_filtered_image()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.display_filtered_image", "type": "viewer", "text": " \ndisplay_filtered_image(image) [source]\n \nDisplay filtered image as an overlay on top of image in viewer. \n"}, {"name": "viewer.plugins.OverlayPlugin.filtered_image()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.filtered_image", "type": "viewer", "text": " \nproperty filtered_image  \nReturn filtered image. This \u201cfiltered image\u201d is used when saving from the plugin. \n"}, {"name": "viewer.plugins.OverlayPlugin.output()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.output", "type": "viewer", "text": " \noutput() [source]\n \nReturn the overlaid image.  Returns \n \noverlayarray, same shape as image \n\nThe overlay currently displayed.  \ndataNone \n   \n"}, {"name": "viewer.plugins.OverlayPlugin.overlay()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.overlay", "type": "viewer", "text": " \nproperty overlay \n"}, {"name": "viewer.plugins.OverlayPlugin.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.__init__", "type": "viewer", "text": " \n__init__(**kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.plugins.PlotPlugin", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.PlotPlugin", "type": "viewer", "text": " \nclass skimage.viewer.plugins.PlotPlugin(image_filter=None, height=150, width=400, **kwargs) [source]\n \nBases: skimage.viewer.plugins.base.Plugin Plugin for ImageViewer that contains a plot canvas. Base class for plugins that contain a Matplotlib plot canvas, which can, for example, display an image histogram. See base Plugin class for additional details.  \n__init__(image_filter=None, height=150, width=400, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nadd_plot() [source]\n\n  \nadd_tool(tool) [source]\n\n  \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n  \nredraw() [source]\n \nRedraw plot. \n  \nremove_tool(tool) [source]\n\n \n"}, {"name": "viewer.plugins.PlotPlugin.add_plot()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.PlotPlugin.add_plot", "type": "viewer", "text": " \nadd_plot() [source]\n\n"}, {"name": "viewer.plugins.PlotPlugin.add_tool()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.PlotPlugin.add_tool", "type": "viewer", "text": " \nadd_tool(tool) [source]\n\n"}, {"name": "viewer.plugins.PlotPlugin.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.PlotPlugin.attach", "type": "viewer", "text": " \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n"}, {"name": "viewer.plugins.PlotPlugin.redraw()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.PlotPlugin.redraw", "type": "viewer", "text": " \nredraw() [source]\n \nRedraw plot. \n"}, {"name": "viewer.plugins.PlotPlugin.remove_tool()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.PlotPlugin.remove_tool", "type": "viewer", "text": " \nremove_tool(tool) [source]\n\n"}, {"name": "viewer.plugins.PlotPlugin.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.PlotPlugin.__init__", "type": "viewer", "text": " \n__init__(image_filter=None, height=150, width=400, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.plugins.Plugin", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin", "type": "viewer", "text": " \nclass skimage.viewer.plugins.Plugin(image_filter=None, height=0, width=400, useblit=True, dock='bottom') [source]\n \nBases: object Base class for plugins that interact with an ImageViewer. A plugin connects an image filter (or another function) to an image viewer. Note that a Plugin is initialized without an image viewer and attached in a later step. See example below for details.  Parameters \n \nimage_viewerImageViewer \n\nWindow containing image used in measurement/manipulation.  \nimage_filterfunction \n\nFunction that gets called to update image in image viewer. This value can be None if, for example, you have a plugin that extracts information from an image and doesn\u2019t manipulate it. Alternatively, this function can be defined as a method in a Plugin subclass.  \nheight, widthint \n\nSize of plugin window in pixels. Note that Qt will automatically resize a window to fit components. So if you\u2019re adding rows of components, you can leave height = 0 and just let Qt determine the final height.  \nuseblitbool \n\nIf True, use blitting to speed up animation. Only available on some Matplotlib backends. If None, set to True when using Agg backend. This only has an effect if you draw on top of an image viewer.     Examples >>> from skimage.viewer import ImageViewer\n>>> from skimage.viewer.widgets import Slider\n>>> from skimage import data\n>>>\n>>> plugin = Plugin(image_filter=lambda img,\n...                 threshold: img > threshold) \n>>> plugin += Slider('threshold', 0, 255)       \n>>>\n>>> image = data.coins()\n>>> viewer = ImageViewer(image)       \n>>> viewer += plugin                  \n>>> thresholded = viewer.show()[0][0] \n The plugin will automatically delegate parameters to image_filter based on its parameter type, i.e., ptype (widgets for required arguments must be added in the order they appear in the function). The image attached to the viewer is automatically passed as the first argument to the filter function. #TODO: Add flag so image is not passed to filter function by default. ptype = \u2018kwarg\u2019 is the default for most widgets so it\u2019s unnecessary here.  Attributes \n \nimage_viewerImageViewer \n\nWindow containing image used in measurement.  \nnamestr \n\nName of plugin. This is displayed as the window title.  \nartistlist \n\nList of Matplotlib artists and canvastools. Any artists created by the plugin should be added to this list so that it gets cleaned up on close.      \n__init__(image_filter=None, height=0, width=400, useblit=True, dock='bottom') [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nadd_widget(widget) [source]\n \nAdd widget to plugin. Alternatively, Plugin\u2019s __add__ method is overloaded to add widgets: plugin += Widget(...)\n Widgets can adjust required or optional arguments of filter function or parameters for the plugin. This is specified by the Widget\u2019s ptype. \n  \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n  \nclean_up() [source]\n\n  \ncloseEvent(event) [source]\n \nOn close disconnect all artists and events from ImageViewer. Note that artists must be appended to self.artists. \n  \ndisplay_filtered_image(image) [source]\n \nDisplay the filtered image on image viewer. If you don\u2019t want to simply replace the displayed image with the filtered image (e.g., you want to display a transparent overlay), you can override this method. \n  \nfilter_image(*widget_arg) [source]\n \nCall image_filter with widget args and kwargs Note: display_filtered_image is automatically called. \n  \nproperty filtered_image  \nReturn filtered image. \n  \nimage_changed = None \n  \nimage_viewer = 'Plugin is not attached to ImageViewer' \n  \nname = 'Plugin' \n  \noutput() [source]\n \nReturn the plugin\u2019s representation and data.  Returns \n \nimagearray, same shape as self.image_viewer.image, or None \n\nThe filtered image.  \ndataNone \n\nAny data associated with the plugin.     Notes Derived classes should override this method to return a tuple containing an overlay of the same shape of the image, and a data object. Either of these is optional: return None if you don\u2019t want to return a value. \n  \nremove_image_artists() [source]\n \nRemove artists that are connected to the image viewer. \n  \nshow(main_window=True) [source]\n \nShow plugin. \n  \nupdate_plugin(name, value) [source]\n \nUpdate keyword parameters of the plugin itself. These parameters will typically be implemented as class properties so that they update the image or some other component. \n \n"}, {"name": "viewer.plugins.Plugin.add_widget()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.add_widget", "type": "viewer", "text": " \nadd_widget(widget) [source]\n \nAdd widget to plugin. Alternatively, Plugin\u2019s __add__ method is overloaded to add widgets: plugin += Widget(...)\n Widgets can adjust required or optional arguments of filter function or parameters for the plugin. This is specified by the Widget\u2019s ptype. \n"}, {"name": "viewer.plugins.Plugin.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.attach", "type": "viewer", "text": " \nattach(image_viewer) [source]\n \nAttach the plugin to an ImageViewer. Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example: viewer += Plugin(...)\n Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets. \n"}, {"name": "viewer.plugins.Plugin.clean_up()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.clean_up", "type": "viewer", "text": " \nclean_up() [source]\n\n"}, {"name": "viewer.plugins.Plugin.closeEvent()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.closeEvent", "type": "viewer", "text": " \ncloseEvent(event) [source]\n \nOn close disconnect all artists and events from ImageViewer. Note that artists must be appended to self.artists. \n"}, {"name": "viewer.plugins.Plugin.display_filtered_image()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.display_filtered_image", "type": "viewer", "text": " \ndisplay_filtered_image(image) [source]\n \nDisplay the filtered image on image viewer. If you don\u2019t want to simply replace the displayed image with the filtered image (e.g., you want to display a transparent overlay), you can override this method. \n"}, {"name": "viewer.plugins.Plugin.filtered_image()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.filtered_image", "type": "viewer", "text": " \nproperty filtered_image  \nReturn filtered image. \n"}, {"name": "viewer.plugins.Plugin.filter_image()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.filter_image", "type": "viewer", "text": " \nfilter_image(*widget_arg) [source]\n \nCall image_filter with widget args and kwargs Note: display_filtered_image is automatically called. \n"}, {"name": "viewer.plugins.Plugin.image_changed", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.image_changed", "type": "viewer", "text": " \nimage_changed = None \n"}, {"name": "viewer.plugins.Plugin.image_viewer", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.image_viewer", "type": "viewer", "text": " \nimage_viewer = 'Plugin is not attached to ImageViewer' \n"}, {"name": "viewer.plugins.Plugin.name", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.name", "type": "viewer", "text": " \nname = 'Plugin' \n"}, {"name": "viewer.plugins.Plugin.output()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.output", "type": "viewer", "text": " \noutput() [source]\n \nReturn the plugin\u2019s representation and data.  Returns \n \nimagearray, same shape as self.image_viewer.image, or None \n\nThe filtered image.  \ndataNone \n\nAny data associated with the plugin.     Notes Derived classes should override this method to return a tuple containing an overlay of the same shape of the image, and a data object. Either of these is optional: return None if you don\u2019t want to return a value. \n"}, {"name": "viewer.plugins.Plugin.remove_image_artists()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.remove_image_artists", "type": "viewer", "text": " \nremove_image_artists() [source]\n \nRemove artists that are connected to the image viewer. \n"}, {"name": "viewer.plugins.Plugin.show()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.show", "type": "viewer", "text": " \nshow(main_window=True) [source]\n \nShow plugin. \n"}, {"name": "viewer.plugins.Plugin.update_plugin()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.update_plugin", "type": "viewer", "text": " \nupdate_plugin(name, value) [source]\n \nUpdate keyword parameters of the plugin itself. These parameters will typically be implemented as class properties so that they update the image or some other component. \n"}, {"name": "viewer.plugins.Plugin.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.__init__", "type": "viewer", "text": " \n__init__(image_filter=None, height=0, width=400, useblit=True, dock='bottom') [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.utils", "path": "api/skimage.viewer.utils", "type": "viewer", "text": "Module: viewer.utils  \nskimage.viewer.utils.figimage(image[, \u2026]) Return figure and axes with figure tightly surrounding image.  \nskimage.viewer.utils.init_qtapp() Initialize QAppliction.  \nskimage.viewer.utils.new_plot([parent, \u2026]) Return new figure and axes.  \nskimage.viewer.utils.start_qtapp([app]) Start Qt mainloop  \nskimage.viewer.utils.update_axes_image(\u2026) Update the image displayed by an image plot.  \nskimage.viewer.utils.ClearColormap(rgb[, \u2026]) Color map that varies linearly from alpha = 0 to 1  \nskimage.viewer.utils.FigureCanvas(figure, \u2026) Canvas for displaying images.  \nskimage.viewer.utils.LinearColormap(name, \u2026) LinearSegmentedColormap in which color varies smoothly.  \nskimage.viewer.utils.RequiredAttr([init_val]) A class attribute that must be set before use.  \nskimage.viewer.utils.canvas   \nskimage.viewer.utils.core   \nskimage.viewer.utils.dialogs    figimage  \nskimage.viewer.utils.figimage(image, scale=1, dpi=None, **kwargs) [source]\n \nReturn figure and axes with figure tightly surrounding image. Unlike pyplot.figimage, this actually plots onto an axes object, which fills the figure. Plotting the image onto an axes allows for subsequent overlays of axes artists.  Parameters \n \nimagearray \n\nimage to plot  \nscalefloat \n\nIf scale is 1, the figure and axes have the same dimension as the image. Smaller values of scale will shrink the figure.  \ndpiint \n\nDots per inch for figure. If None, use the default rcParam.     \n init_qtapp  \nskimage.viewer.utils.init_qtapp() [source]\n \nInitialize QAppliction. The QApplication needs to be initialized before creating any QWidgets \n new_plot  \nskimage.viewer.utils.new_plot(parent=None, subplot_kw=None, **fig_kw) [source]\n \nReturn new figure and axes.  Parameters \n \nparentQtWidget \n\nQt widget that displays the plot objects. If None, you must manually call canvas.setParent and pass the parent widget.  \nsubplot_kwdict \n\nKeyword arguments passed matplotlib.figure.Figure.add_subplot.  \nfig_kwdict \n\nKeyword arguments passed matplotlib.figure.Figure.     \n start_qtapp  \nskimage.viewer.utils.start_qtapp(app=None) [source]\n \nStart Qt mainloop \n update_axes_image  \nskimage.viewer.utils.update_axes_image(image_axes, image) [source]\n \nUpdate the image displayed by an image plot. This sets the image plot\u2019s array and updates its shape appropriately  Parameters \n \nimage_axesmatplotlib.image.AxesImage \n\nImage axes to update.  \nimagearray \n\nImage array.     \n ClearColormap  \nclass skimage.viewer.utils.ClearColormap(rgb, max_alpha=1, name='clear_color') [source]\n \nBases: skimage.viewer.utils.core.LinearColormap Color map that varies linearly from alpha = 0 to 1  \n__init__(rgb, max_alpha=1, name='clear_color') [source]\n \nCreate color map from linear mapping segments segmentdata argument is a dictionary with a red, green and blue entries. Each entry should be a list of x, y0, y1 tuples, forming rows in a table. Entries for alpha are optional. Example: suppose you want red to increase from 0 to 1 over the bottom half, green to do the same over the middle half, and blue over the top half. Then you would use: cdict = {'red':   [(0.0,  0.0, 0.0),\n                   (0.5,  1.0, 1.0),\n                   (1.0,  1.0, 1.0)],\n\n         'green': [(0.0,  0.0, 0.0),\n                   (0.25, 0.0, 0.0),\n                   (0.75, 1.0, 1.0),\n                   (1.0,  1.0, 1.0)],\n\n         'blue':  [(0.0,  0.0, 0.0),\n                   (0.5,  0.0, 0.0),\n                   (1.0,  1.0, 1.0)]}\n Each row in the table for a given color is a sequence of x, y0, y1 tuples. In each sequence, x must increase monotonically from 0 to 1. For any input value z falling between x[i] and x[i+1], the output value of a given color will be linearly interpolated between y1[i] and y0[i+1]: row i:   x  y0  y1\n               /\n              /\nrow i+1: x  y0  y1\n Hence y0 in the first row and y1 in the last row are never used.  See also  \nLinearSegmentedColormap.from_list \n\nStatic method; factory function for generating a smoothly-varying LinearSegmentedColormap.  \nmakeMappingArray \n\nFor information about making a mapping array.    \n \n FigureCanvas  \nclass skimage.viewer.utils.FigureCanvas(figure, **kwargs) [source]\n \nBases: object Canvas for displaying images.  \n__init__(figure, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nresizeEvent(event) [source]\n\n \n LinearColormap  \nclass skimage.viewer.utils.LinearColormap(name, segmented_data, **kwargs) [source]\n \nBases: matplotlib.colors.LinearSegmentedColormap LinearSegmentedColormap in which color varies smoothly. This class is a simplification of LinearSegmentedColormap, which doesn\u2019t support jumps in color intensities.  Parameters \n \nnamestr \n\nName of colormap.  \nsegmented_datadict \n\nDictionary of \u2018red\u2019, \u2018green\u2019, \u2018blue\u2019, and (optionally) \u2018alpha\u2019 values. Each color key contains a list of x, y tuples. x must increase monotonically from 0 to 1 and corresponds to input values for a mappable object (e.g. an image). y corresponds to the color intensity.      \n__init__(name, segmented_data, **kwargs) [source]\n \nCreate color map from linear mapping segments segmentdata argument is a dictionary with a red, green and blue entries. Each entry should be a list of x, y0, y1 tuples, forming rows in a table. Entries for alpha are optional. Example: suppose you want red to increase from 0 to 1 over the bottom half, green to do the same over the middle half, and blue over the top half. Then you would use: cdict = {'red':   [(0.0,  0.0, 0.0),\n                   (0.5,  1.0, 1.0),\n                   (1.0,  1.0, 1.0)],\n\n         'green': [(0.0,  0.0, 0.0),\n                   (0.25, 0.0, 0.0),\n                   (0.75, 1.0, 1.0),\n                   (1.0,  1.0, 1.0)],\n\n         'blue':  [(0.0,  0.0, 0.0),\n                   (0.5,  0.0, 0.0),\n                   (1.0,  1.0, 1.0)]}\n Each row in the table for a given color is a sequence of x, y0, y1 tuples. In each sequence, x must increase monotonically from 0 to 1. For any input value z falling between x[i] and x[i+1], the output value of a given color will be linearly interpolated between y1[i] and y0[i+1]: row i:   x  y0  y1\n               /\n              /\nrow i+1: x  y0  y1\n Hence y0 in the first row and y1 in the last row are never used.  See also  \nLinearSegmentedColormap.from_list \n\nStatic method; factory function for generating a smoothly-varying LinearSegmentedColormap.  \nmakeMappingArray \n\nFor information about making a mapping array.    \n \n RequiredAttr  \nclass skimage.viewer.utils.RequiredAttr(init_val=None) [source]\n \nBases: object A class attribute that must be set before use.  \n__init__(init_val=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \ninstances = {(<skimage.viewer.utils.core.RequiredAttr object>, None): 'Widget is not attached to a Plugin.', (<skimage.viewer.utils.core.RequiredAttr object>, None): 'Plugin is not attached to ImageViewer'} \n \n\n"}, {"name": "viewer.utils.ClearColormap", "path": "api/skimage.viewer.utils#skimage.viewer.utils.ClearColormap", "type": "viewer", "text": " \nclass skimage.viewer.utils.ClearColormap(rgb, max_alpha=1, name='clear_color') [source]\n \nBases: skimage.viewer.utils.core.LinearColormap Color map that varies linearly from alpha = 0 to 1  \n__init__(rgb, max_alpha=1, name='clear_color') [source]\n \nCreate color map from linear mapping segments segmentdata argument is a dictionary with a red, green and blue entries. Each entry should be a list of x, y0, y1 tuples, forming rows in a table. Entries for alpha are optional. Example: suppose you want red to increase from 0 to 1 over the bottom half, green to do the same over the middle half, and blue over the top half. Then you would use: cdict = {'red':   [(0.0,  0.0, 0.0),\n                   (0.5,  1.0, 1.0),\n                   (1.0,  1.0, 1.0)],\n\n         'green': [(0.0,  0.0, 0.0),\n                   (0.25, 0.0, 0.0),\n                   (0.75, 1.0, 1.0),\n                   (1.0,  1.0, 1.0)],\n\n         'blue':  [(0.0,  0.0, 0.0),\n                   (0.5,  0.0, 0.0),\n                   (1.0,  1.0, 1.0)]}\n Each row in the table for a given color is a sequence of x, y0, y1 tuples. In each sequence, x must increase monotonically from 0 to 1. For any input value z falling between x[i] and x[i+1], the output value of a given color will be linearly interpolated between y1[i] and y0[i+1]: row i:   x  y0  y1\n               /\n              /\nrow i+1: x  y0  y1\n Hence y0 in the first row and y1 in the last row are never used.  See also  \nLinearSegmentedColormap.from_list \n\nStatic method; factory function for generating a smoothly-varying LinearSegmentedColormap.  \nmakeMappingArray \n\nFor information about making a mapping array.    \n \n"}, {"name": "viewer.utils.ClearColormap.__init__()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.ClearColormap.__init__", "type": "viewer", "text": " \n__init__(rgb, max_alpha=1, name='clear_color') [source]\n \nCreate color map from linear mapping segments segmentdata argument is a dictionary with a red, green and blue entries. Each entry should be a list of x, y0, y1 tuples, forming rows in a table. Entries for alpha are optional. Example: suppose you want red to increase from 0 to 1 over the bottom half, green to do the same over the middle half, and blue over the top half. Then you would use: cdict = {'red':   [(0.0,  0.0, 0.0),\n                   (0.5,  1.0, 1.0),\n                   (1.0,  1.0, 1.0)],\n\n         'green': [(0.0,  0.0, 0.0),\n                   (0.25, 0.0, 0.0),\n                   (0.75, 1.0, 1.0),\n                   (1.0,  1.0, 1.0)],\n\n         'blue':  [(0.0,  0.0, 0.0),\n                   (0.5,  0.0, 0.0),\n                   (1.0,  1.0, 1.0)]}\n Each row in the table for a given color is a sequence of x, y0, y1 tuples. In each sequence, x must increase monotonically from 0 to 1. For any input value z falling between x[i] and x[i+1], the output value of a given color will be linearly interpolated between y1[i] and y0[i+1]: row i:   x  y0  y1\n               /\n              /\nrow i+1: x  y0  y1\n Hence y0 in the first row and y1 in the last row are never used.  See also  \nLinearSegmentedColormap.from_list \n\nStatic method; factory function for generating a smoothly-varying LinearSegmentedColormap.  \nmakeMappingArray \n\nFor information about making a mapping array.    \n"}, {"name": "viewer.utils.figimage()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.figimage", "type": "viewer", "text": " \nskimage.viewer.utils.figimage(image, scale=1, dpi=None, **kwargs) [source]\n \nReturn figure and axes with figure tightly surrounding image. Unlike pyplot.figimage, this actually plots onto an axes object, which fills the figure. Plotting the image onto an axes allows for subsequent overlays of axes artists.  Parameters \n \nimagearray \n\nimage to plot  \nscalefloat \n\nIf scale is 1, the figure and axes have the same dimension as the image. Smaller values of scale will shrink the figure.  \ndpiint \n\nDots per inch for figure. If None, use the default rcParam.     \n"}, {"name": "viewer.utils.FigureCanvas", "path": "api/skimage.viewer.utils#skimage.viewer.utils.FigureCanvas", "type": "viewer", "text": " \nclass skimage.viewer.utils.FigureCanvas(figure, **kwargs) [source]\n \nBases: object Canvas for displaying images.  \n__init__(figure, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nresizeEvent(event) [source]\n\n \n"}, {"name": "viewer.utils.FigureCanvas.resizeEvent()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.FigureCanvas.resizeEvent", "type": "viewer", "text": " \nresizeEvent(event) [source]\n\n"}, {"name": "viewer.utils.FigureCanvas.__init__()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.FigureCanvas.__init__", "type": "viewer", "text": " \n__init__(figure, **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.utils.init_qtapp()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.init_qtapp", "type": "viewer", "text": " \nskimage.viewer.utils.init_qtapp() [source]\n \nInitialize QAppliction. The QApplication needs to be initialized before creating any QWidgets \n"}, {"name": "viewer.utils.LinearColormap", "path": "api/skimage.viewer.utils#skimage.viewer.utils.LinearColormap", "type": "viewer", "text": " \nclass skimage.viewer.utils.LinearColormap(name, segmented_data, **kwargs) [source]\n \nBases: matplotlib.colors.LinearSegmentedColormap LinearSegmentedColormap in which color varies smoothly. This class is a simplification of LinearSegmentedColormap, which doesn\u2019t support jumps in color intensities.  Parameters \n \nnamestr \n\nName of colormap.  \nsegmented_datadict \n\nDictionary of \u2018red\u2019, \u2018green\u2019, \u2018blue\u2019, and (optionally) \u2018alpha\u2019 values. Each color key contains a list of x, y tuples. x must increase monotonically from 0 to 1 and corresponds to input values for a mappable object (e.g. an image). y corresponds to the color intensity.      \n__init__(name, segmented_data, **kwargs) [source]\n \nCreate color map from linear mapping segments segmentdata argument is a dictionary with a red, green and blue entries. Each entry should be a list of x, y0, y1 tuples, forming rows in a table. Entries for alpha are optional. Example: suppose you want red to increase from 0 to 1 over the bottom half, green to do the same over the middle half, and blue over the top half. Then you would use: cdict = {'red':   [(0.0,  0.0, 0.0),\n                   (0.5,  1.0, 1.0),\n                   (1.0,  1.0, 1.0)],\n\n         'green': [(0.0,  0.0, 0.0),\n                   (0.25, 0.0, 0.0),\n                   (0.75, 1.0, 1.0),\n                   (1.0,  1.0, 1.0)],\n\n         'blue':  [(0.0,  0.0, 0.0),\n                   (0.5,  0.0, 0.0),\n                   (1.0,  1.0, 1.0)]}\n Each row in the table for a given color is a sequence of x, y0, y1 tuples. In each sequence, x must increase monotonically from 0 to 1. For any input value z falling between x[i] and x[i+1], the output value of a given color will be linearly interpolated between y1[i] and y0[i+1]: row i:   x  y0  y1\n               /\n              /\nrow i+1: x  y0  y1\n Hence y0 in the first row and y1 in the last row are never used.  See also  \nLinearSegmentedColormap.from_list \n\nStatic method; factory function for generating a smoothly-varying LinearSegmentedColormap.  \nmakeMappingArray \n\nFor information about making a mapping array.    \n \n"}, {"name": "viewer.utils.LinearColormap.__init__()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.LinearColormap.__init__", "type": "viewer", "text": " \n__init__(name, segmented_data, **kwargs) [source]\n \nCreate color map from linear mapping segments segmentdata argument is a dictionary with a red, green and blue entries. Each entry should be a list of x, y0, y1 tuples, forming rows in a table. Entries for alpha are optional. Example: suppose you want red to increase from 0 to 1 over the bottom half, green to do the same over the middle half, and blue over the top half. Then you would use: cdict = {'red':   [(0.0,  0.0, 0.0),\n                   (0.5,  1.0, 1.0),\n                   (1.0,  1.0, 1.0)],\n\n         'green': [(0.0,  0.0, 0.0),\n                   (0.25, 0.0, 0.0),\n                   (0.75, 1.0, 1.0),\n                   (1.0,  1.0, 1.0)],\n\n         'blue':  [(0.0,  0.0, 0.0),\n                   (0.5,  0.0, 0.0),\n                   (1.0,  1.0, 1.0)]}\n Each row in the table for a given color is a sequence of x, y0, y1 tuples. In each sequence, x must increase monotonically from 0 to 1. For any input value z falling between x[i] and x[i+1], the output value of a given color will be linearly interpolated between y1[i] and y0[i+1]: row i:   x  y0  y1\n               /\n              /\nrow i+1: x  y0  y1\n Hence y0 in the first row and y1 in the last row are never used.  See also  \nLinearSegmentedColormap.from_list \n\nStatic method; factory function for generating a smoothly-varying LinearSegmentedColormap.  \nmakeMappingArray \n\nFor information about making a mapping array.    \n"}, {"name": "viewer.utils.new_plot()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.new_plot", "type": "viewer", "text": " \nskimage.viewer.utils.new_plot(parent=None, subplot_kw=None, **fig_kw) [source]\n \nReturn new figure and axes.  Parameters \n \nparentQtWidget \n\nQt widget that displays the plot objects. If None, you must manually call canvas.setParent and pass the parent widget.  \nsubplot_kwdict \n\nKeyword arguments passed matplotlib.figure.Figure.add_subplot.  \nfig_kwdict \n\nKeyword arguments passed matplotlib.figure.Figure.     \n"}, {"name": "viewer.utils.RequiredAttr", "path": "api/skimage.viewer.utils#skimage.viewer.utils.RequiredAttr", "type": "viewer", "text": " \nclass skimage.viewer.utils.RequiredAttr(init_val=None) [source]\n \nBases: object A class attribute that must be set before use.  \n__init__(init_val=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \ninstances = {(<skimage.viewer.utils.core.RequiredAttr object>, None): 'Widget is not attached to a Plugin.', (<skimage.viewer.utils.core.RequiredAttr object>, None): 'Plugin is not attached to ImageViewer'} \n \n"}, {"name": "viewer.utils.RequiredAttr.instances", "path": "api/skimage.viewer.utils#skimage.viewer.utils.RequiredAttr.instances", "type": "viewer", "text": " \ninstances = {(<skimage.viewer.utils.core.RequiredAttr object>, None): 'Widget is not attached to a Plugin.', (<skimage.viewer.utils.core.RequiredAttr object>, None): 'Plugin is not attached to ImageViewer'} \n"}, {"name": "viewer.utils.RequiredAttr.__init__()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.RequiredAttr.__init__", "type": "viewer", "text": " \n__init__(init_val=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.utils.start_qtapp()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.start_qtapp", "type": "viewer", "text": " \nskimage.viewer.utils.start_qtapp(app=None) [source]\n \nStart Qt mainloop \n"}, {"name": "viewer.utils.update_axes_image()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.update_axes_image", "type": "viewer", "text": " \nskimage.viewer.utils.update_axes_image(image_axes, image) [source]\n \nUpdate the image displayed by an image plot. This sets the image plot\u2019s array and updates its shape appropriately  Parameters \n \nimage_axesmatplotlib.image.AxesImage \n\nImage axes to update.  \nimagearray \n\nImage array.     \n"}, {"name": "viewer.viewers", "path": "api/skimage.viewer.viewers", "type": "viewer", "text": "Module: viewer.viewers  \nskimage.viewer.viewers.CollectionViewer(\u2026) Viewer for displaying image collections.  \nskimage.viewer.viewers.ImageViewer(image[, \u2026]) Viewer for displaying images.  \nskimage.viewer.viewers.core ImageViewer class for viewing and interacting with images.   CollectionViewer  \nclass skimage.viewer.viewers.CollectionViewer(image_collection, update_on='move', **kwargs) [source]\n \nBases: skimage.viewer.viewers.core.ImageViewer Viewer for displaying image collections. Select the displayed frame of the image collection using the slider or with the following keyboard shortcuts:  left/right arrows\n\nPrevious/next image in collection.  number keys, 0\u20139\n\n0% to 90% of collection. For example, \u201c5\u201d goes to the image in the middle (i.e. 50%) of the collection.  home/end keys\n\nFirst/last image in collection.    Parameters \n \nimage_collectionlist of images \n\nList of images to be displayed.  \nupdate_on{\u2018move\u2019 | \u2018release\u2019} \n\nControl whether image is updated on slide or release of the image slider. Using \u2018on_release\u2019 will give smoother behavior when displaying large images or when writing a plugin/subclass that requires heavy computation.      \n__init__(image_collection, update_on='move', **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nkeyPressEvent(event) [source]\n\n  \nupdate_index(name, index) [source]\n \nSelect image on display using index into image collection. \n \n ImageViewer  \nclass skimage.viewer.viewers.ImageViewer(image, useblit=True) [source]\n \nBases: object Viewer for displaying images. This viewer is a simple container object that holds a Matplotlib axes for showing images. ImageViewer doesn\u2019t subclass the Matplotlib axes (or figure) because of the high probability of name collisions. Subclasses and plugins will likely extend the update_image method to add custom overlays or filter the displayed image.  Parameters \n \nimagearray \n\nImage being viewed.     Examples >>> from skimage import data\n>>> image = data.coins()\n>>> viewer = ImageViewer(image) \n>>> viewer.show()               \n  Attributes \n \ncanvas, fig, axMatplotlib canvas, figure, and axes \n\nMatplotlib canvas, figure, and axes used to display image.  \nimagearray \n\nImage being viewed. Setting this value will update the displayed frame.  \noriginal_imagearray \n\nPlugins typically operate on (but don\u2019t change) the original image.  \npluginslist \n\nList of attached plugins.      \n__init__(image, useblit=True) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nadd_tool(tool) [source]\n\n  \ncloseEvent(event) [source]\n\n  \nconnect_event(event, callback) [source]\n \nConnect callback function to matplotlib event and return id. \n  \ndisconnect_event(callback_id) [source]\n \nDisconnect callback by its id (returned by connect_event). \n  \ndock_areas = {'bottom': None, 'left': None, 'right': None, 'top': None} \n  \nproperty image \n  \nopen_file(filename=None) [source]\n \nOpen image file and display in viewer. \n  \noriginal_image_changed = None \n  \nredraw() [source]\n\n  \nremove_tool(tool) [source]\n\n  \nreset_image() [source]\n\n  \nsave_to_file(filename=None) [source]\n \nSave current image to file. The current behavior is not ideal: It saves the image displayed on screen, so all images will be converted to RGB, and the image size is not preserved (resizing the viewer window will alter the size of the saved image). \n  \nshow(main_window=True) [source]\n \nShow ImageViewer and attached plugins. This behaves much like matplotlib.pyplot.show and QWidget.show. \n  \nupdate_image(image) [source]\n \nUpdate displayed image. This method can be overridden or extended in subclasses and plugins to react to image changes. \n \n\n"}, {"name": "viewer.viewers.CollectionViewer", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.CollectionViewer", "type": "viewer", "text": " \nclass skimage.viewer.viewers.CollectionViewer(image_collection, update_on='move', **kwargs) [source]\n \nBases: skimage.viewer.viewers.core.ImageViewer Viewer for displaying image collections. Select the displayed frame of the image collection using the slider or with the following keyboard shortcuts:  left/right arrows\n\nPrevious/next image in collection.  number keys, 0\u20139\n\n0% to 90% of collection. For example, \u201c5\u201d goes to the image in the middle (i.e. 50%) of the collection.  home/end keys\n\nFirst/last image in collection.    Parameters \n \nimage_collectionlist of images \n\nList of images to be displayed.  \nupdate_on{\u2018move\u2019 | \u2018release\u2019} \n\nControl whether image is updated on slide or release of the image slider. Using \u2018on_release\u2019 will give smoother behavior when displaying large images or when writing a plugin/subclass that requires heavy computation.      \n__init__(image_collection, update_on='move', **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nkeyPressEvent(event) [source]\n\n  \nupdate_index(name, index) [source]\n \nSelect image on display using index into image collection. \n \n"}, {"name": "viewer.viewers.CollectionViewer.keyPressEvent()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.CollectionViewer.keyPressEvent", "type": "viewer", "text": " \nkeyPressEvent(event) [source]\n\n"}, {"name": "viewer.viewers.CollectionViewer.update_index()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.CollectionViewer.update_index", "type": "viewer", "text": " \nupdate_index(name, index) [source]\n \nSelect image on display using index into image collection. \n"}, {"name": "viewer.viewers.CollectionViewer.__init__()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.CollectionViewer.__init__", "type": "viewer", "text": " \n__init__(image_collection, update_on='move', **kwargs) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.viewers.ImageViewer", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer", "type": "viewer", "text": " \nclass skimage.viewer.viewers.ImageViewer(image, useblit=True) [source]\n \nBases: object Viewer for displaying images. This viewer is a simple container object that holds a Matplotlib axes for showing images. ImageViewer doesn\u2019t subclass the Matplotlib axes (or figure) because of the high probability of name collisions. Subclasses and plugins will likely extend the update_image method to add custom overlays or filter the displayed image.  Parameters \n \nimagearray \n\nImage being viewed.     Examples >>> from skimage import data\n>>> image = data.coins()\n>>> viewer = ImageViewer(image) \n>>> viewer.show()               \n  Attributes \n \ncanvas, fig, axMatplotlib canvas, figure, and axes \n\nMatplotlib canvas, figure, and axes used to display image.  \nimagearray \n\nImage being viewed. Setting this value will update the displayed frame.  \noriginal_imagearray \n\nPlugins typically operate on (but don\u2019t change) the original image.  \npluginslist \n\nList of attached plugins.      \n__init__(image, useblit=True) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nadd_tool(tool) [source]\n\n  \ncloseEvent(event) [source]\n\n  \nconnect_event(event, callback) [source]\n \nConnect callback function to matplotlib event and return id. \n  \ndisconnect_event(callback_id) [source]\n \nDisconnect callback by its id (returned by connect_event). \n  \ndock_areas = {'bottom': None, 'left': None, 'right': None, 'top': None} \n  \nproperty image \n  \nopen_file(filename=None) [source]\n \nOpen image file and display in viewer. \n  \noriginal_image_changed = None \n  \nredraw() [source]\n\n  \nremove_tool(tool) [source]\n\n  \nreset_image() [source]\n\n  \nsave_to_file(filename=None) [source]\n \nSave current image to file. The current behavior is not ideal: It saves the image displayed on screen, so all images will be converted to RGB, and the image size is not preserved (resizing the viewer window will alter the size of the saved image). \n  \nshow(main_window=True) [source]\n \nShow ImageViewer and attached plugins. This behaves much like matplotlib.pyplot.show and QWidget.show. \n  \nupdate_image(image) [source]\n \nUpdate displayed image. This method can be overridden or extended in subclasses and plugins to react to image changes. \n \n"}, {"name": "viewer.viewers.ImageViewer.add_tool()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.add_tool", "type": "viewer", "text": " \nadd_tool(tool) [source]\n\n"}, {"name": "viewer.viewers.ImageViewer.closeEvent()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.closeEvent", "type": "viewer", "text": " \ncloseEvent(event) [source]\n\n"}, {"name": "viewer.viewers.ImageViewer.connect_event()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.connect_event", "type": "viewer", "text": " \nconnect_event(event, callback) [source]\n \nConnect callback function to matplotlib event and return id. \n"}, {"name": "viewer.viewers.ImageViewer.disconnect_event()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.disconnect_event", "type": "viewer", "text": " \ndisconnect_event(callback_id) [source]\n \nDisconnect callback by its id (returned by connect_event). \n"}, {"name": "viewer.viewers.ImageViewer.dock_areas", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.dock_areas", "type": "viewer", "text": " \ndock_areas = {'bottom': None, 'left': None, 'right': None, 'top': None} \n"}, {"name": "viewer.viewers.ImageViewer.image()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.image", "type": "viewer", "text": " \nproperty image \n"}, {"name": "viewer.viewers.ImageViewer.open_file()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.open_file", "type": "viewer", "text": " \nopen_file(filename=None) [source]\n \nOpen image file and display in viewer. \n"}, {"name": "viewer.viewers.ImageViewer.original_image_changed", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.original_image_changed", "type": "viewer", "text": " \noriginal_image_changed = None \n"}, {"name": "viewer.viewers.ImageViewer.redraw()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.redraw", "type": "viewer", "text": " \nredraw() [source]\n\n"}, {"name": "viewer.viewers.ImageViewer.remove_tool()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.remove_tool", "type": "viewer", "text": " \nremove_tool(tool) [source]\n\n"}, {"name": "viewer.viewers.ImageViewer.reset_image()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.reset_image", "type": "viewer", "text": " \nreset_image() [source]\n\n"}, {"name": "viewer.viewers.ImageViewer.save_to_file()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.save_to_file", "type": "viewer", "text": " \nsave_to_file(filename=None) [source]\n \nSave current image to file. The current behavior is not ideal: It saves the image displayed on screen, so all images will be converted to RGB, and the image size is not preserved (resizing the viewer window will alter the size of the saved image). \n"}, {"name": "viewer.viewers.ImageViewer.show()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.show", "type": "viewer", "text": " \nshow(main_window=True) [source]\n \nShow ImageViewer and attached plugins. This behaves much like matplotlib.pyplot.show and QWidget.show. \n"}, {"name": "viewer.viewers.ImageViewer.update_image()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.update_image", "type": "viewer", "text": " \nupdate_image(image) [source]\n \nUpdate displayed image. This method can be overridden or extended in subclasses and plugins to react to image changes. \n"}, {"name": "viewer.viewers.ImageViewer.__init__()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.__init__", "type": "viewer", "text": " \n__init__(image, useblit=True) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.widgets", "path": "api/skimage.viewer.widgets", "type": "viewer", "text": "Module: viewer.widgets Widgets for interacting with ImageViewer. These widgets should be added to a Plugin subclass using its add_widget method or calling: plugin += Widget(...)\n on a Plugin instance. The Plugin will delegate action based on the widget\u2019s parameter type specified by its ptype attribute, which can be: 'arg' : positional argument passed to Plugin's `filter_image` method.\n'kwarg' : keyword argument passed to Plugin's `filter_image` method.\n'plugin' : attribute of Plugin. You'll probably need to add a class\n    property of the same name that updates the display.\n  \nskimage.viewer.widgets.BaseWidget(name[, \u2026])   \nskimage.viewer.widgets.Button(name, callback) Button which calls callback upon click.  \nskimage.viewer.widgets.CheckBox(name[, \u2026]) CheckBox widget  \nskimage.viewer.widgets.ComboBox(name, items) ComboBox widget for selecting among a list of choices.  \nskimage.viewer.widgets.OKCancelButtons([\u2026]) Buttons that close the parent plugin.  \nskimage.viewer.widgets.SaveButtons([name, \u2026]) Buttons to save image to io.stack or to a file.  \nskimage.viewer.widgets.Slider(name[, low, \u2026]) Slider widget for adjusting numeric parameters.  \nskimage.viewer.widgets.Text([name, text])   \nskimage.viewer.widgets.core   \nskimage.viewer.widgets.history    BaseWidget  \nclass skimage.viewer.widgets.BaseWidget(name, ptype=None, callback=None) [source]\n \nBases: object  \n__init__(name, ptype=None, callback=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nplugin = 'Widget is not attached to a Plugin.' \n  \nproperty val \n \n Button  \nclass skimage.viewer.widgets.Button(name, callback) [source]\n \nBases: skimage.viewer.widgets.core.BaseWidget Button which calls callback upon click.  Parameters \n \nnamestr \n\nName of button.  \ncallbackcallable f() \n\nFunction to call when button is clicked.      \n__init__(name, callback) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n \n CheckBox  \nclass skimage.viewer.widgets.CheckBox(name, value=False, alignment='center', ptype='kwarg', callback=None) [source]\n \nBases: skimage.viewer.widgets.core.BaseWidget CheckBox widget  Parameters \n \nnamestr \n\nName of CheckBox parameter. If this parameter is passed as a keyword argument, it must match the name of that keyword argument (spaces are replaced with underscores). In addition, this name is displayed as the name of the CheckBox.  value: {False, True}, optional\n\nInitial state of the CheckBox.  alignment: {\u2018center\u2019,\u2019left\u2019,\u2019right\u2019}, optional\n\nCheckbox alignment  \nptype{\u2018arg\u2019 | \u2018kwarg\u2019 | \u2018plugin\u2019}, optional \n\nParameter type  \ncallbackcallable f(widget_name, value), optional \n\nCallback function called in response to checkbox changes. Note: This function is typically set (overridden) when the widget is added to a plugin.      \n__init__(name, value=False, alignment='center', ptype='kwarg', callback=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nproperty val \n \n ComboBox  \nclass skimage.viewer.widgets.ComboBox(name, items, ptype='kwarg', callback=None) [source]\n \nBases: skimage.viewer.widgets.core.BaseWidget ComboBox widget for selecting among a list of choices.  Parameters \n \nnamestr \n\nName of ComboBox parameter. If this parameter is passed as a keyword argument, it must match the name of that keyword argument (spaces are replaced with underscores). In addition, this name is displayed as the name of the ComboBox.  items: list of str\n\nAllowed parameter values.  \nptype{\u2018arg\u2019 | \u2018kwarg\u2019 | \u2018plugin\u2019}, optional \n\nParameter type.  \ncallbackcallable f(widget_name, value), optional \n\nCallback function called in response to combobox changes. Note: This function is typically set (overridden) when the widget is added to a plugin.      \n__init__(name, items, ptype='kwarg', callback=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nproperty index \n  \nproperty val \n \n OKCancelButtons  \nclass skimage.viewer.widgets.OKCancelButtons(button_width=80) [source]\n \nBases: skimage.viewer.widgets.core.BaseWidget Buttons that close the parent plugin. OK will replace the original image with the current (filtered) image. Cancel will just close the plugin.  \n__init__(button_width=80) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nclose_plugin() [source]\n\n  \nupdate_original_image() [source]\n\n \n SaveButtons  \nclass skimage.viewer.widgets.SaveButtons(name='Save to:', default_format='png') [source]\n \nBases: skimage.viewer.widgets.core.BaseWidget Buttons to save image to io.stack or to a file.  \n__init__(name='Save to:', default_format='png') [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nsave_to_file(filename=None) [source]\n\n  \nsave_to_stack() [source]\n\n \n Slider  \nclass skimage.viewer.widgets.Slider(name, low=0.0, high=1.0, value=None, value_type='float', ptype='kwarg', callback=None, max_edit_width=60, orientation='horizontal', update_on='release') [source]\n \nBases: skimage.viewer.widgets.core.BaseWidget Slider widget for adjusting numeric parameters.  Parameters \n \nnamestr \n\nName of slider parameter. If this parameter is passed as a keyword argument, it must match the name of that keyword argument (spaces are replaced with underscores). In addition, this name is displayed as the name of the slider.  \nlow, highfloat \n\nRange of slider values.  \nvaluefloat \n\nDefault slider value. If None, use midpoint between low and high.  \nvalue_type{\u2018float\u2019 | \u2018int\u2019}, optional \n\nNumeric type of slider value.  \nptype{\u2018kwarg\u2019 | \u2018arg\u2019 | \u2018plugin\u2019}, optional \n\nParameter type.  \ncallbackcallable f(widget_name, value), optional \n\nCallback function called in response to slider changes. Note: This function is typically set (overridden) when the widget is added to a plugin.  \norientation{\u2018horizontal\u2019 | \u2018vertical\u2019}, optional \n\nSlider orientation.  \nupdate_on{\u2018release\u2019 | \u2018move\u2019}, optional \n\nControl when callback function is called: on slider move or release.      \n__init__(name, low=0.0, high=1.0, value=None, value_type='float', ptype='kwarg', callback=None, max_edit_width=60, orientation='horizontal', update_on='release') [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nproperty val \n \n Text  \nclass skimage.viewer.widgets.Text(name=None, text='') [source]\n \nBases: skimage.viewer.widgets.core.BaseWidget  \n__init__(name=None, text='') [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nproperty text \n \n\n"}, {"name": "viewer.widgets.BaseWidget", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.BaseWidget", "type": "viewer", "text": " \nclass skimage.viewer.widgets.BaseWidget(name, ptype=None, callback=None) [source]\n \nBases: object  \n__init__(name, ptype=None, callback=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nplugin = 'Widget is not attached to a Plugin.' \n  \nproperty val \n \n"}, {"name": "viewer.widgets.BaseWidget.plugin", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.BaseWidget.plugin", "type": "viewer", "text": " \nplugin = 'Widget is not attached to a Plugin.' \n"}, {"name": "viewer.widgets.BaseWidget.val()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.BaseWidget.val", "type": "viewer", "text": " \nproperty val \n"}, {"name": "viewer.widgets.BaseWidget.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.BaseWidget.__init__", "type": "viewer", "text": " \n__init__(name, ptype=None, callback=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.widgets.Button", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Button", "type": "viewer", "text": " \nclass skimage.viewer.widgets.Button(name, callback) [source]\n \nBases: skimage.viewer.widgets.core.BaseWidget Button which calls callback upon click.  Parameters \n \nnamestr \n\nName of button.  \ncallbackcallable f() \n\nFunction to call when button is clicked.      \n__init__(name, callback) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n \n"}, {"name": "viewer.widgets.Button.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Button.__init__", "type": "viewer", "text": " \n__init__(name, callback) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.widgets.CheckBox", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.CheckBox", "type": "viewer", "text": " \nclass skimage.viewer.widgets.CheckBox(name, value=False, alignment='center', ptype='kwarg', callback=None) [source]\n \nBases: skimage.viewer.widgets.core.BaseWidget CheckBox widget  Parameters \n \nnamestr \n\nName of CheckBox parameter. If this parameter is passed as a keyword argument, it must match the name of that keyword argument (spaces are replaced with underscores). In addition, this name is displayed as the name of the CheckBox.  value: {False, True}, optional\n\nInitial state of the CheckBox.  alignment: {\u2018center\u2019,\u2019left\u2019,\u2019right\u2019}, optional\n\nCheckbox alignment  \nptype{\u2018arg\u2019 | \u2018kwarg\u2019 | \u2018plugin\u2019}, optional \n\nParameter type  \ncallbackcallable f(widget_name, value), optional \n\nCallback function called in response to checkbox changes. Note: This function is typically set (overridden) when the widget is added to a plugin.      \n__init__(name, value=False, alignment='center', ptype='kwarg', callback=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nproperty val \n \n"}, {"name": "viewer.widgets.CheckBox.val()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.CheckBox.val", "type": "viewer", "text": " \nproperty val \n"}, {"name": "viewer.widgets.CheckBox.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.CheckBox.__init__", "type": "viewer", "text": " \n__init__(name, value=False, alignment='center', ptype='kwarg', callback=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.widgets.ComboBox", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.ComboBox", "type": "viewer", "text": " \nclass skimage.viewer.widgets.ComboBox(name, items, ptype='kwarg', callback=None) [source]\n \nBases: skimage.viewer.widgets.core.BaseWidget ComboBox widget for selecting among a list of choices.  Parameters \n \nnamestr \n\nName of ComboBox parameter. If this parameter is passed as a keyword argument, it must match the name of that keyword argument (spaces are replaced with underscores). In addition, this name is displayed as the name of the ComboBox.  items: list of str\n\nAllowed parameter values.  \nptype{\u2018arg\u2019 | \u2018kwarg\u2019 | \u2018plugin\u2019}, optional \n\nParameter type.  \ncallbackcallable f(widget_name, value), optional \n\nCallback function called in response to combobox changes. Note: This function is typically set (overridden) when the widget is added to a plugin.      \n__init__(name, items, ptype='kwarg', callback=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nproperty index \n  \nproperty val \n \n"}, {"name": "viewer.widgets.ComboBox.index()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.ComboBox.index", "type": "viewer", "text": " \nproperty index \n"}, {"name": "viewer.widgets.ComboBox.val()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.ComboBox.val", "type": "viewer", "text": " \nproperty val \n"}, {"name": "viewer.widgets.ComboBox.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.ComboBox.__init__", "type": "viewer", "text": " \n__init__(name, items, ptype='kwarg', callback=None) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.widgets.OKCancelButtons", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.OKCancelButtons", "type": "viewer", "text": " \nclass skimage.viewer.widgets.OKCancelButtons(button_width=80) [source]\n \nBases: skimage.viewer.widgets.core.BaseWidget Buttons that close the parent plugin. OK will replace the original image with the current (filtered) image. Cancel will just close the plugin.  \n__init__(button_width=80) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nclose_plugin() [source]\n\n  \nupdate_original_image() [source]\n\n \n"}, {"name": "viewer.widgets.OKCancelButtons.close_plugin()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.OKCancelButtons.close_plugin", "type": "viewer", "text": " \nclose_plugin() [source]\n\n"}, {"name": "viewer.widgets.OKCancelButtons.update_original_image()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.OKCancelButtons.update_original_image", "type": "viewer", "text": " \nupdate_original_image() [source]\n\n"}, {"name": "viewer.widgets.OKCancelButtons.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.OKCancelButtons.__init__", "type": "viewer", "text": " \n__init__(button_width=80) [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.widgets.SaveButtons", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.SaveButtons", "type": "viewer", "text": " \nclass skimage.viewer.widgets.SaveButtons(name='Save to:', default_format='png') [source]\n \nBases: skimage.viewer.widgets.core.BaseWidget Buttons to save image to io.stack or to a file.  \n__init__(name='Save to:', default_format='png') [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nsave_to_file(filename=None) [source]\n\n  \nsave_to_stack() [source]\n\n \n"}, {"name": "viewer.widgets.SaveButtons.save_to_file()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.SaveButtons.save_to_file", "type": "viewer", "text": " \nsave_to_file(filename=None) [source]\n\n"}, {"name": "viewer.widgets.SaveButtons.save_to_stack()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.SaveButtons.save_to_stack", "type": "viewer", "text": " \nsave_to_stack() [source]\n\n"}, {"name": "viewer.widgets.SaveButtons.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.SaveButtons.__init__", "type": "viewer", "text": " \n__init__(name='Save to:', default_format='png') [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.widgets.Slider", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Slider", "type": "viewer", "text": " \nclass skimage.viewer.widgets.Slider(name, low=0.0, high=1.0, value=None, value_type='float', ptype='kwarg', callback=None, max_edit_width=60, orientation='horizontal', update_on='release') [source]\n \nBases: skimage.viewer.widgets.core.BaseWidget Slider widget for adjusting numeric parameters.  Parameters \n \nnamestr \n\nName of slider parameter. If this parameter is passed as a keyword argument, it must match the name of that keyword argument (spaces are replaced with underscores). In addition, this name is displayed as the name of the slider.  \nlow, highfloat \n\nRange of slider values.  \nvaluefloat \n\nDefault slider value. If None, use midpoint between low and high.  \nvalue_type{\u2018float\u2019 | \u2018int\u2019}, optional \n\nNumeric type of slider value.  \nptype{\u2018kwarg\u2019 | \u2018arg\u2019 | \u2018plugin\u2019}, optional \n\nParameter type.  \ncallbackcallable f(widget_name, value), optional \n\nCallback function called in response to slider changes. Note: This function is typically set (overridden) when the widget is added to a plugin.  \norientation{\u2018horizontal\u2019 | \u2018vertical\u2019}, optional \n\nSlider orientation.  \nupdate_on{\u2018release\u2019 | \u2018move\u2019}, optional \n\nControl when callback function is called: on slider move or release.      \n__init__(name, low=0.0, high=1.0, value=None, value_type='float', ptype='kwarg', callback=None, max_edit_width=60, orientation='horizontal', update_on='release') [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nproperty val \n \n"}, {"name": "viewer.widgets.Slider.val()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Slider.val", "type": "viewer", "text": " \nproperty val \n"}, {"name": "viewer.widgets.Slider.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Slider.__init__", "type": "viewer", "text": " \n__init__(name, low=0.0, high=1.0, value=None, value_type='float', ptype='kwarg', callback=None, max_edit_width=60, orientation='horizontal', update_on='release') [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}, {"name": "viewer.widgets.Text", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Text", "type": "viewer", "text": " \nclass skimage.viewer.widgets.Text(name=None, text='') [source]\n \nBases: skimage.viewer.widgets.core.BaseWidget  \n__init__(name=None, text='') [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n  \nproperty text \n \n"}, {"name": "viewer.widgets.Text.text()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Text.text", "type": "viewer", "text": " \nproperty text \n"}, {"name": "viewer.widgets.Text.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Text.__init__", "type": "viewer", "text": " \n__init__(name=None, text='') [source]\n \nInitialize self. See help(type(self)) for accurate signature. \n"}]